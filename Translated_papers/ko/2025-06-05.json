[
  {
    "paper": {
      "id": "2506.03569",
      "authors": [
        {
          "_id": "6841003e45e7d8a890731765",
          "name": "Xiaomi LLM-Core Team",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731767",
          "name": "Zihao Yue",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731768",
          "name": "Zhenru Lin",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731769",
          "name": "Yifan Song",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073176a",
          "name": "Weikun Wang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073176b",
          "user": {
            "_id": "60d2e681b8448e1785bbda06",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1624434302056-noauth.jpeg",
            "isPro": false,
            "fullname": "Shuhuai Ren",
            "user": "ShuhuaiRen",
            "type": "user"
          },
          "name": "Shuhuai Ren",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:27:00.497Z",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073176c",
          "user": {
            "_id": "642e72cec1b0f8e4e76af16d",
            "avatarUrl": "/avatars/f900811d3c22a114c67283b646949f86.svg",
            "isPro": false,
            "fullname": "shuhao gu",
            "user": "gsh33",
            "type": "user"
          },
          "name": "Shuhao Gu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:27:04.948Z",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073176d",
          "name": "Shicheng Li",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073176e",
          "name": "Peidian Li",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073176f",
          "name": "Liang Zhao",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731770",
          "user": {
            "_id": "6038d6d0612f5eef3cc05ea9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6038d6d0612f5eef3cc05ea9/ryhvAX5djQpD5OrIlZQ1f.jpeg",
            "isPro": false,
            "fullname": "Lei Li",
            "user": "tobiaslee",
            "type": "user"
          },
          "name": "Lei Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:27:07.044Z",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731771",
          "name": "Kainan Bao",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731772",
          "name": "Hao Tian",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731773",
          "name": "Hailin Zhang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731774",
          "name": "Gang Wang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731775",
          "user": {
            "_id": "64d2fce8129a210e569e0c76",
            "avatarUrl": "/avatars/a79a832dc3a46ece1b9e542369fc4888.svg",
            "isPro": false,
            "fullname": "Dawei Zhu",
            "user": "dwzhu",
            "type": "user"
          },
          "name": "Dawei Zhu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:27:02.720Z",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731776",
          "name": "Cici",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731777",
          "name": "Chenhong He",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731778",
          "name": "Bowen Ye",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731779",
          "name": "Bowen Shen",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073177a",
          "name": "Zihan Zhang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073177b",
          "name": "Zihan Jiang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073177c",
          "name": "Zhixian Zheng",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073177d",
          "name": "Zhichao Song",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073177e",
          "name": "Zhenbo Luo",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073177f",
          "name": "Yue Yu",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731780",
          "name": "Yudong Wang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731781",
          "name": "Yuanyuan Tian",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731782",
          "name": "Yu Tu",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731783",
          "name": "Yihan Yan",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731784",
          "name": "Yi Huang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731785",
          "name": "Xu Wang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731786",
          "name": "Xinzhe Xu",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731787",
          "name": "Xingchen Song",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731788",
          "name": "Xing Zhang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731789",
          "name": "Xing Yong",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073178a",
          "name": "Xin Zhang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073178b",
          "name": "Xiangwei Deng",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073178c",
          "name": "Wenyu Yang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073178d",
          "name": "Wenhan Ma",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073178e",
          "name": "Weiwei Lv",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073178f",
          "name": "Weiji Zhuang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731790",
          "name": "Wei Liu",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731791",
          "name": "Sirui Deng",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731792",
          "name": "Shuo Liu",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731793",
          "name": "Shimao Chen",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731794",
          "name": "Shihua Yu",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731795",
          "name": "Shaohui Liu",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731796",
          "name": "Shande Wang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731797",
          "name": "Rui Ma",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731798",
          "name": "Qiantong Wang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731799",
          "name": "Peng Wang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073179a",
          "name": "Nuo Chen",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073179b",
          "name": "Menghang Zhu",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073179c",
          "name": "Kangyang Zhou",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073179d",
          "name": "Kang Zhou",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073179e",
          "name": "Kai Fang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073179f",
          "name": "Jun Shi",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a8907317a0",
          "name": "Jinhao Dong",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a8907317a1",
          "name": "Jiebao Xiao",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a8907317a2",
          "name": "Jiaming Xu",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a8907317a3",
          "user": {
            "_id": "680e9a219e529f779991be0c",
            "avatarUrl": "/avatars/327b945649192b0881fe290298d10e23.svg",
            "isPro": false,
            "fullname": "Huaqiu Liu",
            "user": "Prestonprom",
            "type": "user"
          },
          "name": "Huaqiu Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:26:58.279Z",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a8907317a4",
          "name": "Hongshen Xu",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a8907317a5",
          "name": "Heng Qu",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a8907317a6",
          "name": "Haochen Zhao",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a8907317a7",
          "name": "Hanglong Lv",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a8907317a8",
          "name": "Guoan Wang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a8907317a9",
          "name": "Duo Zhang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a8907317aa",
          "name": "Dong Zhang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a8907317ab",
          "name": "Di Zhang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a8907317ac",
          "name": "Chong Ma",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a8907317ad",
          "name": "Chang Liu",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a8907317ae",
          "name": "Can Cai",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a8907317af",
          "name": "Bingquan Xia",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-04T04:32:54.000Z",
      "submittedOnDailyAt": "2025-06-05T00:57:27.734Z",
      "title": "MiMo-VL 기술보고서\n\nMiMo-VL (Multiple Instance Multimodal Vision-Language) 기술보고서는 Vision-Language(V&L) 분야의 최신 기술과 발전 방향을 제시하고 있으며, 이는 시각 정보와 언어 정보의 상호작용을 통해 더욱 효율적이고 효과적인 정보 처리를 가능하게 하는 핵심 기술이다. 이 보고서는 MiMo-VL 기술의 기본 원리, 개발 배경, 그리고 현재의 연구 동향과 미래의 발전 방향을 자세히 설명하고 있으며, 이는 인공지능 분야의 연구자들과 개발자들에게 중요한 정보를 제공하며, 다양한 분야에서 응용할 수 있는 가능성을 강조한다.",
      "submittedOnDailyBy": {
        "_id": "6038d6d0612f5eef3cc05ea9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6038d6d0612f5eef3cc05ea9/ryhvAX5djQpD5OrIlZQ1f.jpeg",
        "isPro": false,
        "fullname": "Lei Li",
        "user": "tobiaslee",
        "type": "user"
      },
      "summary": "우리는 MiMo-VL-7B-SFT와 MiMo-VL-7B-RL, 두 가지 강력한 시각-언어 모델을 오픈소스로 공개합니다. 이 모델은 일반적인 시각 이해와 멀티모달 논리 처리에서 최신 수준의 성능을 제공합니다. MiMo-VL-7B-RL은 40개의 평가된 작업 중 35개를 능가하며, OlympiadBench에서 59.4점을 기록하여 78B 파라미터의 모델을 능가합니다. GUI 고정 애플리케이션에서 OSWorld-G에서 56.1점을 기록하여, UI-TARS와 같은 전문 모델을 능가합니다. 우리의 훈련은 4단계의 사전 훈련(2.4조 토큰)과 혼합된 정책 강화 학습(MORL)을 결합하여 다양한 보상 신호를 통합합니다. 우리는 사전 훈련 단계에서 고품질의 Chain-of-Thought 데이터를 포함하는 중요성과 동시에 다중 영역 최적화의 어려움 속에서 혼합된 RL의 이점을 강조합니다. 또한, 50개 이상의 태스크를 포함하는 종합적인 평가 세트를 제공하여 재현성과 분야의 발전을 촉진합니다. 모델 체크포인트와 전체 평가 세트는 https://github.com/XiaomiMiMo/MiMo-VL에 있습니다.",
      "upvotes": 44,
      "discussionId": "6841004145e7d8a890731853",
      "githubRepo": "https://github.com/XiaomiMiMo/MiMo-VL",
      "ai_summary": "MiMo-VL-7B-SFT and MiMo-VL-7B-RL provide state-of-the-art general visual understanding and multimodal reasoning through four-stage pre-training and Mixed On-policy Reinforcement Learning, outperforming models with up to 78B parameters.",
      "ai_keywords": [
        "vision-language models",
        "multimodal reasoning",
        "four-stage pre-training",
        "Mixed On-policy Reinforcement Learning",
        "MORL",
        "Chain-of-Thought",
        "reproducibility"
      ]
    },
    "publishedAt": "2025-06-04T00:32:54.000Z",
    "title": "MiMo-VL Technical Report",
    "summary": "We open-source MiMo-VL-7B-SFT and MiMo-VL-7B-RL, two powerful vision-language\nmodels delivering state-of-the-art performance in both general visual\nunderstanding and multimodal reasoning. MiMo-VL-7B-RL outperforms Qwen2.5-VL-7B\non 35 out of 40 evaluated tasks, and scores 59.4 on OlympiadBench, surpassing\nmodels with up to 78B parameters. For GUI grounding applications, it sets a new\nstandard with 56.1 on OSWorld-G, even outperforming specialized models such as\nUI-TARS. Our training combines four-stage pre-training (2.4 trillion tokens)\nwith Mixed On-policy Reinforcement Learning (MORL) integrating diverse reward\nsignals. We identify the importance of incorporating high-quality reasoning\ndata with long Chain-of-Thought into pre-training stages, and the benefits of\nmixed RL despite challenges in simultaneous multi-domain optimization. We also\ncontribute a comprehensive evaluation suite covering 50+ tasks to promote\nreproducibility and advance the field. The model checkpoints and full\nevaluation suite are available at https://github.com/XiaomiMiMo/MiMo-VL.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03569.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6038d6d0612f5eef3cc05ea9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6038d6d0612f5eef3cc05ea9/ryhvAX5djQpD5OrIlZQ1f.jpeg",
      "fullname": "Lei Li",
      "name": "tobiaslee",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.04207",
      "authors": [
        {
          "_id": "684117e22db29aa7b403af8d",
          "name": "Shuang Chen",
          "hidden": false
        },
        {
          "_id": "684117e22db29aa7b403af8e",
          "name": "Yue Guo",
          "hidden": false
        },
        {
          "_id": "684117e22db29aa7b403af8f",
          "user": {
            "_id": "64264095ba51f8a2136946a0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64264095ba51f8a2136946a0/FR33boVpkDXcrvGMBmprF.jpeg",
            "isPro": false,
            "fullname": "Zhaochen Su",
            "user": "Warrieryes",
            "type": "user"
          },
          "name": "Zhaochen Su",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:26:45.759Z",
          "hidden": false
        },
        {
          "_id": "684117e22db29aa7b403af90",
          "name": "Yafu Li",
          "hidden": false
        },
        {
          "_id": "684117e22db29aa7b403af91",
          "name": "Yulun Wu",
          "hidden": false
        },
        {
          "_id": "684117e22db29aa7b403af92",
          "user": {
            "_id": "65352acb7139c5dd8d9a8590",
            "avatarUrl": "/avatars/e2ff22b596aee45cdfb8f68dc15572f9.svg",
            "isPro": false,
            "fullname": "JiachengChen",
            "user": "JC-Chen",
            "type": "user"
          },
          "name": "Jiacheng Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:48:38.463Z",
          "hidden": false
        },
        {
          "_id": "684117e22db29aa7b403af93",
          "name": "Jiayu Chen",
          "hidden": false
        },
        {
          "_id": "684117e22db29aa7b403af94",
          "name": "Weijie Wang",
          "hidden": false
        },
        {
          "_id": "684117e22db29aa7b403af95",
          "name": "Xiaoye Qu",
          "hidden": false
        },
        {
          "_id": "684117e22db29aa7b403af96",
          "name": "Yu Cheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-04T17:51:08.000Z",
      "submittedOnDailyAt": "2025-06-05T02:38:24.366Z",
      "title": "진보하는 다 모델 논리: 최적화된 춥고 춥은 시작부터 단계별 강화 학습",
      "submittedOnDailyBy": {
        "_id": "65352acb7139c5dd8d9a8590",
        "avatarUrl": "/avatars/e2ff22b596aee45cdfb8f68dc15572f9.svg",
        "isPro": false,
        "fullname": "JiachengChen",
        "user": "JC-Chen",
        "type": "user"
      },
      "summary": "DEEP SEEKI-R1는 복잡한 문맥 태스크에서 뛰어난 논리 능력으로 이어져 있으며, 이를 위해 많은 연구가 직접적인 강화학습(RL)을 적용하고 있습니다. 그러나 복잡한 논리를 활성화하는 것은 어렵습니다. 이 논문에서는, 논리 능력을 향상시키기 위한 효과적인 초기화가 중요하다는 점을 강조합니다. 흥미롭게도, 컨텍스트 데이터만으로 더 잘 선택된 데이터를 사용하여 초기화하면, 여러 모델의 논리 모델보다 뛰어난 성능을 얻을 수 있습니다. 또한, 표준의 GRPO를 여러 모델의 RL에 적용할 경우, 경사 감소 현상이 발생하여 학습의 안정성과 성능이 떨어집니다. 마지막으로, 여러 모델의 논리 능력을 더욱 향상시키기 위해, 여러 모델의 RL 학습 후 텍스트만 위한 RL 학습이 필요합니다. 이 단계별 학습 접근법은 시각적 기초와 인지적 논리 개발을 균형을 유지할 수 있습니다. 이러한 지침을 통합하여 여러 모델의 RL 문제를 해결하고, ReVisual-R1을 소개합니다. MathVerse, MathVision, WeMath, LogicVista, DynaMath, 그리고 도전적인 AIME2024과 AIME2025의 어려운 벤치마크에서, 오픈 소스 7B MLLMs의 새로운 최선 성능을 달성합니다.",
      "upvotes": 35,
      "discussionId": "684117e32db29aa7b403afc2",
      "githubRepo": "https://github.com/CSfufu/Revisual-R1"
    },
    "publishedAt": "2025-06-04T13:51:08.000Z",
    "title": "Advancing Multimodal Reasoning: From Optimized Cold Start to Staged\n  Reinforcement Learning",
    "summary": "Inspired by the remarkable reasoning capabilities of Deepseek-R1 in complex\ntextual tasks, many works attempt to incentivize similar capabilities in\nMultimodal Large Language Models (MLLMs) by directly applying reinforcement\nlearning (RL). However, they still struggle to activate complex reasoning. In\nthis paper, rather than examining multimodal RL in isolation, we delve into\ncurrent training pipelines and identify three crucial phenomena: 1) Effective\ncold start initialization is critical for enhancing MLLM reasoning.\nIntriguingly, we find that initializing with carefully selected text data alone\ncan lead to performance surpassing many recent multimodal reasoning models,\neven before multimodal RL. 2) Standard GRPO applied to multimodal RL suffers\nfrom gradient stagnation, which degrades training stability and performance. 3)\nSubsequent text-only RL training, following the multimodal RL phase, further\nenhances multimodal reasoning. This staged training approach effectively\nbalances perceptual grounding and cognitive reasoning development. By\nincorporating the above insights and addressing multimodal RL issues, we\nintroduce ReVisual-R1, achieving a new state-of-the-art among open-source 7B\nMLLMs on challenging benchmarks including MathVerse, MathVision, WeMath,\nLogicVista, DynaMath, and challenging AIME2024 and AIME2025.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04207.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "65352acb7139c5dd8d9a8590",
      "avatarUrl": "/avatars/e2ff22b596aee45cdfb8f68dc15572f9.svg",
      "fullname": "JiachengChen",
      "name": "JC-Chen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.04089",
      "authors": [
        {
          "_id": "684153cf911d1b3135fa5dfe",
          "name": "Anastasiia Ivanova",
          "hidden": false
        },
        {
          "_id": "684153cf911d1b3135fa5dff",
          "user": {
            "_id": "661af24d8328f43c6abc2d11",
            "avatarUrl": "/avatars/afe7eaf1f7a378dbcdba5cd3e86adf9c.svg",
            "isPro": false,
            "fullname": "Eva",
            "user": "tenebrissilvam",
            "type": "user"
          },
          "name": "Eva Bakaeva",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T10:00:02.435Z",
          "hidden": false
        },
        {
          "_id": "684153cf911d1b3135fa5e00",
          "user": {
            "_id": "64198f70ed725fef6442b37e",
            "avatarUrl": "/avatars/580ab07a3067a9deb2977b0894226fe3.svg",
            "isPro": false,
            "fullname": "Alexey Kovalev",
            "user": "AlexeyKov",
            "type": "user"
          },
          "name": "Zoya Volovikova",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-05T08:22:39.926Z",
          "hidden": false
        },
        {
          "_id": "684153cf911d1b3135fa5e01",
          "name": "Alexey K. Kovalev",
          "hidden": false
        },
        {
          "_id": "684153cf911d1b3135fa5e02",
          "name": "Aleksandr I. Panov",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-04T15:47:07.000Z",
      "submittedOnDailyAt": "2025-06-05T07:08:16.935Z",
      "title": "AmbiK: キッチン 환경에서 불확실한 작업의 데이터 세트",
      "submittedOnDailyBy": {
        "_id": "64198f70ed725fef6442b37e",
        "avatarUrl": "/avatars/580ab07a3067a9deb2977b0894226fe3.svg",
        "isPro": false,
        "fullname": "Alexey Kovalev",
        "user": "AlexeyKov",
        "type": "user"
      },
      "summary": "LLMs는 기계화 에이전트의 일부로, 사용자로부터의 자연어 명령에 따라 행동 계획을 결정하기 위해 일반적으로 사용됩니다. 그러나 현실적인 환경에서 문법 불분명한 명령을 처리하는 것은 LLMs에게 매우 어려운 일이다. 다양한 문법 불분명한 태스크 감지 방법들이 제안되었지만, 이들을 비교하는 것은 어려워하며, 데이터셋이나 일반적인 벤치마크가 없기 때문이다. 이에 AmbiK(식당 환경에서 문법 불분명한 태스크)을 제안합니다. AmbiK은 기계에 대한 문법 불분명한 지시를 처리하기 위한 완전한 맥락 데이터셋이며, LLMs의 도움을 받아 수집되어, 인간이 검증되어 있습니다. AmbiK은 문법 불분명한 태스크와 그 맞춤형 대응을 포함하여 1000 쌍을 구성하고, 문법 불분명한 종류(인간의 취향, 일반 지식, 안전성)에 카테고리가 되어, 환경 설명, 설명의 질의와 답, 사용자의 의지, 태스크 계획을 포함하여 2000개의 종합적인 데이터셋입니다. AmbiK은 문법 불분명한 감지 방법의 통일적인 비교를 가능하게 하며, 랜덤입니다. AmbiK은 https://github.com/cog-model/AmbiK-dataset에서 사용 가능합니다.",
      "upvotes": 31,
      "discussionId": "684153cf911d1b3135fa5e2e",
      "ai_summary": "AmbiK, a textual dataset of ambiguous instructions for kitchen robots, enables unified comparison of ambiguity detection methods.",
      "ai_keywords": [
        "Large Language Models",
        "LLMs",
        "behavior planning",
        "ambiguous instructions",
        "task ambiguity detection",
        "AmbiK",
        "dataset",
        "human-validated",
        "ambiguity types",
        "Human Preferences",
        "Common Sense Knowledge",
        "Safety",
        "environment descriptions",
        "clarifying questions",
        "user intents",
        "task plans"
      ]
    },
    "publishedAt": "2025-06-04T11:47:07.000Z",
    "title": "AmbiK: Dataset of Ambiguous Tasks in Kitchen Environment",
    "summary": "As a part of an embodied agent, Large Language Models (LLMs) are typically\nused for behavior planning given natural language instructions from the user.\nHowever, dealing with ambiguous instructions in real-world environments remains\na challenge for LLMs. Various methods for task ambiguity detection have been\nproposed. However, it is difficult to compare them because they are tested on\ndifferent datasets and there is no universal benchmark. For this reason, we\npropose AmbiK (Ambiguous Tasks in Kitchen Environment), the fully textual\ndataset of ambiguous instructions addressed to a robot in a kitchen\nenvironment. AmbiK was collected with the assistance of LLMs and is\nhuman-validated. It comprises 1000 pairs of ambiguous tasks and their\nunambiguous counterparts, categorized by ambiguity type (Human Preferences,\nCommon Sense Knowledge, Safety), with environment descriptions, clarifying\nquestions and answers, user intents, and task plans, for a total of 2000 tasks.\nWe hope that AmbiK will enable researchers to perform a unified comparison of\nambiguity detection methods. AmbiK is available at\nhttps://github.com/cog-model/AmbiK-dataset.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04089.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64198f70ed725fef6442b37e",
      "avatarUrl": "/avatars/580ab07a3067a9deb2977b0894226fe3.svg",
      "fullname": "Alexey Kovalev",
      "name": "AlexeyKov",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.02921",
      "authors": [
        {
          "_id": "683ff4dcfbc9041ef7274c51",
          "user": {
            "_id": "657eea68f4f72f2c4c44640d",
            "avatarUrl": "/avatars/033bc4f063cd36a79a0b4761f6ebe32c.svg",
            "isPro": false,
            "fullname": "Yijun YANG",
            "user": "thomasyyj",
            "type": "user"
          },
          "name": "Yijun Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:53:47.455Z",
          "hidden": false
        },
        {
          "_id": "683ff4dcfbc9041ef7274c52",
          "name": "Zeyu Huang",
          "hidden": false
        },
        {
          "_id": "683ff4dcfbc9041ef7274c53",
          "name": "Wenhao Zhu",
          "hidden": false
        },
        {
          "_id": "683ff4dcfbc9041ef7274c54",
          "name": "Zihan Qiu",
          "hidden": false
        },
        {
          "_id": "683ff4dcfbc9041ef7274c55",
          "name": "Fei Yuan",
          "hidden": false
        },
        {
          "_id": "683ff4dcfbc9041ef7274c56",
          "name": "Jeff Z. Pan",
          "hidden": false
        },
        {
          "_id": "683ff4dcfbc9041ef7274c57",
          "name": "Ivan Titov",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T14:23:06.000Z",
      "submittedOnDailyAt": "2025-06-05T02:04:55.586Z",
      "title": "장문맥언어모의 제어 가능한 검사",
      "submittedOnDailyBy": {
        "_id": "657eea68f4f72f2c4c44640d",
        "avatarUrl": "/avatars/033bc4f063cd36a79a0b4761f6ebe32c.svg",
        "isPro": false,
        "fullname": "Yijun YANG",
        "user": "thomasyyj",
        "type": "user"
      },
      "summary": "현재의 긴 문맥 언어 모델(LCLM) 평가 프레임워크는 실세계 태스크와 합성 태스크로 분류됩니다. 두 가지 접근 방식은 각각 고유한 제한을 가지고 있습니다. 실세계 태스크는 복잡해져 해석이나 특징화가 어려워서, 데이터 오염에 취약합니다. 반면, 합성 태스크는 \"Naive in the High Stack\" (NIAH) 형식을 채택하고, \"Naive\"와 \"High Stack\"의 불연속성은 실용적인 애플리케이션의 대리인으로서의 정당성을 파괴합니다. 이러한 도전에 대응하여, ideale한 긴 문맥 평가 프레임워크는 3가지의 기본적인 특성을 갖추어야 한다고 주장합니다: 연속된 컨텍스트, 제어 가능한 설정, 보안 평가. 본 연구에서는, 인공적으로 생성된 비즈니스 로그를 사용하여 새로운 벤치마크 \"LongBioBench\"를 소개하고, LCLM의 이해, 추론, 신뢰성 평가를 수행합니다.\n\n실험적 평가에서, 18개의 LCLM을 포함하고 있으며, 여러 모델이 검색 결과를 의미적으로 이해하고 기본적인 추론에 오류를 발견하고, 문맥 길이를 늘리면 신뢰도가 떨어집니다. 진한 분석에서는, 현재의 합성 벤치마크에서 사용되고 있는 것과 같이, 컨텍스트의 불연속성, 숫자의 \"Naive\", 디트랙터의 없음 등 설계 선택지는 모델의 긴 문맥 능력 검증에 취약한 취약점을 나타냅니다. 또한, 긴 문맥의 연속적인 예측 훈련은 RoPE 인베딩을 긴 문맥 길이에 적응하여 주로 조정되어 있음을 명확히 나타났습니다. 총括的に言えば, 기존의 합성 벤치마크와 비교하여, LongBioBench는 실제 언어 태스크를 미러링하면서 제어 가능성의 유지로 더 좋은 균형을 실현하고, 고도 해석 가능하고 설정 가능합니다.",
      "upvotes": 25,
      "discussionId": "683ff4ddfbc9041ef7274c73",
      "githubRepo": "https://github.com/Thomasyyj/LongBio-Benchmark",
      "ai_summary": "LongBioBench is a new benchmark using artificially generated biographies to evaluate long-context language models across understanding, reasoning, and trustworthiness dimensions, addressing limitations in existing frameworks.",
      "ai_keywords": [
        "long-context language models (LCLM)",
        "real-world tasks",
        "synthetic tasks",
        "needle-in-the-haystack (NIAH)",
        "seamless context",
        "controllable setting",
        "sound evaluation",
        "LongBioBench",
        "semantic understanding",
        "elementary reasoning",
        "trustworthiness",
        "long-context continual pretraining",
        "RoPE embedding"
      ]
    },
    "publishedAt": "2025-06-03T10:23:06.000Z",
    "title": "A Controllable Examination for Long-Context Language Models",
    "summary": "Existing frameworks for evaluating long-context language models (LCLM) can be\nbroadly categorized into real-world and synthetic tasks. Despite their utility,\nboth approaches are accompanied by certain intrinsic limitations. Real-world\ntasks are too complex to interpret or characterize and are susceptible to data\ncontamination. In contrast, synthetic tasks often adopt the\nneedle-in-the-haystack (NIAH) format, wherein a lack of coherence between the\n\"needle\" and the \"haystack\" compromises their validity as proxies for realistic\napplications. In response to these challenges, we posit that an ideal\nlong-context evaluation framework should be characterized by three essential\nfeatures: seamless context, controllable setting, and\nsound evaluation. This study introduces LongBioBench, a\nnovel benchmark that utilizes artificially generated biographies as a\ncontrolled environment for assessing LCLMs across dimensions of\nunderstanding, reasoning, and trustworthiness.\nOur experimental evaluation, which includes 18 LCLMs in total,\ndemonstrates that most models still exhibit deficiencies in semantic\nunderstanding and elementary reasoning over retrieved results and are less\ntrustworthy as context length increases. Our further analysis indicates some\ndesign choices employed by existing synthetic benchmarks, such as contextual\nnon-coherence, numerical needles, and the absence of distractors, rendering\nthem vulnerable to test the model long-context capabilities. Moreover, we also\nreveal that long-context continual pretraining primarily adjusts RoPE embedding\nto accommodate extended context lengths. To sum up, compared to previous\nsynthetic benchmarks, LongBioBench achieves a better trade-off between\nmirroring authentic language tasks and maintaining controllability, and is\nhighly interpretable and configurable.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.02921.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "657eea68f4f72f2c4c44640d",
      "avatarUrl": "/avatars/033bc4f063cd36a79a0b4761f6ebe32c.svg",
      "fullname": "Yijun YANG",
      "name": "thomasyyj",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.16968",
      "authors": [
        {
          "_id": "683656aefd55e753bf26ed3e",
          "user": {
            "_id": "656864e12d73834278a8dea7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg",
            "isPro": true,
            "fullname": "Ahmed Heakl",
            "user": "ahmedheakl",
            "type": "user"
          },
          "name": "Ahmed Heakl",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-28T08:58:30.760Z",
          "hidden": false
        },
        {
          "_id": "683656aefd55e753bf26ed3f",
          "user": {
            "_id": "62676a94dacab364889bb36c",
            "avatarUrl": "/avatars/0ead41b44957eb30564ea685ed22781a.svg",
            "isPro": false,
            "fullname": "SARIM HASHMI",
            "user": "Sarim-Hash",
            "type": "user"
          },
          "name": "Sarim Hashmi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-02T07:49:01.879Z",
          "hidden": false
        },
        {
          "_id": "683656aefd55e753bf26ed40",
          "user": {
            "_id": "62eaadf4086bd1debb30a122",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62eaadf4086bd1debb30a122/wgxsPVnkOuEfq1oqlUhiB.jpeg",
            "isPro": false,
            "fullname": "Gustavo Stahl",
            "user": "GustavoStahl",
            "type": "user"
          },
          "name": "Gustavo Bertolo Stahl",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T08:31:48.782Z",
          "hidden": false
        },
        {
          "_id": "683656aefd55e753bf26ed41",
          "name": "Seung Hun Eddie Han",
          "hidden": false
        },
        {
          "_id": "683656aefd55e753bf26ed42",
          "name": "Salman Khan",
          "hidden": false
        },
        {
          "_id": "683656aefd55e753bf26ed43",
          "name": "Abdulrahman Mahmoud",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/T4ESSrZsC7163P3I8p17C.png",
        "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/WF-SJEyKKtpa3Zq0JvBXA.png",
        "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/Hl8Dkgmc4QL_l9YKPhRvD.png",
        "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/p-io7OU8TtxwvBp4_M1Hd.png",
        "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/bu6bpeVfonZgrXopd9f79.png"
      ],
      "publishedAt": "2025-05-22T17:48:53.000Z",
      "submittedOnDailyAt": "2025-06-05T06:33:02.615Z",
      "title": "CASS: Nvidia의 데이터, 모델, 벤치마크를 활용한 AMD로의 트랜스플레어링",
      "submittedOnDailyBy": {
        "_id": "656864e12d73834278a8dea7",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg",
        "isPro": true,
        "fullname": "Ahmed Heakl",
        "user": "ahmedheakl",
        "type": "user"
      },
      "summary": "CASS를 소개합니다. 이것은 처음의 대규모 데이터 세트와 모델 시스템で, 컴퓨터 아키텍처 간의 GPU 코드의 트랜스픽션을 목표로 합니다. 이는 소스 수준(CUDA ⇔ HIP)과 어셈블리 수준(Nvidia SASS ⇔ AMD RDNA3)의 번역을 대상으로 합니다. 이 데이터 세트는 70k개의 검증된 코드 쌍을 포함하고 있으며, 호스트와 장치 모두에서 낮은 수준의 GPU 코드의 포터빌리티의 중요한 결함이 해결됩니다. 이 리소스를 활용하여, CASS famliy의 분야 전문 언어 모델을 훈련하고, 95%의 소스 번역 정확도와 37.5%의 어셈블리 번역 정확도를 달성하여, GPT-4o, Claude, Hipify 등 상업 기반 라인을 크게 초월했습니다. 생성된 코드는 85% 이상의 테스트 케이스에서 네티브 성능을 유지하며, 실행 시간과 메모리 균형을 유지합니다. 엄격한 평가의 위해, CASS-Bench를 소개합니다. 이는 16가지 게임 타입의 GPU 영역을 확장한 맞춤화된 벤치마크이며, 실제 실행 결과를 포함합니다. 모든 데이터, 모델, 평가 도구는 GPU 컴파일러 트루링, 바이너리 바이어스, LLM 가이드된 하드웨어 번역의 발전을 촉진하기 위해 오픈 소스로 릴리즈되어 있습니다. 데이터 세트와 벤치마크는 https://huggingface.co/datasets/MBZUAI/cass{blue{HuggingFace}}에 있고, 코드는 https://github.com/GustavoStahl/CASS{blue{GitHub}}에 있습니다.",
      "upvotes": 23,
      "discussionId": "683656b0fd55e753bf26edf7",
      "githubRepo": "https://github.com/GustavoStahl/CASS",
      "ai_summary": "CASS is a dataset and model suite for GPU code transpilation at both source and assembly levels, achieving high accuracy and performance matching with native code.",
      "ai_keywords": [
        "cross-architecture GPU code transpilation",
        "CASS",
        "CUDA",
        "HIP",
        "Nvidia SASS",
        "AMD RDNA3",
        "domain-specific language models",
        "source translation accuracy",
        "assembly translation accuracy",
        "native performance",
        "CASS-Bench",
        "GPU compiler tooling",
        "binary compatibility",
        "LLM-guided hardware translation"
      ]
    },
    "publishedAt": "2025-05-22T13:48:53.000Z",
    "title": "CASS: Nvidia to AMD Transpilation with Data, Models, and Benchmark",
    "summary": "We introduce CASS, the first large-scale dataset and model suite for\ncross-architecture GPU code transpilation, targeting both source-level (CUDA\nleftrightarrow HIP) and assembly-level (Nvidia SASS leftrightarrow AMD\nRDNA3) translation. The dataset comprises 70k verified code pairs across host\nand device, addressing a critical gap in low-level GPU code portability.\nLeveraging this resource, we train the CASS family of domain-specific language\nmodels, achieving 95% source translation accuracy and 37.5% assembly\ntranslation accuracy, substantially outperforming commercial baselines such as\nGPT-4o, Claude, and Hipify. Our generated code matches native performance in\nover 85% of test cases, preserving runtime and memory behavior. To support\nrigorous evaluation, we introduce CASS-Bench, a curated benchmark spanning 16\nGPU domains with ground-truth execution. All data, models, and evaluation tools\nare released as open source to foster progress in GPU compiler tooling, binary\ncompatibility, and LLM-guided hardware translation. Dataset and benchmark are\non\nhttps://huggingface.co/datasets/MBZUAI/cass{blue{HuggingFace}},\nwith code at\nhttps://github.com/GustavoStahl/CASS{blue{GitHub}}.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/T4ESSrZsC7163P3I8p17C.png",
      "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/WF-SJEyKKtpa3Zq0JvBXA.png",
      "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/Hl8Dkgmc4QL_l9YKPhRvD.png",
      "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/p-io7OU8TtxwvBp4_M1Hd.png",
      "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/bu6bpeVfonZgrXopd9f79.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16968.png",
    "numComments": 5,
    "submittedBy": {
      "_id": "656864e12d73834278a8dea7",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg",
      "fullname": "Ahmed Heakl",
      "name": "ahmedheakl",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 39
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.04180",
      "authors": [
        {
          "_id": "6840fefb3098ab525906d852",
          "name": "Yuhao Wu",
          "hidden": false
        },
        {
          "_id": "6840fefb3098ab525906d853",
          "name": "Yushi Bai",
          "hidden": false
        },
        {
          "_id": "6840fefb3098ab525906d854",
          "user": {
            "_id": "637f228152229c63921119c3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637f228152229c63921119c3/acwXorra1r9_7i3KlBFjS.jpeg",
            "isPro": false,
            "fullname": "Zhiqiang Hu",
            "user": "Zhiqiang007",
            "type": "user"
          },
          "name": "Zhiqiang Hu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:27:09.305Z",
          "hidden": false
        },
        {
          "_id": "6840fefb3098ab525906d855",
          "name": "Juanzi Li",
          "hidden": false
        },
        {
          "_id": "6840fefb3098ab525906d856",
          "name": "Roy Ka-Wei Lee",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-04T17:27:42.000Z",
      "submittedOnDailyAt": "2025-06-05T00:58:27.883Z",
      "title": "スーパーライター：반성한 긴 문장 생성 모델",
      "submittedOnDailyBy": {
        "_id": "64ed568ccf6118a9379a61b8",
        "avatarUrl": "/avatars/6d040cbcb4a9b624cbe64c9d01cd5c88.svg",
        "isPro": false,
        "fullname": "Yushi Bai",
        "user": "bys0318",
        "type": "user"
      },
      "summary": "장문 생성은 대규모 언어 모델(LLMs)에 대한 장기적인 도전으로, 특히 장문 생성의 연속성 유지, 논리적 일관성 보장, 시퀀스 길이 증가에 따른 문질 유지가 중요한 문제입니다. 이러한 제한을 해결하기 위해, 우리는 SuperWriter-Agent를 제안합니다. SuperWriter-Agent는 장문 생성의 품질과 일관성을 향상시키기 위해 설계된 에이전트 기반 프레임워크입니다. SuperWriter-Agent는 생성 파이프라인에 명시적인 구조화 된 생각 및 개선 단계를 도입하여, 모델을 더 엄격하고 인지적으로 기반한 과정을 따라 유도합니다. 이 프레임워크를 기반으로, 우리는 7B의 SuperWriter-LM을 훈련하기 위한 표준화된 미세 조정 데이터 세트를 구축하고, 계층적인 Direct Preference Optimization(DPO) 프로세스를 개발합니다. 이는 Monte Carlo 트리 탐색(MCTS)을 사용하여 최종적인 품질 평가가 전달되고, 각 생성 단계를 적절히 최적화하는 것입니다. 다양한 벤치마크에서의 실험 결과를 통해, SuperWriter-LM은 최신의 성능을 달성했으며, 자동 평가 및 인간 평가에서도 더 큰 규모의 베이스 모델을 초월하는 것을 입증했습니다. 또한, 상세한 제거 테스트는 계층적인 DPO의 효과성을 보여주고, 장문 생성의 품질 향상에 대한 구조화된 생각 단계의 동작을 강조합니다.",
      "upvotes": 20,
      "discussionId": "6840fefc3098ab525906d89c",
      "ai_summary": "SuperWriter-Agent enhances long-form text generation by integrating structured planning and refinement, achieving top performance with a 7B model and hierarchical Direct Preference Optimization.",
      "ai_keywords": [
        "agent-based framework",
        "structured thinking-through planning",
        "refinement stages",
        "SuperWriter-Agent",
        "SuperWriter-LM",
        "hierarchical Direct Preference Optimization",
        "Monte Carlo Tree Search",
        "DPO",
        "automatic evaluation",
        "human evaluation",
        "ablation studies"
      ]
    },
    "publishedAt": "2025-06-04T13:27:42.000Z",
    "title": "SuperWriter: Reflection-Driven Long-Form Generation with Large Language\n  Models",
    "summary": "Long-form text generation remains a significant challenge for large language\nmodels (LLMs), particularly in maintaining coherence, ensuring logical\nconsistency, and preserving text quality as sequence length increases. To\naddress these limitations, we propose SuperWriter-Agent, an agent-based\nframework designed to enhance the quality and consistency of long-form text\ngeneration. SuperWriter-Agent introduces explicit structured thinking-through\nplanning and refinement stages into the generation pipeline, guiding the model\nto follow a more deliberate and cognitively grounded process akin to that of a\nprofessional writer. Based on this framework, we construct a supervised\nfine-tuning dataset to train a 7B SuperWriter-LM. We further develop a\nhierarchical Direct Preference Optimization (DPO) procedure that uses Monte\nCarlo Tree Search (MCTS) to propagate final quality assessments and optimize\neach generation step accordingly. Empirical results across diverse benchmarks\ndemonstrate that SuperWriter-LM achieves state-of-the-art performance,\nsurpassing even larger-scale baseline models in both automatic evaluation and\nhuman evaluation. Furthermore, comprehensive ablation studies demonstrate the\neffectiveness of hierarchical DPO and underscore the value of incorporating\nstructured thinking steps to improve the quality of long-form text generation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04180.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ed568ccf6118a9379a61b8",
      "avatarUrl": "/avatars/6d040cbcb4a9b624cbe64c9d01cd5c88.svg",
      "fullname": "Yushi Bai",
      "name": "bys0318",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.01320",
      "authors": [
        {
          "_id": "684124368cb0edba3ab8f738",
          "name": "Taehoon Yoon",
          "hidden": false
        },
        {
          "_id": "684124368cb0edba3ab8f739",
          "name": "Yunhong Min",
          "hidden": false
        },
        {
          "_id": "684124368cb0edba3ab8f73a",
          "name": "Kyeongmin Yeo",
          "hidden": false
        },
        {
          "_id": "684124368cb0edba3ab8f73b",
          "name": "Minhyuk Sung",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-02T05:02:33.000Z",
      "submittedOnDailyAt": "2025-06-05T03:38:33.152Z",
      "title": "Ψ-Sampler: 초기 입자 샘플링에 의한 SMC 기반 추론 시 보상의 조정을 수행하는 스코어 모델",
      "submittedOnDailyBy": {
        "_id": "66ee81b676a8038cb42c8caa",
        "avatarUrl": "/avatars/9b4c5ded9c94788c35ce7ffbc2f8d24b.svg",
        "isPro": false,
        "fullname": "Yunhong Min",
        "user": "myhong",
        "type": "user"
      },
      "summary": "Psi-Sampler를 소개합니다. 이 프레임워크는 스코어 기반의 생성 모델과 관련된 추론 시 보상 조정을 효과적으로 지원하며, pCNL 기반의 초기 입자 샘플링을 사용합니다. 스코어 기반의 생성 모델과 관련된 추론 시 보상 조정은 최근부터 학습 후 최적화의 플러그인 변경으로부터 광범위한 패러다임 변경으로 인해 더 많이 다루어지는 주제입니다. 이 흐름의 핵심은 순차적 모ン테카르로 법(SMC)의 적용으로 이루어지며, 현재의 방법들은 일반적으로 가우시안 선두로 입자를 초기화하고 보상과 관련된 영역을 충분히 파악하지 못하여 샘플링 효율을 저하시킵니다. 보상에 관심 있는 후추를 초기에 초기화함으로써 조정 성능을 크게 향상시킬 수 있음을 보여주고 있습니다. 고차원 잠재 공간에서 후추 샘플링을 가능하게 하기 위해 preconditioned Crank-Nicolson Langevin(pCNL) 알고리즘을 소개하고, 차원별 제안과 경사를 기반으로 하는 동력학을 조합하여 있습니다. 이 접근법은 효율적이고 scalable한 후추 샘플링을 가능하게 하며, 레이어어 모델 생성, 감각적 생성, 예술적 취향 생성 등 다양한 보상 조정 태스크에서 긍정적인 성능 향상을 나타냅니다.",
      "upvotes": 15,
      "discussionId": "6841243c8cb0edba3ab8f8bf",
      "ai_summary": "The framework $\\Psi$-Sampler uses SMC with pCNL for efficient posterior sampling and reward alignment in score-based generative models, enhancing performance across various tasks.",
      "ai_keywords": [
        "SMC-based framework",
        "pCNL-based initial particle sampling",
        "inference-time reward alignment",
        "score-based generative model",
        "Sequential Monte Carlo",
        "denoising process",
        "Gaussian prior",
        "reward-aware posterior",
        "preconditioned Crank-Nicolson Langevin",
        "layout-to-image generation",
        "quantity-aware generation",
        "aesthetic-preference generation"
      ]
    },
    "publishedAt": "2025-06-02T01:02:33.000Z",
    "title": "Ψ-Sampler: Initial Particle Sampling for SMC-Based Inference-Time\n  Reward Alignment in Score Models",
    "summary": "We introduce Psi-Sampler, an SMC-based framework incorporating pCNL-based\ninitial particle sampling for effective inference-time reward alignment with a\nscore-based generative model. Inference-time reward alignment with score-based\ngenerative models has recently gained significant traction, following a broader\nparadigm shift from pre-training to post-training optimization. At the core of\nthis trend is the application of Sequential Monte Carlo (SMC) to the denoising\nprocess. However, existing methods typically initialize particles from the\nGaussian prior, which inadequately captures reward-relevant regions and results\nin reduced sampling efficiency. We demonstrate that initializing from the\nreward-aware posterior significantly improves alignment performance. To enable\nposterior sampling in high-dimensional latent spaces, we introduce the\npreconditioned Crank-Nicolson Langevin (pCNL) algorithm, which combines\ndimension-robust proposals with gradient-informed dynamics. This approach\nenables efficient and scalable posterior sampling and consistently improves\nperformance across various reward alignment tasks, including layout-to-image\ngeneration, quantity-aware generation, and aesthetic-preference generation, as\ndemonstrated in our experiments.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01320.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66ee81b676a8038cb42c8caa",
      "avatarUrl": "/avatars/9b4c5ded9c94788c35ce7ffbc2f8d24b.svg",
      "fullname": "Yunhong Min",
      "name": "myhong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.04225",
      "authors": [
        {
          "_id": "68413366adeec0116d071af2",
          "user": {
            "_id": "63425d394c9a81858b36aeb5",
            "avatarUrl": "/avatars/511ad6a75bd1c10fc510ef527e7f8e5b.svg",
            "isPro": false,
            "fullname": "Tianyu Huang",
            "user": "tyhuang",
            "type": "user"
          },
          "name": "Tianyu Huang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:26:41.198Z",
          "hidden": false
        },
        {
          "_id": "68413366adeec0116d071af3",
          "name": "Wangguandong Zheng",
          "hidden": false
        },
        {
          "_id": "68413366adeec0116d071af4",
          "name": "Tengfei Wang",
          "hidden": false
        },
        {
          "_id": "68413366adeec0116d071af5",
          "name": "Yuhao Liu",
          "hidden": false
        },
        {
          "_id": "68413366adeec0116d071af6",
          "name": "Zhenwei Wang",
          "hidden": false
        },
        {
          "_id": "68413366adeec0116d071af7",
          "name": "Junta Wu",
          "hidden": false
        },
        {
          "_id": "68413366adeec0116d071af8",
          "name": "Jie Jiang",
          "hidden": false
        },
        {
          "_id": "68413366adeec0116d071af9",
          "name": "Hui Li",
          "hidden": false
        },
        {
          "_id": "68413366adeec0116d071afa",
          "name": "Rynson W. H. Lau",
          "hidden": false
        },
        {
          "_id": "68413366adeec0116d071afb",
          "name": "Wangmeng Zuo",
          "hidden": false
        },
        {
          "_id": "68413366adeec0116d071afc",
          "name": "Chunchao Guo",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63425d394c9a81858b36aeb5/cjZH2kR6B3y9IAmmRHNJS.mp4"
      ],
      "publishedAt": "2025-06-04T17:59:04.000Z",
      "submittedOnDailyAt": "2025-06-05T06:00:26.815Z",
      "title": "Voyager: 긴 거리와 세계적인 视频 디퓨저를 활용한 탐험 가능한 3D 화면 생성",
      "submittedOnDailyBy": {
        "_id": "63425d394c9a81858b36aeb5",
        "avatarUrl": "/avatars/511ad6a75bd1c10fc510ef527e7f8e5b.svg",
        "isPro": false,
        "fullname": "Tianyu Huang",
        "user": "tyhuang",
        "type": "user"
      },
      "summary": "실세계의 앱 개발에서, 3D 모델링 기술은 3D 게임, 가상현실(VR) 등 다양한 분야에서 필수적입니다. 최근, 3D 모델을 생성하는 기술은 문장나 이미지로부터의 발전이 있었지만, 3D 스케네를 구성하는 것은 복잡하고 어려운 문제입니다. 본 논문에서는, 사용자가 지정한 카메라 경로를 바탕으로 1개의 이미지로부터 세계적으로 일치하는 3D 포인트 클러스터 시퀀스를 생성하는 새로운 프레임워크를 소개합니다. 이 프레임워크는 'Voyager'라는 이름으로 불립니다. 기존의 접근 방식과는 달리, Voyager는 각 프레임의 고유한 일치성을 유지하면서, 일관된 3D 스케네 생성과 재구성을 실현하고, 3D 재구성 프로세스(예: 구조에 따른 동작, 다중점 스테레오)의 필요성을 줄입니다. 우리 방법론은 세 가지 핵심 요소로 구성됩니다: 1) 세계적으로 일관된 비디오 디퓨전: 통일된 아키텍처를 통해, 기존의 세계 관찰에 기반하여 일관된 RGB와 깊이의 비디오 시퀀스를 생성하고, 글로벌 일관성을 보장합니다. 2) 긴거리의 세계 탐색: 효율적인 세계 캐시와 포인트 제거, 자동 회귀 추론 및 평활한 비디오 샘플링을 활용하여, 반복적인 스케네 확장과 컨텍스트에 따른 일관성을 실현합니다. 3) 교환성 있는 데이터 엔진: 임의의 비디오에 대한 카메라 자세 추정과 깊이 예측을 자동화하고, 큰 규모, 다양한 훈련 데이터의 수집을 가능하게 하며, 3D Annotation을 필요로 하지 않습니다. 이러한 설계는 현재의 방법과 비교하여, 시각적 품질과 일반화 정확도를 명확히 향상시키고, 다양한 애플리케이션에 적용할 수 있습니다.",
      "upvotes": 14,
      "discussionId": "6841336badeec0116d071c2b",
      "projectPage": "https://voyager-world.github.io",
      "githubRepo": "https://github.com/Voyager-World/Voyager",
      "ai_summary": "Voyager is a video diffusion framework that generates world-consistent 3D point-cloud sequences from a single image, enabling long-range, consistent 3D scene exploration with user-defined camera paths.",
      "ai_keywords": [
        "video diffusion",
        "world-consistent video diffusion",
        "3D point-cloud sequences",
        "camera path",
        "end-to-end scene generation",
        "consistent frames",
        "unified architecture",
        "RGB and depth video sequences",
        "world observation",
        "global coherence",
        "long-range world exploration",
        "world cache",
        "point culling",
        "auto-regressive inference",
        "smooth video sampling",
        "scene extension",
        "context-aware consistency",
        "scalable data engine",
        "camera pose estimation",
        "metric depth prediction",
        "large-scale",
        "diverse training data"
      ]
    },
    "publishedAt": "2025-06-04T13:59:04.000Z",
    "title": "Voyager: Long-Range and World-Consistent Video Diffusion for Explorable\n  3D Scene Generation",
    "summary": "Real-world applications like video gaming and virtual reality often demand\nthe ability to model 3D scenes that users can explore along custom camera\ntrajectories. While significant progress has been made in generating 3D objects\nfrom text or images, creating long-range, 3D-consistent, explorable 3D scenes\nremains a complex and challenging problem. In this work, we present Voyager, a\nnovel video diffusion framework that generates world-consistent 3D point-cloud\nsequences from a single image with user-defined camera path. Unlike existing\napproaches, Voyager achieves end-to-end scene generation and reconstruction\nwith inherent consistency across frames, eliminating the need for 3D\nreconstruction pipelines (e.g., structure-from-motion or multi-view stereo).\nOur method integrates three key components: 1) World-Consistent Video\nDiffusion: A unified architecture that jointly generates aligned RGB and depth\nvideo sequences, conditioned on existing world observation to ensure global\ncoherence 2) Long-Range World Exploration: An efficient world cache with point\nculling and an auto-regressive inference with smooth video sampling for\niterative scene extension with context-aware consistency, and 3) Scalable Data\nEngine: A video reconstruction pipeline that automates camera pose estimation\nand metric depth prediction for arbitrary videos, enabling large-scale, diverse\ntraining data curation without manual 3D annotations. Collectively, these\ndesigns result in a clear improvement over existing methods in visual quality\nand geometric accuracy, with versatile applications.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63425d394c9a81858b36aeb5/cjZH2kR6B3y9IAmmRHNJS.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04225.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63425d394c9a81858b36aeb5",
      "avatarUrl": "/avatars/511ad6a75bd1c10fc510ef527e7f8e5b.svg",
      "fullname": "Tianyu Huang",
      "name": "tyhuang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.04228",
      "authors": [
        {
          "_id": "684103aed45a1fc5540ddc10",
          "name": "Sihui Ji",
          "hidden": false
        },
        {
          "_id": "684103aed45a1fc5540ddc11",
          "name": "Hao Luo",
          "hidden": false
        },
        {
          "_id": "684103aed45a1fc5540ddc12",
          "user": {
            "_id": "644a1b6401e18bf93a6f45c1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644a1b6401e18bf93a6f45c1/P0i_CgCrIzOS2tYRlxoE9.png",
            "isPro": false,
            "fullname": "xichen",
            "user": "xichenhku",
            "type": "user"
          },
          "name": "Xi Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:26:50.027Z",
          "hidden": false
        },
        {
          "_id": "684103aed45a1fc5540ddc13",
          "name": "Yuanpeng Tu",
          "hidden": false
        },
        {
          "_id": "684103aed45a1fc5540ddc14",
          "name": "Yiyang Wang",
          "hidden": false
        },
        {
          "_id": "684103aed45a1fc5540ddc15",
          "name": "Hengshuang Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-04T17:59:58.000Z",
      "submittedOnDailyAt": "2025-06-05T01:11:16.967Z",
      "title": "LayerFlow: 층에 관심 있는 비디오 생성의 유닛 모델",
      "submittedOnDailyBy": {
        "_id": "644a1b6401e18bf93a6f45c1",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644a1b6401e18bf93a6f45c1/P0i_CgCrIzOS2tYRlxoE9.png",
        "isPro": false,
        "fullname": "xichen",
        "user": "xichenhku",
        "type": "user"
      },
      "summary": "LayerFlow는 레이어 정보를 인식한 이미지 생성의 통합 솔루션입니다. 레이어별로 프로ン퓰트를 제공하면 LayerFlow는 투명한 폰트 로크, 깨끗한 배경, 그리고 브렌드된 시나리오를 포함하는 이미지를 생성합니다. 또한 브렌드된 이미지의 분해, 제공된 폰트 로크의 배경을 생성 등 다양한 버전도 지원합니다. 맥락에서 비디오 디퓨저 트랜스포머를 시작하여 각 레이어의 이미지를 서브 克립으로 정리하고 레이어 엔베디ン그를 사용하여 각 克립과 레이어별로 프로ン퓰트를 구분합니다. 이와 같이, 앞서 언급한 버전은 쉽게 하나의 통합 프레임워크에서 지원할 수 있습니다. 고품질의 레이어별 훈련 비디오의 부족으로 인해, 고품질의 레이어 注解付き의 정적 이미지에 대응하기 위해, 단계별 훈련 전략을 설계합니다. 특히, 처음에 저품질의 비디오 데이터로 모델을 훈련시키고, 다음으로 동작의 LoRA를 조정하여 정적 프레임에 대응시키고, 그 후, 고품질의 레이어별 이미지와 복사 붙여넣기된 비디오 데이터의 혼합으로 된 데이터로 콘텐츠의 LoRA를 훈련합니다. 추론 시에는 동작의 LoRA를 제거하고 원하는 레이어의mooth한 비디오를 생성합니다.",
      "upvotes": 12,
      "discussionId": "684103b0d45a1fc5540ddca8",
      "ai_summary": "LayerFlow is a unified framework for generating layer-aware videos using a text-to-video diffusion transformer and layer embeddings, supporting various video generation tasks with a multi-stage training strategy.",
      "ai_keywords": [
        "LayerFlow",
        "text-to-video diffusion transformer",
        "layer embeddings",
        "sub-clips",
        "multi-stage training strategy",
        "motion LoRA",
        "content LoRA",
        "layered images",
        "copy-pasted video data",
        "smooth videos"
      ]
    },
    "publishedAt": "2025-06-04T13:59:58.000Z",
    "title": "LayerFlow: A Unified Model for Layer-aware Video Generation",
    "summary": "We present LayerFlow, a unified solution for layer-aware video generation.\nGiven per-layer prompts, LayerFlow generates videos for the transparent\nforeground, clean background, and blended scene. It also supports versatile\nvariants like decomposing a blended video or generating the background for the\ngiven foreground and vice versa. Starting from a text-to-video diffusion\ntransformer, we organize the videos for different layers as sub-clips, and\nleverage layer embeddings to distinguish each clip and the corresponding\nlayer-wise prompts. In this way, we seamlessly support the aforementioned\nvariants in one unified framework. For the lack of high-quality layer-wise\ntraining videos, we design a multi-stage training strategy to accommodate\nstatic images with high-quality layer annotations. Specifically, we first train\nthe model with low-quality video data. Then, we tune a motion LoRA to make the\nmodel compatible with static frames. Afterward, we train the content LoRA on\nthe mixture of image data with high-quality layered images along with\ncopy-pasted video data. During inference, we remove the motion LoRA thus\ngenerating smooth videos with desired layers.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04228.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "644a1b6401e18bf93a6f45c1",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644a1b6401e18bf93a6f45c1/P0i_CgCrIzOS2tYRlxoE9.png",
      "fullname": "xichen",
      "name": "xichenhku",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 41
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.03139",
      "authors": [
        {
          "_id": "683fb0a7be8421eda3152283",
          "user": {
            "_id": "676b86e79ff0244316f7202f",
            "avatarUrl": "/avatars/3e1d26312a96752356895ab88eeb3ce0.svg",
            "isPro": false,
            "fullname": "chensiqi",
            "user": "xiaoooobai",
            "type": "user"
          },
          "name": "Siqi Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:28:25.792Z",
          "hidden": false
        },
        {
          "_id": "683fb0a7be8421eda3152284",
          "name": "Xinyu Dong",
          "hidden": false
        },
        {
          "_id": "683fb0a7be8421eda3152285",
          "user": {
            "_id": "6692aff88db712bad780f02a",
            "avatarUrl": "/avatars/5dc4b1c27c70f6a64864711dbff4910f.svg",
            "isPro": false,
            "fullname": "xhl",
            "user": "zjuxhl",
            "type": "user"
          },
          "name": "Haolei Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:56:42.705Z",
          "hidden": false
        },
        {
          "_id": "683fb0a7be8421eda3152286",
          "name": "Xingyu Wu",
          "hidden": false
        },
        {
          "_id": "683fb0a7be8421eda3152287",
          "name": "Fei Tang",
          "hidden": false
        },
        {
          "_id": "683fb0a7be8421eda3152288",
          "name": "Hang Zhang",
          "hidden": false
        },
        {
          "_id": "683fb0a7be8421eda3152289",
          "user": {
            "_id": "64098738342c26884c792c93",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64098738342c26884c792c93/SxBUd-wLrl-PjQsrVYJte.jpeg",
            "isPro": false,
            "fullname": "Yuchen Yan",
            "user": "yanyc",
            "type": "user"
          },
          "name": "Yuchen Yan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:56:47.244Z",
          "hidden": false
        },
        {
          "_id": "683fb0a7be8421eda315228a",
          "name": "Linjuan Wu",
          "hidden": false
        },
        {
          "_id": "683fb0a7be8421eda315228b",
          "name": "Wenqi Zhang",
          "hidden": false
        },
        {
          "_id": "683fb0a7be8421eda315228c",
          "name": "Guiyang Hou",
          "hidden": false
        },
        {
          "_id": "683fb0a7be8421eda315228d",
          "name": "Yongliang Shen",
          "hidden": false
        },
        {
          "_id": "683fb0a7be8421eda315228e",
          "name": "Weiming Lu",
          "hidden": false
        },
        {
          "_id": "683fb0a7be8421eda315228f",
          "name": "Yueting Zhuang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T17:58:57.000Z",
      "submittedOnDailyAt": "2025-06-05T03:44:24.728Z",
      "title": "SVGenius: SVG 이해, 편집 및 생성에 대한 LLM의 벤치마크",
      "submittedOnDailyBy": {
        "_id": "5e1058e9fcf41d740b69966d",
        "avatarUrl": "/avatars/ce74839ba871f2b54313a670a233ba82.svg",
        "isPro": false,
        "fullname": "Yongliang Shen",
        "user": "tricktreat",
        "type": "user"
      },
      "summary": "대 언어 모델(LLMs)와 다모뎀 LLMs는 SVG 처리에 있어 기대되는 능력을 보여주고 있지만, 현재의 벤치마크는 현실적인 커버 면적이 제한되어 있고, 복잡도 분류가 없고 평가 패러다임이 분리되어 있습니다. 우리는 이해, 편집, 생성의 3가지 단계를 아우르는 2,377개의 질문을 포함하여, 현실적인 데이터로부터 구축된, 체계적인 복잡도 분류를 갖춘, SVGenius라는 엄격한 벤치마크를 소개합니다. 이 벤치마크는 8개의 태스크 카테고리와 18개의 메트릭을 사용하여 모델을 평가합니다. 22개의主流 모델(각자의 규모, 아키텍처, 학습 패러다임, 접근 가능한 수준이 다릅니다)을 평가합니다. 분석에 따르면, 오픈 소스 모델에 대해서는 프로피어 모델이 유의미하게 우위를 줍니다が, 모든 모델은 복잡도가 증가함에 따라 체계적인 성능 저하를 나타내며, 현재의 접근 방식의 기본적인 한계를 보여주고 있습니다. 하지만 논리론을 강화한 학습은 이러한 한계를 극복하는 데 더 효과적이지만, 모든 모델 종류에서 가장 어려운 능력으로 스타일 트랜스폼이 있습니다. SVGenius는 SVG 처리의 첫 번째 엄격한 평가 프레임워크를 구축하고, 벡터 그래픽 모델의 개발과 자동화 그래픽 디자인 애플리케이션의 발전에 중요한 통찰을 제공합니다. 추가 자료와 보충 데이터(데이터와 코드를 포함)는 https://zju-real.github.io/SVGenius에서 사용 가능합니다.",
      "upvotes": 12,
      "discussionId": "683fb0a7be8421eda31522ca",
      "projectPage": "https://zju-real.github.io/SVGenius/",
      "githubRepo": "https://github.com/ZJU-REAL/SVGenius",
      "ai_summary": "SVGenius evaluates Large Language Models and Multimodal LLMs for SVG processing using a comprehensive benchmark across three dimensions: understanding, editing, and generation, revealing insights into model capabilities and limitations.",
      "ai_keywords": [
        "Large Language Models",
        "Multimodal LLMs",
        "SVG processing",
        "SVGenius",
        "complexity stratification",
        "reasoning-enhanced training",
        "style transfer"
      ]
    },
    "publishedAt": "2025-06-03T13:58:57.000Z",
    "title": "SVGenius: Benchmarking LLMs in SVG Understanding, Editing and Generation",
    "summary": "Large Language Models (LLMs) and Multimodal LLMs have shown promising\ncapabilities for SVG processing, yet existing benchmarks suffer from limited\nreal-world coverage, lack of complexity stratification, and fragmented\nevaluation paradigms. We introduce SVGenius, a comprehensive benchmark\ncomprising 2,377 queries across three progressive dimensions: understanding,\nediting, and generation. Built on real-world data from 24 application domains\nwith systematic complexity stratification, SVGenius evaluates models through 8\ntask categories and 18 metrics. We assess 22 mainstream models spanning\ndifferent scales, architectures, training paradigms, and accessibility levels.\nOur analysis reveals that while proprietary models significantly outperform\nopen-source counterparts, all models exhibit systematic performance degradation\nwith increasing complexity, indicating fundamental limitations in current\napproaches; however, reasoning-enhanced training proves more effective than\npure scaling for overcoming these limitations, though style transfer remains\nthe most challenging capability across all model types. SVGenius establishes\nthe first systematic evaluation framework for SVG processing, providing crucial\ninsights for developing more capable vector graphics models and advancing\nautomated graphic design applications. Appendix and supplementary materials\n(including all data and code) are available at\nhttps://zju-real.github.io/SVGenius.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03139.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5e1058e9fcf41d740b69966d",
      "avatarUrl": "/avatars/ce74839ba871f2b54313a670a233ba82.svg",
      "fullname": "Yongliang Shen",
      "name": "tricktreat",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 22
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.03295",
      "authors": [
        {
          "_id": "6840e7d81fadbc85ae3bdc0f",
          "name": "Yubo Wang",
          "hidden": false
        },
        {
          "_id": "6840e7d81fadbc85ae3bdc10",
          "name": "Ping Nie",
          "hidden": false
        },
        {
          "_id": "6840e7d81fadbc85ae3bdc11",
          "name": "Kai Zou",
          "hidden": false
        },
        {
          "_id": "6840e7d81fadbc85ae3bdc12",
          "name": "Lijun Wu",
          "hidden": false
        },
        {
          "_id": "6840e7d81fadbc85ae3bdc13",
          "name": "Wenhu Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T18:35:52.000Z",
      "submittedOnDailyAt": "2025-06-05T02:48:14.083Z",
      "title": "통찰에 의해 사전 학습된 LLM의 논리적인 잠재력을 해방\n문제에 대한 미세 조정",
      "submittedOnDailyBy": {
        "_id": "636a35eff8d9af4aea181608",
        "avatarUrl": "/avatars/d9c5cf3491243d1f2b1c5df1873ee8e7.svg",
        "isPro": false,
        "fullname": "yubo",
        "user": "ubowang",
        "type": "user"
      },
      "summary": "우리는 강력한 LLMs（예：Qwen-Math、MiMo、Phi-4）가 학습 전 단계부터 기적적으로 큰 논리적인 가능성을 증명하는 것을 보았다. 강화학습(RL)을 사용하여 이러한 모델들은 논리적인 태스크에서 크게 개선할 수 있다. 최근의 연구에 따르면, 단일 문제를 위한 RL이 이 모델의 논리적인 능력을 해방할 수 있다는 것을 보여주고 있다. 그러나 RL은 상당히 비용적으로 고비하고 안정적이지 않다. 간단한 RL도, 수백 힙웨어 시간이 필요하며, 이러한 이유로 중요한 질문이 제기된다: 이러한 강력한 기초 모델의 논리적인 가능성을 해방하기 위해 더 효율적인 방법이 있을까? 이 연구에서는 이러한 방법을 보여주고, 단일 문제를 위한 Critique Fine-Tuning(CFT)이 LLMs의 논리적인 가능성을 효과적으로 해방하는 것을 보여준다. 우리 방법은 단일 문제를 위한 다양한 모델이 생성된 해결책을 모으고, 교사 모델 LLMs가 상세한 비판을 제공하여 비판 데이터를 구축한다. Qwen 및 Llama 가족 모델(파라미터 수: 1.5B에서 14B)을 CFT 데이터에 대해 미세 조정하고, 다양한 논리적인 태스크에서 명백한 성능 향상을 보였다. 예를 들어, 5 힙웨어 시간의 훈련에서도 Qwen-Math-7B-CFT는 6개의 수학 벤치마크에서 평균 15%의 향상, 3개의 로직이론 벤치마크에서 16%의 향상을 보여주었다. 이러한 결과는 20배의 계산량을 줄인 RL의 결과와 비교하여 비슷한 것일 뿐만 아니라, 단일 문제를 위한 CFT의 효율성과 일반성을 보여주고 있다. 소멸 조사에 따르면, 단일 문제를 위한 CFT의 강건성은 다른 프롬프트 문제에서도 나타난다. 이러한 결과는 현대의 LLMs의 논리적인 능력을 해방하기 위해 간단하고 계산량에 여유가 있는 방법이라는 것을 보여준다.",
      "upvotes": 10,
      "discussionId": "6840e7d81fadbc85ae3bdc45",
      "ai_summary": "Critique Fine-Tuning on a single problem can efficiently enhance the reasoning capabilities of large language models with significant performance gains and reduced computational cost compared to reinforcement learning.",
      "ai_keywords": [
        "Critique Fine-Tuning",
        "teacher LLMs",
        "Qwen-Math",
        "Llama family models",
        "reasoning tasks",
        "one-shot CFT",
        "performance gains",
        "logic reasoning benchmarks",
        "math benchmarks",
        "prompt problems"
      ]
    },
    "publishedAt": "2025-06-03T14:35:52.000Z",
    "title": "Unleashing the Reasoning Potential of Pre-trained LLMs by Critique\n  Fine-Tuning on One Problem",
    "summary": "We have witnessed that strong LLMs like Qwen-Math, MiMo, and Phi-4 possess\nimmense reasoning potential inherited from the pre-training stage. With\nreinforcement learning (RL), these models can improve dramatically on reasoning\ntasks. Recent studies have shown that even RL on a single problem can unleash\nthese models' reasoning capabilities. However, RL is not only expensive but\nalso unstable. Even one-shot RL requires hundreds of GPU hours. This raises a\ncritical question: Is there a more efficient way to unleash the reasoning\npotential of these powerful base LLMs? In this work, we demonstrate that\nCritique Fine-Tuning (CFT) on only one problem can effectively unleash the\nreasoning potential of LLMs. Our method constructs critique data by collecting\ndiverse model-generated solutions to a single problem and using teacher LLMs to\nprovide detailed critiques. We fine-tune Qwen and Llama family models, ranging\nfrom 1.5B to 14B parameters, on the CFT data and observe significant\nperformance gains across diverse reasoning tasks. For example, with just 5 GPU\nhours of training, Qwen-Math-7B-CFT show an average improvement of 15% on six\nmath benchmarks and 16% on three logic reasoning benchmarks. These results are\ncomparable to or even surpass the results from RL with 20x less compute.\nAblation studies reveal the robustness of one-shot CFT across different prompt\nproblems. These results highlight one-shot CFT as a simple, general, and\ncompute-efficient approach to unleashing the reasoning capabilities of modern\nLLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03295.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "636a35eff8d9af4aea181608",
      "avatarUrl": "/avatars/d9c5cf3491243d1f2b1c5df1873ee8e7.svg",
      "fullname": "yubo",
      "name": "ubowang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.24500",
      "authors": [
        {
          "_id": "683fb063ef97de05eb2a44cc",
          "name": "Guiyang Hou",
          "hidden": false
        },
        {
          "_id": "683fb063ef97de05eb2a44cd",
          "name": "Xing Gao",
          "hidden": false
        },
        {
          "_id": "683fb063ef97de05eb2a44ce",
          "name": "Yuchuan Wu",
          "hidden": false
        },
        {
          "_id": "683fb063ef97de05eb2a44cf",
          "name": "Xiang Huang",
          "hidden": false
        },
        {
          "_id": "683fb063ef97de05eb2a44d0",
          "name": "Wenqi Zhang",
          "hidden": false
        },
        {
          "_id": "683fb063ef97de05eb2a44d1",
          "name": "Zhe Zheng",
          "hidden": false
        },
        {
          "_id": "683fb063ef97de05eb2a44d2",
          "name": "Yongliang Shen",
          "hidden": false
        },
        {
          "_id": "683fb063ef97de05eb2a44d3",
          "name": "Jialu Du",
          "hidden": false
        },
        {
          "_id": "683fb063ef97de05eb2a44d4",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "683fb063ef97de05eb2a44d5",
          "name": "Yongbin Li",
          "hidden": false
        },
        {
          "_id": "683fb063ef97de05eb2a44d6",
          "name": "Weiming Lu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-30T12:01:06.000Z",
      "submittedOnDailyAt": "2025-06-05T00:45:50.911Z",
      "title": "시간 인지 하이퍼 코너 강화 학습: 시간 시퀀스 정보 인식을 위한 하이퍼 코너 강화 학습의 적용",
      "submittedOnDailyBy": {
        "_id": "67c03110e8c7d56a8e135ac8",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/eP3y_8_tyB8tcrT7py4L7.png",
        "isPro": false,
        "fullname": "Hou",
        "user": "Guiyang1001",
        "type": "user"
      },
      "summary": "최근, 대언어 모뎀(LLMs)는 수학이나 코딩과 같은 IQ 관련 분야에서 신중한 사고가 필요한 분야에서 뚜렷한 진전을 이룩하고 있습니다. 그러나, LLMs의 인지 개발이 사회 영역에 특히 훈련 후의 관점에서 고취하는 것은 아직 조사 부족한 상태입니다.\n\n사회의 세계는 다른 시간선을 가지고 있으며, 직감적인 반응(System 1)과 표면 수준의 생각과 신중한 생각(System 2)의 풍부한 조합이 수학보다 더 명확하게 필요합니다. 수학은 주로 System 2의 인지(신중한, 단계별로 이유를 설명하는)를 의존하기 때문에, 이러한 인지 모드의 조합이 필요함을 알 수 있습니다. 이에 대처하여, 우리는 사회 지능을 향상시키기 위한 시간과 관련된 인지의 강화 학습(TimeHC-RL)을 도입합니다.\n\n실험에서, 8개의 데이터 세트에 대해 5개의 훈련 후의 패러다임과 2개의 테스트 시의 인터랙션 패러다임으로 LLMs의 사회 지능을 향상시키고 TimeHC-RL의 방법의 효과를 평가했습니다. 실험 결과를 통해, 제안한 TimeHC-RL 방법이 제안된 System 2 RL 방법보다 더 높은 성능을 보여주는 것을 명확히 알 수 있었습니다. 이 방법은 7B 기반 모델을 날개로, DeepSeek-R1이나 OpenAI-O3과 같은 발전된 모델의 성능을 비교할 수 있도록 하였습니다. 또한, 훈련 후와 테스트 시의 인터랙션을 통해 사회 지능을 향상시키기 위한 체계적인 탐색은 많은 가치 있는 피드백을 제공했습니다.",
      "upvotes": 10,
      "discussionId": "683fb064ef97de05eb2a452b",
      "ai_summary": "Temporal-aware Hierarchical Cognitive Reinforcement Learning enhances LLMs' social intelligence by addressing the distinct cognitive demands of social domains.",
      "ai_keywords": [
        "Large Language Models",
        "Temporal-aware Hierarchical Cognitive Reinforcement Learning",
        "TimeHC-RL",
        "System 1",
        "System 2",
        "RL",
        "DeepSeek-R1",
        "OpenAI-O3"
      ]
    },
    "publishedAt": "2025-05-30T08:01:06.000Z",
    "title": "TimeHC-RL: Temporal-aware Hierarchical Cognitive Reinforcement Learning\n  for Enhancing LLMs' Social Intelligence",
    "summary": "Recently, Large Language Models (LLMs) have made significant progress in\nIQ-related domains that require careful thinking, such as mathematics and\ncoding. However, enhancing LLMs' cognitive development in social domains,\nparticularly from a post-training perspective, remains underexplored.\nRecognizing that the social world follows a distinct timeline and requires a\nricher blend of cognitive modes (from intuitive reactions (System 1) and\nsurface-level thinking to deliberate thinking (System 2)) than mathematics,\nwhich primarily relies on System 2 cognition (careful, step-by-step reasoning),\nwe introduce Temporal-aware Hierarchical Cognitive Reinforcement Learning\n(TimeHC-RL) for enhancing LLMs' social intelligence. In our experiments, we\nsystematically explore improving LLMs' social intelligence and validate the\neffectiveness of the TimeHC-RL method, through five other post-training\nparadigms and two test-time intervention paradigms on eight datasets with\ndiverse data patterns. Experimental results reveal the superiority of our\nproposed TimeHC-RL method compared to the widely adopted System 2 RL method. It\ngives the 7B backbone model wings, enabling it to rival the performance of\nadvanced models like DeepSeek-R1 and OpenAI-O3. Additionally, the systematic\nexploration from post-training and test-time interventions perspectives to\nimprove LLMs' social intelligence has uncovered several valuable insights.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24500.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67c03110e8c7d56a8e135ac8",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/eP3y_8_tyB8tcrT7py4L7.png",
      "fullname": "Hou",
      "name": "Guiyang1001",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.04158",
      "authors": [
        {
          "_id": "6840fb71d4e16ff5f95108aa",
          "name": "Yujia Hu",
          "hidden": false
        },
        {
          "_id": "6840fb71d4e16ff5f95108ab",
          "name": "Songhua Liu",
          "hidden": false
        },
        {
          "_id": "6840fb71d4e16ff5f95108ac",
          "name": "Zhenxiong Tan",
          "hidden": false
        },
        {
          "_id": "6840fb71d4e16ff5f95108ad",
          "user": {
            "_id": "634cfebc350bcee9bed20a4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/634cfebc350bcee9bed20a4d/fN47nN5rhw-HJaFLBZWQy.png",
            "isPro": false,
            "fullname": "Xingyi Yang",
            "user": "adamdad",
            "type": "user"
          },
          "name": "Xingyi Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:27:11.256Z",
          "hidden": false
        },
        {
          "_id": "6840fb71d4e16ff5f95108ae",
          "name": "Xinchao Wang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/634cfebc350bcee9bed20a4d/tp0cLz8OZhdI3vs50cxhF.jpeg"
      ],
      "publishedAt": "2025-06-04T16:57:24.000Z",
      "submittedOnDailyAt": "2025-06-05T00:38:20.484Z",
      "title": "이미지 편집은 Diffusion Models를 사용하는 프로그램입니다.",
      "submittedOnDailyBy": {
        "_id": "634cfebc350bcee9bed20a4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/634cfebc350bcee9bed20a4d/fN47nN5rhw-HJaFLBZWQy.png",
        "isPro": false,
        "fullname": "Xingyi Yang",
        "user": "adamdad",
        "type": "user"
      },
      "summary": "Diffusion 모델은 텍스트로부터 이미지 생성에서 놀라울 정도로 성공을 거뒀지만, 명령어에 의한 이미지 편집에 있어 큰 문제를 발견하고 있습니다. 본 연구에서는 중요한 문제를 밝혀내고 있습니다: 이러한 모델은 구조적으로 불일치한 편집에 특히 어려움을 겪습니다. 이를 보완하기 위해, Image Editing As Programs (IEAP)라는 Diffusion Transformer (DiT) 아키텍처를 기반으로 한 통합된 이미지 편집 프레임워크를 사용합니다. IEAP의 핵심은 명령어 기반 편집을 단순화된 관점에서 처리하고, 복잡한 편집 지시를 원操作의 순서로 분해합니다. 각 동작은 동일한 DiT 백엔드를 공유하며, 특정 편집의 종류에 전문화되어 있습니다. Vision-Language Model (VLM) 기반의 에이전트가 프로그래밍되어, 이러한 동작은 임의적이고 구조적으로 불일치한 변환을 지원합니다. 이러한 모듈화와 편집의 순서대로 처리함으로써, IEAP는 단순한 조정부터 큰 구조적인 변경까지 광범위하게 편집 태스크에 강하게 일반화합니다. 확장된 실험은 표준 벤치마크에서 IEAP가 다양한 편집 시나리오에 대해 가장 先進한 방법보다 크게 뛰어넘는 것을 보여줍니다. 이러한 평가에서, 우리의 프레임워크는 특히 복잡한, 단계별 지시에 대해 높은 정확성과 의미적 충실도를 제공합니다. 코드는 https://github.com/YujiaHu1109/IEAP에 접근 가능합니다.",
      "upvotes": 8,
      "discussionId": "6840fb73d4e16ff5f9510950",
      "projectPage": "https://yujiahu1109.github.io/IEAP/",
      "githubRepo": "https://github.com/YujiaHu1109/IEAP",
      "ai_summary": "A unified image editing framework, IEAP, built on Diffusion Transformer (DiT) decomposes complex editing instructions into operations performed by vision-language models for robust editing across various tasks.",
      "ai_keywords": [
        "diffusion models",
        "text-to-image generation",
        "instruction-driven image editing",
        "structurally inconsistent edits",
        "Image Editing As Programs (IEAP)",
        "Diffusion Transformer (DiT)",
        "atomic operations",
        "lightweight adapter",
        "vision-language model (VLM)",
        "modularizing edits"
      ]
    },
    "publishedAt": "2025-06-04T12:57:24.000Z",
    "title": "Image Editing As Programs with Diffusion Models",
    "summary": "While diffusion models have achieved remarkable success in text-to-image\ngeneration, they encounter significant challenges with instruction-driven image\nediting. Our research highlights a key challenge: these models particularly\nstruggle with structurally inconsistent edits that involve substantial layout\nchanges. To mitigate this gap, we introduce Image Editing As Programs (IEAP), a\nunified image editing framework built upon the Diffusion Transformer (DiT)\narchitecture. At its core, IEAP approaches instructional editing through a\nreductionist lens, decomposing complex editing instructions into sequences of\natomic operations. Each operation is implemented via a lightweight adapter\nsharing the same DiT backbone and is specialized for a specific type of edit.\nProgrammed by a vision-language model (VLM)-based agent, these operations\ncollaboratively support arbitrary and structurally inconsistent\ntransformations. By modularizing and sequencing edits in this way, IEAP\ngeneralizes robustly across a wide range of editing tasks, from simple\nadjustments to substantial structural changes. Extensive experiments\ndemonstrate that IEAP significantly outperforms state-of-the-art methods on\nstandard benchmarks across various editing scenarios. In these evaluations, our\nframework delivers superior accuracy and semantic fidelity, particularly for\ncomplex, multi-step instructions. Codes are available at\nhttps://github.com/YujiaHu1109/IEAP.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/634cfebc350bcee9bed20a4d/tp0cLz8OZhdI3vs50cxhF.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04158.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "634cfebc350bcee9bed20a4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/634cfebc350bcee9bed20a4d/fN47nN5rhw-HJaFLBZWQy.png",
      "fullname": "Xingyi Yang",
      "name": "adamdad",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 17
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.04141",
      "authors": [
        {
          "_id": "684106fc8cb0edba3ab212bb",
          "name": "Kejian Zhu",
          "hidden": false
        },
        {
          "_id": "684106fc8cb0edba3ab212bc",
          "name": "Zhuoran Jin",
          "hidden": false
        },
        {
          "_id": "684106fc8cb0edba3ab212bd",
          "name": "Hongbang Yuan",
          "hidden": false
        },
        {
          "_id": "684106fc8cb0edba3ab212be",
          "name": "Jiachun Li",
          "hidden": false
        },
        {
          "_id": "684106fc8cb0edba3ab212bf",
          "name": "Shangqing Tu",
          "hidden": false
        },
        {
          "_id": "684106fc8cb0edba3ab212c0",
          "name": "Pengfei Cao",
          "hidden": false
        },
        {
          "_id": "684106fc8cb0edba3ab212c1",
          "name": "Yubo Chen",
          "hidden": false
        },
        {
          "_id": "684106fc8cb0edba3ab212c2",
          "name": "Kang Liu",
          "hidden": false
        },
        {
          "_id": "684106fc8cb0edba3ab212c3",
          "name": "Jun Zhao",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/648c48d8c0ddeee6df5b6d22/_-WD0IU9jQMInqhXgGfUP.jpeg"
      ],
      "publishedAt": "2025-06-04T16:33:41.000Z",
      "submittedOnDailyAt": "2025-06-05T04:38:26.132Z",
      "title": "MMR-V: 어떤 것이 말하지 않고 남아있는가? 비디오에서 다모형 구조의 깊은 논리론의 기준점",
      "submittedOnDailyBy": {
        "_id": "648c48d8c0ddeee6df5b6d22",
        "avatarUrl": "/avatars/8706b0b16dfc332b96c91d3ced31bd0b.svg",
        "isPro": false,
        "fullname": "Shangqing Tu",
        "user": "tsq2000",
        "type": "user"
      },
      "summary": "ビデオ의 순차적 구조는 다모형 대 언어 모델(MLLM)이 다양한 프레임의 증거를 특정하고 다모형 논리로 수행하는 능력을 도전하고 있습니다. 그러나 현재의 비디오 벤치마크는 주로 이해 태스크에 초점을 맞추어 있으며, 모델이 문제에 언급된 프레임(이하 \"문제 프레임\"으로 불리며)과 인접한 몇 프레임을 인식하는 정도만 충분하다고 요구하고 있습니다. 이러한 결점을 해결하기 위해, 우리는 MMR-V(영상 중 다모형 심층 논리 벤치마크)를 제안합니다. 이 벤치마크는 다음과 같은 특징을 가지고 있습니다. (1) 긴 거리, 다 프레임 논리: 모델은 문제 프레임으로부터 멀리 있는 증거 프레임을 추론하고 분석해야 합니다. (2) 인식보다 멀리: 문제는 직접적인 인식으로 해결할 수 없으며, 숨겨진 정보를 논리적으로 처리해야 합니다. (3) 신뢰성: 모든 태스크는 직접적인 Annotation으로 이루어지며, 광범위한 실세계 사용자의 이해를 참조하여 일반적인 인식에 맞습니다. (4) 혼동성: 신중하게 설계된 데트랙터 Annotation 전략을 활용하여 모델의 단점을 줄였습니다. MMR-V는 317개의 비디오와 1,257개의 태스크로 구성되어 있습니다. 우리의 실험에 따르면 현재의 모델은 다모형 논리에 어려움을 겪고 있으며, 가장 우수한 모델인 o4-mini도 정확도가 52.5%입니다. 또한 현재의 논리 확장 전략(Chain-of-Thought와 테스트 시간 컴퓨팅의 스케일링)은 제한된 효과를 보였습니다. 진화하는 분석에 따르면 Chain-of-Thought는 문헌 논리와 다름이 있음을 알게 되었으며, 이는 성능의 한계 등 이유의 일부를 설명합니다. 우리는 MMR-V가 다모형 논리 능력 향상에 대한 발전을 촉진하고자 합니다.",
      "upvotes": 7,
      "discussionId": "684106ff8cb0edba3ab21374",
      "ai_summary": "A new benchmark, MMR-V, is proposed to challenge multimodal large language models with long-range, multi-frame reasoning and hidden information processing in videos, revealing their limitations and inspiring further research.",
      "ai_keywords": [
        "multimodal large language models",
        "MMR-V",
        "long-range",
        "multi-frame reasoning",
        "multimodal reasoning",
        "manual annotation",
        "distractor annotation strategy",
        "Chain-of-Thought"
      ]
    },
    "publishedAt": "2025-06-04T12:33:41.000Z",
    "title": "MMR-V: What's Left Unsaid? A Benchmark for Multimodal Deep Reasoning in\n  Videos",
    "summary": "The sequential structure of videos poses a challenge to the ability of\nmultimodal large language models (MLLMs) to locate multi-frame evidence and\nconduct multimodal reasoning. However, existing video benchmarks mainly focus\non understanding tasks, which only require models to match frames mentioned in\nthe question (hereafter referred to as \"question frame\") and perceive a few\nadjacent frames. To address this gap, we propose MMR-V: A Benchmark for\nMultimodal Deep Reasoning in Videos. The benchmark is characterized by the\nfollowing features. (1) Long-range, multi-frame reasoning: Models are required\nto infer and analyze evidence frames that may be far from the question frame.\n(2) Beyond perception: Questions cannot be answered through direct perception\nalone but require reasoning over hidden information. (3) Reliability: All tasks\nare manually annotated, referencing extensive real-world user understanding to\nalign with common perceptions. (4) Confusability: Carefully designed distractor\nannotation strategies to reduce model shortcuts. MMR-V consists of 317 videos\nand 1,257 tasks. Our experiments reveal that current models still struggle with\nmulti-modal reasoning; even the best-performing model, o4-mini, achieves only\n52.5% accuracy. Additionally, current reasoning enhancement strategies\n(Chain-of-Thought and scaling test-time compute) bring limited gains. Further\nanalysis indicates that the CoT demanded for multi-modal reasoning differs from\nit in textual reasoning, which partly explains the limited performance gains.\nWe hope that MMR-V can inspire further research into enhancing multi-modal\nreasoning capabilities.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/648c48d8c0ddeee6df5b6d22/_-WD0IU9jQMInqhXgGfUP.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04141.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "648c48d8c0ddeee6df5b6d22",
      "avatarUrl": "/avatars/8706b0b16dfc332b96c91d3ced31bd0b.svg",
      "fullname": "Shangqing Tu",
      "name": "tsq2000",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.03930",
      "authors": [
        {
          "_id": "6841090145662bb7d322ecc6",
          "user": {
            "_id": "64de37ee5e192985054be575",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64de37ee5e192985054be575/fVV7JQMtp_J3uFqszJJHH.jpeg",
            "isPro": false,
            "fullname": "Yuansheng Ni",
            "user": "yuanshengni",
            "type": "user"
          },
          "name": "Yuansheng Ni",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T10:00:04.196Z",
          "hidden": false
        },
        {
          "_id": "6841090145662bb7d322ecc7",
          "name": "Ping Nie",
          "hidden": false
        },
        {
          "_id": "6841090145662bb7d322ecc8",
          "name": "Kai Zou",
          "hidden": false
        },
        {
          "_id": "6841090145662bb7d322ecc9",
          "name": "Xiang Yue",
          "hidden": false
        },
        {
          "_id": "6841090145662bb7d322ecca",
          "name": "Wenhu Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-04T13:24:44.000Z",
      "submittedOnDailyAt": "2025-06-05T05:54:39.464Z",
      "title": "VisCoder: LLM를 통해 실행 가능한 Python 시각화 코드의 생성\n\n(注意：虽然您要求不添加解释或额外的文本，但为了确保翻译的准确性和专业性，我在翻译中保留了原文的格式和结构，以保持一致性。)",
      "submittedOnDailyBy": {
        "_id": "64de37ee5e192985054be575",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64de37ee5e192985054be575/fVV7JQMtp_J3uFqszJJHH.jpeg",
        "isPro": false,
        "fullname": "Yuansheng Ni",
        "user": "yuanshengni",
        "type": "user"
      },
      "summary": "대 언어 모형(LLMs)는 시각화 작업에 있어서 코드의 정확성과 시각적 의미의 둘 다 고려가 필요합니다. 현재의 인스톰션 튜닝 데이터 셋은 실행에 기반한 지원을 미비하고 반복적인 코드 수정에만 국한되어 취약하고 불신뢰적인 플롯 생성에 이르게 됩니다. 우리는 Python 기반의 시각화 및 자기 수정을 위한 대형 인스톰션 튜닝 데이터 셋 \"VisCode-200K\"을 소개합니다. 이는 200K 이상의 예를 포함하며 다음과 같은 내용을 가지고 있습니다: 1) 오픈 소스 리포지토리에서 검증된 시각화 코드, 자연어 인스톰션과 시각화된 플롯의 조합, 2) Code-Feedback에서 45K회의 여러 회차의 수정 다이어로그, 이를 모델이 실행 시의 피드백을 사용하여 필터링된 코드를 수정할 수 있게 합니다. Qwen2.5-Coder-Instruct를 VisCode-200K에 미세 조정하여 \"VisCoder\"을 만들었습니다. VisCoder은 강력한 오픈 소스 기반 기준과 비교하여 유의미하게 뛰어나며, GPT-4o-mini와 같은 프로피티 모형의 성능에 가까워집니다. 또한 반복적인 수정을 평가하기 위해 자신의 디버깅 평가 프로토콜을 도입하여 실행 가능한 시각적으로 정확한 코드 생성에 대한 피드백 주도 학습의 베타를 보여줍니다.",
      "upvotes": 7,
      "discussionId": "6841090245662bb7d322ed1f",
      "projectPage": "https://tiger-ai-lab.github.io/VisCoder/",
      "githubRepo": "https://github.com/TIGER-AI-Lab/VisCoder",
      "ai_summary": "VisCode-200K, a large-scale dataset for visualization, improves plot generation performance by integrating execution-grounded supervision and iterative code correction, outperforming open-source models and rivaling proprietary ones.",
      "ai_keywords": [
        "large language models",
        "LLMs",
        "visualization tasks",
        "plot generation",
        "execution-grounded supervision",
        "iterative code correction",
        "VisCode-200K",
        "Python-based visualization",
        "validated plotting code",
        "natural language instructions",
        "rendered plots",
        "correction dialogues",
        "Qwen2.5-Coder-Instruct",
        "VisCoder",
        "PandasPlotBench",
        "self-debug evaluation",
        "feedback-driven learning"
      ]
    },
    "publishedAt": "2025-06-04T09:24:44.000Z",
    "title": "VisCoder: Fine-Tuning LLMs for Executable Python Visualization Code\n  Generation",
    "summary": "Large language models (LLMs) often struggle with visualization tasks like\nplotting diagrams, charts, where success depends on both code correctness and\nvisual semantics. Existing instruction-tuning datasets lack execution-grounded\nsupervision and offer limited support for iterative code correction, resulting\nin fragile and unreliable plot generation. We present VisCode-200K, a\nlarge-scale instruction tuning dataset for Python-based visualization and\nself-correction. It contains over 200K examples from two sources: (1) validated\nplotting code from open-source repositories, paired with natural language\ninstructions and rendered plots; and (2) 45K multi-turn correction dialogues\nfrom Code-Feedback, enabling models to revise faulty code using runtime\nfeedback. We fine-tune Qwen2.5-Coder-Instruct on VisCode-200K to create\nVisCoder, and evaluate it on PandasPlotBench. VisCoder significantly\noutperforms strong open-source baselines and approaches the performance of\nproprietary models like GPT-4o-mini. We further adopt a self-debug evaluation\nprotocol to assess iterative repair, demonstrating the benefits of\nfeedback-driven learning for executable, visually accurate code generation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03930.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64de37ee5e192985054be575",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64de37ee5e192985054be575/fVV7JQMtp_J3uFqszJJHH.jpeg",
      "fullname": "Yuansheng Ni",
      "name": "yuanshengni",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 13
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.03517",
      "authors": [
        {
          "_id": "68412f853c22997c7329f3a0",
          "user": {
            "_id": "62980664ff0acd7e027d6686",
            "avatarUrl": "/avatars/364d4c8432c24775a099641fc576dbdc.svg",
            "isPro": false,
            "fullname": "Ziyi Wu",
            "user": "Dazitu616",
            "type": "user"
          },
          "name": "Ziyi Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:26:43.314Z",
          "hidden": false
        },
        {
          "_id": "68412f853c22997c7329f3a1",
          "name": "Anil Kag",
          "hidden": false
        },
        {
          "_id": "68412f853c22997c7329f3a2",
          "name": "Ivan Skorokhodov",
          "hidden": false
        },
        {
          "_id": "68412f853c22997c7329f3a3",
          "name": "Willi Menapace",
          "hidden": false
        },
        {
          "_id": "68412f853c22997c7329f3a4",
          "name": "Ashkan Mirzaei",
          "hidden": false
        },
        {
          "_id": "68412f853c22997c7329f3a5",
          "name": "Igor Gilitschenski",
          "hidden": false
        },
        {
          "_id": "68412f853c22997c7329f3a6",
          "name": "Sergey Tulyakov",
          "hidden": false
        },
        {
          "_id": "68412f853c22997c7329f3a7",
          "name": "Aliaksandr Siarohin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-04T03:06:08.000Z",
      "submittedOnDailyAt": "2025-06-05T04:18:57.242Z",
      "title": "DenseDPO: 비디오의 미세한 시간적 선호 최적화",
      "submittedOnDailyBy": {
        "_id": "62980664ff0acd7e027d6686",
        "avatarUrl": "/avatars/364d4c8432c24775a099641fc576dbdc.svg",
        "isPro": false,
        "fullname": "Ziyi Wu",
        "user": "Dazitu616",
        "type": "user"
      },
      "summary": "直接偏好最適化（Direct Preference Optimization, DPO）는 최근에 文から映画의 拡散モデル의トレーニング後の技術であり、トレーニングデータを得るためには、2つの独立したノイズから生成された映画の好みを提供することを要求されます。しかし、このアプローチは細かい比較を禁止し、動きの少ないクリップにバイアスを与えることを指摘します。本研究では、DenseDPOという方法を導入し、これらの欠点を解決するために3つの貢献を提供します。まず、DPOでは、真の映画のノイズをコーラストしたものを用いてビデオペアを作成します。これにより、運動構造が類似していて、局所的な詳細が異なるようなアラインされたペアを生成し、動きのバイアスを中和します。次に、これによって得られた時間的なアラインメントを利用して、短いセグメントでの好みをラベル付けすることで、より稠密で正確な学習信号を得ます。ラベルされたデータの1/3だけで、DenseDPOは動きの生成を大幅に向上させ、文のアラインメント、視覚的な質、時間的な一貫性についてはベージャDPOと同じ性能を維持します。最後に、DenseDPOはオフショットビジョン言語モデル（VLMs）を使用して自動的な好みラベルを生成することを示します：GPTはタスクに特化された訓練された映画報酬モデルと同様に段階レベルの好みを予測し、これによって訓練されたDenseDPOは人間のラベルを使用した場合と近い性能を達成します。",
      "upvotes": 7,
      "discussionId": "68412f8a3c22997c7329f4ff",
      "projectPage": "https://snap-research.github.io/DenseDPO/"
    },
    "publishedAt": "2025-06-03T23:06:08.000Z",
    "title": "DenseDPO: Fine-Grained Temporal Preference Optimization for Video\n  Diffusion Models",
    "summary": "Direct Preference Optimization (DPO) has recently been applied as a\npost-training technique for text-to-video diffusion models. To obtain training\ndata, annotators are asked to provide preferences between two videos generated\nfrom independent noise. However, this approach prohibits fine-grained\ncomparisons, and we point out that it biases the annotators towards low-motion\nclips as they often contain fewer visual artifacts. In this work, we introduce\nDenseDPO, a method that addresses these shortcomings by making three\ncontributions. First, we create each video pair for DPO by denoising corrupted\ncopies of a ground truth video. This results in aligned pairs with similar\nmotion structures while differing in local details, effectively neutralizing\nthe motion bias. Second, we leverage the resulting temporal alignment to label\npreferences on short segments rather than entire clips, yielding a denser and\nmore precise learning signal. With only one-third of the labeled data, DenseDPO\ngreatly improves motion generation over vanilla DPO, while matching it in text\nalignment, visual quality, and temporal consistency. Finally, we show that\nDenseDPO unlocks automatic preference annotation using off-the-shelf Vision\nLanguage Models (VLMs): GPT accurately predicts segment-level preferences\nsimilar to task-specifically fine-tuned video reward models, and DenseDPO\ntrained on these labels achieves performance close to using human labels.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03517.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62980664ff0acd7e027d6686",
      "avatarUrl": "/avatars/364d4c8432c24775a099641fc576dbdc.svg",
      "fullname": "Ziyi Wu",
      "name": "Dazitu616",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.04142",
      "authors": [
        {
          "_id": "684132cb725b7fb67f68ffb8",
          "name": "Kejian Zhu",
          "hidden": false
        },
        {
          "_id": "684132cb725b7fb67f68ffb9",
          "name": "Shangqing Tu",
          "hidden": false
        },
        {
          "_id": "684132cb725b7fb67f68ffba",
          "name": "Zhuoran Jin",
          "hidden": false
        },
        {
          "_id": "684132cb725b7fb67f68ffbb",
          "name": "Lei Hou",
          "hidden": false
        },
        {
          "_id": "684132cb725b7fb67f68ffbc",
          "name": "Juanzi Li",
          "hidden": false
        },
        {
          "_id": "684132cb725b7fb67f68ffbd",
          "name": "Jun Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-04T16:33:44.000Z",
      "submittedOnDailyAt": "2025-06-05T04:32:19.273Z",
      "title": "신뢰성 있는 LLM 평가를 실현하기 위한 단축 뉴론 분석",
      "submittedOnDailyBy": {
        "_id": "648c48d8c0ddeee6df5b6d22",
        "avatarUrl": "/avatars/8706b0b16dfc332b96c91d3ced31bd0b.svg",
        "isPro": false,
        "fullname": "Shangqing Tu",
        "user": "tsq2000",
        "type": "user"
      },
      "summary": "대 언어 모델(LLMs)의 개발에 신뢰성을 보장하는 평가가 중요하지만, 현재의 평가는 주로 공개된 벤치마크에 의존하며, 데이터의 contemporaneity 문제로 인해 공정성을 크게 영향을 미칩니다. 기존 연구에서는 contemporaneity 문제를 해결하기 위해 동적인 벤치마크 구축에 집중하였으나, 새로운 벤치마크의 연속적인 구축은 고비율적이고 순환적이다. 본 논문에서는 contemporaneity 문제를 해결하기 위해 contemporaneity 된 모델의 구조를 분석하여 문제를 해결하고자 합니다. 실험에서, contemporaneity 된 모델의 과대평가는 훈련 중 파라미터가 shortcut solution을 얻은 것이며, 이를 확인했습니다. 또한, 상대적인 분석자와因果 분석자를 사용하여 shortcut 뉴런을 특정한 새로운 방법을 제안하였습니다. 이를 통해 shortcut 뉴런을 억제하기 위한 평가 방법인 \"shortcut 뉴런 패치\"를 도입하였습니다. 실험은 이 접근이 contemporaneity를 완화시키는 것을 증명하였습니다. 또한, 최근 공개된 신뢰성이 높은 벤치마크인 MixEval과 함께 평가한 결과는 강한 선형 상관관계를 나타내며, Shurman의 계수(rho)가 0.95를 초과하였습니다. 이러한 높은 상관관계는 이 접근이 모델의 진정한 능력을 실제적으로 보여주고 신뢰성을 갖춘다는 것을 의미합니다. 또한, 이 접근의 일반화 성능을 보여주기 위해 다양한 벤치마크와 파라미터 설정 범위에서 추가적인 실험을 수행하였습니다. 코드: https://github.com/GaryStack/Trustworthy-Evaluation",
      "upvotes": 6,
      "discussionId": "684132cc725b7fb67f68fff5",
      "githubRepo": "https://github.com/GaryStack/Trustworthy-Evaluation",
      "ai_summary": "A method called shortcut neuron patching identifies and suppresses shortcut neurons in language models to mitigate data contamination issues in trustworthy evaluations.",
      "ai_keywords": [
        "large language models",
        "LLMs",
        "trustworthy evaluation",
        "data contamination",
        "benchmarks",
        "dynamic benchmarks",
        "shortcut solutions",
        "shortcut neurons",
        "comparative analysis",
        "causal analysis",
        "shortcut neuron patching",
        "MixEval",
        "Spearman coefficient"
      ]
    },
    "publishedAt": "2025-06-04T12:33:44.000Z",
    "title": "Establishing Trustworthy LLM Evaluation via Shortcut Neuron Analysis",
    "summary": "The development of large language models (LLMs) depends on trustworthy\nevaluation. However, most current evaluations rely on public benchmarks, which\nare prone to data contamination issues that significantly compromise fairness.\nPrevious researches have focused on constructing dynamic benchmarks to address\ncontamination. However, continuously building new benchmarks is costly and\ncyclical. In this work, we aim to tackle contamination by analyzing the\nmechanisms of contaminated models themselves. Through our experiments, we\ndiscover that the overestimation of contaminated models is likely due to\nparameters acquiring shortcut solutions in training. We further propose a novel\nmethod for identifying shortcut neurons through comparative and causal\nanalysis. Building on this, we introduce an evaluation method called shortcut\nneuron patching to suppress shortcut neurons. Experiments validate the\neffectiveness of our approach in mitigating contamination. Additionally, our\nevaluation results exhibit a strong linear correlation with MixEval, a recently\nreleased trustworthy benchmark, achieving a Spearman coefficient (rho)\nexceeding 0.95. This high correlation indicates that our method closely reveals\ntrue capabilities of the models and is trustworthy. We conduct further\nexperiments to demonstrate the generalizability of our method across various\nbenchmarks and hyperparameter settings. Code:\nhttps://github.com/GaryStack/Trustworthy-Evaluation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04142.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "648c48d8c0ddeee6df5b6d22",
      "avatarUrl": "/avatars/8706b0b16dfc332b96c91d3ced31bd0b.svg",
      "fullname": "Shangqing Tu",
      "name": "tsq2000",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.04108",
      "authors": [
        {
          "_id": "684104a16b106ae42f5acc1a",
          "name": "Yutao Sun",
          "hidden": false
        },
        {
          "_id": "684104a16b106ae42f5acc1b",
          "name": "Tianzhu Ye",
          "hidden": false
        },
        {
          "_id": "684104a16b106ae42f5acc1c",
          "name": "Li Dong",
          "hidden": false
        },
        {
          "_id": "684104a16b106ae42f5acc1d",
          "name": "Yuqing Xia",
          "hidden": false
        },
        {
          "_id": "684104a16b106ae42f5acc1e",
          "name": "Jian Chen",
          "hidden": false
        },
        {
          "_id": "684104a16b106ae42f5acc1f",
          "name": "Yizhao Gao",
          "hidden": false
        },
        {
          "_id": "684104a16b106ae42f5acc20",
          "name": "Shijie Cao",
          "hidden": false
        },
        {
          "_id": "684104a16b106ae42f5acc21",
          "name": "Jianyong Wang",
          "hidden": false
        },
        {
          "_id": "684104a16b106ae42f5acc22",
          "name": "Furu Wei",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-04T16:01:48.000Z",
      "submittedOnDailyAt": "2025-06-05T01:16:28.444Z",
      "title": "Rectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention\n\nRectified Sparse Attention",
      "submittedOnDailyBy": {
        "_id": "6300ef4779c5ddbc6cf83e1a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1661005591657-noauth.jpeg",
        "isPro": false,
        "fullname": "Yutao Sun",
        "user": "sunyt32",
        "type": "user"
      },
      "summary": "효율적인 긴 문장 헤드 생성은 대규모 언어 모델에서 중요한 문제입니다. 최근의 희소 디코딩 방법들은 효율을 높일 수 있습니다만, KV 캐시의 비대칭성 문제를 겪으며 근사 오차가 쌓임으로 생성 품질이 떨어집니다. 본 논문에서는 블록 희소 어텐션과 주기적인 밀한 보정을 조합한 간단하고 효과적인 방법인 Rectified Sparse Attention (ReSA)를 제안합니다. ReSA는 고정 간격으로 밀한 흐름 패스를 사용하여 KV 캐시를 업데이트함으로써 오차의 쌓임을 제한하고 사전 학습 분포와의 대응을 유지합니다. 수학 논리, 언어 모델링, 검색 태스크의 다양한 실험에서 ReSA는 근사 무손실의 생성 품질을 구현하며, 상당한 효율 향상을 달성했습니다. 특히, 256K 문장 길이의 디코딩에서 2.42배의 종료부터 속도 업을 달성하며, 손실 가능한 긴 문장 맥락 추론의 실용적인 해결책으로 사용될 수 있습니다. 코드는 https://aka.ms/ReSA-LM에서 사용 가능합니다.",
      "upvotes": 6,
      "discussionId": "684104a26b106ae42f5acc50",
      "ai_summary": "Rectified Sparse Attention (ReSA) improves the efficiency of long-sequence generation in Large Language Models by combining block-sparse attention with periodic dense rectification, maintaining high-quality generation.",
      "ai_keywords": [
        "sparse decoding",
        "KV cache misalignment",
        "Rectified Sparse Attention",
        "ReSA",
        "block-sparse attention",
        "dense rectification",
        "pretraining distribution",
        "long-context inference"
      ]
    },
    "publishedAt": "2025-06-04T12:01:48.000Z",
    "title": "Rectified Sparse Attention",
    "summary": "Efficient long-sequence generation is a critical challenge for Large Language\nModels. While recent sparse decoding methods improve efficiency, they suffer\nfrom KV cache misalignment, where approximation errors accumulate and degrade\ngeneration quality. In this work, we propose Rectified Sparse Attention (ReSA),\na simple yet effective method that combines block-sparse attention with\nperiodic dense rectification. By refreshing the KV cache at fixed intervals\nusing a dense forward pass, ReSA bounds error accumulation and preserves\nalignment with the pretraining distribution. Experiments across math reasoning,\nlanguage modeling, and retrieval tasks demonstrate that ReSA achieves\nnear-lossless generation quality with significantly improved efficiency.\nNotably, ReSA delivers up to 2.42times end-to-end speedup under decoding at\n256K sequence length, making it a practical solution for scalable long-context\ninference. Code is available at https://aka.ms/ReSA-LM.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04108.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6300ef4779c5ddbc6cf83e1a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1661005591657-noauth.jpeg",
      "fullname": "Yutao Sun",
      "name": "sunyt32",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.03099",
      "authors": [
        {
          "_id": "684107c6142b5c0b4226025f",
          "name": "Chetwin Low",
          "hidden": false
        },
        {
          "_id": "684107c6142b5c0b42260260",
          "name": "Weimin Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T17:29:28.000Z",
      "submittedOnDailyAt": "2025-06-05T01:30:00.782Z",
      "title": "TalkingMachines: 시간대별 음성 드라이브 피치차 스타일의 비디오를 분별모델로 구현하기",
      "submittedOnDailyBy": {
        "_id": "62b43ffec624a43b1a1ada46",
        "avatarUrl": "/avatars/77298e99d2797cf917fdddc6d6de46eb.svg",
        "isPro": false,
        "fullname": "weimin wang ",
        "user": "weiminwang",
        "type": "user"
      },
      "summary": "이 논문에서는 TalkingMachines라는 효율적인 프레임워크를 제안합니다. 이 프레임워크는 사전 학습된 비디오 생성 모델을 음성 구동의 캐릭터 애니메이션 모델로 변환하는 것입니다. TalkingMachines는 음성 대 언어 모델(LLM)과 우리의 비디오 생성 기반 모델을 통합하여 자연스러운 대화적인 경험을 가능하게 합니다. 우리의 주요 기여는 다음과 같습니다.\n\n(1) SOTA의 사전 학습된 이미지에서 DiT를 음성 구동의 아바터 생성 모델로 적용하여 18억 파라미터를 가진 모델을 만들었습니다.\n(2) 대칭적인 교정 모델으로부터 비대칭적인 지식의 경험을 받아 오차를 축적하는 것을 방지하고 무한한 비디오 스트리밍을 가능하게 했습니다.\n(3) 고 하이프로 플로우, 저 라틴 시의 추론 파이프라인을 설계하여 다음과 같은 핵심 최적화 기법을 사용했습니다.\n(a) DiT와 VAE 디코더를 분리된 장치에서 분리합니다.\n(b) CUDA 스트림을 사용하여 장치 간 통신과 계산을 효율적으로 병렬 처리합니다.\n(c) 중복 계산을 줄이고 프레임 생성의 트랜스포트 흐름을 최대화합니다.\n\nDEMO 비디오는 여기를 통해 볼 수 있습니다 - https://aaxwaz.github.io/TalkingMachines/",
      "upvotes": 6,
      "discussionId": "684107c8142b5c0b42260293",
      "projectPage": "https://aaxwaz.github.io/TalkingMachines/",
      "githubRepo": "https://github.com/aaxwaz/TalkingMachines",
      "ai_summary": "TalkingMachines transforms a pretrained image-to-video model into an audio-driven avatar generator, supports infinite video streaming, and uses engineering optimizations for real-time performance.",
      "ai_keywords": [
        "DiT",
        "audio large language model",
        "asymmetric knowledge distillation",
        "bidirectional teacher model",
        "sparse causal",
        "autoregressive student model",
        "inference pipeline",
        "CUDA streams",
        "frame-generation throughput"
      ]
    },
    "publishedAt": "2025-06-03T13:29:28.000Z",
    "title": "TalkingMachines: Real-Time Audio-Driven FaceTime-Style Video via\n  Autoregressive Diffusion Models",
    "summary": "In this paper, we present TalkingMachines -- an efficient framework that\ntransforms pretrained video generation models into real-time, audio-driven\ncharacter animators. TalkingMachines enables natural conversational experiences\nby integrating an audio large language model (LLM) with our video generation\nfoundation model. Our primary contributions include: (1) We adapt a pretrained\nSOTA image-to-video DiT into an audio-driven avatar generation model of 18\nbillion parameters; (2) We enable infinite video streaming without error\naccumulation through asymmetric knowledge distillation from a bidirectional\nteacher model into a sparse causal, autoregressive student model; (3) We design\na high-throughput, low-latency inference pipeline incorporating several key\nengineering optimizations such as: (a) disaggregation of the DiT and VAE\ndecoder across separate devices, (b) efficient overlap of inter-device\ncommunication and computation using CUDA streams, (c) elimination of redundant\nrecomputations to maximize frame-generation throughput. Please see demo videos\nhere - https://aaxwaz.github.io/TalkingMachines/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03099.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62b43ffec624a43b1a1ada46",
      "avatarUrl": "/avatars/77298e99d2797cf917fdddc6d6de46eb.svg",
      "fullname": "weimin wang ",
      "name": "weiminwang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.02592",
      "authors": [
        {
          "_id": "684104c89ec96d9991484c24",
          "user": {
            "_id": "65309a1d657ae56cdb65e0e7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/lHQI9RNjfz8E5v1uyCGeV.png",
            "isPro": false,
            "fullname": "Zhi-Yuan Chen",
            "user": "JaxChen",
            "type": "user"
          },
          "name": "Zhi-Yuan Chen",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-05T02:45:29.221Z",
          "hidden": false
        },
        {
          "_id": "684104c89ec96d9991484c25",
          "name": "Hao Wang",
          "hidden": false
        },
        {
          "_id": "684104c89ec96d9991484c26",
          "name": "Xinyu Zhang",
          "hidden": false
        },
        {
          "_id": "684104c89ec96d9991484c27",
          "name": "Enrui Hu",
          "hidden": false
        },
        {
          "_id": "684104c89ec96d9991484c28",
          "name": "Yankai Lin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T08:12:47.000Z",
      "submittedOnDailyAt": "2025-06-05T01:30:17.419Z",
      "title": "표면보다 심각히: LLM의 판단에서 자신의 취향의 측정",
      "submittedOnDailyBy": {
        "_id": "65309a1d657ae56cdb65e0e7",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/lHQI9RNjfz8E5v1uyCGeV.png",
        "isPro": false,
        "fullname": "Zhi-Yuan Chen",
        "user": "JaxChen",
        "type": "user"
      },
      "summary": "최근의 연구에 따르면, 대규모 언어 모델(LLMs)은 자신의 답변을 판단자로 취급할 때 자신의 답변을 선호하는 경향이 있는 것으로 밝혀졌다. 이는 다른 모델이 생성한 답변보다 자신의 답변을 더 좋아서 하는 경향을 의미한다. 현재의 방법은 일반적으로 판단자 모델이 자신의 답변에 할당하는 점수와 다른 모델이 생성한 답변에 할당하는 점수의 차이를 계산하여 이 편향을 측정하는 것이다. 그러나 이 접근법은 판단자 모델이 생성한 고품질의 답변에 따라도 편향이 없는 경우도 양의 점수의 차이를 인정하고 있기 때문에, 답변의 품질과 편향을 혼동시키며 있다. 이러한 문제를 해결하기 위해, 우리는 실제 답변의 품질을 대리로 하는 「금판정」을 사용하며, 판단자 모델이 자신의 답변에 할당하는 점수와 대응하는 금판정 점수의 차이를 DBG 점수로 제안한다. 금판정은 실제 답변의 품질을 반영하기 때문에, DBG 점수는 답변의 품질이 편향 측정에 혼동되는 영향을 줄일 수 있다. DBG 점수를 사용하여, 우리는 LLMs의 다양한 버전, 크기, 논리력의 능력에 따른 자신의 답변의 편향을 평가하기 위한 검토를 수행한다. 또한, 우리는 자신의 답변의 편향을 영향을 미친 두 가지 요인을 조사하고, 답변의 텍스트 스타일과 판단자 모델의 후학습 데이터에 대해 검토한다. 마지막으로, 우리는 주의 기반의 관점에서 자신의 답변의 편향의 잠재적 구조를 조사한다. 우리의 코드와 데이터는 https://github.com/zhiyuanc2001/self-preference에 공개되어 있다.",
      "upvotes": 6,
      "discussionId": "684104c99ec96d9991484c5e",
      "githubRepo": "https://github.com/zhiyuanc2001/self-preference",
      "ai_summary": "The DBG score is introduced to measure self-preference bias in large language models by using gold judgments as proxies for response quality, addressing the confounding effect of response quality.",
      "ai_keywords": [
        "large language models",
        "self-preference bias",
        "judge model",
        "gold judgments",
        "DBG score",
        "response quality",
        "attention-based perspective"
      ]
    },
    "publishedAt": "2025-06-03T04:12:47.000Z",
    "title": "Beyond the Surface: Measuring Self-Preference in LLM Judgments",
    "summary": "Recent studies show that large language models (LLMs) exhibit self-preference\nbias when serving as judges, meaning they tend to favor their own responses\nover those generated by other models. Existing methods typically measure this\nbias by calculating the difference between the scores a judge model assigns to\nits own responses and those it assigns to responses from other models. However,\nthis approach conflates self-preference bias with response quality, as\nhigher-quality responses from the judge model may also lead to positive score\ndifferences, even in the absence of bias. To address this issue, we introduce\ngold judgments as proxies for the actual quality of responses and propose the\nDBG score, which measures self-preference bias as the difference between the\nscores assigned by the judge model to its own responses and the corresponding\ngold judgments. Since gold judgments reflect true response quality, the DBG\nscore mitigates the confounding effect of response quality on bias measurement.\nUsing the DBG score, we conduct comprehensive experiments to assess\nself-preference bias across LLMs of varying versions, sizes, and reasoning\nabilities. Additionally, we investigate two factors that influence and help\nalleviate self-preference bias: response text style and the post-training data\nof judge models. Finally, we explore potential underlying mechanisms of\nself-preference bias from an attention-based perspective. Our code and data are\navailable at https://github.com/zhiyuanc2001/self-preference.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.02592.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65309a1d657ae56cdb65e0e7",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/lHQI9RNjfz8E5v1uyCGeV.png",
      "fullname": "Zhi-Yuan Chen",
      "name": "JaxChen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.03106",
      "authors": [
        {
          "_id": "684104d9ee7646c073776b2e",
          "name": "Xiaoying Zhang",
          "hidden": false
        },
        {
          "_id": "684104d9ee7646c073776b2f",
          "name": "Hao Sun",
          "hidden": false
        },
        {
          "_id": "684104d9ee7646c073776b30",
          "user": {
            "_id": "666e91b1623133f1ce35acc5",
            "avatarUrl": "/avatars/cc78520e6cfb83817c1d0c1ac867ebdd.svg",
            "isPro": false,
            "fullname": "YipengZhang",
            "user": "YipengZhang",
            "type": "user"
          },
          "name": "Yipeng Zhang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-05T02:45:46.851Z",
          "hidden": false
        },
        {
          "_id": "684104d9ee7646c073776b31",
          "name": "Kaituo Feng",
          "hidden": false
        },
        {
          "_id": "684104d9ee7646c073776b32",
          "name": "Chaochao Lu",
          "hidden": false
        },
        {
          "_id": "684104d9ee7646c073776b33",
          "name": "Chao Yang",
          "hidden": false
        },
        {
          "_id": "684104d9ee7646c073776b34",
          "name": "Helen Meng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T17:39:02.000Z",
      "submittedOnDailyAt": "2025-06-05T01:16:53.836Z",
      "title": "Critique-GRPO: 언어와 숫자의 피드백을 이용한 LLM 논리 추론의 발전",
      "submittedOnDailyBy": {
        "_id": "67079840a9bcb7459b8d2a46",
        "avatarUrl": "/avatars/32466863c5554f20cb2775b138832ac3.svg",
        "isPro": false,
        "fullname": "Kaituo Feng",
        "user": "KaituoFeng",
        "type": "user"
      },
      "summary": "최근의 수치적인 피드백(예: 스칼라리드)를 이용한 강화학습(RL)의 발전은 대규모 언어 모델(LLMs)의 복잡한 논리 능력에 크게 업그레이드 해 주었습니다. 이 성공에도 불구하고, 우리는 수치 피드백을만 사용하는 RL에 3가지 중요한 문제점을 파악했습니다: 성능의 평탄화, 자기 반성의 한계성, 장기적인 실패. 또한, RL에서 미세 조정된 모델은 성능의 평탄화가 나타난 후에도, 자연어 피드백(평가의 형태)을 사용하여 장기적인 실패 문제를 해결하기 위해 올바른 미세 조정을 생성할 수 있음을 보여주었습니다. 이러한 전망에 기반하여, 우리는 자연어와 수치 피드백을 통합한 효과적인 정책 최적화의 온라인 RL 프레임워크인 Critique-GRPO를 제안했습니다. Critique-GRPO는 LLMs가 초기의 답변과 평가에 의한 개선을 동시에 학습하면서 탐색을 유지할 수 있게 됩니다. Qwen2.5-7B-Base와 Qwen3-8B-Base를 사용하며, 8가지 어려운 수학, STEM, 일반적인 논리 태스크를 통해 검증한 결과, Critique-GRPO는 4.5% ~ 5% 정도의 평균 pass@1 스코어를 향상시키고, 이 기본선을 초월했습니다. 특히, Critique-GRPO는 온라인 RL에 의한 전문자의 지침을 포함하는 강력한 기본선을 초월했습니다. 나아가는 분석에서, 정책 탐색에 대한 2가지 중요한 전망이 명확히 되었습니다: (1) 높은 엔트로피는 탐색에서 효율적인 학습을 보장하는 것은 항상 보장되지 않음, (2) 긴 답변은 탐색의 효율성을 불러일으키는 것은 항상 보장되지 않음.",
      "upvotes": 5,
      "discussionId": "684104daee7646c073776b88",
      "githubRepo": "https://github.com/zhangxy-2019/critique-GRPO",
      "ai_summary": "Critique-GRPO, an RL framework combining numerical and natural language feedback, enhances LLM reasoning across tasks and outperforms existing methods.",
      "ai_keywords": [
        "reinforcement learning",
        "RL",
        "large language models",
        "LLMs",
        "scalar rewards",
        "performance plateaus",
        "self-reflection",
        "persistent failures",
        "natural language feedback",
        "critiques",
        "policy optimization",
        "Qwen2.5-7B-Base",
        "Qwen3-8B-Base",
        "pass@1",
        "policy exploration",
        "entropy",
        "response length"
      ]
    },
    "publishedAt": "2025-06-03T13:39:02.000Z",
    "title": "Critique-GRPO: Advancing LLM Reasoning with Natural Language and\n  Numerical Feedback",
    "summary": "Recent advances in reinforcement learning (RL) with numerical feedback, such\nas scalar rewards, have significantly enhanced the complex reasoning\ncapabilities of large language models (LLMs). Despite this success, we identify\nthree key challenges encountered by RL with solely numerical feedback:\nperformance plateaus, limited effectiveness of self-reflection, and persistent\nfailures. We then demonstrate that RL-finetuned models, even after exhibiting\nperformance plateaus, can generate correct refinements on persistently failed\nproblems by leveraging natural language feedback in the form of critiques.\nBuilding on this insight, we propose Critique-GRPO, an online RL framework that\nintegrates both natural language and numerical feedback for effective policy\noptimization. Critique-GRPO enables LLMs to learn from initial responses and\ncritique-guided refinements simultaneously while maintaining exploration.\nExtensive experiments using Qwen2.5-7B-Base and Qwen3-8B-Base show that\nCritique-GRPO consistently outperforms supervised learning-based and RL-based\nfine-tuning approaches across eight challenging mathematical, STEM, and general\nreasoning tasks, improving average pass@1 scores by approximately 4.5% and 5%,\nrespectively. Notably, Critique-GRPO surpasses a strong baseline that\nincorporates expert demonstrations within online RL. Further analysis reveals\ntwo critical insights about policy exploration: (1) higher entropy does not\nalways guarantee efficient learning from exploration, and (2) longer responses\ndo not necessarily lead to more effective exploration.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03106.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67079840a9bcb7459b8d2a46",
      "avatarUrl": "/avatars/32466863c5554f20cb2775b138832ac3.svg",
      "fullname": "Kaituo Feng",
      "name": "KaituoFeng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.21541",
      "authors": [
        {
          "_id": "6840f79ceb249b555b244efc",
          "name": "Zitong Wang",
          "hidden": false
        },
        {
          "_id": "6840f79ceb249b555b244efd",
          "name": "Hang Zhao",
          "hidden": false
        },
        {
          "_id": "6840f79ceb249b555b244efe",
          "name": "Qianyu Zhou",
          "hidden": false
        },
        {
          "_id": "6840f79ceb249b555b244eff",
          "name": "Xuequan Lu",
          "hidden": false
        },
        {
          "_id": "6840f79ceb249b555b244f00",
          "name": "Xiangtai Li",
          "hidden": false
        },
        {
          "_id": "6840f79ceb249b555b244f01",
          "name": "Yiren Song",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-24T16:08:04.000Z",
      "submittedOnDailyAt": "2025-06-05T00:21:34.798Z",
      "title": "DiffDecompose: 알파 합성 이미지의 레이어별로 분해에 의한 분기 트레너의 분해",
      "submittedOnDailyBy": {
        "_id": "64311a95034ecbefddd141ef",
        "avatarUrl": "/avatars/b6dc5ca373bedbaa368208517954c375.svg",
        "isPro": true,
        "fullname": "Yiren Song",
        "user": "yiren98",
        "type": "user"
      },
      "summary": "디퓨션 모델은 최근 물체 제거 등 다양한 생성 태스크에서 매우 성공적으로 사용되고 있습니다. 그러나 현재의 이미지 분해 기술은 마스크 프로이드어 의존성, 정적 물체 가정, 데이터 세트의 부족으로 인해 투명하거나 반투명한 레이어의 가려기를 분리하는 것이 어렵습니다. 본 논문에서는 새로운 태스크를 다루고자 합니다: 알파 합성 이미지의 레이어별 분해, 반투명/투명한 알파 레이어의 비선형 가려기의 상황에서, 겹치는 이미지로부터 구성 요소의 레이어를 복원하는 것입니다. 레이어의 불확실성, 일반화, 데이터의 부족에 대처하기 위해 먼저 알파 블렌드(AlphaBlend)을 소개합니다. 알파 블렌드는 처음의 규모가 큰 고품질 데이터 세트를 사용하여 투명 및 반투명한 레이어의 분해에 대응하며, 6가지의 실세계적인 서브 태스크를 지원합니다(예: 반투명한 화염의 제거, 반투명한 셀의 분해, 유리기품의 분해). 이 데이터 세트를 기반으로 DiffDecompose를 제안합니다. DiffDecompose는 입력 이미지, 세ман틱 프로ン프트, 브렌딩 타입에 따라 가능한 레이어의 분해 과정을 학습합니다. 직접 알파 매트릭스를 예측하는 대신, DiffDecompose는 In-Context Decomposition을 수행하고, 모델이 한 개 또는 여러 개의 레이어를 예측할 수 있도록 합니다. 레이어 간에 픽셀 수준의 대응을 유지하기 위해 레이어 위치 인코딩 克隆(Layer Position Encoding Cloning)을 도입합니다. 제안된 알파 블렌드 데이터 세트와 공개된 로고 데이터 세트에서 DiffDecompose의 효과를 평가합니다. 코드와 데이터 세트는 논문 심사 후 제공됩니다. 코드는 다음 URL에서 사용 가능합니다: https://github.com/Wangzt1121/DiffDecompose.",
      "upvotes": 5,
      "discussionId": "6840f7a1eb249b555b244ffe",
      "ai_summary": "DiffDecompose, a diffusion Transformer-based framework, effectively decomposes images into constituent layers with semantic prompts, addressing challenges in transparent layer decomposition.",
      "ai_keywords": [
        "diffusion models",
        "diffusion Transformer",
        "posterior",
        "semantic prompts",
        "blending type",
        "In-Context Decomposition",
        "Layer Position Encoding Cloning",
        "AlphaBlend dataset",
        "translucent flare removal",
        "semi-transparent cell decomposition",
        "glassware decomposition",
        "LOGO dataset"
      ]
    },
    "publishedAt": "2025-05-24T12:08:04.000Z",
    "title": "DiffDecompose: Layer-Wise Decomposition of Alpha-Composited Images via\n  Diffusion Transformers",
    "summary": "Diffusion models have recently motivated great success in many generation\ntasks like object removal. Nevertheless, existing image decomposition methods\nstruggle to disentangle semi-transparent or transparent layer occlusions due to\nmask prior dependencies, static object assumptions, and the lack of datasets.\nIn this paper, we delve into a novel task: Layer-Wise Decomposition of\nAlpha-Composited Images, aiming to recover constituent layers from single\noverlapped images under the condition of semi-transparent/transparent alpha\nlayer non-linear occlusion. To address challenges in layer ambiguity,\ngeneralization, and data scarcity, we first introduce AlphaBlend, the first\nlarge-scale and high-quality dataset for transparent and semi-transparent layer\ndecomposition, supporting six real-world subtasks (e.g., translucent flare\nremoval, semi-transparent cell decomposition, glassware decomposition).\nBuilding on this dataset, we present DiffDecompose, a diffusion\nTransformer-based framework that learns the posterior over possible layer\ndecompositions conditioned on the input image, semantic prompts, and blending\ntype. Rather than regressing alpha mattes directly, DiffDecompose performs\nIn-Context Decomposition, enabling the model to predict one or multiple layers\nwithout per-layer supervision, and introduces Layer Position Encoding Cloning\nto maintain pixel-level correspondence across layers. Extensive experiments on\nthe proposed AlphaBlend dataset and public LOGO dataset verify the\neffectiveness of DiffDecompose. The code and dataset will be available upon\npaper acceptance. Our code will be available at:\nhttps://github.com/Wangzt1121/DiffDecompose.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21541.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64311a95034ecbefddd141ef",
      "avatarUrl": "/avatars/b6dc5ca373bedbaa368208517954c375.svg",
      "fullname": "Yiren Song",
      "name": "yiren98",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 21
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.03956",
      "authors": [
        {
          "_id": "6841396eee27975702b57e87",
          "user": {
            "_id": "6759546743971eff5a12a087",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/esJm_83zW1R6NqWltof8P.png",
            "isPro": false,
            "fullname": "Aojun Lu",
            "user": "Kurt1024",
            "type": "user"
          },
          "name": "Aojun Lu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:26:29.156Z",
          "hidden": false
        },
        {
          "_id": "6841396eee27975702b57e88",
          "name": "Tao Feng",
          "hidden": false
        },
        {
          "_id": "6841396eee27975702b57e89",
          "user": {
            "_id": "649d54b314afbb10ce2a9eeb",
            "avatarUrl": "/avatars/15c325d8c2273ff63569f23015e98486.svg",
            "isPro": false,
            "fullname": "Hangjie Yuan",
            "user": "JacobYuan",
            "type": "user"
          },
          "name": "Hangjie Yuan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:26:31.411Z",
          "hidden": false
        },
        {
          "_id": "6841396eee27975702b57e8a",
          "name": "Chunhui Ding",
          "hidden": false
        },
        {
          "_id": "6841396eee27975702b57e8b",
          "name": "Yanan Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-04T13:46:33.000Z",
      "submittedOnDailyAt": "2025-06-05T05:00:41.771Z",
      "title": "Adapt before Continual Learning",
      "submittedOnDailyBy": {
        "_id": "649d54b314afbb10ce2a9eeb",
        "avatarUrl": "/avatars/15c325d8c2273ff63569f23015e98486.svg",
        "isPro": false,
        "fullname": "Hangjie Yuan",
        "user": "JacobYuan",
        "type": "user"
      },
      "summary": "継続的 학습 (CL)은 신경 네트워크가 새로운 지식을 추가하면서 기존의 지식을 유지하는 것을 목표로 하고 있습니다 (소성). 예측 모델 (PTMs)은 CL에 중요한 역할을 수행하지만, 현재의 접근 방식에서 PTM의 본체는 고정되어 안정성을 유지하며 소성을 제한하고, 증분 태스크에서 큰 도메인 간 차이를 경험할 때 특히 문제가 있습니다. 반면, PTM 전체를 순차적으로 미세 조정하는 것은 확산된 지식의 잊혀짐에 노출되고 안정성과 소성 사이의 중요한 트레이드오프를 노출합니다. 이러한 도전에 대처하기 위해, 우리는 CL의 핵심 프로세스 전에 PTM 본체를 개선하는 새로운 프레임워크 \"Adapting PTMs before the core CL process (ACL)\"를 제안합니다. ACL은 각 새로운 태스크를 학습하기 전에, 기존의 CL 접근 방식 (예: 프로ン퓰트 튜닝)을 사용하여 플러그와 파트너십의 아다퍼트 페이즈로 PTM 본체를 개선합니다. ACL은 이론적으로와 실험적으로 입증된 것처럼, 안정성과 소성을 균형을 위해 임베딩을 원래의 클래스 프로토타입과 일치시키면서 다른 클래스으로부터 떨어지게 하여 소성을 향상시킵니다. 확장된 실험은 ACL이 벤치마크와 통합된 방법으로 CL의 성능을 크게 향상시키는 것을 보여줍니다. PTM 기반의 CL의 광범위한 해결책을 제공합니다.",
      "upvotes": 4,
      "discussionId": "6841396eee27975702b57eb7",
      "projectPage": "https://github.com/byyx666/ACL_code",
      "githubRepo": "https://github.com/byyx666/ACL_code",
      "ai_summary": "Adapting Pre-trained Models before the core CL process (ACL) improves Continual Learning by enhancing plasticity while maintaining stability.",
      "ai_keywords": [
        "Continual Learning",
        "CL",
        "Pre-trained models",
        "PTMs",
        "plasticity",
        "stability",
        "domain gaps",
        "catastrophic forgetting",
        "prompt tuning",
        "embeddings",
        "class prototypes"
      ]
    },
    "publishedAt": "2025-06-04T09:46:33.000Z",
    "title": "Adapt before Continual Learning",
    "summary": "Continual Learning (CL) seeks to enable neural networks to incrementally\nacquire new knowledge (plasticity) while retaining existing knowledge\n(stability). While pre-trained models (PTMs) have become pivotal in CL,\nprevailing approaches freeze the PTM backbone to preserve stability, limiting\ntheir plasticity, particularly when encountering significant domain gaps in\nincremental tasks. Conversely, sequentially finetuning the entire PTM risks\ncatastrophic forgetting of generalizable knowledge, exposing a critical\nstability-plasticity trade-off. To address this challenge, we propose Adapting\nPTMs before the core CL process (ACL), a novel framework that refines the PTM\nbackbone through a plug-and-play adaptation phase before learning each new task\nwith existing CL approaches (e.g., prompt tuning). ACL enhances plasticity by\naligning embeddings with their original class prototypes while distancing them\nfrom others, theoretically and empirically shown to balance stability and\nplasticity. Extensive experiments demonstrate that ACL significantly improves\nCL performance across benchmarks and integrated methods, offering a versatile\nsolution for PTM-based CL.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03956.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "649d54b314afbb10ce2a9eeb",
      "avatarUrl": "/avatars/15c325d8c2273ff63569f23015e98486.svg",
      "fullname": "Hangjie Yuan",
      "name": "JacobYuan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.03448",
      "authors": [
        {
          "_id": "684105479060432bf302b432",
          "name": "Bimsara Pathiraja",
          "hidden": false
        },
        {
          "_id": "684105479060432bf302b433",
          "user": {
            "_id": "622d2ff38d04fd29a9ccf1a7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/622d2ff38d04fd29a9ccf1a7/ORpGrlU8Lm_oSEVftcZEK.jpeg",
            "isPro": false,
            "fullname": "Maitreya Patel",
            "user": "mpatel57",
            "type": "user"
          },
          "name": "Maitreya Patel",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:26:48.135Z",
          "hidden": false
        },
        {
          "_id": "684105479060432bf302b434",
          "name": "Shivam Singh",
          "hidden": false
        },
        {
          "_id": "684105479060432bf302b435",
          "name": "Yezhou Yang",
          "hidden": false
        },
        {
          "_id": "684105479060432bf302b436",
          "name": "Chitta Baral",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T23:20:24.000Z",
      "submittedOnDailyAt": "2025-06-05T01:19:11.023Z",
      "title": "RefEdit: 기준 표현에 기반한 이미지 편집 모델의 향상에 대한 벤치마크와 방법론",
      "submittedOnDailyBy": {
        "_id": "622d2ff38d04fd29a9ccf1a7",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/622d2ff38d04fd29a9ccf1a7/ORpGrlU8Lm_oSEVftcZEK.jpeg",
        "isPro": false,
        "fullname": "Maitreya Patel",
        "user": "mpatel57",
        "type": "user"
      },
      "summary": "최근의 반전 및 지시 기반의 이미지 편집 기술의 발전에 따라, 현재의 방법들은 주로 주요 물체의 단일 편집에 능숙하지만, 복잡한 스키마에 다수의 존재를 포함하는 경우 큰 문제를 겪습니다. 이러한 간격을 정량화하기 위해, 우리는 RefEdit-Bench를 소개합니다. 이는 엄격한 현실 세계 벤치마크로, RefCOCO에 기반하여 만의 샘플을 훈련시킨 기본 라인이 거의 뛰어나지 않습니다. 이러한 제한을 극복하기 위해, 우리는 RefEdit를 소개합니다. 이는 우리의 scalable 합성 데이터 생성 파이프라인을 사용하여 훈련된 지시 기반 편집 모델입니다. RefEdit는 20,000개의 편집 트라이플렛을 훈련시키도, 만의 데이터로 훈련된 Flux/SD3 모델 기반의 기본 라인보다 뛰어나며, 다양한 벤치마크에서 확장된 평가에 따라, 우리의 모델은 참조 표현 태스크에도 뛰어나고, 전통적인 벤치마크의 성능을 향상시키고, 클로즈드 사운드 코피의 방법과 같은 최신 결과를 구현합니다. 우리는 재현성을 위해 데이터와 체크포인트를 공개합니다.",
      "upvotes": 4,
      "discussionId": "6841054b9060432bf302b559",
      "projectPage": "https://refedit.vercel.app",
      "githubRepo": "https://github.com/bimsarapathiraja/refedit",
      "ai_summary": "RefEdit, an instruction-based editing model trained on synthetic data, outperforms baselines in complex scene editing and referring expression tasks.",
      "ai_keywords": [
        "RefEdit-Bench",
        "RefCOCO",
        "instruction-based editing model",
        "scalable synthetic data generation pipeline",
        "Flux/SD3",
        "state-of-the-art results"
      ]
    },
    "publishedAt": "2025-06-03T19:20:24.000Z",
    "title": "RefEdit: A Benchmark and Method for Improving Instruction-based Image\n  Editing Model on Referring Expressions",
    "summary": "Despite recent advances in inversion and instruction-based image editing,\nexisting approaches primarily excel at editing single, prominent objects but\nsignificantly struggle when applied to complex scenes containing multiple\nentities. To quantify this gap, we first introduce RefEdit-Bench, a rigorous\nreal-world benchmark rooted in RefCOCO, where even baselines trained on\nmillions of samples perform poorly. To overcome this limitation, we introduce\nRefEdit -- an instruction-based editing model trained on our scalable synthetic\ndata generation pipeline. Our RefEdit, trained on only 20,000 editing triplets,\noutperforms the Flux/SD3 model-based baselines trained on millions of data.\nExtensive evaluations across various benchmarks demonstrate that our model not\nonly excels in referring expression tasks but also enhances performance on\ntraditional benchmarks, achieving state-of-the-art results comparable to\nclosed-source methods. We release data \\& checkpoint for reproducibility.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03448.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "622d2ff38d04fd29a9ccf1a7",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/622d2ff38d04fd29a9ccf1a7/ORpGrlU8Lm_oSEVftcZEK.jpeg",
      "fullname": "Maitreya Patel",
      "name": "mpatel57",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.03355",
      "authors": [
        {
          "_id": "68415c5cce09e3eca94e9839",
          "name": "Elias Abad Rocamora",
          "hidden": false
        },
        {
          "_id": "68415c5cce09e3eca94e983a",
          "user": {
            "_id": "6310a6bb0a43f97f6c5567d3",
            "avatarUrl": "/avatars/04b07c3a6c811337939c951567cb2bf2.svg",
            "isPro": false,
            "fullname": "Christian Schlarmann",
            "user": "chs20",
            "type": "user"
          },
          "name": "Christian Schlarmann",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T09:59:48.051Z",
          "hidden": false
        },
        {
          "_id": "68415c5cce09e3eca94e983b",
          "name": "Naman Deep Singh",
          "hidden": false
        },
        {
          "_id": "68415c5cce09e3eca94e983c",
          "name": "Yongtao Wu",
          "hidden": false
        },
        {
          "_id": "68415c5cce09e3eca94e983d",
          "name": "Matthias Hein",
          "hidden": false
        },
        {
          "_id": "68415c5cce09e3eca94e983e",
          "name": "Volkan Cevher",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T19:57:09.000Z",
      "submittedOnDailyAt": "2025-06-05T07:29:30.561Z",
      "title": "领域双方的稳健性：CLIP는 강력한 텍스트 인코더가 필요합니다.",
      "submittedOnDailyBy": {
        "_id": "6310a6bb0a43f97f6c5567d3",
        "avatarUrl": "/avatars/04b07c3a6c811337939c951567cb2bf2.svg",
        "isPro": false,
        "fullname": "Christian Schlarmann",
        "user": "chs20",
        "type": "user"
      },
      "summary": "敵対의 입력 공격이 CLIP 엔베딩을 크게 변화시킬 수 있습니다. 이는 CLIP를 중간에 삽입한 모델의 하류의 강건성을 영향을 미칩니다. 예를 들어, 문에서 이미지 생성 모델이나 대형 시각 언어 모델 등이 있습니다. CLIP 이미지 엔코더의 강건성에 대한 노력이 이루어져 있지만, 문 엔코더의 강건성은 불확실한 범위입니다. 본 논문에서는 이러한 문헌의 부족점을 보완합니다. LEAF(Locally-Enhanced Adversarial Fine-tuning)을 제안합니다. LEAF는 문 영역에 대한 효율적인 적대적 微调 훈련 방법이며, 큰 규모의 CLIP 모델에도 적용할 수 있습니다. 우리의 모델은 문 영역의 0 shot 적대적 정확도를 크게 향상시키고, 강건한 이미지 엔코더가 제공하는 시각 성능을 유지할 수 있습니다. 문에서 이미지 확장 모델과 함께, 적대적 노이즈 하에서 생성 품질을 향상시킵니다. 다형성 평가 처리에서 우리의 강건한 CLIP 엔코더를 사용하면, 표준 CLIP 모델에 대한 적대적 노이즈 하에서 호출 확률을 향상시킬 수 있습니다. 마지막으로, 강건한 문 엔코더는 입력 문의 재구성을 직접 최적화하여 개선하는 것을 보여줍니다.",
      "upvotes": 4,
      "discussionId": "68415c5ece09e3eca94e98e4",
      "ai_summary": "LEAF, an adversarial finetuning method, enhances the robustness of CLIP text encoders, improving zero-shot accuracy and multimodal retrieval performance under adversarial noise.",
      "ai_keywords": [
        "adversarial input attacks",
        "CLIP embeddings",
        "text-to-image generative models",
        "large vision language models",
        "adversarial finetuning",
        "zero-shot adversarial accuracy",
        "text-to-image diffusion models",
        "multimodal retrieval tasks",
        "robust text encoders"
      ]
    },
    "publishedAt": "2025-06-03T15:57:09.000Z",
    "title": "Robustness in Both Domains: CLIP Needs a Robust Text Encoder",
    "summary": "Adversarial input attacks can cause a significant shift of CLIP embeddings.\nThis can affect the downstream robustness of models incorporating CLIP in the\npipeline, such as text-to-image generative models or large vision language\nmodels. While some efforts have been done towards making the CLIP image\nencoders robust, the robustness of text encoders remains unexplored. In this\nwork, we cover this gap in the literature. We propose LEAF: an efficient\nadversarial finetuning method for the text domain, with the ability to scale to\nlarge CLIP models. Our models significantly improve the zero-shot adversarial\naccuracy in the text domain, while maintaining the vision performance provided\nby robust image encoders. When combined with text-to-image diffusion models, we\ncan improve the generation quality under adversarial noise. When employing our\nrobust CLIP encoders in multimodal retrieval tasks, we improve the recall under\nadversarial noise over standard CLIP models. Finally, we show that robust text\nencoders facilitate better reconstruction of input text from its embedding via\ndirect optimization.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03355.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6310a6bb0a43f97f6c5567d3",
      "avatarUrl": "/avatars/04b07c3a6c811337939c951567cb2bf2.svg",
      "fullname": "Christian Schlarmann",
      "name": "chs20",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.02945",
      "authors": [
        {
          "_id": "6841008f2f66f731bf010feb",
          "name": "Aishwarya Sahoo",
          "hidden": false
        },
        {
          "_id": "6841008f2f66f731bf010fec",
          "name": "Jeevana Kruthi Karnuthala",
          "hidden": false
        },
        {
          "_id": "6841008f2f66f731bf010fed",
          "name": "Tushar Parmanand Budhwani",
          "hidden": false
        },
        {
          "_id": "6841008f2f66f731bf010fee",
          "name": "Pranchal Agarwal",
          "hidden": false
        },
        {
          "_id": "6841008f2f66f731bf010fef",
          "name": "Sankaran Vaidyanathan",
          "hidden": false
        },
        {
          "_id": "6841008f2f66f731bf010ff0",
          "name": "Alexa Siu",
          "hidden": false
        },
        {
          "_id": "6841008f2f66f731bf010ff1",
          "user": {
            "_id": "62c5947524171688a9feb992",
            "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
            "isPro": false,
            "fullname": "Franck Dernoncourt",
            "user": "Franck-Dernoncourt",
            "type": "user"
          },
          "name": "Franck Dernoncourt",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:26:55.170Z",
          "hidden": false
        },
        {
          "_id": "6841008f2f66f731bf010ff2",
          "name": "Jennifer Healey",
          "hidden": false
        },
        {
          "_id": "6841008f2f66f731bf010ff3",
          "name": "Nedim Lipka",
          "hidden": false
        },
        {
          "_id": "6841008f2f66f731bf010ff4",
          "name": "Ryan Rossi",
          "hidden": false
        },
        {
          "_id": "6841008f2f66f731bf010ff5",
          "name": "Uttaran Bhattacharya",
          "hidden": false
        },
        {
          "_id": "6841008f2f66f731bf010ff6",
          "name": "Branislav Kveton",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T14:44:23.000Z",
      "submittedOnDailyAt": "2025-06-05T00:57:33.123Z",
      "title": "Quantitative LLM Judges\n\n삽질 LLM 챌린스\n\n(注意：虽然您要求不添加任何解释或额外的文本，但为了确保翻译的准确性和专业性，我将确保翻译内容符合韩国语的习惯用法和专业术语。)",
      "submittedOnDailyBy": {
        "_id": "62c5947524171688a9feb992",
        "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
        "isPro": false,
        "fullname": "Franck Dernoncourt",
        "user": "Franck-Dernoncourt",
        "type": "user"
      },
      "summary": "LLM-as-a-judge는 대규모 언어 모델(LLM)가 다른 LLM의 출력을 자동으로 평가하는 프레임워크입니다. 우리는 quantitative LLM judge를 제안합니다. 이것은 현재의 LLM judge의 평가 점수를 회귀 모델을 사용하여 특정 영역에서 인간이 평가한 점수와 일치하도록 조정하는 것입니다. 이 모델은 평가자의 문맥 평가와 점수를 사용하여 원래의 평가자의 점수를 향상시키기 위해 훈련되었습니다. 우리는 4가지의 quantitative judge를 소개합니다. 이 것은 우리 프레임워크의 일반성과 다양성을 보여주는 것입니다. 우리 프레임워크는 관찰 학습이 아니라, 훈련된 feedback를 사용하여 계산적으로 효율적이며, 인간 feedback가 제한된 경우 통계적으로 효율적이기 때문에 기대됩니다. 우리 프레임워크의 효과를 4가지의 데이터 세트와 2가지의 기본 judge를 사용하여 실험적으로 검증했습니다. 우리의 실험은 quantitative judge가 기존의 judge의 예측력을 효과적으로 향상시키는 것을 보여줍니다.",
      "upvotes": 4,
      "discussionId": "684100902f66f731bf01101e",
      "ai_summary": "A framework uses quantitative LLM judges to align existing LLM evaluation scores with human scores, improving predictive power and efficiency through regression models.",
      "ai_keywords": [
        "LLM-as-a-judge",
        "large language model",
        "quantitative LLM judges",
        "regression models",
        "score alignment",
        "predictive power",
        "post-hoc modeling"
      ]
    },
    "publishedAt": "2025-06-03T10:44:23.000Z",
    "title": "Quantitative LLM Judges",
    "summary": "LLM-as-a-judge is a framework in which a large language model (LLM)\nautomatically evaluates the output of another LLM. We propose quantitative LLM\njudges, which align evaluation scores of existing LLM judges to human scores in\na given domain using regression models. The models are trained to improve the\nscore of the original judge by using the judge's textual evaluation and score.\nWe present four quantitative judges for different types of absolute and\nrelative feedback, which showcases the generality and versatility of our\nframework. Our framework is more computationally efficient than supervised\nfine-tuning and can be more statistically efficient when human feedback is\nlimited, which is expected in most applications of our work. We validate these\nclaims empirically on four datasets using two base judges. Our experiments show\nthat quantitative judges can effectively improve the predictive power of\nexisting judges through post-hoc modeling.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.02945.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62c5947524171688a9feb992",
      "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
      "fullname": "Franck Dernoncourt",
      "name": "Franck-Dernoncourt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.00482",
      "authors": [
        {
          "_id": "68402986a50b67f983749710",
          "user": {
            "_id": "6576ace7769f3ee9bd7b1b88",
            "avatarUrl": "/avatars/5b5921e54413a37afde6ce017809c86e.svg",
            "isPro": false,
            "fullname": "Eunsu Kim",
            "user": "EunsuKim",
            "type": "user"
          },
          "name": "Eunsu Kim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:27:49.056Z",
          "hidden": false
        },
        {
          "_id": "68402986a50b67f983749711",
          "name": "Haneul Yoo",
          "hidden": false
        },
        {
          "_id": "68402986a50b67f983749712",
          "name": "Guijin Son",
          "hidden": false
        },
        {
          "_id": "68402986a50b67f983749713",
          "name": "Hitesh Patel",
          "hidden": false
        },
        {
          "_id": "68402986a50b67f983749714",
          "name": "Amit Agarwal",
          "hidden": false
        },
        {
          "_id": "68402986a50b67f983749715",
          "user": {
            "_id": "60e0251ea9b5d8282481f2b7",
            "avatarUrl": "/avatars/43441373af054a6184c22097bfeb97e4.svg",
            "isPro": false,
            "fullname": "Alice Oh",
            "user": "aliceoh",
            "type": "user"
          },
          "name": "Alice Oh",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-06-05T06:37:21.889Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-31T09:24:32.000Z",
      "submittedOnDailyAt": "2025-06-05T04:40:48.000Z",
      "title": "BenchHub: 유닛 벤치마크 시트입니다. LLM의 전반적인, 사용자 정의 가능한 평가입니다.",
      "submittedOnDailyBy": {
        "_id": "6576ace7769f3ee9bd7b1b88",
        "avatarUrl": "/avatars/5b5921e54413a37afde6ce017809c86e.svg",
        "isPro": false,
        "fullname": "Eunsu Kim",
        "user": "EunsuKim",
        "type": "user"
      },
      "summary": "LLMs가 발전하는 가운데, 최신 데이터셋과 정비된 벤치마크의 필요성이 날로 중요해졌습니다. 그러나 현재의 데이터셋은 분산되어 있고, 관리가 어려워서 특정 요구사항이나 분야에 맞는 평가를 수행하는 것이 어렵습니다. 특히, 수학이나 코딩 등 특정 분야의 모델의 중요성이 증가하고 있습니다. 이 논문에서는 연구자와 개발자가 LLMs를 효과적으로 평가할 수 있도록 벤치마크 데이터셋을 수집하고, 동적으로 클래스 피드하는 BenchHub라는 벤치마크 리포지토리를 소개합니다. BenchHub은 38개의 벤치마크를 가로지르는 303K개의 질문을 통합하고, 연속적인 업데이트와 scalable한 데이터 관리를 지원합니다. 이로써 다양한 분야와 사용 사례에 맞는 유연한 평가가 가능합니다. LLM Families의 다양한 실험을 통해, 특정 분야의 서브셋에서 모델의 성능이 현저히 다르다는 것을 보여주고, 분야에 대한 벤치마크의 중요성을 강조합니다. BenchHub은 데이터셋의 재활용을 촉진하고, 모델 비교의 투명화, 기존 벤치마크에서 표현이 부족한 분야의 쉽게 식별을 가능하게 하고, LLM 평가 연구의 발전에 중요한 인프라를 제공합니다.",
      "upvotes": 3,
      "discussionId": "68402987a50b67f983749746",
      "projectPage": "https://huggingface.co/BenchHub",
      "githubRepo": "https://github.com/rladmstn1714/BenchHub",
      "ai_summary": "BenchHub is a dynamic benchmark repository that aggregates and classifies datasets for large language models, facilitating domain-specific evaluations and improving model comparisons.",
      "ai_keywords": [
        "large language models",
        "LLMs",
        "BenchHub",
        "benchmark repository",
        "domain-specific models",
        "benchmark datasets",
        "continuous updates",
        "scalable data management",
        "model performance",
        "domain-aware benchmarking"
      ]
    },
    "publishedAt": "2025-05-31T05:24:32.000Z",
    "title": "BenchHub: A Unified Benchmark Suite for Holistic and Customizable LLM\n  Evaluation",
    "summary": "As large language models (LLMs) continue to advance, the need for up-to-date\nand well-organized benchmarks becomes increasingly critical. However, many\nexisting datasets are scattered, difficult to manage, and make it challenging\nto perform evaluations tailored to specific needs or domains, despite the\ngrowing importance of domain-specific models in areas such as math or code. In\nthis paper, we introduce BenchHub, a dynamic benchmark repository that empowers\nresearchers and developers to evaluate LLMs more effectively. BenchHub\naggregates and automatically classifies benchmark datasets from diverse\ndomains, integrating 303K questions across 38 benchmarks. It is designed to\nsupport continuous updates and scalable data management, enabling flexible and\ncustomizable evaluation tailored to various domains or use cases. Through\nextensive experiments with various LLM families, we demonstrate that model\nperformance varies significantly across domain-specific subsets, emphasizing\nthe importance of domain-aware benchmarking. We believe BenchHub can encourage\nbetter dataset reuse, more transparent model comparisons, and easier\nidentification of underrepresented areas in existing benchmarks, offering a\ncritical infrastructure for advancing LLM evaluation research.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.00482.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6576ace7769f3ee9bd7b1b88",
      "avatarUrl": "/avatars/5b5921e54413a37afde6ce017809c86e.svg",
      "fullname": "Eunsu Kim",
      "name": "EunsuKim",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.23807",
      "authors": [
        {
          "_id": "683fec0a9f37285365be6142",
          "user": {
            "_id": "656201912d309fa7e27ddf40",
            "avatarUrl": "/avatars/d1bb9b263a758a0b0e7f803f4f888e95.svg",
            "isPro": false,
            "fullname": "Yuli chen",
            "user": "yulichen",
            "type": "user"
          },
          "name": "Yuli Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T15:03:32.826Z",
          "hidden": false
        },
        {
          "_id": "683fec0a9f37285365be6143",
          "name": "Bo Cheng",
          "hidden": false
        },
        {
          "_id": "683fec0a9f37285365be6144",
          "name": "Jiale Han",
          "hidden": false
        },
        {
          "_id": "683fec0a9f37285365be6145",
          "name": "Yingying Zhang",
          "hidden": false
        },
        {
          "_id": "683fec0a9f37285365be6146",
          "name": "Yingting Li",
          "hidden": false
        },
        {
          "_id": "683fec0a9f37285365be6147",
          "name": "Shuhao Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-27T07:35:00.000Z",
      "submittedOnDailyAt": "2025-06-05T00:42:31.891Z",
      "title": "DLP: 각 레이어별 동적 축소화 (Dynamic Layer Pruning)",
      "submittedOnDailyBy": {
        "_id": "656201912d309fa7e27ddf40",
        "avatarUrl": "/avatars/d1bb9b263a758a0b0e7f803f4f888e95.svg",
        "isPro": false,
        "fullname": "Yuli chen",
        "user": "yulichen",
        "type": "user"
      },
      "summary": "プリニング는 최근 대규모 언어 모델(LLMs)의 파라미터 사이즈를 줄이고 추론 효율을 향상시키기 위해 광범위하게 도입되고 있습니다.主流의 プリニング 방법들은 일반적으로 균일한 레이어별 プリニング 전략을 기반으로 하고 있으며, 높은 스パース레벨에서 성능 저하가 심각합니다. LLMs의 각 레이어의 다른 기여를 인식하고 최근의 연구는 비균일한 레이어별 プリニング에 초점을 맞추고 있지만, 이러한 접근 방식은 일반적으로 사전 정의된 값을 기반으로 하고, 최적의 성능을 얻을 수 없습니다. 이러한 한계를 극복하기 위해 우리는 새로운 방법인 \"Dynamic Layerwise Pruning(DLP)\"를 제안합니다. 이 접근 방식은 모델의 가중치와 입력 활성화 정보를 통합하여 각 레이어의 상대적인 중요성을 적응적으로 결정하고, プリニング 레이트를 적절히 할당합니다. 실험 결과를 통해, DLP는 높은 스パース레벨에서 모델의 성능을 유지하고 여러 LLMs에 효과적으로 적용됩니다. 특히, 70%의 스パース레벨에서, DLP는 LLaMA2-7B의 perplexity를 7.79% 감소시켰으며, 가장 선진한 방법과 비교하여 평균 정확도를 2.7% 향상시켰습니다. 또한, DLP는 현재의 LLM 압축 기술과 통합이 가능하며, Parameter-Efficient Fine-Tuning(PEFT)에도 쉽게 통합할 수 있습니다. 우리는 https://github.com/ironartisan/DLP에서 코드를 릴리스하고 있으며, 향후 연구를 촉진하기 위해 있습니다.",
      "upvotes": 3,
      "discussionId": "683fec0a9f37285365be617f",
      "ai_summary": "A dynamic layerwise pruning method adaptively determines layer importance by combining model weights and activation information to maintain performance in large language models at high sparsity.",
      "ai_keywords": [
        "pruning",
        "Large Language Models (LLMs)",
        "uniform layerwise pruning",
        "non-uniform layerwise pruning",
        "Dynamic Layerwise Pruning (DLP)",
        "perplexity",
        "Parameter-Efficient Fine-Tuning (PEFT)"
      ]
    },
    "publishedAt": "2025-05-27T03:35:00.000Z",
    "title": "DLP: Dynamic Layerwise Pruning in Large Language Models",
    "summary": "Pruning has recently been widely adopted to reduce the parameter scale and\nimprove the inference efficiency of Large Language Models (LLMs). Mainstream\npruning techniques often rely on uniform layerwise pruning strategies, which\ncan lead to severe performance degradation at high sparsity levels. Recognizing\nthe varying contributions of different layers in LLMs, recent studies have\nshifted their focus toward non-uniform layerwise pruning. However, these\napproaches often rely on pre-defined values, which can result in suboptimal\nperformance. To overcome these limitations, we propose a novel method called\nDynamic Layerwise Pruning (DLP). This approach adaptively determines the\nrelative importance of each layer by integrating model weights with input\nactivation information, assigning pruning rates accordingly. Experimental\nresults show that DLP effectively preserves model performance at high sparsity\nlevels across multiple LLMs. Specifically, at 70% sparsity, DLP reduces the\nperplexity of LLaMA2-7B by 7.79 and improves the average accuracy by 2.7%\ncompared to state-of-the-art methods. Moreover, DLP is compatible with various\nexisting LLM compression techniques and can be seamlessly integrated into\nParameter-Efficient Fine-Tuning (PEFT). We release the code at\nhttps://github.com/ironartisan/DLP to facilitate future research.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23807.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "656201912d309fa7e27ddf40",
      "avatarUrl": "/avatars/d1bb9b263a758a0b0e7f803f4f888e95.svg",
      "fullname": "Yuli chen",
      "name": "yulichen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.04133",
      "authors": [
        {
          "_id": "6840f32dda736de98e843831",
          "user": {
            "_id": "64d3c16a0553a2522f1aa792",
            "avatarUrl": "/avatars/951e272ffccf2388f138b248e5ef7142.svg",
            "isPro": false,
            "fullname": "Shaina Raza",
            "user": "shainar",
            "type": "user"
          },
          "name": "Shaina Raza",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-06-05T01:39:24.184Z",
          "hidden": false
        },
        {
          "_id": "6840f32dda736de98e843832",
          "name": "Ranjan Sapkota",
          "hidden": false
        },
        {
          "_id": "6840f32dda736de98e843833",
          "name": "Manoj Karkee",
          "hidden": false
        },
        {
          "_id": "6840f32dda736de98e843834",
          "name": "Christos Emmanouilidis",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/67ddd80896ac367438d400a6/7jK4mzUkVjPRUDMAacaCO.jpeg"
      ],
      "publishedAt": "2025-06-04T16:26:11.000Z",
      "submittedOnDailyAt": "2025-06-05T00:02:27.010Z",
      "title": "TRiSM for Agentic AI: 에이전트형 AI의 신뢰성, 위험, 그리고 보안 관리 검토\n\nLLM 기반의 에이전트형 다 에이전트 시스템에서 신뢰성, 위험, 그리고 보안 관리 검토",
      "submittedOnDailyBy": {
        "_id": "67ddd80896ac367438d400a6",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/C1NY6Nv5i0erwLnzCrTUM.png",
        "isPro": false,
        "fullname": "Ranjan Sapkota",
        "user": "RanjanSapkota",
        "type": "user"
      },
      "summary": "Agentic AI 시스템은 대규모 언어 모델(LLM)에 기반하여, 다 에이전트 구성으로 구축되어 기업 및 사회 영역에서의 뇌의 자율성, 협력과 의사결정을 재 정의하고 있습니다. 본 리뷰는 LLM 기반의 다 에이전트 시스템(AMAS)의 맥락에서 신뢰, 리스크, 보안 관리(TRiSM)에 대한 구조화된 분석을 제공합니다. 먼저, Agentic AI의 개념적 기초, 프레임워크적 AI 에이전트와의 차이, 에이전트의 기능을 확장하는 시스템 설계에 대해 조사합니다. 이어서, Agentic AI 프레임워크에서 TRiSM는 4가지 기둥인 조이니스, 설명성, ModelOps, 프라이버시/보안에 의해 상세히 설명됩니다. Agentic AI 애플리케이션의 특징적인 리스크 벡터를 특정하고, 리스크 타크로니미를 지원하는 현실적인 취약성을 보여주는 사례 연구를 제시합니다. 또한, 신뢰 구축 기관, 투명성 및 감시 방법, 분산된 LLM 에이전트 시스템의 최신 설명성 전략을 조사합니다. 또한, 신뢰, 설명성, 인간공학에 대한 성능을 평가하는 메트릭도 조사되며, 오픈 벤치마크 도전을 포함하는 오픈 벤치마크 도전을 조사합니다. 보안 및 프라이버시는 암호화, 적대적 방어, AI 법규의 진화에 대응하는 조사가 있습니다.本研究는 책임 있는 Agentic AI의 지도를 제공하며, 강력한 TRiSM 원칙에 맞는 안전, 책임, 투명한 adop이 위한 연구 방향을 제안합니다.",
      "upvotes": 2,
      "discussionId": "6840f32eda736de98e843858",
      "ai_summary": "A review of trust, risk, and security management in LLM-based agentic multi-agent systems, examining governance, explainability, ModelOps, and privacy/security.",
      "ai_keywords": [
        "LLMs",
        "agentic AI",
        "multi-agent systems",
        "TRiSM",
        "governance",
        "explainability",
        "ModelOps",
        "privacy",
        "security",
        "encryption",
        "adversarial defense",
        "compliance",
        "AI regulations",
        "trust-building mechanisms",
        "transparency",
        "oversight",
        "interpretability",
        "human-centered performance",
        "benchmarking",
        "responsible AI",
        "research directions"
      ]
    },
    "publishedAt": "2025-06-04T12:26:11.000Z",
    "title": "TRiSM for Agentic AI: A Review of Trust, Risk, and Security Management\n  in LLM-based Agentic Multi-Agent Systems",
    "summary": "Agentic AI systems, built on large language models (LLMs) and deployed in\nmulti-agent configurations, are redefining intelligent autonomy, collaboration\nand decision-making across enterprise and societal domains. This review\npresents a structured analysis of Trust, Risk, and Security Management (TRiSM)\nin the context of LLM-based agentic multi-agent systems (AMAS). We begin by\nexamining the conceptual foundations of agentic AI, its architectural\ndifferences from traditional AI agents, and the emerging system designs that\nenable scalable, tool-using autonomy. The TRiSM in the agentic AI framework is\nthen detailed through four pillars governance, explainability, ModelOps, and\nprivacy/security each contextualized for agentic LLMs. We identify unique\nthreat vectors and introduce a comprehensive risk taxonomy for the agentic AI\napplications, supported by case studies illustrating real-world\nvulnerabilities. Furthermore, the paper also surveys trust-building mechanisms,\ntransparency and oversight techniques, and state-of-the-art explainability\nstrategies in distributed LLM agent systems. Additionally, metrics for\nevaluating trust, interpretability, and human-centered performance are reviewed\nalongside open benchmarking challenges. Security and privacy are addressed\nthrough encryption, adversarial defense, and compliance with evolving AI\nregulations. The paper concludes with a roadmap for responsible agentic AI,\nproposing research directions to align emerging multi-agent systems with robust\nTRiSM principles for safe, accountable, and transparent deployment.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/67ddd80896ac367438d400a6/7jK4mzUkVjPRUDMAacaCO.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04133.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67ddd80896ac367438d400a6",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/C1NY6Nv5i0erwLnzCrTUM.png",
      "fullname": "Ranjan Sapkota",
      "name": "RanjanSapkota",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.04034",
      "authors": [
        {
          "_id": "6840ff0b535bfb4942b31576",
          "name": "Qing Jiang",
          "hidden": false
        },
        {
          "_id": "6840ff0b535bfb4942b31577",
          "name": "Xingyu Chen",
          "hidden": false
        },
        {
          "_id": "6840ff0b535bfb4942b31578",
          "name": "Zhaoyang Zeng",
          "hidden": false
        },
        {
          "_id": "6840ff0b535bfb4942b31579",
          "name": "Junzhi Yu",
          "hidden": false
        },
        {
          "_id": "6840ff0b535bfb4942b3157a",
          "name": "Lei Zhang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/647f46b6838ac3601fc89852/0J-cvgz2dA6bVQJJNb_Yz.jpeg"
      ],
      "publishedAt": "2025-06-04T14:56:57.000Z",
      "submittedOnDailyAt": "2025-06-05T00:56:30.698Z",
      "title": "Rex-Thinker: 연결된 컨텍스트 논리 기반의 물체 참조\n\n(Note: The original text \"Rex-Thinker\" was not translated as it appears to be a proper noun or a specific identifier.)",
      "submittedOnDailyBy": {
        "_id": "647f46b6838ac3601fc89852",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647f46b6838ac3601fc89852/N5cr1MFEtgKLJ4sVAhS04.jpeg",
        "isPro": true,
        "fullname": "Qing Jiang",
        "user": "Mountchicken",
        "type": "user"
      },
      "summary": "目標物検出의 목적은 이미지 내에 있는 모든 물체를 주어진 자연어 설명과 일치하는 것을 검출하는 것입니다. 우리는 강력한 목표물 검출 모델이 설명 가능한 시각적 콘텐츠에 대한 정확한 예측을 수행하는 것이 중요하다고 주장합니다. 특히, 이는 다음 2가지 주요 특성을 충족해야 합니다: 1) 시각성, 시각적 증거와 명확한 연관성을 가지는 해석 가능한 이유를 제공하여 예측을 정당화하는 것입니다; 2) 신뢰성, 이미지 내에 없는 목표가 표현될 때도, 해당 표현에 대응하는 물체가 존재하지 않는 경우 그 표현을 거부하여 신뢰할 수 있는 것입니다. 그러나 많은 방법은 목표물 검출을 직접 bounding box 예측 태스크로 취급하여 해석성이 제한되어, 무목표 표현에 대한 거부가 불가능합니다. 본 연구에서는 Rex-Thinker 모델을 제안하여 목표물 검출을 명시적인 CoT(컨텍스트 기반 추론) 태스크로 구성합니다. 목표의 표현을 제공하면, 먼저 목표의 물체 카테고리에 해당하는 모든 후보 물체 인스턴스를 식별합니다. Rex-Thinker는 각 후보에 대해 단계별로 추론을 수행하여 주어진 표현과 일치하는지 평가하고 최종적인 예측을 수행합니다. 이를 지원하기 위해, HumanRef 데이터셋에 대해 GPT-4o를 Prompt하여 큰 규모의 CoT 스타일의 목표물 검출 데이터셋 HumanRef-CoT을 구축했습니다. 각 이유의 흔적은 구축, 행동, 요약의 구조화된 포맷으로 나타내며, 모델이 물체 후보에 대한 해석 가능한 이유를 학습할 수 있도록 합니다. 그리고 Rex-Thinker는 2단계로 훈련됩니다: 냉시작의 단계별 지도 학습 최종 훈련 단계에서 모델이 구축된 이유를 수행하는 것을 가르칩니다, 다음으로 GRPO 기반의 RL 학습으로 정확성과 일반화 능력을 향상시킵니다. 실험은 우리의 접근法是 영역 내 평가에서 정확성과 해석성 기준 라인을 초과하고, 거짓 표현의 출력을 거부할 수 있는 능력과 영역 외 설정에서 강한 일반화 능력을 보여주었습니다.",
      "upvotes": 2,
      "discussionId": "6840ff0e535bfb4942b3165f",
      "projectPage": "https://rexthinker.github.io/",
      "githubRepo": "https://github.com/IDEA-Research/Rex-Thinker",
      "ai_summary": "Rex-Thinker is a CoT-based model that enhances object referring by performing step-by-step reasoning over candidate objects, leading to improved interpretability and rejection of mismatched queries.",
      "ai_keywords": [
        "CoT reasoning",
        "HumanRef-CoT",
        "GPT-4o",
        "structured reasoning",
        "cold-start supervised fine-tuning",
        "GRPO-based RL learning"
      ]
    },
    "publishedAt": "2025-06-04T10:56:57.000Z",
    "title": "Rex-Thinker: Grounded Object Referring via Chain-of-Thought Reasoning",
    "summary": "Object referring aims to detect all objects in an image that match a given\nnatural language description. We argue that a robust object referring model\nshould be grounded, meaning its predictions should be both explainable and\nfaithful to the visual content. Specifically, it should satisfy two key\nproperties: 1) Verifiable, by producing interpretable reasoning that justifies\nits predictions and clearly links them to visual evidence; and 2) Trustworthy,\nby learning to abstain when no object in the image satisfies the given\nexpression. However, most methods treat referring as a direct bounding box\nprediction task, offering limited interpretability and struggling to reject\nexpressions with no matching object. In this work, we propose Rex-Thinker, a\nmodel that formulates object referring as an explicit CoT reasoning task. Given\na referring expression, we first identify all candidate object instances\ncorresponding to the referred object category. Rex-Thinker then performs\nstep-by-step reasoning over each candidate to assess whether it matches the\ngiven expression, before making a final prediction. To support this paradigm,\nwe construct a large-scale CoT-style referring dataset named HumanRef-CoT by\nprompting GPT-4o on the HumanRef dataset. Each reasoning trace follows a\nstructured planning, action, and summarization format, enabling the model to\nlearn decomposed, interpretable reasoning over object candidates. We then train\nRex-Thinker in two stages: a cold-start supervised fine-tuning phase to teach\nthe model how to perform structured reasoning, followed by GRPO-based RL\nlearning to improve accuracy and generalization. Experiments show that our\napproach outperforms standard baselines in both precision and interpretability\non in-domain evaluation, while also demonstrating improved ability to reject\nhallucinated outputs and strong generalization in out-of-domain settings.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/647f46b6838ac3601fc89852/0J-cvgz2dA6bVQJJNb_Yz.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04034.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "647f46b6838ac3601fc89852",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647f46b6838ac3601fc89852/N5cr1MFEtgKLJ4sVAhS04.jpeg",
      "fullname": "Qing Jiang",
      "name": "Mountchicken",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.03951",
      "authors": [
        {
          "_id": "68415a1cce09e3eca94e02ef",
          "user": {
            "_id": "6759546743971eff5a12a087",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/esJm_83zW1R6NqWltof8P.png",
            "isPro": false,
            "fullname": "Aojun Lu",
            "user": "Kurt1024",
            "type": "user"
          },
          "name": "Aojun Lu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T09:59:50.242Z",
          "hidden": false
        },
        {
          "_id": "68415a1cce09e3eca94e02f0",
          "user": {
            "_id": "649d54b314afbb10ce2a9eeb",
            "avatarUrl": "/avatars/15c325d8c2273ff63569f23015e98486.svg",
            "isPro": false,
            "fullname": "Hangjie Yuan",
            "user": "JacobYuan",
            "type": "user"
          },
          "name": "Hangjie Yuan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T09:59:52.907Z",
          "hidden": false
        },
        {
          "_id": "68415a1cce09e3eca94e02f1",
          "name": "Tao Feng",
          "hidden": false
        },
        {
          "_id": "68415a1cce09e3eca94e02f2",
          "name": "Yanan Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-04T13:40:41.000Z",
      "submittedOnDailyAt": "2025-06-05T07:19:58.382Z",
      "title": "계속된 학습의 구조적인 관점에서의 안정성과 유연성의 트레이드오프를 재검토합니다.",
      "submittedOnDailyBy": {
        "_id": "649d54b314afbb10ce2a9eeb",
        "avatarUrl": "/avatars/15c325d8c2273ff63569f23015e98486.svg",
        "isPro": false,
        "fullname": "Hangjie Yuan",
        "user": "JacobYuan",
        "type": "user"
      },
      "summary": "カンティニュアル・ラーニング（CL）의 문제점은 신경망에 학습과 발전적인 적응력을 부여하는 데 목표를 두고 있습니다. 이 문제의 핵심은 \"안정성과 가변성의 두난제\"를 해결하는 것입니다. 이는 과거 학습한 지식의 보존과 새로운 지식을 획득하는 것을 동시에 허용하는 것입니다. CL의 방법들은 이러한 균형을 이루는 데 노력하지만, 이들은 네트워크의 구조가 안정성과 가변성에 미치는 영향을 간과하고 있습니다. 이 논문에서는 구조적 수준에서 안정성과 가변성의 충돌에 초점을 맞추었습니다. 여기에서는 같은 파라미터 제약 하에서 깊은 네트워크는 더 좋은 가변성을 나타내고, 넓은 네트워크는 더 좋은 안정성을 특징으로 하고 있습니다. 이 구조적 수준의 두난제를 해결하기 위해, Dual-Arch라는 새로운 프레임워크를 제안합니다. 이 프레임워크는 CL의 플러그인 구성 요소로 작동하며, 두 개의 서로 다른 독립적인 네트워크의 보완적인 강점을 활용합니다. 이 네트워크들은 각각의 목적에 맞는 다양한 구조를 가지고 있으며, 확장된 실험은 Dual-Arch는 기존의 CL 방법의 성능을 향상시키고 파라미터의 압축성을 87% 이상 향상시키는 것을 보여주었습니다.",
      "upvotes": 2,
      "discussionId": "68415a1dce09e3eca94e0314",
      "projectPage": "https://github.com/byyx666/Dual-Arch",
      "githubRepo": "https://github.com/byyx666/Dual-Arch",
      "ai_summary": "A novel framework, Dual-Arch, enhances Continual Learning by addressing the stability-plasticity dilemma at the architectural level using two specialized networks.",
      "ai_keywords": [
        "Continual Learning",
        "stability-plasticity dilemma",
        "deep networks",
        "wide networks",
        "Dual-Arch"
      ]
    },
    "publishedAt": "2025-06-04T09:40:41.000Z",
    "title": "Rethinking the Stability-Plasticity Trade-off in Continual Learning from\n  an Architectural Perspective",
    "summary": "The quest for Continual Learning (CL) seeks to empower neural networks with\nthe ability to learn and adapt incrementally. Central to this pursuit is\naddressing the stability-plasticity dilemma, which involves striking a balance\nbetween two conflicting objectives: preserving previously learned knowledge and\nacquiring new knowledge. While numerous CL methods aim to achieve this\ntrade-off, they often overlook the impact of network architecture on stability\nand plasticity, restricting the trade-off to the parameter level. In this\npaper, we delve into the conflict between stability and plasticity at the\narchitectural level. We reveal that under an equal parameter constraint, deeper\nnetworks exhibit better plasticity, while wider networks are characterized by\nsuperior stability. To address this architectural-level dilemma, we introduce a\nnovel framework denoted Dual-Arch, which serves as a plug-in component for CL.\nThis framework leverages the complementary strengths of two distinct and\nindependent networks: one dedicated to plasticity and the other to stability.\nEach network is designed with a specialized and lightweight architecture,\ntailored to its respective objective. Extensive experiments demonstrate that\nDual-Arch enhances the performance of existing CL methods while being up to 87%\nmore compact in terms of parameters.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03951.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "649d54b314afbb10ce2a9eeb",
      "avatarUrl": "/avatars/15c325d8c2273ff63569f23015e98486.svg",
      "fullname": "Hangjie Yuan",
      "name": "JacobYuan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.03614",
      "authors": [
        {
          "_id": "684134ca20ff8abcccb11302",
          "name": "Zhanhui Zhou",
          "hidden": false
        },
        {
          "_id": "684134ca20ff8abcccb11303",
          "name": "Lingjie Chen",
          "hidden": false
        },
        {
          "_id": "684134ca20ff8abcccb11304",
          "name": "Chao Yang",
          "hidden": false
        },
        {
          "_id": "684134ca20ff8abcccb11305",
          "name": "Chaochao Lu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-04T06:46:06.000Z",
      "submittedOnDailyAt": "2025-06-05T04:41:50.905Z",
      "title": "VLMs는 분산된 훈련 데이터 포치를 집중할 수 있습니다.",
      "submittedOnDailyBy": {
        "_id": "642e5a7ba0b65dce1f87a7a2",
        "avatarUrl": "/avatars/3ae01c9330a47e98fac9f1eb0ba94073.svg",
        "isPro": false,
        "fullname": "Zhanhui Zhou",
        "user": "ZHZisZZ",
        "type": "user"
      },
      "summary": "ビジョン言語モデル(VLMs)의 위험을 줄이기 위한 방법 중 하나는, 훈련 데이터에서 위험한 샘플을 제거하는 것입니다. 그러나, 유해한 이미지가 작은, 좋은 보이는 패치로 분리되어, 여러 훈련 샘플에 분산되어 있을 경우, 이러한 데이터 모델링은 쉽게 회避될 수 있습니다. 그 결과, VLMs는 훈련 중이므로, 이러한 패치를 조합하여, 추론 시 유해한 응답을 생성할 수 있습니다. 예를 들어,鲜血淋漓의 장면의 이미지 패치와 \"안전\"이라는 설명을 조합하여 훈련된 경우, VLMs는 그 장면의 전체 이미지 또는 문맥을 \"안전\"이라는 설명으로 설명할 수 있습니다.\n\n이러한 공격을 가능하게 하는 VLMs의 핵심 능력은 \"시각 스테이징\"이라고 정의합니다. 이는, 같은 문맥 설명을 공유하는 여러 훈련 샘플에 분산된 시각 정보를 통합하는 능력입니다. 우리의 연구에서, 3개의 데이터 세트에서, 각 이미지에 고유한 합성 ID를 부여한 경우, VLMs의 시각 스테이징 능력을 보여줍니다. 먼저, 각 (이미지, ID) 쌍을, 다른 수준으로 분할한 (패치, ID) 쌍으로 변환하여 훈련을 수행하고, 훈련된 모델이 전체 이미지 또는 문맥을 통해 정확한 ID를 말할 수 있음을 보여줍니다. 이를 기반으로, 위의 대책을 모방하여, 위험한 이미지의 패치를 사용하며, ID를 \"안전\"이나 \"불안정\"이라는 문맥 설명에 대체하여, 유해한 콘텐츠가 패치로 모델링을 회피하고, 이후 시각 스테이징에 의해 재구성되는 것을 보여, VLMs의 안전성 위험을 심각하게 평가합니다. 코드는, https://github.com/ZHZisZZ/visual-stitching에 공개되어 있습니다.",
      "upvotes": 2,
      "discussionId": "684134cb20ff8abcccb11334",
      "githubRepo": "https://github.com/ZHZisZZ/visual-stitching",
      "ai_summary": "VLMs exhibit visual stitching, an ability to integrate fragmented visual information, which enables harmful content to evade data moderation and be reconstructed during inference.",
      "ai_keywords": [
        "vision-language models",
        "VLMs",
        "visual stitching",
        "data moderation",
        "adversarial data poisoning",
        "image patches",
        "textual descriptions",
        "inference"
      ]
    },
    "publishedAt": "2025-06-04T02:46:06.000Z",
    "title": "VLMs Can Aggregate Scattered Training Patches",
    "summary": "One way to mitigate risks in vision-language models (VLMs) is to remove\ndangerous samples in their training data. However, such data moderation can be\neasily bypassed when harmful images are split into small, benign-looking\npatches, scattered across many training samples. VLMs may then learn to piece\nthese fragments together during training and generate harmful responses at\ninference, either from full images or text references. For instance, if trained\non image patches from a bloody scene paired with the descriptions \"safe,\" VLMs\nmay later describe, the full image or a text reference to the scene, as \"safe.\"\nWe define the core ability of VLMs enabling this attack as visual\nstitching -- the ability to integrate visual information spread across\nmultiple training samples that share the same textual descriptions. In our\nwork, we first demonstrate visual stitching abilities in common open-source\nVLMs on three datasets where each image is labeled with a unique synthetic ID:\nwe split each (image, ID) pair into {(patch,\nID)} pairs at different granularity for finetuning, and we find that\ntuned models can verbalize the correct IDs from full images or text reference.\nBuilding on this, we simulate the adversarial data poisoning scenario mentioned\nabove by using patches from dangerous images and replacing IDs with text\ndescriptions like ``safe'' or ``unsafe'', demonstrating how harmful content can\nevade moderation in patches and later be reconstructed through visual\nstitching, posing serious VLM safety risks. Code is available at\nhttps://github.com/ZHZisZZ/visual-stitching.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03614.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642e5a7ba0b65dce1f87a7a2",
      "avatarUrl": "/avatars/3ae01c9330a47e98fac9f1eb0ba94073.svg",
      "fullname": "Zhanhui Zhou",
      "name": "ZHZisZZ",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.01344",
      "authors": [
        {
          "_id": "6841009bdf863485e04879c8",
          "name": "Manan Suri",
          "hidden": false
        },
        {
          "_id": "6841009bdf863485e04879c9",
          "user": {
            "_id": "65c16444d4c3b8dff2f0d78d",
            "avatarUrl": "/avatars/4ed764c1657bd260d2a12ba61c111062.svg",
            "isPro": false,
            "fullname": "Puneet Mathur",
            "user": "puneetm",
            "type": "user"
          },
          "name": "Puneet Mathur",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-05T02:27:41.018Z",
          "hidden": false
        },
        {
          "_id": "6841009bdf863485e04879ca",
          "name": "Nedim Lipka",
          "hidden": false
        },
        {
          "_id": "6841009bdf863485e04879cb",
          "user": {
            "_id": "62c5947524171688a9feb992",
            "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
            "isPro": false,
            "fullname": "Franck Dernoncourt",
            "user": "Franck-Dernoncourt",
            "type": "user"
          },
          "name": "Franck Dernoncourt",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:26:52.205Z",
          "hidden": false
        },
        {
          "_id": "6841009bdf863485e04879cc",
          "name": "Ryan A. Rossi",
          "hidden": false
        },
        {
          "_id": "6841009bdf863485e04879cd",
          "name": "Vivek Gupta",
          "hidden": false
        },
        {
          "_id": "6841009bdf863485e04879ce",
          "name": "Dinesh Manocha",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-02T06:02:41.000Z",
      "submittedOnDailyAt": "2025-06-05T00:57:44.204Z",
      "title": "흐름을 초월하기: 뉴로신보리크 에이전트에 의한 미세한 흐름 차트 특성화\n\n(请注意，虽然翻译保持了原文的专业性和准确性，但“뉴로신보리크 에이전트”这一术语在韩语中可能需要根据具体上下文进行调整，以确保最准确的翻译。)",
      "submittedOnDailyBy": {
        "_id": "62c5947524171688a9feb992",
        "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
        "isPro": false,
        "fullname": "Franck Dernoncourt",
        "user": "Franck-Dernoncourt",
        "type": "user"
      },
      "summary": "フローチャート는 결산 과정의 시각화를 위해 중요한 도구입니다. 그러나, 그 비선형적인 구조와 복잡한 시각적, 문법적 관계는 LLMs를 사용하여 フローチャート를 해석하는 데 어려움을 초래합니다. 이로 인해 물류, 건강, 공학 등 중요한 분야에서 자동화 フローチャート 처리의 신뢰도가 떨어집니다. 우리는 LLM의 응답에 기반한 フローチャート의 특정 구성 요소를 추적하는 Fine-grained Flowchart Attribution의 작업을 도입합니다. フローチャート Attribution은 LLM의 예측의 증명성을 보장하고, 생성된 응답을 フローチャート의 구조와 연결하여 해석성을 높입니다. 우리는 graph-based reasoning를 사용하여 fine-grained post hoc Attribution을 수행하는 neurosymbolic agent로서 FlowPathAgent를 제안합니다. 이는 フローチャート를 분할하고 구조화된 기호적 그래프로 변환하여, 그래프와 동적으로 상호작용하며 Attribution Path를 생성하는 어그リ언스 접근 방식을 사용합니다. 또한, FlowExplainBench라는 새로운 벤치마크를 소개합니다. 이는 다양한 스타일, 분야, 질문 유형의 フローチャート Attribution을 평가하기 위한 것입니다. 실험 결과를 통해, FlowPathAgent는 フローチャート QA에서 LLM의 응답의 시각적 해설을 줄이고, FlowExplainBench 데이터 세트에서 10-14% 이상 높은 베이스라인을 초과합니다.",
      "upvotes": 2,
      "discussionId": "6841009ddf863485e0487a38",
      "ai_summary": "FlowPathAgent, a neurosymbolic agent, enhances the reliability of LLM predictions for flowchart interpretation by tracing specific components and generating accurate attribution paths.",
      "ai_keywords": [
        "Flowcharts",
        "Fine-grained Flowchart Attribution",
        "FlowPathAgent",
        "graph-based reasoning",
        "symbolic graph",
        "neurosymbolic agent",
        "flowExplainBench",
        "flowchart QA"
      ]
    },
    "publishedAt": "2025-06-02T02:02:41.000Z",
    "title": "Follow the Flow: Fine-grained Flowchart Attribution with Neurosymbolic\n  Agents",
    "summary": "Flowcharts are a critical tool for visualizing decision-making processes.\nHowever, their non-linear structure and complex visual-textual relationships\nmake it challenging to interpret them using LLMs, as vision-language models\nfrequently hallucinate nonexistent connections and decision paths when\nanalyzing these diagrams. This leads to compromised reliability for automated\nflowchart processing in critical domains such as logistics, health, and\nengineering. We introduce the task of Fine-grained Flowchart Attribution, which\ntraces specific components grounding a flowchart referring LLM response.\nFlowchart Attribution ensures the verifiability of LLM predictions and improves\nexplainability by linking generated responses to the flowchart's structure. We\npropose FlowPathAgent, a neurosymbolic agent that performs fine-grained post\nhoc attribution through graph-based reasoning. It first segments the flowchart,\nthen converts it into a structured symbolic graph, and then employs an agentic\napproach to dynamically interact with the graph, to generate attribution paths.\nAdditionally, we present FlowExplainBench, a novel benchmark for evaluating\nflowchart attributions across diverse styles, domains, and question types.\nExperimental results show that FlowPathAgent mitigates visual hallucinations in\nLLM answers over flowchart QA, outperforming strong baselines by 10-14% on our\nproposed FlowExplainBench dataset.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01344.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62c5947524171688a9feb992",
      "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
      "fullname": "Franck Dernoncourt",
      "name": "Franck-Dernoncourt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.03817",
      "authors": [
        {
          "_id": "68415ee454d7c6b3f9786deb",
          "name": "Julius Gonsior",
          "hidden": false
        },
        {
          "_id": "68415ee454d7c6b3f9786dec",
          "name": "Tim Rieß",
          "hidden": false
        },
        {
          "_id": "68415ee454d7c6b3f9786ded",
          "name": "Anja Reusch",
          "hidden": false
        },
        {
          "_id": "68415ee454d7c6b3f9786dee",
          "name": "Claudio Hartmann",
          "hidden": false
        },
        {
          "_id": "68415ee454d7c6b3f9786def",
          "name": "Maik Thiele",
          "hidden": false
        },
        {
          "_id": "68415ee454d7c6b3f9786df0",
          "name": "Wolfgang Lehner",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-04T10:41:37.000Z",
      "submittedOnDailyAt": "2025-06-05T07:40:58.805Z",
      "title": "アクティブ 학습의 파라미터 조사：대규모 실험 그리드에서의 어인사이즈",
      "submittedOnDailyBy": {
        "_id": "637638fa1f0421002b42facb",
        "avatarUrl": "/avatars/ed1e3024cd2ed7284b437db4bbeb2668.svg",
        "isPro": false,
        "fullname": "Julius Gonsior",
        "user": "jgonsior",
        "type": "user"
      },
      "summary": "데이터의 설명은 시간이 걸리며 비용이 들며, 그러나 홈라인 기계 학습에 고유한 필요성이 있습니다. 활성 학습(AL)은 가장 정보량을 가진 무 라벨 샘플을 연속적으로 선택하고 전문가의 설명을 요구하여, 인간의 라벨 작업의 최소化 및 전체의 분류 성능을 향상시키기 위한 이미 확립된 방법입니다. AL은 수십 년간 알려져 있지만, 실세계의 응용에서는 아직 희귀하게 사용되고 있습니다. NLP 커뮤니티의 두 개의 웹 조사에서 나타난 것처럼, AL의 사용이 방해되는 두 가지 주요 이유는 있습니다: 1. AL의 설정의 복잡성, 2. 그 효과성의 신뢰가 부족합니다. 두 가지 이유를 원인으로 생각하고 있습니다. 이 큰 초 파라미터 공간은, AL의 실험 결과가 잘못되어 있거나 재현 가능한 경우가 많은 경우를 일으키는 원인입니다. 본 연구에서는, 1. 460만 이상의 초 파라미터 조합을 큰 초 파라미터 그리드으로 생성하고, 2. 그 모든 조합의 성능을 기록하고, 3. 실험 결과에서 각 초 파라미터의 영향을 분석했습니다. 최종적으로, 각 초 파라미터의 영향을 대해 추천을 제공하고, 구체적인 AL 전략의 구현에 놀라울 정도로 많은 영향을 보여, 최소한의 계산 노력을 통해 재현 가능한 AL 실험의 설계를 개요화하고, 미래의 재현 가능한 신뢰할 수 있는 AL 연구에 기여합니다.",
      "upvotes": 1,
      "discussionId": "68415ee554d7c6b3f9786e15",
      "githubRepo": "https://github.com/jgonsior/olympic-games-of-active-learning",
      "ai_summary": "The study investigates the impact of hyperparameters on Active Learning performance, providing insights to improve its practical application and reproducibility.",
      "ai_keywords": [
        "Active Learning",
        "hyperparameter space",
        "hyperparameter grid",
        "experimental study design",
        "reproducibility",
        "trustworthiness"
      ]
    },
    "publishedAt": "2025-06-04T06:41:37.000Z",
    "title": "Survey of Active Learning Hyperparameters: Insights from a Large-Scale\n  Experimental Grid",
    "summary": "Annotating data is a time-consuming and costly task, but it is inherently\nrequired for supervised machine learning. Active Learning (AL) is an\nestablished method that minimizes human labeling effort by iteratively\nselecting the most informative unlabeled samples for expert annotation, thereby\nimproving the overall classification performance. Even though AL has been known\nfor decades, AL is still rarely used in real-world applications. As indicated\nin the two community web surveys among the NLP community about AL, two main\nreasons continue to hold practitioners back from using AL: first, the\ncomplexity of setting AL up, and second, a lack of trust in its effectiveness.\nWe hypothesize that both reasons share the same culprit: the large\nhyperparameter space of AL. This mostly unexplored hyperparameter space often\nleads to misleading and irreproducible AL experiment results. In this study, we\nfirst compiled a large hyperparameter grid of over 4.6 million hyperparameter\ncombinations, second, recorded the performance of all combinations in the\nso-far biggest conducted AL study, and third, analyzed the impact of each\nhyperparameter in the experiment results. In the end, we give recommendations\nabout the influence of each hyperparameter, demonstrate the surprising\ninfluence of the concrete AL strategy implementation, and outline an\nexperimental study design for reproducible AL experiments with minimal\ncomputational effort, thus contributing to more reproducible and trustworthy AL\nresearch in the future.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03817.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "637638fa1f0421002b42facb",
      "avatarUrl": "/avatars/ed1e3024cd2ed7284b437db4bbeb2668.svg",
      "fullname": "Julius Gonsior",
      "name": "jgonsior",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.03538",
      "authors": [
        {
          "_id": "6841585dd777f13c59460b47",
          "name": "Chengqi Li",
          "hidden": false
        },
        {
          "_id": "6841585dd777f13c59460b48",
          "name": "Zhihao Shi",
          "hidden": false
        },
        {
          "_id": "6841585dd777f13c59460b49",
          "name": "Yangdi Lu",
          "hidden": false
        },
        {
          "_id": "6841585dd777f13c59460b4a",
          "name": "Wenbo He",
          "hidden": false
        },
        {
          "_id": "6841585dd777f13c59460b4b",
          "user": {
            "_id": "634e60454677a5891c0902f4",
            "avatarUrl": "/avatars/4dc143719afe7686e05b7f2c2c5c1871.svg",
            "isPro": false,
            "fullname": "Xiangyu Xu",
            "user": "xjcvcvxj",
            "type": "user"
          },
          "name": "Xiangyu Xu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-05T08:42:10.367Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-04T03:40:33.000Z",
      "submittedOnDailyAt": "2025-06-05T07:13:24.079Z",
      "title": "野生의 안정적인 뉴럴 렌더링에 대해, 아스미움 듀얼 3차 가우스 스프레팅을 사용합니다.",
      "submittedOnDailyBy": {
        "_id": "634e60454677a5891c0902f4",
        "avatarUrl": "/avatars/4dc143719afe7686e05b7f2c2c5c1871.svg",
        "isPro": false,
        "fullname": "Xiangyu Xu",
        "user": "xjcvcvxj",
        "type": "user"
      },
      "summary": "3D 재구성은 불균일한 조명 조건과 순간적인 분산 요인에 의해 어려움을 겪고 있습니다. 현재의 방법들은 일반적으로 낮은 품질의 훈련 데이터 처리를 위해 휴리스틱적인 전략을 사용하지만, 이들은 안정적이고 일관된 재구성을 생성하는 것이 어려워 시각적인artifacts가 발생합니다. 본 논문에서는 이러한 artifacts의 랜덤성을 활용한 새로운 프레임워크인 Asymmetric Dual 3DGS를 제안합니다. 특히, 우리의 방법들은 두 개의 3D Gaussian Splatting(3DGS) 모델을 병렬로 훈련시키고, 신뢰성 있는 시어닝을 촉진하는 일관성 제약을 강제하고, 불확실한 artifacts를 억제합니다. 두 모델이 같은 실패 모드에 수렴하지 않도록, 우리는 다른 마스크를 적용하여 체크포인트 바이어스에 의한 파일드 모드에 의한 붕괴를 방지합니다. 구체적으로, 다중 케이스의 적용 가능한 마스크와 자동 경량 마스크를 사용하여 두 모델의 불균형한 훈련 프로세스를 구현하고, 공유 오류 모드를 줄입니다. 또한, 모델 훈련의 효율화를 위해 Dynamic EMA Proxy라는 가벼운 버전을 도입하고, 하나의 모델을 동적으로 업데이트하는 지수 이동 평균(EMA) 프로кси로 대체하고, 교환적인 마스크 프로세스를 사용하여 다양성을 유지합니다. 어려운 실세계 데이터셋에서 광범위한 실험을 통해, 우리의 방법들은 현재의 접근 방식을 초월하고 높은 효율성을 달성합니다. 코드와 훈련 모델은 공개됩니다.",
      "upvotes": 1,
      "discussionId": "68415862d777f13c59460c85",
      "ai_summary": "A novel Asymmetric Dual 3DGS framework improves 3D reconstruction by training dual models with consistency constraints and divergent masking, outperforming existing methods with high efficiency.",
      "ai_keywords": [
        "3D reconstruction",
        "3D Gaussian Splatting (3DGS)",
        "stochastic artifacts",
        "consistency constraint",
        "confirmation bias",
        "divergent masking",
        "multi-cue adaptive mask",
        "self-supervised soft mask",
        "Dynamic EMA Proxy",
        "lightweight variant",
        "Exponential Moving Average (EMA)",
        "alternating masking strategy"
      ]
    },
    "publishedAt": "2025-06-03T23:40:33.000Z",
    "title": "Robust Neural Rendering in the Wild with Asymmetric Dual 3D Gaussian\n  Splatting",
    "summary": "3D reconstruction from in-the-wild images remains a challenging task due to\ninconsistent lighting conditions and transient distractors. Existing methods\ntypically rely on heuristic strategies to handle the low-quality training data,\nwhich often struggle to produce stable and consistent reconstructions,\nfrequently resulting in visual artifacts. In this work, we propose Asymmetric\nDual 3DGS, a novel framework that leverages the stochastic nature of these\nartifacts: they tend to vary across different training runs due to minor\nrandomness. Specifically, our method trains two 3D Gaussian Splatting (3DGS)\nmodels in parallel, enforcing a consistency constraint that encourages\nconvergence on reliable scene geometry while suppressing inconsistent\nartifacts. To prevent the two models from collapsing into similar failure modes\ndue to confirmation bias, we introduce a divergent masking strategy that\napplies two complementary masks: a multi-cue adaptive mask and a\nself-supervised soft mask, which leads to an asymmetric training process of the\ntwo models, reducing shared error modes. In addition, to improve the efficiency\nof model training, we introduce a lightweight variant called Dynamic EMA Proxy,\nwhich replaces one of the two models with a dynamically updated Exponential\nMoving Average (EMA) proxy, and employs an alternating masking strategy to\npreserve divergence. Extensive experiments on challenging real-world datasets\ndemonstrate that our method consistently outperforms existing approaches while\nachieving high efficiency. Codes and trained models will be released.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03538.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "634e60454677a5891c0902f4",
      "avatarUrl": "/avatars/4dc143719afe7686e05b7f2c2c5c1871.svg",
      "fullname": "Xiangyu Xu",
      "name": "xjcvcvxj",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.02294",
      "authors": [
        {
          "_id": "6840cd169241913d43af9d28",
          "name": "Niclas Popp",
          "hidden": false
        },
        {
          "_id": "6840cd169241913d43af9d29",
          "name": "Kevin Alexander Laube",
          "hidden": false
        },
        {
          "_id": "6840cd169241913d43af9d2a",
          "name": "Matthias Hein",
          "hidden": false
        },
        {
          "_id": "6840cd169241913d43af9d2b",
          "name": "Lukas Schott",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-02T22:15:59.000Z",
      "submittedOnDailyAt": "2025-06-05T00:15:23.870Z",
      "title": "未知의 동변 이동으로 인한 지식 축소 개선을 통해 신뢰성 가이드가 된 데이터 프로세싱",
      "submittedOnDailyBy": {
        "_id": "655646baf8a2d3c020546ec8",
        "avatarUrl": "/avatars/4ca8de82745bb5a4fda511569bb6bd94.svg",
        "isPro": false,
        "fullname": "Niclas P",
        "user": "NPBP26",
        "type": "user"
      },
      "summary": "큰 데이터 세트로 훈련된 기초 모델은 여러 분야에서 강력한 0-shot 능력을 나타냅니다. 데이터 크기와 모델 크기가 제한되어 있을 때 그 성공을 재현하기 위해 지식의 흡수는 기초 모델에서 작은 학생 네트워크로 지식의 흡수를 통해 도구로 자리잡고 있습니다. 그러나 지식의 흡수의 효과는 가능한 훈련 데이터에 따라 긴장적으로 제한되어 있습니다. 본 논문은 지식의 흡수에 있어서 일반적인 유용한 문제인 변분 변환을 해결하고, 훈련 시에 나타나지만 테스트 시에는 나타나지 않는 스파이라스 특성을 지적합니다. 스파이라스 특성은 알지 못하지만, 강한 교사가 존재할 경우 학생도 그에 대해 강한 것을 할 수 있는지 문제를 조사합니다. 이를 해결하기 위해 교사와 학생의 의견 분리를 최대화하여 이미지를 생성하는 새로운 Dif-fusion 기반의 데이터 확장 전략을 도입합니다. 실험은 이 접근법은 코바이안 shift의 영향을 받지만 CelebA, SpuCo Birds, 그리고 스파이라스 ImageNet의 spurious mAUC에서 최악의 그룹과 평균 그룹의 정확도를 크게 향상시키고, 가장 先端의 Dif-fusion 기반의 데이터 확장 기반 라인어르를 초월하는 것을 보여줍니다.",
      "upvotes": 1,
      "discussionId": "6840cd199241913d43af9dac",
      "ai_summary": "A diffusion-based data augmentation strategy improves robustness in knowledge distillation by generating challenging samples, enhancing accuracy and spurious feature resilience.",
      "ai_keywords": [
        "knowledge distillation",
        "diffusion-based data augmentation",
        "covariate shift",
        "teacher-student model",
        "CelebA",
        "SpuCo Birds",
        "spurious ImageNet",
        "mean group accuracy",
        "worst group accuracy",
        "spurious mAUC"
      ]
    },
    "publishedAt": "2025-06-02T18:15:59.000Z",
    "title": "Improving Knowledge Distillation Under Unknown Covariate Shift Through\n  Confidence-Guided Data Augmentation",
    "summary": "Large foundation models trained on extensive datasets demonstrate strong\nzero-shot capabilities in various domains. To replicate their success when data\nand model size are constrained, knowledge distillation has become an\nestablished tool for transferring knowledge from foundation models to small\nstudent networks. However, the effectiveness of distillation is critically\nlimited by the available training data. This work addresses the common\npractical issue of covariate shift in knowledge distillation, where spurious\nfeatures appear during training but not at test time. We ask the question: when\nthese spurious features are unknown, yet a robust teacher is available, is it\npossible for a student to also become robust to them? We address this problem\nby introducing a novel diffusion-based data augmentation strategy that\ngenerates images by maximizing the disagreement between the teacher and the\nstudent, effectively creating challenging samples that the student struggles\nwith. Experiments demonstrate that our approach significantly improves worst\ngroup and mean group accuracy on CelebA and SpuCo Birds as well as the spurious\nmAUC on spurious ImageNet under covariate shift, outperforming state-of-the-art\ndiffusion-based data augmentation baselines",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.02294.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "655646baf8a2d3c020546ec8",
      "avatarUrl": "/avatars/4ca8de82745bb5a4fda511569bb6bd94.svg",
      "fullname": "Niclas P",
      "name": "NPBP26",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  }
]