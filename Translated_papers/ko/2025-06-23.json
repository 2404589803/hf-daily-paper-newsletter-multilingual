[
  {
    "paper": {
      "id": "2506.16406",
      "authors": [
        {
          "_id": "6858d099c0c8e29df8ea3ccb",
          "name": "Zhiyuan Liang",
          "hidden": false
        },
        {
          "_id": "6858d099c0c8e29df8ea3ccc",
          "name": "Dongwen Tang",
          "hidden": false
        },
        {
          "_id": "6858d099c0c8e29df8ea3ccd",
          "name": "Yuhao Zhou",
          "hidden": false
        },
        {
          "_id": "6858d099c0c8e29df8ea3cce",
          "name": "Xuanlei Zhao",
          "hidden": false
        },
        {
          "_id": "6858d099c0c8e29df8ea3ccf",
          "name": "Mingjia Shi",
          "hidden": false
        },
        {
          "_id": "6858d099c0c8e29df8ea3cd0",
          "name": "Wangbo Zhao",
          "hidden": false
        },
        {
          "_id": "6858d099c0c8e29df8ea3cd1",
          "name": "Zekai Li",
          "hidden": false
        },
        {
          "_id": "6858d099c0c8e29df8ea3cd2",
          "name": "Peihao Wang",
          "hidden": false
        },
        {
          "_id": "6858d099c0c8e29df8ea3cd3",
          "name": "Konstantin Schürholt",
          "hidden": false
        },
        {
          "_id": "6858d099c0c8e29df8ea3cd4",
          "name": "Damian Borth",
          "hidden": false
        },
        {
          "_id": "6858d099c0c8e29df8ea3cd5",
          "name": "Michael M. Bronstein",
          "hidden": false
        },
        {
          "_id": "6858d099c0c8e29df8ea3cd6",
          "name": "Yang You",
          "hidden": false
        },
        {
          "_id": "6858d099c0c8e29df8ea3cd7",
          "name": "Zhangyang Wang",
          "hidden": false
        },
        {
          "_id": "6858d099c0c8e29df8ea3cd8",
          "user": {
            "_id": "655452b8432af1b1116394d1",
            "avatarUrl": "/avatars/85860fb3c2d09c9c23e7677d7129cca3.svg",
            "isPro": false,
            "fullname": "Kai Wang",
            "user": "VictorKai1996NUS",
            "type": "user"
          },
          "name": "Kai Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-23T08:14:35.657Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/655452b8432af1b1116394d1/ZdZe7m_v8V6alBd1NZa-m.mp4"
      ],
      "publishedAt": "2025-06-19T15:38:21.000Z",
      "submittedOnDailyAt": "2025-06-23T02:39:37.807Z",
      "title": "Drag-and-Drop LLMs: Zero-Shot Prompt-to-Weights\n\nDrag-and-Drop LLMs: 0-Shot Prompt-to-Weights\n\n(注意：虽然要求不添加解释或额外的文本，但为了确保翻译的准确性和专业性，这里提供了一个更符合韩语表达习惯的翻译版本。)",
      "submittedOnDailyBy": {
        "_id": "655452b8432af1b1116394d1",
        "avatarUrl": "/avatars/85860fb3c2d09c9c23e7677d7129cca3.svg",
        "isPro": false,
        "fullname": "Kai Wang",
        "user": "VictorKai1996NUS",
        "type": "user"
      },
      "summary": "현대의 파라미터 효율적인 조정(PEFT) 방법의 예로, 저레ン지 아다댄티션(LoRA)는 대규모 언어 모델(LLMs)의 사용자定制 비용 감소를 위해 사용되지만, 이들은 다운 스트림 데이터 세트마다 다른 최적화 실행이 필요합니다. 우리는 Drag-and-Drop LLMs(DnD)를 소개합니다. DnD는 다수의 무 라벨 태스크 프로ン틑을 직접 LoRA의 가중치 업데이트에 매핑하는 프로ン틑 조건付き 파라미터 생성기로, 태스크별 훈련을 제외합니다. 가벼운 텍스트 인코더는 각 프로ン틑 배치를 조건 매핑 벡터로 변환하고, 이들은 연속 하이퍼 컴비네이셔널 디코더로 모든 LoRA 행렬에 변환됩니다. 다양한 프로ン틑 체크 포인트 쌍의 집합으로 훈련된 DnD는 태스크 전용 파라미터를 분발적으로 생성하며, i) 전체 파일 튜닝보다 12,000배의 오버헤드 감소, ii) 이전에 본 적이 없는 지식 추론, 수학, 코딩, 모델 다양성 벤치마크에서 평균 30%의 성능 향상을 달성, iii) 목표 데이터 또는 라벨을 본 적이 없는 경우도 강력한 크로스 데이터 영역 확장성을 나타냅니다. 우리의 결과를 통해, 프로ン틑 조건付き 파라미터 생성은 경사 기반 조정에 의한 LLMs의 신속한 특수화에 대체的手段임을 보여줍니다. 우리의 프로젝트는 https://jerryliang24.github.io/DnD{https://jerryliang24.github.io/DnD}에서 접근할 수 있습니다.",
      "upvotes": 56,
      "discussionId": "6858d099c0c8e29df8ea3cd9",
      "projectPage": "https://jerryliang24.github.io/DnD/",
      "ai_summary": "Drag-and-Drop LLMs generate task-specific parameters through prompt-conditioned parameter generation, achieving significant efficiency gains and cross-domain generalization without per-task training.",
      "ai_keywords": [
        "Parameter-Efficient Fine-Tuning",
        "PEFT",
        "low-rank adaptation",
        "LoRA",
        "large language models",
        "prompts",
        "condition embeddings",
        "hyper-convolutional decoder",
        "LoRA matrices",
        "common-sense reasoning",
        "math",
        "coding",
        "multimodal benchmarks"
      ]
    },
    "publishedAt": "2025-06-19T11:38:21.000Z",
    "title": "Drag-and-Drop LLMs: Zero-Shot Prompt-to-Weights",
    "summary": "Modern Parameter-Efficient Fine-Tuning (PEFT) methods such as low-rank\nadaptation (LoRA) reduce the cost of customizing large language models (LLMs),\nyet still require a separate optimization run for every downstream dataset. We\nintroduce Drag-and-Drop LLMs (\\textit{DnD)}, a prompt-conditioned\nparameter generator that eliminates per-task training by mapping a handful of\nunlabeled task prompts directly to LoRA weight updates. A lightweight text\nencoder distills each prompt batch into condition embeddings, which are then\ntransformed by a cascaded hyper-convolutional decoder into the full set of LoRA\nmatrices. Once trained in a diverse collection of prompt-checkpoint pairs, DnD\nproduces task-specific parameters in seconds, yielding i) up to\n12,000times lower overhead than full fine-tuning, ii) average gains\nup to 30\\% in performance over the strongest training LoRAs on unseen\ncommon-sense reasoning, math, coding, and multimodal benchmarks, and iii)\nrobust cross-domain generalization despite never seeing the target data or\nlabels. Our results demonstrate that prompt-conditioned parameter generation is\na viable alternative to gradient-based adaptation for rapidly specializing\nLLMs. Our project is available at\nhttps://jerryliang24.github.io/DnD{https://jerryliang24.github.io/DnD}.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/655452b8432af1b1116394d1/ZdZe7m_v8V6alBd1NZa-m.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.16406.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "655452b8432af1b1116394d1",
      "avatarUrl": "/avatars/85860fb3c2d09c9c23e7677d7129cca3.svg",
      "fullname": "Kai Wang",
      "name": "VictorKai1996NUS",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.16054",
      "authors": [
        {
          "_id": "6858e225c0c8e29df8ea3d0f",
          "user": {
            "_id": "6454568636821f6860fed410",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6454568636821f6860fed410/VsKn0GawOBZo1hmqHnmV1.png",
            "isPro": false,
            "fullname": "Tianchen Zhao",
            "user": "A-suozhang",
            "type": "user"
          },
          "name": "Tianchen Zhao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-23T08:14:13.425Z",
          "hidden": false
        },
        {
          "_id": "6858e225c0c8e29df8ea3d10",
          "name": "Ke Hong",
          "hidden": false
        },
        {
          "_id": "6858e225c0c8e29df8ea3d11",
          "name": "Xinhao Yang",
          "hidden": false
        },
        {
          "_id": "6858e225c0c8e29df8ea3d12",
          "name": "Xuefeng Xiao",
          "hidden": false
        },
        {
          "_id": "6858e225c0c8e29df8ea3d13",
          "name": "Huixia Li",
          "hidden": false
        },
        {
          "_id": "6858e225c0c8e29df8ea3d14",
          "name": "Feng Ling",
          "hidden": false
        },
        {
          "_id": "6858e225c0c8e29df8ea3d15",
          "name": "Ruiqi Xie",
          "hidden": false
        },
        {
          "_id": "6858e225c0c8e29df8ea3d16",
          "name": "Siqi Chen",
          "hidden": false
        },
        {
          "_id": "6858e225c0c8e29df8ea3d17",
          "name": "Hongyu Zhu",
          "hidden": false
        },
        {
          "_id": "6858e225c0c8e29df8ea3d18",
          "name": "Yichong Zhang",
          "hidden": false
        },
        {
          "_id": "6858e225c0c8e29df8ea3d19",
          "name": "Yu Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-19T06:25:02.000Z",
      "submittedOnDailyAt": "2025-06-23T03:52:52.372Z",
      "title": "파로안테션: 패턴 학습을 위한 효율적인 스패르스 및 쉘티드 어텐션을 구현하는 방법",
      "submittedOnDailyBy": {
        "_id": "6454568636821f6860fed410",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6454568636821f6860fed410/VsKn0GawOBZo1hmqHnmV1.png",
        "isPro": false,
        "fullname": "Tianchen Zhao",
        "user": "A-suozhang",
        "type": "user"
      },
      "summary": "비주얼 생성에서, 어텐션 구조의 2차원 복잡성은 메모리와 계산 비용의 증가를 초래하며, 고해상도 이미지 및 다 프레임 비디오 생성에 필요한 긴 토큰 시퀀스를 특히 엄격하게 요구합니다. 이에 대처하여, 기존 연구에서는 스패르시 피션과 큐언티 피션 등의 방법들을 시도하고 있지만, 낮은 밀도와 줄인 비트 폭의 상황에서는 큰 문제를 겪고 있습니다. 시스템적인 분석을 통해, 우리는 어텐션 패턴의 분산 및 불규칙한 특성이 본질적인 어려움으로 인정했습니다. 따라서, 이러한 패턴을 만족하는 특수화된 스패르시 피션이나 큐언티 피션의 설계를 도입하는 것보다, 어텐션 패턴을 재구성하는 전략을 제안하고 있습니다. 비주얼 특징 추출의 지역적 집중성으로 영감을 받아, 새로운 **패턴 관점의 토큰 재 정렬(PARO)** 기술을 설계하고, 다양한 어텐션 패턴을 하드웨어에 가까운 블록 위스어지 패턴으로 통합합니다. 이 통합은 스패르시 피션과 큐언티 피션을 크게 단순화하고 강화합니다. 각 설계 선택의 성능·효율성을 평가하고, 통일적인 패턴에 맞는 메소드를 최종적으로 결정합니다. 우리의 접근법, **PAROAttention**,는 무손실 메트릭에서 비디오 및 이미지 생성을 실현하며, FP 기반 라인에서 근似的한 결과를 얻을 수 있으며, 明顯하게 낮은 밀도(약 20%-30%)와 비트 폭(INT8/INT4)에서 작동하며, 단말에서의 라테니 스피드 업이 1.9x에서 2.7x까지 달성할 수 있습니다.",
      "upvotes": 35,
      "discussionId": "6858e225c0c8e29df8ea3d1a",
      "projectPage": "https://a-suozhang.xyz/paroattn.github.io/",
      "ai_summary": "PAROAttention reorganizes visual attention patterns to enable efficient sparsification and quantization, reducing memory and computational costs with minimal impact on performance.",
      "ai_keywords": [
        "attention mechanisms",
        "sparsification",
        "quantization",
        "visual attention patterns",
        "Pattern-Aware token ReOrdering (PARO)",
        "local aggregation",
        "hardware-friendly block-wise pattern",
        "end-to-end latency speedup",
        "INT8/INT4",
        "PAROAttention"
      ]
    },
    "publishedAt": "2025-06-19T02:25:02.000Z",
    "title": "PAROAttention: Pattern-Aware ReOrdering for Efficient Sparse and\n  Quantized Attention in Visual Generation Models",
    "summary": "In visual generation, the quadratic complexity of attention mechanisms\nresults in high memory and computational costs, especially for longer token\nsequences required in high-resolution image or multi-frame video generation. To\naddress this, prior research has explored techniques such as sparsification and\nquantization. However, these techniques face significant challenges under low\ndensity and reduced bitwidths. Through systematic analysis, we identify that\nthe core difficulty stems from the dispersed and irregular characteristics of\nvisual attention patterns. Therefore, instead of introducing specialized\nsparsification and quantization design to accommodate such patterns, we propose\nan alternative strategy: *reorganizing* the attention pattern to alleviate the\nchallenges. Inspired by the local aggregation nature of visual feature\nextraction, we design a novel **Pattern-Aware token ReOrdering (PARO)**\ntechnique, which unifies the diverse attention patterns into a\nhardware-friendly block-wise pattern. This unification substantially simplifies\nand enhances both sparsification and quantization. We evaluate the\nperformance-efficiency trade-offs of various design choices and finalize a\nmethodology tailored for the unified pattern. Our approach, **PAROAttention**,\nachieves video and image generation with lossless metrics, and nearly identical\nresults from full-precision (FP) baselines, while operating at notably lower\ndensity (~20%-30%) and bitwidth (**INT8/INT4**), achieving a **1.9x** to\n**2.7x** end-to-end latency speedup.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.16054.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6454568636821f6860fed410",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6454568636821f6860fed410/VsKn0GawOBZo1hmqHnmV1.png",
      "fullname": "Tianchen Zhao",
      "name": "A-suozhang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.16035",
      "authors": [
        {
          "_id": "6858d76cc0c8e29df8ea3cdb",
          "user": {
            "_id": "638828121901766b88076aa1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638828121901766b88076aa1/rXlOO7eewmmaSN_hQIVz7.jpeg",
            "isPro": false,
            "fullname": "Vishesh Tripathi",
            "user": "vishesh-t27",
            "type": "user"
          },
          "name": "Vishesh Tripathi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-23T08:14:30.878Z",
          "hidden": false
        },
        {
          "_id": "6858d76cc0c8e29df8ea3cdc",
          "name": "Tanmay Odapally",
          "hidden": false
        },
        {
          "_id": "6858d76cc0c8e29df8ea3cdd",
          "name": "Indraneel Das",
          "hidden": false
        },
        {
          "_id": "6858d76cc0c8e29df8ea3cde",
          "user": {
            "_id": "64103f66928400b4164308f0",
            "avatarUrl": "/avatars/6799d4a365776f83cecf7b9f468f3d4f.svg",
            "isPro": false,
            "fullname": "uday allu",
            "user": "udayallu",
            "type": "user"
          },
          "name": "Uday Allu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-23T08:14:33.310Z",
          "hidden": false
        },
        {
          "_id": "6858d76cc0c8e29df8ea3cdf",
          "name": "Biddwan Ahmed",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-19T05:11:43.000Z",
      "submittedOnDailyAt": "2025-06-23T03:13:41.130Z",
      "title": "비젼 가이드 드래그는 모두입니다: 다 모델 문서 이해를 이용한 RAG의 확장",
      "submittedOnDailyBy": {
        "_id": "638828121901766b88076aa1",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638828121901766b88076aa1/rXlOO7eewmmaSN_hQIVz7.jpeg",
        "isPro": false,
        "fullname": "Vishesh Tripathi",
        "user": "vishesh-t27",
        "type": "user"
      },
      "summary": "レタイブレーディングアウゲンザション(RAG) 시스템은 정보 검색 및 질문에 대한 답변에 혁신적인 영향을 미치고 있으며, 전통적인 텍스트 기반의 블록화 방법은 복잡한 문서 구조, 다 페이지 테이블, 삽입된 시각적 요소, 페이지 경계를 초월하는 컨텍스트 의존 관계에 대해 어려움을 겪고 있습니다. 우리는 규모가 큰 다모달 모델(LMMs)을 활용한 새로운 다모달 문서 블록 접근 방식을 제안하고 있으며, PDF 문서들을 배치 처리하면서 의미적 일관성과 구조적 정비성을 유지하는 것을 목표로 합니다. 우리의 방법은 구축 가능한 페이지 배치로 문서들을 처리하고 배치 간 컨텍스트 보존을 실현하며, 다 페이지 테이블, 삽입된 시각적 요소, 프로세스 내용을 정확한 처리할 수 있게 합니다. 우리의 접근 방식은 자동 생성된 검색어에 포함된 칼레이러 데이터 세트에서 평가되었으며, 블록 품질과 RAG 시스템의 하류 성능에 대한 향상을 보여주고 있습니다. 우리의 시각적 가이드가 지휘하는 접근 방식은 전통적인 밴들 RAG 시스템에 비해 더 높은 정확도를 달성하며, 질적 분석에서 문서 구조와 의미적 일관성의 더 큰 보존을 나타냅니다.",
      "upvotes": 33,
      "discussionId": "6858d76cc0c8e29df8ea3ce0",
      "ai_summary": "A novel multimodal document chunking approach using Large Multimodal Models (LMMs) enhances RAG performance by accurately processing complex PDF documents, including multi-page tables and embedded visuals.",
      "ai_keywords": [
        "Retrieval-Augmented Generation (RAG)",
        "Large Multimodal Models (LMMs)",
        "document chunking",
        "semantic coherence",
        "structural integrity",
        "cross-batch context preservation",
        "vision-guided approach"
      ]
    },
    "publishedAt": "2025-06-19T01:11:43.000Z",
    "title": "Vision-Guided Chunking Is All You Need: Enhancing RAG with Multimodal\n  Document Understanding",
    "summary": "Retrieval-Augmented Generation (RAG) systems have revolutionized information\nretrieval and question answering, but traditional text-based chunking methods\nstruggle with complex document structures, multi-page tables, embedded figures,\nand contextual dependencies across page boundaries. We present a novel\nmultimodal document chunking approach that leverages Large Multimodal Models\n(LMMs) to process PDF documents in batches while maintaining semantic coherence\nand structural integrity. Our method processes documents in configurable page\nbatches with cross-batch context preservation, enabling accurate handling of\ntables spanning multiple pages, embedded visual elements, and procedural\ncontent. We evaluate our approach on a curated dataset of PDF documents with\nmanually crafted queries, demonstrating improvements in chunk quality and\ndownstream RAG performance. Our vision-guided approach achieves better accuracy\ncompared to traditional vanilla RAG systems, with qualitative analysis showing\nsuperior preservation of document structure and semantic coherence.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.16035.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "638828121901766b88076aa1",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638828121901766b88076aa1/rXlOO7eewmmaSN_hQIVz7.jpeg",
      "fullname": "Vishesh Tripathi",
      "name": "vishesh-t27",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.09049",
      "authors": [
        {
          "_id": "6858c341c0c8e29df8ea3c7f",
          "user": {
            "_id": "64eadcb03d76028d805a7818",
            "avatarUrl": "/avatars/528e4fded4419caf08589b2ed40437bc.svg",
            "isPro": false,
            "fullname": "Li Kang",
            "user": "FACEONG",
            "type": "user"
          },
          "name": "Li Kang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-23T08:14:50.165Z",
          "hidden": false
        },
        {
          "_id": "6858c341c0c8e29df8ea3c80",
          "name": "Xiufeng Song",
          "hidden": false
        },
        {
          "_id": "6858c341c0c8e29df8ea3c81",
          "name": "Heng Zhou",
          "hidden": false
        },
        {
          "_id": "6858c341c0c8e29df8ea3c82",
          "name": "Yiran Qin",
          "hidden": false
        },
        {
          "_id": "6858c341c0c8e29df8ea3c83",
          "name": "Jie Yang",
          "hidden": false
        },
        {
          "_id": "6858c341c0c8e29df8ea3c84",
          "name": "Xiaohong Liu",
          "hidden": false
        },
        {
          "_id": "6858c341c0c8e29df8ea3c85",
          "name": "Philip Torr",
          "hidden": false
        },
        {
          "_id": "6858c341c0c8e29df8ea3c86",
          "name": "Lei Bai",
          "hidden": false
        },
        {
          "_id": "6858c341c0c8e29df8ea3c87",
          "name": "Zhenfei Yin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-10T17:59:44.000Z",
      "submittedOnDailyAt": "2025-06-23T01:31:50.738Z",
      "title": "VIKI-R: 강화학습에 의한 체화 다중 아그리언의 협조협력",
      "submittedOnDailyBy": {
        "_id": "64eadcb03d76028d805a7818",
        "avatarUrl": "/avatars/528e4fded4419caf08589b2ed40437bc.svg",
        "isPro": false,
        "fullname": "Li Kang",
        "user": "FACEONG",
        "type": "user"
      },
      "summary": "다중체 실험체의 협조는 동적인 환경에서 인공지능의 핵심적인 문제로, 시각적 추론과 scalable한 협업 전략이 필요합니다. 최근의 연구에서는 다중체 계획에 LLMs(Language Large Models)를 사용하지만, 시각 언어 모델(VLMs)을 시각적 추론에 활용하는 것이 시작되었습니다. 그러나 VLM 기반의 접근 방식은 다양한 실험체 유형의 지원을 한계로 제한되어 있습니다. 본 연구에서는 VIKI-Bench를 소개합니다. VIKI-Bench는 실험체 다중체 협조에 적합한 첫 번째 계층적인 벤치마크이며, 기계체 활성화, 태스크 계획, 프로젝트 시각 인식의 3가지 구조화된 수준을 특징으로 합니다. VIKI-Bench는 다양한 로봇 실험체, 다각의 시각 관측, 구조화된 서브 제컷 신호를 포함하며, 시각 입력에 기반한 추론을 평가합니다. VIKI-R라는 2단계 프레임워크를 제안하고, 연속적인 OBS(Observation) 설명을 포함하여 학습된 시각 언어 모델(VLM)을 최적화하고, 다단계 보상 신호 하에서 강화 학습을 수행합니다. 확장된 실험에 따라, VIKI-R은 모든 태스크 수준에서 기준 방법보다 유의적으로 우수합니다. 또한, 강화 학습에 의해, 서로 다른 기계체 간의 복합적인 협업 패턴의 출현 방식이 나타납니다. VIKI-Bench와 VIKI-R은 실험체 AI 시스템에서 다중체, 시각적 도리빈의 협조에 대한 통일된 테스트 벤치와 방법을 제공합니다.",
      "upvotes": 24,
      "discussionId": "6858c341c0c8e29df8ea3c88",
      "projectPage": "https://faceong.github.io/VIKI-R/",
      "githubRepo": "https://github.com/MARS-EAI/VIKI-R",
      "ai_summary": "VIKI-Bench and VIKI-R provide a benchmark and framework for evaluating and improving visual-driven cooperation among diverse embodied agents using vision-language models and reinforcement learning.",
      "ai_keywords": [
        "embodied agents",
        "VIKI-Bench",
        "multi-agent cooperation",
        "vision-language models",
        "VIKI-R",
        "Chain-of-Thought",
        "reinforcement learning",
        "compositional cooperation",
        "multi-level reward signals",
        "robot embodiments",
        "multi-view visual observations",
        "structured supervision signals"
      ]
    },
    "publishedAt": "2025-06-10T13:59:44.000Z",
    "title": "VIKI-R: Coordinating Embodied Multi-Agent Cooperation via Reinforcement\n  Learning",
    "summary": "Coordinating multiple embodied agents in dynamic environments remains a core\nchallenge in artificial intelligence, requiring both perception-driven\nreasoning and scalable cooperation strategies. While recent works have\nleveraged large language models (LLMs) for multi-agent planning, a few have\nbegun to explore vision-language models (VLMs) for visual reasoning. However,\nthese VLM-based approaches remain limited in their support for diverse\nembodiment types. In this work, we introduce VIKI-Bench, the first hierarchical\nbenchmark tailored for embodied multi-agent cooperation, featuring three\nstructured levels: agent activation, task planning, and trajectory perception.\nVIKI-Bench includes diverse robot embodiments, multi-view visual observations,\nand structured supervision signals to evaluate reasoning grounded in visual\ninputs. To demonstrate the utility of VIKI-Bench, we propose VIKI-R, a\ntwo-stage framework that fine-tunes a pretrained vision-language model (VLM)\nusing Chain-of-Thought annotated demonstrations, followed by reinforcement\nlearning under multi-level reward signals. Our extensive experiments show that\nVIKI-R significantly outperforms baselines method across all task levels.\nFurthermore, we show that reinforcement learning enables the emergence of\ncompositional cooperation patterns among heterogeneous agents. Together,\nVIKI-Bench and VIKI-R offer a unified testbed and method for advancing\nmulti-agent, visual-driven cooperation in embodied AI systems.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09049.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64eadcb03d76028d805a7818",
      "avatarUrl": "/avatars/528e4fded4419caf08589b2ed40437bc.svg",
      "fullname": "Li Kang",
      "name": "FACEONG",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.17206",
      "authors": [
        {
          "_id": "6858c5b5c0c8e29df8ea3c95",
          "name": "Yukun Huang",
          "hidden": false
        },
        {
          "_id": "6858c5b5c0c8e29df8ea3c96",
          "name": "Yanning Zhou",
          "hidden": false
        },
        {
          "_id": "6858c5b5c0c8e29df8ea3c97",
          "name": "Jianan Wang",
          "hidden": false
        },
        {
          "_id": "6858c5b5c0c8e29df8ea3c98",
          "name": "Kaiyi Huang",
          "hidden": false
        },
        {
          "_id": "6858c5b5c0c8e29df8ea3c99",
          "name": "Xihui Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-20T17:55:06.000Z",
      "submittedOnDailyAt": "2025-06-23T01:41:47.575Z",
      "title": "DreamCube: 3D 평면생성에 의한 다면체 동시화\n\n(请注意，虽然翻译保持了专业性和准确性，但\"동시화\"一词在技术上可能需要根据具体应用场景进行调整，例如\"동시 시각화\"或\"동시 표현\"，以确保最准确的含义。)",
      "submittedOnDailyBy": {
        "_id": "638ee900ee7e45e0474a5712",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638ee900ee7e45e0474a5712/KLli_eCbWwffKR7oLDmV3.jpeg",
        "isPro": false,
        "fullname": "Yukun Huang",
        "user": "KevinHuang",
        "type": "user"
      },
      "summary": "3D 패노라마 합성은 고품질의 다양한 시각성과 기오메트리를 요구하는 바람직한 일이지만, 어려운 과제로서 쫒아내기가 어렵습니다. 현재의 방법들은 3D 패노라마 데이터의 부족을 피하기 위해 사전 학습된 2D 기반 모델에서 풍부한 이미지 프로이슴을 활용하고 있지만, 3D 패노라마와 2D 단일 뷰의 불적합성은 그 효과성을 제한하고 있습니다. 본 논문에서는 2D 기반 모델에서 연산자를 적용하여 다면체 동기화를 적용함으로써 그 능력을 360도 영역에 무한히 확장할 수 있음을 보여줍니다. 이 설계에 기초하여, 또 DreamCube라는 3D 패노라마 생성용 다면체 RGB-D 분산 모델을 소개합니다. 이 모델은 2D 기반 모델의 프로이슴을 최대한 재활용하고 다양한 외관과 정확한 기오메트리를 달성하면서, 다중 방식의 일치성을 유지하는 것을 목표로 합니다. 광범위한 실험은 패노라마 이미지 생성, 패노라마 데풀 시미션, 3D 시나리오 생성에서 우리 접근법의 효과성을 보여주고 있습니다.",
      "upvotes": 13,
      "discussionId": "6858c5b6c0c8e29df8ea3c9a",
      "projectPage": "https://yukun-huang.github.io/DreamCube/",
      "githubRepo": "https://github.com/yukun-huang/DreamCube",
      "ai_summary": "Multi-plane synchronization extends 2D foundation models to 3D panorama generation, introducing DreamCube to achieve diverse appearances and accurate geometry.",
      "ai_keywords": [
        "multi-plane synchronization",
        "2D foundation models",
        "DreamCube",
        "RGB-D diffusion model",
        "panoramic image generation",
        "panoramic depth estimation",
        "3D scene generation"
      ]
    },
    "publishedAt": "2025-06-20T13:55:06.000Z",
    "title": "DreamCube: 3D Panorama Generation via Multi-plane Synchronization",
    "summary": "3D panorama synthesis is a promising yet challenging task that demands\nhigh-quality and diverse visual appearance and geometry of the generated\nomnidirectional content. Existing methods leverage rich image priors from\npre-trained 2D foundation models to circumvent the scarcity of 3D panoramic\ndata, but the incompatibility between 3D panoramas and 2D single views limits\ntheir effectiveness. In this work, we demonstrate that by applying multi-plane\nsynchronization to the operators from 2D foundation models, their capabilities\ncan be seamlessly extended to the omnidirectional domain. Based on this design,\nwe further introduce DreamCube, a multi-plane RGB-D diffusion model for 3D\npanorama generation, which maximizes the reuse of 2D foundation model priors to\nachieve diverse appearances and accurate geometry while maintaining multi-view\nconsistency. Extensive experiments demonstrate the effectiveness of our\napproach in panoramic image generation, panoramic depth estimation, and 3D\nscene generation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.17206.png",
    "numComments": 4,
    "submittedBy": {
      "_id": "638ee900ee7e45e0474a5712",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638ee900ee7e45e0474a5712/KLli_eCbWwffKR7oLDmV3.jpeg",
      "fullname": "Yukun Huang",
      "name": "KevinHuang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.17201",
      "authors": [
        {
          "_id": "6858c46fc0c8e29df8ea3c8a",
          "name": "Jiaqi Li",
          "hidden": false
        },
        {
          "_id": "6858c46fc0c8e29df8ea3c8b",
          "name": "Junshu Tang",
          "hidden": false
        },
        {
          "_id": "6858c46fc0c8e29df8ea3c8c",
          "name": "Zhiyong Xu",
          "hidden": false
        },
        {
          "_id": "6858c46fc0c8e29df8ea3c8d",
          "name": "Longhuang Wu",
          "hidden": false
        },
        {
          "_id": "6858c46fc0c8e29df8ea3c8e",
          "name": "Yuan Zhou",
          "hidden": false
        },
        {
          "_id": "6858c46fc0c8e29df8ea3c8f",
          "name": "Shuai Shao",
          "hidden": false
        },
        {
          "_id": "6858c46fc0c8e29df8ea3c90",
          "name": "Tianbao Yu",
          "hidden": false
        },
        {
          "_id": "6858c46fc0c8e29df8ea3c91",
          "name": "Zhiguo Cao",
          "hidden": false
        },
        {
          "_id": "6858c46fc0c8e29df8ea3c92",
          "name": "Qinglin Lu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-20T17:50:37.000Z",
      "submittedOnDailyAt": "2025-06-23T01:35:50.876Z",
      "title": "폼 게임 클래프트: 하이다이나믹 인터랙티브 게임 비디오 생성에 대해 하이버드 히스토리 조건을 사용합니다.",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": true,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "최근의 확산 기반 및 제어 가능한 비디오 생성 기술은 고품질 및 시간적 일관성을 가진 비디오 합성에 성공하여, 만족스러운 인터랙티브 게임을 구축한 바 있다. 그러나 현재의 방법들은 동작, 일반성, 장기적 일관성과 효율성에 제한이 있으며, 다양한 게임 플레이 비디오의 제작 능력을 제한하고 있다. 이러한 단점을 해결하기 위해, 우리는 게임 환경에서 고동작성 인터랙티브 비디오 생성의 새로운 프레임워크인 \"Contribute Game Craft\"를 소개한다. 동작 제어를 상세히 구현하기 위해, 표준 키보드와 마우스 입력을 공유하여 카메라 표현 공간에 통합하고, 다양한 카메라 및 이동 조작 사이의 평활한 인터럽션을 촉진한다. 다음으로, 비디오 시퀀스를 자동 회귀적으로 확장하면서 게임 시야 정보를 저장하기 위한 혼합 히스토리 조건부 훈련 전략을 제안한다. 또한, 추론 효율성과 재미를 향상시키기 위해, 모델을 변환하여 장기적인 시간 시퀀스 일관성을 유지하면서 계산 오버헤드를 줄이고, 복잡한 인터랙티브 환경에서 실시간 데플로yment에 적합한 모델을 구축한다. 모델은 100여 개 AAA 게임의 게임 플레이 기록 100만 개를 포함하는 큰 데이터 세트에서 훈련되어, 광범위한 다양한 데이터 세트를 확보하고, 더 조정된 합성 데이터 세트를 사용하여 정밀성과 제어성을 향상시킨다. 편집된 게임 시야 데이터는 시각적 질의, 현실성과 동작 제어 가능도를 크게 향상시킨다. 확장된 실험은 \"Contribute Game Craft\"는 기존 모델을 크게 초월하고, 인터랙티브 비디오 생성의 현실성과 재미를 향상시키는 것을 보여주었다.",
      "upvotes": 11,
      "discussionId": "6858c46fc0c8e29df8ea3c93",
      "ai_summary": "Hunyuan-GameCraft is a novel framework for high-dynamic interactive video generation in game environments that addresses limitations in dynamics, generality, and efficiency through unified input representation, hybrid history-conditioned training, and model distillation.",
      "ai_keywords": [
        "diffusion-based",
        "controllable video generation",
        "temporally coherent video synthesis",
        "high-dynamic interactive video generation",
        "shared camera representation space",
        "hybrid history-conditioned training strategy",
        "model distillation",
        "real-time deployment",
        "large-scale dataset",
        "synthetic dataset",
        "game scene data"
      ]
    },
    "publishedAt": "2025-06-20T13:50:37.000Z",
    "title": "Hunyuan-GameCraft: High-dynamic Interactive Game Video Generation with\n  Hybrid History Condition",
    "summary": "Recent advances in diffusion-based and controllable video generation have\nenabled high-quality and temporally coherent video synthesis, laying the\ngroundwork for immersive interactive gaming experiences. However, current\nmethods face limitations in dynamics, generality, long-term consistency, and\nefficiency, which limit the ability to create various gameplay videos. To\naddress these gaps, we introduce Hunyuan-GameCraft, a novel framework for\nhigh-dynamic interactive video generation in game environments. To achieve\nfine-grained action control, we unify standard keyboard and mouse inputs into a\nshared camera representation space, facilitating smooth interpolation between\nvarious camera and movement operations. Then we propose a hybrid\nhistory-conditioned training strategy that extends video sequences\nautoregressively while preserving game scene information. Additionally, to\nenhance inference efficiency and playability, we achieve model distillation to\nreduce computational overhead while maintaining consistency across long\ntemporal sequences, making it suitable for real-time deployment in complex\ninteractive environments. The model is trained on a large-scale dataset\ncomprising over one million gameplay recordings across over 100 AAA games,\nensuring broad coverage and diversity, then fine-tuned on a carefully annotated\nsynthetic dataset to enhance precision and control. The curated game scene data\nsignificantly improves the visual fidelity, realism and action controllability.\nExtensive experiments demonstrate that Hunyuan-GameCraft significantly\noutperforms existing models, advancing the realism and playability of\ninteractive game video generation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.17201.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": true,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7170
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.16504",
      "authors": [
        {
          "_id": "6858c1fcc0c8e29df8ea3c63",
          "name": "Zeqiang Lai",
          "hidden": false
        },
        {
          "_id": "6858c1fcc0c8e29df8ea3c64",
          "name": "Yunfei Zhao",
          "hidden": false
        },
        {
          "_id": "6858c1fcc0c8e29df8ea3c65",
          "name": "Haolin Liu",
          "hidden": false
        },
        {
          "_id": "6858c1fcc0c8e29df8ea3c66",
          "name": "Zibo Zhao",
          "hidden": false
        },
        {
          "_id": "6858c1fcc0c8e29df8ea3c67",
          "name": "Qingxiang Lin",
          "hidden": false
        },
        {
          "_id": "6858c1fcc0c8e29df8ea3c68",
          "name": "Huiwen Shi",
          "hidden": false
        },
        {
          "_id": "6858c1fcc0c8e29df8ea3c69",
          "name": "Xianghui Yang",
          "hidden": false
        },
        {
          "_id": "6858c1fcc0c8e29df8ea3c6a",
          "name": "Mingxin Yang",
          "hidden": false
        },
        {
          "_id": "6858c1fcc0c8e29df8ea3c6b",
          "name": "Shuhui Yang",
          "hidden": false
        },
        {
          "_id": "6858c1fcc0c8e29df8ea3c6c",
          "name": "Yifei Feng",
          "hidden": false
        },
        {
          "_id": "6858c1fcc0c8e29df8ea3c6d",
          "name": "Sheng Zhang",
          "hidden": false
        },
        {
          "_id": "6858c1fcc0c8e29df8ea3c6e",
          "name": "Xin Huang",
          "hidden": false
        },
        {
          "_id": "6858c1fcc0c8e29df8ea3c6f",
          "name": "Di Luo",
          "hidden": false
        },
        {
          "_id": "6858c1fcc0c8e29df8ea3c70",
          "name": "Fan Yang",
          "hidden": false
        },
        {
          "_id": "6858c1fcc0c8e29df8ea3c71",
          "name": "Fang Yang",
          "hidden": false
        },
        {
          "_id": "6858c1fcc0c8e29df8ea3c72",
          "name": "Lifu Wang",
          "hidden": false
        },
        {
          "_id": "6858c1fcc0c8e29df8ea3c73",
          "name": "Sicong Liu",
          "hidden": false
        },
        {
          "_id": "6858c1fcc0c8e29df8ea3c74",
          "name": "Yixuan Tang",
          "hidden": false
        },
        {
          "_id": "6858c1fcc0c8e29df8ea3c75",
          "name": "Yulin Cai",
          "hidden": false
        },
        {
          "_id": "6858c1fcc0c8e29df8ea3c76",
          "name": "Zebin He",
          "hidden": false
        },
        {
          "_id": "6858c1fcc0c8e29df8ea3c77",
          "name": "Tian Liu",
          "hidden": false
        },
        {
          "_id": "6858c1fcc0c8e29df8ea3c78",
          "name": "Yuhong Liu",
          "hidden": false
        },
        {
          "_id": "6858c1fcc0c8e29df8ea3c79",
          "name": "Jie Jiang",
          "hidden": false
        },
        {
          "_id": "6858c1fcc0c8e29df8ea3c7a",
          "name": "Linus",
          "hidden": false
        },
        {
          "_id": "6858c1fcc0c8e29df8ea3c7b",
          "name": "Jingwei Huang",
          "hidden": false
        },
        {
          "_id": "6858c1fcc0c8e29df8ea3c7c",
          "name": "Chunchao Guo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-19T17:57:40.000Z",
      "submittedOnDailyAt": "2025-06-23T01:25:30.602Z",
      "title": "최종 디테일을 위한 고품질 3D 자산 생성을 위해, 핸드 젯 3D 2.5",
      "submittedOnDailyBy": {
        "_id": "63044b89eedc089484c995ad",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/T4mOIQLaQdM5_oviaw_Cp.png",
        "isPro": false,
        "fullname": "Zeqiang Lai",
        "user": "ZeqiangLai",
        "type": "user"
      },
      "summary": "이 보고서에서는 강력한 3D 디퓨전 모델 시트 Hunyuan3D 2.5를 소개합니다. 이 모델들은 고품질의 상세한 3D 자산을 생성하는 것을 목표로 합니다. Hunyuan3D 2.5는 이전 버전 Hunyuan3D 2.0와 같은 2 단계 파이프라인을 유지하면서, 모양과 테크스처의 생성에서 큰 진전을 보여주고 있습니다. 모양 생성에서 새로운 모양 fundament 모델 LATTICE를 도입하고, 스케일된 고품질 데이터 세트, 모델 크기, 컴퓨팅을 사용하여 훈련되었습니다. 우리 최대 모델은 10B 파라미터를 달성하며, 이미지-3D의 적절한 추적을 유지하면서, 쎄프하고 상세한 3D 모양을 생성하고, 메쉬 표면이 깨끗하고 滑라가 잦아, 생성된 3D 모양과手工艺 3D 모양 사이의 차이를 크게 좁혔습니다. 테크스처 생성에서 Hunyuan3D 2.0의 Paint 모델로부터 확장된 새로운 다각 아키텍처를 기반으로 물리 기반 렌더링(PBR)을 도입하고, 우리 확장 평가에 따라 Hunyuan3D 2.5는 모양과 끝에서 끝까지의 테크스처 생성에서 이전 방법보다 뚜렷하게 뛰어납니다.",
      "upvotes": 9,
      "discussionId": "6858c1fdc0c8e29df8ea3c7d",
      "ai_summary": "Hunyuan3D 2.5, a suite of 3D diffusion models, advances shape and texture generation with a new LATTICE model and physical-based rendering in a multi-view architecture.",
      "ai_keywords": [
        "3D diffusion models",
        "LATTICE",
        "scaled high-quality datasets",
        "model-size",
        "compute",
        "parameters",
        "sharp and detailed 3D shape",
        "mesh surface",
        "precise image-3D",
        "physical-based rendering",
        "multi-view architecture",
        "end-to-end texture generation"
      ]
    },
    "publishedAt": "2025-06-19T13:57:40.000Z",
    "title": "Hunyuan3D 2.5: Towards High-Fidelity 3D Assets Generation with Ultimate\n  Details",
    "summary": "In this report, we present Hunyuan3D 2.5, a robust suite of 3D diffusion\nmodels aimed at generating high-fidelity and detailed textured 3D assets.\nHunyuan3D 2.5 follows two-stages pipeline of its previous version Hunyuan3D\n2.0, while demonstrating substantial advancements in both shape and texture\ngeneration. In terms of shape generation, we introduce a new shape foundation\nmodel -- LATTICE, which is trained with scaled high-quality datasets,\nmodel-size, and compute. Our largest model reaches 10B parameters and generates\nsharp and detailed 3D shape with precise image-3D following while keeping mesh\nsurface clean and smooth, significantly closing the gap between generated and\nhandcrafted 3D shapes. In terms of texture generation, it is upgraded with\nphyiscal-based rendering (PBR) via a novel multi-view architecture extended\nfrom Hunyuan3D 2.0 Paint model. Our extensive evaluation shows that Hunyuan3D\n2.5 significantly outperforms previous methods in both shape and end-to-end\ntexture generation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.16504.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63044b89eedc089484c995ad",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/T4mOIQLaQdM5_oviaw_Cp.png",
      "fullname": "Zeqiang Lai",
      "name": "ZeqiangLai",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.15745",
      "authors": [
        {
          "_id": "685911a30e4ad7e2197582f3",
          "name": "Minsoo Kim",
          "hidden": false
        },
        {
          "_id": "685911a30e4ad7e2197582f4",
          "name": "Kyuhong Shim",
          "hidden": false
        },
        {
          "_id": "685911a30e4ad7e2197582f5",
          "name": "Jungwook Choi",
          "hidden": false
        },
        {
          "_id": "685911a30e4ad7e2197582f6",
          "name": "Simyung Chang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-18T02:22:14.000Z",
      "submittedOnDailyAt": "2025-06-23T07:10:57.325Z",
      "title": "InfiniPot-V: 메모리 제한을 고려한 KV 캐시의 압축 기술에 기반한 스트리밍 비디오 이해\n\n(请注意，虽然要求不添加额外文本，但为了确保翻译的准确性和专业性，我在翻译中保留了原文的格式和结构，同时保持了韩国语的语法和表达习惯。)",
      "submittedOnDailyBy": {
        "_id": "63c0e2503bdc86f8108da51b",
        "avatarUrl": "/avatars/7d47f11992f030b3d831e45102581d1f.svg",
        "isPro": false,
        "fullname": "Minsoo Kim",
        "user": "minsoo2333",
        "type": "user"
      },
      "summary": "현대의 다모달 대언어 모델(MLLMs)은 긴 영상에 대한 논리적 분석이 가능하지만, 그 Key Value(KV) 캐시는 시간과 선형적으로 증가하여 휴대전화, AR 렌즈, 에지로보트의 고정 메모리를 초과하는 속도로 증가합니다. 기존의 압축 알고리즘은 전체 비디오와 사용자 요청이 오프라인에서 사용할 수 있는 가정을 기반으로하거나, 완전히 캐시를 구축해야 하며, 그 결과 메모리는 스트리밍 길이에 비례하여 증가합니다. InfiniPot-V는 첫 번째 훈련 없이, 요청과 상관없는 프레임워크이며, 스트리밍 영상의 이해에 대해 엄격한 길이 무관 메모리 캡을 강제합니다. 영상을 인코딩하는 동안, 캐시를 모니터링하고, 사용자 설정의 스로프를 달성하면 가벼운 압축 패스를 실행합니다. 이는 (i) 시간적 반복 텍스트를 Temporal-axis Redundancy(TaR) 메트릭을 사용하여 제거하거나, (ii) Value-Norm(VaN) 순위를 사용하여 의미적으로 중요한 텍스트를 유지합니다. 4개의 오픈 소스 MLLM과 4개의 긴 영상 벤치마크, 2개의 스트리밍 영상 벤치마크를 통해, InfiniPot-V는 GPU 메모리의 최대 사용량을 94%까지 줄였으며, 실시간 생성을 유지하며, 완전한 캐시의 정확도를 만족하거나 초과합니다. 훈련 및 요청 지식이 필요하지 않도록, KV 캐시의 한계점을 해결하고, 온라인 스트리밍 영상 보조사의 오류를 폐쇄합니다.",
      "upvotes": 4,
      "discussionId": "685911a30e4ad7e2197582f7",
      "ai_summary": "InfiniPot-V is a training-free, query-agnostic framework that compresses the key-value cache during video encoding to maintain a fixed memory cap for streaming video understanding, enhancing real-time performance and accuracy.",
      "ai_keywords": [
        "multimodal large language models",
        "key-value cache",
        "Temporal-axis Redundancy",
        "Value-Norm ranking",
        "long-video benchmarks",
        "streaming-video benchmarks",
        "real-time generation",
        "multi-turn dialogues",
        "on-device streaming video assistants"
      ]
    },
    "publishedAt": "2025-06-17T22:22:14.000Z",
    "title": "InfiniPot-V: Memory-Constrained KV Cache Compression for Streaming Video\n  Understanding",
    "summary": "Modern multimodal large language models (MLLMs) can reason over hour-long\nvideo, yet their key-value (KV) cache grows linearly with time--quickly\nexceeding the fixed memory of phones, AR glasses, and edge robots. Prior\ncompression schemes either assume the whole video and user query are available\noffline or must first build the full cache, so memory still scales with stream\nlength. InfiniPot-V is the first training-free, query-agnostic framework that\nenforces a hard, length-independent memory cap for streaming video\nunderstanding. During video encoding it monitors the cache and, once a user-set\nthreshold is reached, runs a lightweight compression pass that (i) removes\ntemporally redundant tokens via Temporal-axis Redundancy (TaR) metric and (ii)\nkeeps semantically significant tokens via Value-Norm (VaN) ranking. Across four\nopen-source MLLMs and four long-video and two streaming-video benchmarks,\nInfiniPot-V cuts peak GPU memory by up to 94%, sustains real-time generation,\nand matches or surpasses full-cache accuracy--even in multi-turn dialogues. By\ndissolving the KV cache bottleneck without retraining or query knowledge,\nInfiniPot-V closes the gap for on-device streaming video assistants.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.15745.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63c0e2503bdc86f8108da51b",
      "avatarUrl": "/avatars/7d47f11992f030b3d831e45102581d1f.svg",
      "fullname": "Minsoo Kim",
      "name": "minsoo2333",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.17213",
      "authors": [
        {
          "_id": "685914860e4ad7e219758301",
          "name": "Xiuyu Yang",
          "hidden": false
        },
        {
          "_id": "685914860e4ad7e219758302",
          "name": "Shuhan Tan",
          "hidden": false
        },
        {
          "_id": "685914860e4ad7e219758303",
          "name": "Philipp Krähenbühl",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-20T17:59:21.000Z",
      "submittedOnDailyAt": "2025-06-23T07:17:59.369Z",
      "title": "장기 교통 시뮬레이션에서 교차 자동 귀납적 이동과 스케너 생성",
      "submittedOnDailyBy": {
        "_id": "634aab35dcf125e4dafc87b1",
        "avatarUrl": "/avatars/aa2db84fd423e9eefe3ef3167c9d3999.svg",
        "isPro": false,
        "fullname": "YangXiuyu",
        "user": "gzzyyxy",
        "type": "user"
      },
      "summary": "理想한 교통 시뮬레이터는 자동주행 시스템이 설치될 때부터 현실적인 장기적인 이동을 재현하는 데 사용됩니다. 기존 모델과 벤치마크는 초기 효과 에이전트의 폐로 이동 시뮬레이션에 초점을 맞추고 있습니다. 이는 장기 시뮬레이션에서 문제가 발생합니다. 효과 에이전트는 효과 비크가 새로운 영역에 들어갈 때 장소에서 들어와 나갑니다. 우리는 폐로 이동 시뮬레이션과 장소 생성을 교환할 단위 모델인 InfGen을 제안합니다. InfGen은 폐로 이동 시뮬레이션과 장소 생성 모드 사이에서 자동적으로 전환합니다. 이로 인해 장기적인 롤아웃 시뮬레이션이 안정화됩니다. InfGen은 9초의 단기 교통 시뮬레이션에서 가장 최신으로, 30초의 장기 시뮬레이션에서는 모든 방법보다 크게 뛰어넘습니다. InfGen의 코드와 모델은 https://orangesodahub.github.io/InfGen에서 공개됩니다.",
      "upvotes": 2,
      "discussionId": "685914860e4ad7e219758304",
      "projectPage": "https://orangesodahub.github.io/InfGen/",
      "githubRepo": "https://github.com/OrangeSodahub/infgen/",
      "ai_summary": "InfGen, a unified next-token prediction model, enables stable long-term traffic simulation by interleaving closed-loop motion simulation and scene generation.",
      "ai_keywords": [
        "next-token prediction",
        "closed-loop motion simulation",
        "scene generation",
        "long-term traffic simulation"
      ]
    },
    "publishedAt": "2025-06-20T13:59:21.000Z",
    "title": "Long-term Traffic Simulation with Interleaved Autoregressive Motion and\n  Scenario Generation",
    "summary": "An ideal traffic simulator replicates the realistic long-term point-to-point\ntrip that a self-driving system experiences during deployment. Prior models and\nbenchmarks focus on closed-loop motion simulation for initial agents in a\nscene. This is problematic for long-term simulation. Agents enter and exit the\nscene as the ego vehicle enters new regions. We propose InfGen, a unified\nnext-token prediction model that performs interleaved closed-loop motion\nsimulation and scene generation. InfGen automatically switches between\nclosed-loop motion simulation and scene generation mode. It enables stable\nlong-term rollout simulation. InfGen performs at the state-of-the-art in\nshort-term (9s) traffic simulation, and significantly outperforms all other\nmethods in long-term (30s) simulation. The code and model of InfGen will be\nreleased at https://orangesodahub.github.io/InfGen",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.17213.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "634aab35dcf125e4dafc87b1",
      "avatarUrl": "/avatars/aa2db84fd423e9eefe3ef3167c9d3999.svg",
      "fullname": "YangXiuyu",
      "name": "gzzyyxy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.17202",
      "authors": [
        {
          "_id": "68591c310e4ad7e219758306",
          "name": "Teng Li",
          "hidden": false
        },
        {
          "_id": "68591c310e4ad7e219758307",
          "name": "Quanfeng Lu",
          "hidden": false
        },
        {
          "_id": "68591c310e4ad7e219758308",
          "name": "Lirui Zhao",
          "hidden": false
        },
        {
          "_id": "68591c310e4ad7e219758309",
          "name": "Hao Li",
          "hidden": false
        },
        {
          "_id": "68591c310e4ad7e21975830a",
          "name": "Xizhou Zhu",
          "hidden": false
        },
        {
          "_id": "68591c310e4ad7e21975830b",
          "name": "Yu Qiao",
          "hidden": false
        },
        {
          "_id": "68591c310e4ad7e21975830c",
          "name": "Jun Zhang",
          "hidden": false
        },
        {
          "_id": "68591c310e4ad7e21975830d",
          "name": "Wenqi Shao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-20T17:52:31.000Z",
      "submittedOnDailyAt": "2025-06-23T07:51:10.698Z",
      "title": "UniFork: 모델 다양성을 조사하여 단일 모델에서 다양한 이해와 생성을 실현합니다.",
      "submittedOnDailyBy": {
        "_id": "64897b1f0ec897cfe579a399",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64897b1f0ec897cfe579a399/ICR_75b877BaSE94gjBuj.jpeg",
        "isPro": false,
        "fullname": "wenq",
        "user": "wenqsun",
        "type": "user"
      },
      "summary": "통합 이미지 이해와 생성은 다 모델 인공지능의 중で 원하는 패러다임으로 등장했습니다. 최근의 진보에도 불구하고, 이러한 통합 모델의 최적 아키텍처 설계는 개방적인 도전입니다. 본 연구에서는, 태스크 전문의 경험 모델의 모델 대응 행동을 분석하고, 현재의 통합 모델을 포함하여 시작합니다. 분석에 따라, 중요한 발견이 밝혀졌습니다: 이해 태스크는 네트워크의 깊이로 발전적으로 모델 대응이 증가하고, 이는 의미 정보의 구축에 의해 더 좋은 이해를 촉진합니다. 반대로, 생성 태스크는 다른 경향을 보여주고 있습니다: 초기의 계층에서 모델 대응이 증가하고, 깊은 계층에서 감소하며, 공간의 세부 사항을 회복합니다. 이러한 다른 모델 대응 패턴은 완전히 공유된 Transformer 백본에서 기본적인 충돌을 초래합니다. 이 발견에 기반하여, UniFork라는 새로운 Y자형 아키텍처를 통해,shallow 계층에서 태스크 간 표현 학습을 공유하고, 깊은 계층에서 태스크专用의 분기를 사용하여 태스크 간섭을 피하는 것을 제안합니다. 이 설계는 공유 학습과 태스크 특수화를 더 균등하게 조화시킵니다. 확장된 소멸 실험을 통해, UniFork는 일반적인 완전히 공유된 Transformer 아키텍처를 초과하고, 태스크专用의 모델과 같은 또는 더 좋은 성능을 보여주었습니다.",
      "upvotes": 2,
      "discussionId": "68591c310e4ad7e21975830e",
      "ai_summary": "A Y-shaped architecture, UniFork, balances shared learning and task specialization for unified image understanding and generation, outperforming conventional fully shared Transformer models.",
      "ai_keywords": [
        "modality alignment",
        "network depth",
        "semantic information",
        "spatial details",
        "Transformer backbones",
        "Y-shaped architecture",
        "task-specific branches",
        "task interference",
        "ablation experiments",
        "fully shared Transformer architectures",
        "task-specific models"
      ]
    },
    "publishedAt": "2025-06-20T13:52:31.000Z",
    "title": "UniFork: Exploring Modality Alignment for Unified Multimodal\n  Understanding and Generation",
    "summary": "Unified image understanding and generation has emerged as a promising\nparadigm in multimodal artificial intelligence. Despite recent progress, the\noptimal architectural design for such unified models remains an open challenge.\nIn this work, we start by analyzing the modality alignment behaviors of\ntask-specific expert models for understanding and generation, as well as\ncurrent unified models. Our analysis reveals a crucial observation:\nunderstanding tasks benefit from a progressively increasing modality alignment\nacross network depth, which helps build up semantic information for better\ncomprehension; In contrast, generation tasks follow a different trend: modality\nalignment increases in the early layers but decreases in the deep layers to\nrecover spatial details. These divergent alignment patterns create a\nfundamental conflict in fully shared Transformer backbones, where a uniform\nrepresentational flow often leads to performance compromises across two tasks.\nMotivated by this finding, we introduce UniFork, a novel Y-shaped architecture\nthat shares the shallow layers for cross-task representation learning, while\nemploying task-specific branches in deeper layers to avoid task interference.\nThis design effectively balances shared learning and task specialization.\nThrough extensive ablation experiments, we demonstrate that Unifork\nconsistently outperforms conventional fully shared Transformer architectures,\nand achieves performance on par with or better than task-specific models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.17202.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64897b1f0ec897cfe579a399",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64897b1f0ec897cfe579a399/ICR_75b877BaSE94gjBuj.jpeg",
      "fullname": "wenq",
      "name": "wenqsun",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.15442",
      "authors": [
        {
          "_id": "68552b394f1add9d4c5c5cd4",
          "name": "Team Hunyuan3D",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5cd5",
          "name": "Shuhui Yang",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5cd6",
          "name": "Mingxin Yang",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5cd7",
          "name": "Yifei Feng",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5cd8",
          "name": "Xin Huang",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5cd9",
          "name": "Sheng Zhang",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5cda",
          "name": "Zebin He",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5cdb",
          "name": "Di Luo",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5cdc",
          "name": "Haolin Liu",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5cdd",
          "name": "Yunfei Zhao",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5cde",
          "name": "Qingxiang Lin",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5cdf",
          "name": "Zeqiang Lai",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5ce0",
          "name": "Xianghui Yang",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5ce1",
          "name": "Huiwen Shi",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5ce2",
          "name": "Zibo Zhao",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5ce3",
          "name": "Bowen Zhang",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5ce4",
          "name": "Hongyu Yan",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5ce5",
          "name": "Lifu Wang",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5ce6",
          "name": "Sicong Liu",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5ce7",
          "name": "Jihong Zhang",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5ce8",
          "name": "Meng Chen",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5ce9",
          "name": "Liang Dong",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5cea",
          "name": "Yiwen Jia",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5ceb",
          "name": "Yulin Cai",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5cec",
          "name": "Jiaao Yu",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5ced",
          "name": "Yixuan Tang",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5cee",
          "name": "Dongyuan Guo",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5cef",
          "name": "Junlin Yu",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5cf0",
          "name": "Hao Zhang",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5cf1",
          "name": "Zheng Ye",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5cf2",
          "name": "Peng He",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5cf3",
          "name": "Runzhou Wu",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5cf4",
          "name": "Shida Wei",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5cf5",
          "name": "Chao Zhang",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5cf6",
          "name": "Yonghao Tan",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5cf7",
          "name": "Yifu Sun",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5cf8",
          "name": "Lin Niu",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5cf9",
          "name": "Shirui Huang",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5cfa",
          "name": "Bojian Zheng",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5cfb",
          "name": "Shu Liu",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5cfc",
          "name": "Shilin Chen",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5cfd",
          "name": "Xiang Yuan",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5cfe",
          "name": "Xiaofeng Yang",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5cff",
          "name": "Kai Liu",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5d00",
          "name": "Jianchen Zhu",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5d01",
          "name": "Peng Chen",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5d02",
          "name": "Tian Liu",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5d03",
          "name": "Di Wang",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5d04",
          "name": "Yuhong Liu",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5d05",
          "name": "Linus",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5d06",
          "name": "Jie Jiang",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5d07",
          "name": "Jingwei Huang",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5d08",
          "name": "Chunchao Guo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-18T13:14:46.000Z",
      "submittedOnDailyAt": "2025-06-23T07:17:03.771Z",
      "title": "폼유ン 3D 2.1: 사진으로부터 고품질의 3D 자산으로, 생산기준을 준수하는 PBR 재료를 사용합니다.",
      "submittedOnDailyBy": {
        "_id": "647d9e881a1fcad2fdbf4954",
        "avatarUrl": "/avatars/92ee8727d5c9063d852d3537b7690843.svg",
        "isPro": false,
        "fullname": "SeanYoung",
        "user": "SeanYoungxh",
        "type": "user"
      },
      "summary": "3D AI 생성 콘텐츠(AIGC)는 게임, 영화, 디자인 분야에서 3D 모델의 제작을 크게 가속화하고 있는 열정적인 분야입니다. 많은 혁신적인 모델이 개발되었으며, 3D 생성에서 디렉토리 배정을 변경했지만, 이 분야는 3D 모델의 수집, 처리, 훈련에 대한 복잡성을 때문에 연구자, 개발자, 디자인가만 할 수 있는 영역입니다. 이러한 도전에 대처하기 위해, 이 튜토리얼에서는 Hunyuan3D 2.1를 활용한 사례 연구를 소개합니다. 이 튜토리얼은 Hunyuan3D 2.1(고해상도, 테크스쳐付き 3D 어셈블리를 생성하는 고급 시스템)을 사용하여 3D 데이터 처리, 3D 생성 모델의 훈련, 그리고 성능 평가를 위한 일련의 단계별 가이드를 제공합니다. 이 시스템은 형태 생성을 위한 Hunyuan3D-DiT과 테크스쳐 합성을 위한 Hunyuan3D-Paint의 2개의 핵심 구성 요소로 이루어져 있습니다. 데이터 준비, 모델 아키텍처, 훈련 전략, 평가 지표, 배포의 작업 흐름을 전체적으로 검토하고, 이 튜토리얼의 끝에는 게임, 가상 현실, 산업 디자인에 적합한 강력한 3D 생성 모델의 미세 조정이나 개발에 필요한 지식이 필요합니다.",
      "upvotes": 2,
      "discussionId": "68552b394f1add9d4c5c5d09",
      "ai_summary": "The tutorial provides a comprehensive guide on using Hunyuan3D 2.1 for generating high-resolution, textured 3D models, covering data preparation, model architecture, training, evaluation, and deployment.",
      "ai_keywords": [
        "Hunyuan3D-DiT",
        "Hunyuan3D-Paint",
        "3D generative model",
        "texture synthesis",
        "data preparation",
        "model architecture",
        "training strategies",
        "evaluation metrics",
        "deployment"
      ]
    },
    "publishedAt": "2025-06-18T09:14:46.000Z",
    "title": "Hunyuan3D 2.1: From Images to High-Fidelity 3D Assets with\n  Production-Ready PBR Material",
    "summary": "3D AI-generated content (AIGC) is a passionate field that has significantly\naccelerated the creation of 3D models in gaming, film, and design. Despite the\ndevelopment of several groundbreaking models that have revolutionized 3D\ngeneration, the field remains largely accessible only to researchers,\ndevelopers, and designers due to the complexities involved in collecting,\nprocessing, and training 3D models. To address these challenges, we introduce\nHunyuan3D 2.1 as a case study in this tutorial. This tutorial offers a\ncomprehensive, step-by-step guide on processing 3D data, training a 3D\ngenerative model, and evaluating its performance using Hunyuan3D 2.1, an\nadvanced system for producing high-resolution, textured 3D assets. The system\ncomprises two core components: the Hunyuan3D-DiT for shape generation and the\nHunyuan3D-Paint for texture synthesis. We will explore the entire workflow,\nincluding data preparation, model architecture, training strategies, evaluation\nmetrics, and deployment. By the conclusion of this tutorial, you will have the\nknowledge to finetune or develop a robust 3D generative model suitable for\napplications in gaming, virtual reality, and industrial design.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.15442.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "647d9e881a1fcad2fdbf4954",
      "avatarUrl": "/avatars/92ee8727d5c9063d852d3537b7690843.svg",
      "fullname": "SeanYoung",
      "name": "SeanYoungxh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.15925",
      "authors": [
        {
          "_id": "6858c714c0c8e29df8ea3c9c",
          "user": {
            "_id": "64698ed0dcbb937d56b9dd02",
            "avatarUrl": "/avatars/407afd70356f9d0669d4e2d39b9a8740.svg",
            "isPro": false,
            "fullname": "Narutatsu Ri",
            "user": "narutatsuri",
            "type": "user"
          },
          "name": "Narutatsu Ri",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-23T08:14:39.250Z",
          "hidden": false
        },
        {
          "_id": "6858c714c0c8e29df8ea3c9d",
          "name": "Nicholas Deas",
          "hidden": false
        },
        {
          "_id": "6858c714c0c8e29df8ea3c9e",
          "name": "Kathleen McKeown",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-19T00:01:43.000Z",
      "submittedOnDailyAt": "2025-06-23T01:47:16.594Z",
      "title": "偏향된 시각의 요약생성을 위한 재랭킹 기반의 생성기법",
      "submittedOnDailyBy": {
        "_id": "64698ed0dcbb937d56b9dd02",
        "avatarUrl": "/avatars/407afd70356f9d0669d4e2d39b9a8740.svg",
        "isPro": false,
        "fullname": "Narutatsu Ri",
        "user": "narutatsuri",
        "type": "user"
      },
      "summary": "정치와 같은 현실적인 환경에서 무 편향적인 요약을 생성하는 것은 대규모 언어 모델(LLMs)의 중요한 응용 분야 중 하나입니다. 그러나 현재의 평가 프레임워크는 엮임이나 충실성 등 주요 특성을 측정하기 위해 전통적인 메트릭을 사용하며, 그 적용 가능성을 확인하지 않고 있습니다. 또한 개선된 요약 도구의 개발에 대한 노력을 아직도 초기 단계에 있습니다. 우리는 이러한 결함이 해결하기 위해 다음과 같은 두 가지를 수행하고자 합니다: (1) 관점 요약의 품질을 측정하기 위한 신뢰성 있는 메트릭을 특정하고, (2) LLM 기반의 방법의 효과를 0-shot 추론보다 더 발전시킬 수 있는지 조사합니다. 특히, 인간의 注釈을 사용하여 메트릭의 신뢰성을 벤치마크로 하는 테스트 세트를 구축하고, 전통적인 메트릭이 언어 모델 기반의 메트릭에 비해서 떨어지며, 강력한 평가자로 인정받는 것을 보여줍니다. 이러한 메트릭을 사용하여, 리라ン크 기반의 방법이 강력한 결과를 얻는 것을 보여주고, 합성적으로 생성된 데이터와 리라ン크 라벨을 사용하여 선호 조정은 성능을 향상시킬 수 있음을 강조합니다. 우리가 발견한 것은 관점 요약의 방법의 신뢰적인 평가와 개발에 기여하는 것을 목표로 합니다.",
      "upvotes": 1,
      "discussionId": "6858c714c0c8e29df8ea3c9f",
      "githubRepo": "https://github.com/narutatsuri/Unbiased-Perspective-Summarization",
      "ai_summary": "Reranking and preference tuning improve the quality of perspective summaries generated by LLMs, as measured by language model-based metrics that outperform traditional ones.",
      "ai_keywords": [
        "Large Language Models",
        "perspective summarization",
        "coverage",
        "faithfulness",
        "metric reliability",
        "reranking-based methods",
        "preference tuning",
        "synthetically generated data",
        "reranking-labeled data"
      ]
    },
    "publishedAt": "2025-06-18T20:01:43.000Z",
    "title": "Reranking-based Generation for Unbiased Perspective Summarization",
    "summary": "Generating unbiased summaries in real-world settings such as political\nperspective summarization remains a crucial application of Large Language\nModels (LLMs). Yet, existing evaluation frameworks rely on traditional metrics\nfor measuring key attributes such as coverage and faithfulness without\nverifying their applicability, and efforts to develop improved summarizers are\nstill nascent. We address these gaps by (1) identifying reliable metrics for\nmeasuring perspective summary quality, and (2) investigating the efficacy of\nLLM-based methods beyond zero-shot inference. Namely, we build a test set for\nbenchmarking metric reliability using human annotations and show that\ntraditional metrics underperform compared to language model-based metrics,\nwhich prove to be strong evaluators. Using these metrics, we show that\nreranking-based methods yield strong results, and preference tuning with\nsynthetically generated and reranking-labeled data further boosts performance.\nOur findings aim to contribute to the reliable evaluation and development of\nperspective summarization methods.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.15925.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64698ed0dcbb937d56b9dd02",
      "avatarUrl": "/avatars/407afd70356f9d0669d4e2d39b9a8740.svg",
      "fullname": "Narutatsu Ri",
      "name": "narutatsuri",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  }
]