[
  {
    "paper": {
      "id": "2505.21115",
      "authors": [
        {
          "_id": "68372d97e4af3c39dcec8e65",
          "user": {
            "_id": "5dfa8e07da6d0311fd3d5430",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1651090418656-5dfa8e07da6d0311fd3d5430.png",
            "isPro": false,
            "fullname": "Sergey Pletenev",
            "user": "memyprokotow",
            "type": "user"
          },
          "name": "Sergey Pletenev",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-09T10:12:55.604Z",
          "hidden": false
        },
        {
          "_id": "68372d97e4af3c39dcec8e66",
          "user": {
            "_id": "660ee18e2dcd816ad14b3739",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/660ee18e2dcd816ad14b3739/2pPMurtSOHMA96eVk0k7w.jpeg",
            "isPro": false,
            "fullname": "Maria Marina",
            "user": "zlatamaria",
            "type": "user"
          },
          "name": "Maria Marina",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-09T10:12:59.278Z",
          "hidden": false
        },
        {
          "_id": "68372d97e4af3c39dcec8e67",
          "user": {
            "_id": "6682607ece294ddc5e72f4fb",
            "avatarUrl": "/avatars/2a304bc3eb56ec7d13297d28fbb062ae.svg",
            "isPro": false,
            "fullname": "Ivanov",
            "user": "VirVen",
            "type": "user"
          },
          "name": "Nikolay Ivanov",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-02T07:48:42.811Z",
          "hidden": false
        },
        {
          "_id": "68372d97e4af3c39dcec8e68",
          "name": "Daria Galimzianova",
          "hidden": false
        },
        {
          "_id": "68372d97e4af3c39dcec8e69",
          "user": {
            "_id": "643010b2ff56d6c2004699a6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/OYsf1Hp-KAievw_M8XBG9.jpeg",
            "isPro": false,
            "fullname": "Krayko Nikita",
            "user": "nakrayko",
            "type": "user"
          },
          "name": "Nikita Krayko",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-09T10:12:53.523Z",
          "hidden": false
        },
        {
          "_id": "68372d97e4af3c39dcec8e6a",
          "name": "Mikhail Salnikov",
          "hidden": false
        },
        {
          "_id": "68372d97e4af3c39dcec8e6b",
          "name": "Vasily Konovalov",
          "hidden": false
        },
        {
          "_id": "68372d97e4af3c39dcec8e6c",
          "name": "Alexander Panchenko",
          "hidden": false
        },
        {
          "_id": "68372d97e4af3c39dcec8e6d",
          "user": {
            "_id": "63bbfd74141c7d395c471768",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1673264437106-noauth.jpeg",
            "isPro": false,
            "fullname": "Viktor Moskvoretskii",
            "user": "VityaVitalich",
            "type": "user"
          },
          "name": "Viktor Moskvoretskii",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-09T10:12:57.325Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-27T12:35:13.000Z",
      "submittedOnDailyAt": "2025-06-09T07:27:12.232Z",
      "title": "Sera-t-elle aussi le même jour matin ? Problème de l'opéra multilingue\nClassification pour améliorer la confiance dans la réponse à la question\n\nCette traduction maintient la professionnalité et la précision.",
      "submittedOnDailyBy": {
        "_id": "660ee18e2dcd816ad14b3739",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/660ee18e2dcd816ad14b3739/2pPMurtSOHMA96eVk0k7w.jpeg",
        "isPro": false,
        "fullname": "Maria Marina",
        "user": "zlatamaria",
        "type": "user"
      },
      "summary": "Les modèles de langage grands (LLMs) souvent produisent des 'hallucinations' (c'est-à-dire, le contenu de la sortie est différent du fait ou de l'information). Ce phénomène n'est pas seulement un élément important, mais aussi une des causes pour laquelle l'investigation n'a pas encore été suffisamment menée, en raison de la caractéristique temporelle d'identifier le problème. Cette caractéristique se divise en deux types : les réponses qui ne changent pas au fil du temps, les 'evergreen', et les réponses qui changent au fil du temps, les 'mutables'. Dans cette étude, nous présentons EverGreenQA, le premier ensemble de questions et réponses multilingues, qui fournit des étiquettes 'evergreen' pour évaluer et entraîner. En utilisant EverGreenQA, nous évaluons 12 modèles de LLMs modernes, et nous examinons si ces modèles expriment clairement la caractéristique temporelle des questions (exprimée de manière explicite ou implicite, selon le critère de jugement ou les signaux d'incertitude). De plus, nous entraînons un léger classifieur multilingue nommé EG-E5, pour atteindre les meilleurs résultats dans cette tâche. Enfin, nous présentons trois applications pratiques qui démontrent l'utilité de la classification 'evergreen' : améliorer l'estimation de la autoperception, filtrer des ensembles de données de questions et réponses, et expliquer les actions de recherche de GPT-4o.",
      "upvotes": 41,
      "discussionId": "68372d98e4af3c39dcec8e88",
      "githubRepo": "https://github.com/s-nlp/Evergreen-classification",
      "ai_summary": "EverGreenQA, a multilingual QA dataset with evergreen labels, is introduced to benchmark LLMs on temporality encoding and assess their performance through verbalized judgments and uncertainty signals.",
      "ai_keywords": [
        "Large Language Models",
        "QA",
        "evergreen",
        "mutable",
        "temporality",
        "Multilingual QA dataset",
        "EG-E5",
        "lightweight multilingual classifier",
        "SoTA performance",
        "self-knowledge estimation",
        "filtering QA datasets",
        "GPT-4o retrieval behavior"
      ]
    },
    "publishedAt": "2025-05-27T08:35:13.000Z",
    "title": "Will It Still Be True Tomorrow? Multilingual Evergreen Question\n  Classification to Improve Trustworthy QA",
    "summary": "Large Language Models (LLMs) often hallucinate in question answering (QA)\ntasks. A key yet underexplored factor contributing to this is the temporality\nof questions -- whether they are evergreen (answers remain stable over time) or\nmutable (answers change). In this work, we introduce EverGreenQA, the first\nmultilingual QA dataset with evergreen labels, supporting both evaluation and\ntraining. Using EverGreenQA, we benchmark 12 modern LLMs to assess whether they\nencode question temporality explicitly (via verbalized judgments) or implicitly\n(via uncertainty signals). We also train EG-E5, a lightweight multilingual\nclassifier that achieves SoTA performance on this task. Finally, we demonstrate\nthe practical utility of evergreen classification across three applications:\nimproving self-knowledge estimation, filtering QA datasets, and explaining\nGPT-4o retrieval behavior.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21115.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "660ee18e2dcd816ad14b3739",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/660ee18e2dcd816ad14b3739/2pPMurtSOHMA96eVk0k7w.jpeg",
      "fullname": "Maria Marina",
      "name": "zlatamaria",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.01111",
      "authors": [
        {
          "_id": "6845b6a33ec10bdd8ab4da1b",
          "name": "Shunian Chen",
          "hidden": false
        },
        {
          "_id": "6845b6a33ec10bdd8ab4da1c",
          "user": {
            "_id": "66440e86bfe15e84d369cb03",
            "avatarUrl": "/avatars/d15b3b3831bc74138206071612169f64.svg",
            "isPro": false,
            "fullname": "Xinyuan Xie",
            "user": "SatsukiVie",
            "type": "user"
          },
          "name": "Xinyuan Xie",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-09T10:11:42.759Z",
          "hidden": false
        },
        {
          "_id": "6845b6a33ec10bdd8ab4da1d",
          "name": "Zheshu Chen",
          "hidden": false
        },
        {
          "_id": "6845b6a33ec10bdd8ab4da1e",
          "name": "Liyan Zhao",
          "hidden": false
        },
        {
          "_id": "6845b6a33ec10bdd8ab4da1f",
          "name": "Owen Lee",
          "hidden": false
        },
        {
          "_id": "6845b6a33ec10bdd8ab4da20",
          "name": "Zhan Su",
          "hidden": false
        },
        {
          "_id": "6845b6a33ec10bdd8ab4da21",
          "name": "Qilin Sun",
          "hidden": false
        },
        {
          "_id": "6845b6a33ec10bdd8ab4da22",
          "name": "Benyou Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-01T18:29:17.000Z",
      "submittedOnDailyAt": "2025-06-09T01:59:54.914Z",
      "title": "FusionAudio-1.2M : Fusion de contexte multimodal pour la captation de voix avec détails fins",
      "submittedOnDailyBy": {
        "_id": "623be9e1d1eb227788764959",
        "avatarUrl": "/avatars/b6521b795a59754dbb40123fd4f63b8c.svg",
        "isPro": false,
        "fullname": "Shunian Chen",
        "user": "Shunian",
        "type": "user"
      },
      "summary": "La capture de haute qualité de grands sonores est crucial pour le développement de la compréhension vocale, mais les méthodes actuelles d'automatisation génèrent de nombreuses captures qui négligent les détails minutieux et la précision du contexte. Cela est dû à la relation avec des modes uniques limités ou à des informations multimodales superficielles. Inspirés de la perception auditive humaine et en intégrant différents cours adéquatement, nous introduisons une nouvelle pipeline en deux étapes pour atteindre un haut niveau de reconnaissance auditive complexe. Cette pipeline utilise des modèles spécialement entraînés pour extraire différents cours de contexte (par exemple, la voix de conversation, la musique, les sons généraux, l'information visuelle provenant de vidéos liées). Ensuite, les grands modèles de langage (LLM) combinent ces entrées multimodales riches pour générer des captures de voix détaillées et contextuelles. Les principales contributions de cette étude sont : (1) un nouveau méthode de génération de captures de voix détaillées et scalables, (2) l'ajout de fonctions, un nouveau ensemble de données grand qui comprend 1,2 millions de ces captures détaillées, combiné avec 6 millions de couples de questions et réponses (QA), et (3) l'amélioration du modèle de voix, en particulier l'amélioration de la correspondance sonore-document et la capacité de suivi d'instructions dans l'encodeur de voix basé sur CLAP. Cet article présente un jeu de films pour la compréhension automatique précise dans des environnements complexes de voix. Le code et les données peuvent être trouvés sur https://github.com/satsuki2486441738/FusionAudio.",
      "upvotes": 21,
      "discussionId": "6845b6a43ec10bdd8ab4da23",
      "githubRepo": "https://github.com/FreedomIntelligence/FusionAudio",
      "ai_summary": "A novel two-stage pipeline using specialized pretrained models and a large language model enhances audio caption quality by integrating diverse multimodal cues and contextual information.",
      "ai_keywords": [
        "audio captioning",
        "auditory perception",
        "auditory scene analysis",
        "pretrained models",
        "large language model",
        "FusionAudio",
        "CLAP-based audio encoder",
        "audio-text alignment",
        "instruction following"
      ]
    },
    "publishedAt": "2025-06-01T14:29:17.000Z",
    "title": "FusionAudio-1.2M: Towards Fine-grained Audio Captioning with Multimodal\n  Contextual Fusion",
    "summary": "High-quality, large-scale audio captioning is crucial for advancing audio\nunderstanding, yet current automated methods often generate captions that lack\nfine-grained detail and contextual accuracy, primarily due to their reliance on\nlimited unimodal or superficial multimodal information. Drawing inspiration\nfrom human auditory perception, which adeptly integrates cross-modal cues and\nperforms sophisticated auditory scene analysis, we introduce a novel two-stage\nautomated pipeline. This pipeline first employs specialized pretrained models\nto extract diverse contextual cues (e.g., speech, music, general sounds, and\nvisual information from associated video). A large language model (LLM) then\nsynthesizes these rich, multimodal inputs to generate detailed and\ncontext-aware audio captions. Key contributions of this work include: (1) the\nproposed scalable method for fine-grained audio caption generation; (2)\nFusionAudio, a new large-scale dataset comprising 1.2 million such detailed\ncaptions, combined with 6 million QA pairs; and (3) enhanced audio models\ndeveloped using FusionAudio, specifically a CLAP-based audio encoder with\nsuperior audio-text alignment and instruction following. This paper paves the\nway for more nuanced and accurate automated understanding of complex audio\nenvironments. Code and data can be found in\nhttps://github.com/satsuki2486441738/FusionAudio.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01111.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "623be9e1d1eb227788764959",
      "avatarUrl": "/avatars/b6521b795a59754dbb40123fd4f63b8c.svg",
      "fullname": "Shunian Chen",
      "name": "Shunian",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.05984",
      "authors": [
        {
          "_id": "68463ee43ec10bdd8ab4da6f",
          "user": {
            "_id": "622326ae0129f2097d69a3e2",
            "avatarUrl": "/avatars/7665223e3fc8b820ce001e6003daf4d2.svg",
            "isPro": false,
            "fullname": "Cheng-Han Chiang",
            "user": "dcml0714",
            "type": "user"
          },
          "name": "Cheng-Han Chiang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-09T10:11:26.041Z",
          "hidden": false
        },
        {
          "_id": "68463ee43ec10bdd8ab4da70",
          "user": {
            "_id": "64dc191bc307ee5369fbcb04",
            "avatarUrl": "/avatars/5a8a0db63a187e85d4ae2fff93a838f0.svg",
            "isPro": false,
            "fullname": "Xiaofei Wang",
            "user": "xiaofei-wang",
            "type": "user"
          },
          "name": "Xiaofei Wang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-09T01:54:46.319Z",
          "hidden": false
        },
        {
          "_id": "68463ee43ec10bdd8ab4da71",
          "name": "Chung-Ching Lin",
          "hidden": false
        },
        {
          "_id": "68463ee43ec10bdd8ab4da72",
          "name": "Kevin Lin",
          "hidden": false
        },
        {
          "_id": "68463ee43ec10bdd8ab4da73",
          "name": "Linjie Li",
          "hidden": false
        },
        {
          "_id": "68463ee43ec10bdd8ab4da74",
          "name": "Radu Kopetz",
          "hidden": false
        },
        {
          "_id": "68463ee43ec10bdd8ab4da75",
          "name": "Yao Qian",
          "hidden": false
        },
        {
          "_id": "68463ee43ec10bdd8ab4da76",
          "name": "Zhendong Wang",
          "hidden": false
        },
        {
          "_id": "68463ee43ec10bdd8ab4da77",
          "name": "Zhengyuan Yang",
          "hidden": false
        },
        {
          "_id": "68463ee43ec10bdd8ab4da78",
          "name": "Hung-yi Lee",
          "hidden": false
        },
        {
          "_id": "68463ee43ec10bdd8ab4da79",
          "name": "Lijuan Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-06T11:05:48.000Z",
      "submittedOnDailyAt": "2025-06-09T00:28:11.753Z",
      "title": "Les grands modèles de langage liés au langage vocal jouent un rôle décisif dans la forme de la conversation.",
      "submittedOnDailyBy": {
        "_id": "622326ae0129f2097d69a3e2",
        "avatarUrl": "/avatars/7665223e3fc8b820ce001e6003daf4d2.svg",
        "isPro": false,
        "fullname": "Cheng-Han Chiang",
        "user": "dcml0714",
        "type": "user"
      },
      "summary": "Les modèles de grande taille de voix liés au langage (ALLMs) sont capables de comprendre tant l'information textuelle que la non textuelle des données de voix. Dans cet article, on étudie comment utiliser les ALLMs comme jurés automatiques pour évaluer l'expression des discours. On utilise des jurés d'ALLMs pour évaluer les discours générés par les modèles de grande taille de langage (SLMs). Cette évaluation utilise deux tâches : l'exécution du style de voix et l'interprétation du rôle. Les expressions évaluées incluent les émotions, le volume, la vitesse de parole, le poids des mots, le contrôle de la mélodie, les éléments non linguistiques et plus. On utilise quatre modèles de langage (SLMs) pour réaliser ces deux tâches et on compare les jugements des humains et ceux des ALLMs sur les réponses des SLMs. On compare la fréquence de concordance entre les jurés de Gemini et les humains avec la fréquence de concordance entre les jurés humains, et on constate que la fréquence de concordance entre Gemini et les humains est relativement élevée. Ces résultats attendus démontrent que les ALLMs peuvent être utilisés comme jurés pour évaluer les SLMs. De plus, on constate clairement que les SLMs actuels, en particulier GPT-4o-audio, ont des avantages dans le contrôle de l'expression et la génération de dialogues naturels.",
      "upvotes": 9,
      "discussionId": "68463ee43ec10bdd8ab4da7a",
      "ai_summary": "Audio-aware large language models can assess speaking styles in audio inputs, demonstrating performance comparable to human judges in evaluating synthesized speech along dimensions like emotion, volume, and pitch.",
      "ai_keywords": [
        "audio-aware large language models",
        "ALLMs",
        "speaking styles",
        "SLMs",
        "voice style instruction",
        "role-playing",
        "emotion",
        "volume",
        "speaking pace",
        "word emphasis",
        "pitch control",
        "non-verbal elements",
        "GPT-4o-audio",
        "Gemini-2.5-pro",
        "human evaluation",
        "agreement",
        "speaking style control",
        "natural dialogues"
      ]
    },
    "publishedAt": "2025-06-06T07:05:48.000Z",
    "title": "Audio-Aware Large Language Models as Judges for Speaking Styles",
    "summary": "Audio-aware large language models (ALLMs) can understand the textual and\nnon-textual information in the audio input. In this paper, we explore using\nALLMs as an automatic judge to assess the speaking styles of speeches. We use\nALLM judges to evaluate the speeches generated by SLMs on two tasks: voice\nstyle instruction following and role-playing. The speaking style we consider\nincludes emotion, volume, speaking pace, word emphasis, pitch control, and\nnon-verbal elements. We use four spoken language models (SLMs) to complete the\ntwo tasks and use humans and ALLMs to judge the SLMs' responses. We compare two\nALLM judges, GPT-4o-audio and Gemini-2.5-pro, with human evaluation results and\nshow that the agreement between Gemini and human judges is comparable to the\nagreement between human evaluators. These promising results show that ALLMs can\nbe used as a judge to evaluate SLMs. Our results also reveal that current SLMs,\neven GPT-4o-audio, still have room for improvement in controlling the speaking\nstyle and generating natural dialogues.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05984.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "622326ae0129f2097d69a3e2",
      "avatarUrl": "/avatars/7665223e3fc8b820ce001e6003daf4d2.svg",
      "fullname": "Cheng-Han Chiang",
      "name": "dcml0714",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.05629",
      "authors": [
        {
          "_id": "68464b273ec10bdd8ab4da86",
          "user": {
            "_id": "64a6518132cf858d6386ac52",
            "avatarUrl": "/avatars/4cabf3dab8b1ba06245ad8024f334181.svg",
            "isPro": false,
            "fullname": "Ananth Muppidi",
            "user": "ananthmuppidi",
            "type": "user"
          },
          "name": "Ananth Muppidi",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-09T02:47:03.971Z",
          "hidden": false
        },
        {
          "_id": "68464b273ec10bdd8ab4da87",
          "user": {
            "_id": "5f89da6c5d083370c711f37c",
            "avatarUrl": "/avatars/c2f17a4a636973817fd5da2ae6dbaac3.svg",
            "isPro": false,
            "fullname": "Abhilash Nandy",
            "user": "abhi1nandy2",
            "type": "user"
          },
          "name": "Abhilash Nandy",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-06-09T03:49:29.446Z",
          "hidden": false
        },
        {
          "_id": "68464b273ec10bdd8ab4da88",
          "user": {
            "_id": "65238ea295df08170c93933d",
            "avatarUrl": "/avatars/8364301e324274a550d12f2b184ea10e.svg",
            "isPro": false,
            "fullname": "Sambaran Bandyopadhyay",
            "user": "sambaran",
            "type": "user"
          },
          "name": "Sambaran Bandyopadhyay",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-09T02:47:03.971Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-05T23:13:22.000Z",
      "submittedOnDailyAt": "2025-06-09T01:27:41.831Z",
      "title": "Plan de Promotion pour l'Ecologisation de l'Impact Environnemental",
      "submittedOnDailyBy": {
        "_id": "5f89da6c5d083370c711f37c",
        "avatarUrl": "/avatars/c2f17a4a636973817fd5da2ae6dbaac3.svg",
        "isPro": false,
        "fullname": "Abhilash Nandy",
        "user": "abhi1nandy2",
        "type": "user"
      },
      "summary": "Dans certains domaines de tâches, l'ajuste de modèles de langage à grande échelle est coûteux en termes de calcul et techniquement difficile. Cet article se concentre sur un ajuste efficace de paramètres pour adapter des modèles pré-entraînés à des tâches de bas niveau, en utilisant un pruning doux. Nous proposons un nouveau méthode de pruning doux basée sur les tokens d'entrée (ID-SPAM) pour traiter différemment les tokens de différentes importance. Notre méthode réduit le nombre de paramètres d'entraînement, est simple et efficace. Nous montrons les avantages comparatifs avec la technologie la plus récente et démontrons une meilleure capacité de transfert dans le domaine 0 shot.",
      "upvotes": 9,
      "discussionId": "68464b273ec10bdd8ab4da89",
      "ai_summary": "A new method using input-dependent soft prompting with a self-attention mechanism improves parameter-efficient fine-tuning for large language models, enhancing zero-shot domain transfer.",
      "ai_keywords": [
        "soft prompting",
        "parameter-efficient fine-tuning",
        "pre-trained models",
        "downstream tasks",
        "Input Dependent Soft Prompting technique",
        "self-Attention Mechanism",
        "zero shot domain transfer"
      ]
    },
    "publishedAt": "2025-06-05T19:13:22.000Z",
    "title": "Leveraging Self-Attention for Input-Dependent Soft Prompting in LLMs",
    "summary": "The performance of large language models in domain-specific tasks\nnecessitates fine-tuning, which is computationally expensive and technically\nchallenging. This paper focuses on parameter-efficient fine-tuning using soft\nprompting, a promising approach that adapts pre-trained models to downstream\ntasks by learning a small set of parameters. We propose a novel Input Dependent\nSoft Prompting technique with a self-Attention Mechanism (ID-SPAM) that\ngenerates soft prompts based on the input tokens and attends different tokens\nwith varying importance. Our method is simple and efficient, keeping the number\nof trainable parameters small. We show the merits of the proposed approach\ncompared to state-of-the-art techniques on various tasks and show the improved\nzero shot domain transfer capability.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05629.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5f89da6c5d083370c711f37c",
      "avatarUrl": "/avatars/c2f17a4a636973817fd5da2ae6dbaac3.svg",
      "fullname": "Abhilash Nandy",
      "name": "abhi1nandy2",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.01872",
      "authors": [
        {
          "_id": "683e77d41417d107337abf6e",
          "user": {
            "_id": "643f9e2288d9d4488fd81c52",
            "avatarUrl": "/avatars/e589c9cbd47022883cf33d7555bee89c.svg",
            "isPro": false,
            "fullname": "Tinghui Zhu",
            "user": "DarthZhu",
            "type": "user"
          },
          "name": "Tinghui Zhu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-09T10:12:47.572Z",
          "hidden": false
        },
        {
          "_id": "683e77d41417d107337abf6f",
          "name": "Kai Zhang",
          "hidden": false
        },
        {
          "_id": "683e77d41417d107337abf70",
          "name": "Muhao Chen",
          "hidden": false
        },
        {
          "_id": "683e77d41417d107337abf71",
          "name": "Yu Su",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-02T17:01:40.000Z",
      "submittedOnDailyAt": "2025-06-09T07:51:42.399Z",
      "title": "Le design de l'extension de modèle s'adapte-t-il à tous les designs de modèle correctement ?",
      "submittedOnDailyBy": {
        "_id": "643f9e2288d9d4488fd81c52",
        "avatarUrl": "/avatars/e589c9cbd47022883cf33d7555bee89c.svg",
        "isPro": false,
        "fullname": "Tinghui Zhu",
        "user": "DarthZhu",
        "type": "user"
      },
      "summary": "Les modèles de langue multimodales (OLMs) ont pour objectif de fournir des explications pour différentes modalités d'entrée, comme le texte, les images, les vidéos et la voix, tout en maintenant une forte capacité linguistique. Malgré les récents progrès, les modèles actuels, en particulier les ouverts, sont encore loin d'atteindre une véritable multimodalité. En particulier, ils font face à des défis lors de la généralisation sur des paires de modalités qui n'ont pas été entraînées et lors du traitement d'entrées multimodales à haut rendement. Dans ce travail, on étudie l'un des principaux méthodes d'entraînement de modèles : l'extension de modalités. Plus spécifiquement, on examine si l'extension de modalités complète les capacités linguistiques essentielles, si on peut intégrer efficacement des modèles de modalités entraînés de manière indépendante, et si cet effet est plus prononcé avec l'extension séquentielle par rapport à l'amélioration de la compatibilité et de la généralisation. A travers des expériences détaillées, on analyse ces compromis et on fournit des feedbacks sur la possibilité d'atteindre une véritable multimodalité dans les méthodes actuelles d'entraînement.",
      "upvotes": 9,
      "discussionId": "683e77d41417d107337abf8f",
      "projectPage": "https://darthzhu.github.io/lm-extend-page/",
      "githubRepo": "https://github.com/DarthZhu/lm-extend",
      "ai_summary": "Research investigates the impact of extending modality and model merging on maintaining language abilities and generalization in omni-modal language models.",
      "ai_keywords": [
        "omni-modal language models",
        "modality extension",
        "fine-tuning",
        "language abilities",
        "model merging",
        "generalization",
        "true omni-modality"
      ]
    },
    "publishedAt": "2025-06-02T13:01:40.000Z",
    "title": "Is Extending Modality The Right Path Towards Omni-Modality?",
    "summary": "Omni-modal language models (OLMs) aim to integrate and reason over diverse\ninput modalities--such as text, images, video, and audio--while maintaining\nstrong language capabilities. Despite recent advancements, existing models,\nespecially open-source ones, remain far from true omni-modality, struggling to\ngeneralize beyond the specific modality pairs they are trained on or to achieve\nstrong performance when processing multi-modal inputs. We study the effect of\nextending modality, the dominant technique for training multimodal models,\nwhere an off-the-shelf language model is fine-tuned on target-domain and\nlanguage data. Specifically, we investigate three key questions: (1) Does\nmodality extension compromise core language abilities? (2) Can model merging\neffectively integrate independently fine-tuned modality-specific models to\nachieve omni-modality? (3) Does omni-modality extension lead to better\nknowledge sharing and generalization compared to sequential extension? Through\nextensive experiments, we analyze these trade-offs and provide insights into\nthe feasibility of achieving true omni-modality using current approaches.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01872.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "643f9e2288d9d4488fd81c52",
      "avatarUrl": "/avatars/e589c9cbd47022883cf33d7555bee89c.svg",
      "fullname": "Tinghui Zhu",
      "name": "DarthZhu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.06276",
      "authors": [
        {
          "_id": "68466dfb3ec10bdd8ab4dae2",
          "name": "Jiatao Gu",
          "hidden": false
        },
        {
          "_id": "68466dfb3ec10bdd8ab4dae3",
          "name": "Tianrong Chen",
          "hidden": false
        },
        {
          "_id": "68466dfb3ec10bdd8ab4dae4",
          "name": "David Berthelot",
          "hidden": false
        },
        {
          "_id": "68466dfb3ec10bdd8ab4dae5",
          "name": "Huangjie Zheng",
          "hidden": false
        },
        {
          "_id": "68466dfb3ec10bdd8ab4dae6",
          "name": "Yuyang Wang",
          "hidden": false
        },
        {
          "_id": "68466dfb3ec10bdd8ab4dae7",
          "name": "Ruixiang Zhang",
          "hidden": false
        },
        {
          "_id": "68466dfb3ec10bdd8ab4dae8",
          "name": "Laurent Dinh",
          "hidden": false
        },
        {
          "_id": "68466dfb3ec10bdd8ab4dae9",
          "name": "Miguel Angel Bautista",
          "hidden": false
        },
        {
          "_id": "68466dfb3ec10bdd8ab4daea",
          "name": "Josh Susskind",
          "hidden": false
        },
        {
          "_id": "68466dfb3ec10bdd8ab4daeb",
          "name": "Shuangfei Zhai",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-06T17:58:39.000Z",
      "submittedOnDailyAt": "2025-06-09T03:58:52.022Z",
      "title": "STARFlow : Échelonnage des formes de normalisation potentielles pour la synthèse d'images à haute résolution",
      "submittedOnDailyBy": {
        "_id": "6164e72d73996c363c52e66d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1634002684894-noauth.png",
        "isPro": false,
        "fullname": "Jiatao Gu",
        "user": "thomagram",
        "type": "user"
      },
      "summary": "STARFlow est un modèle génératif échelonnable. Ce modèle est construit sur des flux de normalisation et montre un excellent rendement dans la synthèse d'images à haute résolution. Le cœur de STARFlow est le Transformer Autoregressive Flow (TARFlow). TARFlow combine la capacité de représentation des flux de normalisation et la modélisation structurée des canaux de surveillance autonomes. Tout d'abord, on démontre la généralité théorique de TARFlow. Sur cette base, on introduit les trois innovations architecturales et algorithmiques essentielles pour améliorer significativement l'échelle : 1) un design profond pour capturer au maximum la représentation du modèle avec des blocs de Transformer profonds, tandis que les blocs de Transformer superficiels plus efficaces sont les plus avantageux. 2) On modèle dans l'espace potentiel d'un codificateur entraîné, ce qui est plus efficace que la modélisation au niveau de pixel. 3) On introduit un nouvel algorithme de guidage pour améliorer significativement la qualité des échantillons. Il est important de souligner que le modèle maintient la cohérence de départ à arrivée comme flux de normalisation, permettant l'entraînement de probabilité maximale précis dans l'espace continu. STARFlow montre un excellent rendement dans les tâches de génération d'images conditionnées par classe et contexte, atteignant une qualité d'échantillons similaire aux meilleurs modèles diffusés, et, selon notre information, constitue le premier succès significatif en montrant que les flux de normalisation fonctionnent efficacement à cette échelle et cette résolution.",
      "upvotes": 5,
      "discussionId": "68466dfb3ec10bdd8ab4daec",
      "ai_summary": "STARFlow, a generative model combining normalizing flows with autoregressive Transformers, achieves competitive image synthesis performance with innovations in architecture and latent space modeling.",
      "ai_keywords": [
        "normalizing flows",
        "Transformer Autoregressive Flow",
        "TARFlow",
        "theoretical universality",
        "deep-shallow design",
        "pretrained autoencoders",
        "latent space",
        "guidance algorithm",
        "end-to-end normalizing flow",
        "exact maximum likelihood training",
        "class-conditional",
        "text-conditional image generation",
        "state-of-the-art diffusion models"
      ]
    },
    "publishedAt": "2025-06-06T13:58:39.000Z",
    "title": "STARFlow: Scaling Latent Normalizing Flows for High-resolution Image\n  Synthesis",
    "summary": "We present STARFlow, a scalable generative model based on normalizing flows\nthat achieves strong performance in high-resolution image synthesis. The core\nof STARFlow is Transformer Autoregressive Flow (TARFlow), which combines the\nexpressive power of normalizing flows with the structured modeling capabilities\nof Autoregressive Transformers. We first establish the theoretical universality\nof TARFlow for modeling continuous distributions. Building on this foundation,\nwe introduce several key architectural and algorithmic innovations to\nsignificantly enhance scalability: (1) a deep-shallow design, wherein a deep\nTransformer block captures most of the model representational capacity,\ncomplemented by a few shallow Transformer blocks that are computationally\nefficient yet substantially beneficial; (2) modeling in the latent space of\npretrained autoencoders, which proves more effective than direct pixel-level\nmodeling; and (3) a novel guidance algorithm that significantly boosts sample\nquality. Crucially, our model remains an end-to-end normalizing flow, enabling\nexact maximum likelihood training in continuous spaces without discretization.\nSTARFlow achieves competitive performance in both class-conditional and\ntext-conditional image generation tasks, approaching state-of-the-art diffusion\nmodels in sample quality. To our knowledge, this work is the first successful\ndemonstration of normalizing flows operating effectively at this scale and\nresolution.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.06276.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6164e72d73996c363c52e66d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1634002684894-noauth.png",
      "fullname": "Jiatao Gu",
      "name": "thomagram",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.06253",
      "authors": [
        {
          "_id": "68469b773ec10bdd8ab4db88",
          "name": "Yuping He",
          "hidden": false
        },
        {
          "_id": "68469b773ec10bdd8ab4db89",
          "name": "Yifei Huang",
          "hidden": false
        },
        {
          "_id": "68469b773ec10bdd8ab4db8a",
          "user": {
            "_id": "6392c73390b8e99a6779a7b0",
            "avatarUrl": "/avatars/9ff824ab02848120aec5e8de6780bcf1.svg",
            "isPro": false,
            "fullname": "Guo Chen",
            "user": "cg1177",
            "type": "user"
          },
          "name": "Guo Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-09T10:11:02.757Z",
          "hidden": false
        },
        {
          "_id": "68469b773ec10bdd8ab4db8b",
          "name": "Lidong Lu",
          "hidden": false
        },
        {
          "_id": "68469b773ec10bdd8ab4db8c",
          "name": "Baoqi Pei",
          "hidden": false
        },
        {
          "_id": "68469b773ec10bdd8ab4db8d",
          "name": "Jilan Xu",
          "hidden": false
        },
        {
          "_id": "68469b773ec10bdd8ab4db8e",
          "name": "Tong Lu",
          "hidden": false
        },
        {
          "_id": "68469b773ec10bdd8ab4db8f",
          "name": "Yoichi Sato",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-06T17:25:48.000Z",
      "submittedOnDailyAt": "2025-06-09T07:00:04.801Z",
      "title": "Biraldía Poertreid: Échos-contributions et Échos-contributions : Colouration de la Sémitique Kaintek Intelligence Recherche",
      "submittedOnDailyBy": {
        "_id": "6392c73390b8e99a6779a7b0",
        "avatarUrl": "/avatars/9ff824ab02848120aec5e8de6780bcf1.svg",
        "isPro": false,
        "fullname": "Guo Chen",
        "user": "cg1177",
        "type": "user"
      },
      "summary": "Le monde perçu à partir de deux points de vue virtuels (d'abord depuis un centre virtuel propre et, ensuite, depuis un deuxième centre virtuel) constitue la base de la perception humaine et permet une compréhension riche et complémentaire d'environnements dynamiques. Récemment, le développement de fonctions de complémentation de ces deux perspectives par l'intelligence artificielle a émergé comme un domaine d'étude intéressant dans l'interprétation d'images. Ce travail explore en profondeur l'interprétation d'images à partir de ces deux perspectives. Tout d'abord, des exemples pratiques d'applications qui intègrent ces méthodes de deux centres virtuels sont présentés, et des minerais de données potentiels sont anticipés. Ensuite, les défis cruciaux pour réaliser ces applications sont identifiés. Ensuite, les derniers avancés dans trois directions principales de recherche sont regroupés, et des travaux divers liés à des tâches spécifiques de chacune de ces directions sont analysés. De plus, les ensembles de données de référence qui soutiennent la recherche de ces deux perspectives sont discutés ensemble, et leur portée, diversité et possibilité d'application sont évaluées. Enfin, les limites actuelles de la recherche sont discutées, et des directions futures de recherche sont proposées. L'intégration de ces deux perspectives permet de favoriser l'interprétation d'images et le développement de l'intelligence artificielle, afin que l'intelligence artificielle puisse voir le monde comme la humanité le fait. Pour étudier ces thèmes, on recommande le dépôt GitHub https://github.com/ayiyayi/Awesome-Egocentric-and-Exocentric-Vision.",
      "upvotes": 5,
      "discussionId": "68469b773ec10bdd8ab4db90",
      "projectPage": "https://github.com/ayiyayi/Awesome-Egocentric-and-Exocentric-Vision",
      "githubRepo": "https://github.com/ayiyayi/Awesome-Egocentric-and-Exocentric-Vision",
      "ai_summary": "A survey on leveraging both egocentric and exocentric video understanding for enhancing complementary tasks with a focus on three research directions and benchmark datasets.",
      "ai_keywords": [
        "egocentric",
        "exocentric",
        "video understanding",
        "research tasks",
        "benchmark datasets"
      ]
    },
    "publishedAt": "2025-06-06T13:25:48.000Z",
    "title": "Bridging Perspectives: A Survey on Cross-view Collaborative Intelligence\n  with Egocentric-Exocentric Vision",
    "summary": "Perceiving the world from both egocentric (first-person) and exocentric\n(third-person) perspectives is fundamental to human cognition, enabling rich\nand complementary understanding of dynamic environments. In recent years,\nallowing the machines to leverage the synergistic potential of these dual\nperspectives has emerged as a compelling research direction in video\nunderstanding. In this survey, we provide a comprehensive review of video\nunderstanding from both exocentric and egocentric viewpoints. We begin by\nhighlighting the practical applications of integrating egocentric and\nexocentric techniques, envisioning their potential collaboration across\ndomains. We then identify key research tasks to realize these applications.\nNext, we systematically organize and review recent advancements into three main\nresearch directions: (1) leveraging egocentric data to enhance exocentric\nunderstanding, (2) utilizing exocentric data to improve egocentric analysis,\nand (3) joint learning frameworks that unify both perspectives. For each\ndirection, we analyze a diverse set of tasks and relevant works. Additionally,\nwe discuss benchmark datasets that support research in both perspectives,\nevaluating their scope, diversity, and applicability. Finally, we discuss\nlimitations in current works and propose promising future research directions.\nBy synthesizing insights from both perspectives, our goal is to inspire\nadvancements in video understanding and artificial intelligence, bringing\nmachines closer to perceiving the world in a human-like manner. A GitHub repo\nof related works can be found at\nhttps://github.com/ayiyayi/Awesome-Egocentric-and-Exocentric-Vision.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.06253.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6392c73390b8e99a6779a7b0",
      "avatarUrl": "/avatars/9ff824ab02848120aec5e8de6780bcf1.svg",
      "fullname": "Guo Chen",
      "name": "cg1177",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 19
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.05523",
      "authors": [
        {
          "_id": "6846657d3ec10bdd8ab4daca",
          "user": {
            "_id": "630bc5ae86b8b9904c33e94b",
            "avatarUrl": "/avatars/b176d9b1691c05cc941409dd6c2b2228.svg",
            "isPro": false,
            "fullname": "Zikui Cai",
            "user": "Zikui",
            "type": "user"
          },
          "name": "Zikui Cai",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-09T10:11:17.551Z",
          "hidden": false
        },
        {
          "_id": "6846657d3ec10bdd8ab4dacb",
          "name": "Andrew Wang",
          "hidden": false
        },
        {
          "_id": "6846657d3ec10bdd8ab4dacc",
          "name": "Anirudh Satheesh",
          "hidden": false
        },
        {
          "_id": "6846657d3ec10bdd8ab4dacd",
          "name": "Ankit Nakhawa",
          "hidden": false
        },
        {
          "_id": "6846657d3ec10bdd8ab4dace",
          "name": "Hyunwoo Jae",
          "hidden": false
        },
        {
          "_id": "6846657d3ec10bdd8ab4dacf",
          "name": "Keenan Powell",
          "hidden": false
        },
        {
          "_id": "6846657d3ec10bdd8ab4dad0",
          "name": "Minghui Liu",
          "hidden": false
        },
        {
          "_id": "6846657d3ec10bdd8ab4dad1",
          "name": "Neel Jay",
          "hidden": false
        },
        {
          "_id": "6846657d3ec10bdd8ab4dad2",
          "name": "Sungbin Oh",
          "hidden": false
        },
        {
          "_id": "6846657d3ec10bdd8ab4dad3",
          "name": "Xiyao Wang",
          "hidden": false
        },
        {
          "_id": "6846657d3ec10bdd8ab4dad4",
          "name": "Yongyuan Liang",
          "hidden": false
        },
        {
          "_id": "6846657d3ec10bdd8ab4dad5",
          "name": "Tom Goldstein",
          "hidden": false
        },
        {
          "_id": "6846657d3ec10bdd8ab4dad6",
          "name": "Furong Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-05T19:12:45.000Z",
      "submittedOnDailyAt": "2025-06-09T03:10:23.755Z",
      "title": "MORSE-500 : Marqueur de vidéo contrôlable par programmation qui effectue des tests de type pour logique à deux chemins.",
      "submittedOnDailyBy": {
        "_id": "655fed9fdef5905d38b84af3",
        "avatarUrl": "/avatars/2cda4182dfd11a1e94743639e62328ea.svg",
        "isPro": false,
        "fullname": "Xiyao Wang",
        "user": "russwang",
        "type": "user"
      },
      "summary": "Bien sûr, voici la traduction en français de l'article en anglais :\n\nMalgré le rapide développement des modèles de vidéo de vidéo (VLMs), les marqueurs actuels de logique multiformes manquent de trois aspects importants. Premièrement, ils dépendent principalement des images statiques et ne comprennent pas la complexité temporelle du monde réel. Deuxièmement, ils se concentrent sur la résolution de problèmes mathématiques, ignorant largement le vaste champ des techniques logiques abstraites, physiques, planifiantes, spatiales et temporelles. Troisièmement, beaucoup de marqueurs s'inactivent rapidement et sont limités dans le diagnostic des erreurs et l'évaluation du progrès. Pour aborder ces problèmes, nous présentons MORSE-500 (environnement de test de stress pour logique multiforme), un marqueur de 500 vidéos qui étend les six catégories de logique intermédiaire. Chaque instance est générée à l'aide d'un script Python spécifique, en utilisant MANIM, Matplotlib, MoviePy, modèles de vidéo générés et aliments de vie édités. Ce script permet de contrôler la complexité visuelle, la densité de détection et la dynamique temporelle, et peut augmenter la difficulté systématiquement en fonction du progrès du modèle. Au contraire des marqueurs statiques, MORSE-500 est conçu pour encourager le développement : la génération de nouvelles instances avec une chaîne de commande contrôlable permet de tester les modèles de la prochaine génération. Les expériences initiales, qui incluent les modèles les plus puissants tels que GEMINI 2.5 Pro, OpenAI o3 et modèles open-source forts, ont montré de grandes différences de rendement dans toutes les catégories, avec des pertes particulières dans les tâches abstraites et planifiantes. Les données de base, les scripts de génération et les outils d'évaluation sont publiés pour soutenir une recherche logique multiforme transparente et reproductible.",
      "upvotes": 5,
      "discussionId": "6846657d3ec10bdd8ab4dad7",
      "projectPage": "https://morse-500.github.io/",
      "githubRepo": "https://github.com/morse-benchmark/morse-500",
      "ai_summary": "MORSE-500, a video benchmark with 500 scripted clips, evaluates multimodal reasoning across six categories, highlighting performance gaps in abstract and planning tasks.",
      "ai_keywords": [
        "Vision-language models",
        "MORSE-500",
        "multimodal reasoning",
        "video benchmark",
        "scripted clips",
        "reasoning categories",
        "Manim",
        "Matplotlib",
        "MoviePy",
        "generative video models",
        "controllable generation",
        "spatial capabilities",
        "temporal capabilities",
        "abstract reasoning",
        "planning tasks"
      ]
    },
    "publishedAt": "2025-06-05T15:12:45.000Z",
    "title": "MORSE-500: A Programmatically Controllable Video Benchmark to\n  Stress-Test Multimodal Reasoning",
    "summary": "Despite rapid advances in vision-language models (VLMs), current benchmarks\nfor multimodal reasoning fall short in three key dimensions. First, they\noverwhelmingly rely on static images, failing to capture the temporal\ncomplexity of real-world environments. Second, they narrowly focus on\nmathematical problem-solving, neglecting the broader spectrum of reasoning\nskills -- including abstract, physical, planning, spatial, and temporal\ncapabilities -- required for robust multimodal intelligence. Third, many\nbenchmarks quickly saturate, offering limited headroom for diagnosing failure\nmodes or measuring continued progress. We introduce MORSE-500 (Multimodal\nReasoning Stress-test Environment), a video benchmark composed of 500 fully\nscripted clips with embedded questions spanning six complementary reasoning\ncategories. Each instance is programmatically generated using deterministic\nPython scripts (via Manim, Matplotlib, MoviePy), generative video models, and\ncurated real footage. This script-driven design allows fine-grained control\nover visual complexity, distractor density, and temporal dynamics -- enabling\ndifficulty to be scaled systematically as models improve. Unlike static\nbenchmarks that become obsolete once saturated, MORSE-500 is built to evolve:\nits controllable generation pipeline supports the creation of arbitrarily\nchallenging new instances, making it ideally suited for stress-testing\nnext-generation models. Initial experiments with state-of-the-art systems --\nincluding various Gemini 2.5 Pro and OpenAI o3 which represent the strongest\navailable at the time, alongside strong open-source models -- reveal\nsubstantial performance gaps across all categories, with particularly large\ndeficits in abstract and planning tasks. We release the full dataset,\ngeneration scripts, and evaluation harness to support transparent,\nreproducible, and forward-looking multimodal reasoning research.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05523.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "655fed9fdef5905d38b84af3",
      "avatarUrl": "/avatars/2cda4182dfd11a1e94743639e62328ea.svg",
      "fullname": "Xiyao Wang",
      "name": "russwang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.05573",
      "authors": [
        {
          "_id": "6846902a3ec10bdd8ab4db61",
          "name": "Yuchen Lin",
          "hidden": false
        },
        {
          "_id": "6846902a3ec10bdd8ab4db62",
          "user": {
            "_id": "62e18206926f4892a4c782bd",
            "avatarUrl": "/avatars/0f89091a5eb72165d2e860d15b339539.svg",
            "isPro": false,
            "fullname": "Chenguo Lin",
            "user": "chenguolin",
            "type": "user"
          },
          "name": "Chenguo Lin",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-09T10:11:09.640Z",
          "hidden": false
        },
        {
          "_id": "6846902a3ec10bdd8ab4db63",
          "name": "Panwang Pan",
          "hidden": false
        },
        {
          "_id": "6846902a3ec10bdd8ab4db64",
          "name": "Honglei Yan",
          "hidden": false
        },
        {
          "_id": "6846902a3ec10bdd8ab4db65",
          "name": "Yiqiang Feng",
          "hidden": false
        },
        {
          "_id": "6846902a3ec10bdd8ab4db66",
          "name": "Yadong Mu",
          "hidden": false
        },
        {
          "_id": "6846902a3ec10bdd8ab4db67",
          "name": "Katerina Fragkiadaki",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/62e18206926f4892a4c782bd/iNc42ij-Kj0Z4V0jD5lCp.mp4"
      ],
      "publishedAt": "2025-06-05T20:30:28.000Z",
      "submittedOnDailyAt": "2025-06-09T06:14:01.450Z",
      "title": "PartCrafter : Génération de structures mécaniques 3D par le Transformateur de Potentiel Distributif Constitutionnel",
      "submittedOnDailyBy": {
        "_id": "62e18206926f4892a4c782bd",
        "avatarUrl": "/avatars/0f89091a5eb72165d2e860d15b339539.svg",
        "isPro": false,
        "fullname": "Chenguo Lin",
        "user": "chenguolin",
        "type": "user"
      },
      "summary": "PartCrafter est le premier modèle de génération 3D structurée qui synthétise à partir d'une seule image RGB des maillages 3D significatifs et géométriquement différents. Au contraire des méthodes existantes, au lieu de créer une seule forme 3D ou de diviser l'image pour reconstruire chaque segment dans un processus à deux étapes, PartCrafter adopte une architecture unifiée et structurée sans dépendre des segments préalablement divisés. En se basant sur une seule image, PartCrafter permet la génération simultanée de plusieurs parties 3D, en éliminant le bruit et en reconnaissant les relations entre les parties d'objets individuels ou les espaces multi-objets complexes. PartCrafter hérite des poids, de l'encodeur et du décodeur d'un 3D meshDIFUージョントランスフォーマー (DiT) entraîné avec l'objet complet, introduisant deux innovations : (1) un espace de potentiels structuré, où chaque partie 3D est représentée par un ensemble de tokens de potentiel séparés ; et (2) une structure de couches d'attention qui permet un flux d'information structuré, assurant une cohérence globale et maintenant les détails au niveau de la partie pendant la génération. Pour soutenir la supervision au niveau de la partie, des notes au niveau de la partie ont été collectées dans une grande collection de données d'objets 3D et un nouveau ensemble de données a été créé. Les expériences montrent que PartCrafter effectue la génération de maillages 3D plus faisables que les méthodes existantes, y compris les parties qui ne sont pas visibles directement dans l'image d'entrée, et qu'elle est à l'avant-garde de la génération d'information structurée et de la compréhension 3D. Le code et les données d'entraînement sont lancés.",
      "upvotes": 4,
      "discussionId": "6846902a3ec10bdd8ab4db68",
      "projectPage": "https://wgsxm.github.io/projects/partcrafter",
      "githubRepo": "https://github.com/wgsxm/PartCrafter",
      "ai_summary": "PartCrafter is a unified 3D generative model that synthesizes multiple semantically meaningful 3D meshes from a single image using a compositional latent space and hierarchical attention mechanism.",
      "ai_keywords": [
        "3D generative model",
        "multiple 3D meshes",
        "RGB image",
        "unified compositional generation architecture",
        "denoising",
        "3D diffusion transformer (DiT)",
        "compositional latent space",
        "disentangled latent tokens",
        "hierarchical attention mechanism",
        "part-level supervision",
        "part-aware generative priors"
      ]
    },
    "publishedAt": "2025-06-05T16:30:28.000Z",
    "title": "PartCrafter: Structured 3D Mesh Generation via Compositional Latent\n  Diffusion Transformers",
    "summary": "We introduce PartCrafter, the first structured 3D generative model that\njointly synthesizes multiple semantically meaningful and geometrically distinct\n3D meshes from a single RGB image. Unlike existing methods that either produce\nmonolithic 3D shapes or follow two-stage pipelines, i.e., first segmenting an\nimage and then reconstructing each segment, PartCrafter adopts a unified,\ncompositional generation architecture that does not rely on pre-segmented\ninputs. Conditioned on a single image, it simultaneously denoises multiple 3D\nparts, enabling end-to-end part-aware generation of both individual objects and\ncomplex multi-object scenes. PartCrafter builds upon a pretrained 3D mesh\ndiffusion transformer (DiT) trained on whole objects, inheriting the pretrained\nweights, encoder, and decoder, and introduces two key innovations: (1) A\ncompositional latent space, where each 3D part is represented by a set of\ndisentangled latent tokens; (2) A hierarchical attention mechanism that enables\nstructured information flow both within individual parts and across all parts,\nensuring global coherence while preserving part-level detail during generation.\nTo support part-level supervision, we curate a new dataset by mining part-level\nannotations from large-scale 3D object datasets. Experiments show that\nPartCrafter outperforms existing approaches in generating decomposable 3D\nmeshes, including parts that are not directly visible in input images,\ndemonstrating the strength of part-aware generative priors for 3D understanding\nand synthesis. Code and training data will be released.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62e18206926f4892a4c782bd/iNc42ij-Kj0Z4V0jD5lCp.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05573.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62e18206926f4892a4c782bd",
      "avatarUrl": "/avatars/0f89091a5eb72165d2e860d15b339539.svg",
      "fullname": "Chenguo Lin",
      "name": "chenguolin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.06199",
      "authors": [
        {
          "_id": "684637733ec10bdd8ab4da66",
          "user": {
            "_id": "674b2406591d7232820252cd",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/bhDlUVkFQ66yt3BEVs6WU.png",
            "isPro": false,
            "fullname": "Hongyan Zhi",
            "user": "Hoyard",
            "type": "user"
          },
          "name": "Hongyan Zhi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-09T10:11:27.821Z",
          "hidden": false
        },
        {
          "_id": "684637733ec10bdd8ab4da67",
          "name": "Peihao Chen",
          "hidden": false
        },
        {
          "_id": "684637733ec10bdd8ab4da68",
          "name": "Siyuan Zhou",
          "hidden": false
        },
        {
          "_id": "684637733ec10bdd8ab4da69",
          "name": "Yubo Dong",
          "hidden": false
        },
        {
          "_id": "684637733ec10bdd8ab4da6a",
          "name": "Quanxi Wu",
          "hidden": false
        },
        {
          "_id": "684637733ec10bdd8ab4da6b",
          "name": "Lei Han",
          "hidden": false
        },
        {
          "_id": "684637733ec10bdd8ab4da6c",
          "name": "Mingkui Tan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-06T16:00:31.000Z",
      "submittedOnDailyAt": "2025-06-09T01:42:52.611Z",
      "title": "3DFlowAction : Modèle d'apprentissage pour les opérations de croisement de dimensionnement en monde 3D du flux",
      "submittedOnDailyBy": {
        "_id": "674b2406591d7232820252cd",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/bhDlUVkFQ66yt3BEVs6WU.png",
        "isPro": false,
        "fullname": "Hongyan Zhi",
        "user": "Hoyard",
        "type": "user"
      },
      "summary": "L'opération était durement difficile pour les robots, tandis que les humains peuvent interagir facilement avec des objets complexes. Par exemple, nous pouvons coller un vase à un microscope. La cause de cette difficulté réside dans la manque de grandes bases de données uniques nécessaires pour enseigner des habiletés de mouvement aux robots. Les bases de données actuelles des robots enregistrent des actions de robots dans des espaces d'action simples. Cela empêche l'apprentissage de représentations continues et fortes d'actions dans d'autres scénarios. Il a été découvert que la compréhension des tâches de mouvement par les humains et comment les objets se déplacent dans l'espace 3D sont essentiels pour l'instruction d'actions. Cette instruction ne dépend pas de la machine et est adaptée aux humains et à différents robots. Par conséquent, l'objectif est d'apprendre un modèle de monde 3D de flux à partir de données de mouvements humains et robots. Ce modèle prédit le mouvement futur des objets dans l'espace 3D et planifie les actions. Spécifiquement, un grand ensemble de données de flux optique 3D est généré par un processus automatique de détection de grands mouvements. Ensuite, un modèle de monde basé sur la diffusion de vidéo apprend la physique du mouvement à partir de ces données et génère un entraîneur de flux optique 3D basé sur des instructions linguistiques. Grâce au flux optique 3D généré, une structure de rendu de guide de flux est proposée et le statut final prédit est rendu, évaluant si le flux prédit coïncide avec la description de la tâche en utilisant GPT-4o. De cette manière, les robots acquièrent la capacité de planifier des plans d'action dans des environnements fermés. Enfin, le flux optique 3D prédit est optimisé sous les contraintes des politiques d'optimisation, en considérant les actions de robot nécessaires pour la tâche. Les expériences prolongées montrent une généralisation robuste dans différentes tâches de mouvement des robots et une adaptation émotionnelle de format fermé fiable, sans nécessité d'entraînement en matériel spécifique.",
      "upvotes": 3,
      "discussionId": "684637733ec10bdd8ab4da6d",
      "githubRepo": "https://github.com/Hoyyyaard/3DFlowAction/",
      "ai_summary": "A 3D flow world model learned from human and robot manipulation data, using video diffusion and GPT-4o, enables robots to perform diverse manipulation tasks with strong generalization and cross-embodiment adaptation.",
      "ai_keywords": [
        "3D flow world model",
        "moving object auto-detect pipeline",
        "video diffusion-based world model",
        "3D optical flow dataset",
        "ManiFlow-110k",
        "3D optical flow trajectories",
        "flow-guided rendering mechanism",
        "GPT-4o",
        "closed-loop planning",
        "optimization policy",
        "cross-embodiment adaptation"
      ]
    },
    "publishedAt": "2025-06-06T12:00:31.000Z",
    "title": "3DFlowAction: Learning Cross-Embodiment Manipulation from 3D Flow World\n  Model",
    "summary": "Manipulation has long been a challenging task for robots, while humans can\neffortlessly perform complex interactions with objects, such as hanging a cup\non the mug rack. A key reason is the lack of a large and uniform dataset for\nteaching robots manipulation skills. Current robot datasets often record robot\naction in different action spaces within a simple scene. This hinders the robot\nto learn a unified and robust action representation for different robots within\ndiverse scenes. Observing how humans understand a manipulation task, we find\nthat understanding how the objects should move in the 3D space is a critical\nclue for guiding actions. This clue is embodiment-agnostic and suitable for\nboth humans and different robots. Motivated by this, we aim to learn a 3D flow\nworld model from both human and robot manipulation data. This model predicts\nthe future movement of the interacting objects in 3D space, guiding action\nplanning for manipulation. Specifically, we synthesize a large-scale 3D optical\nflow dataset, named ManiFlow-110k, through a moving object auto-detect\npipeline. A video diffusion-based world model then learns manipulation physics\nfrom these data, generating 3D optical flow trajectories conditioned on\nlanguage instructions. With the generated 3D object optical flow, we propose a\nflow-guided rendering mechanism, which renders the predicted final state and\nleverages GPT-4o to assess whether the predicted flow aligns with the task\ndescription. This equips the robot with a closed-loop planning ability.\nFinally, we consider the predicted 3D optical flow as constraints for an\noptimization policy to determine a chunk of robot actions for manipulation.\nExtensive experiments demonstrate strong generalization across diverse robotic\nmanipulation tasks and reliable cross-embodiment adaptation without\nhardware-specific training.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.06199.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "674b2406591d7232820252cd",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/bhDlUVkFQ66yt3BEVs6WU.png",
      "fullname": "Hongyan Zhi",
      "name": "Hoyard",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.05433",
      "authors": [
        {
          "_id": "68469df13ec10bdd8ab4db92",
          "name": "Zikang Liu",
          "hidden": false
        },
        {
          "_id": "68469df13ec10bdd8ab4db93",
          "name": "Tongtian Yue",
          "hidden": false
        },
        {
          "_id": "68469df13ec10bdd8ab4db94",
          "name": "Yepeng Tang",
          "hidden": false
        },
        {
          "_id": "68469df13ec10bdd8ab4db95",
          "name": "Longteng Guo",
          "hidden": false
        },
        {
          "_id": "68469df13ec10bdd8ab4db96",
          "name": "Junxian Cai",
          "hidden": false
        },
        {
          "_id": "68469df13ec10bdd8ab4db97",
          "name": "Qingbin Liu",
          "hidden": false
        },
        {
          "_id": "68469df13ec10bdd8ab4db98",
          "name": "Xi Chen",
          "hidden": false
        },
        {
          "_id": "68469df13ec10bdd8ab4db99",
          "name": "Jing Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-05T09:13:37.000Z",
      "submittedOnDailyAt": "2025-06-09T07:17:09.252Z",
      "title": "Prefix Grouper : Utilisation de Préfixes Communs pour l'Entraînement Efficace de GRPO",
      "submittedOnDailyBy": {
        "_id": "6448dcf1b6ac93fe6512e342",
        "avatarUrl": "/avatars/a6441f89eabd156181bafc47c0b2f8c8.svg",
        "isPro": false,
        "fullname": "Zikang Liu",
        "user": "JohnCage",
        "type": "user"
      },
      "summary": "Le Groupe de Comparaison de Politiques Optimisées (GCPO) calcule les avantages de manière efficace à partir de la comparaison relative des entrées de préfixe commun partagé. GCPO est efficace, mais lorsqu'il s'agit de traiter de longs préfixes communs, chaque membre du groupe doit les codifier de manière étendue, ce qui augmente significativement la charge de calcul. Cet inconvénient est particulièrement significatif dans l'apprentissage de contextes longs, où il devient l'une des principales limitations d'échelle. Nous proposons l'algorithme d'entraînement efficace GCPO appelé Prefix Grouper, utilisant la stratégie de Forward avec Préfixe Commun pour éliminer les calculs étendus de préfixes. Spécifiquement, nous réconfigureons l'attention automatique en deux parties, ce qui permet de codifier un seul fois le préfixe commun et de maintenir la compatibilité avec l'entraînement de tous les gradients et points de sortie. Théoriquement et expérimentalement, Prefix Grouper se comporte comme le GCPO standard : obtient le même sortie de forward et rétropropagation, et garantit que le rendement du calcul final du valeur ne soit pas affecté. Expérimentalement, nos résultats montrent que Prefix Grouper obtient des résultats cohérents, réduisant significativement les coûts de calcul d'entraînement, spécialement lorsqu'il s'agit de longs préfixes. Le méthode est complètement pluggable : coïncide avec l'architecture GCPO actuelle, nécessite seulement des changements minimes dans la configuration d'entrée et du calcul d'attention, et ne nécessite pas de modifications structurelles. Prefix Grouper permet l'utilisation de grands groupes dans la même calcul, améliorant l'échelle de GCPO pour des tâches complexes ou des modèles grands. Le code est disponible sur https://github.com/johncaged/PrefixGrouper.",
      "upvotes": 2,
      "discussionId": "68469df13ec10bdd8ab4db9a",
      "githubRepo": "https://github.com/johncaged/PrefixGrouper",
      "ai_summary": "Prefix Grouper reduces computational overhead in GRPO by encoding shared prefixes only once, improving scalability in long-context scenarios without altering training dynamics or policy performance.",
      "ai_keywords": [
        "Group Relative Policy Optimization (GRPO)",
        "self-attention",
        "Shared-Prefix Forward strategy",
        "computational overhead",
        "long-context learning scenarios",
        "differentiability",
        "end-to-end training",
        "training-equivalent"
      ]
    },
    "publishedAt": "2025-06-05T05:13:37.000Z",
    "title": "Prefix Grouper: Efficient GRPO Training through Shared-Prefix Forward",
    "summary": "Group Relative Policy Optimization (GRPO) enhances policy learning by\ncomputing gradients from relative comparisons among candidate outputs that\nshare a common input prefix. Despite its effectiveness, GRPO introduces\nsubstantial computational overhead when processing long shared prefixes, which\nmust be redundantly encoded for each group member. This inefficiency becomes a\nmajor scalability bottleneck in long-context learning scenarios. We propose\nPrefix Grouper, an efficient GRPO training algorithm that eliminates redundant\nprefix computation via a Shared-Prefix Forward strategy. In particular, by\nrestructuring self-attention into two parts, our method enables the shared\nprefix to be encoded only once, while preserving full differentiability and\ncompatibility with end-to-end training. We provide both theoretical and\nempirical evidence that Prefix Grouper is training-equivalent to standard GRPO:\nit yields identical forward outputs and backward gradients, ensuring that the\noptimization dynamics and final policy performance remain unchanged.\nEmpirically, our experiments confirm that Prefix Grouper achieves consistent\nresults while significantly reducing the computational cost of training,\nparticularly in long-prefix scenarios. The proposed method is fully\nplug-and-play: it is compatible with existing GRPO-based architectures and can\nbe seamlessly integrated into current training pipelines as a drop-in\nreplacement, requiring no structural modifications and only minimal changes to\ninput construction and attention computation. Prefix Grouper enables the use of\nlarger group sizes under the same computational budget, thereby improving the\nscalability of GRPO to more complex tasks and larger models. Code is now\navailable at https://github.com/johncaged/PrefixGrouper",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05433.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6448dcf1b6ac93fe6512e342",
      "avatarUrl": "/avatars/a6441f89eabd156181bafc47c0b2f8c8.svg",
      "fullname": "Zikang Liu",
      "name": "JohnCage",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.04255",
      "authors": [
        {
          "_id": "684312988f9ec8394c514883",
          "user": {
            "_id": "65c43d6d2b723dbc4ddc29d2",
            "avatarUrl": "/avatars/ffd685be7f309866c38a164245a917aa.svg",
            "isPro": false,
            "fullname": "Kunal Pai",
            "user": "guineapig",
            "type": "user"
          },
          "name": "Kunal Pai",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-07T05:45:12.032Z",
          "hidden": false
        },
        {
          "_id": "684312988f9ec8394c514884",
          "user": {
            "_id": "62a0dbe7bff710e3fb05f9ae",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62a0dbe7bff710e3fb05f9ae/uZK0Zkv7YG7jWbweh5tQb.png",
            "isPro": false,
            "fullname": "Parth Shah",
            "user": "helloparthshah",
            "type": "user"
          },
          "name": "Parth Shah",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-06T16:08:57.423Z",
          "hidden": false
        },
        {
          "_id": "684312988f9ec8394c514885",
          "name": "Harshil Patel",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-01T17:33:16.000Z",
      "submittedOnDailyAt": "2025-06-09T03:49:09.306Z",
      "title": "Hyirae Key Agent System Hybrid Intelligent Resource Utilization",
      "submittedOnDailyBy": {
        "_id": "65c43d6d2b723dbc4ddc29d2",
        "avatarUrl": "/avatars/ffd685be7f309866c38a164245a917aa.svg",
        "isPro": false,
        "fullname": "Kunal Pai",
        "user": "guineapig",
        "type": "user"
      },
      "summary": "Le rapide développement des Modèles de Langue Grands (LLM) est entraînant le développement de systèmes de conversation multi-agente (MAS). Cependant, les cadres actuels sont limités en flexibilité, reconnaissance de ressources, diversité de modèles et développement de outils de conversion automatique. Dans cet article, nous présentons \"HASHIRU\" (Système d'Agents Heuristiques pour l'Utilisation Intelligente de Ressources), un nouveau cadre de travail de MAS. Ce cadre est conçu pour améliorer la flexibilité, efficacité des ressources et adaptabilité. HASHIRU est caractérisé par le management dynamique d'agents \"EMPOI\" (Experts Multi-Purpose) basés sur la nécessité de la tâche et les limites des ressources (coût, mémoire). Ce hybride d'intelligence utilise de manière flexible de petits modèles locaux et, selon la nécessité, des API externes et grands modèles, fournis par Ollama. Un modèle économique de contrat/libération incite la stabilité de l'équipe et l'efficience de la distribution des ressources. Ce système inclut des outils de conversion automatique et des fonctions de mémoire. HASHIRU a été évalué sur différentes tâches, démontrant ses capacités en évaluation académique (58% de succès), sécurité (100% de succès sur certains points de JailbreakBench), évaluation de raisonnement complexe (GSM8K: 96% vs. 61%; JEEBench: 80% vs. 68.3%; SVAMP: 92% vs. 84%). Les études de cas montrent la génération du modèle de coût de conversion automatique, l'intégration d'outils et l'amélioration automatique par gestion de versions. HASHIRU offre un contrôle heuristique dynamique, une intelligence hybride de reconnaissance de ressources et une extension des fonctions de conversion automatique, fournissant un MAS plus robuste, efficace et adaptable. Le code source et les benchmarks sont disponibles sur https://github.com/HASHIRU-AI/HASHIRU et https://github.com/HASHIRU-AI/HASHIRUBench. Pour consulter sur la démonstration gratuite, vous pouvez contacter par https://hashiruagentx-hashiruai.hf.space.",
      "upvotes": 2,
      "discussionId": "684312998f9ec8394c514886",
      "githubRepo": "https://github.com/HASHIRU-AI/HASHIRU",
      "ai_summary": "HASHIRU, a novel MAS framework, enhances flexibility, resource efficiency, and adaptability by dynamically managing specialized agents and using a hybrid intelligence approach with smaller, local LLMs and external APIs.",
      "ai_keywords": [
        "Hierarchical Agent System",
        "Hybrid Intelligent Resource Utilization",
        "HASHIRU",
        "CEO agent",
        "employee agents",
        "Ollama",
        "external APIs",
        "economic model",
        "hiring/firing costs",
        "autonomous API tool creation",
        "academic paper review",
        "safety assessments",
        "GSM8K",
        "JEEBench",
        "SVAMP",
        "Gemini 2.0 Flash",
        "self-improvement",
        "autonomous cost model generation",
        "tool integration",
        "budget management"
      ]
    },
    "publishedAt": "2025-06-01T13:33:16.000Z",
    "title": "HASHIRU: Hierarchical Agent System for Hybrid Intelligent Resource\n  Utilization",
    "summary": "Rapid Large Language Model (LLM) advancements are fueling autonomous\nMulti-Agent System (MAS) development. However, current frameworks often lack\nflexibility, resource awareness, model diversity, and autonomous tool creation.\nThis paper introduces HASHIRU (Hierarchical Agent System for Hybrid Intelligent\nResource Utilization), a novel MAS framework enhancing flexibility, resource\nefficiency, and adaptability. HASHIRU features a \"CEO\" agent dynamically\nmanaging specialized \"employee\" agents, instantiated based on task needs and\nresource constraints (cost, memory). Its hybrid intelligence prioritizes\nsmaller, local LLMs (via Ollama) while flexibly using external APIs and larger\nmodels when necessary. An economic model with hiring/firing costs promotes team\nstability and efficient resource allocation. The system also includes\nautonomous API tool creation and a memory function. Evaluations on tasks like\nacademic paper review (58% success), safety assessments (100% on a\nJailbreakBench subset), and complex reasoning (outperforming Gemini 2.0 Flash\non GSM8K: 96% vs. 61%; JEEBench: 80% vs. 68.3%; SVAMP: 92% vs. 84%) demonstrate\nHASHIRU's capabilities. Case studies illustrate its self-improvement via\nautonomous cost model generation, tool integration, and budget management.\nHASHIRU offers a promising approach for more robust, efficient, and adaptable\nMAS through dynamic hierarchical control, resource-aware hybrid intelligence,\nand autonomous functional extension. Source code and benchmarks are available\nat https://github.com/HASHIRU-AI/HASHIRU and\nhttps://github.com/HASHIRU-AI/HASHIRUBench respectively, and a live demo is\navailable at https://hashiruagentx-hashiruai.hf.space upon request.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04255.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65c43d6d2b723dbc4ddc29d2",
      "avatarUrl": "/avatars/ffd685be7f309866c38a164245a917aa.svg",
      "fullname": "Kunal Pai",
      "name": "guineapig",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  }
]