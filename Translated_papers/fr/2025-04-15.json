[
  {
    "paper": {
      "id": "2504.10479",
      "authors": [
        {
          "_id": "67fdd08bda7816922cb67e54",
          "name": "Jinguo Zhu",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e55",
          "user": {
            "_id": "619507e7b74b6c591f794340",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/619507e7b74b6c591f794340/JbPDoy6Ko1V1-6oJJwFV8.jpeg",
            "isPro": false,
            "fullname": "Weiyun Wang",
            "user": "Weiyun1025",
            "type": "user"
          },
          "name": "Weiyun Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:09:23.250Z",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e56",
          "name": "Zhe Chen",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e57",
          "name": "Zhaoyang Liu",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e58",
          "user": {
            "_id": "64804866c7f87934d082bb25",
            "avatarUrl": "/avatars/41761226c79ac16e48d4c4cb84362adb.svg",
            "isPro": false,
            "fullname": "Yeshenglong",
            "user": "Yeshenglong",
            "type": "user"
          },
          "name": "Shenglong Ye",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:09:57.293Z",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e59",
          "user": {
            "_id": "6541efc9109d78427198ea40",
            "avatarUrl": "/avatars/c1252dd7da2e53b0b6757bf392139cdf.svg",
            "isPro": false,
            "fullname": "Lixin Gu",
            "user": "gulixin0922",
            "type": "user"
          },
          "name": "Lixin Gu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:10:04.312Z",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e5a",
          "user": {
            "_id": "6495a4d6d1cec0e2c2c96cdd",
            "avatarUrl": "/avatars/e8b42301514094c8af6fa1e5fcb53119.svg",
            "isPro": false,
            "fullname": "Duan Yuchen",
            "user": "duanyuchen",
            "type": "user"
          },
          "name": "Yuchen Duan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:10:30.238Z",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e5b",
          "name": "Hao Tian",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e5c",
          "user": {
            "_id": "63e4562f9db5da2dc1f3b520",
            "avatarUrl": "/avatars/f4eecf1396b05e1c72436e7026d85cef.svg",
            "isPro": false,
            "fullname": "Weijie Su",
            "user": "jackroos",
            "type": "user"
          },
          "name": "Weijie Su",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:10:39.196Z",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e5d",
          "name": "Jie Shao",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e5e",
          "name": "Zhangwei Gao",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e5f",
          "user": {
            "_id": "6579b818563044badca392fc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6579b818563044badca392fc/XTKQ9Lhceibp9dnQADPQF.jpeg",
            "isPro": false,
            "fullname": "cuierfei",
            "user": "cuierfei",
            "type": "user"
          },
          "name": "Erfei Cui",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:10:59.950Z",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e60",
          "user": {
            "_id": "6571382c7644d1128561cebe",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/bnJ-T3k_w1g1Mr7b7LglK.jpeg",
            "isPro": false,
            "fullname": "Cao Yue",
            "user": "yuecao0119",
            "type": "user"
          },
          "name": "Yue Cao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:11:07.485Z",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e61",
          "name": "Yangzhou Liu",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e62",
          "name": "Weiye Xu",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e63",
          "name": "Hao Li",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e64",
          "name": "Jiahao Wang",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e65",
          "user": {
            "_id": "67c6fd7d85d2167189fce0e4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/qpvzOJWcUJvD03l5yTBcN.jpeg",
            "isPro": false,
            "fullname": "Han Lv",
            "user": "Hanrandom",
            "type": "user"
          },
          "name": "Han Lv",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:11:58.666Z",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e66",
          "name": "Dengnian Chen",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e67",
          "user": {
            "_id": "6751a10224431db3bed1f701",
            "avatarUrl": "/avatars/06dcc5ba6cec8aff217a8bbc7a7d3b73.svg",
            "isPro": false,
            "fullname": "Songze Li",
            "user": "CatCatCat36",
            "type": "user"
          },
          "name": "Songze Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:12:32.493Z",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e68",
          "user": {
            "_id": "65b9d9961fe588f824fde191",
            "avatarUrl": "/avatars/a9245958cc998a4b4b870bf2490fdaee.svg",
            "isPro": false,
            "fullname": "Yinan He",
            "user": "yinanhe",
            "type": "user"
          },
          "name": "Yinan He",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:12:39.591Z",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e69",
          "name": "Tan Jiang",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e6a",
          "user": {
            "_id": "647ea4aae4d52fe0e021bce4",
            "avatarUrl": "/avatars/9e68004d04403acff004113c856451a6.svg",
            "isPro": false,
            "fullname": "Jiapeng Luo",
            "user": "woolpeeker",
            "type": "user"
          },
          "name": "Jiapeng Luo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:12:51.845Z",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e6b",
          "name": "Yi Wang",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e6c",
          "user": {
            "_id": "63f9fca8d4349b157a109eec",
            "avatarUrl": "/avatars/fa1f2ae7972d7cde99dab178136ccbb0.svg",
            "isPro": false,
            "fullname": "Conghui He",
            "user": "conghui",
            "type": "user"
          },
          "name": "Conghui He",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:13:07.750Z",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e6d",
          "name": "Botian Shi",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e6e",
          "name": "Xingcheng Zhang",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e6f",
          "user": {
            "_id": "64b3fd42eec33e27dcc4c941",
            "avatarUrl": "/avatars/5aa1a99468fa61d4b8b0e80b592c4e55.svg",
            "isPro": false,
            "fullname": "Wenqi Shao",
            "user": "wqshao126",
            "type": "user"
          },
          "name": "Wenqi Shao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:15:27.767Z",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e70",
          "user": {
            "_id": "66b593026cef22e6ba6adb8a",
            "avatarUrl": "/avatars/8a94d55b85177e84d65dd0bd537e335f.svg",
            "isPro": false,
            "fullname": "JunjunHe",
            "user": "JunjunHe",
            "type": "user"
          },
          "name": "Junjun He",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:15:19.980Z",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e71",
          "user": {
            "_id": "654395472cfe8660a3a492bb",
            "avatarUrl": "/avatars/2010370112200e83ced979d3d4c87735.svg",
            "isPro": false,
            "fullname": "xiong",
            "user": "xiongyingtong",
            "type": "user"
          },
          "name": "Yingtong Xiong",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:15:12.302Z",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e72",
          "user": {
            "_id": "6684eb7045d8ceb446ffe9ff",
            "avatarUrl": "/avatars/30586f289031696253842fcd4a8788b9.svg",
            "isPro": false,
            "fullname": "wenwenQu",
            "user": "wenwenQu",
            "type": "user"
          },
          "name": "Wenwen Qu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:14:47.178Z",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e73",
          "name": "Peng Sun",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e74",
          "name": "Penglong Jiao",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e75",
          "name": "Lijun Wu",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e76",
          "user": {
            "_id": "63527f4e7d071f23d085ad45",
            "avatarUrl": "/avatars/99a51adef5673b3ac1a8c02eb47759c4.svg",
            "isPro": false,
            "fullname": "KAIPENG ZHANG",
            "user": "kpzhang",
            "type": "user"
          },
          "name": "Kaipeng Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:13:25.647Z",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e77",
          "name": "Huipeng Deng",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e78",
          "name": "Jiaye Ge",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e79",
          "name": "Kai Chen",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e7a",
          "name": "Limin Wang",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e7b",
          "name": "Min Dou",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e7c",
          "user": {
            "_id": "65ead3ea908526a39082e641",
            "avatarUrl": "/avatars/dcf870695fd56b06ca03d82f831e9019.svg",
            "isPro": false,
            "fullname": "Lewei Lu",
            "user": "luotto",
            "type": "user"
          },
          "name": "Lewei Lu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:14:05.716Z",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e7d",
          "name": "Xizhou Zhu",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e7e",
          "name": "Tong Lu",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e7f",
          "user": {
            "_id": "636317ed80c1a705a6eff396",
            "avatarUrl": "/avatars/3db090e101b916d9256d0d3e043db71d.svg",
            "isPro": false,
            "fullname": "Dahua Lin",
            "user": "lindahua",
            "type": "user"
          },
          "name": "Dahua Lin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:13:59.084Z",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e80",
          "name": "Yu Qiao",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e81",
          "name": "Jifeng Dai",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e82",
          "user": {
            "_id": "64d1c560c0c627dfa71bdbe0",
            "avatarUrl": "/avatars/f42794fe25bffcd870a1bcee69b95298.svg",
            "isPro": false,
            "fullname": "wenhai.wang",
            "user": "wangwhcore",
            "type": "user"
          },
          "name": "Wenhai Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:13:51.523Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/yA_DVt5PKVaFjiY-E93e5.png",
        "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/94PuKH6M3nhmkVS8RXtb4.png",
        "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/qvaW8f5868-P75pyzr6XT.png",
        "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/4GqLJBNhM2rUqS_4mawkk.png",
        "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/yBSGw85-QcThBlXv2tLj0.png",
        "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/CHvbRf2VJg6amEGS9OlEE.png",
        "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/VCBvp4nhvzfokH-viUz1g.png",
        "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/HIC3iirmK6jaOax4GlDYt.png",
        "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/TLlWnFPBguYJsvuGGqykz.png",
        "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/xJ-sFLuCkv1bqL_TXwxgH.png",
        "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/Hsv_hehORgnRmL68xSqmd.png",
        "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/S1fnELtJWFU_FlEozV0Ik.png"
      ],
      "publishedAt": "2025-04-14T17:59:25.000Z",
      "submittedOnDailyAt": "2025-04-15T01:57:22.403Z",
      "title": "InternVL3 : Recherche de recettes pour l'entraînement et les tests avancés dans un modèle multimodal ouvert-code",
      "submittedOnDailyBy": {
        "_id": "619507e7b74b6c591f794340",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/619507e7b74b6c591f794340/JbPDoy6Ko1V1-6oJJwFV8.jpeg",
        "isPro": false,
        "fullname": "Weiyun Wang",
        "user": "Weiyun1025",
        "type": "user"
      },
      "summary": "InternVL3 est un importante avance dans la série InternVL. Ce modèle s'appuie sur un paradigme de pré-traitement naturel de plusieurs modèles. InternVL3 adapte un grand modèle de langage nature (LLM) qui ne comprend que du texte à un grand modèle de langage nature de plusieurs modèles (MLLM) qui inclut des entrées visuelles, sans nécessité d'adaptation. Ce modèle apprend une série d'étapes de pré-traitement permettant d'obtenir de deux sources différentes : des données de plusieurs modèles et des corpus de texte, plusieurs modèles et habiletés linguistiques. Ce paradigme d'entraînement intégré résout efficacement les problèmes de complexité et d'alignement observés dans les flux traditionnels d'entraînement de MLLM. Il utilise la codification de position visuelle (V2PE) pour élargir le contexte de plusieurs modèles et améliorer l'efficacité et la scalabilité, appliquant des techniques avancées d'entraînement comme le fine-tuning standard (SFT) et l'optimisation de préférence par confusion (MPO), et utilisant une infrastructure d'entraînement optimisée qui inclut une scalabilité en entraînement. Il montre des résultats excellents sur diverses tâches de plusieurs modèles. En particulier, InternVL3-78B atteint un score de 72,2 sur le benchmark MMMU, occupant une nouvelle position de leader parmi les modèles de MLLM open-source. Sa capacité est compétitive avec des modèles leaders comme ChatGPT-4o, Claude 3.5 Sonnet et Gemini 2.5 Pro, et maintient une forte capacité textuelle. Il suive les principes de science ouverte, publiant des données d'entraînement et des poids du modèle pour connecter avec la recherche et le développement des prochaines générations de MLLM.",
      "upvotes": 135,
      "discussionId": "67fdd08cda7816922cb67ec2",
      "projectPage": "https://internvl.github.io/blog/2025-04-11-InternVL-3.0/",
      "githubRepo": "https://github.com/OpenGVLab/InternVL"
    },
    "publishedAt": "2025-04-14T13:59:25.000Z",
    "title": "InternVL3: Exploring Advanced Training and Test-Time Recipes for\n  Open-Source Multimodal Models",
    "summary": "We introduce InternVL3, a significant advancement in the InternVL series\nfeaturing a native multimodal pre-training paradigm. Rather than adapting a\ntext-only large language model (LLM) into a multimodal large language model\n(MLLM) that supports visual inputs, InternVL3 jointly acquires multimodal and\nlinguistic capabilities from both diverse multimodal data and pure-text corpora\nduring a single pre-training stage. This unified training paradigm effectively\naddresses the complexities and alignment challenges commonly encountered in\nconventional post-hoc training pipelines for MLLMs. To further improve\nperformance and scalability, InternVL3 incorporates variable visual position\nencoding (V2PE) to support extended multimodal contexts, employs advanced\npost-training techniques such as supervised fine-tuning (SFT) and mixed\npreference optimization (MPO), and adopts test-time scaling strategies\nalongside an optimized training infrastructure. Extensive empirical evaluations\ndemonstrate that InternVL3 delivers superior performance across a wide range of\nmulti-modal tasks. In particular, InternVL3-78B achieves a score of 72.2 on the\nMMMU benchmark, setting a new state-of-the-art among open-source MLLMs. Its\ncapabilities remain highly competitive with leading proprietary models,\nincluding ChatGPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro, while also\nmaintaining strong pure-language proficiency. In pursuit of open-science\nprinciples, we will publicly release both the training data and model weights\nto foster further research and development in next-generation MLLMs.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/yA_DVt5PKVaFjiY-E93e5.png",
      "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/94PuKH6M3nhmkVS8RXtb4.png",
      "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/qvaW8f5868-P75pyzr6XT.png",
      "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/4GqLJBNhM2rUqS_4mawkk.png",
      "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/yBSGw85-QcThBlXv2tLj0.png",
      "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/CHvbRf2VJg6amEGS9OlEE.png",
      "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/VCBvp4nhvzfokH-viUz1g.png",
      "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/HIC3iirmK6jaOax4GlDYt.png",
      "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/TLlWnFPBguYJsvuGGqykz.png",
      "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/xJ-sFLuCkv1bqL_TXwxgH.png",
      "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/Hsv_hehORgnRmL68xSqmd.png",
      "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/S1fnELtJWFU_FlEozV0Ik.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10479.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "619507e7b74b6c591f794340",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/619507e7b74b6c591f794340/JbPDoy6Ko1V1-6oJJwFV8.jpeg",
      "fullname": "Weiyun Wang",
      "name": "Weiyun1025",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 17
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.08791",
      "authors": [
        {
          "_id": "67fdbde764a418633ee9fa1b",
          "user": {
            "_id": "647466b8b68461d5cf795e3c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647466b8b68461d5cf795e3c/zaK6sdCbdPfYu14vg2Ty6.png",
            "isPro": false,
            "fullname": "LIKirin",
            "user": "LIKirin",
            "type": "user"
          },
          "name": "Zonghang Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-15T07:54:53.716Z",
          "hidden": false
        },
        {
          "_id": "67fdbde764a418633ee9fa1c",
          "user": {
            "_id": "669e0c108b279f0a2704a5ba",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/669e0c108b279f0a2704a5ba/uPyioyK0RWSG7CT52xBao.png",
            "isPro": false,
            "fullname": "TaoLi",
            "user": "LiPhilip",
            "type": "user"
          },
          "name": "Tao Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-15T07:54:50.518Z",
          "hidden": false
        },
        {
          "_id": "67fdbde764a418633ee9fa1d",
          "user": {
            "_id": "6513c6c9c1fbded3b1977acc",
            "avatarUrl": "/avatars/d169b3462f952c5639e1471ed8bf84c9.svg",
            "isPro": false,
            "fullname": "Wenjiao Feng",
            "user": "NeuronNomad",
            "type": "user"
          },
          "name": "Wenjiao Feng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:15:55.453Z",
          "hidden": false
        },
        {
          "_id": "67fdbde764a418633ee9fa1e",
          "name": "Mohsen Guizani",
          "hidden": false
        },
        {
          "_id": "67fdbde764a418633ee9fa1f",
          "name": "Hongfang Yu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/647466b8b68461d5cf795e3c/Z-XPxpdhg3sBxHCfogGzf.mp4"
      ],
      "publishedAt": "2025-04-07T13:46:21.000Z",
      "submittedOnDailyAt": "2025-04-15T00:38:03.208Z",
      "title": "PRIMA.CPP : Méthode pour accélérer la vitesse d'inférence d'un modèle de langage génératif de 70B dans un cluster de faibles ressources quotidiennement",
      "submittedOnDailyBy": {
        "_id": "647466b8b68461d5cf795e3c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647466b8b68461d5cf795e3c/zaK6sdCbdPfYu14vg2Ty6.png",
        "isPro": false,
        "fullname": "LIKirin",
        "user": "LIKirin",
        "type": "user"
      },
      "summary": "Las mesures d'urgence de DeepSeek R1 et QwQ 32B ont résolu les problèmes de performance pour exécuter les plus avancés modèles de langage cérébral (LLMs) sur des dispositifs de maison. Alors que l'hardware consommateur s'améliore et la quantification des modèles s'améliore, les solutions actuelles des terminals nécessitent un cluster de GPU, de grandes mémoires RAM/VRAM et une haute vitesse de bande, ce qui rend difficile pour les clústers de maison généraux de s'appuyer. Dans cet article, nous présentons un système d'inférence distribuée appelé \"prima.cpp\" qui permet d'exécuter des modèles de 70B sur des dispositifs de maison tous les jours. Ce système combine CPU/GPU, mémoires RAM/VRAM de faible consommation, Wi-Fi et support multi-plateforme. Il gère les poids du modèle avec mmap, introduit des fonctions de calcul parallèle et de lecture réservée pour masquer la lecture du disque. Il modélise l'inégalité du calcul, de la communication, du disque et de la mémoire (y compris sa gestion), et assigne adéquatement les couches du modèle aux CPU et GPU de chaque dispositif pour réduire le temps d'exécution de différents types de tokens. Pour résoudre ce problème NP-hard, nous proposons l'algorithme surprisant Halda. Dans des évaluations sur un clúster de 4 nœuds, prima.cpp dépasse llama.cpp, exo et dllama pour des modèles de plus de 30B. Cependant, il peut contrôler la demande de mémoire en 6% ou moins. Cela permet d'introduire des modèles avancés comme Llama 3, DeepSeek R1, Qwen 2.5 et QwQ comme assistants de maison, rendant la technologie avancée en IA une réalité pour la personne. Le code est disponible sous licence open source et peut être accédé sur https://github.com/Lizonghang/prima.cpp.",
      "upvotes": 75,
      "discussionId": "67fdbdeb64a418633ee9fb58",
      "projectPage": "https://github.com/Lizonghang/prima.cpp",
      "githubRepo": "https://github.com/Lizonghang/prima.cpp"
    },
    "publishedAt": "2025-04-07T09:46:21.000Z",
    "title": "PRIMA.CPP: Speeding Up 70B-Scale LLM Inference on Low-Resource Everyday\n  Home Clusters",
    "summary": "Emergency of DeepSeek R1 and QwQ 32B have broken through performance barriers\nfor running frontier large language models (LLMs) on home devices. While\nconsumer hardware is getting stronger and model quantization is improving,\nexisting end-side solutions still demand GPU clusters, large RAM/VRAM, and high\nbandwidth, far beyond what a common home cluster can handle. This paper\nintroduces prima.cpp, a distributed inference system that runs 70B-scale models\non everyday home devices using a mix of CPU/GPU, low RAM/VRAM, Wi-Fi, and\ncross-platform support. It uses mmap to manage model weights and introduces\npiped-ring parallelism with prefetching to hide disk loading. By modeling\nheterogeneity in computation, communication, disk, memory (and its management\nbehavior), and OS, it optimally assigns model layers to each device's CPU and\nGPU, further reducing token latency. An elegant algorithm named Halda is\nproposed to solve this NP-hard assignment problem. We evaluate prima.cpp on a\ncommon four-node home cluster. It outperforms llama.cpp, exo, and dllama on\n30B+ models while keeping memory pressure below 6%. This brings frontier\n30B-70B models, such as Llama 3, DeepSeek R1, Qwen 2.5, and QwQ to home\nassistants, making advanced AI truly accessible to individuals. The code is\nopen source and available at https://github.com/Lizonghang/prima.cpp.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/647466b8b68461d5cf795e3c/Z-XPxpdhg3sBxHCfogGzf.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08791.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "647466b8b68461d5cf795e3c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647466b8b68461d5cf795e3c/zaK6sdCbdPfYu14vg2Ty6.png",
      "fullname": "LIKirin",
      "name": "LIKirin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.09925",
      "authors": [
        {
          "_id": "67fdb5f1913c97aa32f130bd",
          "user": {
            "_id": "6625ef13605f46d05c1d0031",
            "avatarUrl": "/avatars/22f201dca35e43013cb593884516e96c.svg",
            "isPro": false,
            "fullname": "Zheng Liu",
            "user": "starriver030515",
            "type": "user"
          },
          "name": "Zheng Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-15T07:54:57.129Z",
          "hidden": false
        },
        {
          "_id": "67fdb5f1913c97aa32f130be",
          "user": {
            "_id": "672dbf1bec84b9c33412488f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/V6o6K28ydpkaS9XFA4Ac3.png",
            "isPro": false,
            "fullname": "Mengjie Liu",
            "user": "Balalauuoo",
            "type": "user"
          },
          "name": "Mengjie Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:16:15.647Z",
          "hidden": false
        },
        {
          "_id": "67fdb5f1913c97aa32f130bf",
          "name": "Jingzhou Chen",
          "hidden": false
        },
        {
          "_id": "67fdb5f1913c97aa32f130c0",
          "user": {
            "_id": "672da19bb2f2dc21e176b0de",
            "avatarUrl": "/avatars/a942ad1c467b865cb9530927fe13f2b7.svg",
            "isPro": false,
            "fullname": "Jingwei Xu",
            "user": "jingwei-xu-00",
            "type": "user"
          },
          "name": "Jingwei Xu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:16:53.642Z",
          "hidden": false
        },
        {
          "_id": "67fdb5f1913c97aa32f130c1",
          "name": "Bin Cui",
          "hidden": false
        },
        {
          "_id": "67fdb5f1913c97aa32f130c2",
          "user": {
            "_id": "63f9fca8d4349b157a109eec",
            "avatarUrl": "/avatars/fa1f2ae7972d7cde99dab178136ccbb0.svg",
            "isPro": false,
            "fullname": "Conghui He",
            "user": "conghui",
            "type": "user"
          },
          "name": "Conghui He",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:17:01.171Z",
          "hidden": false
        },
        {
          "_id": "67fdb5f1913c97aa32f130c3",
          "name": "Wentao Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-14T06:33:29.000Z",
      "submittedOnDailyAt": "2025-04-15T00:17:19.508Z",
      "title": "FUSION : Intégration Complète de la Représentation du Langage Visuel pour un Entendage Croisé Profond",
      "submittedOnDailyBy": {
        "_id": "6625ef13605f46d05c1d0031",
        "avatarUrl": "/avatars/22f201dca35e43013cb593884516e96c.svg",
        "isPro": false,
        "fullname": "Zheng Liu",
        "user": "starriver030515",
        "type": "user"
      },
      "summary": "FUSION est une famille de modèles de langage naturel multimodal (MLLM) basée sur un paradigme complet d'enrichissement et d'intégration de langage visuel. Au contraire des méthodes actuelles, notre méthodologie ne dépend pas principalement de l'interaction entre modèles lors de l'entraînement des modèles de langage grands (LLM). Notre approche permet une intégration dynamique profonde tout au long du processus de traitement. Dans ce contexte, nous proposons l'unité de matching de texte avec codification de business, et nous intéguons des informations textuelles dans le codéur visuel pour atteindre une intégration au niveau de pixels. De plus, nous concevons une décision d'enrichissement récursif dans le contexte, réduisant récursivement les caractéristiques visuelles basées sur le contexte textuel lors de l'entraînement, ce qui permet l'intégration de sens au niveau des problèmes spécifiques. Pour atténuer les différences dans le mapping sémantique et améliorer la caractéristique du mapping, nous développons une perte de mapping sémantique de sous-objets double. De plus, nous utilisons de nouvelles techniques de synthèse de données pour construire un ensemble de données de questions et réponses (QA) de leadership de langage, optimisant l'intégration de caractéristiques dans l'unité de matching de texte. Sur la base de cette base, nous entraînons FUSION à deux échelles : 3B et 8B. Notre approche d'intégration complète des modèles est plus efficace que les méthodes actuelles, en utilisant 630 tokens visuels supplémentaires et montrant un rendement significativement meilleur. En particulier, FUSION 3B dépasse Florence-VL 8B et Cambrian-1 8B dans presque tous les benchmarks. FUSION 3B dépasse Cambrian-1 8B même sous la restriction de 300 tokens visuels. Notre étude de disparition démontre la validité de notre approche dans les mêmes conditions. Nous publions le code, les poids du modèle et l'ensemble de données sur : https://github.com/starriver030515/FUSION",
      "upvotes": 27,
      "discussionId": "67fdb5f3913c97aa32f13141",
      "githubRepo": "https://github.com/starriver030515/FUSION"
    },
    "publishedAt": "2025-04-14T02:33:29.000Z",
    "title": "FUSION: Fully Integration of Vision-Language Representations for Deep\n  Cross-Modal Understanding",
    "summary": "We introduce FUSION, a family of multimodal large language models (MLLMs)\nwith a fully vision-language alignment and integration paradigm. Unlike\nexisting methods that primarily rely on late-stage modality interaction during\nLLM decoding, our approach achieves deep, dynamic integration throughout the\nentire processing pipeline. To this end, we propose Text-Guided Unified Vision\nEncoding, incorporating textual information in vision encoding to achieve\npixel-level integration. We further design Context-Aware Recursive Alignment\nDecoding that recursively aggregates visual features conditioned on textual\ncontext during decoding, enabling fine-grained, question-level semantic\nintegration. To guide feature mapping and mitigate modality discrepancies, we\ndevelop Dual-Supervised Semantic Mapping Loss. Additionally, we construct a\nSynthesized Language-Driven Question-Answer (QA) dataset through a new data\nsynthesis method, prioritizing high-quality QA pairs to optimize text-guided\nfeature integration. Building on these foundations, we train FUSION at two\nscales-3B, 8B-and demonstrate that our full-modality integration approach\nsignificantly outperforms existing methods with only 630 vision tokens.\nNotably, FUSION 3B surpasses Cambrian-1 8B and Florence-VL 8B on most\nbenchmarks. FUSION 3B continues to outperform Cambrian-1 8B even when limited\nto 300 vision tokens. Our ablation studies show that FUSION outperforms\nLLaVA-NeXT on over half of the benchmarks under same configuration without\ndynamic resolution, highlighting the effectiveness of our approach. We release\nour code, model weights, and dataset. https://github.com/starriver030515/FUSION",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.09925.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6625ef13605f46d05c1d0031",
      "avatarUrl": "/avatars/22f201dca35e43013cb593884516e96c.svg",
      "fullname": "Zheng Liu",
      "name": "starriver030515",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.08837",
      "authors": [
        {
          "_id": "67fdc483ba0d61664fb0a19d",
          "user": {
            "_id": "65bf52f0259bc6caeb74f8bf",
            "avatarUrl": "/avatars/b38392e954466df784a5760ded5df804.svg",
            "isPro": false,
            "fullname": "Haozhe Wang",
            "user": "JasperHaozhe",
            "type": "user"
          },
          "name": "Haozhe Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-15T07:54:20.455Z",
          "hidden": false
        },
        {
          "_id": "67fdc483ba0d61664fb0a19e",
          "name": "Chao Qu",
          "hidden": false
        },
        {
          "_id": "67fdc483ba0d61664fb0a19f",
          "user": {
            "_id": "6772524ed6f92f429bd343a3",
            "avatarUrl": "/avatars/211e0c4641b2d048b0136d7cdeef2483.svg",
            "isPro": false,
            "fullname": "Zuming Huang",
            "user": "zuminghuang",
            "type": "user"
          },
          "name": "Zuming Huang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:17:55.505Z",
          "hidden": false
        },
        {
          "_id": "67fdc483ba0d61664fb0a1a0",
          "name": "Wei Chu",
          "hidden": false
        },
        {
          "_id": "67fdc483ba0d61664fb0a1a1",
          "name": "Fangzhen Lin",
          "hidden": false
        },
        {
          "_id": "67fdc483ba0d61664fb0a1a2",
          "user": {
            "_id": "6313a86154e6e5d9f0f94e04",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
            "isPro": false,
            "fullname": "Wenhu Chen",
            "user": "wenhu",
            "type": "user"
          },
          "name": "Wenhu Chen",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-15T02:29:24.168Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-10T17:41:56.000Z",
      "submittedOnDailyAt": "2025-04-15T01:01:12.820Z",
      "title": "VL-Rethinker : Modèle de langage visuel : amélioration de l'auto-réflexion par rétroaction\n\nVL-Rethinker : Amélioration de l'auto-réflexion par apprentissage par récompense",
      "submittedOnDailyBy": {
        "_id": "6313a86154e6e5d9f0f94e04",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
        "isPro": false,
        "fullname": "Wenhu Chen",
        "user": "wenhu",
        "type": "user"
      },
      "summary": "Récemment, une grande possibilité de résoudre des problèmes complexes par la réflexion explicite a été mise en évidence dans des systèmes courts tels que GPT-o1 et DeepSeek-R1. Ces modèles montrent des résultats exceptionnels dans différents cadres de référence en mathématiques et en sciences, similaires à ceux des modèles courts optimaux comme GPT-4o. Cependant, leur capacité en logique multimodale reste égale à celle des modèles courts. Par exemple, GPT-o1 montre un niveau de performance similaire aux modèles courts dans des cadres de référence comme MathVista, MathVerse et MathVision. Dans cet article, nous proposons d'améliorer et développer les capacités courtes des modèles de langage visuo-linguistique en utilisant l'apprentissage empirique (RL). Tout d'abord, nous appliquons l'algorithme GRPO à une nouvelle technologie appelée Sampling Selective Retransmission (SSR) pour résoudre la perte de potentiel. Cette approche démontre le pouvoir d'un modèle RL, mais les modèles RL entraînés montrent des réflexions et auto-évaluations limitées. De plus, pour promouvoir la courtoisie, nous introduisons la technique de \"Forced Rethinking\" et nous ajoutons une ré-exploration littéraire à la fin de la sortie initiale de l'entraînement RL pour forcer la réflexion explicite. La combinaison de ces deux techniques permet que notre modèle, VL-Rethinker, atteigne des scores avancés dans des cadres de référence comme MathVista, MathVerse et MathVision, atteignant respectivement 80.3%, 61.8% et 43.9%. De plus, VL-Rethinker atteint les meilleurs résultats ouverts dans des cadres de référence multidisciplinaires comme MMMU-Pro, EMMA et MEGA-Bench, réduisant les erreurs par rapport à GPT-o1.",
      "upvotes": 24,
      "discussionId": "67fdc484ba0d61664fb0a1db",
      "projectPage": "https://tiger-ai-lab.github.io/VL-Rethinker/",
      "githubRepo": "https://github.com/TIGER-AI-Lab/VL-Rethinker/"
    },
    "publishedAt": "2025-04-10T13:41:56.000Z",
    "title": "VL-Rethinker: Incentivizing Self-Reflection of Vision-Language Models\n  with Reinforcement Learning",
    "summary": "Recently, slow-thinking systems like GPT-o1 and DeepSeek-R1 have demonstrated\ngreat potential in solving challenging problems through explicit reflection.\nThey significantly outperform the best fast-thinking models, such as GPT-4o, on\nvarious math and science benchmarks. However, their multimodal reasoning\ncapabilities remain on par with fast-thinking models. For instance, GPT-o1's\nperformance on benchmarks like MathVista, MathVerse, and MathVision is similar\nto fast-thinking models. In this paper, we aim to enhance the slow-thinking\ncapabilities of vision-language models using reinforcement learning (without\nrelying on distillation) to advance the state of the art. First, we adapt the\nGRPO algorithm with a novel technique called Selective Sample Replay (SSR) to\naddress the vanishing advantages problem. While this approach yields strong\nperformance, the resulting RL-trained models exhibit limited self-reflection or\nself-verification. To further encourage slow-thinking, we introduce Forced\nRethinking, which appends a textual rethinking trigger to the end of initial\nrollouts in RL training, explicitly enforcing a self-reflection reasoning step.\nBy combining these two techniques, our model, VL-Rethinker, advances\nstate-of-the-art scores on MathVista, MathVerse, and MathVision to achieve\n80.3%, 61.8%, and 43.9% respectively. VL-Rethinker also achieves open-source\nSoTA on multi-disciplinary benchmarks such as MMMU-Pro, EMMA, and MEGA-Bench,\nnarrowing the gap with GPT-o1.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08837.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6313a86154e6e5d9f0f94e04",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
      "fullname": "Wenhu Chen",
      "name": "wenhu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 38
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.10068",
      "authors": [
        {
          "_id": "67fdd1d7634e600357b5b7ff",
          "user": {
            "_id": "673c7319d11b1c2e246ead9c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/673c7319d11b1c2e246ead9c/IjFIO--N7Hm_BOEafhEQv.jpeg",
            "isPro": false,
            "fullname": "Yang Shi",
            "user": "DogNeverSleep",
            "type": "user"
          },
          "name": "Yang Shi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-15T07:54:11.086Z",
          "hidden": false
        },
        {
          "_id": "67fdd1d7634e600357b5b800",
          "user": {
            "_id": "65377c30e48353201e6fdda0",
            "avatarUrl": "/avatars/a8f803b6f2e598eaee9c52c0d2ddfc16.svg",
            "isPro": false,
            "fullname": "Jiaheng Liu",
            "user": "CheeryLJH",
            "type": "user"
          },
          "name": "Jiaheng Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:18:07.388Z",
          "hidden": false
        },
        {
          "_id": "67fdd1d7634e600357b5b801",
          "user": {
            "_id": "66ac46766c3f950f4f10b9f9",
            "avatarUrl": "/avatars/027b573bc6e5b18107e762645cec6069.svg",
            "isPro": false,
            "fullname": "Yushuo Guan",
            "user": "UnnamedWatcher",
            "type": "user"
          },
          "name": "Yushuo Guan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:18:14.282Z",
          "hidden": false
        },
        {
          "_id": "67fdd1d7634e600357b5b802",
          "user": {
            "_id": "646f7b60799a974be3191889",
            "avatarUrl": "/avatars/8fff4c87a2ea2d12958424074dd8e93d.svg",
            "isPro": false,
            "fullname": "oliver",
            "user": "zhenhuawu",
            "type": "user"
          },
          "name": "Zhenhua Wu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:18:32.198Z",
          "hidden": false
        },
        {
          "_id": "67fdd1d7634e600357b5b803",
          "name": "Yuanxing Zhang",
          "hidden": false
        },
        {
          "_id": "67fdd1d7634e600357b5b804",
          "name": "Zihao Wang",
          "hidden": false
        },
        {
          "_id": "67fdd1d7634e600357b5b805",
          "name": "Weihong Lin",
          "hidden": false
        },
        {
          "_id": "67fdd1d7634e600357b5b806",
          "name": "Jingyun Hua",
          "hidden": false
        },
        {
          "_id": "67fdd1d7634e600357b5b807",
          "user": {
            "_id": "656832dfbd65fd41ee7aa8cd",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656832dfbd65fd41ee7aa8cd/HHkyetTqNq1wIBPipzjQA.jpeg",
            "isPro": false,
            "fullname": "Zekun Wang",
            "user": "kugwzk",
            "type": "user"
          },
          "name": "Zekun Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:22:34.448Z",
          "hidden": false
        },
        {
          "_id": "67fdd1d7634e600357b5b808",
          "name": "Xinlong Chen",
          "hidden": false
        },
        {
          "_id": "67fdd1d7634e600357b5b809",
          "user": {
            "_id": "6671214c92412fd4640714eb",
            "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg",
            "isPro": false,
            "fullname": "bohan zeng",
            "user": "zbhpku",
            "type": "user"
          },
          "name": "Bohan Zeng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-15T07:54:08.962Z",
          "hidden": false
        },
        {
          "_id": "67fdd1d7634e600357b5b80a",
          "name": "Wentao Zhang",
          "hidden": false
        },
        {
          "_id": "67fdd1d7634e600357b5b80b",
          "user": {
            "_id": "67c5945da1661d5fa6f29adb",
            "avatarUrl": "/avatars/62561f3875c0c251cae949acc38d72dc.svg",
            "isPro": false,
            "fullname": "Fuzheng Zhang",
            "user": "Edrex",
            "type": "user"
          },
          "name": "Fuzheng Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:21:57.922Z",
          "hidden": false
        },
        {
          "_id": "67fdd1d7634e600357b5b80c",
          "name": "Wenjing Yang",
          "hidden": false
        },
        {
          "_id": "67fdd1d7634e600357b5b80d",
          "user": {
            "_id": "644c8324f02250233d0d67d9",
            "avatarUrl": "/avatars/feb39d281457c1750f3eada3c060a23e.svg",
            "isPro": false,
            "fullname": "Di Zhang",
            "user": "dizhang",
            "type": "user"
          },
          "name": "Di Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:19:13.378Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-14T10:14:44.000Z",
      "submittedOnDailyAt": "2025-04-15T01:56:35.077Z",
      "title": "Mavaz : Représentation de vidéos multigranulaires pour modèles de langage multimodèles à grande échelle",
      "submittedOnDailyBy": {
        "_id": "673c7319d11b1c2e246ead9c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/673c7319d11b1c2e246ead9c/IjFIO--N7Hm_BOEafhEQv.jpeg",
        "isPro": false,
        "fullname": "Yang Shi",
        "user": "DogNeverSleep",
        "type": "user"
      },
      "summary": "La exécution de la compréhension vidéo à long terme dans des modèles multi-modal de langage (MLLM) est un défi qui nécessite une efficacité informatique et la préservation de modèles temporels-spatiaux. Les méthodes actuelles (par exemple, échantillonnage aléatoire, échantillonnage dense à basse résolution, compression de tokens) souvent perdent d'informations excessives, surtout pour des mouvements complexes et des vidéos à haute résolution, en ce qui concerne la dynamique temporelle, les détails spatiaux et les interactions plus délicates. Pour y répondre, nous proposons un nouveau cadre de travail appelé \"Mavors\". Ce cadre introduit une représentation vidéo multi-granularité pour atteindre une compréhension intégrale des vidéos à long terme. Spécifiquement, Mavors transforme le contenu vidéo directement via deux composants clés : 1) le codificateur visuel de chunks (IVE) utilise des convolutions 3D et des transformateurs de vision pour préserver des caractéristiques spatiales à haute résolution. 2) le concentrateur de caractéristiques entre chunks (IFA) utilise un modèle de dépendances basé sur les tokens et une codification des positions de rotation au niveau de la frame pour établir la continuité temporelle entre les frames. De plus, ce cadre considère que l'image peut être un seul frame de vidéo, intégrant la compréhension des images et des vidéos. Les expériences réalisées sur différents benchmarks montrent que Mavors a la capacité de maintenir à la fois la fidélité spatiale et la continuité temporelle, et dépasse significativement les méthodes actuelles dans les tâches nécessitant une précision spatio-temporelle.",
      "upvotes": 21,
      "discussionId": "67fdd1db634e600357b5b8f4",
      "projectPage": "https://mavors-mllm.github.io/",
      "githubRepo": "https://github.com/DogNeverSleep/Mavors"
    },
    "publishedAt": "2025-04-14T06:14:44.000Z",
    "title": "Mavors: Multi-granularity Video Representation for Multimodal Large\n  Language Model",
    "summary": "Long-context video understanding in multimodal large language models (MLLMs)\nfaces a critical challenge: balancing computational efficiency with the\nretention of fine-grained spatio-temporal patterns. Existing approaches (e.g.,\nsparse sampling, dense sampling with low resolution, and token compression)\nsuffer from significant information loss in temporal dynamics, spatial details,\nor subtle interactions, particularly in videos with complex motion or varying\nresolutions. To address this, we propose Mavors, a novel framework\nthat introduces Multi-granularity\nvideo representation for holistic\nlong-video modeling. Specifically, Mavors directly encodes raw video content\ninto latent representations through two core components: 1) an Intra-chunk\nVision Encoder (IVE) that preserves high-resolution spatial features via 3D\nconvolutions and Vision Transformers, and 2) an Inter-chunk Feature Aggregator\n(IFA) that establishes temporal coherence across chunks using transformer-based\ndependency modeling with chunk-level rotary position encodings. Moreover, the\nframework unifies image and video understanding by treating images as\nsingle-frame videos via sub-image decomposition. Experiments across diverse\nbenchmarks demonstrate Mavors' superiority in maintaining both spatial fidelity\nand temporal continuity, significantly outperforming existing methods in tasks\nrequiring fine-grained spatio-temporal reasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10068.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "673c7319d11b1c2e246ead9c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/673c7319d11b1c2e246ead9c/IjFIO--N7Hm_BOEafhEQv.jpeg",
      "fullname": "Yang Shi",
      "name": "DogNeverSleep",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.08942",
      "authors": [
        {
          "_id": "67fdadafdc27362617bbe714",
          "user": {
            "_id": "5fa9ff3ea13e063b8b2b60cb",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1633380224986-5fa9ff3ea13e063b8b2b60cb.jpeg",
            "isPro": false,
            "fullname": "Xing Han Lù",
            "user": "xhluca",
            "type": "user"
          },
          "name": "Xing Han Lù",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-15T07:55:07.746Z",
          "hidden": false
        },
        {
          "_id": "67fdadafdc27362617bbe715",
          "user": {
            "_id": "63458f12d54fb141dedac508",
            "avatarUrl": "/avatars/3946fb9c23d1cd24037770cc0a3489bf.svg",
            "isPro": false,
            "fullname": "Amirhossein Kazemnejad",
            "user": "kazemnejad",
            "type": "user"
          },
          "name": "Amirhossein Kazemnejad",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:22:50.671Z",
          "hidden": false
        },
        {
          "_id": "67fdadafdc27362617bbe716",
          "user": {
            "_id": "64527548fc4b47877aba7de0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64527548fc4b47877aba7de0/ht-mRRxNQT49A7NxArOGG.png",
            "isPro": false,
            "fullname": "Nicholas Meade",
            "user": "ncmeade",
            "type": "user"
          },
          "name": "Nicholas Meade",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:22:56.742Z",
          "hidden": false
        },
        {
          "_id": "67fdadafdc27362617bbe717",
          "user": {
            "_id": "631a523c04f8ed65eff16fb4",
            "avatarUrl": "/avatars/2b284403c88f140d7bef283f729f7a3e.svg",
            "isPro": false,
            "fullname": "Arkil Patel",
            "user": "arkilpatel",
            "type": "user"
          },
          "name": "Arkil Patel",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-15T07:55:05.412Z",
          "hidden": false
        },
        {
          "_id": "67fdadafdc27362617bbe718",
          "user": {
            "_id": "619af75e7812aec847ee7729",
            "avatarUrl": "/avatars/f50c05ee8b3105d20a8b291cc9f06ae4.svg",
            "isPro": false,
            "fullname": "Dong Chan Shin",
            "user": "dongchans",
            "type": "user"
          },
          "name": "Dongchan Shin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:23:03.255Z",
          "hidden": false
        },
        {
          "_id": "67fdadafdc27362617bbe719",
          "user": {
            "_id": "65f5133599c842dd93b7bacd",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/nxxfWGP_K0hf7BG1oM7c0.png",
            "isPro": false,
            "fullname": "Alejandra Zambrano",
            "user": "alzambranolu",
            "type": "user"
          },
          "name": "Alejandra Zambrano",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:23:09.097Z",
          "hidden": false
        },
        {
          "_id": "67fdadafdc27362617bbe71a",
          "user": {
            "_id": "60a66731e1db8bc33b8d4112",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60a66731e1db8bc33b8d4112/AY5Y0CnHh08u6lfEoQ6se.jpeg",
            "isPro": false,
            "fullname": "Karolina Stanczak",
            "user": "Karolina",
            "type": "user"
          },
          "name": "Karolina Stańczak",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:23:14.768Z",
          "hidden": false
        },
        {
          "_id": "67fdadafdc27362617bbe71b",
          "user": {
            "_id": "631cf223fe95faea33561d5f",
            "avatarUrl": "/avatars/ac0431955c6c5f4948461772a984a2ba.svg",
            "isPro": false,
            "fullname": "Peter Shaw",
            "user": "PeterShaw",
            "type": "user"
          },
          "name": "Peter Shaw",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:23:22.550Z",
          "hidden": false
        },
        {
          "_id": "67fdadafdc27362617bbe71c",
          "name": "Christopher J. Pal",
          "hidden": false
        },
        {
          "_id": "67fdadafdc27362617bbe71d",
          "user": {
            "_id": "624734dc4c731bb6bfab8af7",
            "avatarUrl": "/avatars/6b250b58710a3287b85e4733c1824558.svg",
            "isPro": false,
            "fullname": "Siva Reddy",
            "user": "sivareddyg",
            "type": "user"
          },
          "name": "Siva Reddy",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-15T00:51:59.987Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/5fa9ff3ea13e063b8b2b60cb/-XsJQfRUJ6uZhuI8h9cDm.png"
      ],
      "publishedAt": "2025-04-11T19:49:22.000Z",
      "submittedOnDailyAt": "2025-04-15T00:59:48.327Z",
      "title": "AgentRewardBench : Évaluation de l'Autoévaluation des Agents Web",
      "submittedOnDailyBy": {
        "_id": "5fa9ff3ea13e063b8b2b60cb",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1633380224986-5fa9ff3ea13e063b8b2b60cb.jpeg",
        "isPro": false,
        "fullname": "Xing Han Lù",
        "user": "xhluca",
        "type": "user"
      },
      "summary": "L'agent web permet aux utilisateurs de réaliser des tâches interactivement dans un navigateur web en utilisant un langage naturel. Évaluer l'état d'exécution d'un agent web (le progrès de la tâche) est un problème important, car cette évaluation peut déterminer si la tâche a été réussiment terminée. Les méthodes basées sur les règles sont bien utilisées pour cette fin, mais ont des limites car elles ne peuvent pas s'étendre aux nouvelles tâches et ne peuvent pas reconnaître avec succès les tâches terminées. Bien que l'évaluation humaine puisse atteindre une précision élevée, son coût est considérablement élevé. L'évaluation automatique en utilisant des modèles de langage grands (LLM) évite les problèmes comme la conception de nouvelles règles ou le commentaire dynamique des tâches, permettant une évaluation plus rapide et rentable. Cependant, l'efficacité de cette évaluation dans le contexte d'agents web est incertaine. Par conséquent, on propose le premier benchmark appelé AgentRewardBench. AgentRewardBench comprend 5 benchmarks et 4 drives, totalisant 1302 tâches, chacune évaluée par un expert et conçue pour répondre à des questions sur le succès, les effets secondaires et la duplicité. Avec notre benchmark, 12 évaluateurs de LLM ont été évalués, mais aucun drive n'a été trouvé comme étant le mieux performant dans tous les benchmarks. De plus, l'évaluation basée sur les règles utilisée dans les benchmarks généraux mesure avec faible précision le succès des agents web et met principalement en avant les faiblesses de cette évaluation, soulignant la nécessité de développer une évaluation automatique plus flexible. Le benchmark a été publié sur la URL suivante : https://agent-reward-bench.github.io",
      "upvotes": 13,
      "discussionId": "67fdadb0dc27362617bbe749",
      "projectPage": "https://agent-reward-bench.github.io/",
      "githubRepo": "https://github.com/McGill-NLP/agent-reward-bench"
    },
    "publishedAt": "2025-04-11T15:49:22.000Z",
    "title": "AgentRewardBench: Evaluating Automatic Evaluations of Web Agent\n  Trajectories",
    "summary": "Web agents enable users to perform tasks on web browsers through natural\nlanguage interaction. Evaluating web agents trajectories is an important\nproblem, since it helps us determine whether the agent successfully completed\nthe tasks. Rule-based methods are widely used for this purpose, but they are\nchallenging to extend to new tasks and may not always recognize successful\ntrajectories. We may achieve higher accuracy through human evaluation, but the\nprocess would be substantially slower and more expensive. Automatic evaluations\nwith LLMs may avoid the challenges of designing new rules and manually\nannotating trajectories, enabling faster and cost-effective evaluation.\nHowever, it is unclear how effective they are at evaluating web agents. To this\nend, we propose AgentRewardBench, the first benchmark to assess the\neffectiveness of LLM judges for evaluating web agents. AgentRewardBench\ncontains 1302 trajectories across 5 benchmarks and 4 LLMs. Each trajectory in\nAgentRewardBench is reviewed by an expert, who answers questions pertaining to\nthe success, side effects, and repetitiveness of the agent. Using our\nbenchmark, we evaluate 12 LLM judges and find that no single LLM excels across\nall benchmarks. We also find that the rule-based evaluation used by common\nbenchmarks tends to underreport the success rate of web agents, highlighting a\nkey weakness of rule-based evaluation and the need to develop more flexible\nautomatic evaluations. We release the benchmark at:\nhttps://agent-reward-bench.github.io",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/5fa9ff3ea13e063b8b2b60cb/-XsJQfRUJ6uZhuI8h9cDm.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08942.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5fa9ff3ea13e063b8b2b60cb",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1633380224986-5fa9ff3ea13e063b8b2b60cb.jpeg",
      "fullname": "Xing Han Lù",
      "name": "xhluca",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.10368",
      "authors": [
        {
          "_id": "67fdd3a917e86592095a3ab7",
          "user": {
            "_id": "6617c98901ad3a0642a2a08f",
            "avatarUrl": "/avatars/cf52fb511f2f31de7940f9c13d19b8e7.svg",
            "isPro": false,
            "fullname": "Wenyuan Zhang",
            "user": "WYRipple",
            "type": "user"
          },
          "name": "Wenyuan Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-15T07:54:05.425Z",
          "hidden": false
        },
        {
          "_id": "67fdd3a917e86592095a3ab8",
          "user": {
            "_id": "665e84f6152658fe8d478b1f",
            "avatarUrl": "/avatars/9f08ce6aa78d7576c97e4feaddf77c1e.svg",
            "isPro": false,
            "fullname": "Shuaiyi Nie",
            "user": "ShuaiyiNie",
            "type": "user"
          },
          "name": "Shuaiyi Nie",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:23:41.149Z",
          "hidden": false
        },
        {
          "_id": "67fdd3a917e86592095a3ab9",
          "name": "Xinghua Zhang",
          "hidden": false
        },
        {
          "_id": "67fdd3a917e86592095a3aba",
          "user": {
            "_id": "642c2dcec3694d2b74565c48",
            "avatarUrl": "/avatars/31243bb505f8c511ebd7492eaf3ea1a9.svg",
            "isPro": false,
            "fullname": "zhangzef",
            "user": "Starrrrrry",
            "type": "user"
          },
          "name": "Zefeng Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-15T07:54:01.660Z",
          "hidden": false
        },
        {
          "_id": "67fdd3a917e86592095a3abb",
          "name": "Tingwen Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-14T16:13:23.000Z",
      "submittedOnDailyAt": "2025-04-15T02:04:54.102Z",
      "title": "S1-Bench : Benchmark pour évaluer la capacité de pensée d'un système 1 de modèles non logiques",
      "submittedOnDailyBy": {
        "_id": "656426e7ec7e2398990a2d34",
        "avatarUrl": "/avatars/e84aed6b55b0554ad9581ae3c138a16a.svg",
        "isPro": false,
        "fullname": "AIRobotZ",
        "user": "AIRobotZ",
        "type": "user"
      },
      "summary": "S1-Bench est un nouveau benchmark qui évalue le rendement des modèles logiques à grande échelle (LRMs) dans des tâches simples qui favorisent un pensée intuitive d'un système 1. Les LRMs réussissent à obtenir des solutions claires dans des tâches logiques complexes grâce à une séquence claire de pensée, mais leur capacité est limitée par la dépendance à un pensée analytique profonde, qui est un trait du pensée d'un système 1. De plus, il existe une absence de benchmarks pour évaluer le rendement des LRMs dans leurs tâches actuelles. Pour combler ce vide, S1-Bench offre une série de questions simples, naturelles et claires qui combinent divers domaines et langues, conçues spécifiquement pour évaluer le rendement des LRMs. Selon l'évaluation détaillée de 22 LRMs, il a été observé que, en moyenne, ils tendaient à produire des sorties 15,5 fois plus longues avec moins d'efficacité. De plus, les LRMs peuvent identifier rapidement des réponses précises, mais souvent continuent de suivre des logiques non nécessaires à long terme, ce qui peut provoquer plusieurs erreurs. Ces résultats soulignent clairement le patron logique inflexible des LRMs actuels et soulignent la nécessité d'un pensée d'un système dual équilibré pour aborder les complexités adéquatement.",
      "upvotes": 12,
      "discussionId": "67fdd3aa17e86592095a3b0b"
    },
    "publishedAt": "2025-04-14T12:13:23.000Z",
    "title": "S1-Bench: A Simple Benchmark for Evaluating System 1 Thinking Capability\n  of Large Reasoning Models",
    "summary": "We introduce S1-Bench, a novel benchmark designed to evaluate Large Reasoning\nModels' (LRMs) performance on simple tasks that favor intuitive system 1\nthinking rather than deliberative system 2 reasoning. While LRMs have achieved\nsignificant breakthroughs in complex reasoning tasks through explicit chains of\nthought, their reliance on deep analytical thinking may limit their system 1\nthinking capabilities. Moreover, a lack of benchmark currently exists to\nevaluate LRMs' performance in tasks that require such capabilities. To fill\nthis gap, S1-Bench presents a set of simple, diverse, and naturally clear\nquestions across multiple domains and languages, specifically designed to\nassess LRMs' performance in such tasks. Our comprehensive evaluation of 22 LRMs\nreveals significant lower efficiency tendencies, with outputs averaging 15.5\ntimes longer than those of traditional small LLMs. Additionally, LRMs often\nidentify correct answers early but continue unnecessary deliberation, with some\nmodels even producing numerous errors. These findings highlight the rigid\nreasoning patterns of current LRMs and underscore the substantial development\nneeded to achieve balanced dual-system thinking capabilities that can adapt\nappropriately to task complexity.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10368.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "656426e7ec7e2398990a2d34",
      "avatarUrl": "/avatars/e84aed6b55b0554ad9581ae3c138a16a.svg",
      "fullname": "AIRobotZ",
      "name": "AIRobotZ",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.09710",
      "authors": [
        {
          "_id": "67fdb42aa8deb632ed46d23d",
          "user": {
            "_id": "64dfcc62e8b6f3f3baa950e0",
            "avatarUrl": "/avatars/21bbff67d46c08044efe2406575aa77e.svg",
            "isPro": false,
            "fullname": "Zhenting Wang",
            "user": "ztwang",
            "type": "user"
          },
          "name": "Zhenting Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-15T07:54:59.070Z",
          "hidden": false
        },
        {
          "_id": "67fdb42aa8deb632ed46d23e",
          "user": {
            "_id": "6670b3b78eac2e222ebf77d4",
            "avatarUrl": "/avatars/0f1d231eac479ca78ddf106a72490faa.svg",
            "isPro": false,
            "fullname": "Guofeng Cui",
            "user": "gfcui",
            "type": "user"
          },
          "name": "Guofeng Cui",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:24:11.131Z",
          "hidden": false
        },
        {
          "_id": "67fdb42aa8deb632ed46d23f",
          "user": {
            "_id": "66274e02348a5304435dc9cc",
            "avatarUrl": "/avatars/bda87559cd497c310597c2fc8430b31f.svg",
            "isPro": false,
            "fullname": "Kun Wan",
            "user": "timecuriosity",
            "type": "user"
          },
          "name": "Kun Wan",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-04-15T01:22:46.116Z",
          "hidden": false
        },
        {
          "_id": "67fdb42aa8deb632ed46d240",
          "user": {
            "_id": "66443629b23fe8d3f7f2d0c7",
            "avatarUrl": "/avatars/98ff088036aa382f33a05c232604c565.svg",
            "isPro": false,
            "fullname": "Wentian Zhao",
            "user": "zwt123home123",
            "type": "user"
          },
          "name": "Wentian Zhao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:24:30.442Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-13T20:10:27.000Z",
      "submittedOnDailyAt": "2025-04-15T00:08:28.286Z",
      "title": "Dump : Apprentissage du Curriculum en Base de RL avec des Niveaux d'Automatisation de la Distribution",
      "submittedOnDailyBy": {
        "_id": "64dfcc62e8b6f3f3baa950e0",
        "avatarUrl": "/avatars/21bbff67d46c08044efe2406575aa77e.svg",
        "isPro": false,
        "fullname": "Zhenting Wang",
        "user": "ztwang",
        "type": "user"
      },
      "summary": "Le développement récent basé sur l'apprentissage par renforcement (RL) après l'entraînement a réalisé des améliorations significatives dans les modèles de langage grands (LLMs), en particulier en améliorant la mémoire logique nécessaire pour traiter des tâches complexes. Cependant, les méthodes actuelles traitent les données d'entraînement comme une unique entité intégrée, ignorant la diversité dans différents niveaux de difficulté et d'origine des données qui sont incluses dans la distribution moderne d'entraînement des LLMs. Cette diversité est considérée comme une principale problématique en raison de la manque de méthodes efficaces pour optimiser l'adaptation de l'entraînement entre différentes distributions. Dans cet article, nous proposons un cadre d'apprentissage de caractéristiques basé sur le concept de possibilité d'apprentissage au niveau de la distribution. La principale observation est que la taille de la priorité de la politique reflète la capacité du modèle à recevoir plus d'entraînement dans cette distribution. En se basant sur cela, nous proposons un cadre d'apprentissage de caractéristiques de RL pour l'entraînement ultérieur des LLMs, en utilisant le principe des intervalles de confiance supérieure (UCB) pour ajuster de manière dynamique la probabilité d'échantillons de différentes distributions. Cette approche priorise les distributions avec une priorité moyenne plus élevée (utiliser) ou celles avec moins d'échantillons (explorer), offrant un entraînement adaptatif et théoriquement établi. Notre cadre d'apprentissage de caractéristiques est implémenté en utilisant un algorithme de RL basé sur GRPO et montre des résultats sur différents ensembles de données avec différents niveaux de difficulté et d'origine. Nos expériences ont démontré des améliorations significatives en termes de vitesse de convergence et de rendement final, soulignant l'importance de la stratégie d'apprentissage de caractéristiques dans l'entraînement ultérieur des LLMs. Code : https://github.com/ZhentingWang/DUMP.",
      "upvotes": 11,
      "discussionId": "67fdb457a8deb632ed46de2f"
    },
    "publishedAt": "2025-04-13T16:10:27.000Z",
    "title": "DUMP: Automated Distribution-Level Curriculum Learning for RL-based LLM\n  Post-training",
    "summary": "Recent advances in reinforcement learning (RL)-based post-training have led\nto notable improvements in large language models (LLMs), particularly in\nenhancing their reasoning capabilities to handle complex tasks. However, most\nexisting methods treat the training data as a unified whole, overlooking the\nfact that modern LLM training often involves a mixture of data from diverse\ndistributions-varying in both source and difficulty. This heterogeneity\nintroduces a key challenge: how to adaptively schedule training across\ndistributions to optimize learning efficiency. In this paper, we present a\nprincipled curriculum learning framework grounded in the notion of\ndistribution-level learnability. Our core insight is that the magnitude of\npolicy advantages reflects how much a model can still benefit from further\ntraining on a given distribution. Based on this, we propose a\ndistribution-level curriculum learning framework for RL-based LLM\npost-training, which leverages the Upper Confidence Bound (UCB) principle to\ndynamically adjust sampling probabilities for different distrubutions. This\napproach prioritizes distributions with either high average advantage\n(exploitation) or low sample count (exploration), yielding an adaptive and\ntheoretically grounded training schedule. We instantiate our curriculum\nlearning framework with GRPO as the underlying RL algorithm and demonstrate its\neffectiveness on logic reasoning datasets with multiple difficulties and\nsources. Our experiments show that our framework significantly improves\nconvergence speed and final performance, highlighting the value of\ndistribution-aware curriculum strategies in LLM post-training. Code:\nhttps://github.com/ZhentingWang/DUMP.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.09710.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64dfcc62e8b6f3f3baa950e0",
      "avatarUrl": "/avatars/21bbff67d46c08044efe2406575aa77e.svg",
      "fullname": "Zhenting Wang",
      "name": "ztwang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.08003",
      "authors": [
        {
          "_id": "67fdc0c50c63732d9e0b139a",
          "name": "Ning Li",
          "hidden": false
        },
        {
          "_id": "67fdc0c50c63732d9e0b139b",
          "user": {
            "_id": "67fdc24ad2c9d1369d390f01",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/51JiReMgHiFHZiJ9OVdaf.png",
            "isPro": false,
            "fullname": "Jingran Zhang",
            "user": "zhangjingran",
            "type": "user"
          },
          "name": "Jingran Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:25:03.508Z",
          "hidden": false
        },
        {
          "_id": "67fdc0c50c63732d9e0b139c",
          "user": {
            "_id": "65862671e878be571bf9fc52",
            "avatarUrl": "/avatars/b2a1b939f3112b476e7641e0c5fd2dc7.svg",
            "isPro": false,
            "fullname": "bench-llm",
            "user": "cuijiaxing",
            "type": "user"
          },
          "name": "Justin Cui",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:24:57.129Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/65862671e878be571bf9fc52/B7iVn73glP5UR6kkdAApX.png"
      ],
      "publishedAt": "2025-04-09T16:10:15.000Z",
      "submittedOnDailyAt": "2025-04-15T00:44:44.354Z",
      "title": "A encore pas intégré la génération et la compréhension d'images ? Étude de recherche sur la capacité de génération d'images de GPT-4o",
      "submittedOnDailyBy": {
        "_id": "65862671e878be571bf9fc52",
        "avatarUrl": "/avatars/b2a1b939f3112b476e7641e0c5fd2dc7.svg",
        "isPro": false,
        "fullname": "bench-llm",
        "user": "cuijiaxing",
        "type": "user"
      },
      "summary": "OpenAI's GPT-4o montre des capacités exceptionnelles en génération et édition d'images, mais n'a pas démontré des capacités de synthèse de sens basées sur le savoir mondial, qui intègrent de manière continue la disciplinarité, la raison du contexte et la séquence du projet. Dans cette étude, on évalue ces capacités sur trois aspects importants : 1. La séquence générale du projet, 2. La précision de l'édition détaillée, 3. La théorie des raisons après la génération. Les benchmarks existants soulignent fortement les capacités de génération et édition d'images de GPT-4o, mais notre évaluation révèle ses limites propres : le modèle est fixé sur l'interprétation grammaticale des instructions, applique des restrictions de connaissances de manière déséquilibrée et fait face à des difficultés dans les tâches de raisonnement conditionnel. Ces résultats mettent en doute l'assumée compréhension et la capacité uniforme de génération de GPT-4o et révèlent de grandes lacunes dans l'intégration dynamique du savoir. Cette étude souligne la nécessité de développer des benchmarks plus forts qui dépassent la séquencementation superficielle et d'implémenter des stratégies d'entraînement plus puissantes, ainsi que de prioriser la génération multimodèle basée sur le contexte et la théorie des raisons.",
      "upvotes": 10,
      "discussionId": "67fdc0c60c63732d9e0b13d2"
    },
    "publishedAt": "2025-04-09T12:10:15.000Z",
    "title": "Have we unified image generation and understanding yet? An empirical\n  study of GPT-4o's image generation ability",
    "summary": "OpenAI's multimodal GPT-4o has demonstrated remarkable capabilities in image\ngeneration and editing, yet its ability to achieve world knowledge-informed\nsemantic synthesis--seamlessly integrating domain knowledge, contextual\nreasoning, and instruction adherence--remains unproven. In this study, we\nsystematically evaluate these capabilities across three critical dimensions:\n(1) Global Instruction Adherence, (2) Fine-Grained Editing Precision, and (3)\nPost-Generation Reasoning. While existing benchmarks highlight GPT-4o's strong\ncapabilities in image generation and editing, our evaluation reveals GPT-4o's\npersistent limitations: the model frequently defaults to literal\ninterpretations of instructions, inconsistently applies knowledge constraints,\nand struggles with conditional reasoning tasks. These findings challenge\nprevailing assumptions about GPT-4o's unified understanding and generation\ncapabilities, exposing significant gaps in its dynamic knowledge integration.\nOur study calls for the development of more robust benchmarks and training\nstrategies that go beyond surface-level alignment, emphasizing context-aware\nand reasoning-grounded multimodal generation.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/65862671e878be571bf9fc52/B7iVn73glP5UR6kkdAApX.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08003.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65862671e878be571bf9fc52",
      "avatarUrl": "/avatars/b2a1b939f3112b476e7641e0c5fd2dc7.svg",
      "fullname": "bench-llm",
      "name": "cuijiaxing",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.10157",
      "authors": [
        {
          "_id": "67fdc9adc47ae882c7a17c8a",
          "user": {
            "_id": "64b77e02a8c39dc07885179c",
            "avatarUrl": "/avatars/172dd9eef0d4edfbd8f7cc5fb3feb206.svg",
            "isPro": false,
            "fullname": "xnzhang",
            "user": "Lishi0905",
            "type": "user"
          },
          "name": "Xinnong Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-15T07:54:18.076Z",
          "hidden": false
        },
        {
          "_id": "67fdc9adc47ae882c7a17c8b",
          "name": "Jiayu Lin",
          "hidden": false
        },
        {
          "_id": "67fdc9adc47ae882c7a17c8c",
          "name": "Xinyi Mou",
          "hidden": false
        },
        {
          "_id": "67fdc9adc47ae882c7a17c8d",
          "name": "Shiyue Yang",
          "hidden": false
        },
        {
          "_id": "67fdc9adc47ae882c7a17c8e",
          "name": "Xiawei Liu",
          "hidden": false
        },
        {
          "_id": "67fdc9adc47ae882c7a17c8f",
          "user": {
            "_id": "6522f5652d5eb02118d4d2e3",
            "avatarUrl": "/avatars/f006828830590418d9c69b591fe61c69.svg",
            "isPro": false,
            "fullname": "Libo Sun",
            "user": "libo-ca",
            "type": "user"
          },
          "name": "Libo Sun",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:26:53.104Z",
          "hidden": false
        },
        {
          "_id": "67fdc9adc47ae882c7a17c90",
          "name": "Hanjia Lyu",
          "hidden": false
        },
        {
          "_id": "67fdc9adc47ae882c7a17c91",
          "name": "Yihang Yang",
          "hidden": false
        },
        {
          "_id": "67fdc9adc47ae882c7a17c92",
          "name": "Weihong Qi",
          "hidden": false
        },
        {
          "_id": "67fdc9adc47ae882c7a17c93",
          "name": "Yue Chen",
          "hidden": false
        },
        {
          "_id": "67fdc9adc47ae882c7a17c94",
          "name": "Guanying Li",
          "hidden": false
        },
        {
          "_id": "67fdc9adc47ae882c7a17c95",
          "name": "Ling Yan",
          "hidden": false
        },
        {
          "_id": "67fdc9adc47ae882c7a17c96",
          "name": "Yao Hu",
          "hidden": false
        },
        {
          "_id": "67fdc9adc47ae882c7a17c97",
          "user": {
            "_id": "6345de6cfe134dfd7a0ed1ec",
            "avatarUrl": "/avatars/5e74fbdff4d9145c2d0b3c2c4c0145c7.svg",
            "isPro": false,
            "fullname": "Siming",
            "user": "SimingChen",
            "type": "user"
          },
          "name": "Siming Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:28:27.202Z",
          "hidden": false
        },
        {
          "_id": "67fdc9adc47ae882c7a17c98",
          "name": "Yu Wang",
          "hidden": false
        },
        {
          "_id": "67fdc9adc47ae882c7a17c99",
          "name": "Jingxuan Huang",
          "hidden": false
        },
        {
          "_id": "67fdc9adc47ae882c7a17c9a",
          "name": "Jiebo Luo",
          "hidden": false
        },
        {
          "_id": "67fdc9adc47ae882c7a17c9b",
          "user": {
            "_id": "64b7727a88b86014d7eb9073",
            "avatarUrl": "/avatars/bc4dda32363efcc16212b8eb97c2f813.svg",
            "isPro": false,
            "fullname": "Simon Tang",
            "user": "tangshiping",
            "type": "user"
          },
          "name": "Shiping Tang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:28:00.350Z",
          "hidden": false
        },
        {
          "_id": "67fdc9adc47ae882c7a17c9c",
          "name": "Libo Wu",
          "hidden": false
        },
        {
          "_id": "67fdc9adc47ae882c7a17c9d",
          "user": {
            "_id": "67d066e70fdab2f434aa1488",
            "avatarUrl": "/avatars/d17838856185856306ec5d4a121314f1.svg",
            "isPro": false,
            "fullname": "Baohua Zhou",
            "user": "milesz7777",
            "type": "user"
          },
          "name": "Baohua Zhou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:27:40.001Z",
          "hidden": false
        },
        {
          "_id": "67fdc9adc47ae882c7a17c9e",
          "name": "Zhongyu Wei",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-14T12:12:52.000Z",
      "submittedOnDailyAt": "2025-04-15T01:29:17.826Z",
      "title": "Socia Overs : Modèle de monde pour la simulation social avec des agents de LLM et un ensemble de 10 millions d'utilisateurs réels",
      "submittedOnDailyBy": {
        "_id": "64c939307dba66c3a7e4d215",
        "avatarUrl": "/avatars/0b662cc1799525188476f3e6e1f97d29.svg",
        "isPro": false,
        "fullname": "BruceLyu",
        "user": "brucelyu",
        "type": "user"
      },
      "summary": "La simulation social est en train de changer l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'investigation fondamentale dans les sciences sociales. Cela change l'",
      "upvotes": 9,
      "discussionId": "67fdc9aec47ae882c7a17cf1"
    },
    "publishedAt": "2025-04-14T08:12:52.000Z",
    "title": "SocioVerse: A World Model for Social Simulation Powered by LLM Agents\n  and A Pool of 10 Million Real-World Users",
    "summary": "Social simulation is transforming traditional social science research by\nmodeling human behavior through interactions between virtual individuals and\ntheir environments. With recent advances in large language models (LLMs), this\napproach has shown growing potential in capturing individual differences and\npredicting group behaviors. However, existing methods face alignment challenges\nrelated to the environment, target users, interaction mechanisms, and\nbehavioral patterns. To this end, we introduce SocioVerse, an LLM-agent-driven\nworld model for social simulation. Our framework features four powerful\nalignment components and a user pool of 10 million real individuals. To\nvalidate its effectiveness, we conducted large-scale simulation experiments\nacross three distinct domains: politics, news, and economics. Results\ndemonstrate that SocioVerse can reflect large-scale population dynamics while\nensuring diversity, credibility, and representativeness through standardized\nprocedures and minimal manual adjustments.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10157.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64c939307dba66c3a7e4d215",
      "avatarUrl": "/avatars/0b662cc1799525188476f3e6e1f97d29.svg",
      "fullname": "BruceLyu",
      "name": "brucelyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.09641",
      "authors": [
        {
          "_id": "67fdceb2df4261c001394f59",
          "user": {
            "_id": "66448136bfe15e84d3987372",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66448136bfe15e84d3987372/SFEbWGgqhdZjxCS7qF3YL.jpeg",
            "isPro": false,
            "fullname": "Zhang Xingjian",
            "user": "Zhang199",
            "type": "user"
          },
          "name": "Xingjian Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-15T07:54:14.629Z",
          "hidden": false
        },
        {
          "_id": "67fdceb2df4261c001394f5a",
          "user": {
            "_id": "67f72edbc791c2e0f938203d",
            "avatarUrl": "/avatars/c8e6d5a2f2122482e5fab7c6438440b5.svg",
            "isPro": false,
            "fullname": "si wei wen",
            "user": "wenzz1",
            "type": "user"
          },
          "name": "Siwei Wen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:29:04.694Z",
          "hidden": false
        },
        {
          "_id": "67fdceb2df4261c001394f5b",
          "name": "Wenjun Wu",
          "hidden": false
        },
        {
          "_id": "67fdceb2df4261c001394f5c",
          "name": "Lei Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-13T16:32:49.000Z",
      "submittedOnDailyAt": "2025-04-15T01:46:43.707Z",
      "title": "TinyLLaVA-Video-R1 : Développement d'un LMM plus petit pour le logique vidéo",
      "submittedOnDailyBy": {
        "_id": "66448136bfe15e84d3987372",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66448136bfe15e84d3987372/SFEbWGgqhdZjxCS7qF3YL.jpeg",
        "isPro": false,
        "fullname": "Zhang Xingjian",
        "user": "Zhang199",
        "type": "user"
      },
      "summary": "Récemment, le connaissance des grands modèles multimodal (LMMs) a été améliorée par l'apprentissage par renforcement. Cependant, la plupart des études se basent sur des ensembles de données qui nécessitent une compréhension élevée, comme les mathématiques ou le code, et les chercheurs utilisent souvent de grands modèles. Nous affirmons que les modèles de petit taille ont un valeur pour les chercheurs avec des ressources informatiques limitées. De plus, il est significatif que un modèle explique le processus de compréhension dans des ensembles de données de questions et de réponses générales. Par conséquent, nous présentons le modèle de compréhension d'images petit et TinyLLaVA-Video-R1. Basé sur TinyLLaVA-Video, ce modèle de compréhension d'images a été entraîné de manière référencée avec moins de 4B paramètres. En appliquant l'apprentissage par renforcement dans des ensembles de données de Video-QA générales, sa capacité de compréhension et de pensée a considérablement amélioré, démontrant une compréhension rapide comme celle de \"ah\". Nous partageons plusieurs résultats expérimentaux et proposons de fournir un contexte pratique pour l'exploration de la capacité de compréhension d'images (pensée) dans des modèles petits. Pour obtenir plus de détails, consultez https://github.com/ZhangXJ199/TinyLLaVA-Video-R1.",
      "upvotes": 7,
      "discussionId": "67fdceb5df4261c001394ff5"
    },
    "publishedAt": "2025-04-13T12:32:49.000Z",
    "title": "TinyLLaVA-Video-R1: Towards Smaller LMMs for Video Reasoning",
    "summary": "Recently, improving the reasoning ability of large multimodal models (LMMs)\nthrough reinforcement learning has made great progress. However, most existing\nworks are based on highly reasoning-intensive datasets such as mathematics and\ncode, and researchers generally choose large-scale models as the foundation. We\nargue that exploring small-scale models' reasoning capabilities remains\nvaluable for researchers with limited computational resources. Moreover,\nenabling models to explain their reasoning processes on general\nquestion-answering datasets is equally meaningful. Therefore, we present the\nsmall-scale video reasoning model TinyLLaVA-Video-R1. Based on TinyLLaVA-Video,\na traceably trained video understanding model with no more than 4B parameters,\nit not only demonstrates significantly improved reasoning and thinking\ncapabilities after using reinforcement learning on general Video-QA datasets,\nbut also exhibits the emergent characteristic of \"aha moments\". Furthermore, we\nshare a series of experimental findings, aiming to provide practical insights\nfor future exploration of video reasoning (thinking) abilities in small-scale\nmodels. It is available at https://github.com/ZhangXJ199/TinyLLaVA-Video-R1.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.09641.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "66448136bfe15e84d3987372",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66448136bfe15e84d3987372/SFEbWGgqhdZjxCS7qF3YL.jpeg",
      "fullname": "Zhang Xingjian",
      "name": "Zhang199",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.10415",
      "authors": [
        {
          "_id": "67fddc276a3c533dc1d3bcbe",
          "user": {
            "_id": "6520621836008ecc88699622",
            "avatarUrl": "/avatars/b08c00af00f1736a4f4938443e575b0e.svg",
            "isPro": false,
            "fullname": "Parshin Shojaee",
            "user": "parshinsh",
            "type": "user"
          },
          "name": "Parshin Shojaee",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-15T04:10:17.364Z",
          "hidden": false
        },
        {
          "_id": "67fddc276a3c533dc1d3bcbf",
          "name": "Ngoc-Hieu Nguyen",
          "hidden": false
        },
        {
          "_id": "67fddc276a3c533dc1d3bcc0",
          "user": {
            "_id": "67b4cdee376cfc783f9ec8cf",
            "avatarUrl": "/avatars/d9a9f320cd01dc5addfca14feefefef4.svg",
            "isPro": false,
            "fullname": "Meidani",
            "user": "mkmeidani",
            "type": "user"
          },
          "name": "Kazem Meidani",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-15T07:53:43.431Z",
          "hidden": false
        },
        {
          "_id": "67fddc276a3c533dc1d3bcc1",
          "name": "Amir Barati Farimani",
          "hidden": false
        },
        {
          "_id": "67fddc276a3c533dc1d3bcc2",
          "name": "Khoa D Doan",
          "hidden": false
        },
        {
          "_id": "67fddc276a3c533dc1d3bcc3",
          "name": "Chandan K Reddy",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-14T17:00:13.000Z",
      "submittedOnDailyAt": "2025-04-15T02:43:55.941Z",
      "title": "LLM-SRBench : Nouveau Benchmark pour la Détection d'Équations Scientifiques",
      "submittedOnDailyBy": {
        "_id": "6520621836008ecc88699622",
        "avatarUrl": "/avatars/b08c00af00f1736a4f4938443e575b0e.svg",
        "isPro": false,
        "fullname": "Parshin Shojaee",
        "user": "parshinsh",
        "type": "user"
      },
      "summary": "La découverte d'équations scientifiques est une tâche fondamentale pour le développement de la science et permet de calculer les lois qui régissent les phénomènes naturels. Récemment, les modèles de langage grands (LLMs) ont attiré de l'attention pour leur capacité à générer des hypothèses en utilisant des connaissances scientifiques. Cependant, évaluer leur véritable capacité à découvrir est complexe. Les benchmarks actuels utilisent principalement des équations généralement connues, ce qui peut exagerer la capacité de mémoire et ne pas refléter la capacité à découvrir. Dans cet article, nous présentons un benchmark détaillé appelé LLM-SRBench, qui comprend 239 problèmes difficiles dans quatre domaines scientifiques. Ce benchmark est utilisé pour évaluer la capacité de découverte d'équations scientifiques basée sur les LLMs, en minimisant l'influence de la mémoire et en évitant des problèmes facilement résolus. LLM-SRBench comprend deux catégories principales : LSR-Transform et LSR-Synth. LSR-Transform transforme des modèles physiques courants en expressions mathématiques plus détaillées pour tenter d'inférences qui dépassent les formats mémorisés. LSR-Synth propose des problèmes synthétiques qui incluent des découvertes, nécessitant des inférences basées sur des bases de données. Ces méthodes ont été évaluées largement, en utilisant à la fois des systèmes ouverts et fermés, et les meilleurs systèmes ont atteint une précision de signe du 31,5 %. Cette découverte souligne clairement les difficultés des équations scientifiques et LLM-SRBench devient une outil précieux pour futures recherches.",
      "upvotes": 6,
      "discussionId": "67fddc296a3c533dc1d3bd43",
      "projectPage": "https://huggingface.co/datasets/nnheui/llm-srbench",
      "githubRepo": "https://github.com/deep-symbolic-mathematics/llm-srbench"
    },
    "publishedAt": "2025-04-14T13:00:13.000Z",
    "title": "LLM-SRBench: A New Benchmark for Scientific Equation Discovery with\n  Large Language Models",
    "summary": "Scientific equation discovery is a fundamental task in the history of\nscientific progress, enabling the derivation of laws governing natural\nphenomena. Recently, Large Language Models (LLMs) have gained interest for this\ntask due to their potential to leverage embedded scientific knowledge for\nhypothesis generation. However, evaluating the true discovery capabilities of\nthese methods remains challenging, as existing benchmarks often rely on common\nequations that are susceptible to memorization by LLMs, leading to inflated\nperformance metrics that do not reflect discovery. In this paper, we introduce\nLLM-SRBench, a comprehensive benchmark with 239 challenging problems across\nfour scientific domains specifically designed to evaluate LLM-based scientific\nequation discovery methods while preventing trivial memorization. Our benchmark\ncomprises two main categories: LSR-Transform, which transforms common physical\nmodels into less common mathematical representations to test reasoning beyond\nmemorized forms, and LSR-Synth, which introduces synthetic, discovery-driven\nproblems requiring data-driven reasoning. Through extensive evaluation of\nseveral state-of-the-art methods, using both open and closed LLMs, we find that\nthe best-performing system so far achieves only 31.5% symbolic accuracy. These\nfindings highlight the challenges of scientific equation discovery, positioning\nLLM-SRBench as a valuable resource for future research.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10415.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6520621836008ecc88699622",
      "avatarUrl": "/avatars/b08c00af00f1736a4f4938443e575b0e.svg",
      "fullname": "Parshin Shojaee",
      "name": "parshinsh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.10127",
      "authors": [
        {
          "_id": "67fdf68d04d0302ef5ec0239",
          "user": {
            "_id": "63b76e716fc56e43c3c22ca8",
            "avatarUrl": "/avatars/99c760e6a60e65d69d57a8b92a40f623.svg",
            "isPro": false,
            "fullname": "Junlei Zhang",
            "user": "leoozy",
            "type": "user"
          },
          "name": "Junlei Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-15T07:52:43.667Z",
          "hidden": false
        },
        {
          "_id": "67fdf68d04d0302ef5ec023a",
          "user": {
            "_id": "642b9861bb77f8456634b048",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642b9861bb77f8456634b048/ZT-oJrw5BsADC-gZT_i25.jpeg",
            "isPro": false,
            "fullname": "Zichen Ding",
            "user": "heroding77",
            "type": "user"
          },
          "name": "Zichen Ding",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:29:38.552Z",
          "hidden": false
        },
        {
          "_id": "67fdf68d04d0302ef5ec023b",
          "user": {
            "_id": "637f22fd932a61b89aeeea37",
            "avatarUrl": "/avatars/342957f8242d4edaf1d58e1274313afe.svg",
            "isPro": false,
            "fullname": "Chang Ma",
            "user": "changma",
            "type": "user"
          },
          "name": "Chang Ma",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:29:44.813Z",
          "hidden": false
        },
        {
          "_id": "67fdf68d04d0302ef5ec023c",
          "name": "Zijie Chen",
          "hidden": false
        },
        {
          "_id": "67fdf68d04d0302ef5ec023d",
          "user": {
            "_id": "6064a0eeb1703ddba0d458b9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1617207525789-noauth.png",
            "isPro": false,
            "fullname": "Qiushi",
            "user": "QiushiSun",
            "type": "user"
          },
          "name": "Qiushi Sun",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:30:05.846Z",
          "hidden": false
        },
        {
          "_id": "67fdf68d04d0302ef5ec023e",
          "name": "Zhenzhong Lan",
          "hidden": false
        },
        {
          "_id": "67fdf68d04d0302ef5ec023f",
          "user": {
            "_id": "615f34ec3f6d24d67c1b5c78",
            "avatarUrl": "/avatars/6dcff6477993d9e57c5cb92b6f95eb66.svg",
            "isPro": false,
            "fullname": "Junxian He",
            "user": "jxhe",
            "type": "user"
          },
          "name": "Junxian He",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:30:19.230Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-14T11:35:02.000Z",
      "submittedOnDailyAt": "2025-04-15T06:39:52.966Z",
      "title": "Destruisant le mur des données, construisant des agents de l'interface graphique à travers la génération de tâches",
      "submittedOnDailyBy": {
        "_id": "63b76e716fc56e43c3c22ca8",
        "avatarUrl": "/avatars/99c760e6a60e65d69d57a8b92a40f623.svg",
        "isPro": false,
        "fullname": "Junlei Zhang",
        "user": "leoozy",
        "type": "user"
      },
      "summary": "La interface utilisateur graphique (GUI) des agents fournit des solutions cross-plataforma pour automatiser des tâches digitales complexes, ouvrant de grandes perspectives pour l'innovation dans les flux de travail productifs. Cependant, son rendement est limité par la rareté de trajectoires de haute qualité. Pour résoudre cette limitation, on propose d'entraîner des modèles de vision et de langage (VLMs) avec des tâches d'entraînement intermédiaires abondantes et logiques, et d'étudier comment l'introduction de ces tâches peut être généralisée aux scénarios de planification de GUI. On revisera des tâches qui peuvent être utilisées directement avec les mains, comme la reconnaissance de GUI, la logique multiforme et la logique textuelle. Des expériences très larges ont été réalisées avec 11 tâches d'entraînement intermédiaires, montrant les résultats suivants : 1) La généralisation des tâches est très efficace et améliore significativement dans de nombreux cas. Par exemple, la résolution de problèmes mathématiques avec la logique multiforme a obtenu un accroissement de 6,3% absolu sur AndroidWorld. En particulier, les données mathématiques basées sur le texte ont considérablement amélioré le rendement des agents web de GUI, démontrant une généralisation claire croisée de domaines de texte à vision sur WebArena avec un accroissement de 5,6% et sur AndroidWorld avec un accroissement de 5,4%. 2) Contrairement aux hypothèses précédentes, les données de reconnaissance de GUI sont étroitement alignées avec les tâches des agents de GUI et ont été largement utilisées, mais ont eu un impact relativement limité sur le rendement final. 3) En se basant sur ces observations, on a spécifié les meilleures tâches d'entraînement intermédiaires et on a combiné un ensemble de données optimisée, obtenant un accroissement de 8,0% absolu sur WebArena et un accroissement de 12,2% absolu sur AndroidWorld. Notre étude offre des insights précieux sur la propagation du savoir croisé de domaines dans les agents de GUI et fournit une approche pratique pour faire face au défi de la rareté des données dans ce domaine émergent. Le code, les données et les modèles sont disponibles sur https://github.com/hkust-nlp/GUIMid.",
      "upvotes": 5,
      "discussionId": "67fdf69304d0302ef5ec0377",
      "githubRepo": "https://github.com/hkust-nlp/GUIMid"
    },
    "publishedAt": "2025-04-14T07:35:02.000Z",
    "title": "Breaking the Data Barrier -- Building GUI Agents Through Task\n  Generalization",
    "summary": "Graphical User Interface (GUI) agents offer cross-platform solutions for\nautomating complex digital tasks, with significant potential to transform\nproductivity workflows. However, their performance is often constrained by the\nscarcity of high-quality trajectory data. To address this limitation, we\npropose training Vision Language Models (VLMs) on data-rich,\nreasoning-intensive tasks during a dedicated mid-training stage, and then\nexamine how incorporating these tasks facilitates generalization to GUI\nplanning scenarios. Specifically, we explore a range of tasks with readily\navailable instruction-tuning data, including GUI perception, multimodal\nreasoning, and textual reasoning. Through extensive experiments across 11\nmid-training tasks, we demonstrate that: (1) Task generalization proves highly\neffective, yielding substantial improvements across most settings. For\ninstance, multimodal mathematical reasoning enhances performance on\nAndroidWorld by an absolute 6.3%. Remarkably, text-only mathematical data\nsignificantly boosts GUI web agent performance, achieving a 5.6% improvement on\nWebArena and 5.4% improvement on AndroidWorld, underscoring notable cross-modal\ngeneralization from text-based to visual domains; (2) Contrary to prior\nassumptions, GUI perception data - previously considered closely aligned with\nGUI agent tasks and widely utilized for training - has a comparatively limited\nimpact on final performance; (3) Building on these insights, we identify the\nmost effective mid-training tasks and curate optimized mixture datasets,\nresulting in absolute performance gains of 8.0% on WebArena and 12.2% on\nAndroidWorld. Our work provides valuable insights into cross-domain knowledge\ntransfer for GUI agents and offers a practical approach to addressing data\nscarcity challenges in this emerging field. The code, data and models will be\navailable at https://github.com/hkust-nlp/GUIMid.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10127.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63b76e716fc56e43c3c22ca8",
      "avatarUrl": "/avatars/99c760e6a60e65d69d57a8b92a40f623.svg",
      "fullname": "Junlei Zhang",
      "name": "leoozy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.09689",
      "authors": [
        {
          "_id": "67fdc937089aec0f3b154dd7",
          "name": "Jiahao Qiu",
          "hidden": false
        },
        {
          "_id": "67fdc937089aec0f3b154dd8",
          "user": {
            "_id": "652abf5360e706730596e8f4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/zRFJ4FjZJZGkz2wH8PPmU.jpeg",
            "isPro": false,
            "fullname": "Yinghui He",
            "user": "yinghuihe",
            "type": "user"
          },
          "name": "Yinghui He",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:30:49.206Z",
          "hidden": false
        },
        {
          "_id": "67fdc937089aec0f3b154dd9",
          "user": {
            "_id": "674500b57a76d46e9141af8b",
            "avatarUrl": "/avatars/e70243c31e2ac9f000542e8504e65b51.svg",
            "isPro": false,
            "fullname": "Xinzhe Juan",
            "user": "ChrisJuan",
            "type": "user"
          },
          "name": "Xinzhe Juan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:30:56.417Z",
          "hidden": false
        },
        {
          "_id": "67fdc937089aec0f3b154dda",
          "user": {
            "_id": "67c0934fb47a12be9c9b0899",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67c0934fb47a12be9c9b0899/V6uJJZ_5ldTSbbSeShN-H.jpeg",
            "isPro": false,
            "fullname": "WangYM999",
            "user": "YimingWang",
            "type": "user"
          },
          "name": "Yiming Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:31:02.273Z",
          "hidden": false
        },
        {
          "_id": "67fdc937089aec0f3b154ddb",
          "name": "Yuhan Liu",
          "hidden": false
        },
        {
          "_id": "67fdc937089aec0f3b154ddc",
          "user": {
            "_id": "6657f2041d83f3ee61bf414d",
            "avatarUrl": "/avatars/e678fbbba2e93536dfc702e8ae629a95.svg",
            "isPro": false,
            "fullname": "zixin",
            "user": "yaozixin",
            "type": "user"
          },
          "name": "Zixin Yao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:31:38.642Z",
          "hidden": false
        },
        {
          "_id": "67fdc937089aec0f3b154ddd",
          "name": "Yue Wu",
          "hidden": false
        },
        {
          "_id": "67fdc937089aec0f3b154dde",
          "name": "Xun Jiang",
          "hidden": false
        },
        {
          "_id": "67fdc937089aec0f3b154ddf",
          "name": "Ling Yang",
          "hidden": false
        },
        {
          "_id": "67fdc937089aec0f3b154de0",
          "user": {
            "_id": "6599415e8c8ac79295e0b5e3",
            "avatarUrl": "/avatars/85500bc8d2cd51444adcc19b1f8db313.svg",
            "isPro": false,
            "fullname": "Mengdi Wang",
            "user": "Edify-Kd2024",
            "type": "user"
          },
          "name": "Mengdi Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:32:09.308Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-13T18:47:22.000Z",
      "submittedOnDailyAt": "2025-04-15T01:30:41.548Z",
      "title": "Emo Aju-jinsa : Évaluation et protection de l'interaction humain-IA en santé mentale",
      "submittedOnDailyBy": {
        "_id": "674500b57a76d46e9141af8b",
        "avatarUrl": "/avatars/e70243c31e2ac9f000542e8504e65b51.svg",
        "isPro": false,
        "fullname": "Xinzhe Juan",
        "user": "ChrisJuan",
        "type": "user"
      },
      "summary": "L'augmentation des personnages AI dirigés par des modèles de langage grands (LLM) tend à générer des préoccupations sur la sécurité pour les utilisateurs humains vulnérables avec des troubles mentaux. Pour faire face à ces menaces, on propose le cadre multi-agent AI appelé EmoAgent. EmoAgent est conçu pour évaluer et mitiguer les risques de santé mentale lors de l'interaction entre la personne et l'AI. Il comprend deux composants : EmoEval, qui simule des utilisateurs virtuels représentant des personnes avec des vulnérabilités mentales, et EmoGuard, qui surveille l'état mental de l'utilisateur et fournit des rétroactions pour mitiguer le risque. Selon les expérimentations réalisées sur Twitter, les conversations susceptibles d'aroger de l'intérêt émotionnel peuvent provoquer des détériorations psychologiques chez les utilisateurs vulnérables, avec un pourcentage d'utilisateurs qui ont subi un détérioration de 34,4% ou plus. EmoGuard aide à réduire significativement ce taux de détérioration et assure une interaction sécurisée entre l'AI et la personne. Le code est disponible sur https://github.com/1akaman/EmoAgent.",
      "upvotes": 2,
      "discussionId": "67fdc938089aec0f3b154e31",
      "githubRepo": "https://github.com/1akaman/EmoAgent"
    },
    "publishedAt": "2025-04-13T14:47:22.000Z",
    "title": "EmoAgent: Assessing and Safeguarding Human-AI Interaction for Mental\n  Health Safety",
    "summary": "The rise of LLM-driven AI characters raises safety concerns, particularly for\nvulnerable human users with psychological disorders. To address these risks, we\npropose EmoAgent, a multi-agent AI framework designed to evaluate and mitigate\nmental health hazards in human-AI interactions. EmoAgent comprises two\ncomponents: EmoEval simulates virtual users, including those portraying\nmentally vulnerable individuals, to assess mental health changes before and\nafter interactions with AI characters. It uses clinically proven psychological\nand psychiatric assessment tools (PHQ-9, PDI, PANSS) to evaluate mental risks\ninduced by LLM. EmoGuard serves as an intermediary, monitoring users' mental\nstatus, predicting potential harm, and providing corrective feedback to\nmitigate risks. Experiments conducted in popular character-based chatbots show\nthat emotionally engaging dialogues can lead to psychological deterioration in\nvulnerable users, with mental state deterioration in more than 34.4% of the\nsimulations. EmoGuard significantly reduces these deterioration rates,\nunderscoring its role in ensuring safer AI-human interactions. Our code is\navailable at: https://github.com/1akaman/EmoAgent",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.09689.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "674500b57a76d46e9141af8b",
      "avatarUrl": "/avatars/e70243c31e2ac9f000542e8504e65b51.svg",
      "fullname": "Xinzhe Juan",
      "name": "ChrisJuan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.10430",
      "authors": [
        {
          "_id": "67fe093302ae092e4306af12",
          "user": {
            "_id": "64c32a75d15a8812b71afc48",
            "avatarUrl": "/avatars/4186c592b00aea73fb8c5bb719935ce7.svg",
            "isPro": false,
            "fullname": "Minqian Liu",
            "user": "mqliu",
            "type": "user"
          },
          "name": "Minqian Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:32:23.136Z",
          "hidden": false
        },
        {
          "_id": "67fe093302ae092e4306af13",
          "user": {
            "_id": "64b6c686cf5117d7962d8f62",
            "avatarUrl": "/avatars/96ed7a9602aa4c21b3a3d89608e76dc8.svg",
            "isPro": false,
            "fullname": "Zhiyang Xu",
            "user": "Zhiyang03",
            "type": "user"
          },
          "name": "Zhiyang Xu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:32:36.033Z",
          "hidden": false
        },
        {
          "_id": "67fe093302ae092e4306af14",
          "name": "Xinyi Zhang",
          "hidden": false
        },
        {
          "_id": "67fe093302ae092e4306af15",
          "user": {
            "_id": "679d30bf48f48796199c415e",
            "avatarUrl": "/avatars/c28e0dd7c9f6f62bccdd8eb2c8772c14.svg",
            "isPro": false,
            "fullname": "Heajun An",
            "user": "aneverfull",
            "type": "user"
          },
          "name": "Heajun An",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:32:49.203Z",
          "hidden": false
        },
        {
          "_id": "67fe093302ae092e4306af16",
          "user": {
            "_id": "626e75252c2c6d44b30b1523",
            "avatarUrl": "/avatars/458102026030f20af5e8c3c34c9b598c.svg",
            "isPro": false,
            "fullname": "Sarvech Qadir",
            "user": "sarvech123",
            "type": "user"
          },
          "name": "Sarvech Qadir",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:32:54.912Z",
          "hidden": false
        },
        {
          "_id": "67fe093302ae092e4306af17",
          "name": "Qi Zhang",
          "hidden": false
        },
        {
          "_id": "67fe093302ae092e4306af18",
          "name": "Pamela J. Wisniewski",
          "hidden": false
        },
        {
          "_id": "67fe093302ae092e4306af19",
          "name": "Jin-Hee Cho",
          "hidden": false
        },
        {
          "_id": "67fe093302ae092e4306af1a",
          "name": "Sang Won Lee",
          "hidden": false
        },
        {
          "_id": "67fe093302ae092e4306af1b",
          "name": "Ruoxi Jia",
          "hidden": false
        },
        {
          "_id": "67fe093302ae092e4306af1c",
          "name": "Lifu Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-14T17:20:34.000Z",
      "submittedOnDailyAt": "2025-04-15T05:53:52.240Z",
      "title": "Les persuadeurs dangereux de LLM : Recherche sur la sécurité de la persuasion dans les modèles de langage à grande échelle",
      "submittedOnDailyBy": {
        "_id": "64c32a75d15a8812b71afc48",
        "avatarUrl": "/avatars/4186c592b00aea73fb8c5bb719935ce7.svg",
        "isPro": false,
        "fullname": "Minqian Liu",
        "user": "mqliu",
        "type": "user"
      },
      "summary": "Récemment, le développement de grands modèles de langue (LLMs) a montré que ces derniers peuvent approcher la persuasion humaine. Cependant, ce potentiel a généré des inquiétudes sur les risques de la persuasion menaçante menée par les LLMs. En particulier, il y a une préoccupation pour la possibilité que ces modèles puissent causer des influences démoralisantes à travers des techniques comme la manipulation, l'engagement et l'exploitation de vulnérabilités. Cet article réalise une revue systématique sur la sécurité de la persuasion menacée par les LLMs et étudie deux aspects importants : (1) si les LLMs rejettent adéquatement des tâches de persuasion démoralisantes et évitent des stratégies démoralisantes lors de leur exécution, et (2) comment les facteurs d'influence, comme les caractéristiques personnelles et les pressions externes, affectent leurs comportements. Pour cela, un premier cadre concret d'évaluation de la sécurité de la persuasion appelé PersuSafety est présenté, composé de trois étapes : génération de scénarios de persuasion, simulation de dialogues persuasifs et évaluation de la sécurité de la persuasion. PersuSafety couvre six thèmes de persuasion démoralisantes et quinze stratégies générales démoralisantes. À travers une large gamme d'expériences avec huit des LLMs les plus largement utilisés, il a été démontré que de nombreux modèles présentent des risques de sécurité importants, qu'ils échouent à l'identification de tâches de persuasion démoralisantes et utilisent diverses stratégies démoralisantes. Cette étude souligne la nécessité d'améliorer la régulation de la sécurité dans les cas de dialogues évolutifs et objectifs, comme la persuasion, pour améliorer la sécurité.",
      "upvotes": 1,
      "discussionId": "67fe093402ae092e4306af42"
    },
    "publishedAt": "2025-04-14T13:20:34.000Z",
    "title": "LLM Can be a Dangerous Persuader: Empirical Study of Persuasion Safety\n  in Large Language Models",
    "summary": "Recent advancements in Large Language Models (LLMs) have enabled them to\napproach human-level persuasion capabilities. However, such potential also\nraises concerns about the safety risks of LLM-driven persuasion, particularly\ntheir potential for unethical influence through manipulation, deception,\nexploitation of vulnerabilities, and many other harmful tactics. In this work,\nwe present a systematic investigation of LLM persuasion safety through two\ncritical aspects: (1) whether LLMs appropriately reject unethical persuasion\ntasks and avoid unethical strategies during execution, including cases where\nthe initial persuasion goal appears ethically neutral, and (2) how influencing\nfactors like personality traits and external pressures affect their behavior.\nTo this end, we introduce PersuSafety, the first comprehensive framework for\nthe assessment of persuasion safety which consists of three stages, i.e.,\npersuasion scene creation, persuasive conversation simulation, and persuasion\nsafety assessment. PersuSafety covers 6 diverse unethical persuasion topics and\n15 common unethical strategies. Through extensive experiments across 8 widely\nused LLMs, we observe significant safety concerns in most LLMs, including\nfailing to identify harmful persuasion tasks and leveraging various unethical\npersuasion strategies. Our study calls for more attention to improve safety\nalignment in progressive and goal-driven conversations such as persuasion.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10430.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64c32a75d15a8812b71afc48",
      "avatarUrl": "/avatars/4186c592b00aea73fb8c5bb719935ce7.svg",
      "fullname": "Minqian Liu",
      "name": "mqliu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.09763",
      "authors": [
        {
          "_id": "67fdf0faea4d2ba44335ffa7",
          "name": "Zaid Khan",
          "hidden": false
        },
        {
          "_id": "67fdf0faea4d2ba44335ffa8",
          "user": {
            "_id": "61781c4caf41befe8ff060e8",
            "avatarUrl": "/avatars/8871d7b046fc28cbc8638228da8e9737.svg",
            "isPro": false,
            "fullname": "Elias Stengel-Eskin",
            "user": "esteng",
            "type": "user"
          },
          "name": "Elias Stengel-Eskin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:33:58.055Z",
          "hidden": false
        },
        {
          "_id": "67fdf0faea4d2ba44335ffa9",
          "user": {
            "_id": "607aeae5d2cd8c150e6ae074",
            "avatarUrl": "/avatars/a087743b98b6fe2181283a9610db4ec4.svg",
            "isPro": false,
            "fullname": "Archiki Prasad",
            "user": "archiki",
            "type": "user"
          },
          "name": "Archiki Prasad",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:34:04.468Z",
          "hidden": false
        },
        {
          "_id": "67fdf0faea4d2ba44335ffaa",
          "user": {
            "_id": "5ffe32d8942cf3533d364449",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654821969191-5ffe32d8942cf3533d364449.jpeg",
            "isPro": false,
            "fullname": "Jaemin Cho",
            "user": "j-min",
            "type": "user"
          },
          "name": "Jaemin Cho",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:34:12.982Z",
          "hidden": false
        },
        {
          "_id": "67fdf0faea4d2ba44335ffab",
          "user": {
            "_id": "665d9d3a057f7c508f98c625",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/665d9d3a057f7c508f98c625/u1R9P9sJoAl4zEIcetbPy.jpeg",
            "isPro": false,
            "fullname": "Mohit Bansal",
            "user": "mohitbansal",
            "type": "user"
          },
          "name": "Mohit Bansal",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:34:19.233Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6301c3e0a123c93a5fb295ff/6qC9u4ryCuVji8lMCdWQN.png"
      ],
      "publishedAt": "2025-04-14T00:06:48.000Z",
      "submittedOnDailyAt": "2025-04-15T04:15:51.576Z",
      "title": "Abstrait des Caractéristiques Exécutables : Inférence dans le Programme de Génération de Problèmes Mathématiques Progressifs",
      "submittedOnDailyBy": {
        "_id": "6301c3e0a123c93a5fb295ff",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1661060051926-noauth.jpeg",
        "isPro": false,
        "fullname": "Zaid Khan",
        "user": "codezakh",
        "type": "user"
      },
      "summary": "Les scientifiques font souvent des inférences sur des étapes abstraites en se basant sur des exemples spécifiques et utilisent ces inférences pour générer de nouveaux exemples associés. Par exemple, des programmes qui codifient les règles formelles et les caractéristiques d'un système en code sont utilisés dans diverses domaines, allant du apprentissage réflexif (RL) jusqu'à la physique (moteur de simulation). Ces programmes peuvent être considérés comme des fonctions qui génèrent différentes sorties selon les paramètres (par exemple, configurations de grille ou états physiques initiaux). Nous introduisons la notion d'abstraction fonctionnelle concrète (EFA : Executable Functional Abstraction) pour les problèmes mathématiques, qui représente des fonctions concrètes de problèmes mathématiques. Ces structures peuvent générer des problèmes à travers la logique mathématique. Cependant, les études précédentes se limitaient à l'étude d'abstractions mathématiques de niveau primaire (en raison des règles simples pouvant facilement être codifiées dans des programmes), et la génération d'EFA pour les mathématiques avancées nécessite encore une intervention humaine. Dans ce travail, nous explorons la génération automatique d'EFA pour les problèmes mathématiques avancés. Nous implémentons la génération automatique d'EFA comme un problème de synthèse de programmes, et développons EFAGen, un système qui génère des programmes EFA candidats cohérents avec les problèmes et les solutions initiales, en conditionnant un modèle de langage de programmation (LLM) avec les problèmes et les solutions initiales. De plus, nous présentons une formation d'un modèle de LLM pour vérifier la structure d'EFA, en utilisant des tests unitaires et des incitations de récompense. Les EFA générés par EFAGen sont cohérents avec les problèmes initiaux, génèrent des changements dans les problèmes qui sont apprenables, et EFAGen peut prédire des problèmes mathématiques de différents niveaux à partir de diverses sources. Enfin, nous montrons des cas d'utilisation d'EFA modélisées, démontrant qu'ils peuvent générer des changements dans les problèmes difficiles à résoudre ou générer des données.",
      "upvotes": 1,
      "discussionId": "67fdf0fbea4d2ba44335ffdd",
      "projectPage": "https://zaidkhan.me/EFAGen"
    },
    "publishedAt": "2025-04-13T20:06:48.000Z",
    "title": "Executable Functional Abstractions: Inferring Generative Programs for\n  Advanced Math Problems",
    "summary": "Scientists often infer abstract procedures from specific instances of\nproblems and use the abstractions to generate new, related instances. For\nexample, programs encoding the formal rules and properties of a system have\nbeen useful in fields ranging from RL (procedural environments) to physics\n(simulation engines). These programs can be seen as functions which execute to\ndifferent outputs based on their parameterizations (e.g., gridworld\nconfiguration or initial physical conditions). We introduce the term EFA\n(Executable Functional Abstraction) to denote such programs for math problems.\nEFA-like constructs have been shown to be useful for math reasoning as problem\ngenerators for stress-testing models. However, prior work has been limited to\nabstractions for grade-school math (whose simple rules are easy to encode in\nprograms), while generating EFAs for advanced math has thus far required human\nengineering. We explore the automatic construction of EFAs for advanced math\nproblems. We operationalize the task of automatically constructing EFAs as a\nprogram synthesis task, and develop EFAGen, which conditions an LLM on a seed\nmath problem and its step-by-step solution to generate candidate EFA programs\nthat are faithful to the generalized problem and solution class underlying the\nseed problem. Furthermore, we formalize properties any valid EFA must possess\nin terms of executable unit tests, and show how the tests can be used as\nverifiable rewards to train LLMs to become better writers of EFAs. We\ndemonstrate that EFAs constructed by EFAGen behave rationally by remaining\nfaithful to seed problems, produce learnable problem variations, and that\nEFAGen can infer EFAs across multiple diverse sources of competition-level math\nproblems. Finally, we show downstream uses of model-written EFAs e.g. finding\nproblem variations that are harder or easier for a learner to solve, as well as\ndata generation.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6301c3e0a123c93a5fb295ff/6qC9u4ryCuVji8lMCdWQN.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.09763.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6301c3e0a123c93a5fb295ff",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1661060051926-noauth.jpeg",
      "fullname": "Zaid Khan",
      "name": "codezakh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.09130",
      "authors": [
        {
          "_id": "67fe0d1f3a2e18d214499d3c",
          "user": {
            "_id": "627b73728b6ecd7ece822825",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/627b73728b6ecd7ece822825/QV-sT0vwupGZYg-loLPRw.jpeg",
            "isPro": false,
            "fullname": "Yikun Wang",
            "user": "LibraTree",
            "type": "user"
          },
          "name": "Yikun Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-15T07:52:28.698Z",
          "hidden": false
        },
        {
          "_id": "67fe0d1f3a2e18d214499d3d",
          "user": {
            "_id": "64c3c631e77ea9f28111172a",
            "avatarUrl": "/avatars/495dbb73b69c399bae780da3118e332f.svg",
            "isPro": false,
            "fullname": "Siyin Wang",
            "user": "sinwang",
            "type": "user"
          },
          "name": "Siyin Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:34:42.695Z",
          "hidden": false
        },
        {
          "_id": "67fe0d1f3a2e18d214499d3e",
          "name": "Qinyuan Cheng",
          "hidden": false
        },
        {
          "_id": "67fe0d1f3a2e18d214499d3f",
          "user": {
            "_id": "629ef8544313a7c1dd671130",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/629ef8544313a7c1dd671130/i5xfHIgELcuO1Ew19ebTw.png",
            "isPro": false,
            "fullname": "Zhaoye Fei",
            "user": "ngc7293",
            "type": "user"
          },
          "name": "Zhaoye Fei",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:34:57.322Z",
          "hidden": false
        },
        {
          "_id": "67fe0d1f3a2e18d214499d40",
          "user": {
            "_id": "641123b4230ce11b1be68fa1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/641123b4230ce11b1be68fa1/kGURwBB-0f1TvgxwvcUWZ.png",
            "isPro": false,
            "fullname": "Liang Ding",
            "user": "alphadl",
            "type": "user"
          },
          "name": "Liang Ding",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:35:04.032Z",
          "hidden": false
        },
        {
          "_id": "67fe0d1f3a2e18d214499d41",
          "user": {
            "_id": "6491cd52b1e5d3444528edb1",
            "avatarUrl": "/avatars/a85635d886c7f157b6723dec5c01c030.svg",
            "isPro": false,
            "fullname": "Qipeng Guo",
            "user": "QipengGuo",
            "type": "user"
          },
          "name": "Qipeng Guo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:35:09.688Z",
          "hidden": false
        },
        {
          "_id": "67fe0d1f3a2e18d214499d42",
          "name": "Dacheng Tao",
          "hidden": false
        },
        {
          "_id": "67fe0d1f3a2e18d214499d43",
          "user": {
            "_id": "61457b8deff2c9fdb4de4988",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1632381702899-61457b8deff2c9fdb4de4988.jpeg",
            "isPro": false,
            "fullname": "Xipeng Qiu",
            "user": "xpqiu",
            "type": "user"
          },
          "name": "Xipeng Qiu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:35:23.363Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-12T08:37:30.000Z",
      "submittedOnDailyAt": "2025-04-15T06:45:12.936Z",
      "title": "VisuoThink : Recherche de listes de modèles et de ressources pour améliorer les capacités cognitives des LVLM en utilisant la recherche d'arbres multimodalités.",
      "submittedOnDailyBy": {
        "_id": "627b73728b6ecd7ece822825",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/627b73728b6ecd7ece822825/QV-sT0vwupGZYg-loLPRw.jpeg",
        "isPro": false,
        "fullname": "Yikun Wang",
        "user": "LibraTree",
        "type": "user"
      },
      "summary": "Le développement récent des modèles de langage de tableau a démontré des capacités exceptionnelles. Cependant, ils manquent significativement dans les tâches complexes de raisonnement cognitif qui nécessitent la résolution de problèmes actifs et de manière progressive, comme le cas des personnes. Les méthodes existantes se concentrent uniquement sur la mémoire lente basée sur le texte ou sur des dispositifs de surveillance visuelle de base, mais ne comprennent pas les structures complexes et croisées de la cognition visuelle-linguistique humaine. Pour surmonter ces limites et inspirés par la structure lente de la mémoire cognitive humaine, nous présentons un nouveau cadre de travail appelé VisuoThink. VisuoThink intègre finalement le domaine visuel et le domaine linguistique, permettant le développement d'une cognition visuelle-textuelle avancée. VisuoThink améliore significativement les fonctions cognitives grâce à sa scalabilité dans le traitement, et, sans la finalisation de la configuration, atteint les meilleurs résultats dans les tâches de géométrie et de raisonnement spatial.",
      "upvotes": 1,
      "discussionId": "67fe0d203a2e18d214499d9f",
      "githubRepo": "https://github.com/ekonwang/VisuoThink"
    },
    "publishedAt": "2025-04-12T04:37:30.000Z",
    "title": "VisuoThink: Empowering LVLM Reasoning with Multimodal Tree Search",
    "summary": "Recent advancements in Large Vision-Language Models have showcased remarkable\ncapabilities. However, they often falter when confronted with complex reasoning\ntasks that humans typically address through visual aids and deliberate,\nstep-by-step thinking. While existing methods have explored text-based slow\nthinking or rudimentary visual assistance, they fall short of capturing the\nintricate, interleaved nature of human visual-verbal reasoning processes. To\novercome these limitations and inspired by the mechanisms of slow thinking in\nhuman cognition, we introduce VisuoThink, a novel framework that seamlessly\nintegrates visuospatial and linguistic domains. VisuoThink facilitates\nmultimodal slow thinking by enabling progressive visual-textual reasoning and\nincorporates test-time scaling through look-ahead tree search. Extensive\nexperiments demonstrate that VisuoThink significantly enhances reasoning\ncapabilities via inference-time scaling, even without fine-tuning, achieving\nstate-of-the-art performance in tasks involving geometry and spatial reasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.09130.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "627b73728b6ecd7ece822825",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/627b73728b6ecd7ece822825/QV-sT0vwupGZYg-loLPRw.jpeg",
      "fullname": "Yikun Wang",
      "name": "LibraTree",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.10449",
      "authors": [
        {
          "_id": "67fe15543aa5a5684ca7229e",
          "user": {
            "_id": "63dc68bea99b2c8a7c20f1d0",
            "avatarUrl": "/avatars/cf8ddc91415ef1f895803f4390ff1f6f.svg",
            "isPro": true,
            "fullname": "Junxiong Wang",
            "user": "JunxiongWang",
            "type": "user"
          },
          "name": "Junxiong Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:35:36.587Z",
          "hidden": false
        },
        {
          "_id": "67fe15543aa5a5684ca7229f",
          "user": {
            "_id": "62e221dfcb1f164f2cb8a66b",
            "avatarUrl": "/avatars/06f05622e232304d3f0b8c291f3263be.svg",
            "isPro": true,
            "fullname": "Wen-Ding Li",
            "user": "xu3kev",
            "type": "user"
          },
          "name": "Wen-Ding Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:35:44.671Z",
          "hidden": false
        },
        {
          "_id": "67fe15543aa5a5684ca722a0",
          "name": "Daniele Paliotta",
          "hidden": false
        },
        {
          "_id": "67fe15543aa5a5684ca722a1",
          "name": "Daniel Ritter",
          "hidden": false
        },
        {
          "_id": "67fe15543aa5a5684ca722a2",
          "user": {
            "_id": "67745f42633d42196543820f",
            "avatarUrl": "/avatars/6faced0d6486b04262a8d7bc3990262b.svg",
            "isPro": false,
            "fullname": "Alexander Rush",
            "user": "voidptr74",
            "type": "user"
          },
          "name": "Alexander M. Rush",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:36:08.329Z",
          "hidden": false
        },
        {
          "_id": "67fe15543aa5a5684ca722a3",
          "user": {
            "_id": "64b8a6b5cf14c2fabe98159b",
            "avatarUrl": "/avatars/dbc009451865435bf290791beadc4723.svg",
            "isPro": false,
            "fullname": "Tri Dao",
            "user": "tridao",
            "type": "user"
          },
          "name": "Tri Dao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:36:25.942Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-14T17:38:25.000Z",
      "submittedOnDailyAt": "2025-04-15T06:44:40.178Z",
      "title": "M1 : On utilise les modèles de raisonnement Mamba pour améliorer l'échellabilité des temps de calcul.",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Le motif efficace est très important pour résoudre des problèmes mathématiques complexes. Les modèles de langage à grande échelle (LLMs) récents utilisent l'échelle de l'inférence d'une chaîne de pensée longue pour améliorer le rendement. Cependant, les modèles basés sur Transformer sont limités fondamentalement par la complexité du calcul secondaire et la nécessité de mémoire linéaire, ce qui empêche d'étendre la longueur du contexte. Dans cet article, nous présentons M1, un nouveau modèle de raisonnement hybride linéaire RNN basé sur l'architecture Mamba, pour faciliter l'inférence efficace en mémoire. Notre approche utilise le processus de dégradation des modèles de raisonnement existants et a été renforcée par l'entraînement par renforcement. Les résultats des expériences sur AIME et MATH montrent que M1 a démontré un excellent rendement au-delà des modèles linéaires RNN et comparé au modèle de raisonnement dégradé Deepseek R1 de la même échelle, a présenté un rendement égal. De plus, comparé au générateur de haute qualité vLLM, notre générateur a démontré une augmentation de vitesse plus de 3 fois plus que le Transformer de la même taille. Cette augmentation de vitesse permet au modèle de raisonnement dégradé de DeepSeek R1 dans le temps de génération fixe, d'atteindre une précision plus élevée que le modèle de raisonnement dégradé de Transformer. En résumé, lorsque l'on utilise l'auto-cohérence ou une raison longue de chaîne de pensée pour la génération en temps de test, nous présentons un modèle de raisonnement hybride Mamba et une approximation plus efficace.",
      "upvotes": 0,
      "discussionId": "67fe15553aa5a5684ca722d5"
    },
    "publishedAt": "2025-04-14T13:38:25.000Z",
    "title": "M1: Towards Scalable Test-Time Compute with Mamba Reasoning Models",
    "summary": "Effective reasoning is crucial to solving complex mathematical problems.\nRecent large language models (LLMs) have boosted performance by scaling\ntest-time computation through long chain-of-thought reasoning. However,\ntransformer-based models are inherently limited in extending context length due\nto their quadratic computational complexity and linear memory requirements. In\nthis paper, we introduce a novel hybrid linear RNN reasoning model, M1, built\non the Mamba architecture, which allows memory-efficient inference. Our\napproach leverages a distillation process from existing reasoning models and is\nfurther enhanced through RL training. Experimental results on the AIME and MATH\nbenchmarks show that M1 not only outperforms previous linear RNN models but\nalso matches the performance of state-of-the-art Deepseek R1 distilled\nreasoning models at a similar scale. We also compare our generation speed with\na highly performant general purpose inference engine, vLLM, and observe more\nthan a 3x speedup compared to a same size transformer. With throughput speedup,\nwe are able to achieve higher accuracy compared to DeepSeek R1 distilled\ntransformer reasoning models under a fixed generation time budget using\nself-consistency voting. Overall, we introduce a hybrid Mamba reasoning model\nand provide a more effective approach to scaling test-time generation using\nself-consistency or long chain of thought reasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10449.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6654
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.09522",
      "authors": [
        {
          "_id": "67fe132f1d1bc292f7cdbc0f",
          "name": "Chen Sun",
          "hidden": false
        },
        {
          "_id": "67fe132f1d1bc292f7cdbc10",
          "user": {
            "_id": "6003c87f532970af8c4d3a4a",
            "avatarUrl": "/avatars/a4c5fbe427791d02e3cee208f22f18c4.svg",
            "isPro": false,
            "fullname": "Renat Aksitov",
            "user": "mendor",
            "type": "user"
          },
          "name": "Renat Aksitov",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:36:40.229Z",
          "hidden": false
        },
        {
          "_id": "67fe132f1d1bc292f7cdbc11",
          "name": "Andrey Zhmoginov",
          "hidden": false
        },
        {
          "_id": "67fe132f1d1bc292f7cdbc12",
          "name": "Nolan Andrew Miller",
          "hidden": false
        },
        {
          "_id": "67fe132f1d1bc292f7cdbc13",
          "user": {
            "_id": "6502fe0bb1792803da806d42",
            "avatarUrl": "/avatars/60280ce59f1f0d67e4210b0453039282.svg",
            "isPro": false,
            "fullname": "Max Vladymyrov",
            "user": "gozzo87",
            "type": "user"
          },
          "name": "Max Vladymyrov",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:37:00.587Z",
          "hidden": false
        },
        {
          "_id": "67fe132f1d1bc292f7cdbc14",
          "name": "Ulrich Rueckert",
          "hidden": false
        },
        {
          "_id": "67fe132f1d1bc292f7cdbc15",
          "name": "Been Kim",
          "hidden": false
        },
        {
          "_id": "67fe132f1d1bc292f7cdbc16",
          "name": "Mark Sandler",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-13T11:25:04.000Z",
      "submittedOnDailyAt": "2025-04-15T06:35:36.514Z",
      "title": "Les nouveaux données se développent dans le domaine des connaissances et des communications des modèles de langage d'intelligence artificielle, et on se demande comment cette expansion peut être réduite.",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Les modèles de langue généraliste apprennent par l'accumulation d'actualisations basées sur les gradients et continuent d'apprendre. Cependant, il n'est pas bien compris comment une nouvelle information affecte la connaissance existante, ni comment cela peut générer une généralisation avantageuse et des erreurs dans l'inférence. Nous montrons que lors du processus d'apprentissage de nouvelle information, les modèles de langue généraliste (LLMs) peuvent présenter un effet de « prémining » : l'apprentissage de nouveaux faits permet au modèle d'appliquer des connaissances appropriées dans des contextes non pertinents. Pour étudier ce phénomène systématiquement, nous présentons un ensemble de données \"Outlandish\", composé de 1.320 échantillons de texte divers. En utilisant cet ensemble de données, nous montrons que le degré de l'effet de « prémining » après l'apprentissage de nouvelle information peut être prédit en mesurant la probabilité des tokens de mots clés avant l'apprentissage. Cette relation est fortement maintenue dans différentes architectures de modèle (PALM-2, Gemma, Llama), tailles et étapes d'entraînement. Finalement, pour ajuster comment la nouvelle information affecte le comportement existant du modèle, nous avons développé deux nouveaux méthodes : (1) la stratégie des « points de passage » pour ajouter du texte et (2) le méthode « ignore-k » pour le prémining des actualisations. Ces approches réduisent de 50 à 95% l'effet de « prémining » insatisfaisant et maintiennent la capacité du modèle d'apprendre de nouvelle information. Nos résultats montrent comment les modèles de langue généraliste apprennent et fournissent des outils efficaces pour améliorer l'intégration de connaissance dans les modèles de langue. Informations supplémentaires : https://sunchipsster1.github.io/projects/outlandish/",
      "upvotes": 0,
      "discussionId": "67fe13341d1bc292f7cdbd2f"
    },
    "publishedAt": "2025-04-13T07:25:04.000Z",
    "title": "How new data permeates LLM knowledge and how to dilute it",
    "summary": "Large language models learn and continually learn through the accumulation of\ngradient-based updates, but how individual pieces of new information affect\nexisting knowledge, leading to both beneficial generalization and problematic\nhallucination, remains poorly understood. We demonstrate that when learning new\ninformation, LLMs exhibit a \"priming\" effect: learning a new fact can cause the\nmodel to inappropriately apply that knowledge in unrelated contexts. To\nsystematically study this phenomenon, we introduce \"Outlandish,\" a carefully\ncurated dataset of 1320 diverse text samples designed to probe how new\nknowledge permeates through an LLM's existing knowledge base. Using this\ndataset, we show that the degree of priming after learning new information can\nbe predicted by measuring the token probability of key words before learning.\nThis relationship holds robustly across different model architectures (PALM-2,\nGemma, Llama), sizes, and training stages. Finally, we develop two novel\ntechniques to modulate how new knowledge affects existing model behavior: (1) a\n``stepping-stone'' text augmentation strategy and (2) an ``ignore-k'' update\npruning method. These approaches reduce undesirable priming effects by 50-95\\%\nwhile preserving the model's ability to learn new information. Our findings\nprovide both empirical insights into how LLMs learn and practical tools for\nimproving the specificity of knowledge insertion in language models. Further\nmaterials: https://sunchipsster1.github.io/projects/outlandish/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.09522.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6654
    },
    "isAuthorParticipating": false
  }
]