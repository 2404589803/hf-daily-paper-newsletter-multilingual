[
  {
    "paper": {
      "id": "2503.05236",
      "authors": [
        {
          "_id": "67ce37239f9aaaae837f3894",
          "user": {
            "_id": "654c6845bac6e6e49895a5b5",
            "avatarUrl": "/avatars/ed1f140abcd4d76669e2e48db1d1193f.svg",
            "isPro": false,
            "fullname": "Yibin Wang",
            "user": "CodeGoat24",
            "type": "user"
          },
          "name": "Yibin Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T09:37:51.835Z",
          "hidden": false
        },
        {
          "_id": "67ce37239f9aaaae837f3895",
          "user": {
            "_id": "63859cf3b2906edaf83af9f0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63859cf3b2906edaf83af9f0/iUQm5FAomzqYi6fkqIn9F.jpeg",
            "isPro": false,
            "fullname": "Yuhang Zang",
            "user": "yuhangzang",
            "type": "user"
          },
          "name": "Yuhang Zang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-10T08:01:24.660Z",
          "hidden": false
        },
        {
          "_id": "67ce37239f9aaaae837f3896",
          "name": "Hao Li",
          "hidden": false
        },
        {
          "_id": "67ce37239f9aaaae837f3897",
          "name": "Cheng Jin",
          "hidden": false
        },
        {
          "_id": "67ce37239f9aaaae837f3898",
          "user": {
            "_id": "64638c4d51fa6e63060521b5",
            "avatarUrl": "/avatars/c863ace5b1dc788a341bcf4ddbdfaec1.svg",
            "isPro": false,
            "fullname": "JIaqi",
            "user": "Jiaqiwang",
            "type": "user"
          },
          "name": "Jiaqi Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T09:38:17.938Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-07T08:36:05.000Z",
      "title": "Modèle de prix intégré : compréhension et génération de multiples modes",
      "summary": "Durant l'actualisation des activités humaines, un grand développement a été observé dans la génération et la compréhension de modèles multimodal. L'une des principales façons d'aborder ce sujet est par l'entraînement de modèles de récompense pour optimiser les activités. Cependant, actuellement les modèles sont généralement spécialisés dans des tâches spécifiques, ce qui limite leur adaptation à diverses applications visuelles. Il a également été argumenté que l'évaluation de plusieurs tâches en même temps peut encourager l'interaction. Dans ce contexte, cet article propose le premier modèle de base, \"UnifiedReward\". Ce modèle permet la compréhension et l'évaluation de plusieurs modèles, ainsi que la réalisation d'un classement partiel et de noter. Concrètement, (1) nous avons développé un modèle de base sur un ensemble de données humaines d'activités à grande échelle, qui comprend des tâches de génération et de compréhension d'images et de vidéos. (2) Ensuite, des données de paires d'activités de haute qualité ont été automatiquement construites et filtrées par classement partiel et noter. (3) Finalement, ces données sont directement utilisées pour l'optimisation des activités (DPO) pour ajuster les activités. Les résultats des expérimentations montrent qu'il est possible d'obtenir un grand bénéfice mutuel par l'évaluation de différentes tâches visuelles, et que ce processus appliqué à la compréhension et à la génération d'images et de vidéos améliore significativement le rendement dans chaque domaine.",
      "upvotes": 72,
      "discussionId": "67ce37259f9aaaae837f3948",
      "projectPage": "https://codegoat24.github.io/UnifiedReward/",
      "githubRepo": "https://github.com/CodeGoat24/UnifiedReward"
    },
    "publishedAt": "2025-03-09T22:20:09.137Z",
    "title": "Unified Reward Model for Multimodal Understanding and Generation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05236.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "654c6845bac6e6e49895a5b5",
      "avatarUrl": "/avatars/ed1f140abcd4d76669e2e48db1d1193f.svg",
      "fullname": "Yibin Wang",
      "name": "CodeGoat24",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.05179",
      "authors": [
        {
          "_id": "67ce4bff5847e4787a7ebedd",
          "user": {
            "_id": "65f4060754ecda1ecb5797a0",
            "avatarUrl": "/avatars/f8b44524d36b505673cb538fd7895a82.svg",
            "isPro": false,
            "fullname": "Simon Aytes",
            "user": "saytes",
            "type": "user"
          },
          "name": "Simon A. Aytes",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-10T08:01:10.363Z",
          "hidden": false
        },
        {
          "_id": "67ce4bff5847e4787a7ebede",
          "user": {
            "_id": "63036b6c5c70c21d0ea79d48",
            "avatarUrl": "/avatars/a7eb03f5cbd4eaa09fe807bbed8bc0f7.svg",
            "isPro": false,
            "fullname": "Jinheon Baek",
            "user": "jinheon",
            "type": "user"
          },
          "name": "Jinheon Baek",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-10T08:01:13.328Z",
          "hidden": false
        },
        {
          "_id": "67ce4bff5847e4787a7ebedf",
          "name": "Sung Ju Hwang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-07T06:57:17.000Z",
      "title": "Le schéma d'escrime : un modèle basé sur la cognition adaptative à travers la logique de modèles de grands langages efficaces",
      "summary": "Le développement récent de modèles de langage grands a démontré une capacité logique impressionnante grâce à la technique de Prompting Chain of Thought (CoT), mais la longueur excessive des résultats intermédiaires a augmenté l'encombrement informatique. Nous présentons un nouveau cadre de Prompting appelé Sketch-of-Thought (SoT). Cette approche vise à minimiser la quantité de tokens utilisés tout en maintenant la précision logique, en combinant des paradigmes logiques basés sur les sciences cognitives avec des contraintes linguistiques. SoT est un cadre flexible conçu pour inclure n'importe quel paradigme logique défini par l'utilisateur basé sur les sciences cognitives. Nous avons mis en œuvre trois paradigmes logiques : Conceptual Chaining, Chunked Symbolism et Expert Lexicons, chacun adapté à des tâches logiques spécifiques et sélectionnables dynamiquement par des modèles légers. SoT a été validé dans de multiples scénarios de langages et de modèles, composés de 15 ensembles de données logiques, et a montré réduire la quantité de tokens d'un 76 %, sans affecter la taux d'erreurs. Dans des domaines spécifiques comme les mathématiques ou la logique multiniveau, il peut améliorer la précision avec un nombre minimal de tokens. Notre code est disponible pour l'utilisation publique : https://www.github.com/SimonAytes/SoT.",
      "upvotes": 27,
      "discussionId": "67ce4c035847e4787a7ebf4c",
      "projectPage": "https://huggingface.co/saytes/SoT_DistilBERT",
      "githubRepo": "https://github.com/SimonAytes/SoT"
    },
    "publishedAt": "2025-03-09T22:25:52.244Z",
    "title": "Sketch-of-Thought: Efficient LLM Reasoning with Adaptive Cognitive-Inspired Sketching",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05179.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "63036b6c5c70c21d0ea79d48",
      "avatarUrl": "/avatars/a7eb03f5cbd4eaa09fe807bbed8bc0f7.svg",
      "fullname": "Jinheon Baek",
      "name": "jinheon",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.05500",
      "authors": [
        {
          "_id": "67ce9626e5cdfda52b9e8839",
          "user": {
            "_id": "62be186a5f59ff2320e6e32b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62be186a5f59ff2320e6e32b/W_emoC2uItM-MJZyCfIKI.png",
            "isPro": false,
            "fullname": "Nicolas-BZRD",
            "user": "Nicolas-BZRD",
            "type": "user"
          },
          "name": "Nicolas Boizard",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T09:42:06.860Z",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e883a",
          "user": {
            "_id": "65fa95405355a52c784633fc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65fa95405355a52c784633fc/rSfBUHPa7eSAsLd8DuOq4.png",
            "isPro": false,
            "fullname": "Hippolyte Gisserot-Boukhlef",
            "user": "hgissbkh",
            "type": "user"
          },
          "name": "Hippolyte Gisserot-Boukhlef",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T09:42:13.176Z",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e883b",
          "user": {
            "_id": "64132452d8a418df415a6ded",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64132452d8a418df415a6ded/qkjL5G89uldHUXlCI3n4f.jpeg",
            "isPro": false,
            "fullname": "Duarte Alves",
            "user": "DuarteMRAlves",
            "type": "user"
          },
          "name": "Duarte M. Alves",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T09:42:23.055Z",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e883c",
          "name": "André Martins",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e883d",
          "user": {
            "_id": "63937b399762cdd66be2a32f",
            "avatarUrl": "/avatars/7aefd888a3c54673d5881dcef61f771b.svg",
            "isPro": false,
            "fullname": "Ayoub Hammal",
            "user": "ayoubhammal",
            "type": "user"
          },
          "name": "Ayoub Hammal",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T09:42:42.527Z",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e883e",
          "user": {
            "_id": "677bedd522ca8585ede98470",
            "avatarUrl": "/avatars/54bca410c446610f02aca55918c74518.svg",
            "isPro": false,
            "fullname": "Caio Corro",
            "user": "caiocorro",
            "type": "user"
          },
          "name": "Caio Corro",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T09:42:48.603Z",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e883f",
          "user": {
            "_id": "61efea03a57920a251ec19b8",
            "avatarUrl": "/avatars/f47c8e3cb17a2bf7d43f2c152bb86885.svg",
            "isPro": false,
            "fullname": "Celine Hudelot",
            "user": "CelineH",
            "type": "user"
          },
          "name": "Céline Hudelot",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T09:51:41.273Z",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e8840",
          "user": {
            "_id": "66f2d6a684a241caac8e16dc",
            "avatarUrl": "/avatars/81acb87c2b07bea938251b40a2139911.svg",
            "isPro": false,
            "fullname": "Emmanuel Malherbe",
            "user": "emmanuelmalherbe",
            "type": "user"
          },
          "name": "Emmanuel Malherbe",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T09:51:47.996Z",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e8841",
          "name": "Etienne Malaboeuf",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e8842",
          "user": {
            "_id": "6708db59caf70ddea8e1355d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6708db59caf70ddea8e1355d/C6T16AdpqoeWCk7Gg9wSH.jpeg",
            "isPro": false,
            "fullname": "Fanny Jourdan",
            "user": "Fannyjrd",
            "type": "user"
          },
          "name": "Fanny Jourdan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T09:52:09.223Z",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e8843",
          "user": {
            "_id": "67cafedda972115e89972cd7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/P_xComqG9IttvluN-6tyB.png",
            "isPro": false,
            "fullname": "Gabriel Hautreux",
            "user": "GabrielHau",
            "type": "user"
          },
          "name": "Gabriel Hautreux",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T09:52:15.512Z",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e8844",
          "user": {
            "_id": "6772bde5c997eeb5550e80ea",
            "avatarUrl": "/avatars/8134a4d9330317e748dc7b33e1bb25f6.svg",
            "isPro": false,
            "fullname": "João Alves",
            "user": "albusonrails",
            "type": "user"
          },
          "name": "João Alves",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T09:52:22.630Z",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e8845",
          "user": {
            "_id": "66e2c22d7cc3edd60d725267",
            "avatarUrl": "/avatars/b217c5708c7dba8b1c220f37984ccc1e.svg",
            "isPro": false,
            "fullname": "Kevin El Haddad",
            "user": "kelhad",
            "type": "user"
          },
          "name": "Kevin El-Haddad",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T09:52:31.191Z",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e8846",
          "user": {
            "_id": "60f2e021adf471cbdf8bb660",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654090481550-60f2e021adf471cbdf8bb660.jpeg",
            "isPro": false,
            "fullname": "Manuel Faysse",
            "user": "manu",
            "type": "user"
          },
          "name": "Manuel Faysse",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T09:52:38.114Z",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e8847",
          "user": {
            "_id": "6369394dd322a76e1ea4bdf6",
            "avatarUrl": "/avatars/a4e5ab0167025fbbfc970d54630ce754.svg",
            "isPro": false,
            "fullname": "Maxime Peyrard",
            "user": "peyrardm",
            "type": "user"
          },
          "name": "Maxime Peyrard",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T09:52:44.389Z",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e8848",
          "user": {
            "_id": "67b622d2df3a86fbca306c43",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/lNlshrl56oaKslArMzSzj.png",
            "isPro": false,
            "fullname": "Nuno  Guerreiro",
            "user": "nunogj",
            "type": "user"
          },
          "name": "Nuno M. Guerreiro",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T09:52:54.367Z",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e8849",
          "name": "Patrick Fernandes",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e884a",
          "name": "Ricardo Rei",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e884b",
          "user": {
            "_id": "644a900e3a619fe72b14af0f",
            "avatarUrl": "/avatars/e2d5dac3d92757ed48e37e126a3464a3.svg",
            "isPro": false,
            "fullname": "Colombo",
            "user": "PierreColombo",
            "type": "user"
          },
          "name": "Pierre Colombo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T09:41:26.353Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-07T15:13:58.000Z",
      "title": "Eurobert : Méthode pour étendre l'encodeur de langue orientale européenne",
      "summary": "La représentation vectorielle multilingue est généralement utilisée dans les recherches, les régressions et les classifications. Ces représentations peuvent être obtenues à partir de modèles de codificateurs bidirectionnels. De plus, elles sont largement appliquées. Cependant, récemment, les codificateurs ont démontré des résultats exceptionnels grâce au développement de modèles de codificateurs génératifs. Cependant, la majorité de ces progrès est directement liée aux codificateurs. Dans cet article, on réévalue le développement des codificateurs multilingues en se concentrant sur EuroBERT, une famille de codificateurs multilingues couvrant plusieurs langues d'Europe et du monde. Notre modèle a amélioré la capacité multilingue, mathématique et de code dans différentes tâches de diverses domaines. De plus, il supporte des séquences de 8 192 tokens. De plus, on examine les décisions de conception de EuroBERT, fournissant des rétroactions sur la configuration du jeu de données et le processus d'entraînement. On publie le modèle EuroBERT, y compris les points intermédiaires d'entraînement et le cadre d'entraînement.",
      "upvotes": 26,
      "discussionId": "67ce9627e5cdfda52b9e88a4"
    },
    "publishedAt": "2025-03-10T03:42:45.848Z",
    "title": "EuroBERT: Scaling Multilingual Encoders for European Languages",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62be186a5f59ff2320e6e32b/NxwS9WJrRc9D3q9awbn_X.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05500.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "62be186a5f59ff2320e6e32b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62be186a5f59ff2320e6e32b/W_emoC2uItM-MJZyCfIKI.png",
      "fullname": "Nicolas-BZRD",
      "name": "Nicolas-BZRD",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.02130",
      "authors": [
        {
          "_id": "67cc697fa029f09af72cca01",
          "user": {
            "_id": "6694cc1009326cb83f2d11bb",
            "avatarUrl": "/avatars/1ddaaed70a16ac475a9404848aef5d48.svg",
            "isPro": false,
            "fullname": "Zhixuan Lin",
            "user": "zhixuan-lin",
            "type": "user"
          },
          "name": "Zhixuan Lin",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-03-08T16:00:21.933Z",
          "hidden": false
        },
        {
          "_id": "67cc697fa029f09af72cca02",
          "user": {
            "_id": "64234eadd654afd6931a288b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/UTU-XcO_ssKpIYr5MBujK.jpeg",
            "isPro": false,
            "fullname": "Evgenii Nikishin",
            "user": "nikishin",
            "type": "user"
          },
          "name": "Evgenii Nikishin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T09:54:09.945Z",
          "hidden": false
        },
        {
          "_id": "67cc697fa029f09af72cca03",
          "user": {
            "_id": "66906c4e37eadb9c577984d3",
            "avatarUrl": "/avatars/b81765472942fdf94c0ee885ca62df2d.svg",
            "isPro": false,
            "fullname": "Owen He",
            "user": "littleowen",
            "type": "user"
          },
          "name": "Xu Owen He",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-08T16:00:01.392Z",
          "hidden": false
        },
        {
          "_id": "67cc697fa029f09af72cca04",
          "name": "Aaron Courville",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-03T23:35:23.000Z",
      "title": "Forgetting Transformer: Softmax d'Attention avec une Porte d'Oubli",
      "summary": "Une des composantes importantes de la séquence de modèles de ricarent modernes est la porte d'oubli. Le Transformer n'a pas une forme explicite de ricarent, mais peut intégrer de manière naturelle une porte d'oubli, réduisant de manière dépendante des données la note d'attention non normalisée. Ce processus est appelé \"porte d'oubli dans la note d'attention\" et le modèle résultant est appelé \"Transformer avec porte d'oubli (FoX)\". FoX montre un rendement meilleur que le Transformer dans la modélisation de longues séquences de langue, l'estimation de la longueur de séquence et dans les tâches de flux de travail avec des séquences courtes, et montre également un rendement comparable au Transformer dans les tâches de flux de travail avec des séquences longues. De plus, il est compatible avec l'algorithme FlashAttention et ne nécessite pas de vecteurs de position. Par l'analyse, comme le \"test de nid de la pile de haut niveau\", FoX démontre qu'il maintient la excellente capacité de gestion de longues séquences du Transformer, en comparaison avec d'autres modèles de séquences de ricarent comme Mamba-2, HGRN2 et DeltaNet. De plus, un design de bloc \"pro\" a été introduit et des composants architecturaux communs ont été intégrés dans les modèles de séquences de ricarent, ce qui a considérablement amélioré le rendement de FoX et du Transformer. Le code est disponible sur la URL suivante.\nhttps://github.com/zhixuan-lin/forgetting-transformer",
      "upvotes": 12,
      "discussionId": "67cc6981a029f09af72ccac1",
      "githubRepo": "https://github.com/zhixuan-lin/forgetting-transformer"
    },
    "publishedAt": "2025-03-09T22:02:39.842Z",
    "title": "Forgetting Transformer: Softmax Attention with a Forget Gate",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.02130.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6694cc1009326cb83f2d11bb",
      "avatarUrl": "/avatars/1ddaaed70a16ac475a9404848aef5d48.svg",
      "fullname": "Zhixuan Lin",
      "name": "zhixuan-lin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.05639",
      "authors": [
        {
          "_id": "67ce5ad85847e4787a82242d",
          "user": {
            "_id": "650447dd52ca06fef957f05d",
            "avatarUrl": "/avatars/511c11ac9b3cc7a162bda5e07f6ee0a3.svg",
            "isPro": true,
            "fullname": "Yuxuan BIAN",
            "user": "BianYx",
            "type": "user"
          },
          "name": "Yuxuan Bian",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-10T03:22:04.947Z",
          "hidden": false
        },
        {
          "_id": "67ce5ad85847e4787a82242e",
          "name": "Zhaoyang Zhang",
          "hidden": false
        },
        {
          "_id": "67ce5ad85847e4787a82242f",
          "user": {
            "_id": "62d4577bc85b0fcf7fde39bb",
            "avatarUrl": "/avatars/a3a5729e33ae89ce9ba408830db3c835.svg",
            "isPro": false,
            "fullname": "Xuan Ju",
            "user": "juxuan27",
            "type": "user"
          },
          "name": "Xuan Ju",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T09:57:06.955Z",
          "hidden": false
        },
        {
          "_id": "67ce5ad85847e4787a822430",
          "user": {
            "_id": "6374a02d0856ac905bfc6113",
            "avatarUrl": "/avatars/2cbe75c9cc818a647ca6e416f129c96f.svg",
            "isPro": false,
            "fullname": "Mingdeng Cao",
            "user": "Ljzycmd",
            "type": "user"
          },
          "name": "Mingdeng Cao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T09:56:45.412Z",
          "hidden": false
        },
        {
          "_id": "67ce5ad85847e4787a822431",
          "name": "Liangbin Xie",
          "hidden": false
        },
        {
          "_id": "67ce5ad85847e4787a822432",
          "user": {
            "_id": "63ca3ddc04c979828310bfcb",
            "avatarUrl": "/avatars/615e0d8622950b4408b40d550f02a894.svg",
            "isPro": false,
            "fullname": "Ying Shan",
            "user": "yshan2u",
            "type": "user"
          },
          "name": "Ying Shan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T09:56:34.001Z",
          "hidden": false
        },
        {
          "_id": "67ce5ad85847e4787a822433",
          "name": "Qiang Xu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-07T17:59:46.000Z",
      "title": "VideoPainter : Injection et édition de vidéos de longueur de navigation avec des plugins, contrôle du contexte",
      "summary": "La vidéo inpainting, la technologie de vidéo inpainting a été développée pour restaurer des vidéos, mais continue d'évoluer. Malgré ce progrès, les méthodes actuelles ont des difficultés à étendre les pixels des régions cachées en utilisant des flux optiques et des connaissances préalables des zones de réception, ou à étendre des modèles d'inpainting d'images temporels pour créer des objets complètement cachés et maintenir le contexte de fond. Pour résoudre ces limitations, nous proposons un nouveau modèle appelé VideoPainter. Cette méthode inclut un encodeur de contexte efficace avec seulement 6% de paramètres, traite des vidéos cachées et introduit de l'information de contexte de fond basée sur le fond pour générer du contenu cohérent. Cette architecture séparée réduit significativement la complexité de l'entraînement et permet une intégration complexe du contexte du fond. De plus, nous introduisons une nouvelle technique de resampling des régions d'intérêt pour améliorer l'application pratique du vidéo inpainting et traiter des vidéos longues. De plus, nous construisons une chaîne de travail d'entraînement scalable en utilisant des modèles de compréhension visuelle actuels, offrant VPData et VPBench pour encourager l'entraînement et l'évaluation de l'inpainting basé sur la segmentation, et offrant le plus grand ensemble de données de vidéo inpainting jusqu'à présent, qui comprend plus de 390K clips de différents types. En basant nous sur l'inpainting comme base du flux, nous examinons des applications descendantes comme l'édition de vidéo et la génération de paires de données d'édition de vidéo, présentant des preuves de compétitivité et de viabilité pratique. Les expériences étendues montrent un excellent rendement sur 8 métriques principales de VideoPainter et des applications efficaces tant dans le vidéo inpainting que dans l'édition de vidéo.",
      "upvotes": 10,
      "discussionId": "67ce5adc5847e4787a822524",
      "projectPage": "https://yxbian23.github.io/project/video-painter/",
      "githubRepo": "https://github.com/TencentARC/VideoPainter"
    },
    "publishedAt": "2025-03-10T04:30:00.983Z",
    "title": "VideoPainter: Any-length Video Inpainting and Editing with Plug-and-Play Context Control",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/650447dd52ca06fef957f05d/VSg-Ti5epQJbVp20s1ILN.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05639.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "650447dd52ca06fef957f05d",
      "avatarUrl": "/avatars/511c11ac9b3cc7a162bda5e07f6ee0a3.svg",
      "fullname": "Yuxuan BIAN",
      "name": "BianYx",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.05592",
      "authors": [
        {
          "_id": "67ce5fd2e5cdfda52b9123a4",
          "user": {
            "_id": "66163dc8c7f45b3f893ff40b",
            "avatarUrl": "/avatars/801043dac0caae90bbca8c9d3e2e203b.svg",
            "isPro": false,
            "fullname": "Song Huatong",
            "user": "XXsongLALA",
            "type": "user"
          },
          "name": "Huatong Song",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T10:03:49.730Z",
          "hidden": false
        },
        {
          "_id": "67ce5fd2e5cdfda52b9123a5",
          "user": {
            "_id": "61b8405b516a20acdf3b85ff",
            "avatarUrl": "/avatars/3d2eae7c163a80b73260087b05a4230b.svg",
            "isPro": false,
            "fullname": "Jinhao Jiang",
            "user": "Boru",
            "type": "user"
          },
          "name": "Jinhao Jiang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T10:04:22.446Z",
          "hidden": false
        },
        {
          "_id": "67ce5fd2e5cdfda52b9123a6",
          "user": {
            "_id": "6703ac76ea890f0ca5b225eb",
            "avatarUrl": "/avatars/5f56c49a1940143d47dd484782a4abbf.svg",
            "isPro": false,
            "fullname": "Yingqian Min",
            "user": "EliverQ",
            "type": "user"
          },
          "name": "Yingqian Min",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-10T09:40:54.171Z",
          "hidden": false
        },
        {
          "_id": "67ce5fd2e5cdfda52b9123a7",
          "name": "Jie Chen",
          "hidden": false
        },
        {
          "_id": "67ce5fd2e5cdfda52b9123a8",
          "name": "Zhipeng Chen",
          "hidden": false
        },
        {
          "_id": "67ce5fd2e5cdfda52b9123a9",
          "name": "Wayne Xin Zhao",
          "hidden": false
        },
        {
          "_id": "67ce5fd2e5cdfda52b9123aa",
          "name": "Lei Fang",
          "hidden": false
        },
        {
          "_id": "67ce5fd2e5cdfda52b9123ab",
          "user": {
            "_id": "64b8c89052b7353d8c6a1013",
            "avatarUrl": "/avatars/cd59fffe81f6b07b4519540b8ff3d95f.svg",
            "isPro": false,
            "fullname": "Ji-Rong Wen",
            "user": "jrwen",
            "type": "user"
          },
          "name": "Ji-Rong Wen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T10:04:33.194Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-07T17:14:44.000Z",
      "title": "R1-Searcher : Amélioration de la capacité de recherche d'un modèle de langage par apprentissage par renforcement",
      "summary": "Les modèles actuels de raisonnement à grande échelle (LRMs) montrent la possibilité que l'apprentissage par renforcement (RL) puisse renforcer les capacités de raisonnement complexe des modèles de langage à grande échelle (LLMs). Ces modèles montrent des résultats excellents dans des tâches difficiles comme la mathématiques ou la programmation, mais présentent des limites lorsqu'il s'agit de résoudre des problèmes qui dépendent de connaissances internes, surtout dans des situations d'urgence temporelle ou pour des questions de type connaissance concentrée. Pour aborder ces défis, nous proposons le R1-Searcher. C'est un nouveau cadre d'apprentissage par renforcement basé sur deux étapes conçu pour renforcer les capacités de recherche des LLMs. Ce méthode permet une appel automatique à des systèmes de recherche externes lors du processus de raisonnement. Notre cadre de travail ne nécessite pas de débuts de récompense ou de modélisation de la distribution gaussienne. Ce méthode généralise efficacement sur des ensembles de données perturbés et soutient à la fois des modèles de base et des modèles de commande. Nos expériences montrent des résultats significativement meilleurs que ceux des méthodes existantes de RAG, et obtiennent des résultats équivalents à ceux de GPT-4o-mini.",
      "upvotes": 9,
      "discussionId": "67ce5fd3e5cdfda52b912436",
      "githubRepo": "https://github.com/SsmallSong/R1-Searcher"
    },
    "publishedAt": "2025-03-09T23:43:27.151Z",
    "title": "R1-Searcher: Incentivizing the Search Capability in LLMs via Reinforcement Learning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05592.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6317
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.05638",
      "authors": [
        {
          "_id": "67ce8388764226f050ad18b3",
          "name": "Mark YU",
          "hidden": false
        },
        {
          "_id": "67ce8388764226f050ad18b4",
          "user": {
            "_id": "657a7458afbb0117ba15c59f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/657a7458afbb0117ba15c59f/8_iwTS1UG_mKnfylFbLsY.jpeg",
            "isPro": false,
            "fullname": "Wenbo Hu",
            "user": "wbhu-tc",
            "type": "user"
          },
          "name": "Wenbo Hu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-10T08:00:42.681Z",
          "hidden": false
        },
        {
          "_id": "67ce8388764226f050ad18b5",
          "user": {
            "_id": "64770e86d7cf39f2e937ae9a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64770e86d7cf39f2e937ae9a/pLqGg2z1KzQxCGpMwds-9.jpeg",
            "isPro": false,
            "fullname": "Jinbo Xing",
            "user": "Doubiiu",
            "type": "user"
          },
          "name": "Jinbo Xing",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-10T08:00:40.328Z",
          "hidden": false
        },
        {
          "_id": "67ce8388764226f050ad18b6",
          "user": {
            "_id": "63ca3ddc04c979828310bfcb",
            "avatarUrl": "/avatars/615e0d8622950b4408b40d550f02a894.svg",
            "isPro": false,
            "fullname": "Ying Shan",
            "user": "yshan2u",
            "type": "user"
          },
          "name": "Ying Shan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T10:06:56.510Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-07T17:57:53.000Z",
      "title": "Traducteur : Chef du Projet : Redirige le trajet de la caméra vidéo monolithique à travers un modèle de diffusion.",
      "summary": "Voici la traduction en français :\n\nOn présente un nouvel approche appelée TrajectoryCrafter. Ce méthode permet de rediriger les trajectoires de la caméra des vidéos avec des défauts. Elle sépare clairement la transformation visuelle et la génération de contenu stridé, permettant au utilisateur de contrôler rigoureusement le trajet de la caméra choisi. On propose un nouveau modèle de diffusion vidéo avec des conditions de double streaming. Ce modèle intègre le rendu de nuages ponctuels et le vidéo original, garantissant une transformation visuelle précise et la génération de contenu 4D coloré. Bien que le vidéo utilisée soit de multiples points, elle est combinée avec des vidéos de défaut de taille de réseau et des données statiques de multiples points pour créer un ensemble de données d'apprentissage hybride. Cela favorise une robustesse généralisée à différentes échelles grâce à notre stratégie de double reprojection. Les évaluations sur des vidéos de multiples points et de grands défauts montrent le haut rendement de notre méthode, démontrant ainsi son excellence.",
      "upvotes": 8,
      "discussionId": "67ce838a764226f050ad1952",
      "projectPage": "https://trajectorycrafter.github.io/",
      "githubRepo": "https://github.com/TrajectoryCrafter/TrajectoryCrafter"
    },
    "publishedAt": "2025-03-10T02:24:39.763Z",
    "title": "TrajectoryCrafter: Redirecting Camera Trajectory for Monocular Videos via Diffusion Models",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/657a7458afbb0117ba15c59f/lpXbCmGz-upwRVSEUzBjV.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05638.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "657a7458afbb0117ba15c59f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/657a7458afbb0117ba15c59f/8_iwTS1UG_mKnfylFbLsY.jpeg",
      "fullname": "Wenbo Hu",
      "name": "wbhu-tc",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.05652",
      "authors": [
        {
          "_id": "67ce524ee969bc5fd69c9388",
          "user": {
            "_id": "61e9f5398c237a147a3f4ab5",
            "avatarUrl": "/avatars/afd4ec17cb132b5ab56e50a678c4786d.svg",
            "isPro": false,
            "fullname": "Yunfan Jiang",
            "user": "yunfanj",
            "type": "user"
          },
          "name": "Yunfan Jiang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-10T08:01:07.901Z",
          "hidden": false
        },
        {
          "_id": "67ce524ee969bc5fd69c9389",
          "name": "Ruohan Zhang",
          "hidden": false
        },
        {
          "_id": "67ce524ee969bc5fd69c938a",
          "name": "Josiah Wong",
          "hidden": false
        },
        {
          "_id": "67ce524ee969bc5fd69c938b",
          "name": "Chen Wang",
          "hidden": false
        },
        {
          "_id": "67ce524ee969bc5fd69c938c",
          "user": {
            "_id": "63509bc859bfa9a85d4220aa",
            "avatarUrl": "/avatars/ca2cc9b87f5ca5cd51606b2f9edf89d0.svg",
            "isPro": false,
            "fullname": "Yanjie Ze",
            "user": "yjze",
            "type": "user"
          },
          "name": "Yanjie Ze",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-10T08:01:05.242Z",
          "hidden": false
        },
        {
          "_id": "67ce524ee969bc5fd69c938d",
          "name": "Hang Yin",
          "hidden": false
        },
        {
          "_id": "67ce524ee969bc5fd69c938e",
          "name": "Cem Gokmen",
          "hidden": false
        },
        {
          "_id": "67ce524ee969bc5fd69c938f",
          "name": "Shuran Song",
          "hidden": false
        },
        {
          "_id": "67ce524ee969bc5fd69c9390",
          "name": "Jiajun Wu",
          "hidden": false
        },
        {
          "_id": "67ce524ee969bc5fd69c9391",
          "name": "Li Fei-Fei",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-07T18:15:21.000Z",
      "title": "Boy Autrobotto Suit : Permet de réaliser des mouvements de la vie quotidienne avec facilité.",
      "summary": "La vie quotidienne des ménages dans la réalité présente de grands défis pour les robots mobiles. Dans l'analyse des référentiels de la technologie des robots, il a été démontré que l'exécution de tâches réussies dépend de trois principales capacités de contrôle corporel générales : la manipulation d'objets, la navigation stable et précise, et l'accessibilité des extrémités réparties. Pour atteindre ces capacités, il est nécessaire de concevoir du matériel exigeant, mais le résultat est que l'apprentissage de politiques commerciales devient plus complexe. Pour faire face à ces défis, nous présentons un cadre rigoureux pour le fonctionnement général dans diverses tâches de ménage, nommé Suit de Robot Behavior (BRS). La manipulation d'objets est réalisée par un robot de support (holder robot) avec un bras de 4 degrés de liberté, ce qui intègre une interface efficace pour la collecte de données et la téléopération, et un algorithme nouveau est utilisé pour intégrer des algorithmes pour l'apprentissage de politiques commerciales. La BRS met en avant trois capacités clés : la navigation à longue distance, l'interaction avec des objets d'architecture et de variabilité, et le travail dans des espaces étroits, évaluées sur cinq tâches de ménage supplémentairesment complexes. L'intégration de l'interface de collecte de données, le robot Efemeba, et le cadre d'apprentissage de la BRS permet un fonctionnement général dans les tâches quotidiennes de ménage, considéré comme un pas important. La BRS est disponible sous forme de code open sur https://behavior-robot-suite.github.io/.",
      "upvotes": 7,
      "discussionId": "67ce5294e969bc5fd69c9a2c",
      "projectPage": "https://behavior-robot-suite.github.io/",
      "githubRepo": "https://github.com/behavior-robot-suite/brs-algo"
    },
    "publishedAt": "2025-03-09T22:51:04.616Z",
    "title": "BEHAVIOR Robot Suite: Streamlining Real-World Whole-Body Manipulation for Everyday Household Activities",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/61e9f5398c237a147a3f4ab5/WD3cV-QLiHRgh4IUOrikN.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05652.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "61e9f5398c237a147a3f4ab5",
      "avatarUrl": "/avatars/afd4ec17cb132b5ab56e50a678c4786d.svg",
      "fullname": "Yunfan Jiang",
      "name": "yunfanj",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.05379",
      "authors": [
        {
          "_id": "67ce5f2389663abdbc364495",
          "name": "Jiaxing Zhao",
          "hidden": false
        },
        {
          "_id": "67ce5f2389663abdbc364496",
          "name": "Xihan Wei",
          "hidden": false
        },
        {
          "_id": "67ce5f2389663abdbc364497",
          "name": "Liefeng Bo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-07T12:46:42.000Z",
      "title": "R1-Omni : Reconnaissance des émotions omnimodale interprétable et apprentissage par renforcement utilisés",
      "summary": "Dans cette étude, une application de l'apprentissage par renforcement (RLVR) est utilisée pour la première fois pour reconnaître les émotions en utilisant une récompense vérifiable, en appliquant cette méthode à un modèle multimodal omnilinguistique. Dans ce travail, les modèles visuels et acoustiques jouent un rôle important. En utilisant RLVR, le modèle Omni est optimisé, améliorant considérablement sa capacité logique, sa précision dans le reconnaissance des émotions et sa capacité à généraliser dans trois aspects principaux. L'introduction de RLVR non seulement améliore la qualité générale des données internes, mais montre également une excellente robustesse sur des ensembles de données externes. Ce qui est le plus important, l'augmentation de la capacité logique permet d'analyser clairement l'influence du processus de reconnaissance des émotions dans différents modèles. Cela contribue à l'optimisation générale du modèle multimodal linguistique.",
      "upvotes": 6,
      "discussionId": "67ce5f2489663abdbc3644d0"
    },
    "publishedAt": "2025-03-09T23:40:46.906Z",
    "title": "R1-Omni: Explainable Omni-Multimodal Emotion Recognition with Reinforcing Learning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05379.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6317
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.04872",
      "authors": [
        {
          "_id": "67ce5deedb623d45a95deb72",
          "user": {
            "_id": "632c30576bcb864974cc40a8",
            "avatarUrl": "/avatars/96aa948ad1dd35d355e20b5765a2563a.svg",
            "isPro": false,
            "fullname": "sunlin",
            "user": "lincharliesun",
            "type": "user"
          },
          "name": "Lin Sun",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-10T08:00:47.601Z",
          "hidden": false
        },
        {
          "_id": "67ce5deedb623d45a95deb73",
          "name": "Guangxiang Zhao",
          "hidden": false
        },
        {
          "_id": "67ce5deedb623d45a95deb74",
          "name": "Xiaoqi Jian",
          "hidden": false
        },
        {
          "_id": "67ce5deedb623d45a95deb75",
          "name": "Yuhan Wu",
          "hidden": false
        },
        {
          "_id": "67ce5deedb623d45a95deb76",
          "name": "Weihong Lin",
          "hidden": false
        },
        {
          "_id": "67ce5deedb623d45a95deb77",
          "name": "Yongfu Zhu",
          "hidden": false
        },
        {
          "_id": "67ce5deedb623d45a95deb78",
          "name": "Change Jia",
          "hidden": false
        },
        {
          "_id": "67ce5deedb623d45a95deb79",
          "name": "Linglin Zhang",
          "hidden": false
        },
        {
          "_id": "67ce5deedb623d45a95deb7a",
          "name": "Jinzhu Wu",
          "hidden": false
        },
        {
          "_id": "67ce5deedb623d45a95deb7b",
          "name": "Junfeng Ran",
          "hidden": false
        },
        {
          "_id": "67ce5deedb623d45a95deb7c",
          "name": "Sai-er Hu",
          "hidden": false
        },
        {
          "_id": "67ce5deedb623d45a95deb7d",
          "name": "Zihan Jiang",
          "hidden": false
        },
        {
          "_id": "67ce5deedb623d45a95deb7e",
          "name": "Junting Zhou",
          "hidden": false
        },
        {
          "_id": "67ce5deedb623d45a95deb7f",
          "name": "Wenrui Liu",
          "hidden": false
        },
        {
          "_id": "67ce5deedb623d45a95deb80",
          "name": "Bin Cui",
          "hidden": false
        },
        {
          "_id": "67ce5deedb623d45a95deb81",
          "name": "Tong Yang",
          "hidden": false
        },
        {
          "_id": "67ce5deedb623d45a95deb82",
          "name": "Xiangzheng Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-06T16:25:53.000Z",
      "title": "TinyR1-32B-Preview : Amélioration de la précision grâce à l'entraînement d'ajustements de branches",
      "summary": "Se concentre sur le problème de réduire le taille d'un modèle grand (LLM) tout en maintenant son rendement. Cependant, actuellement, les méthodes rencontrent des difficultés pour atteindre une précision élevée dans des modèles nécessitant un apprentissage continu et un apprentissage sur des données transférees. Pour résoudre cette limitation, on propose un approche d'apprentissage continu avec Branch-Merge. Cette approche renforce la compression du modèle en deux étapes. Tout d'abord, dans la phase de Branch, des connaissances d'un grand modèle d'entraînement sont sélectionnées et les modèles élèves sont spécifiquement chauffés par un apprentissage observatoire spécialisé (SFT). Ensuite, dans la phase de Merge, ces modèles élèves sont intégrés pour faciliter la transfert de connaissances entre domaines et améliorer la généralisation. On valide cette approche en utilisant DeepSeek-R1 comme modèle d'entraînement et DeepSeek-R1-Distill-Qwen-32B comme modèle élève. Le résultat est que le modèle synthétisé, TinyR1-32B-Preview, dépasse DeepSeek-R1-Distill-Qwen-32B sur de multiples benchmarks, y compris les mathématiques (+5,5 points), la programmation (+4,4 points) et les sciences (+2,9 points). De plus, lors de l'AIME 2024, il a atteint un rendement similaire à DeepSeek-R1. L'approche d'apprentissage continu avec Branch-Merge fournit une solution scalable pour créer de petits modèles grands de haut rendement, réduisant les coûts de calcul et le temps.",
      "upvotes": 5,
      "discussionId": "67ce5df0db623d45a95dec1f"
    },
    "publishedAt": "2025-03-09T23:35:58.424Z",
    "title": "TinyR1-32B-Preview: Boosting Accuracy with Branch-Merge Distillation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04872.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6317
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.05447",
      "authors": [
        {
          "_id": "67ceab4132a6585cecad2c36",
          "user": {
            "_id": "6246bb33da617c00b48e4d92",
            "avatarUrl": "/avatars/0304a9f6eb7f5dee4d933d03222f94e9.svg",
            "isPro": false,
            "fullname": "Weigao Sun",
            "user": "weigao266",
            "type": "user"
          },
          "name": "Weigao Sun",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-03-10T09:09:22.436Z",
          "hidden": false
        },
        {
          "_id": "67ceab4132a6585cecad2c37",
          "name": "Disen Lan",
          "hidden": false
        },
        {
          "_id": "67ceab4132a6585cecad2c38",
          "name": "Tong Zhu",
          "hidden": false
        },
        {
          "_id": "67ceab4132a6585cecad2c39",
          "name": "Xiaoye Qu",
          "hidden": false
        },
        {
          "_id": "67ceab4132a6585cecad2c3a",
          "name": "Yu Cheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-07T14:17:45.000Z",
      "title": "Linear-MoE : Modèle de séquence linéaire et mélange de sources",
      "summary": "Linear-MoE est un système de production pour modéliser et entraîner des grands modèles qui intègrent le Modèle de Séquence Linéaire (LSM) et le Mixture of Experts (MoE). Ce système profite de la capacité excellente de modélisation de séquences de faible complexité linéaire et de l'efficience de l'activation sparsifiée fournie par la couche MoE, ce qui permet d'entraîner de manière efficace et d'obtenir de hauts rendements. Le système Linear-MoE est constitué de deux sous-systèmes : le sous-système de modélisation, qui offre un cadre intégré qui supporte toutes les instances de LSM, et le sous-système d'entraînement, qui promeut l'entraînement efficace grâce à des techniques de parallélisation progressive. En particulier, une parallélisation de séquences a été conçue pour le modèle Linear-MoE. De plus, des modèles hybrides qui combinent les couches Linear-MoE et les couches standards Transformer-MoE ont été examinés, ce qui permet d'améliorer la flexibilité et le rendement du modèle en utilisant la parallélisation de séquences. Les évaluations de deux séries de modèles, A0.3B-2B et A1B-7B, montrent que Linear-MoE améliore l'efficacité de l'entraînement tout en maintenant un rendement compétitif dans différents cadres de référence, démontrant la possibilité de l'architecture de base des modèles futurs. Le code est disponible sur https://github.com/OpenSparseLLMs/Linear-MoE.",
      "upvotes": 3,
      "discussionId": "67ceab4232a6585cecad2c82",
      "githubRepo": "https://github.com/OpenSparseLLMs/Linear-MoE"
    },
    "publishedAt": "2025-03-10T05:05:22.522Z",
    "title": "Linear-MoE: Linear Sequence Modeling Meets Mixture-of-Experts",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05447.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6246bb33da617c00b48e4d92",
      "avatarUrl": "/avatars/0304a9f6eb7f5dee4d933d03222f94e9.svg",
      "fullname": "Weigao Sun",
      "name": "weigao266",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.01713",
      "authors": [
        {
          "_id": "67c75e18cb29e2a4b0eb0293",
          "user": {
            "_id": "66c0a08bac74db25de8427ec",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c0a08bac74db25de8427ec/9D-piDBZqSt6KNkHImmkv.jpeg",
            "isPro": false,
            "fullname": "Jintao Zhang",
            "user": "jt-zhang",
            "type": "user"
          },
          "name": "Jintao Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-07T09:26:50.367Z",
          "hidden": false
        },
        {
          "_id": "67c75e18cb29e2a4b0eb0294",
          "name": "Guoliang Li",
          "hidden": false
        },
        {
          "_id": "67c75e18cb29e2a4b0eb0295",
          "name": "Jinyang Su",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-03T16:25:58.000Z",
      "title": "SAGE : Marco de évaluation adéquate",
      "summary": "Le Rétorique de la Lecture (RAG) montre des résultats significatifs dans des tâches de réponse à des questions (QA) dans un corpus spécifique. Cependant, de nombreux cas de défaut surviennent lors de l'implémentation de RAG. Ces défauts ne sont pas dues aux limites des modèles de langage grands (LLMs), mais à deux limites principales : 1. Les méthodes actuelles de RAG divisent le corpus sans considérer la sémantique, ce qui perd la relation entre la question et les segments divisés, rendant difficile l'identification de contextes pertinents. 2. Il existe une discordance entre la quantité de contexte et la quantité de contexte extrait. Dans cet article, nous présentons un nouveau cadre de RAG appelé SAGE pour surmonter ces limites. Tout d'abord, nous proposons un modèle de division sémantique pour résoudre le problème de la division sans considérer la sémantique. Ce modèle a été entraîné pour diviser le corpus en contextes complets basés sur la sémantique. 2. Pour obtenir seulement le contexte le plus approprié et ignorer les contextes non pertinents, nous concevons un algorithme de sélection de contexte qui décide du contexte en basant-se sur la vitesse de chute du score de pertinence. 3. Pour améliorer la précision du contexte, nous proposons évaluer si le contexte obtenu par les LLMs est excessif ou insuffisant et ajuste sa quantité. Les expérimentations montrent que SAGE améliore la qualité de la réponse en moyenne de 61,25%. De plus, il évite d'obtenir des contextes contenant du bruit, réduisant les coûts de tokens utilisés dans l'inférence des LLMs et atteignant une amélioration de l'efficacité de coût de 49,41%. Notre étude offre une vision précieuse pour le renforcement de RAG.",
      "upvotes": 3,
      "discussionId": "67c75e1ccb29e2a4b0eb03a9"
    },
    "publishedAt": "2025-03-10T03:23:51.482Z",
    "title": "SAGE: A Framework of Precise Retrieval for RAG",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01713.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66c0a08bac74db25de8427ec",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c0a08bac74db25de8427ec/9D-piDBZqSt6KNkHImmkv.jpeg",
      "fullname": "Jintao Zhang",
      "name": "jt-zhang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.05315",
      "authors": [
        {
          "_id": "67ce6db07110b8bedb3344a7",
          "name": "Saumya Chaturvedi",
          "hidden": false
        },
        {
          "_id": "67ce6db07110b8bedb3344a8",
          "user": {
            "_id": "63a4754927f1f64ed7238dac",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
            "isPro": false,
            "fullname": "Aman Chadha",
            "user": "amanchadha",
            "type": "user"
          },
          "name": "Aman Chadha",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-10T08:00:45.150Z",
          "hidden": false
        },
        {
          "_id": "67ce6db07110b8bedb3344a9",
          "name": "Laurent Bindschaedler",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-07T10:50:45.000Z",
      "title": "LoRACode : Adaptateurs pour Code pour Code 🔗",
      "summary": "Comment l'encodage est crucial pour les recherches de code sémantique, mais la méthodologie actuelle souvent ne capture pas facilement la grammaire précise et le contexte nouveau d'un code. Des modèles comme CodeBERT et UniXcoder, basés sur le code ouvert, ont des limitations en termes d'échelle et d'efficacité, et les systèmes propriétaires avec un haut rendement incurrent des coûts de calcul élevés. Nous introduisons un méthode d'adaptation basée sur LoRA (Adaptation de Rang Bas) pour des fins de paramètres efficaces, et nous construisons des adaptateurs de tâche pour l'appliquer dans les recherches de code. Notre approche limite le nombre de paramètres d'entraînement du modèle de base à moins de 2%, permettant un ajustement efficace sur un corpus de code large de 2 millions d'échantillons en seulement 25 minutes avec deux GPU H100. Les expériences ont montré un augmentation du Mean Reciprocal Rank (MRR) dans les recherches de Code2Code de 9,1% et un augmentation de 86,69% dans les tâches de recherche de Code2Text dans plusieurs langages de programmation. Nous avons étudié les différences d'adaptation selon la tâche et le langage, ainsi que la sensibilité aux changements grammaticaux et linguistiques dans les recherches de code.",
      "upvotes": 2,
      "discussionId": "67ce6db17110b8bedb3344c5"
    },
    "publishedAt": "2025-03-10T00:51:02.203Z",
    "title": "LoRACode: LoRA Adapters for Code Embeddings",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05315.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a4754927f1f64ed7238dac",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
      "fullname": "Aman Chadha",
      "name": "amanchadha",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.05132",
      "authors": [
        {
          "_id": "67ce5ec17c6e6ea1cc5649c2",
          "name": "Hengguang Zhou",
          "hidden": false
        },
        {
          "_id": "67ce5ec17c6e6ea1cc5649c3",
          "name": "Xirui Li",
          "hidden": false
        },
        {
          "_id": "67ce5ec17c6e6ea1cc5649c4",
          "name": "Ruochen Wang",
          "hidden": false
        },
        {
          "_id": "67ce5ec17c6e6ea1cc5649c5",
          "name": "Minhao Cheng",
          "hidden": false
        },
        {
          "_id": "67ce5ec17c6e6ea1cc5649c6",
          "name": "Tianyi Zhou",
          "hidden": false
        },
        {
          "_id": "67ce5ec17c6e6ea1cc5649c7",
          "name": "Cho-Jui Hsieh",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-07T04:21:47.000Z",
      "title": "\"R1-Zero's 'Ahaモーメント' et l'étude de la théorie de la vision dans des modèles 2B sans SFT\"",
      "summary": "Récemment, DeepSeek R1 a montré que l'apprentissage par récompense utilisant des récompenses basées sur des règles simples peut automatiquement développer des théories complexes de raisonnement dans des modèles de langage grands, montrant une caractéristique appelée \"agent moment\". Cependant, les essais pour étendre ces succès à d'autres théories de raisonnement ont échoué à reproduire ces caractéristiques importantes. Dans ce rapport, nous signalons le premier succès significatif de la reproduction de cette caractéristique dans un modèle non SFT de 2B. À partir de Qwen2-VL-2B, l'apprentissage par récompense a été appliqué directement sur le jeu de données SAT, atteignant une précision de 59.47% sur CVBench, améliorant considérablement (plus de 30%) par rapport au modèle de base et (environ 2%) par rapport aux ajustements SFT. De plus, nous partageons les échecs et les retours des tentatives d'utiliser l'apprentissage par récompense pour atteindre la théorie de R1, et nous avons effectué des efforts pour clarifier les problèmes. Les principales observations sont : (1) l'accumulation logique dans les modèles d'instructions introduisant l'apprentissage par récompense peut être légère, et (2) la récompense par longueur aléatoire n'a pas d'effet sur la capacité de raisonnement. Le code du projet est disponible sur https://github.com/turningpoint-ai/VisualThinker-R1-Zero.",
      "upvotes": 2,
      "discussionId": "67ce5ec27c6e6ea1cc564a01"
    },
    "publishedAt": "2025-03-09T23:39:12.374Z",
    "title": "R1-Zero's \"Aha Moment\" in Visual Reasoning on a 2B Non-SFT Model",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05132.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6317
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.04808",
      "authors": [
        {
          "_id": "67ce5c7065b141ae6b0d3957",
          "name": "Stephen Chung",
          "hidden": false
        },
        {
          "_id": "67ce5c7065b141ae6b0d3958",
          "name": "Wenyu Du",
          "hidden": false
        },
        {
          "_id": "67ce5c7065b141ae6b0d3959",
          "name": "Jie Fu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-04T02:53:39.000Z",
      "title": "La répétition de l'apprentissage comme réponse au échec de l'apprentissage",
      "summary": "Récemment, le développement de l'apprentissage par renforcement (RL) dans les grands modèles de langue (LLM) a montré des améliorations notables, comme dans le cas de DeepSeek R1, qui a montré un grand gain de capacité logique des LLM, même dans des tâches de réponse à des questions simples. Dans cet article, cette méthodologie est étendue pour changer d'un approche multi-essai. Au lieu de répondre directement aux questions, le modèle peut effectuer plusieurs essais et recevoir une rétroaction après chaque échec. Cette approche est particulièrement utile pour des tâches multi-essai, où le modèle peut améliorer ses essais précédents et augmenter l'efficacité de la recherche. Les résultats des expérimentations montrent que la précision a augmenté significativement dans le cadre d'un benchmark de mathématiques, passant de 45,6% dans l'approche d'un essai à 52,5% avec l'approche de deux essais. De manière intéressante, lorsqu'on compare avec un LLM entraîné dans des tâches standard d'un essai, cette amélioration n'est pas observée lorsqu'on fournit plus d'essais pendant l'évaluation. Ces résultats indiquent que un LLM entraîné dans des tâches multi-essai peut atteindre un rendement meilleur dans les benchmarks de mathématiques et, en même temps, peut améliorer efficacement sa réponse selon la rétroaction du utilisateur. Le code complet est disponible sur : https://github.com/DualityRL/multi-attempt.",
      "upvotes": 2,
      "discussionId": "67ce5c7165b141ae6b0d39c6"
    },
    "publishedAt": "2025-03-09T23:29:35.505Z",
    "title": "Learning from Failures in Multi-Attempt Reinforcement Learning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04808.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6317
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.04548",
      "authors": [
        {
          "_id": "67cbff8e4dedec48bdec8a99",
          "name": "Zhipeng Chen",
          "hidden": false
        },
        {
          "_id": "67cbff8e4dedec48bdec8a9a",
          "user": {
            "_id": "6703ac76ea890f0ca5b225eb",
            "avatarUrl": "/avatars/5f56c49a1940143d47dd484782a4abbf.svg",
            "isPro": false,
            "fullname": "Yingqian Min",
            "user": "EliverQ",
            "type": "user"
          },
          "name": "Yingqian Min",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-10T08:02:04.349Z",
          "hidden": false
        },
        {
          "_id": "67cbff8e4dedec48bdec8a9b",
          "name": "Beichen Zhang",
          "hidden": false
        },
        {
          "_id": "67cbff8e4dedec48bdec8a9c",
          "name": "Jie Chen",
          "hidden": false
        },
        {
          "_id": "67cbff8e4dedec48bdec8a9d",
          "name": "Jinhao Jiang",
          "hidden": false
        },
        {
          "_id": "67cbff8e4dedec48bdec8a9e",
          "name": "Daixuan Cheng",
          "hidden": false
        },
        {
          "_id": "67cbff8e4dedec48bdec8a9f",
          "name": "Wayne Xin Zhao",
          "hidden": false
        },
        {
          "_id": "67cbff8e4dedec48bdec8aa0",
          "name": "Zheng Liu",
          "hidden": false
        },
        {
          "_id": "67cbff8e4dedec48bdec8aa1",
          "name": "Xu Miao",
          "hidden": false
        },
        {
          "_id": "67cbff8e4dedec48bdec8aa2",
          "name": "Yang Lu",
          "hidden": false
        },
        {
          "_id": "67cbff8e4dedec48bdec8aa3",
          "name": "Lei Fang",
          "hidden": false
        },
        {
          "_id": "67cbff8e4dedec48bdec8aa4",
          "name": "Zhongyuan Wang",
          "hidden": false
        },
        {
          "_id": "67cbff8e4dedec48bdec8aa5",
          "name": "Ji-Rong Wen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-06T15:34:27.000Z",
      "title": "« Étude sur la production et l'amélioration du modèle R1 »",
      "summary": "Ce rapport présente le troisième type de rapport technique sur le développement du modèle \"slow thinking\" du projet STILL. Avec le modèle de technologie défini, l'accélération de l'entraînement de RL (Apprentissage par renforcement) a devenu le cœur de l'implémentation de ce modèle logique. Des expériences ont été menées et des effets de divers facteurs influençant l'entraînement de RL ont été enregistrés de manière systématique. Des modèles de base ainsi que des modèles fine-tunés ont été testés. En particulier, il a été démontré que notre approche d'entraînement de RL améliore de manière cohérente le modèle de base Qwen2.5-32B, en augmentant la longueur des réponses et la précision des tests. De plus, il a été montré que des modèles comme DeepSeek-R1-Distill-Qwen-1.5B, lorsqu'ils sont développés avec RL, atteignent des niveaux de performance élevés, comme la précision de 39,33% sur l'AIME 2024. Au-delà de l'entraînement de RL, une revue des outils a été effectuée et il a été découvert que ceux-ci améliorent significativement la performance logique des modèles grands. Cette approche a démontré son efficacité sur l'AIME 2024, en atteignant une précision de 86,67% en utilisant une recherche stratégique, montrant clairement le potentiel d'améliorer la capacité du modèle. Les ressources du projet STILL sont publiées sur son site web : https://github.com/RUCAIBox/Slow_Thinking_with_LLMs.",
      "upvotes": 1,
      "discussionId": "67cbff8f4dedec48bdec8af3"
    },
    "publishedAt": "2025-03-10T05:23:26.375Z",
    "title": "An Empirical Study on Eliciting and Improving R1-like Reasoning Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04548.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6703ac76ea890f0ca5b225eb",
      "avatarUrl": "/avatars/5f56c49a1940143d47dd484782a4abbf.svg",
      "fullname": "Yingqian Min",
      "name": "EliverQ",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.04504",
      "authors": [
        {
          "_id": "67cb8e882cfa481bcee9455e",
          "user": {
            "_id": "66a07c07b7f0bb64d3b35497",
            "avatarUrl": "/avatars/c38af1ddb7a5b625e26b7ff05957ff7c.svg",
            "isPro": false,
            "fullname": "SunghyunAhn",
            "user": "SkiddieAhn",
            "type": "user"
          },
          "name": "Sunghyun Ahn",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-10T08:02:12.798Z",
          "hidden": false
        },
        {
          "_id": "67cb8e882cfa481bcee9455f",
          "user": {
            "_id": "673d7b70713e4b8db2d5ca94",
            "avatarUrl": "/avatars/b9e89eba62eb939ddd93c1cb91744e93.svg",
            "isPro": false,
            "fullname": "Youngwan Jo",
            "user": "jyy1551",
            "type": "user"
          },
          "name": "Youngwan Jo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-10T08:02:08.710Z",
          "hidden": false
        },
        {
          "_id": "67cb8e882cfa481bcee94560",
          "name": "Kijung Lee",
          "hidden": false
        },
        {
          "_id": "67cb8e882cfa481bcee94561",
          "name": "Sein Kwon",
          "hidden": false
        },
        {
          "_id": "67cb8e882cfa481bcee94562",
          "name": "Inpyo Hong",
          "hidden": false
        },
        {
          "_id": "67cb8e882cfa481bcee94563",
          "name": "Sanghyun Park",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-06T14:52:34.000Z",
      "title": "AnyAnomaly : Détection d'anomalies dans les vidéos basée sur 0-shot avec LVLM pour personnaliser l'utilisateur",
      "summary": "La détection d'anomalies en vidéo (VAD) est cruciale pour l'analyse des vidéos en informatique et la santé des consommateurs. Cependant, les modèles actuels de VAD dépendent de modèles normaux entraînés et ont des difficultés à s'appliquer dans différents environnements. Par conséquent, les utilisateurs doivent ré-entraîner le modèle ou développer d'autres modèles d'IA pour s'adapter à de nouveaux environnements. Cela nécessite des connaissances professionnelles en apprentissage automatique, des matériels de haute efficacité et des méthodes de collecte de données étendues, ce qui limite l'utilisation pratique de VAD. Pour résoudre ces problèmes, dans cette étude, nous proposons la technologie de détection d'anomalies en vidéo (C-VAD) et le modèle AnyAnomaly. C-VAD traite des événements anormaux en se basant sur des textes définis par l'utilisateur et détecte les frames qui incluent des événements spécifiques dans la vidéo. AnyAnomaly est implémenté efficacement grâce à des ajustements de modèles de langage vidéo à grande échelle, en excluant les ajustements de fine-tuning. Pour tester l'efficacité des modèles proposés, nous avons construit un ensemble de données C-VAD et nous avons démontré la supériorité de AnyAnomaly. De plus, notre approche a montré un rendement concurrentiel dans les ensembles de données de benchmark de VAD, a atteint les résultats les plus avancés dans l'ensemble de données UBnormal, et a montré une supériorité en termes de scalabilité dans tous les ensembles de données. Notre code est disponible sur github.com/SkiddieAhn/Paper-AnyAnomaly.",
      "upvotes": 0,
      "discussionId": "67cb8e8a2cfa481bcee945cd",
      "githubRepo": "https://github.com/SkiddieAhn/Paper-AnyAnomaly"
    },
    "publishedAt": "2025-03-10T04:06:49.447Z",
    "title": "AnyAnomaly: Zero-Shot Customizable Video Anomaly Detection with LVLM",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/66a07c07b7f0bb64d3b35497/TNPrQD3FdFBVVW2pUoEnU.gif",
      "https://cdn-uploads.huggingface.co/production/uploads/66a07c07b7f0bb64d3b35497/cA30QnSF_7AeHciWhCFSN.gif",
      "https://cdn-uploads.huggingface.co/production/uploads/66a07c07b7f0bb64d3b35497/T9EwE4Ea7DrH3XVZ_eJ1g.gif",
      "https://cdn-uploads.huggingface.co/production/uploads/66a07c07b7f0bb64d3b35497/Duuqk_Ph1GNfz6HW2Qv5g.gif",
      "https://cdn-uploads.huggingface.co/production/uploads/66a07c07b7f0bb64d3b35497/-Dze7JwIaBTbCfXUetsCd.gif"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04504.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66a07c07b7f0bb64d3b35497",
      "avatarUrl": "/avatars/c38af1ddb7a5b625e26b7ff05957ff7c.svg",
      "fullname": "SunghyunAhn",
      "name": "SkiddieAhn",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]