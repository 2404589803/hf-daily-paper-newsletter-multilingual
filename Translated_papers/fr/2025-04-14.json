[
  {
    "paper": {
      "id": "2504.08685",
      "authors": [
        {
          "_id": "67fc6ffc59b22e7c34d64c2e",
          "name": "Team Seawead",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c2f",
          "name": "Ceyuan Yang",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c30",
          "name": "Zhijie Lin",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c31",
          "name": "Yang Zhao",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c32",
          "name": "Shanchuan Lin",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c33",
          "name": "Zhibei Ma",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c34",
          "name": "Haoyuan Guo",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c35",
          "name": "Hao Chen",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c36",
          "name": "Lu Qi",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c37",
          "name": "Sen Wang",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c38",
          "name": "Feng Cheng",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c39",
          "name": "Feilong Zuo Xuejiao Zeng",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c3a",
          "name": "Ziyan Yang",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c3b",
          "name": "Fangyuan Kong",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c3c",
          "name": "Zhiwu Qing",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c3d",
          "name": "Fei Xiao",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c3e",
          "name": "Meng Wei",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c3f",
          "name": "Tuyen Hoang",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c40",
          "name": "Siyu Zhang",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c41",
          "name": "Peihao Zhu",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c42",
          "name": "Qi Zhao",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c43",
          "name": "Jiangqiao Yan",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c44",
          "name": "Liangke Gui",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c45",
          "name": "Sheng Bi",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c46",
          "name": "Jiashi Li",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c47",
          "name": "Yuxi Ren",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c48",
          "name": "Rui Wang",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c49",
          "name": "Huixia Li",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c4a",
          "name": "Xuefeng Xiao",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c4b",
          "name": "Shu Liu",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c4c",
          "user": {
            "_id": "636a4e4fa55bbbdf8c877667",
            "avatarUrl": "/avatars/efdb68c56a4a44fdac52750c07a6cc35.svg",
            "isPro": false,
            "fullname": "Ling Feng",
            "user": "lingff",
            "type": "user"
          },
          "name": "Feng Ling",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-14T09:46:31.964Z",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c4d",
          "name": "Heng Zhang",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c4e",
          "name": "Houmin Wei",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c4f",
          "name": "Huafeng Kuang",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c50",
          "name": "Jerry Duncan",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c51",
          "name": "Junda Zhang",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c52",
          "name": "Junru Zheng",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c53",
          "name": "Li Sun",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c54",
          "name": "Manlin Zhang",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c55",
          "name": "Renfei Sun",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c56",
          "name": "Xiaobin Zhuang",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c57",
          "name": "Xiaojie Li",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c58",
          "name": "Xin Xia",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c59",
          "name": "Xuyan Chi",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c5a",
          "name": "Yanghua Peng",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c5b",
          "name": "Yuping Wang",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c5c",
          "name": "Yuxuan Wang",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c5d",
          "name": "Zhongkai Zhao",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c5e",
          "name": "Zhuo Chen",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c5f",
          "name": "Zuquan Song",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c60",
          "user": {
            "_id": "6421183b69a2c2933882d652",
            "avatarUrl": "/avatars/66813a8fa22915087cccd4dbfb945ca7.svg",
            "isPro": false,
            "fullname": "Zhenheng Yang",
            "user": "zhenheny",
            "type": "user"
          },
          "name": "Zhenheng Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-14T09:46:34.053Z",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c61",
          "name": "Jiashi Feng",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c62",
          "name": "Jianchao Yang",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c63",
          "name": "Lu Jiang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64a5cba3bea0116f8f7187a7/Zl6Xq55_4dviWTCMwOptb.jpeg"
      ],
      "publishedAt": "2025-04-11T16:46:20.000Z",
      "submittedOnDailyAt": "2025-04-14T00:55:27.428Z",
      "title": "Algorithme-7B : Apprentissage efficace de modèles basés sur la génération vidéo",
      "submittedOnDailyBy": {
        "_id": "64a5cba3bea0116f8f7187a7",
        "avatarUrl": "/avatars/97b3bad82e359c57d1ab7d6e3aa7748b.svg",
        "isPro": false,
        "fullname": "Lu Jiang",
        "user": "roadjiang",
        "type": "user"
      },
      "summary": "Dans ce rapport technique, des stratégies efficaces pour l'entraînement de modèles basés sur la génération de vidéos sont proposées. Un modèle de taille interne appelé 'Seaweed-7B' est utilisé, qui comporte environ 700 millions de paramètres (7B), entraîné sur un total de 665 000 heures de GPU H100 dans un format réduit. Même lorsque les ressources informatiques sont limitées, 'Seaweed-7B' montre un rendement compétitif par rapport à des modèles grands de génération de vidéos, ce qui est particulièrement important dans des environnements avec des restrictions en ressources. Dans ce rapport technique, les décisions de conception clés qui contribuent à l'amélioration de la performance du modèle interne Difu-Jetion sont présentées de manière centrale. Deux observations expérimentales sont faites : 1) 'Seaweed-7B' peut être comparé et dépasser le rendement des modèles grands entraînés avec de grands ressources de GPU. 2) Les modèles avec une forte capacité de généralisation peuvent s'adapter efficacement à une large gamme d'applications de vidéo. Pour plus d'informations, vous pouvez consulter la page du projet sur https://seaweed.video/.",
      "upvotes": 57,
      "discussionId": "67fc700159b22e7c34d64d78",
      "projectPage": "https://seaweed.video/"
    },
    "publishedAt": "2025-04-11T12:46:20.000Z",
    "title": "Seaweed-7B: Cost-Effective Training of Video Generation Foundation Model",
    "summary": "This technical report presents a cost-efficient strategy for training a video\ngeneration foundation model. We present a mid-sized research model with\napproximately 7 billion parameters (7B) called Seaweed-7B trained from scratch\nusing 665,000 H100 GPU hours. Despite being trained with moderate computational\nresources, Seaweed-7B demonstrates highly competitive performance compared to\ncontemporary video generation models of much larger size. Design choices are\nespecially crucial in a resource-constrained setting. This technical report\nhighlights the key design decisions that enhance the performance of the\nmedium-sized diffusion model. Empirically, we make two observations: (1)\nSeaweed-7B achieves performance comparable to, or even surpasses, larger models\ntrained on substantially greater GPU resources, and (2) our model, which\nexhibits strong generalization ability, can be effectively adapted across a\nwide range of downstream applications either by lightweight fine-tuning or\ncontinue training. See the project page at https://seaweed.video/",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64a5cba3bea0116f8f7187a7/Zl6Xq55_4dviWTCMwOptb.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08685.png",
    "numComments": 6,
    "submittedBy": {
      "_id": "64a5cba3bea0116f8f7187a7",
      "avatarUrl": "/avatars/97b3bad82e359c57d1ab7d6e3aa7748b.svg",
      "fullname": "Lu Jiang",
      "name": "roadjiang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.08736",
      "authors": [
        {
          "_id": "67fc8e37864dfcbd93d3b802",
          "user": {
            "_id": "668125557b50b433cda2a211",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/668125557b50b433cda2a211/j3z3wT5Rv9IyUKtbzQpnc.png",
            "isPro": false,
            "fullname": "Tianwei Xiong",
            "user": "YuuTennYi",
            "type": "user"
          },
          "name": "Tianwei Xiong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-14T09:46:29.330Z",
          "hidden": false
        },
        {
          "_id": "67fc8e37864dfcbd93d3b803",
          "name": "Jun Hao Liew",
          "hidden": false
        },
        {
          "_id": "67fc8e37864dfcbd93d3b804",
          "name": "Zilong Huang",
          "hidden": false
        },
        {
          "_id": "67fc8e37864dfcbd93d3b805",
          "name": "Jiashi Feng",
          "hidden": false
        },
        {
          "_id": "67fc8e37864dfcbd93d3b806",
          "user": {
            "_id": "65d5ec74cd05bc1eaa125040",
            "avatarUrl": "/avatars/2de1b1539a86452c2c89570eeb02f5ab.svg",
            "isPro": false,
            "fullname": "Xihui Liu",
            "user": "XihuiLiu",
            "type": "user"
          },
          "name": "Xihui Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-14T09:46:26.140Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-11T17:59:58.000Z",
      "submittedOnDailyAt": "2025-04-14T02:57:04.488Z",
      "title": "GigaTok : Expansion de 30 milliards de paramètres pour un tokeniseur visuel pour la génération automatique de récupération d'images",
      "submittedOnDailyBy": {
        "_id": "668125557b50b433cda2a211",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/668125557b50b433cda2a211/j3z3wT5Rv9IyUKtbzQpnc.png",
        "isPro": false,
        "fullname": "Tianwei Xiong",
        "user": "YuuTennYi",
        "type": "user"
      },
      "summary": "En la génération d'images par auto-régression (AR), le tokeniseur visuel compresse les images dans un ensemble de potentiels distribués pour faciliter l'apprentissage efficace de la génération visuelle par prédiction des tokens suivants. Bien que la qualité de reconstruction des images puisse s'améliorer en élargissant le tokeniseur visuel, on observe souvent une détérioration de la qualité de la génération dans la partie posteriore. C'est un problème mal abordé dans la littérature actuelle. En réponse à ce défi, nous proposons GigaTok, la première approche qui vise à améliorer simultanément la qualité de reconstruction, de génération et de représentation des images en élargissant le tokeniseur visuel. On a reconnu que l'élargissement de l'espace potentiel est la principale cause du contre-partie entre reconstruction et génération. Pour atténuer cela, nous proposons une normalisation sémantique qui aligne les caractéristiques du tokeniseur avec celles de l'encodeur pré-traité, ce qui aide à éviter la complexité excessive de l'espace potentiel et améliore à la fois la reconstruction et la génération automatique de l'AR. En se basant sur cette normalisation, nous examinons trois stratégies principales pour élargir le tokeniseur : (1) utiliser un tokeniseur 1D pour améliorer la scalabilité, (2) prioriser la scalabilité du décodateur en élargissant aussi l'encodeur, et (3) améliorer la stabilité de l'entraînement du tokeniseur de tailles de berry en utilisant la perte d'entropie. L'élargissement de GigaTok à 30 milliards de paramètres atteint la meilleure qualité en reconstruction, génération et représentation de l'AR.",
      "upvotes": 23,
      "discussionId": "67fc8e38864dfcbd93d3b836",
      "projectPage": "https://silentview.github.io/GigaTok/",
      "githubRepo": "https://github.com/SilentView/GigaTok"
    },
    "publishedAt": "2025-04-11T13:59:58.000Z",
    "title": "GigaTok: Scaling Visual Tokenizers to 3 Billion Parameters for\n  Autoregressive Image Generation",
    "summary": "In autoregressive (AR) image generation, visual tokenizers compress images\ninto compact discrete latent tokens, enabling efficient training of downstream\nautoregressive models for visual generation via next-token prediction. While\nscaling visual tokenizers improves image reconstruction quality, it often\ndegrades downstream generation quality -- a challenge not adequately addressed\nin existing literature. To address this, we introduce GigaTok, the first\napproach to simultaneously improve image reconstruction, generation, and\nrepresentation learning when scaling visual tokenizers. We identify the growing\ncomplexity of latent space as the key factor behind the reconstruction vs.\ngeneration dilemma. To mitigate this, we propose semantic regularization, which\naligns tokenizer features with semantically consistent features from a\npre-trained visual encoder. This constraint prevents excessive latent space\ncomplexity during scaling, yielding consistent improvements in both\nreconstruction and downstream autoregressive generation. Building on semantic\nregularization, we explore three key practices for scaling tokenizers:(1) using\n1D tokenizers for better scalability, (2) prioritizing decoder scaling when\nexpanding both encoder and decoder, and (3) employing entropy loss to stabilize\ntraining for billion-scale tokenizers. By scaling to 3 space billion\nparameters, GigaTok achieves state-of-the-art performance in reconstruction,\ndownstream AR generation, and downstream AR representation quality.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08736.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "668125557b50b433cda2a211",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/668125557b50b433cda2a211/j3z3wT5Rv9IyUKtbzQpnc.png",
      "fullname": "Tianwei Xiong",
      "name": "YuuTennYi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.08388",
      "authors": [
        {
          "_id": "67fc7367df5f5d1e87c14c6a",
          "name": "Junliang Guo",
          "hidden": false
        },
        {
          "_id": "67fc7367df5f5d1e87c14c6b",
          "name": "Yang Ye",
          "hidden": false
        },
        {
          "_id": "67fc7367df5f5d1e87c14c6c",
          "name": "Tianyu He",
          "hidden": false
        },
        {
          "_id": "67fc7367df5f5d1e87c14c6d",
          "name": "Haoyu Wu",
          "hidden": false
        },
        {
          "_id": "67fc7367df5f5d1e87c14c6e",
          "name": "Yushu Jiang",
          "hidden": false
        },
        {
          "_id": "67fc7367df5f5d1e87c14c6f",
          "name": "Tim Pearce",
          "hidden": false
        },
        {
          "_id": "67fc7367df5f5d1e87c14c70",
          "name": "Jiang Bian",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-11T09:41:04.000Z",
      "submittedOnDailyAt": "2025-04-14T02:07:06.500Z",
      "title": "MineWorld : Modèle de monde interactif en temps réel de Minecraft ouvert source",
      "submittedOnDailyBy": {
        "_id": "63468720dd6d90d82ccf3450",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
        "isPro": false,
        "fullname": "YSH",
        "user": "BestWishYsh",
        "type": "user"
      },
      "summary": "Le modélisation du monde est une tâche importante qui permet aux intelligences artificielles d'interagir efficacement avec les êtres humains et de fonctionner dans des environnements dynamiques. Dans cet article, nous proposons un nouveau modèle de monde interactif en séquence, appelé \"MineWorld\", pour le jeu de sandbox \"Minecraft\" en utilisant une approche ouverte et accessible via un \"Test Box\". \"MineWorld\" utilise un auto-régressif de rétroalimentation visuelle pour générer les scènes suivantes à partir de la perception visuelle et des actions. Spécifiquement, nous utilisons la tokenisation d'images et d'actions pour convertir la perception visuelle et les actions en IDs discrètes, ce qui est combiné pour configurer l'entrée du modèle. Ensuite, grâce à la prédiction de la tokenisation future, nous entraînons le modèle pour représenter de manière riche l'état du jeu et les relations entre l'état et les actions. Pendant l'inférence, nous développons un nouvel algorithme de décodage parallèle qui prédit simultanément les répétitions spatiales de chaque frame, permettant aux joueurs d'interagir de manière séquentielle et aux modèles à différentes échelles de générer entre 4 et 7 frames. Dans l'évaluation, nous proposons de nouvelles métriques pour évaluer la qualité visuelle des nouvelles scènes et l'adaptabilité des actions, démontrant l'importance du modèle de monde. Dans des évaluations détaillées, l'effet de \"MineWorld\" dépasse significativement les modèles de monde basés sur la ramification de versions ordonnées. Les codes et modèles ont été publiés.",
      "upvotes": 12,
      "discussionId": "67fc7367df5f5d1e87c14ca6",
      "githubRepo": "https://github.com/microsoft/MineWorld"
    },
    "publishedAt": "2025-04-11T05:41:04.000Z",
    "title": "MineWorld: a Real-Time and Open-Source Interactive World Model on\n  Minecraft",
    "summary": "World modeling is a crucial task for enabling intelligent agents to\neffectively interact with humans and operate in dynamic environments. In this\nwork, we propose MineWorld, a real-time interactive world model on Minecraft,\nan open-ended sandbox game which has been utilized as a common testbed for\nworld modeling. MineWorld is driven by a visual-action autoregressive\nTransformer, which takes paired game scenes and corresponding actions as input,\nand generates consequent new scenes following the actions. Specifically, by\ntransforming visual game scenes and actions into discrete token ids with an\nimage tokenizer and an action tokenizer correspondingly, we consist the model\ninput with the concatenation of the two kinds of ids interleaved. The model is\nthen trained with next token prediction to learn rich representations of game\nstates as well as the conditions between states and actions simultaneously. In\ninference, we develop a novel parallel decoding algorithm that predicts the\nspatial redundant tokens in each frame at the same time, letting models in\ndifferent scales generate 4 to 7 frames per second and enabling real-time\ninteractions with game players. In evaluation, we propose new metrics to assess\nnot only visual quality but also the action following capacity when generating\nnew scenes, which is crucial for a world model. Our comprehensive evaluation\nshows the efficacy of MineWorld, outperforming SoTA open-sourced diffusion\nbased world models significantly. The code and model have been released.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08388.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "63468720dd6d90d82ccf3450",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
      "fullname": "YSH",
      "name": "BestWishYsh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 46
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.07963",
      "authors": [
        {
          "_id": "67f86da6ac109135e18e150f",
          "name": "Shoufa Chen",
          "hidden": false
        },
        {
          "_id": "67f86da6ac109135e18e1510",
          "name": "Chongjian Ge",
          "hidden": false
        },
        {
          "_id": "67f86da6ac109135e18e1511",
          "name": "Shilong Zhang",
          "hidden": false
        },
        {
          "_id": "67f86da6ac109135e18e1512",
          "name": "Peize Sun",
          "hidden": false
        },
        {
          "_id": "67f86da6ac109135e18e1513",
          "name": "Ping Luo",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6412a33900634c4fe9873652/tQXTl59CIq5IxKj_5dk7f.jpeg"
      ],
      "publishedAt": "2025-04-10T17:59:56.000Z",
      "submittedOnDailyAt": "2025-04-14T04:05:17.975Z",
      "title": "PixelFlow : Modèle génératif dans l'espace des pixels en utilisant des flux",
      "submittedOnDailyBy": {
        "_id": "6412a33900634c4fe9873652",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6412a33900634c4fe9873652/Nmn_yRA1gGD2VO1YbSOYF.jpeg",
        "isPro": false,
        "fullname": "Shoufa Chen",
        "user": "ShoufaChen",
        "type": "user"
      },
      "summary": "PixelFlow est un ensemble de modèles de génération d'images qui opèrent directement dans l'espace des pixels réels, en opposition aux modèles communs de l'espace potentiel. Cette approche réduit la nécessité de former préalablement un Autoencodeur Variantiel (VAE) et permet que le modèle soit entraînable de départ à arrivée. Grâce à un modèle efficace hiérarchique, PixelFlow peut réduire les coûts de calcul dans l'espace des pixels réels. Dans le benchmark de génération d'images conditionnées par classes d'ImageNet de 256x256, le score FID est de 1,98. Les résultats d'images générées à partir de textes de haute qualité démontrent que PixelFlow excelle en qualité, artisanat et contrôle sémantique des images. Nous espérons que ce nouveau paradigme ouvre de nouvelles opportunités et inspirations pour les prochaines générations de modèles de génération d'images. Les codes et modèles sont disponibles sur https://github.com/ShoufaChen/PixelFlow.",
      "upvotes": 8,
      "discussionId": "67f86da7ac109135e18e154b",
      "githubRepo": "https://github.com/ShoufaChen/PixelFlow"
    },
    "publishedAt": "2025-04-10T13:59:56.000Z",
    "title": "PixelFlow: Pixel-Space Generative Models with Flow",
    "summary": "We present PixelFlow, a family of image generation models that operate\ndirectly in the raw pixel space, in contrast to the predominant latent-space\nmodels. This approach simplifies the image generation process by eliminating\nthe need for a pre-trained Variational Autoencoder (VAE) and enabling the whole\nmodel end-to-end trainable. Through efficient cascade flow modeling, PixelFlow\nachieves affordable computation cost in pixel space. It achieves an FID of 1.98\non 256times256 ImageNet class-conditional image generation benchmark. The\nqualitative text-to-image results demonstrate that PixelFlow excels in image\nquality, artistry, and semantic control. We hope this new paradigm will inspire\nand open up new opportunities for next-generation visual generation models.\nCode and models are available at https://github.com/ShoufaChen/PixelFlow.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6412a33900634c4fe9873652/tQXTl59CIq5IxKj_5dk7f.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07963.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6412a33900634c4fe9873652",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6412a33900634c4fe9873652/Nmn_yRA1gGD2VO1YbSOYF.jpeg",
      "fullname": "Shoufa Chen",
      "name": "ShoufaChen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 19
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.08600",
      "authors": [
        {
          "_id": "67fc9e72b2383c63dc413dcb",
          "name": "Peixian Ma",
          "hidden": false
        },
        {
          "_id": "67fc9e72b2383c63dc413dcc",
          "user": {
            "_id": "6575a625b951d40e7a4d8685",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6575a625b951d40e7a4d8685/mcnyqU--r-vi11aXN02cZ.jpeg",
            "isPro": false,
            "fullname": "zhuangxialie",
            "user": "ZhuangXialie",
            "type": "user"
          },
          "name": "Xialie Zhuang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-14T09:46:23.810Z",
          "hidden": false
        },
        {
          "_id": "67fc9e72b2383c63dc413dcd",
          "name": "Chengjin Xu",
          "hidden": false
        },
        {
          "_id": "67fc9e72b2383c63dc413dce",
          "name": "Xuhui Jiang",
          "hidden": false
        },
        {
          "_id": "67fc9e72b2383c63dc413dcf",
          "name": "Ran Chen",
          "hidden": false
        },
        {
          "_id": "67fc9e72b2383c63dc413dd0",
          "name": "Jian Guo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-11T15:01:30.000Z",
      "submittedOnDailyAt": "2025-04-14T04:07:17.501Z",
      "title": "Entraînement d'un modèle d'inférence SQL à partir du langage nature par apprentissage par renforcement",
      "submittedOnDailyBy": {
        "_id": "6575a625b951d40e7a4d8685",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6575a625b951d40e7a4d8685/mcnyqU--r-vi11aXN02cZ.jpeg",
        "isPro": false,
        "fullname": "zhuangxialie",
        "user": "ZhuangXialie",
        "type": "user"
      },
      "summary": "La conversion de la nature en SQL (NL2SQL) transforme les requêtes en nature en phrases SQL structurées pour faciliter l'interaction intuitive avec les bases de données. Il y a eu un développement récent dans l'amélioration de l'interaction humain-ordinateur dans les applications de bases de données, mais il reste des problèmes significatifs dans la capacité d'inférence pour relier plusieurs tables et pour les requêtes cachées dans des scénarios complexes. Actuellement, on utilise principalement des ajustements personnalisés micro (SFT) pour entraîner des modèles NL2SQL, mais ces méthodes ont des limitations dans l'adaptation et la compréhension dans de nouveaux environnements (comme financiers ou de santé). Pour améliorer l'efficacité de l'inférence des modèles NL2SQL dans des situations complexes, on introduit SQL-R1, un nouveau modèle d'inférence NL2SQL. Ce modèle a été entraîné en utilisant des algorithmes d'apprentissage par renforcement (RL). On a conçu une fonction de récompense basée sur l'apprentissage par renforcement spécifique pour le travail NL2SQL et on a analysé l'impact initial des effets de l'apprentissage par renforcement. De plus, on a atteint une précision compétitive avec un petit ensemble de données synthétiques de NL2SQL et on a exploré de nouvelles formes d'ingénierie de données pour améliorer cette précision. Dans des expériences précédentes, SQL-R1 a atteint une précision de 88,6% sur VIEWER et 66,6% sur BIRD.",
      "upvotes": 6,
      "discussionId": "67fc9e73b2383c63dc413e19"
    },
    "publishedAt": "2025-04-11T11:01:30.000Z",
    "title": "SQL-R1: Training Natural Language to SQL Reasoning Model By\n  Reinforcement Learning",
    "summary": "Natural Language to SQL (NL2SQL) enables intuitive interactions with\ndatabases by transforming natural language queries into structured SQL\nstatements. Despite recent advancements in enhancing human-computer interaction\nwithin database applications, significant challenges persist, particularly\nregarding the inference performance in complex scenarios involving multi-table\njoins and nested queries. Current methodologies primarily utilize supervised\nfine-tuning (SFT) to train the NL2SQL model, which may limit adaptability and\ninterpretability in new environments (e.g., finance and healthcare). In order\nto enhance the reasoning performance of the NL2SQL model in the above complex\nsituations, we introduce SQL-R1, a novel NL2SQL reasoning model trained by the\nreinforcement learning (RL) algorithms. We design a specialized RL-based reward\nfunction tailored for NL2SQL tasks and discussed the impact of cold start on\nthe effectiveness of intensive training. In addition, we achieve competitive\naccuracy using only a tiny amount of synthetic NL2SQL data for augmented\ntraining and further explore data engineering for RL. In existing experiments,\nSQL-R1 achieves execution accuracy of 88.6% and 66.6% on the benchmark Spider\nand BIRD, respectively, only using the 7B base model.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08600.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6575a625b951d40e7a4d8685",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6575a625b951d40e7a4d8685/mcnyqU--r-vi11aXN02cZ.jpeg",
      "fullname": "zhuangxialie",
      "name": "ZhuangXialie",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.07405",
      "authors": [
        {
          "_id": "67fa383909d06d0501a5e34e",
          "user": {
            "_id": "660d844462d63ad0009a9859",
            "avatarUrl": "/avatars/6822fc1a82c64dbfd40b88080a5fb1ae.svg",
            "isPro": false,
            "fullname": "Linyan Huang",
            "user": "DevLinyan",
            "type": "user"
          },
          "name": "Linyan Huang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-13T19:24:47.680Z",
          "hidden": false
        },
        {
          "_id": "67fa383909d06d0501a5e34f",
          "name": "Haonan Lin",
          "hidden": false
        },
        {
          "_id": "67fa383909d06d0501a5e350",
          "name": "Yanning Zhou",
          "hidden": false
        },
        {
          "_id": "67fa383909d06d0501a5e351",
          "name": "Kaiwen Xiao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-10T02:58:22.000Z",
      "submittedOnDailyAt": "2025-04-14T00:54:04.513Z",
      "title": "FlexIP : Contrôle Dynamique de Précision (Précision) et de Personnalité (Personnalité) pour la Génération d'Images Personnalisées pour l'Utilisateur",
      "submittedOnDailyBy": {
        "_id": "63468720dd6d90d82ccf3450",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
        "isPro": false,
        "fullname": "YSH",
        "user": "BestWishYsh",
        "type": "user"
      },
      "summary": "Le rapide développement de modèles de génération 2D a conduit à ce que le maintien de la reconnaissance de thèmes et la possibilité d'édition de manière variée soient des points d'accueil importants de recherche. Les méthodes actuelles présentent généralement un compromis spécial entre la protection de la reconnaissance et la régulation de la représentation de personnes. Nous présentons FlexIP, un nouveau cadre de travail qui inclut un Adapter de personnalisation pour les opérations de style et un Adapter de préservation pour maintenir la reconnaissance. Ce cadre de travail injecte une structure de contrôle direct dans les modèles génératifs et permet un contrôle dynamique des ajustements de poids lors de l'inférence, facilitant un contrôle flexible des paramètres. Les résultats des expériences montrent que notre approche dépasse les limites des méthodes d'évaluation et peut effectuer une protection de la reconnaissance de haut niveau et soutenir la génération de représentations de personnes plus diverses (page du projet : https://flexip-tech.github.io/flexip/).",
      "upvotes": 6,
      "discussionId": "67fa383c09d06d0501a5e3ef",
      "projectPage": "https://flexip-tech.github.io/flexip"
    },
    "publishedAt": "2025-04-09T22:58:22.000Z",
    "title": "FlexIP: Dynamic Control of Preservation and Personality for Customized\n  Image Generation",
    "summary": "With the rapid advancement of 2D generative models, preserving subject\nidentity while enabling diverse editing has emerged as a critical research\nfocus. Existing methods typically face inherent trade-offs between identity\npreservation and personalized manipulation. We introduce FlexIP, a novel\nframework that decouples these objectives through two dedicated components: a\nPersonalization Adapter for stylistic manipulation and a Preservation Adapter\nfor identity maintenance. By explicitly injecting both control mechanisms into\nthe generative model, our framework enables flexible parameterized control\nduring inference through dynamic tuning of the weight adapter. Experimental\nresults demonstrate that our approach breaks through the performance\nlimitations of conventional methods, achieving superior identity preservation\nwhile supporting more diverse personalized generation capabilities (Project\nPage: https://flexip-tech.github.io/flexip/).",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07405.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63468720dd6d90d82ccf3450",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
      "fullname": "YSH",
      "name": "BestWishYsh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 46
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.08366",
      "authors": [
        {
          "_id": "67fc6d8d74c3c0f0d6f24c3b",
          "name": "Sauradip Nag",
          "hidden": false
        },
        {
          "_id": "67fc6d8d74c3c0f0d6f24c3c",
          "name": "Daniel Cohen-Or",
          "hidden": false
        },
        {
          "_id": "67fc6d8d74c3c0f0d6f24c3d",
          "name": "Hao Zhang",
          "hidden": false
        },
        {
          "_id": "67fc6d8d74c3c0f0d6f24c3e",
          "name": "Ali Mahdavi-Amiri",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-11T09:01:09.000Z",
      "submittedOnDailyAt": "2025-04-14T00:36:54.457Z",
      "title": "In-2-4D : 2 images à vue unique à la génération indirecte en 4D",
      "submittedOnDailyBy": {
        "_id": "6399ab3296ce14c5dcf4ccbf",
        "avatarUrl": "/avatars/89aeedc96f73d76f7a6da96454f40fd2.svg",
        "isPro": false,
        "fullname": "Sauradip Nag",
        "user": "sauradip",
        "type": "user"
      },
      "summary": "Nous proposons une nouvelle problématique appelée \"In-2-4D\" en relation avec la génération d'indirectivité 4D (c'est-à-dire, 3D + mouvement) à partir de configurations d'entrée minimales : nous capturons deux états de mouvement différents d'un objet à partir de deux images en un seul regard. Lorsque ces deux images représentent le début et la fin du mouvement, notre objectif est de générer et de reconstruire le mouvement en 4D. Nous utilisons des modèles d'interface vidéo pour prédire le mouvement, évitant ainsi des interprétations incertaines du mouvement sur de grands intervalles de frames. Pour résoudre ce problème, nous identifions des points clés visuellement proches à l'état d'entrée et nous cherchons des mouvements notables pour générer des frames lisses entre eux, en adoptant une approche hiérarchique. Dans chaque frame, la représentation 3D des points clés est construite en utilisant un lissage gaussien. La structure temporelle des frames permet de déformer le mouvement de manière gaussienne, améliorant la consistence temporelle et la précision du mouvement 3D. Pour améliorer cette consistence et cette précision, nous étendons l'attention auto-diffusée de multiples vues et appliquons une normalisation de déformations rigides à chaque étape temporelle. Enfin, les segments de mouvement 3D générés indépendamment sont intégrés par un champ de déformation des bords pour qu'ils coincident avec l'interface vidéo, garantissant un contexte lisse et continu. Dans des expériences détaillées, tant qualitatives que quantitatives, et dans un scénario d'utilisateur, nous montrons les effets de notre méthode et ses composants. Le site web du projet est disponible sur https://in-2-4d.github.io/.",
      "upvotes": 4,
      "discussionId": "67fc6d9374c3c0f0d6f24e12",
      "projectPage": "https://in-2-4d.github.io/",
      "githubRepo": "https://github.com/sauradip/In-2-4D"
    },
    "publishedAt": "2025-04-11T05:01:09.000Z",
    "title": "In-2-4D: Inbetweening from Two Single-View Images to 4D Generation",
    "summary": "We propose a new problem, In-2-4D, for generative 4D (i.e., 3D + motion)\ninbetweening from a minimalistic input setting: two single-view images\ncapturing an object in two distinct motion states. Given two images\nrepresenting the start and end states of an object in motion, our goal is to\ngenerate and reconstruct the motion in 4D. We utilize a video interpolation\nmodel to predict the motion, but large frame-to-frame motions can lead to\nambiguous interpretations. To overcome this, we employ a hierarchical approach\nto identify keyframes that are visually close to the input states and show\nsignificant motion, then generate smooth fragments between them. For each\nfragment, we construct the 3D representation of the keyframe using Gaussian\nSplatting. The temporal frames within the fragment guide the motion, enabling\ntheir transformation into dynamic Gaussians through a deformation field. To\nimprove temporal consistency and refine 3D motion, we expand the self-attention\nof multi-view diffusion across timesteps and apply rigid transformation\nregularization. Finally, we merge the independently generated 3D motion\nsegments by interpolating boundary deformation fields and optimizing them to\nalign with the guiding video, ensuring smooth and flicker-free transitions.\nThrough extensive qualitative and quantitiave experiments as well as a user\nstudy, we show the effectiveness of our method and its components. The project\npage is available at https://in-2-4d.github.io/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08366.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6399ab3296ce14c5dcf4ccbf",
      "avatarUrl": "/avatars/89aeedc96f73d76f7a6da96454f40fd2.svg",
      "fullname": "Sauradip Nag",
      "name": "sauradip",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.08716",
      "authors": [
        {
          "_id": "67fca0ca05cd5b5035123b7e",
          "name": "Wissam Antoun",
          "hidden": false
        },
        {
          "_id": "67fca0ca05cd5b5035123b7f",
          "name": "Benoît Sagot",
          "hidden": false
        },
        {
          "_id": "67fca0ca05cd5b5035123b80",
          "name": "Djamé Seddah",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-11T17:29:35.000Z",
      "submittedOnDailyAt": "2025-04-14T04:20:01.034Z",
      "title": "ModernBERT vs DeBERTaV3 ? Analyse de l'influence de l'architecture et des données sur le rendement des modèles encoder de Transformer.",
      "submittedOnDailyBy": {
        "_id": "5e6a3d4ea9afd5125d9ec064",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1584020801691-noauth.jpeg",
        "isPro": true,
        "fullname": "Stefan Schweter",
        "user": "stefan-it",
        "type": "user"
      },
      "summary": "Dans le modèle de prédiction, des développements structurels ont été introduits pour améliorer l'efficacité et la performance, comme le deBERTaV3 et le modernBERT, parmi d'autres modèles de codificateurs pré-entraînés de tenseurs. Les auteurs du modernBERT ont rapporté des améliorations de rendement sur la plupart des benchmarks par rapport au deBERTaV3, mais, en raison de l'absence de comparaison avec des données d'entraînement non publiées et des ensembles de données partagés, il est difficile de déterminer si ces améliorations sont dues à des améliorations structurelles ou à des différences dans les données d'entraînement. Dans cette étude, des ensembles de données comme celui du modernBERT ont été utilisés pour entraîner le modernBERT, et une recherche contrôlée a été réalisée pour séparer l'impact du design du modèle. En fin de compte, la génération des modèles précédents surpasse les effets de l'échantillon et le rendement général des benchmarks, mais la principale avantage de modernBERT est sa vitesse d'entraînement et d'inférence. Cependant, le nouveau modèle proposé offre des améliorations structurelles significatives par rapport à BERT et RoBERTa. De plus, les données d'entraînement de haute qualité accélèrent la convergence, mais n'ont pas un impact significatif sur le rendement final. Cela montre la possibilité de biais dans les benchmarks. Ces résultats démontrent l'importance de distinguer entre les données d'entraînement et les innovations structurelles lors de l'évaluation de modèles pré-entraînés.",
      "upvotes": 3,
      "discussionId": "67fca0ca05cd5b5035123ba6"
    },
    "publishedAt": "2025-04-11T13:29:35.000Z",
    "title": "ModernBERT or DeBERTaV3? Examining Architecture and Data Influence on\n  Transformer Encoder Models Performance",
    "summary": "Pretrained transformer-encoder models like DeBERTaV3 and ModernBERT introduce\narchitectural advancements aimed at improving efficiency and performance.\nAlthough the authors of ModernBERT report improved performance over DeBERTaV3\non several benchmarks, the lack of disclosed training data and the absence of\ncomparisons using a shared dataset make it difficult to determine whether these\ngains are due to architectural improvements or differences in training data. In\nthis work, we conduct a controlled study by pretraining ModernBERT on the same\ndataset as CamemBERTaV2, a DeBERTaV3 French model, isolating the effect of\nmodel design. Our results show that the previous model generation remains\nsuperior in sample efficiency and overall benchmark performance, with\nModernBERT's primary advantage being faster training and inference speed.\nHowever, the new proposed model still provides meaningful architectural\nimprovements compared to earlier models such as BERT and RoBERTa. Additionally,\nwe observe that high-quality pre-training data accelerates convergence but does\nnot significantly improve final performance, suggesting potential benchmark\nsaturation. These findings show the importance of disentangling pretraining\ndata from architectural innovations when evaluating transformer models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08716.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5e6a3d4ea9afd5125d9ec064",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1584020801691-noauth.jpeg",
      "fullname": "Stefan Schweter",
      "name": "stefan-it",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2491
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.08192",
      "authors": [
        {
          "_id": "67fcb3584a92187863e732d5",
          "name": "Aashiq Muhamed",
          "hidden": false
        },
        {
          "_id": "67fcb3584a92187863e732d6",
          "name": "Jacopo Bonato",
          "hidden": false
        },
        {
          "_id": "67fcb3584a92187863e732d7",
          "name": "Mona Diab",
          "hidden": false
        },
        {
          "_id": "67fcb3584a92187863e732d8",
          "name": "Virginia Smith",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-11T01:24:03.000Z",
      "submittedOnDailyAt": "2025-04-14T05:34:15.388Z",
      "title": "SAEs peuvent améliorer la désaprentissage : Codificateur Autonome Dynamique\nLigne de Protection pour l'Apprentissage de Précision dans les LLM",
      "submittedOnDailyBy": {
        "_id": "64755a83e0b188d3cb2579d8",
        "avatarUrl": "/avatars/2c50590905f4bd398a4c9991e1b4b5bb.svg",
        "isPro": false,
        "fullname": "Aashiq Muhamed",
        "user": "aashiqmuhamed",
        "type": "user"
      },
      "summary": "La Réseau Neuronal de Unités est un méthode potentielle pour améliorer la sécurité des modèles LLM en supprimant les connaissances insatisfaisantes. Cependant, les méthodes de unités basées sur le gradient utilisées généralement rencontrent des problèmes tels que un coût de calcul élevé, l'instabilité des paramètres, la diminution de la capacité des unités en séquence, la vulnérabilité aux attaques de ré-entraînement, une faible efficacité des données et la manque d'interprétabilité. Le désactivateur des activations (sparsity out-weights) permet aux unités basées sur des activations spécifiques et améliore ces aspects, mais les méthodes existantes sont moins efficaces que celles basées sur le gradient. Dans cet article, les résultats antérieurs sont contrariés et il est montré que le SAE améliore significativement les unités lorsqu'il est utilisé dynamiquement. On présente le Dynamic Denoising Autoencoder Guardrails (DSG). Le DSG est un nouveau méthode pour les unités de précision utilisant la sélection de caractéristiques basiques et un classificateur dynamique de caractéristiques. Selon les résultats des tests, le DSG dépasse considérablement les principales méthodes de unités et équilibre bien entre oublier et aider. Le DSG résout les principaux défis des méthodes basées sur le gradient, offrant un haut rendement de unités en séquence, une résistance aux attaques de ré-entraînement, une amélioration de l'efficacité des données (y compris les ensembles de 0 shot) et fournissant une interprétabilité des unités.",
      "upvotes": 2,
      "discussionId": "67fcb3594a92187863e732fa"
    },
    "publishedAt": "2025-04-10T21:24:03.000Z",
    "title": "SAEs Can Improve Unlearning: Dynamic Sparse Autoencoder\n  Guardrails for Precision Unlearning in LLMs",
    "summary": "Machine unlearning is a promising approach to improve LLM safety by removing\nunwanted knowledge from the model. However, prevailing gradient-based\nunlearning methods suffer from issues such as high computational costs,\nhyperparameter instability, poor sequential unlearning capability,\nvulnerability to relearning attacks, low data efficiency, and lack of\ninterpretability. While Sparse Autoencoders are well-suited to improve these\naspects by enabling targeted activation-based unlearning, prior approaches\nunderperform gradient-based methods. This work demonstrates that, contrary to\nthese earlier findings, SAEs can significantly improve unlearning when employed\ndynamically. We introduce Dynamic DAE Guardrails (DSG), a novel\nmethod for precision unlearning that leverages principled feature selection and\na dynamic classifier. Our experiments show DSG substantially outperforms\nleading unlearning methods, achieving superior forget-utility trade-offs. DSG\naddresses key drawbacks of gradient-based approaches for unlearning -- offering\nenhanced computational efficiency and stability, robust performance in\nsequential unlearning, stronger resistance to relearning attacks, better data\nefficiency including zero-shot settings, and more interpretable unlearning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08192.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64755a83e0b188d3cb2579d8",
      "avatarUrl": "/avatars/2c50590905f4bd398a4c9991e1b4b5bb.svg",
      "fullname": "Aashiq Muhamed",
      "name": "aashiqmuhamed",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.01883",
      "authors": [
        {
          "_id": "67fcb50ea69150c25fb4b645",
          "name": "Aashiq Muhamed",
          "hidden": false
        },
        {
          "_id": "67fcb50ea69150c25fb4b646",
          "name": "Mona Diab",
          "hidden": false
        },
        {
          "_id": "67fcb50ea69150c25fb4b647",
          "name": "Virginia Smith",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-02T16:40:43.000Z",
      "submittedOnDailyAt": "2025-04-14T05:41:34.796Z",
      "title": "CoRAG : Génération de Données de Signature de Collaboration",
      "submittedOnDailyBy": {
        "_id": "64755a83e0b188d3cb2579d8",
        "avatarUrl": "/avatars/2c50590905f4bd398a4c9991e1b4b5bb.svg",
        "isPro": false,
        "fullname": "Aashiq Muhamed",
        "user": "aashiqmuhamed",
        "type": "user"
      },
      "summary": "Le modèle de Révision et Augmentation de la Personne (RAG) est un modèle spécialisé dans les tâches de concentration des connaissances et montre un excellent rendement même sous les contraintes de l'entraînement d'images caractéristiques. Nous présentons CoRAG. CoRAG est un cadre de travail qui étend le RAG, permettant aux utilisateurs de partager un stockage de pas pour entraîner un modèle ensemble. En ce qui concerne l'évaluation de CoRAG, nous présentons CRAB. CRAB est un cadre d'évaluation pour les réponses aux questions de domaine ouvert dans l'identité de l'apprentissage partagé. Nos expériences montrent que CoRAG peut apprendre partagé avec un échelonneur de ressources bas, alignant les modèles RAG entraînés localement et obtenant un excellent rendement. L'analyse réalisée révèle l'importance des pas partagés pertinents à l'intérieur du stockage partagé, le bénéfice surprisant des pas non pertinents, et la possibilité que les pas négatifs affectent négativement le rendement. Cela introduit de nouvelles considérations dans le RAG partagé : l'utilisation d'un savoir enrichi partagé et le risque de comportements nocifs dérivés de pas nocifs d'autres utilisateurs. Nos résultats soulignent la possibilité de CoRAG et identifient les principales questions de conception et les voies de recherche futures possibles.",
      "upvotes": 2,
      "discussionId": "67fcb510a69150c25fb4b6b1"
    },
    "publishedAt": "2025-04-02T12:40:43.000Z",
    "title": "CoRAG: Collaborative Retrieval-Augmented Generation",
    "summary": "Retrieval-Augmented Generation (RAG) models excel in knowledge-intensive\ntasks, especially under few-shot learning constraints. We introduce CoRAG, a\nframework extending RAG to collaborative settings, where clients jointly train\na shared model using a collaborative passage store. To evaluate CoRAG, we\nintroduce CRAB, a benchmark for collaborative homogeneous open-domain question\nanswering. Our experiments demonstrate that CoRAG consistently outperforms both\nparametric collaborative learning methods and locally trained RAG models in\nlow-resource scenarios. Further analysis reveals the critical importance of\nrelevant passages within the shared store, the surprising benefits of\nincorporating irrelevant passages, and the potential for hard negatives to\nnegatively impact performance. This introduces a novel consideration in\ncollaborative RAG: the trade-off between leveraging a collectively enriched\nknowledge base and the potential risk of incorporating detrimental passages\nfrom other clients. Our findings underscore the viability of CoRAG, while also\nhighlighting key design challenges and promising avenues for future research.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.01883.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64755a83e0b188d3cb2579d8",
      "avatarUrl": "/avatars/2c50590905f4bd398a4c9991e1b4b5bb.svg",
      "fullname": "Aashiq Muhamed",
      "name": "aashiqmuhamed",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.05303",
      "authors": [
        {
          "_id": "67fcbbe8daf0cf6803943949",
          "user": {
            "_id": "6492bf9681d93008eb33f167",
            "avatarUrl": "/avatars/3efa34de104ae400ba5a75b54f0f5b6e.svg",
            "isPro": false,
            "fullname": "Sai Kumar Dwivedi",
            "user": "saidwivedi",
            "type": "user"
          },
          "name": "Sai Kumar Dwivedi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-14T09:46:12.532Z",
          "hidden": false
        },
        {
          "_id": "67fcbbe8daf0cf680394394a",
          "name": "Dimitrije Antić",
          "hidden": false
        },
        {
          "_id": "67fcbbe8daf0cf680394394b",
          "name": "Shashank Tripathi",
          "hidden": false
        },
        {
          "_id": "67fcbbe8daf0cf680394394c",
          "name": "Omid Taheri",
          "hidden": false
        },
        {
          "_id": "67fcbbe8daf0cf680394394d",
          "name": "Cordelia Schmid",
          "hidden": false
        },
        {
          "_id": "67fcbbe8daf0cf680394394e",
          "name": "Michael J. Black",
          "hidden": false
        },
        {
          "_id": "67fcbbe8daf0cf680394394f",
          "name": "Dimitrios Tzionas",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-07T17:59:33.000Z",
      "submittedOnDailyAt": "2025-04-14T06:14:23.936Z",
      "title": "InteractVLM: Inférence d'interaction 3D à partir de modèles de base 2D",
      "submittedOnDailyBy": {
        "_id": "6492bf9681d93008eb33f167",
        "avatarUrl": "/avatars/3efa34de104ae400ba5a75b54f0f5b6e.svg",
        "isPro": false,
        "fullname": "Sai Kumar Dwivedi",
        "user": "saidwivedi",
        "type": "user"
      },
      "summary": "InteractVLM est un nouveau méthode pour inférer les points de contact 3D de personnes ou d'objets à partir d'une seule image naturelle de l'environnement. Cela permet de reconstruire exactement la structure 3D commune de personnes et d'objets. Cependant, il en face des défis complexes en raison d'obstructions, d'incertitude de profondeur et de changements extrêmes dans la forme des objets. Les méthodes actuelles obtiennent des annotations 3D de contact ou des étiquetages de mouvements de mains longs et coûteux à partir de systèmes de détection de mouvement 3D. Cela limite l'échelle et la capacité de généralisation. Pour résoudre ces problèmes, InteractVLM utilise le savoir visuel des modèles de langage visuel (VLMs) larges et fine-tune avec des données limitées de contact 3D. Cependant, l'application directe de ces modèles est complexe, car il nécessite réaliser une logique en 2D. Par conséquent, InteractVLM introduit un nouveau module Render-Localize-Lift. Ce module (1) insère les surfaces du corps et des objets dans l'espace 2D à travers plusieurs images, (2) entraîne un nouveau modèle pour estimer de nouvelles positions d'images en 2D pour inférer le contact, et (3) les réalise en 3D. De plus, InteractVLM propose un nouveau défi d'estimation de contact humain sémantique. Cela permet de prédire le contact humain en fonction de la sémantique des objets et de modéliser une interaction riche. InteractVLM dépasse les méthodes actuelles en termes d'inférence de contact et de reconstruction 3D, et soutient la reconstruction 3D à partir d'images naturelles de l'environnement. Le code et les modèles sont disponibles sur https://interactvlm.is.tue.mpg.de.",
      "upvotes": 0,
      "discussionId": "67fcbbeadaf0cf68039439b9",
      "projectPage": "https://interactvlm.is.tue.mpg.de/",
      "githubRepo": "https://github.com/saidwivedi/InteractVLM"
    },
    "publishedAt": "2025-04-07T13:59:33.000Z",
    "title": "InteractVLM: 3D Interaction Reasoning from 2D Foundational Models",
    "summary": "We introduce InteractVLM, a novel method to estimate 3D contact points on\nhuman bodies and objects from single in-the-wild images, enabling accurate\nhuman-object joint reconstruction in 3D. This is challenging due to occlusions,\ndepth ambiguities, and widely varying object shapes. Existing methods rely on\n3D contact annotations collected via expensive motion-capture systems or\ntedious manual labeling, limiting scalability and generalization. To overcome\nthis, InteractVLM harnesses the broad visual knowledge of large Vision-Language\nModels (VLMs), fine-tuned with limited 3D contact data. However, directly\napplying these models is non-trivial, as they reason only in 2D, while\nhuman-object contact is inherently 3D. Thus we introduce a novel\nRender-Localize-Lift module that: (1) embeds 3D body and object surfaces in 2D\nspace via multi-view rendering, (2) trains a novel multi-view localization\nmodel (MV-Loc) to infer contacts in 2D, and (3) lifts these to 3D.\nAdditionally, we propose a new task called Semantic Human Contact estimation,\nwhere human contact predictions are conditioned explicitly on object semantics,\nenabling richer interaction modeling. InteractVLM outperforms existing work on\ncontact estimation and also facilitates 3D reconstruction from an in-the wild\nimage. Code and models are available at https://interactvlm.is.tue.mpg.de.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.05303.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6492bf9681d93008eb33f167",
      "avatarUrl": "/avatars/3efa34de104ae400ba5a75b54f0f5b6e.svg",
      "fullname": "Sai Kumar Dwivedi",
      "name": "saidwivedi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.05262",
      "authors": [
        {
          "_id": "67fcc9a980568c7ef6180dcb",
          "name": "Yang Yan",
          "hidden": false
        },
        {
          "_id": "67fcc9a980568c7ef6180dcc",
          "name": "Yu Lu",
          "hidden": false
        },
        {
          "_id": "67fcc9a980568c7ef6180dcd",
          "name": "Renjun Xu",
          "hidden": false
        },
        {
          "_id": "67fcc9a980568c7ef6180dce",
          "name": "Zhenzhong Lan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-07T16:57:10.000Z",
      "submittedOnDailyAt": "2025-04-14T07:11:56.706Z",
      "title": "Les modèles de langue de niveau de dépendance dans les LLMs ne comprennent pas vraiment la base de la somme. Comparaison de l'apprentissage des règles et de la mémorisation.",
      "submittedOnDailyBy": {
        "_id": "62ce6dd785cfd21c04c7e6f5",
        "avatarUrl": "/avatars/89837a5dea6e2d753d59caad142bed4a.svg",
        "isPro": false,
        "fullname": "ZhenzhongLan",
        "user": "DannyLan",
        "type": "user"
      },
      "summary": "Bien sûr, voici la traduction en français de l'texte anglais, en maintenant la professionnalité et l'exactitude :\n\nMalgré les scores du référentiel étant élevés, les modèles de langage à grande échelle (LLMs) échouent dans des problèmes simples et soulèvent des questions importantes : apprennent-ils les principes de la mathématique les LLMs ou seulement dépendent-ils de la mémoire de patrons ? Au lieu de concevoir des référentiels plus complexes que les derniers études, on étudie actuellement deux caractéristiques fondamentales en utilisant la somme de deux nombres entiers (de 0 à 2^64) : l'interchangeabilité (A+B=B+A) et la généralisation structurelle (en utilisant des cartes de formules, par exemple, 7→y). Les meilleurs LLMs actuels atteignent une précision de 73,8-99,8% dans la somme de nombres, mais leur rendement dans les cartes de formules baisse à 7,5% ou moins, échouant dans la généralisation des règles apprises. De plus, on observe une croissance non linéaire du rendement avec le nombre de nombres et la fréquente destruction de l'interchangeabilité (1,700 ou plus de cas de A+B≠B+A). La claire présentation de la règle de somme provoque un déclin moyen de 81,2% du rendement, tandis que les explications automatiques maintiennent la précision du modèle, montrant que le traitement des calculs des LLMs ne se conforme pas aux règles humaines. Notre résultat est que les LLMs actuels dépendent de patrons de mémoire pour apprendre réellement les règles, révélant des limitations structurelles et la nécessité de nouvelles approches.",
      "upvotes": 0,
      "discussionId": "67fcc9aa80568c7ef6180e24"
    },
    "publishedAt": "2025-04-07T12:57:10.000Z",
    "title": "Do PhD-level LLMs Truly Grasp Elementary Addition? Probing Rule Learning\n  vs. Memorization in Large Language Models",
    "summary": "Despite high benchmark scores, Large Language Models (LLMs) often fail simple\nproblem, raising a critical question: Do LLMs learn mathematical principles or\nmerely memorize patterns? Rather than designing increasingly complex benchmarks\nlike recent works, we investigate this using elementary two-integer addition\n(0 to 2^{64}), probing two core properties: commutativity (A+B=B+A) and\ncompositional generalization (via isomorphic symbolic mappings, e.g., 7\nrightarrow y). While state-of-the-art LLMs achieve 73.8-99.8\\% accuracy on\nnumerical addition, performance collapses to leq7.5\\% under symbolic\nmapping, indicating failure to generalize learned rules. Non-monotonic\nperformance scaling with digit count and frequent commutativity violations\n(over 1,700 cases of A+B neq B+A) further support this. Explicitly providing\naddition rules degrades performance by 81.2\\% on average, while\nself-explanation maintains baseline accuracy, suggesting LLM arithmetic\nprocessing is misaligned with human-defined principles. Our findings indicate\ncurrent LLMs rely on memory pattern over genuine rule learning, highlighting\narchitectural limitations and the need for new approaches to achieve true\nmathematical reasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.05262.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62ce6dd785cfd21c04c7e6f5",
      "avatarUrl": "/avatars/89837a5dea6e2d753d59caad142bed4a.svg",
      "fullname": "ZhenzhongLan",
      "name": "DannyLan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  }
]