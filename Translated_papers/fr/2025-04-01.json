[
  {
    "paper": {
      "id": "2503.23307",
      "authors": [
        {
          "_id": "67eb4bd0eca57c4eebbb343a",
          "user": {
            "_id": "64f8e358766ff9f3d2b0de84",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f8e358766ff9f3d2b0de84/R2P1YG-mRBh7TU9wkjGGk.jpeg",
            "isPro": true,
            "fullname": "Cong Wei",
            "user": "lim142857",
            "type": "user"
          },
          "name": "Cong Wei",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-01T07:47:21.554Z",
          "hidden": false
        },
        {
          "_id": "67eb4bd0eca57c4eebbb343b",
          "name": "Bo Sun",
          "hidden": false
        },
        {
          "_id": "67eb4bd0eca57c4eebbb343c",
          "user": {
            "_id": "650a8979c19e5b4c8a6ff062",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/650a8979c19e5b4c8a6ff062/64_JuECX_k_-uK7m7nlua.jpeg",
            "isPro": false,
            "fullname": "Haoyu Ma",
            "user": "haoyum1997",
            "type": "user"
          },
          "name": "Haoyu Ma",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T07:54:47.847Z",
          "hidden": false
        },
        {
          "_id": "67eb4bd0eca57c4eebbb343d",
          "name": "Ji Hou",
          "hidden": false
        },
        {
          "_id": "67eb4bd0eca57c4eebbb343e",
          "user": {
            "_id": "6444e8911cfc9ae6bb3ad216",
            "avatarUrl": "/avatars/8c06e064cf24789e4131f7af06dac86b.svg",
            "isPro": false,
            "fullname": "Xu",
            "user": "FelixXu",
            "type": "user"
          },
          "name": "Felix Juefei-Xu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T07:55:03.326Z",
          "hidden": false
        },
        {
          "_id": "67eb4bd0eca57c4eebbb343f",
          "name": "Zecheng He",
          "hidden": false
        },
        {
          "_id": "67eb4bd0eca57c4eebbb3440",
          "user": {
            "_id": "6549417b3ce45eb764faf993",
            "avatarUrl": "/avatars/d310f475d0697f5f13b3d4141ea0ccaf.svg",
            "isPro": false,
            "fullname": "Xiaoliang Dai",
            "user": "daixl1992",
            "type": "user"
          },
          "name": "Xiaoliang Dai",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T07:54:01.490Z",
          "hidden": false
        },
        {
          "_id": "67eb4bd0eca57c4eebbb3441",
          "user": {
            "_id": "65a4fa7d2548c41ad9d9b710",
            "avatarUrl": "/avatars/3cace2d2f11f7194d8eca4b95b0b57cc.svg",
            "isPro": false,
            "fullname": "Luxin Zhang",
            "user": "Luczzz",
            "type": "user"
          },
          "name": "Luxin Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T07:53:55.152Z",
          "hidden": false
        },
        {
          "_id": "67eb4bd0eca57c4eebbb3442",
          "name": "Kunpeng Li",
          "hidden": false
        },
        {
          "_id": "67eb4bd0eca57c4eebbb3443",
          "user": {
            "_id": "655846d7ed8df83128f5826a",
            "avatarUrl": "/avatars/d7ce174d7d1b8614d5f6f071225c0057.svg",
            "isPro": false,
            "fullname": "Hou",
            "user": "Tingbo",
            "type": "user"
          },
          "name": "Tingbo Hou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T07:53:10.214Z",
          "hidden": false
        },
        {
          "_id": "67eb4bd0eca57c4eebbb3444",
          "name": "Animesh Sinha",
          "hidden": false
        },
        {
          "_id": "67eb4bd0eca57c4eebbb3445",
          "name": "Peter Vajda",
          "hidden": false
        },
        {
          "_id": "67eb4bd0eca57c4eebbb3446",
          "user": {
            "_id": "6313a86154e6e5d9f0f94e04",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
            "isPro": false,
            "fullname": "Wenhu Chen",
            "user": "wenhu",
            "type": "user"
          },
          "name": "Wenhu Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T07:52:50.248Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-30T04:22:09.000Z",
      "submittedOnDailyAt": "2025-04-01T00:46:45.446Z",
      "title": "Mojirad : Défi de la synthèse de personnages dialogués au niveau de la cinématographie",
      "submittedOnDailyBy": {
        "_id": "64f8e358766ff9f3d2b0de84",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f8e358766ff9f3d2b0de84/R2P1YG-mRBh7TU9wkjGGk.jpeg",
        "isPro": true,
        "fullname": "Cong Wei",
        "user": "lim142857",
        "type": "user"
      },
      "summary": "Le progrès récent dans la génération d'images a atteint des avancées impressionnantes, mais il ne se concentre pas sur les modes de transmission de histoires centrées sur des personnages. Cela constitue un problème important pour la génération automatique de films et d'animations. Nous présentons 'Talking Characters', un travail plus réaliste qui génère des animations interactifs de personnages à travers la voix directe et des phrases. Au contraire de Talking Head, Talking Characters se concentre sur la génération d'images complètes de personnages. Dans cet article, nous proposons MoCha, le premier modèle qui génère des animations interactifs de personnages. Pour garantir la sinchronisation précise des images et des sons, nous proposons une structure de fenêtre d'attention visuo-sonore efficace pour la correspondance entre tokens de son et tokens d'image. Pour aborder la rareté de grands ensembles de données d'images avec des étiquettes de son, nous proposons une stratégie d'apprentissage bidirectionnelle qui utilise à la fois des images avec des étiquettes de son et des étiquettes de phrases. Cela améliore significativement la généralisation pour différentes actions de personnages. De plus, nous avons conçu un template de prompt structuré avec des étiquettes de personnages, ce qui permet à l'IA d'interagir dans des contextes cinématographiques et d'adapter à divers personnages. L'évaluation très détaillée, tant qualitative que quantitative, comprend des études de préférences humaines et des comparaisons de benchmark. Cela établit de nouveaux standards dans la transmission de histoires cinématographiques générées par l'IA, atteignant un niveau élevé de réalisme, expressivité, contrôle et généralisation.",
      "upvotes": 31,
      "discussionId": "67eb4bd3eca57c4eebbb34c7",
      "projectPage": "https://congwei1230.github.io/MoCha/",
      "ai_keywords": [
        "speech-video window attention mechanism",
        "speech-labeled video datasets",
        "text-labeled video data",
        "structured prompt templates",
        "character tags",
        "multi-character conversation",
        "turn-based dialogue",
        "context-aware conversations",
        "cinematic coherence"
      ]
    },
    "publishedAt": "2025-03-30T00:22:09.000Z",
    "title": "MoCha: Towards Movie-Grade Talking Character Synthesis",
    "summary": "Recent advancements in video generation have achieved impressive motion\nrealism, yet they often overlook character-driven storytelling, a crucial task\nfor automated film, animation generation. We introduce Talking Characters, a\nmore realistic task to generate talking character animations directly from\nspeech and text. Unlike talking head, Talking Characters aims at generating the\nfull portrait of one or more characters beyond the facial region. In this\npaper, we propose MoCha, the first of its kind to generate talking characters.\nTo ensure precise synchronization between video and speech, we propose a\nspeech-video window attention mechanism that effectively aligns speech and\nvideo tokens. To address the scarcity of large-scale speech-labeled video\ndatasets, we introduce a joint training strategy that leverages both\nspeech-labeled and text-labeled video data, significantly improving\ngeneralization across diverse character actions. We also design structured\nprompt templates with character tags, enabling, for the first time,\nmulti-character conversation with turn-based dialogue-allowing AI-generated\ncharacters to engage in context-aware conversations with cinematic coherence.\nExtensive qualitative and quantitative evaluations, including human preference\nstudies and benchmark comparisons, demonstrate that MoCha sets a new standard\nfor AI-generated cinematic storytelling, achieving superior realism,\nexpressiveness, controllability and generalization.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.23307.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64f8e358766ff9f3d2b0de84",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f8e358766ff9f3d2b0de84/R2P1YG-mRBh7TU9wkjGGk.jpeg",
      "fullname": "Cong Wei",
      "name": "lim142857",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.23461",
      "authors": [
        {
          "_id": "67eb594988a08fae617242f1",
          "name": "Nikai Du",
          "hidden": false
        },
        {
          "_id": "67eb594988a08fae617242f2",
          "user": {
            "_id": "66449e619ff401732687f013",
            "avatarUrl": "/avatars/251897d1324a70a9bf761513871c5841.svg",
            "isPro": false,
            "fullname": "chen",
            "user": "zhen-nan",
            "type": "user"
          },
          "name": "Zhennan Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-01T07:46:46.364Z",
          "hidden": false
        },
        {
          "_id": "67eb594988a08fae617242f3",
          "user": {
            "_id": "637c22183d8e2e9c40c09fcf",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669079538761-noauth.jpeg",
            "isPro": false,
            "fullname": "Zhizhou Chen",
            "user": "Chenzzzzzz",
            "type": "user"
          },
          "name": "Zhizhou Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T07:57:44.605Z",
          "hidden": false
        },
        {
          "_id": "67eb594988a08fae617242f4",
          "name": "Shan Gao",
          "hidden": false
        },
        {
          "_id": "67eb594988a08fae617242f5",
          "name": "Xi Chen",
          "hidden": false
        },
        {
          "_id": "67eb594988a08fae617242f6",
          "user": {
            "_id": "67593dd0f522f4409e614ba0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67593dd0f522f4409e614ba0/cvb9w_8seu3Kbjg_XAnNj.jpeg",
            "isPro": false,
            "fullname": "Jiang Zhengkai",
            "user": "jzzzzk",
            "type": "user"
          },
          "name": "Zhengkai Jiang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T07:58:12.614Z",
          "hidden": false
        },
        {
          "_id": "67eb594988a08fae617242f7",
          "name": "Jian Yang",
          "hidden": false
        },
        {
          "_id": "67eb594988a08fae617242f8",
          "user": {
            "_id": "65734004769f3ee9bde1af10",
            "avatarUrl": "/avatars/d6310ed861972fd691687d8f47413f33.svg",
            "isPro": false,
            "fullname": "Ying Tai",
            "user": "yingtai",
            "type": "user"
          },
          "name": "Ying Tai",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-01T07:46:44.350Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-30T14:36:55.000Z",
      "submittedOnDailyAt": "2025-04-01T01:44:26.275Z",
      "title": "Texte Classifieur : Classification de motifs visuels complexes pour grapher des textes avec précision",
      "submittedOnDailyBy": {
        "_id": "66449e619ff401732687f013",
        "avatarUrl": "/avatars/251897d1324a70a9bf761513871c5841.svg",
        "isPro": false,
        "fullname": "chen",
        "user": "zhen-nan",
        "type": "user"
      },
      "summary": "Dans cet article, nous examinons le défi de la génération visuelle de texte complexe (CVTG). Cette discipline se concentre sur la génération de contenus de texte complexes répartis dans différentes zones d'une image visuelle. Dans la CVTG, le modèle de génération d'images peut créer des textes visuels distorsionnés ou omettre des parties du texte visuel. Pour résoudre ces problèmes, nous proposons une nouvelle technique de graphisme multidimensionnel de texte visuel appelée \"TextCrafter\". TextCrafter décompose les textes visuels complexes par stéréographie progressive et assure une forte correspondance entre le contenu du texte et la mesure visuelle. De plus, il introduit une fonction de focalisation de tokens pour renforcer l'importance du texte visuel. TextCrafter résout efficacement les principales difficultés telles que la confusion du texte, l'omission et le retrait dans la CVTG. De plus, nous présentons un nouveau jeu de données de référence \"CVTG-2K\" pour évaluer rigoureusement le rendement des modèles de génération dans la CVTG. Les expériences étendues montrent que notre approche dépasse les plus avancées dans le domaine.",
      "upvotes": 26,
      "discussionId": "67eb594b88a08fae617243ac",
      "projectPage": "https://dnknju.github.io/textcrafter-vue/",
      "githubRepo": "https://github.com/NJU-PCALab/TextCrafter",
      "ai_keywords": [
        "complex visual text",
        "TextCrafter",
        "multi-visual text rendering",
        "progressive strategy",
        "token focus enhancement",
        "CVTG-2K",
        "generative models",
        "CVTG tasks",
        "state-of-the-art approaches"
      ]
    },
    "publishedAt": "2025-03-30T10:36:55.000Z",
    "title": "TextCrafter: Accurately Rendering Multiple Texts in Complex Visual\n  Scenes",
    "summary": "This paper explores the task of Complex Visual Text Generation (CVTG), which\ncenters on generating intricate textual content distributed across diverse\nregions within visual images. In CVTG, image generation models often rendering\ndistorted and blurred visual text or missing some visual text. To tackle these\nchallenges, we propose TextCrafter, a novel multi-visual text rendering method.\nTextCrafter employs a progressive strategy to decompose complex visual text\ninto distinct components while ensuring robust alignment between textual\ncontent and its visual carrier. Additionally, it incorporates a token focus\nenhancement mechanism to amplify the prominence of visual text during the\ngeneration process. TextCrafter effectively addresses key challenges in CVTG\ntasks, such as text confusion, omissions, and blurriness. Moreover, we present\na new benchmark dataset, CVTG-2K, tailored to rigorously evaluate the\nperformance of generative models on CVTG tasks. Extensive experiments\ndemonstrate that our method surpasses state-of-the-art approaches.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.23461.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66449e619ff401732687f013",
      "avatarUrl": "/avatars/251897d1324a70a9bf761513871c5841.svg",
      "fullname": "chen",
      "name": "zhen-nan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.24235",
      "authors": [
        {
          "_id": "67eb57023475e7b135788500",
          "user": {
            "_id": "62a42f22c683d02f5b63320c",
            "avatarUrl": "/avatars/bc611abe9c4ef8d378123cb8ac9fdbf2.svg",
            "isPro": false,
            "fullname": "Qiyuan Zhang",
            "user": "DonJoey",
            "type": "user"
          },
          "name": "Qiyuan Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T07:57:25.339Z",
          "hidden": false
        },
        {
          "_id": "67eb57023475e7b135788501",
          "user": {
            "_id": "65d2bb5c6130ef7be012d235",
            "avatarUrl": "/avatars/1c1e3bbb2c683a5c9d1f792a2c13fc4a.svg",
            "isPro": false,
            "fullname": "Fuyuan Lyu",
            "user": "silentspring2",
            "type": "user"
          },
          "name": "Fuyuan Lyu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T07:56:19.295Z",
          "hidden": false
        },
        {
          "_id": "67eb57023475e7b135788502",
          "user": {
            "_id": "65d1b42f3da87ce21e33261a",
            "avatarUrl": "/avatars/041cb441fa3871acde4ba565632056bf.svg",
            "isPro": false,
            "fullname": "RubinSun",
            "user": "RubinSun",
            "type": "user"
          },
          "name": "Zexu Sun",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-01T07:46:51.229Z",
          "hidden": false
        },
        {
          "_id": "67eb57023475e7b135788503",
          "user": {
            "_id": "646def60df618b303b419323",
            "avatarUrl": "/avatars/97aa761d5255abf230304cfeade87835.svg",
            "isPro": false,
            "fullname": "Lei Wang",
            "user": "demolei",
            "type": "user"
          },
          "name": "Lei Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-01T08:01:21.676Z",
          "hidden": false
        },
        {
          "_id": "67eb57023475e7b135788504",
          "user": {
            "_id": "63414659c5565a4b8d41bc42",
            "avatarUrl": "/avatars/25b4ca3002edf4c35cded0902c26632a.svg",
            "isPro": false,
            "fullname": "Weixu Zhang",
            "user": "nancy-zwx",
            "type": "user"
          },
          "name": "Weixu Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T07:56:26.532Z",
          "hidden": false
        },
        {
          "_id": "67eb57023475e7b135788505",
          "user": {
            "_id": "63c0c2497f52541dfc7d7567",
            "avatarUrl": "/avatars/16c174e2803ef86d09815b36a666ee0e.svg",
            "isPro": false,
            "fullname": "ZhihanGUO",
            "user": "ZhihanGUO",
            "type": "user"
          },
          "name": "Zhihan Guo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T07:56:31.979Z",
          "hidden": false
        },
        {
          "_id": "67eb57023475e7b135788506",
          "name": "Yufei Wang",
          "hidden": false
        },
        {
          "_id": "67eb57023475e7b135788507",
          "name": "Irwin King",
          "hidden": false
        },
        {
          "_id": "67eb57023475e7b135788508",
          "name": "Xue Liu",
          "hidden": false
        },
        {
          "_id": "67eb57023475e7b135788509",
          "name": "Chen Ma",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-31T15:46:15.000Z",
      "submittedOnDailyAt": "2025-04-01T01:37:27.268Z",
      "title": "Comment, comment, où, combien améliorent-ils ? Recherche sur l'échelle des tests de modèles de langage de grande taille",
      "submittedOnDailyBy": {
        "_id": "62a42f22c683d02f5b63320c",
        "avatarUrl": "/avatars/bc611abe9c4ef8d378123cb8ac9fdbf2.svg",
        "isPro": false,
        "fullname": "Qiyuan Zhang",
        "user": "DonJoey",
        "type": "user"
      },
      "summary": "La scalabilité dans les entreprises inclut la scalabilité des données et des paramètres lors de la phase préparatoire, et la scalabilité en temps de test (TTS) a été concentrée sur \"calcul en temps de test\" dans des recherches rigoureuses. Des études récentes ont montré que la TTS affecte la capacité de résolution de problèmes dans des modèles de langage à grande échelle (LLMs), non seulement dans des tâches de logique spécialisée comme les mathématiques ou la programmation, mais également dans des tâches générales comme la réponse à des questions ouvertes. Cependant, avec l'essor rapide des efforts récents dans ce domaine, une recherche intégrée qui fournit un compréhension systématique est urgente. Pour cela, nous proposons un cadre intégré et varié basé sur quatre aspects clés : \"Qu'est-ce que nous allons écheller ?\", \"Comment nous allons écheller ?\", \"Où nous allons écheller ?\", \"Comment nous allons écheller de manière efficace ?\". Avec ce cadre, nous effectuons une revue large sur les méthodes, les scénarios d'application et l'évaluation, analysant et présentant de manière systématique les fonctions spécifiques de chaque technologie sur différentes échelles de TTS. Dans cette analyse, nous extrayons les principaux développements historiques de TTS et nous proposons des guides pour sa mise en pratique. De plus, nous dévoilons plusieurs problèmes publics et projetons les possibilités de l'avenir, proposant également l'extension à des échelles plus grandes, la clarification de la réalité fonctionnelle de la technologie, la généralisation à de plusieurs tâches et l'explication de son apport.",
      "upvotes": 24,
      "discussionId": "67eb57053475e7b135788624",
      "ai_keywords": [
        "test-time scaling",
        "test-time computing",
        "large language models",
        "specialized reasoning tasks",
        "open-ended Q&A",
        "multidimensional framework",
        "what to scale",
        "how to scale",
        "where to scale",
        "how well to scale",
        "assessment aspects",
        "functional roles",
        "developmental trajectories",
        "practical deployment",
        "open challenges",
        "attributions"
      ]
    },
    "publishedAt": "2025-03-31T11:46:15.000Z",
    "title": "What, How, Where, and How Well? A Survey on Test-Time Scaling in Large\n  Language Models",
    "summary": "As enthusiasm for scaling computation (data and parameters) in the\npretraining era gradually diminished, test-time scaling (TTS), also referred to\nas ``test-time computing'' has emerged as a prominent research focus. Recent\nstudies demonstrate that TTS can further elicit the problem-solving\ncapabilities of large language models (LLMs), enabling significant\nbreakthroughs not only in specialized reasoning tasks, such as mathematics and\ncoding, but also in general tasks like open-ended Q&A. However, despite the\nexplosion of recent efforts in this area, there remains an urgent need for a\ncomprehensive survey offering a systemic understanding. To fill this gap, we\npropose a unified, multidimensional framework structured along four core\ndimensions of TTS research: what to scale, how to scale, where to scale, and\nhow well to scale. Building upon this taxonomy, we conduct an extensive review\nof methods, application scenarios, and assessment aspects, and present an\norganized decomposition that highlights the unique functional roles of\nindividual techniques within the broader TTS landscape. From this analysis, we\ndistill the major developmental trajectories of TTS to date and offer hands-on\nguidelines for practical deployment. Furthermore, we identify several open\nchallenges and offer insights into promising future directions, including\nfurther scaling, clarifying the functional essence of techniques, generalizing\nto more tasks, and more attributions.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.24235.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62a42f22c683d02f5b63320c",
      "avatarUrl": "/avatars/bc611abe9c4ef8d378123cb8ac9fdbf2.svg",
      "fullname": "Qiyuan Zhang",
      "name": "DonJoey",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.24388",
      "authors": [
        {
          "_id": "67eb544113ca8dcb9ccb991b",
          "name": "Zhonghan Zhao",
          "hidden": false
        },
        {
          "_id": "67eb544113ca8dcb9ccb991c",
          "user": {
            "_id": "64e8505321540e1da3226b54",
            "avatarUrl": "/avatars/18958b8406d1ce492b54c1c839f18c54.svg",
            "isPro": false,
            "fullname": "Wenwei Zhang",
            "user": "ZwwWayne",
            "type": "user"
          },
          "name": "Wenwei Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T07:59:04.743Z",
          "hidden": false
        },
        {
          "_id": "67eb544113ca8dcb9ccb991d",
          "name": "Haian Huang",
          "hidden": false
        },
        {
          "_id": "67eb544113ca8dcb9ccb991e",
          "name": "Kuikun Liu",
          "hidden": false
        },
        {
          "_id": "67eb544113ca8dcb9ccb991f",
          "user": {
            "_id": "64070c5c4dc5f2846c925e93",
            "avatarUrl": "/avatars/ac2d7c1cd4ecccd6a88b85767c963ec7.svg",
            "isPro": false,
            "fullname": "Gao Jianfei",
            "user": "pppppM",
            "type": "user"
          },
          "name": "Jianfei Gao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T07:59:34.182Z",
          "hidden": false
        },
        {
          "_id": "67eb544113ca8dcb9ccb9920",
          "user": {
            "_id": "64c9033c5381684d3eaac7f1",
            "avatarUrl": "/avatars/07d36ca193826044b0df04e3602b9ef8.svg",
            "isPro": false,
            "fullname": "Gaoang Wang",
            "user": "GaoangWang",
            "type": "user"
          },
          "name": "Gaoang Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T07:59:40.156Z",
          "hidden": false
        },
        {
          "_id": "67eb544113ca8dcb9ccb9921",
          "name": "Kai Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-31T17:59:52.000Z",
      "submittedOnDailyAt": "2025-04-01T01:27:12.837Z",
      "title": "De l'extrême du monde à l'extrême du monde, raisons communes des politiques et harmonie de l'imagination",
      "submittedOnDailyBy": {
        "_id": "6601196cc91ba4c08ad6e270",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6601196cc91ba4c08ad6e270/X2YPNzUOQXBz5Gv-xR9LW.jpeg",
        "isPro": false,
        "fullname": "yuzhe gu",
        "user": "vanilla1116",
        "type": "user"
      },
      "summary": "Inférence et imagination avant de l'action sont essentielles pour un agent corporé qui opère dans un environnement complexe et ouvert. Cependant, les études antérieures ont limité l'intégration de ces deux compétences dans l'agent, soit en les intégrant de manière unilatérale, soit en combinant plusieurs modèles spécialisés dans l'apprentissage de politiques, ce qui a limité l'efficacité et la généralisation. Par conséquent, cet article propose, pour la première fois, l'intégration de l'inférence et de l'imagination dans une politique générale appelée RIG (Reasoning and Imagination Generalist). Pour entraîner RIG de manière intégrée, un pipeline de données a été construit pour intégrer et enrichir progressivement les contenus d'imagination et d'inférence collectés dans les trajectoires des agents existants. L'apprentissage conjoint de l'inférence et de la génération d'images futures a amélioré l'efficacité des échantillons et la généralisation de 17 fois plus que les études précédentes, modélisant clairement la corrélation implicite entre l'inférence, l'action et la dynamique de l'environnement. Dans la phase d'inférence, RIG infère l'action suivante, génère des actions potentielles et, plus tard, prédit les résultats des actions, fournissant l'agent la possibilité de réviser et de corriger ses actions avant qu'elles soient exécutées, en se basant sur l'imagination. Les résultats des expériences montrent que l'harmonie entre l'inférence et l'imagination améliore la robustesse, la généralisation et l'interaction de la politique générale, et permet d'améliorer le rendement en temps de test grâce à l'extension du temps de test.",
      "upvotes": 20,
      "discussionId": "67eb544213ca8dcb9ccb9963"
    },
    "publishedAt": "2025-03-31T13:59:52.000Z",
    "title": "RIG: Synergizing Reasoning and Imagination in End-to-End Generalist\n  Policy",
    "summary": "Reasoning before action and imagining potential outcomes (i.e., world models)\nare essential for embodied agents operating in complex open-world environments.\nYet, prior work either incorporates only one of these abilities in an\nend-to-end agent or integrates multiple specialized models into an agent\nsystem, limiting the learning efficiency and generalization of the policy.\nThus, this paper makes the first attempt to synergize Reasoning and Imagination\nin an end-to-end Generalist policy, termed RIG. To train RIG in an end-to-end\nmanner, we construct a data pipeline that progressively integrates and enriches\nthe content of imagination and reasoning in the trajectories collected from\nexisting agents. The joint learning of reasoning and next image generation\nexplicitly models the inherent correlation between reasoning, action, and\ndynamics of environments, and thus exhibits more than 17times sample\nefficiency improvements and generalization in comparison with previous works.\nDuring inference, RIG first reasons about the next action, produces potential\naction, and then predicts the action outcomes, which offers the agent a chance\nto review and self-correct based on the imagination before taking real actions.\nExperimental results show that the synergy of reasoning and imagination not\nonly improves the robustness, generalization, and interoperability of\ngeneralist policy but also enables test-time scaling to enhance overall\nperformance.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.24388.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6601196cc91ba4c08ad6e270",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6601196cc91ba4c08ad6e270/X2YPNzUOQXBz5Gv-xR9LW.jpeg",
      "fullname": "yuzhe gu",
      "name": "vanilla1116",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.24370",
      "authors": [
        {
          "_id": "67eb4fff13ca8dcb9cca5f9b",
          "user": {
            "_id": "62fae9328e137d7c4b896498",
            "avatarUrl": "/avatars/1bda39dec585c099417cc9daa9f53c42.svg",
            "isPro": false,
            "fullname": "Tong Wu",
            "user": "tongwu2020",
            "type": "user"
          },
          "name": "Tong Wu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-01T02:31:28.689Z",
          "hidden": false
        },
        {
          "_id": "67eb4fff13ca8dcb9cca5f9c",
          "user": {
            "_id": "653319c135ad8b9e58a6b874",
            "avatarUrl": "/avatars/755efb5829f3b6d3cef886fee26e1ba9.svg",
            "isPro": false,
            "fullname": "Chong Xiang",
            "user": "cxiang",
            "type": "user"
          },
          "name": "Chong Xiang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:00:19.547Z",
          "hidden": false
        },
        {
          "_id": "67eb4fff13ca8dcb9cca5f9d",
          "name": "Jiachen T. Wang",
          "hidden": false
        },
        {
          "_id": "67eb4fff13ca8dcb9cca5f9e",
          "name": "Prateek Mittal",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-31T17:50:13.000Z",
      "submittedOnDailyAt": "2025-04-01T01:02:34.304Z",
      "title": "Contrôle des modèles d'inférence efficaces par une approche conceptuelle",
      "submittedOnDailyBy": {
        "_id": "62fae9328e137d7c4b896498",
        "avatarUrl": "/avatars/1bda39dec585c099417cc9daa9f53c42.svg",
        "isPro": false,
        "fullname": "Tong Wu",
        "user": "tongwu2020",
        "type": "user"
      },
      "summary": "Les théories assignées aux grands modèles de langue (LLMs) permettent d'améliorer leur performance en générant une séquence logique théorique explicite avant de produire la réponse finale, résolvant ainsi efficacement des problèmes complexes. Dans cet article, nous montrons que ce nouveau cadre de génération offre une opportunité spéciale pour contrôler plus précisément le comportement du modèle. Nous proposons un nouveau paradigme appelé « Intervention dans le Pensée », qui consiste à insérer ou modifier des tokens de pensée de manière stratégique pour expliciter et guider le processus logique interne des LLMs. Des évaluations détaillées ont été effectuées sur des tâches comme IFEval, la structure directrice de SEP, la cohérence de XSTest et la sécurité de SORRY-Bench. Il en résulte que l'Intervention dans le Pensée dépasse significativement l'approche traditionnelle des prompts, améliorant la précision de 6,7% en utilisant le modèle DeepSeek R1 ouvert, améliorant la logique théorique de 15,4% et augmentant de 40,0% le rejet de prompts insecurisés. Notre étude ouvre des perspectives pour explorer de nouvelles recherches sur le contrôle des logiques théoriques dans les LLMs.",
      "upvotes": 11,
      "discussionId": "67eb500013ca8dcb9cca5fe0",
      "ai_keywords": [
        "Reasoning-enhanced large language models (LLMs)",
        "intermediate reasoning steps",
        "Thinking Intervention",
        "thinking tokens",
        "instruction following",
        "IFEval",
        "instruction hierarchy",
        "SEP",
        "safety alignment",
        "XSTest",
        "SORRY-Bench",
        "open-source DeepSeek R1 models"
      ]
    },
    "publishedAt": "2025-03-31T13:50:13.000Z",
    "title": "Effectively Controlling Reasoning Models through Thinking Intervention",
    "summary": "Reasoning-enhanced large language models (LLMs) explicitly generate\nintermediate reasoning steps prior to generating final answers, helping the\nmodel excel in complex problem-solving. In this paper, we demonstrate that this\nemerging generation framework offers a unique opportunity for more fine-grained\ncontrol over model behavior. We propose Thinking Intervention, a novel paradigm\ndesigned to explicitly guide the internal reasoning processes of LLMs by\nstrategically inserting or revising specific thinking tokens. We conduct\ncomprehensive evaluations across multiple tasks, including instruction\nfollowing on IFEval, instruction hierarchy on SEP, and safety alignment on\nXSTest and SORRY-Bench. Our results demonstrate that Thinking Intervention\nsignificantly outperforms baseline prompting approaches, achieving up to 6.7%\naccuracy gains in instruction-following scenarios, 15.4% improvements in\nreasoning about instruction hierarchies, and a 40.0% increase in refusal rates\nfor unsafe prompts using open-source DeepSeek R1 models. Overall, our work\nopens a promising new research avenue for controlling reasoning LLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.24370.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62fae9328e137d7c4b896498",
      "avatarUrl": "/avatars/1bda39dec585c099417cc9daa9f53c42.svg",
      "fullname": "Tong Wu",
      "name": "tongwu2020",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.24364",
      "authors": [
        {
          "_id": "67eb6e6088a08fae617860f3",
          "user": {
            "_id": "600b381d3cc3b87db94bc0ce",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/600b381d3cc3b87db94bc0ce/I3xpr4gzcG1uXawXBpWpD.jpeg",
            "isPro": false,
            "fullname": "Łukasz Borchmann",
            "user": "Borchmann",
            "type": "user"
          },
          "name": "Łukasz Borchmann",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-04-01T05:01:23.216Z",
          "hidden": false
        },
        {
          "_id": "67eb6e6088a08fae617860f4",
          "user": {
            "_id": "66c5e93e8f14c260be9d9f63",
            "avatarUrl": "/avatars/f4bf15e23923ef3256d3f01a3278d8bc.svg",
            "isPro": false,
            "fullname": "Marek Wydmuch",
            "user": "sfc-mwydmuch",
            "type": "user"
          },
          "name": "Marek Wydmuch",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:01:57.075Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/600b381d3cc3b87db94bc0ce/ucxS0pF0YXK8TDRhoezTE.qt"
      ],
      "publishedAt": "2025-03-31T17:43:36.000Z",
      "submittedOnDailyAt": "2025-04-01T03:14:34.239Z",
      "title": "Query and Conquer: Execution-Guided SQL Generation  \nConsulta y Vencer: Génération de SQL orientée à l'exécution",
      "submittedOnDailyBy": {
        "_id": "600b381d3cc3b87db94bc0ce",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/600b381d3cc3b87db94bc0ce/I3xpr4gzcG1uXawXBpWpD.jpeg",
        "isPro": false,
        "fullname": "Łukasz Borchmann",
        "user": "Borchmann",
        "type": "user"
      },
      "summary": "Nous proposons un nouvel approche pour augmenter significativement la précision des tâches SQL à partir de texte. Notre méthode sélectionne plusieurs candidats pour choisir la requête qui est la plus grammaticalement adaptée aux résultats d'exécution, permettant ainsi aux modèles petits et coûteux d'atteindre des performances supérieures à ceux des méthodes d'inférence à coût élevé (comme o1, o3-mini, DeepSeek R1). De plus, nous réduisons les coûts d'inférence d'un tiers. Cette approche s'intègre facilement avec les modèles existants et fournit une clé pour la création de SQL pratique et scalable.",
      "upvotes": 10,
      "discussionId": "67eb6e6188a08fae6178613f",
      "ai_keywords": [
        "text-to-SQL",
        "execution results",
        "semantically consistent",
        "query",
        "candidates",
        "models",
        "reasoning methods",
        "o1",
        "o3-mini",
        "DeepSeek R1",
        "inference cost",
        "SQL generation"
      ]
    },
    "publishedAt": "2025-03-31T13:43:36.000Z",
    "title": "Query and Conquer: Execution-Guided SQL Generation",
    "summary": "We propose a novel approach for generating complex outputs that significantly\nimproves accuracy in text-to-SQL tasks. Our method leverages execution results\nto select the most semantically consistent query from multiple candidates,\nenabling smaller, cost-effective models to surpass computationally intensive\nreasoning methods such as o1, o3-mini, and DeepSeek R1 while reducing inference\ncost by as much as 30 times. It integrates effortlessly with existing models,\noffering a practical and scalable pathway to state-of-the-art SQL generation.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/600b381d3cc3b87db94bc0ce/ucxS0pF0YXK8TDRhoezTE.qt"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.24364.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "600b381d3cc3b87db94bc0ce",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/600b381d3cc3b87db94bc0ce/I3xpr4gzcG1uXawXBpWpD.jpeg",
      "fullname": "Łukasz Borchmann",
      "name": "Borchmann",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.23284",
      "authors": [
        {
          "_id": "67eb5280aeab4ce97de07134",
          "user": {
            "_id": "6424538b9f9e65b42389920e",
            "avatarUrl": "/avatars/9b912e2af9eebe9a481181f006765059.svg",
            "isPro": false,
            "fullname": "Feng-Lin Liu",
            "user": "Okrin",
            "type": "user"
          },
          "name": "Feng-Lin Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-01T07:47:05.907Z",
          "hidden": false
        },
        {
          "_id": "67eb5280aeab4ce97de07135",
          "user": {
            "_id": "662cd8b9322afcbae53fb06e",
            "avatarUrl": "/avatars/9847f5c2282d49e61e76a0a303e0b2b1.svg",
            "isPro": false,
            "fullname": "fuhongbo",
            "user": "fuhongbo",
            "type": "user"
          },
          "name": "Hongbo Fu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:07:01.616Z",
          "hidden": false
        },
        {
          "_id": "67eb5280aeab4ce97de07136",
          "user": {
            "_id": "60e272ca6c78a8c122b12127",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60e272ca6c78a8c122b12127/xldEGBzGrU-bX6IwAw0Ie.jpeg",
            "isPro": false,
            "fullname": "Xintao Wang",
            "user": "Xintao",
            "type": "user"
          },
          "name": "Xintao Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:07:09.910Z",
          "hidden": false
        },
        {
          "_id": "67eb5280aeab4ce97de07137",
          "user": {
            "_id": "6360d9f0472131c3bc4f61df",
            "avatarUrl": "/avatars/c5d884e5ef19b781e3405aba6dd68ca8.svg",
            "isPro": false,
            "fullname": "WeicaiYe",
            "user": "WeicaiYe",
            "type": "user"
          },
          "name": "Weicai Ye",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:07:16.323Z",
          "hidden": false
        },
        {
          "_id": "67eb5280aeab4ce97de07138",
          "name": "Pengfei Wan",
          "hidden": false
        },
        {
          "_id": "67eb5280aeab4ce97de07139",
          "user": {
            "_id": "644c8324f02250233d0d67d9",
            "avatarUrl": "/avatars/feb39d281457c1750f3eada3c060a23e.svg",
            "isPro": false,
            "fullname": "Di Zhang",
            "user": "dizhang",
            "type": "user"
          },
          "name": "Di Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:07:30.236Z",
          "hidden": false
        },
        {
          "_id": "67eb5280aeab4ce97de0713a",
          "name": "Lin Gao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-30T02:44:09.000Z",
      "submittedOnDailyAt": "2025-04-01T02:19:10.110Z",
      "title": "Sketch Video: Génération et édition de vidéos basées sur des dessins",
      "submittedOnDailyBy": {
        "_id": "6424538b9f9e65b42389920e",
        "avatarUrl": "/avatars/9b912e2af9eebe9a481181f006765059.svg",
        "isPro": false,
        "fullname": "Feng-Lin Liu",
        "user": "Okrin",
        "type": "user"
      },
      "summary": "En la génération et l'édition de vidéos basées sur des graphiques de texte ou d'images, des progrès significatifs ont été réalisés, mais un problème persiste : contrôler avec précision la disposition globale et les détails de la généralisation uniquement avec du texte, ainsi que la réalisation de contrôle d'actions et d'éditions locales à travers des images, est une tâche complexe. Dans cet article, nous proposons de réaliser un contrôle spatial et d'actions basé sur des dessins pour la génération de vidéos, et de fournir des éditions en grand détail à des vidéos réelles ou synthétisées. En nous appuyant sur le modèle DiT pour la génération de vidéos, nous proposons une structure de contrôle efficace en mémoire qui comprend des blocs de contrôle basés sur des dessins. Les dessins sont effectués dans un ou deux cadres clés (à n'importe quel moment), facilitant une interaction simple. Pour propager ces scènes temporellement peu denses à tous les frames, nous proposons une structure d'attention entre les cadres clés et les frames de vidéo. Pour maintenir les nouveaux contenus édités dans l'édition basée sur des dessins, nous concevons un module d'insertion de vidéos. Pendant l'inférence, nous utilisons la fusion potentielle pour l'enregistrement précis du midi. Les expériences étendues montrent que nos dessins de vidéos permettent d'atteindre un rendement supérieur en génération et édition de vidéos avec contrôle.",
      "upvotes": 9,
      "discussionId": "67eb5286aeab4ce97de07320",
      "githubRepo": "https://github.com/IGLICT/SketchVideo",
      "ai_keywords": [
        "DiT video generation model",
        "memory-efficient control structure",
        "sketch control blocks",
        "residual features",
        "skipped DiT blocks",
        "temporally sparse sketch conditions",
        "inter-frame attention mechanism",
        "keyframes",
        "video insertion module",
        "spatial feature",
        "dynamic motion",
        "latent fusion",
        "SketchVideo"
      ]
    },
    "publishedAt": "2025-03-29T22:44:09.000Z",
    "title": "SketchVideo: Sketch-based Video Generation and Editing",
    "summary": "Video generation and editing conditioned on text prompts or images have\nundergone significant advancements. However, challenges remain in accurately\ncontrolling global layout and geometry details solely by texts, and supporting\nmotion control and local modification through images. In this paper, we aim to\nachieve sketch-based spatial and motion control for video generation and\nsupport fine-grained editing of real or synthetic videos. Based on the DiT\nvideo generation model, we propose a memory-efficient control structure with\nsketch control blocks that predict residual features of skipped DiT blocks.\nSketches are drawn on one or two keyframes (at arbitrary time points) for easy\ninteraction. To propagate such temporally sparse sketch conditions across all\nframes, we propose an inter-frame attention mechanism to analyze the\nrelationship between the keyframes and each video frame. For sketch-based video\nediting, we design an additional video insertion module that maintains\nconsistency between the newly edited content and the original video's spatial\nfeature and dynamic motion. During inference, we use latent fusion for the\naccurate preservation of unedited regions. Extensive experiments demonstrate\nthat our SketchVideo achieves superior performance in controllable video\ngeneration and editing.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.23284.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6424538b9f9e65b42389920e",
      "avatarUrl": "/avatars/9b912e2af9eebe9a481181f006765059.svg",
      "fullname": "Feng-Lin Liu",
      "name": "Okrin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.18809",
      "authors": [
        {
          "_id": "67eaa0f83ace6eb46745a9fe",
          "user": {
            "_id": "674f43d6df6fa102409f6d1a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/KT_-dmXWshUKvbhtn-LSs.png",
            "isPro": false,
            "fullname": "Augusto B. Corrêa",
            "user": "abcorrea",
            "type": "user"
          },
          "name": "Augusto B. Corrêa",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-31T15:15:12.565Z",
          "hidden": false
        },
        {
          "_id": "67eaa0f83ace6eb46745a9ff",
          "user": {
            "_id": "662fb9c891587703a677856e",
            "avatarUrl": "/avatars/9cb7f035a513279532fc205ce9c5902c.svg",
            "isPro": false,
            "fullname": "Andre Grahl Pereira",
            "user": "andregrahl",
            "type": "user"
          },
          "name": "André G. Pereira",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-31T15:15:13.942Z",
          "hidden": false
        },
        {
          "_id": "67eaa0f83ace6eb46745aa00",
          "user": {
            "_id": "66f3dfd4b8703dde248f6d26",
            "avatarUrl": "/avatars/c199c91d422500cc7c7556569291644d.svg",
            "isPro": false,
            "fullname": "Jendrik Seipp",
            "user": "jendrikseipp",
            "type": "user"
          },
          "name": "Jendrik Seipp",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-01T07:47:39.862Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-24T15:50:20.000Z",
      "submittedOnDailyAt": "2025-04-01T00:45:14.321Z",
      "title": "Appliquer un générateur de modèles de langue appelé LLM avec une heuristique dans un plan classique : défier l'état de l'art avec du code Python",
      "submittedOnDailyBy": {
        "_id": "674f43d6df6fa102409f6d1a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/KT_-dmXWshUKvbhtn-LSs.png",
        "isPro": false,
        "fullname": "Augusto B. Corrêa",
        "user": "abcorrea",
        "type": "user"
      },
      "summary": "Récemment, les modèles de langue de grande taille (LLMs) ont démontré des capacités impressionnantes dans diverses problématiques d'intelligence artificielle. Cependant, il n'est pas encore possible de réaliser des plans fiables malgré la présence d'une définition détaillée des plans, et les efforts pour améliorer la capacité de planification des LLMs, tels que l'enchaînement de pensée, le fine-tuning et la configuration explicite de 'raisons', souvent ne réussissent pas à générer des plans corrects ni les généraliser aux tâches plus vastes. Dans cet article, nous présentons un méthode pour générer des plans corrects dans des tâches de distribution externe plus grandes en utilisant les LLMs. Nous utilisons les LLMs pour créer des fonctions heuristiques sous forme de code Python qui dépendent du domaine spécifique, qui sont entraînées par la recherche de meilleures stratégies et qui sont sélectionnées pour leur puissance. En conséquence, les fonctions heuristiques générées par les LLMs résolvent plus de tâches non vues que les fonctions heuristiques traditionnelles du domaine indépendant et montrent une force comparable aux algorithmes d'apprentissage les plus puissants du domaine. Ces résultats sont surprenants, car les fonctions heuristiques générées par les LLMs, bien que basées sur une implémentation conceptuelle non optimisée d'un programmeur de plans en Python, dépassent les fonctions heuristiques avancées dans des tâches du domaine indépendant, et leur rendement est comparable à ceux des algorithmes d'apprentissage les plus puissants du domaine. Dans un domaine spécifique, les fonctions heuristiques générées par les LLMs étendent moins d'états que les fonctions heuristiques basées sur un domaine indépendant, et non seulement étendent-les de manière efficace, mais montrent également un niveau d'information supérieur. En résumé, nos résultats montrent que la génération de fonctions heuristiques de plans à partir de LLMs peut significativement améliorer la capacité de planification de ces modèles.",
      "upvotes": 8,
      "discussionId": "67eaa0f93ace6eb46745aa3e",
      "ai_keywords": [
        "large language models (LLMs)",
        "chain-of-thought prompting",
        "fine-tuning",
        "reasoning",
        "planning domain",
        "domain-dependent heuristic functions",
        "Python code",
        "greedy best-first search",
        "state-of-the-art domain-independent heuristics",
        "domain-dependent planning",
        "unoptimized Python planner",
        "highly optimized C++ code",
        "planning heuristic function programs"
      ]
    },
    "publishedAt": "2025-03-24T11:50:20.000Z",
    "title": "Classical Planning with LLM-Generated Heuristics: Challenging the State\n  of the Art with Python Code",
    "summary": "In recent years, large language models (LLMs) have shown remarkable\ncapabilities in various artificial intelligence problems. However, they fail to\nplan reliably, even when prompted with a detailed definition of the planning\ntask. Attempts to improve their planning capabilities, such as chain-of-thought\nprompting, fine-tuning, and explicit \"reasoning\" still yield incorrect plans\nand usually fail to generalize to larger tasks. In this paper, we show how to\nuse LLMs to generate correct plans, even for out-of-distribution tasks of\nincreasing size. For a given planning domain, we ask an LLM to generate several\ndomain-dependent heuristic functions in the form of Python code, evaluate them\non a set of training tasks within a greedy best-first search, and choose the\nstrongest one. The resulting LLM-generated heuristics solve many more unseen\ntest tasks than state-of-the-art domain-independent heuristics for classical\nplanning. They are even competitive with the strongest learning algorithm for\ndomain-dependent planning. These findings are especially remarkable given that\nour proof-of-concept implementation is based on an unoptimized Python planner\nand the baselines all build upon highly optimized C++ code. In some domains,\nthe LLM-generated heuristics expand fewer states than the baselines, revealing\nthat they are not only efficiently computable, but sometimes even more\ninformative than the state-of-the-art heuristics. Overall, our results show\nthat sampling a set of planning heuristic function programs can significantly\nimprove the planning capabilities of LLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18809.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "674f43d6df6fa102409f6d1a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/KT_-dmXWshUKvbhtn-LSs.png",
      "fullname": "Augusto B. Corrêa",
      "name": "abcorrea",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.24115",
      "authors": [
        {
          "_id": "67eb5116d3a707c0a5b02bd1",
          "user": {
            "_id": "64a0ed5ed5374ca472cfb0ac",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a0ed5ed5374ca472cfb0ac/n_wXamXfR_PPn0hRbnR1X.jpeg",
            "isPro": false,
            "fullname": "ZhimingMa",
            "user": "JimmyMa99",
            "type": "user"
          },
          "name": "Zhiming Ma",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-01T02:36:07.612Z",
          "hidden": false
        },
        {
          "_id": "67eb5116d3a707c0a5b02bd2",
          "user": {
            "_id": "6385f7b969634850f8ddd541",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669723465271-noauth.png",
            "isPro": false,
            "fullname": "Peidong Wang",
            "user": "WDong",
            "type": "user"
          },
          "name": "Peidong Wang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-01T02:36:07.612Z",
          "hidden": false
        },
        {
          "_id": "67eb5116d3a707c0a5b02bd3",
          "user": {
            "_id": "6466d57bf3e78d1d6be0505c",
            "avatarUrl": "/avatars/9659b7d0f6fa51efc127afb7a1ba14b1.svg",
            "isPro": false,
            "fullname": "HuangMinhua",
            "user": "HuangMinhua",
            "type": "user"
          },
          "name": "Minhua Huang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:02:15.224Z",
          "hidden": false
        },
        {
          "_id": "67eb5116d3a707c0a5b02bd4",
          "name": "Jingpeng Wang",
          "hidden": false
        },
        {
          "_id": "67eb5116d3a707c0a5b02bd5",
          "name": "Kai Wu",
          "hidden": false
        },
        {
          "_id": "67eb5116d3a707c0a5b02bd6",
          "name": "Xiangzhao Lv",
          "hidden": false
        },
        {
          "_id": "67eb5116d3a707c0a5b02bd7",
          "name": "Yachun Pang",
          "hidden": false
        },
        {
          "_id": "67eb5116d3a707c0a5b02bd8",
          "name": "Yin Yang",
          "hidden": false
        },
        {
          "_id": "67eb5116d3a707c0a5b02bd9",
          "name": "Wenjie Tang",
          "hidden": false
        },
        {
          "_id": "67eb5116d3a707c0a5b02bda",
          "name": "Yuchen Kang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64a0ed5ed5374ca472cfb0ac/PsAJTW9JTyrHqtjQP0lLJ.png"
      ],
      "publishedAt": "2025-03-31T14:06:17.000Z",
      "submittedOnDailyAt": "2025-04-01T01:08:09.201Z",
      "title": "TeleAntiFraud-28k : Ensemble de données par voix-texte pour prévenir les violations des appels téléphoniques",
      "submittedOnDailyBy": {
        "_id": "64a0ed5ed5374ca472cfb0ac",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a0ed5ed5374ca472cfb0ac/n_wXamXfR_PPn0hRbnR1X.jpeg",
        "isPro": false,
        "fullname": "ZhimingMa",
        "user": "JimmyMa99",
        "type": "user"
      },
      "summary": "Le sens de la fraude téléphonique se trouve dans un grand problème en raison de la rareté de données d'entraînement de haute qualité pour des modèles multimodal. Pour combler ce vide, nous présentons le premier ensemble de données audio-texte court-temps ouvert et adapté pour l'analyse automatique de fraudes téléphoniques, appelé \"TeleAntiFraud-28k\". Cet ensemble de données a été construit en suivant trois stratégies : 1) la génération de échantillons de texte protégé de la privaté en utilisant les enregistrements de voix téléphoniques (y compris des données anonymes) avec le reconnaissance automatique de la voix (ASR), assurant la cohérence avec le texte original par le modèle de texte de reconstruction ; 2) l'expansion du spectre d'échantillons en utilisant l'auto-sampling basé sur de grands modèles de langage (LLM) avec les sorties réelles de l'ASR ; 3) la simulation des techniques de fraude actuelles en utilisant un mélange d'agents. L'ensemble de données généré comprend 28 511 couples de textes de parole strictement traités, avec des notes détaillées sur les raisons de la fraude. Cet ensemble de données est divisé en trois tâches : la classification des scénarios, la détection de la fraude et la classification des types de fraude. De plus, nous avons construit un cadre d'évaluation standard appelé \"TeleAntiFraud-Bench\", qui inclut des instances échantillonnées proportionnellement, pour promouvoir la vérification systématique du rendement des modèles dans la tâche de détection de fraude téléphonique. Nous avons également fourni des modèles d'entraînement optimisés pour l'industrie basés sur des données mixtes et nous avons ouvert le code de notre cadre de traitement de données pour faciliter l'expansion de l'ensemble de données dirigée par la communauté. Cette recherche vise à aborder les problèmes importants de la privaté des données et de la diversité des scénarios, tout en établissant une base pour l'étude multimodale de l'anti-fraude. Ce projet est lancé sur GitHub à l'adresse https://github.com/JimmyMa99/TeleAntiFraud.",
      "upvotes": 7,
      "discussionId": "67eb5117d3a707c0a5b02c4c",
      "ai_keywords": [
        "automatically speech recognition (ASR)",
        "text-to-speech (TTS)",
        "large language model (LLM)",
        "self-instruction sampling",
        "multi-agent adversarial synthesis",
        "supervised fine-tuning (SFT)",
        "hybrid real/synthetic data"
      ]
    },
    "publishedAt": "2025-03-31T10:06:17.000Z",
    "title": "TeleAntiFraud-28k: A Audio-Text Slow-Thinking Dataset for Telecom Fraud\n  Detection",
    "summary": "The detection of telecom fraud faces significant challenges due to the lack\nof high-quality multimodal training data that integrates audio signals with\nreasoning-oriented textual analysis. To address this gap, we present\nTeleAntiFraud-28k, the first open-source audio-text slow-thinking dataset\nspecifically designed for automated telecom fraud analysis. Our dataset is\nconstructed through three strategies: (1) Privacy-preserved text-truth sample\ngeneration using automatically speech recognition (ASR)-transcribed call\nrecordings (with anonymized original audio), ensuring real-world consistency\nthrough text-to-speech (TTS) model regeneration; (2) Semantic enhancement via\nlarge language model (LLM)-based self-instruction sampling on authentic ASR\noutputs to expand scenario coverage; (3) Multi-agent adversarial synthesis that\nsimulates emerging fraud tactics through predefined communication scenarios and\nfraud typologies. The generated dataset contains 28,511 rigorously processed\nspeech-text pairs, complete with detailed annotations for fraud reasoning. The\ndataset is divided into three tasks: scenario classification, fraud detection,\nfraud type classification. Furthermore, we construct TeleAntiFraud-Bench, a\nstandardized evaluation benchmark comprising proportionally sampled instances\nfrom the dataset, to facilitate systematic testing of model performance on\ntelecom fraud detection tasks. We also contribute a production-optimized\nsupervised fine-tuning (SFT) model trained on hybrid real/synthetic data, while\nopen-sourcing the data processing framework to enable community-driven dataset\nexpansion. This work establishes a foundational framework for multimodal\nanti-fraud research while addressing critical challenges in data privacy and\nscenario diversity. The project will be released at\nhttps://github.com/JimmyMa99/TeleAntiFraud.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64a0ed5ed5374ca472cfb0ac/PsAJTW9JTyrHqtjQP0lLJ.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.24115.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64a0ed5ed5374ca472cfb0ac",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a0ed5ed5374ca472cfb0ac/n_wXamXfR_PPn0hRbnR1X.jpeg",
      "fullname": "ZhimingMa",
      "name": "JimmyMa99",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.23077",
      "authors": [
        {
          "_id": "67eb58c71e23a7499b683cce",
          "user": {
            "_id": "6650c77a74664a42ddfb9187",
            "avatarUrl": "/avatars/92001bbe0ae9b14309730316b639cede.svg",
            "isPro": false,
            "fullname": "yueliu1999",
            "user": "yueliu1999",
            "type": "user"
          },
          "name": "Yue Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:11:01.812Z",
          "hidden": false
        },
        {
          "_id": "67eb58c71e23a7499b683ccf",
          "name": "Jiaying Wu",
          "hidden": false
        },
        {
          "_id": "67eb58c71e23a7499b683cd0",
          "name": "Yufei He",
          "hidden": false
        },
        {
          "_id": "67eb58c71e23a7499b683cd1",
          "user": {
            "_id": "62728f4f6253fe2068da1021",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62728f4f6253fe2068da1021/KZ65X0EH98AF3zXemPiap.jpeg",
            "isPro": false,
            "fullname": "Hongcheng Gao",
            "user": "HongchengGao",
            "type": "user"
          },
          "name": "Hongcheng Gao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:11:54.944Z",
          "hidden": false
        },
        {
          "_id": "67eb58c71e23a7499b683cd2",
          "user": {
            "_id": "67c7a9fb27c2e81cf8660375",
            "avatarUrl": "/avatars/a5129cca93a31d4b730af4c543051d8e.svg",
            "isPro": false,
            "fullname": "Hongyu Chen",
            "user": "HongyuChen",
            "type": "user"
          },
          "name": "Hongyu Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:12:05.159Z",
          "hidden": false
        },
        {
          "_id": "67eb58c71e23a7499b683cd3",
          "user": {
            "_id": "642577e06d0f0f5f1dc68904",
            "avatarUrl": "/avatars/3df7cfcf9ca6cd9c9e9b10a73f4efc35.svg",
            "isPro": false,
            "fullname": "Bibaolong",
            "user": "Bibaolong",
            "type": "user"
          },
          "name": "Baolong Bi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:12:18.251Z",
          "hidden": false
        },
        {
          "_id": "67eb58c71e23a7499b683cd4",
          "user": {
            "_id": "669e19e5dac1eb34c0f5f505",
            "avatarUrl": "/avatars/bec7d1d1dac2ad6570844d1f00e7df0a.svg",
            "isPro": false,
            "fullname": "Jiaheng Zhang",
            "user": "jiaheng233",
            "type": "user"
          },
          "name": "Jiaheng Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:12:23.776Z",
          "hidden": false
        },
        {
          "_id": "67eb58c71e23a7499b683cd5",
          "user": {
            "_id": "66221f1a90f3fd333c4ec52e",
            "avatarUrl": "/avatars/a3173d9603a69020ec24170831c97c2f.svg",
            "isPro": false,
            "fullname": "Zhiqi Huang",
            "user": "Angelalilyer",
            "type": "user"
          },
          "name": "Zhiqi Huang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:12:36.398Z",
          "hidden": false
        },
        {
          "_id": "67eb58c71e23a7499b683cd6",
          "user": {
            "_id": "651d8032c50012d33e914f2f",
            "avatarUrl": "/avatars/0a44c9f51fc50ce86582e328c361ea00.svg",
            "isPro": false,
            "fullname": "Bryan Hooi",
            "user": "bhooi",
            "type": "user"
          },
          "name": "Bryan Hooi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:12:42.880Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-29T13:27:46.000Z",
      "submittedOnDailyAt": "2025-04-01T01:39:12.154Z",
      "title": "Évaluation de grands modèles de logique qui atteignent une efficacité dans l'inférence : résumé",
      "submittedOnDailyBy": {
        "_id": "6650c77a74664a42ddfb9187",
        "avatarUrl": "/avatars/92001bbe0ae9b14309730316b639cede.svg",
        "isPro": false,
        "fullname": "yueliu1999",
        "user": "yueliu1999",
        "type": "user"
      },
      "summary": "Les modèles de Grande Logique (LGLs) ont connu un augmentation clair de leurs capacités logiques et ont démontré un rendement attendu dans des tâches complexes. Cependant, ils présentent des caractéristiques défavorables en utilisation de tokens, consommation de mémoire et temps d'inférence en raison de processus logiques. Par conséquent, cette recherche se concentre sur l'examen de méthodes efficaces d'inférence spécifiques pour les LGLs, avec l'objectif de mitiger les inconvénients des tokens tout en maintenant la qualité logique. Tout d'abord, on présente des technologies pour classer les méthodes récentes en deux catégories principales : (a) le Décalage Cognitif Explicit (DCE) réduit les tokens tout en maintenant une structure logique explicite. (b) le Décalage Cognitif Ocult (DCO) codifie les états logiques dans des représentations cachées. On discute leurs avantages et inconvénients. Ensuite, on analyse expérimentalement le rendement et l'efficacité des méthodes existantes. On aborde également les problèmes ouverts dans ce domaine. On discute des thèmes tels que le contrôle logique centré sur l'être humain, l'explicabilité logique et l'équilibre entre efficacité et qualité logique, la sécurité de la logique efficace, et l'application large de la logique efficace. De plus, on souligne les principaux éléments pour améliorer l'efficacité d'inférence dans les LGLs, comme l'intégration de modèles, de nouvelles architectures et l'utilisation de techniques comme l'agent rotatif. Cette étude joue un rôle de guidance pour les chercheurs dans la résolution de problèmes dans ce domaine riche.",
      "upvotes": 7,
      "discussionId": "67eb58c81e23a7499b683d12",
      "ai_keywords": [
        "Chain-of-Thought (CoT)",
        "explicit compact Chain-of-Thought (CoT)",
        "implicit latent CoT",
        "model merging",
        "agent routers"
      ]
    },
    "publishedAt": "2025-03-29T09:27:46.000Z",
    "title": "Efficient Inference for Large Reasoning Models: A Survey",
    "summary": "Large Reasoning Models (LRMs) significantly improve the reasoning ability of\nLarge Language Models (LLMs) by learning to reason, exhibiting promising\nperformance in complex task-solving. However, their deliberative reasoning\nprocess leads to inefficiencies in token usage, memory consumption, and\ninference time. Thus, this survey provides a review of efficient inference\nmethods designed specifically for LRMs, focusing on mitigating token\ninefficiency while preserving the reasoning quality. First, we introduce a\ntaxonomy to group the recent methods into two main categories: (a) explicit\ncompact Chain-of-Thought (CoT), which reduces tokens while keeping the explicit\nreasoning structure, and (b) implicit latent CoT, which encodes reasoning steps\nwithin hidden representations instead of explicit tokens. Meanwhile, we discuss\ntheir strengths and weaknesses. Then, we conduct empirical analyses on existing\nmethods from performance and efficiency aspects. Besides, we present open\nchallenges in this field, including human-centric controllable reasoning,\ntrade-off between interpretability and efficiency of reasoning, ensuring safety\nof efficient reasoning, and broader applications of efficient reasoning. In\naddition, we highlight key insights for enhancing LRMs' inference efficiency\nvia techniques such as model merging, new architectures, and agent routers. We\nhope this work serves as a valuable guide, helping researchers overcome\nchallenges in this vibrant\nfieldhttps://github.com/yueliu1999/Awesome-Efficient-Inference-for-LRMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.23077.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6650c77a74664a42ddfb9187",
      "avatarUrl": "/avatars/92001bbe0ae9b14309730316b639cede.svg",
      "fullname": "yueliu1999",
      "name": "yueliu1999",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.24290",
      "authors": [
        {
          "_id": "67eb762381e530baa56dc830",
          "user": {
            "_id": "625026b7d2d191ac43320c5e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg",
            "isPro": false,
            "fullname": "Jingcheng Hu",
            "user": "reign12",
            "type": "user"
          },
          "name": "Jingcheng Hu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:09:02.123Z",
          "hidden": false
        },
        {
          "_id": "67eb762381e530baa56dc831",
          "user": {
            "_id": "664ae39ab5e5f95dc6209365",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/664ae39ab5e5f95dc6209365/8Z9ERYhX6URXh4si6jWGm.jpeg",
            "isPro": false,
            "fullname": "Yinmin Zhang",
            "user": "YinminZhang",
            "type": "user"
          },
          "name": "Yinmin Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:09:09.884Z",
          "hidden": false
        },
        {
          "_id": "67eb762381e530baa56dc832",
          "name": "Qi Han",
          "hidden": false
        },
        {
          "_id": "67eb762381e530baa56dc833",
          "user": {
            "_id": "60d4440fe648443279aaffd8",
            "avatarUrl": "/avatars/bf7209c1f14ae120f5bfda5fda1301b7.svg",
            "isPro": false,
            "fullname": "Daxin Jiang",
            "user": "djiang",
            "type": "user"
          },
          "name": "Daxin Jiang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:09:22.914Z",
          "hidden": false
        },
        {
          "_id": "67eb762381e530baa56dc834",
          "name": "Xiangyu Zhang",
          "hidden": false
        },
        {
          "_id": "67eb762381e530baa56dc835",
          "name": "Heung-Yeung Shum",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-31T16:36:05.000Z",
      "submittedOnDailyAt": "2025-04-01T03:44:53.609Z",
      "title": "Open-Reasoner-Zero : Expansion de l'apprentissage par renforcement basé sur des modèles de base avec un approche ouverte-source",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Open-Radion-Zero introduit la première implémentation open-source de l'entraînement théorique de RL à grande échelle. Cette implémentation est conçue sur des principes de scalabilité, simplicité et accessibilité. Grâce à des expériences de scalabilité, elle montre la capacité d'atteindre des performances scalables en termes de longueur de réponse et de scores de benchmark, sans utiliser de normalisation KL, en adoptant des approches minimalistes, PPO bayésien (GAE(lambda=1, gamma=1)) et des récompenses basées sur des règles intuitives. En utilisant des modèles de base comme DeepSeek-R1-Zero, elle atteint des performances les plus élevées sur des benchmarks tels que AIME2024, MATH500 et GPQA Diamond, tout en nécessitant une phase d'entraînement efficace et suffisante par rapport au flux de DeepSeek-R1-Zero. En s'inspirant de l'esprit de l'open-source, elle publie en libre accès le code source, les paramètres, les données d'entraînement et les poids du modèle pour chaque taille.",
      "upvotes": 6,
      "discussionId": "67eb762481e530baa56dc872",
      "ai_keywords": [
        "Reinforcement Learning (RL)",
        "vanilla PPO",
        "GAE ($\\lambda=1$, $\\gamma=1$)",
        "rule-based rewards",
        "KL regularization",
        "response length",
        "benchmark performance",
        "AIME2024",
        "MATH500",
        "GPQA Diamond benchmark",
        "training steps"
      ]
    },
    "publishedAt": "2025-03-31T12:36:05.000Z",
    "title": "Open-Reasoner-Zero: An Open Source Approach to Scaling Up Reinforcement\n  Learning on the Base Model",
    "summary": "We introduce Open-Reasoner-Zero, the first open source implementation of\nlarge-scale reasoning-oriented RL training focusing on scalability, simplicity\nand accessibility. Through extensive experiments, we demonstrate that a\nminimalist approach, vanilla PPO with GAE (lambda=1, gamma=1) and\nstraightforward rule-based rewards, without any KL regularization, is\nsufficient to scale up both response length and benchmark performance, similar\nto the phenomenon observed in DeepSeek-R1-Zero. Using the same base model as\nDeepSeek-R1-Zero-Qwen-32B, our implementation achieves superior performance on\nAIME2024, MATH500, and the GPQA Diamond benchmark while demonstrating\nremarkable efficiency -- requiring only a tenth of the training steps, compared\nto DeepSeek-R1-Zero pipeline. In the spirit of open source, we release our\nsource code, parameter settings, training data, and model weights across\nvarious sizes.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.24290.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6543
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.23829",
      "authors": [
        {
          "_id": "67eb759cb9fa8908e1934f21",
          "name": "Yi Su",
          "hidden": false
        },
        {
          "_id": "67eb759cb9fa8908e1934f22",
          "user": {
            "_id": "62d58fd53bf5e059f7cc3245",
            "avatarUrl": "/avatars/7a4f3ee4a37245f67efd26749d66a706.svg",
            "isPro": false,
            "fullname": "Dian Yu",
            "user": "yudian",
            "type": "user"
          },
          "name": "Dian Yu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:13:47.119Z",
          "hidden": false
        },
        {
          "_id": "67eb759cb9fa8908e1934f23",
          "user": {
            "_id": "64c94eddcb2f1bf0e7db5a4d",
            "avatarUrl": "/avatars/f7e2532d3c85d5e5b5a02c579ea68c3a.svg",
            "isPro": false,
            "fullname": "Linfeng Song",
            "user": "freesunshine0316",
            "type": "user"
          },
          "name": "Linfeng Song",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:13:54.065Z",
          "hidden": false
        },
        {
          "_id": "67eb759cb9fa8908e1934f24",
          "user": {
            "_id": "6670e285b0c03c4e9d6e0985",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/uCZHm4gKSHZ2b0hpHWgZv.jpeg",
            "isPro": false,
            "fullname": "Juntao Li",
            "user": "douvleplus",
            "type": "user"
          },
          "name": "Juntao Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:14:03.120Z",
          "hidden": false
        },
        {
          "_id": "67eb759cb9fa8908e1934f25",
          "user": {
            "_id": "65147a1426fbd558dbd08f1b",
            "avatarUrl": "/avatars/86574ee2d5c22e940be1c4e50be88675.svg",
            "isPro": false,
            "fullname": "Haitao Mi",
            "user": "haitaominlp",
            "type": "user"
          },
          "name": "Haitao Mi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:14:10.594Z",
          "hidden": false
        },
        {
          "_id": "67eb759cb9fa8908e1934f26",
          "user": {
            "_id": "67485743561b1e6f9579389f",
            "avatarUrl": "/avatars/8a4cc63bd7be388010bc329bb74582a1.svg",
            "isPro": false,
            "fullname": "Zhaopeng Tu",
            "user": "zptu",
            "type": "user"
          },
          "name": "Zhaopeng Tu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:14:16.978Z",
          "hidden": false
        },
        {
          "_id": "67eb759cb9fa8908e1934f27",
          "name": "Min Zhang",
          "hidden": false
        },
        {
          "_id": "67eb759cb9fa8908e1934f28",
          "name": "Dong Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-31T08:22:49.000Z",
      "submittedOnDailyAt": "2025-04-01T03:42:19.595Z",
      "title": "Extension de la RL avec des récompenses vérifiables dans diverses domaines",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "L'introduction de récompenses dans l'apprentissage par renforcement (RL) est possible en raison de raisons mathématiques et parce qu'elles montrent des résultats attendus dans des tâches de codification avec des réponses structurées. Cependant, la possibilité d'application large de cette méthode a été peu explorée. Dans cet article, nous étudions l'extension de RLVR dans plusieurs domaines divers tels que la médecine, la chimie, la psychologie et l'économie. Lorsque les réponses sont structurées, nous observons que les modèles de langage à grande échelle (LLMs) montrent une haute concordance dans les deux évaluations, et nous doutons de la nécessité de grandes notes de domaine. Pour résoudre les limitations des deux évaluations dans le traitement de réponses non structurées, nous ajoutons un score basé sur des modèles au RLVR et nous améliorons sa flexibilité. Les résultats des expériences montrent que les modèles génératifs de récompenses ne nécessitent pas de grandes notes de domaine et fournissent des signaux de récompenses fiables pour l'apprentissage par renforcement. En optimisant des modèles de base avec d'autres algorithmes de RL et en utilisant comme exemple les meilleurs LLMs open-source comme Qwen2.5-72B-Instruct et DeepSeek-R1-Distill-Qwen-32B, nous obtenons des politiques qui dépassent significativement les limites de ces modèles. Cela montre un excellent rendement dans les réponses libres, renforce la robustesse et l'échellabilité du RLVR, et démontre son utilité même dans des applications réelles qui incluent du bruit et des étiquettes faibles.",
      "upvotes": 5,
      "discussionId": "67eb759db9fa8908e1934f62",
      "ai_keywords": [
        "reinforcement learning (RL)",
        "verifiable rewards (RLVR)",
        "mathematical reasoning",
        "coding tasks",
        "well-structured reference answers",
        "diverse domains",
        "medicine",
        "chemistry",
        "psychology",
        "economics",
        "large language models (LLMs)",
        "binary judgments",
        "domain-specific reward models",
        "model-based soft scoring",
        "distilled generative reward model",
        "effective cross-domain verifier",
        "reward signals",
        "fine-tuning",
        "base 7B model",
        "RL algorithms",
        "state-of-the-art open-source aligned LLMs",
        "Qwen2.5-72B-Instruct",
        "DeepSeek-R1-Distill-Qwen-32B",
        "free-form answer settings",
        "robustness",
        "scalability",
        "real-world applications",
        "noisy labels",
        "weak labels"
      ]
    },
    "publishedAt": "2025-03-31T04:22:49.000Z",
    "title": "Expanding RL with Verifiable Rewards Across Diverse Domains",
    "summary": "Reinforcement learning (RL) with verifiable rewards (RLVR) has shown\npromising results in mathematical reasoning and coding tasks where\nwell-structured reference answers are available. However, its applicability to\nbroader domains remains underexplored. In this work, we study the extension of\nRLVR to more diverse domains such as medicine, chemistry, psychology, and\neconomics. We observe high agreement in binary judgments across different large\nlanguage models (LLMs) when objective reference answers exist, which challenges\nthe necessity of large-scale annotation for training domain-specific reward\nmodels. To address the limitations of binary rewards when handling unstructured\nreference answers, we further incorporate model-based soft scoring into RLVR to\nimprove its flexibility. Our experiments show that a distilled generative\nreward model can serve as an effective cross-domain verifier, providing\nreliable reward signals for RL without requiring domain-specific annotations.\nBy fine-tuning a base 7B model using various RL algorithms against our reward\nmodel, we obtain policies that outperform state-of-the-art open-source aligned\nLLMs such as Qwen2.5-72B-Instruct and DeepSeek-R1-Distill-Qwen-32B by a large\nmargin, across domains in free-form answer settings. This also strengthens\nRLVR's robustness and scalability, highlighting its potential for real-world\napplications with noisy or weak labels.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.23829.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6543
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.21694",
      "authors": [
        {
          "_id": "67eb92defa85fe030e2db9e2",
          "user": {
            "_id": "64295d1f4e073875f6a605ac",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64295d1f4e073875f6a605ac/HEK8dluqPUhiARW-0eBZG.jpeg",
            "isPro": true,
            "fullname": "Zhiyuan Ma",
            "user": "ZhiyuanthePony",
            "type": "user"
          },
          "name": "Zhiyuan Ma",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:14:51.486Z",
          "hidden": false
        },
        {
          "_id": "67eb92defa85fe030e2db9e3",
          "user": {
            "_id": "672111333ced358bdac2925d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/qHNDI3zYRkJZmhCHkSZwK.png",
            "isPro": false,
            "fullname": "Xinyue Liang",
            "user": "DarklordLeto",
            "type": "user"
          },
          "name": "Xinyue Liang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:14:58.992Z",
          "hidden": false
        },
        {
          "_id": "67eb92defa85fe030e2db9e4",
          "name": "Rongyuan Wu",
          "hidden": false
        },
        {
          "_id": "67eb92defa85fe030e2db9e5",
          "name": "Xiangyu Zhu",
          "hidden": false
        },
        {
          "_id": "67eb92defa85fe030e2db9e6",
          "name": "Zhen Lei",
          "hidden": false
        },
        {
          "_id": "67eb92defa85fe030e2db9e7",
          "name": "Lei Zhang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64295d1f4e073875f6a605ac/F5lBcbDKq1_K31O-vxWBg.mp4"
      ],
      "publishedAt": "2025-03-27T16:59:15.000Z",
      "submittedOnDailyAt": "2025-04-01T06:05:21.846Z",
      "title": "Traduction en français : \n\n\"Extraction avancée : Technologie d'apprentissage profond pour la génération de masques 3D à partir de texte intuitif sans données 3D, adaptée à la création de masques 3D.\"",
      "submittedOnDailyBy": {
        "_id": "64295d1f4e073875f6a605ac",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64295d1f4e073875f6a605ac/HEK8dluqPUhiARW-0eBZG.jpeg",
        "isPro": true,
        "fullname": "Zhiyuan Ma",
        "user": "ZhiyuanthePony",
        "type": "user"
      },
      "summary": "Avoir des hautes attentes permet d'obtenir un modèle capable de générer des qualités élevées de maillages 3D à travers des coupes de texte. Les tentatives récentes, comme par exemple, l'application de modèles qui étendent des textes pré-entraînés à des images (comme Stable Diffusion, SD) pour générer des modèles de représentation 3D (comme Triplane), mais la qualité peut diminuer en raison de la pénurie de données d'entraînement de haute qualité. Pour surmonter cette limitation, nous proposons un nouveau schéma d'entraînement, appelé Progressive Rendering Distillation (PRD). Ce méthode élimine la nécessité de données 3D réelles et entraîne expérimentalement des modèles de multiples points de vue pour les appliquer aux fichiers 3D de SD. À chaque étape d'entraînement, le PRD utilise un U-Net pour débruiter progressivement le bruit aléatoire et interprète les retenues à chaque étape dans la sortie 3D. Les modèles de multiples points de vue, comme MVDream et RichDreamer, ainsi que SD, se concentrent sur les textures et la généralité qui correspondent au texte à travers la distillation de coupes de texte. Le PRD facilite l'expansion des données d'entraînement et l'amélioration de la qualité de génération pour des textes complexes, en éliminant la nécessité de données 3D réelles. De plus, le PRD peut accélérer la vitesse d'inférence du modèle de génération à plusieurs étapes. En utilisant le PRD, nous entraînons le modèle Triplane et le nommons TriplaneTurbo. TriplaneTurbo augmente les paramètres entrainables seulement de 2,5% pour appliquer à la génération 3D de SD, mais est plus efficace et de meilleure qualité que les modèles précédents de génération 3D à partir de texte. En particulier, TriplaneTurbo peut générer une amélioration de qualité de maillage 3D en un second et s'adapte bien aux entrées de texte complexes. Le code est disponible sur https://github.com/theEricMa/TriplaneTurbo.",
      "upvotes": 5,
      "discussionId": "67eb92e2fa85fe030e2dbc04",
      "projectPage": "https://theericma.github.io/TriplaneTurbo/",
      "githubRepo": "https://github.com/theEricMa/TriplaneTurbo",
      "ai_keywords": [
        "diffusion models",
        "Stable Diffusion (SD)",
        "3D representations",
        "Progressive Rendering Distillation (PRD)",
        "U-Net",
        "latent from random noise",
        "denoise the latent",
        "3D output",
        "Multi-view diffusion models",
        "MVDream",
        "RichDreamer",
        "score distillation",
        "text-consistent textures",
        "geometries",
        "Triplane generator",
        "TriplaneTurbo",
        "high-quality 3D meshes"
      ]
    },
    "publishedAt": "2025-03-27T12:59:15.000Z",
    "title": "Progressive Rendering Distillation: Adapting Stable Diffusion for\n  Instant Text-to-Mesh Generation without 3D Data",
    "summary": "It is highly desirable to obtain a model that can generate high-quality 3D\nmeshes from text prompts in just seconds. While recent attempts have adapted\npre-trained text-to-image diffusion models, such as Stable Diffusion (SD), into\ngenerators of 3D representations (e.g., Triplane), they often suffer from poor\nquality due to the lack of sufficient high-quality 3D training data. Aiming at\novercoming the data shortage, we propose a novel training scheme, termed as\nProgressive Rendering Distillation (PRD), eliminating the need for 3D\nground-truths by distilling multi-view diffusion models and adapting SD into a\nnative 3D generator. In each iteration of training, PRD uses the U-Net to\nprogressively denoise the latent from random noise for a few steps, and in each\nstep it decodes the denoised latent into 3D output. Multi-view diffusion\nmodels, including MVDream and RichDreamer, are used in joint with SD to distill\ntext-consistent textures and geometries into the 3D outputs through score\ndistillation. Since PRD supports training without 3D ground-truths, we can\neasily scale up the training data and improve generation quality for\nchallenging text prompts with creative concepts. Meanwhile, PRD can accelerate\nthe inference speed of the generation model in just a few steps. With PRD, we\ntrain a Triplane generator, namely TriplaneTurbo, which adds only 2.5%\ntrainable parameters to adapt SD for Triplane generation. TriplaneTurbo\noutperforms previous text-to-3D generators in both efficiency and quality.\nSpecifically, it can produce high-quality 3D meshes in 1.2 seconds and\ngeneralize well for challenging text input. The code is available at\nhttps://github.com/theEricMa/TriplaneTurbo.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64295d1f4e073875f6a605ac/F5lBcbDKq1_K31O-vxWBg.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.21694.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64295d1f4e073875f6a605ac",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64295d1f4e073875f6a605ac/HEK8dluqPUhiARW-0eBZG.jpeg",
      "fullname": "Zhiyuan Ma",
      "name": "ZhiyuanthePony",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.19901",
      "authors": [
        {
          "_id": "67eac6433755a17e3cbff585",
          "user": {
            "_id": "6630cc7e9ee8861dd0b9bdbd",
            "avatarUrl": "/avatars/01da7d60cbb107d58329bbb80d924eb4.svg",
            "isPro": false,
            "fullname": "Liang Pan",
            "user": "lianganimation",
            "type": "user"
          },
          "name": "Liang Pan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-01T07:47:34.754Z",
          "hidden": false
        },
        {
          "_id": "67eac6433755a17e3cbff586",
          "user": {
            "_id": "649c178950b4be74229d680f",
            "avatarUrl": "/avatars/941dec90fdfc46b9ae23378e3a3113f4.svg",
            "isPro": false,
            "fullname": "Zeshi Yang",
            "user": "Zeshi209",
            "type": "user"
          },
          "name": "Zeshi Yang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:21:03.851Z",
          "hidden": false
        },
        {
          "_id": "67eac6433755a17e3cbff587",
          "user": {
            "_id": "645223fb01d7bd9555ea399a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645223fb01d7bd9555ea399a/fVR7XmGg6pMSRKx9sEdvT.png",
            "isPro": false,
            "fullname": "Zhiyang Dou",
            "user": "frankzydou",
            "type": "user"
          },
          "name": "Zhiyang Dou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:21:09.328Z",
          "hidden": false
        },
        {
          "_id": "67eac6433755a17e3cbff588",
          "user": {
            "_id": "6437a813fac5ea753f1c72d2",
            "avatarUrl": "/avatars/69e60e60497e404149a1dad46649dad4.svg",
            "isPro": false,
            "fullname": "wenjia Wang",
            "user": "WenjiaWang",
            "type": "user"
          },
          "name": "Wenjia Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:21:22.428Z",
          "hidden": false
        },
        {
          "_id": "67eac6433755a17e3cbff589",
          "name": "Buzhen Huang",
          "hidden": false
        },
        {
          "_id": "67eac6433755a17e3cbff58a",
          "user": {
            "_id": "635f93577c05eb9f59966209",
            "avatarUrl": "/avatars/add48d7e3790a6b500d6c451ef8b0f75.svg",
            "isPro": false,
            "fullname": "Intelligent Digital Creation",
            "user": "BoDai",
            "type": "user"
          },
          "name": "Bo Dai",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:21:59.939Z",
          "hidden": false
        },
        {
          "_id": "67eac6433755a17e3cbff58b",
          "name": "Taku Komura",
          "hidden": false
        },
        {
          "_id": "67eac6433755a17e3cbff58c",
          "user": {
            "_id": "669cb638666901f41dae51bf",
            "avatarUrl": "/avatars/a7cc19e3db84bd86bee3eb6fd4897959.svg",
            "isPro": false,
            "fullname": "Jingbo Wang",
            "user": "jingbocuhk",
            "type": "user"
          },
          "name": "Jingbo Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:21:43.455Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6630cc7e9ee8861dd0b9bdbd/2bErZDgOyv5xeo1wDfMz3.mp4"
      ],
      "publishedAt": "2025-03-25T17:57:46.000Z",
      "submittedOnDailyAt": "2025-04-01T06:24:17.315Z",
      "title": "TokenHSI : Intégration de l'Interaction Physique Homo-Scène par une Synthèse Complémentaire\n  Tokenisation des Tâches",
      "submittedOnDailyBy": {
        "_id": "6630cc7e9ee8861dd0b9bdbd",
        "avatarUrl": "/avatars/01da7d60cbb107d58329bbb80d924eb4.svg",
        "isPro": false,
        "fullname": "Liang Pan",
        "user": "lianganimation",
        "type": "user"
      },
      "summary": "La synthèse des interactions physiquement possibles entre humains et scènes (HSI) est cruciale tant pour l'animation informatique que pour la vision artificielle. Alors que ces méthodes sont en développement, les méthodes actuelles se concentrent principalement sur la création de contrôleurs spécialisés pour des tâches d'interaction, ce qui limite considérablement leur capacité à faire face à des tâches complexes de HSI. Pour résoudre ces problèmes, nous présentons TokenHSI. TokenHSI utilise une unique politique intégrée basée sur les Transformer, qui intègre diverses technologies et permet une adaptation flexible. Le point clé est de modéliser le reconnaissance des familles humaines individuellement en utilisant des tokens partagés et de les combiner avec des tokens de tâches spécifiques à travers une structure de masque. Cette politique intégrée facilite l'échange de connaissances entre les technologies et encourage le training de multiples tâches. De plus, notre architecture de politique supporte des entrées variables et encourage l'adaptabilité des technologies entraînées. De plus, on peut entraîner plus de tokens de tâches pour changer la géométrie des objectifs d'interaction ou résoudre des tâches complexes en collaboration de multiples technologies. Nos expériences montrent que notre approche améliore significativement la diversité, l'adaptabilité et l'extensibilité. Page web : https://liangpan99.github.io/TokenHSI/",
      "upvotes": 5,
      "discussionId": "67eac6443755a17e3cbff5cf",
      "projectPage": "https://liangpan99.github.io/TokenHSI/",
      "githubRepo": "https://github.com/liangpan99/TokenHSI",
      "ai_keywords": [
        "transformer-based policy",
        "proprioception",
        "shared token",
        "task tokens",
        "masking mechanism",
        "multi-task training",
        "variable length inputs",
        "task tokenizers"
      ]
    },
    "publishedAt": "2025-03-25T13:57:46.000Z",
    "title": "TokenHSI: Unified Synthesis of Physical Human-Scene Interactions through\n  Task Tokenization",
    "summary": "Synthesizing diverse and physically plausible Human-Scene Interactions (HSI)\nis pivotal for both computer animation and embodied AI. Despite encouraging\nprogress, current methods mainly focus on developing separate controllers, each\nspecialized for a specific interaction task. This significantly hinders the\nability to tackle a wide variety of challenging HSI tasks that require the\nintegration of multiple skills, e.g., sitting down while carrying an object. To\naddress this issue, we present TokenHSI, a single, unified transformer-based\npolicy capable of multi-skill unification and flexible adaptation. The key\ninsight is to model the humanoid proprioception as a separate shared token and\ncombine it with distinct task tokens via a masking mechanism. Such a unified\npolicy enables effective knowledge sharing across skills, thereby facilitating\nthe multi-task training. Moreover, our policy architecture supports variable\nlength inputs, enabling flexible adaptation of learned skills to new scenarios.\nBy training additional task tokenizers, we can not only modify the geometries\nof interaction targets but also coordinate multiple skills to address complex\ntasks. The experiments demonstrate that our approach can significantly improve\nversatility, adaptability, and extensibility in various HSI tasks. Website:\nhttps://liangpan99.github.io/TokenHSI/",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6630cc7e9ee8861dd0b9bdbd/2bErZDgOyv5xeo1wDfMz3.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19901.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6630cc7e9ee8861dd0b9bdbd",
      "avatarUrl": "/avatars/01da7d60cbb107d58329bbb80d924eb4.svg",
      "fullname": "Liang Pan",
      "name": "lianganimation",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.14941",
      "authors": [
        {
          "_id": "67eb932522a341478ae86cb6",
          "user": {
            "_id": "67a99d1fef1439e285c4cbec",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/VrwUmrY2wsg4sVSIMc--K.png",
            "isPro": false,
            "fullname": "Qihui Zhang",
            "user": "77Hui",
            "type": "user"
          },
          "name": "Qihui Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-01T07:46:00.373Z",
          "hidden": false
        },
        {
          "_id": "67eb932522a341478ae86cb7",
          "user": {
            "_id": "65e14c28b1a6de8a71e70172",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65e14c28b1a6de8a71e70172/D097SILGsqoufpp3sG8tV.jpeg",
            "isPro": false,
            "fullname": "Munan Ning",
            "user": "MunanNing",
            "type": "user"
          },
          "name": "Munan Ning",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:18:39.336Z",
          "hidden": false
        },
        {
          "_id": "67eb932522a341478ae86cb8",
          "name": "Zheyuan Liu",
          "hidden": false
        },
        {
          "_id": "67eb932522a341478ae86cb9",
          "name": "Yanbo Wang",
          "hidden": false
        },
        {
          "_id": "67eb932522a341478ae86cba",
          "name": "Jiayi Ye",
          "hidden": false
        },
        {
          "_id": "67eb932522a341478ae86cbb",
          "user": {
            "_id": "6637443ecd9097ac3c996d3c",
            "avatarUrl": "/avatars/d1c38bf03c2517ba0a7004b2f9f9bc96.svg",
            "isPro": false,
            "fullname": "yue",
            "user": "yuehuang",
            "type": "user"
          },
          "name": "Yue Huang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:19:26.627Z",
          "hidden": false
        },
        {
          "_id": "67eb932522a341478ae86cbc",
          "name": "Shuo Yang",
          "hidden": false
        },
        {
          "_id": "67eb932522a341478ae86cbd",
          "name": "Xiao Chen",
          "hidden": false
        },
        {
          "_id": "67eb932522a341478ae86cbe",
          "user": {
            "_id": "62c51800cb7033fd49b8efb7",
            "avatarUrl": "/avatars/06c2be0015f8022f9912f2279f2b3597.svg",
            "isPro": false,
            "fullname": "Song",
            "user": "Yibing",
            "type": "user"
          },
          "name": "Yibing Song",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:19:07.616Z",
          "hidden": false
        },
        {
          "_id": "67eb932522a341478ae86cbf",
          "name": "Li Yuan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-19T07:15:41.000Z",
      "submittedOnDailyAt": "2025-04-01T05:48:16.581Z",
      "title": "UPME : Cadre pour l'Évaluation de Modèles de Langue Multilingue avec Réasonnement en Pair et sans Vérification",
      "submittedOnDailyBy": {
        "_id": "67a99d1fef1439e285c4cbec",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/VrwUmrY2wsg4sVSIMc--K.png",
        "isPro": false,
        "fullname": "Qihui Zhang",
        "user": "77Hui",
        "type": "user"
      },
      "summary": "Les modèles de langage multilingue multimodal (MLLMs) ont émergé pour aborder des problèmes de réponse à des questions visuelles (VQA), et ont conduit à un nouvel enfoque de recherche sur l'évaluation subjective de ces modèles. Les méthodes d'évaluation actuelles sont limitées par une forte charge de travail liée au design de paires de questions et de réponses pour les images, ce qui réduit l'échelle et le domaine de l'évaluation. L'approche automatisée des MLLM en tant qu'examinateurs vise à réduire cette charge de travail, mais se heurte à des difficultés en raison de ses biais. Pour résoudre ces problèmes, nous proposons un cadre d'évaluation des MLLM basé sur l'évaluation de pairs sans supervision. Ce cadre utilise uniquement des données d'images, où le modèle génère automatiquement les questions et évalue les réponses de autres modèles. De cette manière, on élimine la dépendance à la charge de travail. De plus, nous présentons un système de notation visuelle-langue (Vision Language Score System) sur trois aspects : compréhension visuelle et logique, relation entre image et question, et précision de la réponse, pour atténuer les biais. Les résultats des expérimentations montrent que UPME obtient une corrélation de Pearson de 0,944 dans le jeu de données MMstar et 0,814 dans ScienceQA, qui correspondent à l'évaluation humaine, démontrant que notre cadre de travail montre une similitude extrêmement élevée avec les benchmarks humains et les préférences humaines.",
      "upvotes": 3,
      "discussionId": "67eb932622a341478ae86d15",
      "ai_keywords": [
        "Multimodal Large Language Models (MLLMs)",
        "Visual Question Answering (VQA)",
        "Q&A pairs",
        "MLLM-as-judge",
        "Unsupervised Peer review MLLM Evaluation (UPME)",
        "vision-language scoring system",
        "response correctness",
        "visual understanding and reasoning",
        "image-text correlation",
        "Pearson correlation",
        "MMstar dataset",
        "ScienceQA dataset"
      ]
    },
    "publishedAt": "2025-03-19T03:15:41.000Z",
    "title": "UPME: An Unsupervised Peer Review Framework for Multimodal Large\n  Language Model Evaluation",
    "summary": "Multimodal Large Language Models (MLLMs) have emerged to tackle the\nchallenges of Visual Question Answering (VQA), sparking a new research focus on\nconducting objective evaluations of these models. Existing evaluation methods\nface limitations due to the significant human workload required to design Q&A\npairs for visual images, which inherently restricts the scale and scope of\nevaluations. Although automated MLLM-as-judge approaches attempt to reduce the\nhuman workload through automatic evaluations, they often introduce biases. To\naddress these problems, we propose an Unsupervised Peer review MLLM Evaluation\nframework. It utilizes only image data, allowing models to automatically\ngenerate questions and conduct peer review assessments of answers from other\nmodels, effectively alleviating the reliance on human workload. Additionally,\nwe introduce the vision-language scoring system to mitigate the bias issues,\nwhich focuses on three aspects: (i) response correctness; (ii) visual\nunderstanding and reasoning; and (iii) image-text correlation. Experimental\nresults demonstrate that UPME achieves a Pearson correlation of 0.944 with\nhuman evaluations on the MMstar dataset and 0.814 on the ScienceQA dataset,\nindicating that our framework closely aligns with human-designed benchmarks and\ninherent human preferences.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14941.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67a99d1fef1439e285c4cbec",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/VrwUmrY2wsg4sVSIMc--K.png",
      "fullname": "Qihui Zhang",
      "name": "77Hui",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.24391",
      "authors": [
        {
          "_id": "67eb72a2291b56e50b66a063",
          "user": {
            "_id": "66606a13fc6c0816442bd161",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66606a13fc6c0816442bd161/tS8pBDXEb3QkIjvZao55l.jpeg",
            "isPro": false,
            "fullname": "Xingyu Chen",
            "user": "rover-xingyu",
            "type": "user"
          },
          "name": "Xingyu Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-01T07:46:33.467Z",
          "hidden": false
        },
        {
          "_id": "67eb72a2291b56e50b66a064",
          "user": {
            "_id": "66f80281d88dc2ad510663e9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/5HV3mTu-cxPCnWlCi_2wB.jpeg",
            "isPro": false,
            "fullname": "Yue Chen",
            "user": "faneggg",
            "type": "user"
          },
          "name": "Yue Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-01T07:46:31.158Z",
          "hidden": false
        },
        {
          "_id": "67eb72a2291b56e50b66a065",
          "name": "Yuliang Xiu",
          "hidden": false
        },
        {
          "_id": "67eb72a2291b56e50b66a066",
          "name": "Andreas Geiger",
          "hidden": false
        },
        {
          "_id": "67eb72a2291b56e50b66a067",
          "name": "Anpei Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-31T17:59:58.000Z",
      "submittedOnDailyAt": "2025-04-01T07:01:19.970Z",
      "title": "Easi3R : Estimation de mouvements séparés du DUSt3R sans nécessité d'entraînement",
      "submittedOnDailyBy": {
        "_id": "66606a13fc6c0816442bd161",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66606a13fc6c0816442bd161/tS8pBDXEb3QkIjvZao55l.jpeg",
        "isPro": false,
        "fullname": "Xingyu Chen",
        "user": "rover-xingyu",
        "type": "user"
      },
      "summary": "Récemment, le développement de DUSt3R a permis une estimation forte des points et des paramètres de caméra dans des scènes statiques de haute densité. Ceci a été réalisé grâce à la structure de réseau de Transformer et à la normalisation directe des ensembles de données 3D. D'autre part, l'insuffisance et la diversité limitée des ensembles de données 4D agissent comme des grands limites pour atteindre un haut rendement de généralisation dans les modèles 4D. Cette restriction empêche d'appliquer des méthodes existantes 4D (comme, par exemple, les ajustements de modèles 3D aux données vidéo dynamiquement échelonnables, le flux optique, la profondeur et d'autres informations géométriques supplémentaires). Dans cette étude, nous avons suivi l'itinéraire inverse et nous sommes introduits Easi3R. Easi3R est un méthode simple et efficace pour la reconstruction 4D sans entraînement. Notre approche applique un attribut d'attention dans l'inférence, sans aucune nécessité de recommencer à partir des marques de glissement ou d'ajuster la réseau. Nous avons découvert que les couches d'attention de DUSt3R contiennent de riches informations sur le mouvement de la caméra et des objets. En séparant soigneusement ces cartes d'attention, nous avons réussi à la segmentation dynamique des zones, l'estimation de la posture de la caméra et la reconstruction de points de haute densité en 4D. Les expériences étendues sur des vidéos dynamiques réelles ont montré que notre léger attribut d'attention dépasse considérablement les méthodes les plus avancées précédentes, même entraînées. Notre code est disponible sur https://easi3r.github.io/ pour la recherche.",
      "upvotes": 2,
      "discussionId": "67eb72a5291b56e50b66a152",
      "ai_keywords": [
        "Transformer network architectures",
        "dense point clouds",
        "camera parameters",
        "direct supervision",
        "3D datasets",
        "4D datasets",
        "4D model",
        "fine-tune",
        "dynamic video data",
        "geometric priors",
        "optical flow",
        "depths",
        "training-free method",
        "4D reconstruction",
        "attention adaptation",
        "inference",
        "attention layers",
        "dynamic region segmentation",
        "camera pose estimation",
        "4D dense point map reconstruction"
      ]
    },
    "publishedAt": "2025-03-31T13:59:58.000Z",
    "title": "Easi3R: Estimating Disentangled Motion from DUSt3R Without Training",
    "summary": "Recent advances in DUSt3R have enabled robust estimation of dense point\nclouds and camera parameters of static scenes, leveraging Transformer network\narchitectures and direct supervision on large-scale 3D datasets. In contrast,\nthe limited scale and diversity of available 4D datasets present a major\nbottleneck for training a highly generalizable 4D model. This constraint has\ndriven conventional 4D methods to fine-tune 3D models on scalable dynamic video\ndata with additional geometric priors such as optical flow and depths. In this\nwork, we take an opposite path and introduce Easi3R, a simple yet efficient\ntraining-free method for 4D reconstruction. Our approach applies attention\nadaptation during inference, eliminating the need for from-scratch pre-training\nor network fine-tuning. We find that the attention layers in DUSt3R inherently\nencode rich information about camera and object motion. By carefully\ndisentangling these attention maps, we achieve accurate dynamic region\nsegmentation, camera pose estimation, and 4D dense point map reconstruction.\nExtensive experiments on real-world dynamic videos demonstrate that our\nlightweight attention adaptation significantly outperforms previous\nstate-of-the-art methods that are trained or finetuned on extensive dynamic\ndatasets. Our code is publicly available for research purpose at\nhttps://easi3r.github.io/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.24391.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66606a13fc6c0816442bd161",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66606a13fc6c0816442bd161/tS8pBDXEb3QkIjvZao55l.jpeg",
      "fullname": "Xingyu Chen",
      "name": "rover-xingyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.23730",
      "authors": [
        {
          "_id": "67eb567141abf40cd86e0e15",
          "user": {
            "_id": "67038a66eb760972bcb62c70",
            "avatarUrl": "/avatars/8cc82af8f11cae994bc83f4bd99b51bc.svg",
            "isPro": false,
            "fullname": "김윤식",
            "user": "yoonshik1205",
            "type": "user"
          },
          "name": "Yoonshik Kim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-01T07:46:53.729Z",
          "hidden": false
        },
        {
          "_id": "67eb567141abf40cd86e0e16",
          "user": {
            "_id": "646484cfb90150b2706df03b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646484cfb90150b2706df03b/8ocSbXBSbrruhlhcxwzEt.png",
            "isPro": true,
            "fullname": "Jaeyoon Jung",
            "user": "lastdefiance20",
            "type": "user"
          },
          "name": "Jaeyoon Jung",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-01T07:46:56.151Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-31T05:04:25.000Z",
      "submittedOnDailyAt": "2025-04-01T01:43:40.701Z",
      "title": "KOFFVQA : Benchmark de VQA libre évalué subjectivement en langue coréenne",
      "submittedOnDailyBy": {
        "_id": "646484cfb90150b2706df03b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646484cfb90150b2706df03b/8ocSbXBSbrruhlhcxwzEt.png",
        "isPro": true,
        "fullname": "Jaeyoon Jung",
        "user": "lastdefiance20",
        "type": "user"
      },
      "summary": "Récemment, des marques d'évaluation pour les grands modèles de vision et de langage (VLMs) ont émergé. Cependant, ces méthodes d'évaluation peuvent être subjectives et peu fiables lorsque les modèles doivent sélectionner des réponses prédéfinies ou lorsqu'un modèle de classification est utilisé pour évaluer. De plus, les marques d'évaluation pour les VLMs en coréen nécessitent un critère indépendant de l'anglais, mais ceux-ci n'existent pas. Par conséquent, il peut y avoir de grandes différences dans le rendement des modèles selon le langage. Par conséquent, on propose KOFFVQA, un marque d'évaluation général pour les questions visuelles sous forme libre en coréen. Ce marque inclut 275 problèmes conçus en détail, chacun relié à une image, et évalue 10 aspects différents de la performance des VLMs. Les critères d'évaluation abordent la faible fiabilité en évaluant chaque réponse selon des règles prédéfinies par un modèle de classification. En définissant les critères objectivement, il est possible que les petits modèles open-source puissent être évalués de manière fiable dans ce cadre. De plus, il a été confirmé expérimentalement que l'évaluation de la plupart des VLMs existants par ce méthode est beaucoup plus fiable que les méthodes actuelles. Le code d'évaluation est disponible sur https://github.com/maum-ai/KOFFVQA.",
      "upvotes": 2,
      "discussionId": "67eb567341abf40cd86e0e63",
      "githubRepo": "https://github.com/maum-ai/KOFFVQA",
      "ai_keywords": [
        "Large Vision-Language Models (VLMs)",
        "visual question answering benchmark",
        "grading criteria"
      ]
    },
    "publishedAt": "2025-03-31T01:04:25.000Z",
    "title": "KOFFVQA: An Objectively Evaluated Free-form VQA Benchmark for Large\n  Vision-Language Models in the Korean Language",
    "summary": "The recent emergence of Large Vision-Language Models(VLMs) has resulted in a\nvariety of different benchmarks for evaluating such models. Despite this, we\nobserve that most existing evaluation methods suffer from the fact that they\neither require the model to choose from pre-determined responses, sacrificing\nopen-endedness, or evaluate responses using a judge model, resulting in\nsubjective and unreliable evaluation. In addition, we observe a lack of\nbenchmarks for VLMs in the Korean language, which are necessary as a separate\nmetric from more common English language benchmarks, as the performance of\ngenerative language models can differ significantly based on the language being\nused. Therefore, we present KOFFVQA, a general-purpose free-form visual\nquestion answering benchmark in the Korean language for the evaluation of VLMs.\nOur benchmark consists of 275 carefully crafted questions each paired with an\nimage and grading criteria covering 10 different aspects of VLM performance.\nThe grading criteria eliminate the problem of unreliability by allowing the\njudge model to grade each response based on a pre-determined set of rules. By\ndefining the evaluation criteria in an objective manner, even a small\nopen-source model can be used to evaluate models on our benchmark reliably. In\naddition to evaluating a large number of existing VLMs on our benchmark, we\nalso experimentally verify that our method of using pre-existing grading\ncriteria for evaluation is much more reliable than existing methods. Our\nevaluation code is available at https://github.com/maum-ai/KOFFVQA",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.23730.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "646484cfb90150b2706df03b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646484cfb90150b2706df03b/8ocSbXBSbrruhlhcxwzEt.png",
      "fullname": "Jaeyoon Jung",
      "name": "lastdefiance20",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.23022",
      "authors": [
        {
          "_id": "67ebadecf9f9390b4cd1c6d9",
          "name": "Xianglong He",
          "hidden": false
        },
        {
          "_id": "67ebadecf9f9390b4cd1c6da",
          "name": "Junyi Chen",
          "hidden": false
        },
        {
          "_id": "67ebadecf9f9390b4cd1c6db",
          "name": "Di Huang",
          "hidden": false
        },
        {
          "_id": "67ebadecf9f9390b4cd1c6dc",
          "name": "Zexiang Liu",
          "hidden": false
        },
        {
          "_id": "67ebadecf9f9390b4cd1c6dd",
          "name": "Xiaoshui Huang",
          "hidden": false
        },
        {
          "_id": "67ebadecf9f9390b4cd1c6de",
          "name": "Wanli Ouyang",
          "hidden": false
        },
        {
          "_id": "67ebadecf9f9390b4cd1c6df",
          "name": "Chun Yuan",
          "hidden": false
        },
        {
          "_id": "67ebadecf9f9390b4cd1c6e0",
          "name": "Yangguang Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-29T09:21:50.000Z",
      "submittedOnDailyAt": "2025-04-01T07:42:31.137Z",
      "title": "MeshCraft : Recherche sur la génération efficace et contrôlable de maillages en utilisant des DiTs flexibles",
      "submittedOnDailyBy": {
        "_id": "64d71083a787c9bc7b9f1238",
        "avatarUrl": "/avatars/d0b0546dec7fc5792921154bec41385a.svg",
        "isPro": false,
        "fullname": "Yangguang Li",
        "user": "Lp256",
        "type": "user"
      },
      "summary": "Dans le domaine de la production de contenu 3D, atteindre la meilleure topologie de maillage avec des modèles d'IA a été un objectif poursuivi par les artistes 3D pendant de nombreuses années. Les méthodes passées ont essayé de créer des objets 3D prêts en utilisant l'apprentissage automatique comme le GPT de maillage. Ces méthodes ont réussi à obtenir des résultats visuellement impressionnants, mais dépendaient de prédictions par blocs de token dans le processus de récupération automatique, ce qui impliquait une vitesse de génération très lente et des limitations sur le nombre de surfaces de maillage. Dans cet article, on présente un nouveau cadre pour la génération efficace et contrôlée de maillages en utilisant la diffusion continue pour créer des surfaces de triangles continus. En particulier, Mesh Crafter est constitué de deux composants clés : 1) Le générateur basé sur les transformers de VAE transforme des structures hyperniques en tokens de niveau de surfaces continus et les retourne à la maillage original. 2) La transformation de diffusion basée sur le nombre de surfaces permet de générer des maillages de haute qualité avec un nombre de surfaces spécifique. Mesh Crafter utilise des modèles de diffusion pour générer la topologie de la maillage de la maille entière, atteignant une génération de maillages de haute qualité beaucoup plus rapide que les modèles de récupération automatique. En particulier, Mesh Crafter peut générer une maille de 800 surfaces en 3,2 secondes (35 fois plus rapide que les méthodes actuelles). Les expériences distribuées sur le jeu de données ShapeNet ont montré surpasser les techniques les plus avancées à travers des évaluations qualitatives et quantitatives, et ont également montré des résultats excellents sur le jeu de données Objaverse. De plus, il est possible d'intégrer avec des stratégies de guidage conditionnel et sans intervalle, libérant les artistes des tâches manuelles longues qui étaient consommées dans la génération de maillages.",
      "upvotes": 1,
      "discussionId": "67ebadeef9f9390b4cd1c7b3",
      "ai_keywords": [
        "mesh auto-regressive techniques",
        "continuous spatial diffusion",
        "transformer-based VAE",
        "flow-based diffusion transformer",
        "high-fidelity mesh generation",
        "ShapeNet dataset",
        "Objaverse dataset",
        "conditional guidance"
      ]
    },
    "publishedAt": "2025-03-29T05:21:50.000Z",
    "title": "MeshCraft: Exploring Efficient and Controllable Mesh Generation with\n  Flow-based DiTs",
    "summary": "In the domain of 3D content creation, achieving optimal mesh topology through\nAI models has long been a pursuit for 3D artists. Previous methods, such as\nMeshGPT, have explored the generation of ready-to-use 3D objects via mesh\nauto-regressive techniques. While these methods produce visually impressive\nresults, their reliance on token-by-token predictions in the auto-regressive\nprocess leads to several significant limitations. These include extremely slow\ngeneration speeds and an uncontrollable number of mesh faces. In this paper, we\nintroduce MeshCraft, a novel framework for efficient and controllable mesh\ngeneration, which leverages continuous spatial diffusion to generate discrete\ntriangle faces. Specifically, MeshCraft consists of two core components: 1) a\ntransformer-based VAE that encodes raw meshes into continuous face-level tokens\nand decodes them back to the original meshes, and 2) a flow-based diffusion\ntransformer conditioned on the number of faces, enabling the generation of\nhigh-quality 3D meshes with a predefined number of faces. By utilizing the\ndiffusion model for the simultaneous generation of the entire mesh topology,\nMeshCraft achieves high-fidelity mesh generation at significantly faster speeds\ncompared to auto-regressive methods. Specifically, MeshCraft can generate an\n800-face mesh in just 3.2 seconds (35times faster than existing baselines).\nExtensive experiments demonstrate that MeshCraft outperforms state-of-the-art\ntechniques in both qualitative and quantitative evaluations on ShapeNet dataset\nand demonstrates superior performance on Objaverse dataset. Moreover, it\nintegrates seamlessly with existing conditional guidance strategies, showcasing\nits potential to relieve artists from the time-consuming manual work involved\nin mesh creation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.23022.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64d71083a787c9bc7b9f1238",
      "avatarUrl": "/avatars/d0b0546dec7fc5792921154bec41385a.svg",
      "fullname": "Yangguang Li",
      "name": "Lp256",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.20286",
      "authors": [
        {
          "_id": "67eaa88c40bebc3127ade04c",
          "user": {
            "_id": "67e77099284080c98d8c9bfc",
            "avatarUrl": "/avatars/a3120e8d9b1312d8a670161b674f3196.svg",
            "isPro": false,
            "fullname": "Zhenyu Liang",
            "user": "ZhenyuLiang",
            "type": "user"
          },
          "name": "Zhenyu Liang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-31T15:15:16.646Z",
          "hidden": false
        },
        {
          "_id": "67eaa88c40bebc3127ade04d",
          "name": "Hao Li",
          "hidden": false
        },
        {
          "_id": "67eaa88c40bebc3127ade04e",
          "name": "Naiwei Yu",
          "hidden": false
        },
        {
          "_id": "67eaa88c40bebc3127ade04f",
          "name": "Kebin Sun",
          "hidden": false
        },
        {
          "_id": "67eaa88c40bebc3127ade050",
          "name": "Ran Cheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-26T07:30:23.000Z",
      "submittedOnDailyAt": "2025-04-01T00:34:02.763Z",
      "title": "Évolution de l'optimisation multiobjectif et accélération avec GPU en utilisant TensorRT",
      "submittedOnDailyBy": {
        "_id": "67e77099284080c98d8c9bfc",
        "avatarUrl": "/avatars/a3120e8d9b1312d8a670161b674f3196.svg",
        "isPro": false,
        "fullname": "Zhenyu Liang",
        "user": "ZhenyuLiang",
        "type": "user"
      },
      "summary": "L'évolution de l'algorithme d'optimisation multiobjectif (EMO) a connu un développement clair au cours des 20 dernières années. Cependant, avec l'augmentation de la taille et la complexité des problèmes, les algorithmes traditionnels d'EMO ont montré des limites dans leur capacité à s'écaller et à paralléliser. De nombreuses recherches ont été centrées sur la conception d'algorithmes capables de résoudre ces problèmes, mais une attention insuffisante a été accordée à l'architecture du matériel et à l'écart entre les algorithmes d'EMO et les dispositifs de calcul avancés tels que les GPU. Pour combler cette lacune, nous proposons la parallélisation des algorithmes d'EMO sur les GPU. En utilisant le méthode des tenseurs, nous avons transformé la structure de données et les opérations des algorithmes d'EMO en une représentation tensorielle claire, permettant l'utilisation automatique des calculs sur les GPU. Nous avons démontré l'effet de notre méthode en l'appliquant à trois algorithmes d'EMO représentatifs : NSGA-III, MOEA/D et HypE. Nous avons évalué notre méthode de manière global en introduisant un benchmark de contrôle de robots multiobjectifs en utilisant un refroidisseur physique accéléré sur les GPU. Les résultats des expériences montrent que les algorithmes d'EMO tensorisés atteignent une augmentation de vitesse de 1113 fois par rapport aux ordinateurs basés sur la CPU, tout en maintenant la qualité des solutions et en pouvant simplement augmenter la population de solutions en augmentant la quantité. De plus, les algorithmes tensorisés d'EMO résolvent efficacement des tâches de contrôle de robots multiobjectifs complexes, générant des solutions de haute qualité avec une variété d'actions. Le code source est disponible sur https://github.com/EMI-Group/evomo.",
      "upvotes": 1,
      "discussionId": "67eaa88e40bebc3127ade0eb",
      "githubRepo": "https://github.com/EMI-Group/evomo",
      "ai_keywords": [
        "evolutionary multiobjective optimization (EMO)",
        "parallelism",
        "scalability",
        "GPU",
        "tensorization",
        "tensor representations",
        "NSGA-III",
        "MOEA/D",
        "HypE",
        "GPU-accelerated physics engine",
        "multiobjective robot control benchmark",
        "population sizes",
        "high-quality solutions",
        "diverse behaviors"
      ]
    },
    "publishedAt": "2025-03-26T03:30:23.000Z",
    "title": "Bridging Evolutionary Multiobjective Optimization and GPU Acceleration\n  via Tensorization",
    "summary": "Evolutionary multiobjective optimization (EMO) has made significant strides\nover the past two decades. However, as problem scales and complexities\nincrease, traditional EMO algorithms face substantial performance limitations\ndue to insufficient parallelism and scalability. While most work has focused on\nalgorithm design to address these challenges, little attention has been given\nto hardware acceleration, thereby leaving a clear gap between EMO algorithms\nand advanced computing devices, such as GPUs. To bridge the gap, we propose to\nparallelize EMO algorithms on GPUs via the tensorization methodology. By\nemploying tensorization, the data structures and operations of EMO algorithms\nare transformed into concise tensor representations, which seamlessly enables\nautomatic utilization of GPU computing. We demonstrate the effectiveness of our\napproach by applying it to three representative EMO algorithms: NSGA-III,\nMOEA/D, and HypE. To comprehensively assess our methodology, we introduce a\nmultiobjective robot control benchmark using a GPU-accelerated physics engine.\nOur experiments show that the tensorized EMO algorithms achieve speedups of up\nto 1113x compared to their CPU-based counterparts, while maintaining solution\nquality and effectively scaling population sizes to hundreds of thousands.\nFurthermore, the tensorized EMO algorithms efficiently tackle complex\nmultiobjective robot control tasks, producing high-quality solutions with\ndiverse behaviors. Source codes are available at\nhttps://github.com/EMI-Group/evomo.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20286.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "67e77099284080c98d8c9bfc",
      "avatarUrl": "/avatars/a3120e8d9b1312d8a670161b674f3196.svg",
      "fullname": "Zhenyu Liang",
      "name": "ZhenyuLiang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.18225",
      "authors": [
        {
          "_id": "67ebabe3c545cab686735182",
          "name": "Massimo Bini",
          "hidden": false
        },
        {
          "_id": "67ebabe3c545cab686735183",
          "name": "Leander Girrbach",
          "hidden": false
        },
        {
          "_id": "67ebabe3c545cab686735184",
          "name": "Zeynep Akata",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-23T22:00:56.000Z",
      "submittedOnDailyAt": "2025-04-01T07:38:06.739Z",
      "title": "Réajuste des priorités : indépendance d'angle et intensité",
      "submittedOnDailyBy": {
        "_id": "63f62ee3b29015adc33aafa0",
        "avatarUrl": "/avatars/b7d7fe1b65333fb698402ed065fc5d13.svg",
        "isPro": false,
        "fullname": "Massimo Bini",
        "user": "mwbini",
        "type": "user"
      },
      "summary": "Le méthode de mise à jour paramètre efficace (Parameter-Efficient FineTuning, PEFT) a récemment gagné une popularité grâce à l'expansion des modèles d'apprentissage pré-entraînés à grande échelle. Ces méthodes permettent d'adapter rapidement des tâches de mise à jour en ligne avec des coûts computationnels minimisés. Cependant, des méthodes comme LoRA, bien connues, ont une robustesse limitée face à la sélection de hyperparamètres ou à des plans d'apprentissage à long terme, et ne peuvent pas montrer une amélioration de la performance de produit. En contraste, des approches restrictives comme ETHER peuvent améliorer la robustesse, mais sont limitées à des adaptations de très faible rang et des transformations avec une flexibilité fixe, ce qui réduit leur capacité d'adaptation. Dans cet article, nous proposons un nouveau méthode d'ajustement, DeLoRA (Découplée Adaptation de Basse Réalité), qui normalise et amplifie les matrices de basse réalité adaptables. DeLoRA limite la distance des transformations pour séparer l'apprentissage des angles du rendement de l'adaptation, améliorant ainsi la robustesse sans augmenter les coûts. Évaluée à travers la génération d'images dirigées par thèmes, la compréhension du langage naturel et les instructions de mise à jour, DeLoRA a démontré être aussi efficace que d'autres méthodes de PEFT, et dans certains cas, meilleure, montrant une robustesse élevée. Le code est disponible sur : https://github.com/ExplainableML/DeLoRA.",
      "upvotes": 1,
      "discussionId": "67ebabe5c545cab68673521b",
      "githubRepo": "https://github.com/ExplainableML/DeLoRA",
      "ai_keywords": [
        "Parameter-Efficient FineTuning (PEFT)",
        "LoRA",
        "bounded approaches",
        "ETHER",
        "Decoupled Low-rank Adaptation (DeLoRA)",
        "learnable low-rank matrices",
        "angular learning",
        "adaptation strength",
        "subject-driven image generation",
        "natural language understanding",
        "instruction tuning"
      ]
    },
    "publishedAt": "2025-03-23T18:00:56.000Z",
    "title": "Decoupling Angles and Strength in Low-rank Adaptation",
    "summary": "Parameter-Efficient FineTuning (PEFT) methods have recently gained\nsignificant popularity thanks to the widespread availability of large-scale\npretrained models. These methods allow for quick adaptation to downstream tasks\nwith minimal computational cost. However, popular finetuning methods such as\nLoRA exhibit limited robustness when it comes to hyperparameter choices or\nextended training regimes, preventing optimal out-of-the-box performance. In\ncontrast, bounded approaches, such as ETHER, provide greater robustness but are\nlimited to extremely low-rank adaptations and fixed-strength transformations,\nreducing their adaptation expressive power. In this work, we propose Decoupled\nLow-rank Adaptation (DeLoRA), a novel finetuning method that normalizes and\nscales learnable low-rank matrices. By bounding the distance of the\ntransformation, DeLoRA effectively decouples the angular learning from the\nadaptation strength, enhancing robustness without compromising performance.\nThrough evaluations on subject-driven image generation, natural language\nunderstanding, and instruction tuning, we show that DeLoRA matches or surpasses\nperformance of competing PEFT methods, while exhibiting stronger robustness.\nCode is available at https://github.com/ExplainableML/DeLoRA.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18225.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63f62ee3b29015adc33aafa0",
      "avatarUrl": "/avatars/b7d7fe1b65333fb698402ed065fc5d13.svg",
      "fullname": "Massimo Bini",
      "name": "mwbini",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.23913",
      "authors": [
        {
          "_id": "67ebaea15baac6e5085afcb9",
          "name": "Xiaoxuan Wang",
          "hidden": false
        },
        {
          "_id": "67ebaea15baac6e5085afcba",
          "name": "Yihe Deng",
          "hidden": false
        },
        {
          "_id": "67ebaea15baac6e5085afcbb",
          "name": "Mingyu Derek Ma",
          "hidden": false
        },
        {
          "_id": "67ebaea15baac6e5085afcbc",
          "name": "Wei Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-31T10:04:35.000Z",
      "submittedOnDailyAt": "2025-04-01T07:47:10.184Z",
      "title": "Histologie basée sur l'ajustement automatique des poids pour l'apprentissage autodidacte",
      "submittedOnDailyBy": {
        "_id": "64ba5946c0f19c9025665a3c",
        "avatarUrl": "/avatars/bb148094ce52f1f385d30968dc22e0e6.svg",
        "isPro": false,
        "fullname": "Xiaoxuan Wang",
        "user": "xw27",
        "type": "user"
      },
      "summary": "La capacité des modèles de langage à résoudre des problèmes mathématiques est un thème central de la recherche, et l'intérêt pour l'amélioration de ces modèles en utilisant des méthodes automatiques de génération de raisons a augmenté. Ces méthodes permettent de comprendre le processus logique des étapes qui mènent à la solution. Les méthodes d'apprentissage automatique sont efficaces pour des tâches de raisonnement sans besoin de modèles externes ou de manuelité et peuvent être mises en œuvre de cette manière. Cependant, l'optimisation de l'entraînement de modèles avec des données générées automatiquement est un problème ouvert. Dans cet article, nous proposons \"Entropie-Based Adaptive Weighting for Self-Training (EAST)\", une stratégie pour mettre en œuvre des poids adaptatifs dans l'apprentissage automatique. EAST priorise l'incertitude des données dans l'apprentissage automatique, appliquant des poids adaptatifs. Spécifiquement, EAST utilise une fonction de carte avec des paramètres d'ajustement pour contrôler les points de rupture des poids, attribuant des poids plus élevés aux données plus incertaines. Cette approche aide le modèle à se concentrer plus sur des exemples avec une plus grande quantité d'information et une plus grande difficulté, améliorant ainsi sa capacité de raisonnement. Les résultats des tests sur les benchmarks GSM8K et MATH montrent que le méthode virtuelle ne présente pas d'améliorations significatives sur MATH, tandis que EAST améliore le modèle de base d'environ 1%. Sur GSM8K, EAST atteint un amélioration de performance de 1 à 2% plus que le méthode virtuelle.",
      "upvotes": 0,
      "discussionId": "67ebaea25baac6e5085afcfe",
      "ai_keywords": [
        "Entropy-Based Adaptive Weighting for Self-Training (EAST)",
        "mapping function",
        "tunable parameter",
        "weighting strategy",
        "uncertainty",
        "informative examples",
        "challenging examples"
      ]
    },
    "publishedAt": "2025-03-31T06:04:35.000Z",
    "title": "Entropy-Based Adaptive Weighting for Self-Training",
    "summary": "The mathematical problem-solving capabilities of large language models have\nbecome a focal point of research, with growing interests in leveraging\nself-generated reasoning paths as a promising way to refine and enhance these\nmodels. These paths capture step-by-step logical processes while requiring only\nthe correct answer for supervision. The self-training method has been shown to\nbe effective in reasoning tasks while eliminating the need for external models\nand manual annotations. However, optimizing the use of self-generated data for\nmodel training remains an open challenge. In this work, we propose\nEntropy-Based Adaptive Weighting for Self-Training (EAST), an adaptive\nweighting strategy designed to prioritize uncertain data during self-training.\nSpecifically, EAST employs a mapping function with a tunable parameter that\ncontrols the sharpness of the weighting, assigning higher weights to data where\nthe model exhibits greater uncertainty. This approach guides the model to focus\non more informative and challenging examples, thereby enhancing its reasoning\nability. We evaluate our approach on GSM8K and MATH benchmarks. Empirical\nresults show that, while the vanilla method yields virtually no improvement\n(0%) on MATH, EAST achieves around a 1% gain over backbone model. On GSM8K,\nEAST attains a further 1-2% performance boost compared to the vanilla method.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.23913.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ba5946c0f19c9025665a3c",
      "avatarUrl": "/avatars/bb148094ce52f1f385d30968dc22e0e6.svg",
      "fullname": "Xiaoxuan Wang",
      "name": "xw27",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  }
]