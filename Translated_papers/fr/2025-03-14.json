[
  {
    "paper": {
      "id": "2503.10613",
      "authors": [
        {
          "_id": "67d393ca336d57afb21bbf63",
          "user": {
            "_id": "67a99ec47b754f038d110926",
            "avatarUrl": "/avatars/e1ff318a42ccb75b094bbe7dae0cabec.svg",
            "isPro": false,
            "fullname": "Advait Gupta",
            "user": "advaitgupta",
            "type": "user"
          },
          "name": "Advait Gupta",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-14T08:56:36.855Z",
          "hidden": false
        },
        {
          "_id": "67d393ca336d57afb21bbf64",
          "user": {
            "_id": "672f89e6d7f4171f374dacea",
            "avatarUrl": "/avatars/4a8b378e13e862586bb428fdf000b3cc.svg",
            "isPro": false,
            "fullname": "NandaKiran Velaga",
            "user": "nandakiran09",
            "type": "user"
          },
          "name": "NandaKiran Velaga",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-14T08:56:34.327Z",
          "hidden": false
        },
        {
          "_id": "67d393ca336d57afb21bbf65",
          "name": "Dang Nguyen",
          "hidden": false
        },
        {
          "_id": "67d393ca336d57afb21bbf66",
          "user": {
            "_id": "647f5af5b0e96764589f3b2a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
            "isPro": false,
            "fullname": "Tianyi Zhou",
            "user": "zhoutianyi",
            "type": "user"
          },
          "name": "Tianyi Zhou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-14T08:56:39.157Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-13T17:55:45.000Z",
      "submittedOnDailyAt": "2025-03-14T01:33:20.201Z",
      "title": "CoSTAast : Agente de segmentation côtier et agente de logiciel pour éditer des images en temps réel.",
      "submittedOnDailyBy": {
        "_id": "647f5af5b0e96764589f3b2a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
        "isPro": false,
        "fullname": "Tianyi Zhou",
        "user": "zhoutianyi",
        "type": "user"
      },
      "summary": "Les modèles texte à image comme Stable Diffusion et DALLE-3 révèlent des difficultés dans l'édition d'images multiniveaux. On considère la possibilité de décomposer cette tâche dans un flux de travail d'agents efficaces, résolvant plusieurs sous-tâches séquentiellement. Les algorithmes de recherche traditionnels nécessitent une recherche coûteuse pour trouver les étapes des outils. D'autre part, les modèles de langage grands (LLMs) ont des connaissances préalables sur la planification des sous-tâches, mais ils ne savent pas si ils peuvent évaluer de manière précise la capacité et le coût des outils dans chaque sous-tâche. On cherche si on peut combiner les avantages des LLMs et l'algorithme de recherche de graphes pour trouver des étapes d'outils coûte-efficaces. On propose un approche tridimensionnelle appelée \"CoSTA*\" qui divise le travail en trois étapes : construire un arbre de sous-tâches en utilisant les LLMs, réduire le graphe des outils liés au travail, et ensuite effectuer une recherche de A* dans des petits sous-graphes. Pour équilibrer mieux le coût et la qualité du travail complet, CoSTA* intègre deux critères d'évaluation des outils dans chaque sous-tâche et guide la recherche de A*. Le résultat de chaque sous-tâche est évalué par un modèle de langage visuel (VLM), et si il échoue, on actualise le coût et la qualité des outils. Ainsi, la recherche de A* peut être rapidement rétablie d'un échouement et explorer d'autres étapes. De plus, CoSTA* change automatiquement la modélisation entre sous-tâches pour atteindre un meilleur équilibre de coût et qualité. Un nouveau benchmark d'édition d'images multiniveaux a été construit, et CoSTA* dépasse les modèles d'édition d'images les plus avancés et les agents en termes de coût et qualité, réalisant différents équilibres adaptés aux préférences de l'utilisateur.",
      "upvotes": 32,
      "discussionId": "67d393cf336d57afb21bc0db",
      "githubRepo": "https://github.com/tianyi-lab/CoSTAR",
      "ai_keywords": [
        "text-to-image models",
        "stable diffusion",
        "DALLE-3",
        "multi-turn image editing",
        "agentic workflow",
        "tool use",
        "subtasks",
        "AI tools",
        "cost-efficient",
        "large language models (LLMs)",
        "subtask planning",
        "graph search",
        "three-stage approach",
        "CoSTA*",
        "subtask tree",
        "pruning",
        "A* search",
        "subgraph",
        "cost-quality trade-off",
        "vision-language model (VLM)",
        "failure",
        "total cost",
        "quality",
        "modality switching",
        "benchmark",
        "state-of-the-art image-editing models",
        "user preference"
      ]
    },
    "publishedAt": "2025-03-13T13:55:45.000Z",
    "title": "CoSTAast: Cost-Sensitive Toolpath Agent for Multi-turn Image Editing",
    "summary": "Text-to-image models like stable diffusion and DALLE-3 still struggle with\nmulti-turn image editing. We decompose such a task as an agentic workflow\n(path) of tool use that addresses a sequence of subtasks by AI tools of varying\ncosts. Conventional search algorithms require expensive exploration to find\ntool paths. While large language models (LLMs) possess prior knowledge of\nsubtask planning, they may lack accurate estimations of capabilities and costs\nof tools to determine which to apply in each subtask. Can we combine the\nstrengths of both LLMs and graph search to find cost-efficient tool paths? We\npropose a three-stage approach \"CoSTA*\" that leverages LLMs to create a subtask\ntree, which helps prune a graph of AI tools for the given task, and then\nconducts A* search on the small subgraph to find a tool path. To better balance\nthe total cost and quality, CoSTA* combines both metrics of each tool on every\nsubtask to guide the A* search. Each subtask's output is then evaluated by a\nvision-language model (VLM), where a failure will trigger an update of the\ntool's cost and quality on the subtask. Hence, the A* search can recover from\nfailures quickly to explore other paths. Moreover, CoSTA* can automatically\nswitch between modalities across subtasks for a better cost-quality trade-off.\nWe build a novel benchmark of challenging multi-turn image editing, on which\nCoSTA* outperforms state-of-the-art image-editing models or agents in terms of\nboth cost and quality, and performs versatile trade-offs upon user preference.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10613.png",
    "numComments": 7,
    "submittedBy": {
      "_id": "647f5af5b0e96764589f3b2a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
      "fullname": "Tianyi Zhou",
      "name": "zhoutianyi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.10480",
      "authors": [
        {
          "_id": "67d38a42d3d16e1166d81bed",
          "user": {
            "_id": "64c3c631e77ea9f28111172a",
            "avatarUrl": "/avatars/495dbb73b69c399bae780da3118e332f.svg",
            "isPro": false,
            "fullname": "Siyin Wang",
            "user": "sinwang",
            "type": "user"
          },
          "name": "Siyin Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-14T08:56:44.686Z",
          "hidden": false
        },
        {
          "_id": "67d38a42d3d16e1166d81bee",
          "user": {
            "_id": "629ef8544313a7c1dd671130",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/629ef8544313a7c1dd671130/i5xfHIgELcuO1Ew19ebTw.png",
            "isPro": false,
            "fullname": "Zhaoye Fei",
            "user": "ngc7293",
            "type": "user"
          },
          "name": "Zhaoye Fei",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-14T09:05:04.607Z",
          "hidden": false
        },
        {
          "_id": "67d38a42d3d16e1166d81bef",
          "name": "Qinyuan Cheng",
          "hidden": false
        },
        {
          "_id": "67d38a42d3d16e1166d81bf0",
          "user": {
            "_id": "64196e45060a651c415d5cf7",
            "avatarUrl": "/avatars/71a43232a7bae851eca252782387a63d.svg",
            "isPro": false,
            "fullname": "Shiduo Zhang",
            "user": "CyberDJ",
            "type": "user"
          },
          "name": "Shiduo Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-14T09:05:20.440Z",
          "hidden": false
        },
        {
          "_id": "67d38a42d3d16e1166d81bf1",
          "name": "Panpan Cai",
          "hidden": false
        },
        {
          "_id": "67d38a42d3d16e1166d81bf2",
          "user": {
            "_id": "618497ea8aaadc9253c2dfa9",
            "avatarUrl": "/avatars/2eb3954a99f5aede6f31b8ae49b8c910.svg",
            "isPro": false,
            "fullname": "Fu Jinlan",
            "user": "Jinlan",
            "type": "user"
          },
          "name": "Jinlan Fu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-14T09:05:38.981Z",
          "hidden": false
        },
        {
          "_id": "67d38a42d3d16e1166d81bf3",
          "user": {
            "_id": "61457b8deff2c9fdb4de4988",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1632381702899-61457b8deff2c9fdb4de4988.jpeg",
            "isPro": false,
            "fullname": "Xipeng Qiu",
            "user": "xpqiu",
            "type": "user"
          },
          "name": "Xipeng Qiu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-14T09:05:46.041Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-13T15:49:56.000Z",
      "submittedOnDailyAt": "2025-03-14T01:42:40.120Z",
      "title": "Le monde du modélisage peut créer des planificateurs plus performants : optimisation de deux préférences dans la planification de tâches structurées.",
      "submittedOnDailyBy": {
        "_id": "64c3c631e77ea9f28111172a",
        "avatarUrl": "/avatars/495dbb73b69c399bae780da3118e332f.svg",
        "isPro": false,
        "fullname": "Siyin Wang",
        "user": "sinwang",
        "type": "user"
      },
      "summary": "Le développement récent des grands modèles de langue visuelle et linguistique (LVLMs) a montré la possibilité de potentiels pour des tâches de planification concrètes, mais il en face des limites de dépendance et des problèmes d'efficience. Les méthodes existantes ne sont que des optimisations des choix d'action ou utilisent des modèles du monde pendant l'inférence, mais ne profitent pas des avantages de modéliser le monde pour améliorer la capacité de planification. Nous proposons un nouveau cadre d'apprentissage \"Dual Preference Optimization (D^2PO)\", qui optimise la prédiction de l'état et la sélection d'action de manière conjointe par apprentissage. De cette manière, les LVLMs peuvent comprendre la dynamique de l'environnement et réaliser de meilleurs plans. En plus du feedback humain, les données de routes et de préférences d'étapes sont collectées automatiquement en utilisant une structure de recherche d'arbre pour amplifier l'exploration. Les expériences étendues réalisées sur VoTa-Bench montrent que le méthode basée sur D^2PO dépasse considérablement les méthodes actuelles et GPT-4o, en atteignant une forte taux de succès dans la tâche, accompagnée de des trajets d'exécution efficaces.",
      "upvotes": 25,
      "discussionId": "67d38a44d3d16e1166d81c54",
      "ai_keywords": [
        "Dual Preference Optimization (D$^2$PO)",
        "preference learning",
        "state prediction",
        "action selection",
        "environment dynamics",
        "tree search mechanism",
        "VoTa-Bench",
        "Qwen2-VL",
        "LLaVA-1.6",
        "LLaMA-3.2",
        "task success rates",
        "efficient execution paths"
      ]
    },
    "publishedAt": "2025-03-13T11:49:56.000Z",
    "title": "World Modeling Makes a Better Planner: Dual Preference Optimization for\n  Embodied Task Planning",
    "summary": "Recent advances in large vision-language models (LVLMs) have shown promise\nfor embodied task planning, yet they struggle with fundamental challenges like\ndependency constraints and efficiency. Existing approaches either solely\noptimize action selection or leverage world models during inference,\noverlooking the benefits of learning to model the world as a way to enhance\nplanning capabilities. We propose Dual Preference Optimization (D^2PO), a new\nlearning framework that jointly optimizes state prediction and action selection\nthrough preference learning, enabling LVLMs to understand environment dynamics\nfor better planning. To automatically collect trajectories and stepwise\npreference data without human annotation, we introduce a tree search mechanism\nfor extensive exploration via trial-and-error. Extensive experiments on\nVoTa-Bench demonstrate that our D^2PO-based method significantly outperforms\nexisting methods and GPT-4o when applied to Qwen2-VL (7B), LLaVA-1.6 (7B), and\nLLaMA-3.2 (11B), achieving superior task success rates with more efficient\nexecution paths.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10480.png",
    "numComments": 4,
    "submittedBy": {
      "_id": "64c3c631e77ea9f28111172a",
      "avatarUrl": "/avatars/495dbb73b69c399bae780da3118e332f.svg",
      "fullname": "Siyin Wang",
      "name": "sinwang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.09669",
      "authors": [
        {
          "_id": "67d37754e07f664c7325f236",
          "user": {
            "_id": "63bbf972d8d676a2299cdb44",
            "avatarUrl": "/avatars/cd038f11dc1007b1267324b34c165dda.svg",
            "isPro": false,
            "fullname": "Sangwon",
            "user": "agwmon",
            "type": "user"
          },
          "name": "Sangwon Jang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-14T08:56:49.038Z",
          "hidden": false
        },
        {
          "_id": "67d37754e07f664c7325f237",
          "user": {
            "_id": "66c6edcc91dced946471bc13",
            "avatarUrl": "/avatars/55cc8593da6540e1566e1de9d7133f9f.svg",
            "isPro": false,
            "fullname": "June Suk Choi",
            "user": "wchoi403",
            "type": "user"
          },
          "name": "June Suk Choi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-14T09:05:59.636Z",
          "hidden": false
        },
        {
          "_id": "67d37754e07f664c7325f238",
          "user": {
            "_id": "65e5bd4568234ef5d6decadc",
            "avatarUrl": "/avatars/c41095a946c0176b949c0b3566136c05.svg",
            "isPro": false,
            "fullname": "Jaehyeong Jo",
            "user": "harryjo97",
            "type": "user"
          },
          "name": "Jaehyeong Jo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-14T09:06:19.773Z",
          "hidden": false
        },
        {
          "_id": "67d37754e07f664c7325f239",
          "user": {
            "_id": "635097ec59bfa9a85d4207b2",
            "avatarUrl": "/avatars/787085894e9e6538b6b3e3051efe9eea.svg",
            "isPro": false,
            "fullname": "Kimin Lee",
            "user": "kiminle2",
            "type": "user"
          },
          "name": "Kimin Lee",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-14T09:06:26.649Z",
          "hidden": false
        },
        {
          "_id": "67d37754e07f664c7325f23a",
          "name": "Sung Ju Hwang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-12T17:21:57.000Z",
      "submittedOnDailyAt": "2025-03-14T02:05:42.787Z",
      "title": "L'attaque de marqueur silencieux : attaque de poisonnement de données sans déclencheur contre un modèle de diffusion d'image à partir de texte",
      "submittedOnDailyBy": {
        "_id": "63bbf972d8d676a2299cdb44",
        "avatarUrl": "/avatars/cd038f11dc1007b1267324b34c165dda.svg",
        "isPro": false,
        "fullname": "Sangwon",
        "user": "agwmon",
        "type": "user"
      },
      "summary": "Les modèles qui se diffusent sous forme d'images à partir du texte ont connu un succès notable dans la génération de contenu de haute qualité à partir du texte. Cependant, la dépendance aux données disponibles et l'augmentation du partage de données rendent ces modèles particulièrement vulnérables aux attaques de population de données. Dans cet article, nous présentons un nouveau méthode de population de données appelé «attaque du marqueur silencieux». Ce méthode manipule les modèles qui se diffusent sous forme d'images à partir du texte pour générer des images qui incluent des logos ou des symboles d'une marque, même sans nécessité d'un trigger de texte. Nous avons découvert que les modèles apprennent naturellement à répliquer des motifs visuels répétés dans les données d'entraînement. Nous avons utilisé cela pour développer un algorithme automatique de population de données et nous avons ajouté des motifs sans interrompre la détection, sans ajouter de marques aux images originales. Avec cette base de données de population, les modèles entraînés peuvent générer des images qui incluent des marques, évitant la dégradation de la qualité de l'image et le désordre du texte. Nous avons expérimenté que notre méthode atteint de hauts rendements dans deux configurations pratiques : un grand ensemble d'images de haute qualité et un ensemble de données de style pour les narrations. Nous avons démontré, y compris des évaluations humaines et des métriques quantitatives de détection de logos, que notre méthode peut insérer des marques sans interrompre la détection.",
      "upvotes": 25,
      "discussionId": "67d37759e07f664c7325f3c5",
      "projectPage": "https://silent-branding.github.io/",
      "ai_keywords": [
        "text-to-image diffusion models",
        "high-quality contents",
        "text prompts",
        "data poisoning attacks",
        "Silent Branding Attack",
        "visual patterns",
        "data poisoning algorithm",
        "logos",
        "style personalization datasets",
        "logo detection"
      ]
    },
    "publishedAt": "2025-03-12T13:21:57.000Z",
    "title": "Silent Branding Attack: Trigger-free Data Poisoning Attack on\n  Text-to-Image Diffusion Models",
    "summary": "Text-to-image diffusion models have achieved remarkable success in generating\nhigh-quality contents from text prompts. However, their reliance on publicly\navailable data and the growing trend of data sharing for fine-tuning make these\nmodels particularly vulnerable to data poisoning attacks. In this work, we\nintroduce the Silent Branding Attack, a novel data poisoning method that\nmanipulates text-to-image diffusion models to generate images containing\nspecific brand logos or symbols without any text triggers. We find that when\ncertain visual patterns are repeatedly in the training data, the model learns\nto reproduce them naturally in its outputs, even without prompt mentions.\nLeveraging this, we develop an automated data poisoning algorithm that\nunobtrusively injects logos into original images, ensuring they blend naturally\nand remain undetected. Models trained on this poisoned dataset generate images\ncontaining logos without degrading image quality or text alignment. We\nexperimentally validate our silent branding attack across two realistic\nsettings on large-scale high-quality image datasets and style personalization\ndatasets, achieving high success rates even without a specific text trigger.\nHuman evaluation and quantitative metrics including logo detection show that\nour method can stealthily embed logos.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.09669.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63bbf972d8d676a2299cdb44",
      "avatarUrl": "/avatars/cd038f11dc1007b1267324b34c165dda.svg",
      "fullname": "Sangwon",
      "name": "agwmon",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.10639",
      "authors": [
        {
          "_id": "67d3a632db36a4d5d95dbcff",
          "user": {
            "_id": "65b8724123d948d884b379b1",
            "avatarUrl": "/avatars/ce189d1d8d688c17912f9b869035b2d0.svg",
            "isPro": false,
            "fullname": "Rongyao Fang",
            "user": "LucasFang",
            "type": "user"
          },
          "name": "Rongyao Fang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-14T08:56:14.110Z",
          "hidden": false
        },
        {
          "_id": "67d3a632db36a4d5d95dbd00",
          "user": {
            "_id": "64a2b496e2e19de17db7de65",
            "avatarUrl": "/avatars/241448ca487833d6cc5d57bb1fdb6ee5.svg",
            "isPro": false,
            "fullname": "Duan Chengqi",
            "user": "gogoduan",
            "type": "user"
          },
          "name": "Chengqi Duan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-14T09:07:01.612Z",
          "hidden": false
        },
        {
          "_id": "67d3a632db36a4d5d95dbd01",
          "name": "Kun Wang",
          "hidden": false
        },
        {
          "_id": "67d3a632db36a4d5d95dbd02",
          "user": {
            "_id": "65fc7c824d36be78e66ba92d",
            "avatarUrl": "/avatars/d4a55c820cae533f91724e062427516a.svg",
            "isPro": false,
            "fullname": "Linjiang Huang",
            "user": "LjHuang",
            "type": "user"
          },
          "name": "Linjiang Huang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-14T09:07:08.707Z",
          "hidden": false
        },
        {
          "_id": "67d3a632db36a4d5d95dbd03",
          "name": "Hao Li",
          "hidden": false
        },
        {
          "_id": "67d3a632db36a4d5d95dbd04",
          "user": {
            "_id": "65273fea0ef49cfb783fa5c1",
            "avatarUrl": "/avatars/0c9e204bc2151c8cc533311900d05a36.svg",
            "isPro": false,
            "fullname": "shilinyan",
            "user": "shilinyan",
            "type": "user"
          },
          "name": "Shilin Yan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-14T09:07:18.212Z",
          "hidden": false
        },
        {
          "_id": "67d3a632db36a4d5d95dbd05",
          "name": "Hao Tian",
          "hidden": false
        },
        {
          "_id": "67d3a632db36a4d5d95dbd06",
          "user": {
            "_id": "666d4a0fe70e5838d95aebee",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/6dkjoFA_sOjCkjvcvozZ5.jpeg",
            "isPro": false,
            "fullname": "zengxingyu",
            "user": "zengxingyu",
            "type": "user"
          },
          "name": "Xingyu Zeng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-14T09:07:42.264Z",
          "hidden": false
        },
        {
          "_id": "67d3a632db36a4d5d95dbd07",
          "name": "Rui Zhao",
          "hidden": false
        },
        {
          "_id": "67d3a632db36a4d5d95dbd08",
          "user": {
            "_id": "64686f7172d9180d4ac8b4e4",
            "avatarUrl": "/avatars/db67dd6c4b2b41054ddcce5a18ade6f8.svg",
            "isPro": false,
            "fullname": "Jifeng Dai",
            "user": "daijifeng",
            "type": "user"
          },
          "name": "Jifeng Dai",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-14T09:14:59.460Z",
          "hidden": false
        },
        {
          "_id": "67d3a632db36a4d5d95dbd09",
          "user": {
            "_id": "65d5ec74cd05bc1eaa125040",
            "avatarUrl": "/avatars/2de1b1539a86452c2c89570eeb02f5ab.svg",
            "isPro": false,
            "fullname": "Xihui Liu",
            "user": "XihuiLiu",
            "type": "user"
          },
          "name": "Xihui Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-14T09:14:50.924Z",
          "hidden": false
        },
        {
          "_id": "67d3a632db36a4d5d95dbd0a",
          "user": {
            "_id": "65c04e9c27a5fdca81abcbd9",
            "avatarUrl": "/avatars/12a155683c824fa23da4a9e2bed4f64e.svg",
            "isPro": false,
            "fullname": "Hongsheng LI",
            "user": "hsli-cuhk",
            "type": "user"
          },
          "name": "Hongsheng Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-14T09:14:43.402Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-13T17:59:59.000Z",
      "submittedOnDailyAt": "2025-03-14T02:16:04.349Z",
      "title": "GoT : Liberté de la capacité logique pour la génération et l'édition visuelles avec un grand modèle de langage multimodal",
      "submittedOnDailyBy": {
        "_id": "65b8724123d948d884b379b1",
        "avatarUrl": "/avatars/ce189d1d8d688c17912f9b869035b2d0.svg",
        "isPro": false,
        "fullname": "Rongyao Fang",
        "user": "LucasFang",
        "type": "user"
      },
      "summary": "Actualement, les méthodes de génération et d'édition d'images traitent directement les textes de prompts sans considérer les raisons visuelles et spécifiques de la composition et de la manipulation. Nous proposons un nouveau paradigme appelé \"Generation Chain-of-Thought (GoT)\", qui permet de générer et d'éditer des images via un processus de raisonnement linguistique explicite. Cette approche améliore le valeur par un cadre de guidance de raisonnement qui analyse les relations significatives et la position spatiale. Nous définissons la normalisation de GoT et construisons un ensemble de données de GoT à grande échelle, qui comprend plus de 9M échantillons avec des chaînes de raisonnement spécifiques pour comprendre les relations significatives-spatiales. Pour exploiter les excellences de GoT, nous implémentons une série de cadres qui intègrent la génération de chaînes de raisonnement avec Qwen2.5-VL et le nouveau modèle de guidance significative-spatiale, ce qui permet d'étendre le modèle de la fin à la début. Les expériences ont démontré un grand améliorament dans les tâches de génération et d'édition, permettant un processus de raisonnement linguistique explicite et la génération de visualisations interactives qui ajustent précisément les images. GoT ouvre une nouvelle direction dans la génération et l'édition de visualisations, offrant une façon de créer des images qui correspondent mieux aux intentions humaines. Pour les études futures, nous publions l'ensemble de données, le code et les modèles pré-entraînés.",
      "upvotes": 21,
      "discussionId": "67d3a636db36a4d5d95dbdeb",
      "githubRepo": "https://github.com/rongyaofang/GoT",
      "ai_keywords": [
        "Generation Chain-of-Thought (GoT)",
        "text-to-image generation",
        "editing tasks",
        "reasoning chain generation",
        "end-to-end diffusion model",
        "Semantic-Spatial Guidance Module",
        "semantic relationships",
        "spatial arrangements",
        "large-scale GoT datasets",
        "detailed reasoning chains",
        "semantic-spatial relationships",
        "interactive visual generation",
        "reasoning steps",
        "human intent"
      ]
    },
    "publishedAt": "2025-03-13T13:59:59.000Z",
    "title": "GoT: Unleashing Reasoning Capability of Multimodal Large Language Model\n  for Visual Generation and Editing",
    "summary": "Current image generation and editing methods primarily process textual\nprompts as direct inputs without reasoning about visual composition and\nexplicit operations. We present Generation Chain-of-Thought (GoT), a novel\nparadigm that enables generation and editing through an explicit language\nreasoning process before outputting images. This approach transforms\nconventional text-to-image generation and editing into a reasoning-guided\nframework that analyzes semantic relationships and spatial arrangements. We\ndefine the formulation of GoT and construct large-scale GoT datasets containing\nover 9M samples with detailed reasoning chains capturing semantic-spatial\nrelationships. To leverage the advantages of GoT, we implement a unified\nframework that integrates Qwen2.5-VL for reasoning chain generation with an\nend-to-end diffusion model enhanced by our novel Semantic-Spatial Guidance\nModule. Experiments show our GoT framework achieves excellent performance on\nboth generation and editing tasks, with significant improvements over\nbaselines. Additionally, our approach enables interactive visual generation,\nallowing users to explicitly modify reasoning steps for precise image\nadjustments. GoT pioneers a new direction for reasoning-driven visual\ngeneration and editing, producing images that better align with human intent.\nTo facilitate future research, we make our datasets, code, and pretrained\nmodels publicly available at https://github.com/rongyaofang/GoT.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10639.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65b8724123d948d884b379b1",
      "avatarUrl": "/avatars/ce189d1d8d688c17912f9b869035b2d0.svg",
      "fullname": "Rongyao Fang",
      "name": "LucasFang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.10633",
      "authors": [
        {
          "_id": "67d3ba5e4d3a41ed9f8651eb",
          "user": {
            "_id": "630dd4218df86f1e5beb2ed7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/630dd4218df86f1e5beb2ed7/fKvNWyWv6CVBdbXXUlrYv.jpeg",
            "isPro": false,
            "fullname": "Eliahu Horwitz",
            "user": "Eliahu",
            "type": "user"
          },
          "name": "Eliahu Horwitz",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-14T08:56:04.270Z",
          "hidden": false
        },
        {
          "_id": "67d3ba5e4d3a41ed9f8651ec",
          "user": {
            "_id": "674ec6d1ce68874ee4f2d53b",
            "avatarUrl": "/avatars/4c15c9bdcf51d4bf5e6fceb86195e480.svg",
            "isPro": false,
            "fullname": "Nitzan Kurer",
            "user": "nitzankur",
            "type": "user"
          },
          "name": "Nitzan Kurer",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-14T10:09:18.093Z",
          "hidden": false
        },
        {
          "_id": "67d3ba5e4d3a41ed9f8651ed",
          "user": {
            "_id": "6465fd33dac127ac80f0b334",
            "avatarUrl": "/avatars/113f02c1b1f8d33d3487daa867afcd3f.svg",
            "isPro": false,
            "fullname": "Jonathan Kahana",
            "user": "jonkahana",
            "type": "user"
          },
          "name": "Jonathan Kahana",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-14T10:09:25.338Z",
          "hidden": false
        },
        {
          "_id": "67d3ba5e4d3a41ed9f8651ee",
          "user": {
            "_id": "669ffff5944b597ce2a1aa5b",
            "avatarUrl": "/avatars/559ca0ad82b1a52208510f09492fafa6.svg",
            "isPro": false,
            "fullname": "Liel Amar",
            "user": "LielAmar",
            "type": "user"
          },
          "name": "Liel Amar",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-14T08:56:01.634Z",
          "hidden": false
        },
        {
          "_id": "67d3ba5e4d3a41ed9f8651ef",
          "user": {
            "_id": "646cfc3b4220471ca0c56b20",
            "avatarUrl": "/avatars/19d6ab141ec2cd25c1c3b45fd8f69910.svg",
            "isPro": false,
            "fullname": "Yedid Hoshen",
            "user": "yedid",
            "type": "user"
          },
          "name": "Yedid Hoshen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-14T10:09:39.859Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/630dd4218df86f1e5beb2ed7/HClm12KfVuYMozbJaIp9_.png"
      ],
      "publishedAt": "2025-03-13T17:59:53.000Z",
      "submittedOnDailyAt": "2025-03-14T03:51:41.703Z",
      "title": "Créez un diagramme et une navigation pour le modèle AtoRA de la Huezing Face.",
      "submittedOnDailyBy": {
        "_id": "630dd4218df86f1e5beb2ed7",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/630dd4218df86f1e5beb2ed7/fKvNWyWv6CVBdbXXUlrYv.jpeg",
        "isPro": false,
        "fullname": "Eliahu Horwitz",
        "user": "Eliahu",
        "type": "user"
      },
      "summary": "Actuellement, il existe des millions de réseaux neuronaux disponibles publiquement, ce qui a transformé la recherche et l'analyse de leurs dossiers en une tâche cruciale. Pour sélectionner ces modèles de diverses manières, il est nécessaire d'un atlas, mais la plupart d'entre eux ont peu d'explications, rendant ainsi la construction de l'atlas difficile. Pour explorer le potentiel des dossiers de modèles, nous avons créé un atlas initial incluant des descriptions sur Hugging Face. Ce atlas permet de visualiser la structure et l'évolution des modèles de manière impressionnante. Nous montrons dans cet atlas comment les attributs des modèles (par exemple, la précision) peuvent être prédits ou les tendances dans les modèles de vision par ordinateur analysées. Cependant, l'atlas actuel n'est pas complet, et nous proposons un méthode pour décrire les zones non expliquées. En particulier, nous identifions des structures structurales de haute confiance basées sur des processus d'apprentissage réalistes et nous utilisons ces structures pour cartographier précisément les parties de l'atlas qui n'avaient pas été expliquées précédemment. Nous publions des ensembles de données, du code et un atlas interactif.",
      "upvotes": 20,
      "discussionId": "67d3ba634d3a41ed9f86533a",
      "projectPage": "https://horwitz.ai/model-atlas",
      "githubRepo": "https://github.com/eliahuhorwitz/Model-Atlas",
      "ai_keywords": [
        "neural networks",
        "model repositories",
        "atlas",
        "model landscape",
        "model evolution",
        "predicting model attributes",
        "trends in computer vision models",
        "high-confidence structural priors",
        "dominant real-world model training practices",
        "interactive atlas"
      ]
    },
    "publishedAt": "2025-03-13T13:59:53.000Z",
    "title": "Charting and Navigating Hugging Face's Model Atlas",
    "summary": "As there are now millions of publicly available neural networks, searching\nand analyzing large model repositories becomes increasingly important.\nNavigating so many models requires an atlas, but as most models are poorly\ndocumented charting such an atlas is challenging. To explore the hidden\npotential of model repositories, we chart a preliminary atlas representing the\ndocumented fraction of Hugging Face. It provides stunning visualizations of the\nmodel landscape and evolution. We demonstrate several applications of this\natlas including predicting model attributes (e.g., accuracy), and analyzing\ntrends in computer vision models. However, as the current atlas remains\nincomplete, we propose a method for charting undocumented regions.\nSpecifically, we identify high-confidence structural priors based on dominant\nreal-world model training practices. Leveraging these priors, our approach\nenables accurate mapping of previously undocumented areas of the atlas. We\npublicly release our datasets, code, and interactive atlas.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/630dd4218df86f1e5beb2ed7/HClm12KfVuYMozbJaIp9_.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10633.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "630dd4218df86f1e5beb2ed7",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/630dd4218df86f1e5beb2ed7/fKvNWyWv6CVBdbXXUlrYv.jpeg",
      "fullname": "Eliahu Horwitz",
      "name": "Eliahu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.09662",
      "authors": [
        {
          "_id": "67d3daf40034469b0d6cc872",
          "name": "Shitong Shao",
          "hidden": false
        },
        {
          "_id": "67d3daf40034469b0d6cc873",
          "name": "Zikai Zhou",
          "hidden": false
        },
        {
          "_id": "67d3daf40034469b0d6cc874",
          "name": "Dian Xie",
          "hidden": false
        },
        {
          "_id": "67d3daf40034469b0d6cc875",
          "name": "Yuetong Fang",
          "hidden": false
        },
        {
          "_id": "67d3daf40034469b0d6cc876",
          "name": "Tian Ye",
          "hidden": false
        },
        {
          "_id": "67d3daf40034469b0d6cc877",
          "name": "Lichen Bai",
          "hidden": false
        },
        {
          "_id": "67d3daf40034469b0d6cc878",
          "name": "Zeke Xie",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-12T15:15:25.000Z",
      "submittedOnDailyAt": "2025-03-14T06:07:49.038Z",
      "title": "Kore^2 : Récupération, réflexion, réinitialisation pour générer plus rapidement des nombres aléatoires les plus optimaux.",
      "submittedOnDailyBy": {
        "_id": "66015e8aa4d296af07de538e",
        "avatarUrl": "/avatars/a1295c631cc2646282c545859975ce4c.svg",
        "isPro": false,
        "fullname": "Ye",
        "user": "Owen777",
        "type": "user"
      },
      "summary": "Améliorer la génération de modèles d'images à partir du texte (T2I) dans des directions rapides et de haute qualité est une orientation de recherche prometteuse, reconnue comme l'une des lignes de travail les plus représentatives. Les études précédentes ont sacrifié l'efficacité de l'échantillonnage pour améliorer la qualité visuelle des images synthétiques ou ont accéléré significativement l'échantillonnage sans améliorer la capacité générative du modèle de base. De plus, la plupart des méthodes d'inférence n'ont pas réussi à garantir un rendement stable tant dans les modèles de diffusion (DMs) que dans les modèles auto-régressifs de vision (ARMs).\n\nDans cet article, nous présentons un nouveau paradigme d'inférence basé sur des plugins et des paramètres appelé CoRe^2. CoRe^2 est composé de trois processus : Collection, Réflexion et Raffinage. Initialement, CoRe^2 collecte la trajectoire de rétroaction d'un guide (CFG) sans restrictions de filtrage de classe, et utilise ces données pour réfléchir aux contenus faciles à apprendre, réduisant ainsi la quantité d'évaluations fonctionnelles lors de l'inférence. Ensuite, CoRe^2 améliore les sorties conditionnelles à partir d'un guide faible vers un guide fort, et améliore la capacité de génération de contenus de haute fréquence et réalistes qui sont faciles à comprendre par le modèle de base.\n\nMalgré ses limites connues, CoRe^2 est le premier méthode qui a démontré qu'il peut montrer à la fois une efficacité et une efficience dans une large gamme de modèles, y compris les DMs comme SDXL, SD3.5 et FLUX, et les ARMs comme LlamaGen. Un améliorament significatif a été observé dans les tests comme HPD v2, Pick-of-Pic, Drawbench, GenEval et T2I-Compbench. De plus, CoRe^2 permet l'intégration de la meilleure technique de sampling Z et la combinaison de la pureté, améliorant les scores PickScore et AES de 0.3 et 0.16 respectivement, et réduisant le temps de génération en SD3.5 de 5.64 secondes. Le code est disponible sur https://github.com/xie-lab-ml/CoRe/tree/main.",
      "upvotes": 20,
      "discussionId": "67d3dafb0034469b0d6ccac0",
      "ai_keywords": [
        "diffusion models (DMs)",
        "visual autoregressive models (ARMs)",
        "classifier-free guidance (CFG)",
        "HPD v2",
        "Pick-of-Pic",
        "Drawbench",
        "GenEval",
        "T2I-Compbench",
        "PickScore",
        "AES",
        "Z-Sampling",
        "SDXL",
        "SD3.5",
        "FLUX",
        "LlamaGen"
      ]
    },
    "publishedAt": "2025-03-12T11:15:25.000Z",
    "title": "CoRe^2: Collect, Reflect and Refine to Generate Better and Faster",
    "summary": "Making text-to-image (T2I) generative model sample both fast and well\nrepresents a promising research direction. Previous studies have typically\nfocused on either enhancing the visual quality of synthesized images at the\nexpense of sampling efficiency or dramatically accelerating sampling without\nimproving the base model's generative capacity. Moreover, nearly all inference\nmethods have not been able to ensure stable performance simultaneously on both\ndiffusion models (DMs) and visual autoregressive models (ARMs). In this paper,\nwe introduce a novel plug-and-play inference paradigm, CoRe^2, which comprises\nthree subprocesses: Collect, Reflect, and Refine. CoRe^2 first collects\nclassifier-free guidance (CFG) trajectories, and then use collected data to\ntrain a weak model that reflects the easy-to-learn contents while reducing\nnumber of function evaluations during inference by half. Subsequently, CoRe^2\nemploys weak-to-strong guidance to refine the conditional output, thereby\nimproving the model's capacity to generate high-frequency and realistic\ncontent, which is difficult for the base model to capture. To the best of our\nknowledge, CoRe^2 is the first to demonstrate both efficiency and effectiveness\nacross a wide range of DMs, including SDXL, SD3.5, and FLUX, as well as ARMs\nlike LlamaGen. It has exhibited significant performance improvements on HPD v2,\nPick-of-Pic, Drawbench, GenEval, and T2I-Compbench. Furthermore, CoRe^2 can be\nseamlessly integrated with the state-of-the-art Z-Sampling, outperforming it by\n0.3 and 0.16 on PickScore and AES, while achieving 5.64s time saving using\nSD3.5.Code is released at https://github.com/xie-lab-ml/CoRe/tree/main.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.09662.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "66015e8aa4d296af07de538e",
      "avatarUrl": "/avatars/a1295c631cc2646282c545859975ce4c.svg",
      "fullname": "Ye",
      "name": "Owen777",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.10622",
      "authors": [
        {
          "_id": "67d3b0a87443e648e8aa1ea6",
          "user": {
            "_id": "6552126dd8a8835b66653767",
            "avatarUrl": "/avatars/0b1dad9ebaeada8f5e7ebe453123960b.svg",
            "isPro": false,
            "fullname": "Jiachen Zhu",
            "user": "JiachenZhu",
            "type": "user"
          },
          "name": "Jiachen Zhu",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-03-14T05:13:31.648Z",
          "hidden": false
        },
        {
          "_id": "67d3b0a87443e648e8aa1ea7",
          "name": "Xinlei Chen",
          "hidden": false
        },
        {
          "_id": "67d3b0a87443e648e8aa1ea8",
          "name": "Kaiming He",
          "hidden": false
        },
        {
          "_id": "67d3b0a87443e648e8aa1ea9",
          "name": "Yann LeCun",
          "hidden": false
        },
        {
          "_id": "67d3b0a87443e648e8aa1eaa",
          "name": "Zhuang Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-13T17:59:06.000Z",
      "submittedOnDailyAt": "2025-03-14T02:59:49.783Z",
      "title": "Transformers sans Normalisation",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Les couches de normalisation ont été considérées essentielles dans les réseaux neuronaux modernes, bien que leur utilisation ait diminué au fil du temps. Cet article montre quelqu'un peut atteindre ou dépasser le rendement d'un Transformer avec des couches de normalisation en utilisant une technique très simple. On présente le Dynamic Tanh (DyT), un élément qui fonctionne dynamiquement dans DyT(x) = tanh(alpha x), remplaçant le dropout des couches de normalisation dans un Transformer. Le DyT a été conçu en se basant sur l'observation que les couches de normalisation dans un Transformer génèrent un mapping S-formé d'entrée-sortie, et a été adapté pour avoir les caractéristiques de la fonction tanh. En appliquant le DyT, un Transformer sans couches de normalisation peut dépasser presque sans ajustements de paramètres le rendement d'un Transformer avec des couches de normalisation. Les effets du DyT sur un Transformer ont été vérifiés dans différentes configurations, comme l'apprentissage génératif, supervisé, automatique, vision par ordinateur et modèles de langage. Ces résultats soulèvent des doutes sur l'idée traditionnelle selon laquelle les couches de normalisation sont essentielles dans les réseaux neuronaux modernes, et fournissent une nouvelle perspective sur le rôle des couches de normalisation dans les réseaux neuronaux profonds.",
      "upvotes": 16,
      "discussionId": "67d3b0a97443e648e8aa1f22",
      "ai_keywords": [
        "Dynamic Tanh (DyT)",
        "Transformers",
        "normalization layers",
        "layer normalization",
        "hyperparameter tuning",
        "supervised learning",
        "self-supervised learning",
        "computer vision",
        "language models"
      ]
    },
    "publishedAt": "2025-03-13T13:59:06.000Z",
    "title": "Transformers without Normalization",
    "summary": "Normalization layers are ubiquitous in modern neural networks and have long\nbeen considered essential. This work demonstrates that Transformers without\nnormalization can achieve the same or better performance using a remarkably\nsimple technique. We introduce Dynamic Tanh (DyT), an element-wise operation\nDyT(x) = tanh(alpha x), as a drop-in replacement for normalization\nlayers in Transformers. DyT is inspired by the observation that layer\nnormalization in Transformers often produces tanh-like, S-shaped input-output\nmappings. By incorporating DyT, Transformers without normalization can match or\nexceed the performance of their normalized counterparts, mostly without\nhyperparameter tuning. We validate the effectiveness of Transformers with DyT\nacross diverse settings, ranging from recognition to generation, supervised to\nself-supervised learning, and computer vision to language models. These\nfindings challenge the conventional understanding that normalization layers are\nindispensable in modern neural networks, and offer new insights into their role\nin deep networks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10622.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6364
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.10596",
      "authors": [
        {
          "_id": "67d3a8950ada3dfbf617fc23",
          "name": "Rui Hu",
          "hidden": false
        },
        {
          "_id": "67d3a8950ada3dfbf617fc24",
          "name": "Lianghui Zhu",
          "hidden": false
        },
        {
          "_id": "67d3a8950ada3dfbf617fc25",
          "name": "Yuxuan Zhang",
          "hidden": false
        },
        {
          "_id": "67d3a8950ada3dfbf617fc26",
          "name": "Tianheng Cheng",
          "hidden": false
        },
        {
          "_id": "67d3a8950ada3dfbf617fc27",
          "name": "Lei Liu",
          "hidden": false
        },
        {
          "_id": "67d3a8950ada3dfbf617fc28",
          "name": "Heng Liu",
          "hidden": false
        },
        {
          "_id": "67d3a8950ada3dfbf617fc29",
          "name": "Longjin Ran",
          "hidden": false
        },
        {
          "_id": "67d3a8950ada3dfbf617fc2a",
          "name": "Xiaoxin Chen",
          "hidden": false
        },
        {
          "_id": "67d3a8950ada3dfbf617fc2b",
          "name": "Wenyu Liu",
          "hidden": false
        },
        {
          "_id": "67d3a8950ada3dfbf617fc2c",
          "name": "Xinggang Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-13T17:43:10.000Z",
      "submittedOnDailyAt": "2025-03-14T02:31:30.611Z",
      "title": "GroundingSuite : Évaluation de la Détection de Pixels en Multigranularité",
      "submittedOnDailyBy": {
        "_id": "646b3db131968a60a01e4cf5",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646b3db131968a60a01e4cf5/DhfdqUYQaD1Qa8Svw996J.jpeg",
        "isPro": false,
        "fullname": "Tianheng Cheng",
        "user": "wondervictor",
        "type": "user"
      },
      "summary": "Pixel grounding est un domaine qui a récemment connu beaucoup d'intérêt en raison de son potentiel pour combler la lacune entre modèles de vision et de langage, y compris des tâches telles que la Segmentation d'Expressions Référentes (RES). Cependant, son développement actuel est limité par des catégories d'objets restreintes, la faible diversité de contextes et la manque de descriptions de haute qualité. Pour atténuer ces limitations, nous présentons GroundingSuite. GroundingSuite comprend trois composants : 1) un cadre de travail pour décrire des données en utilisant des agents de modèles de langage et de vision, 2) un ensemble de données d'entraînement de 9,56 millions d'expressions référentes et leurs segmentations correspondantes, et 3) un cadre d'évaluation benchmark de base de données avec 3,800 images. L'ensemble de données d'entraînement de GroundingSuite est conçu pour permettre aux modèles entraînés d'atteindre des résultats plus récents. En particulier, on a atteint un cIoU de 68,9 sur gRefCOCO et un gIoU de 55,3 sur RefCOCOm. De plus, le cadre de travail de description de GroundingSuite est 4,5 fois plus efficace que les méthodes de description actuelles, ce qui affecte directement la vitesse et la qualité de la tâche de description.",
      "upvotes": 15,
      "discussionId": "67d3a8960ada3dfbf617fc8d",
      "ai_keywords": [
        "Referring Expression Segmentation (RES)",
        "Vision-Language Model (VLM)",
        "GroundingSuite",
        "cIoU",
        "gIoU",
        "gRefCOCO",
        "RefCOCOm",
        "GLaMM"
      ]
    },
    "publishedAt": "2025-03-13T13:43:10.000Z",
    "title": "GroundingSuite: Measuring Complex Multi-Granular Pixel Grounding",
    "summary": "Pixel grounding, encompassing tasks such as Referring Expression Segmentation\n(RES), has garnered considerable attention due to its immense potential for\nbridging the gap between vision and language modalities. However, advancements\nin this domain are currently constrained by limitations inherent in existing\ndatasets, including limited object categories, insufficient textual diversity,\nand a scarcity of high-quality annotations. To mitigate these limitations, we\nintroduce GroundingSuite, which comprises: (1) an automated data annotation\nframework leveraging multiple Vision-Language Model (VLM) agents; (2) a\nlarge-scale training dataset encompassing 9.56 million diverse referring\nexpressions and their corresponding segmentations; and (3) a meticulously\ncurated evaluation benchmark consisting of 3,800 images. The GroundingSuite\ntraining dataset facilitates substantial performance improvements, enabling\nmodels trained on it to achieve state-of-the-art results. Specifically, a cIoU\nof 68.9 on gRefCOCO and a gIoU of 55.3 on RefCOCOm. Moreover, the\nGroundingSuite annotation framework demonstrates superior efficiency compared\nto the current leading data annotation method, i.e., 4.5 times faster than\nthe GLaMM.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10596.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "646b3db131968a60a01e4cf5",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646b3db131968a60a01e4cf5/DhfdqUYQaD1Qa8Svw996J.jpeg",
      "fullname": "Tianheng Cheng",
      "name": "wondervictor",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 27
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.10351",
      "authors": [
        {
          "_id": "67d39b35acb72b994659d4fd",
          "name": "Sinuo Liu",
          "hidden": false
        },
        {
          "_id": "67d39b35acb72b994659d4fe",
          "name": "Chenyang Lyu",
          "hidden": false
        },
        {
          "_id": "67d39b35acb72b994659d4ff",
          "name": "Minghao Wu",
          "hidden": false
        },
        {
          "_id": "67d39b35acb72b994659d500",
          "name": "Longyue Wang",
          "hidden": false
        },
        {
          "_id": "67d39b35acb72b994659d501",
          "name": "Weihua Luo",
          "hidden": false
        },
        {
          "_id": "67d39b35acb72b994659d502",
          "name": "Kaifu Zhang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6527d8b077bceabaab382a75/3_8muRazw1wwHmG9IxRGk.png"
      ],
      "publishedAt": "2025-03-13T13:27:53.000Z",
      "submittedOnDailyAt": "2025-03-14T01:29:07.562Z",
      "title": "Nouvelles tendances en traduction automatique moderne : introduction aux méthodes utilisant des modèles logiques d'échelle grande.",
      "submittedOnDailyBy": {
        "_id": "6527d8b077bceabaab382a75",
        "avatarUrl": "/avatars/69caacf9153dbf6a3796693a968b363f.svg",
        "isPro": false,
        "fullname": "Chenyang Lyu",
        "user": "ChenyangLyu",
        "type": "user"
      },
      "summary": "Récemment, le développement de grands modèles d'inférence (LRMs) a ouvert de nouvelles perspectives dans le domaine de la traduction automatique (MT), notamment grâce à l'inférence de chaîne de pensée (CoT). Cet article soutient que les LRMs ont réorganisé la tâche de traduction en tant qu'entendement contextuel, culturel et linguistique, ainsi qu'en tant que tâche dynamique d'inférence, modifiant significativement le paradigme traditionnel de MT basé sur des réseaux neuronaux ou des grands modèles de langage (LLMs). Dans ce sens, nous mettons en avant trois changements fondamentaux : 1) La cohérence contextuelle, car les LRMs peuvent inférer explicitement sur des phrases complexes ou des contextes, ou la manque de contexte, résolvant les incertitudes et maintenant la structure logique ; 2) L'intentionnalité culturelle, car les modèles peuvent adapter leur sortie à l'intention de l'interlocuteur, aux attentes du récepteur et aux règles de langage social ; 3) L'auto-reconnaissance, car les LRMs peuvent corriger les erreurs potentielles dans la traduction et montrent une plus grande robustesse, surtout dans des situations très bruyantes. Nous avons étudié divers scénarios de traduction, comme la traduction stylisée, la traduction au niveau documentaire et la traduction avec diversité de modèles, démontrant la supériorité des LRMs par des expériences. De plus, nous mettons en lumière des phénomènes intéressants en traduction, comme la traduction automatique des fibres, la sur-expansion régionale de la traduction et l'efficacité de l'inférence. En conclusion, nous pensons que les LRMs redéfiniront la traduction pas seulement comme un simple traducteur de texte, mais comme un équipe cognitif multilingue, en considérant le problème dans un contexte plus large et ouvrant de nouvelles possibilités. Ce changement de paradigme encourage à considérer les problèmes de traduction dans un contexte plus large, ouvrant des voies pour explorer de nouvelles possibilités.",
      "upvotes": 14,
      "discussionId": "67d39b40acb72b994659d916",
      "ai_keywords": [
        "Chain-of-Thought reasoning (CoT)",
        "Large Reasoning Models (LRMs)",
        "Neural MT",
        "Contextual coherence",
        "Cultural intentionality",
        "Self-reflection",
        "Stylized translation",
        "Document-level translation",
        "Multimodal translation",
        "Auto-pivot translation",
        "Over-localisation",
        "Inference efficiency",
        "Multilingual cognitive agents"
      ]
    },
    "publishedAt": "2025-03-13T09:27:53.000Z",
    "title": "New Trends for Modern Machine Translation with Large Reasoning Models",
    "summary": "Recent advances in Large Reasoning Models (LRMs), particularly those\nleveraging Chain-of-Thought reasoning (CoT), have opened brand new possibility\nfor Machine Translation (MT). This position paper argues that LRMs\nsubstantially transformed traditional neural MT as well as LLMs-based MT\nparadigms by reframing translation as a dynamic reasoning task that requires\ncontextual, cultural, and linguistic understanding and reasoning. We identify\nthree foundational shifts: 1) contextual coherence, where LRMs resolve\nambiguities and preserve discourse structure through explicit reasoning over\ncross-sentence and complex context or even lack of context; 2) cultural\nintentionality, enabling models to adapt outputs by inferring speaker intent,\naudience expectations, and socio-linguistic norms; 3) self-reflection, LRMs can\nperform self-reflection during the inference time to correct the potential\nerrors in translation especially extremely noisy cases, showing better\nrobustness compared to simply mapping X->Y translation. We explore various\nscenarios in translation including stylized translation, document-level\ntranslation and multimodal translation by showcasing empirical examples that\ndemonstrate the superiority of LRMs in translation. We also identify several\ninteresting phenomenons for LRMs for MT including auto-pivot translation as\nwell as the critical challenges such as over-localisation in translation and\ninference efficiency. In conclusion, we think that LRMs redefine translation\nsystems not merely as text converters but as multilingual cognitive agents\ncapable of reasoning about meaning beyond the text. This paradigm shift reminds\nus to think of problems in translation beyond traditional translation scenarios\nin a much broader context with LRMs - what we can achieve on top of it.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6527d8b077bceabaab382a75/3_8muRazw1wwHmG9IxRGk.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10351.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6527d8b077bceabaab382a75",
      "avatarUrl": "/avatars/69caacf9153dbf6a3796693a968b363f.svg",
      "fullname": "Chenyang Lyu",
      "name": "ChenyangLyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.04723",
      "authors": [
        {
          "_id": "67d39576de5ce3cc428b1909",
          "name": "Yuhao Wu",
          "hidden": false
        },
        {
          "_id": "67d39576de5ce3cc428b190a",
          "name": "Yushi Bai",
          "hidden": false
        },
        {
          "_id": "67d39576de5ce3cc428b190b",
          "user": {
            "_id": "637f228152229c63921119c3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637f228152229c63921119c3/acwXorra1r9_7i3KlBFjS.jpeg",
            "isPro": false,
            "fullname": "Zhiqiang Hu",
            "user": "Zhiqiang007",
            "type": "user"
          },
          "name": "Zhiqing Hu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-14T08:56:31.682Z",
          "hidden": false
        },
        {
          "_id": "67d39576de5ce3cc428b190c",
          "name": "Shangqing Tu",
          "hidden": false
        },
        {
          "_id": "67d39576de5ce3cc428b190d",
          "name": "Ming Shan Hee",
          "hidden": false
        },
        {
          "_id": "67d39576de5ce3cc428b190e",
          "name": "Juanzi Li",
          "hidden": false
        },
        {
          "_id": "67d39576de5ce3cc428b190f",
          "name": "Roy Ka-Wei Lee",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-06T18:59:37.000Z",
      "submittedOnDailyAt": "2025-03-14T01:04:48.148Z",
      "title": "1. La transition de l'étude des LLMs de long contexte du input au output\n2. La transition de l'étude des LLMs de long contexte vers l'output\n3. La transition de l'étude des LLMs de long contexte du input au output\n4. La transition de l'étude des LLMs de long contexte du input au output\n5. La transition de l'étude des LLMs de long contexte vers l'output\n6. La transition de l'étude des LLMs de long contexte vers l'output\n7. La transition de l'étude des LLMs de long contexte vers l'output\n8. La transition de l'étude des LLMs de long contexte vers l'output\n9. La transition de l'étude des LLMs de long contexte vers l'output\n10. La transition de l'étude des LLMs de long contexte vers l'output",
      "submittedOnDailyBy": {
        "_id": "63369da91ba5d5ece24118a4",
        "avatarUrl": "/avatars/67889e1ecadb04100a77bc8b5284c6fd.svg",
        "isPro": false,
        "fullname": "wuyuhao",
        "user": "mozhu",
        "type": "user"
      },
      "summary": "Récemment, le développement des modèles de langue de contexte long (LLM) a concentré son attention sur la compréhension de grands contextes, réalisant des progrès considérables dans ce domaine, à l'exception de la génération de sorties dans des contextes longs, qui a reçu moins d'attention. Cet article se concentre sur le changement du paradigme de la recherche en IANLP et sur la résolution des défis liés à la génération de sorties dans des contextes longs. De nouvelles tâches nécessitant de rédiger des textes longs, une planification à long terme et des explications complexes exigent que les modèles comprennent des contextes vastes et génèrent des textes longs cohérents, riches en contexte et harmonieux. Ces exigences révèlent des limites importantes dans les capacités actuelles des LLM. Ce travail souligne l'importance d'explorer ce domaine peu exploré et demande un effort concentré pour le développement de LLM capables de générer des sorties de haute qualité dans des contextes longs. Ces modèles ont un grand potentiel pour des applications réelles.",
      "upvotes": 12,
      "discussionId": "67d39577de5ce3cc428b194f",
      "ai_keywords": [
        "long-context Large Language Models (LLMs)",
        "long-context comprehension",
        "long-output generation",
        "novel writing",
        "long-term planning",
        "complex reasoning",
        "coherent",
        "contextually rich",
        "logically consistent",
        "extended text",
        "high-quality",
        "long-form outputs"
      ]
    },
    "publishedAt": "2025-03-06T13:59:37.000Z",
    "title": "Shifting Long-Context LLMs Research from Input to Output",
    "summary": "Recent advancements in long-context Large Language Models (LLMs) have\nprimarily concentrated on processing extended input contexts, resulting in\nsignificant strides in long-context comprehension. However, the equally\ncritical aspect of generating long-form outputs has received comparatively less\nattention. This paper advocates for a paradigm shift in NLP research toward\naddressing the challenges of long-output generation. Tasks such as novel\nwriting, long-term planning, and complex reasoning require models to understand\nextensive contexts and produce coherent, contextually rich, and logically\nconsistent extended text. These demands highlight a critical gap in current LLM\ncapabilities. We underscore the importance of this under-explored domain and\ncall for focused efforts to develop foundational LLMs tailored for generating\nhigh-quality, long-form outputs, which hold immense potential for real-world\napplications.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04723.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63369da91ba5d5ece24118a4",
      "avatarUrl": "/avatars/67889e1ecadb04100a77bc8b5284c6fd.svg",
      "fullname": "wuyuhao",
      "name": "mozhu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.10582",
      "authors": [
        {
          "_id": "67d387ff45b17e31c16d05d1",
          "name": "Yiming Jia",
          "hidden": false
        },
        {
          "_id": "67d387ff45b17e31c16d05d2",
          "name": "Jiachen Li",
          "hidden": false
        },
        {
          "_id": "67d387ff45b17e31c16d05d3",
          "name": "Xiang Yue",
          "hidden": false
        },
        {
          "_id": "67d387ff45b17e31c16d05d4",
          "name": "Bo Li",
          "hidden": false
        },
        {
          "_id": "67d387ff45b17e31c16d05d5",
          "name": "Ping Nie",
          "hidden": false
        },
        {
          "_id": "67d387ff45b17e31c16d05d6",
          "name": "Kai Zou",
          "hidden": false
        },
        {
          "_id": "67d387ff45b17e31c16d05d7",
          "user": {
            "_id": "6313a86154e6e5d9f0f94e04",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
            "isPro": false,
            "fullname": "Wenhu Chen",
            "user": "wenhu",
            "type": "user"
          },
          "name": "Wenhu Chen",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-14T01:36:13.720Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6313a86154e6e5d9f0f94e04/VBzj4fQkEBEzfx26BsANS.png"
      ],
      "publishedAt": "2025-03-13T17:32:48.000Z",
      "submittedOnDailyAt": "2025-03-14T00:47:38.699Z",
      "title": "VisualWebInstruct : Expansion des données d'instructions multimodales par recherche web",
      "submittedOnDailyBy": {
        "_id": "6313a86154e6e5d9f0f94e04",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
        "isPro": false,
        "fullname": "Wenhu Chen",
        "user": "wenhu",
        "type": "user"
      },
      "summary": "Du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur des thèmes similaires, du à la pénurie de jeux de données sur",
      "upvotes": 9,
      "discussionId": "67d3880d45b17e31c16d09d1",
      "projectPage": "https://tiger-ai-lab.github.io/VisualWebInstruct/",
      "githubRepo": "https://github.com/TIGER-AI-Lab/VisualWebInstruct",
      "ai_keywords": [
        "Vision-Language Models",
        "VisualWebInstruct",
        "search engine",
        "question-answer pairs",
        "visual QA pairs",
        "text QA pairs",
        "fine-tuned",
        "Llava-OV-mid",
        "MAmmoTH-VL",
        "MAmmoTH-VL2",
        "MMMU-Pro-std",
        "MathVerse",
        "DynaMath"
      ]
    },
    "publishedAt": "2025-03-13T13:32:48.000Z",
    "title": "VisualWebInstruct: Scaling up Multimodal Instruction Data through Web\n  Search",
    "summary": "Vision-Language Models have made significant progress on many\nperception-focused tasks, however, their progress on reasoning-focused tasks\nseem to be limited due to the lack of high-quality and diverse training data.\nIn this work, we aim to address the scarcity issue of reasoning-focused\nmultimodal datasets. We propose VisualWebInstruct - a novel approach that\nleverages search engine to create a diverse, and high-quality dataset spanning\nmultiple disciplines like math, physics, finance, chemistry, etc. Starting with\nmeticulously selected 30,000 seed images, we employ Google Image search to\nidentify websites containing similar images. We collect and process the HTMLs\nfrom over 700K unique URL sources. Through a pipeline of content extraction,\nfiltering and synthesis, we build a dataset of approximately 900K\nquestion-answer pairs, with 40% being visual QA pairs and the rest as text QA\npairs. Models fine-tuned on VisualWebInstruct demonstrate significant\nperformance gains: (1) training from Llava-OV-mid shows 10-20% absolute point\ngains across benchmarks, (2) training from MAmmoTH-VL shows 5% absoluate gain.\nOur best model MAmmoTH-VL2 shows state-of-the-art performance within the 10B\nparameter class on MMMU-Pro-std (40.7%), MathVerse (42.6%), and DynaMath\n(55.7%). These remarkable results highlight the effectiveness of our dataset in\nenhancing VLMs' reasoning capabilities for complex multimodal tasks.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6313a86154e6e5d9f0f94e04/VBzj4fQkEBEzfx26BsANS.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10582.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6313a86154e6e5d9f0f94e04",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
      "fullname": "Wenhu Chen",
      "name": "wenhu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 33
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.10618",
      "authors": [
        {
          "_id": "67d3d2dec4a225b653154b3a",
          "name": "Chen Chen",
          "hidden": false
        },
        {
          "_id": "67d3d2dec4a225b653154b3b",
          "name": "Rui Qian",
          "hidden": false
        },
        {
          "_id": "67d3d2dec4a225b653154b3c",
          "name": "Wenze Hu",
          "hidden": false
        },
        {
          "_id": "67d3d2dec4a225b653154b3d",
          "name": "Tsu-Jui Fu",
          "hidden": false
        },
        {
          "_id": "67d3d2dec4a225b653154b3e",
          "name": "Lezhi Li",
          "hidden": false
        },
        {
          "_id": "67d3d2dec4a225b653154b3f",
          "name": "Bowen Zhang",
          "hidden": false
        },
        {
          "_id": "67d3d2dec4a225b653154b40",
          "name": "Alex Schwing",
          "hidden": false
        },
        {
          "_id": "67d3d2dec4a225b653154b41",
          "name": "Wei Liu",
          "hidden": false
        },
        {
          "_id": "67d3d2dec4a225b653154b42",
          "name": "Yinfei Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-13T17:57:25.000Z",
      "submittedOnDailyAt": "2025-03-14T05:26:46.588Z",
      "title": "DiT-Air : Réévaluation de l'efficacité, étude sur le design de la génération d'images à partir du texte",
      "submittedOnDailyBy": {
        "_id": "656c2fa772c19de72367bd69",
        "avatarUrl": "/avatars/540bb3d8a2afe2ef927b80d895cae28b.svg",
        "isPro": false,
        "fullname": "Alex Yang",
        "user": "yyf86",
        "type": "user"
      },
      "summary": "Dans cette étude, une recherche expérimentale a été menée pour la génération d'images à partir de texte en utilisant des Transformers de Diffusion (DiTs), avec un accent sur la sélection structurale, les stratégies de conditionnement textuel et les protocoles d'entraînement. Des structures différentes basées sur les DiTs ont été évaluées, et la structure standard DiTs, qui traite directement le texte et le bruit d'entrée, a été comparée. Il a été constaté que le rendement de la DiTs standard est relativement élevé par rapport aux modèles spécialisés, surtout en termes d'efficacité en paramètres. En utilisant une stratégie de partage de paramètres par couches, on a réussi à réduire la taille du modèle de 66% par rapport à MMDiT, avec un impact minimal sur le rendement. Une analyse détaillée des composants importants tels que l'encodeur de texte et les Auto-Encodeurs Variatio-nels (VAEs) a été présentée, et DiT-Air et DiT-Air-Lite ont été introduits. DiT-Air a atteint le meilleur rendement sur GenEval et T2I CompBench en utilisant des ajustements micro de sous-structures et de récompenses, tandis que DiT-Air-Lite montre une forte compétitivité à petite échelle, dépassant actuellement les modèles existants.",
      "upvotes": 8,
      "discussionId": "67d3d302c4a225b6531556d6",
      "ai_keywords": [
        "Diffusion Transformers (DiTs)",
        "text-to-image generation",
        "architectural choices",
        "text-conditioning strategies",
        "training protocols",
        "PixArt-style",
        "MMDiT variants",
        "concatenated text and noise inputs",
        "parameter-efficiency",
        "layer-wise parameter sharing strategy",
        "Variational Auto-Encoders (VAEs)",
        "DiT-Air",
        "DiT-Air-Lite",
        "supervised and reward fine-tuning",
        "GenEval",
        "T2I CompBench",
        "state-of-the-art performance"
      ]
    },
    "publishedAt": "2025-03-13T13:57:25.000Z",
    "title": "DiT-Air: Revisiting the Efficiency of Diffusion Model Architecture\n  Design in Text to Image Generation",
    "summary": "In this work, we empirically study Diffusion Transformers (DiTs) for\ntext-to-image generation, focusing on architectural choices, text-conditioning\nstrategies, and training protocols. We evaluate a range of DiT-based\narchitectures--including PixArt-style and MMDiT variants--and compare them with\na standard DiT variant which directly processes concatenated text and noise\ninputs. Surprisingly, our findings reveal that the performance of standard DiT\nis comparable with those specialized models, while demonstrating superior\nparameter-efficiency, especially when scaled up. Leveraging the layer-wise\nparameter sharing strategy, we achieve a further reduction of 66% in model size\ncompared to an MMDiT architecture, with minimal performance impact. Building on\nan in-depth analysis of critical components such as text encoders and\nVariational Auto-Encoders (VAEs), we introduce DiT-Air and DiT-Air-Lite. With\nsupervised and reward fine-tuning, DiT-Air achieves state-of-the-art\nperformance on GenEval and T2I CompBench, while DiT-Air-Lite remains highly\ncompetitive, surpassing most existing models despite its compact size.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10618.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "656c2fa772c19de72367bd69",
      "avatarUrl": "/avatars/540bb3d8a2afe2ef927b80d895cae28b.svg",
      "fullname": "Alex Yang",
      "name": "yyf86",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.10589",
      "authors": [
        {
          "_id": "67d3adc5c14480a46038cecf",
          "name": "Yuwei Guo",
          "hidden": false
        },
        {
          "_id": "67d3adc5c14480a46038ced0",
          "name": "Ceyuan Yang",
          "hidden": false
        },
        {
          "_id": "67d3adc5c14480a46038ced1",
          "name": "Ziyan Yang",
          "hidden": false
        },
        {
          "_id": "67d3adc5c14480a46038ced2",
          "name": "Zhibei Ma",
          "hidden": false
        },
        {
          "_id": "67d3adc5c14480a46038ced3",
          "name": "Zhijie Lin",
          "hidden": false
        },
        {
          "_id": "67d3adc5c14480a46038ced4",
          "name": "Zhenheng Yang",
          "hidden": false
        },
        {
          "_id": "67d3adc5c14480a46038ced5",
          "name": "Dahua Lin",
          "hidden": false
        },
        {
          "_id": "67d3adc5c14480a46038ced6",
          "name": "Lu Jiang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-13T17:40:07.000Z",
      "submittedOnDailyAt": "2025-03-14T02:47:38.511Z",
      "title": "Génération de Films par Ajustement de Contexte de Chapitres",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Récemment, la technologie de génération d'images a permis la création d'images réalistes en un seul seconde en utilisant des transformateurs distribués échelonnables. Cependant, les images mondialement connues nécessitent une cohérence visuelle et dynamique, ce qui implique de multiples scénarios à différentes perspectives. Dans cet article, nous proposons l'introduction de la Long Context Tuning (LCT) pour élargir le contexte des modèles distribués préalablement entraînés avec des images d'une seule perspective et pour fournir un approche d'entraînement directement à partir des données pour la cohérence au niveau de scénario. Notre méthode permet que chaque perspective complète dans sa fonction d'attention soit élargie à toutes les perspectives au sein de chaque scénario, en combinant un mapping 3D et des scénarios de bruit asynchrones, sans inclure de paramètres supplémentaires. Après LCT, le modèle à attention bidirectionnelle permet une terminaison efficace avec caching de KV pour la génération automatique et cohérente. Les expériences montrent que le modèle d'une seule perspective après LCT peut créer des scénarios cohérents avec plusieurs perspectives, démontrant des capacités novatrices telles que l'expansion structurale et l'interaction, ouvrant des voies pour la création de contenu visuel pratique. Pour plus de détails, consultez https://guoyww.github.io/projects/long-context-video/.",
      "upvotes": 6,
      "discussionId": "67d3adc7c14480a46038cf5e",
      "ai_keywords": [
        "scalable diffusion transformers",
        "context window",
        "scene-level consistency",
        "Long Context Tuning (LCT)",
        "full attention mechanisms",
        "interleaved 3D position embedding",
        "asynchronous noise strategy",
        "joint and auto-regressive shot generation",
        "bidirectional attention",
        "context-causal attention",
        "efficient KV-cache",
        "compositional generation",
        "interactive shot extension"
      ]
    },
    "publishedAt": "2025-03-13T13:40:07.000Z",
    "title": "Long Context Tuning for Video Generation",
    "summary": "Recent advances in video generation can produce realistic, minute-long\nsingle-shot videos with scalable diffusion transformers. However, real-world\nnarrative videos require multi-shot scenes with visual and dynamic consistency\nacross shots. In this work, we introduce Long Context Tuning (LCT), a training\nparadigm that expands the context window of pre-trained single-shot video\ndiffusion models to learn scene-level consistency directly from data. Our\nmethod expands full attention mechanisms from individual shots to encompass all\nshots within a scene, incorporating interleaved 3D position embedding and an\nasynchronous noise strategy, enabling both joint and auto-regressive shot\ngeneration without additional parameters. Models with bidirectional attention\nafter LCT can further be fine-tuned with context-causal attention, facilitating\nauto-regressive generation with efficient KV-cache. Experiments demonstrate\nsingle-shot models after LCT can produce coherent multi-shot scenes and exhibit\nemerging capabilities, including compositional generation and interactive shot\nextension, paving the way for more practical visual content creation. See\nhttps://guoyww.github.io/projects/long-context-video/ for more details.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10589.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6364
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.10437",
      "authors": [
        {
          "_id": "67d3adf57360ea908cf5f0bc",
          "name": "Wanhua Li",
          "hidden": false
        },
        {
          "_id": "67d3adf57360ea908cf5f0bd",
          "user": {
            "_id": "66f0d2036a483077eed42bfb",
            "avatarUrl": "/avatars/f7f3f726842c26b8e52c9bdd48774b8e.svg",
            "isPro": false,
            "fullname": "Renping Zhou",
            "user": "rpzhou",
            "type": "user"
          },
          "name": "Renping Zhou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-14T08:56:08.159Z",
          "hidden": false
        },
        {
          "_id": "67d3adf57360ea908cf5f0be",
          "name": "Jiawei Zhou",
          "hidden": false
        },
        {
          "_id": "67d3adf57360ea908cf5f0bf",
          "name": "Yingwei Song",
          "hidden": false
        },
        {
          "_id": "67d3adf57360ea908cf5f0c0",
          "name": "Johannes Herter",
          "hidden": false
        },
        {
          "_id": "67d3adf57360ea908cf5f0c1",
          "name": "Minghan Qin",
          "hidden": false
        },
        {
          "_id": "67d3adf57360ea908cf5f0c2",
          "name": "Gao Huang",
          "hidden": false
        },
        {
          "_id": "67d3adf57360ea908cf5f0c3",
          "name": "Hanspeter Pfister",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/658bb7e47459b6e471b9d2e6/H5DUFDgeiid82tHVuW9cx.mp4"
      ],
      "publishedAt": "2025-03-13T14:58:22.000Z",
      "submittedOnDailyAt": "2025-03-14T02:49:34.475Z",
      "title": "4D LangSplat : 4D Langue Gaussian Splat Implémenté par Diversification dans les Modèles de Langue",
      "submittedOnDailyBy": {
        "_id": "658bb7e47459b6e471b9d2e6",
        "avatarUrl": "/avatars/efd8051b468b4dbcb5d149479de67c58.svg",
        "isPro": false,
        "fullname": "Wanhua Li",
        "user": "EthanTaylor",
        "type": "user"
      },
      "summary": "L'apprentissage du domaine de la langue 4D est essentiel pour réaliser des consultations linguistiques ouvertes et adaptables au temps dans un scénario dynamique. LangSplat a atteint une précision et une efficacité dans un scénario 3D statique en basant sur des caractéristiques de CLIP transformées en représentations gaussiennes 3D, mais CLIP a été conçu pour des tâches de texte-image statiques, ce qui le rend incapable de comprendre les actions temporelles d'un vidéo. Dans des environnements réels, la signification des objets change dynamiquement au fil du temps. Pour construire un domaine de la langue 4D avec précision, il est nécessaire d'obtenir les caractéristiques vidéo des objets qui correspondent aux images de pixels. Pour résoudre ce problème, nous proposons 4D LangSplat. 4D LangSplat se concentre sur l'apprentissage du domaine de la langue 4D pour traiter des consultations ouvertes et adaptables au temps ou indépendantes du temps de manière efficace dans des scénarios dynamiques. 4D LangSplat évite l'apprentissage du domaine de la langue à partir de caractéristiques visuelles et utilise des textes générés à partir de captures vidéos pour entraîner directement des Modèles de Langue Multimodal Grands (MLLMs). En particulier, nous proposons le méthode de prompt de vidéo, qui utilise des prompts visuels et textuels pour générer des captures temporelles profondes et de haute qualité dans les MLLMs. Ces captures sont utilisées avec des modèles de langage grands pour effectuer des consultations ouvertes qui supportent les caractéristiques des objets et se communiquent à travers des espaces d'embedding partagés. Les objets dans le scénario 4D montrent des mouvements doux entre états. Par conséquent, nous proposons une réseau de déformation d'états pour modéliser efficacement les changements temporels. A travers des résultats sur plusieurs benchmarks, 4D LangSplat a atteint des résultats précis et efficaces pour des consultations ouvertes et adaptables au temps ou indépendantes du temps.",
      "upvotes": 6,
      "discussionId": "67d3adf87360ea908cf5f182",
      "ai_keywords": [
        "4D language fields",
        "time-sensitive queries",
        "open-ended language queries",
        "LangSplat",
        "CLIP features",
        "3D Gaussian representations",
        "dynamic 4D fields",
        "temporal dynamics",
        "pixel-aligned",
        "object-wise video features",
        "4D LangSplat",
        "time-agnostic queries",
        "Multimodal Large Language Models (MLLMs)",
        "multimodal object-wise video prompting",
        "visual prompts",
        "text prompts",
        "detailed captions",
        "temporally consistent captions",
        "sentence embeddings",
        "shared embedding spaces",
        "status deformable network",
        "continuous changes"
      ]
    },
    "publishedAt": "2025-03-13T10:58:22.000Z",
    "title": "4D LangSplat: 4D Language Gaussian Splatting via Multimodal Large\n  Language Models",
    "summary": "Learning 4D language fields to enable time-sensitive, open-ended language\nqueries in dynamic scenes is essential for many real-world applications. While\nLangSplat successfully grounds CLIP features into 3D Gaussian representations,\nachieving precision and efficiency in 3D static scenes, it lacks the ability to\nhandle dynamic 4D fields as CLIP, designed for static image-text tasks, cannot\ncapture temporal dynamics in videos. Real-world environments are inherently\ndynamic, with object semantics evolving over time. Building a precise 4D\nlanguage field necessitates obtaining pixel-aligned, object-wise video\nfeatures, which current vision models struggle to achieve. To address these\nchallenges, we propose 4D LangSplat, which learns 4D language fields to handle\ntime-agnostic or time-sensitive open-vocabulary queries in dynamic scenes\nefficiently. 4D LangSplat bypasses learning the language field from vision\nfeatures and instead learns directly from text generated from object-wise video\ncaptions via Multimodal Large Language Models (MLLMs). Specifically, we propose\na multimodal object-wise video prompting method, consisting of visual and text\nprompts that guide MLLMs to generate detailed, temporally consistent,\nhigh-quality captions for objects throughout a video. These captions are\nencoded using a Large Language Model into high-quality sentence embeddings,\nwhich then serve as pixel-aligned, object-specific feature supervision,\nfacilitating open-vocabulary text queries through shared embedding spaces.\nRecognizing that objects in 4D scenes exhibit smooth transitions across states,\nwe further propose a status deformable network to model these continuous\nchanges over time effectively. Our results across multiple benchmarks\ndemonstrate that 4D LangSplat attains precise and efficient results for both\ntime-sensitive and time-agnostic open-vocabulary queries.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/658bb7e47459b6e471b9d2e6/H5DUFDgeiid82tHVuW9cx.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10437.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "658bb7e47459b6e471b9d2e6",
      "avatarUrl": "/avatars/efd8051b468b4dbcb5d149479de67c58.svg",
      "fullname": "Wanhua Li",
      "name": "EthanTaylor",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.10357",
      "authors": [
        {
          "_id": "67d3da3ce0b767b3d0fae2a4",
          "user": {
            "_id": "63bbfd74141c7d395c471768",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1673264437106-noauth.jpeg",
            "isPro": false,
            "fullname": "Viktor Moskvoretskii",
            "user": "VityaVitalich",
            "type": "user"
          },
          "name": "Viktor Moskvoretskii",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-03-14T07:38:54.624Z",
          "hidden": false
        },
        {
          "_id": "67d3da3ce0b767b3d0fae2a5",
          "name": "Alina Lobanova",
          "hidden": false
        },
        {
          "_id": "67d3da3ce0b767b3d0fae2a6",
          "name": "Ekaterina Neminova",
          "hidden": false
        },
        {
          "_id": "67d3da3ce0b767b3d0fae2a7",
          "name": "Chris Biemann",
          "hidden": false
        },
        {
          "_id": "67d3da3ce0b767b3d0fae2a8",
          "name": "Alexander Panchenko",
          "hidden": false
        },
        {
          "_id": "67d3da3ce0b767b3d0fae2a9",
          "name": "Irina Nikishina",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63bbfd74141c7d395c471768/7wZapqnRg87iwXLqxiKQL.jpeg"
      ],
      "publishedAt": "2025-03-13T13:37:54.000Z",
      "submittedOnDailyAt": "2025-03-14T05:58:28.512Z",
      "title": "Voyez-vous quelque chose comme `cat.n.01` ? Classification d'images génération de benchmark.",
      "submittedOnDailyBy": {
        "_id": "63bbfd74141c7d395c471768",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1673264437106-noauth.jpeg",
        "isPro": false,
        "fullname": "Viktor Moskvoretskii",
        "user": "VityaVitalich",
        "type": "user"
      },
      "summary": "Cet article explore la possibilité de générer des images à l'aide de modèles d'images en utilisant du texte dans un ensemble de tests sans exemples (0-shot set), en examinant comment on peut créer des images représentant des concepts textuels. Les méthodes basées sur le texte sont déjà établies pour l'extension textuelle, mais la possibilité visuelle n'a pas encore été explorée. Dans ce contexte, nous proposons un critère d'évaluation pour la génération d'images textuelles. Ce critère évalue la capacité des modèles à comprendre des concepts textuels et à générer des images de haute qualité liées. Il inclut des connaissances générales, des concepts de WordNet sélectionnés aléatoirement et des prédictions générées par des modèles de grande maîtrise (LLMs). On a évalué 12 modèles en utilisant 9 nouveaux textes liés à la texturalisation, en les comparant à un critère d'évaluation et à une rétroaction humaine. De plus, nous avons introduit pour la première fois une évaluation relative en utilisant la rétroaction de GPT-4 pour la génération d'images. Les résultats des expérimentations montrent que la classification des modèles est significativement différente des tâches standard de T2I. Playground-v2 et FLUX occupent une position supérieure sur plusieurs métriques et sous-ensembles, tandis que l'approche basée sur la recherche est inadéquate. Ces résultats clairement démontrent la possibilité de la calédoscopie automatique des ressources de données structurées.",
      "upvotes": 6,
      "discussionId": "67d3da42e0b767b3d0fae455",
      "projectPage": "https://huggingface.co/collections/VityaVitalich/generated-image-wordnet-67d2c868ff1414ec2f8e0d3d",
      "ai_keywords": [
        "text-to-image models",
        "zero-shot setup",
        "taxonomy concepts",
        "text-based methods",
        "taxonomy enrichment",
        "comprehensive benchmark",
        "WordNet",
        "LLM generated predictions",
        "taxonomy-related text-to-image metrics",
        "pairwise evaluation",
        "GPT-4 feedback",
        "image generation",
        "ranking of models",
        "retrieval-based approach",
        "automating the curation of structured data resources"
      ]
    },
    "publishedAt": "2025-03-13T09:37:54.000Z",
    "title": "Do I look like a `cat.n.01` to you? A Taxonomy Image Generation\n  Benchmark",
    "summary": "This paper explores the feasibility of using text-to-image models in a\nzero-shot setup to generate images for taxonomy concepts. While text-based\nmethods for taxonomy enrichment are well-established, the potential of the\nvisual dimension remains unexplored. To address this, we propose a\ncomprehensive benchmark for Taxonomy Image Generation that assesses models'\nabilities to understand taxonomy concepts and generate relevant, high-quality\nimages. The benchmark includes common-sense and randomly sampled WordNet\nconcepts, alongside the LLM generated predictions. The 12 models are evaluated\nusing 9 novel taxonomy-related text-to-image metrics and human feedback.\nMoreover, we pioneer the use of pairwise evaluation with GPT-4 feedback for\nimage generation. Experimental results show that the ranking of models differs\nsignificantly from standard T2I tasks. Playground-v2 and FLUX consistently\noutperform across metrics and subsets and the retrieval-based approach performs\npoorly. These findings highlight the potential for automating the curation of\nstructured data resources.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63bbfd74141c7d395c471768/7wZapqnRg87iwXLqxiKQL.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10357.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63bbfd74141c7d395c471768",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1673264437106-noauth.jpeg",
      "fullname": "Viktor Moskvoretskii",
      "name": "VityaVitalich",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.09642",
      "authors": [
        {
          "_id": "67d3ab8d032b54ab876cb7ec",
          "name": "Xiangyu Peng",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb7ed",
          "name": "Zangwei Zheng",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb7ee",
          "name": "Chenhui Shen",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb7ef",
          "name": "Tom Young",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb7f0",
          "name": "Xinying Guo",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb7f1",
          "name": "Binluo Wang",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb7f2",
          "name": "Hang Xu",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb7f3",
          "name": "Hongxin Liu",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb7f4",
          "name": "Mingyan Jiang",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb7f5",
          "name": "Wenjun Li",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb7f6",
          "name": "Yuhui Wang",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb7f7",
          "name": "Anbang Ye",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb7f8",
          "name": "Gang Ren",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb7f9",
          "name": "Qianran Ma",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb7fa",
          "name": "Wanying Liang",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb7fb",
          "name": "Xiang Lian",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb7fc",
          "name": "Xiwen Wu",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb7fd",
          "name": "Yuting Zhong",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb7fe",
          "name": "Zhuangyan Li",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb7ff",
          "name": "Chaoyu Gong",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb800",
          "name": "Guojun Lei",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb801",
          "name": "Leijun Cheng",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb802",
          "name": "Limin Zhang",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb803",
          "name": "Minghao Li",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb804",
          "name": "Ruijie Zhang",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb805",
          "name": "Silan Hu",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb806",
          "name": "Shijie Huang",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb807",
          "name": "Xiaokang Wang",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb808",
          "name": "Yuanheng Zhao",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb809",
          "name": "Yuqi Wang",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb80a",
          "name": "Ziang Wei",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb80b",
          "name": "Yang You",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-12T05:00:07.000Z",
      "submittedOnDailyAt": "2025-03-14T02:39:06.938Z",
      "title": "Open-Sora 2.0 : 200 000 $ pour entraîner un modèle de génération vidéo au niveau commercial",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Le modèle de génération de vidéo a connu une transition impressionnante au cours des 12 derniers mois. Bien que la qualité des vidéos générées par l'IA ait augmenté, le taille du modèle a augmenté, la quantité de données a croît et la demande en calculs d'entraînement a augmenté. Dans ce rapport, nous présentons Open-Sora 2.0, un modèle de génération de vidéo à l'échelle commerciale. Ce modèle a été entraîné à un coût de 200 000 $, démontrant comment il est possible de contrôler de manière significative le coût d'entraînement d'un modèle de haut rendement. Nous détaillons en profondeur toutes les technologies qui ont contribué à cette amélioration efficace, y compris la charge de données, l'architecture du modèle, la stratégie d'entraînement et l'optimisation du système. En se basant sur les résultats d'évaluation humaine et sur les scores de VBench, Open-Sora 2.0 est comparé à d'autres modèles de génération de vidéo reconnus mondialement, comme le code ouvert HunyuanVideo et le Runway Gen-3 Alpha, un modèle de code fermé. L'objectif de Open-Sora 2.0 est de le rendre complètement ouvert, de démocratiser l'accès aux technologies avancées de génération de vidéo et de favoriser une large innovation et créativité dans la création de contenu. Tous les ressources sont disponibles sur https://github.com/hpcaitech/Open-Sora.",
      "upvotes": 6,
      "discussionId": "67d3ab93032b54ab876cb9b0",
      "ai_keywords": [
        "video generation models",
        "data curation",
        "model architecture",
        "training strategy",
        "system optimization",
        "human evaluation",
        "VBench scores",
        "HunyuanVideo",
        "Runway Gen-3 Alpha",
        "open-source"
      ]
    },
    "publishedAt": "2025-03-12T01:00:07.000Z",
    "title": "Open-Sora 2.0: Training a Commercial-Level Video Generation Model in\n  $200k",
    "summary": "Video generation models have achieved remarkable progress in the past year.\nThe quality of AI video continues to improve, but at the cost of larger model\nsize, increased data quantity, and greater demand for training compute. In this\nreport, we present Open-Sora 2.0, a commercial-level video generation model\ntrained for only $200k. With this model, we demonstrate that the cost of\ntraining a top-performing video generation model is highly controllable. We\ndetail all techniques that contribute to this efficiency breakthrough,\nincluding data curation, model architecture, training strategy, and system\noptimization. According to human evaluation results and VBench scores,\nOpen-Sora 2.0 is comparable to global leading video generation models including\nthe open-source HunyuanVideo and the closed-source Runway Gen-3 Alpha. By\nmaking Open-Sora 2.0 fully open-source, we aim to democratize access to\nadvanced video generation technology, fostering broader innovation and\ncreativity in content creation. All resources are publicly available at:\nhttps://github.com/hpcaitech/Open-Sora.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.09642.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6364
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.10630",
      "authors": [
        {
          "_id": "67d39f4828221b583a33be2c",
          "name": "Hang Yin",
          "hidden": false
        },
        {
          "_id": "67d39f4828221b583a33be2d",
          "name": "Xiuwei Xu",
          "hidden": false
        },
        {
          "_id": "67d39f4828221b583a33be2e",
          "name": "Lingqing Zhao",
          "hidden": false
        },
        {
          "_id": "67d39f4828221b583a33be2f",
          "name": "Ziwei Wang",
          "hidden": false
        },
        {
          "_id": "67d39f4828221b583a33be30",
          "name": "Jie Zhou",
          "hidden": false
        },
        {
          "_id": "67d39f4828221b583a33be31",
          "name": "Jiwen Lu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-13T17:59:48.000Z",
      "submittedOnDailyAt": "2025-03-14T01:45:48.071Z",
      "title": "UniGoal : Objectif vers lequel est orientée la Navigation Objectif-Orientée de Zero-shot",
      "submittedOnDailyBy": {
        "_id": "648ac65fd044b25978015634",
        "avatarUrl": "/avatars/2278a66fdc953220e9f8fc0ccce3ff00.svg",
        "isPro": false,
        "fullname": "Xiuwei Xu",
        "user": "xuxw98",
        "type": "user"
      },
      "summary": "Dans cet article, nous proposons un cadre généralisé pour l'apprentissage supervisé orienté vers des objectifs en 0 dimensions. Les méthodes actuelles en 0 dimensions construisent un cadre d'inférence basé sur de grands modèles de langage (LLM) pour une tâche spécifique, ce qui entraîne un flux de travail très complexe et ne peut pas être généralisé entre différents objectifs. Pour aligner avec l'objectif d'un apprentissage supervisé général en 0 dimensions, nous proposons une représentation graphique cohérente pour unifier différents objectifs, qui inclut des catégories d'objets, des images d'instances et des explications textuelles. De plus, les scénarios qui maintiennent l'observation des sorties en ligne sont transformés en graphes et nous utilisons cette représentation cohérente de scénario et d'objectif pour préserver plus d'information structurelle que le texte, permettant l'utilisation de LLM basés sur des graphes. Spécifiquement, nous effectuons l'ajustement de graphes entre le scénario et l'objectif à chaque étape de temps, et nous proposons différentes stratégies pour explorer des objectifs à long terme en fonction de l'état d'ajustement. Dans le cas d'un ajustement en 0 dimensions, nous explorons des graphes partiels d'objectifs répétitivement, tandis que dans le cas d'un ajustement partiel, nous estimons la position de l'objectif en utilisant des projections de coordonnées et des alignements de paires. Enfin, nous appliquons des corrections au scénario et des vérifications d'objectif pour atteindre un ajustement complet. De plus, nous introduisons la fonction de négation pour permettre des changements appropriés dans l'ordre. A travers des expériences larges sur plusieurs benchmarks, notre UniGoal atteint le meilleur rendement en 0 dimensions pour trois tâches d'apprentissage supervisé, dépassant à la fois les méthodes spécifiques en 0 dimensions et le méthode générale observée.",
      "upvotes": 5,
      "discussionId": "67d39f4928221b583a33be7f",
      "ai_keywords": [
        "universal zero-shot goal-oriented navigation",
        "zero-shot methods",
        "large language models (LLM)",
        "uniform graph representation",
        "object category",
        "instance image",
        "text description",
        "scene graph",
        "explicit graph-based reasoning",
        "graph matching",
        "long-term goal of exploration",
        "subgraph search",
        "coordinate projection",
        "anchor pair alignment",
        "scene graph correction",
        "goal verification",
        "blacklist mechanism",
        "UniGoal"
      ]
    },
    "publishedAt": "2025-03-13T13:59:48.000Z",
    "title": "UniGoal: Towards Universal Zero-shot Goal-oriented Navigation",
    "summary": "In this paper, we propose a general framework for universal zero-shot\ngoal-oriented navigation. Existing zero-shot methods build inference framework\nupon large language models (LLM) for specific tasks, which differs a lot in\noverall pipeline and fails to generalize across different types of goal.\nTowards the aim of universal zero-shot navigation, we propose a uniform graph\nrepresentation to unify different goals, including object category, instance\nimage and text description. We also convert the observation of agent into an\nonline maintained scene graph. With this consistent scene and goal\nrepresentation, we preserve most structural information compared with pure text\nand are able to leverage LLM for explicit graph-based reasoning. Specifically,\nwe conduct graph matching between the scene graph and goal graph at each time\ninstant and propose different strategies to generate long-term goal of\nexploration according to different matching states. The agent first iteratively\nsearches subgraph of goal when zero-matched. With partial matching, the agent\nthen utilizes coordinate projection and anchor pair alignment to infer the goal\nlocation. Finally scene graph correction and goal verification are applied for\nperfect matching. We also present a blacklist mechanism to enable robust switch\nbetween stages. Extensive experiments on several benchmarks show that our\nUniGoal achieves state-of-the-art zero-shot performance on three studied\nnavigation tasks with a single model, even outperforming task-specific\nzero-shot methods and supervised universal methods.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10630.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "648ac65fd044b25978015634",
      "avatarUrl": "/avatars/2278a66fdc953220e9f8fc0ccce3ff00.svg",
      "fullname": "Xiuwei Xu",
      "name": "xuxw98",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.10460",
      "authors": [
        {
          "_id": "67d3a87df6ea55297c3cd071",
          "name": "Liang Wen",
          "hidden": false
        },
        {
          "_id": "67d3a87df6ea55297c3cd072",
          "name": "Yunke Cai",
          "hidden": false
        },
        {
          "_id": "67d3a87df6ea55297c3cd073",
          "name": "Fenrui Xiao",
          "hidden": false
        },
        {
          "_id": "67d3a87df6ea55297c3cd074",
          "name": "Xin He",
          "hidden": false
        },
        {
          "_id": "67d3a87df6ea55297c3cd075",
          "name": "Qi An",
          "hidden": false
        },
        {
          "_id": "67d3a87df6ea55297c3cd076",
          "name": "Zhenyu Duan",
          "hidden": false
        },
        {
          "_id": "67d3a87df6ea55297c3cd077",
          "name": "Yimin Du",
          "hidden": false
        },
        {
          "_id": "67d3a87df6ea55297c3cd078",
          "name": "Junchen Liu",
          "hidden": false
        },
        {
          "_id": "67d3a87df6ea55297c3cd079",
          "name": "Lifu Tang",
          "hidden": false
        },
        {
          "_id": "67d3a87df6ea55297c3cd07a",
          "name": "Xiaowei Lv",
          "hidden": false
        },
        {
          "_id": "67d3a87df6ea55297c3cd07b",
          "name": "Haosheng Zou",
          "hidden": false
        },
        {
          "_id": "67d3a87df6ea55297c3cd07c",
          "name": "Yongchao Deng",
          "hidden": false
        },
        {
          "_id": "67d3a87df6ea55297c3cd07d",
          "name": "Shousheng Jia",
          "hidden": false
        },
        {
          "_id": "67d3a87df6ea55297c3cd07e",
          "name": "Xiangzheng Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-13T15:29:22.000Z",
      "submittedOnDailyAt": "2025-03-14T02:25:27.538Z",
      "title": "Light-R1 : La Curriculum SFT, DPO et RL montrent des résultats plus avancés que le scratch à long terme par rapport au COT.",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Dans cet article, nous présentons les résultats de la recherche de la série Light-R1. Le modèle, les données et le code ont été publiés.\n\nTout d'abord, nous nous concentrerons sur l'initiation des modèles de COT à long terme. En particulier, nous soutenons que c'est crucial d'initialiser un modèle qui n'a pas de capacités de COT à long terme à l'origine. En utilisant un curriculum d'entraînement composé de deux étapes de SFT et une politique de paramètres réversibles DPO, nous avons entraîné le modèle Light-R1-32B à partir de Qwen2.5-32B-Instruct, démontrant un rendement supérieur en mathématiques par rapport à DeepSeek-R1-Distill-Qwen-32B. Le modèle Light-R1-32B, entraîné uniquement avec des données de mathématiques, a démontré un fort rendement de généralisation dans d'autres domaines.\n\nDans le prochain pas, nous affirmons l'importance de la collection de 3k données construite à la seconde étape de SFT et nous l'avons utilisée pour fine-tuning le modèle DeepSeek-R1-Distilled, obtenant un nouveau modèle SOTA. Le modèle Light-R1-32B-DS, tant pour le modèle de 7B que pour celui de 14B, a présenté un rendement comparable à celui de QwQ-32B et DeepSeek-R1.\n\nDe plus, nous avons essayé d'améliorer le rendement des modèles de COT à long terme par l'apprentissage par reforcement, en particulier en appliquant GRPO. Le modèle final Light-R1-14B-DS a été entraîné en utilisant RL et a atteint le rendement SOTA dans le domaine de la mathématiques, avec des scores de 74.0 et 60.2 sur les tests AIME24 et AIME25, respectivement. Ce modèle a dépassé les modèles de 32B comme DeepSeek-R1-Distill-Llama-70B. De plus, il a montré des comportements d'action attendus par RL, avec un augmentation tant de la longueur des réponses que des scores de récompense.\n\nLa recherche de la série Light-R1 démontre l'initiation des modèles de COT à long terme, l'artisanat des données de SFT et la publication d'un modèle SOTA en utilisant RL.",
      "upvotes": 5,
      "discussionId": "67d3a87ef6ea55297c3cd0d0",
      "ai_keywords": [
        "COT models",
        "curriculum training",
        "two-stage SFT",
        "semi-on-policy DPO",
        "long COT capabilities",
        "SOTA",
        "generalization",
        "3k dataset",
        "fine-tuning",
        "GRPO",
        "reasoning performance",
        "AIME24 & 25 scores",
        "response length",
        "reward score"
      ]
    },
    "publishedAt": "2025-03-13T11:29:22.000Z",
    "title": "Light-R1: Curriculum SFT, DPO and RL for Long COT from Scratch and\n  Beyond",
    "summary": "This paper presents our work on the Light-R1 series, with models, data, and\ncode all released.\n  We first focus on training long COT models from scratch, specifically\nstarting from models initially lacking long COT capabilities. Using a\ncurriculum training recipe consisting of two-stage SFT and semi-on-policy DPO,\nwe train our model Light-R1-32B from Qwen2.5-32B-Instruct, resulting in\nsuperior math performance compared to DeepSeek-R1-Distill-Qwen-32B. Despite\nbeing trained exclusively on math data, Light-R1-32B shows strong\ngeneralization across other domains. In the subsequent phase of this work, we\nhighlight the significant benefit of the 3k dataset constructed for the second\nSFT stage on enhancing other models. By fine-tuning DeepSeek-R1-Distilled\nmodels using this dataset, we obtain new SOTA models in 7B and 14B, while the\n32B model, Light-R1-32B-DS performed comparably to QwQ-32B and DeepSeek-R1.\n  Furthermore, we extend our work by applying reinforcement learning,\nspecifically GRPO, on long-COT models to further improve reasoning performance.\nWe successfully train our final Light-R1-14B-DS with RL, achieving SOTA\nperformance among 14B parameter models in math. With AIME24 & 25 scores of 74.0\nand 60.2 respectively, Light-R1-14B-DS surpasses even many 32B models and\nDeepSeek-R1-Distill-Llama-70B. Its RL training also exhibits well expected\nbehavior, showing simultaneous increase in response length and reward score.\n  The Light-R1 series of work validates training long-COT models from scratch,\nshowcases the art in SFT data and releases SOTA models from RL.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10460.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6364
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.09905",
      "authors": [
        {
          "_id": "67d393660d51cf27930a5e5d",
          "user": {
            "_id": "67d3930e4d3a41ed9f7ac71e",
            "avatarUrl": "/avatars/4829b1d58376f7987a8891d41108c9c9.svg",
            "isPro": false,
            "fullname": "Allison Andreyev",
            "user": "allisonandreyev",
            "type": "user"
          },
          "name": "Allison Andreyev",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-03-14T02:25:50.445Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-12T23:50:35.000Z",
      "submittedOnDailyAt": "2025-03-14T00:57:22.699Z",
      "title": "OpenAI's Whisper Model Digitalization : Une Approche Relativement Analytique",
      "submittedOnDailyBy": {
        "_id": "67d3930e4d3a41ed9f7ac71e",
        "avatarUrl": "/avatars/4829b1d58376f7987a8891d41108c9c9.svg",
        "isPro": false,
        "fullname": "Allison Andreyev",
        "user": "allisonandreyev",
        "type": "user"
      },
      "summary": "Le modèle de reconnaissance automatique de voix (ASR) joue un rôle important dans diverses applications telles que la capture de voix, la traduction des langues et le texte en ligne. Dans cet article, deux variantes du modèle Whisper sont étudiées et comparées : l'une conçue pour le streaming de voix en ligne et l'autre pour le texte en ligne dans des environnements off-line. En particulier, il est observé que ces modèles peuvent réduire la confiance dans le texte généré en raison d'erreurs de lecture ou de la génération de contenu virtuel. De plus, les grands modèles présentent des problèmes de radiation lorsqu'ils sont traités sur des dispositifs avec des restrictions en termes de radiation. L'article analyse les similitudes et différences entre trois modèles Whisper et effectue une évaluation qualitative. Ensuite, on évalue dans quelle mesure la stratégie de traitement des données affecte la radiation et on évalue la possibilité de traitement sur des dispositifs de pointe. En utilisant le jeu de données LibriSpeech, des analyses de taux d'erreur de mot (WER) et de radiation de Whispercpp sont réalisées avec trois méthodes de traitement des données : INT4, INT5 et INT8. Enfin, on conclut que le traitement des données peut réduire la radiation d'un 19% et réduire le taille du modèle d'un 45% tout en maintenant la précision du texte. Ces résultats fournissent des cas d'utilisation optimaux pour différents modèles Whisper et la possibilité de traitement sur des dispositifs de pointe. Tout le code, les jeux de données et les détails d'implémentation sont disponibles sur le dépôt GitHub : https://github.com/allisonandreyev/WhisperQuantization.git",
      "upvotes": 5,
      "discussionId": "67d393670d51cf27930a5e98",
      "githubRepo": "https://github.com/allisonandreyev/WhisperQuantization",
      "ai_keywords": [
        "Whisper",
        "live speech streaming",
        "offline transcription",
        "hallucinated content",
        "latency",
        "deployment",
        "quantization",
        "LibriSpeech dataset",
        "word error rate (WER)",
        "whispercpp",
        "INT4",
        "INT5",
        "INT8",
        "speech recognition (ASR)",
        "transcription accuracy",
        "edge deployment"
      ]
    },
    "publishedAt": "2025-03-12T19:50:35.000Z",
    "title": "Quantization for OpenAI's Whisper Models: A Comparative Analysis",
    "summary": "Automated speech recognition (ASR) models have gained prominence for\napplications such as captioning, speech translation, and live transcription.\nThis paper studies Whisper and two model variants: one optimized for live\nspeech streaming and another for offline transcription. Notably, these models\nhave been found to generate hallucinated content, reducing transcription\nreliability. Furthermore, larger model variants exhibit increased latency and\npose challenges for deployment on resource-constrained devices. This study\nanalyzes the similarities and differences between three Whisper models,\nqualitatively examining their distinct capabilities. Next, this study\nquantifies the impact of model quantization on latency and evaluates its\nviability for edge deployment. Using the open source LibriSpeech dataset, this\npaper evaluates the word error rate (WER) along with latency analysis of\nwhispercpp using 3 quantization methods (INT4, INT5, INT8). Results show that\nquantization reduces latency by 19\\% and model size by 45\\%, while preserving\ntranscription accuracy. These findings provide insights into the optimal use\ncases of different Whisper models and edge device deployment possibilities. All\ncode, datasets, and implementation details are available in a public GitHub\nrepository: https://github.com/allisonandreyev/WhisperQuantization.git",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.09905.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67d3930e4d3a41ed9f7ac71e",
      "avatarUrl": "/avatars/4829b1d58376f7987a8891d41108c9c9.svg",
      "fullname": "Allison Andreyev",
      "name": "allisonandreyev",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.09641",
      "authors": [
        {
          "_id": "67d3b008e18f86384bd33fa1",
          "name": "Junsong Chen",
          "hidden": false
        },
        {
          "_id": "67d3b008e18f86384bd33fa2",
          "name": "Shuchen Xue",
          "hidden": false
        },
        {
          "_id": "67d3b008e18f86384bd33fa3",
          "name": "Yuyang Zhao",
          "hidden": false
        },
        {
          "_id": "67d3b008e18f86384bd33fa4",
          "name": "Jincheng Yu",
          "hidden": false
        },
        {
          "_id": "67d3b008e18f86384bd33fa5",
          "user": {
            "_id": "5f7fbd813e94f16a85448745",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1649681653581-5f7fbd813e94f16a85448745.jpeg",
            "isPro": false,
            "fullname": "Sayak Paul",
            "user": "sayakpaul",
            "type": "user"
          },
          "name": "Sayak Paul",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-14T08:56:06.327Z",
          "hidden": false
        },
        {
          "_id": "67d3b008e18f86384bd33fa6",
          "name": "Junyu Chen",
          "hidden": false
        },
        {
          "_id": "67d3b008e18f86384bd33fa7",
          "name": "Han Cai",
          "hidden": false
        },
        {
          "_id": "67d3b008e18f86384bd33fa8",
          "name": "Enze Xie",
          "hidden": false
        },
        {
          "_id": "67d3b008e18f86384bd33fa9",
          "name": "Song Han",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-12T04:53:07.000Z",
      "submittedOnDailyAt": "2025-03-14T02:57:11.544Z",
      "title": "SANA-Sprint : Un paso imprecise et une cohérence temporelle expérimentale de transmission",
      "submittedOnDailyBy": {
        "_id": "5f7fbd813e94f16a85448745",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1649681653581-5f7fbd813e94f16a85448745.jpeg",
        "isPro": false,
        "fullname": "Sayak Paul",
        "user": "sayakpaul",
        "type": "user"
      },
      "summary": "Dans cet article, nous présentons le modèle SANA-Sprint efficace et nous mettons en œuvre une méthodologie pour générer des images à partir de textes à haute vitesse (T2I). Le SANA-Sprint est construit sur un modèle pré-entraîné, en ajoutant une distillation hybride pour réduire significativement le temps de calcul (de 20 étapes à 1-4 étapes). Nous présentons trois innovations clés : (1) nous proposons un approche sans entraînement et nous appliquons un modèle d'ajustement de fleurs à la distillation des coincidences en temps continu (sCM) pour éliminer le haut coût d'entraînement à partir du départ et atteindre une grande efficacité d'entraînement. La stratégie de distillation hybride combine sCM et la distillation de défi relatif (LADD) : sCM assure l'harmonie entre le modèle d'apprentissage et LADD améliore la confiance dans la génération en un seul pas. (2) Le SANA-Sprint réussit à générer de haute qualité en 1-4 étapes, en éliminant l'entraînement par étapes pour améliorer l'efficience. Cette innovation continue établit un nouveau paradigme dans le compromis entre vitesse et qualité, en atteignant un rendement récent de 7.59 FID et 0.74 GenEval en un seul pas (superant FLUX-schnell avec 7.94 FID et 0.71 GenEval, et étant 10 fois plus rapide sur H100 (0.1s vs 1.1s)). De plus, sur H100, nous atteignons un temps de 0.1s pour la génération d'images T2I de 1024 x 1024 et 0.25s avec ControlNet, démontrant les fonctions et les potentiels d'applications basées sur l'intelligence artificielle (AIPC). Le code et les modèles pré-entraînés sont publiés sous une licence de code open.",
      "upvotes": 5,
      "discussionId": "67d3b00be18f86384bd3408f",
      "ai_keywords": [
        "diffusion model",
        "text-to-image (T2I)",
        "pre-trained foundation model",
        "hybrid distillation",
        "flow-matching model",
        "continuous-time consistency distillation (sCM)",
        "latent adversarial distillation (LADD)",
        "unified step-adaptive model",
        "ControlNet",
        "real-time interactive image generation",
        "FID",
        "GenEval",
        "FLUX-schnell",
        "H100",
        "RTX 4090",
        "AI-powered consumer applications (AIPC)"
      ]
    },
    "publishedAt": "2025-03-12T00:53:07.000Z",
    "title": "SANA-Sprint: One-Step Diffusion with Continuous-Time Consistency\n  Distillation",
    "summary": "This paper presents SANA-Sprint, an efficient diffusion model for ultra-fast\ntext-to-image (T2I) generation. SANA-Sprint is built on a pre-trained\nfoundation model and augmented with hybrid distillation, dramatically reducing\ninference steps from 20 to 1-4. We introduce three key innovations: (1) We\npropose a training-free approach that transforms a pre-trained flow-matching\nmodel for continuous-time consistency distillation (sCM), eliminating costly\ntraining from scratch and achieving high training efficiency. Our hybrid\ndistillation strategy combines sCM with latent adversarial distillation (LADD):\nsCM ensures alignment with the teacher model, while LADD enhances single-step\ngeneration fidelity. (2) SANA-Sprint is a unified step-adaptive model that\nachieves high-quality generation in 1-4 steps, eliminating step-specific\ntraining and improving efficiency. (3) We integrate ControlNet with SANA-Sprint\nfor real-time interactive image generation, enabling instant visual feedback\nfor user interaction. SANA-Sprint establishes a new Pareto frontier in\nspeed-quality tradeoffs, achieving state-of-the-art performance with 7.59 FID\nand 0.74 GenEval in only 1 step - outperforming FLUX-schnell (7.94 FID / 0.71\nGenEval) while being 10x faster (0.1s vs 1.1s on H100). It also achieves 0.1s\n(T2I) and 0.25s (ControlNet) latency for 1024 x 1024 images on H100, and 0.31s\n(T2I) on an RTX 4090, showcasing its exceptional efficiency and potential for\nAI-powered consumer applications (AIPC). Code and pre-trained models will be\nopen-sourced.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.09641.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5f7fbd813e94f16a85448745",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1649681653581-5f7fbd813e94f16a85448745.jpeg",
      "fullname": "Sayak Paul",
      "name": "sayakpaul",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 586
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.10637",
      "authors": [
        {
          "_id": "67d3a2a6977358f62157977d",
          "user": {
            "_id": "636daf1b56c0762cfda074b5",
            "avatarUrl": "/avatars/f44be5eb110acfa2efbd09de6b416239.svg",
            "isPro": false,
            "fullname": "Rohit Gandikota",
            "user": "RohitGandikota",
            "type": "user"
          },
          "name": "Rohit Gandikota",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-14T08:56:16.089Z",
          "hidden": false
        },
        {
          "_id": "67d3a2a6977358f62157977e",
          "name": "David Bau",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-13T17:59:56.000Z",
      "submittedOnDailyAt": "2025-03-14T02:03:33.686Z",
      "title": "Utilisation de modèles distribués pour la diversité et le contrôle",
      "submittedOnDailyBy": {
        "_id": "636daf1b56c0762cfda074b5",
        "avatarUrl": "/avatars/f44be5eb110acfa2efbd09de6b416239.svg",
        "isPro": false,
        "fullname": "Rohit Gandikota",
        "user": "RohitGandikota",
        "type": "user"
      },
      "summary": "Le modèle de distillation présente un défi fondamental majeur : une réduction significative de la diversité des exemples par rapport aux modèles de base. Dans cette étude, nous montrons que malgré cette réduction, les modèles distillés conservent la représentation des concepts fondamentaux du modèle de base. Cela ouvre la possibilité que les structures de contrôle entraînées dans le modèle de base (comme les Concept Sliders et LoRAs) puissent être transférées aux modèles distillés sans être ignorées, et vice versa, maintenant le même effet. La conservation de ces structures de représentation a permis d'initier une recherche sur la perte de diversité lors du processus de distillation.\n\nNous introduisons le DT-Visualization, un outil d'analyse et de débogage qui permet de visualiser le comportement du modèle à chaque étape intermédiaire pour prédire l'output final. Grâce au DT-Visualization, nous identifions la rétroaction générée, les discontinuités et observons que les premières étapes de difficulté déterminent de manière inambigue la diversité de l'output, tandis que les étapes suivantes se concentrent sur la précision des détails. Dès cette perspective, nous proposons un approche hybride d'inférence appelée \"Distillation de Diversité-Difficulté\", qui permet que le modèle de base se transforme en un modèle distillé efficace en utilisant uniquement les étapes initiales cruciales. Cette simple modification récupère la fonction de diversité du modèle de base et dépasse à des niveaux surprenants, tout en maintenant l'efficacité de l'inférence distillée et en évitant la nécessité d'un entraînement supplémentaire ou de modifications du modèle.\n\nLe code et les données de l'étude sont disponibles sur https://distillation.baulab.info.",
      "upvotes": 4,
      "discussionId": "67d3a2ab977358f6215798fc",
      "projectPage": "https://distillation.baulab.info",
      "githubRepo": "https://github.com/rohitgandikota/distillation",
      "ai_keywords": [
        "distilled diffusion models",
        "sample diversity",
        "base models",
        "Concept Sliders",
        "LoRAs",
        "control distillation",
        "representational structure",
        "diversity collapse",
        "Diffusion Target (DT) Visualization",
        "intermediate steps",
        "generation artifacts",
        "inconsistencies",
        "diffusion timesteps",
        "diversity distillation",
        "hybrid inference approach"
      ]
    },
    "publishedAt": "2025-03-13T13:59:56.000Z",
    "title": "Distilling Diversity and Control in Diffusion Models",
    "summary": "Distilled diffusion models suffer from a critical limitation: reduced sample\ndiversity compared to their base counterparts. In this work, we uncover that\ndespite this diversity loss, distilled models retain the fundamental concept\nrepresentations of base models. We demonstrate control distillation - where\ncontrol mechanisms like Concept Sliders and LoRAs trained on base models can be\nseamlessly transferred to distilled models and vice-versa, effectively\ndistilling control without any retraining. This preservation of\nrepresentational structure prompted our investigation into the mechanisms of\ndiversity collapse during distillation. To understand how distillation affects\ndiversity, we introduce Diffusion Target (DT) Visualization, an analysis and\ndebugging tool that reveals how models predict final outputs at intermediate\nsteps. Through DT-Visualization, we identify generation artifacts,\ninconsistencies, and demonstrate that initial diffusion timesteps\ndisproportionately determine output diversity, while later steps primarily\nrefine details. Based on these insights, we introduce diversity distillation -\na hybrid inference approach that strategically employs the base model for only\nthe first critical timestep before transitioning to the efficient distilled\nmodel. Our experiments demonstrate that this simple modification not only\nrestores the diversity capabilities from base to distilled models but\nsurprisingly exceeds it, while maintaining nearly the computational efficiency\nof distilled inference, all without requiring additional training or model\nmodifications. Our code and data are available at\nhttps://distillation.baulab.info",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10637.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "636daf1b56c0762cfda074b5",
      "avatarUrl": "/avatars/f44be5eb110acfa2efbd09de6b416239.svg",
      "fullname": "Rohit Gandikota",
      "name": "RohitGandikota",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.10615",
      "authors": [
        {
          "_id": "67d3aa3f2f42ed5552e8ea0c",
          "name": "Yi Yang",
          "hidden": false
        },
        {
          "_id": "67d3aa3f2f42ed5552e8ea0d",
          "name": "Xiaoxuan He",
          "hidden": false
        },
        {
          "_id": "67d3aa3f2f42ed5552e8ea0e",
          "name": "Hongkun Pan",
          "hidden": false
        },
        {
          "_id": "67d3aa3f2f42ed5552e8ea0f",
          "name": "Xiyan Jiang",
          "hidden": false
        },
        {
          "_id": "67d3aa3f2f42ed5552e8ea10",
          "name": "Yan Deng",
          "hidden": false
        },
        {
          "_id": "67d3aa3f2f42ed5552e8ea11",
          "name": "Xingtao Yang",
          "hidden": false
        },
        {
          "_id": "67d3aa3f2f42ed5552e8ea12",
          "name": "Haoyu Lu",
          "hidden": false
        },
        {
          "_id": "67d3aa3f2f42ed5552e8ea13",
          "name": "Dacheng Yin",
          "hidden": false
        },
        {
          "_id": "67d3aa3f2f42ed5552e8ea14",
          "name": "Fengyun Rao",
          "hidden": false
        },
        {
          "_id": "67d3aa3f2f42ed5552e8ea15",
          "name": "Minfeng Zhu",
          "hidden": false
        },
        {
          "_id": "67d3aa3f2f42ed5552e8ea16",
          "name": "Bo Zhang",
          "hidden": false
        },
        {
          "_id": "67d3aa3f2f42ed5552e8ea17",
          "name": "Wei Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-13T17:56:05.000Z",
      "submittedOnDailyAt": "2025-03-14T02:32:20.499Z",
      "title": "R1-Onevision : Formation Crossmodal pour l'Inférence Multimodal Extendue",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Les modèles de langage général ont un excellent rendement dans des tâches de contexte complexe grâce à leur forte logique. Cependant, l'intégration de l'information visuelle et textuelle dans la théorie logique multimodal reste un grand défi. Les modèles de labellisation visuelle actuels rencontrent des difficultés pour analyser efficacement le contenu visuel et appliquer la logique, et souvent ne montrent pas le meilleur rendement dans des tâches logiques complexes. De plus, l'absence de référentiels détaillés empêche une évaluation précise de la capacité logique multimodale. Dans cet article, nous présentons R1-Onevision, un modèle de logique multimodale. Ce modèle a été conçu pour relier le reconnaissance visuelle à la logique profonde. Pour y parvenir, nous proposons un pipeline logique multimodal pour convertir des images en une représentation formelle du contexte. Ce pipeline est utilisé pour construire le jeu de données R1-Onevision et fournir des étiquettes logiques multimodales détaillées dans différentes domaines. De plus, le modèle R1-Onevision a été optimisé par des ajustements micro de normalisation et d'apprentissage par reforcement, ce qui a permis d'augmenter sa logique et sa capacité de généralisation. Pour évaluer la capacité logique multimodale de manière intégrale, nous présentons le référentiel R1-Onevision-Bench, qui s'adapte aux normes éducatives humaines, de l'école primaire à l'université. Les résultats des expérimentations montrent que R1-Onevision a atteint les meilleurs rendements, dépassant les modèles comme GPT-4o et Qwen2.5-VL dans les benchmarks de logique multimodale complexe.",
      "upvotes": 4,
      "discussionId": "67d3aa422f42ed5552e8eaee",
      "ai_keywords": [
        "multimodal reasoning",
        "cross-modal reasoning",
        "formal textural representations",
        "R1-Onevision dataset",
        "supervised fine-tuning",
        "reinforcement learning",
        "R1-Onevision-Bench",
        "GPT-4o",
        "Qwen2.5-VL"
      ]
    },
    "publishedAt": "2025-03-13T13:56:05.000Z",
    "title": "R1-Onevision: Advancing Generalized Multimodal Reasoning through\n  Cross-Modal Formalization",
    "summary": "Large Language Models have demonstrated remarkable reasoning capability in\ncomplex textual tasks. However, multimodal reasoning, which requires\nintegrating visual and textual information, remains a significant challenge.\nExisting visual-language models often struggle to effectively analyze and\nreason visual content, resulting in suboptimal performance on complex reasoning\ntasks. Moreover, the absence of comprehensive benchmarks hinders the accurate\nassessment of multimodal reasoning capabilities. In this paper, we introduce\nR1-Onevision, a multimodal reasoning model designed to bridge the gap between\nvisual perception and deep reasoning. To achieve this, we propose a cross-modal\nreasoning pipeline that transforms images into formal textural representations,\nenabling precise language-based reasoning. Leveraging this pipeline, we\nconstruct the R1-Onevision dataset which provides detailed, step-by-step\nmultimodal reasoning annotations across diverse domains. We further develop the\nR1-Onevision model through supervised fine-tuning and reinforcement learning to\ncultivate advanced reasoning and robust generalization abilities. To\ncomprehensively evaluate multimodal reasoning performance across different\ngrades, we introduce R1-Onevision-Bench, a benchmark aligned with human\neducational stages, covering exams from junior high school to university and\nbeyond. Experimental results show that R1-Onevision achieves state-of-the-art\nperformance, outperforming models such as GPT-4o and Qwen2.5-VL on multiple\nchallenging multimodal reasoning benchmarks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10615.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6364
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.10568",
      "authors": [
        {
          "_id": "67d3a952687a7a8a4963a030",
          "user": {
            "_id": "64e86fbd0c2413c3571ef7a6",
            "avatarUrl": "/avatars/960e17ba6a0d03fbd4700cd198adf5af.svg",
            "isPro": false,
            "fullname": "Haopeng Li",
            "user": "hp-l33",
            "type": "user"
          },
          "name": "Haopeng Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-14T08:56:10.236Z",
          "hidden": false
        },
        {
          "_id": "67d3a952687a7a8a4963a031",
          "name": "Jinyue Yang",
          "hidden": false
        },
        {
          "_id": "67d3a952687a7a8a4963a032",
          "name": "Guoqi Li",
          "hidden": false
        },
        {
          "_id": "67d3a952687a7a8a4963a033",
          "name": "Huan Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-13T17:19:51.000Z",
      "submittedOnDailyAt": "2025-03-14T02:32:07.479Z",
      "title": "Generation d'Images par Auto-Regression avec Décodage Parallèle et Randomisation",
      "submittedOnDailyBy": {
        "_id": "64e86fbd0c2413c3571ef7a6",
        "avatarUrl": "/avatars/960e17ba6a0d03fbd4700cd198adf5af.svg",
        "isPro": false,
        "fullname": "Haopeng Li",
        "user": "hp-l33",
        "type": "user"
      },
      "summary": "Introduisons ARPG. ARPG est un nouveau modèle de régression fonctionnelle visuelle qui aborde les limitations propres à l'approche séquentielle actuelle de LasTAR, permettant une génération parallèle aléatoire pour améliorer l'efficacité de l'inférence et éviter la charge de généralisation Zero-Shot. Notre perspective principale est que pour modéliser des séquences aléatoires efficacement, il est nécessaire d'avoir des guides explicites pour déterminer la position des étiquettes de prédiction. Par conséquent, nous proposons un nouveau cadre de guides qui sépare la guidance de la position du contenu et les codifie dans des paires de requête et clé-valeur. En intégrant cette guidance directement dans une structure d'attention causale, notre approche permet un entraînement et une génération dans des séquences complètement aléatoires, éliminant la nécessité d'attention bidirectionnelle. De cette manière, ARPG généralise naturellement dans des tâches Zero-Shot comme l'entrée d'images, OOP et des expansions de réseau-compte. De plus, en effectuant l'étape de sampling aléatoire sur 64 pas, ARPG réduit de plus de 75% le consommation de mémoire, atteignant un accroissement de 20 fois dans la quantité de transactions dans le cadre de référence ImageNet-1K 256, tout en se compagniant des modèles de régression fonctionnelle automatique récents de taille représentative.",
      "upvotes": 4,
      "discussionId": "67d3a957687a7a8a4963a179",
      "projectPage": "https://hp-l33.github.io/projects/arpg",
      "githubRepo": "https://github.com/hp-l33/ARPG",
      "ai_keywords": [
        "autoregressive model",
        "randomized parallel generation",
        "raster-order approaches",
        "inference efficiency",
        "zero-shot generalization",
        "sequential, predefined token generation order",
        "guided decoding framework",
        "positional guidance",
        "content representation",
        "queries",
        "key-value pairs",
        "causal attention mechanism",
        "fully random-order training",
        "bidirectional attention",
        "image inpainting",
        "outpainting",
        "resolution expansion",
        "parallel inference",
        "ImageNet-1K 256 benchmark",
        "FID",
        "sampling steps",
        "throughput",
        "memory consumption"
      ]
    },
    "publishedAt": "2025-03-13T13:19:51.000Z",
    "title": "Autoregressive Image Generation with Randomized Parallel Decoding",
    "summary": "We introduce ARPG, a novel visual autoregressive model that enables\nrandomized parallel generation, addressing the inherent limitations of\nconventional raster-order approaches, which hinder inference efficiency and\nzero-shot generalization due to their sequential, predefined token generation\norder. Our key insight is that effective random-order modeling necessitates\nexplicit guidance for determining the position of the next predicted token. To\nthis end, we propose a novel guided decoding framework that decouples\npositional guidance from content representation, encoding them separately as\nqueries and key-value pairs. By directly incorporating this guidance into the\ncausal attention mechanism, our approach enables fully random-order training\nand generation, eliminating the need for bidirectional attention. Consequently,\nARPG readily generalizes to zero-shot tasks such as image inpainting,\noutpainting, and resolution expansion. Furthermore, it supports parallel\ninference by concurrently processing multiple queries using a shared KV cache.\nOn the ImageNet-1K 256 benchmark, our approach attains an FID of 1.94 with only\n64 sampling steps, achieving over a 20-fold increase in throughput while\nreducing memory consumption by over 75% compared to representative recent\nautoregressive models at a similar scale.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10568.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64e86fbd0c2413c3571ef7a6",
      "avatarUrl": "/avatars/960e17ba6a0d03fbd4700cd198adf5af.svg",
      "fullname": "Haopeng Li",
      "name": "hp-l33",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.10391",
      "authors": [
        {
          "_id": "67d39679ea264394acf948ad",
          "name": "Yufan Deng",
          "hidden": false
        },
        {
          "_id": "67d39679ea264394acf948ae",
          "name": "Xun Guo",
          "hidden": false
        },
        {
          "_id": "67d39679ea264394acf948af",
          "name": "Yizhi Wang",
          "hidden": false
        },
        {
          "_id": "67d39679ea264394acf948b0",
          "name": "Jacob Zhiyuan Fang",
          "hidden": false
        },
        {
          "_id": "67d39679ea264394acf948b1",
          "name": "Angtian Wang",
          "hidden": false
        },
        {
          "_id": "67d39679ea264394acf948b2",
          "user": {
            "_id": "63468720dd6d90d82ccf3450",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
            "isPro": false,
            "fullname": "YSH",
            "user": "BestWishYsh",
            "type": "user"
          },
          "name": "Shenghai Yuan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-14T08:56:29.782Z",
          "hidden": false
        },
        {
          "_id": "67d39679ea264394acf948b3",
          "name": "Yiding Yang",
          "hidden": false
        },
        {
          "_id": "67d39679ea264394acf948b4",
          "name": "Bo Liu",
          "hidden": false
        },
        {
          "_id": "67d39679ea264394acf948b5",
          "name": "Haibin Huang",
          "hidden": false
        },
        {
          "_id": "67d39679ea264394acf948b6",
          "name": "Chongyang Ma",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-13T14:07:58.000Z",
      "submittedOnDailyAt": "2025-03-14T01:07:59.040Z",
      "title": "CINÉMA : Génération de multiples sous-titres vidéo par MLLM basée sur des guides",
      "submittedOnDailyBy": {
        "_id": "63468720dd6d90d82ccf3450",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
        "isPro": false,
        "fullname": "YSH",
        "user": "BestWishYsh",
        "type": "user"
      },
      "summary": "La génération de vidéos est en train d'expérimenter une révolution impressionnante grâce au développement de modèles d'apprentissage profond, notamment les modèles distribués. Les méthodes précédentes ont échoué à générer des qualités de vidéo élevées à partir de projets de texte ou simples images. Cependant, la génération de vidéos personnalisées pour plusieurs thèmes reste un grand défi peu exploré. Ce problème réside dans la complexité de synthétiser des vidéos qui incluent plusieurs thèmes et de maintenir une cohérence temporelle et spatiale. Les approches actuelles dépendent principalement de la correspondance entre les images de thème et les mots clés de projets de texte, ce qui génère de l'incertitude et rend difficile la modélisation efficace des relations entre thèmes. Dans cet article, nous proposons un nouveau cadre de travail pour la génération de vidéos cohérentes de plusieurs thèmes en étendant les modèles grands de langage multimodal (MLLM). Notre approche évite la nécessité d'une correspondance claire entre les images de thème et les entités textuelles, réduisant ainsi l'incertitude et l'effort d'annotation. En interprétant les relations entre thèmes, notre méthode favorise l'échelle et permet d'entraîner avec de grands ensembles de données variés. De plus, notre cadre est conditionnel par rapport à la quantité de thèmes variables, offrant une plus grande flexibilité dans la création de contenu personnalisé. À travers des évaluations étendues, nous montrons que notre approche significativement améliore la cohérence des thèmes et de toute la vidéo, ouvrant des voies pour des applications avancées en narration, interaction multimédia et génération de vidéos personnalisées.",
      "upvotes": 4,
      "discussionId": "67d3967aea264394acf94915",
      "ai_keywords": [
        "diffusion models",
        "multimodal large language model (MLLM)"
      ]
    },
    "publishedAt": "2025-03-13T10:07:58.000Z",
    "title": "CINEMA: Coherent Multi-Subject Video Generation via MLLM-Based Guidance",
    "summary": "Video generation has witnessed remarkable progress with the advent of deep\ngenerative models, particularly diffusion models. While existing methods excel\nin generating high-quality videos from text prompts or single images,\npersonalized multi-subject video generation remains a largely unexplored\nchallenge. This task involves synthesizing videos that incorporate multiple\ndistinct subjects, each defined by separate reference images, while ensuring\ntemporal and spatial consistency. Current approaches primarily rely on mapping\nsubject images to keywords in text prompts, which introduces ambiguity and\nlimits their ability to model subject relationships effectively. In this paper,\nwe propose CINEMA, a novel framework for coherent multi-subject video\ngeneration by leveraging Multimodal Large Language Model (MLLM). Our approach\neliminates the need for explicit correspondences between subject images and\ntext entities, mitigating ambiguity and reducing annotation effort. By\nleveraging MLLM to interpret subject relationships, our method facilitates\nscalability, enabling the use of large and diverse datasets for training.\nFurthermore, our framework can be conditioned on varying numbers of subjects,\noffering greater flexibility in personalized content creation. Through\nextensive evaluations, we demonstrate that our approach significantly improves\nsubject consistency, and overall video coherence, paving the way for advanced\napplications in storytelling, interactive media, and personalized video\ngeneration.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10391.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63468720dd6d90d82ccf3450",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
      "fullname": "YSH",
      "name": "BestWishYsh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 34
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.10636",
      "authors": [
        {
          "_id": "67d39ae1faaad4ed2df1cc61",
          "user": {
            "_id": "63041b541dd5d3c62486c294",
            "avatarUrl": "/avatars/a5286d562f7b9082730f760e66c3bf29.svg",
            "isPro": false,
            "fullname": "Ho Kei Cheng",
            "user": "hkchengrex",
            "type": "user"
          },
          "name": "Ho Kei Cheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-14T08:56:19.361Z",
          "hidden": false
        },
        {
          "_id": "67d39ae1faaad4ed2df1cc62",
          "name": "Alexander Schwing",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63041b541dd5d3c62486c294/ZHc69zwDiWkyl0cNnp5zn.png"
      ],
      "publishedAt": "2025-03-13T17:59:56.000Z",
      "submittedOnDailyAt": "2025-03-14T01:27:30.007Z",
      "title": "Condition du sortilège : Analyser et améliorer l'optimisation de la distribution du flux de base du flux conditionnel.",
      "submittedOnDailyBy": {
        "_id": "63041b541dd5d3c62486c294",
        "avatarUrl": "/avatars/a5286d562f7b9082730f760e66c3bf29.svg",
        "isPro": false,
        "fullname": "Ho Kei Cheng",
        "user": "hkchengrex",
        "type": "user"
      },
      "summary": "Minibatch óptimo de transmission toujours trie la route du flux. Cela permet de réduire la quantité de pas d'intégration et la complexité de la résolution numérique des équations différentielles lors de la validation, réduisant ainsi la quantité de calculs. Cependant, dans des cas conditionnels, l'optimum de transmission minibatch n'est pas suffisant. Cela se produit car le cartographage optimum de transmission standard ignore les conditions et une tendance conditionnelle apparaît lors de l'entraînement. En revanche, lors de la validation, on ne peut accéder à une distribution initiale biaisée et on observe des échantillons depuis toutes les distributions initiales sans biais. Cet écart entre l'entraînement et la validation dégrade le rendement des fonctions. Pour atténuer cet écart, nous proposons l'optimum de transmission conditionnel C²OT. C²OT ajoute des poids conditionnels à la matrice de coût lorsqu'on calcule le cartographage optimum de transmission. Les expériences ont été réalisées sur 8gaussians-to-moons, CIFAR-10, ImageNet-32x32 et ImageNet-256x256, et ont été adaptées tant à des conditions discrètes que continues. Notre méthode montre un meilleur rendement général dans différents panneaux de fonctions évaluées, comparé aux baselines actuels. Le code est disponible sur https://hkchengrex.github.io/C2OT.",
      "upvotes": 3,
      "discussionId": "67d39ae4faaad4ed2df1cd42",
      "projectPage": "https://hkchengrex.com/C2OT/",
      "githubRepo": "https://github.com/hkchengrex/C2OT",
      "ai_keywords": [
        "minibatch optimal transport",
        "flow matching",
        "integration steps",
        "numerical solvers",
        "ordinary differential equation",
        "optimal transport mapping",
        "conditional optimal transport",
        "C^2OT",
        "cost matrix",
        "optimal transport assignment",
        "discrete conditions",
        "continuous conditions",
        "8gaussians-to-moons",
        "CIFAR-10",
        "ImageNet-32x32",
        "ImageNet-256x256",
        "function evaluation budgets"
      ]
    },
    "publishedAt": "2025-03-13T13:59:56.000Z",
    "title": "The Curse of Conditions: Analyzing and Improving Optimal Transport for\n  Conditional Flow-Based Generation",
    "summary": "Minibatch optimal transport coupling straightens paths in unconditional flow\nmatching. This leads to computationally less demanding inference as fewer\nintegration steps and less complex numerical solvers can be employed when\nnumerically solving an ordinary differential equation at test time. However, in\nthe conditional setting, minibatch optimal transport falls short. This is\nbecause the default optimal transport mapping disregards conditions, resulting\nin a conditionally skewed prior distribution during training. In contrast, at\ntest time, we have no access to the skewed prior, and instead sample from the\nfull, unbiased prior distribution. This gap between training and testing leads\nto a subpar performance. To bridge this gap, we propose conditional optimal\ntransport C^2OT that adds a conditional weighting term in the cost matrix when\ncomputing the optimal transport assignment. Experiments demonstrate that this\nsimple fix works with both discrete and continuous conditions in\n8gaussians-to-moons, CIFAR-10, ImageNet-32x32, and ImageNet-256x256. Our method\nperforms better overall compared to the existing baselines across different\nfunction evaluation budgets. Code is available at\nhttps://hkchengrex.github.io/C2OT",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63041b541dd5d3c62486c294/ZHc69zwDiWkyl0cNnp5zn.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10636.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63041b541dd5d3c62486c294",
      "avatarUrl": "/avatars/a5286d562f7b9082730f760e66c3bf29.svg",
      "fullname": "Ho Kei Cheng",
      "name": "hkchengrex",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 25
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.10072",
      "authors": [
        {
          "_id": "67d390de29a092bdbb0a2aeb",
          "user": {
            "_id": "6331c3f618711776b468e9ec",
            "avatarUrl": "/avatars/af2c4bba031e474bf4fd2ea19e415aaf.svg",
            "isPro": false,
            "fullname": "Mia Mohammad Imran",
            "user": "imranraad",
            "type": "user"
          },
          "name": "Mia Mohammad Imran",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-03-14T02:14:33.331Z",
          "hidden": false
        },
        {
          "_id": "67d390de29a092bdbb0a2aec",
          "name": "Jaydeb Sarker",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-13T05:39:29.000Z",
      "submittedOnDailyAt": "2025-03-14T00:44:11.345Z",
      "title": "\"Nada hecho\" : Analyse de la recherche sur les erreurs dans le rapport d'erreurs",
      "submittedOnDailyBy": {
        "_id": "6331c3f618711776b468e9ec",
        "avatarUrl": "/avatars/af2c4bba031e474bf4fd2ea19e415aaf.svg",
        "isPro": false,
        "fullname": "Mia Mohammad Imran",
        "user": "imranraad",
        "type": "user"
      },
      "summary": "Le caractère des rapports d'erreurs est un problème grave qui affecte la structure coopérative du développement de logiciels open source. Les rapports d'erreurs sont essentiels pour identifier et résoudre les défauts, mais leur nature peut conduire à des interactions caractérisées par une émotionnalité excessive et un contexte riche, ce qui peut entraîner un comportement excessivement émotionnel. Cette étude a analysé 203 rapports d'erreurs sur GitHub, dont 81 présentaient des caractéristiques d'une émotionnalité excessive (81 rapports d'une émotionnalité excessive). Nos résultats montrent que les erreurs graves et la mauvaise prioritisation, la dissatisfaction avec les outils qui ne fonctionnent pas et la manque de communication professionnelle sont des causes fréquentes de ce type de comportement. Ces interactions caractérisées par un caractère excessive peuvent perdre des opportunités pour générer des débats productifs et obtenir des résultats efficaces. Nos résultats initials fournissent une proposition concrète et mise en œuvre pour améliorer la résolution d'erreurs, axée sur la réduction du caractère émotionnel excessif.",
      "upvotes": 2,
      "discussionId": "67d390df29a092bdbb0a2b2d",
      "projectPage": "https://zenodo.org/records/15015619"
    },
    "publishedAt": "2025-03-13T01:39:29.000Z",
    "title": "\"Silent Is Not Actually Silent\": An Investigation of Toxicity on Bug\n  Report Discussion",
    "summary": "Toxicity in bug report discussions poses significant challenges to the\ncollaborative dynamics of open-source software development. Bug reports are\ncrucial for identifying and resolving defects, yet their inherently\nproblem-focused nature and emotionally charged context make them susceptible to\ntoxic interactions. This study explores toxicity in GitHub bug reports through\na qualitative analysis of 203 bug threads, including 81 toxic ones. Our\nfindings reveal that toxicity frequently arises from misaligned perceptions of\nbug severity and priority, unresolved frustrations with tools, and lapses in\nprofessional communication. These toxic interactions not only derail productive\ndiscussions but also reduce the likelihood of actionable outcomes, such as\nlinking issues with pull requests. Our preliminary findings offer actionable\nrecommendations to improve bug resolution by mitigating toxicity.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10072.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6331c3f618711776b468e9ec",
      "avatarUrl": "/avatars/af2c4bba031e474bf4fd2ea19e415aaf.svg",
      "fullname": "Mia Mohammad Imran",
      "name": "imranraad",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.10291",
      "authors": [
        {
          "_id": "67d3cbea16d1ecea57ed096c",
          "name": "Weiyun Wang",
          "hidden": false
        },
        {
          "_id": "67d3cbea16d1ecea57ed096d",
          "name": "Zhangwei Gao",
          "hidden": false
        },
        {
          "_id": "67d3cbea16d1ecea57ed096e",
          "name": "Lianjie Chen",
          "hidden": false
        },
        {
          "_id": "67d3cbea16d1ecea57ed096f",
          "name": "Zhe Chen",
          "hidden": false
        },
        {
          "_id": "67d3cbea16d1ecea57ed0970",
          "name": "Jinguo Zhu",
          "hidden": false
        },
        {
          "_id": "67d3cbea16d1ecea57ed0971",
          "name": "Xiangyu Zhao",
          "hidden": false
        },
        {
          "_id": "67d3cbea16d1ecea57ed0972",
          "name": "Yangzhou Liu",
          "hidden": false
        },
        {
          "_id": "67d3cbea16d1ecea57ed0973",
          "name": "Yue Cao",
          "hidden": false
        },
        {
          "_id": "67d3cbea16d1ecea57ed0974",
          "name": "Shenglong Ye",
          "hidden": false
        },
        {
          "_id": "67d3cbea16d1ecea57ed0975",
          "name": "Xizhou Zhu",
          "hidden": false
        },
        {
          "_id": "67d3cbea16d1ecea57ed0976",
          "name": "Lewei Lu",
          "hidden": false
        },
        {
          "_id": "67d3cbea16d1ecea57ed0977",
          "name": "Haodong Duan",
          "hidden": false
        },
        {
          "_id": "67d3cbea16d1ecea57ed0978",
          "name": "Yu Qiao",
          "hidden": false
        },
        {
          "_id": "67d3cbea16d1ecea57ed0979",
          "name": "Jifeng Dai",
          "hidden": false
        },
        {
          "_id": "67d3cbea16d1ecea57ed097a",
          "name": "Wenhai Wang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/VPQRFEXv78LkPf2h5dgXm.png",
        "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/w91GHKp_sv2u4U-8VFLOn.png",
        "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/rG_h65J1RHcO0Lkbo5rg4.png"
      ],
      "publishedAt": "2025-03-13T12:03:37.000Z",
      "submittedOnDailyAt": "2025-03-14T06:40:16.854Z",
      "title": "VisualPRM : Diversées raisons pour modéliser un processus de récompense efficace des modèles",
      "submittedOnDailyBy": {
        "_id": "619507e7b74b6c591f794340",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/619507e7b74b6c591f794340/JbPDoy6Ko1V1-6oJJwFV8.jpeg",
        "isPro": false,
        "fullname": "Weiyun Wang",
        "user": "Weiyun1025",
        "type": "user"
      },
      "summary": "VisualPRM est une modèle de processus de récompense multi-modèle (PRM) avec 8B paramètres. Ce modèle améliore les capacités actuelles des modèles multi-modèles de langage (MLLMs) en utilisant une stratégie d'évaluation Best-of-N (BoN) entre différentes échelles et familles de modèles. En particulier, notre modèle améliore le rendement de 3 MLLMs et 4 échelles de modèle. Il a également réussi à améliorer de 5,9 points sur 7 benchmarks multi-modèles pour le modèle de haute capacité, InternVL2.5-78B. Les résultats des expérimentations montrent que notre modèle dépasse les modèles de récompense des résultats et la auto-cohérence dans l'évaluation BoN. De plus, pour encourager l'entraînement du modèle multi-modèle PRM, nous avons construit le sous-ensemble de données multi-modèle processus VisualPRM400K en utilisant une pipeline de données automatisée. Nous proposons également le benchmark VisualProcessBench, qui a des étiquettes de précision par étape expliquées par des humains, pour évaluer la capacité de notre modèle à détecter les erreurs dans les tâches de raisonnement multi-modèles. Notre recherche contribue au développement futur des modèles multi-modèles de langage et encourage leur progrès supplémentaire. Notre modèle, les données et les benchmarks sont disponibles sur https://internvl.github.io/blog/2025-03-13-VisualPRM/.",
      "upvotes": 1,
      "discussionId": "67d3cbed16d1ecea57ed0a75",
      "projectPage": "https://internvl.github.io/blog/2025-03-13-VisualPRM/",
      "githubRepo": "https://github.com/OpenGVLab/InternVL",
      "ai_keywords": [
        "Process Reward Model (PRM)",
        "Multimodal Large Language Models (MLLMs)",
        "Best-of-N (BoN)",
        "multimodal reasoning benchmarks",
        "Automated data pipeline",
        "VisualPRM400K",
        "VisualProcessBench",
        "human-annotated step-wise correctness labels",
        "multimodal PRMs",
        "erroneous steps in multimodal reasoning tasks"
      ]
    },
    "publishedAt": "2025-03-13T08:03:37.000Z",
    "title": "VisualPRM: An Effective Process Reward Model for Multimodal Reasoning",
    "summary": "We introduce VisualPRM, an advanced multimodal Process Reward Model (PRM)\nwith 8B parameters, which improves the reasoning abilities of existing\nMultimodal Large Language Models (MLLMs) across different model scales and\nfamilies with Best-of-N (BoN) evaluation strategies. Specifically, our model\nimproves the reasoning performance of three types of MLLMs and four different\nmodel scales. Even when applied to the highly capable InternVL2.5-78B, it\nachieves a 5.9-point improvement across seven multimodal reasoning benchmarks.\nExperimental results show that our model exhibits superior performance compared\nto Outcome Reward Models and Self-Consistency during BoN evaluation. To\nfacilitate the training of multimodal PRMs, we construct a multimodal process\nsupervision dataset VisualPRM400K using an automated data pipeline. For the\nevaluation of multimodal PRMs, we propose VisualProcessBench, a benchmark with\nhuman-annotated step-wise correctness labels, to measure the abilities of PRMs\nto detect erroneous steps in multimodal reasoning tasks. We hope that our work\ncan inspire more future research and contribute to the development of MLLMs.\nOur model, data, and benchmark are released in\nhttps://internvl.github.io/blog/2025-03-13-VisualPRM/.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/VPQRFEXv78LkPf2h5dgXm.png",
      "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/w91GHKp_sv2u4U-8VFLOn.png",
      "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/rG_h65J1RHcO0Lkbo5rg4.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10291.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "619507e7b74b6c591f794340",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/619507e7b74b6c591f794340/JbPDoy6Ko1V1-6oJJwFV8.jpeg",
      "fullname": "Weiyun Wang",
      "name": "Weiyun1025",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.09368",
      "authors": [
        {
          "_id": "67d2ca4be4696fda20bac029",
          "user": {
            "_id": "656c8721e8bf55919a9732c5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/bvIaJLEvlWal1PHCL4tDo.jpeg",
            "isPro": false,
            "fullname": "Nikolai",
            "user": "Nikolai10",
            "type": "user"
          },
          "name": "Nikolai Körber",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-14T08:57:39.634Z",
          "hidden": false
        },
        {
          "_id": "67d2ca4be4696fda20bac02a",
          "name": "Eduard Kromer",
          "hidden": false
        },
        {
          "_id": "67d2ca4be4696fda20bac02b",
          "name": "Andreas Siebert",
          "hidden": false
        },
        {
          "_id": "67d2ca4be4696fda20bac02c",
          "name": "Sascha Hauke",
          "hidden": false
        },
        {
          "_id": "67d2ca4be4696fda20bac02d",
          "name": "Daniel Mueller-Gritschneder",
          "hidden": false
        },
        {
          "_id": "67d2ca4be4696fda20bac02e",
          "name": "Björn Schuller",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-12T13:14:51.000Z",
      "submittedOnDailyAt": "2025-03-14T07:51:16.414Z",
      "title": "PerCoV2 : Amélioration de la compression visuelle d'images à haute résolution sous bitrate par modélisation d'images de masques de héros cachés",
      "submittedOnDailyBy": {
        "_id": "656c8721e8bf55919a9732c5",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/bvIaJLEvlWal1PHCL4tDo.jpeg",
        "isPro": false,
        "fullname": "Nikolai",
        "user": "Nikolai10",
        "type": "user"
      },
      "summary": "PerCoV2 est un nouveau système de compression visuelle d'images à faible bitrate ouvert conçu pour des applications limitées telles que les bannières et les storajons. Basé sur l'étude précédente de Careil, PerCoV2 s'étend dans l'écosystème de Stable Diffusion 3, améliorant l'efficacité de la codification d'entropie grâce à la modélisation explicite de la distribution discrète des ultra-sous-pixels. Cela permet une comparaison détaillée avec des méthodes automatiques de rétro-projection de jeux (VAR et MaskGIT) et une validation dans le cadre d'un grand benchmark de MSCOCO-30k. PerCoV2 dépasse les études précédentes sur trois aspects : (i) il réduit encore plus le bitrate tout en améliorant la précision des images, en maintenant une qualité visuelle compétitive, (ii) il présente un mode génératif hybride pour réduire le bitrate, et (iii) il est construit en utilisant uniquement des composants publiés. Les codes et modèles entraînés sont disponibles sur https://github.com/Nikolai10/PerCoV2.",
      "upvotes": 1,
      "discussionId": "67d2ca50e4696fda20bac1a8",
      "githubRepo": "https://github.com/Nikolai10/PerCoV2",
      "ai_keywords": [
        "PerCoV2",
        "ultralow bit-rate",
        "perceptual image compression",
        "bandwidth-constrained applications",
        "Stable Diffusion 3",
        "entropy coding",
        "discrete hyper-latent image distribution",
        "autoregressive methods",
        "VAR",
        "MaskGIT",
        "MSCOCO-30k",
        "image fidelity",
        "perceptual quality",
        "hybrid generation mode",
        "public components"
      ]
    },
    "publishedAt": "2025-03-12T09:14:51.000Z",
    "title": "PerCoV2: Improved Ultra-Low Bit-Rate Perceptual Image Compression with\n  Implicit Hierarchical Masked Image Modeling",
    "summary": "We introduce PerCoV2, a novel and open ultra-low bit-rate perceptual image\ncompression system designed for bandwidth- and storage-constrained\napplications. Building upon prior work by Careil et al., PerCoV2 extends the\noriginal formulation to the Stable Diffusion 3 ecosystem and enhances entropy\ncoding efficiency by explicitly modeling the discrete hyper-latent image\ndistribution. To this end, we conduct a comprehensive comparison of recent\nautoregressive methods (VAR and MaskGIT) for entropy modeling and evaluate our\napproach on the large-scale MSCOCO-30k benchmark. Compared to previous work,\nPerCoV2 (i) achieves higher image fidelity at even lower bit-rates while\nmaintaining competitive perceptual quality, (ii) features a hybrid generation\nmode for further bit-rate savings, and (iii) is built solely on public\ncomponents. Code and trained models will be released at\nhttps://github.com/Nikolai10/PerCoV2.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.09368.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "656c8721e8bf55919a9732c5",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/bvIaJLEvlWal1PHCL4tDo.jpeg",
      "fullname": "Nikolai",
      "name": "Nikolai10",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]