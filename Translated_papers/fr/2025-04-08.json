[
  {
    "paper": {
      "id": "2504.05298",
      "authors": [
        {
          "_id": "67f4a0ccfefcc542f83f84e7",
          "user": {
            "_id": "646ebf2298e8f749fc60ce38",
            "avatarUrl": "/avatars/5a76c9343451c24e9697d3165cdc0af6.svg",
            "isPro": false,
            "fullname": "Karan Dalal",
            "user": "karansdalal",
            "type": "user"
          },
          "name": "Karan Dalal",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-08T09:09:24.283Z",
          "hidden": false
        },
        {
          "_id": "67f4a0ccfefcc542f83f84e8",
          "user": {
            "_id": "66b01b126038fe024ae979d3",
            "avatarUrl": "/avatars/75b561be803fa28cb78c6cff9eb5ac54.svg",
            "isPro": false,
            "fullname": "Daniel Koceja",
            "user": "koceja",
            "type": "user"
          },
          "name": "Daniel Koceja",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-08T09:09:31.602Z",
          "hidden": false
        },
        {
          "_id": "67f4a0ccfefcc542f83f84e9",
          "user": {
            "_id": "638835fede1fa6adc5485a05",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669871090087-noauth.jpeg",
            "isPro": false,
            "fullname": "Gashon Hussein",
            "user": "GashonHussein",
            "type": "user"
          },
          "name": "Gashon Hussein",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-08T09:09:39.483Z",
          "hidden": false
        },
        {
          "_id": "67f4a0ccfefcc542f83f84ea",
          "name": "Jiarui Xu",
          "hidden": false
        },
        {
          "_id": "67f4a0ccfefcc542f83f84eb",
          "user": {
            "_id": "638fe91639f7e2a7f9d2a8c6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638fe91639f7e2a7f9d2a8c6/hB7DMVODcdAEUdQnXxWA8.jpeg",
            "isPro": false,
            "fullname": "Yue Zhao",
            "user": "zhaoyue-zephyrus",
            "type": "user"
          },
          "name": "Yue Zhao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-08T06:51:29.995Z",
          "hidden": false
        },
        {
          "_id": "67f4a0ccfefcc542f83f84ec",
          "name": "Youjin Song",
          "hidden": false
        },
        {
          "_id": "67f4a0ccfefcc542f83f84ed",
          "name": "Shihao Han",
          "hidden": false
        },
        {
          "_id": "67f4a0ccfefcc542f83f84ee",
          "name": "Ka Chun Cheung",
          "hidden": false
        },
        {
          "_id": "67f4a0ccfefcc542f83f84ef",
          "name": "Jan Kautz",
          "hidden": false
        },
        {
          "_id": "67f4a0ccfefcc542f83f84f0",
          "user": {
            "_id": "677c45511fbb93b90f1c6f3d",
            "avatarUrl": "/avatars/3a78fdd8d1debc8d267e80e4b7a6bf77.svg",
            "isPro": false,
            "fullname": "Carlos Guestrin",
            "user": "guestrin",
            "type": "user"
          },
          "name": "Carlos Guestrin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-08T09:10:26.609Z",
          "hidden": false
        },
        {
          "_id": "67f4a0ccfefcc542f83f84f1",
          "user": {
            "_id": "67ecbb7eb57a7b083182ea3f",
            "avatarUrl": "/avatars/8fb800e0729771e61ff0d2f9a05eb5b9.svg",
            "isPro": false,
            "fullname": "Tatsunori Hashimoto",
            "user": "hashtag56",
            "type": "user"
          },
          "name": "Tatsunori Hashimoto",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-08T09:10:20.291Z",
          "hidden": false
        },
        {
          "_id": "67f4a0ccfefcc542f83f84f2",
          "user": {
            "_id": "64931e7e2da595588288f161",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64931e7e2da595588288f161/4jOhJOFsU7RVFMgGk5kO7.jpeg",
            "isPro": false,
            "fullname": "Sanmi Koyejo",
            "user": "sanmikoyejo",
            "type": "user"
          },
          "name": "Sanmi Koyejo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-08T09:10:14.122Z",
          "hidden": false
        },
        {
          "_id": "67f4a0ccfefcc542f83f84f3",
          "user": {
            "_id": "64d42729f63b01b7f676b176",
            "avatarUrl": "/avatars/52e54bdd6a1fb6c774a40cd70f3d7925.svg",
            "isPro": false,
            "fullname": "Yejin Choi",
            "user": "yejinchoinka",
            "type": "user"
          },
          "name": "Yejin Choi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-08T09:10:07.480Z",
          "hidden": false
        },
        {
          "_id": "67f4a0ccfefcc542f83f84f4",
          "name": "Yu Sun",
          "hidden": false
        },
        {
          "_id": "67f4a0ccfefcc542f83f84f5",
          "name": "Xiaolong Wang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/638fe91639f7e2a7f9d2a8c6/cTDthYKFDTs8NDfvfVKJI.mp4"
      ],
      "publishedAt": "2025-04-07T17:56:31.000Z",
      "submittedOnDailyAt": "2025-04-08T02:38:37.647Z",
      "title": "Nous utilisons l'entraînement pour la génération de vidéos d'une minute.",
      "submittedOnDailyBy": {
        "_id": "638fe91639f7e2a7f9d2a8c6",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638fe91639f7e2a7f9d2a8c6/hB7DMVODcdAEUdQnXxWA8.jpeg",
        "isPro": false,
        "fullname": "Yue Zhao",
        "user": "zhaoyue-zephyrus",
        "type": "user"
      },
      "summary": "Transformers se heurtant aux difficultés à générer des vidéos de 1 minute de durée. Cela est dû au fait que les couches d'autonomie ne sont pas efficaces dans des contextes longs. D'autre part, des méthodes comme les couches Mandala, Gated Delta Network et Sliding Window Attention présentent une faible capacité à représenter des états cachés dans des histoires spatio-temporelles complexes. Nous avons expérimenté avec la couche TTT. Cette couche doit que la représentation cachée soit une réseau neuronal, ce qui lui donne une représentation plus forte. En ajoutant la couche TTT à un transformer pré-entraîné, nous pouvons générer des vidéos de 1 minute de durée à partir d'un board d'histoires textuelles. Pour tester cela, nous avons construit un dataset basé sur la bande de Tom et Jerry. En comparant avec des modèles de base comme Mandala~2, Gated Delta Network et Sliding Window Attention, la couche TTT transmet des histoires complexes de manière plus continue et dépasse en évaluations humaines les 34 points Elo sur 100 vidéos. C'est une nouvelle promotionnelle, mais les résultats peuvent être limités par les erreurs d'un modèle pré-entraîné de 5B. Nous pourrions également améliorer l'efficacité de notre implémentation. En raison des contraintes de ressources, nous n'avons que expérimenté avec des vidéos de 1 minute, mais cette méthodologie peut être étendue à des vidéos plus longues ou des histoires plus complexes. Les vidéos de mise en évidence, le code et les commentaires sont disponibles sur la suivante URL : https://test-time-training.github.io/video-dit",
      "upvotes": 29,
      "discussionId": "67f4a0cefefcc542f83f8592",
      "projectPage": "https://test-time-training.github.io/video-dit/",
      "githubRepo": "https://github.com/test-time-training/ttt-video-dit"
    },
    "publishedAt": "2025-04-07T13:56:31.000Z",
    "title": "One-Minute Video Generation with Test-Time Training",
    "summary": "Transformers today still struggle to generate one-minute videos because\nself-attention layers are inefficient for long context. Alternatives such as\nMamba layers struggle with complex multi-scene stories because their hidden\nstates are less expressive. We experiment with Test-Time Training (TTT) layers,\nwhose hidden states themselves can be neural networks, therefore more\nexpressive. Adding TTT layers into a pre-trained Transformer enables it to\ngenerate one-minute videos from text storyboards. For proof of concept, we\ncurate a dataset based on Tom and Jerry cartoons. Compared to baselines such as\nMamba~2, Gated DeltaNet, and sliding-window attention layers, TTT layers\ngenerate much more coherent videos that tell complex stories, leading by 34 Elo\npoints in a human evaluation of 100 videos per method. Although promising,\nresults still contain artifacts, likely due to the limited capability of the\npre-trained 5B model. The efficiency of our implementation can also be\nimproved. We have only experimented with one-minute videos due to resource\nconstraints, but the approach can be extended to longer videos and more complex\nstories. Sample videos, code and annotations are available at:\nhttps://test-time-training.github.io/video-dit",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/638fe91639f7e2a7f9d2a8c6/cTDthYKFDTs8NDfvfVKJI.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.05298.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "638fe91639f7e2a7f9d2a8c6",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638fe91639f7e2a7f9d2a8c6/hB7DMVODcdAEUdQnXxWA8.jpeg",
      "fullname": "Yue Zhao",
      "name": "zhaoyue-zephyrus",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.04718",
      "authors": [
        {
          "_id": "67f48eff463c4d1c4ae5284b",
          "user": {
            "_id": "64b74920fe6a108d03fed767",
            "avatarUrl": "/avatars/a2c05b809c36fa5fab8e1a43b3e67051.svg",
            "isPro": false,
            "fullname": "Minki Kang",
            "user": "Nardien",
            "type": "user"
          },
          "name": "Minki Kang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-08T09:12:42.979Z",
          "hidden": false
        },
        {
          "_id": "67f48eff463c4d1c4ae5284c",
          "name": "Jongwon Jeong",
          "hidden": false
        },
        {
          "_id": "67f48eff463c4d1c4ae5284d",
          "name": "Jaewoong Cho",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-07T04:01:17.000Z",
      "submittedOnDailyAt": "2025-04-08T06:04:55.039Z",
      "title": "Outil intégré pour l'authentification automatique de l'échelle de calcul dans les tests intégrés avec des outils pour petits modèles de langage",
      "submittedOnDailyBy": {
        "_id": "64b74920fe6a108d03fed767",
        "avatarUrl": "/avatars/a2c05b809c36fa5fab8e1a43b3e67051.svg",
        "isPro": false,
        "fullname": "Minki Kang",
        "user": "Nardien",
        "type": "user"
      },
      "summary": "Selon des études récentes, l'accroissement du calcul lors du test peut améliorer de manière significative les performances des petits modèles de langage (sLMs). Cependant, dans les études précédentes, des modèles plus grands étaient utilisés pour confirmer l'accroissement du calcul lors du test, mais l'étude sur la manière dont les sLMs peuvent être confirmés autonomement était insuffisante. Dans cette étude, on examine si les sLMs peuvent être confirmés avec confiance lors de l'accroissement du calcul lors du test. Enfin, il a été démontré que les sLMs ont des difficultés dans des tâches de vérification qui nécessitent de la mémoire, comme les calculs numériques ou la vérification de faits. Pour résoudre ces limitations, on propose la \"autoconfirmation intégrée avec des outils\" (T1), où des étapes de vérification avec de grande mémoire sont déléguées à des outils externes, comme des interpréteurs de code. Une analyse théorique montre que l'intégration d'outils réduit la nécessité de mémoire et améliore le rendement de l'accroissement lors du test. Dans les expériences avec le benchmark MATH, le modèle Llama-3.2 1B qui utilise T1 dépasse considérablement un modèle beaucoup plus grand, Llama-3.1 8B, lors de l'accroissement du calcul lors du test. De plus, T1 s'étend efficacement aux tâches de connaissance dense dans diverses domaines, comme MATH500 et MMLU-Pro. Ce travail montre la possibilité que l'intégration d'outils peut significativement améliorer la capacité d'autoconfirmation des sLMs.",
      "upvotes": 20,
      "discussionId": "67f48f00463c4d1c4ae52882"
    },
    "publishedAt": "2025-04-07T00:01:17.000Z",
    "title": "T1: Tool-integrated Self-verification for Test-time Compute Scaling in\n  Small Language Models",
    "summary": "Recent studies have demonstrated that test-time compute scaling effectively\nimproves the performance of small language models (sLMs). However, prior\nresearch has mainly examined test-time compute scaling with an additional\nlarger model as a verifier, leaving self-verification by sLMs underexplored. In\nthis work, we investigate whether sLMs can reliably self-verify their outputs\nunder test-time scaling. We find that even with knowledge distillation from\nlarger verifiers, sLMs struggle with verification tasks requiring memorization,\nsuch as numerical calculations and fact-checking. To address this limitation,\nwe propose Tool-integrated self-verification (T1), which delegates\nmemorization-heavy verification steps to external tools, such as a code\ninterpreter. Our theoretical analysis shows that tool integration reduces\nmemorization demands and improves test-time scaling performance. Experiments on\nthe MATH benchmark demonstrate that, with T1, a Llama-3.2 1B model under\ntest-time scaling outperforms the significantly larger Llama-3.1 8B model.\nMoreover, T1 generalizes effectively to both mathematical (MATH500) and\nmulti-domain knowledge-intensive tasks (MMLU-Pro). Our findings highlight the\npotential of tool integration to substantially improve the self-verification\nabilities of sLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.04718.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b74920fe6a108d03fed767",
      "avatarUrl": "/avatars/a2c05b809c36fa5fab8e1a43b3e67051.svg",
      "fullname": "Minki Kang",
      "name": "Nardien",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.05299",
      "authors": [
        {
          "_id": "67f4cf5b504263bce1236d87",
          "user": {
            "_id": "65d66b494bbd0d92b641cdbb",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d66b494bbd0d92b641cdbb/6-7dm7B-JxcoS1QlCPdMN.jpeg",
            "isPro": false,
            "fullname": "Andres Marafioti",
            "user": "andito",
            "type": "user"
          },
          "name": "Andrés Marafioti",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-08T08:46:33.366Z",
          "hidden": false
        },
        {
          "_id": "67f4cf5b504263bce1236d88",
          "user": {
            "_id": "648c9605565e3a44f3c9bb7b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/648c9605565e3a44f3c9bb7b/W5chvk17Zol6-2QSWkFVR.jpeg",
            "isPro": true,
            "fullname": "Orr Zohar",
            "user": "orrzohar",
            "type": "user"
          },
          "name": "Orr Zohar",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-08T09:11:26.498Z",
          "hidden": false
        },
        {
          "_id": "67f4cf5b504263bce1236d89",
          "user": {
            "_id": "61ed0ff29539bc0a3bbc89f4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61ed0ff29539bc0a3bbc89f4/iYWK7GParA7Ke5F6q132W.jpeg",
            "isPro": false,
            "fullname": "Miquel Farré",
            "user": "mfarre",
            "type": "user"
          },
          "name": "Miquel Farré",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-08T09:11:24.824Z",
          "hidden": false
        },
        {
          "_id": "67f4cf5b504263bce1236d8a",
          "name": "Merve Noyan",
          "hidden": false
        },
        {
          "_id": "67f4cf5b504263bce1236d8b",
          "name": "Elie Bakouch",
          "hidden": false
        },
        {
          "_id": "67f4cf5b504263bce1236d8c",
          "name": "Pedro Cuenca",
          "hidden": false
        },
        {
          "_id": "67f4cf5b504263bce1236d8d",
          "name": "Cyril Zakka",
          "hidden": false
        },
        {
          "_id": "67f4cf5b504263bce1236d8e",
          "user": {
            "_id": "61c141342aac764ce1654e43",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61c141342aac764ce1654e43/81AwoT5IQ_Xdw0OVw7TKu.jpeg",
            "isPro": false,
            "fullname": "Loubna Ben Allal",
            "user": "loubnabnl",
            "type": "user"
          },
          "name": "Loubna Ben Allal",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-08T09:11:22.957Z",
          "hidden": false
        },
        {
          "_id": "67f4cf5b504263bce1236d8f",
          "name": "Anton Lozhkov",
          "hidden": false
        },
        {
          "_id": "67f4cf5b504263bce1236d90",
          "name": "Nouamane Tazi",
          "hidden": false
        },
        {
          "_id": "67f4cf5b504263bce1236d91",
          "name": "Vaibhav Srivastav",
          "hidden": false
        },
        {
          "_id": "67f4cf5b504263bce1236d92",
          "name": "Joshua Lochner",
          "hidden": false
        },
        {
          "_id": "67f4cf5b504263bce1236d93",
          "name": "Hugo Larcher",
          "hidden": false
        },
        {
          "_id": "67f4cf5b504263bce1236d94",
          "name": "Mathieu Morlon",
          "hidden": false
        },
        {
          "_id": "67f4cf5b504263bce1236d95",
          "name": "Lewis Tunstall",
          "hidden": false
        },
        {
          "_id": "67f4cf5b504263bce1236d96",
          "name": "Leandro von Werra",
          "hidden": false
        },
        {
          "_id": "67f4cf5b504263bce1236d97",
          "name": "Thomas Wolf",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-07T17:58:57.000Z",
      "submittedOnDailyAt": "2025-04-08T06:10:52.675Z",
      "title": "SmolVLM : Une manière de rédefinir un modèle multimodal efficace pour des tailles réduites",
      "submittedOnDailyBy": {
        "_id": "65d66b494bbd0d92b641cdbb",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d66b494bbd0d92b641cdbb/6-7dm7B-JxcoS1QlCPdMN.jpeg",
        "isPro": false,
        "fullname": "Andres Marafioti",
        "user": "andito",
        "type": "user"
      },
      "summary": "Les modèles de langue visuelle (VLMs) offrent des résultats exceptionnels, mais leur forte consommation de ressources limite leur mise en œuvre sur des appareils portables ou de bord. Les petits VLMs généralement adoptent des décisions de conception de grands modèles, ce qui limite l'efficacité dans la tokenisation d'images et l'utilisation appropriée de la mémoire GPU, ainsi que la praticité des applications sur des appareils.\n\nNous présentons la série de modèles multi-phases SmolVLM, conçus pour une compression efficace des ressources avec l'objectif d'inférence économique en ressources. Nous avons conçu une architecture optimisée pour un faible coût de calcul, des stratégies de tokenisation et une exploration systématique par collecte de données, ce qui permet de prendre des décisions clés pour améliorer significativement le rendement dans les tâches d'images et de vidéos.\n\nNotre modèle le plus petit, SmolVLM-256M, utilise moins de 1GB de mémoire GPU lors de l'inférence et dépasse le modèle Idefics-80B, qui est 300 fois plus grand, après un développement de 18 mois. Notre modèle le plus grand, avec 2,2 milliards de paramètres, montre un rendement excellent en comparaison avec les VLMs les plus récents qui utilisent plus de la moitié de leur mémoire GPU. Les modèles SmolVLM montrent une compréhension forte des vidéos, au-delà de la compréhension des images statiques.\n\nNos résultats montrent que l'optimisation stratégique de l'architecture, une forte tokenisation efficace et une choix soigné de meilleurs données peuvent atteindre un grand amélioration du rendement, favorisant l'introduction pratique et énergétiquement efficace de modèles petits.",
      "upvotes": 17,
      "discussionId": "67f4cf5d504263bce1236dda",
      "projectPage": "https://huggingface.co/collections/HuggingFaceTB/smolvlm2-smallest-video-lm-ever-67ab6b5e84bf8aaa60cb17c7",
      "githubRepo": "https://github.com/huggingface/smollm"
    },
    "publishedAt": "2025-04-07T13:58:57.000Z",
    "title": "SmolVLM: Redefining small and efficient multimodal models",
    "summary": "Large Vision-Language Models (VLMs) deliver exceptional performance but\nrequire significant computational resources, limiting their deployment on\nmobile and edge devices. Smaller VLMs typically mirror design choices of larger\nmodels, such as extensive image tokenization, leading to inefficient GPU memory\nusage and constrained practicality for on-device applications.\n  We introduce SmolVLM, a series of compact multimodal models specifically\nengineered for resource-efficient inference. We systematically explore\narchitectural configurations, tokenization strategies, and data curation\noptimized for low computational overhead. Through this, we identify key design\nchoices that yield substantial performance gains on image and video tasks with\nminimal memory footprints.\n  Our smallest model, SmolVLM-256M, uses less than 1GB GPU memory during\ninference and outperforms the 300-times larger Idefics-80B model, despite an\n18-month development gap. Our largest model, at 2.2B parameters, rivals\nstate-of-the-art VLMs consuming twice the GPU memory. SmolVLM models extend\nbeyond static images, demonstrating robust video comprehension capabilities.\n  Our results emphasize that strategic architectural optimizations, aggressive\nyet efficient tokenization, and carefully curated training data significantly\nenhance multimodal performance, facilitating practical, energy-efficient\ndeployments at significantly smaller scales.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.05299.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65d66b494bbd0d92b641cdbb",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d66b494bbd0d92b641cdbb/6-7dm7B-JxcoS1QlCPdMN.jpeg",
      "fullname": "Andres Marafioti",
      "name": "andito",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 131
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.05305",
      "authors": [
        {
          "_id": "67f495437e1624ebbaf2d90e",
          "user": {
            "_id": "64842d1edc475c315e41123a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64842d1edc475c315e41123a/Yqam_dy0MnQm4Rn1_1_4N.jpeg",
            "isPro": false,
            "fullname": "Sangbeom Lim",
            "user": "SammyLim",
            "type": "user"
          },
          "name": "Sangbeom Lim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-08T06:51:37.341Z",
          "hidden": false
        },
        {
          "_id": "67f495437e1624ebbaf2d90f",
          "user": {
            "_id": "64c2c45ae818eec6128fdda3",
            "avatarUrl": "/avatars/d4399e25e6399345e263c7902789047e.svg",
            "isPro": false,
            "fullname": "Jun-Wan KIM",
            "user": "junwann",
            "type": "user"
          },
          "name": "Junwan Kim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-08T06:51:34.302Z",
          "hidden": false
        },
        {
          "_id": "67f495437e1624ebbaf2d910",
          "name": "Heeji Yoon",
          "hidden": false
        },
        {
          "_id": "67f495437e1624ebbaf2d911",
          "user": {
            "_id": "65d02dc017e2b305e0d7bf4f",
            "avatarUrl": "/avatars/2a50fd0541e7b0e200c577a661956696.svg",
            "isPro": false,
            "fullname": "Jaewoo Jung",
            "user": "crepejung00",
            "type": "user"
          },
          "name": "Jaewoo Jung",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-08T06:51:32.321Z",
          "hidden": false
        },
        {
          "_id": "67f495437e1624ebbaf2d912",
          "user": {
            "_id": "65cf717450818a335a1d3021",
            "avatarUrl": "/avatars/382a0e0f40f661cda1b2531e3e6ea2ee.svg",
            "isPro": false,
            "fullname": "Seungryong Kim",
            "user": "seungryong",
            "type": "user"
          },
          "name": "Seungryong Kim",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-08T09:13:34.541Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-07T17:59:44.000Z",
      "submittedOnDailyAt": "2025-04-08T01:57:21.829Z",
      "title": "URECA : Dans d'autres domaines également, retournez des commentaires à n'importe où.",
      "submittedOnDailyBy": {
        "_id": "64842d1edc475c315e41123a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64842d1edc475c315e41123a/Yqam_dy0MnQm4Rn1_1_4N.jpeg",
        "isPro": false,
        "fullname": "Sangbeom Lim",
        "user": "SammyLim",
        "type": "user"
      },
      "summary": "Le captioning à l'échelle d'une zone vise à générer des descriptions en langage naturel sur certaines zones d'une image, en mettant en avant leurs caractéristiques. Cependant, actuellement, les méthodes en cours de travail rencontrent des difficultés pour générer des captions uniques et précises pour des zones diversifiées, ce qui limite leur utilisabilité pratique. Pour résoudre ces problèmes, nous présentons le dataset URECA, qui considère l'appréhension à l'échelle d'une zone. URECA est différent de d'autres datasets existants, car il inclut des éléments de objets, de parties et de fonds variés, et a été construit de manière cohérente pour garantir les correspondances entre zones et captions. En se basant sur URECA, nous avons construit une chaîne d'opérations de données itérative, améliorant à chaque étape la sélection des zones et la génération des captions. A chaque étape de la chaîne d'opérations, nous utilisons des modèles de langage multimodales (MLLMs) pour créer des captions avec un contexte unique, améliorant à la fois la précision et la diversité littéraire. En se basant sur URECA, nous présentons un nouveau modèle de captioning, URECA, conçu pour codifier efficacement différentes zones. URECA introduit des changements simples mais puissants dans les MLLMs actuels, en maintenant les caractéristiques spatiales tout en permettant des descriptions détaillées et littéraires. Notre approche comprend la modélisation dynamique de masques et un encodeur de haute résolution de masques pour améliorer la nature des captions. Les expérimentations montrent que URECA atteint les meilleurs résultats sur URECA et présente une meilleure extensibilité sur les benchmarks de captioning à l'échelle d'une zone actuels.",
      "upvotes": 16,
      "discussionId": "67f495477e1624ebbaf2da2f",
      "projectPage": "https://cvlab-kaist.github.io/URECA/",
      "githubRepo": "https://github.com/cvlab-kaist/URECA"
    },
    "publishedAt": "2025-04-07T13:59:44.000Z",
    "title": "URECA: Unique Region Caption Anything",
    "summary": "Region-level captioning aims to generate natural language descriptions for\nspecific image regions while highlighting their distinguishing features.\nHowever, existing methods struggle to produce unique captions across\nmulti-granularity, limiting their real-world applicability. To address the need\nfor detailed region-level understanding, we introduce URECA dataset, a\nlarge-scale dataset tailored for multi-granularity region captioning. Unlike\nprior datasets that focus primarily on salient objects, URECA dataset ensures a\nunique and consistent mapping between regions and captions by incorporating a\ndiverse set of objects, parts, and background elements. Central to this is a\nstage-wise data curation pipeline, where each stage incrementally refines\nregion selection and caption generation. By leveraging Multimodal Large\nLanguage Models (MLLMs) at each stage, our pipeline produces distinctive and\ncontextually grounded captions with improved accuracy and semantic diversity.\nBuilding upon this dataset, we present URECA, a novel captioning model designed\nto effectively encode multi-granularity regions. URECA maintains essential\nspatial properties such as position and shape through simple yet impactful\nmodifications to existing MLLMs, enabling fine-grained and semantically rich\nregion descriptions. Our approach introduces dynamic mask modeling and a\nhigh-resolution mask encoder to enhance caption uniqueness. Experiments show\nthat URECA achieves state-of-the-art performance on URECA dataset and\ngeneralizes well to existing region-level captioning benchmarks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.05305.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64842d1edc475c315e41123a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64842d1edc475c315e41123a/Yqam_dy0MnQm4Rn1_1_4N.jpeg",
      "fullname": "Sangbeom Lim",
      "name": "SammyLim",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.04823",
      "authors": [
        {
          "_id": "67f4a8ead83d88e30c450332",
          "user": {
            "_id": "6411c22dd52c57f628f7c331",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678885386428-noauth.jpeg",
            "isPro": false,
            "fullname": "ruikang liu",
            "user": "ruikangliu",
            "type": "user"
          },
          "name": "Ruikang Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-08T06:51:24.004Z",
          "hidden": false
        },
        {
          "_id": "67f4a8ead83d88e30c450333",
          "name": "Yuxuan Sun",
          "hidden": false
        },
        {
          "_id": "67f4a8ead83d88e30c450334",
          "name": "Manyi Zhang",
          "hidden": false
        },
        {
          "_id": "67f4a8ead83d88e30c450335",
          "name": "Haoli Bai",
          "hidden": false
        },
        {
          "_id": "67f4a8ead83d88e30c450336",
          "name": "Xianzhi Yu",
          "hidden": false
        },
        {
          "_id": "67f4a8ead83d88e30c450337",
          "name": "Tiezheng Yu",
          "hidden": false
        },
        {
          "_id": "67f4a8ead83d88e30c450338",
          "name": "Chun Yuan",
          "hidden": false
        },
        {
          "_id": "67f4a8ead83d88e30c450339",
          "name": "Lu Hou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-07T08:22:45.000Z",
      "submittedOnDailyAt": "2025-04-08T03:14:58.007Z",
      "title": "Kammichizehd Heltsing ? Recherche expérimentale du module de Kammichizehd Iterus Sintining",
      "submittedOnDailyBy": {
        "_id": "6411c22dd52c57f628f7c331",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678885386428-noauth.jpeg",
        "isPro": false,
        "fullname": "ruikang liu",
        "user": "ruikangliu",
        "type": "user"
      },
      "summary": "Le développement récent de modèles de langue a démontré des résultats exceptionnels dans des tâches complexes, mais le processus de raisonnement de chaîne de pensée à long terme augmente l'encombrement de l'inférence. L'optimisation est largement utilisée pour réduire les coûts d'inférence dans des modèles de grande taille, mais son impact sur des modèles d'optimisation n'a pas été suffisamment étudié. Dans cette étude, une étude systématique initiale est effectuée, évaluant DeepSeek-R1-Distilled Qwen et les familles de LLaMA (de 1,5B à 70B en paramètres) et QwQ-32B. L'étude inclut des optimisations de poids, de cache KV, d'activations et elle est effectuée avec des algorithmes plus récents sur des largeurs de bits différentes. Des évaluations extrêmes sont effectuées dans des référentiels de mathématiques (AIME, MATH-500), de science (GPQA) et de programmation (LiveCodeBench). Il a été découvert que l'optimisation sans perte de W8A8 ou W4A16 est possible, mais l'utilisation de largeurs de bits basses peut affecter la précision. De plus, il a été trouvé que le taille du modèle, son origine et la difficulté de la tâche sont des facteurs clés pour le rendement. Le plus inattendu est que les modèles optimisés ne augmentent pas la longueur de la sortie. De plus, le rendement peut être amélioré par l'échelle des modèles et les pas de raisonnement stratégiques. Tous les modèles optimisés et les codes sont disponibles sur https://github.com/ruikangliu/Quantized-Reasoning-Models.",
      "upvotes": 12,
      "discussionId": "67f4a8efd83d88e30c45043c"
    },
    "publishedAt": "2025-04-07T04:22:45.000Z",
    "title": "Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning\n  Models",
    "summary": "Recent advancements in reasoning language models have demonstrated remarkable\nperformance in complex tasks, but their extended chain-of-thought reasoning\nprocess increases inference overhead. While quantization has been widely\nadopted to reduce the inference cost of large language models, its impact on\nreasoning models remains understudied. In this study, we conduct the first\nsystematic study on quantized reasoning models, evaluating the open-sourced\nDeepSeek-R1-Distilled Qwen and LLaMA families ranging from 1.5B to 70B\nparameters, and QwQ-32B. Our investigation covers weight, KV cache, and\nactivation quantization using state-of-the-art algorithms at varying\nbit-widths, with extensive evaluation across mathematical (AIME, MATH-500),\nscientific (GPQA), and programming (LiveCodeBench) reasoning benchmarks. Our\nfindings reveal that while lossless quantization can be achieved with W8A8 or\nW4A16 quantization, lower bit-widths introduce significant accuracy risks. We\nfurther identify model size, model origin, and task difficulty as critical\ndeterminants of performance. Contrary to expectations, quantized models do not\nexhibit increased output lengths. In addition, strategically scaling the model\nsizes or reasoning steps can effectively enhance the performance. All quantized\nmodels and codes will be open-sourced in\nhttps://github.com/ruikangliu/Quantized-Reasoning-Models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.04823.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6411c22dd52c57f628f7c331",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678885386428-noauth.jpeg",
      "fullname": "ruikang liu",
      "name": "ruikangliu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.02828",
      "authors": [
        {
          "_id": "67f01efe81f4f7a1b43f4930",
          "user": {
            "_id": "6279a4f6812ee439d9c72d3f",
            "avatarUrl": "/avatars/a35a5674d1168d345d9fc5018485283e.svg",
            "isPro": false,
            "fullname": "Jinqi Luo",
            "user": "peterljq",
            "type": "user"
          },
          "name": "Jinqi Luo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-06T08:11:28.131Z",
          "hidden": false
        },
        {
          "_id": "67f01efe81f4f7a1b43f4931",
          "name": "Tianjiao Ding",
          "hidden": false
        },
        {
          "_id": "67f01efe81f4f7a1b43f4932",
          "user": {
            "_id": "6627043551cedbbb0b352047",
            "avatarUrl": "/avatars/8c4f80a24f8ba8761d33692c4ed28c29.svg",
            "isPro": false,
            "fullname": "Kwan Ho Ryan Chan",
            "user": "ryanckh",
            "type": "user"
          },
          "name": "Kwan Ho Ryan Chan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-08T09:13:58.769Z",
          "hidden": false
        },
        {
          "_id": "67f01efe81f4f7a1b43f4933",
          "name": "Hancheng Min",
          "hidden": false
        },
        {
          "_id": "67f01efe81f4f7a1b43f4934",
          "name": "Chris Callison-Burch",
          "hidden": false
        },
        {
          "_id": "67f01efe81f4f7a1b43f4935",
          "name": "René Vidal",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-03T17:59:58.000Z",
      "submittedOnDailyAt": "2025-04-08T02:59:02.267Z",
      "title": "Lancet : Édition d'images avec représentations combinées\nTraduction\n\n(Note : Bien que cela n'ait pas été demandé, pour assurer la précision et la profondeur de la traduction, j'ai ajouté une expression plus naturelle en français. Si une traduction complète selon le format original est nécessaire, vous pouvez utiliser la traduction fournie.)",
      "submittedOnDailyBy": {
        "_id": "6279a4f6812ee439d9c72d3f",
        "avatarUrl": "/avatars/a35a5674d1168d345d9fc5018485283e.svg",
        "isPro": false,
        "fullname": "Jinqi Luo",
        "user": "peterljq",
        "type": "user"
      },
      "summary": "Le design de modifications pour des images (CoLan) est largement utilisé dans des modèles d'édition d'images. Les méthodes actuelles d'édition sélectionnent la direction d'édition dans des zones d'insertion de texte ou des zones de ponctuation et planifient l'ordre de manipulation de l'expression. Cependant, cet ordre présente des problèmes importants : évaluer trop l'intensité de l'édition peut nuire à la cohérence visuelle, tandis qu'une évaluation insuffisante peut sous-estimer le succès de l'édition dans le modèle. En particulier, les images sources requièrent des intensités de modification différentes, et répéter des essais et des échecs pour trouver la bonne est coûteusement inefficace. Pour résoudre ces problèmes, nous proposons le Concept Lancet (CoLan). CoLan est un cadre de framework pour des plugins d'édition d'images basé sur DeepImage, conçu pour effectuer des manipulations de l'expression fondamentales. Pendant l'inférence, la source d'entrée est décomposée en une combinaison linéaire dispersée dans l'espace potentiel (espace d'insertion de texte ou espace de ponctuation de DeepImage). Cela permet d'évaluer avec précision si un concept existe dans chaque image, ce qui facilite l'édition. En fonction des types d'édition (substitution/addition/suppression), un processus de propagation personnalisé est exécuté et la direction d'édition correspondante est attribuée. Pour modéliser adéquatement l'espace de concepts, nous avons créé le jeu de données CoLan-150K de représentations conceptuelles. Ce jeu de données inclut des explications et des scénarios visuels et verbals utilisant différents termes et phrases dans le dossier de l'espace potentiel. Dans des expériences basées sur des lignes d'édition d'images basées sur DeepImage, le méthode appliquée avec CoLan a atteint les meilleurs résultats en termes d'efficacité d'édition et de préservation de cohérence.",
      "upvotes": 11,
      "discussionId": "67f01eff81f4f7a1b43f4971",
      "projectPage": "https://peterljq.github.io/project/colan",
      "githubRepo": "https://github.com/peterljq/Concept-Lancet",
      "ai_keywords": [
        "diffusion models",
        "image editing",
        "text embedding",
        "score space",
        "latent space",
        "sparse linear combination",
        "visual concepts",
        "concept transplantation",
        "conceptual representation dataset",
        "CoLan-150K",
        "CoLan"
      ]
    },
    "publishedAt": "2025-04-03T13:59:58.000Z",
    "title": "Concept Lancet: Image Editing with Compositional Representation\n  Transplant",
    "summary": "Diffusion models are widely used for image editing tasks. Existing editing\nmethods often design a representation manipulation procedure by curating an\nedit direction in the text embedding or score space. However, such a procedure\nfaces a key challenge: overestimating the edit strength harms visual\nconsistency while underestimating it fails the editing task. Notably, each\nsource image may require a different editing strength, and it is costly to\nsearch for an appropriate strength via trial-and-error. To address this\nchallenge, we propose Concept Lancet (CoLan), a zero-shot plug-and-play\nframework for principled representation manipulation in diffusion-based image\nediting. At inference time, we decompose the source input in the latent (text\nembedding or diffusion score) space as a sparse linear combination of the\nrepresentations of the collected visual concepts. This allows us to accurately\nestimate the presence of concepts in each image, which informs the edit. Based\non the editing task (replace/add/remove), we perform a customized concept\ntransplant process to impose the corresponding editing direction. To\nsufficiently model the concept space, we curate a conceptual representation\ndataset, CoLan-150K, which contains diverse descriptions and scenarios of\nvisual terms and phrases for the latent dictionary. Experiments on multiple\ndiffusion-based image editing baselines show that methods equipped with CoLan\nachieve state-of-the-art performance in editing effectiveness and consistency\npreservation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02828.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6279a4f6812ee439d9c72d3f",
      "avatarUrl": "/avatars/a35a5674d1168d345d9fc5018485283e.svg",
      "fullname": "Jinqi Luo",
      "name": "peterljq",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.05288",
      "authors": [
        {
          "_id": "67f4adf70864398533373364",
          "name": "Mingyang Fu",
          "hidden": false
        },
        {
          "_id": "67f4adf70864398533373365",
          "name": "Yuyang Peng",
          "hidden": false
        },
        {
          "_id": "67f4adf70864398533373366",
          "name": "Benlin Liu",
          "hidden": false
        },
        {
          "_id": "67f4adf70864398533373367",
          "name": "Yao Wan",
          "hidden": false
        },
        {
          "_id": "67f4adf70864398533373368",
          "name": "Dongping Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-07T17:39:31.000Z",
      "submittedOnDailyAt": "2025-04-08T03:33:51.436Z",
      "title": "LiveVQA : Bibliothèque de Connaissances Culturelles Vives",
      "submittedOnDailyBy": {
        "_id": "643be8879f5d314db2d9ed23",
        "avatarUrl": "/avatars/64e9bb2c4e10fbe03e2b81afedf40865.svg",
        "isPro": false,
        "fullname": "Chen Dongping",
        "user": "shuaishuaicdp",
        "type": "user"
      },
      "summary": "LiveVQA est un ensemble de données qui recoit automatiquement les derniers connaissances visuelles de l'internet et les intègre dans des problèmes de VQA synthétiques. LiveVQA comprend 3,602 questions visuelles à des niveaux unique et multiple, provenant de 14 catégories de nouvelles de 6 sites web de nouvelles. Il est caractérisé par une haute qualité de la concordance entre les images et le texte et par la véracité des données. Dans des évaluations avec 15 modèles de MLLM (par exemple, la famille GPT-4o, Gemma-3, Qwen-2.5-VL), les modèles améliorés montrent un rendement généralement meilleur et ont démontré une capacité importante pour faire des inférences visuelles complexes. Les modèles appropriés pour des problèmes contextuels clairement montrent que, lorsqu'ils sont utilisés avec des cartes d'analyse ou d'autres instruments pour répondre à des questions visuelles qui nécessitent le plus récent des connaissances visuelles, ils peuvent commettre de grands erreurs. Cela révèle une zone importante pour des recherches futures.",
      "upvotes": 8,
      "discussionId": "67f4adfb086439853337346e"
    },
    "publishedAt": "2025-04-07T13:39:31.000Z",
    "title": "LiveVQA: Live Visual Knowledge Seeking",
    "summary": "We introduce LiveVQA, an automatically collected dataset of latest visual\nknowledge from the Internet with synthesized VQA problems. LiveVQA consists of\n3,602 single- and multi-hop visual questions from 6 news websites across 14\nnews categories, featuring high-quality image-text coherence and authentic\ninformation. Our evaluation across 15 MLLMs (e.g., GPT-4o, Gemma-3, and\nQwen-2.5-VL family) demonstrates that stronger models perform better overall,\nwith advanced visual reasoning capabilities proving crucial for complex\nmulti-hop questions. Despite excellent performance on textual problems, models\nwith tools like search engines still show significant gaps when addressing\nvisual questions requiring latest visual knowledge, highlighting important\nareas for future research.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.05288.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "643be8879f5d314db2d9ed23",
      "avatarUrl": "/avatars/64e9bb2c4e10fbe03e2b81afedf40865.svg",
      "fullname": "Chen Dongping",
      "name": "shuaishuaicdp",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.04715",
      "authors": [
        {
          "_id": "67f4a59bf51362f606eb84e8",
          "name": "Will Cai",
          "hidden": false
        },
        {
          "_id": "67f4a59bf51362f606eb84e9",
          "name": "Tianneng Shi",
          "hidden": false
        },
        {
          "_id": "67f4a59bf51362f606eb84ea",
          "name": "Xuandong Zhao",
          "hidden": false
        },
        {
          "_id": "67f4a59bf51362f606eb84eb",
          "name": "Dawn Song",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-07T03:57:41.000Z",
      "submittedOnDailyAt": "2025-04-08T02:57:43.185Z",
      "title": "Puis-je recevoir quelque chose en échange de paiement via l'API du modèle de langage ? Surveillance de la substitution des modèles",
      "submittedOnDailyBy": {
        "_id": "6275a465597c70eb8949fce5",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6275a465597c70eb8949fce5/ph4UogqMurMB0hSXZC38w.png",
        "isPro": false,
        "fullname": "Xuandong Zhao",
        "user": "Xuandong",
        "type": "user"
      },
      "summary": "La propagation de grands modèles de langage (LLM) développés à l'aide de l'API des LLM génère de grands problèmes de fiabilité : les utilisateurs paient pour les fonctions (par exemple, taille, performance) des modèles annoncés, mais les fournisseurs peuvent remplacer un modèle spécifique par un plus léger et de moindre qualité pour réduire les coûts d'exploitation. Cette incertitude détruit l'équité et réduit la confiance, complicant la création de référentiels fiables. Détecter ces remplacements est difficile en raison de la nature des modèles comme \"boîte noire\", bien qu'il s'agisse généralement de l'interaction entre les requêtes d'entrée et les sorties. Dans cet article, le problème de la détection de remplacements de modèles dans l'API des LLM est formulé. Les méthodes de vérification actuelles (tests statistiques basés sur les sorties, évaluation de référentiels, analyse des probabilités des registres) sont évaluées systématiquement dans le contexte de scénarios d'attaques réels (par exemple, entraînement du modèle, remplacement aléatoire, évasion de référentiels). Nos résultats révèlent les limitations des méthodes qui dépendent uniquement des résultats de la requête, en particulier dans les attaques avec des mouvements minimaux. L'analyse des probabilités des registres fournit une garantie forte dans ces cas, mais son accessibilité est limitée. Enfin, la possibilité de solutions basées sur du matériel comme les Environnements d'Exécution Sécurisé (TEEs) est discutée, et l'équilibre entre sécurité, performance et acceptation par les fournisseurs est analysé. Le code est disponible sur https://github.com/sunblaze-ucb/llm-api-audit.",
      "upvotes": 3,
      "discussionId": "67f4a59cf51362f606eb8529",
      "githubRepo": "https://github.com/sunblaze-ucb/llm-api-audit"
    },
    "publishedAt": "2025-04-06T23:57:41.000Z",
    "title": "Are You Getting What You Pay For? Auditing Model Substitution in LLM\n  APIs",
    "summary": "The proliferation of Large Language Models (LLMs) accessed via black-box APIs\nintroduces a significant trust challenge: users pay for services based on\nadvertised model capabilities (e.g., size, performance), but providers may\ncovertly substitute the specified model with a cheaper, lower-quality\nalternative to reduce operational costs. This lack of transparency undermines\nfairness, erodes trust, and complicates reliable benchmarking. Detecting such\nsubstitutions is difficult due to the black-box nature, typically limiting\ninteraction to input-output queries. This paper formalizes the problem of model\nsubstitution detection in LLM APIs. We systematically evaluate existing\nverification techniques, including output-based statistical tests, benchmark\nevaluations, and log probability analysis, under various realistic attack\nscenarios like model quantization, randomized substitution, and benchmark\nevasion. Our findings reveal the limitations of methods relying solely on text\noutputs, especially against subtle or adaptive attacks. While log probability\nanalysis offers stronger guarantees when available, its accessibility is often\nlimited. We conclude by discussing the potential of hardware-based solutions\nlike Trusted Execution Environments (TEEs) as a pathway towards provable model\nintegrity, highlighting the trade-offs between security, performance, and\nprovider adoption. Code is available at\nhttps://github.com/sunblaze-ucb/llm-api-audit",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.04715.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6275a465597c70eb8949fce5",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6275a465597c70eb8949fce5/ph4UogqMurMB0hSXZC38w.png",
      "fullname": "Xuandong Zhao",
      "name": "Xuandong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.05304",
      "authors": [
        {
          "_id": "67f48c11412c65a9d4e3e552",
          "user": {
            "_id": "638067fcb334960c987fbeda",
            "avatarUrl": "/avatars/87f1eaaf6b3a9c0d47d6f406261ccc18.svg",
            "isPro": false,
            "fullname": "Hansheng Chen",
            "user": "Lakonik",
            "type": "user"
          },
          "name": "Hansheng Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-08T06:51:39.675Z",
          "hidden": false
        },
        {
          "_id": "67f48c11412c65a9d4e3e553",
          "name": "Kai Zhang",
          "hidden": false
        },
        {
          "_id": "67f48c11412c65a9d4e3e554",
          "name": "Hao Tan",
          "hidden": false
        },
        {
          "_id": "67f48c11412c65a9d4e3e555",
          "name": "Zexiang Xu",
          "hidden": false
        },
        {
          "_id": "67f48c11412c65a9d4e3e556",
          "name": "Fujun Luan",
          "hidden": false
        },
        {
          "_id": "67f48c11412c65a9d4e3e557",
          "name": "Leonidas Guibas",
          "hidden": false
        },
        {
          "_id": "67f48c11412c65a9d4e3e558",
          "name": "Gordon Wetzstein",
          "hidden": false
        },
        {
          "_id": "67f48c11412c65a9d4e3e559",
          "name": "Sai Bi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-07T17:59:42.000Z",
      "submittedOnDailyAt": "2025-04-08T01:13:17.992Z",
      "title": "Gaussian Mixture Flow Model",
      "submittedOnDailyBy": {
        "_id": "638067fcb334960c987fbeda",
        "avatarUrl": "/avatars/87f1eaaf6b3a9c0d47d6f406261ccc18.svg",
        "isPro": false,
        "fullname": "Hansheng Chen",
        "user": "Lakonik",
        "type": "user"
      },
      "summary": "Les modèles de diffusion prédisent la distribution de bruit en approximant une distribution gaussienne et en estimant la moyenne. D'autre part, les modèles de matching de flux réestablissent la moyenne de la distribution gaussienne en utilisant des vitesses de flux. Cependant, ces modèles souvent perdent de la performance lorsqu'ils sont présentés à peu de pas en raison d'erreurs de discrétisation et tendent à générer des couleurs oversampled pour guider un classificateur de classes libres (CFG). Pour relever ces limitations, nous proposons un nouveau modèle de matching de flux de mélanges gaussiens (GMFlow) : GMFlow prédit des paramètres de mélanges gaussiens dynamiques au lieu de prédire la moyenne, capturant une distribution dynamique des vitesses de flux. Cela peut être entraîné en utilisant la perte de variance KL. GMFlow généralise les modèles antérieurs entraînés avec une seule distribution gaussienne en utilisant la perte d'élimination de bruit L_2, comme les modèles de diffusion et les modèles de matching de flux. Pendant l'inférence, nous proposons un solveur SDE/ODE analytique en utilisant la distribution de bruit analytique et la vitesse, ce qui permet de réaliser des échantillonnages précis à peu de pas. De plus, nous introduisons une nouvelle technique de guide probabiliste pour atténuer les problèmes d'oversampling du CFG et améliorer la qualité de la génération d'images. Dans des expériences extensives, GMFlow atteint une précision de 0.942 sur ImageNet 256×256 en seulement 6 pas, et montre un rendement constante en qualité de la génération par rapport aux lignes basées sur le matching de flux.",
      "upvotes": 2,
      "discussionId": "67f48c13412c65a9d4e3e5d7",
      "githubRepo": "https://github.com/Lakonik/GMFlow"
    },
    "publishedAt": "2025-04-07T13:59:42.000Z",
    "title": "Gaussian Mixture Flow Matching Models",
    "summary": "Diffusion models approximate the denoising distribution as a Gaussian and\npredict its mean, whereas flow matching models reparameterize the Gaussian mean\nas flow velocity. However, they underperform in few-step sampling due to\ndiscretization error and tend to produce over-saturated colors under\nclassifier-free guidance (CFG). To address these limitations, we propose a\nnovel Gaussian mixture flow matching (GMFlow) model: instead of predicting the\nmean, GMFlow predicts dynamic Gaussian mixture (GM) parameters to capture a\nmulti-modal flow velocity distribution, which can be learned with a KL\ndivergence loss. We demonstrate that GMFlow generalizes previous diffusion and\nflow matching models where a single Gaussian is learned with an L_2 denoising\nloss. For inference, we derive GM-SDE/ODE solvers that leverage analytic\ndenoising distributions and velocity fields for precise few-step sampling.\nFurthermore, we introduce a novel probabilistic guidance scheme that mitigates\nthe over-saturation issues of CFG and improves image generation quality.\nExtensive experiments demonstrate that GMFlow consistently outperforms flow\nmatching baselines in generation quality, achieving a Precision of 0.942 with\nonly 6 sampling steps on ImageNet 256times256.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.05304.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "638067fcb334960c987fbeda",
      "avatarUrl": "/avatars/87f1eaaf6b3a9c0d47d6f406261ccc18.svg",
      "fullname": "Hansheng Chen",
      "name": "Lakonik",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.03193",
      "authors": [
        {
          "_id": "67f3f25019592b36b6d8b30c",
          "user": {
            "_id": "6436290e76dbfd731bcf1f55",
            "avatarUrl": "/avatars/63af84ca382d799c4a000594c994d4bb.svg",
            "isPro": false,
            "fullname": "Xin Zhang",
            "user": "XinNUS",
            "type": "user"
          },
          "name": "Xin Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-08T06:52:23.735Z",
          "hidden": false
        },
        {
          "_id": "67f3f25019592b36b6d8b30d",
          "name": "Robby T. Tan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-04T05:44:45.000Z",
      "submittedOnDailyAt": "2025-04-08T06:12:20.759Z",
      "title": "Versión ManDaBar : On examine l'intersection entre le modèle de base Vision et le modèle de langue Vision, et on étudie le modèle de segmentation sémantique généralisée par domaine.",
      "submittedOnDailyBy": {
        "_id": "6436290e76dbfd731bcf1f55",
        "avatarUrl": "/avatars/63af84ca382d799c4a000594c994d4bb.svg",
        "isPro": false,
        "fullname": "Xin Zhang",
        "user": "XinNUS",
        "type": "user"
      },
      "summary": "Vision Foundation Models (VFMs) et Vision-Langue Models (VLMs) possèdent une capacité de généralisation puissante et se sont concentrés sur la séparation de représentations (DGSS) dans une large gamme de domaines. Cependant, les méthodes actuelles de DGSS ignorent les avantages complémentaires des VFMs et VLMs. Les VFMs (par exemple, DINOv2) montrent des résultats excellents dans le reconnaissance de caractéristiques détaillées, tandis que les VLMs (par exemple, CLIP) fournissent un alignement documentaire fort, bien que confrontés à des difficultés lors du traitement d'échelles grandes. Bien que les VFMs et VLMs aient des avantages complémentaires, il est difficile d'intégrer eux de manière efficace en utilisant des structures d'attention. L'augmentation des tokens complique le modèle de séquences longues. Dans ce contexte, nous proposons un nouveau cadre de combinaison basé sur Mamba, appelé MFuser, qui permet l'intégration efficace des forces des VFMs et VLMs tout en maintenant la scalabilité linéaire. MFuser est composé de deux composants clés : MVFuser et MTEnhancer. MVFuser ajuste les deux modèles simultanément et applique un appliquateur commun qui capture tant l'action temporelle que l'espacelle. MTEnhancer, de son côté, enveloppe un module d'attention-Mamba intégré pour le profil d'image, améliorant l'encapsulation des documents. Notre approche ne augmente pas significativement le consommation de calcul, mais atteint la précision des caractéristiques locales et un alignement documentaire fort. Des expériences extensives montrent que MFuser dépasse significativement les meilleures techniques de DGSS, atteignant un 68.20 mIoU sur les benchmarks synthèse-réel et 71.87 mIoU sur les benchmarks réel-réel. Le code est disponible sur https://github.com/devinxzhang/MFuser.",
      "upvotes": 2,
      "discussionId": "67f3f25219592b36b6d8b3d0",
      "projectPage": "https://devinxzhang.github.io/MFuser_ProjPage/",
      "githubRepo": "https://github.com/devinxzhang/MFuser"
    },
    "publishedAt": "2025-04-04T01:44:45.000Z",
    "title": "Mamba as a Bridge: Where Vision Foundation Models Meet Vision Language\n  Models for Domain-Generalized Semantic Segmentation",
    "summary": "Vision Foundation Models (VFMs) and Vision-Language Models (VLMs) have gained\ntraction in Domain Generalized Semantic Segmentation (DGSS) due to their strong\ngeneralization capabilities. However, existing DGSS methods often rely\nexclusively on either VFMs or VLMs, overlooking their complementary strengths.\nVFMs (e.g., DINOv2) excel at capturing fine-grained features, while VLMs (e.g.,\nCLIP) provide robust text alignment but struggle with coarse granularity.\nDespite their complementary strengths, effectively integrating VFMs and VLMs\nwith attention mechanisms is challenging, as the increased patch tokens\ncomplicate long-sequence modeling. To address this, we propose MFuser, a novel\nMamba-based fusion framework that efficiently combines the strengths of VFMs\nand VLMs while maintaining linear scalability in sequence length. MFuser\nconsists of two key components: MVFuser, which acts as a co-adapter to jointly\nfine-tune the two models by capturing both sequential and spatial dynamics; and\nMTEnhancer, a hybrid attention-Mamba module that refines text embeddings by\nincorporating image priors. Our approach achieves precise feature locality and\nstrong text alignment without incurring significant computational overhead.\nExtensive experiments demonstrate that MFuser significantly outperforms\nstate-of-the-art DGSS methods, achieving 68.20 mIoU on synthetic-to-real and\n71.87 mIoU on real-to-real benchmarks. The code is available at\nhttps://github.com/devinxzhang/MFuser.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.03193.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6436290e76dbfd731bcf1f55",
      "avatarUrl": "/avatars/63af84ca382d799c4a000594c994d4bb.svg",
      "fullname": "Xin Zhang",
      "name": "XinNUS",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.02812",
      "authors": [
        {
          "_id": "67f4b54ddf6757586b52a4eb",
          "name": "Van Nguyen Nguyen",
          "hidden": false
        },
        {
          "_id": "67f4b54ddf6757586b52a4ec",
          "name": "Stephen Tyree",
          "hidden": false
        },
        {
          "_id": "67f4b54ddf6757586b52a4ed",
          "name": "Andrew Guo",
          "hidden": false
        },
        {
          "_id": "67f4b54ddf6757586b52a4ee",
          "name": "Mederic Fourmy",
          "hidden": false
        },
        {
          "_id": "67f4b54ddf6757586b52a4ef",
          "name": "Anas Gouda",
          "hidden": false
        },
        {
          "_id": "67f4b54ddf6757586b52a4f0",
          "name": "Taeyeop Lee",
          "hidden": false
        },
        {
          "_id": "67f4b54ddf6757586b52a4f1",
          "name": "Sungphill Moon",
          "hidden": false
        },
        {
          "_id": "67f4b54ddf6757586b52a4f2",
          "name": "Hyeontae Son",
          "hidden": false
        },
        {
          "_id": "67f4b54ddf6757586b52a4f3",
          "name": "Lukas Ranftl",
          "hidden": false
        },
        {
          "_id": "67f4b54ddf6757586b52a4f4",
          "name": "Jonathan Tremblay",
          "hidden": false
        },
        {
          "_id": "67f4b54ddf6757586b52a4f5",
          "name": "Eric Brachmann",
          "hidden": false
        },
        {
          "_id": "67f4b54ddf6757586b52a4f6",
          "name": "Bertram Drost",
          "hidden": false
        },
        {
          "_id": "67f4b54ddf6757586b52a4f7",
          "name": "Vincent Lepetit",
          "hidden": false
        },
        {
          "_id": "67f4b54ddf6757586b52a4f8",
          "name": "Carsten Rother",
          "hidden": false
        },
        {
          "_id": "67f4b54ddf6757586b52a4f9",
          "name": "Stan Birchfield",
          "hidden": false
        },
        {
          "_id": "67f4b54ddf6757586b52a4fa",
          "name": "Jiri Matas",
          "hidden": false
        },
        {
          "_id": "67f4b54ddf6757586b52a4fb",
          "name": "Yann Labbe",
          "hidden": false
        },
        {
          "_id": "67f4b54ddf6757586b52a4fc",
          "name": "Martin Sundermeyer",
          "hidden": false
        },
        {
          "_id": "67f4b54ddf6757586b52a4fd",
          "name": "Tomas Hodan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-03T17:55:19.000Z",
      "submittedOnDailyAt": "2025-04-08T04:04:46.852Z",
      "title": "BOP 2024 Modèle-Basé et Modèle-Libre Estimation de Position 3D d'Objets",
      "submittedOnDailyBy": {
        "_id": "67400e2bf2d0992e1f373b9c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/lysHv-F1aIjZXPaApGS8k.png",
        "isPro": false,
        "fullname": "hugw",
        "user": "hugw",
        "type": "user"
      },
      "summary": "BOP チャレンジ 2024 présente le méthode d'évaluation, les ensembles de données et les résultats. Ce concours est la sixième édition d'une compétition publique sur l'estimation des positions 6D d'objets et des technologies liées. En 2024, l'objectif est de transformer BOP en un concours dans une scène mondialement célèbre.\n\nTout d'abord, on a introduit la tâche sans modèles. Dans cette tâche, les méthodes doivent localiser les objets sur le tableau en utilisant seulement des vidéos de référence. Ensuite, on a rédefini la tâche de détection des objets 6D, en supprimant l'information de reconnaissance des objets visibles dans les images de test. De plus, on a introduit le nouveau ensemble de données BOP-H3 en utilisant des capteurs de haute résolution et un écran AR/VR, offrant une expérience proche de scènes mondialement célèbres. BOP-H3 inclut aussi bien des modèles 3D que des vidéos de référence pour soutenir des tâches avec et sans modèles.\n\nLes participants ont concurrence dans 7 trajectoires. Ces dernières ont été définies selon la tâche, l'ensemble de référence et le groupe de données. En particulier, le méthode optimale pour la décision de position 6D basée sur des modèles (FreeZeV2.1) en 2024 a montré un amélioration de précision de 22% sur BOP-Classic-Core par rapport au méthode optimale de 2023 (GenFlow), bien que soit considérablement plus lente (24,9 secondes par image). La meilleure pratique pour ce travail en 2024 est Co-op, qui fonctionne en 0,8 secondes par image, est 25 fois plus rapide que GenFlow et présente un amélioration de précision de 13%.\n\nLe méthode présente des classifications comme la détection 6D et la décision de position 6D, mais il faut prendre en compte que les temps d'exécution sont longs. Dans la détection 2D basée sur des modèles, le méthode optimale de 2024 (MUSE) a amélioré relativement de 21% par rapport au méthode optimale de 2023 (CNOS). Cependant, la précision de détection d'objets non visibles est notablement moins (-53%) par rapport à GDet2023. Le système d'évaluation en ligne est ouvert et peut être utilisé à l'adresse http://bop.felk.cvut.cz/.",
      "upvotes": 2,
      "discussionId": "67f4b552df6757586b52a613"
    },
    "publishedAt": "2025-04-03T13:55:19.000Z",
    "title": "BOP Challenge 2024 on Model-Based and Model-Free 6D Object Pose\n  Estimation",
    "summary": "We present the evaluation methodology, datasets and results of the BOP\nChallenge 2024, the sixth in a series of public competitions organized to\ncapture the state of the art in 6D object pose estimation and related tasks. In\n2024, our goal was to transition BOP from lab-like setups to real-world\nscenarios. First, we introduced new model-free tasks, where no 3D object models\nare available and methods need to onboard objects just from provided reference\nvideos. Second, we defined a new, more practical 6D object detection task where\nidentities of objects visible in a test image are not provided as input. Third,\nwe introduced new BOP-H3 datasets recorded with high-resolution sensors and\nAR/VR headsets, closely resembling real-world scenarios. BOP-H3 include 3D\nmodels and onboarding videos to support both model-based and model-free tasks.\nParticipants competed on seven challenge tracks, each defined by a task, object\nonboarding setup, and dataset group. Notably, the best 2024 method for\nmodel-based 6D localization of unseen objects (FreeZeV2.1) achieves 22% higher\naccuracy on BOP-Classic-Core than the best 2023 method (GenFlow), and is only\n4% behind the best 2023 method for seen objects (GPose2023) although being\nsignificantly slower (24.9 vs 2.7s per image). A more practical 2024 method for\nthis task is Co-op which takes only 0.8s per image and is 25X faster and 13%\nmore accurate than GenFlow. Methods have a similar ranking on 6D detection as\non 6D localization but higher run time. On model-based 2D detection of unseen\nobjects, the best 2024 method (MUSE) achieves 21% relative improvement compared\nto the best 2023 method (CNOS). However, the 2D detection accuracy for unseen\nobjects is still noticealy (-53%) behind the accuracy for seen objects\n(GDet2023). The online evaluation system stays open and is available at\nhttp://bop.felk.cvut.cz/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02812.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67400e2bf2d0992e1f373b9c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/lysHv-F1aIjZXPaApGS8k.png",
      "fullname": "hugw",
      "name": "hugw",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.02882",
      "authors": [
        {
          "_id": "67f4d15a8f00d281c155880f",
          "name": "Sunghee Jung",
          "hidden": false
        },
        {
          "_id": "67f4d15a8f00d281c1558810",
          "name": "Donghun Lee",
          "hidden": false
        },
        {
          "_id": "67f4d15a8f00d281c1558811",
          "name": "Shinbok Lee",
          "hidden": false
        },
        {
          "_id": "67f4d15a8f00d281c1558812",
          "name": "Gaeun Seo",
          "hidden": false
        },
        {
          "_id": "67f4d15a8f00d281c1558813",
          "name": "Daniel Lee",
          "hidden": false
        },
        {
          "_id": "67f4d15a8f00d281c1558814",
          "name": "Byeongil Ko",
          "hidden": false
        },
        {
          "_id": "67f4d15a8f00d281c1558815",
          "name": "Junrae Cho",
          "hidden": false
        },
        {
          "_id": "67f4d15a8f00d281c1558816",
          "name": "Kihyun Kim",
          "hidden": false
        },
        {
          "_id": "67f4d15a8f00d281c1558817",
          "name": "Eunggyun Kim",
          "hidden": false
        },
        {
          "_id": "67f4d15a8f00d281c1558818",
          "name": "Myeongcheol Shin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-02T05:47:28.000Z",
      "submittedOnDailyAt": "2025-04-08T06:11:32.041Z",
      "title": "DiaTool-DPO : Technique d'Optimisation Directe des Intérêts dans des Modèles de Langue de Grande Taille Associés à des Outils",
      "submittedOnDailyBy": {
        "_id": "65e30342e8b017ee1384824c",
        "avatarUrl": "/avatars/e5d07b037f611ccfaf719959d971d102.svg",
        "isPro": false,
        "fullname": "Sunghee Jung",
        "user": "hash2430",
        "type": "user"
      },
      "summary": "Les modèles de langage de grande taille (TA-LLMs) avec augmentation de tools montrent de bons résultats dans des applications réelles, mais présentent des problèmes avec des demandes incomplètes et des demandes en dehors du domaine. L'approche actuelle se base principalement sur plusieurs entraînements supervisés basés sur des projets d'experts, et dans ce travail, nous proposons un nouveau méthode appelé DiaTool-DPO, qui utilise l'optimisation de préférences directes. L'interface des TA-LLMs est configurée à travers un processus d'encodage de Markov de 5 états de dialogue différents, et les demandes des utilisateurs sont classifiées en trois types selon le chemin de mouvement des états de dialogue. Un ensemble de données de projets représentant des paires de flux de dialogue corrects est automatiquement construit, et une perte de fonction de perte spécifique est introduite pour le contrôle du dialogue. Selon des évaluations détaillées, DiaTool-DPO approche le rendement de GPT-4o (94,8% de collecte d'information, 91% de refus de requêtes de tools), présente une amélioration significative par rapport aux normes (44% vs 9,6% de valeurs) et maintient les fonctions clés. Cette approche ouvre de nouvelles perspectives pour le développement des TA-LLMs pour traiter divers scénarios réels sans nécessité d'autres simulations d'experts ou de l'étiquetage humain.",
      "upvotes": 2,
      "discussionId": "67f4d15c8f00d281c1558870"
    },
    "publishedAt": "2025-04-02T01:47:28.000Z",
    "title": "DiaTool-DPO: Multi-Turn Direct Preference Optimization for\n  Tool-Augmented Large Language Models",
    "summary": "Tool-Augmented Larage Language Models (TA-LLMs) have shown promise in\nreal-world applications, but face challenges in handling incomplete queries and\nout-of-scope requests. While existing approaches rely mainly on Supervised\nFine-Tuning with expert trajectories, we propose DiaTool-DPO, a novel method\nthat enhances TA-LLM's dialogue capabilities through Direct Preference\nOptimization. We model TA-LLM interactions as a Markov Decision Process with 5\ndistinct dialogue states and categorize user queries into 3 types based on\ntheir state transition trajectories. We automatically construct paired\ntrajectory datasets of correct and incorrect dialogue flows and introduce a\nspecialized objective loss for dialogue control. Our comprehensive evaluation\ndemonstrates that DiaTool-DPO approaches GPT-4o's performance (94.8% in\ninformation gathering, 91% in tool call rejection) with substantial\nimprovements over baseline (44% and 9.6% respectively) while maintaining core\nfunctionality. Our approach opens new possibilities for developing TA-LLMs that\ncan handle diverse real-world scenarios without requiring additional expert\ndemonstrations or human labeling.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02882.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65e30342e8b017ee1384824c",
      "avatarUrl": "/avatars/e5d07b037f611ccfaf719959d971d102.svg",
      "fullname": "Sunghee Jung",
      "name": "hash2430",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.03964",
      "authors": [
        {
          "_id": "67f4889c7e1624ebbaef710d",
          "user": {
            "_id": "652ee41e7b0079ff035b2269",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652ee41e7b0079ff035b2269/jv-2-UPHN_e9Lhwl3Evwh.jpeg",
            "isPro": false,
            "fullname": "Simon Lee",
            "user": "Simonlee711",
            "type": "user"
          },
          "name": "Simon A. Lee",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-08T06:51:44.271Z",
          "hidden": false
        },
        {
          "_id": "67f4889c7e1624ebbaef710e",
          "name": "Anthony Wu",
          "hidden": false
        },
        {
          "_id": "67f4889c7e1624ebbaef710f",
          "name": "Jeffrey N. Chiang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-04T22:14:12.000Z",
      "submittedOnDailyAt": "2025-04-08T05:40:27.532Z",
      "title": "Clinical Modern BERT : Efficient Encoder de Contextes Longs dans les Documents Biomédicaux",
      "submittedOnDailyBy": {
        "_id": "652ee41e7b0079ff035b2269",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652ee41e7b0079ff035b2269/jv-2-UPHN_e9Lhwl3Evwh.jpeg",
        "isPro": false,
        "fullname": "Simon Lee",
        "user": "Simonlee711",
        "type": "user"
      },
      "summary": "Introducing Clinical Modern BERT. Cet modèle est un encodeur basé sur le Transformer qui intègre une grande échelle de littérature biomédicale, des enregistrements cliniques, des ontologies médicales, des résumés de PubMed, des données cliniques de MIMIC IV et des codes médicaux avec leurs explications contextuelles. Construit sur l'encodeur de contexte naturel le plus avancé, ModernBERT, il introduit diverses mises à jour architecturales telles que l'Embedding de Position Rotatoire (RoPE), l'Attention Flash et une longueur de contexte étendue de 8 192 tokens, adaptées spécifiquement aux innovations dans les domaines biomédicaux et cliniques. Clinical Modern BERT excelle en générant des représentations riches et significatives capables d'adapter aux contextes longs. Ceci est démontré par une analyse des poids et des évaluations expérimentales sur les benchmarks de NLP clinique.",
      "upvotes": 1,
      "discussionId": "67f4889d7e1624ebbaef715a",
      "githubRepo": "https://github.com/Simonlee711/Clinical_ModernBERT"
    },
    "publishedAt": "2025-04-04T18:14:12.000Z",
    "title": "Clinical ModernBERT: An efficient and long context encoder for\n  biomedical text",
    "summary": "We introduce Clinical ModernBERT, a transformer based encoder pretrained on\nlarge scale biomedical literature, clinical notes, and medical ontologies,\nincorporating PubMed abstracts, MIMIC IV clinical data, and medical codes with\ntheir textual descriptions. Building on ModernBERT the current state of the art\nnatural language text encoder featuring architectural upgrades such as rotary\npositional embeddings (RoPE), Flash Attention, and extended context length up\nto 8,192 tokens our model adapts these innovations specifically for biomedical\nand clinical domains. Clinical ModernBERT excels at producing semantically rich\nrepresentations tailored for long context tasks. We validate this both by\nanalyzing its pretrained weights and through empirical evaluation on a\ncomprehensive suite of clinical NLP benchmarks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.03964.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "652ee41e7b0079ff035b2269",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652ee41e7b0079ff035b2269/jv-2-UPHN_e9Lhwl3Evwh.jpeg",
      "fullname": "Simon Lee",
      "name": "Simonlee711",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.03770",
      "authors": [
        {
          "_id": "67f4a80a261cb1c328d9b0a2",
          "name": "Yi Nian",
          "hidden": false
        },
        {
          "_id": "67f4a80a261cb1c328d9b0a3",
          "user": {
            "_id": "6614243f67d7bfc73afc6b77",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6614243f67d7bfc73afc6b77/ifdjAb_BPraEJ2eIySk_K.jpeg",
            "isPro": false,
            "fullname": "Shenzhe Zhu",
            "user": "Chouoftears",
            "type": "user"
          },
          "name": "Shenzhe Zhu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-08T06:51:27.601Z",
          "hidden": false
        },
        {
          "_id": "67f4a80a261cb1c328d9b0a4",
          "name": "Yuehan Qin",
          "hidden": false
        },
        {
          "_id": "67f4a80a261cb1c328d9b0a5",
          "name": "Li Li",
          "hidden": false
        },
        {
          "_id": "67f4a80a261cb1c328d9b0a6",
          "name": "Ziyi Wang",
          "hidden": false
        },
        {
          "_id": "67f4a80a261cb1c328d9b0a7",
          "name": "Chaowei Xiao",
          "hidden": false
        },
        {
          "_id": "67f4a80a261cb1c328d9b0a8",
          "name": "Yue Zhao",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6614243f67d7bfc73afc6b77/z0oklTQ627SEQN42jYoat.png"
      ],
      "publishedAt": "2025-04-03T05:00:28.000Z",
      "submittedOnDailyAt": "2025-04-08T03:10:00.355Z",
      "title": "JailDAM : Détection de Jail Breaks à l'aide de la Mémoire Adaptative des Modèles de Vision-Langue",
      "submittedOnDailyBy": {
        "_id": "6614243f67d7bfc73afc6b77",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6614243f67d7bfc73afc6b77/ifdjAb_BPraEJ2eIySk_K.jpeg",
        "isPro": false,
        "fullname": "Shenzhe Zhu",
        "user": "Chouoftears",
        "type": "user"
      },
      "summary": "La Modèle Large de Langue Multimodal (MLLM) démontre une excellente performance dans des tâches de langage visuel, mais présente un risque grave, en particulier lors de la génération de contenus nocifs, en raison d'attaques par picardage de bras. Ce type d'attaque consiste à manipuler délibérément les fonctions de sécurité du modèle, ce qui peut conduire à la génération de contenus inappropriés ou instables. La détection de ces attaques est cruciale pour garantir l'introduction responsable de MLLM. Actuellement, les méthodes de détection de picardage de bras rencontrent trois défis principaux : (1) elles se basent sur l'état caché ou les gradients du modèle, ce qui limite leur application aux modèles de type blanc (en gros, modèles qui permettent de l'accès à ses fonctions internes) ; (2) elles requièrent une charge de calcul élevée, ce qui limite la détection en temps réel ; (3) les ensembles de données étiquetés complètement avec des contenus nocifs sont rares dans les environnements réels. Pour faire face à ces défis, nous utilisons un cadre de travail applicable dans des tests appelé \"JAILDAM\". Notre méthode utilise un approche basée sur la mémoire pour guider la représentation du savoir instable, évitant directement l'interaction avec des données nocives. En mettant à jour dynamiquement le savoir instable lors des tests, notre cadre améliore la généralisation pour des scénarios non vus et maintient l'efficacité. Les expériences sur des benchmarks de picardage de bras dans les VLM montrent que JAILDAM fournit le meilleur rendement dans la détection de contenus nocifs, améliorant à la fois la précision et la vitesse.",
      "upvotes": 1,
      "discussionId": "67f4a80b261cb1c328d9b0e9",
      "githubRepo": "https://github.com/ShenzheZhu/JailDAM"
    },
    "publishedAt": "2025-04-03T01:00:28.000Z",
    "title": "JailDAM: Jailbreak Detection with Adaptive Memory for Vision-Language\n  Model",
    "summary": "Multimodal large language models (MLLMs) excel in vision-language tasks but\nalso pose significant risks of generating harmful content, particularly through\njailbreak attacks. Jailbreak attacks refer to intentional manipulations that\nbypass safety mechanisms in models, leading to the generation of inappropriate\nor unsafe content. Detecting such attacks is critical to ensuring the\nresponsible deployment of MLLMs. Existing jailbreak detection methods face\nthree primary challenges: (1) Many rely on model hidden states or gradients,\nlimiting their applicability to white-box models, where the internal workings\nof the model are accessible; (2) They involve high computational overhead from\nuncertainty-based analysis, which limits real-time detection, and (3) They\nrequire fully labeled harmful datasets, which are often scarce in real-world\nsettings. To address these issues, we introduce a test-time adaptive framework\ncalled JAILDAM. Our method leverages a memory-based approach guided by\npolicy-driven unsafe knowledge representations, eliminating the need for\nexplicit exposure to harmful data. By dynamically updating unsafe knowledge\nduring test-time, our framework improves generalization to unseen jailbreak\nstrategies while maintaining efficiency. Experiments on multiple VLM jailbreak\nbenchmarks demonstrate that JAILDAM delivers state-of-the-art performance in\nharmful content detection, improving both accuracy and speed.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6614243f67d7bfc73afc6b77/z0oklTQ627SEQN42jYoat.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.03770.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6614243f67d7bfc73afc6b77",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6614243f67d7bfc73afc6b77/ifdjAb_BPraEJ2eIySk_K.jpeg",
      "fullname": "Shenzhe Zhu",
      "name": "Chouoftears",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  }
]