[
  {
    "paper": {
      "id": "2505.24120",
      "authors": [
        {
          "_id": "683fc08da33aeee1124887c4",
          "name": "Ai Jian",
          "hidden": false
        },
        {
          "_id": "683fc08da33aeee1124887c5",
          "user": {
            "_id": "660aab2c878289c5b34f9e97",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/660aab2c878289c5b34f9e97/yxx1-lR8x5o6KaEpZDXQq.jpeg",
            "isPro": false,
            "fullname": "weijie qiu",
            "user": "qiuwj",
            "type": "user"
          },
          "name": "Weijie Qiu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T09:56:39.772Z",
          "hidden": false
        },
        {
          "_id": "683fc08da33aeee1124887c6",
          "user": {
            "_id": "62be9b5aae56e75e4d689e7c",
            "avatarUrl": "/avatars/6772bc09d6eeb4e86b1210481be91720.svg",
            "isPro": false,
            "fullname": "wangxiaokun",
            "user": "shawn0wang",
            "type": "user"
          },
          "name": "Xiaokun Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:55:01.766Z",
          "hidden": false
        },
        {
          "_id": "683fc08da33aeee1124887c7",
          "name": "Peiyu Wang",
          "hidden": false
        },
        {
          "_id": "683fc08da33aeee1124887c8",
          "name": "Yunzhuo Hao",
          "hidden": false
        },
        {
          "_id": "683fc08da33aeee1124887c9",
          "name": "Jiangbo Pei",
          "hidden": false
        },
        {
          "_id": "683fc08da33aeee1124887ca",
          "name": "Yichen Wei",
          "hidden": false
        },
        {
          "_id": "683fc08da33aeee1124887cb",
          "name": "Yi Peng",
          "hidden": false
        },
        {
          "_id": "683fc08da33aeee1124887cc",
          "name": "Xuchen Song",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/620f5a1c3f76c50e6458a9b6/l5zN70u8jAQBCjl41TWoi.png"
      ],
      "publishedAt": "2025-05-30T01:34:25.000Z",
      "submittedOnDailyAt": "2025-06-04T05:57:36.826Z",
      "title": "CSVQA : Évaluation de la capacité d'inférence scientifique théorique des modèles VLM dans le benchmark des modèles de Chine",
      "submittedOnDailyBy": {
        "_id": "620f5a1c3f76c50e6458a9b6",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/620f5a1c3f76c50e6458a9b6/pXh_f5F0UvufxuUa-eS-v.jpeg",
        "isPro": false,
        "fullname": "Peiyu Wang",
        "user": "OrlandoHugBot",
        "type": "user"
      },
      "summary": "Les modèles de langue visuelle (VLMs) montrent un progrès impressionnant dans diverses compréhensions, mais leur capacité en logique scientifique n'est pas évaluée suffisamment. Actuellement, plusieurs benchmarks évaluent la compréhension générale d'images ou de texte basée sur la logique, mais ne traitent pas des problèmes nécessitant un contexte scientifique réel, ce qui résulte être insuffisant pour l'analyse d'évidences visuelles et le savoir spécifique de la discipline. Pour corriger cette lacune, nous présentons CSVQA (Benchmark de Réponses de Catégorie de la Version de la Virtual QA), conçu pour évaluer la logique scientifique à travers des questions visuelles basées sur des domaines spécifiques. CSVQA est un diagnostic de plusieurs benchmarks qui évaluent la logique scientifique par des réponses à des questions visuelles. Ce benchmark comprend 1,378 couples de questions et réponses bien construites, s'étend à diverses zones STEM et nécessite le savoir du domaine, l'intégration d'évidences visuelles et une logique de haut niveau. En comparaison avec d'autres benchmarks, CSVQA met l'accent sur le contenu scientifique réel et la complexité de la logique. De plus, il propose un protocole de évaluation strict pour justifier systématiquement si les prédictions du modèle sont basées sur une logique adéquate. Les résultats détaillés des 15 modèles évalués dans une épreuve de ligne de calcul montrent des différences claires de performance. En particulier, le modèle avec la propriété la plus élevée n'a pas atteint la précision de 49.6%. Ces résultats expérimentaux soulignent la nécessité de développer la logique scientifique dans les VLMs. CSVQA est disponible sur https://huggingface.co/datasets/Skywork/CSVQA.",
      "upvotes": 41,
      "discussionId": "683fc091a33aeee1124888a8",
      "ai_summary": "A new benchmark, CSVQA, evaluates scientific reasoning in vision-language models through domain-specific visual question answering, highlighting the need for improvement in these models.",
      "ai_keywords": [
        "Vision-Language Models",
        "multimodal benchmark",
        "scientific reasoning",
        "domain-grounded",
        "visual question answering",
        "domain-specific knowledge",
        "higher-order reasoning",
        "evaluation protocol",
        "intermediate reasoning steps",
        "curated explanations"
      ]
    },
    "publishedAt": "2025-05-29T21:34:25.000Z",
    "title": "CSVQA: A Chinese Multimodal Benchmark for Evaluating STEM Reasoning\n  Capabilities of VLMs",
    "summary": "Vision-Language Models (VLMs) have demonstrated remarkable progress in\nmultimodal understanding, yet their capabilities for scientific reasoning\nremains inadequately assessed. Current multimodal benchmarks predominantly\nevaluate generic image comprehension or text-driven reasoning, lacking\nauthentic scientific contexts that require domain-specific knowledge\nintegration with visual evidence analysis. To fill this gap, we present CSVQA,\na diagnostic multimodal benchmark specifically designed for evaluating\nscientific reasoning through domain-grounded visual question answering.Our\nbenchmark features 1,378 carefully constructed question-answer pairs spanning\ndiverse STEM disciplines, each demanding domain knowledge, integration of\nvisual evidence, and higher-order reasoning. Compared to prior multimodal\nbenchmarks, CSVQA places greater emphasis on real-world scientific content and\ncomplex reasoning.We additionally propose a rigorous evaluation protocol to\nsystematically assess whether model predictions are substantiated by valid\nintermediate reasoning steps based on curated explanations. Our comprehensive\nevaluation of 15 VLMs on this benchmark reveals notable performance\ndisparities, as even the top-ranked proprietary model attains only 49.6\\%\naccuracy.This empirical evidence underscores the pressing need for advancing\nscientific reasoning capabilities in VLMs. Our CSVQA is released at\nhttps://huggingface.co/datasets/Skywork/CSVQA.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/620f5a1c3f76c50e6458a9b6/l5zN70u8jAQBCjl41TWoi.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24120.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "620f5a1c3f76c50e6458a9b6",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/620f5a1c3f76c50e6458a9b6/pXh_f5F0UvufxuUa-eS-v.jpeg",
      "fullname": "Peiyu Wang",
      "name": "OrlandoHugBot",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.02387",
      "authors": [
        {
          "_id": "683fa95ea0770843560c7ae3",
          "user": {
            "_id": "653a5b0f7c01c693a16dd184",
            "avatarUrl": "/avatars/4b43d88709dc8037250404452e81adcf.svg",
            "isPro": false,
            "fullname": "Zelai Xu",
            "user": "zelaix",
            "type": "user"
          },
          "name": "Zelai Xu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-04T02:03:11.372Z",
          "hidden": false
        },
        {
          "_id": "683fa95ea0770843560c7ae4",
          "name": "Zhexuan Xu",
          "hidden": false
        },
        {
          "_id": "683fa95ea0770843560c7ae5",
          "name": "Xiangmin Yi",
          "hidden": false
        },
        {
          "_id": "683fa95ea0770843560c7ae6",
          "name": "Huining Yuan",
          "hidden": false
        },
        {
          "_id": "683fa95ea0770843560c7ae7",
          "name": "Xinlei Chen",
          "hidden": false
        },
        {
          "_id": "683fa95ea0770843560c7ae8",
          "name": "Yi Wu",
          "hidden": false
        },
        {
          "_id": "683fa95ea0770843560c7ae9",
          "name": "Chao Yu",
          "hidden": false
        },
        {
          "_id": "683fa95ea0770843560c7aea",
          "name": "Yu Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T02:57:38.000Z",
      "submittedOnDailyAt": "2025-06-04T00:58:29.506Z",
      "title": "VS-Bench : Évaluation de la logique stratégique et des décisions dans des environnements multi-agents avec les VLMs",
      "submittedOnDailyBy": {
        "_id": "653a5b0f7c01c693a16dd184",
        "avatarUrl": "/avatars/4b43d88709dc8037250404452e81adcf.svg",
        "isPro": false,
        "fullname": "Zelai Xu",
        "user": "zelaix",
        "type": "user"
      },
      "summary": "Le développement récent des modèles de langue visuolinguistique (VLMs) a étendu leurs capacités pour des tâches interactives d'agents, bien que les benchmarks actuels soient limités aux environnements qui incluent seulement un agent ou uniquement du texte. En contraste, les scénarios réels et globaux impliquent des interactions entre différents agents dans des contextes riches de vision et de langage, résolvant des problèmes à travers des observations multimodales et des interactions stratégiques. Pour aborder ce défi, nous présentons le Vision Strategy Bench (VS-Bench). VS-Bench est un benchmark multiagente qui évalue les VLMs pour évaluer la logique stratégique et les décisions dans des environnements multiagents. Il est configuré avec 8 environnements visuels spécifiques qui incluent des interactions collaboratives, compétitives ou mixtes, et évalue la capacité des agents à prédire l'avenir d'autres agents et à optimiser des objectifs à long terme. Nous examinons deux dimensions d'évaluation complémentaires : l'évaluation de la logique stratégique en ligne et l'évaluation des retours épisodiques normalisés en ligne. A travers d'expériences étendues avec 14 VLMs leaders, nous avons clairement démontré une grande différence entre les modèles actuels et les meilleurs rendements. Le meilleur modèle a atteint une précision de prédiction de 47,8% et un retour normalisé de 24,3%. De plus, nous avons effectué un analyse détaillée des observations multimodales, de l'échelle de test, des comportements sociaux et des cas de défaut des agents VLM, révélant les limites actuelles des modèles et établissant VS-Bench comme la base pour futures recherches en agents multimodal stratégiques. Les codes et les données sont disponibles sur https://vs-bench.github.io.",
      "upvotes": 34,
      "discussionId": "683fa95fa0770843560c7b3d",
      "projectPage": "https://vs-bench.github.io",
      "githubRepo": "https://github.com/zelaix/VS-Bench",
      "ai_summary": "VS-Bench is a multimodal benchmark designed to evaluate Vision Language Models' strategic reasoning and decision-making in complex multi-agent environments.",
      "ai_keywords": [
        "Vision Language Models",
        "VS-Bench",
        "multimodal benchmark",
        "strategic reasoning",
        "decision-making",
        "multi-agent environments",
        "vision-grounded environments",
        "cooperative",
        "competitive",
        "mixed-motive interactions",
        "next-action prediction",
        "normalized episode return",
        "multimodal observations",
        "test-time scaling",
        "social behaviors",
        "failure cases"
      ]
    },
    "publishedAt": "2025-06-02T22:57:38.000Z",
    "title": "VS-Bench: Evaluating VLMs for Strategic Reasoning and Decision-Making in\n  Multi-Agent Environments",
    "summary": "Recent advancements in Vision Language Models (VLMs) have expanded their\ncapabilities to interactive agent tasks, yet existing benchmarks remain limited\nto single-agent or text-only environments. In contrast, real-world scenarios\noften involve multiple agents interacting within rich visual and linguistic\ncontexts, posing challenges with both multimodal observations and strategic\ninteractions. To bridge this gap, we introduce Visual Strategic Bench\n(VS-Bench), a multimodal benchmark that evaluates VLMs for strategic reasoning\nand decision-making in multi-agent environments. VS-Bench comprises eight\nvision-grounded environments spanning cooperative, competitive, and\nmixed-motive interactions, designed to assess agents' ability to predict\nothers' future moves and optimize for long-term objectives. We consider two\ncomplementary evaluation dimensions, including offline evaluation of strategic\nreasoning by next-action prediction accuracy and online evaluation of\ndecision-making by normalized episode return. Extensive experiments of fourteen\nleading VLMs reveal a significant gap between current models and optimal\nperformance, with the best models attaining 47.8% prediction accuracy and 24.3%\nnormalized return. We further conduct in-depth analyses on multimodal\nobservations, test-time scaling, social behaviors, and failure cases of VLM\nagents. By standardizing the evaluation and highlighting the limitations of\nexisting models, we envision VS-Bench as a foundation for future research on\nstrategic multimodal agents. Code and data are available at\nhttps://vs-bench.github.io.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.02387.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "653a5b0f7c01c693a16dd184",
      "avatarUrl": "/avatars/4b43d88709dc8037250404452e81adcf.svg",
      "fullname": "Zelai Xu",
      "name": "zelaix",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.03147",
      "authors": [
        {
          "_id": "683fae55c6b71c5994ccd4fe",
          "user": {
            "_id": "6367a8175bb06007ea099b8f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6367a8175bb06007ea099b8f/IjG7HyWyWRlVt_XwRbxRW.jpeg",
            "isPro": false,
            "fullname": "linbin",
            "user": "LanguageBind",
            "type": "user"
          },
          "name": "Bin Lin",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:56:49.923Z",
          "hidden": false
        },
        {
          "_id": "683fae55c6b71c5994ccd4ff",
          "name": "Zongjian Li",
          "hidden": false
        },
        {
          "_id": "683fae55c6b71c5994ccd500",
          "name": "Xinhua Cheng",
          "hidden": false
        },
        {
          "_id": "683fae55c6b71c5994ccd501",
          "name": "Yuwei Niu",
          "hidden": false
        },
        {
          "_id": "683fae55c6b71c5994ccd502",
          "name": "Yang Ye",
          "hidden": false
        },
        {
          "_id": "683fae55c6b71c5994ccd503",
          "name": "Xianyi He",
          "hidden": false
        },
        {
          "_id": "683fae55c6b71c5994ccd504",
          "user": {
            "_id": "63468720dd6d90d82ccf3450",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
            "isPro": false,
            "fullname": "YSH",
            "user": "BestWishYsh",
            "type": "user"
          },
          "name": "Shenghai Yuan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:56:52.748Z",
          "hidden": false
        },
        {
          "_id": "683fae55c6b71c5994ccd505",
          "name": "Wangbo Yu",
          "hidden": false
        },
        {
          "_id": "683fae55c6b71c5994ccd506",
          "name": "Shaodong Wang",
          "hidden": false
        },
        {
          "_id": "683fae55c6b71c5994ccd507",
          "name": "Yunyang Ge",
          "hidden": false
        },
        {
          "_id": "683fae55c6b71c5994ccd508",
          "name": "Yatian Pang",
          "hidden": false
        },
        {
          "_id": "683fae55c6b71c5994ccd509",
          "name": "Li Yuan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T17:59:33.000Z",
      "submittedOnDailyAt": "2025-06-04T00:55:35.016Z",
      "title": "UniWorld : Intégration de Vision et Compréhension Sémantique d'un Encodeur à Haute Résolution",
      "submittedOnDailyBy": {
        "_id": "63468720dd6d90d82ccf3450",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
        "isPro": false,
        "fullname": "YSH",
        "user": "BestWishYsh",
        "type": "user"
      },
      "summary": "Bien sûr, voici le texte traduit en français :\n\n「D'accord, actuellement le modèle intégré offre un excellent rendement dans la compréhension du langage visuel et la génération d'images à partir de texte, cependant, il présente des limitations dans la reconnaissance d'images et dans des tâches de travail qui nécessitent urgemment de la part des utilisateurs. Récemment, OpenAI a présenté GPT-4o-Image, un modèle qui utilise des caractéristiques significatives d'un modèle de langage visuel pour atteindre une large gamme de fonctions en reconnaissance d'images et de tâches de travail, tout en maintenant l'intérêt de la communauté. À partir de nos expériences détaillées, nous avons déduit que GPT-4o-Image utilise des caractéristiques extraites d'un encodeur de signification au lieu d'un VAE (Autoencoder Variational). Ce fait intéressant nous a incité à proposer un cadre de génération intégré basé sur un modèle de langage visuel robuste et des caractéristiques significatives extraites d'un encodeur de signification. En conséquence, nous avons construit un modèle très puissant en utilisant seulement 1% des données de BAGEL, et nous avons obtenu un rendement supérieur à BAGEL dans un cadre de test d'édition d'images. UniWorld maintient une excellente capacité de compréhension et de génération d'images, et il montre également un rendement fort dans diverses tâches de reconnaissance d'images. Nous avons complètement ouvert notre modèle, les scripts d'entraînement et d'évaluation, ainsi que les ensembles de données.」",
      "upvotes": 33,
      "discussionId": "683fae56c6b71c5994ccd548",
      "githubRepo": "https://github.com/PKU-YuanGroup/UniWorld-V1",
      "ai_summary": "A unified generative framework called UniWorld uses semantic features from visual-language models for image perception and manipulation, outperforming BAGEL with reduced data.",
      "ai_keywords": [
        "GPT-4o-Image",
        "semantic encoders",
        "VAE",
        "UniWorld",
        "visual-language models",
        "contrastive semantic encoders",
        "image editing benchmarks",
        "image perception tasks"
      ]
    },
    "publishedAt": "2025-06-03T13:59:33.000Z",
    "title": "UniWorld: High-Resolution Semantic Encoders for Unified Visual\n  Understanding and Generation",
    "summary": "Although existing unified models deliver strong performance on\nvision-language understanding and text-to-image generation, their models are\nlimited in exploring image perception and manipulation tasks, which are\nurgently desired by users for wide applications. Recently, OpenAI released\ntheir powerful GPT-4o-Image model for comprehensive image perception and\nmanipulation, achieving expressive capability and attracting community\ninterests. By observing the performance of GPT-4o-Image in our carefully\nconstructed experiments, we infer that GPT-4o-Image leverages features\nextracted by semantic encoders instead of VAE, while VAEs are considered\nessential components in many image manipulation models. Motivated by such\ninspiring observations, we present a unified generative framework named\nUniWorld based on semantic features provided by powerful visual-language models\nand contrastive semantic encoders. As a result, we build a strong unified model\nusing only 1% amount of BAGEL's data, which consistently outperforms BAGEL on\nimage editing benchmarks. UniWorld also maintains competitive image\nunderstanding and generation capabilities, achieving strong performance across\nmultiple image perception tasks. We fully open-source our models, including\nmodel weights, training and evaluation scripts, and datasets.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03147.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63468720dd6d90d82ccf3450",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
      "fullname": "YSH",
      "name": "BestWishYsh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 56
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.00123",
      "authors": [
        {
          "_id": "683e709a3a4c2c3b2750fc32",
          "name": "Gen Luo",
          "hidden": false
        },
        {
          "_id": "683e709a3a4c2c3b2750fc33",
          "user": {
            "_id": "6565d7149afd51867e55520b",
            "avatarUrl": "/avatars/027b17651e61df598af53f69b92e7771.svg",
            "isPro": false,
            "fullname": "Ganlin Yang",
            "user": "ganlinyang",
            "type": "user"
          },
          "name": "Ganlin Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:45:03.857Z",
          "hidden": false
        },
        {
          "_id": "683e709a3a4c2c3b2750fc34",
          "user": {
            "_id": "660691330be1fbe3b9e4c33d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/660691330be1fbe3b9e4c33d/TxrDFH_cRu3AlpMC3xmhv.jpeg",
            "isPro": false,
            "fullname": "ZiYang Gong",
            "user": "Cusyoung",
            "type": "user"
          },
          "name": "Ziyang Gong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:45:00.364Z",
          "hidden": false
        },
        {
          "_id": "683e709a3a4c2c3b2750fc35",
          "name": "Guanzhou Chen",
          "hidden": false
        },
        {
          "_id": "683e709a3a4c2c3b2750fc36",
          "user": {
            "_id": "66ab30dfd456f0408b93f27b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66ab30dfd456f0408b93f27b/nps4Kni_eOExO5Z92RiiF.jpeg",
            "isPro": false,
            "fullname": "Haonan Duan",
            "user": "robot-haonan",
            "type": "user"
          },
          "name": "Haonan Duan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T09:03:03.236Z",
          "hidden": false
        },
        {
          "_id": "683e709a3a4c2c3b2750fc37",
          "name": "Erfei Cui",
          "hidden": false
        },
        {
          "_id": "683e709a3a4c2c3b2750fc38",
          "name": "Ronglei Tong",
          "hidden": false
        },
        {
          "_id": "683e709a3a4c2c3b2750fc39",
          "name": "Zhi Hou",
          "hidden": false
        },
        {
          "_id": "683e709a3a4c2c3b2750fc3a",
          "name": "Tianyi Zhang",
          "hidden": false
        },
        {
          "_id": "683e709a3a4c2c3b2750fc3b",
          "name": "Zhe Chen",
          "hidden": false
        },
        {
          "_id": "683e709a3a4c2c3b2750fc3c",
          "name": "Shenglong Ye",
          "hidden": false
        },
        {
          "_id": "683e709a3a4c2c3b2750fc3d",
          "name": "Lewei Lu",
          "hidden": false
        },
        {
          "_id": "683e709a3a4c2c3b2750fc3e",
          "name": "Jingbo Wang",
          "hidden": false
        },
        {
          "_id": "683e709a3a4c2c3b2750fc3f",
          "name": "Wenhai Wang",
          "hidden": false
        },
        {
          "_id": "683e709a3a4c2c3b2750fc40",
          "name": "Jifeng Dai",
          "hidden": false
        },
        {
          "_id": "683e709a3a4c2c3b2750fc41",
          "name": "Yu Qiao",
          "hidden": false
        },
        {
          "_id": "683e709a3a4c2c3b2750fc42",
          "name": "Rongrong Ji",
          "hidden": false
        },
        {
          "_id": "683e709a3a4c2c3b2750fc43",
          "name": "Xizhou Zhu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-30T18:00:34.000Z",
      "submittedOnDailyAt": "2025-06-04T03:43:23.019Z",
      "title": "Le modèle de la langue cérébrale spatiale qui exécute la vision, l'audition, le pensée et le contrôle dans l'espace.",
      "submittedOnDailyBy": {
        "_id": "6565d7149afd51867e55520b",
        "avatarUrl": "/avatars/027b17651e61df598af53f69b92e7771.svg",
        "isPro": false,
        "fullname": "Ganlin Yang",
        "user": "ganlinyang",
        "type": "user"
      },
      "summary": "Le progrès significatif des modèles de langue multimodal (MLLMs) a attiré plus d'attention en étant capables d'étendre leurs compétences à des entités physiques comme les robots. Cela implique que les MLLMs doivent acquérir une compréhension multimodale qui inclut une inférence spatiale visuelle et une interaction physique. Cependant, les différences fondamentales de ces compétences rendent que les méthodes existantes rencontrent des difficultés pour les intégrer. Dans cet article, nous proposons \"VeBrain\" (Visual Body-Brain), un cadre intégré qui combine la perception, l'inférence et le contrôle de la réalité. VeBrain réconfigure le contrôle des robots pour des tâches basées sur le texte dans l'espace visuel 2D, unifiant les objectifs et les espaces de travail de différentes tâches. De plus, nous proposons un nouvel adaptateur pour transformer les signaux de contrôle textuel des MLLM en stratégies de mouvement des robots. Du point de vue des données, nous introduisons VeBrain-600k, un ensemble de données de haute qualité qui inclut diverses capacités de VeBrain. VeBrain-600k a été collecté, sélectionné et étiqueté pendant des centaines d'heures, et nous avons utilisé la chaîne de pensée multimodal (CoT) pour mélanger différentes capacités dans une seule conversation. A travers 13 tests de référence multimodal et 5 d'intelligence spatiale, nous avons démontré que VeBrain présente un rendement exceptionnel par rapport aux autres MLLMs (par exemple, Qwen2.5-VL). En utilisant des robots à pied et des bras mécaniques, VeBrain a démontré une forte adaptabilité, flexibilité et combinaison par rapport aux méthodes existantes. Par exemple, en comparaison avec Qwen2.5-VL, VeBrain a réalisé des améliorations significatives sur MMVet et un accroissement moyen de 50% sur des tâches de robots à pied.",
      "upvotes": 23,
      "discussionId": "683e70a13a4c2c3b2750fd76",
      "projectPage": "https://internvl.github.io/blog/2025-05-26-VeBrain/",
      "githubRepo": "https://github.com/OpenGVLab/VeBrain",
      "ai_summary": "VeBrain is a unified framework that integrates multimodal understanding, visual-spatial reasoning, and physical interaction for legged robots, demonstrating superior performance compared to existing methods across various benchmarks.",
      "ai_keywords": [
        "Multimodal Large Language Models",
        "VeBrain",
        "Visual Embodied Brain",
        "text-based MLLM tasks",
        "robotic adapter",
        "VeBrain-600k",
        "multimodal chain-of-thought",
        "MMVet",
        "compositional capabilities"
      ]
    },
    "publishedAt": "2025-05-30T14:00:34.000Z",
    "title": "Visual Embodied Brain: Let Multimodal Large Language Models See, Think,\n  and Control in Spaces",
    "summary": "The remarkable progress of Multimodal Large Language Models (MLLMs) has\nattracted increasing attention to extend them to physical entities like legged\nrobot. This typically requires MLLMs to not only grasp multimodal understanding\nabilities, but also integrate visual-spatial reasoning and physical interaction\ncapabilities. Nevertheless,existing methods struggle to unify these\ncapabilities due to their fundamental differences.In this paper, we present the\nVisual Embodied Brain (VeBrain), a unified framework for perception, reasoning,\nand control in real world. VeBrain reformulates robotic control into common\ntext-based MLLM tasks in the 2D visual space, thus unifying the objectives and\nmapping spaces of different tasks. Then, a novel robotic adapter is proposed to\nconvert textual control signals from MLLMs to motion policies of real robots.\nFrom the data perspective, we further introduce VeBrain-600k, a high-quality\ninstruction dataset encompassing various capabilities of VeBrain. In\nVeBrain-600k, we take hundreds of hours to collect, curate and annotate the\ndata, and adopt multimodal chain-of-thought(CoT) to mix the different\ncapabilities into a single conversation. Extensive experiments on 13 multimodal\nbenchmarks and 5 spatial intelligence benchmarks demonstrate the superior\nperformance of VeBrain to existing MLLMs like Qwen2.5-VL. When deployed to\nlegged robots and robotic arms, VeBrain shows strong adaptability, flexibility,\nand compositional capabilities compared to existing methods. For example,\ncompared to Qwen2.5-VL, VeBrain not only achieves substantial gains on MMVet by\n+5.6%, but also excels in legged robot tasks with +50% average gains.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.00123.png",
    "numComments": 4,
    "submittedBy": {
      "_id": "6565d7149afd51867e55520b",
      "avatarUrl": "/avatars/027b17651e61df598af53f69b92e7771.svg",
      "fullname": "Ganlin Yang",
      "name": "ganlinyang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.03135",
      "authors": [
        {
          "_id": "683fe55868402c738a8e5ee4",
          "name": "Mengdi Jia",
          "hidden": false
        },
        {
          "_id": "683fe55868402c738a8e5ee5",
          "user": {
            "_id": "63c3e8abc7d7f4c63a515a02",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63c3e8abc7d7f4c63a515a02/npMHnVP2hHLbvoUGe7C4O.jpeg",
            "isPro": false,
            "fullname": "Zekun Qi",
            "user": "qizekun",
            "type": "user"
          },
          "name": "Zekun Qi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:54:22.042Z",
          "hidden": false
        },
        {
          "_id": "683fe55868402c738a8e5ee6",
          "name": "Shaochen Zhang",
          "hidden": false
        },
        {
          "_id": "683fe55868402c738a8e5ee7",
          "name": "Wenyao Zhang",
          "hidden": false
        },
        {
          "_id": "683fe55868402c738a8e5ee8",
          "name": "Xinqiang Yu",
          "hidden": false
        },
        {
          "_id": "683fe55868402c738a8e5ee9",
          "name": "Jiawei He",
          "hidden": false
        },
        {
          "_id": "683fe55868402c738a8e5eea",
          "name": "He Wang",
          "hidden": false
        },
        {
          "_id": "683fe55868402c738a8e5eeb",
          "name": "Li Yi",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63c3e8abc7d7f4c63a515a02/ZZsfC9We_mrTAc2s-h5ng.mp4"
      ],
      "publishedAt": "2025-06-03T17:58:29.000Z",
      "submittedOnDailyAt": "2025-06-04T05:47:51.969Z",
      "title": "OmniSpatial : Logiciel Spatial Extensif pour un Cadre de Test Complet de Logiciel Spatial",
      "submittedOnDailyBy": {
        "_id": "63c3e8abc7d7f4c63a515a02",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63c3e8abc7d7f4c63a515a02/npMHnVP2hHLbvoUGe7C4O.jpeg",
        "isPro": false,
        "fullname": "Zekun Qi",
        "user": "qizekun",
        "type": "user"
      },
      "summary": "La théorie spatiale est l'une des domaines les plus importants en psychologie cognitive et actuellement constitue le principal équilibre des modèles de langage visuel (VLMs). Les études antérieures ont été principalement utilisées pour évaluer ou améliorer la compréhension des relations spatiales basiques. Cependant, ces travaux ne traitent que les niveaux les plus basiques de l'inférence spatiale. Dans cet article, nous présentons OmniSpatial, une évaluation détaillée de l'inférence spatiale basée sur la psychologie cognitive. OmniSpatial inclut quatre catégories principales : inférence dynamique, logique spatiale complexe, interaction spatiale et révision de points, et comprend 50 catégories détaillées. À travers des données de crôniques de l'internet et des notes rigoureusement écrites, plus de 1 500 paires de questions et réponses ont été construites. À travers une large gamme d'expériments, nous avons démontré que les VLMs ouverts et fermés, ainsi que les modèles actuels de logique et compréhension spatiale, présentent des déficiences dans la compréhension spatiale détaillée. De plus, nous avons analysé des cas de défaillance et proposé des directions potentielles pour des recherches futures.",
      "upvotes": 22,
      "discussionId": "683fe55c68402c738a8e5ff4",
      "ai_summary": "A comprehensive benchmark called OmniSpatial evaluates vision-language models' understanding of advanced spatial reasoning tasks, revealing significant limitations across various models.",
      "ai_keywords": [
        "vision-language models",
        "spatial reasoning",
        "cognitive psychology",
        "dynamic reasoning",
        "complex spatial logic",
        "spatial interaction",
        "perspective-taking",
        "question-answer pairs"
      ]
    },
    "publishedAt": "2025-06-03T13:58:29.000Z",
    "title": "OmniSpatial: Towards Comprehensive Spatial Reasoning Benchmark for\n  Vision Language Models",
    "summary": "Spatial reasoning is a key aspect of cognitive psychology and remains a major\nbottleneck for current vision-language models (VLMs). While extensive research\nhas aimed to evaluate or improve VLMs' understanding of basic spatial\nrelations, such as distinguishing left from right, near from far, and object\ncounting, these tasks represent only the most fundamental level of spatial\nreasoning. In this work, we introduce OmniSpatial, a comprehensive and\nchallenging benchmark for spatial reasoning, grounded in cognitive psychology.\nOmniSpatial covers four major categories: dynamic reasoning, complex spatial\nlogic, spatial interaction, and perspective-taking, with 50 fine-grained\nsubcategories. Through Internet data crawling and careful manual annotation, we\nconstruct over 1.5K question-answer pairs. Extensive experiments show that both\nopen- and closed-source VLMs, as well as existing reasoning and spatial\nunderstanding models, exhibit significant limitations in comprehensive spatial\nunderstanding. We further analyze failure cases and propose potential\ndirections for future research.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63c3e8abc7d7f4c63a515a02/ZZsfC9We_mrTAc2s-h5ng.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03135.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63c3e8abc7d7f4c63a515a02",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63c3e8abc7d7f4c63a515a02/npMHnVP2hHLbvoUGe7C4O.jpeg",
      "fullname": "Zekun Qi",
      "name": "qizekun",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.01674",
      "authors": [
        {
          "_id": "683faa6515abeae85e13336b",
          "name": "Yipeng Du",
          "hidden": false
        },
        {
          "_id": "683faa6515abeae85e13336c",
          "name": "Tiehan Fan",
          "hidden": false
        },
        {
          "_id": "683faa6515abeae85e13336d",
          "name": "Kepan Nan",
          "hidden": false
        },
        {
          "_id": "683faa6515abeae85e13336e",
          "name": "Rui Xie",
          "hidden": false
        },
        {
          "_id": "683faa6515abeae85e13336f",
          "name": "Penghao Zhou",
          "hidden": false
        },
        {
          "_id": "683faa6515abeae85e133370",
          "name": "Xiang Li",
          "hidden": false
        },
        {
          "_id": "683faa6515abeae85e133371",
          "name": "Jian Yang",
          "hidden": false
        },
        {
          "_id": "683faa6515abeae85e133372",
          "name": "Zhenheng Yang",
          "hidden": false
        },
        {
          "_id": "683faa6515abeae85e133373",
          "user": {
            "_id": "65734004769f3ee9bde1af10",
            "avatarUrl": "/avatars/d6310ed861972fd691687d8f47413f33.svg",
            "isPro": false,
            "fullname": "Ying Tai",
            "user": "yingtai",
            "type": "user"
          },
          "name": "Ying Tai",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:56:59.253Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-02T13:44:56.000Z",
      "submittedOnDailyAt": "2025-06-04T00:38:03.935Z",
      "title": "MotionSight : Améliorer la capacité d'interpréter les petites mouvements dans différents modes",
      "submittedOnDailyBy": {
        "_id": "65927f3b754092f6b1e187a7",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65927f3b754092f6b1e187a7/gUrNvIQHmsl1vLwSUxpmL.jpeg",
        "isPro": false,
        "fullname": "tiehan fan",
        "user": "AnonMegumi",
        "type": "user"
      },
      "summary": "Bien sûr, voici la traduction en français :\n\nMalgré les avancés des Modèles de Langage Large Multimodal (MLLMs) qui présentent également des limitations dans la compréhension de petites mouvements. En général, il manque une différenciation temporelle adéquate, et souvent, le compteur visuel entre les images est souvent moyenné ou ignoré. De plus, les prompts visuels dans des images statiques ont montré des potentiels, mais leur application dans des vidéos pour comprendre des mouvements petits n'a pas encore été largement explorée en raison de la complexité temporelle. Nous étudions si les MLLMs peuvent améliorer leur reconnaissance de mouvements et distinguer entre le mouvement d'objets et le mouvement de la caméra, en générant des signaux visuels spéciaux. Dans cette étude, nous présentons une nouvelle méthodologie zero-shot appelée \"MotionSight\", qui utilise un spectre d'objets centré et un bref de mouvement comme prompts visuels, sans nécessité d'entraînement, pour comprendre efficacement des petits mouvements. Pour convertir cela en un ensemble de données utile, nous avons créé le premier grand ensemble de données de grande échelle, MotionVid-QA. Cet ensemble de données comporte des annotations logiques qui incluent l'entraînement et les données de préférence, et est composé d'environ 40K de clips de vidéo et de 87K de questions et réponses. Les résultats des expériences montrent que MotionSight atteint les meilleurs rendements et démontre sa compétitivité avec les modèles commerciaux. En particulier, il offre une nouvelle technologie zero-shot et un grand ensemble de données de haute qualité pour comprendre des mouvements petits. Tout le code et les données d'annotation sont disponibles publiquement.",
      "upvotes": 19,
      "discussionId": "683faa6615abeae85e1333c2",
      "projectPage": "https://nju-pcalab.github.io/projects/MotionSight/",
      "githubRepo": "https://github.com/NJU-PCALab/MotionSight",
      "ai_summary": "MotionSight, a zero-shot method using object-centric visual spotlight and motion blur as prompts, enhances fine-grained video motion understanding and achieves state-of-the-art performance on MotionVid-QA, a large-scale dataset with hierarchical annotations.",
      "ai_keywords": [
        "Multimodal Large Language Models",
        "MLLMs",
        "fine-grained video motion understanding",
        "inter-frame differencing",
        "visual prompting",
        "temporal complexities",
        "MotionSight",
        "object-centric visual spotlight",
        "motion blur",
        "MotionVid-QA",
        "hierarchical annotations",
        "SFT",
        "preference data",
        "state-of-the-art performance"
      ]
    },
    "publishedAt": "2025-06-02T09:44:56.000Z",
    "title": "MotionSight: Boosting Fine-Grained Motion Understanding in Multimodal\n  LLMs",
    "summary": "Despite advancements in Multimodal Large Language Models (MLLMs), their\nproficiency in fine-grained video motion understanding remains critically\nlimited. They often lack inter-frame differencing and tend to average or ignore\nsubtle visual cues. Furthermore, while visual prompting has shown potential in\nstatic images, its application to video's temporal complexities, particularly\nfor fine-grained motion understanding, remains largely unexplored. We\ninvestigate whether inherent capability can be unlocked and boost MLLMs' motion\nperception and enable distinct visual signatures tailored to decouple object\nand camera motion cues. In this study, we introduce MotionSight, a novel\nzero-shot method pioneering object-centric visual spotlight and motion blur as\nvisual prompts to effectively improve fine-grained motion understanding without\ntraining. To convert this into valuable data assets, we curated MotionVid-QA,\nthe first large-scale dataset for fine-grained video motion understanding, with\nhierarchical annotations including SFT and preference data, {\\Theta}(40K) video\nclips and {\\Theta}(87K) QAs. Experiments show MotionSight achieves\nstate-of-the-art open-source performance and competitiveness with commercial\nmodels. In particular, for fine-grained motion understanding we present a novel\nzero-shot technique and a large-scale, high-quality dataset. All the code and\nannotations will be publicly available.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01674.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65927f3b754092f6b1e187a7",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65927f3b754092f6b1e187a7/gUrNvIQHmsl1vLwSUxpmL.jpeg",
      "fullname": "tiehan fan",
      "name": "AnonMegumi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.03065",
      "authors": [
        {
          "_id": "683fc6241de14546d5e02775",
          "user": {
            "_id": "64c9bac33cfe45b07179568d",
            "avatarUrl": "/avatars/4a8206cdb1770a8cdaae0d0a2b7b59f2.svg",
            "isPro": false,
            "fullname": "Pengtao Chen",
            "user": "PengtaoChen",
            "type": "user"
          },
          "name": "Pengtao Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:54:45.116Z",
          "hidden": false
        },
        {
          "_id": "683fc6241de14546d5e02776",
          "name": "Xianfang Zeng",
          "hidden": false
        },
        {
          "_id": "683fc6241de14546d5e02777",
          "name": "Maosen Zhao",
          "hidden": false
        },
        {
          "_id": "683fc6241de14546d5e02778",
          "name": "Peng Ye",
          "hidden": false
        },
        {
          "_id": "683fc6241de14546d5e02779",
          "name": "Mingzhu Shen",
          "hidden": false
        },
        {
          "_id": "683fc6241de14546d5e0277a",
          "user": {
            "_id": "64b914c8ace99c0723ad83a9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b914c8ace99c0723ad83a9/B4gxNByeVY_xaOcjwiN1j.jpeg",
            "isPro": false,
            "fullname": "Wei Cheng",
            "user": "wchengad",
            "type": "user"
          },
          "name": "Wei Cheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:54:42.268Z",
          "hidden": false
        },
        {
          "_id": "683fc6241de14546d5e0277b",
          "user": {
            "_id": "63417332c5565a4b8d43a0d8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63417332c5565a4b8d43a0d8/MmnYG7Wu2Z_lqCBvTfJmy.png",
            "isPro": false,
            "fullname": "Gang Yu",
            "user": "skicy",
            "type": "user"
          },
          "name": "Gang Yu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:54:39.068Z",
          "hidden": false
        },
        {
          "_id": "683fc6241de14546d5e0277c",
          "name": "Tao Chen",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64b914c8ace99c0723ad83a9/iewcVFqJgelW9EQTlH_LH.png"
      ],
      "publishedAt": "2025-06-03T16:42:37.000Z",
      "submittedOnDailyAt": "2025-06-04T04:37:13.352Z",
      "title": "Sparse-vDiT : Le pouvoir de l'attention peu dense accélère le Transformer Diffusion Vidéo.",
      "submittedOnDailyBy": {
        "_id": "64b914c8ace99c0723ad83a9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b914c8ace99c0723ad83a9/B4gxNByeVY_xaOcjwiN1j.jpeg",
        "isPro": false,
        "fullname": "Wei Cheng",
        "user": "wchengad",
        "type": "user"
      },
      "summary": "DiTs a été capables de réaliser des avancées dans la génération de films, mais le défi de la génération de séquences longues est limité par une structure d'attention bidimensionnelle complexe et nécessite des temps d'inférence significatifs. En analysant la carte d'attention du Diffuseur Transformer (vDiT), trois patrons de hyperspatialité possibles ont été identifiés : structures diagonales, multidiagonales et verticales. Ces patrons limitent la dépendance entre les couches et les positions des têtes d'attention en fonction de la profondeur des couches. Ces résultats ont été utilisés pour proposer un cadre d'accélération de hyperspatialité appelé Sparse-vDiT. Sparse-vDiT remplace l'attention dense par un noyau de hyperspatialité optimisé pour chaque patron de hyperspatialité, ce qui est implémenté de manière efficace en termes de calcul. Il inclut également un algorithme de recherche de hyperspatialité en ligne qui modélise les coûts de matériel pour sélectionner la stratégie de calcul de hyperspatialité optimale pour chaque couche et tête. Après avoir déterminé la configuration optimale, les têtes qui partagent la même stratégie d'attention sont intégrées au sein de la même couche pour améliorer l'efficacité de l'inférence. En intégrant le modèle vDiT plus récent (CogVideoX1.5, HunyuanVideo, Wan2.1), Sparse-vDiT a atteint des réductions théoriques de FLOP de 2.09, 2.38 et 1.67 fois, des augmentations de la vitesse d'inférence de 1.76, 1.85 et 1.58 fois, et a maintenu des niveaux de qualité visuelle élevés, atteignant des valeurs de PSNR de 24.13, 27.09 et 22.59. Notre étude montre que le potentiel structural de hyperspatialité dans vDiT peut être systématiquement exploitée dans la génération de films longs.",
      "upvotes": 18,
      "discussionId": "683fc62b1de14546d5e02931",
      "githubRepo": "https://github.com/Peyton-Chen/Sparse-vDiT",
      "ai_summary": "Sparse-vDiT accelerates Video Diffusion Transformer (vDiT) by leveraging sparsity patterns in attention maps, reducing theoretical FLOPs and improving inference speed without significant loss in visual quality.",
      "ai_keywords": [
        "Diffusion Transformers",
        "DiTs",
        "Video Diffusion Transformer",
        "vDiT",
        "sparsity patterns",
        "diagonal",
        "multi-diagonal",
        "vertical-stripe structures",
        "sparse kernels",
        "sparse diffusion search algorithm",
        "FLOP reduction",
        "inference speedups",
        "PSNR",
        "latent structural sparsity"
      ]
    },
    "publishedAt": "2025-06-03T12:42:37.000Z",
    "title": "Sparse-vDiT: Unleashing the Power of Sparse Attention to Accelerate\n  Video Diffusion Transformers",
    "summary": "While Diffusion Transformers (DiTs) have achieved breakthroughs in video\ngeneration, this long sequence generation task remains constrained by the\nquadratic complexity of attention mechanisms, resulting in significant\ninference latency. Through detailed analysis of attention maps in Video\nDiffusion Transformer (vDiT), we identify three recurring sparsity patterns:\ndiagonal, multi-diagonal, and vertical-stripe structures. And even 3-6\\%\nattention heads can be skipped. Crucially, these patterns exhibit strong\nlayer-depth and head-position correlations but show limited dependence on the\ninput content. Leveraging these findings, we propose Sparse-vDiT, a sparsity\nacceleration framework for vDiT comprising: 1) Pattern-optimized sparse kernels\nthat replace dense attention with computationally efficient implementations for\neach identified sparsity pattern. 2) An offline sparse diffusion search\nalgorithm that selects the optimal sparse computation strategy per layer and\nhead via hardware-aware cost modeling. After determining the optimal\nconfiguration, we fuse heads within the same layer that share the same\nattention strategy, enhancing inference efficiency. Integrated into\nstate-of-the-art vDiT models (CogVideoX1.5, HunyuanVideo, and Wan2.1),\nSparse-vDiT achieves 2.09times, 2.38times, and 1.67times theoretical\nFLOP reduction, and actual inference speedups of 1.76times, 1.85times,\nand 1.58times, respectively, while maintaining high visual fidelity, with\nPSNR values reaching 24.13, 27.09, and 22.59. Our work demonstrates that latent\nstructural sparsity in vDiTs can be systematically exploited for long video\nsynthesis.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64b914c8ace99c0723ad83a9/iewcVFqJgelW9EQTlH_LH.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03065.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b914c8ace99c0723ad83a9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b914c8ace99c0723ad83a9/B4gxNByeVY_xaOcjwiN1j.jpeg",
      "fullname": "Wei Cheng",
      "name": "wchengad",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.02096",
      "authors": [
        {
          "_id": "683fa36f7ed99d0040761114",
          "user": {
            "_id": "626d268d5f7327906f05cad1",
            "avatarUrl": "/avatars/18bda74612a3ee63a17f991bcc695106.svg",
            "isPro": true,
            "fullname": "Zijian Wu",
            "user": "Jakumetsu",
            "type": "user"
          },
          "name": "Zijian Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:57:09.765Z",
          "hidden": false
        },
        {
          "_id": "683fa36f7ed99d0040761115",
          "name": "Jinjie Ni",
          "hidden": false
        },
        {
          "_id": "683fa36f7ed99d0040761116",
          "name": "Xiangyan Liu",
          "hidden": false
        },
        {
          "_id": "683fa36f7ed99d0040761117",
          "name": "Zichen Liu",
          "hidden": false
        },
        {
          "_id": "683fa36f7ed99d0040761118",
          "name": "Hang Yan",
          "hidden": false
        },
        {
          "_id": "683fa36f7ed99d0040761119",
          "name": "Michael Qizhe Shieh",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-02T17:45:16.000Z",
      "submittedOnDailyAt": "2025-06-04T00:44:38.612Z",
      "title": "SynthRL : Expansion de l'Inférence Visuelle Sûre Demonstrable par Synthèse de Données",
      "submittedOnDailyBy": {
        "_id": "6486b09e8315b19342f0bf5e",
        "avatarUrl": "/avatars/bc5f22f231c884146d373fe1042d81bd.svg",
        "isPro": false,
        "fullname": "Xiangyan Liu",
        "user": "xyliu6",
        "type": "user"
      },
      "summary": "Les modèles de vision-langue (VLMs) sont construits grâce à l'apprentissage par renforcement avec récompenses vérifiables (RLVR) et ont démontré des avancées significatives dans l'échelle efficace au cours du temps de test. Ce papier explorerait comment les données d'apprentissage par renforcement synthétisées peuvent améliorer considérablement le RLVR. Pour cela, il propose un pipeline scalable et sûr pour l'auto-échelle des données lors de l'entraînement de RL logique. SynthRL est composé de trois étapes clés : (1) sélection de questions seed avec des distributions appropriées, (2) amélioration de ces questions en plus de variantes développées tout en maintenant les réponses originales, et (3) une étape de vérification sûre pour garantir une excellente exactitude et augmenter la difficulté. Les résultats expérimentaux montrent la scalabilité et l'efficacité de SynthRL. Appliqué au jeu de données MMK12, SynthRL synthétise approximativement 3.3K questions supplémentaires difficiles et vérifiables à partir de 8K échantillons seed. Le modèle entraîné sur les données synthétisées continue de progresser dans cinq domaines distincts du benchmark de vision-langue mathématique et démontre également des avancées significatives par rapport au modèle de base entraîné uniquement sur les données seed. Notablement, un analyse détaillée montre que le gain est beaucoup plus prononcé dans les échantillons d'évaluation plus développés et que SynthRL effectivement extrait des motifs complexes de raisonnement.",
      "upvotes": 18,
      "discussionId": "683fa3707ed99d0040761154",
      "ai_summary": "SynthRL, a scalable pipeline for data synthesis in reinforcement learning with verifiable rewards, enhances visual math reasoning VLMs by generating challenging, verifiable questions.",
      "ai_keywords": [
        "reinforcement learning",
        "verifiable reward",
        "RLVR",
        "SynthRL",
        "seed questions",
        "data augmentation",
        "verification stage",
        "MMK12 dataset",
        "visual math reasoning benchmarks"
      ]
    },
    "publishedAt": "2025-06-02T13:45:16.000Z",
    "title": "SynthRL: Scaling Visual Reasoning with Verifiable Data Synthesis",
    "summary": "Vision-language models (VLMs) trained via reinforcement learning with\nverifiable reward (RLVR) have shown notable progress in scaling test-time\ncompute effectively. In this work, we investigate how synthesized RL data can\nfurther improve RLVR. To this end, we propose SynthRL-a scalable and\nguaranteed pipeline for automatic data scaling in reasoning-oriented RL\ntraining. SynthRL comprises three key stages: (1) selecting seed questions with\nappropriate distribution, (2) augmenting them into more challenging variants\nwhile preserving the original answers, and (3) a guaranteed verification stage\nthat ensures near-perfect correctness and difficulty enhancement. Our empirical\nexperiments demonstrate SynthRL's scalability and effectiveness. When applied\nto the MMK12 dataset, SynthRL synthesizes over 3.3K additional verifiable,\nchallenging questions from approximately 8K seed samples. Models trained with\nour synthesized data achieve consistent gains across five out-of-domain visual\nmath reasoning benchmarks, with a significant improvement over baseline models\ntrained on seed data alone. Notably, detailed analysis reveals that the gains\nare more pronounced on the most challenging evaluation samples, highlighting\nSynthRL's effectiveness in eliciting deeper and more complex reasoning\npatterns.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.02096.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6486b09e8315b19342f0bf5e",
      "avatarUrl": "/avatars/bc5f22f231c884146d373fe1042d81bd.svg",
      "fullname": "Xiangyan Liu",
      "name": "xyliu6",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.03143",
      "authors": [
        {
          "_id": "683fc5599363b50c19f17d42",
          "user": {
            "_id": "63ef330b1e695b35aa484e11",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ef330b1e695b35aa484e11/bXwpGy0dl8JXeJwJ--ilr.jpeg",
            "isPro": false,
            "fullname": "Qianhui WU",
            "user": "qianhuiwu",
            "type": "user"
          },
          "name": "Qianhui Wu",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-06-04T04:21:31.171Z",
          "hidden": false
        },
        {
          "_id": "683fc5599363b50c19f17d43",
          "user": {
            "_id": "63340dbbd92c5842ae71d1e9",
            "avatarUrl": "/avatars/3a3182996bd41b526dcbfa8687d91963.svg",
            "isPro": false,
            "fullname": "Kanzhi Cheng",
            "user": "cckevinn",
            "type": "user"
          },
          "name": "Kanzhi Cheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:54:48.476Z",
          "hidden": false
        },
        {
          "_id": "683fc5599363b50c19f17d44",
          "user": {
            "_id": "64d45451c34a346181b130dd",
            "avatarUrl": "/avatars/9bb8205b889337df5d321539c9b5d69d.svg",
            "isPro": false,
            "fullname": "Rui Yang",
            "user": "Ray2333",
            "type": "user"
          },
          "name": "Rui Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:54:51.070Z",
          "hidden": false
        },
        {
          "_id": "683fc5599363b50c19f17d45",
          "user": {
            "_id": "654dbac9938fbf1e696be8aa",
            "avatarUrl": "/avatars/b3c4035c48169c1bfb04a439fce3499f.svg",
            "isPro": false,
            "fullname": "Chaoyun Zhang",
            "user": "vyokky",
            "type": "user"
          },
          "name": "Chaoyun Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:54:54.267Z",
          "hidden": false
        },
        {
          "_id": "683fc5599363b50c19f17d46",
          "name": "Jianwei Yang",
          "hidden": false
        },
        {
          "_id": "683fc5599363b50c19f17d47",
          "name": "Huiqiang Jiang",
          "hidden": false
        },
        {
          "_id": "683fc5599363b50c19f17d48",
          "name": "Jian Mu",
          "hidden": false
        },
        {
          "_id": "683fc5599363b50c19f17d49",
          "name": "Baolin Peng",
          "hidden": false
        },
        {
          "_id": "683fc5599363b50c19f17d4a",
          "name": "Bo Qiao",
          "hidden": false
        },
        {
          "_id": "683fc5599363b50c19f17d4b",
          "name": "Reuben Tan",
          "hidden": false
        },
        {
          "_id": "683fc5599363b50c19f17d4c",
          "name": "Si Qin",
          "hidden": false
        },
        {
          "_id": "683fc5599363b50c19f17d4d",
          "name": "Lars Liden",
          "hidden": false
        },
        {
          "_id": "683fc5599363b50c19f17d4e",
          "name": "Qingwei Lin",
          "hidden": false
        },
        {
          "_id": "683fc5599363b50c19f17d4f",
          "name": "Huan Zhang",
          "hidden": false
        },
        {
          "_id": "683fc5599363b50c19f17d50",
          "name": "Tong Zhang",
          "hidden": false
        },
        {
          "_id": "683fc5599363b50c19f17d51",
          "name": "Jianbing Zhang",
          "hidden": false
        },
        {
          "_id": "683fc5599363b50c19f17d52",
          "name": "Dongmei Zhang",
          "hidden": false
        },
        {
          "_id": "683fc5599363b50c19f17d53",
          "name": "Jianfeng Gao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T17:59:08.000Z",
      "submittedOnDailyAt": "2025-06-04T02:44:41.730Z",
      "title": "Acteur GUI : Interface graphique du groupe de visualisation sans coordonnées",
      "submittedOnDailyBy": {
        "_id": "654dbac9938fbf1e696be8aa",
        "avatarUrl": "/avatars/b3c4035c48169c1bfb04a439fce3499f.svg",
        "isPro": false,
        "fullname": "Chaoyun Zhang",
        "user": "vyokky",
        "type": "user"
      },
      "summary": "L'exploitation d'un agent pour la protection des interfaces utilisateur graphiques (GUI) dans le contexte des modèles de langage visuolinguistiques (VLM) constitue l'un des principaux défis majeurs en termes de visualisation, c'est-à-dire la détection d'areas de la fenêtre appropriées basées sur des contenus visuels et la planification du contexte. Actuellement, de nombreux études abordent ce problème comme un défi de génération de coordonnées basées sur le texte, mais cette approche présente plusieurs limitations : déficiences dans l'alignement spatial et syntaxique, incapacité de traiter des objectifs de sous-areas incertaines, et une attribution inadéquate en raison de la densité des coordonnées de la fenêtre et de la grande quantité de caractéristiques visuelles extraites par les Transformers Vision. Dans cet article, nous proposons un méthode de détection d'areas de GUI sans coordonnées. L'agent GUI-Actor surpasse les limitations de la génération de coordonnées basée sur le texte en introduisant une tête d'action basée sur des patches et en garantissant que le token <ACTOR> est aligné avec tous les tokens de patches visuels pertinents, permettant ainsi au modèle de proposer plusieurs areas d'action dans une seule passe. Pour évaluer et sélectionner l'area d'action la plus probable, nous concevons une fonction de vérification d'areas. Des expériences extensives montrent que l'agent GUI-Actor surpasse les méthodes supérieures sur plusieurs benchmarks d'actions de GUI, améliorant la généralisation à de nouvelles résolutions de la fenêtre et de nouveaux designs de la fenêtre. Spécifiquement, le modèle GUI-Actor-7B surpasse UI-TARS-72B sur ScreenSpot-Pro, atteignant des scores de 40,7 et 44,6 avec Qwen2-VL et Qwen2,5-VL, respectivement. De plus, en introduisant la fonction de vérification d'areas, nous obtenons un modèle de VLM qui, avec un ajustement minimal de la tête d'action, atteint le même rendement que les modèles supérieurs précédents, démontrant que l'agent GUI-Actor peut effectivement doter d'un modèle de VLM des habiletés de détection d'areas, sans perdre ses avantages généraux.",
      "upvotes": 17,
      "discussionId": "683fc55d9363b50c19f17e6b",
      "projectPage": "https://microsoft.github.io/GUI-Actor/",
      "githubRepo": "https://github.com/microsoft/GUI-Actor",
      "ai_summary": "GUI-Actor, a VLM-based method using attention for coordinate-free GUI grounding, outperforms existing methods with better generalization and efficient fine-tuning.",
      "ai_keywords": [
        "visual grounding",
        "text-based coordinate generation",
        "spatial-semantic alignment",
        "Vision Transformers",
        "attention-based action head",
        "grounding verifier",
        "GUI action grounding benchmarks",
        "GUI-Actor",
        "GUI-Actor-7B",
        "UI-TARS-72B",
        "ScreenSpot-Pro",
        "Qwen2-VL",
        "Qwen2.5-VL",
        "parameter-efficient fine-tuning"
      ]
    },
    "publishedAt": "2025-06-03T13:59:08.000Z",
    "title": "GUI-Actor: Coordinate-Free Visual Grounding for GUI Agents",
    "summary": "One of the principal challenges in building VLM-powered GUI agents is visual\ngrounding, i.e., localizing the appropriate screen region for action execution\nbased on both the visual content and the textual plans. Most existing work\nformulates this as a text-based coordinate generation task. However, these\napproaches suffer from several limitations: weak spatial-semantic alignment,\ninability to handle ambiguous supervision targets, and a mismatch between the\ndense nature of screen coordinates and the coarse, patch-level granularity of\nvisual features extracted by models like Vision Transformers. In this paper, we\npropose GUI-Actor, a VLM-based method for coordinate-free GUI grounding. At its\ncore, GUI-Actor introduces an attention-based action head that learns to align\na dedicated <ACTOR> token with all relevant visual patch tokens, enabling the\nmodel to propose one or more action regions in a single forward pass. In line\nwith this, we further design a grounding verifier to evaluate and select the\nmost plausible action region from the candidates proposed for action execution.\nExtensive experiments show that GUI-Actor outperforms prior state-of-the-art\nmethods on multiple GUI action grounding benchmarks, with improved\ngeneralization to unseen screen resolutions and layouts. Notably, GUI-Actor-7B\neven surpasses UI-TARS-72B (38.1) on ScreenSpot-Pro, achieving scores of 40.7\nwith Qwen2-VL and 44.6 with Qwen2.5-VL as backbones. Furthermore, by\nincorporating the verifier, we find that fine-tuning only the newly introduced\naction head (~100M parameters for 7B model) while keeping the VLM backbone\nfrozen is sufficient to achieve performance comparable to previous\nstate-of-the-art models, highlighting that GUI-Actor can endow the underlying\nVLM with effective grounding capabilities without compromising its\ngeneral-purpose strengths.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03143.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "654dbac9938fbf1e696be8aa",
      "avatarUrl": "/avatars/b3c4035c48169c1bfb04a439fce3499f.svg",
      "fullname": "Chaoyun Zhang",
      "name": "vyokky",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.03131",
      "authors": [
        {
          "_id": "683fb2786a2b978ca4e62493",
          "user": {
            "_id": "64b7aa374df206a3ed1947d2",
            "avatarUrl": "/avatars/a7c7e703ccf8824259fc5a8a90a25746.svg",
            "isPro": false,
            "fullname": "wzd",
            "user": "GoodEnough",
            "type": "user"
          },
          "name": "Zidong Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:56:13.762Z",
          "hidden": false
        },
        {
          "_id": "683fb2786a2b978ca4e62494",
          "name": "Lei Bai",
          "hidden": false
        },
        {
          "_id": "683fb2786a2b978ca4e62495",
          "name": "Xiangyu Yue",
          "hidden": false
        },
        {
          "_id": "683fb2786a2b978ca4e62496",
          "name": "Wanli Ouyang",
          "hidden": false
        },
        {
          "_id": "683fb2786a2b978ca4e62497",
          "name": "Yiyuan Zhang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63176933b58b0184630d2c74/VS2G-SBuSY5ltQmCLAaq3.png"
      ],
      "publishedAt": "2025-06-03T17:57:33.000Z",
      "submittedOnDailyAt": "2025-06-04T01:13:44.155Z",
      "title": "Nebit Registración Imagen Sintética",
      "submittedOnDailyBy": {
        "_id": "63176933b58b0184630d2c74",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63176933b58b0184630d2c74/53b5EASwW76zeyyqeJA3O.jpeg",
        "isPro": false,
        "fullname": "Yiyuan Zhang",
        "user": "Yiyuan",
        "type": "user"
      },
      "summary": "Introduis la notation de synthèse d'images. C'est un nouveau paradigme de modélisation générative qui permet la synthèse d'images à n'importe quelle résolution et proportion largeur à hauteur. Cet approche dépasse les limites des méthodes traditionnelles basées sur une seule résolution fixe et des images carrées, et traite naturellement des tokens visuels de longueur variable. De cette manière, on résout les principales limitations des méthodes traditionnelles. Par conséquent, on introduit le Native-resolution diffusion Transformer (NiT). Cette architecture vise à modéliser explicitement la résolution et la proportion largeur à hauteur qui varient au cours du processus de traitement du bruit. Libérée des contraintes de format fixe, la NiT apprend une distribution visuelle unique pour des images de différentes résolutions et proportions largeur à hauteur. En particulier, un seul modèle de NiT atteint le meilleur rendement dans les référentiels de ImageNet-256x256 et 512x512. Surprenant, la NiT, qui a été entraînée uniquement sur ImageNet et montre une capacité d'expansion 0-shot similaire à celle des grands modèles de langage, réussit à générer des images de haute qualité à des résolutions élevées (par exemple, 1536 x 1536) et différentes proportions largeur à hauteur (par exemple, 16:9, 3:1, 4:3). Ces résultats montrent que la modélisation d'images par notation d'opérateurs a un potentiel important entre la modélisation visuelle et les matériaux avancés de modèles de langage de grande échelle.",
      "upvotes": 13,
      "discussionId": "683fb27e6a2b978ca4e625bd",
      "projectPage": "https://wzdthu.github.io/NiT/",
      "githubRepo": "https://github.com/WZDTHU/NiT",
      "ai_summary": "A novel generative model, Native-resolution diffusion Transformer (NiT), synthesizes high-resolution and varied aspect ratio images with state-of-the-art performance and zero-shot generalization capabilities.",
      "ai_keywords": [
        "native-resolution image synthesis",
        "generative modeling paradigm",
        "variable-length visual tokens",
        "diffusion Transformer",
        "denoising process",
        "ImageNet-256x256",
        "ImageNet-512x512",
        "high-fidelity images",
        "aspect ratios",
        "zero-shot generalization"
      ]
    },
    "publishedAt": "2025-06-03T13:57:33.000Z",
    "title": "Native-Resolution Image Synthesis",
    "summary": "We introduce native-resolution image synthesis, a novel generative modeling\nparadigm that enables the synthesis of images at arbitrary resolutions and\naspect ratios. This approach overcomes the limitations of conventional\nfixed-resolution, square-image methods by natively handling variable-length\nvisual tokens, a core challenge for traditional techniques. To this end, we\nintroduce the Native-resolution diffusion Transformer (NiT), an architecture\ndesigned to explicitly model varying resolutions and aspect ratios within its\ndenoising process. Free from the constraints of fixed formats, NiT learns\nintrinsic visual distributions from images spanning a broad range of\nresolutions and aspect ratios. Notably, a single NiT model simultaneously\nachieves the state-of-the-art performance on both ImageNet-256x256 and 512x512\nbenchmarks. Surprisingly, akin to the robust zero-shot capabilities seen in\nadvanced large language models, NiT, trained solely on ImageNet, demonstrates\nexcellent zero-shot generalization performance. It successfully generates\nhigh-fidelity images at previously unseen high resolutions (e.g., 1536 x 1536)\nand diverse aspect ratios (e.g., 16:9, 3:1, 4:3), as shown in Figure 1. These\nfindings indicate the significant potential of native-resolution modeling as a\nbridge between visual generative modeling and advanced LLM methodologies.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63176933b58b0184630d2c74/VS2G-SBuSY5ltQmCLAaq3.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03131.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "63176933b58b0184630d2c74",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63176933b58b0184630d2c74/53b5EASwW76zeyyqeJA3O.jpeg",
      "fullname": "Yiyuan Zhang",
      "name": "Yiyuan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.00070",
      "authors": [
        {
          "_id": "683fd7d79d4fb703271ad9ac",
          "name": "Dongyoung Kim",
          "hidden": false
        },
        {
          "_id": "683fd7d79d4fb703271ad9ad",
          "name": "Sumin Park",
          "hidden": false
        },
        {
          "_id": "683fd7d79d4fb703271ad9ae",
          "name": "Huiwon Jang",
          "hidden": false
        },
        {
          "_id": "683fd7d79d4fb703271ad9af",
          "name": "Jinwoo Shin",
          "hidden": false
        },
        {
          "_id": "683fd7d79d4fb703271ad9b0",
          "name": "Jaehyung Kim",
          "hidden": false
        },
        {
          "_id": "683fd7d79d4fb703271ad9b1",
          "name": "Younggyo Seo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T16:41:12.000Z",
      "submittedOnDailyAt": "2025-06-04T03:53:04.651Z",
      "title": "Robotique R1 : Amélioration de la logique mécanique par apprentissage par renforcement dans le cadre de l'ingénierie robotique",
      "submittedOnDailyBy": {
        "_id": "658d79663a5202a485a76d9b",
        "avatarUrl": "/avatars/1893384f28a7cb82d4588576c4c264f1.svg",
        "isPro": false,
        "fullname": "dongyoung kim",
        "user": "vangard703",
        "type": "user"
      },
      "summary": "Les grands modèles de langage de vision (LVLMs) montrent un grand potentiel pour le développement de la technologie des robots en combinant des logiques mécanisées de contrôle de robots basées sur l'apprentissage statistique. L'approche générale utilise principalement l'entraînement de tâches logiques mécanisées avec Fine-Tuning Supervisé (SFT), mais les ensembles de données SFT sont souvent construits heuristiquement, ce qui limite leur optimisation explicite pour améliorer le contrôle des robots. De plus, l'SFT souvent présente des problèmes comme la perte catastrophique et la dégradation de la généralisation. Pour surmonter ces limites, un nouveau cadre de référence est en développement pour améliorer les logiques mécanisées dans le contrôle des robots en utilisant l'apprentissage par renforcement. Le robot R1 apprend à prédire les points clés nécessaires pour compléter une tâche en se basant sur des images et des métadonnées de l'environnement actuels de démonstrations spécifiques. Avec son approche d'apprentissage, le robot R1 montre des réponses basées sur le raisonnement et améliore sa prédiction de manière plus précise. Les résultats des expériences montrent que le modèle entraîné sur le robot R1 dépasse les méthodes SFT dans les tâches logiques mécanisées. Bien que limité à 7B paramètres, il est capable de dépasser GPT-4 dans les tâches de raisonnement liées au contrôle d'actions avancées.",
      "upvotes": 13,
      "discussionId": "683fd7d79d4fb703271ad9d9",
      "ai_summary": "Robot-R1, a reinforcement learning framework, enhances embodied reasoning for robotics by predicting keypoint states, outperforming supervised fine-tuning methods and even surpassing GPT-4o in low-level action control tasks.",
      "ai_keywords": [
        "Large Vision-Language Models (LVLMs)",
        "embodied reasoning",
        "robot control",
        "Supervised Fine-Tuning (SFT)",
        "catastrophic forgetting",
        "generalization performance",
        "reinforcement learning",
        "keypoint state",
        "scene image",
        "environment metadata",
        "expert demonstrations",
        "DeepSeek-R1",
        "reasoning-based responses",
        "parameter-efficient fine-tuning"
      ]
    },
    "publishedAt": "2025-05-29T12:41:12.000Z",
    "title": "Robot-R1: Reinforcement Learning for Enhanced Embodied Reasoning in\n  Robotics",
    "summary": "Large Vision-Language Models (LVLMs) have recently shown great promise in\nadvancing robotics by combining embodied reasoning with robot control. A common\napproach involves training on embodied reasoning tasks related to robot control\nusing Supervised Fine-Tuning (SFT). However, SFT datasets are often\nheuristically constructed and not explicitly optimized for improving robot\ncontrol. Furthermore, SFT often leads to issues such as catastrophic forgetting\nand reduced generalization performance. To address these limitations, we\nintroduce Robot-R1, a novel framework that leverages reinforcement learning to\nenhance embodied reasoning specifically for robot control. Robot-R1 learns to\npredict the next keypoint state required for task completion, conditioned on\nthe current scene image and environment metadata derived from expert\ndemonstrations. Inspired by the DeepSeek-R1 learning approach, Robot-R1 samples\nreasoning-based responses and reinforces those that lead to more accurate\npredictions. Our experiments show that models trained with Robot-R1 outperform\nSFT methods on embodied reasoning tasks. Despite having only 7B parameters,\nRobot-R1 even surpasses GPT-4o on reasoning tasks related to low-level action\ncontrol, such as spatial and primitive movement reasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.00070.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "658d79663a5202a485a76d9b",
      "avatarUrl": "/avatars/1893384f28a7cb82d4588576c4c264f1.svg",
      "fullname": "dongyoung kim",
      "name": "vangard703",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.03136",
      "authors": [
        {
          "_id": "683fa2ddbde0ae60c2f16183",
          "name": "Yinjie Wang",
          "hidden": false
        },
        {
          "_id": "683fa2ddbde0ae60c2f16184",
          "name": "Ling Yang",
          "hidden": false
        },
        {
          "_id": "683fa2ddbde0ae60c2f16185",
          "name": "Ye Tian",
          "hidden": false
        },
        {
          "_id": "683fa2ddbde0ae60c2f16186",
          "name": "Ke Shen",
          "hidden": false
        },
        {
          "_id": "683fa2ddbde0ae60c2f16187",
          "name": "Mengdi Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T17:58:42.000Z",
      "submittedOnDailyAt": "2025-06-04T00:39:07.151Z",
      "title": "Co-Evolving LLM Coder and Unit Tester via Reinforcement Learning",
      "submittedOnDailyBy": {
        "_id": "64fde4e252e82dd432b74ce9",
        "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
        "isPro": false,
        "fullname": "Ling Yang",
        "user": "Lingaaaaaaa",
        "type": "user"
      },
      "summary": "Nous proposons le nouveau cadre d'apprentissage par récompense appelé CURE. Ce cadre utilise une conception de récompenses spéciales pour que les résultats de l'interaction entre code et tests unitaires évoluent simultanément, sans recevoir des directives via des codes réels. Cet approche permet des entraînements flexibles et extensibles et permet que les tests unitaires apprennent directement des erreurs du code. Après avoir optimisé le modèle Qwen2.5-Instruct, nos modèles ReasonFlux-Coder-7B et 14B, que nous avons développés, ont augmenté la précision de génération de code de 5,3% et la précision du meilleur de N de 9,0%. Ces modèles dépassent en taille Qwen-Coder, DeepSeek-Coder et Seed-Coder, qui sont capables de s'étendre naturellement des tâches telles que l'ajustement du temps de test et du code d'agent. De plus, le modèle ReasonFlux-Coder-4B pour le modèle LONG-COT a atteint une efficacité d'inférence dans la génération de tests unitaires de 64,8%, dépassant Qwen3-4B. En particulier, il est notable que nos modèles peuvent être utilisés comme modèles de récompense efficaces dans l'apprentissage par récompense des modèles de base. Projet : https://github.com/Gen-Verse/CURE",
      "upvotes": 12,
      "discussionId": "683fa2debde0ae60c2f161cb",
      "projectPage": "https://huggingface.co/collections/Gen-Verse/reasonflux-coder-6833109ed9300c62deb32c6b",
      "githubRepo": "https://github.com/Gen-Verse/CURE",
      "ai_summary": "CURE, a reinforcement learning framework, improves code and unit test generation accuracy without ground-truth supervision, enhancing performance in various coding and testing tasks.",
      "ai_keywords": [
        "reinforcement learning",
        "reward design",
        "coding",
        "unit test generation",
        "ReasonFlux-Coder",
        "Qwen2.5-Instruct",
        "Qwen-Coder",
        "DeepSeek-Coder",
        "Seed-Coder",
        "test-time scaling",
        "agentic coding",
        "long-CoT",
        "inference efficiency",
        "reward model"
      ]
    },
    "publishedAt": "2025-06-03T13:58:42.000Z",
    "title": "Co-Evolving LLM Coder and Unit Tester via Reinforcement Learning",
    "summary": "We propose CURE, a novel reinforcement learning framework with a dedicated\nreward design that co-evolves coding and unit test generation capabilities\nbased on their interaction outcomes, without any ground-truth code as\nsupervision. This approach enables flexible and scalable training and allows\nthe unit tester to learn directly from the coder's mistakes. Our derived\nReasonFlux-Coder-7B and 14B models improve code generation accuracy by 5.3% and\nBest-of-N accuracy by 9.0% after optimization on Qwen2.5-Instruct models,\noutperforming similarly sized Qwen-Coder, DeepSeek-Coder, and Seed-Coder. They\nnaturally extend to downstream tasks such as test-time scaling and agentic\ncoding-achieving a 8.1% improvement over the base model. For the long-CoT\nmodel, our ReasonFlux-Coder-4B consistently outperforms Qwen3-4B while\nachieving 64.8% inference efficiency in unit test generation. Notably, we also\nfind that our model can serve as an effective reward model for reinforcement\nlearning on base models. Project: https://github.com/Gen-Verse/CURE",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03136.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64fde4e252e82dd432b74ce9",
      "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
      "fullname": "Ling Yang",
      "name": "Lingaaaaaaa",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.23061",
      "authors": [
        {
          "_id": "683fc4028de3ffc5838c3fa8",
          "name": "Tarun Suresh",
          "hidden": false
        },
        {
          "_id": "683fc4028de3ffc5838c3fa9",
          "name": "Debangshu Banerjee",
          "hidden": false
        },
        {
          "_id": "683fc4028de3ffc5838c3faa",
          "name": "Shubham Ugare",
          "hidden": false
        },
        {
          "_id": "683fc4028de3ffc5838c3fab",
          "name": "Sasa Misailovic",
          "hidden": false
        },
        {
          "_id": "683fc4028de3ffc5838c3fac",
          "name": "Gagandeep Singh",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T04:04:54.000Z",
      "submittedOnDailyAt": "2025-06-04T02:27:09.092Z",
      "title": "DINGO: La Défense des LLM contre l'Inférence Limitée",
      "submittedOnDailyBy": {
        "_id": "65e7bb35e5e78134ab049942",
        "avatarUrl": "/avatars/3c0972f0d59e51ebb5c218ee736d4458.svg",
        "isPro": false,
        "fullname": "Tarun Suresh",
        "user": "tarsur909",
        "type": "user"
      },
      "summary": "La diffusion des LLMs montre un grand potentiel pour l'efficacité en temps de exécution par rapport aux LLMs traditionnels autorégressifs. Cependant, les modèles de diffusion actuels ne peuvent pas forcément de manière démonstrative des restrictions formelles comme des expressions régulières, ce qui les rend moins fiables pour des tâches nécessitant des sorties structurées comme des JSON. Contrairement aux modèles autorégressifs, les LLMs de diffusion prédisent des blocs de tokens complets. Cette parallélisation rend impossible pour les algorithmes de décodage avec des contraintes conçus pour prédire la séquence de tokens de maintenir la distribution réelle de l'output. Pour résoudre ce problème, nous proposons DINGO, une stratégie de décodage avec des contraintes basée sur la programmation dynamique et efficace qui maintient la distribution. DINGO permet de visualiser des chaînes de caractères de sortie avec la plus grande probabilité de la distribution de prédiction du modèle et qui strictement respecte les expressions régulières spécifiées par l'utilisateur. Dans les benchmarks standards de création de symboles mathématiques et de JSON, DINGO atteint des améliorations maximales de 68% par rapport à l'inférence illimitée.",
      "upvotes": 12,
      "discussionId": "683fc4038de3ffc5838c3fd8",
      "ai_summary": "DINGO, a dynamic programming-based decoding strategy, enhances diffusion language models by enforcing structured output constraints, significantly improving performance on symbolic math and JSON generation tasks.",
      "ai_keywords": [
        "diffusion LLMs",
        "autoregressive LLMs",
        "formal constraints",
        "regular expressions",
        "sequential token prediction",
        "parallel token prediction",
        "dynamic programming",
        "constrained decoding",
        "output distribution",
        "symbolic math generation",
        "JSON generation"
      ]
    },
    "publishedAt": "2025-05-29T00:04:54.000Z",
    "title": "DINGO: Constrained Inference for Diffusion LLMs",
    "summary": "Diffusion LLMs have emerged as a promising alternative to conventional\nautoregressive LLMs, offering significant potential for improved runtime\nefficiency. However, existing diffusion models lack the ability to provably\nenforce user-specified formal constraints, such as regular expressions, which\nmakes them unreliable for tasks that require structured outputs, such as\nfixed-schema JSON generation. Unlike autoregressive models that generate tokens\nsequentially, diffusion LLMs predict a block of tokens in parallel. This\nparallelism makes traditional constrained decoding algorithms, which are\ndesigned for sequential token prediction, ineffective at preserving the true\noutput distribution. To address this limitation, we propose DINGO, a dynamic\nprogramming-based constrained decoding strategy that is both efficient and\nprovably distribution-preserving. DINGO enables sampling of output strings with\nthe highest probability under the model's predicted distribution, while\nstrictly satisfying any user-specified regular expression. On standard symbolic\nmath and JSON generation benchmarks, DINGO achieves up to a 68 percentage point\nimprovement over unconstrained inference",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23061.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65e7bb35e5e78134ab049942",
      "avatarUrl": "/avatars/3c0972f0d59e51ebb5c218ee736d4458.svg",
      "fullname": "Tarun Suresh",
      "name": "tarsur909",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.02528",
      "authors": [
        {
          "_id": "683fb8bc3bcb592f18f5b866",
          "name": "Yan Gong",
          "hidden": false
        },
        {
          "_id": "683fb8bc3bcb592f18f5b867",
          "name": "Yiren Song",
          "hidden": false
        },
        {
          "_id": "683fb8bc3bcb592f18f5b868",
          "name": "Yicheng Li",
          "hidden": false
        },
        {
          "_id": "683fb8bc3bcb592f18f5b869",
          "name": "Chenglin Li",
          "hidden": false
        },
        {
          "_id": "683fb8bc3bcb592f18f5b86a",
          "name": "Yin Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T07:06:35.000Z",
      "submittedOnDailyAt": "2025-06-04T01:39:07.675Z",
      "title": "RelationAdapter : Apprentissage et Transmission de Relations Visuelles avec le Transformer à Différenciation",
      "submittedOnDailyBy": {
        "_id": "64311a95034ecbefddd141ef",
        "avatarUrl": "/avatars/b6dc5ca373bedbaa368208517954c375.svg",
        "isPro": true,
        "fullname": "Yiren Song",
        "user": "yiren98",
        "type": "user"
      },
      "summary": "Le nouveau paradigme de l'édition généralisée d'images s'appuie sur la structure d'apprentissage des grands modèles de langage (LLMs). Actuellement, le méthode de référence unique se concentre généralement sur des styles ou des caractéristiques, ce qui entraîne souvent des difficultés pour appliquer des transformations fixes. Pour résoudre ces limitations, on propose d'utiliser des paires d'images source et cible pour extraire et transmettre le sens d'édition du contenu lié aux nouvelles images. Pour cela, on introduit un module léger appelé RelationAdapter, qui permet aux modèles basés sur les Transformers de Diffusion (DiT) de détecter et d'appliquer des transformations visuelles même dans des cas extrêmement limités, facilitant ainsi la compréhension et la transmission de l'intention d'édition. De plus, on introduit le jeu de données détaillé Relation252K, qui comprend 218 tâches d'édition, avec l'objectif d'évaluer la capacité de généralisation et l'applicabilité du modèle. Les expériences réalisées sur Relation252K montrent que RelationAdapter améliore significativement la compréhension et la transmission de l'intention d'édition, et que, en général, améliore notablement la qualité de la génération et le rendement de l'édition.",
      "upvotes": 11,
      "discussionId": "683fb8bd3bcb592f18f5b89f",
      "ai_summary": "RelationAdapter, a lightweight module, enhances Diffusion Transformer models to capture and apply visual transformations effectively using source-target image pairs, improving editing performance and generalization on diverse tasks.",
      "ai_keywords": [
        "RelationAdapter",
        "Diffusion Transformer",
        "DiT",
        "visual prompt-based image editing",
        "content-aware editing intent",
        "Relation252K",
        "visual transformations",
        "editing intent",
        "generation quality",
        "overall editing performance"
      ]
    },
    "publishedAt": "2025-06-03T03:06:35.000Z",
    "title": "RelationAdapter: Learning and Transferring Visual Relation with\n  Diffusion Transformers",
    "summary": "Inspired by the in-context learning mechanism of large language models\n(LLMs), a new paradigm of generalizable visual prompt-based image editing is\nemerging. Existing single-reference methods typically focus on style or\nappearance adjustments and struggle with non-rigid transformations. To address\nthese limitations, we propose leveraging source-target image pairs to extract\nand transfer content-aware editing intent to novel query images. To this end,\nwe introduce RelationAdapter, a lightweight module that enables Diffusion\nTransformer (DiT) based models to effectively capture and apply visual\ntransformations from minimal examples. We also introduce Relation252K, a\ncomprehensive dataset comprising 218 diverse editing tasks, to evaluate model\ngeneralization and adaptability in visual prompt-driven scenarios. Experiments\non Relation252K show that RelationAdapter significantly improves the model's\nability to understand and transfer editing intent, leading to notable gains in\ngeneration quality and overall editing performance.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.02528.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64311a95034ecbefddd141ef",
      "avatarUrl": "/avatars/b6dc5ca373bedbaa368208517954c375.svg",
      "fullname": "Yiren Song",
      "name": "yiren98",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 21
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.24714",
      "authors": [
        {
          "_id": "683d04c751706d12b2c262ea",
          "user": {
            "_id": "642da1cd99f3110ac27caca5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642da1cd99f3110ac27caca5/C1QJY3R_ZdaeANG1y8iW7.jpeg",
            "isPro": false,
            "fullname": "junyu",
            "user": "luojunyu",
            "type": "user"
          },
          "name": "Junyu Luo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T09:03:26.483Z",
          "hidden": false
        },
        {
          "_id": "683d04c751706d12b2c262eb",
          "user": {
            "_id": "65a9c8652bf3e0cbbfcab2c8",
            "avatarUrl": "/avatars/fc690a78b5f2e94e08a40059ae40625c.svg",
            "isPro": false,
            "fullname": "Alan KOU",
            "user": "alan1027",
            "type": "user"
          },
          "name": "Zhizhuo Kou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T09:03:22.133Z",
          "hidden": false
        },
        {
          "_id": "683d04c751706d12b2c262ec",
          "user": {
            "_id": "6434c115a5aed21dd11981c5",
            "avatarUrl": "/avatars/d51e6e384cfc3affe578e7816bcebb35.svg",
            "isPro": false,
            "fullname": "Yang Liming",
            "user": "chunfenri",
            "type": "user"
          },
          "name": "Liming Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T09:03:24.346Z",
          "hidden": false
        },
        {
          "_id": "683d04c751706d12b2c262ed",
          "name": "Xiao Luo",
          "hidden": false
        },
        {
          "_id": "683d04c751706d12b2c262ee",
          "name": "Jinsheng Huang",
          "hidden": false
        },
        {
          "_id": "683d04c751706d12b2c262ef",
          "name": "Zhiping Xiao",
          "hidden": false
        },
        {
          "_id": "683d04c751706d12b2c262f0",
          "name": "Jingshu Peng",
          "hidden": false
        },
        {
          "_id": "683d04c751706d12b2c262f1",
          "name": "Chengzhong Liu",
          "hidden": false
        },
        {
          "_id": "683d04c751706d12b2c262f2",
          "name": "Jiaming Ji",
          "hidden": false
        },
        {
          "_id": "683d04c751706d12b2c262f3",
          "name": "Xuanzhe Liu",
          "hidden": false
        },
        {
          "_id": "683d04c751706d12b2c262f4",
          "name": "Sirui Han",
          "hidden": false
        },
        {
          "_id": "683d04c751706d12b2c262f5",
          "name": "Ming Zhang",
          "hidden": false
        },
        {
          "_id": "683d04c751706d12b2c262f6",
          "name": "Yike Guo",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/642da1cd99f3110ac27caca5/ilkQ2K5LD4SPQ5oCcV2aB.png"
      ],
      "publishedAt": "2025-05-30T15:36:19.000Z",
      "submittedOnDailyAt": "2025-06-04T01:04:36.480Z",
      "title": "FinMME : Ensemble de données d'évaluation pour le cadre de référence des données de modèles financiers",
      "submittedOnDailyBy": {
        "_id": "642da1cd99f3110ac27caca5",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642da1cd99f3110ac27caca5/C1QJY3R_ZdaeANG1y8iW7.jpeg",
        "isPro": false,
        "fullname": "junyu",
        "user": "luojunyu",
        "type": "user"
      },
      "summary": "Les modèles de langue de machine (MLLMs) connaissent un développement rapide récent. Cependant, il existe une manque de jeux de données d'évaluation efficaces et professionnels pour les MLLMs dans le domaine financier. Pour encourager le développement de MLLMs dans le secteur financier, nous présentons FinMME. FinMME étend ses domaines financiers à 18 secteurs et ses actifs à 6 catégories, aborde 10 types de graphiques principaux et 21 sous-types, et inclut plus de 11 000 échantillons de recherche financière de haute qualité. Pour garantir la qualité des données, nous avons utilisé 20 annotateurs et une structure de vérification soigneusement conçue. De plus, nous avons développé FinScore, qui introduit des pénalités de simulation et des évaluations multidimensionnelles pour fournir des évaluations non biaisées. À travers de nombreux expérimentations, nous avons démontré que même le plus récent des modèles, GPT-4o, présente un comportement insatisfaisant dans FinMME, révélant les difficultés qu'il rencontre. Le benchmark montre que les variations dans les prédictions par rapport aux autres modèles sont de 1% ou moins, montrant une grande confiance par rapport aux jeux de données actuels. Notre jeu de données et notre protocole d'évaluation sont disponibles sur https://huggingface.co/datasets/luojunyu/FinMME et https://github.com/luo-junyu/FinMME.",
      "upvotes": 11,
      "discussionId": "683d04c951706d12b2c26367",
      "projectPage": "https://huggingface.co/datasets/luojunyu/FinMME",
      "githubRepo": "https://github.com/luo-junyu/FinMME",
      "ai_summary": "FinMME is a comprehensive multimodal dataset for financial research and FinScore is an evaluation system that highlights the challenges faced by even advanced models like GPT-4o in the finance domain.",
      "ai_keywords": [
        "Multimodal Large Language Models",
        "FinMME",
        "financial research samples",
        "high-quality dataset",
        "financial domains",
        "asset classes",
        "chart types",
        "data quality",
        "FinScore",
        "hallucination penalties",
        "multi-dimensional capability assessment",
        "benchmark dataset",
        "prediction robustness"
      ]
    },
    "publishedAt": "2025-05-30T11:36:19.000Z",
    "title": "FinMME: Benchmark Dataset for Financial Multi-Modal Reasoning Evaluation",
    "summary": "Multimodal Large Language Models (MLLMs) have experienced rapid development\nin recent years. However, in the financial domain, there is a notable lack of\neffective and specialized multimodal evaluation datasets. To advance the\ndevelopment of MLLMs in the finance domain, we introduce FinMME, encompassing\nmore than 11,000 high-quality financial research samples across 18 financial\ndomains and 6 asset classes, featuring 10 major chart types and 21 subtypes. We\nensure data quality through 20 annotators and carefully designed validation\nmechanisms. Additionally, we develop FinScore, an evaluation system\nincorporating hallucination penalties and multi-dimensional capability\nassessment to provide an unbiased evaluation. Extensive experimental results\ndemonstrate that even state-of-the-art models like GPT-4o exhibit\nunsatisfactory performance on FinMME, highlighting its challenging nature. The\nbenchmark exhibits high robustness with prediction variations under different\nprompts remaining below 1%, demonstrating superior reliability compared to\nexisting datasets. Our dataset and evaluation protocol are available at\nhttps://huggingface.co/datasets/luojunyu/FinMME and\nhttps://github.com/luo-junyu/FinMME.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/642da1cd99f3110ac27caca5/ilkQ2K5LD4SPQ5oCcV2aB.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24714.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "642da1cd99f3110ac27caca5",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642da1cd99f3110ac27caca5/C1QJY3R_ZdaeANG1y8iW7.jpeg",
      "fullname": "junyu",
      "name": "luojunyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.03126",
      "authors": [
        {
          "_id": "683fb3719f37285365b080c9",
          "user": {
            "_id": "672a037c19f1f942483f680c",
            "avatarUrl": "/avatars/a48464044e9eb11a2bc062be05d9aa9a.svg",
            "isPro": false,
            "fullname": "qiulu",
            "user": "qiulu66",
            "type": "user"
          },
          "name": "Lu Qiu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:56:04.988Z",
          "hidden": false
        },
        {
          "_id": "683fb3719f37285365b080ca",
          "user": {
            "_id": "630b094f8b327c7b8b94d24c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/630b094f8b327c7b8b94d24c/Fd0ugLOHJZIi8oUZScOmX.jpeg",
            "isPro": false,
            "fullname": "Yizhuo Li",
            "user": "liyz",
            "type": "user"
          },
          "name": "Yizhuo Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:56:02.456Z",
          "hidden": false
        },
        {
          "_id": "683fb3719f37285365b080cb",
          "name": "Yuying Ge",
          "hidden": false
        },
        {
          "_id": "683fb3719f37285365b080cc",
          "name": "Yixiao Ge",
          "hidden": false
        },
        {
          "_id": "683fb3719f37285365b080cd",
          "name": "Ying Shan",
          "hidden": false
        },
        {
          "_id": "683fb3719f37285365b080ce",
          "name": "Xihui Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T17:55:18.000Z",
      "submittedOnDailyAt": "2025-06-04T03:58:52.500Z",
      "title": "Guide de référence pour la génération de vidéos dans le jeu de données d'animation",
      "submittedOnDailyBy": {
        "_id": "630b094f8b327c7b8b94d24c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/630b094f8b327c7b8b94d24c/Fd0ugLOHJZIi8oUZScOmX.jpeg",
        "isPro": false,
        "fullname": "Yizhuo Li",
        "user": "liyz",
        "type": "user"
      },
      "summary": "Le développement récent du contenu généré par l'IA (AIGC) a considérablement accéléré la production d'animations. Pour créer une animation intéressante, il est crucial de créer des clips de courte durée qui incluent une note de bord et un lien vers un personnage dans un video de courte durée. Cependant, les données publiées actuellement se concentrent principalement sur des scénarios réalistes mondiaux, ce qui entraîne une déficience d'images de référence pour garantir la cohérence des personnages. Pour compléter cette lacune, nous présentons le jeu de données d'animation de courte durée qui inclut des références, appelé AnimeShooter. AnimeShooter caractérise son processus d'automatisation par des analyses heuristiques détaillées et une forte cohérence visuelle entre les shots. Les analyses au niveau de l'histoire ont pour but de fournir des résumés des notes de bord, y compris l'histoire, les personnages clés, les profils des personnages principaux et des images de référence. D'autre part, les analyses au niveau de shot s'occupent de diviser l'histoire en shots continus et incluent des descriptions visuelles détaillées de la scène, des personnages, des notes de bord et des captions explicatives. De plus, le sous-ensemble spécial AnimeShooter-audio fournit des morceaux d'audio synchronisés, des explications vocales et des sons pour chaque shot. Pour démontrer l'effet de AnimeShooter, nous avons présenté le modèle de génération de vidéos de courte durée avec références, appelé AnimeShooterGen. Ce modèle étend les modèles de langage grand multimodal (MLLM) et de diffusion de vidéo. Les images de référence et les shots générés précédemment sont traités pour la première fois par le MLLM, générant des représentations qui comprennent un compréhension de la référence et du contexte. Ces représentations sont utilisées comme conditions pour le modèle de diffusion, décidant le prochain shot. Selon les résultats des expériences, les modèles entraînés sur AnimeShooter fonctionnent de manière cohérente visuelle entre les shots et selon les guides visuels de référence, démontrant la contribution précieuse de notre jeu de données pour la génération de vidéos d'animation de courte durée.",
      "upvotes": 10,
      "discussionId": "683fb3749f37285365b08167",
      "projectPage": "https://qiulu66.github.io/animeshooter",
      "githubRepo": "https://github.com/qiulu66/Anime-Shooter",
      "ai_summary": "AnimeShooter, a reference-guided multi-shot animation dataset, enhances coherent animated video generation by incorporating comprehensive hierarchical annotations and visual consistency, and AnimeShooterGen leverages MLLMs and video diffusion models to achieve superior results.",
      "ai_keywords": [
        "AnimeShooter",
        "reference-guided",
        "multimodal large language models (MLLMs)",
        "video diffusion models",
        "hierarchical annotations",
        "visual consistency",
        "cross-shot visual consistency",
        "reference visual guidance"
      ]
    },
    "publishedAt": "2025-06-03T13:55:18.000Z",
    "title": "AnimeShooter: A Multi-Shot Animation Dataset for Reference-Guided Video\n  Generation",
    "summary": "Recent advances in AI-generated content (AIGC) have significantly accelerated\nanimation production. To produce engaging animations, it is essential to\ngenerate coherent multi-shot video clips with narrative scripts and character\nreferences. However, existing public datasets primarily focus on real-world\nscenarios with global descriptions, and lack reference images for consistent\ncharacter guidance. To bridge this gap, we present AnimeShooter, a\nreference-guided multi-shot animation dataset. AnimeShooter features\ncomprehensive hierarchical annotations and strong visual consistency across\nshots through an automated pipeline. Story-level annotations provide an\noverview of the narrative, including the storyline, key scenes, and main\ncharacter profiles with reference images, while shot-level annotations\ndecompose the story into consecutive shots, each annotated with scene,\ncharacters, and both narrative and descriptive visual captions. Additionally, a\ndedicated subset, AnimeShooter-audio, offers synchronized audio tracks for each\nshot, along with audio descriptions and sound sources. To demonstrate the\neffectiveness of AnimeShooter and establish a baseline for the reference-guided\nmulti-shot video generation task, we introduce AnimeShooterGen, which leverages\nMultimodal Large Language Models (MLLMs) and video diffusion models. The\nreference image and previously generated shots are first processed by MLLM to\nproduce representations aware of both reference and context, which are then\nused as the condition for the diffusion model to decode the subsequent shot.\nExperimental results show that the model trained on AnimeShooter achieves\nsuperior cross-shot visual consistency and adherence to reference visual\nguidance, which highlight the value of our dataset for coherent animated video\ngeneration.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03126.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "630b094f8b327c7b8b94d24c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/630b094f8b327c7b8b94d24c/Fd0ugLOHJZIi8oUZScOmX.jpeg",
      "fullname": "Yizhuo Li",
      "name": "liyz",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.00910",
      "authors": [
        {
          "_id": "683fcd9956d0f1cfb34f1d51",
          "name": "Seongjae Kang",
          "hidden": false
        },
        {
          "_id": "683fcd9956d0f1cfb34f1d52",
          "name": "Dong Bok Lee",
          "hidden": false
        },
        {
          "_id": "683fcd9956d0f1cfb34f1d53",
          "name": "Hyungjoon Jang",
          "hidden": false
        },
        {
          "_id": "683fcd9956d0f1cfb34f1d54",
          "name": "Dongseop Kim",
          "hidden": false
        },
        {
          "_id": "683fcd9956d0f1cfb34f1d55",
          "name": "Sung Ju Hwang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-01T08:54:37.000Z",
      "submittedOnDailyAt": "2025-06-04T03:10:05.743Z",
      "title": "PCoreSet : Efficacité de l'Apprentissage des Activations dans des Modèles de Langage Visuo-Sémantique par la Compréhension du Savoir",
      "submittedOnDailyBy": {
        "_id": "6357a08f8ed056fa1ccd3b38",
        "avatarUrl": "/avatars/07d4ca8f3197a6945ad71e6150801135.svg",
        "isPro": false,
        "fullname": "erjui",
        "user": "erjui",
        "type": "user"
      },
      "summary": "La Transfert de Connaissance (TC) est un cadre largement utilisé pour entraîner des modèles légers ou spécialisés dans des tâches spécifiques, en utilisant le savoir d'un modèle enseignant. Cependant, dans l'apprentissage actif (AA) comme application de TC, l'objectif de minimiser les coûts de notation par la sélection répétitive d'échantillons reste à un état d'investigation très superficiel. Ce domaine s'appuie sur l'hypothèse d'accès à une quantité suffisante de données étiquetées pour la TC, tandis que l'AA se concentre sur des scénarios de données limitées, où un modèle enseignant spécialisé est généralement inaccessible. Dans cet article, nous présentons le cadre ActiveKD, qui intègre l'AA et la TC en utilisant les capacités de 0 shot et de peu shot de grands modèles de langage visuel (VLMs). Un point clé de ActiveKD est l'inclinaison structurée des VLMs, qui forment des clusters dans l'espace de probabilités. Cette structure peut être interprétée comme une inclinaison d'inférence du modèle enseignant et aide à comprendre des patrons de sortie généralisés qui bénéficient de l'apprentissage de l'étudiant. Pour exploiter cette inclinaison, nous proposons le Probabilistic CoreSet (PCoreSet). Contrairement au CoreSet, PCoreSet maximise la couverture dans l'espace de probabilités, ce qui permet de transmettre efficacement le savoir du modèle enseignant dans les limites de la gestion de notation, sans perdre d'efficacité. Les résultats d'évaluation sur 11 ensembles de données montrent que PCoreSet coïncide avec les méthodes actuelles de sélection et favorise l'investigation aux points de croisement entre AA et TC.",
      "upvotes": 9,
      "discussionId": "683fcd9d56d0f1cfb34f1eb1",
      "githubRepo": "https://github.com/erjui/PCoreSet",
      "ai_summary": "ActiveKD integrates active learning with knowledge distillation using large vision-language models to efficiently select diverse, unlabeled samples for annotation.",
      "ai_keywords": [
        "knowledge distillation",
        "active learning",
        "task-specific models",
        "teacher models",
        "zero-shot",
        "few-shot",
        "large vision-language models",
        "structured prediction bias",
        "inductive bias",
        "Probabilistic CoreSet",
        "PCoreSet",
        "probability space",
        "categorically diverse samples"
      ]
    },
    "publishedAt": "2025-06-01T04:54:37.000Z",
    "title": "PCoreSet: Effective Active Learning through Knowledge Distillation from\n  Vision-Language Models",
    "summary": "Knowledge distillation (KD) is a widely used framework for training compact,\ntask-specific models by leveraging the knowledge of teacher models. However,\nits application to active learning (AL), which aims to minimize annotation\ncosts through iterative sample selection, remains underexplored. This gap stems\nfrom the fact that KD typically assumes access to sufficient labeled data,\nwhereas AL operates in data-scarce scenarios where task-specific teacher models\nare often unavailable. In this paper, we introduce ActiveKD, a framework that\nintegrates AL with KD by leveraging the zero- and few-shot capabilities of\nlarge vision-language models (VLMs). A key aspect of ActiveKD is the structured\nprediction bias of VLMs -- i.e., their predictions form clusters in the\nprobability space. We regard this structure as an inductive bias of the teacher\nmodel, capturing generalizable output patterns beneficial to student learning.\nTo exploit this bias, we propose Probabilistic CoreSet (PCoreSet), a selection\nstrategy that maximizes coverage in the probability space rather than the\nfeature space. PCoreSet strategically selects categorically diverse unlabeled\nsamples, facilitating more efficient transfer of teacher knowledge under\nlimited annotation budgets. Evaluations on 11 datasets show that PCoreSet\nconsistently outperforms existing selection methods within the ActiveKD\nframework, advancing research at the intersection of AL and KD.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.00910.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6357a08f8ed056fa1ccd3b38",
      "avatarUrl": "/avatars/07d4ca8f3197a6945ad71e6150801135.svg",
      "fullname": "erjui",
      "name": "erjui",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.02497",
      "authors": [
        {
          "_id": "683fbc32917306517315589f",
          "name": "Jiahao Chen",
          "hidden": false
        },
        {
          "_id": "683fbc3291730651731558a0",
          "name": "Hangjie Yuan",
          "hidden": false
        },
        {
          "_id": "683fbc3291730651731558a1",
          "name": "Yichen Qian",
          "hidden": false
        },
        {
          "_id": "683fbc3291730651731558a2",
          "name": "Jingyun Liang",
          "hidden": false
        },
        {
          "_id": "683fbc3291730651731558a3",
          "name": "Jiazheng Xing",
          "hidden": false
        },
        {
          "_id": "683fbc3291730651731558a4",
          "name": "Pengwei Liu",
          "hidden": false
        },
        {
          "_id": "683fbc3291730651731558a5",
          "name": "Weihua Chen",
          "hidden": false
        },
        {
          "_id": "683fbc3291730651731558a6",
          "name": "Fan Wang",
          "hidden": false
        },
        {
          "_id": "683fbc3291730651731558a7",
          "name": "Bing Su",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T06:25:00.000Z",
      "submittedOnDailyAt": "2025-06-04T01:55:34.274Z",
      "title": "LumosFlow : Création de vidéos longues qui guident l'action",
      "submittedOnDailyBy": {
        "_id": "649d54b314afbb10ce2a9eeb",
        "avatarUrl": "/avatars/15c325d8c2273ff63569f23015e98486.svg",
        "isPro": false,
        "fullname": "Hangjie Yuan",
        "user": "JacobYuan",
        "type": "user"
      },
      "summary": "La génération de vidéos longues est largement utilisée dans diverses domaines comme l'entertainment et la simulation, et ce domaine reçoit beaucoup d'attention. Bien que progresse, la synthèse de séquences visuelles longues dans le temps est un défi complexe. Les méthodes existantes génèrent des courts clips séquentiellement et les combinent ou créent des marques clés et interpolent les cadres intermédiaires de manière heuristique pour former un vidéo long. Cependant, les deux enfrentent des défis significatifs, notamment des répétitions temporelles et des transitions non naturelles. Dans cet article, nous examinons le pipeline de génération de vidéos longues et nous présentons LumosFlow, une nouvelle structure. LumosFlow adopte une guidée explicite du mouvement. Concrètement, on utilise un modèle de diffusion vidéo (LMTV-DM) basé sur le texte pour générer des marques clés à intervalles de grands mouvements, assurant ainsi la diversité du contenu du vidéo. Étant donné que les transitions contextuelles entre les marques clés sont complexes, on divise l'interpolation des cadres intermédiaires en la génération du mouvement et le traitement postérieur. Pour chaque paire de marques clés, on utilise un modèle de diffusion de flux optique (LOF-DM) pour synthétiser des flux optiques complexes de grands mouvements, et ensuite on utilise MotionControlNet pour guider la génération des cadres, améliorant sa qualité. Comparée à l'interpolation des cadres existantes, LumosFlow atteint une interpolation 15 fois plus efficace, assurant des mouvements continus adéquats entre les cadres adjacents. Les expériences montrent que notre méthode permet de générer des vidéos longues avec des mouvements cohérents et des caractéristiques définies. Les codes et modèles seront publiés après l'approbation. La page du projet est disponible sur https://jiahaochen1.github.io/LumosFlow/.",
      "upvotes": 7,
      "discussionId": "683fbc36917306517315597d",
      "projectPage": "https://jiahaochen1.github.io/LumosFlow/",
      "ai_summary": "LumosFlow uses LMTV-DM for key frame generation and LOF-DM followed by MotionControlNet for smooth intermediate frame interpolation, ensuring temporally coherent long video generation.",
      "ai_keywords": [
        "Large Motion Text-to-Video Diffusion Model",
        "LMTV-DM",
        "Latent Optical Flow Diffusion Model",
        "LOF-DM",
        "MotionControlNet",
        "optical flows",
        "frame interpolation",
        "key frames",
        "long video generation",
        "motion guidance",
        "synthetic long videos"
      ]
    },
    "publishedAt": "2025-06-03T02:25:00.000Z",
    "title": "LumosFlow: Motion-Guided Long Video Generation",
    "summary": "Long video generation has gained increasing attention due to its widespread\napplications in fields such as entertainment and simulation. Despite advances,\nsynthesizing temporally coherent and visually compelling long sequences remains\na formidable challenge. Conventional approaches often synthesize long videos by\nsequentially generating and concatenating short clips, or generating key frames\nand then interpolate the intermediate frames in a hierarchical manner. However,\nboth of them still remain significant challenges, leading to issues such as\ntemporal repetition or unnatural transitions. In this paper, we revisit the\nhierarchical long video generation pipeline and introduce LumosFlow, a\nframework introduce motion guidance explicitly. Specifically, we first employ\nthe Large Motion Text-to-Video Diffusion Model (LMTV-DM) to generate key frames\nwith larger motion intervals, thereby ensuring content diversity in the\ngenerated long videos. Given the complexity of interpolating contextual\ntransitions between key frames, we further decompose the intermediate frame\ninterpolation into motion generation and post-hoc refinement. For each pair of\nkey frames, the Latent Optical Flow Diffusion Model (LOF-DM) synthesizes\ncomplex and large-motion optical flows, while MotionControlNet subsequently\nrefines the warped results to enhance quality and guide intermediate frame\ngeneration. Compared with traditional video frame interpolation, we achieve 15x\ninterpolation, ensuring reasonable and continuous motion between adjacent\nframes. Experiments show that our method can generate long videos with\nconsistent motion and appearance. Code and models will be made publicly\navailable upon acceptance. Our project page:\nhttps://jiahaochen1.github.io/LumosFlow/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.02497.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "649d54b314afbb10ce2a9eeb",
      "avatarUrl": "/avatars/15c325d8c2273ff63569f23015e98486.svg",
      "fullname": "Hangjie Yuan",
      "name": "JacobYuan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.01144",
      "authors": [
        {
          "_id": "683fca92c1e51fea3a470e93",
          "name": "Ariel Shaulov",
          "hidden": false
        },
        {
          "_id": "683fca92c1e51fea3a470e94",
          "user": {
            "_id": "64972b8d27e41e26a32835d4",
            "avatarUrl": "/avatars/00c76991b5421f592d632a750ec8b998.svg",
            "isPro": false,
            "fullname": "Itay Hazan",
            "user": "itayhzn",
            "type": "user"
          },
          "name": "Itay Hazan",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-06-04T06:31:03.556Z",
          "hidden": false
        },
        {
          "_id": "683fca92c1e51fea3a470e95",
          "name": "Lior Wolf",
          "hidden": false
        },
        {
          "_id": "683fca92c1e51fea3a470e96",
          "user": {
            "_id": "6181c72cdcc1df2c9de8a4d8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1655248010394-6181c72cdcc1df2c9de8a4d8.jpeg",
            "isPro": false,
            "fullname": "Hila Chefer",
            "user": "Hila",
            "type": "user"
          },
          "name": "Hila Chefer",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:54:36.782Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6181c72cdcc1df2c9de8a4d8/z3VoHZmxOtL3agHCaWWg4.mp4"
      ],
      "publishedAt": "2025-06-01T19:55:33.000Z",
      "submittedOnDailyAt": "2025-06-04T02:58:51.483Z",
      "title": "FlowMo : Carte de flux basée sur le registre de mouvements continus de vidéos",
      "submittedOnDailyBy": {
        "_id": "6181c72cdcc1df2c9de8a4d8",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1655248010394-6181c72cdcc1df2c9de8a4d8.jpeg",
        "isPro": false,
        "fullname": "Hila Chefer",
        "user": "Hila",
        "type": "user"
      },
      "summary": "Le modèle de distribution d'animation en contexte a des limitations mémoirières pour modéliser l'apparence temporelle (par exemple, le mouvement, la physique, les interactions dynamiques). Les méthodes actuelles pour aborder ce problème impliquent le rétention de modèles ou l'introduction de signaux de conditions externes pour imposer la cohérence temporelle. Dans cette étude, nous avons investigué si il est possible d'extraire une représentation significative du temps à partir des prédictions d'un modèle entraîné précédemment, sans inclure plus d'entraînement ou d'entrées supplémentaires. Nous avons introduit un nouveau méthode de guidage sans entraînement appelé FlowMo, qui améliore la cohérence du mouvement en utilisant les prédictions du modèle à chaque étape de la distribution. FlowMo mesure la distance entre les variables latentes correspondantes aux frames continus pour obtenir une représentation du temps indépendante de la surface, ce qui permet aux modèles de prédire clairement la structure potentielle du temps. Ensuite, la variance par dimensions de temps est mesurée et l'cohérence du mouvement est évaluée, guidant le modèle vers une réduction de cette variance. Dans une large gamme d'expériences avec des modèles d'animation en contexte, FlowMo a amélioré significativement la cohérence du mouvement, maintenant la qualité ou la compatibilité avec le projet, et a amélioré la précision du temps dans des modèles de distribution d'animation entraînés précédemment, offrant une solution efficace \"jouer et jouer\".",
      "upvotes": 7,
      "discussionId": "683fca94c1e51fea3a470eee",
      "projectPage": "https://arielshaulov.github.io/FlowMo/",
      "githubRepo": "https://github.com/arielshaulov/FlowMo",
      "ai_summary": "FlowMo, a training-free method, enhances motion coherence in pre-trained text-to-video diffusion models by leveraging their own predictions to reduce patch-wise temporal variance.",
      "ai_keywords": [
        "text-to-video diffusion models",
        "temporal aspects",
        "motion",
        "physics",
        "dynamic interactions",
        "pre-trained model",
        "FlowMo",
        "guidance method",
        "appearance-debiased",
        "temporal representation",
        "latents",
        "patch-wise variance",
        "sampling",
        "temporal fidelity"
      ]
    },
    "publishedAt": "2025-06-01T15:55:33.000Z",
    "title": "FlowMo: Variance-Based Flow Guidance for Coherent Motion in Video\n  Generation",
    "summary": "Text-to-video diffusion models are notoriously limited in their ability to\nmodel temporal aspects such as motion, physics, and dynamic interactions.\nExisting approaches address this limitation by retraining the model or\nintroducing external conditioning signals to enforce temporal consistency. In\nthis work, we explore whether a meaningful temporal representation can be\nextracted directly from the predictions of a pre-trained model without any\nadditional training or auxiliary inputs. We introduce FlowMo, a novel\ntraining-free guidance method that enhances motion coherence using only the\nmodel's own predictions in each diffusion step. FlowMo first derives an\nappearance-debiased temporal representation by measuring the distance between\nlatents corresponding to consecutive frames. This highlights the implicit\ntemporal structure predicted by the model. It then estimates motion coherence\nby measuring the patch-wise variance across the temporal dimension and guides\nthe model to reduce this variance dynamically during sampling. Extensive\nexperiments across multiple text-to-video models demonstrate that FlowMo\nsignificantly improves motion coherence without sacrificing visual quality or\nprompt alignment, offering an effective plug-and-play solution for enhancing\nthe temporal fidelity of pre-trained video diffusion models.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6181c72cdcc1df2c9de8a4d8/z3VoHZmxOtL3agHCaWWg4.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01144.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6181c72cdcc1df2c9de8a4d8",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1655248010394-6181c72cdcc1df2c9de8a4d8.jpeg",
      "fullname": "Hila Chefer",
      "name": "Hila",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.01274",
      "authors": [
        {
          "_id": "683fe4caf8916dcd6d1c936a",
          "user": {
            "_id": "673060959e631f353ae1b5e0",
            "avatarUrl": "/avatars/d4b1e23de90ff1d02c38186a259b8d1e.svg",
            "isPro": false,
            "fullname": "Hosu Lee",
            "user": "lakelee",
            "type": "user"
          },
          "name": "Hosu Lee",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:54:26.062Z",
          "hidden": false
        },
        {
          "_id": "683fe4caf8916dcd6d1c936b",
          "user": {
            "_id": "653238bed0f5a9e537ed966d",
            "avatarUrl": "/avatars/e97a83e68e770baa1c5df847777cf213.svg",
            "isPro": false,
            "fullname": "Junho Kim",
            "user": "arkimjh",
            "type": "user"
          },
          "name": "Junho Kim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:54:30.239Z",
          "hidden": false
        },
        {
          "_id": "683fe4caf8916dcd6d1c936c",
          "name": "Hyunjun Kim",
          "hidden": false
        },
        {
          "_id": "683fe4caf8916dcd6d1c936d",
          "name": "Yong Man Ro",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-02T03:08:07.000Z",
      "submittedOnDailyAt": "2025-06-04T04:50:22.651Z",
      "title": "ReFoCUS : Optimisation de Fréquences par Compréhension du Contexte avec Apprentissage par Référence",
      "submittedOnDailyBy": {
        "_id": "653238bed0f5a9e537ed966d",
        "avatarUrl": "/avatars/e97a83e68e770baa1c5df847777cf213.svg",
        "isPro": false,
        "fullname": "Junho Kim",
        "user": "arkimjh",
        "type": "user"
      },
      "summary": "Récemment, le développement de grands modèles de langage et de vision (LMMs) a permis une inférence visuelle efficace, bien que sa compréhension du contenu visuel soit limitée par des problèmes de stratégie de sélection de cadences. Les méthodes actuelles utilisent principalement des heuristiques statiques ou des modules de recherche externes pour fournir des informations sur les cadences aux modèles de vidéo-LLM, ce qui ne fournit pas d'informations pertinentes. Dans cet article, nous présentons ReFoCUS (Reinforcement-guided Frame Optimization for Contextual Understanding). ReFoCUS est un cadre de travail d'optimisation de politiques sur les cadences, qui transfère l'objectif d'optimisation à la sélection de cadences basée sur l'entrée d'image en réponse de chaînes de caractères. ReFoCUS utilise l'apprentissage par renforcement pour entraîner une politique de sélection de cadences qui reflète la préférence interne des cadences optimales pour des réponses temporellement séquentielles, en utilisant des signaux de récompense obtenus dans le LMM de référence. Pour explorer efficacement, une architecture de sélection conditionnelle automatique est utilisée pour garantir la continuité temporelle et réduire la complexité. Notre approche ne requiert pas de contrôle explicite à l'échelle des cadences, mais améliore constamment le rendement logique sur plusieurs benchmarks de questions et réponses vidéo, et démontre la concordance entre la sélection de cadences et l'utilité interne du modèle par le biais de beta.",
      "upvotes": 4,
      "discussionId": "683fe4ccf8916dcd6d1c93b6",
      "ai_summary": "ReFoCUS uses reinforcement learning to optimize frame selection for video-LLM, enhancing reasoning performance in video QA by aligning with model preferences.",
      "ai_keywords": [
        "Large Multi-modal Models",
        "vision-language reasoning",
        "frame selection strategies",
        "reinforcement learning",
        "frame selection policy",
        "reference LMM",
        "autoregressive architecture",
        "conditional selection architecture",
        "temporal coherence",
        "video QA benchmarks"
      ]
    },
    "publishedAt": "2025-06-01T23:08:07.000Z",
    "title": "ReFoCUS: Reinforcement-guided Frame Optimization for Contextual\n  Understanding",
    "summary": "Recent progress in Large Multi-modal Models (LMMs) has enabled effective\nvision-language reasoning, yet the ability to understand video content remains\nconstrained by suboptimal frame selection strategies. Existing approaches often\nrely on static heuristics or external retrieval modules to feed frame\ninformation into video-LLMs, which may fail to provide the query-relevant\ninformation. In this work, we introduce ReFoCUS (Reinforcement-guided Frame\nOptimization for Contextual UnderStanding), a novel frame-level policy\noptimization framework that shifts the optimization target from textual\nresponses to visual input selection. ReFoCUS learns a frame selection policy\nvia reinforcement learning, using reward signals derived from a reference LMM\nto reflect the model's intrinsic preferences for frames that best support\ntemporally grounded responses. To efficiently explore the large combinatorial\nframe space, we employ an autoregressive, conditional selection architecture\nthat ensures temporal coherence while reducing complexity. Our approach does\nnot require explicit supervision at the frame-level and consistently improves\nreasoning performance across multiple video QA benchmarks, highlighting the\nbenefits of aligning frame selection with model-internal utility.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01274.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "653238bed0f5a9e537ed966d",
      "avatarUrl": "/avatars/e97a83e68e770baa1c5df847777cf213.svg",
      "fullname": "Junho Kim",
      "name": "arkimjh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.24726",
      "authors": [
        {
          "_id": "683ffca568402c738a947f4e",
          "name": "Shelly Bensal",
          "hidden": false
        },
        {
          "_id": "683ffca568402c738a947f4f",
          "name": "Umar Jamil",
          "hidden": false
        },
        {
          "_id": "683ffca568402c738a947f50",
          "name": "Christopher Bryant",
          "hidden": false
        },
        {
          "_id": "683ffca568402c738a947f51",
          "user": {
            "_id": "60e61b3969bd0df25c9375da",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1625692968400-noauth.jpeg",
            "isPro": false,
            "fullname": "Melisa Russak",
            "user": "melisa",
            "type": "user"
          },
          "name": "Melisa Russak",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:53:45.047Z",
          "hidden": false
        },
        {
          "_id": "683ffca568402c738a947f52",
          "name": "Kiran Kamble",
          "hidden": false
        },
        {
          "_id": "683ffca568402c738a947f53",
          "name": "Dmytro Mozolevskyi",
          "hidden": false
        },
        {
          "_id": "683ffca568402c738a947f54",
          "name": "Muayad Ali",
          "hidden": false
        },
        {
          "_id": "683ffca568402c738a947f55",
          "name": "Waseem AlShikh",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-30T15:49:42.000Z",
      "submittedOnDailyAt": "2025-06-04T06:34:02.563Z",
      "title": "Réflexion, Test de Nouveau, Récompense : Appliquer l'amélioration personnelle aux modèles d'apprentissage profond (LLMs) avec l'apprentissage par réponse",
      "submittedOnDailyBy": {
        "_id": "60e61b3969bd0df25c9375da",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1625692968400-noauth.jpeg",
        "isPro": false,
        "fullname": "Melisa Russak",
        "user": "melisa",
        "type": "user"
      },
      "summary": "Nous examinons des méthodes pour améliorer le rendement des grands modèles de langue par une réflexion sur eux-mêmes. Nous montrons que, lorsque le modèle répond incorrectement, il reçoit une motivation pour réfléchir davantage et améliorer sa capacité à résoudre des tâches complexes et vérifiables, même dans des cas où la génération de données synthétiques est impossible et lorsqu'un feedback binaire existe. Notre cadre fonctionne en deux étapes : d'abord, si la tâche échoue, le modèle crée des commentaires qui lui aident à réfléchir sur son propre erreur. Ensuite, avec ces commentaires, il essaie la tâche à nouveau. Si l'essai nouveau est réussi, les tokens générés lors de la phase de réflexion reçoivent une récompense. Les résultats des expériences montrent une amélioration significative dans différentes architectures de modèle, avec un augmentation de 34,7% pour les expressions mathématiques et de 18,1% pour les appels à fonctions. En particulier, les modèles d'ajustement micro avec entre 150 et 700 millions de paramètres dépassent les modèles de la même famille d'un ordre de grandeur. Notre nouveau paradigme offre une intéressante voie pour développer des modèles de langue plus utiles et stables qui améliorent automatiquement dans des tâches difficiles, malgré les limitations du feedback externe.",
      "upvotes": 4,
      "discussionId": "683ffca568402c738a947f7b",
      "ai_summary": "A method using self-reflection and reinforcement learning improves the performance of large language models, especially with limited feedback, by rewarding self-reflections that lead to better task performance.",
      "ai_keywords": [
        "self-reflection",
        "reinforcement learning",
        "self-reflective commentary",
        "performance gains",
        "math equation writing",
        "function calling",
        "parameter-efficient fine-tuning"
      ]
    },
    "publishedAt": "2025-05-30T11:49:42.000Z",
    "title": "Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning",
    "summary": "We explore a method for improving the performance of large language models\nthrough self-reflection and reinforcement learning. By incentivizing the model\nto generate better self-reflections when it answers incorrectly, we demonstrate\nthat a model's ability to solve complex, verifiable tasks can be enhanced even\nwhen generating synthetic data is infeasible and only binary feedback is\navailable. Our framework operates in two stages: first, upon failing a given\ntask, the model generates a self-reflective commentary analyzing its previous\nattempt; second, the model is given another attempt at the task with the\nself-reflection in context. If the subsequent attempt succeeds, the tokens\ngenerated during the self-reflection phase are rewarded. Our experimental\nresults show substantial performance gains across a variety of model\narchitectures, as high as 34.7% improvement at math equation writing and 18.1%\nimprovement at function calling. Notably, smaller fine-tuned models (1.5\nbillion to 7 billion parameters) outperform models in the same family that are\n10 times larger. Our novel paradigm is thus an exciting pathway to more useful\nand reliable language models that can self-improve on challenging tasks with\nlimited external feedback.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24726.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60e61b3969bd0df25c9375da",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1625692968400-noauth.jpeg",
      "fullname": "Melisa Russak",
      "name": "melisa",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 30
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.03079",
      "authors": [
        {
          "_id": "683fee0a179d710da07d4352",
          "user": {
            "_id": "634aab35dcf125e4dafc87b1",
            "avatarUrl": "/avatars/aa2db84fd423e9eefe3ef3167c9d3999.svg",
            "isPro": false,
            "fullname": "YangXiuyu",
            "user": "gzzyyxy",
            "type": "user"
          },
          "name": "Xiuyu Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:54:13.216Z",
          "hidden": true
        },
        {
          "_id": "683fee0a179d710da07d4353",
          "name": "Bohan Li",
          "hidden": false
        },
        {
          "_id": "683fee0a179d710da07d4354",
          "name": "Shaocong Xu",
          "hidden": false
        },
        {
          "_id": "683fee0a179d710da07d4355",
          "name": "Nan Wang",
          "hidden": false
        },
        {
          "_id": "683fee0a179d710da07d4356",
          "name": "Chongjie Ye",
          "hidden": false
        },
        {
          "_id": "683fee0a179d710da07d4357",
          "name": "Zhaoxi Chen",
          "hidden": false
        },
        {
          "_id": "683fee0a179d710da07d4358",
          "name": "Minghan Qin",
          "hidden": false
        },
        {
          "_id": "683fee0a179d710da07d4359",
          "name": "Yikang Ding",
          "hidden": false
        },
        {
          "_id": "683fee0a179d710da07d435a",
          "name": "Xin Jin",
          "hidden": false
        },
        {
          "_id": "683fee0a179d710da07d435b",
          "name": "Hang Zhao",
          "hidden": false
        },
        {
          "_id": "683fee0a179d710da07d435c",
          "name": "Hao Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T17:00:32.000Z",
      "submittedOnDailyAt": "2025-06-04T05:27:07.241Z",
      "title": "ORV : Création de vidéos de robot au centre d'opérations 4D",
      "submittedOnDailyBy": {
        "_id": "634aab35dcf125e4dafc87b1",
        "avatarUrl": "/avatars/aa2db84fd423e9eefe3ef3167c9d3999.svg",
        "isPro": false,
        "fullname": "YangXiuyu",
        "user": "gzzyyxy",
        "type": "user"
      },
      "summary": "Obtenir des données de simulation de robots en réalité en téléopération nécessite du temps et de l'effort. Récemment, des modèles de génétique d'actions ont été largement introduits dans l'apprentissage de robots et la simulation, réduisant l'effort de maintenance et évitant des préoccupations de sécurité. Cependant, les séquences d'actions utilisées dans ces méthodes présentent des problèmes de précision de contrôle et de capacité de généralisation en raison de la faible codification globale. Pour résoudre ces limitations, on propose le cadre de création de vidéos de robots (ORV). ORV utilise des séquences 4D significatives de Flux d'Occupation pour représenter l'action linéairement, offrant une orientation plus précise et géométrique. En convertissant la représentation basée sur le Flux d'Occupation, ORV permet une conversion facile des données de simulation en vidéos de robots réalistes, garantissant une haute cohérence temporelle et la possibilité de contrôle précis. De plus, elle supporte la génération simultanée de vidéos multifocales pour des fins de manipulation micro, fournissant des capacités cruciales pour des tâches d'apprentissage de robots. Les résultats expérimentaux étendus montrent que ORV est consistant et dépasse les méthodes de base sur différents ensembles de données et sous-tâches. Demo, code et modèle : https://orangesodahub.github.io/ORV",
      "upvotes": 3,
      "discussionId": "683fee12179d710da07d45f4",
      "projectPage": "https://orangesodahub.github.io/ORV/",
      "githubRepo": "https://github.com/OrangeSodahub/ORV",
      "ai_summary": "ORV, an Occupancy-centric Robot Video generation framework, uses 4D semantic occupancy sequences to produce photorealistic, temporally consistent, and precisely controllable robot videos, enhancing existing methods.",
      "ai_keywords": [
        "action-driven generative models",
        "occupancy-centric",
        "4D semantic occupancy sequences",
        "video generation",
        "photorealistic robot videos",
        "temporal consistency",
        "precise controllability",
        "multi-view videos"
      ]
    },
    "publishedAt": "2025-06-03T13:00:32.000Z",
    "title": "ORV: 4D Occupancy-centric Robot Video Generation",
    "summary": "Acquiring real-world robotic simulation data through teleoperation is\nnotoriously time-consuming and labor-intensive. Recently, action-driven\ngenerative models have gained widespread adoption in robot learning and\nsimulation, as they eliminate safety concerns and reduce maintenance efforts.\nHowever, the action sequences used in these methods often result in limited\ncontrol precision and poor generalization due to their globally coarse\nalignment. To address these limitations, we propose ORV, an Occupancy-centric\nRobot Video generation framework, which utilizes 4D semantic occupancy\nsequences as a fine-grained representation to provide more accurate semantic\nand geometric guidance for video generation. By leveraging occupancy-based\nrepresentations, ORV enables seamless translation of simulation data into\nphotorealistic robot videos, while ensuring high temporal consistency and\nprecise controllability. Furthermore, our framework supports the simultaneous\ngeneration of multi-view videos of robot gripping operations - an important\ncapability for downstream robotic learning tasks. Extensive experimental\nresults demonstrate that ORV consistently outperforms existing baseline methods\nacross various datasets and sub-tasks. Demo, Code and Model:\nhttps://orangesodahub.github.io/ORV",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03079.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "634aab35dcf125e4dafc87b1",
      "avatarUrl": "/avatars/aa2db84fd423e9eefe3ef3167c9d3999.svg",
      "fullname": "YangXiuyu",
      "name": "gzzyyxy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.01789",
      "authors": [
        {
          "_id": "683fb99c7c8d720be438000a",
          "name": "Genta Indra Winata",
          "hidden": false
        },
        {
          "_id": "683fb99c7c8d720be438000b",
          "name": "David Anugraha",
          "hidden": false
        },
        {
          "_id": "683fb99c7c8d720be438000c",
          "name": "Emmy Liu",
          "hidden": false
        },
        {
          "_id": "683fb99c7c8d720be438000d",
          "name": "Alham Fikri Aji",
          "hidden": false
        },
        {
          "_id": "683fb99c7c8d720be438000e",
          "name": "Shou-Yi Hung",
          "hidden": false
        },
        {
          "_id": "683fb99c7c8d720be438000f",
          "name": "Aditya Parashar",
          "hidden": false
        },
        {
          "_id": "683fb99c7c8d720be4380010",
          "user": {
            "_id": "64d1e3a87e20ec9ea0020d03",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64d1e3a87e20ec9ea0020d03/xm-afh1AaqS0e9qpaPo7y.jpeg",
            "isPro": false,
            "fullname": "Patrick Amadeus Irawan",
            "user": "patrickamadeus",
            "type": "user"
          },
          "name": "Patrick Amadeus Irawan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:55:30.937Z",
          "hidden": false
        },
        {
          "_id": "683fb99c7c8d720be4380011",
          "name": "Ruochen Zhang",
          "hidden": false
        },
        {
          "_id": "683fb99c7c8d720be4380012",
          "name": "Zheng-Xin Yong",
          "hidden": false
        },
        {
          "_id": "683fb99c7c8d720be4380013",
          "name": "Jan Christian Blaise Cruz",
          "hidden": false
        },
        {
          "_id": "683fb99c7c8d720be4380014",
          "name": "Niklas Muennighoff",
          "hidden": false
        },
        {
          "_id": "683fb99c7c8d720be4380015",
          "user": {
            "_id": "6469949654873f0043b09c22",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6469949654873f0043b09c22/Lk7IJAR16Wa_sGJ2g81AQ.jpeg",
            "isPro": false,
            "fullname": "Seungone Kim",
            "user": "seungone",
            "type": "user"
          },
          "name": "Seungone Kim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:55:22.158Z",
          "hidden": false
        },
        {
          "_id": "683fb99c7c8d720be4380016",
          "name": "Hanyang Zhao",
          "hidden": false
        },
        {
          "_id": "683fb99c7c8d720be4380017",
          "user": {
            "_id": "63139ff6b46fc4e24332fa84",
            "avatarUrl": "/avatars/ee6923a7cb218f22535064a87761e497.svg",
            "isPro": false,
            "fullname": "Sudipta Kar",
            "user": "cryptexcode",
            "type": "user"
          },
          "name": "Sudipta Kar",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:55:28.286Z",
          "hidden": false
        },
        {
          "_id": "683fb99c7c8d720be4380018",
          "name": "Kezia Erina Suryoraharjo",
          "hidden": false
        },
        {
          "_id": "683fb99c7c8d720be4380019",
          "name": "M. Farid Adilazuarda",
          "hidden": false
        },
        {
          "_id": "683fb99c7c8d720be438001a",
          "name": "En-Shiun Annie Lee",
          "hidden": false
        },
        {
          "_id": "683fb99c7c8d720be438001b",
          "name": "Ayu Purwarianti",
          "hidden": false
        },
        {
          "_id": "683fb99c7c8d720be438001c",
          "name": "Derry Tanti Wijaya",
          "hidden": false
        },
        {
          "_id": "683fb99c7c8d720be438001d",
          "name": "Monojit Choudhury",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-02T15:31:52.000Z",
      "submittedOnDailyAt": "2025-06-04T01:42:44.251Z",
      "title": "Les données ne sont pas suffisantes : mesure et responsabilité de la validité des données et de l'automatisation",
      "submittedOnDailyBy": {
        "_id": "5f5c4b20e56d546cd6233098",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1637813888895-5f5c4b20e56d546cd6233098.jpeg",
        "isPro": false,
        "fullname": "Genta Indra Winata",
        "user": "gentaiscool",
        "type": "user"
      },
      "summary": "Les ensembles de données de haute qualité sont une existence fondamentale pour l'entraînement et l'évaluation de modèles d'apprentissage automatique, et leur génération, en particulier par annotation humaine précise, est considérée comme un grand défi. De nombreux articles sur les ensembles de données présentent des lacunes en termes d'originalité, de diversité ou de gestion de qualité stricte, et ces limitations sont souvent ignorées lors de l'évaluation. De plus, les détails importants sur la construction et les caractéristiques des ensembles de données sont souvent omis dans les articles. Les outils existants, comme les datasheets, sont conçus pour promouvoir la visualisation, mais principalement fournissent des méthodes explicatives et pas une évaluation quantitative et standardisée de la qualité des données. De plus, la exigence de métadonnées dans les conférences a été un incitatif pour la responsabilité, mais aussi une application inégale. Pour résoudre ces limitations, cet article argumente la nécessité d'intégrer des points de révision systématiques dans les critères d'évaluation des ensembles de données. De plus, une méthodologie échelonnable et rentable de génération de données synthétiques est revue, en utilisant des outils appropriés et l'approche d'un LLM comme jury. Comme incitatif pour les actions, on présente DataRubrics, un cadre structuré pour évaluer la qualité des ensembles de données générés par des humains ou des modèles. Les avancées récentes dans l'évaluation basée sur un LLM fournissent des solutions reproductibles, échelonnables et actionnables pour l'évaluation de la qualité des ensembles de données, permettant aux chercheurs de maintenir des normes élevées dans leur recherche axée sur les données. De plus, le code pour soutenir la reproductibilité de l'évaluation basée sur un LLM est disponible sur https://github.com/datarubrics/datarubrics.",
      "upvotes": 3,
      "discussionId": "683fb99e7c8d720be4380090"
    },
    "publishedAt": "2025-06-02T11:31:52.000Z",
    "title": "Datasheets Aren't Enough: DataRubrics for Automated Quality Metrics and\n  Accountability",
    "summary": "High-quality datasets are fundamental to training and evaluating machine\nlearning models, yet their creation-especially with accurate human\nannotations-remains a significant challenge. Many dataset paper submissions\nlack originality, diversity, or rigorous quality control, and these\nshortcomings are often overlooked during peer review. Submissions also\nfrequently omit essential details about dataset construction and properties.\nWhile existing tools such as datasheets aim to promote transparency, they are\nlargely descriptive and do not provide standardized, measurable methods for\nevaluating data quality. Similarly, metadata requirements at conferences\npromote accountability but are inconsistently enforced. To address these\nlimitations, this position paper advocates for the integration of systematic,\nrubric-based evaluation metrics into the dataset review process-particularly as\nsubmission volumes continue to grow. We also explore scalable, cost-effective\nmethods for synthetic data generation, including dedicated tools and\nLLM-as-a-judge approaches, to support more efficient evaluation. As a call to\naction, we introduce DataRubrics, a structured framework for assessing the\nquality of both human- and model-generated datasets. Leveraging recent advances\nin LLM-based evaluation, DataRubrics offers a reproducible, scalable, and\nactionable solution for dataset quality assessment, enabling both authors and\nreviewers to uphold higher standards in data-centric research. We also release\ncode to support reproducibility of LLM-based evaluations at\nhttps://github.com/datarubrics/datarubrics.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01789.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5f5c4b20e56d546cd6233098",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1637813888895-5f5c4b20e56d546cd6233098.jpeg",
      "fullname": "Genta Indra Winata",
      "name": "gentaiscool",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 17
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.03096",
      "authors": [
        {
          "_id": "684002718bd5bff9918ce018",
          "user": {
            "_id": "6310a6bb0a43f97f6c5567d3",
            "avatarUrl": "/avatars/04b07c3a6c811337939c951567cb2bf2.svg",
            "isPro": false,
            "fullname": "Christian Schlarmann",
            "user": "chs20",
            "type": "user"
          },
          "name": "Christian Schlarmann",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:53:34.919Z",
          "hidden": false
        },
        {
          "_id": "684002718bd5bff9918ce019",
          "name": "Francesco Croce",
          "hidden": false
        },
        {
          "_id": "684002718bd5bff9918ce01a",
          "name": "Nicolas Flammarion",
          "hidden": false
        },
        {
          "_id": "684002718bd5bff9918ce01b",
          "name": "Matthias Hein",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T17:27:12.000Z",
      "submittedOnDailyAt": "2025-06-04T08:12:15.755Z",
      "title": "FuseLIP : Fusion Discrète de Tokens Initiaux pour Compréhension Multimodale",
      "submittedOnDailyBy": {
        "_id": "6310a6bb0a43f97f6c5567d3",
        "avatarUrl": "/avatars/04b07c3a6c811337939c951567cb2bf2.svg",
        "isPro": false,
        "fullname": "Christian Schlarmann",
        "user": "chs20",
        "type": "user"
      },
      "summary": "Contrastive language-image pre-training utilise différents encodeurs dans chaque modèle et ajuste les caractéristiques de pairs de texte et d'image dans un espace potentiel commun. Ce cadre de travail montre des résultats notables dans de nombreuses tâches de zero-shot, mais initialement ne peut pas traiter des entrées de plusieurs modèles, ce qui rend impossible la codification d'images et de texte dans un seul vecteur de caractéristiques. Par conséquent, la pratique courante est d'utiliser des modules supplémentaires pour intégrer les caractéristiques extraites d'un seul modèle. Dans cet article, nous proposons une architecture alternative pour un encodeur de plusieurs modèles appelé FuseLIP. En utilisant le développement récent des tokenisateurs discrets d'images, nous proposons un modèle de transformation efficace pour l'espace de vocabulaire étendu des tokenisateurs de texte et d'images. Dans ce cadre d'intégration initial, les modèles différents interagissent mutuellement et obtiennent des représentations plus riches que les fusions générales de retard. Nous collectons un nouveau ensemble de données et concevons des tâches difficiles pour un encodeur de plusieurs modèles. FuseLIP dépasse les autres approches dans des tâches telles que la VQA et la recherche d'images guidées par texte, mais dans des tâches d'un seul modèle, il montre un rendement similaire à celui de référence.",
      "upvotes": 2,
      "discussionId": "684002758bd5bff9918ce109"
    },
    "publishedAt": "2025-06-03T13:27:12.000Z",
    "title": "FuseLIP: Multimodal Embeddings via Early Fusion of Discrete Tokens",
    "summary": "Contrastive language-image pre-training aligns the features of text-image\npairs in a common latent space via distinct encoders for each modality. While\nthis approach achieves impressive performance in several zero-shot tasks, it\ncannot natively handle multimodal inputs, i.e., encoding image and text into a\nsingle feature vector. As a remedy, it is common practice to use additional\nmodules to merge the features extracted by the unimodal encoders. In this work,\nwe present FuseLIP, an alternative architecture for multimodal embedding.\nLeveraging recent progress in discrete image tokenizers, we propose to use a\nsingle transformer model which operates on an extended vocabulary of text and\nimage tokens. This early fusion approach allows the different modalities to\ninteract at each depth of encoding and obtain richer representations compared\nto common late fusion. We collect new datasets for multimodal pre-training and\nevaluation, designing challenging tasks for multimodal encoder models. We show\nthat FuseLIP outperforms other approaches in multimodal embedding tasks such as\nVQA and text-guided image transformation retrieval, while being comparable to\nbaselines on unimodal tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03096.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6310a6bb0a43f97f6c5567d3",
      "avatarUrl": "/avatars/04b07c3a6c811337939c951567cb2bf2.svg",
      "fullname": "Christian Schlarmann",
      "name": "chs20",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.02454",
      "authors": [
        {
          "_id": "683fb5d592425f86f2be5c40",
          "name": "Zhaorui Yang",
          "hidden": false
        },
        {
          "_id": "683fb5d592425f86f2be5c41",
          "name": "Bo Pan",
          "hidden": false
        },
        {
          "_id": "683fb5d592425f86f2be5c42",
          "name": "Han Wang",
          "hidden": false
        },
        {
          "_id": "683fb5d592425f86f2be5c43",
          "name": "Yiyao Wang",
          "hidden": false
        },
        {
          "_id": "683fb5d592425f86f2be5c44",
          "name": "Xingyu Liu",
          "hidden": false
        },
        {
          "_id": "683fb5d592425f86f2be5c45",
          "name": "Minfeng Zhu",
          "hidden": false
        },
        {
          "_id": "683fb5d592425f86f2be5c46",
          "name": "Bo Zhang",
          "hidden": false
        },
        {
          "_id": "683fb5d592425f86f2be5c47",
          "name": "Wei Chen",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64a568f5764b1dce366f9fd2/Jup3eQX_IL2nKhCQVaSWS.mp4"
      ],
      "publishedAt": "2025-06-03T05:18:19.000Z",
      "submittedOnDailyAt": "2025-06-04T01:28:03.959Z",
      "title": "Multimodal Deep Learning : Génération de Rapports Faisant Référence à la Fois au Texte et aux Graphiques\nCommencer par un Cadre Efficace à Partir de Zéro",
      "submittedOnDailyBy": {
        "_id": "64a568f5764b1dce366f9fd2",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a568f5764b1dce366f9fd2/9THW2AJhEVmzltEAm9mMY.jpeg",
        "isPro": false,
        "fullname": "Zhaorui Yang",
        "user": "zhaoruiyang",
        "type": "user"
      },
      "summary": "La visualisation joue un rôle important dans la transmission efficace de concepts et d'informations. Avec le développement récent des raisons et des mots-clés de recherche, les modèles de langage à grande échelle (LLMs) peuvent effectuer des recherches profondes et générer des rapports détaillés. Cependant, les cadres de recherche profonde actuellement se concentrent principalement sur la génération de texte, et l'automatisation de la génération de textes croisés et de visualisations n'a pas encore été étudiée. Cette nouvelle tâche pose des problèmes cruciaux dans le design de visualisations informatives et l'intégration efficace de rapports de texte. Pour résoudre ces problèmes, nous proposons la formalisation de l'explication de visualisation (FDV). La FDV est une représentation de texte structurée qui permet aux LLMs d'apprendre et de générer des visualisations de haute qualité diverses. En se basant sur cette représentation, nous divisons le cadre d'agent Multimodal DeepResearcher en quatre étapes : (1) recherche, (2) texturisation de rapports de cas, (3) planification, et (4) génération de rapports de version modèle. Pour évaluer les rapports de version modèle générés, nous avons développé MultimodalReportBench, qui comprend 100 thèmes divers et utilise 5 métriques professionnelles. Les expériences d'extension de modèles et de méthodes d'évaluation démontrent l'efficacité de Multimodal DeepResearcher. En particulier, en utilisant le même modèle Claude 3.7 Sonnet, Multimodal DeepResearcher a atteint un rendement général de 82%, surpassant significativement les méthodes de référence.",
      "upvotes": 2,
      "discussionId": "683fb5d792425f86f2be5c78",
      "projectPage": "https://rickyang1114.github.io/multimodal-deepresearcher/",
      "ai_summary": "A new framework, Multimodal DeepResearcher, enables Large Language Models to generate high-quality multimodal reports combining text and diverse visualizations through structured textual representations.",
      "ai_keywords": [
        "Formal Description of Visualization",
        "FDV",
        "Multimodal DeepResearcher",
        "researching",
        "exemplar report textualization",
        "planning",
        "multimodal report generation",
        "MultimodalReportBench",
        "multimodal reports"
      ]
    },
    "publishedAt": "2025-06-03T01:18:19.000Z",
    "title": "Multimodal DeepResearcher: Generating Text-Chart Interleaved Reports\n  From Scratch with Agentic Framework",
    "summary": "Visualizations play a crucial part in effective communication of concepts and\ninformation. Recent advances in reasoning and retrieval augmented generation\nhave enabled Large Language Models (LLMs) to perform deep research and generate\ncomprehensive reports. Despite its progress, existing deep research frameworks\nprimarily focus on generating text-only content, leaving the automated\ngeneration of interleaved texts and visualizations underexplored. This novel\ntask poses key challenges in designing informative visualizations and\neffectively integrating them with text reports. To address these challenges, we\npropose Formal Description of Visualization (FDV), a structured textual\nrepresentation of charts that enables LLMs to learn from and generate diverse,\nhigh-quality visualizations. Building on this representation, we introduce\nMultimodal DeepResearcher, an agentic framework that decomposes the task into\nfour stages: (1) researching, (2) exemplar report textualization, (3) planning,\nand (4) multimodal report generation. For the evaluation of generated\nmultimodal reports, we develop MultimodalReportBench, which contains 100\ndiverse topics served as inputs along with 5 dedicated metrics. Extensive\nexperiments across models and evaluation methods demonstrate the effectiveness\nof Multimodal DeepResearcher. Notably, utilizing the same Claude 3.7 Sonnet\nmodel, Multimodal DeepResearcher achieves an 82\\% overall win rate over the\nbaseline method.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64a568f5764b1dce366f9fd2/Jup3eQX_IL2nKhCQVaSWS.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.02454.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64a568f5764b1dce366f9fd2",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a568f5764b1dce366f9fd2/9THW2AJhEVmzltEAm9mMY.jpeg",
      "fullname": "Zhaorui Yang",
      "name": "zhaoruiyang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.02338",
      "authors": [
        {
          "_id": "683fbc730ffa93c1611d513b",
          "user": {
            "_id": "64c8f4cec547ed5243ebd0a8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c8f4cec547ed5243ebd0a8/MiOH5YbMg8Gh9KYlQsLmX.jpeg",
            "isPro": false,
            "fullname": "Hyungjoo Chae",
            "user": "hyungjoochae",
            "type": "user"
          },
          "name": "Hyungjoo Chae",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:55:14.169Z",
          "hidden": false
        },
        {
          "_id": "683fbc730ffa93c1611d513c",
          "name": "Dongjin Kang",
          "hidden": false
        },
        {
          "_id": "683fbc730ffa93c1611d513d",
          "name": "Jihyuk Kim",
          "hidden": false
        },
        {
          "_id": "683fbc730ffa93c1611d513e",
          "name": "Beong-woo Kwak",
          "hidden": false
        },
        {
          "_id": "683fbc730ffa93c1611d513f",
          "name": "Sunghyun Park",
          "hidden": false
        },
        {
          "_id": "683fbc730ffa93c1611d5140",
          "name": "Haeju Park",
          "hidden": false
        },
        {
          "_id": "683fbc730ffa93c1611d5141",
          "name": "Jinyoung Yeo",
          "hidden": false
        },
        {
          "_id": "683fbc730ffa93c1611d5142",
          "name": "Moontae Lee",
          "hidden": false
        },
        {
          "_id": "683fbc730ffa93c1611d5143",
          "name": "Kyungjae Lee",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T00:29:15.000Z",
      "submittedOnDailyAt": "2025-06-04T01:55:22.453Z",
      "title": "Une des inconvénients du modèle de raisonnement ouvert : ensemble de données de LLM à courte contexte pour éviter un départ profond utilisant la RL.",
      "submittedOnDailyBy": {
        "_id": "64c8f4cec547ed5243ebd0a8",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c8f4cec547ed5243ebd0a8/MiOH5YbMg8Gh9KYlQsLmX.jpeg",
        "isPro": false,
        "fullname": "Hyungjoo Chae",
        "user": "hyungjoochae",
        "type": "user"
      },
      "summary": "La publication de R1 a permis l'utilisation de modèles logiques de grande échelle (LRM) et les chercheurs utilisent généralement l'inférence longue de la Chaîne de Pensée (CoT) de R1 pour entraîner de nouveaux LRM. Les études précédentes ont démontré que l'entraînement direct peut reproduire les capacités des LRM. Cependant, il a été révélé que la dépendance actuelle du modèle (par exemple, R1) agit comme une limitation importante pour le développement. En tant que première étape pour le développement indépendant des LRM, cet article revoit la possibilité de construire un ensemble de données de longue CoT adaptée à l'échelle de l'inférence d'un LLM non entraîné. Pour cela, nous proposons le Conjunt de Collection Long CoT qui inclut 100K raisons de CoT. Dans cet ensemble, nous introduisons une nouvelle stratégie logique d'o1 dans un LLM de courte CoT, ce qui permet de faire plus longues les prédictions et de les contrôler de manière plus efficace, ainsi que de gérer mieux les problèmes de pensée excessive. L'analyse large démontre que l'ensemble de données peut atteindre la qualité de R1 ou quelque chose de moins. De plus, les expériences montrent que l'entraînement sur l'ensemble de données fournit un avantage de 2 à 3 fois dans l'amélioration des capacités logiques en RLVR.",
      "upvotes": 2,
      "discussionId": "683fbc760ffa93c1611d51cc",
      "ai_summary": "The Long CoT Collection dataset, generated by short CoT LLMs, enhances general reasoning skills and provides a strong foundation for reinforcement learning, achieving quality comparable to R1.",
      "ai_keywords": [
        "long chain-of-thought",
        "CoT inferences",
        "LRMs",
        "direct distillation",
        "inference-time scaling",
        "CoT rationales",
        "short CoT LLMs",
        "reasoning strategies",
        "thought budget",
        "overthinking",
        "reinforcement learning",
        "RLVR"
      ]
    },
    "publishedAt": "2025-06-02T20:29:15.000Z",
    "title": "One Missing Piece for Open-Source Reasoning Models: A Dataset to\n  Mitigate Cold-Starting Short CoT LLMs in RL",
    "summary": "With the release of R1, a publicly available large reasoning model (LRM),\nresearchers commonly train new LRMs by training language models on R1's long\nchain-of-thought (CoT) inferences. While prior works show that LRMs'\ncapabilities can be reproduced through direct distillation, the continued\nreliance on the existing models (e.g., R1) remains a critical limitation in\nadvancing the field. As a first step toward independent LRM development, this\npaper explores the possibility of constructing a long CoT dataset with LLMs\nthat are not trained for inference-time scaling. To this end, we present the\nLong CoT Collection, a dataset of 100K CoT rationales annotated using existing\nshort CoT LLMs. We develop a pipeline that induces o1's novel reasoning\nstrategies into short CoT LLMs, enabling them to think longer and introducing\ncontrollability over the thought budget to better manage the overthinking\nproblem. Our extensive analyses validate that our dataset achieves quality\ncomparable to--or slightly below--R1. Furthermore, our experiments demonstrate\nthat training on our dataset not only strengthens general reasoning skills, but\nalso provides a strong foundation for reinforcement learning--models\ninitialized on our data achieve 2-3x larger gains with RLVR.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.02338.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64c8f4cec547ed5243ebd0a8",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c8f4cec547ed5243ebd0a8/MiOH5YbMg8Gh9KYlQsLmX.jpeg",
      "fullname": "Hyungjoo Chae",
      "name": "hyungjoochae",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.00413",
      "authors": [
        {
          "_id": "683e703c33ac56778c2e51cd",
          "user": {
            "_id": "630139f1f6bea7dd15bdaf4e",
            "avatarUrl": "/avatars/263536f6160f8c522d2a76ca2c4e4cc0.svg",
            "isPro": false,
            "fullname": "Daniel Israel",
            "user": "danielmisrael",
            "type": "user"
          },
          "name": "Daniel Israel",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:45:22.988Z",
          "hidden": false
        },
        {
          "_id": "683e703c33ac56778c2e51ce",
          "name": "Guy Van den Broeck",
          "hidden": false
        },
        {
          "_id": "683e703c33ac56778c2e51cf",
          "name": "Aditya Grover",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-31T06:10:10.000Z",
      "submittedOnDailyAt": "2025-06-04T03:04:01.663Z",
      "title": "Utilisant le Décodage Parallèle Adaptatif pour Accélérer les Modèles de Diffusion des LLMs.",
      "submittedOnDailyBy": {
        "_id": "630139f1f6bea7dd15bdaf4e",
        "avatarUrl": "/avatars/263536f6160f8c522d2a76ca2c4e4cc0.svg",
        "isPro": false,
        "fullname": "Daniel Israel",
        "user": "danielmisrael",
        "type": "user"
      },
      "summary": "La vitesse de génération des LLMs est marquée par des lignes rouges en raison de la décodification automatique, où chaque token est prédit séquentiellement en lignes. En revanche, les modèles de diffusion et de langage (dLLMs) ont montré théoriquement que la génération en parallèle de tokens est possible, mais en pratique, il a été difficile d'atteindre la vitesse des modèles automatiques de rétroaction sans perdre significativement de qualité. En conséquence, nous présentons un nouveau méthode appelée Décodification Parallèle Automatique Adaptative (APD), qui ajuste dynamiquement la quantité de tokens extraits en parallèle. Pour cela, nous définissons le produit du marge de probabilité du dLLM et la probabilité d'un ensemble de petits modèles automatiques de rétroaction. Cela inverse l'état standard de la décodification de prédiction et cherche à décoder depuis les grands modèles automatiques de rétroaction jusqu'aux petits modèles. De plus, pour améliorer l'APD, nous établissons des limites dans la cache de KV et le taille des inputs masqués. Cette méthodologie fournit trois paramètres d'ajustement flexibles pour s'adapter à la fois aux transformateurs et à la qualité. L'APD minimise la perte de qualité dans les benchmarks de trajets et fournit un transformateur clairement de haute qualité.",
      "upvotes": 2,
      "discussionId": "683e703d33ac56778c2e51fe",
      "ai_summary": "Adaptive parallel decoding (APD) enhances the throughput of diffusion large language models (dLLMs) by dynamically adjusting parallel token generation without significantly diminishing quality.",
      "ai_keywords": [
        "autoregressive decoding",
        "diffusion large language models",
        "dLLMs",
        "adaptive parallel decoding",
        "APD",
        "marginal probabilities",
        "joint probability",
        "speculative decoding",
        "KV caching",
        "masked input"
      ]
    },
    "publishedAt": "2025-05-31T02:10:10.000Z",
    "title": "Accelerating Diffusion LLMs via Adaptive Parallel Decoding",
    "summary": "The generation speed of LLMs are bottlenecked by autoregressive decoding,\nwhere tokens are predicted sequentially one by one. Alternatively, diffusion\nlarge language models (dLLMs) theoretically allow for parallel token\ngeneration, but in practice struggle to achieve the speed of autoregressive\nmodels without significantly sacrificing quality. We therefore introduce\nadaptive parallel decoding (APD), a novel method that dynamically adjusts the\nnumber of tokens sampled in parallel. We achieve this by defining a\nmultiplicative mixture between the dLLM marginal probabilities and the joint\nprobability of sequences under a small auxiliary autoregressive model. This\ninverts the standard setup of speculative decoding, where the goal is to sample\nfrom a large autoregressive verifier by drafting from a smaller model. We\nfurther optimize APD by enabling KV caching and limiting the size of the masked\ninput. Altogether, our method puts forward three tunable parameters to flexibly\ntradeoff throughput and quality. We show that APD provides markedly higher\nthroughput with minimal quality degradations on downstream benchmarks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.00413.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "630139f1f6bea7dd15bdaf4e",
      "avatarUrl": "/avatars/263536f6160f8c522d2a76ca2c4e4cc0.svg",
      "fullname": "Daniel Israel",
      "name": "danielmisrael",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.16994",
      "authors": [
        {
          "_id": "683fb5d3a09aefea70733fa3",
          "user": {
            "_id": "64e14c5b12a5504dda70e60d",
            "avatarUrl": "/avatars/944b486bb037364ef7d9d2c826526708.svg",
            "isPro": false,
            "fullname": "Runyang",
            "user": "dd101bb",
            "type": "user"
          },
          "name": "Runyang You",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-06-04T07:34:53.226Z",
          "hidden": false
        },
        {
          "_id": "683fb5d3a09aefea70733fa4",
          "user": {
            "_id": "674038313ccfb67446ae2b35",
            "avatarUrl": "/avatars/8a3c0fdf971363988731f9eb8b13658c.svg",
            "isPro": false,
            "fullname": "tensorslow",
            "user": "tensorslow",
            "type": "user"
          },
          "name": "Yongqi Li",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-04T02:56:21.008Z",
          "hidden": false
        },
        {
          "_id": "683fb5d3a09aefea70733fa5",
          "name": "Xinyu Lin",
          "hidden": false
        },
        {
          "_id": "683fb5d3a09aefea70733fa6",
          "user": {
            "_id": "63b6dbc8ccebeadccc888456",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1673396893898-63b6dbc8ccebeadccc888456.jpeg",
            "isPro": false,
            "fullname": "Xin Zhang",
            "user": "izhx",
            "type": "user"
          },
          "name": "Xin Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:55:59.107Z",
          "hidden": false
        },
        {
          "_id": "683fb5d3a09aefea70733fa7",
          "name": "Wenjie Wang",
          "hidden": false
        },
        {
          "_id": "683fb5d3a09aefea70733fa8",
          "name": "Wenjie Li",
          "hidden": false
        },
        {
          "_id": "683fb5d3a09aefea70733fa9",
          "name": "Liqiang Nie",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T17:55:43.000Z",
      "submittedOnDailyAt": "2025-06-04T01:27:54.567Z",
      "title": "R^2ec : Méthode de direction pour modules de recommandation grands avec justification",
      "submittedOnDailyBy": {
        "_id": "63b6dbc8ccebeadccc888456",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1673396893898-63b6dbc8ccebeadccc888456.jpeg",
        "isPro": false,
        "fullname": "Xin Zhang",
        "user": "izhx",
        "type": "user"
      },
      "summary": "Nous recommandons d'étendre le modèle de recommandation comme modèle recommandé pour utiliser un fort modèle de recommandation, et le développement récent de l'inférence des LLMs collabore avec la révision de l'inférence de recommandation. L'étude actuelle est en cours de placer les LLMs comme modules d'inférence externes pour renforcer le processus de recommandation. Cependant, ce design séparé a des coûts de ressources importants et manque d'optimisation commune. Pour résoudre ces problèmes, nous proposons un modèle à grande échelle intégré avec des capacités d'inférence propres, appelé \\name\\.\n\nTout d'abord, nous examinons l'architecture du modèle et l'objectif est de promouvoir l'inférence et la recommandation de manière autonome. Ensuite, nous proposons un cadre d'apprentissage par renforcement appelé RecPO, et nous optimisons \\name\\ pour améliorer à la fois la capacité d'inférence et la de recommandation avec une seule mise à jour de politique. RecPO simule la capacité d'inférence en utilisant tous les étiquettes de recommandation, évitant de dépendre d'étiquetages spécifiques d'inférence. Nous avons effectué des expériences sur trois ensembles de données avec des critères différents et démontré l'effet de \\name\\. L'augmentation relative en Hit@5 est de 68.67% et en NDCG@20 est de 45.21%. Le code est disponible sur https://github.com/YRYangang/RRec.",
      "upvotes": 2,
      "discussionId": "683fb5d5a09aefea70734001",
      "githubRepo": "https://github.com/YRYangang/RRec",
      "ai_summary": "A unified large recommender model with intrinsic reasoning capabilities is proposed, facilitating interleaved reasoning and recommendation using a reinforcement learning framework called RecPO.",
      "ai_keywords": [
        "recommender models",
        "LLMs",
        "intrinsic reasoning",
        "autoregressive process",
        "reinforcement learning",
        "RecPO",
        "fused reward scheme",
        "Hit@5",
        "NDCG@20"
      ]
    },
    "publishedAt": "2025-05-22T13:55:43.000Z",
    "title": "R^2ec: Towards Large Recommender Models with Reasoning",
    "summary": "Large recommender models have extended LLMs as powerful recommenders via\nencoding or item generation, and recent breakthroughs in LLM reasoning\nsynchronously motivate the exploration of reasoning in recommendation. Current\nstudies usually position LLMs as external reasoning modules to yield auxiliary\nthought for augmenting conventional recommendation pipelines. However, such\ndecoupled designs are limited in significant resource cost and suboptimal joint\noptimization. To address these issues, we propose \\name, a unified large\nrecommender model with intrinsic reasoning capabilities. Initially, we\nreconceptualize the model architecture to facilitate interleaved reasoning and\nrecommendation in the autoregressive process. Subsequently, we propose RecPO, a\ncorresponding reinforcement learning framework that optimizes \\name\\ both the\nreasoning and recommendation capabilities simultaneously in a single policy\nupdate; RecPO introduces a fused reward scheme that solely leverages\nrecommendation labels to simulate the reasoning capability, eliminating\ndependency on specialized reasoning annotations. Experiments on three datasets\nwith various baselines verify the effectiveness of \\name, showing relative\nimprovements of 68.67\\% in Hit@5 and 45.21\\% in NDCG@20. Code available at\nhttps://github.com/YRYangang/RRec.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16994.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63b6dbc8ccebeadccc888456",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1673396893898-63b6dbc8ccebeadccc888456.jpeg",
      "fullname": "Xin Zhang",
      "name": "izhx",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 19
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.03144",
      "authors": [
        {
          "_id": "684015f08bd5bff99191dff4",
          "user": {
            "_id": "644b71ddb2e7823a76abcf91",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644b71ddb2e7823a76abcf91/JPF7Eqeq2jx8i79nQ962K.jpeg",
            "isPro": false,
            "fullname": "zhou wei",
            "user": "WeiChow",
            "type": "user"
          },
          "name": "Wei Chow",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T09:56:45.423Z",
          "hidden": false
        },
        {
          "_id": "684015f08bd5bff99191dff5",
          "name": "Yuan Gao",
          "hidden": false
        },
        {
          "_id": "684015f08bd5bff99191dff6",
          "name": "Linfeng Li",
          "hidden": false
        },
        {
          "_id": "684015f08bd5bff99191dff7",
          "name": "Xian Wang",
          "hidden": false
        },
        {
          "_id": "684015f08bd5bff99191dff8",
          "name": "Qi Xu",
          "hidden": false
        },
        {
          "_id": "684015f08bd5bff99191dff9",
          "name": "Hang Song",
          "hidden": false
        },
        {
          "_id": "684015f08bd5bff99191dffa",
          "name": "Lingdong Kong",
          "hidden": false
        },
        {
          "_id": "684015f08bd5bff99191dffb",
          "name": "Ran Zhou",
          "hidden": false
        },
        {
          "_id": "684015f08bd5bff99191dffc",
          "name": "Yi Zeng",
          "hidden": false
        },
        {
          "_id": "684015f08bd5bff99191dffd",
          "name": "Yidong Cai",
          "hidden": false
        },
        {
          "_id": "684015f08bd5bff99191dffe",
          "name": "Botian Jiang",
          "hidden": false
        },
        {
          "_id": "684015f08bd5bff99191dfff",
          "name": "Shilin Xu",
          "hidden": false
        },
        {
          "_id": "684015f08bd5bff99191e000",
          "name": "Jiajun Zhang",
          "hidden": false
        },
        {
          "_id": "684015f08bd5bff99191e001",
          "name": "Minghui Qiu",
          "hidden": false
        },
        {
          "_id": "684015f08bd5bff99191e002",
          "name": "Xiangtai Li",
          "hidden": false
        },
        {
          "_id": "684015f08bd5bff99191e003",
          "name": "Tianshu Yang",
          "hidden": false
        },
        {
          "_id": "684015f08bd5bff99191e004",
          "name": "Siliang Tang",
          "hidden": false
        },
        {
          "_id": "684015f08bd5bff99191e005",
          "name": "Juncheng Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T17:59:14.000Z",
      "submittedOnDailyAt": "2025-06-04T08:16:40.362Z",
      "title": "Point de valeur : Exploration de contextes multilingues croisés pour consultations multiconditionnelles",
      "submittedOnDailyBy": {
        "_id": "644b71ddb2e7823a76abcf91",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644b71ddb2e7823a76abcf91/JPF7Eqeq2jx8i79nQ962K.jpeg",
        "isPro": false,
        "fullname": "zhou wei",
        "user": "WeiChow",
        "type": "user"
      },
      "summary": "El Semántico-Retailer esté très important pour les applications modernes, mais il n'a pas encore été étudié son impact actuel. Les jeux de données existants sont limités à un langage, une image ou un seul retailer, et ne peuvent pas exprimer complètement la capacité de représentation visuelle des images lorsqu'elles sont remplacées par des captions, car on considère la continuité du rendement. Cependant, les cas pratiques de retailer incluent des requêtes qui combinent plusieurs images. Dans ce travail, nous présentons MERIT (Semántico-Retailer Multilingual et Multi-Condition), le premier jeu de données multilingue qui inclut 7 catégories de produits dans 5 langues, avec 320 000 requêtes et 135 000 produits. Les expériences réalisées sur MERIT ont mis en évidence les limitations actuelles des modèles : les éléments de condition spécifiques ne sont pas pris en compte et l'accent est mis sur l'information globale. En réponse, nous proposons un nouveau cadre d'ajustement micro, Coral, qui applique des modèles MLLM préalablement entraînés, incluant la réorganisation de la codification, la préservation d'éléments de condition petits et l'apprentissage contrastif pour extraire des informations globales détaillées. Les expériences ont montré un amélioration du rendement de 45,9% sur MERIT et ont démontré une forte capacité de généralisation sur 8 benchmarks existants. Cette contribution établit une base pour l'investigation future en Semántico-Retailer Multilingual et Multi-Condition, en reconnaissant les limitations actuelles et en proposant un nouveau cadre d'ajustement micro.",
      "upvotes": 1,
      "discussionId": "684015f38bd5bff99191e158"
    },
    "publishedAt": "2025-06-03T13:59:14.000Z",
    "title": "MERIT: Multilingual Semantic Retrieval with Interleaved Multi-Condition\n  Query",
    "summary": "Semantic retrieval is crucial for modern applications yet remains\nunderexplored in current research. Existing datasets are limited to single\nlanguages, single images, or singular retrieval conditions, often failing to\nfully exploit the expressive capacity of visual information as evidenced by\nmaintained performance when images are replaced with captions. However,\npractical retrieval scenarios frequently involve interleaved multi-condition\nqueries with multiple images. Hence, this paper introduces MERIT, the first\nmultilingual dataset for interleaved multi-condition semantic retrieval,\ncomprising 320,000 queries with 135,000 products in 5 languages, covering 7\ndistinct product categories. Extensive experiments on MERIT identify existing\nmodels's limitation: focusing solely on global semantic information while\nneglecting specific conditional elements in queries. Consequently, we propose\nCoral, a novel fine-tuning framework that adapts pre-trained MLLMs by\nintegrating embedding reconstruction to preserve fine-grained conditional\nelements and contrastive learning to extract comprehensive global semantics.\nExperiments demonstrate that Coral achieves a 45.9% performance improvement\nover conventional approaches on MERIT, with strong generalization capabilities\nvalidated across 8 established retrieval benchmarks. Collectively, our\ncontributions - a novel dataset, identification of critical limitations in\nexisting approaches, and an innovative fine-tuning framework - establish a\nfoundation for future research in interleaved multi-condition semantic\nretrieval.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03144.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "644b71ddb2e7823a76abcf91",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644b71ddb2e7823a76abcf91/JPF7Eqeq2jx8i79nQ962K.jpeg",
      "fullname": "zhou wei",
      "name": "WeiChow",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.02510",
      "authors": [
        {
          "_id": "683faa31f0564d1fb4b9ffc6",
          "user": {
            "_id": "642656cbad1e3b0e6e91b752",
            "avatarUrl": "/avatars/3bf0ee15fd528e09b2b889f5cce3cbd0.svg",
            "isPro": false,
            "fullname": "Jie Zhu",
            "user": "amazingj",
            "type": "user"
          },
          "name": "Jie Zhu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:57:01.885Z",
          "hidden": false
        },
        {
          "_id": "683faa31f0564d1fb4b9ffc7",
          "name": "Junhui Li",
          "hidden": false
        },
        {
          "_id": "683faa31f0564d1fb4b9ffc8",
          "name": "Yalong Wen",
          "hidden": false
        },
        {
          "_id": "683faa31f0564d1fb4b9ffc9",
          "name": "Xiandong Li",
          "hidden": false
        },
        {
          "_id": "683faa31f0564d1fb4b9ffca",
          "name": "Lifan Guo",
          "hidden": false
        },
        {
          "_id": "683faa31f0564d1fb4b9ffcb",
          "name": "Feng Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T06:41:09.000Z",
      "submittedOnDailyAt": "2025-06-04T00:38:17.377Z",
      "title": "M^3FinMeeting : Dataset de Compréhension et Évaluation de Données de Réunion Financière Multilingue, Multi-industriel et Multi-fonctionnel",
      "submittedOnDailyBy": {
        "_id": "642656cbad1e3b0e6e91b752",
        "avatarUrl": "/avatars/3bf0ee15fd528e09b2b889f5cce3cbd0.svg",
        "isPro": false,
        "fullname": "Jie Zhu",
        "user": "amazingj",
        "type": "user"
      },
      "summary": "Récemment, un nouveau cadre de référence a été développé pour évaluer le rendement des grands modèles de langage (LLMs) dans le secteur financier. Cependant, les cadres de référence financiers actuels se fondent principalement sur des articles de presse, des rapports de bénéfices ou des présentations, et ne sont pas suffisamment adaptés pour capturer facilement les tendances des réunions financières. Pour corriger ce problème, nous proposons un nouveau cadre de référence pour comprendre les réunions financières. Ce cadre de référence inclut des ensembles de données multilingues, multiindustriels et multi-tâches. Tout d'abord, M^3FinMeeting supporte l'anglais, le chinois et le japonais, renforçant la compréhension des discussions financières dans différents contextes linguistiques. Ensuite, selon la classification globale des industries (GICS), le cadre de référence inclut diverses industries, couvrant une large gamme d'activités financières. Enfin, M^3FinMeeting extrait des paires de questions et réponses, y compris trois tâches liées aux réponses, ce qui favorise une compréhension plus réaliste et détaillée. Les résultats des expériences avec 7 modèles LLMs populaires montrent que bien que les modèles de contexte longueur soient toujours à améliorer, M^3FinMeeting a démontré son efficacité comme un cadre de référence nécessaire pour évaluer la capacité des modèles LLMs à comprendre les réunions financières.",
      "upvotes": 1,
      "discussionId": "683faa32f0564d1fb4ba0005",
      "projectPage": "https://github.com/aliyun/qwen-dianjin",
      "githubRepo": "https://github.com/aliyun/qwen-dianjin",
      "ai_summary": "A new multilingual, multi-sector, and multi-task benchmark, M³FinMeeting, evaluates large language models' performance in understanding financial meetings across different languages and industries.",
      "ai_keywords": [
        "large language models",
        "multilingual",
        "multi-sector",
        "multi-task",
        "benchmark",
        "financial meeting understanding",
        "Global Industry Classification Standard (GICS)",
        "summarization",
        "question-answer pair extraction",
        "question answering"
      ]
    },
    "publishedAt": "2025-06-03T02:41:09.000Z",
    "title": "M^3FinMeeting: A Multilingual, Multi-Sector, and Multi-Task Financial\n  Meeting Understanding Evaluation Dataset",
    "summary": "Recent breakthroughs in large language models (LLMs) have led to the\ndevelopment of new benchmarks for evaluating their performance in the financial\ndomain. However, current financial benchmarks often rely on news articles,\nearnings reports, or announcements, making it challenging to capture the\nreal-world dynamics of financial meetings. To address this gap, we propose a\nnovel benchmark called M^3FinMeeting, which is a multilingual,\nmulti-sector, and multi-task dataset designed for financial meeting\nunderstanding. First, M^3FinMeeting supports English, Chinese, and\nJapanese, enhancing comprehension of financial discussions in diverse\nlinguistic contexts. Second, it encompasses various industry sectors defined by\nthe Global Industry Classification Standard (GICS), ensuring that the benchmark\nspans a broad range of financial activities. Finally,\nM^3FinMeeting includes three tasks: summarization, question-answer\n(QA) pair extraction, and question answering, facilitating a more realistic and\ncomprehensive evaluation of understanding. Experimental results with seven\npopular LLMs reveal that even the most advanced long-context models have\nsignificant room for improvement, demonstrating the effectiveness of\nM^3FinMeeting as a benchmark for assessing LLMs' financial meeting\ncomprehension skills.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.02510.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642656cbad1e3b0e6e91b752",
      "avatarUrl": "/avatars/3bf0ee15fd528e09b2b889f5cce3cbd0.svg",
      "fullname": "Jie Zhu",
      "name": "amazingj",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.24362",
      "authors": [
        {
          "_id": "68401045dd25841d998788cc",
          "user": {
            "_id": "61fbe8d2c5e6410373a76b2a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61fbe8d2c5e6410373a76b2a/m4yM6m2uagJxMselzEZfo.jpeg",
            "isPro": false,
            "fullname": "Anum Afzal",
            "user": "anumafzal94",
            "type": "user"
          },
          "name": "Anum Afzal",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-06-04T09:25:18.174Z",
          "hidden": false
        },
        {
          "_id": "68401045dd25841d998788cd",
          "name": "Florian Matthes",
          "hidden": false
        },
        {
          "_id": "68401045dd25841d998788ce",
          "user": {
            "_id": "6493393f357b252af72196c5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6493393f357b252af72196c5/EWSy18XRcMRa_4XMM3Fu-.jpeg",
            "isPro": false,
            "fullname": "Gal Chechik",
            "user": "galchechik",
            "type": "user"
          },
          "name": "Gal Chechik",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-04T09:22:13.900Z",
          "hidden": false
        },
        {
          "_id": "68401045dd25841d998788cf",
          "user": {
            "_id": "66810e5877ed01ba880a4b40",
            "avatarUrl": "/avatars/3068f4b16f03a51772e652d76b37f9c3.svg",
            "isPro": false,
            "fullname": "Yftah Ziser",
            "user": "yziser",
            "type": "user"
          },
          "name": "Yftah Ziser",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-04T09:22:13.900Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-30T08:54:28.000Z",
      "submittedOnDailyAt": "2025-06-04T07:54:00.025Z",
      "title": "Conaissant que : L'expression d'un modèle de langage génératif (LLM) comprend des informations pertinentes qui sont liées aux pensées et aux reconnaissances qui sont accomplies avant que l'expression soit complète.",
      "submittedOnDailyBy": {
        "_id": "61fbe8d2c5e6410373a76b2a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61fbe8d2c5e6410373a76b2a/m4yM6m2uagJxMselzEZfo.jpeg",
        "isPro": false,
        "fullname": "Anum Afzal",
        "user": "anumafzal94",
        "type": "user"
      },
      "summary": "Nous étudions si il est possible de prédire avant que le processus de dimension nulle de la Chain-of-Thought (CoT) soit terminé. Nous avons constaté que un classificateur d'exploration basé sur la représentation du modèle de langage profond (LLM) montre un excellent rendement avant que les tokens ne soient générés, ce qui indique que déjà dans les premières étapes de la représentation, des informations importantes sur le processus de raisonnement sont incluses. D'autre part, un fort bassin de comparaison basé sur BERT dépend uniquement de la création de tokens, montrant un rendement plus bas basé sur des codes de langage simple, et ne peut pas améliorer la classification même avec des étapes ultérieures de raisonnement. Surprenant, il n'est pas possible d'améliorer la classification même lorsque des étapes ultérieures de raisonnement sont utilisées. Si plus de contexte n'est pas fourni, les représentations initiales sont similaires aux suivantes, et le modèle de langage profond montre qu'il codifie des informations clés d'origine. Cela signifie que, même si le processus de raisonnement se termine rapidement, il n'y a pas de perte. Pour vérifier cela, nous avons effectué des expériences d'interruption initiale, montrant que le rendement s'améliore tandis que le processus de raisonnement de la CoT se termine, mais que des erreurs restent par rapport à un processus complet de raisonnement. Cependant, l'utilisation d'approches comme l'entraînement de sous-ensembles pour réduire la CoT ou l'apprentissage par renforcement nous permet de reconnaître si l'interruption initiale est efficace. Ce que nous avons découvert est que ces approximations peuvent être soutenues et aident à optimiser l'efficacité de la CoT tout en maintenant ses avantages.",
      "upvotes": 1,
      "discussionId": "68401045dd25841d998788f9"
    },
    "publishedAt": "2025-05-30T04:54:28.000Z",
    "title": "Knowing Before Saying: LLM Representations Encode Information About\n  Chain-of-Thought Success Before Completion",
    "summary": "We investigate whether the success of a zero-shot Chain-of-Thought (CoT)\nprocess can be predicted before completion. We discover that a probing\nclassifier, based on LLM representations, performs well even before a\nsingle token is generated, suggesting that crucial information about the\nreasoning process is already present in the initial steps representations. In\ncontrast, a strong BERT-based baseline, which relies solely on the generated\ntokens, performs worse, likely because it depends on shallow linguistic cues\nrather than deeper reasoning dynamics. Surprisingly, using later reasoning\nsteps does not always improve classification. When additional context is\nunhelpful, earlier representations resemble later ones more, suggesting LLMs\nencode key information early. This implies reasoning can often stop early\nwithout loss. To test this, we conduct early stopping experiments, showing that\ntruncating CoT reasoning still improves performance over not using CoT at all,\nthough a gap remains compared to full reasoning. However, approaches like\nsupervised learning or reinforcement learning designed to shorten CoT chains\ncould leverage our classifier's guidance to identify when early stopping is\neffective. Our findings provide insights that may support such methods, helping\nto optimize CoT's efficiency while preserving its benefits.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24362.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "61fbe8d2c5e6410373a76b2a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61fbe8d2c5e6410373a76b2a/m4yM6m2uagJxMselzEZfo.jpeg",
      "fullname": "Anum Afzal",
      "name": "anumafzal94",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.24273",
      "authors": [
        {
          "_id": "683f21ed6ba11d78e3e383f6",
          "user": {
            "_id": "65f7c56fc6356b5cc5ab8245",
            "avatarUrl": "/avatars/ff8d3f3526e915f269274cc0dc5ca8ef.svg",
            "isPro": false,
            "fullname": "James Cai",
            "user": "jamescai20",
            "type": "user"
          },
          "name": "Hongyi James Cai",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T09:01:32.571Z",
          "hidden": false
        },
        {
          "_id": "683f21ed6ba11d78e3e383f7",
          "name": "Junlin Wang",
          "hidden": false
        },
        {
          "_id": "683f21ed6ba11d78e3e383f8",
          "user": {
            "_id": "65d66cb2b06abf924b07ff76",
            "avatarUrl": "/avatars/de94e2fe07040b7dc3053bcaafa64ffe.svg",
            "isPro": false,
            "fullname": "Xiaoyin Chen",
            "user": "chenyn66",
            "type": "user"
          },
          "name": "Xiaoyin Chen",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-03T16:25:17.851Z",
          "hidden": false
        },
        {
          "_id": "683f21ed6ba11d78e3e383f9",
          "name": "Bhuwan Dhingra",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-30T06:49:00.000Z",
      "submittedOnDailyAt": "2025-06-04T08:05:26.080Z",
      "title": "Quelle est la quantité appropriée de risque ? Recherche sur l'interaction entre le SFT et le RL pour améliorer la théorie des LLM.",
      "submittedOnDailyBy": {
        "_id": "65f7c56fc6356b5cc5ab8245",
        "avatarUrl": "/avatars/ff8d3f3526e915f269274cc0dc5ca8ef.svg",
        "isPro": false,
        "fullname": "James Cai",
        "user": "jamescai20",
        "type": "user"
      },
      "summary": "Le développement récent des modèles de langage grands (LLMs) a notablement amélioré leur capacité à résoudre des problèmes mathématiques et logiques, étant particulièrement efficaces dans la résolution de problèmes où la réponse est clairement déterminée. En particulier, des techniques comme le fine-tuning normal (SFT) et l'apprentissage par refonte (RL) sont utilisées. Selon des études précédentes, l'apprentissage par refonte inclut des stratégies d'exploration, permet l'inférence de chaînes longues de raisonnement (CoT) et apprend naturellement la capacité de retrouver. Cependant, les avantages concrets de la retrouverie, notamment son impact sur l'amélioration de l'inférence et ses applications les plus appropriées, n'ont pas encore été entièrement compris. Dans cette étude, les changements dynamiques entre SFT et RL ont été investigués, et des expériences ont été réalisées sur 8 tâches (Countdown, Sudoku, Arc 1D, Géométrie, Rotation de cubes de couleurs, Fonctions de listes, Ménages de zebra, Référence autonome). Les résultats indiquent que dans le SFT, des séquences courtes de CoT offrent une contribution modérée par rapport à un RL froid, mais que cette contribution diminue lorsque les tâches deviennent plus difficiles. En se basant sur ces observations, un ensemble de données d'entraînement avec un nombre de pas de retrouverie ajusté de manière systématique a été construit, et l'impact de la précision (contenu) et de la structure (fréquence de retrouverie) a été étudié séparément. Les résultats révèlent que : (1) les chaînes longues de CoT comprenant une rétrouverie généralement stimulent un apprentissage plus bon et stable du RL. (2) Les tâches plus difficiles et avec de plus grands espaces de recherche tendent à avoir un nombre plus élevé de pas de retrouverie lors du processus de SFT. De plus, dans des expériences avec des données limitées, il a été observé que l'apprentissage du RL n'est pas significativement affecté par la précision des séquences longues de CoT, et que le RL privilégie la précision de la structure par rapport à la précision du contenu. En résumé, les résultats de cette étude fournissent des conseils pratiques pour le design de stratégies d'entraînement plus appropriés pour la scalabilité efficace de l'inférence dans les LLMs.",
      "upvotes": 1,
      "discussionId": "683f21ed6ba11d78e3e38434",
      "ai_summary": "This study investigates the interplay between supervised fine-tuning and reinforcement learning in large language models, focusing on the role of backtracking in enhancing reasoning capabilities across various tasks.",
      "ai_keywords": [
        "supervised finetuning",
        "reinforcement learning",
        "chain-of-thought",
        "backtracking",
        "countdown",
        "sudoku",
        "arc 1d",
        "geometry",
        "color cube rotation",
        "list functions",
        "zebra puzzles",
        "self reference",
        "synthetic datasets",
        "distilled data"
      ]
    },
    "publishedAt": "2025-05-30T02:49:00.000Z",
    "title": "How Much Backtracking is Enough? Exploring the Interplay of SFT and RL\n  in Enhancing LLM Reasoning",
    "summary": "Recent breakthroughs in large language models (LLMs) have effectively\nimproved their reasoning abilities, particularly on mathematical and logical\nproblems that have verifiable answers, through techniques such as supervised\nfinetuning (SFT) and reinforcement learning (RL). Prior research indicates that\nRL effectively internalizes search strategies, enabling long chain-of-thought\n(CoT) reasoning, with backtracking emerging naturally as a learned capability.\nHowever, the precise benefits of backtracking, specifically, how significantly\nit contributes to reasoning improvements and the optimal extent of its use,\nremain poorly understood. In this work, we systematically investigate the\ndynamics between SFT and RL on eight reasoning tasks: Countdown, Sudoku, Arc\n1D, Geometry, Color Cube Rotation, List Functions, Zebra Puzzles, and Self\nReference. Our findings highlight that short CoT sequences used in SFT as a\nwarm-up do have moderate contribution to RL training, compared with cold-start\nRL; however such contribution diminishes when tasks become increasingly\ndifficult. Motivated by this observation, we construct synthetic datasets\nvarying systematically in the number of backtracking steps and conduct\ncontrolled experiments to isolate the influence of either the correctness\n(content) or the structure (i.e., backtrack frequency). We find that (1) longer\nCoT with backtracks generally induce better and more stable RL training, (2)\nmore challenging problems with larger search space tend to need higher numbers\nof backtracks during the SFT stage. Additionally, we demonstrate through\nexperiments on distilled data that RL training is largely unaffected by the\ncorrectness of long CoT sequences, suggesting that RL prioritizes structural\npatterns over content correctness. Collectively, our results offer practical\ninsights into designing optimal training strategies to effectively scale\nreasoning in LLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24273.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65f7c56fc6356b5cc5ab8245",
      "avatarUrl": "/avatars/ff8d3f3526e915f269274cc0dc5ca8ef.svg",
      "fullname": "James Cai",
      "name": "jamescai20",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.18079",
      "authors": [
        {
          "_id": "68354eec0830dfc6782ba1c4",
          "name": "Xiaoyi Zhang",
          "hidden": false
        },
        {
          "_id": "68354eec0830dfc6782ba1c5",
          "name": "Zhaoyang Jia",
          "hidden": false
        },
        {
          "_id": "68354eec0830dfc6782ba1c6",
          "name": "Zongyu Guo",
          "hidden": false
        },
        {
          "_id": "68354eec0830dfc6782ba1c7",
          "name": "Jiahao Li",
          "hidden": false
        },
        {
          "_id": "68354eec0830dfc6782ba1c8",
          "name": "Bin Li",
          "hidden": false
        },
        {
          "_id": "68354eec0830dfc6782ba1c9",
          "name": "Houqiang Li",
          "hidden": false
        },
        {
          "_id": "68354eec0830dfc6782ba1ca",
          "name": "Yan Lu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T16:37:36.000Z",
      "submittedOnDailyAt": "2025-06-04T07:20:42.262Z",
      "title": "Deep Video Search : Recherche Agentielle pour des Vidéos Longues Formes y compris l'Utilisation de Outils\n\nComprendre",
      "submittedOnDailyBy": {
        "_id": "6582b79aafc6b50a2cbaa5c8",
        "avatarUrl": "/avatars/65fa21ca144177f232347084b0e057c5.svg",
        "isPro": false,
        "fullname": "Xiaoyi Zhang",
        "user": "xyzhang626",
        "type": "user"
      },
      "summary": "La compréhension de vidéos longs fait face à des défis significatifs en raison de la complexité temporelle et spatiale, ainsi que de la difficulté de traiter des contextes à long terme pour répondre à des questions. Les modèles de langage de grande échelle (LLM), qui ont démontré des avancées significatives dans l'analyse de vidéo et le traitement de contextes à long terme, présentent des limites lorsqu'ils s'agit de traiter des vidéos d'une durée d'une heure avec une forte densité d'information. Pour surmonter ces limites, nous proposons l'agent Deep Video Discovery. Contrairement aux agents vidéo précédents, qui ont été conçus avec un flux de travail rigide et nécessitent des actions manuelles, notre approche met l'autonomie de l'agent au centre. Nous offrons des outils axés sur la recherche dans une base de données de vidéos à plusieurs niveaux de granularité, permettant ainsi que l'agent DVD utilise les capacités avancées d'inférence des LLM pour planifier à partir des observations actuelles, sélectionner stratégiquement des outils, configurer les paramètres appropriés des actions et ajuster de manière itérative son inférence interne en se basant sur les informations recueillies. Par des évaluations détaillées dans le cadre d'un benchmark de compréhension de vidéos à long terme, nous démontrons l'excellence de l'architecture du système. L'agent DVD atteint un performance pionnière sur le jeu de données LVBench difficile, surpassant considérablement les résultats des études précédentes. De plus, nous fournissons des tests de disparition et d'analyse d'outils pour offrir un rétroaction connectée au développement d'agents intelligents appropriés pour des tâches de compréhension de vidéos à long terme. Le code sera publié ultérieurement.",
      "upvotes": 1,
      "discussionId": "68354eed0830dfc6782ba1fe",
      "ai_summary": "The Deep Video Discovery agent uses an autonomous agentic search strategy with large language models to overcome limitations in long-form video understanding, achieving state-of-the-art results on benchmarks like LVBench.",
      "ai_keywords": [
        "Deep Video Discovery agent",
        "agentic search strategy",
        "segmented video clips",
        "multi-granular video database",
        "advanced reasoning capability",
        "LLM",
        "autonomous nature",
        "observation state",
        "search-centric tools",
        "internal reasoning",
        "long video understanding benchmarks",
        "LVBench",
        "ablation studies",
        "tool analyses"
      ]
    },
    "publishedAt": "2025-05-23T12:37:36.000Z",
    "title": "Deep Video Discovery: Agentic Search with Tool Use for Long-form Video\n  Understanding",
    "summary": "Long-form video understanding presents significant challenges due to\nextensive temporal-spatial complexity and the difficulty of question answering\nunder such extended contexts. While Large Language Models (LLMs) have\ndemonstrated considerable advancements in video analysis capabilities and long\ncontext handling, they continue to exhibit limitations when processing\ninformation-dense hour-long videos. To overcome such limitations, we propose\nthe Deep Video Discovery agent to leverage an agentic search strategy over\nsegmented video clips. Different from previous video agents manually designing\na rigid workflow, our approach emphasizes the autonomous nature of agents. By\nproviding a set of search-centric tools on multi-granular video database, our\nDVD agent leverages the advanced reasoning capability of LLM to plan on its\ncurrent observation state, strategically selects tools, formulates appropriate\nparameters for actions, and iteratively refines its internal reasoning in light\nof the gathered information. We perform comprehensive evaluation on multiple\nlong video understanding benchmarks that demonstrates the advantage of the\nentire system design. Our DVD agent achieves SOTA performance, significantly\nsurpassing prior works by a large margin on the challenging LVBench dataset.\nComprehensive ablation studies and in-depth tool analyses are also provided,\nyielding insights to further advance intelligent agents tailored for long-form\nvideo understanding tasks. The code will be released later.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18079.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6582b79aafc6b50a2cbaa5c8",
      "avatarUrl": "/avatars/65fa21ca144177f232347084b0e057c5.svg",
      "fullname": "Xiaoyi Zhang",
      "name": "xyzhang626",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.02138",
      "authors": [
        {
          "_id": "683fe27c4f32bd7bbca087fc",
          "name": "Yarden Bakish",
          "hidden": false
        },
        {
          "_id": "683fe27c4f32bd7bbca087fd",
          "name": "Itamar Zimerman",
          "hidden": false
        },
        {
          "_id": "683fe27c4f32bd7bbca087fe",
          "name": "Hila Chefer",
          "hidden": false
        },
        {
          "_id": "683fe27c4f32bd7bbca087ff",
          "name": "Lior Wolf",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-02T18:07:55.000Z",
      "submittedOnDailyAt": "2025-06-04T04:51:22.070Z",
      "title": "Substituido LRP : La localisation des attributs est un améliorament pour le défaut insuffisant d'explication des transformeurs.",
      "submittedOnDailyBy": {
        "_id": "65376feed325b3f02fb92c69",
        "avatarUrl": "/avatars/e952918cf434d5302e9b1a404eccaf0e.svg",
        "isPro": false,
        "fullname": "Itamar Zimerman",
        "user": "ItamarZ",
        "type": "user"
      },
      "summary": "L'élaboration de outils d'explicabilité efficaces pour les Transformers est une tâche importante dans la recherche d'apprentissage profond. L'une des meilleures façons d'aborder ce domaine est la Régression des Risques de la Propagation des Poids de Réseaux (LRP). La LRP réredistribue les valeurs actives selon des règles prédéfinies et retropropage les scores pertinents vers l'espace d'entrée de la réseau. Cependant, les méthodes basées sur la LRP pour l'explicabilité des Transformers ignorent complètement le composant essentiel de l'architecture Transformer, l'encodage de position (EP). Cela entraîne la perte de conservation et l'importance des caractéristiques structurales et de position. Pour résoudre ces limitations, on remplace l'explicabilité des Transformers par un ensemble de paires de position et de tokens dans l'espace d'entrée. De cette manière, on propose des règles théoriques spécialisées de LRP qui se concentrent sur la propagation de caractéristiques de différents méthodes d'encodage de position, comme Rotary, Learnable et Absolute EP. Les expériences étendues sur des classifieurs fine-tunés et des modèles basés sur 0-shot (par exemple, LLaMA 3) montrent que notre approche dépasse clairement les méthodes les plus avancées dans les deux domaines de travail, visuel et d'explicabilité du NLP. Notre code est disponible pour un usage public.",
      "upvotes": 0,
      "discussionId": "683fe27d4f32bd7bbca08867",
      "githubRepo": "https://github.com/YardenBakish/PE-AWARE-LRP",
      "ai_summary": "A specialized LRP method for Transformer explainability considers positional encoding, improving relevance propagation and outperforming existing methods.",
      "ai_keywords": [
        "Layer-wise Relevance Propagation (LRP)",
        "Transformers",
        "positional encoding (PE)",
        "Rotary",
        "Learnable",
        "Absolute PE",
        "vision",
        "NLP explainability tasks"
      ]
    },
    "publishedAt": "2025-06-02T14:07:55.000Z",
    "title": "Revisiting LRP: Positional Attribution as the Missing Ingredient for\n  Transformer Explainability",
    "summary": "The development of effective explainability tools for Transformers is a\ncrucial pursuit in deep learning research. One of the most promising approaches\nin this domain is Layer-wise Relevance Propagation (LRP), which propagates\nrelevance scores backward through the network to the input space by\nredistributing activation values based on predefined rules. However, existing\nLRP-based methods for Transformer explainability entirely overlook a critical\ncomponent of the Transformer architecture: its positional encoding (PE),\nresulting in violation of the conservation property, and the loss of an\nimportant and unique type of relevance, which is also associated with\nstructural and positional features. To address this limitation, we reformulate\nthe input space for Transformer explainability as a set of position-token\npairs. This allows us to propose specialized theoretically-grounded LRP rules\ndesigned to propagate attributions across various positional encoding methods,\nincluding Rotary, Learnable, and Absolute PE. Extensive experiments with both\nfine-tuned classifiers and zero-shot foundation models, such as LLaMA 3,\ndemonstrate that our method significantly outperforms the state-of-the-art in\nboth vision and NLP explainability tasks. Our code is publicly available.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.02138.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65376feed325b3f02fb92c69",
      "avatarUrl": "/avatars/e952918cf434d5302e9b1a404eccaf0e.svg",
      "fullname": "Itamar Zimerman",
      "name": "ItamarZ",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.01565",
      "authors": [
        {
          "_id": "684009f8a33aeee1125b5765",
          "name": "Li Zhou",
          "hidden": false
        },
        {
          "_id": "684009f8a33aeee1125b5766",
          "name": "Lutong Yu",
          "hidden": false
        },
        {
          "_id": "684009f8a33aeee1125b5767",
          "name": "Dongchu Xie",
          "hidden": false
        },
        {
          "_id": "684009f8a33aeee1125b5768",
          "name": "Shaohuan Cheng",
          "hidden": false
        },
        {
          "_id": "684009f8a33aeee1125b5769",
          "name": "Wenyan Li",
          "hidden": false
        },
        {
          "_id": "684009f8a33aeee1125b576a",
          "name": "Haizhou Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-02T11:43:46.000Z",
      "submittedOnDailyAt": "2025-06-04T07:35:11.966Z",
      "title": "Hanfbarnich : Marqueur d'époque pour la compréhension culturelle et la traduction de réédition multiforme",
      "submittedOnDailyBy": {
        "_id": "619b506f70d03780cbec5806",
        "avatarUrl": "/avatars/ea2b0b8f0a3eb16d53ef40da9981c397.svg",
        "isPro": false,
        "fullname": "wenyan li",
        "user": "lyan62",
        "type": "user"
      },
      "summary": "La culture est un domaine riche et dynamique qui évolue dans les deux directions, territoriale et temporelle. Cependant, les recherches sur la compréhension culturelle basée sur les Modèles de Vision et Langage (VLMs) actuellement mettent principalement l'accent sur la diversité territoriale, mais ignorent l'importance temporelle. Pour corriger cette lacune, nous présentons \"Hanbok Bench\", un nouveau ensemble de données édité par des experts. Le hanbok est un vêtement traditionnel qui a traversé l'ancienne Chine, reflétant sa culture et sa culture temporelle, et continue de maintenir une grande popularité dans la société chinoise contemporaine. Hanbok Bench comprend deux tâches clés : la compréhension visuelle culturelle et le redéveloppement d'images culturelles. La première tâche évalue le reconnaissance de caractéristiques temporelles et culturelles à partir d'une ou plusieurs images, et sélectionne des réponses selon des questions visuelles. La seconde se concentre sur l'adaptation du design traditionnel des vêtements aux contextes modernes, en maintenant les éléments culturels. Notre évaluation montre que les VLMs fermés présentent un rendement relativement bas en compréhension visuelle culturelle, comparés aux non-experts. Les VLMs ouverts présentent également un rendement inférieur aux non-experts. Le redéveloppement des tâches a montré un succès de 42% pour les modèles qui ont obtenu les meilleurs résultats selon des évaluations humaines diverses. Notre benchmark offre une preuve cruciale pour explorer de nouvelles directions dans la compréhension culturelle temporelle et l'adaptation créative, identifiant clairement les grands défis dans ce domaine.",
      "upvotes": 0,
      "discussionId": "684009faa33aeee1125b57bd",
      "githubRepo": "https://github.com/lizhou21/TemporalCulture"
    },
    "publishedAt": "2025-06-02T07:43:46.000Z",
    "title": "Hanfu-Bench: A Multimodal Benchmark on Cross-Temporal Cultural\n  Understanding and Transcreation",
    "summary": "Culture is a rich and dynamic domain that evolves across both geography and\ntime. However, existing studies on cultural understanding with vision-language\nmodels (VLMs) primarily emphasize geographic diversity, often overlooking the\ncritical temporal dimensions. To bridge this gap, we introduce Hanfu-Bench, a\nnovel, expert-curated multimodal dataset. Hanfu, a traditional garment spanning\nancient Chinese dynasties, serves as a representative cultural heritage that\nreflects the profound temporal aspects of Chinese culture while remaining\nhighly popular in Chinese contemporary society. Hanfu-Bench comprises two core\ntasks: cultural visual understanding and cultural image transcreation.The\nformer task examines temporal-cultural feature recognition based on single- or\nmulti-image inputs through multiple-choice visual question answering, while the\nlatter focuses on transforming traditional attire into modern designs through\ncultural element inheritance and modern context adaptation. Our evaluation\nshows that closed VLMs perform comparably to non-experts on visual cutural\nunderstanding but fall short by 10\\% to human experts, while open VLMs lags\nfurther behind non-experts. For the transcreation task, multi-faceted human\nevaluation indicates that the best-performing model achieves a success rate of\nonly 42\\%. Our benchmark provides an essential testbed, revealing significant\nchallenges in this new direction of temporal cultural understanding and\ncreative adaptation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01565.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "619b506f70d03780cbec5806",
      "avatarUrl": "/avatars/ea2b0b8f0a3eb16d53ef40da9981c397.svg",
      "fullname": "wenyan li",
      "name": "lyan62",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  }
]