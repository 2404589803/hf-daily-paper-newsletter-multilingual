[
  {
    "paper": {
      "id": "2505.04620",
      "authors": [
        {
          "_id": "681c6c1817fc8222eff39a1a",
          "user": {
            "_id": "647773a1168cb428e00e9a8f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647773a1168cb428e00e9a8f/NiRR3ScY6Plzjibfwy1hC.jpeg",
            "isPro": false,
            "fullname": "Hao Fei",
            "user": "scofield7419",
            "type": "user"
          },
          "name": "Hao Fei",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-08T09:58:07.591Z",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a1b",
          "name": "Yuan Zhou",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a1c",
          "user": {
            "_id": "67bc247b593452cc18965cb1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/EA3kTYaaff0Hr7-dGiOOj.png",
            "isPro": false,
            "fullname": "JUNCHENG LI",
            "user": "JunchengLi",
            "type": "user"
          },
          "name": "Juncheng Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:36:52.461Z",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a1d",
          "user": {
            "_id": "63958b4414513eaf9029ebf1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/U1g5H071pWRswGAG9UTpo.png",
            "isPro": false,
            "fullname": "Xiangtai Li",
            "user": "LXT",
            "type": "user"
          },
          "name": "Xiangtai Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:36:59.117Z",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a1e",
          "name": "Qingshan Xu",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a1f",
          "name": "Bobo Li",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a20",
          "user": {
            "_id": "64c139d867eff857ea51caa8",
            "avatarUrl": "/avatars/4b7b3f41c2e2cfa21dd43bbac6e081ae.svg",
            "isPro": false,
            "fullname": "Shengqiong Wu",
            "user": "ChocoWu",
            "type": "user"
          },
          "name": "Shengqiong Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-09T07:22:39.333Z",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a21",
          "user": {
            "_id": "64ff369d9abcc85a5519b33e",
            "avatarUrl": "/avatars/4b99cdaf5f970d930b196eddf1e5e499.svg",
            "isPro": false,
            "fullname": "Yaoting Wang",
            "user": "Gh0stAR",
            "type": "user"
          },
          "name": "Yaoting Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:37:27.790Z",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a22",
          "user": {
            "_id": "67e906836c7216f5bf91f70c",
            "avatarUrl": "/avatars/9c7f34d5b1d41ad7231d2733a399abb3.svg",
            "isPro": false,
            "fullname": "junbao.zhou",
            "user": "junbaozhou",
            "type": "user"
          },
          "name": "Junbao Zhou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:37:34.046Z",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a23",
          "user": {
            "_id": "65a28e129acab19980226731",
            "avatarUrl": "/avatars/abc3828f807efc4e03837b0eae063f98.svg",
            "isPro": false,
            "fullname": "Jiahao Meng",
            "user": "marinero4972",
            "type": "user"
          },
          "name": "Jiahao Meng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:37:40.200Z",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a24",
          "user": {
            "_id": "656724074f6ec72017754d33",
            "avatarUrl": "/avatars/e61de248f6f53719b2375077340dd033.svg",
            "isPro": false,
            "fullname": "QingyuShi",
            "user": "QingyuShi",
            "type": "user"
          },
          "name": "Qingyu Shi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-09T07:22:34.088Z",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a25",
          "name": "Zhiyuan Zhou",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a26",
          "name": "Liangtao Shi",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a27",
          "user": {
            "_id": "648ef24dc92367eecac0f4bd",
            "avatarUrl": "/avatars/38f1afd6b52efeee3aa41cc80225d788.svg",
            "isPro": false,
            "fullname": "Minghe Gao",
            "user": "gmh5811",
            "type": "user"
          },
          "name": "Minghe Gao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:38:16.554Z",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a28",
          "user": {
            "_id": "6241b95cfee9374a2598ecfe",
            "avatarUrl": "/avatars/196669df1689a5872fc18b271e80fdc1.svg",
            "isPro": false,
            "fullname": "Zhang Daoan",
            "user": "hazard",
            "type": "user"
          },
          "name": "Daoan Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:38:28.567Z",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a29",
          "name": "Zhiqi Ge",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a2a",
          "name": "Weiming Wu",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a2b",
          "name": "Siliang Tang",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a2c",
          "name": "Kaihang Pan",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a2d",
          "user": {
            "_id": "662917afda1cae6cbb50cd00",
            "avatarUrl": "/avatars/aa66de6cef6665c5d67071d82bac35c4.svg",
            "isPro": false,
            "fullname": "Yaobo Ye",
            "user": "superyyb",
            "type": "user"
          },
          "name": "Yaobo Ye",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:55:06.463Z",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a2e",
          "user": {
            "_id": "6391e41f2e73987364e6bcb2",
            "avatarUrl": "/avatars/d09a9ee329bb8c3a9e2929d67d24e97d.svg",
            "isPro": false,
            "fullname": "Haobo Yuan",
            "user": "HarborYuan",
            "type": "user"
          },
          "name": "Haobo Yuan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:39:11.094Z",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a2f",
          "name": "Tao Zhang",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a30",
          "user": {
            "_id": "6816d98fc075e49c1b15928e",
            "avatarUrl": "/avatars/6b24d047fc25075bedb3e74f78981bc0.svg",
            "isPro": false,
            "fullname": "Tianjie Ju",
            "user": "jometeorieNUS",
            "type": "user"
          },
          "name": "Tianjie Ju",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:53:06.930Z",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a31",
          "name": "Zixiang Meng",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a32",
          "name": "Shilin Xu",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a33",
          "name": "Liyu Jia",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a34",
          "name": "Wentao Hu",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a35",
          "user": {
            "_id": "64ad1c0bad6218d51a07b54e",
            "avatarUrl": "/avatars/0f84d9a51c6ca9bcef44de2d7c707d9b.svg",
            "isPro": false,
            "fullname": "LUO MENG",
            "user": "Eureka-Leo",
            "type": "user"
          },
          "name": "Meng Luo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-09T07:22:37.235Z",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a36",
          "name": "Jiebo Luo",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a37",
          "name": "Tat-Seng Chua",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a38",
          "user": {
            "_id": "67eaa070b9fa8908e151fd7d",
            "avatarUrl": "/avatars/1fe2fd678d2e71099a83a9bcb9ab517e.svg",
            "isPro": false,
            "fullname": "shuicheng yan",
            "user": "shuicheng",
            "type": "user"
          },
          "name": "Shuicheng Yan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:52:30.205Z",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a39",
          "name": "Hanwang Zhang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/TqzNcdmo0rwc-0JkEc0-i.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/HBVHtWBiagedRLXD-lWZc.png",
        "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/X02B6xk6CZDywtG8tGliK.png",
        "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/Xnc89DOmtr5j2hST5Um17.png",
        "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/Awl6jj9MX38cMRkgXpxHp.png",
        "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/HT_9Y1ponvvqUREjMTMBB.png",
        "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/sG3FBiQCx3EONF25cEbED.png"
      ],
      "publishedAt": "2025-05-07T17:59:32.000Z",
      "submittedOnDailyAt": "2025-05-09T01:19:17.510Z",
      "title": "Plan d'Accélération Multimodal : Niveau Général et Niveau de Benchmark Général",
      "submittedOnDailyBy": {
        "_id": "647773a1168cb428e00e9a8f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647773a1168cb428e00e9a8f/NiRR3ScY6Plzjibfwy1hC.jpeg",
        "isPro": false,
        "fullname": "Hao Fei",
        "user": "scofield7419",
        "type": "user"
      },
      "summary": "Le modèle de langue multimodal de diffusion (MLLM) est en pleine croissance grâce aux capacités avancées des modèles de langue de diffusion (LLM). Contrairement à ce que disent les experts, le MLLM actuel montre un développement significatif dans le domaine de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de la diffusion de",
      "upvotes": 42,
      "discussionId": "681c6c1d17fc8222eff39b45",
      "projectPage": "https://generalist.top/",
      "githubRepo": "https://github.com/path2generalist/General-Level",
      "ai_keywords": [
        "Multimodal Large Language Model (MLLM)",
        "Multimodal Generalist",
        "multimodal understanding",
        "comprehension",
        "generation",
        "General-Level",
        "Synergy",
        "General-Bench",
        "AGI (Artificial General Intelligence)"
      ]
    },
    "publishedAt": "2025-05-07T13:59:32.000Z",
    "title": "On Path to Multimodal Generalist: General-Level and General-Bench",
    "summary": "The Multimodal Large Language Model (MLLM) is currently experiencing rapid\ngrowth, driven by the advanced capabilities of LLMs. Unlike earlier\nspecialists, existing MLLMs are evolving towards a Multimodal Generalist\nparadigm. Initially limited to understanding multiple modalities, these models\nhave advanced to not only comprehend but also generate across modalities. Their\ncapabilities have expanded from coarse-grained to fine-grained multimodal\nunderstanding and from supporting limited modalities to arbitrary ones. While\nmany benchmarks exist to assess MLLMs, a critical question arises: Can we\nsimply assume that higher performance across tasks indicates a stronger MLLM\ncapability, bringing us closer to human-level AI? We argue that the answer is\nnot as straightforward as it seems. This project introduces General-Level, an\nevaluation framework that defines 5-scale levels of MLLM performance and\ngenerality, offering a methodology to compare MLLMs and gauge the progress of\nexisting systems towards more robust multimodal generalists and, ultimately,\ntowards AGI. At the core of the framework is the concept of Synergy, which\nmeasures whether models maintain consistent capabilities across comprehension\nand generation, and across multiple modalities. To support this evaluation, we\npresent General-Bench, which encompasses a broader spectrum of skills,\nmodalities, formats, and capabilities, including over 700 tasks and 325,800\ninstances. The evaluation results that involve over 100 existing\nstate-of-the-art MLLMs uncover the capability rankings of generalists,\nhighlighting the challenges in reaching genuine AI. We expect this project to\npave the way for future research on next-generation multimodal foundation\nmodels, providing a robust infrastructure to accelerate the realization of AGI.\nProject page: https://generalist.top/",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/TqzNcdmo0rwc-0JkEc0-i.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/HBVHtWBiagedRLXD-lWZc.png",
      "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/X02B6xk6CZDywtG8tGliK.png",
      "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/Xnc89DOmtr5j2hST5Um17.png",
      "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/Awl6jj9MX38cMRkgXpxHp.png",
      "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/HT_9Y1ponvvqUREjMTMBB.png",
      "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/sG3FBiQCx3EONF25cEbED.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.04620.png",
    "numComments": 5,
    "submittedBy": {
      "_id": "647773a1168cb428e00e9a8f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647773a1168cb428e00e9a8f/NiRR3ScY6Plzjibfwy1hC.jpeg",
      "fullname": "Hao Fei",
      "name": "scofield7419",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.05470",
      "authors": [
        {
          "_id": "681d9829edf34a77aab565eb",
          "name": "Jie Liu",
          "hidden": false
        },
        {
          "_id": "681d9829edf34a77aab565ec",
          "user": {
            "_id": "6553316bf151de82f6a23e1d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6553316bf151de82f6a23e1d/GTBkSj4Fa3OoyM6Muz_Sc.jpeg",
            "isPro": false,
            "fullname": "Gongye Liu",
            "user": "liuhuohuo",
            "type": "user"
          },
          "name": "Gongye Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:41:47.403Z",
          "hidden": false
        },
        {
          "_id": "681d9829edf34a77aab565ed",
          "name": "Jiajun Liang",
          "hidden": false
        },
        {
          "_id": "681d9829edf34a77aab565ee",
          "user": {
            "_id": "64d71083a787c9bc7b9f1238",
            "avatarUrl": "/avatars/d0b0546dec7fc5792921154bec41385a.svg",
            "isPro": false,
            "fullname": "Yangguang Li",
            "user": "Lp256",
            "type": "user"
          },
          "name": "Yangguang Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:43:44.697Z",
          "hidden": false
        },
        {
          "_id": "681d9829edf34a77aab565ef",
          "user": {
            "_id": "65377c30e48353201e6fdda0",
            "avatarUrl": "/avatars/a8f803b6f2e598eaee9c52c0d2ddfc16.svg",
            "isPro": false,
            "fullname": "Jiaheng Liu",
            "user": "CheeryLJH",
            "type": "user"
          },
          "name": "Jiaheng Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:45:07.297Z",
          "hidden": false
        },
        {
          "_id": "681d9829edf34a77aab565f0",
          "user": {
            "_id": "60e272ca6c78a8c122b12127",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60e272ca6c78a8c122b12127/xldEGBzGrU-bX6IwAw0Ie.jpeg",
            "isPro": false,
            "fullname": "Xintao Wang",
            "user": "Xintao",
            "type": "user"
          },
          "name": "Xintao Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:42:27.977Z",
          "hidden": false
        },
        {
          "_id": "681d9829edf34a77aab565f1",
          "name": "Pengfei Wan",
          "hidden": false
        },
        {
          "_id": "681d9829edf34a77aab565f2",
          "user": {
            "_id": "644c8324f02250233d0d67d9",
            "avatarUrl": "/avatars/feb39d281457c1750f3eada3c060a23e.svg",
            "isPro": false,
            "fullname": "Di Zhang",
            "user": "dizhang",
            "type": "user"
          },
          "name": "Di Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:43:01.366Z",
          "hidden": false
        },
        {
          "_id": "681d9829edf34a77aab565f3",
          "name": "Wanli Ouyang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-08T17:58:45.000Z",
      "submittedOnDailyAt": "2025-05-09T05:45:53.355Z",
      "title": "Flow-GRPO : Entraînement d'un modèle de matching de flux en utilisant la RL en ligne",
      "submittedOnDailyBy": {
        "_id": "64d71083a787c9bc7b9f1238",
        "avatarUrl": "/avatars/d0b0546dec7fc5792921154bec41385a.svg",
        "isPro": false,
        "fullname": "Yangguang Li",
        "user": "Lp256",
        "type": "user"
      },
      "summary": "Je propose Flow-GRPO. C'est le premier méthode utile. Notre approche utilise deux stratégies clés.\n\n(1) Transformation ODE-to-SDE : Nous transformons une équation différentielle ordinaire (ODE) déterministe en une équation différentielle stochastique équivalente (SDE) qui est ajustée à la distribution de frontière du modèle original à chaque étape de temps. De cette manière, il est possible de réaliser un échantillonnage statistique lors de la recherche d'RL.\n\n(2) Stratégie de Réduction de Débruitage : En réduisant la quantité de débruitage tout en maintenant le nombre d'étapes de temps d'inférence du modèle, on peut significativement améliorer l'efficacité de l'échantillonnage.\n\nExpérimentalement, Flow-GRPO est efficace dans les organisations complexes et améliore significativement la précision de GenEval, augmentant la précision de 63% à 95%. Il améliore également la précision dans la réalisation de texte visuel, augmentant de 59% à 92%. Flow-GRPO montre également un grand effet sur la concordance avec les préférences humaines. En particulier, aucun problème d'accumulation de récompenses n'a été observé. Cela signifie que la qualité et la diversité des images n'ont pas diminué et que la récompense n'a pas augmenté, ce qui a été stable dans les expériences.",
      "upvotes": 26,
      "discussionId": "681d982aedf34a77aab56635",
      "ai_keywords": [
        "Flow-GRPO",
        "reinforcement learning (RL)",
        "flow matching models",
        "ODE-to-SDE conversion",
        "Ordinary Differential Equation (ODE)",
        "Stochastic Differential Equation (SDE)",
        "Denoising Reduction strategy",
        "GenEval accuracy",
        "text-to-image tasks",
        "SD3.5",
        "visual text rendering",
        "human preference alignment",
        "reward hacking"
      ]
    },
    "publishedAt": "2025-05-08T13:58:45.000Z",
    "title": "Flow-GRPO: Training Flow Matching Models via Online RL",
    "summary": "We propose Flow-GRPO, the first method integrating online reinforcement\nlearning (RL) into flow matching models. Our approach uses two key strategies:\n(1) an ODE-to-SDE conversion that transforms a deterministic Ordinary\nDifferential Equation (ODE) into an equivalent Stochastic Differential Equation\n(SDE) that matches the original model's marginal distribution at all timesteps,\nenabling statistical sampling for RL exploration; and (2) a Denoising Reduction\nstrategy that reduces training denoising steps while retaining the original\ninference timestep number, significantly improving sampling efficiency without\nperformance degradation. Empirically, Flow-GRPO is effective across multiple\ntext-to-image tasks. For complex compositions, RL-tuned SD3.5 generates nearly\nperfect object counts, spatial relations, and fine-grained attributes, boosting\nGenEval accuracy from 63% to 95%. In visual text rendering, its accuracy\nimproves from 59% to 92%, significantly enhancing text generation.\nFlow-GRPO also achieves substantial gains in human preference alignment.\nNotably, little to no reward hacking occurred, meaning rewards did not increase\nat the cost of image quality or diversity, and both remained stable in our\nexperiments.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.05470.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64d71083a787c9bc7b9f1238",
      "avatarUrl": "/avatars/d0b0546dec7fc5792921154bec41385a.svg",
      "fullname": "Yangguang Li",
      "name": "Lp256",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.04921",
      "authors": [
        {
          "_id": "681dbb9988ca86d430f1d0d2",
          "user": {
            "_id": "62fdb01bc1588e1d4c6c1a7c",
            "avatarUrl": "/avatars/bd03085995b1c34e0ac8a845cf2c4e83.svg",
            "isPro": false,
            "fullname": "Yunxin Li",
            "user": "YunxinLi",
            "type": "user"
          },
          "name": "Yunxin Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:05:42.761Z",
          "hidden": false
        },
        {
          "_id": "681dbb9988ca86d430f1d0d3",
          "user": {
            "_id": "64380ae1819f3ab20d17431b",
            "avatarUrl": "/avatars/a36b073c1c783102ddb455204fd816bd.svg",
            "isPro": false,
            "fullname": "ZhenyuLiu",
            "user": "foggyforest",
            "type": "user"
          },
          "name": "Zhenyu Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-09T10:08:42.493Z",
          "hidden": false
        },
        {
          "_id": "681dbb9988ca86d430f1d0d4",
          "user": {
            "_id": "67ecc6a08647cfa1775a9fda",
            "avatarUrl": "/avatars/bb15abd7a3d2c51380b0b1f819ef76e2.svg",
            "isPro": false,
            "fullname": "Zitao Li",
            "user": "TerenceL-TL",
            "type": "user"
          },
          "name": "Zitao Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-09T08:33:26.702Z",
          "hidden": false
        },
        {
          "_id": "681dbb9988ca86d430f1d0d5",
          "name": "Xuanyu Zhang",
          "hidden": false
        },
        {
          "_id": "681dbb9988ca86d430f1d0d6",
          "user": {
            "_id": "639c379cdb7c5f35004066cb",
            "avatarUrl": "/avatars/3e435506ee85aa7d2d0ec2174a07462f.svg",
            "isPro": false,
            "fullname": "Zhenran Xu",
            "user": "imryanxu",
            "type": "user"
          },
          "name": "Zhenran Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-09T10:08:40.339Z",
          "hidden": false
        },
        {
          "_id": "681dbb9988ca86d430f1d0d7",
          "name": "Xinyu Chen",
          "hidden": false
        },
        {
          "_id": "681dbb9988ca86d430f1d0d8",
          "name": "Haoyuan Shi",
          "hidden": false
        },
        {
          "_id": "681dbb9988ca86d430f1d0d9",
          "name": "Shenyuan Jiang",
          "hidden": false
        },
        {
          "_id": "681dbb9988ca86d430f1d0da",
          "name": "Xintong Wang",
          "hidden": false
        },
        {
          "_id": "681dbb9988ca86d430f1d0db",
          "name": "Jifang Wang",
          "hidden": false
        },
        {
          "_id": "681dbb9988ca86d430f1d0dc",
          "name": "Shouzheng Huang",
          "hidden": false
        },
        {
          "_id": "681dbb9988ca86d430f1d0dd",
          "name": "Xinping Zhao",
          "hidden": false
        },
        {
          "_id": "681dbb9988ca86d430f1d0de",
          "name": "Borui Jiang",
          "hidden": false
        },
        {
          "_id": "681dbb9988ca86d430f1d0df",
          "name": "Lanqing Hong",
          "hidden": false
        },
        {
          "_id": "681dbb9988ca86d430f1d0e0",
          "name": "Longyue Wang",
          "hidden": false
        },
        {
          "_id": "681dbb9988ca86d430f1d0e1",
          "name": "Zhuotao Tian",
          "hidden": false
        },
        {
          "_id": "681dbb9988ca86d430f1d0e2",
          "name": "Baoxing Huai",
          "hidden": false
        },
        {
          "_id": "681dbb9988ca86d430f1d0e3",
          "name": "Wenhan Luo",
          "hidden": false
        },
        {
          "_id": "681dbb9988ca86d430f1d0e4",
          "name": "Weihua Luo",
          "hidden": false
        },
        {
          "_id": "681dbb9988ca86d430f1d0e5",
          "name": "Zheng Zhang",
          "hidden": false
        },
        {
          "_id": "681dbb9988ca86d430f1d0e6",
          "name": "Baotian Hu",
          "hidden": false
        },
        {
          "_id": "681dbb9988ca86d430f1d0e7",
          "name": "Min Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-08T03:35:23.000Z",
      "submittedOnDailyAt": "2025-05-09T06:54:36.013Z",
      "title": "Perception, reasoning, thinking, planning: Research on the approach of reasoning in large-scale multimodal models",
      "submittedOnDailyBy": {
        "_id": "64380ae1819f3ab20d17431b",
        "avatarUrl": "/avatars/a36b073c1c783102ddb455204fd816bd.svg",
        "isPro": false,
        "fullname": "ZhenyuLiu",
        "user": "foggyforest",
        "type": "user"
      },
      "summary": "La théorie se concentre sur le centre de l'intelligence, formant des capacités comme la décision, l'extrapolation de conclusions et la généralisation qui transcenden des domaines. Dans le domaine de l'intelligence artificielle, il est crucial que les systèmes soient développés pour fonctionner dans des environnements ouverts et incertains, divers. La théorie doit permettre des actions fortes et adaptatives. Les modèles de théorie multimodal grands (LMRMs) intègrent des modèles de texte, d'images, de voix et de vidéo, soutenant des capacités complexes de reconnaissance, de compréhension précise et de théorie profonde. Avec le progrès de la recherche, la théorie multimodal a évolué rapidement depuis un pipeline modulaire de reconnaissance jusqu'à un cadre continu centré sur le langage. Dans ce processus, l'ajustement des commandes et l'apprentissage par renforcement ont amélioré la théorie du modèle, mais ont laissé de grands problèmes en termes de généralisation, de profondeur de la théorie et de comportements de sortie. Pour résoudre ces problèmes, nous basons notre travail sur une nouvelle philosophie de conception et de nouvelles capacités, offrant un étude détaillée de la recherche sur la théorie multimodal en quatre étapes. Premièrement, nous explorons les premiers efforts modulaires basés sur des tâches spécifiques, où la théorie était cachée dans des étapes de représentation, de démarrage et de fusion. Ensuite, nous examinons les derniers approches, intégrant la théorie dans des modèles multimodal de LLMs et développant des chaînes de pensée multimodal (MCoT) et l'apprentissage par renforcement multimodal, facilitant des chaînes théoriques plus complexes et structurées. Finalement, en basant nous sur les difficiles benchmarks de OpenAI O3 et O4-mini et sur des cas d'expérimentation, nous discutons la direction conceptuelle des modèles de théorie multimodal grands (N-LMRMs) et l'objectif de soutenir des théories adaptatives et scalables dans des environnements complexes et réels.",
      "upvotes": 21,
      "discussionId": "681dbb9b88ca86d430f1d183",
      "projectPage": "https://github.com/HITsz-TMG/Awesome-Large-Multimodal-Reasoning-Models",
      "githubRepo": "https://github.com/HITsz-TMG/Awesome-Large-Multimodal-Reasoning-Models",
      "ai_keywords": [
        "Large Multimodal Reasoning Models (LMRMs)",
        "multimodal reasoning",
        "Cross-modal understanding",
        "task-specific modules",
        "representation",
        "alignment",
        "fusion",
        "Multimodal Chain-of-Thought (MCoT)",
        "multimodal reinforcement learning",
        "native large multimodal reasoning models (N-LMRMs)",
        "scalable",
        "agentic",
        "adaptive reasoning",
        "planning"
      ]
    },
    "publishedAt": "2025-05-07T23:35:23.000Z",
    "title": "Perception, Reason, Think, and Plan: A Survey on Large Multimodal\n  Reasoning Models",
    "summary": "Reasoning lies at the heart of intelligence, shaping the ability to make\ndecisions, draw conclusions, and generalize across domains. In artificial\nintelligence, as systems increasingly operate in open, uncertain, and\nmultimodal environments, reasoning becomes essential for enabling robust and\nadaptive behavior. Large Multimodal Reasoning Models (LMRMs) have emerged as a\npromising paradigm, integrating modalities such as text, images, audio, and\nvideo to support complex reasoning capabilities and aiming to achieve\ncomprehensive perception, precise understanding, and deep reasoning. As\nresearch advances, multimodal reasoning has rapidly evolved from modular,\nperception-driven pipelines to unified, language-centric frameworks that offer\nmore coherent cross-modal understanding. While instruction tuning and\nreinforcement learning have improved model reasoning, significant challenges\nremain in omni-modal generalization, reasoning depth, and agentic behavior. To\naddress these issues, we present a comprehensive and structured survey of\nmultimodal reasoning research, organized around a four-stage developmental\nroadmap that reflects the field's shifting design philosophies and emerging\ncapabilities. First, we review early efforts based on task-specific modules,\nwhere reasoning was implicitly embedded across stages of representation,\nalignment, and fusion. Next, we examine recent approaches that unify reasoning\ninto multimodal LLMs, with advances such as Multimodal Chain-of-Thought (MCoT)\nand multimodal reinforcement learning enabling richer and more structured\nreasoning chains. Finally, drawing on empirical insights from challenging\nbenchmarks and experimental cases of OpenAI O3 and O4-mini, we discuss the\nconceptual direction of native large multimodal reasoning models (N-LMRMs),\nwhich aim to support scalable, agentic, and adaptive reasoning and planning in\ncomplex, real-world environments.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.04921.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64380ae1819f3ab20d17431b",
      "avatarUrl": "/avatars/a36b073c1c783102ddb455204fd816bd.svg",
      "fullname": "ZhenyuLiu",
      "name": "foggyforest",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.02847",
      "authors": [
        {
          "_id": "681d7031e9969eecfcb4eb81",
          "name": "Bang Zhang",
          "hidden": false
        },
        {
          "_id": "681d7031e9969eecfcb4eb82",
          "user": {
            "_id": "648294b2eb4befee378951c1",
            "avatarUrl": "/avatars/da5d8bf9d8662cc2ffa2c0de49bd66a3.svg",
            "isPro": false,
            "fullname": "Ruotian Ma",
            "user": "vvibt",
            "type": "user"
          },
          "name": "Ruotian Ma",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-09T07:21:30.886Z",
          "hidden": false
        },
        {
          "_id": "681d7031e9969eecfcb4eb83",
          "name": "Qingxuan Jiang",
          "hidden": false
        },
        {
          "_id": "681d7031e9969eecfcb4eb84",
          "name": "Peisong Wang",
          "hidden": false
        },
        {
          "_id": "681d7031e9969eecfcb4eb85",
          "name": "Jiaqi Chen",
          "hidden": false
        },
        {
          "_id": "681d7031e9969eecfcb4eb86",
          "name": "Zheng Xie",
          "hidden": false
        },
        {
          "_id": "681d7031e9969eecfcb4eb87",
          "name": "Xingyu Chen",
          "hidden": false
        },
        {
          "_id": "681d7031e9969eecfcb4eb88",
          "name": "Yue Wang",
          "hidden": false
        },
        {
          "_id": "681d7031e9969eecfcb4eb89",
          "name": "Fanghua Ye",
          "hidden": false
        },
        {
          "_id": "681d7031e9969eecfcb4eb8a",
          "name": "Jian Li",
          "hidden": false
        },
        {
          "_id": "681d7031e9969eecfcb4eb8b",
          "name": "Yifan Yang",
          "hidden": false
        },
        {
          "_id": "681d7031e9969eecfcb4eb8c",
          "user": {
            "_id": "67485743561b1e6f9579389f",
            "avatarUrl": "/avatars/8a4cc63bd7be388010bc329bb74582a1.svg",
            "isPro": false,
            "fullname": "Zhaopeng Tu",
            "user": "zptu",
            "type": "user"
          },
          "name": "Zhaopeng Tu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-09T07:21:28.468Z",
          "hidden": false
        },
        {
          "_id": "681d7031e9969eecfcb4eb8d",
          "name": "Xiaolong Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-01T19:06:10.000Z",
      "submittedOnDailyAt": "2025-05-09T01:37:10.548Z",
      "title": "Présente l'utilisation d'agents de langage de grande échelle comme juges pour évaluer la cognition sociale avancée des modèles de langage.",
      "submittedOnDailyBy": {
        "_id": "648294b2eb4befee378951c1",
        "avatarUrl": "/avatars/da5d8bf9d8662cc2ffa2c0de49bd66a3.svg",
        "isPro": false,
        "fullname": "Ruotian Ma",
        "user": "vvibt",
        "type": "user"
      },
      "summary": "L'évaluation du degré auquel les modèles de langage de haut niveau (LLM) comprennent l'humanité, non seulement au sens d'entendre des phrases, est un défi ouvert. Pour aborder ce défi, nous présentons le cadre d'évaluation automatique \"Sentient Agent as a Judge\" (SAGE). Ce cadre mesure le haut niveau de cognition social des LLM. SAGE cherche à ce que l'agent se comporte comme un être humain, en simulant des changements émotionnels et des pensées internes, et évalue le modèle de manière la plus réaliste à travers de multiples conversations. À chaque tour, l'agent fournit une trajectoire émotionnelle numérique et des pensées internes interprétables basées sur comment la change l'émotion, comment il se sent et comment il répond. Les expériences dans des scénarios de 100 conversations ont montré que le score final d'émotion sentimentale a une forte corrélation avec les scores de l'Inventaire de Relations de Barrett-Lennard (BLRI) et avec le niveau d'émotions d'un être humain, démontrant la confiance psychologique. De plus, un laboratoire ouvert a été construit pour couvrir 18 modèles commerciaux et open-source, révélant une grande différence entre les modèles les plus récents (comme GPT-4o-Latest et Gemini2.5-Pro) et les modèles initiaux (maximum 4 fois). Cette différence n'est pas reflétée dans les expériences communes (par exemple, Arena). SAGE fournit une outil débutant, extensible et interprétable nécessaire pour suivre le développement des agents de langage émotionnel et socialement exceptionnels.",
      "upvotes": 15,
      "discussionId": "681d7033e9969eecfcb4ec2d",
      "githubRepo": "https://github.com/Tencent/digitalhuman/tree/main/SAGE",
      "ai_keywords": [
        "Sentient Agent as a Judge (SAGE)",
        "higher-order social cognition",
        "emotional changes",
        "inner thoughts",
        "multi-turn conversations",
        "numerical emotion trajectory",
        "Barrett-Lennard Relationship Inventory (BLRI)",
        "utterance-level empathy metrics",
        "psychological fidelity",
        "Sentient Leaderboard",
        "empathetic",
        "socially adept language agents"
      ]
    },
    "publishedAt": "2025-05-01T15:06:10.000Z",
    "title": "Sentient Agent as a Judge: Evaluating Higher-Order Social Cognition in\n  Large Language Models",
    "summary": "Assessing how well a large language model (LLM) understands human, rather\nthan merely text, remains an open challenge. To bridge the gap, we introduce\nSentient Agent as a Judge (SAGE), an automated evaluation framework that\nmeasures an LLM's higher-order social cognition. SAGE instantiates a Sentient\nAgent that simulates human-like emotional changes and inner thoughts during\ninteraction, providing a more realistic evaluation of the tested model in\nmulti-turn conversations. At every turn, the agent reasons about (i) how its\nemotion changes, (ii) how it feels, and (iii) how it should reply, yielding a\nnumerical emotion trajectory and interpretable inner thoughts. Experiments on\n100 supportive-dialogue scenarios show that the final Sentient emotion score\ncorrelates strongly with Barrett-Lennard Relationship Inventory (BLRI) ratings\nand utterance-level empathy metrics, validating psychological fidelity. We also\nbuild a public Sentient Leaderboard covering 18 commercial and open-source\nmodels that uncovers substantial gaps (up to 4x) between frontier systems\n(GPT-4o-Latest, Gemini2.5-Pro) and earlier baselines, gaps not reflected in\nconventional leaderboards (e.g., Arena). SAGE thus provides a principled,\nscalable and interpretable tool for tracking progress toward genuinely\nempathetic and socially adept language agents.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02847.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "648294b2eb4befee378951c1",
      "avatarUrl": "/avatars/da5d8bf9d8662cc2ffa2c0de49bd66a3.svg",
      "fullname": "Ruotian Ma",
      "name": "vvibt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.05315",
      "authors": [
        {
          "_id": "681d7ccb572e742b3f42d1f3",
          "user": {
            "_id": "6602869253a0518b2a98cafd",
            "avatarUrl": "/avatars/c14b5953a716f42c83ad28147f8308ae.svg",
            "isPro": false,
            "fullname": "Yuhui Xu",
            "user": "yuhuixu",
            "type": "user"
          },
          "name": "Yuhui Xu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:56:04.644Z",
          "hidden": false
        },
        {
          "_id": "681d7ccb572e742b3f42d1f4",
          "user": {
            "_id": "63a3ff69f91ad3ea5703841d",
            "avatarUrl": "/avatars/69227c4bce01d33747c1377b6f9672db.svg",
            "isPro": false,
            "fullname": "Hanze Dong",
            "user": "hendrydong",
            "type": "user"
          },
          "name": "Hanze Dong",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:56:10.829Z",
          "hidden": false
        },
        {
          "_id": "681d7ccb572e742b3f42d1f5",
          "name": "Lei Wang",
          "hidden": false
        },
        {
          "_id": "681d7ccb572e742b3f42d1f6",
          "user": {
            "_id": "65f84fd980481173afd91233",
            "avatarUrl": "/avatars/6ac7bd6beba24d1476c5179b88c9e3fa.svg",
            "isPro": false,
            "fullname": "Doyen",
            "user": "doyensahoo",
            "type": "user"
          },
          "name": "Doyen Sahoo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:56:18.676Z",
          "hidden": false
        },
        {
          "_id": "681d7ccb572e742b3f42d1f7",
          "user": {
            "_id": "61f9d3b54ac99e8a1bae85f4",
            "avatarUrl": "/avatars/ac47d13204dd22452e4bc46e280842d5.svg",
            "isPro": false,
            "fullname": "JunnanLi",
            "user": "JunnanLi",
            "type": "user"
          },
          "name": "Junnan Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:56:32.272Z",
          "hidden": false
        },
        {
          "_id": "681d7ccb572e742b3f42d1f8",
          "user": {
            "_id": "649dbcc4e0fff1ed099dc80a",
            "avatarUrl": "/avatars/c87c273ca628dbcddccbf1ee19b2ce33.svg",
            "isPro": false,
            "fullname": "Caiming Xiong",
            "user": "cxiong",
            "type": "user"
          },
          "name": "Caiming Xiong",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:56:38.430Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-08T15:01:06.000Z",
      "submittedOnDailyAt": "2025-05-09T02:31:21.542Z",
      "title": "Scalable Continous Scope by Elesford Resizing",
      "submittedOnDailyBy": {
        "_id": "6602869253a0518b2a98cafd",
        "avatarUrl": "/avatars/c14b5953a716f42c83ad28147f8308ae.svg",
        "isPro": false,
        "fullname": "Yuhui Xu",
        "user": "yuhuixu",
        "type": "user"
      },
      "summary": "Les modèles de logique générale (LRMs) ont réalisé des progrès impressionnants en générant de longues chaînes de raisonnement (CoT) pour des tâches complexes. Cependant, la longueur illimitée des outputs peut entraîner des problèmes significatifs dans des applications réelles où les tokens, la langue latine ou les méthodes de raisonnement informatique sont strictement limités. Nous proposons un nouveau cadre de travail pour les raisonnements longs et échelonnables. Ce cadre utilise un approche de distribution indépendante et distingue clairement deux étapes : \"penser\" et \"résoudre\". Dans les tests, Elastic Reasoning priorise la complétude de la section des solutions, améliorant considérablement la confiance sous des contraintes strictes de ressources. Pour entraîner des modèles robustes dans des processus de pensée prolongés, nous introduisons une structure légère et robuste basée sur GRPO. Cette structure est adaptée à la breveté du processus de pensée, en enseignant au modèle de présenter les raisons les plus appropriées et de généraliser efficacement à de nouvelles contraintes de formation, sans besoin d'entraînement supplémentaire. Les résultats sur les benchmarks de mathématiques (AIME, MATH500) et de programmation (LiveCodeBench, Codeforces) montrent que Elastic Reasoning fonctionne robustement sous des contraintes strictes de formation et nécessite moins de coût d'entraînement que le méthode de base. En particulier, notre approche génère des raisons plus claires et efficaces dans des environnements sans contraintes. Elastic Reasoning offre une solution logique et pratique pour les problèmes urgents de raisonnement contrôlable et échelonnable.",
      "upvotes": 13,
      "discussionId": "681d7ccc572e742b3f42d21a",
      "ai_keywords": [
        "Large reasoning models (LRMs)",
        "chain of thought (CoT)",
        "inference-time budgets",
        "tokens",
        "latency",
        "compute",
        "Elastic Reasoning",
        "scalable chain of thoughts",
        "thinking phase",
        "solution phase",
        "independently allocated budgets",
        "completeness of solution segments",
        "reliability",
        "resource constraints",
        "lightweight budget-constrained rollout strategy",
        "GRPO",
        "adaptive reasoning",
        "unseen budget constraints",
        "mathematical benchmarks (AIME, MATH500)",
        "programming benchmarks (LiveCodeBench, Codeforces)",
        "unconstrained settings",
        "principled solution"
      ]
    },
    "publishedAt": "2025-05-08T11:01:06.000Z",
    "title": "Scalable Chain of Thoughts via Elastic Reasoning",
    "summary": "Large reasoning models (LRMs) have achieved remarkable progress on complex\ntasks by generating extended chains of thought (CoT). However, their\nuncontrolled output lengths pose significant challenges for real-world\ndeployment, where inference-time budgets on tokens, latency, or compute are\nstrictly constrained. We propose Elastic Reasoning, a novel framework for\nscalable chain of thoughts that explicitly separates reasoning into two\nphases--thinking and solution--with independently allocated budgets. At test\ntime, Elastic Reasoning prioritize that completeness of solution segments,\nsignificantly improving reliability under tight resource constraints. To train\nmodels that are robust to truncated thinking, we introduce a lightweight\nbudget-constrained rollout strategy, integrated into GRPO, which teaches the\nmodel to reason adaptively when the thinking process is cut short and\ngeneralizes effectively to unseen budget constraints without additional\ntraining. Empirical results on mathematical (AIME, MATH500) and programming\n(LiveCodeBench, Codeforces) benchmarks demonstrate that Elastic Reasoning\nperforms robustly under strict budget constraints, while incurring\nsignificantly lower training cost than baseline methods. Remarkably, our\napproach also produces more concise and efficient reasoning even in\nunconstrained settings. Elastic Reasoning offers a principled and practical\nsolution to the pressing challenge of controllable reasoning at scale.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.05315.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6602869253a0518b2a98cafd",
      "avatarUrl": "/avatars/c14b5953a716f42c83ad28147f8308ae.svg",
      "fullname": "Yuhui Xu",
      "name": "yuhuixu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.05071",
      "authors": [
        {
          "_id": "681da6375f701833274a0d21",
          "user": {
            "_id": "6621e591c50869c1e91a1639",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6621e591c50869c1e91a1639/L_PoEn2BRAJcnWZX-JebR.jpeg",
            "isPro": false,
            "fullname": "Chunyu Xie",
            "user": "xiechunyu",
            "type": "user"
          },
          "name": "Chunyu Xie",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-09T07:21:03.296Z",
          "hidden": false
        },
        {
          "_id": "681da6375f701833274a0d22",
          "user": {
            "_id": "5e49e8cf37cb5b49818287ae",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5e49e8cf37cb5b49818287ae/IV9b5Z70NhgmBNfAlc_co.jpeg",
            "isPro": false,
            "fullname": "Bin Wang",
            "user": "binwang",
            "type": "user"
          },
          "name": "Bin Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:58:33.968Z",
          "hidden": true
        },
        {
          "_id": "681da6375f701833274a0d23",
          "user": {
            "_id": "632c098b456c31252774e7c5",
            "avatarUrl": "/avatars/e3720d2fcb69d93c8d5aa5f50aab5f0e.svg",
            "isPro": false,
            "fullname": "kong",
            "user": "fanjing",
            "type": "user"
          },
          "name": "Fanjing Kong",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:58:42.881Z",
          "hidden": false
        },
        {
          "_id": "681da6375f701833274a0d24",
          "user": {
            "_id": "65b793b374698ba5a815bf4f",
            "avatarUrl": "/avatars/44a7e694a5089dbc773018111270ac26.svg",
            "isPro": false,
            "fullname": "Jincheng Li",
            "user": "jinchenglijc",
            "type": "user"
          },
          "name": "Jincheng Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:58:50.538Z",
          "hidden": false
        },
        {
          "_id": "681da6375f701833274a0d25",
          "user": {
            "_id": "659b8576999b82db2ad8a398",
            "avatarUrl": "/avatars/2ec7663e25e4a0238819818e69d9a5bd.svg",
            "isPro": false,
            "fullname": "Liang",
            "user": "DaweiLiang",
            "type": "user"
          },
          "name": "Dawei Liang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:58:56.445Z",
          "hidden": false
        },
        {
          "_id": "681da6375f701833274a0d26",
          "name": "Gengshen Zhang",
          "hidden": false
        },
        {
          "_id": "681da6375f701833274a0d27",
          "user": {
            "_id": "649935abbe8fd92c27ab1ed8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649935abbe8fd92c27ab1ed8/ueWnaZtJa-oWpzupP6FV8.png",
            "isPro": false,
            "fullname": "David Leon",
            "user": "DavidLeon",
            "type": "user"
          },
          "name": "Dawei Leng",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-09T07:00:10.249Z",
          "hidden": false
        },
        {
          "_id": "681da6375f701833274a0d28",
          "name": "Yuhui Yin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-08T09:06:53.000Z",
      "submittedOnDailyAt": "2025-05-09T05:27:38.509Z",
      "title": "FG-CLIP : Détails en arrangements visuels et textuels",
      "submittedOnDailyBy": {
        "_id": "649935abbe8fd92c27ab1ed8",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649935abbe8fd92c27ab1ed8/ueWnaZtJa-oWpzupP6FV8.png",
        "isPro": false,
        "fullname": "David Leon",
        "user": "DavidLeon",
        "type": "user"
      },
      "summary": "Contrastive Language-Image Pre-training (CLIP) montre un excellent rendement dans des tâches telles que la recherche d'images-texte et la classification sans exemples, mais fonctionne principalement sur des courtes descriptions de phrase clé, ce qui n'est pas adapté pour un compréhension détaillée. Pour résoudre ces problèmes, nous proposons Fine-Grained CLIP (FG-CLIP). FG-CLIP renforce le compréhension détaillée grâce à trois innovations clés. Premièrement, il utilise des modèles de diversification à grande échelle pour générer 16 milliards de pairs de descriptions longues et d'images, permettant une compréhension de détails significatifs à l'échelle globale. Ensuite, il construit un jeu de données de haut rendement qui comprend 12 millions d'images et 40 millions de patches avec des bounding boxes, garantissant des expressions précises avec des détails et de contexte. Finalement, il ajoute 10 millions de pairs de données négatives difficiles pour améliorer la capacité du modèle de distinguer des différences significatives. De plus, un méthode d'entraînement a été conçue pour s'adapter à ces données. Des expériences larges montrent que FG-CLIP dépasse CLIP original et d'autres méthodes de pointe dans des tâches d'analyse détaillée, de détection d'objets, de recherche d'images-texte et de benchmark de diversification générale. Ces résultats clairement montrent que FG-CLIP améliore la compréhension des détails dans les images et le rendement général du modèle. Les données, le code et les modèles sont disponibles sur https://github.com/360CVGroup/FG-CLIP.",
      "upvotes": 8,
      "discussionId": "681da6385f701833274a0d8a",
      "githubRepo": "https://github.com/360CVGroup/FG-CLIP",
      "ai_keywords": [
        "Contrastive Language-Image Pre-training (CLIP)",
        "image-text retrieval",
        "zero-shot classification",
        "fine-grained understanding",
        "coarse-grained short captions",
        "multimodal models",
        "1.6 billion long caption-image pairs",
        "high-quality dataset",
        "12 million images",
        "40 million region-specific bounding boxes",
        "detailed captions",
        "10 million hard fine-grained negative samples",
        "fine-grained understanding",
        "open-vocabulary object detection",
        "general multimodal benchmarks",
        "FG-CLIP"
      ]
    },
    "publishedAt": "2025-05-08T05:06:53.000Z",
    "title": "FG-CLIP: Fine-Grained Visual and Textual Alignment",
    "summary": "Contrastive Language-Image Pre-training (CLIP) excels in multimodal tasks\nsuch as image-text retrieval and zero-shot classification but struggles with\nfine-grained understanding due to its focus on coarse-grained short captions.\nTo address this, we propose Fine-Grained CLIP (FG-CLIP), which enhances\nfine-grained understanding through three key innovations. First, we leverage\nlarge multimodal models to generate 1.6 billion long caption-image pairs for\ncapturing global-level semantic details. Second, a high-quality dataset is\nconstructed with 12 million images and 40 million region-specific bounding\nboxes aligned with detailed captions to ensure precise, context-rich\nrepresentations. Third, 10 million hard fine-grained negative samples are\nincorporated to improve the model's ability to distinguish subtle semantic\ndifferences. Corresponding training methods are meticulously designed for these\ndata. Extensive experiments demonstrate that FG-CLIP outperforms the original\nCLIP and other state-of-the-art methods across various downstream tasks,\nincluding fine-grained understanding, open-vocabulary object detection,\nimage-text retrieval, and general multimodal benchmarks. These results\nhighlight FG-CLIP's effectiveness in capturing fine-grained image details and\nimproving overall model performance. The related data, code, and models are\navailable at https://github.com/360CVGroup/FG-CLIP.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.05071.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "649935abbe8fd92c27ab1ed8",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649935abbe8fd92c27ab1ed8/ueWnaZtJa-oWpzupP6FV8.png",
      "fullname": "David Leon",
      "name": "DavidLeon",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.05474",
      "authors": [
        {
          "_id": "681d7b5ae27a030c96a28bde",
          "user": {
            "_id": "672392c4a4c4381cefc06416",
            "avatarUrl": "/avatars/8ee84a7e3e91e5d13074bc3c407ff75d.svg",
            "isPro": false,
            "fullname": "Wen Beichen",
            "user": "wenbc21",
            "type": "user"
          },
          "name": "Beichen Wen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:56:55.900Z",
          "hidden": false
        },
        {
          "_id": "681d7b5ae27a030c96a28bdf",
          "user": {
            "_id": "63f47b5321eb234ab739e91a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f47b5321eb234ab739e91a/vWfFNVtMkHl8gieha5PPd.jpeg",
            "isPro": false,
            "fullname": "Haozhe Xie",
            "user": "hzxie",
            "type": "user"
          },
          "name": "Haozhe Xie",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:57:02.341Z",
          "hidden": false
        },
        {
          "_id": "681d7b5ae27a030c96a28be0",
          "user": {
            "_id": "62fc8cf7ee999004b5a8b982",
            "avatarUrl": "/avatars/6c5dda9e58747054a989f077a078f3dc.svg",
            "isPro": false,
            "fullname": "Zhaoxi Chen",
            "user": "FrozenBurning",
            "type": "user"
          },
          "name": "Zhaoxi Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:57:08.543Z",
          "hidden": false
        },
        {
          "_id": "681d7b5ae27a030c96a28be1",
          "name": "Fangzhou Hong",
          "hidden": false
        },
        {
          "_id": "681d7b5ae27a030c96a28be2",
          "user": {
            "_id": "62ab1ac1d48b4d8b048a3473",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656826685333-62ab1ac1d48b4d8b048a3473.png",
            "isPro": false,
            "fullname": "Ziwei Liu",
            "user": "liuziwei7",
            "type": "user"
          },
          "name": "Ziwei Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:57:27.473Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63f47b5321eb234ab739e91a/PqIosa0nVWamlNxCmd6w1.webp"
      ],
      "publishedAt": "2025-05-08T17:59:54.000Z",
      "submittedOnDailyAt": "2025-05-09T02:21:54.023Z",
      "title": "3D Mécanique des Dispositifs : Résumé",
      "submittedOnDailyBy": {
        "_id": "63f47b5321eb234ab739e91a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f47b5321eb234ab739e91a/vWfFNVtMkHl8gieha5PPd.jpeg",
        "isPro": false,
        "fullname": "Haozhe Xie",
        "user": "hzxie",
        "type": "user"
      },
      "summary": "La génération de scènes 3D est un objectif dans des applications telles que les médias de consommation complète, la robotique, la conduite automatique et l'intelligence concrète, qui vise la structure spatiale, le sens et la synthèse d'environnements réalistes. Les méthodes basées sur la séquence de traitement précoce étaient échellables mais limitaient la diversité. Les avancées récentes dans les modèles de génération profonde (comme les GAN et les modèles différentiels) et dans les représentations 3D (comme NeRF et les gaussiennes 3D) ont permis d'apprendre la distribution de scènes réelles, améliorant la fidélité, la diversité et la cohérence des perspectives. Récemment, les problèmes de synthèse d'images ou de vidéo ont été reconstruits pour associer la génération à la réalisme, grâce à la fonctionnalité des modèles différentiels. Cette recherche fournit un résumé systématique des méthodes les plus avancées, les organisant dans quatre paradigmes : génération séquentielle, génération basée sur des réseaux neuronaux 3D, génération basée sur des images et génération basée sur des vidéos. Les bases techniques, les compromis, les résultats représentatifs et les ensembles de données communs, les protocoles d'évaluation et les applications en ligne sont analysés. Enfin, les principales défis liés à la capacité de génération, la représentation 3D, les données et les commentaires sont discutés, et des directions prometteuses comme les modèles de génération de haute fidélité, avec connaissance physique et interaction, et les modèles de reconnaissance d'unités sont mis en avant. Cette revue rassemble les avancées récentes dans le croisement de l'intelligence générative, la vision 3D et l'intelligence concrète, et met en avant des directions prometteuses dans ce croisement. Pour suivre les avancées, le dépôt des projets les plus récents est maintenu : https://github.com/hzxie/Awesome-3D-Scene-Generation.",
      "upvotes": 7,
      "discussionId": "681d7b5be27a030c96a28c29",
      "githubRepo": "https://github.com/hzxie/Awesome-3D-Scene-Generation",
      "ai_keywords": [
        "deep generative models",
        "GANs",
        "diffusion models",
        "NeRF",
        "3D Gaussians",
        "procedural generation",
        "neural 3D-based generation",
        "image-based generation",
        "video-based generation"
      ]
    },
    "publishedAt": "2025-05-08T13:59:54.000Z",
    "title": "3D Scene Generation: A Survey",
    "summary": "3D scene generation seeks to synthesize spatially structured, semantically\nmeaningful, and photorealistic environments for applications such as immersive\nmedia, robotics, autonomous driving, and embodied AI. Early methods based on\nprocedural rules offered scalability but limited diversity. Recent advances in\ndeep generative models (e.g., GANs, diffusion models) and 3D representations\n(e.g., NeRF, 3D Gaussians) have enabled the learning of real-world scene\ndistributions, improving fidelity, diversity, and view consistency. Recent\nadvances like diffusion models bridge 3D scene synthesis and photorealism by\nreframing generation as image or video synthesis problems. This survey provides\na systematic overview of state-of-the-art approaches, organizing them into four\nparadigms: procedural generation, neural 3D-based generation, image-based\ngeneration, and video-based generation. We analyze their technical foundations,\ntrade-offs, and representative results, and review commonly used datasets,\nevaluation protocols, and downstream applications. We conclude by discussing\nkey challenges in generation capacity, 3D representation, data and annotations,\nand evaluation, and outline promising directions including higher fidelity,\nphysics-aware and interactive generation, and unified perception-generation\nmodels. This review organizes recent advances in 3D scene generation and\nhighlights promising directions at the intersection of generative AI, 3D\nvision, and embodied intelligence. To track ongoing developments, we maintain\nan up-to-date project page:\nhttps://github.com/hzxie/Awesome-3D-Scene-Generation.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63f47b5321eb234ab739e91a/PqIosa0nVWamlNxCmd6w1.webp"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.05474.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63f47b5321eb234ab739e91a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f47b5321eb234ab739e91a/vWfFNVtMkHl8gieha5PPd.jpeg",
      "fullname": "Haozhe Xie",
      "name": "hzxie",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.05327",
      "authors": [
        {
          "_id": "681d95bc11abe59dc97e4c5a",
          "user": {
            "_id": "647e99d9becb41a272970ca4",
            "avatarUrl": "/avatars/291831643937a298c5903c6dc037b950.svg",
            "isPro": false,
            "fullname": "Ann",
            "user": "yyxsghx",
            "type": "user"
          },
          "name": "Yixin Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-09T07:21:18.753Z",
          "hidden": false
        },
        {
          "_id": "681d95bc11abe59dc97e4c5b",
          "user": {
            "_id": "670740744341dcee459fb990",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/66UkZvrAk7fQr5YCylEFk.png",
            "isPro": false,
            "fullname": "Qingxiu Dong",
            "user": "Rsy24",
            "type": "user"
          },
          "name": "Qingxiu Dong",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:57:38.844Z",
          "hidden": false
        },
        {
          "_id": "681d95bc11abe59dc97e4c5c",
          "user": {
            "_id": "655ca347f426a304c6b393a1",
            "avatarUrl": "/avatars/67f0310d59c5912d38c2ad8e6448614d.svg",
            "isPro": false,
            "fullname": "Linli Yao",
            "user": "yaolily",
            "type": "user"
          },
          "name": "Linli Yao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:57:47.561Z",
          "hidden": false
        },
        {
          "_id": "681d95bc11abe59dc97e4c5d",
          "user": {
            "_id": "654cca3fe1b4cd6d40d5a7ae",
            "avatarUrl": "/avatars/4d09531277e16ad71474fc888c16b227.svg",
            "isPro": false,
            "fullname": "Fangwei Zhu",
            "user": "soliz1998",
            "type": "user"
          },
          "name": "Fangwei Zhu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:57:53.796Z",
          "hidden": false
        },
        {
          "_id": "681d95bc11abe59dc97e4c5e",
          "name": "Zhifang Sui",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-08T15:17:37.000Z",
      "submittedOnDailyAt": "2025-05-09T07:13:58.961Z",
      "title": "ICon : Contribution de la sélection automatique de données dans le contexte",
      "submittedOnDailyBy": {
        "_id": "647e99d9becb41a272970ca4",
        "avatarUrl": "/avatars/291831643937a298c5903c6dc037b950.svg",
        "isPro": false,
        "fullname": "Ann",
        "user": "yyxsghx",
        "type": "user"
      },
      "summary": "La sélection des données est essentielle pour améliorer le rendement et réduire les coûts d'entraînement de modèles de langage grands (LLMs). Cependant, les méthodes automatisées actuelles dépendent de métriques de gradiente basées sur des calculs coûteux ou d'heuristiques manuellement conçues, ce qui ne permet pas d'utiliser complètement les caractéristiques uniques du jeu de données. Dans cet article, nous proposons une nouvelle méthodologie sans gradient appelée \"Apprentissage en contexte pour la mesure de la contribution\" (ICon). Cette méthodologie exploite les potentiels ajustements d'apprentissage en contexte (ICL) pour évaluer la contribution des échantillons de manière à ce qu'il n'y ait pas besoin de calculer les gradients ni de concevoir manuellement les heuristiques. ICon est plus efficace en termes de calcul que les méthodes basées sur les gradients et réduit le biais d'inférence humain par rapport aux approches basées sur des heuristiques. ICon est composé de trois composants : l'évaluation du changement du rendement par apprentissage en contexte pour identifier les données de haute contribution. Nous avons démontré l'efficacité de ICon dans des expériences étendues avec 3 modèles LLMs, 12 benchmarks et 5 paires de jeux de données, où le modèle entraîné avec 15% des données sélectionnées par ICon dans LLaMA3.1-8B a dépassé le rendement du modèle entraîné avec le jeu de données complet en 5,42% de points et le meilleur rendement obtenu avec des méthodes communes en 2,06% de points. De plus, nous avons effectué un analyse des échantillons de haute contribution sélectionnés par ICon, qui montrent une variété de tâches et de niveaux de difficulté, et sont les seuls capables de réaliser des tâches impossibles pour les autres.",
      "upvotes": 7,
      "discussionId": "681d95bd11abe59dc97e4c87",
      "ai_keywords": [
        "in-context learning (ICL)",
        "implicit fine-tuning",
        "In-context Learning for Contribution Measurement (ICon)"
      ]
    },
    "publishedAt": "2025-05-08T11:17:37.000Z",
    "title": "ICon: In-Context Contribution for Automatic Data Selection",
    "summary": "Data selection for instruction tuning is essential for improving the\nperformance of Large Language Models (LLMs) and reducing training cost.\nHowever, existing automated selection methods either depend on computationally\nexpensive gradient-based measures or manually designed heuristics, which may\nfail to fully exploit the intrinsic attributes of data. In this paper, we\npropose In-context Learning for Contribution Measurement (ICon), a novel\ngradient-free method that takes advantage of the implicit fine-tuning nature of\nin-context learning (ICL) to measure sample contribution without gradient\ncomputation or manual indicators engineering. ICon offers a computationally\nefficient alternative to gradient-based methods and reduces human inductive\nbias inherent in heuristic-based approaches. ICon comprises three components\nand identifies high-contribution data by assessing performance shifts under\nimplicit learning through ICL. Extensive experiments on three LLMs across 12\nbenchmarks and 5 pairwise evaluation sets demonstrate the effectiveness of\nICon. Remarkably, on LLaMA3.1-8B, models trained on 15% of ICon-selected data\noutperform full datasets by 5.42% points and exceed the best performance of\nwidely used selection methods by 2.06% points. We further analyze\nhigh-contribution samples selected by ICon, which show both diverse tasks and\nappropriate difficulty levels, rather than just the hardest ones.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.05327.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "647e99d9becb41a272970ca4",
      "avatarUrl": "/avatars/291831643937a298c5903c6dc037b950.svg",
      "fullname": "Ann",
      "name": "yyxsghx",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.03981",
      "authors": [
        {
          "_id": "681d7ee755699177c7fb636a",
          "user": {
            "_id": "617e7729129c9e67703ffe61",
            "avatarUrl": "/avatars/f47ee9f2f0e2b1075bebf3682ee2f817.svg",
            "isPro": false,
            "fullname": "qianchu liu",
            "user": "qianchu",
            "type": "user"
          },
          "name": "Qianchu Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:59:18.501Z",
          "hidden": false
        },
        {
          "_id": "681d7ee755699177c7fb636b",
          "user": {
            "_id": "6234c11b7d5de9839bc44163",
            "avatarUrl": "/avatars/3ca569596f9c7134e8d4b560a06ee1e7.svg",
            "isPro": false,
            "fullname": "Sheng Zhang",
            "user": "shengz",
            "type": "user"
          },
          "name": "Sheng Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-09T07:21:22.974Z",
          "hidden": false
        },
        {
          "_id": "681d7ee755699177c7fb636c",
          "user": {
            "_id": "64b8e41d52b7353d8c6dd38f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/IAItP4FvD6JX9s1jwnQwF.png",
            "isPro": false,
            "fullname": "Guanghui Qin",
            "user": "hiaoxui",
            "type": "user"
          },
          "name": "Guanghui Qin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:59:24.320Z",
          "hidden": false
        },
        {
          "_id": "681d7ee755699177c7fb636d",
          "name": "Timothy Ossowski",
          "hidden": false
        },
        {
          "_id": "681d7ee755699177c7fb636e",
          "name": "Yu Gu",
          "hidden": false
        },
        {
          "_id": "681d7ee755699177c7fb636f",
          "name": "Ying Jin",
          "hidden": false
        },
        {
          "_id": "681d7ee755699177c7fb6370",
          "user": {
            "_id": "627bd86f7e62b4bf5c367108",
            "avatarUrl": "/avatars/4e87eea02d51680ebac7992dfe527e07.svg",
            "isPro": false,
            "fullname": "Sid Kiblawi",
            "user": "sidkiblawi",
            "type": "user"
          },
          "name": "Sid Kiblawi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:59:37.798Z",
          "hidden": false
        },
        {
          "_id": "681d7ee755699177c7fb6371",
          "user": {
            "_id": "65a13da85dce70a3025b7534",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/zSPDBEGIULEYN4P7JCdyC.png",
            "isPro": false,
            "fullname": "Sam Preston",
            "user": "RustyArchimedes",
            "type": "user"
          },
          "name": "Sam Preston",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:59:43.654Z",
          "hidden": false
        },
        {
          "_id": "681d7ee755699177c7fb6372",
          "name": "Mu Wei",
          "hidden": false
        },
        {
          "_id": "681d7ee755699177c7fb6373",
          "user": {
            "_id": "6797f24ded1557b14d708541",
            "avatarUrl": "/avatars/d69ac80a9a500764766ce9ac7d549cc2.svg",
            "isPro": false,
            "fullname": "Paul Vozila",
            "user": "Paulvozila",
            "type": "user"
          },
          "name": "Paul Vozila",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:00:01.673Z",
          "hidden": false
        },
        {
          "_id": "681d7ee755699177c7fb6374",
          "user": {
            "_id": "5e5870466bc35159a08ca572",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5e5870466bc35159a08ca572/pT6gEEs8RLRJGeM-faNWj.jpeg",
            "isPro": false,
            "fullname": "Tristan Naumann",
            "user": "tnaumann",
            "type": "user"
          },
          "name": "Tristan Naumann",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:00:15.476Z",
          "hidden": false
        },
        {
          "_id": "681d7ee755699177c7fb6375",
          "user": {
            "_id": "664d07456083f276c4feb1a4",
            "avatarUrl": "/avatars/1bfa6d8f82e9223b47630cefd79d7d0e.svg",
            "isPro": false,
            "fullname": "Hoifung Poon",
            "user": "hoifung",
            "type": "user"
          },
          "name": "Hoifung Poon",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:00:22.468Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-06T21:08:27.000Z",
      "submittedOnDailyAt": "2025-05-09T02:41:23.534Z",
      "title": "X-Reasoner : Direction générale pour la logique générale dans divers modèles et domaines.",
      "submittedOnDailyBy": {
        "_id": "6234c11b7d5de9839bc44163",
        "avatarUrl": "/avatars/3ca569596f9c7134e8d4b560a06ee1e7.svg",
        "isPro": false,
        "fullname": "Sheng Zhang",
        "user": "shengz",
        "type": "user"
      },
      "summary": "Les modèles de propriété récents (par exemple, o3) développent une capacité logique multimodal très puissante. Cependant, les études actuelles ouvertes se concentrent principalement sur l'entraînement de modèles logiques avec des textes de paragraphes et l'évaluation de ces modèles principalement dans des tâches mathématiques et générales. Par conséquent, aucun méthode efficace n'a été clairement identifiée pour élargir les capacités logiques générales qui dépassent les paragraphes et les domaines généraux. Dans cet article, nous explorons les problèmes fondamentaux : est-la la logique généralisable tant dans le modèle que dans le domaine ? Notre résultat soutient une réponse positive : la programmation basée sur des paragraphes dans des domaines généraux permet cette logique généralisable forte. En utilisant cette découverte, nous présentons X-Reasoner. X-Reasoner est un modèle de langage visuel programmé uniquement pour des paragraphes dans des domaines généraux, qui effectue une logique généralisable en deux étapes : d'abord, l'apprentissage d'expériences dans le domaine final, puis l'entraînement final par apprentissage par récompense qui évalue la récompense. Les expériences montrent que X-Reasoner dépasse les modèles actuels en logique visuelle et dans des domaines généraux, et dépasse les meilleurs modèles sur les benchmarks généraux et médicaux (figure 1). De plus, le rendement de X-Reasoner dans des domaines spécifiques est amélioré par un entraînement continu avec des données de paragraphes propres. Par conséquent, nous présentons X-Reasoner-Med. X-Reasoner-Med dépasse les modèles généraux et visuels entraînés, en atteignant un nouveau modèle optimal avec des données d'entraînement.",
      "upvotes": 5,
      "discussionId": "681d7ee855699177c7fb63b7",
      "projectPage": "https://github.com/microsoft/x-reasoner",
      "ai_keywords": [
        "multimodal reasoning",
        "vision-language model",
        "post-training",
        "long chain-of-thoughts",
        "reinforcement learning",
        "verifiable rewards",
        "X-Reasoner",
        "out-of-domain settings",
        "X-Reasoner-Med"
      ]
    },
    "publishedAt": "2025-05-06T17:08:27.000Z",
    "title": "X-Reasoner: Towards Generalizable Reasoning Across Modalities and\n  Domains",
    "summary": "Recent proprietary models (e.g., o3) have begun to demonstrate strong\nmultimodal reasoning capabilities. Yet, most existing open-source research\nconcentrates on training text-only reasoning models, with evaluations limited\nto mainly mathematical and general-domain tasks. Therefore, it remains unclear\nhow to effectively extend reasoning capabilities beyond text input and general\ndomains. This paper explores a fundamental research question: Is reasoning\ngeneralizable across modalities and domains? Our findings support an\naffirmative answer: General-domain text-based post-training can enable such\nstrong generalizable reasoning. Leveraging this finding, we introduce\nX-Reasoner, a vision-language model post-trained solely on general-domain text\nfor generalizable reasoning, using a two-stage approach: an initial supervised\nfine-tuning phase with distilled long chain-of-thoughts, followed by\nreinforcement learning with verifiable rewards. Experiments show that\nX-Reasoner successfully transfers reasoning capabilities to both multimodal and\nout-of-domain settings, outperforming existing state-of-the-art models trained\nwith in-domain and multimodal data across various general and medical\nbenchmarks (Figure 1). Additionally, we find that X-Reasoner's performance in\nspecialized domains can be further enhanced through continued training on\ndomain-specific text-only data. Building upon this, we introduce\nX-Reasoner-Med, a medical-specialized variant that achieves new state of the\nart on numerous text-only and multimodal medical benchmarks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03981.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6234c11b7d5de9839bc44163",
      "avatarUrl": "/avatars/3ca569596f9c7134e8d4b560a06ee1e7.svg",
      "fullname": "Sheng Zhang",
      "name": "shengz",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.05467",
      "authors": [
        {
          "_id": "681d95b5c7ae5f65b0e55ff9",
          "user": {
            "_id": "63fee47352441fe3e87b5088",
            "avatarUrl": "/avatars/c1df1899e3925aa6fdfc8ee0049fa8a7.svg",
            "isPro": false,
            "fullname": "WANG HAIBO",
            "user": "WHB139426",
            "type": "user"
          },
          "name": "Haibo Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-09T07:21:20.829Z",
          "hidden": false
        },
        {
          "_id": "681d95b5c7ae5f65b0e55ffa",
          "name": "Bo Feng",
          "hidden": false
        },
        {
          "_id": "681d95b5c7ae5f65b0e55ffb",
          "user": {
            "_id": "66b5295f83425904fa7a1a6a",
            "avatarUrl": "/avatars/a35568fb933ceef7451bd88fb3d5ab17.svg",
            "isPro": false,
            "fullname": "Zhengfeng Lai",
            "user": "jefflai",
            "type": "user"
          },
          "name": "Zhengfeng Lai",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:00:45.264Z",
          "hidden": false
        },
        {
          "_id": "681d95b5c7ae5f65b0e55ffc",
          "name": "Mingze Xu",
          "hidden": false
        },
        {
          "_id": "681d95b5c7ae5f65b0e55ffd",
          "user": {
            "_id": "67fa856547b40f55b7ff3ce5",
            "avatarUrl": "/avatars/745937497772e9b533ba7940d758d30d.svg",
            "isPro": false,
            "fullname": "Shiyu Li",
            "user": "ShiyuLi",
            "type": "user"
          },
          "name": "Shiyu Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:01:08.366Z",
          "hidden": false
        },
        {
          "_id": "681d95b5c7ae5f65b0e55ffe",
          "name": "Weifeng Ge",
          "hidden": false
        },
        {
          "_id": "681d95b5c7ae5f65b0e55fff",
          "user": {
            "_id": "66fc2377516eaf950d4b8209",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/mcUxUxXy18Gv9KvCW23s0.png",
            "isPro": false,
            "fullname": "Afshin Dehghan",
            "user": "afshindn",
            "type": "user"
          },
          "name": "Afshin Dehghan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:01:21.716Z",
          "hidden": false
        },
        {
          "_id": "681d95b5c7ae5f65b0e56000",
          "name": "Meng Cao",
          "hidden": false
        },
        {
          "_id": "681d95b5c7ae5f65b0e56001",
          "name": "Ping Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-08T17:57:40.000Z",
      "submittedOnDailyAt": "2025-05-09T04:13:07.003Z",
      "title": "Streaming Bridge : Converting des Vidéos Régionales en Modèles de Langue pour des Assistants de Streaming Actifs",
      "submittedOnDailyBy": {
        "_id": "63fee47352441fe3e87b5088",
        "avatarUrl": "/avatars/c1df1899e3925aa6fdfc8ee0049fa8a7.svg",
        "isPro": false,
        "fullname": "WANG HAIBO",
        "user": "WHB139426",
        "type": "user"
      },
      "summary": "StreamBridge est un cadre simple et efficace. Ce cadre permet facilement transformer les Video-LLMs en modèles dynamiques. StreamBridge résout deux problèmes principaux qui apparaissent lorsque des modèles existants sont appliqués dans des scénarios en ligne : (1) la limitation de la capacité à comprendre des unités de temps multiples et (2) la manque de structures de réponse dynamiques. Spécialement, StreamBridge combine (1) un buffer de mémoire et une stratégie de synthèse harmonique pour soutenir l'interaction de plusieurs tours dans des séquences longues et (2) utilise des modèles actifs légers séparés, conçus pour intégrer facilement avec les Video-LLMs existants, permettant des réponses dynamiques continues. De plus, StreamBridge a construit Stream-IT, un ensemble de données plus grand et adapté pour la compréhension dynamique des vidéos, caractérisé par des séquences de vidéo-texte aléatoires et de différents formats d'instructions. Au travers d'expériences étendues, StreamBridge a notablement amélioré la capacité de compréhension dynamique des Video-LLMs en ligne, démontrant des performances supérieures à ceux des modèles propriétaires comme GPT-4o et Gemini 1.5 Pro. En même temps, il a montré un rendement relativement élevé dans les cadres de référence standards de compréhension des vidéos.",
      "upvotes": 4,
      "discussionId": "681d95b6c7ae5f65b0e5606c",
      "ai_keywords": [
        "Video-LLMs",
        "streaming-capable models",
        "multi-turn real-time understanding",
        "proactive response mechanisms",
        "memory buffer",
        "round-decayed compression strategy",
        "long-context multi-turn interactions",
        "decoupled activation model",
        "Stream-IT",
        "interleaved video-text sequences",
        "standard video understanding benchmarks",
        "GPT-4o",
        "Gemini 1.5 Pro"
      ]
    },
    "publishedAt": "2025-05-08T13:57:40.000Z",
    "title": "StreamBridge: Turning Your Offline Video Large Language Model into a\n  Proactive Streaming Assistant",
    "summary": "We present StreamBridge, a simple yet effective framework that seamlessly\ntransforms offline Video-LLMs into streaming-capable models. It addresses two\nfundamental challenges in adapting existing models into online scenarios: (1)\nlimited capability for multi-turn real-time understanding, and (2) lack of\nproactive response mechanisms. Specifically, StreamBridge incorporates (1) a\nmemory buffer combined with a round-decayed compression strategy, supporting\nlong-context multi-turn interactions, and (2) a decoupled, lightweight\nactivation model that can be effortlessly integrated into existing Video-LLMs,\nenabling continuous proactive responses. To further support StreamBridge, we\nconstruct Stream-IT, a large-scale dataset tailored for streaming video\nunderstanding, featuring interleaved video-text sequences and diverse\ninstruction formats. Extensive experiments show that StreamBridge significantly\nimproves the streaming understanding capabilities of offline Video-LLMs across\nvarious tasks, outperforming even proprietary models such as GPT-4o and Gemini\n1.5 Pro. Simultaneously, it achieves competitive or superior performance on\nstandard video understanding benchmarks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.05467.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63fee47352441fe3e87b5088",
      "avatarUrl": "/avatars/c1df1899e3925aa6fdfc8ee0049fa8a7.svg",
      "fullname": "WANG HAIBO",
      "name": "WHB139426",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.05469",
      "authors": [
        {
          "_id": "681db3a8a9286b53a51dc77b",
          "user": {
            "_id": "672403d5f328a3e6638331ee",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/TXr0SKWI-z-6FvUXTNWXT.jpeg",
            "isPro": false,
            "fullname": "Ava Pun",
            "user": "AvaLovelace",
            "type": "user"
          },
          "name": "Ava Pun",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:06:33.422Z",
          "hidden": false
        },
        {
          "_id": "681db3a8a9286b53a51dc77c",
          "user": {
            "_id": "645d34ecce72244df7b29317",
            "avatarUrl": "/avatars/1248933d9f89a15e67086325a8322d5e.svg",
            "isPro": false,
            "fullname": "Kangle Deng",
            "user": "kangled",
            "type": "user"
          },
          "name": "Kangle Deng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:06:39.736Z",
          "hidden": false
        },
        {
          "_id": "681db3a8a9286b53a51dc77d",
          "user": {
            "_id": "658b307b75ddc76f9dc747ca",
            "avatarUrl": "/avatars/fc5393dc0bb33a8c0fea3a6f79640386.svg",
            "isPro": false,
            "fullname": "Ruixuan Liu",
            "user": "RLCMU",
            "type": "user"
          },
          "name": "Ruixuan Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:06:45.442Z",
          "hidden": false
        },
        {
          "_id": "681db3a8a9286b53a51dc77e",
          "user": {
            "_id": "6337151b0267ebcf02640eb6",
            "avatarUrl": "/avatars/14a723cafc5587043bdfb19304fc202d.svg",
            "isPro": false,
            "fullname": "Deva Ramanan",
            "user": "devakramanan",
            "type": "user"
          },
          "name": "Deva Ramanan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:06:52.637Z",
          "hidden": false
        },
        {
          "_id": "681db3a8a9286b53a51dc77f",
          "name": "Changliu Liu",
          "hidden": false
        },
        {
          "_id": "681db3a8a9286b53a51dc780",
          "user": {
            "_id": "63a0acc32fabbbb899952a2b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1671474335794-noauth.jpeg",
            "isPro": false,
            "fullname": "Jun-Yan Zhu",
            "user": "junyanz",
            "type": "user"
          },
          "name": "Jun-Yan Zhu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:07:07.095Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-08T17:58:18.000Z",
      "submittedOnDailyAt": "2025-05-09T06:30:13.597Z",
      "title": "Dessins physiquement stables et constructibles générés à partir du texte",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "REGOGPT présente le premier approche pour générer un modèle de LEGO physiquement stable à partir d'une phrase. Pour y parvenir, il construit un grand ensemble de données physiquement stable de designs de LEGO, et entraîne un grand modèle de langage auto-corrigeant en prédisant les tokens suivants ainsi que des captures liées. Pour améliorer la stabilité des designs, il utilise une vérification efficace de validité et un rétrocours basé sur des connaissances physiques lors de l'inférence de l'auto-corrigeant, et élimine les prédictions de tokens imprévisibles en utilisant les lois physiques et les contraintes d'assemblage. Les expériences montrent que REGOGPT génère des designs stables, divers et élégants de LEGO proches du texte d'entrée. De plus, il développe un méthode pour générer du texte permettant l'auto-assemblage des designs de LEGO par des personnes ou des mains de robot. Il lance également un nouvel ensemble de données \"StableText2Lego\", qui comprend plus de 47 000 structures de LEGO et plus de 28 000 objets 3D, ainsi que des captures détaillées et des codes disponibles sur le site web du projet.",
      "upvotes": 3,
      "discussionId": "681db3aca9286b53a51dc875",
      "ai_keywords": [
        "autoregressive large language model",
        "next-token prediction",
        "validity check",
        "physics-aware rollback",
        "autoregressive inference",
        "physics laws",
        "assembly constraints",
        "text-based LEGO texturing method",
        "automatic assembly",
        "robotic arms"
      ]
    },
    "publishedAt": "2025-05-08T13:58:18.000Z",
    "title": "Generating Physically Stable and Buildable LEGO Designs from Text",
    "summary": "We introduce LegoGPT, the first approach for generating physically stable\nLEGO brick models from text prompts. To achieve this, we construct a\nlarge-scale, physically stable dataset of LEGO designs, along with their\nassociated captions, and train an autoregressive large language model to\npredict the next brick to add via next-token prediction. To improve the\nstability of the resulting designs, we employ an efficient validity check and\nphysics-aware rollback during autoregressive inference, which prunes infeasible\ntoken predictions using physics laws and assembly constraints. Our experiments\nshow that LegoGPT produces stable, diverse, and aesthetically pleasing LEGO\ndesigns that align closely with the input text prompts. We also develop a\ntext-based LEGO texturing method to generate colored and textured designs. We\nshow that our designs can be assembled manually by humans and automatically by\nrobotic arms. We also release our new dataset, StableText2Lego, containing over\n47,000 LEGO structures of over 28,000 unique 3D objects accompanied by detailed\ncaptions, along with our code and models at the project website:\nhttps://avalovelace1.github.io/LegoGPT/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.05469.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6796
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.05408",
      "authors": [
        {
          "_id": "681daba2e3775056736651ce",
          "user": {
            "_id": "61424bf4f0d914a5f606a823",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61424bf4f0d914a5f606a823/0td8lR4elBaVvJUD9Pojh.png",
            "isPro": false,
            "fullname": "Yong Zheng-Xin",
            "user": "yongzx",
            "type": "user"
          },
          "name": "Zheng-Xin Yong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-09T07:20:45.630Z",
          "hidden": false
        },
        {
          "_id": "681daba2e3775056736651cf",
          "name": "M. Farid Adilazuarda",
          "hidden": false
        },
        {
          "_id": "681daba2e3775056736651d0",
          "user": {
            "_id": "6509feb92257a3afbaeecfea",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6509feb92257a3afbaeecfea/a_UbA-2WtZeLTf0ugVzSh.jpeg",
            "isPro": false,
            "fullname": "Jonibek Mansurov",
            "user": "MJonibek",
            "type": "user"
          },
          "name": "Jonibek Mansurov",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:03:39.843Z",
          "hidden": false
        },
        {
          "_id": "681daba2e3775056736651d1",
          "name": "Ruochen Zhang",
          "hidden": false
        },
        {
          "_id": "681daba2e3775056736651d2",
          "user": {
            "_id": "5f1eb362eec0ad2a071ad6e2",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5f1eb362eec0ad2a071ad6e2/IXMYkYKuTwn6kBdWnQeeY.png",
            "isPro": false,
            "fullname": "Niklas Muennighoff",
            "user": "Muennighoff",
            "type": "user"
          },
          "name": "Niklas Muennighoff",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:04:00.900Z",
          "hidden": false
        },
        {
          "_id": "681daba2e3775056736651d3",
          "name": "Carsten Eickhoff",
          "hidden": false
        },
        {
          "_id": "681daba2e3775056736651d4",
          "user": {
            "_id": "5f5c4b20e56d546cd6233098",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1637813888895-5f5c4b20e56d546cd6233098.jpeg",
            "isPro": false,
            "fullname": "Genta Indra Winata",
            "user": "gentaiscool",
            "type": "user"
          },
          "name": "Genta Indra Winata",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:04:13.878Z",
          "hidden": false
        },
        {
          "_id": "681daba2e3775056736651d5",
          "user": {
            "_id": "6544e43b12da508864c38f96",
            "avatarUrl": "/avatars/76f0cd55b4bf9c03d2686e146c6f795f.svg",
            "isPro": false,
            "fullname": "Julia Kreutzer",
            "user": "JuliaKreutzerCohere",
            "type": "user"
          },
          "name": "Julia Kreutzer",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:04:29.257Z",
          "hidden": false
        },
        {
          "_id": "681daba2e3775056736651d6",
          "name": "Stephen H. Bach",
          "hidden": false
        },
        {
          "_id": "681daba2e3775056736651d7",
          "name": "Alham Fikri Aji",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-08T16:50:06.000Z",
      "submittedOnDailyAt": "2025-05-09T05:46:57.523Z",
      "title": "La logique des langages chroniques est réalisée par l'accroissement dans le processus de vérification.",
      "submittedOnDailyBy": {
        "_id": "61424bf4f0d914a5f606a823",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61424bf4f0d914a5f606a823/0td8lR4elBaVvJUD9Pojh.png",
        "isPro": false,
        "fullname": "Yong Zheng-Xin",
        "user": "yongzx",
        "type": "user"
      },
      "summary": "Le pouvoir logique des modèles de langue générale est principalement étudié en anglais et a étendu ses applications en fonction de la diversité linguistique et de l'utilisation de modèles prétrainés. Dans cette étude, on examine dans quelle mesure la régulation logique de longueur CoT en anglais se généralise dans d'autres langues. Tout d'abord, on a étendu le calcul d'inférence des modèles de langue logique centrés sur l'anglais (RLM) pour montrer des résultats exceptionnels en logique mathématique et dans diverses langues, en particulier dans les langues à faible ressource. De plus, les CoTs centrés sur l'anglais occupent naturellement un rôle principal, mais lorsque l'anglais est traité logiquement à partir d'entrées non anglaises, des motifs continus de pensée sont observés qui sont associés à des références. De plus, une stratégie efficace a été découverte pour contrôler les longues CoTs, ce qui permet aux modèles de traiter de manière plus efficace en langues riches en ressources. Enfin, en particulier, la généralisation logique vers le savoir culturel commun dans les domaines de la science, de la technologie, de la mathématique (STEM) est insuffisante, et ce phénomène est également observé en anglais. En résumé, on présente la possibilité, la structure et les limites de la généralisation logique entre langues dans des tests en anglais, on recommande l'utilisation de RLM centrés sur l'anglais pour le traitement logique en langues riches en ressources, et on conclut que les RLM doivent évoluer pour améliorer le traitement logique en langues à faible ressource et dans des situations étrangères.",
      "upvotes": 3,
      "discussionId": "681daba2e3775056736651f9",
      "ai_keywords": [
        "reasoning language models (RLMs)",
        "long chain-of-thoughts (CoTs)",
        "multilingual mathematical reasoning",
        "low-resource languages",
        "quote-and-think pattern",
        "scaling up inference compute",
        "high-resource languages",
        "out-of-domain reasoning generalization",
        "STEM",
        "cultural commonsense knowledge",
        "crosslingual generalization",
        "test-time scaling"
      ]
    },
    "publishedAt": "2025-05-08T12:50:06.000Z",
    "title": "Crosslingual Reasoning through Test-Time Scaling",
    "summary": "Reasoning capabilities of large language models are primarily studied for\nEnglish, even when pretrained models are multilingual. In this work, we\ninvestigate to what extent English reasoning finetuning with long\nchain-of-thoughts (CoTs) can generalize across languages. First, we find that\nscaling up inference compute for English-centric reasoning language models\n(RLMs) improves multilingual mathematical reasoning across many languages\nincluding low-resource languages, to an extent where they outperform models\ntwice their size. Second, we reveal that while English-centric RLM's CoTs are\nnaturally predominantly English, they consistently follow a quote-and-think\npattern to reason about quoted non-English inputs. Third, we discover an\neffective strategy to control the language of long CoT reasoning, and we\nobserve that models reason better and more efficiently in high-resource\nlanguages. Finally, we observe poor out-of-domain reasoning generalization, in\nparticular from STEM to cultural commonsense knowledge, even for English.\nOverall, we demonstrate the potentials, study the mechanisms and outline the\nlimitations of crosslingual generalization of English reasoning test-time\nscaling. We conclude that practitioners should let English-centric RLMs reason\nin high-resource languages, while further work is needed to improve reasoning\nin low-resource languages and out-of-domain contexts.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.05408.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "61424bf4f0d914a5f606a823",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61424bf4f0d914a5f606a823/0td8lR4elBaVvJUD9Pojh.png",
      "fullname": "Yong Zheng-Xin",
      "name": "yongzx",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.19314",
      "authors": [
        {
          "_id": "681d89e6d025518b321f67ce",
          "user": {
            "_id": "6673cf668d570d59b83511cc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/g9sJViXr1Cmx8A_is7Zgq.jpeg",
            "isPro": false,
            "fullname": "Peilin Zhou",
            "user": "PALIN2018",
            "type": "user"
          },
          "name": "Peilin Zhou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:01:42.666Z",
          "hidden": false
        },
        {
          "_id": "681d89e6d025518b321f67cf",
          "name": "Bruce Leon",
          "hidden": false
        },
        {
          "_id": "681d89e6d025518b321f67d0",
          "user": {
            "_id": "64489ca21d52a633c8f55aba",
            "avatarUrl": "/avatars/5199a5e93161c61d14ec13f79dd8c2c5.svg",
            "isPro": false,
            "fullname": "Xiang Ying",
            "user": "MindYing",
            "type": "user"
          },
          "name": "Xiang Ying",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-09T04:51:51.507Z",
          "hidden": false
        },
        {
          "_id": "681d89e6d025518b321f67d1",
          "name": "Can Zhang",
          "hidden": false
        },
        {
          "_id": "681d89e6d025518b321f67d2",
          "name": "Yifan Shao",
          "hidden": false
        },
        {
          "_id": "681d89e6d025518b321f67d3",
          "user": {
            "_id": "636dfa6193d9a0c987d41b73",
            "avatarUrl": "/avatars/14396c8beb376b0d3c27a23fadaeb15e.svg",
            "isPro": false,
            "fullname": "Qichen YE",
            "user": "yeeeqichen99",
            "type": "user"
          },
          "name": "Qichen Ye",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:02:33.873Z",
          "hidden": false
        },
        {
          "_id": "681d89e6d025518b321f67d4",
          "name": "Dading Chong",
          "hidden": false
        },
        {
          "_id": "681d89e6d025518b321f67d5",
          "user": {
            "_id": "66bc683f432c73a183ef787c",
            "avatarUrl": "/avatars/9505a1e6131093a91d0454e50bcbba00.svg",
            "isPro": false,
            "fullname": "Zhiling Jin",
            "user": "HawkFaust",
            "type": "user"
          },
          "name": "Zhiling Jin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:02:48.180Z",
          "hidden": false
        },
        {
          "_id": "681d89e6d025518b321f67d6",
          "name": "Chenxuan Xie",
          "hidden": false
        },
        {
          "_id": "681d89e6d025518b321f67d7",
          "name": "Meng Cao",
          "hidden": false
        },
        {
          "_id": "681d89e6d025518b321f67d8",
          "name": "Yuxin Gu",
          "hidden": false
        },
        {
          "_id": "681d89e6d025518b321f67d9",
          "name": "Sixin Hong",
          "hidden": false
        },
        {
          "_id": "681d89e6d025518b321f67da",
          "name": "Jing Ren",
          "hidden": false
        },
        {
          "_id": "681d89e6d025518b321f67db",
          "name": "Jian Chen",
          "hidden": false
        },
        {
          "_id": "681d89e6d025518b321f67dc",
          "name": "Chao Liu",
          "hidden": false
        },
        {
          "_id": "681d89e6d025518b321f67dd",
          "name": "Yining Hua",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6673cf668d570d59b83511cc/NJsFys1PlInj9W3E7pD8t.png",
        "https://cdn-uploads.huggingface.co/production/uploads/6673cf668d570d59b83511cc/gJJ8b6rc6njgCz7Fepbbd.png"
      ],
      "publishedAt": "2025-04-27T17:32:43.000Z",
      "submittedOnDailyAt": "2025-05-09T03:24:22.739Z",
      "title": "Broadway Competition-ZH : Benchmark de Broadway du Modèle de Langage Chinois Grand",
      "submittedOnDailyBy": {
        "_id": "6673cf668d570d59b83511cc",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/g9sJViXr1Cmx8A_is7Zgq.jpeg",
        "isPro": false,
        "fullname": "Peilin Zhou",
        "user": "PALIN2018",
        "type": "user"
      },
      "summary": "Le programme retourne des réponses traduites dans le langage spécifié par l'utilisateur. Dans ce cas, la traduction de l'anglais au japonais serait :\n\n「Quand les outils d'utilisation de l'IA évoluent vers l'évolution des agents, la capacité d'explorer le web dans des intervalles de temps comme un navigateur est un indicateur crucial pour évaluer leurs habiletés logiques et de recherche. Un exemple actuel de cadre de référence est BrowseComp, qui se concentre principalement sur l'anglais et ignore la complexité linguistique, infrastructurelle et relationnelle d'autres écosystèmes de l'information principales. Pour corriger cela, on présente BrowseComp-ZH. BrowseComp-ZH est un cadre de référence de haut niveau développé pour évaluer les agents d'IA sur le Web chinois. BrowseComp-ZH comprend 289 questions à plusieurs étapes dans 11 domaines, où chaque question est conçue pour obtenir des réponses courtes, objectifs et facilement vérifiables (par exemple, dates, nombres, noms propres). Des protocoles de gestion de qualité sont appliqués à deux étapes, cherchant des réponses de haute qualité et de diversité. Dans le cadre de référence de BrowseComp-ZH, plus de 20 modèles de langage et de systèmes de recherche d'agents ont été évalués. La plupart des modèles ont des fortes habiletés de conversation et de recherche, mais nécessitent un entraînement strict : la précision de beaucoup de modèles est inférieure à 10% et certains dépassent le 20%. Le meilleur système, DeepResearch de OpenAI, atteint exactement 42,9%. Ces résultats montrent la haute difficulté de BrowseComp-ZH et montrent que pour le succès, il faut non seulement une stratégie de recherche efficace, mais également la capacité d'intégrer une logique complexe et de l'information. Ce dataset, les guides de conception et les résultats du cadre de référence sont publiés sur https://github.com/PALIN2018/BrowseComp-ZH.」",
      "upvotes": 3,
      "discussionId": "681d89e7d025518b321f6807",
      "githubRepo": "https://github.com/PALIN2018/BrowseComp-ZH",
      "ai_keywords": [
        "tool-using agents",
        "multihop questions",
        "information ecosystems",
        "quality control protocol",
        "reasoning and information reconciliation"
      ]
    },
    "publishedAt": "2025-04-27T13:32:43.000Z",
    "title": "BrowseComp-ZH: Benchmarking Web Browsing Ability of Large Language\n  Models in Chinese",
    "summary": "As large language models (LLMs) evolve into tool-using agents, the ability to\nbrowse the web in real-time has become a critical yardstick for measuring their\nreasoning and retrieval competence. Existing benchmarks such as BrowseComp\nconcentrate on English and overlook the linguistic, infrastructural, and\ncensorship-related complexities of other major information ecosystems -- most\nnotably Chinese. To address this gap, we introduce BrowseComp-ZH, a\nhigh-difficulty benchmark purpose-built to comprehensively evaluate LLM agents\non the Chinese web. BrowseComp-ZH consists of 289 multi-hop questions spanning\n11 diverse domains. Each question is reverse-engineered from a short,\nobjective, and easily verifiable answer (e.g., a date, number, or proper noun).\nA two-stage quality control protocol is applied to strive for high question\ndifficulty and answer uniqueness. We benchmark over 20 state-of-the-art\nlanguage models and agentic search systems on our proposed BrowseComp-ZH.\nDespite their strong conversational and retrieval capabilities, most models\nstruggle severely: a large number achieve accuracy rates below 10%, and only a\nhandful exceed 20%. Even the best-performing system, OpenAI's DeepResearch,\nreaches just 42.9%. These results demonstrate the considerable difficulty of\nBrowseComp-ZH, where success demands not only effective retrieval strategies,\nbut also sophisticated reasoning and information reconciliation -- capabilities\nthat current models still struggle to master. Our dataset, construction\nguidelines, and benchmark results have been publicly released at\nhttps://github.com/PALIN2018/BrowseComp-ZH.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6673cf668d570d59b83511cc/NJsFys1PlInj9W3E7pD8t.png",
      "https://cdn-uploads.huggingface.co/production/uploads/6673cf668d570d59b83511cc/gJJ8b6rc6njgCz7Fepbbd.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.19314.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6673cf668d570d59b83511cc",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/g9sJViXr1Cmx8A_is7Zgq.jpeg",
      "fullname": "Peilin Zhou",
      "name": "PALIN2018",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.05288",
      "authors": [
        {
          "_id": "681d9dd229119d666079b275",
          "user": {
            "_id": "63a3170f8c0c89dcae316858",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a3170f8c0c89dcae316858/aZjVQl1jwnagTDGxalREE.jpeg",
            "isPro": false,
            "fullname": "Ahmed Abdelreheem",
            "user": "Samir55",
            "type": "user"
          },
          "name": "Ahmed Abdelreheem",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-09T07:21:06.667Z",
          "hidden": false
        },
        {
          "_id": "681d9dd229119d666079b276",
          "user": {
            "_id": "66eae6491cd3794eb4cd1992",
            "avatarUrl": "/avatars/5802de373ccc815f68b98b320aa787bf.svg",
            "isPro": false,
            "fullname": "Filippo Aleotti",
            "user": "Filippo8",
            "type": "user"
          },
          "name": "Filippo Aleotti",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:04:46.129Z",
          "hidden": false
        },
        {
          "_id": "681d9dd229119d666079b277",
          "user": {
            "_id": "63166685f5e32157c51fe616",
            "avatarUrl": "/avatars/5b7d8b0e54339d2dc982676af9e4f4fe.svg",
            "isPro": false,
            "fullname": "Jamie Watson",
            "user": "Aileron",
            "type": "user"
          },
          "name": "Jamie Watson",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:04:52.606Z",
          "hidden": false
        },
        {
          "_id": "681d9dd229119d666079b278",
          "user": {
            "_id": "6703aece547acbd64c531b72",
            "avatarUrl": "/avatars/8042f99c6eb2c9b61be8c9b950818b2f.svg",
            "isPro": false,
            "fullname": "Zawar Qureshi",
            "user": "zuluquebec",
            "type": "user"
          },
          "name": "Zawar Qureshi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:04:58.663Z",
          "hidden": false
        },
        {
          "_id": "681d9dd229119d666079b279",
          "user": {
            "_id": "6577f3bacfb2207f11e847bb",
            "avatarUrl": "/avatars/825998cfebc47d8106f633be5ad10964.svg",
            "isPro": false,
            "fullname": "Abdelrahman Eldesokey",
            "user": "abdo-eldesokey",
            "type": "user"
          },
          "name": "Abdelrahman Eldesokey",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:05:05.828Z",
          "hidden": false
        },
        {
          "_id": "681d9dd229119d666079b27a",
          "name": "Peter Wonka",
          "hidden": false
        },
        {
          "_id": "681d9dd229119d666079b27b",
          "name": "Gabriel Brostow",
          "hidden": false
        },
        {
          "_id": "681d9dd229119d666079b27c",
          "user": {
            "_id": "67f517dcbd75b1099bba2857",
            "avatarUrl": "/avatars/a1a25d7972b1857f8bb49bc9efc02ded.svg",
            "isPro": false,
            "fullname": "Sara Vicente",
            "user": "svicente",
            "type": "user"
          },
          "name": "Sara Vicente",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:05:21.990Z",
          "hidden": false
        },
        {
          "_id": "681d9dd229119d666079b27d",
          "name": "Guillermo Garcia-Hernando",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-08T14:29:11.000Z",
      "submittedOnDailyAt": "2025-05-09T04:50:18.526Z",
      "title": "PlaceIt3D : Positionnement d'Objets 3D avec Orientation Guidée par un Schéma 3D Réaliste",
      "submittedOnDailyBy": {
        "_id": "63a3170f8c0c89dcae316858",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a3170f8c0c89dcae316858/aZjVQl1jwnagTDGxalREE.jpeg",
        "isPro": false,
        "fullname": "Ahmed Abdelreheem",
        "user": "Samir55",
        "type": "user"
      },
      "summary": "Nous présentons un nouveau défi : la disposition d'objets dans un scénario 3D guidée par un langage. Notre modèle reçoit en entrée un point de la surface 3D, un contenu 3D et un texte de prompt qui décrit amplement où les contenus 3D doivent être placés. Le défi est de trouver une disposition valide des contenus 3D qui respecte le prompt. Ce travail présente un défi comparé à d'autres travaux de positionnement de contenus dans différents langages : il a plusieurs solutions valides, par conséquent, il est ambigu et nécessite de considérer des relations géométriques 3D et des espaces libres. Nous proposons de mettre en place ce défi avec un nouveau benchmark et protocole d'évaluation. De plus, nous fournissons un nouveau jeu de données pour l'entraînement de modèles 3D LLM et présentons un méthode pour fournir un premier critère non triviaire. Nous croyons que ce défi et le nouveau benchmark seront partie de la série de benchmarks qui évaluent et comparent généralement les modèles 3D LLM.",
      "upvotes": 2,
      "discussionId": "681d9dd529119d666079b348",
      "projectPage": "https://nianticlabs.github.io/placeit3d/",
      "githubRepo": "https://github.com/nianticlabs/placeit3d",
      "ai_keywords": [
        "point cloud",
        "3D asset",
        "textual prompt",
        "3D LLMs",
        "bounding",
        "grounding",
        "3D geometric relationships",
        "free space",
        "evaluation protocol",
        "benchmark"
      ]
    },
    "publishedAt": "2025-05-08T10:29:11.000Z",
    "title": "PlaceIt3D: Language-Guided Object Placement in Real 3D Scenes",
    "summary": "We introduce the novel task of Language-Guided Object Placement in Real 3D\nScenes. Our model is given a 3D scene's point cloud, a 3D asset, and a textual\nprompt broadly describing where the 3D asset should be placed. The task here is\nto find a valid placement for the 3D asset that respects the prompt. Compared\nwith other language-guided localization tasks in 3D scenes such as grounding,\nthis task has specific challenges: it is ambiguous because it has multiple\nvalid solutions, and it requires reasoning about 3D geometric relationships and\nfree space. We inaugurate this task by proposing a new benchmark and evaluation\nprotocol. We also introduce a new dataset for training 3D LLMs on this task, as\nwell as the first method to serve as a non-trivial baseline. We believe that\nthis challenging task and our new benchmark could become part of the suite of\nbenchmarks used to evaluate and compare generalist 3D LLM models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.05288.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a3170f8c0c89dcae316858",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a3170f8c0c89dcae316858/aZjVQl1jwnagTDGxalREE.jpeg",
      "fullname": "Ahmed Abdelreheem",
      "name": "Samir55",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.03422",
      "authors": [
        {
          "_id": "681db58b1f1c39ba8fbe0162",
          "user": {
            "_id": "681db120007a2d4056d25c70",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/XyX4OSVmH0zv9qQjFSMGd.png",
            "isPro": false,
            "fullname": "yepeng liu",
            "user": "pengliu123",
            "type": "user"
          },
          "name": "Yepeng Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-09T08:33:21.362Z",
          "hidden": false
        },
        {
          "_id": "681db58b1f1c39ba8fbe0163",
          "name": "Wenpeng Lai",
          "hidden": false
        },
        {
          "_id": "681db58b1f1c39ba8fbe0164",
          "name": "Zhou Zhao",
          "hidden": false
        },
        {
          "_id": "681db58b1f1c39ba8fbe0165",
          "name": "Yuxuan Xiong",
          "hidden": false
        },
        {
          "_id": "681db58b1f1c39ba8fbe0166",
          "name": "Jinchi Zhu",
          "hidden": false
        },
        {
          "_id": "681db58b1f1c39ba8fbe0167",
          "name": "Jun Cheng",
          "hidden": false
        },
        {
          "_id": "681db58b1f1c39ba8fbe0168",
          "name": "Yongchao Xu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-06T10:59:23.000Z",
      "submittedOnDailyAt": "2025-05-09T08:38:34.519Z",
      "title": "LiftFeat: Caractéristiques de Coincidence Locale Relevées de la Géométrie 3D de GI",
      "submittedOnDailyBy": {
        "_id": "681db120007a2d4056d25c70",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/XyX4OSVmH0zv9qQjFSMGd.png",
        "isPro": false,
        "fullname": "yepeng liu",
        "user": "pengliu123",
        "type": "user"
      },
      "summary": "Une assignation robuste et efficace de caractéristiques locales joue un rôle important dans des applications telles que le SLAM de robots et la localisation visuelle. Bien que progressive, l'extraction de caractéristiques visuelles robustes et différenciables est très difficile dans des endroits où l'illumination change rapidement, les bords sont faibles ou les motifs sont répétés. Dans cet article, nous proposons une nouvelle réseau léger appelé LiftFeat, qui met l'accent sur la caractéristique géométrique 3D et augmente sa robustesse en utilisant la dichotomie des cercles dans l'espace 2D de Descartes. Spécifiquement, un modèle d'estimation de profondeur d'une caméra unique entraîné précédemment est utilisé pour générer les droites de normale de surface prédites, et des caractéristiques géométriques 3D sont extraites sur la base de ces droites. Ensuite, un module de caractéristiques géométriques 3D est conçu et les caractéristiques des droites de normale de surface sont fusionnées avec les caractéristiques des cercles dans l'espace 2D de Descartes. Cette intégration de caractéristiques géométriques 3D maximise l'identifiabilité des caractéristiques 2D. Les résultats de l'estimation de la position relative, de l'homographie et de la localisation visuelle montrent que notre LiftFeat dépasse les méthodes légères de pointe en comparaison avec le code disponible sur la page suivante : https://github.com/lyp-deeplearning/LiftFeat.",
      "upvotes": 2,
      "discussionId": "681db58d1f1c39ba8fbe01fb",
      "githubRepo": "https://github.com/lyp-deeplearning/LiftFeat",
      "ai_keywords": [
        "monocular depth estimation model",
        "pseudo surface normal label",
        "3D geometric feature",
        "surface normal feature",
        "2D descriptor feature",
        "3D geometry-aware feature lifting module",
        "relative pose estimation",
        "homography estimation"
      ]
    },
    "publishedAt": "2025-05-06T06:59:23.000Z",
    "title": "LiftFeat: 3D Geometry-Aware Local Feature Matching",
    "summary": "Robust and efficient local feature matching plays a crucial role in\napplications such as SLAM and visual localization for robotics. Despite great\nprogress, it is still very challenging to extract robust and discriminative\nvisual features in scenarios with drastic lighting changes, low texture areas,\nor repetitive patterns. In this paper, we propose a new lightweight network\ncalled LiftFeat, which lifts the robustness of raw descriptor by\naggregating 3D geometric feature. Specifically, we first adopt a pre-trained\nmonocular depth estimation model to generate pseudo surface normal label,\nsupervising the extraction of 3D geometric feature in terms of predicted\nsurface normal. We then design a 3D geometry-aware feature lifting module to\nfuse surface normal feature with raw 2D descriptor feature. Integrating such 3D\ngeometric feature enhances the discriminative ability of 2D feature description\nin extreme conditions. Extensive experimental results on relative pose\nestimation, homography estimation, and visual localization tasks, demonstrate\nthat our LiftFeat outperforms some lightweight state-of-the-art methods. Code\nwill be released at : https://github.com/lyp-deeplearning/LiftFeat.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03422.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "681db120007a2d4056d25c70",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/XyX4OSVmH0zv9qQjFSMGd.png",
      "fullname": "yepeng liu",
      "name": "pengliu123",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.04955",
      "authors": [
        {
          "_id": "681dc1b7965798cccfeab83c",
          "user": {
            "_id": "654cca3fe1b4cd6d40d5a7ae",
            "avatarUrl": "/avatars/4d09531277e16ad71474fc888c16b227.svg",
            "isPro": false,
            "fullname": "Fangwei Zhu",
            "user": "soliz1998",
            "type": "user"
          },
          "name": "Fangwei Zhu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:07:17.372Z",
          "hidden": false
        },
        {
          "_id": "681dc1b7965798cccfeab83d",
          "user": {
            "_id": "656873f33fd0bf1f82558695",
            "avatarUrl": "/avatars/7a085da2e2a91d7f41988501a573ebf9.svg",
            "isPro": false,
            "fullname": "PEIYI, WANG",
            "user": "peiyiwang89",
            "type": "user"
          },
          "name": "Peiyi Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:07:23.247Z",
          "hidden": false
        },
        {
          "_id": "681dc1b7965798cccfeab83e",
          "name": "Zhifang Sui",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/654cca3fe1b4cd6d40d5a7ae/omgv1F-oog_5ohzEFDbns.png"
      ],
      "publishedAt": "2025-05-08T05:32:36.000Z",
      "submittedOnDailyAt": "2025-05-09T07:21:04.391Z",
      "title": "Les tokens de variables Chain-of-Thought sont des tokens de variables dans un programme informatique.",
      "submittedOnDailyBy": {
        "_id": "654cca3fe1b4cd6d40d5a7ae",
        "avatarUrl": "/avatars/4d09531277e16ad71474fc888c16b227.svg",
        "isPro": false,
        "fullname": "Fangwei Zhu",
        "user": "soliz1998",
        "type": "user"
      },
      "summary": "Chain-of-thoughts (CoT) nécessite un grand modèle de langue (LLMs) pour générer des étapes intermédiaires avant d'obtenir la réponse finale, et a démontré être efficace dans la résolution de problèmes avec structures logiques complexes. Cependant, la structure interne de CoT est presque incertaine. Dans cet article, on étudie expérimentalement le rôle des tokens de CoT dans des modèles de LLMs : on aborde deux tâches combinées, le calcul de multiplication de multiples chiffres et la programmation dynamique, et on découvre que CoT est essentiel pour résoudre ces problèmes, mais ne parvient qu'à un rendement acceptable en stockant des résultats intermédiaires. De plus, stocker des résultats intermédiaires sous formes générales potentielles n'affecte pas le rendement du modèle. De plus, en interrompant des valeurs de manière aléatoire dans CoT, on observe que les tokens de CoT et la réponse finale changent ensuite. Ces résultats suggèrent que les tokens de CoT pourraient jouer un rôle similaire à celle des variables en programmation informatique. De plus, ils incluent des limites potentielles possibles comme des solutions simples de manière non consciente et des limites sur la quantité de calculs entre tokens. Le code et les données sont disponibles sur https://github.com/solitaryzero/CoTs_are_Variables.",
      "upvotes": 0,
      "discussionId": "681dc1b8965798cccfeab86d",
      "ai_keywords": [
        "chain-of-thought (CoT)",
        "large language models (LLMs)",
        "intermediate steps",
        "reasoning tasks",
        "inner mechanism",
        "CoT tokens",
        "compositional tasks",
        "multi-digit multiplication",
        "dynamic programming",
        "intermediate results",
        "latent form",
        "variables",
        "unintended shortcuts",
        "computational complexity limits"
      ]
    },
    "publishedAt": "2025-05-08T01:32:36.000Z",
    "title": "Chain-of-Thought Tokens are Computer Program Variables",
    "summary": "Chain-of-thoughts (CoT) requires large language models (LLMs) to generate\nintermediate steps before reaching the final answer, and has been proven\neffective to help LLMs solve complex reasoning tasks. However, the inner\nmechanism of CoT still remains largely unclear. In this paper, we empirically\nstudy the role of CoT tokens in LLMs on two compositional tasks: multi-digit\nmultiplication and dynamic programming. While CoT is essential for solving\nthese problems, we find that preserving only tokens that store intermediate\nresults would achieve comparable performance. Furthermore, we observe that\nstoring intermediate results in an alternative latent form will not affect\nmodel performance. We also randomly intervene some values in CoT, and notice\nthat subsequent CoT tokens and the final answer would change correspondingly.\nThese findings suggest that CoT tokens may function like variables in computer\nprograms but with potential drawbacks like unintended shortcuts and\ncomputational complexity limits between tokens. The code and data are available\nat https://github.com/solitaryzero/CoTs_are_Variables.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/654cca3fe1b4cd6d40d5a7ae/omgv1F-oog_5ohzEFDbns.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.04955.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "654cca3fe1b4cd6d40d5a7ae",
      "avatarUrl": "/avatars/4d09531277e16ad71474fc888c16b227.svg",
      "fullname": "Fangwei Zhu",
      "name": "soliz1998",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]