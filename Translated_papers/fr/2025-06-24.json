[
  {
    "paper": {
      "id": "2506.18882",
      "authors": [
        {
          "_id": "685a163c0e4ad7e219758569",
          "name": "Hong Li",
          "hidden": false
        },
        {
          "_id": "685a163c0e4ad7e21975856a",
          "name": "Houyuan Chen",
          "hidden": false
        },
        {
          "_id": "685a163c0e4ad7e21975856b",
          "name": "Chongjie Ye",
          "hidden": false
        },
        {
          "_id": "685a163c0e4ad7e21975856c",
          "name": "Zhaoxi Chen",
          "hidden": false
        },
        {
          "_id": "685a163c0e4ad7e21975856d",
          "name": "Bohan Li",
          "hidden": false
        },
        {
          "_id": "685a163c0e4ad7e21975856e",
          "name": "Shaocong Xu",
          "hidden": false
        },
        {
          "_id": "685a163c0e4ad7e21975856f",
          "name": "Xianda Guo",
          "hidden": false
        },
        {
          "_id": "685a163c0e4ad7e219758570",
          "name": "Xuhui Liu",
          "hidden": false
        },
        {
          "_id": "685a163c0e4ad7e219758571",
          "name": "Yikai Wang",
          "hidden": false
        },
        {
          "_id": "685a163c0e4ad7e219758572",
          "name": "Baochang Zhang",
          "hidden": false
        },
        {
          "_id": "685a163c0e4ad7e219758573",
          "name": "Satoshi Ikehata",
          "hidden": false
        },
        {
          "_id": "685a163c0e4ad7e219758574",
          "name": "Boxin Shi",
          "hidden": false
        },
        {
          "_id": "685a163c0e4ad7e219758575",
          "name": "Anyi Rao",
          "hidden": false
        },
        {
          "_id": "685a163c0e4ad7e219758576",
          "name": "Hao Zhao",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/643a1f5b58cb07c2a3745116/4JHMhL80xxrkPBOIt9-Cg.mp4"
      ],
      "publishedAt": "2025-06-23T17:53:11.000Z",
      "submittedOnDailyAt": "2025-06-24T02:59:45.489Z",
      "title": "Illumination normale sous lumière : éclairage général pour la photographie en studio pour représenter des caractéristiques cohérentes.",
      "submittedOnDailyBy": {
        "_id": "643a1f5b58cb07c2a3745116",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643a1f5b58cb07c2a3745116/OiSDfgfcCUWu0X4-FiNm0.jpeg",
        "isPro": false,
        "fullname": "Hugo",
        "user": "chongjie",
        "type": "user"
      },
      "summary": "La technique générale de prise de photographies (PS) a pour objectif de restaurer les hautes qualités des normales de surfaces (vecteurs normaux) d'objets sous toute condition d'illumination. Cependant, malgré les avancées récentes, comme SDM-UniPS et Uni MS-PS, deux problèmes fondamentaux restent. 1) La profonde corrélation entre l'illumination variable et les caractéristiques des normales de surfaces, ce qui rend difficile l'interprétation des incertitudes dans l'intensité observée en fonction des changements d'illumination ou de la direction de la surface. 2) La conservation de la géométrie de haute fréquence sur des surfaces complexes, où ces géométries complexes peuvent entraîner des changements dans les images propres, des réflexions mutuelles et des changements dans les normales de surfaces, ce qui rend difficile leur reconnaissance avec précision par les opérations traditionnelles de traitement des caractéristiques.",
      "upvotes": 62,
      "discussionId": "685a163c0e4ad7e219758577",
      "githubRepo": "https://github.com/houyuanchen111/LINO_UniPS",
      "ai_summary": "Photometric stereo aims to recover high-quality surface normals under arbitrary lighting conditions, addressing challenges related to illumination-surface normal coupling and high-frequency geometric detail preservation.",
      "ai_keywords": [
        "photometric stereo",
        "deep coupling",
        "surface normals",
        "illumination conditions",
        "intensity variations",
        "self-shadowing",
        "inter-reflections",
        "subtle normal variations"
      ]
    },
    "publishedAt": "2025-06-23T13:53:11.000Z",
    "title": "Light of Normals: Unified Feature Representation for Universal\n  Photometric Stereo",
    "summary": "Universal photometric stereo (PS) aims to recover high-quality surface\nnormals from objects under arbitrary lighting conditions without relying on\nspecific illumination models. Despite recent advances such as SDM-UniPS and Uni\nMS-PS, two fundamental challenges persist: 1) the deep coupling between varying\nillumination and surface normal features, where ambiguity in observed intensity\nmakes it difficult to determine whether brightness variations stem from\nlighting changes or surface orientation; and 2) the preservation of\nhigh-frequency geometric details in complex surfaces, where intricate\ngeometries create self-shadowing, inter-reflections, and subtle normal\nvariations that conventional feature processing operations struggle to capture\naccurately.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/643a1f5b58cb07c2a3745116/4JHMhL80xxrkPBOIt9-Cg.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18882.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "643a1f5b58cb07c2a3745116",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643a1f5b58cb07c2a3745116/OiSDfgfcCUWu0X4-FiNm0.jpeg",
      "fullname": "Hugo",
      "name": "chongjie",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.18871",
      "authors": [
        {
          "_id": "685a0be90e4ad7e2197584f4",
          "user": {
            "_id": "65f19fa7f591e4538b65dea5",
            "avatarUrl": "/avatars/a38e65701e1d2eb3eb93335d6d0b937c.svg",
            "isPro": false,
            "fullname": "Chenyuan Wu",
            "user": "wcyno23",
            "type": "user"
          },
          "name": "Chenyuan Wu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-24T09:41:49.563Z",
          "hidden": false
        },
        {
          "_id": "685a0be90e4ad7e2197584f5",
          "name": "Pengfei Zheng",
          "hidden": false
        },
        {
          "_id": "685a0be90e4ad7e2197584f6",
          "user": {
            "_id": "661ac5b53d7248a6f20080c1",
            "avatarUrl": "/avatars/26aef5944759c2e4366a71eb8c7fc50a.svg",
            "isPro": false,
            "fullname": "Ruiran Yan",
            "user": "Ruiran",
            "type": "user"
          },
          "name": "Ruiran Yan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-24T09:41:58.820Z",
          "hidden": false
        },
        {
          "_id": "685a0be90e4ad7e2197584f7",
          "user": {
            "_id": "62612679bbcbd1c34f1638af",
            "avatarUrl": "/avatars/c0675d05a52192ee14e9ab1633353956.svg",
            "isPro": false,
            "fullname": "Xiao",
            "user": "Shitao",
            "type": "user"
          },
          "name": "Shitao Xiao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-24T09:42:07.997Z",
          "hidden": false
        },
        {
          "_id": "685a0be90e4ad7e2197584f8",
          "user": {
            "_id": "641bd1737c21ab946bf69aff",
            "avatarUrl": "/avatars/83759075ad893a69a0c2cf5493d7e988.svg",
            "isPro": false,
            "fullname": "xin luo",
            "user": "sienna223",
            "type": "user"
          },
          "name": "Xin Luo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-24T08:08:42.720Z",
          "hidden": false
        },
        {
          "_id": "685a0be90e4ad7e2197584f9",
          "user": {
            "_id": "6458b59c7a7e192202df8fa0",
            "avatarUrl": "/avatars/33ee716477e5686da8723d01e199cd27.svg",
            "isPro": false,
            "fullname": "Yueze Wang",
            "user": "yzwang",
            "type": "user"
          },
          "name": "Yueze Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-24T09:33:03.979Z",
          "hidden": false
        },
        {
          "_id": "685a0be90e4ad7e2197584fa",
          "user": {
            "_id": "675bcb9ce16de4a95aac9950",
            "avatarUrl": "/avatars/a1d0a2fd96ddee9cdea4f97819233fe5.svg",
            "isPro": false,
            "fullname": "Wanli Li",
            "user": "liwanli",
            "type": "user"
          },
          "name": "Wanli Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-24T09:42:15.514Z",
          "hidden": false
        },
        {
          "_id": "685a0be90e4ad7e2197584fb",
          "user": {
            "_id": "674972973dc92067bd606877",
            "avatarUrl": "/avatars/8366448b45baf7d7f3d3d2b8793479ed.svg",
            "isPro": false,
            "fullname": "Jiang Xiyan",
            "user": "Emilia515",
            "type": "user"
          },
          "name": "Xiyan Jiang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-24T09:42:29.622Z",
          "hidden": false
        },
        {
          "_id": "685a0be90e4ad7e2197584fc",
          "name": "Yexin Liu",
          "hidden": false
        },
        {
          "_id": "685a0be90e4ad7e2197584fd",
          "user": {
            "_id": "6564a2ceedae9c33b7654a1f",
            "avatarUrl": "/avatars/42f09356a1282896573ccb44830cd327.svg",
            "isPro": false,
            "fullname": "JUNJIE ZHOU",
            "user": "JUNJIE99",
            "type": "user"
          },
          "name": "Junjie Zhou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-24T08:08:40.699Z",
          "hidden": false
        },
        {
          "_id": "685a0be90e4ad7e2197584fe",
          "user": {
            "_id": "66164f6245336ca774679611",
            "avatarUrl": "/avatars/9baf0ab475bc8d5997abda9ffe8cfa28.svg",
            "isPro": false,
            "fullname": "Ze Liu",
            "user": "marsh123",
            "type": "user"
          },
          "name": "Ze Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-24T08:08:38.616Z",
          "hidden": false
        },
        {
          "_id": "685a0be90e4ad7e2197584ff",
          "user": {
            "_id": "6540617c7cadb2d1b42007c5",
            "avatarUrl": "/avatars/b1877fd0564c362a0d4a064d4ec43a73.svg",
            "isPro": false,
            "fullname": "Ziyi Xia",
            "user": "ZiyiXia",
            "type": "user"
          },
          "name": "Ziyi Xia",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-24T09:42:47.445Z",
          "hidden": false
        },
        {
          "_id": "685a0be90e4ad7e219758500",
          "name": "Chaofan Li",
          "hidden": false
        },
        {
          "_id": "685a0be90e4ad7e219758501",
          "name": "Haoge Deng",
          "hidden": false
        },
        {
          "_id": "685a0be90e4ad7e219758502",
          "name": "Jiahao Wang",
          "hidden": false
        },
        {
          "_id": "685a0be90e4ad7e219758503",
          "name": "Kun Luo",
          "hidden": false
        },
        {
          "_id": "685a0be90e4ad7e219758504",
          "name": "Bo Zhang",
          "hidden": false
        },
        {
          "_id": "685a0be90e4ad7e219758505",
          "user": {
            "_id": "66ed026076a8038cb4ae6053",
            "avatarUrl": "/avatars/99b6527da6b66c6b5df3fc8261587322.svg",
            "isPro": false,
            "fullname": "Defu Lian",
            "user": "dove-ustc",
            "type": "user"
          },
          "name": "Defu Lian",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-24T09:43:23.976Z",
          "hidden": false
        },
        {
          "_id": "685a0be90e4ad7e219758506",
          "name": "Xinlong Wang",
          "hidden": false
        },
        {
          "_id": "685a0be90e4ad7e219758507",
          "name": "Zhongyuan Wang",
          "hidden": false
        },
        {
          "_id": "685a0be90e4ad7e219758508",
          "name": "Tiejun Huang",
          "hidden": false
        },
        {
          "_id": "685a0be90e4ad7e219758509",
          "name": "Zheng Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-23T17:38:54.000Z",
      "submittedOnDailyAt": "2025-06-24T01:06:04.763Z",
      "title": "OmniGen2 : Exploration de la Génération Monorale Avancée",
      "submittedOnDailyBy": {
        "_id": "6564a2ceedae9c33b7654a1f",
        "avatarUrl": "/avatars/42f09356a1282896573ccb44830cd327.svg",
        "isPro": false,
        "fullname": "JUNJIE ZHOU",
        "user": "JUNJIE99",
        "type": "user"
      },
      "summary": "Dans cet étude, nous présentons OmniGen2, un modèle génératif large et open-source. Cette plateforme vise à offrir des solutions pour différentes tâches de génération, tels que la génération d'images à partir de texte, l'édition d'images, ou la génération de texte dans un cadre textuel, au cours d'un mois. Contrairement à OmniGen v1, OmniGen2 utilise deux étapes sécurisées différentes pour le modèle de texte et d'images, ainsi que des paramètres partagés et un ensemble de tokens d'images combinés. Cette architecture permet à OmniGen2 de se baser sur des modèles de compréhension multimodal existants, réduisant ainsi la nécessité de remplacer les entrées de VAE et de maintenir sa capacité d'originalité de génération de texte. Pour soutenir l'entraînement de OmniGen2, un pipeline de construction de données détaillé a été développé, incluant des données d'édition d'images et de génération de texte dans un cadre textuel. De plus, une structure appropriée a été introduite pour les tâches de génération d'images, et un ensemble de données spécifique a été construit en fonction de OmniGen2. OmniGen2 montre des résultats compétitifs sur plusieurs cadres d'évaluation, malgré son taille de paramètres relativement petite pour les tâches de génération d'images et d'édition d'images. Pour évaluer la génération de texte dans un cadre textuel, un nouveau cadre d'évaluation appelé OmniContext a été introduit. OmniGen2 est le modèle open-source avec le meilleur rendement. Pour soutenir les futures recherches, le modèle, le code d'entraînement, les ensembles de données et le pipeline de construction de données sont publiés. Page du projet : https://vectorspacelab.github.io/OmniGen2 ; Lien de GitHub : https://github.com/VectorSpaceLab/OmniGen2",
      "upvotes": 33,
      "discussionId": "685a0be90e4ad7e21975850a",
      "projectPage": "https://vectorspacelab.github.io/OmniGen2/",
      "githubRepo": "https://github.com/VectorSpaceLab/OmniGen2",
      "ai_summary": "OmniGen2, a versatile generative model, introduces dual decoding pathways for text and images, preserves original text generation, and achieves competitive results with a new subject-driven benchmark.",
      "ai_keywords": [
        "decoding pathways",
        "unshared parameters",
        "decoupled image tokenizer",
        "multimodal understanding models",
        "reflection mechanism",
        "reflection dataset",
        "OmniContext",
        "state-of-the-art performance"
      ]
    },
    "publishedAt": "2025-06-23T13:38:54.000Z",
    "title": "OmniGen2: Exploration to Advanced Multimodal Generation",
    "summary": "In this work, we introduce OmniGen2, a versatile and open-source generative\nmodel designed to provide a unified solution for diverse generation tasks,\nincluding text-to-image, image editing, and in-context generation. Unlike\nOmniGen v1, OmniGen2 features two distinct decoding pathways for text and image\nmodalities, utilizing unshared parameters and a decoupled image tokenizer. This\ndesign enables OmniGen2 to build upon existing multimodal understanding models\nwithout the need to re-adapt VAE inputs, thereby preserving the original text\ngeneration capabilities. To facilitate the training of OmniGen2, we developed\ncomprehensive data construction pipelines, encompassing image editing and\nin-context generation data. Additionally, we introduce a reflection mechanism\ntailored for image generation tasks and curate a dedicated reflection dataset\nbased on OmniGen2. Despite its relatively modest parameter size, OmniGen2\nachieves competitive results on multiple task benchmarks, including\ntext-to-image and image editing. To further evaluate in-context generation,\nalso referred to as subject-driven tasks, we introduce a new benchmark named\nOmniContext. OmniGen2 achieves state-of-the-art performance among open-source\nmodels in terms of consistency. We will release our models, training code,\ndatasets, and data construction pipeline to support future research in this\nfield. Project Page: https://vectorspacelab.github.io/OmniGen2; GitHub Link:\nhttps://github.com/VectorSpaceLab/OmniGen2",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18871.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6564a2ceedae9c33b7654a1f",
      "avatarUrl": "/avatars/42f09356a1282896573ccb44830cd327.svg",
      "fullname": "JUNJIE ZHOU",
      "name": "JUNJIE99",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.18841",
      "authors": [
        {
          "_id": "685a0f330e4ad7e219758514",
          "name": "Yuhao Wu",
          "hidden": false
        },
        {
          "_id": "685a0f330e4ad7e219758515",
          "name": "Yushi Bai",
          "hidden": false
        },
        {
          "_id": "685a0f330e4ad7e219758516",
          "user": {
            "_id": "637f228152229c63921119c3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637f228152229c63921119c3/acwXorra1r9_7i3KlBFjS.jpeg",
            "isPro": false,
            "fullname": "Zhiqiang Hu",
            "user": "Zhiqiang007",
            "type": "user"
          },
          "name": "Zhiqiang Hu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-24T08:08:35.954Z",
          "hidden": false
        },
        {
          "_id": "685a0f330e4ad7e219758517",
          "name": "Roy Ka-Wei Lee",
          "hidden": false
        },
        {
          "_id": "685a0f330e4ad7e219758518",
          "name": "Juanzi Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-23T16:59:02.000Z",
      "submittedOnDailyAt": "2025-06-24T01:08:07.123Z",
      "title": "LongWriter-Zero : On réalise l'optimisation de la génération de longues phrases par le moyen du méthode d'entraînement par récompense.",
      "submittedOnDailyBy": {
        "_id": "63369da91ba5d5ece24118a4",
        "avatarUrl": "/avatars/67889e1ecadb04100a77bc8b5284c6fd.svg",
        "isPro": false,
        "fullname": "wuyuhao",
        "user": "mozhu",
        "type": "user"
      },
      "summary": "La superlongitudnalité dans les grands modèles de langue (LLMs) est l'un des scénarios qui nécessite une demande élevée, mais est limitée par la longueur maximale de génération et le déclin de la qualité lorsque la longueur augmente, ce qui transforme ce problème en une question de pertinence réelle. Un exemple d'une façon d'aborder ce problème est LongWriter, qui base son approche sur la \"regle\". Cela implique d'entraîner un ajustement de régulation de phrases longues (SFT) pour la génération de longues phrases, mais cette stratégie dépend de données SFT de synthèse, ce qui peut être coûteux et complexe à construire, et peut également se présenter artificiel et trop structuré, ce qui réduit la collaboration et la cohérence. Dans cette étude, nous proposons un approche à partir de zéro, sans dépendre de toute note ou de données de synthèse, en utilisant une approche de récompenses. Cette approche vise à encourager la capacité de génération de haute qualité et de longueur de texte par des modèles de LLMs en utilisant l'apprentissage par renforcement (RL). L'apprentissage RL commence avec un modèle de base et guide le modèle pendant le processus d'écriture pour promouvoir la planification et l'amélioration. Pour atteindre ceci, un modèle de récompenses spécial est utilisé, et le modèle de LLM est ajusté de manière continue pour contrôler la longueur du texte, la qualité des phrases et la structure de la composition. Selon les résultats expérimentaux, notre modèle LongWriter-Zero (entraîné sur Qwen2.5-32B) obtient des résultats constamment meilleurs que les méthodes traditionnelles de SFT. À ce sujet, LongWriter-Zero réalise les meilleurs résultats sur toutes les métriques de WritingBench et Arena-Write, et dépasse les modèles tels que DeepSeek R1 et Qwen3-235B, qui ont plus de 100B de paramètres. Les données et les points de contrôle du modèle sont publiés sur https://huggingface.co/THU-KEG/LongWriter-Zero-32B.",
      "upvotes": 30,
      "discussionId": "685a0f340e4ad7e219758519",
      "ai_summary": "An incentivization-based reinforcement learning approach is used to develop a large language model capable of generating ultra-long, high-quality text without the need for synthetic data or supervised fine-tuning.",
      "ai_keywords": [
        "reinforcement learning",
        "reward models",
        "long-form text generation",
        "ultra-long generation",
        "large language models",
        "synthetic fine-tuning",
        "length control",
        "writing quality",
        "structural formatting",
        "WritingBench",
        "Arena-Write"
      ]
    },
    "publishedAt": "2025-06-23T12:59:02.000Z",
    "title": "LongWriter-Zero: Mastering Ultra-Long Text Generation via Reinforcement\n  Learning",
    "summary": "Ultra-long generation by large language models (LLMs) is a widely demanded\nscenario, yet it remains a significant challenge due to their maximum\ngeneration length limit and overall quality degradation as sequence length\nincreases. Previous approaches, exemplified by LongWriter, typically rely on\n''teaching'', which involves supervised fine-tuning (SFT) on synthetic\nlong-form outputs. However, this strategy heavily depends on synthetic SFT\ndata, which is difficult and costly to construct, often lacks coherence and\nconsistency, and tends to be overly artificial and structurally monotonous. In\nthis work, we propose an incentivization-based approach that, starting entirely\nfrom scratch and without relying on any annotated or synthetic data, leverages\nreinforcement learning (RL) to foster the emergence of ultra-long, high-quality\ntext generation capabilities in LLMs. We perform RL training starting from a\nbase model, similar to R1-Zero, guiding it to engage in reasoning that\nfacilitates planning and refinement during the writing process. To support\nthis, we employ specialized reward models that steer the LLM towards improved\nlength control, writing quality, and structural formatting. Experimental\nevaluations show that our LongWriter-Zero model, trained from Qwen2.5-32B,\nconsistently outperforms traditional SFT methods on long-form writing tasks,\nachieving state-of-the-art results across all metrics on WritingBench and\nArena-Write, and even surpassing 100B+ models such as DeepSeek R1 and\nQwen3-235B. We open-source our data and model checkpoints under\nhttps://huggingface.co/THU-KEG/LongWriter-Zero-32B",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18841.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63369da91ba5d5ece24118a4",
      "avatarUrl": "/avatars/67889e1ecadb04100a77bc8b5284c6fd.svg",
      "fullname": "wuyuhao",
      "name": "mozhu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.18851",
      "authors": [
        {
          "_id": "685a0fb40e4ad7e219758528",
          "user": {
            "_id": "6304e2dabad6ce7fc0287d57",
            "avatarUrl": "/avatars/3fd4a9a62b0ef98db2573411463a9247.svg",
            "isPro": false,
            "fullname": "Zhuowei_Chen",
            "user": "ZhuoweiChen",
            "type": "user"
          },
          "name": "Zhuowei Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-24T09:44:49.590Z",
          "hidden": false
        },
        {
          "_id": "685a0fb40e4ad7e219758529",
          "user": {
            "_id": "63b415037af2e415f2599c18",
            "avatarUrl": "/avatars/4afbe7d6d05a702f1beeed9c53e78153.svg",
            "isPro": false,
            "fullname": "Bingchuan Li",
            "user": "lbc402",
            "type": "user"
          },
          "name": "Bingchuan Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-24T09:44:57.735Z",
          "hidden": false
        },
        {
          "_id": "685a0fb40e4ad7e21975852a",
          "user": {
            "_id": "6804ce31d205d72ddbeec8a0",
            "avatarUrl": "/avatars/772d20a653649063158cba166298801a.svg",
            "isPro": false,
            "fullname": "Tianxiang Ma",
            "user": "TianxiangMa",
            "type": "user"
          },
          "name": "Tianxiang Ma",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-24T09:45:10.156Z",
          "hidden": false
        },
        {
          "_id": "685a0fb40e4ad7e21975852b",
          "name": "Lijie Liu",
          "hidden": false
        },
        {
          "_id": "685a0fb40e4ad7e21975852c",
          "user": {
            "_id": "619b404bab4c7b7f16a7d57d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/619b404bab4c7b7f16a7d57d/coT_UGRfBOUAeSxjyhdlG.jpeg",
            "isPro": false,
            "fullname": "Mingcong Liu",
            "user": "onion-liu",
            "type": "user"
          },
          "name": "Mingcong Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-24T08:08:23.740Z",
          "hidden": false
        },
        {
          "_id": "685a0fb40e4ad7e21975852d",
          "name": "Yi Zhang",
          "hidden": false
        },
        {
          "_id": "685a0fb40e4ad7e21975852e",
          "name": "Gen Li",
          "hidden": false
        },
        {
          "_id": "685a0fb40e4ad7e21975852f",
          "user": {
            "_id": "6752cd83ffaeeb979db974ae",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/ci915Tdmv3uWbCqGy7LqL.png",
            "isPro": false,
            "fullname": "Xinghui Li",
            "user": "Crayon-Shinchan",
            "type": "user"
          },
          "name": "Xinghui Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-24T09:45:40.054Z",
          "hidden": false
        },
        {
          "_id": "685a0fb40e4ad7e219758530",
          "name": "Siyu Zhou",
          "hidden": false
        },
        {
          "_id": "685a0fb40e4ad7e219758531",
          "name": "Qian He",
          "hidden": false
        },
        {
          "_id": "685a0fb40e4ad7e219758532",
          "user": {
            "_id": "67bc6b515d9470ec64bdcc33",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/dttL8Fb3bKCVG5zjg_02q.png",
            "isPro": false,
            "fullname": "Xinglong Wu",
            "user": "Xingzhe-xlwu",
            "type": "user"
          },
          "name": "Xinglong Wu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-24T09:45:50.807Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-23T17:11:56.000Z",
      "submittedOnDailyAt": "2025-06-24T01:12:12.451Z",
      "title": "Fantom Data : Thème Général de Génération de Vidéo pour la Cohérence des Entités\nDataset",
      "submittedOnDailyBy": {
        "_id": "6304e2dabad6ce7fc0287d57",
        "avatarUrl": "/avatars/3fd4a9a62b0ef98db2573411463a9247.svg",
        "isPro": false,
        "fullname": "Zhuowei_Chen",
        "user": "ZhuoweiChen",
        "type": "user"
      },
      "summary": "La génération de vidéos thématiques a connu un progrès considérable récemment. Cependant, les modèles actuels rencontrent un défi à suivre de plus en plus les instructions textuelles contextuelles. Ce limite, connu sous le nom de problème de copie et collage, est causé par l'utilisation large de deux approches d'entraînement. Cette approche vise à relier la reconnaissance du thème à son contexte et à ses attributs en utilisant une échantillonnage d'images de référence dans le même scénario que le vidéo cible. Pour résoudre ce problème, nous présentons Phantom-Data. Phantom-Data est un ensemble de données conçu pour garantir la cohérence du thème dans les vidéos générales. Cet ensemble de données comprend environ un million de paires d'identificateurs répartis dans différentes catégories. Notre ensemble de données a été construit à travers une chaîne de production en trois étapes : (1) un module d'exploration thématique pour les entrées générales, (2) une recherche de thèmes à grande échelle sur des vidéos et des images, avec plus de 53 millions de vidéos et 3 milliards d'images, et (3) une confirmation d'identification par un leader de file qui assure la cohérence visuelle avec les changements de contexte. Les expériences détaillées montrent que, tout en maintenant un niveau similaire de cohérence d'identification par rapport aux bases de paires au sein de l'ensemble de données, Phantom-Data améliore significativement le rendement de ProNuPAMI et la qualité visuelle.",
      "upvotes": 21,
      "discussionId": "685a0fb40e4ad7e219758535",
      "projectPage": "https://phantom-video.github.io/Phantom-Data/",
      "githubRepo": "https://github.com/Phantom-video/Phantom-Data",
      "ai_summary": "A cross-pair dataset called Phantom-Data improves subject-to-video generation by enhancing prompt alignment and visual quality while maintaining identity consistency.",
      "ai_keywords": [
        "Phantom-Data",
        "subject-to-video generation",
        "copy-paste problem",
        "in-pair training paradigm",
        "subject detection",
        "cross-context subject retrieval",
        "prior-guided identity verification"
      ]
    },
    "publishedAt": "2025-06-23T13:11:56.000Z",
    "title": "Phantom-Data : Towards a General Subject-Consistent Video Generation\n  Dataset",
    "summary": "Subject-to-video generation has witnessed substantial progress in recent\nyears. However, existing models still face significant challenges in faithfully\nfollowing textual instructions. This limitation, commonly known as the\ncopy-paste problem, arises from the widely used in-pair training paradigm. This\napproach inherently entangles subject identity with background and contextual\nattributes by sampling reference images from the same scene as the target\nvideo. To address this issue, we introduce Phantom-Data, the first\ngeneral-purpose cross-pair subject-to-video consistency dataset, containing\napproximately one million identity-consistent pairs across diverse categories.\nOur dataset is constructed via a three-stage pipeline: (1) a general and\ninput-aligned subject detection module, (2) large-scale cross-context subject\nretrieval from more than 53 million videos and 3 billion images, and (3)\nprior-guided identity verification to ensure visual consistency under\ncontextual variation. Comprehensive experiments show that training with\nPhantom-Data significantly improves prompt alignment and visual quality while\npreserving identity consistency on par with in-pair baselines.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18851.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6304e2dabad6ce7fc0287d57",
      "avatarUrl": "/avatars/3fd4a9a62b0ef98db2573411463a9247.svg",
      "fullname": "Zhuowei_Chen",
      "name": "ZhuoweiChen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.18896",
      "authors": [
        {
          "_id": "685a02790e4ad7e2197584b2",
          "name": "Jiaru Zou",
          "hidden": false
        },
        {
          "_id": "685a02790e4ad7e2197584b3",
          "name": "Ling Yang",
          "hidden": false
        },
        {
          "_id": "685a02790e4ad7e2197584b4",
          "name": "Jingwen Gu",
          "hidden": false
        },
        {
          "_id": "685a02790e4ad7e2197584b5",
          "name": "Jiahao Qiu",
          "hidden": false
        },
        {
          "_id": "685a02790e4ad7e2197584b6",
          "name": "Ke Shen",
          "hidden": false
        },
        {
          "_id": "685a02790e4ad7e2197584b7",
          "name": "Jingrui He",
          "hidden": false
        },
        {
          "_id": "685a02790e4ad7e2197584b8",
          "name": "Mengdi Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-23T17:59:02.000Z",
      "submittedOnDailyAt": "2025-06-24T01:03:32.146Z",
      "title": "ReasonFlux-PRM : Dans les PRMs intéressés par la trajectoire, une séquence de pensées prolongées et continus d'inférence.",
      "submittedOnDailyBy": {
        "_id": "64fde4e252e82dd432b74ce9",
        "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
        "isPro": false,
        "fullname": "Ling Yang",
        "user": "Lingaaaaaaa",
        "type": "user"
      },
      "summary": "Les Modèles de Récompense de Processus (PRMs) ont évolué depuis un cadre puissant pour réviser la raison intermédiaire des grands modèles de langage (LLMs). Les PRMs de la semaine dernière avaient été principalement entraînés sur des données sur l'output final du modèle, ce qui rendait l'évaluation des chemins intermédiaires plus difficile, surtout dans le nouveau contexte des modèles de raison comme Deepseek-R1. Dans cette étude, nous présentons ReasonFlux-PRM. ReasonFlux-PRM est un nouveau PRM qui se concentre sur les chemins et est conçu pour évaluer les traces de raison chemin-output. ReasonFlux-PRM utilise des sous-objectifs à l'échelle d'étape et de chemin, permettant une distribution de récompenses plus précise pour des données de raisonnement structurée. ReasonFlux-PRM supporte la sous-objectif de récompense dans deux configurations : en ligne et en ligne. Spécifiquement, (i) la sélection de données pour garantir des modèles de haute qualité en entraînement, (ii) la distribution de récompenses au niveau de processus qui nécessitent une forte densité pour l'optimisation de la politique en apprentissage par renforcement, et (iii) l'échelle temporelle de test guidée par la récompense. À travers des expériences sur des benchmarks de formation descendante difficiles comme AIME, MATH500 et GPQA-Diamond, ReasonFlux-PRM-7B a démontré une sélection de données de haute qualité, améliorant les PRMs robustes (comme Qwen2.5-Math-PRM-72B) ou des lignes de modèles basées sur l'humain, avec un rendement moyen de 12.1% en formation, 4.5% en apprentissage par renforcement et 6.3% en échelle temporelle de test. De plus, nous lançons une version efficace de ReasonFlux-PRM-1.5B adaptée aux applications avec des limitations de ressources ou à des déploiements sur les bords. Projet : https://github.com/Gen-Verse/ReasonFlux",
      "upvotes": 20,
      "discussionId": "685a027a0e4ad7e2197584b9",
      "projectPage": "https://huggingface.co/collections/Gen-Verse/reasonflux-prm-68463c73cf1c6a0ec6fafeb5",
      "githubRepo": "https://github.com/Gen-Verse/ReasonFlux",
      "ai_summary": "ReasonFlux-PRM, a novel trajectory-aware Process Reward Model, evaluates reasoning traces with step-level and trajectory-level supervision, enhancing performance in model distillation, reinforcement learning, and test-time scaling.",
      "ai_keywords": [
        "Process Reward Models",
        "trajectory-aware PRM",
        "trajectory-response outputs",
        "step-level supervision",
        "trajectory-level supervision",
        "chain-of-thought data",
        "model distillation",
        "policy optimization",
        "reinforcement learning",
        "Best-of-N test-time scaling",
        "AIME",
        "MATH500",
        "GPQA-Diamond"
      ]
    },
    "publishedAt": "2025-06-23T13:59:02.000Z",
    "title": "ReasonFlux-PRM: Trajectory-Aware PRMs for Long Chain-of-Thought\n  Reasoning in LLMs",
    "summary": "Process Reward Models (PRMs) have recently emerged as a powerful framework\nfor supervising intermediate reasoning steps in large language models (LLMs).\nPrevious PRMs are primarily trained on model final output responses and\nstruggle to evaluate intermediate thinking trajectories robustly, especially in\nthe emerging setting of trajectory-response outputs generated by frontier\nreasoning models like Deepseek-R1. In this work, we introduce ReasonFlux-PRM, a\nnovel trajectory-aware PRM explicitly designed to evaluate the\ntrajectory-response type of reasoning traces. ReasonFlux-PRM incorporates both\nstep-level and trajectory-level supervision, enabling fine-grained reward\nassignment aligned with structured chain-of-thought data. We adapt\nReasonFlux-PRM to support reward supervision under both offline and online\nsettings, including (i) selecting high-quality model distillation data for\ndownstream supervised fine-tuning of smaller models, (ii) providing dense\nprocess-level rewards for policy optimization during reinforcement learning,\nand (iii) enabling reward-guided Best-of-N test-time scaling. Empirical results\non challenging downstream benchmarks such as AIME, MATH500, and GPQA-Diamond\ndemonstrate that ReasonFlux-PRM-7B selects higher quality data than strong PRMs\n(e.g., Qwen2.5-Math-PRM-72B) and human-curated baselines. Furthermore, our\nderived ReasonFlux-PRM-7B yields consistent performance improvements, achieving\naverage gains of 12.1% in supervised fine-tuning, 4.5% in reinforcement\nlearning, and 6.3% in test-time scaling. We also release our efficient\nReasonFlux-PRM-1.5B for resource-constrained applications and edge deployment.\nProjects: https://github.com/Gen-Verse/ReasonFlux",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18896.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64fde4e252e82dd432b74ce9",
      "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
      "fullname": "Ling Yang",
      "name": "Lingaaaaaaa",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.18898",
      "authors": [
        {
          "_id": "685a07510e4ad7e2197584c6",
          "user": {
            "_id": "62318c0386753f5f41d0e261",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62318c0386753f5f41d0e261/xO_5PvOf7lXhQPnQLcmnq.jpeg",
            "isPro": false,
            "fullname": "Jiaming Han",
            "user": "csuhan",
            "type": "user"
          },
          "name": "Jiaming Han",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-24T08:08:47.088Z",
          "hidden": false
        },
        {
          "_id": "685a07510e4ad7e2197584c7",
          "name": "Hao Chen",
          "hidden": false
        },
        {
          "_id": "685a07510e4ad7e2197584c8",
          "name": "Yang Zhao",
          "hidden": false
        },
        {
          "_id": "685a07510e4ad7e2197584c9",
          "user": {
            "_id": "6365a174ad2c9e8b6731dd0f",
            "avatarUrl": "/avatars/2f4dd0eda92bca5a7464129fe7d961f9.svg",
            "isPro": false,
            "fullname": "Hanyu Wang",
            "user": "hywang66",
            "type": "user"
          },
          "name": "Hanyu Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-24T08:08:44.987Z",
          "hidden": false
        },
        {
          "_id": "685a07510e4ad7e2197584ca",
          "name": "Qi Zhao",
          "hidden": false
        },
        {
          "_id": "685a07510e4ad7e2197584cb",
          "name": "Ziyan Yang",
          "hidden": false
        },
        {
          "_id": "685a07510e4ad7e2197584cc",
          "name": "Hao He",
          "hidden": false
        },
        {
          "_id": "685a07510e4ad7e2197584cd",
          "name": "Xiangyu Yue",
          "hidden": false
        },
        {
          "_id": "685a07510e4ad7e2197584ce",
          "name": "Lu Jiang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-23T17:59:14.000Z",
      "submittedOnDailyAt": "2025-06-24T01:02:20.046Z",
      "title": "Vision en langue locale : représentation correspondante à un texte qui intègre la perception et la génération visuelles.",
      "submittedOnDailyBy": {
        "_id": "62318c0386753f5f41d0e261",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62318c0386753f5f41d0e261/xO_5PvOf7lXhQPnQLcmnq.jpeg",
        "isPro": false,
        "fullname": "Jiaming Han",
        "user": "csuhan",
        "type": "user"
      },
      "summary": "Dans cet article, on propose différents cadres qui intègrent la compréhension et la génération d'images à l'aide d'expressions discrètes de sens commun. Un des points clés est le TokenNerf (TA-Tok), qui utilise des boîtes de code de texte correspondantes, poussées par la distribution de grands modèles de langage (LLM), pour convertir des images en tokens discrètes. Ce cadre intègre la vision et le texte dans une distribution large, permettant aux modèles de tout type d'utiliser une interface commune pour les entrées et les sorties croisées. De plus, on propose un codage et décodage adaptatifs à l'échelle pour maintenir un équilibre entre efficacité et détails visuels, et on met en œuvre un détecteur génératif pour produire des sorties visuelles de haute qualité. Pour satisfaire les différentes nécessités de décodage, on utilise deux détecteurs interpolatifs : un modèle automatique de régression rapide et un modèle basé sur des branches. Pour améliorer la fusion du modèle, on revient à des tâches de pré-entraînement avancées et on montre des améliorations tant sur la compréhension visuelle que sur la génération. Les expériences sur les benchmarks dépassent les méthodes actuelles de différents LLM, atteignant une convergence plus rapide et une efficacité d'entraînement plus élevée. Le code, le modèle et les données sont disponibles sur la URL suivante : https://tar.csuhan.com.",
      "upvotes": 18,
      "discussionId": "685a07520e4ad7e2197584cf",
      "projectPage": "https://tar.csuhan.com",
      "githubRepo": "https://github.com/csuhan/Tar",
      "ai_summary": "A multimodal framework uses a Text-Aligned Tokenizer (TA-Tok) to integrate vision and text into a unified space, employing a generative de-tokenizer with autoregressive and diffusion-based models for efficient and high-fidelity visual outputs.",
      "ai_keywords": [
        "Text-Aligned Tokenizer (TA-Tok)",
        "multimodal LLM",
        "Tar",
        "scale-adaptive encoding",
        "diffusion-based model",
        "autoregressive model",
        "modality fusion",
        "pre-training tasks"
      ]
    },
    "publishedAt": "2025-06-23T13:59:14.000Z",
    "title": "Vision as a Dialect: Unifying Visual Understanding and Generation via\n  Text-Aligned Representations",
    "summary": "This paper presents a multimodal framework that attempts to unify visual\nunderstanding and generation within a shared discrete semantic representation.\nAt its core is the Text-Aligned Tokenizer (TA-Tok), which converts images into\ndiscrete tokens using a text-aligned codebook projected from a large language\nmodel's (LLM) vocabulary. By integrating vision and text into a unified space\nwith an expanded vocabulary, our multimodal LLM, Tar, enables cross-modal input\nand output through a shared interface, without the need for modality-specific\ndesigns. Additionally, we propose scale-adaptive encoding and decoding to\nbalance efficiency and visual detail, along with a generative de-tokenizer to\nproduce high-fidelity visual outputs. To address diverse decoding needs, we\nutilize two complementary de-tokenizers: a fast autoregressive model and a\ndiffusion-based model. To enhance modality fusion, we investigate advanced\npre-training tasks, demonstrating improvements in both visual understanding and\ngeneration. Experiments across benchmarks show that Tar matches or surpasses\nexisting multimodal LLM methods, achieving faster convergence and greater\ntraining efficiency. Code, models, and data are available at\nhttps://tar.csuhan.com",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18898.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62318c0386753f5f41d0e261",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62318c0386753f5f41d0e261/xO_5PvOf7lXhQPnQLcmnq.jpeg",
      "fullname": "Jiaming Han",
      "name": "csuhan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 15
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.18254",
      "authors": [
        {
          "_id": "685a39d60e4ad7e219758622",
          "name": "Tianyu Yu",
          "hidden": false
        },
        {
          "_id": "685a39d60e4ad7e219758623",
          "name": "Bo Ji",
          "hidden": false
        },
        {
          "_id": "685a39d60e4ad7e219758624",
          "name": "Shouli Wang",
          "hidden": false
        },
        {
          "_id": "685a39d60e4ad7e219758625",
          "name": "Shu Yao",
          "hidden": false
        },
        {
          "_id": "685a39d60e4ad7e219758626",
          "name": "Zefan Wang",
          "hidden": false
        },
        {
          "_id": "685a39d60e4ad7e219758627",
          "name": "Ganqu Cui",
          "hidden": false
        },
        {
          "_id": "685a39d60e4ad7e219758628",
          "name": "Lifan Yuan",
          "hidden": false
        },
        {
          "_id": "685a39d60e4ad7e219758629",
          "name": "Ning Ding",
          "hidden": false
        },
        {
          "_id": "685a39d60e4ad7e21975862a",
          "name": "Yuan Yao",
          "hidden": false
        },
        {
          "_id": "685a39d60e4ad7e21975862b",
          "name": "Zhiyuan Liu",
          "hidden": false
        },
        {
          "_id": "685a39d60e4ad7e21975862c",
          "name": "Maosong Sun",
          "hidden": false
        },
        {
          "_id": "685a39d60e4ad7e21975862d",
          "name": "Tat-Seng Chua",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-23T02:56:36.000Z",
      "submittedOnDailyAt": "2025-06-24T04:14:54.081Z",
      "title": "RLPR : Inférence de RLVR dans le domaine général à partir de données non valides",
      "submittedOnDailyBy": {
        "_id": "64abc4aa6cadc7aca585dddf",
        "avatarUrl": "/avatars/736afea979cd0021c7a37f68731524ea.svg",
        "isPro": false,
        "fullname": "Tianyu Yu",
        "user": "Yirany",
        "type": "user"
      },
      "summary": "L'apprentissage par renforcement avec récompenses fiables (RLVR) démontre une possibilité prometteuse pour le développement de compétences logiques dans les modèles de langage grands (LLM). Cependant, son succès est principalement limité aux domaines de la mathématiques et du code. Cette limitation principale est due à l'approche qui se concentre sur des bases de données spécifiques, ce qui rend la complexité infinie et la scalabilité limitée. Pour faire face à ces défis, notre perspective principale est que les LLM ont un propre probabilisme intrinsèque pour générer des réponses précises, ce qui les permet d'évaluer les récompenses logiques (comme la conduite des étapes théoriques vers une réponse correcte). En se basant sur cette observation, nous proposons un simple cadre sans base de données appelé RLPR (Apprentissage par Renforcement avec Récompenses Probabilistes). RLPR utilise les scores de probabilité des tokens propres des LLM comme signaux de récompense, maximisant l'espérance de la récompense pendant l'entraînement. Nous reconnaissons l'importance de réduire la forte variation de la récompense probabiliste, ainsi que la nécessité de garantir une récompense stable et précise à partir de la probabilité propre du LLM, proposant des méthodes comme prob-to-reward et stabilisation. Les expériences détaillées sur quatre cadres de référence généraux et trois benchmarks de mathématiques montrent que la capacité logique s'améliore de manière constante dans les modèles basés sur Gemma, Llama et Qwen. En particulier, sur TheoremQA, ils dépassent VeriFree de 7,6 points et Minerva de 7,5 points, et sur les 7 benchmarks généraux, l'augmentation moyenne des scores du Général-Reasoner est supérieure à 1,6 points.",
      "upvotes": 18,
      "discussionId": "685a39d80e4ad7e21975862e",
      "projectPage": "https://github.com/OpenBMB/RLPR",
      "githubRepo": "https://github.com/OpenBMB/RLPR",
      "ai_summary": "RLPR, a verifier-free framework using LLM's token probability scores as reward signals, enhances reasoning capabilities across both general and mathematical domains, outperforming other methods in various benchmarks.",
      "ai_keywords": [
        "Reinforcement Learning with Verifiable Rewards (RLVR)",
        "reasoning capabilities",
        "LLM",
        "RLPR",
        "token probability scores",
        "prob-to-reward",
        "stabilizing methods",
        "TheoremQA",
        "Minerva",
        "General-Reasoner"
      ]
    },
    "publishedAt": "2025-06-22T22:56:36.000Z",
    "title": "RLPR: Extrapolating RLVR to General Domains without Verifiers",
    "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) demonstrates promising\npotential in advancing the reasoning capabilities of LLMs. However, its success\nremains largely confined to mathematical and code domains. This primary\nlimitation stems from the heavy reliance on domain-specific verifiers, which\nresults in prohibitive complexity and limited scalability. To address the\nchallenge, our key observation is that LLM's intrinsic probability of\ngenerating a correct free-form answer directly indicates its own evaluation of\nthe reasoning reward (i.e., how well the reasoning process leads to the correct\nanswer). Building on this insight, we propose RLPR, a simple verifier-free\nframework that extrapolates RLVR to broader general domains. RLPR uses the\nLLM's own token probability scores for reference answers as the reward signal\nand maximizes the expected reward during training. We find that addressing the\nhigh variance of this noisy probability reward is crucial to make it work, and\npropose prob-to-reward and stabilizing methods to ensure a precise and stable\nreward from LLM intrinsic probabilities. Comprehensive experiments in four\ngeneral-domain benchmarks and three mathematical benchmarks show that RLPR\nconsistently improves reasoning capabilities in both areas for Gemma, Llama,\nand Qwen based models. Notably, RLPR outperforms concurrent VeriFree by 7.6\npoints on TheoremQA and 7.5 points on Minerva, and even surpasses strong\nverifier-model-dependent approaches General-Reasoner by 1.6 average points\nacross seven benchmarks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18254.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "64abc4aa6cadc7aca585dddf",
      "avatarUrl": "/avatars/736afea979cd0021c7a37f68731524ea.svg",
      "fullname": "Tianyu Yu",
      "name": "Yirany",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.15741",
      "authors": [
        {
          "_id": "685a48970e4ad7e219758662",
          "name": "He Zhu",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e219758663",
          "user": {
            "_id": "64301abe450c0de9a1d3d18e",
            "avatarUrl": "/avatars/01b284874dadc7d21d656c53dcb77e42.svg",
            "isPro": false,
            "fullname": "tianrui",
            "user": "tianyue818",
            "type": "user"
          },
          "name": "Tianrui Qin",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-24T07:14:35.823Z",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e219758664",
          "user": {
            "_id": "6578265ddea7e2122d02f6ba",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6578265ddea7e2122d02f6ba/Bh6JjoVF5ceLSjV7Z7nTk.jpeg",
            "isPro": false,
            "fullname": "kang zhu",
            "user": "kangz",
            "type": "user"
          },
          "name": "King Zhu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-24T09:33:02.082Z",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e219758665",
          "name": "Heyuan Huang",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e219758666",
          "name": "Yeyi Guan",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e219758667",
          "name": "Jinxiang Xia",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e219758668",
          "name": "Yi Yao",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e219758669",
          "name": "Hanhao Li",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e21975866a",
          "name": "Ningning Wang",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e21975866b",
          "name": "Pai Liu",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e21975866c",
          "name": "Tianhao Peng",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e21975866d",
          "name": "Xin Gui",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e21975866e",
          "name": "Xiaowan Li",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e21975866f",
          "name": "Yuhui Liu",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e219758670",
          "name": "Yuchen Eleanor Jiang",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e219758671",
          "name": "Jun Wang",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e219758672",
          "name": "Changwang Zhang",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e219758673",
          "name": "Xiangru Tang",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e219758674",
          "name": "Ge Zhang",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e219758675",
          "name": "Jian Yang",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e219758676",
          "name": "Minghao Liu",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e219758677",
          "name": "Xitong Gao",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e219758678",
          "name": "Jiaheng Liu",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e219758679",
          "name": "Wangchunshu Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-17T17:59:02.000Z",
      "submittedOnDailyAt": "2025-06-24T05:17:03.413Z",
      "title": "OAgents : Recherche Empirique sur la Construction d'Agents Valides",
      "submittedOnDailyBy": {
        "_id": "628c8598ef14f971b698107f",
        "avatarUrl": "/avatars/3a4ad87e6b5f9e836a1160d869df1447.svg",
        "isPro": false,
        "fullname": "Zhou",
        "user": "Wangchunshu",
        "type": "user"
      },
      "summary": "Récemment, le domaine de recherche de l'IA Agentique a augmenté. Cependant, la pratique actuelle de la recherche en agents présente des problèmes d'standardisation et de manque de science, ce qui rend difficile la comparaison équitable entre méthodes. En conséquence, il n'est pas possible de déterminer quels sont les effets des différents designs d'agents, ni de l'évaluer facilement. Dans cette étude, nous investiguons systématiquement le GAIA benchmark et le BrowseComp, examinant les impacts des designs populaires de manière équitable et rigoureuse. Nous avons découvert des défauts dans les protocoles d'évaluation standard des études antérieures, y compris les codes open, et nous avons observé une manque de reproductibilité et de grandes variations extrêmes entre les expériences aléatoires. Par conséquent, nous avons introduit un protocole d'évaluation plus robuste pour améliorer la stabilité. Notre étude montre que certaines compositions et designs sont essentiels pour un agent efficace, tandis que d'autres sont inefficaces. En nous appuyant sur nos résultats, nous avons construit et fourni de manière ouverte un nouveau cadre d'agents de base, OAgents. OAgents offre un design modulaire qui intègre plusieurs composants d'agents, ce qui encourage la recherche future de l'IA Agentique.",
      "upvotes": 18,
      "discussionId": "685a48980e4ad7e21975867a"
    },
    "publishedAt": "2025-06-17T13:59:02.000Z",
    "title": "OAgents: An Empirical Study of Building Effective Agents",
    "summary": "Recently, Agentic AI has become an increasingly popular research field.\nHowever, we argue that current agent research practices lack standardization\nand scientific rigor, making it hard to conduct fair comparisons among methods.\nAs a result, it is still unclear how different design choices in agent\nframeworks affect effectiveness, and measuring their progress remains\nchallenging. In this work, we conduct a systematic empirical study on GAIA\nbenchmark and BrowseComp to examine the impact of popular design choices in key\nagent components in a fair and rigorous manner. We find that the lack of a\nstandard evaluation protocol makes previous works, even open-sourced ones,\nnon-reproducible, with significant variance between random runs. Therefore, we\nintroduce a more robust evaluation protocol to stabilize comparisons. Our study\nreveals which components and designs are crucial for effective agents, while\nothers are redundant, despite seeming logical. Based on our findings, we build\nand open-source OAgents, a new foundation agent framework that achieves\nstate-of-the-art performance among open-source projects. OAgents offers a\nmodular design for various agent components, promoting future research in\nAgentic AI.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.15741.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "628c8598ef14f971b698107f",
      "avatarUrl": "/avatars/3a4ad87e6b5f9e836a1160d869df1447.svg",
      "fullname": "Zhou",
      "name": "Wangchunshu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.18903",
      "authors": [
        {
          "_id": "685a1ce80e4ad7e2197585b7",
          "name": "Runjia Li",
          "hidden": false
        },
        {
          "_id": "685a1ce80e4ad7e2197585b8",
          "name": "Philip Torr",
          "hidden": false
        },
        {
          "_id": "685a1ce80e4ad7e2197585b9",
          "name": "Andrea Vedaldi",
          "hidden": false
        },
        {
          "_id": "685a1ce80e4ad7e2197585ba",
          "name": "Tomas Jakab",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/638e29cf319f9c746b87ad4b/Gt6zbaM3ILEQIG4Cl1W2J.mp4"
      ],
      "publishedAt": "2025-06-23T17:59:56.000Z",
      "submittedOnDailyAt": "2025-06-24T02:12:37.233Z",
      "title": "VMem : Mémoire visuelle pour l'utilisation d'indices de serpent pour la génération de scénarios interactifs de vidéo cohérents",
      "submittedOnDailyBy": {
        "_id": "638e29cf319f9c746b87ad4b",
        "avatarUrl": "/avatars/70cac8d47847c389eb0393051a64c4a4.svg",
        "isPro": true,
        "fullname": "Runjia Li",
        "user": "liguang0115",
        "type": "user"
      },
      "summary": "Nous proposons une nouvelle structure de mémoire. Cette structure permet de construire un générateur de vidéos capables d'explorer l'environnement de manière interactive. Antérieurement, des résultats similaires ont été atteints en étendant la vision 2D vers l'extérieur à travers un méthode qui reconstruit de manière progressive la géométrie 3D, mais cette méthode accumulait rapidement des erreurs. De plus, les mêmes résultats ont été obtenus avec des générateurs de vidéos qui maintenaient une cohérence à long terme. Pour résoudre ces limitations, nous introduisons un mécanisme d'indexation géométrique basé sur les surfaces d'éléments (surfels) pour mémoriser des visions passées, appelée Mémoire des Visions Indexées par Surfels (VMem). VMem permet d'obtenir des visions passées pertinentes de manière efficace lors de la création d'une nouvelle vision. De cette manière, une exploration cohérente de l'environnement imaginaire peut être réalisée avec un coût informatique moins élevé, comparé à l'utilisation de parties du passé comme contexte. Notre méthode a été évaluée sur des benchmarks de synthèse de vision à long terme et montre un excellent rendement en termes de cohérence de la vision et de contrôle de la caméra, comparé aux méthodes existantes.",
      "upvotes": 8,
      "discussionId": "685a1ce80e4ad7e2197585bb",
      "projectPage": "https://v-mem.github.io/",
      "githubRepo": "https://github.com/runjiali-rl/vmem",
      "ai_summary": "A novel memory mechanism called Surfel-Indexed View Memory enhances video generation by efficiently remembering and retrieving relevant past views, improving long-term scene coherence and reducing computational cost.",
      "ai_keywords": [
        "memory mechanism",
        "video generators",
        "out-painting",
        "3D geometry",
        "context window",
        "Surfel-Indexed View Memory",
        "surfels",
        "scene coherence",
        "camera control",
        "long-term scene synthesis benchmarks"
      ]
    },
    "publishedAt": "2025-06-23T13:59:56.000Z",
    "title": "VMem: Consistent Interactive Video Scene Generation with Surfel-Indexed\n  View Memory",
    "summary": "We propose a novel memory mechanism to build video generators that can\nexplore environments interactively. Similar results have previously been\nachieved by out-painting 2D views of the scene while incrementally\nreconstructing its 3D geometry, which quickly accumulates errors, or by video\ngenerators with a short context window, which struggle to maintain scene\ncoherence over the long term. To address these limitations, we introduce\nSurfel-Indexed View Memory (VMem), a mechanism that remembers past views by\nindexing them geometrically based on the 3D surface elements (surfels) they\nhave observed. VMem enables the efficient retrieval of the most relevant past\nviews when generating new ones. By focusing only on these relevant views, our\nmethod produces consistent explorations of imagined environments at a fraction\nof the computational cost of using all past views as context. We evaluate our\napproach on challenging long-term scene synthesis benchmarks and demonstrate\nsuperior performance compared to existing methods in maintaining scene\ncoherence and camera control.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/638e29cf319f9c746b87ad4b/Gt6zbaM3ILEQIG4Cl1W2J.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18903.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "638e29cf319f9c746b87ad4b",
      "avatarUrl": "/avatars/70cac8d47847c389eb0393051a64c4a4.svg",
      "fullname": "Runjia Li",
      "name": "liguang0115",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.18309",
      "authors": [
        {
          "_id": "685a2df10e4ad7e219758604",
          "name": "Lu Wang",
          "hidden": false
        },
        {
          "_id": "685a2df10e4ad7e219758605",
          "name": "Di Zhang",
          "hidden": false
        },
        {
          "_id": "685a2df10e4ad7e219758606",
          "name": "Fangkai Yang",
          "hidden": false
        },
        {
          "_id": "685a2df10e4ad7e219758607",
          "name": "Pu Zhao",
          "hidden": false
        },
        {
          "_id": "685a2df10e4ad7e219758608",
          "name": "Jianfeng Liu",
          "hidden": false
        },
        {
          "_id": "685a2df10e4ad7e219758609",
          "name": "Yuefeng Zhan",
          "hidden": false
        },
        {
          "_id": "685a2df10e4ad7e21975860a",
          "name": "Hao Sun",
          "hidden": false
        },
        {
          "_id": "685a2df10e4ad7e21975860b",
          "name": "Qingwei Lin",
          "hidden": false
        },
        {
          "_id": "685a2df10e4ad7e21975860c",
          "name": "Weiwei Deng",
          "hidden": false
        },
        {
          "_id": "685a2df10e4ad7e21975860d",
          "name": "Dongmei Zhang",
          "hidden": false
        },
        {
          "_id": "685a2df10e4ad7e21975860e",
          "name": "Feng Sun",
          "hidden": false
        },
        {
          "_id": "685a2df10e4ad7e21975860f",
          "name": "Qi Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-23T05:51:52.000Z",
      "submittedOnDailyAt": "2025-06-24T03:21:15.103Z",
      "title": "RITENGO : Exploration du système de recommandations pour la génération de profils de utilisateurs",
      "submittedOnDailyBy": {
        "_id": "654dbac9938fbf1e696be8aa",
        "avatarUrl": "/avatars/b3c4035c48169c1bfb04a439fce3499f.svg",
        "isPro": false,
        "fullname": "Chaoyun Zhang",
        "user": "vyokky",
        "type": "user"
      },
      "summary": "Le profilage des utilisateurs est essentiel dans les systèmes de recommandation, car il permet de transformer les données d'interaction des utilisateurs en une représentation structurée et simple, ce qui permet de générer des recommandations personnalisées. Le profilage traditionnel basé sur les embeddings présente des limitations en termes d'interprétabilité et d'adaptabilité, mais le développement de grands modèles de langage (LLMs) récents a permis la création d'un profilage basé sur le texte significatif et transparent. Cependant, les méthodes actuelles dépendent de formats fixes et ont des limitations pour comprendre la diversité des actions des utilisateurs. Dans cet article, nous présentons LettinGo, un nouveau cadre de travail qui utilise la capacité des LLMs pour générer un profilage des utilisateurs avec diversité et adaptabilité, et qui reçoit une rétroaction directe des tâches de recommandation. Cette approche évite les restrictions causées par l'ajustement micro (SFT) et utilise l'Optimisation des Préférences Directes (DPO) pour garantir que le générateur de profilages soit adaptable et efficace en fonction du rendement spécifique de la tâche. LettinGo fonctionne en trois étapes : 1. exploration de différents profilages des utilisateurs, 2. évaluation de la qualité du profilage basée sur son impact sur le système de recommandation, et 3. ajustement du générateur de profilages basé sur les données de préférences obtenues du rendement de la tâche. Les résultats des expérimentations montrent que notre cadre de travail améliore significativement la précision de la recommandation, la flexibilité et la compréhension du contexte. Cette recherche représente une innovation importante dans la génération de profilages des utilisateurs pour les futurs systèmes de recommandation.",
      "upvotes": 6,
      "discussionId": "685a2df20e4ad7e219758610",
      "ai_summary": "LettinGo enhances user profiling via diverse, adaptive profiles generated using LLMs and Direct Preference Optimization, improving recommendation accuracy and flexibility.",
      "ai_keywords": [
        "large language models (LLMs)",
        "semantically richer",
        "more transparent",
        "supervised fine-tuning (SFT)",
        "Direct Preference Optimization (DPO)",
        "profile generator",
        "pairwise preference data",
        "recommendation systems",
        "recommendation accuracy",
        "flexibility",
        "contextual awareness"
      ]
    },
    "publishedAt": "2025-06-23T01:51:52.000Z",
    "title": "LettinGo: Explore User Profile Generation for Recommendation System",
    "summary": "User profiling is pivotal for recommendation systems, as it transforms raw\nuser interaction data into concise and structured representations that drive\npersonalized recommendations. While traditional embedding-based profiles lack\ninterpretability and adaptability, recent advances with large language models\n(LLMs) enable text-based profiles that are semantically richer and more\ntransparent. However, existing methods often adhere to fixed formats that limit\ntheir ability to capture the full diversity of user behaviors. In this paper,\nwe introduce LettinGo, a novel framework for generating diverse and adaptive\nuser profiles. By leveraging the expressive power of LLMs and incorporating\ndirect feedback from downstream recommendation tasks, our approach avoids the\nrigid constraints imposed by supervised fine-tuning (SFT). Instead, we employ\nDirect Preference Optimization (DPO) to align the profile generator with\ntask-specific performance, ensuring that the profiles remain adaptive and\neffective. LettinGo operates in three stages: (1) exploring diverse user\nprofiles via multiple LLMs, (2) evaluating profile quality based on their\nimpact in recommendation systems, and (3) aligning the profile generation\nthrough pairwise preference data derived from task performance. Experimental\nresults demonstrate that our framework significantly enhances recommendation\naccuracy, flexibility, and contextual awareness. This work enhances profile\ngeneration as a key innovation for next-generation recommendation systems.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18309.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "654dbac9938fbf1e696be8aa",
      "avatarUrl": "/avatars/b3c4035c48169c1bfb04a439fce3499f.svg",
      "fullname": "Chaoyun Zhang",
      "name": "vyokky",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.18631",
      "authors": [
        {
          "_id": "685a1b390e4ad7e2197585a9",
          "name": "Chenxing Wei",
          "hidden": false
        },
        {
          "_id": "685a1b390e4ad7e2197585aa",
          "name": "Jiarui Yu",
          "hidden": false
        },
        {
          "_id": "685a1b390e4ad7e2197585ab",
          "name": "Ying Tiffany He",
          "hidden": false
        },
        {
          "_id": "685a1b390e4ad7e2197585ac",
          "name": "Hande Dong",
          "hidden": false
        },
        {
          "_id": "685a1b390e4ad7e2197585ad",
          "name": "Yao Shu",
          "hidden": false
        },
        {
          "_id": "685a1b390e4ad7e2197585ae",
          "name": "Fei Yu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/65ed3051492a7f35db21fea2/Oc7EWF5wDYqu1FEvfuAlq.png"
      ],
      "publishedAt": "2025-06-23T13:36:24.000Z",
      "submittedOnDailyAt": "2025-06-24T02:05:49.825Z",
      "title": "Redit: Amélioration de l'Optimisation des Politiques d'Apprentissage des Modèles de Langage avec le Design de Récompenses",
      "submittedOnDailyBy": {
        "_id": "65ed3051492a7f35db21fea2",
        "avatarUrl": "/avatars/4fc0ccc21aa88e4e8ff74a6f850570b8.svg",
        "isPro": false,
        "fullname": "Chenxing Wei",
        "user": "kittttttt",
        "type": "user"
      },
      "summary": "DeepSeek-R1 a amélioré avec succès la capacité d'inférence d'un modèle de langage grand (LLM) en utilisant un système de récompenses basé sur des règles (Reward System). Ce système de récompenses montre une \"complétude\" et inhibe effectivement le hacking de récompenses, mais sa fonction de récompense est généralement discrète (discrete). À partir de nos observations expérimentales, cette discrétude de récompense conduit à des problèmes tels que l'instabilité du gradient, l'optimisation instable et la convergence progressive. Pour résoudre ces problèmes, nous proposons ReDit (Reward Design). ReDit est un méthode pour concevoir des signaux de récompenses discrètes en ajoutant simplement de l'aléa. Avec cette récompense aléatoire, un gradient d'exploration continu est maintenu tout au long du processus d'apprentissage, permettant des mises à jour de gradients doux et accélérant la convergence. L'aléa injecté attire l'attention sur les particularités des domaines doux de récompense et incite le modèle à trouver de nouvelles politiques pour échapper aux optima locaux. Les expériences sur différentes tâches démontrent l'efficacité et l'efficience de ReDit. En moyenne, ReDit atteint un rendement équivalent à GRPO en moins de 10% de phases d'apprentissage et montre un amélioration de rendement de 4% dans une période d'apprentissage de même durée. Un grand effet est confirmé sur l'inhibition des problèmes de gradient dans ReDit. De plus, des analyses théoriques sont fournies pour démontrer davantage cette excellence.",
      "upvotes": 5,
      "discussionId": "685a1b390e4ad7e2197585af",
      "githubRepo": "https://github.com/kithib/ReDit",
      "ai_summary": "ReDit, a reward dithering method, addresses issues in discrete reward systems by introducing noise, leading to smoother optimization and faster convergence compared to standard methods.",
      "ai_keywords": [
        "rule-based reward system",
        "reward hacking",
        "discrete rewards",
        "gradient anomaly",
        "unstable optimization",
        "slow convergence",
        "random noise",
        "exploratory gradients",
        "flat reward regions",
        "local optima",
        "vanilla GRPO",
        "performance improvement",
        "gradient issues"
      ]
    },
    "publishedAt": "2025-06-23T09:36:24.000Z",
    "title": "ReDit: Reward Dithering for Improved LLM Policy Optimization",
    "summary": "DeepSeek-R1 has successfully enhanced Large Language Model (LLM) reasoning\ncapabilities through its rule-based reward system. While it's a ''perfect''\nreward system that effectively mitigates reward hacking, such reward functions\nare often discrete. Our experimental observations suggest that discrete rewards\ncan lead to gradient anomaly, unstable optimization, and slow convergence. To\naddress this issue, we propose ReDit (Reward Dithering), a method that dithers\nthe discrete reward signal by adding simple random noise. With this perturbed\nreward, exploratory gradients are continuously provided throughout the learning\nprocess, enabling smoother gradient updates and accelerating convergence. The\ninjected noise also introduces stochasticity into flat reward regions,\nencouraging the model to explore novel policies and escape local optima.\nExperiments across diverse tasks demonstrate the effectiveness and efficiency\nof ReDit. On average, ReDit achieves performance comparable to vanilla GRPO\nwith only approximately 10% the training steps, and furthermore, still exhibits\na 4% performance improvement over vanilla GRPO when trained for a similar\nduration. Visualizations confirm significant mitigation of gradient issues with\nReDit. Moreover, theoretical analyses are provided to further validate these\nadvantages.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/65ed3051492a7f35db21fea2/Oc7EWF5wDYqu1FEvfuAlq.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18631.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65ed3051492a7f35db21fea2",
      "avatarUrl": "/avatars/4fc0ccc21aa88e4e8ff74a6f850570b8.svg",
      "fullname": "Chenxing Wei",
      "name": "kittttttt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.18527",
      "authors": [
        {
          "_id": "685a0cba0e4ad7e21975850c",
          "name": "JiaKui Hu",
          "hidden": false
        },
        {
          "_id": "685a0cba0e4ad7e21975850d",
          "name": "Yuxiao Yang",
          "hidden": false
        },
        {
          "_id": "685a0cba0e4ad7e21975850e",
          "name": "Jialun Liu",
          "hidden": false
        },
        {
          "_id": "685a0cba0e4ad7e21975850f",
          "name": "Jinbo Wu",
          "hidden": false
        },
        {
          "_id": "685a0cba0e4ad7e219758510",
          "name": "Chen Zhao",
          "hidden": false
        },
        {
          "_id": "685a0cba0e4ad7e219758511",
          "name": "Yanye Lu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-23T11:28:37.000Z",
      "submittedOnDailyAt": "2025-06-24T03:17:08.044Z",
      "title": "Automatiquement, nous générons des images de coincidence sur plusieurs points à travers l'inversion automatique.",
      "submittedOnDailyBy": {
        "_id": "64ccd5cc4726a3f833831087",
        "avatarUrl": "/avatars/6364b1ebb1d06c68245f4c786fb07ee9.svg",
        "isPro": false,
        "fullname": "Hu",
        "user": "Jiakui",
        "type": "user"
      },
      "summary": "Les images multi-vue sont un élément important dans la génération de contenu 3D. L'un des principaux problèmes est la coïncidence en multiple vues et la synthèse valide de formes et de textures dans différentes conditions. Dans cet article, nous proposons le méthode de régression automatique multi-vue (MV-AR) et nous visons à utiliser des modèles de régression automatique pour générer des images multi-vue en séquence à partir de n'importe quel prompt. Tout d'abord, le pouvoir prédictif des tokens suivants du modèle de régression automatique améliore significativement l'efficacité de la synthèse séquentielle d'images multi-vue. Lorsque les vues sont très éloignées, MV-AR peut extraire de l'information de référence valide en utilisant tous les vues antérieures. De plus, nous proposons un modèle intégré qui combine le design structurel et l'étape d'entraînement pour aborder divers prompts. Pour traiter différentes conditions, nous introduisons un module d'entrée de conditions qui prend en compte le contexte, la position de la caméra, les images et les formes. Pour gérer plusieurs conditions simultanément, nous utilisons un entraînement séquentiel. Cette étape permet d'implémenter le modèle MV-AR (X2mv) basé sur le modèle t2mv, où des conditions aléatoires sont générées et combinées à partir de texte. Enfin, pour résoudre le problème d'overfitting due aux données de haute qualité limitées, nous proposons la technique \"Shuffle View\" pour amplifier les données d'entraînement. Les expériences ont démontré l'efficacité et la diversité du MV-AR, la génération d'images multi-vue en conditions variées, et un rendement comparable avec des modèles avancés de génération d'images multi-vue basés sur la diffusion. Les codes et les modèles sont prévus pour être publiés sur https://github.com/MILab-PKU/MVAR.",
      "upvotes": 4,
      "discussionId": "685a0cbb0e4ad7e219758512",
      "ai_summary": "The Multi-View Auto-Regressive (MV-AR) method uses an auto-regressive model to generate consistent multi-view images from prompts, addressing challenges in shape and texture synthesis across diverse conditions.",
      "ai_keywords": [
        "Multi-View Auto-Regressive",
        "MV-AR",
        "auto-regressive model",
        "next-token-prediction",
        "condition injection modules",
        "text-to-multi-view",
        "X-to-multi-view",
        "progressive training strategy",
        "Shuffle View",
        "data augmentation"
      ]
    },
    "publishedAt": "2025-06-23T07:28:37.000Z",
    "title": "Auto-Regressively Generating Multi-View Consistent Images",
    "summary": "Generating multi-view images from human instructions is crucial for 3D\ncontent creation. The primary challenges involve maintaining consistency across\nmultiple views and effectively synthesizing shapes and textures under diverse\nconditions. In this paper, we propose the Multi-View Auto-Regressive (MV-AR)\nmethod, which leverages an auto-regressive model to progressively generate\nconsistent multi-view images from arbitrary prompts. Firstly, the\nnext-token-prediction capability of the AR model significantly enhances its\neffectiveness in facilitating progressive multi-view synthesis. When generating\nwidely-separated views, MV-AR can utilize all its preceding views to extract\neffective reference information. Subsequently, we propose a unified model that\naccommodates various prompts via architecture designing and training\nstrategies. To address multiple conditions, we introduce condition injection\nmodules for text, camera pose, image, and shape. To manage multi-modal\nconditions simultaneously, a progressive training strategy is employed. This\nstrategy initially adopts the text-to-multi-view (t2mv) model as a baseline to\nenhance the development of a comprehensive X-to-multi-view (X2mv) model through\nthe randomly dropping and combining conditions. Finally, to alleviate the\noverfitting problem caused by limited high-quality data, we propose the\n\"Shuffle View\" data augmentation technique, thus significantly expanding the\ntraining data by several magnitudes. Experiments demonstrate the performance\nand versatility of our MV-AR, which consistently generates consistent\nmulti-view images across a range of conditions and performs on par with leading\ndiffusion-based multi-view image generation models. Code and models will be\nreleased at https://github.com/MILab-PKU/MVAR.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18527.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ccd5cc4726a3f833831087",
      "avatarUrl": "/avatars/6364b1ebb1d06c68245f4c786fb07ee9.svg",
      "fullname": "Hu",
      "name": "Jiakui",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.18349",
      "authors": [
        {
          "_id": "685a02460e4ad7e2197584a9",
          "user": {
            "_id": "659c6a50615d5e661222fe16",
            "avatarUrl": "/avatars/8a946482e49a821dbe397dc3898f22c5.svg",
            "isPro": false,
            "fullname": "Zichong Li",
            "user": "Pearush",
            "type": "user"
          },
          "name": "Zichong Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-24T08:08:56.270Z",
          "hidden": false
        },
        {
          "_id": "685a02460e4ad7e2197584aa",
          "name": "Chen Liang",
          "hidden": false
        },
        {
          "_id": "685a02460e4ad7e2197584ab",
          "name": "Zixuan Zhang",
          "hidden": false
        },
        {
          "_id": "685a02460e4ad7e2197584ac",
          "name": "Ilgee Hong",
          "hidden": false
        },
        {
          "_id": "685a02460e4ad7e2197584ad",
          "name": "Young Jin Kim",
          "hidden": false
        },
        {
          "_id": "685a02460e4ad7e2197584ae",
          "name": "Weizhu Chen",
          "hidden": false
        },
        {
          "_id": "685a02460e4ad7e2197584af",
          "name": "Tuo Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-23T07:15:59.000Z",
      "submittedOnDailyAt": "2025-06-24T02:19:52.155Z",
      "title": "SlimMoE : Structure de compression de modèles MoE grands et slicing d'experts avec réchauffement",
      "submittedOnDailyBy": {
        "_id": "63e6b5e22d2c508de9001afd",
        "avatarUrl": "/avatars/43cec9e8b8d490bd259e383954846a1e.svg",
        "isPro": false,
        "fullname": "Chen Liang",
        "user": "cliang1453",
        "type": "user"
      },
      "summary": "La architecture des Experts Mixte (MoE) a émergé comme un paradigme puissant pour élargir les grands modèles de langue (LLMs) tout en maintenant l'efficacité de l'inférence. Cependant, sa forte demande en mémoire pose des défis pour le fine-tuning ou la déploiement dans des environnements à des ressources limitées. Pour aborder ces défis, nous présentons SlimMoE. SlimMoE est un cadre de compression multi-niveau qui transforme des modèles MoE élargis en versions efficaces en les réduisant significativement, conçu pour éviter les coûts prohibitifs d'entraînement à partir du départ. Notre approche consiste à simplifier le modèle d'experts et à transmettre les connaissances en étapes intermédiaires, ce qui permet de réduire systématiquement le nombre de paramètres et de mitigérer la perte de performance due à l'accès de blocs. Ce cadre de travail permet que le modèle Phi 3.5-MoE (avec un total de 41,9B/activation de 16,6B paramètres) soit compressé en Phi-mini-MoE (avec un total de 7,6B/activation de 2,4B paramètres) et Phi-tiny-MoE (avec un total de 3,8B/activation de 1,1B paramètres). Ces modèles compressés peuvent être fine-tuned sur un seul GPU (A100 pour Phi-mini-MoE, A6000 pour Phi-tiny-MoE) et offrent une haute qualité dans des applications académiques ou dans des environnements à des ressources limitées. Les résultats des expériences montrent que ces modèles compressés dépassent les modèles de même taille et concurent avec les modèles plus grands. Par exemple, Phi-mini-MoE utilise 2/3 des paramètres actifs et atteint ou dépasse le rendement de Phi-3-mini, présentant une note MMLU comparable à Llama 3.1 8B avec un important économise de coûts. Notre résultat démontre que la combinaison d'approches structurelles et de dégradation en étapes permet de créer des modèles de haute qualité, favorisant l'adoption large de l'architecture MoE. Nos modèles peuvent être trouvés sur les URLs suivantes :\n\nhttps://huggingface.co/microsoft/Phi-mini-MoE-instruct\nhttps://huggingface.co/microsoft/Phi-tiny-MoE-instruct",
      "upvotes": 4,
      "discussionId": "685a02460e4ad7e2197584b0",
      "ai_summary": "SlimMoE compresses large MoE models into smaller, efficient variants using multi-stage compression without full retraining, maintaining competitive performance with significantly fewer resources.",
      "ai_keywords": [
        "Mixture of Experts (MoE)",
        "large language models (LLMs)",
        "parameter counts",
        "knowledge transfer",
        "one-shot pruning",
        "Phi 3.5-MoE",
        "Phi-mini-MoE",
        "Phi-tiny-MoE",
        "structured pruning",
        "staged distillation",
        "MMLU scores",
        "latency"
      ]
    },
    "publishedAt": "2025-06-23T03:15:59.000Z",
    "title": "SlimMoE: Structured Compression of Large MoE Models via Expert Slimming\n  and Distillation",
    "summary": "The Mixture of Experts (MoE) architecture has emerged as a powerful paradigm\nfor scaling large language models (LLMs) while maintaining inference\nefficiency. However, their enormous memory requirements make them prohibitively\nexpensive to fine-tune or deploy in resource-constrained environments. To\naddress this challenge, we introduce SlimMoE, a multi-stage compression\nframework for transforming large MoE models into much smaller, efficient\nvariants without incurring the prohibitive costs of training from scratch. Our\nmethod systematically reduces parameter counts by slimming experts and\ntransferring knowledge through intermediate stages, effectively mitigating the\nperformance degradation common in one-shot pruning approaches. Using this\nframework, we compress Phi 3.5-MoE (41.9B total/6.6B activated parameters) to\ncreate Phi-mini-MoE (7.6B total/2.4B activated parameters) and Phi-tiny-MoE\n(3.8B total/1.1B activated parameters) using only 400B tokens--less than 10% of\nthe original model's training data. These compressed models can be fine-tuned\non a single GPU (A100 for Phi-mini-MoE, A6000 for Phi-tiny-MoE), making them\nhighly suitable for academic and resource-limited settings. Our experiments\ndemonstrate that these compressed models outperform others of similar size and\nremain competitive with larger models. For instance, Phi-mini-MoE achieves\nsimilar or better performance to Phi-3-mini using only 2/3 of the activated\nparameters and yields comparable MMLU scores to Llama 3.1 8B despite having\nsignificantly lower latency. Our findings demonstrate that structured pruning\ncombined with staged distillation offers an effective path to creating\nhigh-quality, compact MoE models, paving the way for broader adoption of MoE\narchitectures. We make our models publicly available at\nhttps://huggingface.co/microsoft/Phi-mini-MoE-instruct and\nhttps://huggingface.co/microsoft/Phi-tiny-MoE-instruct .",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18349.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63e6b5e22d2c508de9001afd",
      "avatarUrl": "/avatars/43cec9e8b8d490bd259e383954846a1e.svg",
      "fullname": "Chen Liang",
      "name": "cliang1453",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.16962",
      "authors": [
        {
          "_id": "6858d907c0c8e29df8ea3ce2",
          "user": {
            "_id": "67547707f168984215451697",
            "avatarUrl": "/avatars/630329ed6585036d60cdc27490cc01b0.svg",
            "isPro": false,
            "fullname": "manglu",
            "user": "manglu3935",
            "type": "user"
          },
          "name": "Haoran Sun",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-23T15:52:05.938Z",
          "hidden": false
        },
        {
          "_id": "6858d907c0c8e29df8ea3ce3",
          "name": "Yankai Jiang",
          "hidden": false
        },
        {
          "_id": "6858d907c0c8e29df8ea3ce4",
          "name": "Wenjie Lou",
          "hidden": false
        },
        {
          "_id": "6858d907c0c8e29df8ea3ce5",
          "name": "Yujie Zhang",
          "hidden": false
        },
        {
          "_id": "6858d907c0c8e29df8ea3ce6",
          "name": "Wenjie Li",
          "hidden": false
        },
        {
          "_id": "6858d907c0c8e29df8ea3ce7",
          "name": "Lilong Wang",
          "hidden": false
        },
        {
          "_id": "6858d907c0c8e29df8ea3ce8",
          "name": "Mianxin Liu",
          "hidden": false
        },
        {
          "_id": "6858d907c0c8e29df8ea3ce9",
          "name": "Lei Liu",
          "hidden": false
        },
        {
          "_id": "6858d907c0c8e29df8ea3cea",
          "name": "Xiaosong Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-20T12:51:19.000Z",
      "submittedOnDailyAt": "2025-06-24T02:20:40.889Z",
      "title": "Amélioration de la possibilité de vérification logique médicale des micro-bibliothèques par étapes",
      "submittedOnDailyBy": {
        "_id": "67547707f168984215451697",
        "avatarUrl": "/avatars/630329ed6585036d60cdc27490cc01b0.svg",
        "isPro": false,
        "fullname": "manglu",
        "user": "manglu3935",
        "type": "user"
      },
      "summary": "Les modules de langage de l'Université de Damodar (MLLMs) commencent à montrer une forte capacité logique dans des tâches générales, mais leur application dans le domaine médical est encore au stade initial. Pour renforcer la capacité logique des MLLMs médicaux, il est crucial de construire des données d'entraînement de \"Chain-of-Thought\" (CoT). Cependant, la méthodologie actuelle ne fournit pas un cadre pour chercher et évaluer les étapes logiques valides pour des diagnostics importants. Pour résoudre ces défis, nous proposons la Mentor-Intern Collaboration Search (MICS). MICS est un nouveau schéma de recherche d'étapes logiques pour générer des données CoT strictes et valides dans le domaine médical. MICS commence avec le Modèle Majorité pour initier le processus logique, puis active chaque modèle de projet selon l'étape initiale, et finalement sélectionne l'étape logique la plus optimale en fonction du rendement logique de multiples modèles de projet. Le rendement logique est évalué par le MICS-Score, qui mesure la qualité des étapes logiques générées. Enfin, nous avons construit le nouveau MLLM médical Chiron-o1 en utilisant le jeu de données complexes MMRP et des stratégies d'enseignement de matières. Chiron-o1 a été entraîné avec les données CoT générées par MICS et a atteint les meilleurs résultats dans le domaine de la visualisation médicale et dans les cadres de référence logiques. Le code est disponible sur GitHub - manglu097/Chiron-o1: Code pour améliorer la logique et la validation médicale dans les MLLMs de manière itérative.",
      "upvotes": 4,
      "discussionId": "6858d907c0c8e29df8ea3ceb",
      "githubRepo": "https://github.com/manglu097/Chiron-o1",
      "ai_summary": "MICS, a novel reasoning-path searching scheme, enhances medical MLLMs like Chiron-o1 with robust generalizable reasoning and visual question-answering capabilities through comprehensive chain-of-thought data generation.",
      "ai_keywords": [
        "multimodal large language models",
        "MLLMs",
        "chain-of-thought",
        "Mentor-Intern Collaborative Search",
        "MICS",
        "mentor models",
        "intern models",
        "MICS-Score",
        "multi-task medical reasoning dataset",
        "MMRP",
        "curriculum learning",
        "medical visual question answering",
        "reasoning benchmarks"
      ]
    },
    "publishedAt": "2025-06-20T08:51:19.000Z",
    "title": "Enhancing Step-by-Step and Verifiable Medical Reasoning in MLLMs",
    "summary": "Multimodal large language models (MLLMs) have begun to demonstrate robust\nreasoning capabilities on general tasks, yet their application in the medical\ndomain remains in its early stages. Constructing chain-of-thought (CoT)\ntraining data is essential for bolstering the reasoning abilities of medical\nMLLMs. However, existing approaches exhibit a deficiency in offering a\ncomprehensive framework for searching and evaluating effective reasoning paths\ntowards critical diagnosis. To address this challenge, we propose Mentor-Intern\nCollaborative Search (MICS), a novel reasoning-path searching scheme to\ngenerate rigorous and effective medical CoT data. MICS first leverages mentor\nmodels to initialize the reasoning, one step at a time, then prompts each\nintern model to continue the thinking along those initiated paths, and finally\nselects the optimal reasoning path according to the overall reasoning\nperformance of multiple intern models. The reasoning performance is determined\nby an MICS-Score, which assesses the quality of generated reasoning paths.\nEventually, we construct MMRP, a multi-task medical reasoning dataset with\nranked difficulty, and Chiron-o1, a new medical MLLM devised via a curriculum\nlearning strategy, with robust visual question-answering and generalizable\nreasoning capabilities. Extensive experiments demonstrate that Chiron-o1,\ntrained on our CoT dataset constructed using MICS, achieves state-of-the-art\nperformance across a list of medical visual question answering and reasoning\nbenchmarks. Codes are available at GitHub - manglu097/Chiron-o1: Enhancing\nStep-by-Step and Verifiable Medical Reasoning in MLLMs",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.16962.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "67547707f168984215451697",
      "avatarUrl": "/avatars/630329ed6585036d60cdc27490cc01b0.svg",
      "fullname": "manglu",
      "name": "manglu3935",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.18792",
      "authors": [
        {
          "_id": "685a72a00e4ad7e219758702",
          "name": "Michal Nazarczuk",
          "hidden": false
        },
        {
          "_id": "685a72a00e4ad7e219758703",
          "name": "Sibi Catley-Chandar",
          "hidden": false
        },
        {
          "_id": "685a72a00e4ad7e219758704",
          "name": "Thomas Tanay",
          "hidden": false
        },
        {
          "_id": "685a72a00e4ad7e219758705",
          "name": "Zhensong Zhang",
          "hidden": false
        },
        {
          "_id": "685a72a00e4ad7e219758706",
          "name": "Gregory Slabaugh",
          "hidden": false
        },
        {
          "_id": "685a72a00e4ad7e219758707",
          "name": "Eduardo Pérez-Pellitero",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-23T16:01:15.000Z",
      "submittedOnDailyAt": "2025-06-24T08:15:41.084Z",
      "title": "ViDAR : Réconstruction 4D pour caméras d'intérêt - différenciation vidéo",
      "submittedOnDailyBy": {
        "_id": "66f44a3df9252e0f50b59fdb",
        "avatarUrl": "/avatars/4234203c77a0e6f594f3de26bfe8c649.svg",
        "isPro": false,
        "fullname": "Michal Nazarczuk",
        "user": "michaal94",
        "type": "user"
      },
      "summary": "La génération d'images dynamiques de nouveaux points de vue vise à créer des images de corps en mouvement depuis n'importe quel point de vue. Ce travail est particulièrement difficile lorsqu'il dépend de vidéos à une unique perspective, car il est complexe de séparer la structure et de générer des sous-images insuffisantes. Nous utilisons des modèles de diffusion personnalisés pour présenter le cadre de reconstruction 4D Video Diffusion-Aware Reconstruction (ViDAR), qui utilise des signaux de sous-images de multiples vues de Pizemá. ViDAR maintient la qualité visuelle et l'intégrité structurale, réduisant l'incertitude causée par une unique perspective. Pour aborder l'incertitude espace-temporelle des sous-images diffusives, nous proposons une fonction de diffusion Wavelet et l'optimisation de l'angle de la caméra Stereo. Les expériences sur le benchmark difficile DyCheck montrent que ViDAR dépasse tous les limites de base de la qualité visuelle et de l'intégrité structurale. De plus, ViDAR montre des améliorations significatives dans le domaine dynamique et fournit un nouveau benchmark pour comparer la reconstruction de mouvements riches. Le site web du projet est disponible sur https://vidar-4d.github.io.",
      "upvotes": 3,
      "discussionId": "685a72a10e4ad7e219758708",
      "ai_summary": "ViDAR uses diffusion-aware reconstruction to generate high-quality novel views of dynamic scenes from monocular video, outperforming existing methods in visual quality and geometric consistency.",
      "ai_keywords": [
        "Video Diffusion-Aware Reconstruction",
        "ViDAR",
        "Gaussian splatting",
        "diffusion models",
        "spatio-temporal inconsistency",
        "diffusion-aware loss function",
        "camera pose optimisation",
        "DyCheck benchmark"
      ]
    },
    "publishedAt": "2025-06-23T12:01:15.000Z",
    "title": "ViDAR: Video Diffusion-Aware 4D Reconstruction From Monocular Inputs",
    "summary": "Dynamic Novel View Synthesis aims to generate photorealistic views of moving\nsubjects from arbitrary viewpoints. This task is particularly challenging when\nrelying on monocular video, where disentangling structure from motion is\nill-posed and supervision is scarce. We introduce Video Diffusion-Aware\nReconstruction (ViDAR), a novel 4D reconstruction framework that leverages\npersonalised diffusion models to synthesise a pseudo multi-view supervision\nsignal for training a Gaussian splatting representation. By conditioning on\nscene-specific features, ViDAR recovers fine-grained appearance details while\nmitigating artefacts introduced by monocular ambiguity. To address the\nspatio-temporal inconsistency of diffusion-based supervision, we propose a\ndiffusion-aware loss function and a camera pose optimisation strategy that\naligns synthetic views with the underlying scene geometry. Experiments on\nDyCheck, a challenging benchmark with extreme viewpoint variation, show that\nViDAR outperforms all state-of-the-art baselines in visual quality and\ngeometric consistency. We further highlight ViDAR's strong improvement over\nbaselines on dynamic regions and provide a new benchmark to compare performance\nin reconstructing motion-rich parts of the scene. Project page:\nhttps://vidar-4d.github.io",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18792.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66f44a3df9252e0f50b59fdb",
      "avatarUrl": "/avatars/4234203c77a0e6f594f3de26bfe8c649.svg",
      "fullname": "Michal Nazarczuk",
      "name": "michaal94",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.17538",
      "authors": [
        {
          "_id": "685a33c50e4ad7e219758612",
          "name": "Yile Gu",
          "hidden": false
        },
        {
          "_id": "685a33c50e4ad7e219758613",
          "name": "Rohan Kadekodi",
          "hidden": false
        },
        {
          "_id": "685a33c50e4ad7e219758614",
          "name": "Hoang Nguyen",
          "hidden": false
        },
        {
          "_id": "685a33c50e4ad7e219758615",
          "user": {
            "_id": "6304ac1a412a1b9d381ca378",
            "avatarUrl": "/avatars/f4724eb5afc2a3b0e61e6da7bfa7be27.svg",
            "isPro": false,
            "fullname": "Keisuke Kamahori",
            "user": "kamahori",
            "type": "user"
          },
          "name": "Keisuke Kamahori",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-24T08:07:52.905Z",
          "hidden": false
        },
        {
          "_id": "685a33c50e4ad7e219758616",
          "name": "Yiyu Liu",
          "hidden": false
        },
        {
          "_id": "685a33c50e4ad7e219758617",
          "name": "Baris Kasikci",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-21T01:32:22.000Z",
      "submittedOnDailyAt": "2025-06-24T03:49:17.409Z",
      "title": "ConsumerBench : Benchmark des Consommateurs Marque de test pour applications d'IA générative sur des dispositifs de consommateurs",
      "submittedOnDailyBy": {
        "_id": "6304ac1a412a1b9d381ca378",
        "avatarUrl": "/avatars/f4724eb5afc2a3b0e61e6da7bfa7be27.svg",
        "isPro": false,
        "fullname": "Keisuke Kamahori",
        "user": "kamahori",
        "type": "user"
      },
      "summary": "Récemment, les applications de l'Intelligence Artificielle Générative (GenAI) se déplacent vers les dispositifs utilisateurs dans des environnements de nuage, soulevant de nouveaux défis dans la gestion des ressources, l'ingénierie du système et l'expérience utilisateur. Dans cet article, nous présentons un cadre de référence de tests \"ConsumerBench\" pour l'ingénierie du système et l'évaluation du temps de réponse des modèles GenAI exécutés sur les dispositifs utilisateurs. Au contraire des cadres de référence actuels, ConsumerBench ne suppose pas que les modèles aient accès unique à un GPU spécifique. ConsumerBench simule des scénarios d'applications multiples exécutées sur des matériels limités. De plus, ConsumerBench soutient des flux de travail variables qui simulent la collaboration de multiples applications pour des tâches complexes. ConsumerBench considère à la fois des métriques au niveau d'application (points d'entrée, objectifs de niveau de service (SLO)) et des métriques au niveau de système (utilisation de CPU/GPU, largeur de bande de mémoire). A travers des expériences étendues, ConsumerBench révèle l'adéquation de la comparaison des ressources, l'inégalité de la distribution extrême et les points d'efficacité maximale des serveurs statiques de modèle. De plus, il fournit des astuces pratiques pour les développeurs de modèles et les concepteurs de systèmes, révélant les avantages des noyaux personnalisés adaptés à l'architecture de GPU de consommation et le valeur des stratégies de programmation pour les SLO.",
      "upvotes": 3,
      "discussionId": "685a33c70e4ad7e219758618",
      "githubRepo": "https://github.com/efeslab/ConsumerBench",
      "ai_summary": "ConsumerBench evaluates GenAI system efficiency and response time on end-user devices through a comprehensive benchmarking framework, emphasizing realistic multi-application scenarios and customizable workflows.",
      "ai_keywords": [
        "Generative AI",
        "ConsumerBench",
        "system efficiency",
        "response time",
        "benchmarking framework",
        "multi-application scenarios",
        "application-level metrics",
        "latency",
        "Service Level Objective",
        "SLO",
        "system-level metrics",
        "CPU utilization",
        "GPU utilization",
        "memory bandwidth",
        "greedy allocation",
        "static model server configurations",
        "custom kernels",
        "SLO-aware scheduling strategies"
      ]
    },
    "publishedAt": "2025-06-20T21:32:22.000Z",
    "title": "ConsumerBench: Benchmarking Generative AI Applications on End-User\n  Devices",
    "summary": "The recent shift in Generative AI (GenAI) applications from cloud-only\nenvironments to end-user devices introduces new challenges in resource\nmanagement, system efficiency, and user experience. This paper presents\nConsumerBench, a comprehensive benchmarking framework designed to evaluate the\nsystem efficiency and response time of GenAI models running on end-user\ndevices. Unlike existing benchmarks that assume exclusive model access on\ndedicated GPUs, ConsumerBench simulates realistic multi-application scenarios\nexecuting concurrently on constrained hardware. Furthermore, ConsumerBench\nsupports customizable workflows that simulate complex tasks requiring\ncoordination among multiple applications. ConsumerBench captures both\napplication-level metrics, including latency and Service Level Objective (SLO)\nattainment, and system-level metrics like CPU/GPU utilization and memory\nbandwidth. Through extensive experiments, ConsumerBench reveals inefficiencies\nin resource sharing, unfair scheduling under greedy allocation, and performance\npitfalls of static model server configurations. The paper also provides\npractical insights for model developers and system designers, highlighting the\nbenefits of custom kernels tailored to consumer-grade GPU architectures and the\nvalue of implementing SLO-aware scheduling strategies.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.17538.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6304ac1a412a1b9d381ca378",
      "avatarUrl": "/avatars/f4724eb5afc2a3b0e61e6da7bfa7be27.svg",
      "fullname": "Keisuke Kamahori",
      "name": "kamahori",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.18879",
      "authors": [
        {
          "_id": "685a1a090e4ad7e21975859c",
          "name": "Junyan Li",
          "hidden": false
        },
        {
          "_id": "685a1a090e4ad7e21975859d",
          "name": "Yang Zhang",
          "hidden": false
        },
        {
          "_id": "685a1a090e4ad7e21975859e",
          "name": "Muhammad Yusuf Hassan",
          "hidden": false
        },
        {
          "_id": "685a1a090e4ad7e21975859f",
          "name": "Talha Chafekar",
          "hidden": false
        },
        {
          "_id": "685a1a090e4ad7e2197585a0",
          "name": "Tianle Cai",
          "hidden": false
        },
        {
          "_id": "685a1a090e4ad7e2197585a1",
          "name": "Zhile Ren",
          "hidden": false
        },
        {
          "_id": "685a1a090e4ad7e2197585a2",
          "name": "Pengsheng Guo",
          "hidden": false
        },
        {
          "_id": "685a1a090e4ad7e2197585a3",
          "name": "Foroozan Karimzadeh",
          "hidden": false
        },
        {
          "_id": "685a1a090e4ad7e2197585a4",
          "name": "Colorado Reed",
          "hidden": false
        },
        {
          "_id": "685a1a090e4ad7e2197585a5",
          "name": "Chong Wang",
          "hidden": false
        },
        {
          "_id": "685a1a090e4ad7e2197585a6",
          "name": "Chuang Gan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-23T17:50:11.000Z",
      "submittedOnDailyAt": "2025-06-24T01:53:55.727Z",
      "title": "CommVQ : Compression de cache KV en utilisant des résumés vectoriels interchangeables",
      "submittedOnDailyBy": {
        "_id": "62d09eb86a61a88ea0d83918",
        "avatarUrl": "/avatars/81b511d94cced304ffca058caff662d4.svg",
        "isPro": false,
        "fullname": "Junyan Li",
        "user": "senfu",
        "type": "user"
      },
      "summary": "Les modèles de langage grands (LLMs) augmentent leur utilisation dans des applications qui nécessitent de longues séquences de contexte, mais lorsque le contexte s'étend dans la GPU, le cache de clé-valeur (KV) est forcé à rompre la mémoire. Pour y faire face, on propose la technique des Vecteurs de CommVQ (CommVQ) pour réduire significativement l'utilisation de la mémoire lors de l'inférence de LLMs avec de longs contextes. Tout d'abord, on utilise un encodeur léger et un codebook pour compresser le cache de KV par des techniques supplémentaires de cache, et on le transforme en une forme qui permet de le décoder par multiplication de matrices simples. De plus, pour réduire les coûts de calcul lors de la décodification, on utilise des Embeddings de Position Rotative (RoPE) et l'algorithme d'Expectation-Maximization (EM) conçu pour le contexte de la communauté. Ainsi, on peut intégrer la décodification de manière efficace dans la structure d'attention automatique. Notre approche utilise RoPE-CommVQ pour maintenir une haute précision et montrer un faible surcharge, en réduisant significativement le taille du cache de KV FP16 à 87,5% avec 2 bits de cache supplémentaire. Dans les benchmarks de longs contextes et sur GSM8K, on a démontré un excellent rendement comparé à d'autres méthodes de cache de KV. En particulier, on a réussi à réduire l'erreur de précision à un minimum, permettant d'exécuter le modèle LLaMA-3.1 8B avec une longueur de contexte de 128K sur un seul GPU RTX 4090. Le code source est disponible sur la URL suivante : https://github.com/UMass-Embodied-AGI/CommVQ.",
      "upvotes": 2,
      "discussionId": "685a1a090e4ad7e2197585a7",
      "ai_summary": "Commutative Vector Quantization (CommVQ) reduces memory usage in long-context LLM inference by compressing the KV cache with additive quantization and integration of Rotary Position Embedding (RoPE).",
      "ai_keywords": [
        "Commutative Vector Quantization",
        "CommVQ",
        "additive quantization",
        "codebook",
        "Rotary Position Embedding",
        "RoPE",
        "Expectation-Maximization",
        "self-attention",
        "FP16",
        "KV cache quantization",
        "GSM8K",
        "LLaMA-3.1 8B model",
        "RTX 4090 GPU"
      ]
    },
    "publishedAt": "2025-06-23T13:50:11.000Z",
    "title": "CommVQ: Commutative Vector Quantization for KV Cache Compression",
    "summary": "Large Language Models (LLMs) are increasingly used in applications requiring\nlong context lengths, but the key-value (KV) cache often becomes a memory\nbottleneck on GPUs as context grows. To address this, we propose Commutative\nVector Quantization (CommVQ) to significantly reduce memory usage for\nlong-context LLM inference. We first introduce additive quantization with a\nlightweight encoder and codebook to compress the KV cache, which can be decoded\nvia simple matrix multiplication. To further reduce computational costs during\ndecoding, we design the codebook to be commutative with Rotary Position\nEmbedding (RoPE) and train it using an Expectation-Maximization (EM) algorithm.\nThis enables efficient integration of decoding into the self-attention\nmechanism. Our approach achieves high accuracy with additive quantization and\nlow overhead via the RoPE-commutative codebook. Experiments on long-context\nbenchmarks and GSM8K show that our method reduces FP16 KV cache size by 87.5%\nwith 2-bit quantization, while outperforming state-of-the-art KV cache\nquantization methods. Notably, it enables 1-bit KV cache quantization with\nminimal accuracy loss, allowing a LLaMA-3.1 8B model to run with a 128K context\nlength on a single RTX 4090 GPU. The source code is available at:\nhttps://github.com/UMass-Embodied-AGI/CommVQ.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18879.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62d09eb86a61a88ea0d83918",
      "avatarUrl": "/avatars/81b511d94cced304ffca058caff662d4.svg",
      "fullname": "Junyan Li",
      "name": "senfu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.17673",
      "authors": [
        {
          "_id": "685a5c8e0e4ad7e2197586c7",
          "user": {
            "_id": "64105805928400b416439f10",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64105805928400b416439f10/i0jFLo47RTDeNl26hiR9y.jpeg",
            "isPro": false,
            "fullname": "Seonglae Cho",
            "user": "seonglae",
            "type": "user"
          },
          "name": "Seonglae Cho",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-24T08:09:45.757Z",
          "hidden": false
        },
        {
          "_id": "685a5c8e0e4ad7e2197586c8",
          "name": "Harryn Oh",
          "hidden": false
        },
        {
          "_id": "685a5c8e0e4ad7e2197586c9",
          "name": "Donghyun Lee",
          "hidden": false
        },
        {
          "_id": "685a5c8e0e4ad7e2197586ca",
          "name": "Luis Eduardo Rodrigues Vieira",
          "hidden": false
        },
        {
          "_id": "685a5c8e0e4ad7e2197586cb",
          "name": "Andrew Bermingham",
          "hidden": false
        },
        {
          "_id": "685a5c8e0e4ad7e2197586cc",
          "name": "Ziad El Sayed",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-21T10:18:25.000Z",
      "submittedOnDailyAt": "2025-06-24T06:38:36.202Z",
      "title": "FaithfulSAE : En utilisant un autoencodeur autonome pour éliminer la dépendance d'un ensemble de données externe, on extrait des caractéristiques fixes.",
      "submittedOnDailyBy": {
        "_id": "64105805928400b416439f10",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64105805928400b416439f10/i0jFLo47RTDeNl26hiR9y.jpeg",
        "isPro": false,
        "fullname": "Seonglae Cho",
        "user": "seonglae",
        "type": "user"
      },
      "summary": "Les Autoencodeurs Sparses (AEs) ont apparu comme une solution potentielle pour décomposer les caractéristiques de la représentation de grands modèles de langage. Cependant, Paulo et Belrose (2025) ont noté une instabilité dépendante de l'initialisation, et Heap et al. (2025) ont soulevé la possibilité que les AEs ne puissent pas capturer les caractéristiques internes du modèle. Ces problèmes pourraient être dûs au fait que les AEs sont entraînés sur des ensembles de données externes. Ces ensembles de données peuvent être collectés depuis le Web ou générés par d'autres modèles, et peuvent inclure des données hors de l'échantillon (OOD) qui dépassent la capacité de généralisation du modèle, ce qui peut générer des caractéristiques \"fausses\" qui représentent inaccurément l'activité interne du modèle. Pour faire face à ces problèmes, nous proposons le méthode FaithfulSAE. Cette méthode utilise des ensembles de données synthétiques générés par le modèle lui-même pour entraîner les AEs. En utilisant FaithfulSAE, nous avons montré que l'entraînement des AEs sur des ensembles de données OOD peut réduire l'instabilité des AEs dans différents scénarios. Spécifiquement, FaithfulSAE a réduit la proportion de caractéristiques fausses dans 5 des 7 modèles entraînés sur des ensembles de données basés sur le Web. En général, notre approche élimine la dépendance des ensembles de données externes, améliore la compréhension des caractéristiques internes du modèle, améliore l'interprétabilité et souligne l'importance des ensembles de données d'entraînement des AEs.",
      "upvotes": 1,
      "discussionId": "685a5c8e0e4ad7e2197586cd",
      "ai_summary": "FaithfulSAE improves Sparse Autoencoder stability and interpretability by training on synthetic datasets generated by the model itself, reducing the occurrence of fake features and out-of-distribution data issues.",
      "ai_keywords": [
        "Sparse Autoencoders",
        "SAEs",
        "interpretability",
        "instability",
        "initialization seeds",
        "model-internal features",
        "out-of-distribution",
        "OOD",
        "Fake Features",
        "SAE probing task",
        "synthetic dataset"
      ]
    },
    "publishedAt": "2025-06-21T06:18:25.000Z",
    "title": "FaithfulSAE: Towards Capturing Faithful Features with Sparse\n  Autoencoders without External Dataset Dependencies",
    "summary": "Sparse Autoencoders (SAEs) have emerged as a promising solution for\ndecomposing large language model representations into interpretable features.\nHowever, Paulo and Belrose (2025) have highlighted instability across different\ninitialization seeds, and Heap et al. (2025) have pointed out that SAEs may not\ncapture model-internal features. These problems likely stem from training SAEs\non external datasets - either collected from the Web or generated by another\nmodel - which may contain out-of-distribution (OOD) data beyond the model's\ngeneralisation capabilities. This can result in hallucinated SAE features,\nwhich we term \"Fake Features\", that misrepresent the model's internal\nactivations. To address these issues, we propose FaithfulSAE, a method that\ntrains SAEs on the model's own synthetic dataset. Using FaithfulSAEs, we\ndemonstrate that training SAEs on less-OOD instruction datasets results in SAEs\nbeing more stable across seeds. Notably, FaithfulSAEs outperform SAEs trained\non web-based datasets in the SAE probing task and exhibit a lower Fake Feature\nRatio in 5 out of 7 models. Overall, our approach eliminates the dependency on\nexternal datasets, advancing interpretability by better capturing\nmodel-internal features while highlighting the often neglected importance of\nSAE training datasets.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.17673.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64105805928400b416439f10",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64105805928400b416439f10/i0jFLo47RTDeNl26hiR9y.jpeg",
      "fullname": "Seonglae Cho",
      "name": "seonglae",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.16929",
      "authors": [
        {
          "_id": "6858b7c0c0c8e29df8ea3c29",
          "name": "Mohon Raihan",
          "hidden": false
        },
        {
          "_id": "6858b7c0c0c8e29df8ea3c2a",
          "name": "Plabon Kumar Saha",
          "hidden": false
        },
        {
          "_id": "6858b7c0c0c8e29df8ea3c2b",
          "user": {
            "_id": "67a3002c637d195f3c4bf371",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/JbaMFjmWFDd-tZmUxQ8aT.jpeg",
            "isPro": false,
            "fullname": "Rajan Das Gupta",
            "user": "rajandasgupta",
            "type": "user"
          },
          "name": "Rajan Das Gupta",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-23T08:15:03.417Z",
          "hidden": false
        },
        {
          "_id": "6858b7c0c0c8e29df8ea3c2c",
          "name": "A Z M Tahmidul Kabir",
          "hidden": false
        },
        {
          "_id": "6858b7c0c0c8e29df8ea3c2d",
          "name": "Afia Anjum Tamanna",
          "hidden": false
        },
        {
          "_id": "6858b7c0c0c8e29df8ea3c2e",
          "name": "Md. Harun-Ur-Rashid",
          "hidden": false
        },
        {
          "_id": "6858b7c0c0c8e29df8ea3c2f",
          "name": "Adnan Bin Abdus Salam",
          "hidden": false
        },
        {
          "_id": "6858b7c0c0c8e29df8ea3c30",
          "name": "Md Tanvir Anjum",
          "hidden": false
        },
        {
          "_id": "6858b7c0c0c8e29df8ea3c31",
          "name": "A Z M Ahteshamul Kabir",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/67a3002c637d195f3c4bf371/Kx6P69oj873BKEjH3euNc.png",
        "https://cdn-uploads.huggingface.co/production/uploads/67a3002c637d195f3c4bf371/0S6fvcxnAdj4DiU-NEhtz.png",
        "https://cdn-uploads.huggingface.co/production/uploads/67a3002c637d195f3c4bf371/EBuiK2wVbSXnLVWmxDiZ5.png",
        "https://cdn-uploads.huggingface.co/production/uploads/67a3002c637d195f3c4bf371/Id__vCzRydC1Bmxv-j1W0.png"
      ],
      "publishedAt": "2025-06-20T11:44:48.000Z",
      "submittedOnDailyAt": "2025-06-24T08:29:18.461Z",
      "title": "Approche de l'Apprentissage Profond et de l'Apprentissage Automatique pour la Prédiction de la Mortalité des Nouveau-Nés",
      "submittedOnDailyBy": {
        "_id": "67a3002c637d195f3c4bf371",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/JbaMFjmWFDd-tZmUxQ8aT.jpeg",
        "isPro": false,
        "fullname": "Rajan Das Gupta",
        "user": "rajandasgupta",
        "type": "user"
      },
      "summary": "Le décès des nouveau-nés reste une réalité inquietante dans les pays en développement et même dans certains pays développés. Selon les données mondiales de Macro Trades, 26.693 nouveau-nés sur 1.000 meurent. Pour réduire ce taux, il est crucial de prédire les enfants en danger de manière précoce. Cette prédiction offre aux parents l'opportunité de bénéficier d'une surveillance suffisante pour éviter une mort prématurée. Dans ce contexte, l'apprentissage automatique est utilisé pour déterminer si un nouveau-né est en danger. Un modèle de prédiction a été entraîné en utilisant des historiques de 1.4 millions de nouveau-nés. Des modèles utilisant des techniques d'apprentissage automatique et d'apprentissage profond, comme la régression logistique, les k-plus proches voisins, le classifieur d'arbres aléatoires, XGBoost (Extreme Gradient Boosting), la réseau de neurones convolutif (CNN) et la mémoire temporelle longue (LSTM), ont été identifiés. Les algorithmes d'apprentissage automatique, XGBoost et le classifieur d'arbres aléatoires ont atteint une précision de 94%, tandis que le modèle d'apprentissage profond, LSTM, a atteint une précision de 99%. Par conséquent, il est possible de dire que l'LSTM est la meilleure option pour prédire si un enfant nécessite des mesures préventives.",
      "upvotes": 1,
      "discussionId": "6858b7c1c0c8e29df8ea3c32",
      "ai_summary": "Deep learning, specifically LSTM, outperforms other machine learning techniques in predicting neonatal mortality using historical data.",
      "ai_keywords": [
        "logical regression",
        "K-nearest neighbor",
        "random forest classifier",
        "extreme gradient boosting (XGBoost)",
        "convolutional neural network",
        "long short-term memory (LSTM)"
      ]
    },
    "publishedAt": "2025-06-20T07:44:48.000Z",
    "title": "A deep learning and machine learning approach to predict neonatal death\n  in the context of São Paulo",
    "summary": "Neonatal death is still a concerning reality for underdeveloped and even some\ndeveloped countries. Worldwide data indicate that 26.693 babies out of 1,000\nbirths die, according to Macro Trades. To reduce this number, early prediction\nof endangered babies is crucial. Such prediction enables the opportunity to\ntake ample care of the child and mother so that early child death can be\navoided. In this context, machine learning was used to determine whether a\nnewborn baby is at risk. To train the predictive model, historical data of 1.4\nmillion newborns was used. Machine learning and deep learning techniques such\nas logical regression, K-nearest neighbor, random forest classifier, extreme\ngradient boosting (XGBoost), convolutional neural network, and long short-term\nmemory (LSTM) were implemented using the dataset to identify the most accurate\nmodel for predicting neonatal mortality. Among the machine learning algorithms,\nXGBoost and random forest classifier achieved the best accuracy with 94%, while\namong the deep learning models, LSTM delivered the highest accuracy with 99%.\nTherefore, using LSTM appears to be the most suitable approach to predict\nwhether precautionary measures for a child are necessary.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/67a3002c637d195f3c4bf371/Kx6P69oj873BKEjH3euNc.png",
      "https://cdn-uploads.huggingface.co/production/uploads/67a3002c637d195f3c4bf371/0S6fvcxnAdj4DiU-NEhtz.png",
      "https://cdn-uploads.huggingface.co/production/uploads/67a3002c637d195f3c4bf371/EBuiK2wVbSXnLVWmxDiZ5.png",
      "https://cdn-uploads.huggingface.co/production/uploads/67a3002c637d195f3c4bf371/Id__vCzRydC1Bmxv-j1W0.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.16929.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67a3002c637d195f3c4bf371",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/JbaMFjmWFDd-tZmUxQ8aT.jpeg",
      "fullname": "Rajan Das Gupta",
      "name": "rajandasgupta",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.17323",
      "authors": [
        {
          "_id": "685a6c720e4ad7e2197586f2",
          "name": "Tamas Bisztray",
          "hidden": false
        },
        {
          "_id": "685a6c720e4ad7e2197586f3",
          "user": {
            "_id": "64d3db80aea0ccb1b4975d95",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Mi0eKzNp6wKFrqketK-DN.png",
            "isPro": false,
            "fullname": "Bilel Cherif",
            "user": "Neo111x",
            "type": "user"
          },
          "name": "Bilel Cherif",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-24T09:32:57.540Z",
          "hidden": false
        },
        {
          "_id": "685a6c720e4ad7e2197586f4",
          "name": "Richard A. Dubniczky",
          "hidden": false
        },
        {
          "_id": "685a6c720e4ad7e2197586f5",
          "name": "Nils Gruschka",
          "hidden": false
        },
        {
          "_id": "685a6c720e4ad7e2197586f6",
          "name": "Bertalan Borsos",
          "hidden": false
        },
        {
          "_id": "685a6c720e4ad7e2197586f7",
          "name": "Mohamed Amine Ferrag",
          "hidden": false
        },
        {
          "_id": "685a6c720e4ad7e2197586f8",
          "name": "Attila Kovacs",
          "hidden": false
        },
        {
          "_id": "685a6c720e4ad7e2197586f9",
          "name": "Vasileios Mavroeidis",
          "hidden": false
        },
        {
          "_id": "685a6c720e4ad7e2197586fa",
          "name": "Norbert Tihanyi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-18T19:49:41.000Z",
      "submittedOnDailyAt": "2025-06-24T07:45:13.848Z",
      "title": "Je suis conscient que l'un de mes modèles de langage génératif (LLM) a été connu par quelqu'un le dernier jour d'été : \"Code générateur de modèle de langage génératif (LLM) et identification des attributs des générateurs de modèle de langage génératif (LLM) à travers des structures d'identification de style\".",
      "submittedOnDailyBy": {
        "_id": "64d3db80aea0ccb1b4975d95",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Mi0eKzNp6wKFrqketK-DN.png",
        "isPro": false,
        "fullname": "Bilel Cherif",
        "user": "Neo111x",
        "type": "user"
      },
      "summary": "La détection de code AI, deepfakes et autres contenus synthétiques est un nouveau domaine de recherche émergent. Lorsque le code généré par des modèles de langage grands (LLM) devient plus généralisé, il est crucial d'identifier le modèle spécifique de chaque échantillon. Dans cet article, nous présentons une recherche systématique sur l'identification de l'auteur en programmation en langage C en utilisant des modèles de LLM. Nous publions un nouveau modèle appelé CodeT5-Authorship. Ce modèle utilise uniquement l'unité d'encodeur de l'architecture encoder-decoder de CodeT5, en supprimant le décodeur et se concentrant sur la classification. Le output de l'encodeur du modèle (le premier token) est traité par une tête de classification à deux couches comprenant la fonction d'activation GELU et le dropout, générant une distribution de probabilités pour les possibles auteurs. Pour évaluer le modèle, nous utilisons 32 000 programmes générés par les 8 LLM les plus courts. Nous comparons le modèle à 7 classifieurs traditionnels d'apprentissage automatique et à 8 modèles transformateurs fine-tunés (BERT, RoBERTa, CodeBERT, ModernBERT, DistilBERT, DeBERTa-V3, Longformer, LoRA-fine-tuned Qwen2-1.5B). Dans la classification binaire, nous atteignons une précision de 97,56% pour distinguer entre les codes de programmation C générés par des modèles similaires (GPT-4.1 et GPT-4o). Dans l'identification multi-classe, nous atteignons une précision de 95,40% pour les 5 LLM les plus courts (Gemini 2.5 Flash, Claude 3.5 Haiku, GPT-4.1, Llama 3.3, DeepSeek-V3). Pour soutenir la science ouverte, nous publions sur GitHub tous les scripts de Google Colab liés, y compris l'architecture de CodeT5-Authorship, le benchmark LLM-AuthorBench et tous les scripts de Google Colab liés.",
      "upvotes": 1,
      "discussionId": "685a6c720e4ad7e2197586fb",
      "projectPage": "https://github.com/LLMauthorbench",
      "githubRepo": "https://github.com/LLMauthorbench/LLMauthorbench",
      "ai_summary": "A novel model, CodeT5-Authorship, is introduced to classify the authorship of C programs generated by Large Language Models, achieving high accuracy compared to traditional and transformer-based classifiers.",
      "ai_keywords": [
        "Large Language Models",
        "LLM authorship attribution",
        "CodeT5-Authorship",
        "encoder-decoder architecture",
        "GELU activation",
        "dropout",
        "LLM-AuthorBench",
        "traditional ML classifiers",
        "BERT",
        "RoBERTa",
        "CodeBERT",
        "ModernBERT",
        "DistilBERT",
        "DeBERTa-V3",
        "Longformer",
        "LoRA",
        "Qwen2-1.5B",
        "binary classification",
        "multi-class attribution"
      ]
    },
    "publishedAt": "2025-06-18T15:49:41.000Z",
    "title": "I Know Which LLM Wrote Your Code Last Summer: LLM generated Code\n  Stylometry for Authorship Attribution",
    "summary": "Detecting AI-generated code, deepfakes, and other synthetic content is an\nemerging research challenge. As code generated by Large Language Models (LLMs)\nbecomes more common, identifying the specific model behind each sample is\nincreasingly important. This paper presents the first systematic study of LLM\nauthorship attribution for C programs. We released CodeT5-Authorship, a novel\nmodel that uses only the encoder layers from the original CodeT5\nencoder-decoder architecture, discarding the decoder to focus on\nclassification. Our model's encoder output (first token) is passed through a\ntwo-layer classification head with GELU activation and dropout, producing a\nprobability distribution over possible authors. To evaluate our approach, we\nintroduce LLM-AuthorBench, a benchmark of 32,000 compilable C programs\ngenerated by eight state-of-the-art LLMs across diverse tasks. We compare our\nmodel to seven traditional ML classifiers and eight fine-tuned transformer\nmodels, including BERT, RoBERTa, CodeBERT, ModernBERT, DistilBERT, DeBERTa-V3,\nLongformer, and LoRA-fine-tuned Qwen2-1.5B. In binary classification, our model\nachieves 97.56% accuracy in distinguishing C programs generated by closely\nrelated models such as GPT-4.1 and GPT-4o, and 95.40% accuracy for multi-class\nattribution among five leading LLMs (Gemini 2.5 Flash, Claude 3.5 Haiku,\nGPT-4.1, Llama 3.3, and DeepSeek-V3). To support open science, we release the\nCodeT5-Authorship architecture, the LLM-AuthorBench benchmark, and all relevant\nGoogle Colab scripts on GitHub: https://github.com/LLMauthorbench/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.17323.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64d3db80aea0ccb1b4975d95",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Mi0eKzNp6wKFrqketK-DN.png",
      "fullname": "Bilel Cherif",
      "name": "Neo111x",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.16507",
      "authors": [
        {
          "_id": "685a752f0e4ad7e21975870a",
          "name": "Pragya Srivastava",
          "hidden": false
        },
        {
          "_id": "685a752f0e4ad7e21975870b",
          "name": "Harman Singh",
          "hidden": false
        },
        {
          "_id": "685a752f0e4ad7e21975870c",
          "name": "Rahul Madhavan",
          "hidden": false
        },
        {
          "_id": "685a752f0e4ad7e21975870d",
          "name": "Gandharv Patil",
          "hidden": false
        },
        {
          "_id": "685a752f0e4ad7e21975870e",
          "name": "Sravanti Addepalli",
          "hidden": false
        },
        {
          "_id": "685a752f0e4ad7e21975870f",
          "name": "Arun Suggala",
          "hidden": false
        },
        {
          "_id": "685a752f0e4ad7e219758710",
          "name": "Rengarajan Aravamudhan",
          "hidden": false
        },
        {
          "_id": "685a752f0e4ad7e219758711",
          "name": "Soumya Sharma",
          "hidden": false
        },
        {
          "_id": "685a752f0e4ad7e219758712",
          "name": "Anirban Laha",
          "hidden": false
        },
        {
          "_id": "685a752f0e4ad7e219758713",
          "name": "Aravindan Raghuveer",
          "hidden": false
        },
        {
          "_id": "685a752f0e4ad7e219758714",
          "name": "Karthikeyan Shanmugam",
          "hidden": false
        },
        {
          "_id": "685a752f0e4ad7e219758715",
          "name": "Doina Precup",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-19T17:59:47.000Z",
      "submittedOnDailyAt": "2025-06-24T08:24:29.234Z",
      "title": "Nous appliquons les règles de caucus pour modéliser une compensation solide.",
      "submittedOnDailyBy": {
        "_id": "639ccab166106be1436e1640",
        "avatarUrl": "/avatars/1e3806e18ac427be20e93e5400f153d4.svg",
        "isPro": false,
        "fullname": "Pragya Srivastava",
        "user": "pragsri8",
        "type": "user"
      },
      "summary": "Les modèles de récompense (RMs) sont des éléments essentiels pour ajuster les modèles de langage à grande échelle (LLMs) en fonction des réactions humaines, mais ils souffrent souvent de difficultés dues au hacking de la récompense. Ces modèles dépendent par exemple de la longueur ou du format de la réponse, et parfois s'appuient sur des caractéristiques superficielles ou incertaines, ce qui les rend difficiles à interpréter la véritable cause de la demande (comme la vérité ou la pertinence). Cela rend que les fonctions de formation standard ne peuvent pas distinguer ces caractéristiques, ce qui résulte en des RMs vulnérables et des politiques inadéquates. Nous présentons un nouveau cadre de travail appelé Crome (Modélisation de Récompenses Efficaces) pour atténuer le hacking de la récompense. Crome s'appuie sur des modèles causales explicites et utilise un large éventail d'augmentations synthétiques lors de l'entraînement : 1) Les augmentations causales créent des paires basées sur des caractéristiques causales spécifiques, forcent la sensibilité individuelle de chaque caractéristique causal. 2) Les augmentations neutres créent des paires principalement en connectant des résultats à travers des caractéristiques incertaines, forcent des variations dans ces caractéristiques. En particulier, nos augmentations génèrent des réponses dans un intervalle basé sur des règles causales, sans besoin de connaître le savoir sur les causes incertaines. Expérimentalement, Crome dépasse significativement les limites de base des benchmarks de RewardBench, augmentant la précision moyenne de 5,4%, et atteignant des effets de 13,2% et 7,2% dans des catégories spécifiques. La robustesse de Crome est maintenue de manière constante dans différents benchmarks, y compris RewardBench (commentaires, commentaires-hard, sécurité, raisonnement), WildGuardTest (concentré sur la sécurité) et GSM8k (concentré sur la raisonnement), même dans des configurations d'inférence \"Best-of-N\" où le nombre de N est augmenté.",
      "upvotes": 0,
      "discussionId": "685a752f0e4ad7e219758716",
      "ai_summary": "Crome, a novel reward modeling framework using causal and neutral augmentations, significantly improves the robustness and accuracy of reward models against reward hacking.",
      "ai_keywords": [
        "Reward models",
        "Large Language Models",
        "reward hacking",
        "causal model",
        "causal augmentations",
        "neutral augmentations",
        "answer interventions",
        "oracle LLM",
        "RewardBench",
        "WildGuardTest",
        "GSM8k",
        "Best-of-N inference"
      ]
    },
    "publishedAt": "2025-06-19T13:59:47.000Z",
    "title": "Robust Reward Modeling via Causal Rubrics",
    "summary": "Reward models (RMs) are fundamental to aligning Large Language Models (LLMs)\nvia human feedback, yet they often suffer from reward hacking. They tend to\nlatch on to superficial or spurious attributes, such as response length or\nformatting, mistaking these cues learned from correlations in training data for\nthe true causal drivers of quality (e.g., factuality, relevance). This occurs\nbecause standard training objectives struggle to disentangle these factors,\nleading to brittle RMs and misaligned policies. We introduce Crome (Causally\nRobust Reward Modeling), a novel framework grounded in an explicit causal model\ndesigned to mitigate reward hacking. Crome employs the following synthetic\ntargeted augmentations during training: (1) Causal Augmentations, which are\npairs that differ along specific causal attributes, to enforce sensitivity\nalong each causal attribute individually, and (2) Neutral Augmentations, which\nare tie-label pairs varying primarily in spurious attributes, to enforce\ninvariance along spurious attributes. Notably, our augmentations are produced\nwithout any knowledge of spurious factors, via answer interventions only along\ncausal rubrics, that are identified by querying an oracle LLM. Empirically,\nCrome significantly outperforms standard baselines on RewardBench, improving\naverage accuracy by up to 5.4% and achieving gains of up to 13.2% and 7.2% in\nspecific categories. The robustness of Crome is further testified by the\nconsistent gains obtained in a Best-of-N inference setting across increasing N,\nacross various benchmarks, including the popular RewardBench (covering chat,\nchat-hard, safety, and reasoning tasks), the safety-focused WildGuardTest, and\nthe reasoning-specific GSM8k.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.16507.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "639ccab166106be1436e1640",
      "avatarUrl": "/avatars/1e3806e18ac427be20e93e5400f153d4.svg",
      "fullname": "Pragya Srivastava",
      "name": "pragsri8",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.10597",
      "authors": [
        {
          "_id": "685a0fb40e4ad7e219758522",
          "name": "Xunguang Wang",
          "hidden": false
        },
        {
          "_id": "685a0fb40e4ad7e219758523",
          "name": "Zhenlan Ji",
          "hidden": false
        },
        {
          "_id": "685a0fb40e4ad7e219758524",
          "name": "Wenxuan Wang",
          "hidden": false
        },
        {
          "_id": "685a0fb40e4ad7e219758525",
          "name": "Zongjie Li",
          "hidden": false
        },
        {
          "_id": "685a0fb40e4ad7e219758526",
          "name": "Daoyuan Wu",
          "hidden": false
        },
        {
          "_id": "685a0fb40e4ad7e219758527",
          "name": "Shuai Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-12T11:42:40.000Z",
      "submittedOnDailyAt": "2025-06-24T01:16:03.239Z",
      "title": "SoK : Évaluation de la Garde de Bras des Modèles de Langue de Grande Échelle",
      "submittedOnDailyBy": {
        "_id": "6601853162471e0981261241",
        "avatarUrl": "/avatars/ccd1c5ce9d2f6fe7c2aff80fd9c39270.svg",
        "isPro": false,
        "fullname": "XunguangWang",
        "user": "xunguangwang",
        "type": "user"
      },
      "summary": "Les modèles de langage grand (LLMs) ont réalisé une transition impressionnante, mais leur introduction a révélé des vulnérabilités importantes dans les institutions sécurisées, en particulier dans les attaques par palier de frein. En réponse à cette attaque, une structure de défense externe nommée « Guardrail » a été introduite. Cependant, l'état actuel des Guardrail est décohérent et manque d'une taxonomie uniforme et d'un cadre d'évaluation complet. Ce document de synthèse de connaissances (SoK) effectue le premier analyse de sécurité matérielle des attaques par palier de frein sur les LLMs. Nous proposons une nouvelle et diversifiée taxonomie et visons à évaluer les pratiques efficaces à travers des cadres d'évaluation de sécurité, d'efficacité et d'utilité. Par un analyse rigoureuse et des expérimentations, nous dévoilons les forces et les limites des méthodes d'accès actuelles aux Guardrail, nous explorons la généralisation dans une large gamme de types d'attaques et nous fournissons des rétroactions pour l'optimisation des combinaisons de défense. Notre étude fournit une base structurée pour futures recherches et développements, et se concentre sur guider le progrès théorique et l'introduction de fortes Guardrail de LLMs. Le code est disponible sur https://github.com/xunguangwang/SoK4JailbreakGuardrails.",
      "upvotes": 0,
      "discussionId": "685a0fb40e4ad7e219758533",
      "ai_summary": "A systematic analysis and evaluation framework for jailbreak guardrails in Large Language Models is presented, categorizing and assessing their effectiveness and optimization potential.",
      "ai_keywords": [
        "Large Language Models",
        "LLMs",
        "jailbreak attacks",
        "guardrails",
        "Security-Efficiency-Utility framework",
        "multi-dimensional taxonomy"
      ]
    },
    "publishedAt": "2025-06-12T07:42:40.000Z",
    "title": "SoK: Evaluating Jailbreak Guardrails for Large Language Models",
    "summary": "Large Language Models (LLMs) have achieved remarkable progress, but their\ndeployment has exposed critical vulnerabilities, particularly to jailbreak\nattacks that circumvent safety mechanisms. Guardrails--external defense\nmechanisms that monitor and control LLM interaction--have emerged as a\npromising solution. However, the current landscape of LLM guardrails is\nfragmented, lacking a unified taxonomy and comprehensive evaluation framework.\nIn this Systematization of Knowledge (SoK) paper, we present the first holistic\nanalysis of jailbreak guardrails for LLMs. We propose a novel,\nmulti-dimensional taxonomy that categorizes guardrails along six key\ndimensions, and introduce a Security-Efficiency-Utility evaluation framework to\nassess their practical effectiveness. Through extensive analysis and\nexperiments, we identify the strengths and limitations of existing guardrail\napproaches, explore their universality across attack types, and provide\ninsights into optimizing defense combinations. Our work offers a structured\nfoundation for future research and development, aiming to guide the principled\nadvancement and deployment of robust LLM guardrails. The code is available at\nhttps://github.com/xunguangwang/SoK4JailbreakGuardrails.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10597.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6601853162471e0981261241",
      "avatarUrl": "/avatars/ccd1c5ce9d2f6fe7c2aff80fd9c39270.svg",
      "fullname": "XunguangWang",
      "name": "xunguangwang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  }
]