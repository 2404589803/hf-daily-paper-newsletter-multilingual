[
  {
    "paper": {
      "id": "2505.04410",
      "authors": [
        {
          "_id": "681d615fbd89ba9ceb5e94bc",
          "user": {
            "_id": "64a385281cbf675203fbb7df",
            "avatarUrl": "/avatars/f259d080d3127c45bcf564a8d1fafcc6.svg",
            "isPro": false,
            "fullname": "Junjie Wang",
            "user": "xiaomoguhzz",
            "type": "user"
          },
          "name": "Junjie Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-09T07:21:48.630Z",
          "hidden": false
        },
        {
          "_id": "681d615fbd89ba9ceb5e94bd",
          "name": "Bin Chen",
          "hidden": false
        },
        {
          "_id": "681d615fbd89ba9ceb5e94be",
          "name": "Yulin Li",
          "hidden": false
        },
        {
          "_id": "681d615fbd89ba9ceb5e94bf",
          "name": "Bin Kang",
          "hidden": false
        },
        {
          "_id": "681d615fbd89ba9ceb5e94c0",
          "name": "Yichi Chen",
          "hidden": false
        },
        {
          "_id": "681d615fbd89ba9ceb5e94c1",
          "name": "Zhuotao Tian",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64a385281cbf675203fbb7df/MpXgXINxg1RmSFnfeqXqU.mp4"
      ],
      "publishedAt": "2025-05-07T13:46:34.000Z",
      "submittedOnDailyAt": "2025-05-15T06:24:33.810Z",
      "title": "DeCLIP : Apprentissage du décodificateur de boîtes ouvertes pour le reconnaissance dense",
      "submittedOnDailyBy": {
        "_id": "64a385281cbf675203fbb7df",
        "avatarUrl": "/avatars/f259d080d3127c45bcf564a8d1fafcc6.svg",
        "isPro": false,
        "fullname": "Junjie Wang",
        "user": "xiaomoguhzz",
        "type": "user"
      },
      "summary": "La tâche de prédiction visuelle dense dépend de catégories prédéfinies et limite l'applicabilité des concepts visuels dans les innombrables scénarios réels du monde. Les modèles de langage visuo-linguistique comme CLIP montrent des résultats souhaitables dans des tâches de langage ouvertes, mais lorsqu'ils sont directement appliqués à la prédiction dense, leur performance est limitée par la représentation des caractéristiques locales. Dans cette étude, on observe que les tokens d'image de CLIP ne peuvent pas effectivement recueillir de l'information dans des zones spatialement ou sémantiquement associées, et on propose un nouveau cadre de travail appelé DeCLIP pour résoudre ce problème. DeCLIP renforce CLIP en séparant un module d'attention automatique et en obtenant des caractéristiques de \"contenu\" et de \"contexte\" de manière indépendante. Les caractéristiques de \"contenu\" sont alignées avec la représentation d'un coupe d'image, améliorant l'identification locale, tandis que les caractéristiques de \"contexte\" maintiennent la corrélation spatiale sous la supervision de modèles visuels comme DINO. Les résultats d'expériences larges montrent que DeCLIP démontre une amélioration significative dans les tâches de détection d'objets et de segmentation de sens par rapport aux méthodes existantes. Le code est disponible sur https://github.com/xiaomoguhz/DeCLIP.",
      "upvotes": 28,
      "discussionId": "681d6161bd89ba9ceb5e9571",
      "ai_keywords": [
        "Vision-Language Models (VLMs)",
        "CLIP",
        "dense prediction",
        "predefined categories",
        "open-vocabulary tasks",
        "spatially related regions",
        "semantically related regions",
        "local discriminability",
        "spatial consistency",
        "self-attention module",
        "content features",
        "context features",
        "image crop representations",
        "vision foundation models",
        "DINO",
        "object detection",
        "semantic segmentation"
      ]
    },
    "publishedAt": "2025-05-07T09:46:34.000Z",
    "title": "DeCLIP: Decoupled Learning for Open-Vocabulary Dense Perception",
    "summary": "Dense visual prediction tasks have been constrained by their reliance on\npredefined categories, limiting their applicability in real-world scenarios\nwhere visual concepts are unbounded. While Vision-Language Models (VLMs) like\nCLIP have shown promise in open-vocabulary tasks, their direct application to\ndense prediction often leads to suboptimal performance due to limitations in\nlocal feature representation. In this work, we present our observation that\nCLIP's image tokens struggle to effectively aggregate information from\nspatially or semantically related regions, resulting in features that lack\nlocal discriminability and spatial consistency. To address this issue, we\npropose DeCLIP, a novel framework that enhances CLIP by decoupling the\nself-attention module to obtain ``content'' and ``context'' features\nrespectively. The ``content'' features are aligned with image crop\nrepresentations to improve local discriminability, while ``context'' features\nlearn to retain the spatial correlations under the guidance of vision\nfoundation models, such as DINO. Extensive experiments demonstrate that DeCLIP\nsignificantly outperforms existing methods across multiple open-vocabulary\ndense prediction tasks, including object detection and semantic segmentation.\nCode is available at magenta{https://github.com/xiaomoguhz/DeCLIP}.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64a385281cbf675203fbb7df/MpXgXINxg1RmSFnfeqXqU.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.04410.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64a385281cbf675203fbb7df",
      "avatarUrl": "/avatars/f259d080d3127c45bcf564a8d1fafcc6.svg",
      "fullname": "Junjie Wang",
      "name": "xiaomoguhzz",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.09568",
      "authors": [
        {
          "_id": "68254419181d43c25d829239",
          "name": "Jiuhai Chen",
          "hidden": false
        },
        {
          "_id": "68254419181d43c25d82923a",
          "name": "Zhiyang Xu",
          "hidden": false
        },
        {
          "_id": "68254419181d43c25d82923b",
          "name": "Xichen Pan",
          "hidden": false
        },
        {
          "_id": "68254419181d43c25d82923c",
          "name": "Yushi Hu",
          "hidden": false
        },
        {
          "_id": "68254419181d43c25d82923d",
          "name": "Can Qin",
          "hidden": false
        },
        {
          "_id": "68254419181d43c25d82923e",
          "name": "Tom Goldstein",
          "hidden": false
        },
        {
          "_id": "68254419181d43c25d82923f",
          "name": "Lifu Huang",
          "hidden": false
        },
        {
          "_id": "68254419181d43c25d829240",
          "name": "Tianyi Zhou",
          "hidden": false
        },
        {
          "_id": "68254419181d43c25d829241",
          "name": "Saining Xie",
          "hidden": false
        },
        {
          "_id": "68254419181d43c25d829242",
          "name": "Silvio Savarese",
          "hidden": false
        },
        {
          "_id": "68254419181d43c25d829243",
          "name": "Le Xue",
          "hidden": false
        },
        {
          "_id": "68254419181d43c25d829244",
          "name": "Caiming Xiong",
          "hidden": false
        },
        {
          "_id": "68254419181d43c25d829245",
          "name": "Ran Xu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-14T17:11:07.000Z",
      "submittedOnDailyAt": "2025-05-15T00:07:05.564Z",
      "title": "BLIP3-o : Famille de modèles multimodales intégrés de l'union complète des ouverts - Architecture, entraînement et données de base",
      "submittedOnDailyBy": {
        "_id": "6393847e3e30234ae798b7be",
        "avatarUrl": "/avatars/daeb8c37dff4432d837a69b87c196521.svg",
        "isPro": true,
        "fullname": "JiuhaiChen",
        "user": "jiuhai",
        "type": "user"
      },
      "summary": "Dans les derniers études sur les multi-modèles, l'intégration de la compréhension et de la génération d'images a été un thème de préoccupation. Les décisions de conception en matière de compréhension d'images ont été largement étudiées, mais la structure optimale et les décalages d'entraînement d'un cadre de travail d'intégration avec la génération d'images ont reçu peu d'attention. Inspirés par la possibilité de générer de haute qualité et à bonne échelle montrée par les modèles d'auto-reconstruction et les modèles de diffusion, nous avons effectué des recherches sur la configuration de modèles d'intégration de ces derniers. Sur la base de ces recherches, nous avons proposé un nouvel approche utilisant le Transformer de canaux de diffusion pour générer des caractéristiques d'images CLIP de manière plus riche en signification. Cet approche atteint une haute efficacité d'entraînement et améliore la qualité de la génération. De plus, la stratégie d'entraînement séquentiel du modèle intégré (d'abord l'entraînement de la compréhension d'images, puis l'entraînement de la génération d'images) montre une avantage pratique de maintenir les compétences de compréhension d'images tout en développant une forte capacité de génération d'images. Enfin, nous avons construit une large collection de données d'entraînement inspirée de haute qualité pour la génération d'images, BLIP3o-60k, qui inclut des scénarios, des objets et des gestes de personnes, en utilisant GPT-4o. Sur la base de ce nouveau design de modèle, de décalages d'entraînement, et de jeu de données, nous avons développé le système de modèle intégré le plus avancé, BLIP3-o. BLIP3-o atteint les meilleurs résultats sur les deux benchmarks de compréhension et de génération d'images. Pour soutenir futures recherches, nous avons complètement ouvert le code du modèle, ses poids, les scripts d'entraînement, les jeux de données d'entraînement précédent et inspiré.",
      "upvotes": 13,
      "discussionId": "6825441a181d43c25d82927a",
      "ai_keywords": [
        "autoregressive models",
        "diffusion models",
        "semantically rich CLIP image features",
        "diffusion transformer",
        "VAE-based representations",
        "sequential pretraining strategy",
        "image understanding",
        "image generation",
        "instruction-tuning dataset",
        "GPT-4o",
        "state-of-the-art unified multimodal models"
      ]
    },
    "publishedAt": "2025-05-14T13:11:07.000Z",
    "title": "BLIP3-o: A Family of Fully Open Unified Multimodal Models-Architecture,\n  Training and Dataset",
    "summary": "Unifying image understanding and generation has gained growing attention in\nrecent research on multimodal models. Although design choices for image\nunderstanding have been extensively studied, the optimal model architecture and\ntraining recipe for a unified framework with image generation remain\nunderexplored. Motivated by the strong potential of autoregressive and\ndiffusion models for high-quality generation and scalability, we conduct a\ncomprehensive study of their use in unified multimodal settings, with emphasis\non image representations, modeling objectives, and training strategies.\nGrounded in these investigations, we introduce a novel approach that employs a\ndiffusion transformer to generate semantically rich CLIP image features, in\ncontrast to conventional VAE-based representations. This design yields both\nhigher training efficiency and improved generative quality. Furthermore, we\ndemonstrate that a sequential pretraining strategy for unified models-first\ntraining on image understanding and subsequently on image generation-offers\npractical advantages by preserving image understanding capability while\ndeveloping strong image generation ability. Finally, we carefully curate a\nhigh-quality instruction-tuning dataset BLIP3o-60k for image generation by\nprompting GPT-4o with a diverse set of captions covering various scenes,\nobjects, human gestures, and more. Building on our innovative model design,\ntraining recipe, and datasets, we develop BLIP3-o, a suite of state-of-the-art\nunified multimodal models. BLIP3-o achieves superior performance across most of\nthe popular benchmarks spanning both image understanding and generation tasks.\nTo facilitate future research, we fully open-source our models, including code,\nmodel weights, training scripts, and pretraining and instruction tuning\ndatasets.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.09568.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6393847e3e30234ae798b7be",
      "avatarUrl": "/avatars/daeb8c37dff4432d837a69b87c196521.svg",
      "fullname": "JiuhaiChen",
      "name": "jiuhai",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 27
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.09343",
      "authors": [
        {
          "_id": "682578ca1b93095c061429ff",
          "name": "Chenggang Zhao",
          "hidden": false
        },
        {
          "_id": "682578ca1b93095c06142a00",
          "name": "Chengqi Deng",
          "hidden": false
        },
        {
          "_id": "682578ca1b93095c06142a01",
          "name": "Chong Ruan",
          "hidden": false
        },
        {
          "_id": "682578ca1b93095c06142a02",
          "name": "Damai Dai",
          "hidden": false
        },
        {
          "_id": "682578ca1b93095c06142a03",
          "name": "Huazuo Gao",
          "hidden": false
        },
        {
          "_id": "682578ca1b93095c06142a04",
          "name": "Jiashi Li",
          "hidden": false
        },
        {
          "_id": "682578ca1b93095c06142a05",
          "name": "Liyue Zhang",
          "hidden": false
        },
        {
          "_id": "682578ca1b93095c06142a06",
          "name": "Panpan Huang",
          "hidden": false
        },
        {
          "_id": "682578ca1b93095c06142a07",
          "name": "Shangyan Zhou",
          "hidden": false
        },
        {
          "_id": "682578ca1b93095c06142a08",
          "name": "Shirong Ma",
          "hidden": false
        },
        {
          "_id": "682578ca1b93095c06142a09",
          "name": "Wenfeng Liang",
          "hidden": false
        },
        {
          "_id": "682578ca1b93095c06142a0a",
          "name": "Ying He",
          "hidden": false
        },
        {
          "_id": "682578ca1b93095c06142a0b",
          "name": "Yuqing Wang",
          "hidden": false
        },
        {
          "_id": "682578ca1b93095c06142a0c",
          "name": "Yuxuan Liu",
          "hidden": false
        },
        {
          "_id": "682578ca1b93095c06142a0d",
          "name": "Y. X. Wei",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-14T12:39:03.000Z",
      "submittedOnDailyAt": "2025-05-15T05:22:39.526Z",
      "title": "DeepSeek-V3's Insights : Problèmes d'échelle dans l'architecture de l'IA et la réflexion de l'hardware",
      "submittedOnDailyBy": {
        "_id": "5f1158120c833276f61f1a84",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
        "isPro": false,
        "fullname": "Niels Rogge",
        "user": "nielsr",
        "type": "user"
      },
      "summary": "Le rapide croissance des modèles de langage de haut niveau (LLMs) a révélé des limites importantes dans l'architecture actuelle des dispositifs. Ces limites comprennent la capacité de mémoire, l'efficacité du calcul et le mode indirect de connexion. DeepSeek-V3 a été entraîné à l'aide de 2,048 GPU NVIDIA H800, démontrant des méthodes efficaces pour gérer la collaboration entre modèles et matériel, et facilitant l'entraînement et l'inférence de manière économique en termes d'expansion. Dans cet article, nous analysons en détail l'architecture du modèle DeepSeek-V3/R1 et sa structure d'infrastructure d'IA, soulignant comme innovations principales la communication efficace de la mémoire par l'Attention Multi-head Latent (MLA), la complémentation de la communication du calcul par l'architecture d'Experts Mixte (MoE), l'utilisation de FP8 pour maximiser le rendement du matériel et la topologie de réseau Multi-Plane pour minimiser la surcharge de réseau au niveau du cluster. Pendant le développement de DeepSeek-V3, les limites du matériel ont été prises en compte, permettant à de nombreux experts académiques et industriels de discuter des directions futures du matériel, en examinant des aspects tels que les unités de calcul à faible précision, la combinaison d'expansion et de réduction, et les innovations dans les fabrications de communication à faible latence. Ces perspectives mettent en avant l'importance de la collaboration entre matériel et modèles pour répondre aux besoins croissants de charge de travail de l'IA, offrant des plans innovants pour les prochains systèmes d'IA.",
      "upvotes": 10,
      "discussionId": "682578cb1b93095c06142a55",
      "ai_keywords": [
        "Multi-head Latent Attention (MLA)",
        "Mixture of Experts (MoE)",
        "FP8 mixed-precision training",
        "Multi-Plane Network Topology",
        "low-precision computation units",
        "scale-up and scale-out convergence",
        "low-latency communication fabrics"
      ]
    },
    "publishedAt": "2025-05-14T08:39:03.000Z",
    "title": "Insights into DeepSeek-V3: Scaling Challenges and Reflections on\n  Hardware for AI Architectures",
    "summary": "The rapid scaling of large language models (LLMs) has unveiled critical\nlimitations in current hardware architectures, including constraints in memory\ncapacity, computational efficiency, and interconnection bandwidth. DeepSeek-V3,\ntrained on 2,048 NVIDIA H800 GPUs, demonstrates how hardware-aware model\nco-design can effectively address these challenges, enabling cost-efficient\ntraining and inference at scale. This paper presents an in-depth analysis of\nthe DeepSeek-V3/R1 model architecture and its AI infrastructure, highlighting\nkey innovations such as Multi-head Latent Attention (MLA) for enhanced memory\nefficiency, Mixture of Experts (MoE) architectures for optimized\ncomputation-communication trade-offs, FP8 mixed-precision training to unlock\nthe full potential of hardware capabilities, and a Multi-Plane Network Topology\nto minimize cluster-level network overhead. Building on the hardware\nbottlenecks encountered during DeepSeek-V3's development, we engage in a\nbroader discussion with academic and industry peers on potential future\nhardware directions, including precise low-precision computation units,\nscale-up and scale-out convergence, and innovations in low-latency\ncommunication fabrics. These insights underscore the critical role of hardware\nand model co-design in meeting the escalating demands of AI workloads, offering\na practical blueprint for innovation in next-generation AI systems.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.09343.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5f1158120c833276f61f1a84",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
      "fullname": "Niels Rogge",
      "name": "nielsr",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 862
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.08455",
      "authors": [
        {
          "_id": "6824176351679cbc704daa88",
          "user": {
            "_id": "6483b3d52193a1768c00c5ff",
            "avatarUrl": "/avatars/489e7bdcdac4600bd5b136c930a29cd2.svg",
            "isPro": false,
            "fullname": "Pritam Sarkar",
            "user": "pritamqu",
            "type": "user"
          },
          "name": "Pritam Sarkar",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-14T04:09:09.283Z",
          "hidden": false
        },
        {
          "_id": "6824176351679cbc704daa89",
          "name": "Ali Etemad",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-13T11:35:58.000Z",
      "submittedOnDailyAt": "2025-05-15T04:58:35.875Z",
      "title": "VCRBench : Révision de la capacité d'inférence causal à long terme dans les modèles de langage vidéo à grande échelle",
      "submittedOnDailyBy": {
        "_id": "6483b3d52193a1768c00c5ff",
        "avatarUrl": "/avatars/489e7bdcdac4600bd5b136c930a29cd2.svg",
        "isPro": false,
        "fullname": "Pritam Sarkar",
        "user": "pritamqu",
        "type": "user"
      },
      "summary": "Récemment, le développement de la compréhension des images a permis aux modèles de langage d'images à grande échelle (LVLMs) de faire des inférences causales basées sur des images. Cependant, en raison du manque de référentiels pour évaluer ces inférences causales dans des configurations de vison orientées vers des objectifs, ces recherches ont été peu développées. Pour corriger ce problème, on introduit VCRBench, un nouveau référentiel. VCRBench est constitué d'images de processus qui réordonnent de manière décisive l'ordre des activités quotidiennes simples, et chaque shot détecte des événements causales. Ce référentiel évalue si les LVLMs peuvent identifier les événements nécessaires pour atteindre un objectif spécifique et fournir des raisons, ainsi que si ils peuvent identifier la séquence correcte. De plus, VCRBench est conçu pour éviter les problèmes d'évaluation de réponses ouvertes en utilisant des résumés courts de langage, en favorisant une structure de multiples options ou de questions binaires. Dans les évaluations récentes de VCRBench, les LVLMs ont montré des difficultés à faire des inférences causales à long terme basées sur des images, ce qui est attribué à la complexité de modéliser des relations causales à long terme directement à partir de l'observation visuelle. Pour aborder ce problème, on propose un approche modulaire appelée Recognition-Reasoning Decomposition (RRD). RRD divise la compréhension causale basée sur des images en deux sous-tâches : la reconnaissance d'images et la compréhension causale. Les résultats des expérimentations réalisées sur VCRBench montrent que RRD améliore significativement la précision du référentiel, avec un accroissement maximum de 25,2%. Enfin, un analyse détaillée révèle des constats intéressants, comme que les LVLMs principalement dépendent des connaissances linguistiques dans des tâches d'inférence causale complexe basées sur des images de long texte.",
      "upvotes": 2,
      "discussionId": "6824176551679cbc704daafb",
      "projectPage": "https://pritamsarkar.com/VCRBench/",
      "githubRepo": "https://github.com/pritamqu/VCRBench",
      "ai_keywords": [
        "Large Video Language Models (LVLMs)",
        "causal reasoning",
        "benchmarks",
        "procedural videos",
        "causal dependencies",
        "video-based long-form causal reasoning",
        "VCRBench",
        "video recognition",
        "Recognition-Reasoning Decomposition (RRD)"
      ]
    },
    "publishedAt": "2025-05-13T07:35:58.000Z",
    "title": "VCRBench: Exploring Long-form Causal Reasoning Capabilities of Large\n  Video Language Models",
    "summary": "Despite recent advances in video understanding, the capabilities of Large\nVideo Language Models (LVLMs) to perform video-based causal reasoning remains\nunderexplored, largely due to the absence of relevant and dedicated benchmarks\nfor evaluating causal reasoning in visually grounded and goal-driven settings.\nTo fill this gap, we introduce a novel benchmark named Video-based long-form\nCausal Reasoning (VCRBench). We create VCRBench using procedural videos of\nsimple everyday activities, where the steps are deliberately shuffled with each\nclip capturing a key causal event, to test whether LVLMs can identify, reason\nabout, and correctly sequence the events needed to accomplish a specific goal.\nMoreover, the benchmark is carefully designed to prevent LVLMs from exploiting\nlinguistic shortcuts, as seen in multiple-choice or binary QA formats, while\nalso avoiding the challenges associated with evaluating open-ended QA. Our\nevaluation of state-of-the-art LVLMs on VCRBench suggests that these models\nstruggle with video-based long-form causal reasoning, primarily due to their\ndifficulty in modeling long-range causal dependencies directly from visual\nobservations. As a simple step toward enabling such capabilities, we propose\nRecognition-Reasoning Decomposition (RRD), a modular approach that breaks\nvideo-based causal reasoning into two sub-tasks of video recognition and causal\nreasoning. Our experiments on VCRBench show that RRD significantly boosts\naccuracy on VCRBench, with gains of up to 25.2%. Finally, our thorough analysis\nreveals interesting insights, for instance, that LVLMs primarily rely on\nlanguage knowledge for complex video-based long-form causal reasoning tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.08455.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6483b3d52193a1768c00c5ff",
      "avatarUrl": "/avatars/489e7bdcdac4600bd5b136c930a29cd2.svg",
      "fullname": "Pritam Sarkar",
      "name": "pritamqu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]