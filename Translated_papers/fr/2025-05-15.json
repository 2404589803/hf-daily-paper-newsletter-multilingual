[
  {
    "paper": {
      "id": "2505.04410",
      "authors": [
        {
          "_id": "681d615fbd89ba9ceb5e94bc",
          "user": {
            "_id": "64a385281cbf675203fbb7df",
            "avatarUrl": "/avatars/f259d080d3127c45bcf564a8d1fafcc6.svg",
            "isPro": false,
            "fullname": "Junjie Wang",
            "user": "xiaomoguhzz",
            "type": "user"
          },
          "name": "Junjie Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-09T07:21:48.630Z",
          "hidden": false
        },
        {
          "_id": "681d615fbd89ba9ceb5e94bd",
          "name": "Bin Chen",
          "hidden": false
        },
        {
          "_id": "681d615fbd89ba9ceb5e94be",
          "name": "Yulin Li",
          "hidden": false
        },
        {
          "_id": "681d615fbd89ba9ceb5e94bf",
          "name": "Bin Kang",
          "hidden": false
        },
        {
          "_id": "681d615fbd89ba9ceb5e94c0",
          "name": "Yichi Chen",
          "hidden": false
        },
        {
          "_id": "681d615fbd89ba9ceb5e94c1",
          "name": "Zhuotao Tian",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64a385281cbf675203fbb7df/MpXgXINxg1RmSFnfeqXqU.mp4"
      ],
      "publishedAt": "2025-05-07T13:46:34.000Z",
      "submittedOnDailyAt": "2025-05-15T06:24:33.810Z",
      "title": "DeCLIP : Observation de densité par entraînement séparé de boîtes ouvertes",
      "submittedOnDailyBy": {
        "_id": "64a385281cbf675203fbb7df",
        "avatarUrl": "/avatars/f259d080d3127c45bcf564a8d1fafcc6.svg",
        "isPro": false,
        "fullname": "Junjie Wang",
        "user": "xiaomoguhzz",
        "type": "user"
      },
      "summary": "La tâche de prédiction visuelle dense est tendue à être limitée par des catégories prédéfinies, ce qui restreint le concept d'images et réduit son application dans des scénarios réels, en difficile son utilisation pratique. Les modèles de langage visuo-linguistique comme CLIP montrent des résultats exceptionnels dans des tâches de chaînes de mots ouvertes, mais lorsqu'ils sont appliqués directement à la prédiction dense, leur performance est limitée en raison de la restriction dans la représentation des caractéristiques locales. Dans cet article, on observe que les tokens d'images de CLIP ne peuvent pas collecter de l'information efficacement dans des zones spatialement ou sémantiquement liées, et on propose le cadre DeCLIP pour résoudre ce problème. DeCLIP renforce CLIP en fortifiant le module d'attention automatique, permettant d'obtenir des caractéristiques de \"contenu\" et de \"contexte\". Les caractéristiques de \"contenu\" sont alignées avec la représentation de coupes d'images, améliorant l'identification locale et celles de \"contexte\" sont apprises sous la supervision de modèles visuels fondamentaux comme DINO, en maintenant la relation spatiale. Les expériences étendues montrent que DeCLIP fournit des résultats significativement meilleurs que ceux actuels. Le code est disponible sur https://github.com/xiaomoguhz/DeCLIP.",
      "upvotes": 28,
      "discussionId": "681d6161bd89ba9ceb5e9571",
      "ai_keywords": [
        "Vision-Language Models (VLMs)",
        "CLIP",
        "dense prediction",
        "predefined categories",
        "open-vocabulary tasks",
        "spatially related regions",
        "semantically related regions",
        "local discriminability",
        "spatial consistency",
        "self-attention module",
        "content features",
        "context features",
        "image crop representations",
        "vision foundation models",
        "DINO",
        "object detection",
        "semantic segmentation"
      ]
    },
    "publishedAt": "2025-05-07T09:46:34.000Z",
    "title": "DeCLIP: Decoupled Learning for Open-Vocabulary Dense Perception",
    "summary": "Dense visual prediction tasks have been constrained by their reliance on\npredefined categories, limiting their applicability in real-world scenarios\nwhere visual concepts are unbounded. While Vision-Language Models (VLMs) like\nCLIP have shown promise in open-vocabulary tasks, their direct application to\ndense prediction often leads to suboptimal performance due to limitations in\nlocal feature representation. In this work, we present our observation that\nCLIP's image tokens struggle to effectively aggregate information from\nspatially or semantically related regions, resulting in features that lack\nlocal discriminability and spatial consistency. To address this issue, we\npropose DeCLIP, a novel framework that enhances CLIP by decoupling the\nself-attention module to obtain ``content'' and ``context'' features\nrespectively. The ``content'' features are aligned with image crop\nrepresentations to improve local discriminability, while ``context'' features\nlearn to retain the spatial correlations under the guidance of vision\nfoundation models, such as DINO. Extensive experiments demonstrate that DeCLIP\nsignificantly outperforms existing methods across multiple open-vocabulary\ndense prediction tasks, including object detection and semantic segmentation.\nCode is available at magenta{https://github.com/xiaomoguhz/DeCLIP}.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64a385281cbf675203fbb7df/MpXgXINxg1RmSFnfeqXqU.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.04410.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64a385281cbf675203fbb7df",
      "avatarUrl": "/avatars/f259d080d3127c45bcf564a8d1fafcc6.svg",
      "fullname": "Junjie Wang",
      "name": "xiaomoguhzz",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.09568",
      "authors": [
        {
          "_id": "68254419181d43c25d829239",
          "name": "Jiuhai Chen",
          "hidden": false
        },
        {
          "_id": "68254419181d43c25d82923a",
          "name": "Zhiyang Xu",
          "hidden": false
        },
        {
          "_id": "68254419181d43c25d82923b",
          "name": "Xichen Pan",
          "hidden": false
        },
        {
          "_id": "68254419181d43c25d82923c",
          "name": "Yushi Hu",
          "hidden": false
        },
        {
          "_id": "68254419181d43c25d82923d",
          "name": "Can Qin",
          "hidden": false
        },
        {
          "_id": "68254419181d43c25d82923e",
          "name": "Tom Goldstein",
          "hidden": false
        },
        {
          "_id": "68254419181d43c25d82923f",
          "name": "Lifu Huang",
          "hidden": false
        },
        {
          "_id": "68254419181d43c25d829240",
          "name": "Tianyi Zhou",
          "hidden": false
        },
        {
          "_id": "68254419181d43c25d829241",
          "name": "Saining Xie",
          "hidden": false
        },
        {
          "_id": "68254419181d43c25d829242",
          "name": "Silvio Savarese",
          "hidden": false
        },
        {
          "_id": "68254419181d43c25d829243",
          "name": "Le Xue",
          "hidden": false
        },
        {
          "_id": "68254419181d43c25d829244",
          "name": "Caiming Xiong",
          "hidden": false
        },
        {
          "_id": "68254419181d43c25d829245",
          "name": "Ran Xu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-14T17:11:07.000Z",
      "submittedOnDailyAt": "2025-05-15T00:07:05.564Z",
      "title": "BLIP3-o : Famille de modèles multimodals complètement ouverts - Architecture, entraînement, jeux de données",
      "submittedOnDailyBy": {
        "_id": "6393847e3e30234ae798b7be",
        "avatarUrl": "/avatars/daeb8c37dff4432d837a69b87c196521.svg",
        "isPro": true,
        "fullname": "JiuhaiChen",
        "user": "jiuhai",
        "type": "user"
      },
      "summary": "Dans les derniers études de modèles de Damo, une attention particulière a été accordée à l'intégration de la compréhension et de la génération d'images. Les décisions de conception pour la compréhension d'images ont été largement étudiées, mais la structure optimale et les méthodes d'entraînement d'un cadre de travail d'intégration de la génération d'images nécessitent encore plus d'investigation. Étant donné que les modèles d'auto-reconstruction et de diffusion ont démontré la possibilité de générer des images de haute qualité avec une forte scalabilité, une recherche détaillée a été effectuée sur la configuration des modèles d'intégration. Sur la base de ces recherches, un nouvel approche a été proposée. Cette approche diffère des simples VAE-basés en utilisant un transformateur de diffusion pour générer des caractéristiques d'images riches en signification, ce qui permet d'améliorer l'efficacité de l'entraînement et la qualité de la génération. De plus, une stratégie d'entraînement séquentiel prédictif pour l'intégration de modèles est présentée, démontrant les avantages pratiques de renforcer la capacité de génération d'images tout en maintenant la compréhension d'images. Enfin, en utilisant différentes captures sur GPT-4o, un ensemble de données d'entraînement de haute qualité BLIP3o-60k a été sélectionné, qui inclut des vidéos d'objets, des actions de mains humaines et d'autres ensembles spatiaux divers. Sur la base de ce nouveau design de modèle, méthode d'entraînement et ensemble de données, le système de modèle intégré le plus avancé, BLIP3-o, a été développé. BLIP3-o montre le meilleur rendement dans les deux tests de compréhension et de génération d'images. Pour encourager futures recherches, le code du modèle, les poids du modèle, les scripts d'entraînement, l'ensemble de données d'entraînement de prédiction et l'ensemble de données d'entraînement de commandes sont publiés complètement.",
      "upvotes": 13,
      "discussionId": "6825441a181d43c25d82927a",
      "ai_keywords": [
        "autoregressive models",
        "diffusion models",
        "semantically rich CLIP image features",
        "diffusion transformer",
        "VAE-based representations",
        "sequential pretraining strategy",
        "image understanding",
        "image generation",
        "instruction-tuning dataset",
        "GPT-4o",
        "state-of-the-art unified multimodal models"
      ]
    },
    "publishedAt": "2025-05-14T13:11:07.000Z",
    "title": "BLIP3-o: A Family of Fully Open Unified Multimodal Models-Architecture,\n  Training and Dataset",
    "summary": "Unifying image understanding and generation has gained growing attention in\nrecent research on multimodal models. Although design choices for image\nunderstanding have been extensively studied, the optimal model architecture and\ntraining recipe for a unified framework with image generation remain\nunderexplored. Motivated by the strong potential of autoregressive and\ndiffusion models for high-quality generation and scalability, we conduct a\ncomprehensive study of their use in unified multimodal settings, with emphasis\non image representations, modeling objectives, and training strategies.\nGrounded in these investigations, we introduce a novel approach that employs a\ndiffusion transformer to generate semantically rich CLIP image features, in\ncontrast to conventional VAE-based representations. This design yields both\nhigher training efficiency and improved generative quality. Furthermore, we\ndemonstrate that a sequential pretraining strategy for unified models-first\ntraining on image understanding and subsequently on image generation-offers\npractical advantages by preserving image understanding capability while\ndeveloping strong image generation ability. Finally, we carefully curate a\nhigh-quality instruction-tuning dataset BLIP3o-60k for image generation by\nprompting GPT-4o with a diverse set of captions covering various scenes,\nobjects, human gestures, and more. Building on our innovative model design,\ntraining recipe, and datasets, we develop BLIP3-o, a suite of state-of-the-art\nunified multimodal models. BLIP3-o achieves superior performance across most of\nthe popular benchmarks spanning both image understanding and generation tasks.\nTo facilitate future research, we fully open-source our models, including code,\nmodel weights, training scripts, and pretraining and instruction tuning\ndatasets.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.09568.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6393847e3e30234ae798b7be",
      "avatarUrl": "/avatars/daeb8c37dff4432d837a69b87c196521.svg",
      "fullname": "JiuhaiChen",
      "name": "jiuhai",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 27
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.09343",
      "authors": [
        {
          "_id": "682578ca1b93095c061429ff",
          "name": "Chenggang Zhao",
          "hidden": false
        },
        {
          "_id": "682578ca1b93095c06142a00",
          "name": "Chengqi Deng",
          "hidden": false
        },
        {
          "_id": "682578ca1b93095c06142a01",
          "name": "Chong Ruan",
          "hidden": false
        },
        {
          "_id": "682578ca1b93095c06142a02",
          "name": "Damai Dai",
          "hidden": false
        },
        {
          "_id": "682578ca1b93095c06142a03",
          "name": "Huazuo Gao",
          "hidden": false
        },
        {
          "_id": "682578ca1b93095c06142a04",
          "name": "Jiashi Li",
          "hidden": false
        },
        {
          "_id": "682578ca1b93095c06142a05",
          "name": "Liyue Zhang",
          "hidden": false
        },
        {
          "_id": "682578ca1b93095c06142a06",
          "name": "Panpan Huang",
          "hidden": false
        },
        {
          "_id": "682578ca1b93095c06142a07",
          "name": "Shangyan Zhou",
          "hidden": false
        },
        {
          "_id": "682578ca1b93095c06142a08",
          "name": "Shirong Ma",
          "hidden": false
        },
        {
          "_id": "682578ca1b93095c06142a09",
          "name": "Wenfeng Liang",
          "hidden": false
        },
        {
          "_id": "682578ca1b93095c06142a0a",
          "name": "Ying He",
          "hidden": false
        },
        {
          "_id": "682578ca1b93095c06142a0b",
          "name": "Yuqing Wang",
          "hidden": false
        },
        {
          "_id": "682578ca1b93095c06142a0c",
          "name": "Yuxuan Liu",
          "hidden": false
        },
        {
          "_id": "682578ca1b93095c06142a0d",
          "name": "Y. X. Wei",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-14T12:39:03.000Z",
      "submittedOnDailyAt": "2025-05-15T05:22:39.526Z",
      "title": "Vision de DeepSeek-V3 : Problèmes d'échelle dans l'architecture de l'IA et la rétroaction du matériel",
      "submittedOnDailyBy": {
        "_id": "5f1158120c833276f61f1a84",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
        "isPro": false,
        "fullname": "Niels Rogge",
        "user": "nielsr",
        "type": "user"
      },
      "summary": "Le rapide croissance des modèles de langage grands (LLMs) a révélé des limites importantes dans l'architecture actuelle de l'hardware. Ces limites comprennent des restrictions sur la capacité de la mémoire, l'efficacité du calcul et la vitesse de connexion à Internet. DeepSeek-V3, entraîné avec 2,048 NVIDIA H800 GPUs, a démontré son efficacité en abordant ces défis, démontrant son intérêt pour l'hardware et son conception conjointe avec le modèle. Dans cet article, nous fournissons un analyse détaillée de l'architecture du modèle DeepSeek-V3/R1 et de son infrastructure IA, soulignant des innovations telles que l'amélioration de l'efficacité de la mémoire avec l'Attention Multi-head Latent (MLA), l'optimisation de l'équilibre entre calcul et communication par l'architecture Mixture of Experts (MoE), la maximisation des fonctions de l'hardware par l'entraînement en précision FP8, et la minimisation du surcoût de la réseau à l'échelle de cluster par la topologie Multi-Plane. Pendant le développement de DeepSeek-V3, les limites de l'hardware ont été abordées en collaboration avec des partenaires académiques et industriels, discutant des innovations dans les unités de calcul de haute et de basse précision, la convergence des échelles et l'innovation dans les fabriques de faible vitesse de communication. Ces réflexions soulignent l'importance du concepteur conjoint de l'hardware et du modèle pour répondre aux besoins croissants de charge de travail en IA, offrant des plans pratiques pour l'innovation dans les systèmes IA de la prochaine génération.",
      "upvotes": 10,
      "discussionId": "682578cb1b93095c06142a55",
      "ai_keywords": [
        "Multi-head Latent Attention (MLA)",
        "Mixture of Experts (MoE)",
        "FP8 mixed-precision training",
        "Multi-Plane Network Topology",
        "low-precision computation units",
        "scale-up and scale-out convergence",
        "low-latency communication fabrics"
      ]
    },
    "publishedAt": "2025-05-14T08:39:03.000Z",
    "title": "Insights into DeepSeek-V3: Scaling Challenges and Reflections on\n  Hardware for AI Architectures",
    "summary": "The rapid scaling of large language models (LLMs) has unveiled critical\nlimitations in current hardware architectures, including constraints in memory\ncapacity, computational efficiency, and interconnection bandwidth. DeepSeek-V3,\ntrained on 2,048 NVIDIA H800 GPUs, demonstrates how hardware-aware model\nco-design can effectively address these challenges, enabling cost-efficient\ntraining and inference at scale. This paper presents an in-depth analysis of\nthe DeepSeek-V3/R1 model architecture and its AI infrastructure, highlighting\nkey innovations such as Multi-head Latent Attention (MLA) for enhanced memory\nefficiency, Mixture of Experts (MoE) architectures for optimized\ncomputation-communication trade-offs, FP8 mixed-precision training to unlock\nthe full potential of hardware capabilities, and a Multi-Plane Network Topology\nto minimize cluster-level network overhead. Building on the hardware\nbottlenecks encountered during DeepSeek-V3's development, we engage in a\nbroader discussion with academic and industry peers on potential future\nhardware directions, including precise low-precision computation units,\nscale-up and scale-out convergence, and innovations in low-latency\ncommunication fabrics. These insights underscore the critical role of hardware\nand model co-design in meeting the escalating demands of AI workloads, offering\na practical blueprint for innovation in next-generation AI systems.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.09343.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5f1158120c833276f61f1a84",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
      "fullname": "Niels Rogge",
      "name": "nielsr",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 862
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.08455",
      "authors": [
        {
          "_id": "6824176351679cbc704daa88",
          "user": {
            "_id": "6483b3d52193a1768c00c5ff",
            "avatarUrl": "/avatars/489e7bdcdac4600bd5b136c930a29cd2.svg",
            "isPro": false,
            "fullname": "Pritam Sarkar",
            "user": "pritamqu",
            "type": "user"
          },
          "name": "Pritam Sarkar",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-14T04:09:09.283Z",
          "hidden": false
        },
        {
          "_id": "6824176351679cbc704daa89",
          "name": "Ali Etemad",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-13T11:35:58.000Z",
      "submittedOnDailyAt": "2025-05-15T04:58:35.875Z",
      "title": "VCRBench: Révision de la capacité d'inférence causal à long terme dans les grands modèles de langage vidéo",
      "submittedOnDailyBy": {
        "_id": "6483b3d52193a1768c00c5ff",
        "avatarUrl": "/avatars/489e7bdcdac4600bd5b136c930a29cd2.svg",
        "isPro": false,
        "fullname": "Pritam Sarkar",
        "user": "pritamqu",
        "type": "user"
      },
      "summary": "Bien sûr, voici la traduction en français :\n\nMalgré l'avancement du développement de la compréhension des images, les modèles de langage d'images à grande échelle (LVLMs) n'ont encore pas d'évaluations qui confirment leur capacité à faire des inférences causales basées sur les images. En raison de cette lacune, nous présentons un nouveau critère d'évaluation appelé \"Video-based long-form Causal Reasoning (VCRBench)\". VCRBench est composé de vidéos d'activités quotidiennes où les étapes sont mélangées de manière aléatoire. Chaque vidéo capture un événement causal spécifique et évalue les LVLMs dans leur capacité à reconnaître les événements nécessaires pour atteindre un objectif, expliquer la raison et identifier la séquence correcte. De plus, ce critère d'évaluation évite les problèmes d'évaluation ouverte, en utilisant des slots linguistiques, des multiples options ou des formats de questions-réponses binaires, et a été conçu pour éviter la dépendance sur des expressions linguistiques. Les résultats de l'évaluation des modèles les plus récents sur VCRBench montrent que les LVLMs ont des difficultés dans l'inférence causal de longues phrases basées sur les images, ce qui est attribué à la complexité de modéliser des relations causales qui s'éloignent de l'observation visuelle directe. Pour aborder ces difficultés, nous proposons la \"Recognition-Reasoning Decomposition (RRD)\", qui est un approche modulaire qui divise l'inférence causal basée sur les images en deux tâches sous-ordonnées : reconnaissance d'images et inférence causal. Les expériences sur VCRBench montrent que la RRD améliore significativement la précision de VCRBench, avec un accroissement maximum de 25.2%. Enfin, les LVLMs présentent des indications intéressantes sur leur dépendance sur un savoir linguistique dans les tâches d'inférence causal de longues phrases basées sur des images complexes.",
      "upvotes": 2,
      "discussionId": "6824176551679cbc704daafb",
      "projectPage": "https://pritamsarkar.com/VCRBench/",
      "githubRepo": "https://github.com/pritamqu/VCRBench",
      "ai_keywords": [
        "Large Video Language Models (LVLMs)",
        "causal reasoning",
        "benchmarks",
        "procedural videos",
        "causal dependencies",
        "video-based long-form causal reasoning",
        "VCRBench",
        "video recognition",
        "Recognition-Reasoning Decomposition (RRD)"
      ]
    },
    "publishedAt": "2025-05-13T07:35:58.000Z",
    "title": "VCRBench: Exploring Long-form Causal Reasoning Capabilities of Large\n  Video Language Models",
    "summary": "Despite recent advances in video understanding, the capabilities of Large\nVideo Language Models (LVLMs) to perform video-based causal reasoning remains\nunderexplored, largely due to the absence of relevant and dedicated benchmarks\nfor evaluating causal reasoning in visually grounded and goal-driven settings.\nTo fill this gap, we introduce a novel benchmark named Video-based long-form\nCausal Reasoning (VCRBench). We create VCRBench using procedural videos of\nsimple everyday activities, where the steps are deliberately shuffled with each\nclip capturing a key causal event, to test whether LVLMs can identify, reason\nabout, and correctly sequence the events needed to accomplish a specific goal.\nMoreover, the benchmark is carefully designed to prevent LVLMs from exploiting\nlinguistic shortcuts, as seen in multiple-choice or binary QA formats, while\nalso avoiding the challenges associated with evaluating open-ended QA. Our\nevaluation of state-of-the-art LVLMs on VCRBench suggests that these models\nstruggle with video-based long-form causal reasoning, primarily due to their\ndifficulty in modeling long-range causal dependencies directly from visual\nobservations. As a simple step toward enabling such capabilities, we propose\nRecognition-Reasoning Decomposition (RRD), a modular approach that breaks\nvideo-based causal reasoning into two sub-tasks of video recognition and causal\nreasoning. Our experiments on VCRBench show that RRD significantly boosts\naccuracy on VCRBench, with gains of up to 25.2%. Finally, our thorough analysis\nreveals interesting insights, for instance, that LVLMs primarily rely on\nlanguage knowledge for complex video-based long-form causal reasoning tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.08455.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6483b3d52193a1768c00c5ff",
      "avatarUrl": "/avatars/489e7bdcdac4600bd5b136c930a29cd2.svg",
      "fullname": "Pritam Sarkar",
      "name": "pritamqu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]