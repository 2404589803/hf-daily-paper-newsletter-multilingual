[
  {
    "paper": {
      "id": "2503.16905",
      "authors": [
        {
          "_id": "67e0c13fe5fa0da84e134581",
          "user": {
            "_id": "658be7fe135580745c510323",
            "avatarUrl": "/avatars/830e5cec4565efdc23226a86a0fcef0e.svg",
            "isPro": false,
            "fullname": "Jian Zhang",
            "user": "VentureZJ",
            "type": "user"
          },
          "name": "Jian Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-24T08:07:08.476Z",
          "hidden": false
        },
        {
          "_id": "67e0c13fe5fa0da84e134582",
          "name": "Zhiyuan Wang",
          "hidden": false
        },
        {
          "_id": "67e0c13fe5fa0da84e134583",
          "name": "Zhangqi Wang",
          "hidden": false
        },
        {
          "_id": "67e0c13fe5fa0da84e134584",
          "name": "Xinyu Zhang",
          "hidden": false
        },
        {
          "_id": "67e0c13fe5fa0da84e134585",
          "name": "Fangzhi Xu",
          "hidden": false
        },
        {
          "_id": "67e0c13fe5fa0da84e134586",
          "user": {
            "_id": "66ac77011cfb12c087605acb",
            "avatarUrl": "/avatars/54c06bd1c4c9d491470ed4162c2301ae.svg",
            "isPro": false,
            "fullname": "Lin",
            "user": "Qika",
            "type": "user"
          },
          "name": "Qika Lin",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-24T02:19:51.913Z",
          "hidden": false
        },
        {
          "_id": "67e0c13fe5fa0da84e134587",
          "name": "Rui Mao",
          "hidden": false
        },
        {
          "_id": "67e0c13fe5fa0da84e134588",
          "name": "Erik Cambria",
          "hidden": false
        },
        {
          "_id": "67e0c13fe5fa0da84e134589",
          "name": "Jun Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-21T07:13:45.000Z",
      "submittedOnDailyAt": "2025-03-24T00:51:41.644Z",
      "title": "MAPS : Cadre de Multi-Agentes pour la Résolution de Problèmes en Informatique Basée sur les Sept Grandes Heuristiques de Page-Eagle et le Clan de Smalltalk Guide par le Clan de Smalltalk",
      "submittedOnDailyBy": {
        "_id": "658be7fe135580745c510323",
        "avatarUrl": "/avatars/830e5cec4565efdc23226a86a0fcef0e.svg",
        "isPro": false,
        "fullname": "Jian Zhang",
        "user": "VentureZJ",
        "type": "user"
      },
      "summary": "Les problèmes de la science multi-physique (MSPs) nécessitent l'intégration de modèles tels que des textes ou des graphiques, et sont une grande tâche dans l'intelligence artificielle. Bien que des progrès aient été réalisés dans les problèmes scientifiques traditionnels, les MSPs doivent aborder deux problèmes principaux : la nécessité d'une inférence systématique de la diversité et le manque de capacités de rétroaction et de révision. Pour résoudre ces problèmes, nous proposons un cadre de travail multi-agent basé sur le Big Seven Personality et les Socratic Guideline appelé Framework Multi-Agent (MAPS). Ce cadre utilise sept agents différents et guide la résolution des MSPs à travers la structure de rétroaction et l'utilisation du méthode Socratique. Pour aborder le premier problème, nous proposons une stratégie de résolution pour les quatre agents évolutifs, où chaque agent se concentre sur un aspect spécifique du processus de résolution des problèmes. Pour aborder le second problème, nous introduisons un agent Critic modélisé sur des questions Socratiques, qui stimule le pensée critique et l'apprentissage automatique. Nous avons effectué des expériences larges avec les ensembles de données EMMA, Olympia et MathVista, obtenant des résultats enthousiasmants qui dépassent les modèles de l'état de l'art d'un 15,84% dans toutes les tâches. De plus, nous avons effectué des expériences analytiques supplémentaires qui montrent le développement et la capacité de généralisation du modèle.",
      "upvotes": 33,
      "discussionId": "67e0c147e5fa0da84e1347f5",
      "githubRepo": "https://github.com/exoskeletonzj/MAPS"
    },
    "publishedAt": "2025-03-21T03:13:45.000Z",
    "title": "MAPS: A Multi-Agent Framework Based on Big Seven Personality and\n  Socratic Guidance for Multimodal Scientific Problem Solving",
    "summary": "Multimodal scientific problems (MSPs) involve complex issues that require the\nintegration of multiple modalities, such as text and diagrams, presenting a\nsignificant challenge in artificial intelligence. While progress has been made\nin addressing traditional scientific problems, MSPs still face two primary\nissues: the challenge of multi-modal comprehensive reasoning in scientific\nproblem-solving and the lack of reflective and rethinking capabilities. To\naddress these issues, we introduce a Multi-Agent framework based on the Big\nSeven Personality and Socratic guidance (MAPS). This framework employs seven\ndistinct agents that leverage feedback mechanisms and the Socratic method to\nguide the resolution of MSPs. To tackle the first issue, we propose a\nprogressive four-agent solving strategy, where each agent focuses on a specific\nstage of the problem-solving process. For the second issue, we introduce a\nCritic agent, inspired by Socratic questioning, which prompts critical thinking\nand stimulates autonomous learning. We conduct extensive experiments on the\nEMMA, Olympiad, and MathVista datasets, achieving promising results that\noutperform the current SOTA model by 15.84% across all tasks. Meanwhile, the\nadditional analytical experiments also verify the model's progress as well as\ngeneralization ability.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16905.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "658be7fe135580745c510323",
      "avatarUrl": "/avatars/830e5cec4565efdc23226a86a0fcef0e.svg",
      "fullname": "Jian Zhang",
      "name": "VentureZJ",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.16874",
      "authors": [
        {
          "_id": "67e0c1ce151ca9ed9284dc52",
          "user": {
            "_id": "658be7fe135580745c510323",
            "avatarUrl": "/avatars/830e5cec4565efdc23226a86a0fcef0e.svg",
            "isPro": false,
            "fullname": "Jian Zhang",
            "user": "VentureZJ",
            "type": "user"
          },
          "name": "Jian Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-24T08:07:05.271Z",
          "hidden": false
        },
        {
          "_id": "67e0c1ce151ca9ed9284dc53",
          "name": "Zhangqi Wang",
          "hidden": false
        },
        {
          "_id": "67e0c1ce151ca9ed9284dc54",
          "name": "Haiping Zhu",
          "hidden": false
        },
        {
          "_id": "67e0c1ce151ca9ed9284dc55",
          "name": "Jun Liu",
          "hidden": false
        },
        {
          "_id": "67e0c1ce151ca9ed9284dc56",
          "user": {
            "_id": "66ac77011cfb12c087605acb",
            "avatarUrl": "/avatars/54c06bd1c4c9d491470ed4162c2301ae.svg",
            "isPro": false,
            "fullname": "Lin",
            "user": "Qika",
            "type": "user"
          },
          "name": "Qika Lin",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-24T02:22:15.812Z",
          "hidden": false
        },
        {
          "_id": "67e0c1ce151ca9ed9284dc57",
          "name": "Erik Cambria",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-21T06:19:55.000Z",
      "submittedOnDailyAt": "2025-03-24T00:56:26.218Z",
      "title": "MARS : Marco d'optimisation de photos efficace en utilisant le cadre d'optimisation de photos non professionnels avec l'utilisation du Guider Scoratatek",
      "submittedOnDailyBy": {
        "_id": "658be7fe135580745c510323",
        "avatarUrl": "/avatars/830e5cec4565efdc23226a86a0fcef0e.svg",
        "isPro": false,
        "fullname": "Jian Zhang",
        "user": "VentureZJ",
        "type": "user"
      },
      "summary": "La base de la réponse du modèle de langue inclut le formulaire d'entrée, et la qualité du formulaire a un impact direct sur la validité de la réponse. La Technique d'Optimisation Automatique des Formulaires (AOF) libère les concepteurs de la cognition des formulaires conçus et cherche à élargir l'espace de conception. Cependant, les méthodes actuelles d'AOF sont confrontées à des limitations de flexibilité des modèles fixes et à la recherche efficace dans l'espace des formulaires comme des principales défis. Dans ce contexte, nous proposons un cadre de travail Multi-Agent (MARS) basé sur la technologie d'intégration d'agents, qui effectue des plans automatiques. En particulier, MARS est composé de 7 agents, chacun avec une fonction différente, et sa principale fonction est de concevoir des chemins d'optimisation qui garantissent la flexibilité grâce à l'utilisation automatique du Planner. De plus, il utilise le modèle de conversation Socratique de Teacher-Critic-Student pour optimiser de manière continue le formulaire tout en effectuant une recherche efficace. Nous avons effectué des expériences larges avec des ensembles de données différents, démontrant l'efficacité du méthode et évaluant le développement et l'interprétabilité du modèle par des analyses supplémentaires.",
      "upvotes": 31,
      "discussionId": "67e0c1d7151ca9ed9284ded7",
      "githubRepo": "https://github.com/exoskeletonzj/MARS",
      "ai_keywords": [
        "Multi-Agent framework",
        "Socratic guidance",
        "multi-agent fusion technology",
        "Planner",
        "Teacher-Critic-Student Socratic dialogue pattern"
      ]
    },
    "publishedAt": "2025-03-21T02:19:55.000Z",
    "title": "MARS: A Multi-Agent Framework Incorporating Socratic Guidance for\n  Automated Prompt Optimization",
    "summary": "The basic question-answering format of large language models involves\ninputting a prompt and receiving a response, and the quality of the prompt\ndirectly impacts the effectiveness of the response. Automated Prompt\nOptimization (APO) aims to break free from the cognitive biases of manually\ndesigned prompts and explores a broader design space for prompts. However,\nexisting APO methods suffer from limited flexibility of fixed templates and\ninefficient search in prompt spaces as key issues. To this end, we propose a\nMulti-Agent framework Incorporating Socratic guidance (MARS), which utilizes\nmulti-agent fusion technology for automatic planning, with gradual continuous\noptimization and evaluation. Specifically, MARS comprises seven agents, each\nwith distinct functionalities, which autonomously use the Planner to devise an\noptimization path that ensures flexibility. Additionally, it employs a\nTeacher-Critic-Student Socratic dialogue pattern to iteratively optimize the\nprompts while conducting effective search. We conduct extensive experiments on\nvarious datasets to validate the effectiveness of our method, and perform\nadditional analytical experiments to assess the model's advancement as well as\nthe interpretability.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16874.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "658be7fe135580745c510323",
      "avatarUrl": "/avatars/830e5cec4565efdc23226a86a0fcef0e.svg",
      "fullname": "Jian Zhang",
      "name": "VentureZJ",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.16408",
      "authors": [
        {
          "_id": "67dcdedbeff29d0d52c739e4",
          "user": {
            "_id": "658a6c1399ed106ac8c822b1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658a6c1399ed106ac8c822b1/Wk2KXCcK39rUvXx6mpmGD.jpeg",
            "isPro": false,
            "fullname": "yiranqin",
            "user": "IranQin",
            "type": "user"
          },
          "name": "Yiran Qin",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-21T11:40:36.686Z",
          "hidden": false
        },
        {
          "_id": "67dcdedbeff29d0d52c739e5",
          "user": {
            "_id": "64eadcb03d76028d805a7818",
            "avatarUrl": "/avatars/528e4fded4419caf08589b2ed40437bc.svg",
            "isPro": false,
            "fullname": "Li Kang",
            "user": "FACEONG",
            "type": "user"
          },
          "name": "Li Kang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-21T11:40:38.834Z",
          "hidden": false
        },
        {
          "_id": "67dcdedbeff29d0d52c739e6",
          "name": "Xiufeng Song",
          "hidden": false
        },
        {
          "_id": "67dcdedbeff29d0d52c739e7",
          "name": "Zhenfei Yin",
          "hidden": false
        },
        {
          "_id": "67dcdedbeff29d0d52c739e8",
          "name": "Xiaohong Liu",
          "hidden": false
        },
        {
          "_id": "67dcdedbeff29d0d52c739e9",
          "name": "Xihui Liu",
          "hidden": false
        },
        {
          "_id": "67dcdedbeff29d0d52c739ea",
          "name": "Ruimao Zhang",
          "hidden": false
        },
        {
          "_id": "67dcdedbeff29d0d52c739eb",
          "name": "Lei Bai",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-20T17:58:38.000Z",
      "submittedOnDailyAt": "2025-03-24T01:35:09.139Z",
      "title": "Le Rôbofabrica : nous explorons la collaboration d'agents concrets avec des contraintes structurelles.",
      "submittedOnDailyBy": {
        "_id": "658a6c1399ed106ac8c822b1",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658a6c1399ed106ac8c822b1/Wk2KXCcK39rUvXx6mpmGD.jpeg",
        "isPro": false,
        "fullname": "yiranqin",
        "user": "IranQin",
        "type": "user"
      },
      "summary": "Le conception efficace de systèmes multi-agents matérialisés est cruciale pour aborder des tâches complexes dans la réalité. En raison de la complexité de ces systèmes, les méthodes actuelles ne peuvent générer automatiquement des données d'entraînement sûres et efficaces pour eux. Dans ce sens, nous proposons la conception de contraintes de configuration pour systèmes multi-agents matérialisés et nous cherchons à résoudre les problèmes par la collaboration entre agents. Nous désignons des formats de champs qui s'adaptent à différents types de contraintes, facilitant ainsi leur interaction facile avec le monde physique. En utilisant ces contraintes et, en particulier, les champs conçus, nous avons développé un cadre de collecte de données automatique pour systèmes multi-agents matérialisés et avons avancé avec \"RoboFactory\", le premier benchmark de fonctionnement d'un système multi-agent matérialisé. En se basant sur \"RoboFactory\", nous appliquons un méthode d'apprentissage qui simule la réalité et analysons son rendement dans des tâches de différents niveaux de difficulté. De plus, nous examinons l'architecture et les stratégies d'entraînement pour simuler l'apprentissage de multiples agents et nous concentrons sur la construction de systèmes multi-agents matérialisés sûrs et efficaces.",
      "upvotes": 27,
      "discussionId": "67dcdedeeff29d0d52c73abc",
      "projectPage": "https://iranqin.github.io/robofactory/",
      "ai_keywords": [
        "compositional constraints",
        "embodied multi-agent systems",
        "data collection framework",
        "benchmark",
        "RoboFactory",
        "imitation learning",
        "multi-agent imitation learning",
        "training strategies"
      ]
    },
    "publishedAt": "2025-03-20T13:58:38.000Z",
    "title": "RoboFactory: Exploring Embodied Agent Collaboration with Compositional\n  Constraints",
    "summary": "Designing effective embodied multi-agent systems is critical for solving\ncomplex real-world tasks across domains. Due to the complexity of multi-agent\nembodied systems, existing methods fail to automatically generate safe and\nefficient training data for such systems. To this end, we propose the concept\nof compositional constraints for embodied multi-agent systems, addressing the\nchallenges arising from collaboration among embodied agents. We design various\ninterfaces tailored to different types of constraints, enabling seamless\ninteraction with the physical world. Leveraging compositional constraints and\nspecifically designed interfaces, we develop an automated data collection\nframework for embodied multi-agent systems and introduce the first benchmark\nfor embodied multi-agent manipulation, RoboFactory. Based on RoboFactory\nbenchmark, we adapt and evaluate the method of imitation learning and analyzed\nits performance in different difficulty agent tasks. Furthermore, we explore\nthe architectures and training strategies for multi-agent imitation learning,\naiming to build safe and efficient embodied multi-agent systems.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16408.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "658a6c1399ed106ac8c822b1",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658a6c1399ed106ac8c822b1/Wk2KXCcK39rUvXx6mpmGD.jpeg",
      "fullname": "yiranqin",
      "name": "IranQin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.16430",
      "authors": [
        {
          "_id": "67e0bd81b04d9e836829c468",
          "user": {
            "_id": "63ea23b9dedfeebe54d02bdf",
            "avatarUrl": "/avatars/4d9f9a546aa8c63e277161ea700075c4.svg",
            "isPro": false,
            "fullname": "Yuqing Wang",
            "user": "Epiphqny",
            "type": "user"
          },
          "name": "Yuqing Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-24T08:10:42.267Z",
          "hidden": false
        },
        {
          "_id": "67e0bd81b04d9e836829c469",
          "name": "Zhijie Lin",
          "hidden": false
        },
        {
          "_id": "67e0bd81b04d9e836829c46a",
          "name": "Yao Teng",
          "hidden": false
        },
        {
          "_id": "67e0bd81b04d9e836829c46b",
          "name": "Yuanzhi Zhu",
          "hidden": false
        },
        {
          "_id": "67e0bd81b04d9e836829c46c",
          "user": {
            "_id": "60d2e681b8448e1785bbda06",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1624434302056-noauth.jpeg",
            "isPro": false,
            "fullname": "Shuhuai Ren",
            "user": "ShuhuaiRen",
            "type": "user"
          },
          "name": "Shuhuai Ren",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-24T08:10:44.045Z",
          "hidden": false
        },
        {
          "_id": "67e0bd81b04d9e836829c46d",
          "name": "Jiashi Feng",
          "hidden": false
        },
        {
          "_id": "67e0bd81b04d9e836829c46e",
          "name": "Xihui Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-20T17:59:59.000Z",
      "submittedOnDailyAt": "2025-03-24T00:50:05.627Z",
      "title": "Génération Automatique Séquentielle Visuelle Intégrant des Valeurs de Tokens Continus et Discrètes",
      "submittedOnDailyBy": {
        "_id": "63ea23b9dedfeebe54d02bdf",
        "avatarUrl": "/avatars/4d9f9a546aa8c63e277161ea700075c4.svg",
        "isPro": false,
        "fullname": "Yuqing Wang",
        "user": "Epiphqny",
        "type": "user"
      },
      "summary": "Les modèles de génération d'images pour auto-régression généralement utilisent un tokenisateur pour compresser les images en tokens et prédirer les tokens dans l'ordre. Les représentations de tokens ont des problèmes logiques fondamentaux : les tokens discrets peuvent être facilement modélisés avec la perte d'entropie croisée standard, mais subissent une perte d'information et une instabilité dans l'entraînement du tokenisateur ; les tokens continus peuvent conserver mieux les détails visuels, mais nécessitent des modèles de distribution complexes et compliquent la génération. Dans cet article, nous proposons TokenBridge pour surmonter ces différences tout en maintenant la forte capacité de représentation des tokens continus et la simplicité du modèle des tokens discrets. Pour y parvenir, nous séparons la binairisation dans le processus d'entraînement du tokenisateur et nous utilisons une réduction postérieure pour obtenir directement les tokens discrets à partir de représentations continues. En particulier, nous introduisons une stratégie de réduction de dimensions et nous combinons une structure de prédiction automatiquement régressive légère pour modéliser efficacement les résultats de la binairisation de chaque dimension. Les expérimentations rigoureuses montrent que, avec des prédictions de classification standard, TokenBridge atteint la même qualité de reconstruction et de génération que les méthodes continus. Cette recherche montre comment le passage du discret au continu permet effectivement d'exploiter les forces de chaque approche et offre la possibilité de générer des images de haute qualité en utilisant un modèle de auto-régression simple. Page du projet : https://yuqingwang1029.github.io/TokenBridge.",
      "upvotes": 19,
      "discussionId": "67e0bd85b04d9e836829c55f",
      "projectPage": "https://yuqingwang1029.github.io/TokenBridge/",
      "githubRepo": "https://github.com/YuqingWang1029/TokenBridge",
      "ai_keywords": [
        "autoregressive visual generation models",
        "tokenizers",
        "tokens",
        "discrete tokens",
        "continuous tokens",
        "cross-entropy loss",
        "tokenizer training",
        "TokenBridge",
        "post-training quantization",
        "dimension-wise quantization",
        "lightweight autoregressive prediction mechanism",
        "reconstruction quality",
        "generation quality"
      ]
    },
    "publishedAt": "2025-03-20T13:59:59.000Z",
    "title": "Bridging Continuous and Discrete Tokens for Autoregressive Visual\n  Generation",
    "summary": "Autoregressive visual generation models typically rely on tokenizers to\ncompress images into tokens that can be predicted sequentially. A fundamental\ndilemma exists in token representation: discrete tokens enable straightforward\nmodeling with standard cross-entropy loss, but suffer from information loss and\ntokenizer training instability; continuous tokens better preserve visual\ndetails, but require complex distribution modeling, complicating the generation\npipeline. In this paper, we propose TokenBridge, which bridges this gap by\nmaintaining the strong representation capacity of continuous tokens while\npreserving the modeling simplicity of discrete tokens. To achieve this, we\ndecouple discretization from the tokenizer training process through\npost-training quantization that directly obtains discrete tokens from\ncontinuous representations. Specifically, we introduce a dimension-wise\nquantization strategy that independently discretizes each feature dimension,\npaired with a lightweight autoregressive prediction mechanism that efficiently\nmodel the resulting large token space. Extensive experiments show that our\napproach achieves reconstruction and generation quality on par with continuous\nmethods while using standard categorical prediction. This work demonstrates\nthat bridging discrete and continuous paradigms can effectively harness the\nstrengths of both approaches, providing a promising direction for high-quality\nvisual generation with simple autoregressive modeling. Project page:\nhttps://yuqingwang1029.github.io/TokenBridge.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16430.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "63ea23b9dedfeebe54d02bdf",
      "avatarUrl": "/avatars/4d9f9a546aa8c63e277161ea700075c4.svg",
      "fullname": "Yuqing Wang",
      "name": "Epiphqny",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.16660",
      "authors": [
        {
          "_id": "67e0ffb029682c8065e1c223",
          "name": "Eduard Allakhverdov",
          "hidden": false
        },
        {
          "_id": "67e0ffb029682c8065e1c224",
          "name": "Elizaveta Goncharova",
          "hidden": false
        },
        {
          "_id": "67e0ffb029682c8065e1c225",
          "name": "Andrey Kuznetsov",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-20T19:17:08.000Z",
      "submittedOnDailyAt": "2025-03-24T05:20:19.676Z",
      "title": "Il n'y a pas une telle explication.",
      "submittedOnDailyBy": {
        "_id": "6310ff34bc152fa3e810c186",
        "avatarUrl": "/avatars/bfd63bcd81548283f5e496e3693bf143.svg",
        "isPro": false,
        "fullname": "Elizaveta Goncharova",
        "user": "Elizaveta",
        "type": "user"
      },
      "summary": "El Vision Encoder généralement génère beaucoup de tokens de vision et fournit des représentations riches d'information, mais augmente considérablement la charge de calcul. Cela soulève la question de savoir si tous les tokens générés sont égaux ou si certains peuvent être supprimés pour réduire les coûts de calcul. Dans cet article, nous basons notre travail sur l'hypothèse selon laquelle les caractéristiques de faible valeur peuvent être reconstruites à partir de celles de haute valeur, et nous présentons un nouveau méthode pour déterminer l'utilité des caractéristiques. Pour cela, nous intéguons une fonction de sélection GaNBlur-Sofmax dans un autoencodeur automatique, permettant de laisser seulement les tokens de vision contenant la plus grande quantité d'information. Nous avons comparé le rendement du modèle LLaVA-NeXT lorsque les caractéristiques sont sélectionnées en fonction de leur valeur et lorsqu'elles sont supprimées de manière aléatoire. Nous avons confirmé que il est possible de supprimer au moins 50% du contexte de vision dans des tâches basées sur l'OCR, mais que le rendement du modèle est souvent significativement affecté lorsque des caractéristiques sont supprimées de manière aléatoire. De plus, même en laissant seulement 30% des tokens de manière aléatoire dans des tâches générales, nous atteignons un rendement comparable à celui obtenu en utilisant tous les tokens de vision. Ces résultats offrent une direction prometteuse pour l'implémentation d'impressions polymorphes adaptatifs et efficaces, permettant une inférence scalable et de faible charge de calcul sans perte de rendement.",
      "upvotes": 17,
      "discussionId": "67e0ffb229682c8065e1c2c6",
      "ai_keywords": [
        "autoencoder",
        "Gumbel-Softmax selection mechanism",
        "feature utility",
        "LLaVA-NeXT model",
        "OCR-based tasks",
        "visual context",
        "performance loss",
        "general-domain tasks",
        "multimodal pruning"
      ]
    },
    "publishedAt": "2025-03-20T15:17:08.000Z",
    "title": "When Less is Enough: Adaptive Token Reduction for Efficient Image\n  Representation",
    "summary": "Vision encoders typically generate a large number of visual tokens, providing\ninformation-rich representations but significantly increasing computational\ndemands. This raises the question of whether all generated tokens are equally\nvaluable or if some of them can be discarded to reduce computational costs\nwithout compromising quality. In this paper, we introduce a new method for\ndetermining feature utility based on the idea that less valuable features can\nbe reconstructed from more valuable ones. We implement this concept by\nintegrating an autoencoder with a Gumbel-Softmax selection mechanism, that\nallows identifying and retaining only the most informative visual tokens. To\nvalidate our approach, we compared the performance of the LLaVA-NeXT model,\nusing features selected by our method with randomly selected features. We found\nthat on OCR-based tasks, more than 50% of the visual context can be removed\nwith minimal performance loss, whereas randomly discarding the same proportion\nof features significantly affects the model capabilities. Furthermore, in\ngeneral-domain tasks, even randomly retaining only 30% of tokens achieves\nperformance comparable to using the full set of visual tokens. Our results\nhighlight a promising direction towards adaptive and efficient multimodal\npruning that facilitates scalable and low-overhead inference without\ncompromising performance.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16660.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6310ff34bc152fa3e810c186",
      "avatarUrl": "/avatars/bfd63bcd81548283f5e496e3693bf143.svg",
      "fullname": "Elizaveta Goncharova",
      "name": "Elizaveta",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.17352",
      "authors": [
        {
          "_id": "67e0bcc9e5fa0da84e121032",
          "name": "Yihe Deng",
          "hidden": false
        },
        {
          "_id": "67e0bcc9e5fa0da84e121033",
          "name": "Hritik Bansal",
          "hidden": false
        },
        {
          "_id": "67e0bcc9e5fa0da84e121034",
          "name": "Fan Yin",
          "hidden": false
        },
        {
          "_id": "67e0bcc9e5fa0da84e121035",
          "name": "Nanyun Peng",
          "hidden": false
        },
        {
          "_id": "67e0bcc9e5fa0da84e121036",
          "name": "Wei Wang",
          "hidden": false
        },
        {
          "_id": "67e0bcc9e5fa0da84e121037",
          "name": "Kai-Wei Chang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-21T17:52:43.000Z",
      "submittedOnDailyAt": "2025-03-24T00:31:06.884Z",
      "title": "OpenVLThinker : Début de l'Exploration de la Théorie du Langage Visuo-Linguistique Complexe\n\nAlors, grâce à l'amélioration automatique itérative",
      "submittedOnDailyBy": {
        "_id": "642f4c789b2484d7d8551a93",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642f4c789b2484d7d8551a93/0lH4YXcbZa-Xlzj6ESo7F.jpeg",
        "isPro": true,
        "fullname": "Yihe Deng",
        "user": "ydeng9",
        "type": "user"
      },
      "summary": "Le développement récent de DeepSeek-R1 a montré que les grands modèles de langue (LLMs) peuvent implémenter des habiletés complexes en logique, notamment des capacités comme l'auto-démonstration et l'auto-correction, qui sont complexes et pouvantes à réaliser grâce à un apprentissage par renforcement (RL) qui assure une récompense. De plus, il a montré que ces modèles peuvent améliorer significativement leur performance dans des tâches difficiles comme AIME. Ces résultats ont permis à cette étude d'investiguer si ces habiletés en logique peuvent être intégrées avec succès dans les grands modèles de vision et de langue (LVLMs) et d'évaluer leur impact. Cette recherche considère l'utilisation d'apprentissage supervisé avec des données légères (SFT) et d'apprentissage par renforcement (RL) itératif. Tout d'abord, les habiletés en logique sont documentées en utilisant les étapes logiques de haute qualité d'images générées dans différents ensembles de données de vision, transformant les modèles purement textuels (R1) en documents. Ensuite, l'apprentissage par renforcement itératif améliore les habiletés logiques en générant des ensembles de données d'entraînement par SFT pour chaque modèle RL amélioré à chaque itération. Ce processus itératif a permis d'obtenir OpenVLThinker, un LVLM qui améliore de manière continue son rendement logique dans des référentiels difficiles comme MathVista, MathVerse et MathVision, démontrant la possibilité d'une forte logique et de langage visuel dans notre stratégie. Les codes, modèles et données sont disponibles sur https://github.com/yihedeng9/OpenVLThinker.",
      "upvotes": 11,
      "discussionId": "67e0bccae5fa0da84e121079",
      "projectPage": "https://yihe-deng.notion.site/openvlthinker",
      "githubRepo": "https://github.com/yihedeng9/OpenVLThinker",
      "ai_keywords": [
        "Reinforcement Learning (RL)",
        "verifiable rewards",
        "large language models (LLMs)",
        "self-verification",
        "self-correction",
        "large vision-language models (LVLMs)",
        "multimodal reasoning tasks",
        "supervised fine-tuning (SFT)",
        "lightweight training data",
        "reasoning steps",
        "high-quality captions",
        "diversity",
        "visual datasets",
        "iterative process",
        "OpenVLThinker",
        "reasoning performance",
        "challenging benchmarks",
        "MathVista",
        "MathVerse",
        "MathVision",
        "robust vision-language reasoning"
      ]
    },
    "publishedAt": "2025-03-21T13:52:43.000Z",
    "title": "OpenVLThinker: An Early Exploration to Complex Vision-Language Reasoning\n  via Iterative Self-Improvement",
    "summary": "Recent advancements demonstrated by DeepSeek-R1 have shown that complex\nreasoning abilities in large language models (LLMs), including sophisticated\nbehaviors such as self-verification and self-correction, can be achieved by RL\nwith verifiable rewards and significantly improves model performance on\nchallenging tasks such as AIME. Motivated by these findings, our study\ninvestigates whether similar reasoning capabilities can be successfully\nintegrated into large vision-language models (LVLMs) and assesses their impact\non challenging multimodal reasoning tasks. We consider an approach that\niteratively leverages supervised fine-tuning (SFT) on lightweight training data\nand Reinforcement Learning (RL) to further improve model generalization.\nInitially, reasoning capabilities were distilled from pure-text R1 models by\ngenerating reasoning steps using high-quality captions of the images sourced\nfrom diverse visual datasets. Subsequently, iterative RL training further\nenhance reasoning skills, with each iteration's RL-improved model generating\nrefined SFT datasets for the next round. This iterative process yielded\nOpenVLThinker, a LVLM exhibiting consistently improved reasoning performance on\nchallenging benchmarks such as MathVista, MathVerse, and MathVision,\ndemonstrating the potential of our strategy for robust vision-language\nreasoning. The code, model and data are held at\nhttps://github.com/yihedeng9/OpenVLThinker.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17352.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642f4c789b2484d7d8551a93",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642f4c789b2484d7d8551a93/0lH4YXcbZa-Xlzj6ESo7F.jpeg",
      "fullname": "Yihe Deng",
      "name": "ydeng9",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.17126",
      "authors": [
        {
          "_id": "67e0beb474fc794321fb4ad7",
          "name": "John Joon Young Chung",
          "hidden": false
        },
        {
          "_id": "67e0beb474fc794321fb4ad8",
          "name": "Vishakh Padmakumar",
          "hidden": false
        },
        {
          "_id": "67e0beb474fc794321fb4ad9",
          "name": "Melissa Roemmele",
          "hidden": false
        },
        {
          "_id": "67e0beb474fc794321fb4ada",
          "name": "Yuqian Sun",
          "hidden": false
        },
        {
          "_id": "67e0beb474fc794321fb4adb",
          "name": "Max Kreminski",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-21T13:21:45.000Z",
      "submittedOnDailyAt": "2025-03-24T00:39:24.717Z",
      "title": "Ajustes pour diverses entrées créatives après l'entraînement de modèles de langage à grande échelle",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "L'écran de la malédiction de la ombre est un défi qui ne dispose pas d'une réponse précise unique, par conséquent les grands modèles de langue (LLMs) entraînés doivent générer diverses sorties valides. Cependant, le traitement postérieur des modèles LLMs se concentre principalement sur l'amélioration de la qualité de la génération sans encourager la diversité des résultats. Par conséquent, des méthodes de formation postérieures ont été examinées qui promeuvent à la fois la diversité et la qualité des sorties dans la génération de l'écran de la malédiction de la ombre. Notre idée clé est d'inclure la différence (déviation) par rapport à tous les exemples qui ont le même prompt dans l'objectif de formation, ce qui encourage l'apprentissage d'instances de haute qualité rares. Notre approche a été appliquée à l'optimisation directe préférentielle (DPO) et à l'optimisation préférentielle de raisonnements de probabilité (ORPO), démontrant que cela augmente la diversité des résultats du modèle entraîné tout en minimisant la qualité. Notre meilleur modèle (avec 8B paramètres) a atteint la diversité des données créées par les humains, tout en maintenant la qualité de la sortie à la hauteur des meilleurs modèles de direction (comme GPT-4o et DeepSeek-R1). De plus, notre approche a été validée par des évaluations humaines, des tests d'élimination et des comparaisons avec d'autres méthodes de diversité (comme DivPO).",
      "upvotes": 9,
      "discussionId": "67e0beb574fc794321fb4b04",
      "ai_keywords": [
        "direct preference optimization (DPO)",
        "odds ratio preference optimization (ORPO)",
        "parameter-efficient fine-tuning"
      ]
    },
    "publishedAt": "2025-03-21T09:21:45.000Z",
    "title": "Modifying Large Language Model Post-Training for Diverse Creative\n  Writing",
    "summary": "As creative writing tasks do not have singular correct answers, large\nlanguage models (LLMs) trained to perform these tasks should be able to\ngenerate diverse valid outputs. However, LLM post-training often focuses on\nimproving generation quality but neglects to facilitate output diversity.\nHence, in creative writing generation, we investigate post-training approaches\nto promote both output diversity and quality. Our core idea is to include\ndeviation -- the degree of difference between a training sample and all other\nsamples with the same prompt -- in the training objective to facilitate\nlearning from rare high-quality instances. By adopting our approach to direct\npreference optimization (DPO) and odds ratio preference optimization (ORPO), we\ndemonstrate that we can promote the output diversity of trained models while\nminimally decreasing quality. Our best model with 8B parameters could achieve\non-par diversity as a human-created dataset while having output quality similar\nto the best instruction-tuned models we examined, GPT-4o and DeepSeek-R1. We\nfurther validate our approaches with a human evaluation, an ablation, and a\ncomparison to an existing diversification approach, DivPO.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17126.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6443
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.16867",
      "authors": [
        {
          "_id": "67e0d31f151ca9ed92898fff",
          "user": {
            "_id": "63bbf071d8d676a2299c7d0b",
            "avatarUrl": "/avatars/4bb1c86ef8651c75b9761afee2865267.svg",
            "isPro": false,
            "fullname": "Guan",
            "user": "Guan123",
            "type": "user"
          },
          "name": "Kaisi Guan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-24T08:06:57.786Z",
          "hidden": false
        },
        {
          "_id": "67e0d31f151ca9ed92899000",
          "name": "Zhengfeng Lai",
          "hidden": false
        },
        {
          "_id": "67e0d31f151ca9ed92899001",
          "name": "Yuchong Sun",
          "hidden": false
        },
        {
          "_id": "67e0d31f151ca9ed92899002",
          "name": "Peng Zhang",
          "hidden": false
        },
        {
          "_id": "67e0d31f151ca9ed92899003",
          "name": "Wei Liu",
          "hidden": false
        },
        {
          "_id": "67e0d31f151ca9ed92899004",
          "name": "Kieran Liu",
          "hidden": false
        },
        {
          "_id": "67e0d31f151ca9ed92899005",
          "name": "Meng Cao",
          "hidden": false
        },
        {
          "_id": "67e0d31f151ca9ed92899006",
          "name": "Ruihua Song",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-21T05:52:50.000Z",
      "submittedOnDailyAt": "2025-03-24T07:22:50.935Z",
      "title": "ETVA : Génération et Résolution de Problèmes Microscopiques à Travers l'Évaluation d'Éléments d'Array en Vidéo à partir de Texte",
      "submittedOnDailyBy": {
        "_id": "63bbf071d8d676a2299c7d0b",
        "avatarUrl": "/avatars/4bb1c86ef8651c75b9761afee2865267.svg",
        "isPro": false,
        "fullname": "Guan",
        "user": "Guan123",
        "type": "user"
      },
      "summary": "Évaluer la relation de correspondance significative entre le texte et le vidéo généré est un problème complexe dans la génération de vidéo à partir du texte (T2V). La métrique existante pour évaluer la correspondance entre le texte et le vidéo, CLIPScore, n'a pas une correspondance détaillée et génère des scores simplistes qui ne reflètent pas bien les préférences humaines. Pour aborder cette limitation, nous proposons ETVA (Nouveau Méthode pour Évaluer la Correspondance entre Texte et Vidéo). ETVA est un méthode qui évalue la correspondance par la génération de questions détaillées et de leurs réponses. Tout d'abord, un système multivariable interprète le texte comme un graphe d'échelles significatives et génère des questions sur les atomes. Ensuite, nous concevons un cadre d'inférence multiniveau avec extension de connaissances, et un LLM collaboratif recherche des connaissances sur les lois de la physique, tandis qu'un LLM de vidéo répond aux questions générées par un mécanisme d'inférence multiniveau. Dans des expériences étendues, ETVA a atteint un coefficient de corrélation de 58,47 en corrélation de Spearman, montrant une corrélation plus élevée avec la perception humaine et dépassant les coefficients de corrélation de 31,0 des métriques existantes. De plus, ETVA identifie les principales capacités et limites de 15 modèles de texte à vidéo existants et ouvre des voies pour la génération de T2V dans les futurs générations.",
      "upvotes": 5,
      "discussionId": "67e0d322151ca9ed928990c0",
      "projectPage": "https://eftv-eval.github.io/etva-eval/",
      "ai_keywords": [
        "semantic alignment",
        "Text-to-Video (T2V) Generation",
        "CLIPScore",
        "fine-grained alignment details",
        "multi-agent system",
        "semantic scene graphs",
        "atomic questions",
        "knowledge-augmented",
        "multi-stage reasoning framework",
        "auxiliary LLM",
        "common-sense knowledge",
        "video LLM",
        "multi-stage reasoning mechanism",
        "Spearman's correlation coefficient",
        "benchmark",
        "text-to-video models"
      ]
    },
    "publishedAt": "2025-03-21T01:52:50.000Z",
    "title": "ETVA: Evaluation of Text-to-Video Alignment via Fine-grained Question\n  Generation and Answering",
    "summary": "Precisely evaluating semantic alignment between text prompts and generated\nvideos remains a challenge in Text-to-Video (T2V) Generation. Existing\ntext-to-video alignment metrics like CLIPScore only generate coarse-grained\nscores without fine-grained alignment details, failing to align with human\npreference. To address this limitation, we propose ETVA, a novel Evaluation\nmethod of Text-to-Video Alignment via fine-grained question generation and\nanswering. First, a multi-agent system parses prompts into semantic scene\ngraphs to generate atomic questions. Then we design a knowledge-augmented\nmulti-stage reasoning framework for question answering, where an auxiliary LLM\nfirst retrieves relevant common-sense knowledge (e.g., physical laws), and then\nvideo LLM answers the generated questions through a multi-stage reasoning\nmechanism. Extensive experiments demonstrate that ETVA achieves a Spearman's\ncorrelation coefficient of 58.47, showing a much higher correlation with human\njudgment than existing metrics which attain only 31.0. We also construct a\ncomprehensive benchmark specifically designed for text-to-video alignment\nevaluation, featuring 2k diverse prompts and 12k atomic questions spanning 10\ncategories. Through a systematic evaluation of 15 existing text-to-video\nmodels, we identify their key capabilities and limitations, paving the way for\nnext-generation T2V generation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16867.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63bbf071d8d676a2299c7d0b",
      "avatarUrl": "/avatars/4bb1c86ef8651c75b9761afee2865267.svg",
      "fullname": "Guan",
      "name": "Guan123",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.16549",
      "authors": [
        {
          "_id": "67e0d11eb04d9e83682f222a",
          "name": "Felix Chen",
          "hidden": false
        },
        {
          "_id": "67e0d11eb04d9e83682f222b",
          "user": {
            "_id": "649d54b314afbb10ce2a9eeb",
            "avatarUrl": "/avatars/15c325d8c2273ff63569f23015e98486.svg",
            "isPro": false,
            "fullname": "Hangjie Yuan",
            "user": "JacobYuan",
            "type": "user"
          },
          "name": "Hangjie Yuan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-24T08:06:59.943Z",
          "hidden": false
        },
        {
          "_id": "67e0d11eb04d9e83682f222c",
          "name": "Yunqiu Xu",
          "hidden": false
        },
        {
          "_id": "67e0d11eb04d9e83682f222d",
          "name": "Tao Feng",
          "hidden": false
        },
        {
          "_id": "67e0d11eb04d9e83682f222e",
          "name": "Jun Cen",
          "hidden": false
        },
        {
          "_id": "67e0d11eb04d9e83682f222f",
          "name": "Pengwei Liu",
          "hidden": false
        },
        {
          "_id": "67e0d11eb04d9e83682f2230",
          "name": "Zeying Huang",
          "hidden": false
        },
        {
          "_id": "67e0d11eb04d9e83682f2231",
          "name": "Yi Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-19T11:46:19.000Z",
      "submittedOnDailyAt": "2025-03-24T01:59:23.638Z",
      "title": "MathFlow : Augmente le flux de reconnaissance de l'MLLM pour les problèmes de mathématiques visuelles.",
      "submittedOnDailyBy": {
        "_id": "649d54b314afbb10ce2a9eeb",
        "avatarUrl": "/avatars/15c325d8c2273ff63569f23015e98486.svg",
        "isPro": false,
        "fullname": "Hangjie Yuan",
        "user": "JacobYuan",
        "type": "user"
      },
      "summary": "Les modèles de langage multimodal de DamoDal (MLLMs) ont démontré des résultats impressionnants dans diverses tâches, mais leur potentiel dans la résolution de problèmes de mathématiques visuelles n'a pas encore été complètement révélé, notamment en ce qui concerne la perception et l'interprétation précise des images. On suppose que la capacité de reconnaissance visuelle, plus proche des processus humains, est cruciale pour extraire de l'information significative des images. Pour vérifier cette hypothèse, un cadre d'évaluation détaillé appelé FlowVerse a été développé. Ce cadre classe toute l'information utilisée pour résoudre les problèmes en quatre composants et les combine en six versions de problèmes pour évaluer. Les résultats initiaux de FlowVerse montrent que les MLLMs actuels sont très limités dans l'extraction d'information importante des images et dans l'exécution d'inférences complexes basées sur celles-ci. Pour améliorer cet aspect, un flux de résolution modulaire nommé MathFlow a été introduit. Ce flux divise la perception visuelle et l'inférence en différentes étapes et optimise chacune de ces étapes de manière indépendante. Un modèle de reconnaissance spécialisé nommé MathFlow-P-7B a été entraîné pour faire face aux limites de la perception visuelle des MLLMs actuels. Les résultats des expériences montrent une amélioration significative du rendement lorsque MathFlow-P-7B est combiné avec des modèles d'inférence fermés ou ouverts. Cela démontre l'efficacité du flux de MathFlow et sa compatibilité avec différents cadres d'inférence. Le cadre d'évaluation FlowVerse et son code sont disponibles sur https://github.com/MathFlow-zju/MathFlow.",
      "upvotes": 4,
      "discussionId": "67e0d11fb04d9e83682f2267",
      "githubRepo": "https://github.com/MathFlow-zju/MathFlow",
      "ai_keywords": [
        "Multimodal Large Language Models (MLLMs)",
        "visual mathematical problem-solving",
        "diagrams",
        "perception capabilities",
        "inference processes",
        "FlowVerse",
        "problem-solving",
        "essential information",
        "reasoned property",
        "MathFlow",
        "problem-solving pipeline",
        "perception model",
        "MathFlow-P-7B",
        "closed-source inference models",
        "open-source inference models"
      ]
    },
    "publishedAt": "2025-03-19T07:46:19.000Z",
    "title": "MathFlow: Enhancing the Perceptual Flow of MLLMs for Visual Mathematical\n  Problems",
    "summary": "Despite impressive performance across diverse tasks, Multimodal Large\nLanguage Models (MLLMs) have yet to fully demonstrate their potential in visual\nmathematical problem-solving, particularly in accurately perceiving and\ninterpreting diagrams. Inspired by typical processes of humans, we hypothesize\nthat the perception capabilities to extract meaningful information from\ndiagrams is crucial, as it directly impacts subsequent inference processes. To\nvalidate this hypothesis, we developed FlowVerse, a comprehensive benchmark\nthat categorizes all information used during problem-solving into four\ncomponents, which are then combined into six problem versions for evaluation.\nOur preliminary results on FlowVerse reveal that existing MLLMs exhibit\nsubstantial limitations when extracting essential information and reasoned\nproperty from diagrams and performing complex reasoning based on these visual\ninputs. In response, we introduce MathFlow, a modular problem-solving pipeline\nthat decouples perception and inference into distinct stages, thereby\noptimizing each independently. Given the perceptual limitations observed in\ncurrent MLLMs, we trained MathFlow-P-7B as a dedicated perception model.\nExperimental results indicate that MathFlow-P-7B yields substantial performance\ngains when integrated with various closed-source and open-source inference\nmodels. This demonstrates the effectiveness of the MathFlow pipeline and its\ncompatibility to diverse inference frameworks. The FlowVerse benchmark and code\nare available at https://github.com/MathFlow-zju/MathFlow.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16549.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "649d54b314afbb10ce2a9eeb",
      "avatarUrl": "/avatars/15c325d8c2273ff63569f23015e98486.svg",
      "fullname": "Hangjie Yuan",
      "name": "JacobYuan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.16983",
      "authors": [
        {
          "_id": "67e0c303ff27a08e3896134a",
          "name": "Xu Zhang",
          "hidden": false
        },
        {
          "_id": "67e0c303ff27a08e3896134b",
          "name": "Hao Zhou",
          "hidden": false
        },
        {
          "_id": "67e0c303ff27a08e3896134c",
          "name": "Haoming Qin",
          "hidden": false
        },
        {
          "_id": "67e0c303ff27a08e3896134d",
          "name": "Xiaobin Lu",
          "hidden": false
        },
        {
          "_id": "67e0c303ff27a08e3896134e",
          "name": "Jiaxing Yan",
          "hidden": false
        },
        {
          "_id": "67e0c303ff27a08e3896134f",
          "name": "Guanzhong Wang",
          "hidden": false
        },
        {
          "_id": "67e0c303ff27a08e38961350",
          "name": "Zeyu Chen",
          "hidden": false
        },
        {
          "_id": "67e0c303ff27a08e38961351",
          "name": "Yi Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-21T09:48:00.000Z",
      "submittedOnDailyAt": "2025-03-24T00:58:01.102Z",
      "title": "Fonction pour contrôler largement la fonctionnalité du modèle de diffusion vidéo",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Le développement de la génération de marques de texte a conduit à que l'étude de la génération de vidéos soit un défi, car il est nécessaire une précision et une flexibilité dans la représentation spatiale-temporelle. Pour aborder ces limites, on présente un nouveau cadre de travail appelé VCtrl (également connu sous le nom de PP-VCtrl). Ce cadre fournit des orientations cohérentes et permet un contrôle précis sur des modèles de diffusion de vidéo pré-entraînés. VCtrl évite que les signaux de contrôle personnalisés soient modifiés comme les bords de Canny, les masques de segmentation ou les points de keypoints d'HMAN, et utilise des modules de condition généralisés pour configurer de manière cohérente des modèles de diffusion de vidéo pré-entraînés. De plus, il conçoit un système d'encodage de signaux de contrôle uniformes et une structure de connexions résiduelles épars pour configurer des représentations de contrôle de manière efficace. A travers des expériences détaillées et des évaluations humaines, VCtrl améliore la possibilité de contrôle et la qualité de la génération. Les sources de code et les modèles pré-entraînés sont disponibles publiquement en utilisant le framework PaddlePaddle et peuvent être accédés à travers le lien suivant : http://github.com/PaddlePaddle/PaddleMIX/tree/develop/ppdiffusers/examples/ppvctrl.",
      "upvotes": 3,
      "discussionId": "67e0c306ff27a08e38961428",
      "ai_keywords": [
        "VCtrl",
        "PP-VCtrl",
        "fine-grained control",
        "pre-trained video diffusion models",
        "conditional module",
        "Canny edges",
        "segmentation masks",
        "human keypoints",
        "unified control signal encoding pipeline",
        "sparse residual connection mechanism",
        "controllability",
        "PaddlePaddle",
        "PaddleMIX",
        "ppdiffusers"
      ]
    },
    "publishedAt": "2025-03-21T05:48:00.000Z",
    "title": "Enabling Versatile Controls for Video Diffusion Models",
    "summary": "Despite substantial progress in text-to-video generation, achieving precise\nand flexible control over fine-grained spatiotemporal attributes remains a\nsignificant unresolved challenge in video generation research. To address these\nlimitations, we introduce VCtrl (also termed PP-VCtrl), a novel framework\ndesigned to enable fine-grained control over pre-trained video diffusion models\nin a unified manner. VCtrl integrates diverse user-specified control\nsignals-such as Canny edges, segmentation masks, and human keypoints-into\npretrained video diffusion models via a generalizable conditional module\ncapable of uniformly encoding multiple types of auxiliary signals without\nmodifying the underlying generator. Additionally, we design a unified control\nsignal encoding pipeline and a sparse residual connection mechanism to\nefficiently incorporate control representations. Comprehensive experiments and\nhuman evaluations demonstrate that VCtrl effectively enhances controllability\nand generation quality. The source code and pre-trained models are publicly\navailable and implemented using the PaddlePaddle framework at\nhttp://github.com/PaddlePaddle/PaddleMIX/tree/develop/ppdiffusers/examples/ppvctrl.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16983.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6443
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.16921",
      "authors": [
        {
          "_id": "67e0bb1665e294ad989334ea",
          "name": "Lingfan Zhang",
          "hidden": false
        },
        {
          "_id": "67e0bb1665e294ad989334eb",
          "name": "Chen Liu",
          "hidden": false
        },
        {
          "_id": "67e0bb1665e294ad989334ec",
          "name": "Chengming Xu",
          "hidden": false
        },
        {
          "_id": "67e0bb1665e294ad989334ed",
          "name": "Kai Hu",
          "hidden": false
        },
        {
          "_id": "67e0bb1665e294ad989334ee",
          "name": "Donghao Luo",
          "hidden": false
        },
        {
          "_id": "67e0bb1665e294ad989334ef",
          "name": "Chengjie Wang",
          "hidden": false
        },
        {
          "_id": "67e0bb1665e294ad989334f0",
          "name": "Yanwei Fu",
          "hidden": false
        },
        {
          "_id": "67e0bb1665e294ad989334f1",
          "name": "Yuan Yao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-21T07:33:44.000Z",
      "submittedOnDailyAt": "2025-03-24T00:24:41.729Z",
      "title": "Adaptation du DPO pour le reconnaissance des enfants : moment d'ajustement du modèle lorsque la politique s'écarte",
      "submittedOnDailyBy": {
        "_id": "652fab9d04a34a9282bf29d6",
        "avatarUrl": "/avatars/cd5967b37ebb1225e9ae1d46f196e2e2.svg",
        "isPro": false,
        "fullname": "Chengming Xu",
        "user": "ChengmingX",
        "type": "user"
      },
      "summary": "Récemment, on a observé un développement notable dans le domaine de la génération d'images, en particulier dans les méthodes pour ajuster les modèles aux préférences générales de l'humanité. Dans cet article, nous explorons le rôle important des données de préférence dans le processus de génération d'images, et nous expliquons l'importance de ces données dans le contexte de la diffusion-DPO et ses évolutions ultérieures. Nous analysons la complexité des préférences générales de l'humanité dans la génération d'images, ainsi que les problèmes découlant des caractéristiques subjectives de ces préférences et de l'existence de multiples échantillons dans les ensembles de données de préférence. À travers des expériences pilotes, nous montrons l'influence négative que peuvent avoir les multiples échantillons sur le rendement du modèle. Nous proposons un nouvel approche appelé Adaptive-DPO, qui intègre des métriques intéressantes de multiples échantillons comme fonction objectif de DPO. Ces métriques permettent de distinguer entre multiples échantillons et multiples échantillons, y compris la confiance interne et la stabilité externe. Nous présentons la fonction de perte d'Adaptive-DPO et expliquons comment cette fonction améliore la perte de DPO de deux manières : renforçant l'apprentissage de multiples étiquettes et atténuant l'impact négatif des multiples échantillons. Nos expériences montrent comment Adaptive-DPO est efficace dans le traitement de données synthétiques et de préférences réelles, ouvrant des voies pour le développement de méthodes d'entraînement plus efficaces pour des tâches de génération d'images.",
      "upvotes": 3,
      "discussionId": "67e0bb1a65e294ad9893361c",
      "ai_keywords": [
        "diffusion models",
        "Diffusion-DPO",
        "adaptive-DPO",
        "intra-annotator confidence",
        "inter-annotator stability",
        "DPO objective",
        "Adaptive-DPO loss function"
      ]
    },
    "publishedAt": "2025-03-21T03:33:44.000Z",
    "title": "When Preferences Diverge: Aligning Diffusion Models with Minority-Aware\n  Adaptive DPO",
    "summary": "In recent years, the field of image generation has witnessed significant\nadvancements, particularly in fine-tuning methods that align models with\nuniversal human preferences. This paper explores the critical role of\npreference data in the training process of diffusion models, particularly in\nthe context of Diffusion-DPO and its subsequent adaptations. We investigate the\ncomplexities surrounding universal human preferences in image generation,\nhighlighting the subjective nature of these preferences and the challenges\nposed by minority samples in preference datasets. Through pilot experiments, we\ndemonstrate the existence of minority samples and their detrimental effects on\nmodel performance. We propose Adaptive-DPO -- a novel approach that\nincorporates a minority-instance-aware metric into the DPO objective. This\nmetric, which includes intra-annotator confidence and inter-annotator\nstability, distinguishes between majority and minority samples. We introduce an\nAdaptive-DPO loss function which improves the DPO loss in two ways: enhancing\nthe model's learning of majority labels while mitigating the negative impact of\nminority samples. Our experiments demonstrate that this method effectively\nhandles both synthetic minority data and real-world preference data, paving the\nway for more effective training methodologies in image generation tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16921.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "652fab9d04a34a9282bf29d6",
      "avatarUrl": "/avatars/cd5967b37ebb1225e9ae1d46f196e2e2.svg",
      "fullname": "Chengming Xu",
      "name": "ChengmingX",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.16025",
      "authors": [
        {
          "_id": "67dd02594aa37abf77af416b",
          "user": {
            "_id": "63eb8b1113a3eb9b0dc89d8c",
            "avatarUrl": "/avatars/d9cb7bdf4f3d2218f7d84120a00054bb.svg",
            "isPro": false,
            "fullname": "Yair Shpitzer",
            "user": "yairshp",
            "type": "user"
          },
          "name": "Yair Shpitzer",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-21T11:40:09.783Z",
          "hidden": false
        },
        {
          "_id": "67dd02594aa37abf77af416c",
          "name": "Gal Chechik",
          "hidden": false
        },
        {
          "_id": "67dd02594aa37abf77af416d",
          "name": "Idan Schwartz",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-20T10:45:04.000Z",
      "submittedOnDailyAt": "2025-03-24T08:16:33.033Z",
      "title": "Single Image Illustration Theme-Driven Generation and Editing",
      "submittedOnDailyBy": {
        "_id": "63eb8b1113a3eb9b0dc89d8c",
        "avatarUrl": "/avatars/d9cb7bdf4f3d2218f7d84120a00054bb.svg",
        "isPro": false,
        "fullname": "Yair Shpitzer",
        "user": "yairshp",
        "type": "user"
      },
      "summary": "La génération et l'édition d'images personnalisées sont particulièrement difficiles lorsqu'il s'agit de sujets avec peu d'images ou quand il n'y a qu'une seule image. Une approche courante pour aborder la personnalisation est par l'apprentissage de concepts, ce qui permet d'incorporer des thèmes dans des modèles existants, mais lorsque le nombre d'images est faible, la qualité de l'image diminue dramatiquement. Pour améliorer la qualité, un entraînement préalable peut être effectué, mais cela limite la génération à la distribution d'entraînement et nécessite du temps. La génération et l'édition d'images personnalisées à partir d'une seule image est un défi complexe qui n'a pas encore été résolu. Dans ce contexte, un nouvel approche sans nécessité d'entraînement est présentée pour optimiser le score de similitude avec l'image thématique. Spécifiquement, SISO génère des images de manière continue en basant sur la perte de similitude et optimise le modèle, répétant ce processus jusqu'à ce qu'un niveau de similitude satisfaisant soit atteint. De plus, SISO permet l'optimisation de plugins et de motifs dans n'importe quel générateur d'images. SISO a été évalué pour les tâches d'édition et de génération d'images en utilisant différents ensembles de données personnelles, montrant une amélioration significative en termes de qualité de l'image, de précision du thème et de conservation du fond, par rapport aux méthodes existantes.",
      "upvotes": 3,
      "discussionId": "67dd025f4aa37abf77af42db",
      "projectPage": "https://siso-paper.github.io/",
      "githubRepo": "https://github.com/yairshp/SISO",
      "ai_keywords": [
        "concept learning",
        "encoder",
        "pre-training",
        "similarity score",
        "iterative generation",
        "model optimization",
        "plug-and-play optimization"
      ]
    },
    "publishedAt": "2025-03-20T06:45:04.000Z",
    "title": "Single Image Iterative Subject-driven Generation and Editing",
    "summary": "Personalizing image generation and editing is particularly challenging when\nwe only have a few images of the subject, or even a single image. A common\napproach to personalization is concept learning, which can integrate the\nsubject into existing models relatively quickly, but produces images whose\nquality tends to deteriorate quickly when the number of subject images is\nsmall. Quality can be improved by pre-training an encoder, but training\nrestricts generation to the training distribution, and is time consuming. It is\nstill an open hard challenge to personalize image generation and editing from a\nsingle image without training. Here, we present SISO, a novel, training-free\napproach based on optimizing a similarity score with an input subject image.\nMore specifically, SISO iteratively generates images and optimizes the model\nbased on loss of similarity with the given subject image until a satisfactory\nlevel of similarity is achieved, allowing plug-and-play optimization to any\nimage generator. We evaluated SISO in two tasks, image editing and image\ngeneration, using a diverse data set of personal subjects, and demonstrate\nsignificant improvements over existing methods in image quality, subject\nfidelity, and background preservation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16025.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63eb8b1113a3eb9b0dc89d8c",
      "avatarUrl": "/avatars/d9cb7bdf4f3d2218f7d84120a00054bb.svg",
      "fullname": "Yair Shpitzer",
      "name": "yairshp",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.12821",
      "authors": [
        {
          "_id": "67e0cc432bbf376bdb18623b",
          "user": {
            "_id": "66aca01e33f6b27979856f6f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66aca01e33f6b27979856f6f/IyOxv89TudwscGH7tdue3.jpeg",
            "isPro": false,
            "fullname": "Mingyang Song",
            "user": "hitsmy",
            "type": "user"
          },
          "name": "Mingyang Song",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-24T08:07:01.991Z",
          "hidden": false
        },
        {
          "_id": "67e0cc432bbf376bdb18623c",
          "name": "Xiaoye Qu",
          "hidden": false
        },
        {
          "_id": "67e0cc432bbf376bdb18623d",
          "name": "Jiawei Zhou",
          "hidden": false
        },
        {
          "_id": "67e0cc432bbf376bdb18623e",
          "name": "Yu Cheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-17T05:01:09.000Z",
      "submittedOnDailyAt": "2025-03-24T02:58:13.280Z",
      "title": "Pour l'objectif de générer des expressions équilibrées dans des modèles de langue visuelle à grande échelle, on utilise l'ajustement de données adaptatif.",
      "submittedOnDailyBy": {
        "_id": "66aca01e33f6b27979856f6f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66aca01e33f6b27979856f6f/IyOxv89TudwscGH7tdue3.jpeg",
        "isPro": false,
        "fullname": "Mingyang Song",
        "user": "hitsmy",
        "type": "user"
      },
      "summary": "Les modèles grands de langage visuel (LVLMs) ont réalisé un progrès significatif dans l'intégration de la compréhension visuelle et de la génération de langage. Cet succès a été accompagné par le problème de distribution de données très imbalancée connu sous le nom de 'problème de la corde à la queue (LT) problem'. Les études précédentes ont principalement porté sur les architectures traditionnelles de VLM comme CLIP ou ViT, ou sur des tâches spécifiques (par exemple, reconnaissance et classification). Cependant, la recherche sur LVLM (par exemple, LLaVA) et des tâches plus générales (par exemple, résolution de problèmes visuels et inférence visuelle) restent des défis. Dans cet article, nous analysons profondément le problème de LT dans les LVLM, reconnaissant deux causes fondamentales : l'expression excessive de concepts cérébraux et la manque de représentation de concepts de queue. À partir de cette perspective, nous proposons un cadre de précision de données adaptative (ADR) qui consiste en deux étapes : remplacement de données adaptatif (DR) et synthèse de données (DS). Dans l'étape DR, les données inadéquates sont remplacées de manière adaptative en fonction de la distribution d'entités, tandis que dans l'étape DS, des modèles de diffusion avec bruit (DDPMs) et des images rares sont utilisés pour compléter les parties manquantes. À travers 11 cadres de test, nous démontrons que l'ADR proposé résout efficacement le problème de LT dans les données d'entraînement, améliorant en environ 4,36% le rendement moyen de LLaVA 1,5.",
      "upvotes": 3,
      "discussionId": "67e0cc442bbf376bdb186293",
      "ai_keywords": [
        "Large Vision-Language Models (LVLMs)",
        "Long-Tail (LT) problems",
        "CLIP",
        "ViT",
        "LLaVA",
        "Visual Question Answering",
        "Visual Reasoning",
        "Adaptive Data Refinement Framework (ADR)",
        "Data Rebalancing (DR)",
        "Data Synthesis (DS)",
        "Denoising Diffusion Probabilistic Models (DDPMs)"
      ]
    },
    "publishedAt": "2025-03-17T01:01:09.000Z",
    "title": "From Head to Tail: Towards Balanced Representation in Large\n  Vision-Language Models through Adaptive Data Calibration",
    "summary": "Large Vision-Language Models (LVLMs) have achieved significant progress in\ncombining visual comprehension with language generation. Despite this success,\nthe training data of LVLMs still suffers from Long-Tail (LT) problems, where\nthe data distribution is highly imbalanced. Previous works have mainly focused\non traditional VLM architectures, i.e., CLIP or ViT, and specific tasks such as\nrecognition and classification. Nevertheless, the exploration of LVLM (e.g.\nLLaVA) and more general tasks (e.g. Visual Question Answering and Visual\nReasoning) remains under-explored. In this paper, we first conduct an in-depth\nanalysis of the LT issues in LVLMs and identify two core causes: the\noverrepresentation of head concepts and the underrepresentation of tail\nconcepts. Based on the above observation, we propose an Adaptive\nData Refinement Framework (ADR), which\nconsists of two stages: Data Rebalancing (DR)\nand Data Synthesis (DS). In the DR stage, we\nadaptively rebalance the redundant data based on entity distributions, while in\nthe DS stage, we leverage Denoising Diffusion Probabilistic Models (DDPMs) and\nscarce images to supplement underrepresented portions. Through comprehensive\nevaluations across eleven benchmarks, our proposed ADR effectively mitigates\nthe long-tail problem in the training data, improving the average performance\nof LLaVA 1.5 relatively by 4.36%, without increasing the training data volume.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12821.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66aca01e33f6b27979856f6f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66aca01e33f6b27979856f6f/IyOxv89TudwscGH7tdue3.jpeg",
      "fullname": "Mingyang Song",
      "name": "hitsmy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.17287",
      "authors": [
        {
          "_id": "67e0bfcc8fb92b0edaa78dc0",
          "name": "Mingyang Song",
          "hidden": false
        },
        {
          "_id": "67e0bfcc8fb92b0edaa78dc1",
          "name": "Mao Zheng",
          "hidden": false
        },
        {
          "_id": "67e0bfcc8fb92b0edaa78dc2",
          "name": "Zheng Li",
          "hidden": false
        },
        {
          "_id": "67e0bfcc8fb92b0edaa78dc3",
          "name": "Wenjie Yang",
          "hidden": false
        },
        {
          "_id": "67e0bfcc8fb92b0edaa78dc4",
          "name": "Xuan Luo",
          "hidden": false
        },
        {
          "_id": "67e0bfcc8fb92b0edaa78dc5",
          "name": "Yue Pan",
          "hidden": false
        },
        {
          "_id": "67e0bfcc8fb92b0edaa78dc6",
          "name": "Feng Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-21T16:35:31.000Z",
      "submittedOnDailyAt": "2025-03-24T00:43:41.116Z",
      "title": "FastCuRL : Utilisation avancée de contextes pour l'apprentissage de programmes de récompense renforcée\n  Amélioration efficace de l'entraînement de modèles structurés de raisonnement adaptatif à partir de R1",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Dans cet article, nous proposons un approche d'apprentissage par réflexion rapide et efficace appelé \\textsc{FastCuRL}. Cette approche améliore l'efficacité de l'apprentissage des modèles de raisonnement comme R1 en utilisant des stratégies d'expansion de contexte longue, conçue pour aborder des tâches de raisonnement complexes qui comprennent des contextes longs. Spécifiquement, un modèle de langage avec 1.5B paramètres est utilisé.\n\n\\textsc{FastCuRL} est principalement configuré en deux étapes : les données d'entraînement sont divisées en trois niveaux selon la longueur du prompt d'entrée et une entraînement avec expansion de contexte longue est effectué. Concrètement, la première étape divise les données d'entraînement en trois niveaux selon la longueur du prompt d'entrée, et la deuxième étape utilise une fenêtre de contexte d'expansion de contexte longue pour entraîner le modèle de raisonnement avec les données divisées.\n\nLes résultats des expérimentations montrent que \\textsc{FastCuRL}-1.5B-Preview dépasse DeepScaleR-1.5B-Preview dans tous les 5 ensembles de données (MATH 500, AIME 2024, AMC 2023, Minerva Math, OlympiadBench) en utilisant seulement 50% des étapes d'entraînement. De plus, toutes les étapes d'entraînement de \\textsc{FastCuRL}-1.5B-Preview ont été réalisées sur 8 nœuds.",
      "upvotes": 2,
      "discussionId": "67e0bfcd8fb92b0edaa78e17",
      "githubRepo": "https://github.com/nick7nlp/FastCuRL",
      "ai_keywords": [
        "Curriculum Reinforcement Learning",
        "context window",
        "reinforcement learning",
        "training efficiency",
        "R1-like reasoning models",
        "long chain-of-thought",
        "rationales",
        "length-aware training",
        "training data segmentation",
        "context window extension",
        "progressively increasing context window length",
        "DeepScaleR-1.5B-Preview",
        "MATH 500",
        "AIME 2024",
        "AMC 2023",
        "Minerva Math",
        "OlympiadBench",
        "training steps"
      ]
    },
    "publishedAt": "2025-03-21T12:35:31.000Z",
    "title": "FastCuRL: Curriculum Reinforcement Learning with Progressive Context\n  Extension for Efficient Training R1-like Reasoning Models",
    "summary": "In this paper, we propose \\textsc{FastCuRL}, a simple yet efficient\nCurriculum Reinforcement Learning approach with\ncontext window extending strategy to accelerate the reinforcement learning\ntraining efficiency for R1-like reasoning models while enhancing their\nperformance in tackling complex reasoning tasks with long chain-of-thought\nrationales, particularly with a 1.5B parameter language model.\n\\textsc{FastCuRL} consists of two main procedures: length-aware\ntraining data segmentation and context window extension training. Specifically,\nthe former first splits the original training data into three different levels\nby the input prompt length, and then the latter leverages segmented training\ndatasets with a progressively increasing context window length to train the\nreasoning model. Experimental results demonstrate that\n\\textsc{FastCuRL}-1.5B-Preview surpasses DeepScaleR-1.5B-Preview\nacross all five datasets (including MATH 500, AIME 2024, AMC 2023, Minerva\nMath, and OlympiadBench) while only utilizing 50\\% of training steps.\nFurthermore, all training stages for FastCuRL-1.5B-Preview are completed using\njust a single node with 8 GPUs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17287.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6443
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.17069",
      "authors": [
        {
          "_id": "67e0ba47753cfd5e438d3814",
          "user": {
            "_id": "63be636387619d1458c2e8e0",
            "avatarUrl": "/avatars/83e14735760c5cadd5341ebcb4cf9556.svg",
            "isPro": false,
            "fullname": "SHI YUFEI",
            "user": "Master-Shi",
            "type": "user"
          },
          "name": "Yufei Shi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-24T08:10:50.772Z",
          "hidden": false
        },
        {
          "_id": "67e0ba47753cfd5e438d3815",
          "name": "Weilong Yan",
          "hidden": false
        },
        {
          "_id": "67e0ba47753cfd5e438d3816",
          "name": "Gang Xu",
          "hidden": false
        },
        {
          "_id": "67e0ba47753cfd5e438d3817",
          "name": "Yumeng Li",
          "hidden": false
        },
        {
          "_id": "67e0ba47753cfd5e438d3818",
          "name": "Yuchen Li",
          "hidden": false
        },
        {
          "_id": "67e0ba47753cfd5e438d3819",
          "name": "Zhenxi Li",
          "hidden": false
        },
        {
          "_id": "67e0ba47753cfd5e438d381a",
          "name": "Fei Richard Yu",
          "hidden": false
        },
        {
          "_id": "67e0ba47753cfd5e438d381b",
          "name": "Ming Li",
          "hidden": false
        },
        {
          "_id": "67e0ba47753cfd5e438d381c",
          "name": "Si Yong Yeo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-21T11:50:06.000Z",
      "submittedOnDailyAt": "2025-03-24T07:28:42.136Z",
      "title": "PVChat : Personnalisation du chat vidéo (apprentissage par exemple)",
      "submittedOnDailyBy": {
        "_id": "63be636387619d1458c2e8e0",
        "avatarUrl": "/avatars/83e14735760c5cadd5341ebcb4cf9556.svg",
        "isPro": false,
        "fullname": "SHI YUFEI",
        "user": "Master-Shi",
        "type": "user"
      },
      "summary": "Les modèles de grande langue vidéo (ViLLMs) se spécialisent dans la compréhension générale des films, réussissant à reconnaître des activités comme parler et manger, mais se heurtent à des difficultés pour comprendre des informations d'identité, comme \"Wilson est en cours de chimiothérapie\" ou \"Tom discute avec Sarah\", ce qui limite leur application dans les ménages intelligents et les environnements. Pour aborder cette limitation, nous proposons PVChat, un cadre d'apprentissage en un seul pas pour les ViLLMs personnalisés. C'est le premier ViLLM personnalisé qui fournit des réponses à des questions liées aux thèmes d'une seule filmation pour chaque individu. Notre approche optimise un ViLLM (Mixture-of-Heads) amélioré en utilisant un ensemble de données de questions et réponses de films améliorés de manière synthétique et exploite des stratégies avancées d'apprentissage basées sur des images de films. Spécifiquement, nous introduisons une pipeline d'augmentation de données automatique pour rechercher des exemples positifs et négatifs contenant des informations d'identité dans les corpus de films existants, générant des ensembles d'apprentissage divers avec quatre types de questions : existence, apparence, action et localisation. Pour améliorer l'apprentissage spécifique des thèmes, nous proposons des mécanismes d'attention ReLU Routing MoH et introduisons deux nouveaux objectifs : 1) Normalisation des voisins les plus proches en utilisant l'échelle exponentielle, et 2) routage d'attention équilibré à travers l'Amélioration de l'Activation de Cabinets. Enfin, nous adoptons une stratégie d'apprentissage en deux étapes, de la pré-entraînement sur des images à la fine-tuning sur des films, permettant un apprentissage progressif des attributs statiques aux expressions dynamiques. PVChat est évalué sur divers ensembles de données, y compris des dispositifs médicaux, des séries de télévision, des animations et des charges de nourriture réelles, démontrant des niveaux avancés de compréhension de caractéristiques personnalisées et montrant un meilleur comportement par rapport aux autres ViLLMs après avoir appris d'une seule filmation.",
      "upvotes": 2,
      "discussionId": "67e0ba4b753cfd5e438d391e",
      "ai_keywords": [
        "ViLLMs",
        "one-shot learning",
        "PVChat",
        "Mixture-of-Heads (MoH)",
        "image-to-video learning",
        "automated augmentation pipeline",
        "identity-preserving positive samples",
        "hard negatives",
        "QA types",
        "ReLU Routing MoH attention mechanism",
        "Smooth Proximity Regularization",
        "Head Activation Enhancement",
        "two-stage training strategy",
        "image pre-training",
        "video fine-tuning",
        "personalized feature understanding",
        "state-of-the-art ViLLMs"
      ]
    },
    "publishedAt": "2025-03-21T07:50:06.000Z",
    "title": "PVChat: Personalized Video Chat with One-Shot Learning",
    "summary": "Video large language models (ViLLMs) excel in general video understanding,\ne.g., recognizing activities like talking and eating, but struggle with\nidentity-aware comprehension, such as \"Wilson is receiving chemotherapy\" or\n\"Tom is discussing with Sarah\", limiting their applicability in smart\nhealthcare and smart home environments. To address this limitation, we propose\na one-shot learning framework PVChat, the first personalized ViLLM that enables\nsubject-aware question answering (QA) from a single video for each subject. Our\napproach optimizes a Mixture-of-Heads (MoH) enhanced ViLLM on a synthetically\naugmented video-QA dataset, leveraging a progressive image-to-video learning\nstrategy. Specifically, we introduce an automated augmentation pipeline that\nsynthesizes identity-preserving positive samples and retrieves hard negatives\nfrom existing video corpora, generating a diverse training dataset with four QA\ntypes: existence, appearance, action, and location inquiries. To enhance\nsubject-specific learning, we propose a ReLU Routing MoH attention mechanism,\nalongside two novel objectives: (1) Smooth Proximity Regularization for\nprogressive learning through exponential distance scaling and (2) Head\nActivation Enhancement for balanced attention routing. Finally, we adopt a\ntwo-stage training strategy, transitioning from image pre-training to video\nfine-tuning, enabling a gradual learning process from static attributes to\ndynamic representations. We evaluate PVChat on diverse datasets covering\nmedical scenarios, TV series, anime, and real-world footage, demonstrating its\nsuperiority in personalized feature understanding after learning from a single\nvideo, compared to state-of-the-art ViLLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17069.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63be636387619d1458c2e8e0",
      "avatarUrl": "/avatars/83e14735760c5cadd5341ebcb4cf9556.svg",
      "fullname": "SHI YUFEI",
      "name": "Master-Shi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.11572",
      "authors": [
        {
          "_id": "67e0530f151ca9ed9265e949",
          "user": {
            "_id": "64c5d832d68946edad7d5536",
            "avatarUrl": "/avatars/5d38217d4ab99cdcf53c52661b8baa0d.svg",
            "isPro": false,
            "fullname": "Messi Lee",
            "user": "l048596",
            "type": "user"
          },
          "name": "Messi H. J. Lee",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-03-23T18:30:22.473Z",
          "hidden": false
        },
        {
          "_id": "67e0530f151ca9ed9265e94a",
          "name": "Calvin K. Lai",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-14T16:40:02.000Z",
      "submittedOnDailyAt": "2025-03-24T02:15:38.327Z",
      "title": "\"Patrones comme le biais d'entrée dans le modèle théorique\"",
      "submittedOnDailyBy": {
        "_id": "64c5d832d68946edad7d5536",
        "avatarUrl": "/avatars/5d38217d4ab99cdcf53c52661b8baa0d.svg",
        "isPro": false,
        "fullname": "Messi Lee",
        "user": "l048596",
        "type": "user"
      },
      "summary": "Le terme «Bias Implicit» se réfère à un processus automatique et non dynamique qui forme des observations, des jugements et des actions. Dans des études antérieures, la recherche sur «Bias Implicit» dans les modèles de langage grands (LLMs) a été différente du méthode utilisée pour les études humaines, et a été principalement centrée sur les résultats du modèle. Pour explorer le processus du modèle, on propose le méthode de la Test d'Assignation de raisons implicites (RM-IAT). Ce méthode montre que, lorsqu'on compare des informations pertinentes, le modèle de raisons augmente sa quantité de modules lorsqu'il traite d'informations non pertinentes. Ces résultats démontrent que les systèmes d'IA ont des patrons similaires à ceux de «Bias Implicit» humains dans le traitement d'information. On discute les implications de ces patrons sur la fonctionnalité d'applications réelles.",
      "upvotes": 2,
      "discussionId": "67e05311151ca9ed9265e9c1",
      "githubRepo": "https://github.com/lee-messi/RM-IAT",
      "ai_keywords": [
        "Reasoning Model Implicit Association Test (RM-IAT)",
        "reasoning models",
        "LLMs (Large Language Models)",
        "tokens",
        "association-incompatible information",
        "association-compatible information",
        "implicit bias-like patterns"
      ]
    },
    "publishedAt": "2025-03-14T12:40:02.000Z",
    "title": "Implicit Bias-Like Patterns in Reasoning Models",
    "summary": "Implicit bias refers to automatic or spontaneous mental processes that shape\nperceptions, judgments, and behaviors. Previous research examining `implicit\nbias' in large language models (LLMs) has often approached the phenomenon\ndifferently than how it is studied in humans by focusing primarily on model\noutputs rather than on model processing. To examine model processing, we\npresent a method called the Reasoning Model Implicit Association Test (RM-IAT)\nfor studying implicit bias-like patterns in reasoning models: LLMs that employ\nstep-by-step reasoning to solve complex tasks. Using this method, we find that\nreasoning models require more tokens when processing association-incompatible\ninformation compared to association-compatible information. These findings\nsuggest AI systems harbor patterns in processing information that are analogous\nto human implicit bias. We consider the implications of these implicit\nbias-like patterns for their deployment in real-world applications.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.11572.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64c5d832d68946edad7d5536",
      "avatarUrl": "/avatars/5d38217d4ab99cdcf53c52661b8baa0d.svg",
      "fullname": "Messi Lee",
      "name": "l048596",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]