[
  {
    "paper": {
      "id": "2506.09113",
      "authors": [
        {
          "_id": "684a3b0a9b38e1e5a33a683f",
          "name": "Yu Gao",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6840",
          "name": "Haoyuan Guo",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6841",
          "name": "Tuyen Hoang",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6842",
          "name": "Weilin Huang",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6843",
          "name": "Lu Jiang",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6844",
          "name": "Fangyuan Kong",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6845",
          "name": "Huixia Li",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6846",
          "name": "Jiashi Li",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6847",
          "name": "Liang Li",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6848",
          "name": "Xiaojie Li",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6849",
          "name": "Xunsong Li",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a684a",
          "name": "Yifu Li",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a684b",
          "name": "Shanchuan Lin",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a684c",
          "name": "Zhijie Lin",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a684d",
          "name": "Jiawei Liu",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a684e",
          "name": "Shu Liu",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a684f",
          "name": "Xiaonan Nie",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6850",
          "name": "Zhiwu Qing",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6851",
          "name": "Yuxi Ren",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6852",
          "name": "Li Sun",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6853",
          "name": "Zhi Tian",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6854",
          "name": "Rui Wang",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6855",
          "name": "Sen Wang",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6856",
          "name": "Guoqiang Wei",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6857",
          "name": "Guohong Wu",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6858",
          "name": "Jie Wu",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6859",
          "name": "Ruiqi Xia",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a685a",
          "name": "Fei Xiao",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a685b",
          "name": "Xuefeng Xiao",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a685c",
          "name": "Jiangqiao Yan",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a685d",
          "name": "Ceyuan Yang",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a685e",
          "name": "Jianchao Yang",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a685f",
          "name": "Runkai Yang",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6860",
          "name": "Tao Yang",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6861",
          "name": "Yihang Yang",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6862",
          "name": "Zilyu Ye",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6863",
          "name": "Xuejiao Zeng",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6864",
          "name": "Yan Zeng",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6865",
          "name": "Heng Zhang",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6866",
          "name": "Yang Zhao",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6867",
          "name": "Xiaozheng Zheng",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6868",
          "name": "Peihao Zhu",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6869",
          "name": "Jiaxin Zou",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a686a",
          "name": "Feilong Zuo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-10T17:56:11.000Z",
      "submittedOnDailyAt": "2025-06-12T01:08:56.090Z",
      "title": "Seedance 1.0 : Défi des limites des modèles de génération d'images",
      "submittedOnDailyBy": {
        "_id": "6381c5d63680a7cf34e08ca9",
        "avatarUrl": "/avatars/731467e2d80d0ae163c4a00a9e3ff9e5.svg",
        "isPro": false,
        "fullname": "wujie10558@gmail.com",
        "user": "wujie10",
        "type": "user"
      },
      "summary": "Actuellement, des modèles similaires permettent un apprentissage panoramique dans différents scénarios en ajoutant la précision des petits données de somme et des captures vidéo significatives, ce qui facilite l'ajout de ces caractéristiques pour améliorer la précision et la capacité d'apprentissage dans des scénarios variés. Grâce à un design architectural efficace et un paradigme d'apprentissage proposé, on atteint l'apprentissage commun pour deux tâches : la génération de vidéo à partir d'une image et la génération de vidéo à partir d'un texte. En utilisant une structure de récompenses multidimensionnelle et un processus d'ajustement avec rétroaction des utilisateurs, on atteint un amélioration généralisée du rendement. De plus, on accélère le modèle avec un effet dirigé élevé, ce qui permet une augmentation de la vitesse d'inférence d'environ 10 fois grâce à une stratégie de conception multiniveau et d'optimisation du système. Cela résulte en un modèle de haute efficacité et rendement basé sur la vidéo.\n\nSeedance 1.0 peut générer un vidéo de 5 secondes à 1080p en seulement 41,4 secondes sur NVIDIA-L20. Comparé aux modèles de génération de vidéo les plus avancés, il s'agit d'un modèle spécialisé dans la représentation cohérente de thèmes, avec une grande qualité et vitesse de génération de vidéo, une fidélité spectrale et stabilité structurale élevées, une précision dans les contextes complexes et respect des instructions claires, et une division native de vidéo à partir d'une image qui maintient une représentation cohérente du thème.",
      "upvotes": 33,
      "discussionId": "684a3b0b9b38e1e5a33a686b",
      "projectPage": "https://seed.bytedance.com/seedance",
      "ai_summary": "Seedance 1.0 offers high-performance video generation by integrating advanced data curation, efficient architecture, post-training optimization, and model acceleration, resulting in superior quality and speed.",
      "ai_keywords": [
        "diffusion modeling",
        "multi-source data curation",
        "precision and meaningful video captioning",
        "efficient architecture",
        "training paradigm",
        "multi-shot generation",
        "text-to-video",
        "image-to-video",
        "fine-grained supervised fine-tuning",
        "video-specific RLHF",
        "multi-dimensional reward mechanisms",
        "multi-stage distillation strategies",
        "model acceleration",
        "spatiotemporal fluidity",
        "structural stability",
        "instruction adherence",
        "multi-shot narrative coherence",
        "consistent subject representation"
      ]
    },
    "publishedAt": "2025-06-10T13:56:11.000Z",
    "title": "Seedance 1.0: Exploring the Boundaries of Video Generation Models",
    "summary": "Notable breakthroughs in diffusion modeling have propelled rapid improvements\nin video generation, yet current foundational model still face critical\nchallenges in simultaneously balancing prompt following, motion plausibility,\nand visual quality. In this report, we introduce Seedance 1.0, a\nhigh-performance and inference-efficient video foundation generation model that\nintegrates several core technical improvements: (i) multi-source data curation\naugmented with precision and meaningful video captioning, enabling\ncomprehensive learning across diverse scenarios; (ii) an efficient architecture\ndesign with proposed training paradigm, which allows for natively supporting\nmulti-shot generation and jointly learning of both text-to-video and\nimage-to-video tasks. (iii) carefully-optimized post-training approaches\nleveraging fine-grained supervised fine-tuning, and video-specific RLHF with\nmulti-dimensional reward mechanisms for comprehensive performance improvements;\n(iv) excellent model acceleration achieving ~10x inference speedup through\nmulti-stage distillation strategies and system-level optimizations. Seedance\n1.0 can generate a 5-second video at 1080p resolution only with 41.4 seconds\n(NVIDIA-L20). Compared to state-of-the-art video generation models, Seedance\n1.0 stands out with high-quality and fast video generation having superior\nspatiotemporal fluidity with structural stability, precise instruction\nadherence in complex multi-subject contexts, native multi-shot narrative\ncoherence with consistent subject representation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09113.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6381c5d63680a7cf34e08ca9",
      "avatarUrl": "/avatars/731467e2d80d0ae163c4a00a9e3ff9e5.svg",
      "fullname": "wujie10558@gmail.com",
      "name": "wujie10",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.06395",
      "authors": [
        {
          "_id": "68492dcf42e4f9106973f437",
          "user": {
            "_id": "6734e315c1aadce903f73aea",
            "avatarUrl": "/avatars/95d95c49419372debc201cb63c354b86.svg",
            "isPro": false,
            "fullname": "Li Pengyi",
            "user": "LiPengyi29",
            "type": "user"
          },
          "name": "Pengyi Li",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-11T07:18:40.287Z",
          "hidden": false
        },
        {
          "_id": "68492dcf42e4f9106973f438",
          "name": "Matvey Skripkin",
          "hidden": false
        },
        {
          "_id": "68492dcf42e4f9106973f439",
          "name": "Alexander Zubrey",
          "hidden": false
        },
        {
          "_id": "68492dcf42e4f9106973f43a",
          "name": "Andrey Kuznetsov",
          "hidden": false
        },
        {
          "_id": "68492dcf42e4f9106973f43b",
          "name": "Ivan Oseledets",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/643984dceb7c5616ef3f5d54/5kHQrpj1ivFhnzHL36xhr.jpeg"
      ],
      "publishedAt": "2025-06-05T19:55:15.000Z",
      "submittedOnDailyAt": "2025-06-12T07:02:06.762Z",
      "title": "Autoestima peut être tout : Micro-ajustements de RL courts pour modèles de langue",
      "submittedOnDailyBy": {
        "_id": "643984dceb7c5616ef3f5d54",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643984dceb7c5616ef3f5d54/10JRkblrRIEVci6UJwvPz.jpeg",
        "isPro": false,
        "fullname": "Andrey Kuznetsov",
        "user": "kuznetsoffandrey",
        "type": "user"
      },
      "summary": "Les modèles de langage grands (LLMs) dépassent la logique, mais l'entraînement postérieur à l'unité de token est crucial pour ajuster le comportement du modèle aux objectifs de la tâche. Les méthodes actuelles d'apprentissage par renforcement (RL) dépendent d'annotations humaines coûteuses ou de modèles de récompense externes. Nous proposons un apprentissage par renforcement basé sur la confiance du modèle (RLSC), qui utilise des signaux de confiance pour récompenser le modèle. Ce méthode élimine la nécessité d'étiquettes, de modèles de préférence ou d'apprentissage de récompense. Pour Qwen2.5-Math-7B, lors de l'entraînement de 16 problèmes par groupe avec 10 ou 20 étapes, le RLSC améliore la précision sur AIME2024 (+13.4%), MATH500 (+21.2%), Minerva Math (+21.7%), Olympiadbench (+20.8%) et AMC23 (+9.7%). RLSC offre une simple et scalable façon d'entraîner postérieur à l'unité de token, sans besoin d'échantillons avec des décimales ou d'étiquettes, et qui nécessite un supervision de superintelligence.",
      "upvotes": 27,
      "discussionId": "68492dd042e4f9106973f43c",
      "ai_summary": "Reinforcement Learning via Self-Confidence (RLSC) improves large language model accuracy using the model's confidence as a reward signal, eliminating the need for human labels or reward engineering.",
      "ai_keywords": [
        "Reinforcement Learning",
        "Large language models",
        "self-confidence",
        "RLSC"
      ]
    },
    "publishedAt": "2025-06-05T15:55:15.000Z",
    "title": "Confidence Is All You Need: Few-Shot RL Fine-Tuning of Language Models",
    "summary": "Large language models (LLMs) excel at reasoning, yet post-training remains\ncritical for aligning their behavior with task goals. Existing reinforcement\nlearning (RL) methods often depend on costly human annotations or external\nreward models. We propose Reinforcement Learning via Self-Confidence (RLSC),\nwhich uses the model's own confidence as reward signals-eliminating the need\nfor labels, preference models, or reward engineering. Applied to\nQwen2.5-Math-7B with only 16 samples per question and 10 or 20 training steps,\nRLSC improves accuracy by +13.4% on AIME2024, +21.2% on MATH500, +21.7% on\nMinerva Math, +20.8% on Olympiadbench, and +9.7% on AMC23. RLSC provides a\nsimple, scalable post-training method for inference models, requiring only a\nsmall number of samples and unlabelled supervision.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/643984dceb7c5616ef3f5d54/5kHQrpj1ivFhnzHL36xhr.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.06395.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "643984dceb7c5616ef3f5d54",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643984dceb7c5616ef3f5d54/10JRkblrRIEVci6UJwvPz.jpeg",
      "fullname": "Andrey Kuznetsov",
      "name": "kuznetsoffandrey",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 18
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.09995",
      "authors": [
        {
          "_id": "684a39639b38e1e5a33a6837",
          "name": "Yuanpeng Tu",
          "hidden": false
        },
        {
          "_id": "684a39639b38e1e5a33a6838",
          "name": "Hao Luo",
          "hidden": false
        },
        {
          "_id": "684a39639b38e1e5a33a6839",
          "name": "Xi Chen",
          "hidden": false
        },
        {
          "_id": "684a39639b38e1e5a33a683a",
          "name": "Xiang Bai",
          "hidden": false
        },
        {
          "_id": "684a39639b38e1e5a33a683b",
          "name": "Fan Wang",
          "hidden": false
        },
        {
          "_id": "684a39639b38e1e5a33a683c",
          "name": "Hengshuang Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-11T17:59:53.000Z",
      "submittedOnDailyAt": "2025-06-12T00:50:19.796Z",
      "title": "PlayerOne: Monde Simulateur Centré sur Moi",
      "submittedOnDailyBy": {
        "_id": "644a1b6401e18bf93a6f45c1",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644a1b6401e18bf93a6f45c1/P0i_CgCrIzOS2tYRlxoE9.png",
        "isPro": false,
        "fullname": "xichen",
        "user": "xichenhku",
        "type": "user"
      },
      "summary": "PlayerVerse se présente comme le premier simulateur central de mondes réalistes. Cela pousse l'exploration sans limites dans un environnement vibrant. Il accepte une image essentielle d'un schéma fournie par l'utilisateur, construit avec précision un monde correspondant et génère des vidéos centrales. Ces vidéos correspondent aux mouvements humains réels capturés par une caméra central par l'utilisateur. PlayerVerse apprend de manière flottante, de la généralité aux détails comme les feuilles d'un arbre. Tout d'abord, il effectue un apprentissage préliminaire axé sur des paires texte-vidéo à grande échelle, atteignant une compréhension centrale. Ensuite, il effectue un apprentissage d'ajustement basé sur des données de mouvement-vidéo motivées extraites d'un ensemble de données de vidéo central. De plus, il concevoit des séquences d'injection de mouvements au niveau partiel, en considérant l'importance des composants, et peut contrôler les mouvements au niveau partiel avec précision. De plus, il concevoit deux cadres de reconstruction de poids complexes qui modélisent de manière évolutive tant le schéma 4D que les frames de vidéo, assurant la consistance du schéma dans la génération de longs vidéos. Les résultats des expériences montrent la capacité de modéliser des mondes et de contrôler décisivement différents schémas, représentant une première tentative de simulation réaliste du monde central et ouvrant des voies pour la communauté qui défie les nouveaux limites dans la modélisation du monde.",
      "upvotes": 22,
      "discussionId": "684a39639b38e1e5a33a683d",
      "projectPage": "https://playerone-hku.github.io/",
      "ai_summary": "PlayerOne is an egocentric realistic world simulator that constructs and generates videos from user-captured images, using a coarse-to-fine training pipeline and advanced motion injection and reconstruction frameworks.",
      "ai_keywords": [
        "egocentric realistic world simulator",
        "coarse-to-fine pipeline",
        "pretraining",
        "finetuning",
        "synchronous motion-video data",
        "automatic construction pipeline",
        "part-disentangled motion injection",
        "joint reconstruction framework",
        "4D scene",
        "video frames",
        "scene consistency",
        "long-form video generation",
        "worldconsistent modeling"
      ]
    },
    "publishedAt": "2025-06-11T13:59:53.000Z",
    "title": "PlayerOne: Egocentric World Simulator",
    "summary": "We introduce PlayerOne, the first egocentric realistic world simulator,\nfacilitating immersive and unrestricted exploration within vividly dynamic\nenvironments. Given an egocentric scene image from the user, PlayerOne can\naccurately construct the corresponding world and generate egocentric videos\nthat are strictly aligned with the real scene human motion of the user captured\nby an exocentric camera. PlayerOne is trained in a coarse-to-fine pipeline that\nfirst performs pretraining on large-scale egocentric text-video pairs for\ncoarse-level egocentric understanding, followed by finetuning on synchronous\nmotion-video data extracted from egocentric-exocentric video datasets with our\nautomatic construction pipeline. Besides, considering the varying importance of\ndifferent components, we design a part-disentangled motion injection scheme,\nenabling precise control of part-level movements. In addition, we devise a\njoint reconstruction framework that progressively models both the 4D scene and\nvideo frames, ensuring scene consistency in the long-form video generation.\nExperimental results demonstrate its great generalization ability in precise\ncontrol of varying human movements and worldconsistent modeling of diverse\nscenarios. It marks the first endeavor into egocentric real-world simulation\nand can pave the way for the community to delve into fresh frontiers of world\nmodeling and its diverse applications.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09995.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "644a1b6401e18bf93a6f45c1",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644a1b6401e18bf93a6f45c1/P0i_CgCrIzOS2tYRlxoE9.png",
      "fullname": "xichen",
      "name": "xichenhku",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 43
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.09350",
      "authors": [
        {
          "_id": "684a79ca9b38e1e5a33a68bf",
          "name": "Shanchuan Lin",
          "hidden": false
        },
        {
          "_id": "684a79ca9b38e1e5a33a68c0",
          "name": "Ceyuan Yang",
          "hidden": false
        },
        {
          "_id": "684a79ca9b38e1e5a33a68c1",
          "name": "Hao He",
          "hidden": false
        },
        {
          "_id": "684a79ca9b38e1e5a33a68c2",
          "name": "Jianwen Jiang",
          "hidden": false
        },
        {
          "_id": "684a79ca9b38e1e5a33a68c3",
          "name": "Yuxi Ren",
          "hidden": false
        },
        {
          "_id": "684a79ca9b38e1e5a33a68c4",
          "name": "Xin Xia",
          "hidden": false
        },
        {
          "_id": "684a79ca9b38e1e5a33a68c5",
          "name": "Yang Zhao",
          "hidden": false
        },
        {
          "_id": "684a79ca9b38e1e5a33a68c6",
          "name": "Xuefeng Xiao",
          "hidden": false
        },
        {
          "_id": "684a79ca9b38e1e5a33a68c7",
          "name": "Lu Jiang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-11T03:04:23.000Z",
      "submittedOnDailyAt": "2025-06-12T05:25:53.654Z",
      "title": "Génération de vidéos de dialogue par unités de temps en utilisant l'entraînement postérieur de reconnaissance relative à n'importe qui",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": true,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Actuellement, les grands modèles de génération de vidéo nécessitent une grande quantité de calcul et ne peuvent pas être exécutés en temps réel, ce qui rend difficile leur introduction dans des applications interactives ou en temps réel. Dans cette étude, nous proposons la transformation d'un modèle de diffusion latin pré-entraîné en un générateur de vidéos temporelles et interactifs en utilisant l'apprentissage adversaire automatique de rétroalimentation (AAPT). Notre modèle utilise 1NFE (1 Neural Fibonacci) pour générer automatiquement une frame latine en un seul temps. Le modèle peut streamer les résultats temporels aux utilisateurs et répondre interactifment pour générer la prochaine frame latine. Au contraire des méthodes traditionnelles, notre approche révise l'apprentissage adversaire efficace pour la génération automatique. Cela permet de concevoir une architecture efficace en utilisant le caching de KV et de réduire l'accumulation d'erreurs lors de la génération de vidéos longues. À travers les résultats des tests, notre modèle de 8B utilise 1NFE pour générer une frame latine en temps réel, et a été conçu pour générer une architecture efficace en utilisant le caching de KV et réduire l'accumulation d'erreurs lors de la génération de vidéos longues.",
      "upvotes": 22,
      "discussionId": "684a79ca9b38e1e5a33a68c8",
      "projectPage": "https://seaweed-apt.com/2",
      "ai_summary": "Autoregressive adversarial post-training transforms pre-trained latent video diffusion models into real-time, interactive video generators with reduced computational requirements.",
      "ai_keywords": [
        "autoregressive adversarial post-training",
        "latent video diffusion model",
        "autoregressive generation",
        "neural function evaluation",
        "KV cache",
        "student-forcing",
        "real-time video generation",
        "24fps",
        "736x416 resolution",
        "1280x720 resolution",
        "H100"
      ]
    },
    "publishedAt": "2025-06-10T23:04:23.000Z",
    "title": "Autoregressive Adversarial Post-Training for Real-Time Interactive Video\n  Generation",
    "summary": "Existing large-scale video generation models are computationally intensive,\npreventing adoption in real-time and interactive applications. In this work, we\npropose autoregressive adversarial post-training (AAPT) to transform a\npre-trained latent video diffusion model into a real-time, interactive video\ngenerator. Our model autoregressively generates a latent frame at a time using\na single neural function evaluation (1NFE). The model can stream the result to\nthe user in real time and receive interactive responses as controls to generate\nthe next latent frame. Unlike existing approaches, our method explores\nadversarial training as an effective paradigm for autoregressive generation.\nThis not only allows us to design an architecture that is more efficient for\none-step generation while fully utilizing the KV cache, but also enables\ntraining the model in a student-forcing manner that proves to be effective in\nreducing error accumulation during long video generation. Our experiments\ndemonstrate that our 8B model achieves real-time, 24fps, streaming video\ngeneration at 736x416 resolution on a single H100, or 1280x720 on 8xH100 up to\na minute long (1440 frames). Visit our research website at\nhttps://seaweed-apt.com/2",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09350.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": true,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7093
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.08889",
      "authors": [
        {
          "_id": "684a39599b38e1e5a33a6822",
          "name": "Yizhao Gao",
          "hidden": false
        },
        {
          "_id": "684a39599b38e1e5a33a6823",
          "name": "Shuming Guo",
          "hidden": false
        },
        {
          "_id": "684a39599b38e1e5a33a6824",
          "name": "Shijie Cao",
          "hidden": false
        },
        {
          "_id": "684a39599b38e1e5a33a6825",
          "name": "Yuqing Xia",
          "hidden": false
        },
        {
          "_id": "684a39599b38e1e5a33a6826",
          "name": "Yu Cheng",
          "hidden": false
        },
        {
          "_id": "684a39599b38e1e5a33a6827",
          "name": "Lei Wang",
          "hidden": false
        },
        {
          "_id": "684a39599b38e1e5a33a6828",
          "name": "Lingxiao Ma",
          "hidden": false
        },
        {
          "_id": "684a39599b38e1e5a33a6829",
          "name": "Yutao Sun",
          "hidden": false
        },
        {
          "_id": "684a39599b38e1e5a33a682a",
          "name": "Tianzhu Ye",
          "hidden": false
        },
        {
          "_id": "684a39599b38e1e5a33a682b",
          "name": "Li Dong",
          "hidden": false
        },
        {
          "_id": "684a39599b38e1e5a33a682c",
          "name": "Hayden Kwok-Hay So",
          "hidden": false
        },
        {
          "_id": "684a39599b38e1e5a33a682d",
          "name": "Yu Hua",
          "hidden": false
        },
        {
          "_id": "684a39599b38e1e5a33a682e",
          "name": "Ting Cao",
          "hidden": false
        },
        {
          "_id": "684a39599b38e1e5a33a682f",
          "name": "Fan Yang",
          "hidden": false
        },
        {
          "_id": "684a39599b38e1e5a33a6830",
          "name": "Mao Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-10T15:17:26.000Z",
      "submittedOnDailyAt": "2025-06-12T00:54:43.454Z",
      "title": "SeerAttention-R: Attend-R pour l'Attention à Long Terme Sparse",
      "submittedOnDailyBy": {
        "_id": "661c96f48921f03a9dae04c3",
        "avatarUrl": "/avatars/c486a45ffea7d1a8c72bd8512014b07e.svg",
        "isPro": false,
        "fullname": "Yizhao Gao",
        "user": "LongMountain",
        "type": "user"
      },
      "summary": "Nous introduisons SeerAttention-R, un modèle d'attention sparse conçu spécifiquement pour la décodification longue de modèles d'inférence. Extendant la conception de SeerAttention, SeerAttention-R maintient le mécanisme d'apprentissage de la sparsité de l'attention grâce à un processus d'auto-distillation des portes, tout en supprimant la poole des requêtes pour s'adapter à la décodification autorégressive. Grâce à un mécanisme de portes de faible poids et modulaire, SeerAttention-R est flexible et peut être facilement intégré dans des modèles pré-entraînés sans modifier les paramètres originaux. Nous montrons que, avec seulement 0.4B d'étiquettes d'entraînement, SeerAttention-R maintient une précision d'inférence quasiment sans perte sur le benchmark AIME, avec des blocs d'attention sparses importants (64/128) et un budget de 4K d'étiquettes. En utilisant TileLang, nous avons développé un noyau de décodification sparse hautement optimisé, qui atteint une accélération presque au seuil théorique de 9 fois avec un taux de sparsité de 90% sur une GPU H100, dépassant ainsi FlashAttention-3. Le code est disponible sur le lien suivant : https://github.com/microsoft/SeerAttention.",
      "upvotes": 15,
      "discussionId": "684a39599b38e1e5a33a6833",
      "githubRepo": "https://github.com/microsoft/SeerAttention",
      "ai_summary": "SeerAttention-R is a sparse attention framework for reasoning models that maintains high accuracy and achieves significant speedups through optimized sparse decoding kernels.",
      "ai_keywords": [
        "sparse attention",
        "reasoning models",
        "self-distilled gating mechanism",
        "query pooling",
        "lightweight plug-in gating",
        "AIME benchmark",
        "TileLang",
        "sparse decoding kernel",
        "FlashAttention-3",
        "H100 GPU"
      ]
    },
    "publishedAt": "2025-06-10T11:17:26.000Z",
    "title": "SeerAttention-R: Sparse Attention Adaptation for Long Reasoning",
    "summary": "We introduce SeerAttention-R, a sparse attention framework specifically\ntailored for the long decoding of reasoning models. Extended from\nSeerAttention, SeerAttention-R retains the design of learning attention\nsparsity through a self-distilled gating mechanism, while removing query\npooling to accommodate auto-regressive decoding. With a lightweight plug-in\ngating, SeerAttention-R is flexible and can be easily integrated into existing\npretrained model without modifying the original parameters. We demonstrate that\nSeerAttention-R, trained on just 0.4B tokens, maintains near-lossless reasoning\naccuracy with 4K token budget in AIME benchmark under large sparse attention\nblock sizes (64/128). Using TileLang, we develop a highly optimized sparse\ndecoding kernel that achieves near-theoretical speedups of up to 9x over\nFlashAttention-3 on H100 GPU at 90% sparsity. Code is available at:\nhttps://github.com/microsoft/SeerAttention.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08889.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "661c96f48921f03a9dae04c3",
      "avatarUrl": "/avatars/c486a45ffea7d1a8c72bd8512014b07e.svg",
      "fullname": "Yizhao Gao",
      "name": "LongMountain",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.09790",
      "authors": [
        {
          "_id": "684a33989b38e1e5a33a6804",
          "name": "Zhenran Xu",
          "hidden": false
        },
        {
          "_id": "684a33989b38e1e5a33a6805",
          "name": "Yiyu Wang",
          "hidden": false
        },
        {
          "_id": "684a33989b38e1e5a33a6806",
          "name": "Xue Yang",
          "hidden": false
        },
        {
          "_id": "684a33989b38e1e5a33a6807",
          "name": "Longyue Wang",
          "hidden": false
        },
        {
          "_id": "684a33989b38e1e5a33a6808",
          "name": "Weihua Luo",
          "hidden": false
        },
        {
          "_id": "684a33989b38e1e5a33a6809",
          "name": "Kaifu Zhang",
          "hidden": false
        },
        {
          "_id": "684a33989b38e1e5a33a680a",
          "name": "Baotian Hu",
          "hidden": false
        },
        {
          "_id": "684a33989b38e1e5a33a680b",
          "name": "Min Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-11T14:35:15.000Z",
      "submittedOnDailyAt": "2025-06-12T00:38:07.422Z",
      "title": "ComfyUI-R1 : Révision du modèle d'inférence pour la génération de flux de travail",
      "submittedOnDailyBy": {
        "_id": "639c379cdb7c5f35004066cb",
        "avatarUrl": "/avatars/3e435506ee85aa7d2d0ec2174a07462f.svg",
        "isPro": false,
        "fullname": "Zhenran Xu",
        "user": "imryanxu",
        "type": "user"
      },
      "summary": "Le contenu généré par l'IA a évolué depuis des processus de travail modulaires dans un seul modèle vers la possibilité de personnaliser des processus créatifs dans des plateformes comme ComfyUI. Cependant, pour créer des processus de travail efficaces, il est nécessaire une grande connaissance professionnelle pour ajuster de nombreux composants spécialisés. Cela peut imposer une courbe d'apprentissage rapide aux utilisateurs. Pour faire face à ces défis, nous présentons ComfyUI-R1, le premier modèle d'inférence grand à utiliser pour automatiser des processus de travail. Nous avons construit un ensemble de données de processus de travail de 4K, qui inclut des données de CoT (chaîne de raisonnement) pour la sélection de nœuds, la planification des processus de travail et la représentation des processus de travail au niveau de code. ComfyUI-R1 est entraîné à travers un cadre de deux étapes : (1) ajustement de CoT pour adapter le modèle à l'espace de ComfyUI ; (2) après le fine-tuning, une entraînement de reinforcement avec une combinaison de règles et de domaine pour stimuler la capacité d'inférence. Les résultats expérimentaux montrent que notre modèle de 7 milliards de paramètres atteint un rendement de 97% en formation, des rendements élevés et des scores F1 au niveau de nœud et de graphe, surpassant significativement les méthodes précédentes et les modèles de pointe comme GPT-4o et la série Claude. Une analyse supplémentaire souligne l'importance centrale du processus d'inférence et l'avantage de convertir des processus de travail en code. La comparaison qualitative révèle les avantages de notre méthode pour des processus complexes de génération et le potentiel de CoT dans la création artistique avec l'IA.",
      "upvotes": 14,
      "discussionId": "684a33989b38e1e5a33a680c",
      "projectPage": "https://github.com/AIDC-AI/ComfyUI-Copilot",
      "githubRepo": "https://github.com/AIDC-AI/ComfyUI-Copilot",
      "ai_summary": "ComfyUI-R1, a large reasoning model for automated workflow generation, demonstrates superior performance in creating AI art workflows through long chain-of-thought reasoning and reinforcement learning.",
      "ai_keywords": [
        "modular workflows",
        "ComfyUI",
        "large reasoning model",
        "automated workflow generation",
        "chain-of-thought (CoT) reasoning",
        "node selection",
        "workflow planning",
        "code-level workflow representation",
        "CoT fine-tuning",
        "reinforcement learning",
        "fine-grained rule-metric hybrid reward",
        "format validity",
        "structural integrity",
        "node-level fidelity",
        "GPT-4o",
        "Claude series",
        "pass rate",
        "node-level F1 scores",
        "graph-level F1 scores",
        "intricate workflows",
        "diverse nodes",
        "qualitative comparison",
        "AI art creation"
      ]
    },
    "publishedAt": "2025-06-11T10:35:15.000Z",
    "title": "ComfyUI-R1: Exploring Reasoning Models for Workflow Generation",
    "summary": "AI-generated content has evolved from monolithic models to modular workflows,\nparticularly on platforms like ComfyUI, enabling customization in creative\npipelines. However, crafting effective workflows requires great expertise to\norchestrate numerous specialized components, presenting a steep learning curve\nfor users. To address this challenge, we introduce ComfyUI-R1, the first large\nreasoning model for automated workflow generation. Starting with our curated\ndataset of 4K workflows, we construct long chain-of-thought (CoT) reasoning\ndata, including node selection, workflow planning, and code-level workflow\nrepresentation. ComfyUI-R1 is trained through a two-stage framework: (1) CoT\nfine-tuning for cold start, adapting models to the ComfyUI domain; (2)\nreinforcement learning for incentivizing reasoning capability, guided by a\nfine-grained rule-metric hybrid reward, ensuring format validity, structural\nintegrity, and node-level fidelity. Experiments show that our 7B-parameter\nmodel achieves a 97\\% format validity rate, along with high pass rate,\nnode-level and graph-level F1 scores, significantly surpassing prior\nstate-of-the-art methods that employ leading closed-source models such as\nGPT-4o and Claude series. Further analysis highlights the critical role of the\nreasoning process and the advantage of transforming workflows into code.\nQualitative comparison reveals our strength in synthesizing intricate workflows\nwith diverse nodes, underscoring the potential of long CoT reasoning in AI art\ncreation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09790.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "639c379cdb7c5f35004066cb",
      "avatarUrl": "/avatars/3e435506ee85aa7d2d0ec2174a07462f.svg",
      "fullname": "Zhenran Xu",
      "name": "imryanxu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.09003",
      "authors": [
        {
          "_id": "6848eed742e4f9106973f2cf",
          "name": "Lei Zhang",
          "hidden": false
        },
        {
          "_id": "6848eed742e4f9106973f2d0",
          "name": "Jiaxi Yang",
          "hidden": false
        },
        {
          "_id": "6848eed742e4f9106973f2d1",
          "name": "Min Yang",
          "hidden": false
        },
        {
          "_id": "6848eed742e4f9106973f2d2",
          "name": "Jian Yang",
          "hidden": false
        },
        {
          "_id": "6848eed742e4f9106973f2d3",
          "name": "Mouxiang Chen",
          "hidden": false
        },
        {
          "_id": "6848eed742e4f9106973f2d4",
          "name": "Jiajun Zhang",
          "hidden": false
        },
        {
          "_id": "6848eed742e4f9106973f2d5",
          "name": "Zeyu Cui",
          "hidden": false
        },
        {
          "_id": "6848eed742e4f9106973f2d6",
          "name": "Binyuan Hui",
          "hidden": false
        },
        {
          "_id": "6848eed742e4f9106973f2d7",
          "name": "Junyang Lin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-10T17:23:33.000Z",
      "submittedOnDailyAt": "2025-06-12T00:17:18.390Z",
      "title": "SWE-Flow : Synthèse de données de développement de logiciel dirigée par les tests",
      "submittedOnDailyBy": {
        "_id": "64c38871f9cd765462fa1a17",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c38871f9cd765462fa1a17/yuIlVcqeDlQVKsUF8uEl3.jpeg",
        "isPro": false,
        "fullname": "Lei Zhang",
        "user": "Lemoncoke",
        "type": "user"
      },
      "summary": "Nous avons introduit **SWE-Flow**. C'est un nouveau cadre de travail pour la synthèse de données basé sur le développement dirigé par tests (TDD). Les données d'ingénierie logicielle existantes se basent sur des problèmes humains présentés, mais **SWE-Flow** infère automatiquement les étapes de développement progressifs directement à partir des tests unitaires. Les tests unitaires contiennent des niveaux élevés de requis, donc le cœur de **SWE-Flow** est la construction du graphe de dépendances temporelles (RDG). Celui-ci capture exactement les interactions entre fonctions pour générer un *plan de développement structuré par étapes*. À chaque étape, **SWE-Flow** génère un code partiel, les tests unitaires correspondants et les modifications de code nécessaires pour que le travail soit complètement vérifiable sous TDD. Avec cette approche, nous avons généré dans des projets réels de GitHub 16,061 données d'entraînement et 2,020 données de test pour créer le cadre de référence **SWE-Flow-Eval**. Nos expériences montrent que l'amélioration significative du rendement de codage basé sur TDD est réalisée en ajustant des modèles ouverts sur cet ensemble de données. Pour plus de recherche, nous publions sur [Github](https://github.com/Hambaobao/SWE-Flow) tous les codes, ensembles de données, modèles et images de Docker.",
      "upvotes": 13,
      "discussionId": "6848eed842e4f9106973f2d8",
      "githubRepo": "https://github.com/Hambaobao/SWE-Flow",
      "ai_summary": "A novel data synthesis framework, SWE-Flow, uses unit tests to automatically infer development steps and generate a structured schedule for Test-Driven Development (TDD), significantly improving the performance of open models fine-tuned on real-world projects.",
      "ai_keywords": [
        "Test-Driven Development (TDD)",
        "Runtime Dependency Graph (RDG)",
        "SWE-Flow",
        "unit tests",
        "development schedule",
        "SWE-Flow-Eval",
        "fine-tuning",
        "open model"
      ]
    },
    "publishedAt": "2025-06-10T13:23:33.000Z",
    "title": "SWE-Flow: Synthesizing Software Engineering Data in a Test-Driven Manner",
    "summary": "We introduce **SWE-Flow**, a novel data synthesis framework grounded in\nTest-Driven Development (TDD). Unlike existing software engineering data that\nrely on human-submitted issues, **SWE-Flow** automatically infers incremental\ndevelopment steps directly from unit tests, which inherently encapsulate\nhigh-level requirements. The core of **SWE-Flow** is the construction of a\nRuntime Dependency Graph (RDG), which precisely captures function interactions,\nenabling the generation of a structured, step-by-step *development schedule*.\nAt each step, **SWE-Flow** produces a partial codebase, the corresponding unit\ntests, and the necessary code modifications, resulting in fully verifiable TDD\ntasks. With this approach, we generated 16,061 training instances and 2,020\ntest instances from real-world GitHub projects, creating the **SWE-Flow-Eval**\nbenchmark. Our experiments show that fine-tuning open model on this dataset\nsignificantly improves performance in TDD-based coding. To facilitate further\nresearch, we release all code, datasets, models, and Docker images at\n[Github](https://github.com/Hambaobao/SWE-Flow).",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09003.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64c38871f9cd765462fa1a17",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c38871f9cd765462fa1a17/yuIlVcqeDlQVKsUF8uEl3.jpeg",
      "fullname": "Lei Zhang",
      "name": "Lemoncoke",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.09984",
      "authors": [
        {
          "_id": "684a49fa9b38e1e5a33a6884",
          "name": "Zhenzhi Wang",
          "hidden": false
        },
        {
          "_id": "684a49fa9b38e1e5a33a6885",
          "name": "Jiaqi Yang",
          "hidden": false
        },
        {
          "_id": "684a49fa9b38e1e5a33a6886",
          "name": "Jianwen Jiang",
          "hidden": false
        },
        {
          "_id": "684a49fa9b38e1e5a33a6887",
          "name": "Chao Liang",
          "hidden": false
        },
        {
          "_id": "684a49fa9b38e1e5a33a6888",
          "name": "Gaojie Lin",
          "hidden": false
        },
        {
          "_id": "684a49fa9b38e1e5a33a6889",
          "name": "Zerong Zheng",
          "hidden": false
        },
        {
          "_id": "684a49fa9b38e1e5a33a688a",
          "name": "Ceyuan Yang",
          "hidden": false
        },
        {
          "_id": "684a49fa9b38e1e5a33a688b",
          "name": "Dahua Lin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-11T17:57:09.000Z",
      "submittedOnDailyAt": "2025-06-12T02:02:32.983Z",
      "title": "Animation humaine multi-concept et voix qui se coordonnent dans l'ordre avec l'animation",
      "submittedOnDailyBy": {
        "_id": "6519346a186bc3b6997c1aaf",
        "avatarUrl": "/avatars/8981bd278962da50f9bbfb92c2abe2bf.svg",
        "isPro": false,
        "fullname": "Zhenzhi Wang",
        "user": "zhenzhiwang",
        "type": "user"
      },
      "summary": "Depuis le début jusqu'au début, l'animation humaine a connu un développement notable récemment grâce à l'utilisation de conditions riches en multimodalité (par exemple, texte, images, son). Cependant, de nombreux méthodes actuelles créent des animations d'entités uniques, injectant les conditions de manière globale et, lorsque plusieurs concepts sont présentés dans un même vidéo, elles ignorent l'interaction riche entre les personnes et les interactions entre personnes et objets. Cette hypothèse globale empêche un contrôle précis et un travail global sur plusieurs concepts, y compris des personnes ou des objets, et empêche également les applications. Dans cet article, nous renonçons à l'hypothèse d'entités uniques et introduisons un nouveau cadre de travail pour forcer une forte et propre union dans les traces spatio-temporelles de chaque identité à partir des conditions de modalité. En fournissant des références de plusieurs concepts, notre méthode utilise un générateur de masques pour estimer automatiquement l'information de rédaction de la page, alignant le code de couleur facial des images de bruit avec les références. De plus, nous injectons la condition sonore locale dans les zones correspondantes pour réaliser une correspondance de modalité qui coïncide avec les pages de plusieurs concepts. Ce design permet la génération de vidéos de haute qualité centrées sur la personne avec plusieurs concepts. A travers les résultats expérimentaux, nous démontrons l'efficacité de notre contrôle explicite de la rédaction de la page et nous testons l'efficacité relative des conditions riches en multimodalité par rapport aux méthodes existantes.",
      "upvotes": 9,
      "discussionId": "684a49fa9b38e1e5a33a688c",
      "ai_summary": "A new framework enables precise, per-identity control of multiple concepts in end-to-end human animation by enforcing region-specific binding of multi-modal conditions.",
      "ai_keywords": [
        "human animation",
        "multi-modal conditions",
        "mask predictor",
        "denoised video",
        "layout information",
        "local audio condition",
        "controllable multi-concept videos",
        "explicit layout control"
      ]
    },
    "publishedAt": "2025-06-11T13:57:09.000Z",
    "title": "InterActHuman: Multi-Concept Human Animation with Layout-Aligned Audio\n  Conditions",
    "summary": "End-to-end human animation with rich multi-modal conditions, e.g., text,\nimage and audio has achieved remarkable advancements in recent years. However,\nmost existing methods could only animate a single subject and inject conditions\nin a global manner, ignoring scenarios that multiple concepts could appears in\nthe same video with rich human-human interactions and human-object\ninteractions. Such global assumption prevents precise and per-identity control\nof multiple concepts including humans and objects, therefore hinders\napplications. In this work, we discard the single-entity assumption and\nintroduce a novel framework that enforces strong, region-specific binding of\nconditions from modalities to each identity's spatiotemporal footprint. Given\nreference images of multiple concepts, our method could automatically infer\nlayout information by leveraging a mask predictor to match appearance cues\nbetween the denoised video and each reference appearance. Furthermore, we\ninject local audio condition into its corresponding region to ensure\nlayout-aligned modality matching in a iterative manner. This design enables the\nhigh-quality generation of controllable multi-concept human-centric videos.\nEmpirical results and ablation studies validate the effectiveness of our\nexplicit layout control for multi-modal conditions compared to implicit\ncounterparts and other existing methods.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09984.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6519346a186bc3b6997c1aaf",
      "avatarUrl": "/avatars/8981bd278962da50f9bbfb92c2abe2bf.svg",
      "fullname": "Zhenzhi Wang",
      "name": "zhenzhiwang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.09937",
      "authors": [
        {
          "_id": "684a3d639b38e1e5a33a686d",
          "name": "Qiao Gu",
          "hidden": false
        },
        {
          "_id": "684a3d639b38e1e5a33a686e",
          "name": "Yuanliang Ju",
          "hidden": false
        },
        {
          "_id": "684a3d639b38e1e5a33a686f",
          "name": "Shengxiang Sun",
          "hidden": false
        },
        {
          "_id": "684a3d639b38e1e5a33a6870",
          "name": "Igor Gilitschenski",
          "hidden": false
        },
        {
          "_id": "684a3d639b38e1e5a33a6871",
          "name": "Haruki Nishimura",
          "hidden": false
        },
        {
          "_id": "684a3d639b38e1e5a33a6872",
          "name": "Masha Itkina",
          "hidden": false
        },
        {
          "_id": "684a3d639b38e1e5a33a6873",
          "name": "Florian Shkurti",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-11T16:59:13.000Z",
      "submittedOnDailyAt": "2025-06-12T01:08:05.216Z",
      "title": "SAFE : Modèle de Vision, Langue et Action pour la Détection de Pannes dans des Tâches Multiples",
      "submittedOnDailyBy": {
        "_id": "63d1df92f7f31a66a2d7292c",
        "avatarUrl": "/avatars/eb3339c1f2c82742b518b8a7f142e99a.svg",
        "isPro": false,
        "fullname": "Qiao Gu",
        "user": "guqiao",
        "type": "user"
      },
      "summary": "VISION-LANGUAGE-ACTION MODEL (VLAs) a le potentiel de montrer des actions de robot dans diverses tâches mais dans sa mise en œuvre dans de nouvelles tâches, son succès est limité. Ces politiques nécessitent un détecteur de défaillances qui permette au robot de s'arrêter, de reculer ou de demander une aide pour fonctionner de manière sécurisée. Cependant, actuellement, les détecteurs de défaillances ne sont pas généralisables pour détecter des défaillances dans de nouvelles tâches ou dans de nouveaux environnements. Cet article introduit le problème de détecter des défaillances dans plusieurs tâches et propose un détecteur de défaillances SAFE adapté pour des politiques générales comme les VLAs. SAFE est basé sur l'analyse des caractéristiques spécifiques des VLAs et découvre que les VLAs ont un haut niveau de connaissance sur le succès et le défaillance des tâches qui peuvent être appliquées à d'autres tâches. Dès cette perspective, SAFE est conçu pour prédire une unique échelle qui représente la probabilité de défaillance de la tâche à partir des caractéristiques internes des VLAs. SAFE est entraîné tant avec des succès que des défaillances et est évalué dans de nouvelles tâches. SAFE est compatible avec différentes architectures de politiques. Il a été testé rigoureusement sur OpenVLA, pi_0 et pi_0-FAST dans des environnements littéraires et réels. SAFE a atteint la meilleure performance de détection de défaillances, équilibrant la précision et le temps de détection face à différents limites de référence. Pour obtenir des résultats détaillés, consultez https://vla-safe.github.io/.",
      "upvotes": 3,
      "discussionId": "684a3d639b38e1e5a33a6874",
      "ai_summary": "SAFE is a failure detector for vision-language-action models that generalizes to unseen tasks by learning from high-level internal features of the models.",
      "ai_keywords": [
        "vision-language-action models",
        "VLAs",
        "multitask failure detection",
        "failure detector",
        "feature space",
        "high-level knowledge",
        "scalar prediction",
        "rollout",
        "conformal prediction"
      ]
    },
    "publishedAt": "2025-06-11T12:59:13.000Z",
    "title": "SAFE: Multitask Failure Detection for Vision-Language-Action Models",
    "summary": "While vision-language-action models (VLAs) have shown promising robotic\nbehaviors across a diverse set of manipulation tasks, they achieve limited\nsuccess rates when deployed on novel tasks out-of-the-box. To allow these\npolicies to safely interact with their environments, we need a failure detector\nthat gives a timely alert such that the robot can stop, backtrack, or ask for\nhelp. However, existing failure detectors are trained and tested only on one or\na few specific tasks, while VLAs require the detector to generalize and detect\nfailures also in unseen tasks and novel environments. In this paper, we\nintroduce the multitask failure detection problem and propose SAFE, a failure\ndetector for generalist robot policies such as VLAs. We analyze the VLA feature\nspace and find that VLAs have sufficient high-level knowledge about task\nsuccess and failure, which is generic across different tasks. Based on this\ninsight, we design SAFE to learn from VLA internal features and predict a\nsingle scalar indicating the likelihood of task failure. SAFE is trained on\nboth successful and failed rollouts, and is evaluated on unseen tasks. SAFE is\ncompatible with different policy architectures. We test it on OpenVLA, pi_0,\nand pi_0-FAST in both simulated and real-world environments extensively. We\ncompare SAFE with diverse baselines and show that SAFE achieves\nstate-of-the-art failure detection performance and the best trade-off between\naccuracy and detection time using conformal prediction. More qualitative\nresults can be found at https://vla-safe.github.io/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09937.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63d1df92f7f31a66a2d7292c",
      "avatarUrl": "/avatars/eb3339c1f2c82742b518b8a7f142e99a.svg",
      "fullname": "Qiao Gu",
      "name": "guqiao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.08001",
      "authors": [
        {
          "_id": "684a649f9b38e1e5a33a6895",
          "name": "Zeju Qiu",
          "hidden": false
        },
        {
          "_id": "684a649f9b38e1e5a33a6896",
          "name": "Simon Buchholz",
          "hidden": false
        },
        {
          "_id": "684a649f9b38e1e5a33a6897",
          "name": "Tim Z. Xiao",
          "hidden": false
        },
        {
          "_id": "684a649f9b38e1e5a33a6898",
          "name": "Maximilian Dax",
          "hidden": false
        },
        {
          "_id": "684a649f9b38e1e5a33a6899",
          "name": "Bernhard Schölkopf",
          "hidden": false
        },
        {
          "_id": "684a649f9b38e1e5a33a689a",
          "name": "Weiyang Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-09T17:59:34.000Z",
      "submittedOnDailyAt": "2025-06-12T03:55:47.829Z",
      "title": "La transformation orthogonale d'égalité pour la réformulation de l'entraînement de LLM",
      "submittedOnDailyBy": {
        "_id": "648905d1a15c43c791d4381f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/648905d1a15c43c791d4381f/GpqGBzsLiMHX0gWZEz3qn.jpeg",
        "isPro": false,
        "fullname": "Weiyang Liu",
        "user": "wy1iu",
        "type": "user"
      },
      "summary": "La raíz est une architecture de modèle de langage de l'intelligence artificielle (LLMs) qui dirige le développement rapide de l'IA, et son entraînement efficace et fiable est l'un des problèmes majeurs de la discipline. Pour aborder ce problème, nous proposons un nouvel algorithme de réparamétrisation : POET. En particulier, POET utilise deux matrices orthogonaux apprenables et une matrice de poids de nombres aléatoires fixes pour réparamétriser chaque neurone. Avec ce design, POET peut maintenir les caractéristiques spectrales de la matrice de poids de manière démontrée, ce qui permet une optimisation stable du fonctionnel objectif et une amélioration de la capacité de généralisation. De plus, nous avons développé des méthodes efficaces pour renforcer la flexibilité et l'échellabilité de POET. Les expériences réparties montrent l'efficacité et l'échellabilité de POET dans l'entraînement de LLMs.",
      "upvotes": 2,
      "discussionId": "684a649f9b38e1e5a33a689b",
      "projectPage": "https://spherelab.ai/poet/",
      "githubRepo": "https://github.com/Sphere-AI-Lab/poet",
      "ai_summary": "POET is a reParameterized training algorithm using Orthogonal Equivalence Transformation to optimize neurons in large language models, ensuring stable training and improved generalization.",
      "ai_keywords": [
        "POET",
        "reParameterized training algorithm",
        "Orthogonal Equivalence Transformation",
        "orthogonal matrices",
        "spectral properties",
        "weight matrices",
        "large language models",
        "generalization",
        "efficient approximations",
        "training",
        "scalability"
      ]
    },
    "publishedAt": "2025-06-09T13:59:34.000Z",
    "title": "Reparameterized LLM Training via Orthogonal Equivalence Transformation",
    "summary": "While large language models (LLMs) are driving the rapid advancement of\nartificial intelligence, effectively and reliably training these large models\nremains one of the field's most significant challenges. To address this\nchallenge, we propose POET, a novel reParameterized training algorithm that\nuses Orthogonal Equivalence Transformation to optimize neurons. Specifically,\nPOET reparameterizes each neuron with two learnable orthogonal matrices and a\nfixed random weight matrix. Because of its provable preservation of spectral\nproperties of weight matrices, POET can stably optimize the objective function\nwith improved generalization. We further develop efficient approximations that\nmake POET flexible and scalable for training large-scale neural networks.\nExtensive experiments validate the effectiveness and scalability of POET in\ntraining LLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08001.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "648905d1a15c43c791d4381f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/648905d1a15c43c791d4381f/GpqGBzsLiMHX0gWZEz3qn.jpeg",
      "fullname": "Weiyang Liu",
      "name": "wy1iu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.08900",
      "authors": [
        {
          "_id": "684a96ab9aebf043cf7bdb2e",
          "name": "José Morano",
          "hidden": false
        },
        {
          "_id": "684a96ab9aebf043cf7bdb2f",
          "name": "Botond Fazekas",
          "hidden": false
        },
        {
          "_id": "684a96ab9aebf043cf7bdb30",
          "name": "Emese Sükei",
          "hidden": false
        },
        {
          "_id": "684a96ab9aebf043cf7bdb31",
          "name": "Ronald Fecso",
          "hidden": false
        },
        {
          "_id": "684a96ab9aebf043cf7bdb32",
          "name": "Taha Emre",
          "hidden": false
        },
        {
          "_id": "684a96ab9aebf043cf7bdb33",
          "name": "Markus Gumpinger",
          "hidden": false
        },
        {
          "_id": "684a96ab9aebf043cf7bdb34",
          "name": "Georg Faustmann",
          "hidden": false
        },
        {
          "_id": "684a96ab9aebf043cf7bdb35",
          "name": "Marzieh Oghbaie",
          "hidden": false
        },
        {
          "_id": "684a96ab9aebf043cf7bdb36",
          "name": "Ursula Schmidt-Erfurth",
          "hidden": false
        },
        {
          "_id": "684a96ab9aebf043cf7bdb37",
          "name": "Hrvoje Bogunović",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/655b3383ed8df831286969f0/FuwagzdL2fBNhZyfwg22J.png"
      ],
      "publishedAt": "2025-06-10T15:25:55.000Z",
      "submittedOnDailyAt": "2025-06-12T07:31:36.181Z",
      "title": "MIRAGE : Modèle de base de DAMO et benchmark d'analyse d'images OCT avec des couches de cheveux et mystérieuse",
      "submittedOnDailyBy": {
        "_id": "655b3383ed8df831286969f0",
        "avatarUrl": "/avatars/38f9a73c6ec40ba0e00de5bffec03bc0.svg",
        "isPro": false,
        "fullname": "José Morano",
        "user": "j-morano",
        "type": "user"
      },
      "summary": "L'intelligence artificielle (IA) fonctionne comme une outil de base pour l'analyse d'images liées à la vision utilisée par les médecins cliniques. En particulier, elle est utilisée pour l'analyse d'images comme la Topologie Optique Cohérente (OCT). Cependant, le développement de modèles d'IA nécessite une annotation étendue et tend à présenter une perte de performance lorsqu'ils sont appliqués à des données indépendantes et non vues avant. Le Modèle de Base (FM) est un grand modèle d'IA, entraîné avec de grands ensembles de données non étiquetés, et montre un excellent rendement face à ces défis. Cependant, les FM actuels liés à la vision manquent particulièrement d'une validation très détaillée pour des tâches de segmentation et se concentrent uniquement sur un modèle d'images. Dans ce contexte, nous proposons MIRAGE, un nouveau FM multi-modèle pour l'analyse d'images OCT et d'optique de faisceau laser de détection (SLO). De plus, nous proposons également un nouveau cadre d'évaluation pour des tâches de classification et de segmentation OCT/SLO. La comparaison avec des modèles généraux, des FM professionnels et des méthodes de segmentation montre que MIRAGE montre un excellent rendement dans les deux tâches et constitue la base pour le développement d'un système IA puissant pour l'analyse d'images OCT d'optique de faisceau laser de détection. MIRAGE et le cadre d'évaluation sont disponibles pour l'utilisation publique : https://github.com/j-morano/MIRAGE.",
      "upvotes": 1,
      "discussionId": "684a96ab9aebf043cf7bdb38",
      "githubRepo": "https://github.com/j-morano/MIRAGE",
      "ai_summary": "MIRAGE, a multimodal foundation model, outperforms existing models in the classification and segmentation of OCT and SLO images, demonstrating its potential for robust AI in ophthalmologic image analysis.",
      "ai_keywords": [
        "foundational models",
        "multimodal models",
        "optical coherence tomography",
        "scanning laser ophthalmoscopy",
        "image segmentation",
        "evaluation benchmark"
      ]
    },
    "publishedAt": "2025-06-10T11:25:55.000Z",
    "title": "MIRAGE: Multimodal foundation model and benchmark for comprehensive\n  retinal OCT image analysis",
    "summary": "Artificial intelligence (AI) has become a fundamental tool for assisting\nclinicians in analyzing ophthalmic images, such as optical coherence tomography\n(OCT). However, developing AI models often requires extensive annotation, and\nexisting models tend to underperform on independent, unseen data. Foundation\nmodels (FMs), large AI models trained on vast unlabeled datasets, have shown\npromise in overcoming these challenges. Nonetheless, available FMs for\nophthalmology lack extensive validation, especially for segmentation tasks, and\nfocus on a single imaging modality. In this context, we propose MIRAGE, a novel\nmultimodal FM for the analysis of OCT and scanning laser ophthalmoscopy (SLO)\nimages. Additionally, we propose a new evaluation benchmark with OCT/SLO\nclassification and segmentation tasks. The comparison with general and\nspecialized FMs and segmentation methods shows the superiority of MIRAGE in\nboth types of tasks, highlighting its suitability as a basis for the\ndevelopment of robust AI systems for retinal OCT image analysis. Both MIRAGE\nand the evaluation benchmark are publicly available:\nhttps://github.com/j-morano/MIRAGE.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/655b3383ed8df831286969f0/FuwagzdL2fBNhZyfwg22J.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08900.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "655b3383ed8df831286969f0",
      "avatarUrl": "/avatars/38f9a73c6ec40ba0e00de5bffec03bc0.svg",
      "fullname": "José Morano",
      "name": "j-morano",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.09007",
      "authors": [
        {
          "_id": "6849516c42e4f9106973f4d1",
          "name": "Sophia Tang",
          "hidden": false
        },
        {
          "_id": "6849516c42e4f9106973f4d2",
          "name": "Yinuo Zhang",
          "hidden": false
        },
        {
          "_id": "6849516c42e4f9106973f4d3",
          "name": "Alexander Tong",
          "hidden": false
        },
        {
          "_id": "6849516c42e4f9106973f4d4",
          "name": "Pranam Chatterjee",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-10T17:29:48.000Z",
      "submittedOnDailyAt": "2025-06-12T01:32:31.298Z",
      "title": "Punto Shuringger Bridge Matching",
      "submittedOnDailyBy": {
        "_id": "64cd5b3f0494187a9e8b7c69",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/eo9HiGbvCxHQZ-QJB1Y_o.jpeg",
        "isPro": false,
        "fullname": "Pranam Chatterjee",
        "user": "pranamanam",
        "type": "user"
      },
      "summary": "Prédire la trajectoire intermédiaire entre la distribution initiale et la distribution finale est un problème essentiel dans la modélisation générative. Dans les méthodes actuelles, la modélisation d'un pas probabiliste est efficace pour apprendre le mapping entre ces deux distributions, et des techniques comme Flow Matching et Sinkhorn Bridging Matching ont été proposées. Cependant, ces méthodes sont limitées à la mobilité d'une seule modalité et ne permettent pas de comprendre l'évolution divergente depuis un seul point commun vers diverses résultats. Pour aborder ce problème, on présente BranchSBM (Branching Sinkhorn Bridging Matching). BranchSBM introduit un nouveau cadre de travail pour entraîner un Sinkhorn Bridging de type ramifié, où les processus temporels dépendants des champs de vitesse et l'évolution sont modélisés avec plusieurs paramètres, permettant l'intégration de ramifications au niveau de la population dans diverses distributions finales. BranchSBM permet de modéliser la navigation de surfaces multiples, la modélisation de ramifications cellulaires sous conditions uniformes d'ancestres, et la simulation de ramifications dans la réponse cellulaire, ce qui en fait un modèle plus expressif et essentiel pour ces tâches.",
      "upvotes": 0,
      "discussionId": "6849516d42e4f9106973f4d5",
      "ai_summary": "BranchSBM, a novel generative modeling framework, extends Schr\\\"odinger Bridge Matching to model branched stochastic paths and multi-path evolution from a single initial distribution to multiple outcomes.",
      "ai_keywords": [
        "flow matching",
        "Schr\\\"odinger Bridge Matching",
        "Branched Schr\\\"odinger Bridge Matching",
        "BranchSBM",
        "time-dependent velocity fields",
        "growth processes",
        "multi-path surface navigation",
        "cell fate bifurcations",
        "cellular responses to perturbations"
      ]
    },
    "publishedAt": "2025-06-10T13:29:48.000Z",
    "title": "Branched Schrödinger Bridge Matching",
    "summary": "Predicting the intermediate trajectories between an initial and target\ndistribution is a central problem in generative modeling. Existing approaches,\nsuch as flow matching and Schr\\\"odinger Bridge Matching, effectively learn\nmappings between two distributions by modeling a single stochastic path.\nHowever, these methods are inherently limited to unimodal transitions and\ncannot capture branched or divergent evolution from a common origin to multiple\ndistinct outcomes. To address this, we introduce Branched Schr\\\"odinger Bridge\nMatching (BranchSBM), a novel framework that learns branched Schr\\\"odinger\nbridges. BranchSBM parameterizes multiple time-dependent velocity fields and\ngrowth processes, enabling the representation of population-level divergence\ninto multiple terminal distributions. We show that BranchSBM is not only more\nexpressive but also essential for tasks involving multi-path surface\nnavigation, modeling cell fate bifurcations from homogeneous progenitor states,\nand simulating diverging cellular responses to perturbations.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09007.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64cd5b3f0494187a9e8b7c69",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/eo9HiGbvCxHQZ-QJB1Y_o.jpeg",
      "fullname": "Pranam Chatterjee",
      "name": "pranamanam",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  }
]