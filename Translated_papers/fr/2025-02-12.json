[
  {
    "paper": {
      "id": "2502.06807",
      "authors": [
        {
          "_id": "67ac1b080686a1e0690741ce",
          "name": "OpenAI",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741d0",
          "name": "Ahmed El-Kishky",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741d1",
          "name": "Alexander Wei",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741d2",
          "name": "Andre Saraiva",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741d3",
          "name": "Borys Minaev",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741d4",
          "name": "Daniel Selsam",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741d5",
          "name": "David Dohan",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741d6",
          "name": "Francis Song",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741d7",
          "name": "Hunter Lightman",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741d8",
          "name": "Ignasi Clavera",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741d9",
          "name": "Jakub Pachocki",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741da",
          "name": "Jerry Tworek",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741db",
          "name": "Lorenz Kuhn",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741dc",
          "name": "Lukasz Kaiser",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741dd",
          "name": "Mark Chen",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741de",
          "name": "Max Schwarzer",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741df",
          "name": "Mostafa Rohaninejad",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741e0",
          "name": "Nat McAleese",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741e1",
          "name": "o3 contributors",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741e2",
          "name": "Oleg Mürk",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741e3",
          "name": "Rhythm Garg",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741e4",
          "name": "Rui Shu",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741e5",
          "name": "Szymon Sidor",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741e6",
          "name": "Vineet Kosaraju",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741e7",
          "name": "Wenda Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-03T23:00:15.000Z",
      "title": "Compétition de Programmation et Modèles de Logique à Grande Échelle",
      "summary": "Nous montrons que l'apprentissage par renforcement appliqué à des modèles de langue grands (LLMs) améliore considérablement le rendement des codes complexes et des tâches d'inférence. De plus, en comparant les points de départ initials des modèles d'inférence généraux comme o1 et o3 de OpenAI, et en comparant l'aire des systèmes qui utilisent des stratégies d'inférence conçues pour concourir à l'Olympiade Internationale de l'Informatique (IOI) de 2024, comme o1-ioi, qui utilise des stratégies manuelles. Dans l'IOI de 2024, o1-ioi a concurré directement et a occupé 49% des temps de test. Sous conditions de concurrence plus lenientes, o1-ioi a obtenu un or. Cependant, lorsque nous avons évalué le modèle plus tard, le modèle o3 pourrait atteindre un or sans utiliser des stratégies manuelles ni des conditions de concurrence relaxées. Nos résultats de recherche montrent que, comme o1-ioi, un pipeline spécialisé peut apporter de bons résultats, tandis que le modèle général étendu comme o3 peut dépasser ces résultats sans utiliser des stratégies d'inférence manuelles. En particulier, le modèle o3 a obtenu un or au IOI de 2024 et a obtenu une classification élite sur Codeforces. En général, ces résultats indiquent que l'extension de l'apprentissage par renforcement général peut offrir une voie forte dans le domaine de la technologie de l'intelligence artificielle d'inférence, sans dépendre de techniques spécifiques pour un domaine.",
      "upvotes": 28,
      "discussionId": "67ac1b090686a1e069074208"
    },
    "publishedAt": "2025-02-11T22:53:19.310Z",
    "title": "Competitive Programming with Large Reasoning Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06807.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6042
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.07316",
      "authors": [
        {
          "_id": "67ac0ab720e98bddc5c19fed",
          "name": "Junlong Li",
          "hidden": false
        },
        {
          "_id": "67ac0ab720e98bddc5c19fee",
          "name": "Daya Guo",
          "hidden": false
        },
        {
          "_id": "67ac0ab720e98bddc5c19fef",
          "name": "Dejian Yang",
          "hidden": false
        },
        {
          "_id": "67ac0ab720e98bddc5c19ff0",
          "name": "Runxin Xu",
          "hidden": false
        },
        {
          "_id": "67ac0ab720e98bddc5c19ff1",
          "name": "Yu Wu",
          "hidden": false
        },
        {
          "_id": "67ac0ab720e98bddc5c19ff2",
          "name": "Junxian He",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-11T07:26:50.000Z",
      "title": "CodeI/O : Compression de motifs de cause par prédiction d'entrée et de sortie de code",
      "summary": "La raison est une capacité de base des modèles de langage grands. L'étude de la semaine dernière se concentrait sur l'amélioration des compétences spécialisées en mathématiques ou en génération de code, mais l'amélioration du rendement dans les tâches de raisonnement de diverses raisons était rare et difficile en raison des données d'entraînement rares et continues. Pour résoudre ces problèmes, nous proposons un nouvel approche. Cette approche sélectionne des motifs de raisonnement de raisons uniques de manière systématique basée sur le contexte du code. Cela s'appelle CodeI/O. Cette méthode transforme le code original en un format de prédiction d'entrée/sortie de code. En utilisant un modèle qui prédit l'entrée/sortie à partir du code et des cas de test fournis de nature, on entraîne pour apprendre des raisonnements de type \"Chain-of-Thought\" et exposer les éléments de base de raisonnement général. Cela permet de maintenir la structure de raisonnement syntaxiquement indépendante de la codification et de préserver la précision des procédures. Les résultats des expériences montrent un amélioration constante dans les tâches de raisonnement de signes, de science, de logique, de mathématiques et de nombres, ainsi que de connaissances communes. Pour confirmer chaque prédiction, le code est exécuté avec des sorties ou entrées prédites qui correspondent aux réalités, et le \"Chain-of-Thought\" est modifié plusieurs fois pour implémenter CodeI/O++ et obtenir un rendement plus élevé. Les données et le modèle sont disponibles sur https://github.com/hkust-nlp/CodeIO.",
      "upvotes": 14,
      "discussionId": "67ac0ab820e98bddc5c1a039"
    },
    "publishedAt": "2025-02-11T23:00:20.080Z",
    "title": "CodeI/O: Condensing Reasoning Patterns via Code Input-Output Prediction",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07316.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "621e40ac944c7e36aaec2369",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/621e40ac944c7e36aaec2369/Yj-FJRWps3rvsS_B2bnKo.jpeg",
      "fullname": "Junlong Li",
      "name": "lockon",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.07701",
      "authors": [
        {
          "_id": "67ac23166def89f9aae56abd",
          "name": "Hongwei Yi",
          "hidden": false
        },
        {
          "_id": "67ac23166def89f9aae56abe",
          "name": "Shitong Shao",
          "hidden": false
        },
        {
          "_id": "67ac23166def89f9aae56abf",
          "user": {
            "_id": "66015e8aa4d296af07de538e",
            "avatarUrl": "/avatars/a1295c631cc2646282c545859975ce4c.svg",
            "isPro": false,
            "fullname": "Ye",
            "user": "Owen777",
            "type": "user"
          },
          "name": "Tian Ye",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-12T09:16:12.141Z",
          "hidden": false
        },
        {
          "_id": "67ac23166def89f9aae56ac0",
          "name": "Jiantong Zhao",
          "hidden": false
        },
        {
          "_id": "67ac23166def89f9aae56ac1",
          "name": "Qingyu Yin",
          "hidden": false
        },
        {
          "_id": "67ac23166def89f9aae56ac2",
          "name": "Michael Lingelbach",
          "hidden": false
        },
        {
          "_id": "67ac23166def89f9aae56ac3",
          "name": "Li Yuan",
          "hidden": false
        },
        {
          "_id": "67ac23166def89f9aae56ac4",
          "name": "Yonghong Tian",
          "hidden": false
        },
        {
          "_id": "67ac23166def89f9aae56ac5",
          "name": "Enze Xie",
          "hidden": false
        },
        {
          "_id": "67ac23166def89f9aae56ac6",
          "name": "Daquan Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-11T16:58:15.000Z",
      "title": "Magic 1 à 1 : Générez un clip vidéo de 1 minute en 1 minute.",
      "summary": "Dans ce rapport technique, nous présentons le modèle de génération de vidéo \"Magic 1-For-1 (Magic141)\" pour l'utilisation efficace de la mémoire et l'optimisation du temps d'inférence. L'idée principale est simple : la génération de vidéo à partir de texte est divisée en deux tâches simples, implémentées par des processus d'apprentissage différents. Il s'agit de la génération d'images à partir de texte et de la génération de vidéo à partir d'images. En raison de l'utilisation du même algorithme d'optimisation, nous avons confirmé que la génération de vidéo à partir d'images est plus simple et converge plus rapidement que la génération de vidéo à partir de texte. De plus, nous avons étudié des astuces d'optimisation pour réduire les coûts de calcul d'entraînement du modèle de génération de vidéo à partir d'images sur trois aspects : 1) l'injection de conditions multimodales anticipatives pour accélérer la convergence du modèle, 2) l'apprentissage de pas adversaires pour accélérer le temps d'inférence, et 3) l'optimisation du coût de la mémoire pendant l'inférence. Avec ces technologies, il est possible de générer un clip de 5 secondes de vidéo en 3 secondes. En appliquant une fenêtre de temps de test, on peut générer un vidéo de 1 minute en moins d'une minute, avec un grand améliorament en qualité visuelle et en comportement, et le temps moyen pour générer un clip de 1 seconde de vidéo est réduit à moins de 1 seconde. Avec ces outils, nous cherchons à trouver le meilleur compromis entre le coût informatique et la qualité de la vidéo, ce qui nous attend qu'il devienne un guide d'exploration ouvert. Les codes et les poids du modèle sont disponibles sur https://github.com/DA-Group-PKU/Magic-1-For-1.",
      "upvotes": 11,
      "discussionId": "67ac23186def89f9aae56b69"
    },
    "publishedAt": "2025-02-11T23:27:13.769Z",
    "title": "Magic 1-For-1: Generating One Minute Video Clips within One Minute",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07701.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6042
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.03492",
      "authors": [
        {
          "_id": "67a5a8e595df68b0a167c298",
          "user": {
            "_id": "622f103fc78da4c7ebd7c887",
            "avatarUrl": "/avatars/b0c7cd29835d92c2cd584947fcd5d520.svg",
            "isPro": false,
            "fullname": "Xie",
            "user": "Zhihui",
            "type": "user"
          },
          "name": "Zhihui Xie",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-12T09:17:02.682Z",
          "hidden": false
        },
        {
          "_id": "67a5a8e595df68b0a167c299",
          "name": "Jie chen",
          "hidden": false
        },
        {
          "_id": "67a5a8e595df68b0a167c29a",
          "name": "Liyu Chen",
          "hidden": false
        },
        {
          "_id": "67a5a8e595df68b0a167c29b",
          "name": "Weichao Mao",
          "hidden": false
        },
        {
          "_id": "67a5a8e595df68b0a167c29c",
          "name": "Jingjing Xu",
          "hidden": false
        },
        {
          "_id": "67a5a8e595df68b0a167c29d",
          "name": "Lingpeng Kong",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-05T02:18:46.000Z",
      "title": "« Guide d'évaluation de modèles de langage par apprentissage par renforcement »",
      "summary": "Apprendre à critiquer et à améliorer les LMC est important pour la construction de systèmes continuément améliorables, mais il est limité par sa capacité à faire des jugements précis et à proposer des conseils opérationnels. Dans cette étude, nous examinons la critique de la génération de code par les LMC et proposons le cadre de travail CTRL (Apprentissage par répétition pour l'apprentissage critique). CTRL entraîne un modèle de critique qui génère des rétroactions pour améliorer le rendement d'un générateur fixe, sans supervision humaine. Nos résultats montrent que les critiques entraînées avec CTRL améliorent significativement les taux de passage des modèles générateurs basiques et renforcés, et peuvent inhiber les erreurs continuelles. De plus, ces modèles de critique fonctionnent comme des modèles de récompense précise, permettant d'escaler les tests et d'atteindre un amélioration relative de 106,1% du rendement sur les benchmarks de code difficile.",
      "upvotes": 10,
      "discussionId": "67a5a8e695df68b0a167c2c6"
    },
    "publishedAt": "2025-02-11T23:55:37.671Z",
    "title": "Teaching Language Models to Critique via Reinforcement Learning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.03492.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "622f103fc78da4c7ebd7c887",
      "avatarUrl": "/avatars/b0c7cd29835d92c2cd584947fcd5d520.svg",
      "fullname": "Xie",
      "name": "Zhihui",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.06329",
      "authors": [
        {
          "_id": "67ab4174757d2eb190af0375",
          "user": {
            "_id": "621d6f532165dc431641e438",
            "avatarUrl": "/avatars/56ccef10a8426d7160ef3586a771bd63.svg",
            "isPro": false,
            "fullname": "Kiran Kamble",
            "user": "kiranr",
            "type": "user"
          },
          "name": "Kiran Kamble",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-12T09:16:55.367Z",
          "hidden": false
        },
        {
          "_id": "67ab4174757d2eb190af0376",
          "name": "Melisa Russak",
          "hidden": false
        },
        {
          "_id": "67ab4174757d2eb190af0377",
          "name": "Dmytro Mozolevskyi",
          "hidden": false
        },
        {
          "_id": "67ab4174757d2eb190af0378",
          "user": {
            "_id": "6320a906a023aad6a7670e99",
            "avatarUrl": "/avatars/48071559b0c7660bf6861cfe008b3006.svg",
            "isPro": false,
            "fullname": "Muayad Sayed Ali",
            "user": "muayad",
            "type": "user"
          },
          "name": "Muayad Ali",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-12T09:16:53.157Z",
          "hidden": false
        },
        {
          "_id": "67ab4174757d2eb190af0379",
          "name": "Mateusz Russak",
          "hidden": false
        },
        {
          "_id": "67ab4174757d2eb190af037a",
          "name": "Waseem AlShikh",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-10T10:29:28.000Z",
      "title": "Un événement inattendu : Finance's FailSafe dans le contexte des questions basées sur le langage\n\n**Note :** La traduction a été effectuée en maintenant la profondeur et la précision du texte original.",
      "summary": "Il propose un cadre de référence pour le secteur financier appelé \"FailSafeQA\". Ce cadre de référence est conçu pour mesurer la robustesse et la compréhension du contexte d'un système de réponses de questions basé sur des modèles de langage (LLM) face à six interactions avec l'interface humaine. Il se concentre sur deux cas principaux : les fichiers de requêtes et les fichiers de contexte. Dans le cas des fichiers de requêtes, les requêtes originales sont modifiées pour mettre en œuvre des changements en spécialité, en complétude et en précision du langage. Dans les fichiers de contexte, des écritures de documents détériorés, non pertinents ou vides sont simulées. Qwen2.5-72B-Instruct est utilisé pour adopter l'approche d'un modèle de langage comme juré, définissant des scores de robustesse, de contexte et de conformité pour calculer ces valeurs sur 24 modèles de test. Les résultats montrent que certains modèles ont une excellente capacité à atténuer l'influence des données d'entraînement, mais ils démontrent également la nécessité d'équilibrer la capacité à répondre de manière robuste et à éviter la fabrication d'information. En particulier, le modèle avec la meilleure conformité, Palmyra-Fin-128k-Instruct, a maintenu son rendement de base mais a eu des difficultés à maintenir des prédictions robustes dans 17% des cas de test. Par contre, le modèle le plus robuste, OpenAI o3-mini, a fabriqué de l'information dans 41% des cas de test. Ces résultats montrent que bien que les modèles de haut rendement aient beaucoup de potentiel, ils ont encore de grandes opportunités d'amélioration. FailSafeQA joue un rôle crucial en tant que outil pour optimiser la confiance dans le développement de modèles de langage dans des applications financières. Le dataset est disponible sur la URL suivante : https://huggingface.co/datasets/Writer/FailSafeQA",
      "upvotes": 8,
      "discussionId": "67ab4175757d2eb190af03ca"
    },
    "publishedAt": "2025-02-12T02:51:41.003Z",
    "title": "Expect the Unexpected: FailSafe Long Context QA for Finance",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06329.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60e61b3969bd0df25c9375da",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1625692968400-noauth.jpeg",
      "fullname": "Melisa Russak",
      "name": "melisa",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 22
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.07617",
      "authors": [
        {
          "_id": "67ac1d68c29356f92ed772c5",
          "name": "Xiao Wang",
          "hidden": false
        },
        {
          "_id": "67ac1d68c29356f92ed772c6",
          "name": "Ibrahim Alabdulmohsin",
          "hidden": false
        },
        {
          "_id": "67ac1d68c29356f92ed772c7",
          "name": "Daniel Salz",
          "hidden": false
        },
        {
          "_id": "67ac1d68c29356f92ed772c8",
          "name": "Zhe Li",
          "hidden": false
        },
        {
          "_id": "67ac1d68c29356f92ed772c9",
          "name": "Keran Rong",
          "hidden": false
        },
        {
          "_id": "67ac1d68c29356f92ed772ca",
          "name": "Xiaohua Zhai",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-11T15:05:33.000Z",
      "title": "Modèle de langage visuel pour échelonner entre 100 milliards de données",
      "summary": "Nous examinons de manière démonstrative le potentiel de cette entreprise : l'entraînement préalable d'un modèle Vision-Langue à une échelle sans précédent avec 1000 milliards d'exemples. Dans cette échelle, le rendement du modèle s'accroit généralement jusqu'à saturation dans de nombreux classifieurs et de benchmarks de recherche axés sur l'Occident (par exemple, COCO Captions). Cependant, les tâches de diversité culturelle bénéficient encore plus de l'ampleur des 1000 milliards de données de l'internet. Cela est dû au concept de la \"long tail\". De plus, nous analysons la multilinguisme du modèle et obtenons des résultats pour des langues peu répandues. De plus, nous utilisons des filtres comme CLIP pour réduire la taille du jeu de données d'entraînement préalable, nous avons découvert la présence d'exemples négatifs qui affectent négativement. Cette taille de données est cruciale pour construire un système multilingue avec une diversité culturelle.",
      "upvotes": 8,
      "discussionId": "67ac1d6ac29356f92ed77354"
    },
    "publishedAt": "2025-02-11T23:03:08.578Z",
    "title": "Scaling Pre-training to One Hundred Billion Data for Vision Language Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07617.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6042
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.07374",
      "authors": [
        {
          "_id": "67ac1c6436464325ebe3c6e3",
          "name": "Dacheng Li",
          "hidden": false
        },
        {
          "_id": "67ac1c6436464325ebe3c6e4",
          "name": "Shiyi Cao",
          "hidden": false
        },
        {
          "_id": "67ac1c6436464325ebe3c6e5",
          "name": "Tyler Griggs",
          "hidden": false
        },
        {
          "_id": "67ac1c6436464325ebe3c6e6",
          "name": "Shu Liu",
          "hidden": false
        },
        {
          "_id": "67ac1c6436464325ebe3c6e7",
          "name": "Xiangxi Mo",
          "hidden": false
        },
        {
          "_id": "67ac1c6436464325ebe3c6e8",
          "name": "Shishir G. Patil",
          "hidden": false
        },
        {
          "_id": "67ac1c6436464325ebe3c6e9",
          "name": "Matei Zaharia",
          "hidden": false
        },
        {
          "_id": "67ac1c6436464325ebe3c6ea",
          "name": "Joseph E. Gonzalez",
          "hidden": false
        },
        {
          "_id": "67ac1c6436464325ebe3c6eb",
          "name": "Ion Stoica",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-11T08:48:48.000Z",
      "title": "Les LLMs peuvent comprendre pourquoi ils reçoivent un commande et ce qui importe est la structure, pas le contenu.",
      "summary": "Les modèles logiques de longue chaîne (LRMs) résolvent des problèmes logiques complexes en suivant des sources de chaînes longues de off sine. Cela comprend des techniques telles que la réflexion, le retour en arrière et l'auto-test. Cependant, les méthodes d'entraînement et les exigences en données pour extraire ces chaînes longues de off sine ne sont pas bien compris. Dans cette étude, nous avons découvert que les modèles de langage grand (LLMs) peuvent apprendre efficacement la logique de chaînes longues de off sine grâce à des ajustements normatifs efficaces de données (SFT) et des adaptateurs de paramètres efficaces de faible rendement (LoRA). Avec 17k échantillons d'entraînement de chaînes longues de off sine, le modèle Qwen2.5-32B-Instruct a présenté des améliorations notables dans des cadres d'évaluation mathématiques et de programmation, enregistrant un 56.7% (+40.0%) sur l'AIME 2024 et un 57.0% (+8.1%) sur LiveCodeBench. Ces résultats sont compétitifs avec ceux des modèles propres, comme o1-preview (44.6% et 59.1%). Un point important est que la structure des chaînes longues de off sine est cruciale pour le processus d'apprentissage, et le contenu des étapes logiques n'affecte pas significativement son rendement. L'ajout de perturbations liées au contenu (par exemple, l'entraînement avec des échantillons négatifs ou l'élimination de mots clés de logique) a un impact légèrement négatif. En contraste, des changements structurels qui détruisent la cohérence logique (comme le mélange ou l'élimination d'étapes logiques) affectent significativement la précision. Par exemple, l'entraînement avec des échantillons de chaînes longues de off sine qui incluent des réponses négatives entraîne une perte de précision de 3.2% par rapport à l'entraînement avec des échantillons complètement précis. Ces résultats profondent la compréhension de la façon dont se développent les capacités logiques des modèles de langage grand et identifient des points clés pour l'entraînement efficace des modèles logiques futurs. Cet article est une publication académique du modèle Sky-T1-32B-Preview publié précédemment. Le code est disponible sur https://github.com/NovaSky-AI/SkyThought.",
      "upvotes": 8,
      "discussionId": "67ac1c6536464325ebe3c723"
    },
    "publishedAt": "2025-02-11T22:58:37.585Z",
    "title": "LLMs Can Easily Learn to Reason from Demonstrations Structure, not content, is what matters!",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07374.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6042
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.03997",
      "authors": [
        {
          "_id": "67ac206214d5fe7767e7ec4e",
          "name": "Yu Yuan",
          "hidden": false
        },
        {
          "_id": "67ac206214d5fe7767e7ec4f",
          "user": {
            "_id": "63eb00a191a1b8ec4fbba2a9",
            "avatarUrl": "/avatars/0cc7cf9b6d05337603f700e0d592edf5.svg",
            "isPro": false,
            "fullname": "ShizhaoSun",
            "user": "ShizhaoSun",
            "type": "user"
          },
          "name": "Shizhao Sun",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-12T09:16:14.580Z",
          "hidden": false
        },
        {
          "_id": "67ac206214d5fe7767e7ec50",
          "name": "Qi Liu",
          "hidden": false
        },
        {
          "_id": "67ac206214d5fe7767e7ec51",
          "name": "Jiang Bian",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-06T11:57:14.000Z",
      "title": "Editor CAD : Spécification de l'emplacement pour l'insertion du travail de cadre de travail et de la synthèse des données d'entraînement automatique basée sur le texte.",
      "summary": "L'appui au design de calculs (CAD) est un élément essentiel dans diverses industries. L'édition de CAD basée sur le contexte a un grand potentiel pour automatiser les modifications dans les modèles CAD en fonction d'instructions textuelles, bien qu'il n'en ait pas encore été découvert. Les méthodes actuelles se concentrent principalement sur la génération de modifications de design ou sur la création de CAD basée sur le texte, mais elles manquent d'un support textuel ou traitent les modèles CAD existants comme des contraintes. On présente CAD-Editor, le premier cadre pour l'édition de CAD basée sur le contexte. Pour aborder la réponse aux données démocratiques nécessaires pour l'entraînement, on propose un pipeline de synthèse de données automatisée. Ce pipeline génère des pairs de modèles CAD originaux et édités en utilisant des modèles de modifications de design, et résume les différences en utilisant des grands modèles de langage visuel et linguistique (LVLMs). Pour aborder les caractéristiques complexes de l'édition de CAD basée sur le contexte, on propose un cadre de filtrage localisé. Ce cadre divise en deux sous-tâches : l'identification des zones nécessitant des modifications et l'ajout d'éditions appropriées dans ces zones. Des grands modèles de langage (LLMs) sont utilisés basés sur ces deux sous-tâches, exploitant leurs capacités de compréhension naturelle et de connaissance de CAD. Les expériences montrent que CAD-Editor atteint un rendement très efficace.",
      "upvotes": 6,
      "discussionId": "67ac206314d5fe7767e7ec98"
    },
    "publishedAt": "2025-02-11T23:16:28.213Z",
    "title": "CAD-Editor: A Locate-then-Infill Framework with Automated Training Data Synthesis for Text-Based CAD Editing",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.03997.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63eb00a191a1b8ec4fbba2a9",
      "avatarUrl": "/avatars/0cc7cf9b6d05337603f700e0d592edf5.svg",
      "fullname": "ShizhaoSun",
      "name": "ShizhaoSun",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.07527",
      "authors": [
        {
          "_id": "67ac1eaac61306b0ac95d2c6",
          "name": "Yingce Xia",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2c7",
          "name": "Peiran Jin",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2c8",
          "name": "Shufang Xie",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2c9",
          "name": "Liang He",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2ca",
          "name": "Chuan Cao",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2cb",
          "name": "Renqian Luo",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2cc",
          "name": "Guoqing Liu",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2cd",
          "name": "Yue Wang",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2ce",
          "name": "Zequn Liu",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2cf",
          "name": "Yuan-Jyue Chen",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2d0",
          "name": "Zekun Guo",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2d1",
          "name": "Yeqi Bai",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2d2",
          "name": "Pan Deng",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2d3",
          "name": "Yaosen Min",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2d4",
          "name": "Ziheng Lu",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2d5",
          "name": "Hongxia Hao",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2d6",
          "name": "Han Yang",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2d7",
          "name": "Jielan Li",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2d8",
          "name": "Chang Liu",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2d9",
          "name": "Jia Zhang",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2da",
          "name": "Jianwei Zhu",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2db",
          "name": "Kehan Wu",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2dc",
          "name": "Wei Zhang",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2dd",
          "name": "Kaiyuan Gao",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2de",
          "name": "Qizhi Pei",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2df",
          "name": "Qian Wang",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2e0",
          "name": "Xixian Liu",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2e1",
          "name": "Yanting Li",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2e2",
          "name": "Houtian Zhu",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2e3",
          "name": "Yeqing Lu",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2e4",
          "name": "Mingqian Ma",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2e5",
          "name": "Zun Wang",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2e6",
          "name": "Tian Xie",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2e7",
          "name": "Krzysztof Maziarz",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2e8",
          "name": "Marwin Segler",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2e9",
          "name": "Zhao Yang",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2ea",
          "name": "Zilong Chen",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2eb",
          "name": "Yu Shi",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2ec",
          "name": "Shuxin Zheng",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2ed",
          "name": "Lijun Wu",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2ee",
          "name": "Chen Hu",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2ef",
          "name": "Peggy Dai",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2f0",
          "name": "Tie-Yan Liu",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2f1",
          "name": "Haiguang Liu",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2f2",
          "name": "Tao Qin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-11T13:08:03.000Z",
      "title": "Interprète le langage de la nature pour pousser le découverte scientifique.",
      "summary": "Le modèle de base a eu un impact innovant sur le traitement du langage naturel et l'intelligence artificielle, améliorant significativement la capacité des machines pour comprendre et générer des langues humaines. Avec le succès de ces modèles, les chercheurs ont développé des modèles de base dans diverses domaines scientifiques, comme les molécules, les matériaux, les protéines, l'ADN et l'ARN. Cependant, ces modèles sont généralement entraînés de manière indépendante, ce qui limite leur capacité à intégrer différentes domaines scientifiques. En conséquence, des modèles qui peuvent représenter toutes les entités d'une domaine scientifique comme des expressions d'ordre ont été développés, et ceux-ci sont souvent nommés \"langage de la nature\" pour introduire le Nature Language Model (abréviation : NatureLM). Ce modèle est conçu pour être un modèle de base de science basé sur l'ordre, entraîné préalablement avec des données de multiples domaines scientifiques, ce qui permet d'effectuer diverses applications, notamment : (i) la génération et l'optimisation de molécules, protéines, RNA et matériaux via des instructions textuelles ; (ii) la génération et le design entre différents domaines ; et (iii) la traduction de SMILES en IUPAC et l'optimisation de la synthèse dans USPTO-50k, atteignant les meilleurs résultats. NatureLM offre une approche générale pour diverses tâches scientifiques, comme la découverte de médicaments (génération et optimisation visuelle, optimisation de l'ADMET, synthèse), le design de nouveaux matériaux, le développement de protéines thérapeutiques ou de nucléotides, entre autres. Des modèles de NatureLM de différents tailles (environ 100 millions, 800 millions et 4,670 millions de paramètres) ont été développés, et avec l'augmentation du taille, l'amélioration des résultats est clairement notable.",
      "upvotes": 6,
      "discussionId": "67ac1eabc61306b0ac95d346"
    },
    "publishedAt": "2025-02-11T23:10:26.895Z",
    "title": "NatureLM: Deciphering the Language of Nature for Scientific Discovery",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07527.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6042
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.06589",
      "authors": [
        {
          "_id": "67ac1d45e6f1e95ccf6de3b7",
          "user": {
            "_id": "6471bddd609ae9f56368f132",
            "avatarUrl": "/avatars/71a80127a01e662ab2790de0511326b6.svg",
            "isPro": true,
            "fullname": "Yuchen Zhuang",
            "user": "yczhuang",
            "type": "user"
          },
          "name": "Yuchen Zhuang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-02-12T04:02:14.866Z",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3b8",
          "name": "Jingfeng Yang",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3b9",
          "name": "Haoming Jiang",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3ba",
          "name": "Xin Liu",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3bb",
          "name": "Kewei Cheng",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3bc",
          "name": "Sanket Lokegaonkar",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3bd",
          "name": "Yifan Gao",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3be",
          "name": "Qing Ping",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3bf",
          "name": "Tianyi Liu",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3c0",
          "name": "Binxuan Huang",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3c1",
          "name": "Zheng Li",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3c2",
          "name": "Zhengyang Wang",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3c3",
          "name": "Pei Chen",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3c4",
          "name": "Ruijie Wang",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3c5",
          "name": "Rongzhi Zhang",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3c6",
          "name": "Nasser Zalmout",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3c7",
          "name": "Priyanka Nigam",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3c8",
          "name": "Bing Yin",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3c9",
          "name": "Chao Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-10T15:54:34.000Z",
      "title": "Hefey Stews : Amélioration continue des compétences de base des agents du modèle de langage de haut niveau grâce à l'apprentissage continu",
      "summary": "Hefaesthus Forju est le premier grand ensemble de données précédents pour renforcer les capacités de base des agents de LLM. Cet ensemble de données vise à renforcer la capacité de requêtes à des API, la logique interne et la planification, ainsi que l'adaptation au rétroaction de l'environnement. Hefaesthus Forju comprend 103B de données uniques pour les agents et enregistre 76,537 API. Ces API incluent des documents de logiciels qui fournissent des connaissances sur les fonctions de l'API et renforcent le processus de requêtes à des fonctions. De plus, grâce au continu de l'entraînement provoqué par cet ensemble de données, Hefaesthus dépasse les petits LLM ouverts de taille moyenne et atteint des niveaux comparables à ceux des LLM commerciales dans trois cadres de référence d'agents, démontrant à la fois la capacité de base des agents de LLM et l'efficacité de l'extensibilité pour de nouvelles tâches ou environnements.",
      "upvotes": 6,
      "discussionId": "67ac1d46e6f1e95ccf6de419"
    },
    "publishedAt": "2025-02-11T23:04:08.153Z",
    "title": "Hephaestus: Improving Fundamental Agent Capabilities of Large Language Models through Continual Pre-Training",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06589.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6471bddd609ae9f56368f132",
      "avatarUrl": "/avatars/71a80127a01e662ab2790de0511326b6.svg",
      "fullname": "Yuchen Zhuang",
      "name": "yczhuang",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.07508",
      "authors": [
        {
          "_id": "67ac2006a6b5a26040fc94f7",
          "name": "Yang Luo",
          "hidden": false
        },
        {
          "_id": "67ac2006a6b5a26040fc94f8",
          "name": "Xuanlei Zhao",
          "hidden": false
        },
        {
          "_id": "67ac2006a6b5a26040fc94f9",
          "name": "Mengzhao Chen",
          "hidden": false
        },
        {
          "_id": "67ac2006a6b5a26040fc94fa",
          "name": "Kaipeng Zhang",
          "hidden": false
        },
        {
          "_id": "67ac2006a6b5a26040fc94fb",
          "name": "Wenqi Shao",
          "hidden": false
        },
        {
          "_id": "67ac2006a6b5a26040fc94fc",
          "name": "Kai Wang",
          "hidden": false
        },
        {
          "_id": "67ac2006a6b5a26040fc94fd",
          "name": "Zhangyang Wang",
          "hidden": false
        },
        {
          "_id": "67ac2006a6b5a26040fc94fe",
          "name": "Yang You",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-11T12:22:35.000Z",
      "title": "Amélioration du langage dans Abidoo : Créez des vidéos de meilleure qualité gratuitement.",
      "summary": "Le génération de vidéo basée sur DiT a réalisé des résultats impressionnants, bien que l'étude sur l'expansion des modèles existants soit dans un état relativement difficile à atteindre. Dans cette étude, nous présentons un approche sans nécessité d'entraînement pour améliorer la continuité et la qualité de la vidéo générée basée sur DiT. Cette approche repose sur la conception clé de l'amélioration des relations interframis basée sur la distribution non diagonale du temps. Caractérisée par son conception simple, elle peut être facilement appliquée à presque tous les cadres de travail de génération de vidéo basée sur DiT, sans nécessité de stockage ou d'ajustements. Dans chaque modèle de génération de vidéo basée sur DiT, cette approche montre des améliorations claires en termes de cohérence temporelle et de qualité visuelle. Nous espérons que cette étude inspirera l'espoir pour l'étude future en matière d'expansion de la génération de vidéo.",
      "upvotes": 5,
      "discussionId": "67ac200ea6b5a26040fc9709"
    },
    "publishedAt": "2025-02-11T23:14:10.293Z",
    "title": "Enhance-A-Video: Better Generated Video for Free",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07508.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6042
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.04223",
      "authors": [
        {
          "_id": "67ac5e0d653d273eeaf25e59",
          "name": "Ilia Karmanov",
          "hidden": false
        },
        {
          "_id": "67ac5e0d653d273eeaf25e5a",
          "user": {
            "_id": "67ac5d85a19e34140ea1013b",
            "avatarUrl": "/avatars/e5b7446787dbbd17553dc9e11b58a0b4.svg",
            "isPro": false,
            "fullname": "Amala Sanjay Deshmukh",
            "user": "amalad",
            "type": "user"
          },
          "name": "Amala Sanjay Deshmukh",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-12T09:14:49.009Z",
          "hidden": false
        },
        {
          "_id": "67ac5e0d653d273eeaf25e5b",
          "name": "Lukas Voegtle",
          "hidden": false
        },
        {
          "_id": "67ac5e0d653d273eeaf25e5c",
          "name": "Philipp Fischer",
          "hidden": false
        },
        {
          "_id": "67ac5e0d653d273eeaf25e5d",
          "user": {
            "_id": "64c7a43e0d3d1b209df90b9c",
            "avatarUrl": "/avatars/1d0d2f129b799a72345b17fd5307aa5e.svg",
            "isPro": false,
            "fullname": "Kateryna Chumachenko",
            "user": "katerynaCh",
            "type": "user"
          },
          "name": "Kateryna Chumachenko",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-12T09:14:47.025Z",
          "hidden": false
        },
        {
          "_id": "67ac5e0d653d273eeaf25e5e",
          "name": "Timo Roman",
          "hidden": false
        },
        {
          "_id": "67ac5e0d653d273eeaf25e5f",
          "user": {
            "_id": "60098ca06e8ac78787773f85",
            "avatarUrl": "/avatars/be6539e5706bf07c71e553254c1751b5.svg",
            "isPro": false,
            "fullname": "Jarno Seppänen",
            "user": "jseppanen",
            "type": "user"
          },
          "name": "Jarno Seppänen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-12T09:14:51.062Z",
          "hidden": false
        },
        {
          "_id": "67ac5e0d653d273eeaf25e60",
          "name": "Jupinder Parmar",
          "hidden": false
        },
        {
          "_id": "67ac5e0d653d273eeaf25e61",
          "name": "Joseph Jennings",
          "hidden": false
        },
        {
          "_id": "67ac5e0d653d273eeaf25e62",
          "name": "Andrew Tao",
          "hidden": false
        },
        {
          "_id": "67ac5e0d653d273eeaf25e63",
          "name": "Karan Sapra",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-06T17:07:22.000Z",
      "title": "\"CLARIFICATION - EXTRACTION DU CONTENU ET SEQUENCE D'ORDRE INTEGRÉ\"",
      "summary": "La technologie de Reconnaissance Optique de Caractères (OCR) est largement utilisée pour extraire du texte d'images de documents, promouvant une digitalisation efficace et une recherche de données. Cependant, lorsqu'il s'agit de documents complexes, l'extraction seulement du texte n'est pas suffisante. Pour comprendre complètement la structure du document, il est nécessaire d'avoir des informations sur les formats, les formules, les tableaux, l'ordre de lecture de blocs et de colonnes sur plusieurs pages, et le détection d'éléments tels que les références et les captures d'images. Cette compréhension détaillée est essentielle pour des tâches telles que la recherche, la réponse à des questions dans les documents et l'élaboration de données d'entraînement pour des modèles de langage grands (LLMs) et des modèles de langage et de vision grands (VLMs). Pour relever ces défis, nous présentons 'Eclair', une outil spécialement conçu pour extraire du texte général de divers types de documents. Étant donné une image, 'Eclair' extrait du texte formaté de manière ordonnée et également extrait les bordures des blocs et les classes de signification correspondantes. Pour évaluer ces nouvelles capacités, nous présentons différentes bases de données humaines qui évaluent l'OCR au niveau du document et de la classification de signification. 'Eclair' atteint la précision la plus avancée dans ces benchmarks et dépasse les autres méthodes dans les principales métriques de composants. De plus, 'Eclair' a été évalué dans d'autres benchmarks existants, démontrant sa capacité d'adaptation à des critères d'évaluation différents et ses forces.",
      "upvotes": 3,
      "discussionId": "67ac5e0f653d273eeaf25eea"
    },
    "publishedAt": "2025-02-12T04:25:54.558Z",
    "title": "Éclair -- Extracting Content and Layout with Integrated Reading Order for Documents",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/60098ca06e8ac78787773f85/BfZ57W-gCoY32J60tx7dN.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04223.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60098ca06e8ac78787773f85",
      "avatarUrl": "/avatars/be6539e5706bf07c71e553254c1751b5.svg",
      "fullname": "Jarno Seppänen",
      "name": "jseppanen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.07445",
      "authors": [
        {
          "_id": "67ac216d602eb9ca8a517be6",
          "name": "Nurit Cohen-Inger",
          "hidden": false
        },
        {
          "_id": "67ac216d602eb9ca8a517be7",
          "name": "Yehonatan Elisha",
          "hidden": false
        },
        {
          "_id": "67ac216d602eb9ca8a517be8",
          "name": "Bracha Shapira",
          "hidden": false
        },
        {
          "_id": "67ac216d602eb9ca8a517be9",
          "name": "Lior Rokach",
          "hidden": false
        },
        {
          "_id": "67ac216d602eb9ca8a517bea",
          "name": "Seffi Cohen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-11T10:43:36.000Z",
      "title": "¡Dejamos de olvidar que l'évaluation des LLMs est comme celle d'un chamelan!",
      "summary": "Les modèles de langage grands (LLMs) obtiennent souvent de très bons scores sur des échantillons de test publics, mais ces scores élevés peuvent masquer la réalité de l'altération de la compréhension linguistique. Nous introduisons C-BOD (Détecteur de sur-ajustement dans le benchmark Chameleon). C-BOD est un cadre d'évaluation méta qui détecte le sur-ajustement des LLMs en transformant systématiquement les prompts du benchmark à travers la conversion de paramètres. En réorganisant l'entrée tout en maintenant le sens et en gardant les étiquettes, C-BOD évalue si le rendement du modèle est guidé par des patrons mémorisés. Avec 26 modèles de LLMs développés dans le cadre de MMLU, notre méthode montre une perte moyenne du rendement de 2,15% et une différence statistiquement significative dans 20 des 26 modèles. En particulier, les modèles avec une précision élevée présentent une perte de rendement plus importante en raison de la transformation, et les grands LLMs sont plus sensibles à la réorganisation, ce qui suggère une dépendance plus forte de patrons de prompts fixes. En contraste, les modèles de la famille Llama et les modèles avec une précision faible ne montrent pas de différences statistiquement significatives et suggèrent une dépendance moins forte de codes superficiels. De plus, C-BOD, conçu de manière indépendant des données ou des modèles, peut facilement être intégré dans le processus d'entraînement et renforce une compréhension linguistique plus robuste. Nos résultats montrent que la communauté doit prioritiser l'surprise et l'extensibilité dans l'évaluation des LLMs plutôt que les scores de classification.",
      "upvotes": 3,
      "discussionId": "67ac216e602eb9ca8a517c1d"
    },
    "publishedAt": "2025-02-11T23:22:50.454Z",
    "title": "Forget What You Know about LLMs Evaluations - LLMs are Like a Chameleon",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07445.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6731e56a07cf693a1104d2cb",
      "avatarUrl": "/avatars/46a3269a19c7e6bfb7004a5da9701459.svg",
      "fullname": "Seffi Cohen",
      "name": "seffico",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.04465",
      "authors": [
        {
          "_id": "67a953844ea315a67e02461d",
          "user": {
            "_id": "63195d0582e7eec0eac040e3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63195d0582e7eec0eac040e3/0tXOYkMfmv9e53zBWgqz7.png",
            "isPro": false,
            "fullname": "Luca Della Libera",
            "user": "lucadellalib",
            "type": "user"
          },
          "name": "Luca Della Libera",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T10:03:10.257Z",
          "hidden": false
        },
        {
          "_id": "67a953844ea315a67e02461e",
          "name": "Francesco Paissan",
          "hidden": false
        },
        {
          "_id": "67a953844ea315a67e02461f",
          "name": "Cem Subakan",
          "hidden": false
        },
        {
          "_id": "67a953844ea315a67e024620",
          "name": "Mirco Ravanelli",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-06T19:24:50.000Z",
      "title": "FocalCodec : Utilisation d'une réseau de modulation focalisée pour la codification de la voix à faible taux de bitrate",
      "summary": "Les modèles de langage nature ont progressé de manière innovante grâce à l'apprentissage préalable sur de grands ensembles de données, en appliquant des règles internes. Avec ce succès, les chercheurs ont étudié l'utilisation de codificateurs de réseau neuronal pour tokeniser des signaux acoustiques continus et appliquer cette technique à l'exploration des sons. Cependant, l'approche actuelle dépend du conception de plusieurs codebooks pour minimiser les pertes de taux de bit élevé, d'information sémantique ou de données acoustiques, ce qui augmente la complexité architecturale des tâches ultérieures et présente plusieurs limitations. Pour résoudre ces problèmes, nous présentons un codebook efficace de faible taux de bit. Ce codebook, basé sur le Focker Codier, utilise un seul codebook binétique pour compresser des signaux acoustiques dans un intervalle de 0.16 à 0.65 kbps. Le Focker Codier offre un rendement concurrentiel dans la récréation du son et la transformation du son vers des taux de bit plus bas que les technologies actuelles, et est efficace dans les signaux de langage multilingue et dans des environnements bruyants. Dans les évaluations de tâches ultérieures, le Focker Codier conserve suffisamment d'information sémantique et acoustique, et est adapté pour les modèles de génération. Les exemples de démonstration, les codes et les checkpoints sont disponibles sur https://lucadellalib.github.io/focalcodec-web/.",
      "upvotes": 2,
      "discussionId": "67a953854ea315a67e024659"
    },
    "publishedAt": "2025-02-12T01:31:44.368Z",
    "title": "FocalCodec: Low-Bitrate Speech Coding via Focal Modulation Networks",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04465.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63195d0582e7eec0eac040e3",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63195d0582e7eec0eac040e3/0tXOYkMfmv9e53zBWgqz7.png",
      "fullname": "Luca Della Libera",
      "name": "lucadellalib",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.07531",
      "authors": [
        {
          "_id": "67ac21acaa680a0f8782d273",
          "name": "Sixiao Zheng",
          "hidden": false
        },
        {
          "_id": "67ac21acaa680a0f8782d274",
          "name": "Zimian Peng",
          "hidden": false
        },
        {
          "_id": "67ac21acaa680a0f8782d275",
          "name": "Yanpeng Zhou",
          "hidden": false
        },
        {
          "_id": "67ac21acaa680a0f8782d276",
          "name": "Yi Zhu",
          "hidden": false
        },
        {
          "_id": "67ac21acaa680a0f8782d277",
          "name": "Hang Xu",
          "hidden": false
        },
        {
          "_id": "67ac21acaa680a0f8782d278",
          "name": "Xiangru Huang",
          "hidden": false
        },
        {
          "_id": "67ac21acaa680a0f8782d279",
          "name": "Yanwei Fu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-11T13:11:59.000Z",
      "title": "VidCRAFT3 : Création de vidéos à partir d'images qui contrôlent la caméra, l'objet et la lumière.",
      "summary": "Récemment, la technologie de génération de vidéos a démontré sa capacité à contrôler des éléments visuels tels que le mouvement de la caméra ou le mouvement d'objets, mais elle ne dispose pas de capacité pour contrôler de multiples éléments visuels simultanément, en fonction de l'efficacité des données et de la réseau. Dans cet article, nous présentons un nouveau cadre appelé VidCRAFT3, qui permet la génération de vidéos à partir d'images précises, en contrôlant simultanément le mouvement de la caméra, le mouvement d'objets et la direction de la lumière. Pour atteindre un contrôle plus détaillé de chaque élément visuel, nous proposons le transformer triple attention spectral, avec l'objectif d'intégrer de manière symétrique la direction de la lumière, le texte et les images. En raison du fait que de nombreux ensembles de données vidéo réelles ne disposent pas d'analyse de la direction de la lumière, nous avons construit un ensemble de données de qualité élevée, VideoLightingDirection (VLD), qui a été publié pour son utilisation. Cet ensemble de données inclut un analysage de la direction de la lumière et de différents objets extérieurs, ce qui permet à VidCRAFT3 de gérer efficacement la transmission et la réflexion de la lumière. De plus, pour éliminer la nécessité de données d'entraînement analysées pour tous les éléments visuels, nous proposons une stratégie d'entraînement en trois étapes. Les tests sur des ensembles de données de référence montrent que VidCRAFT3 est capable de générer du contenu vidéo de haute qualité, surpassant en précision et cohérence visuelle la technologie actuelle. Tout le code et les données sont disponibles pour son utilisation publique. Page du projet : https://sixiaozheng.github.io/VidCRAFT3/.",
      "upvotes": 2,
      "discussionId": "67ac21b2aa680a0f8782d3bd"
    },
    "publishedAt": "2025-02-11T23:21:13.452Z",
    "title": "VidCRAFT3: Camera, Object, and Lighting Control for Image-to-Video Generation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07531.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6042
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.07776",
      "authors": [
        {
          "_id": "67ac1f7851c7f3b53ffc4def",
          "name": "Chenchen Gu",
          "hidden": false
        },
        {
          "_id": "67ac1f7851c7f3b53ffc4df0",
          "name": "Xiang Lisa Li",
          "hidden": false
        },
        {
          "_id": "67ac1f7851c7f3b53ffc4df1",
          "name": "Rohith Kuditipudi",
          "hidden": false
        },
        {
          "_id": "67ac1f7851c7f3b53ffc4df2",
          "name": "Percy Liang",
          "hidden": false
        },
        {
          "_id": "67ac1f7851c7f3b53ffc4df3",
          "user": {
            "_id": "661595d1b3d0b21da55cde7d",
            "avatarUrl": "/avatars/ba3fa065536518637d21a5c46cee5dd1.svg",
            "isPro": false,
            "fullname": "Tatsu Hashimoto",
            "user": "thashim",
            "type": "user"
          },
          "name": "Tatsunori Hashimoto",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-02-12T04:11:36.912Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-11T18:58:04.000Z",
      "title": "Revue du Caché de Prompts pour l'API du Modèle de Langue",
      "summary": "Le caching de prompts est un élément principal qui provoque des changements temporels dans la dépendance des données dans les modèles de langage grands (LLMs), car les prompts cachés sont traités plus rapidement que ceux qui ne sont pas cachés. Cette différence de temps peut augmenter le risque d'attaques de canaux latéraux. Par exemple, si le cache est partagé entre utilisateurs, un attaquant peut identifier et utiliser les prompts cachés pour obtenir des informations sur les prompts d'autres utilisateurs. Étant donné que le caching de prompts peut être considéré comme une cause de la divulgation de l'information personnelle, la transparence dans les politiques de cache des entreprises qui offrent des API est cruciale. Avec cet enfoque, nous avons développé et effectué des évaluations statistiques pour détecter l'utilisation de caching de prompts dans les API de la réalité. Nous avons détecté l'utilisation d'un cache global entre utilisateurs chez 7 fournisseurs d'API et nous avons confirmé des risques potentiels de divulgation d'information personnelle liés aux prompts. De plus, la variation dans le temps provoquée par le caching de prompts peut également entraîner la divulgation d'information sur l'architecture du modèle. En particulier, nous avons démontré que le modèle intégré d'OpenAI ne disposait qu'd'un décodeur qui n'avait pas été publié précédemment.",
      "upvotes": 2,
      "discussionId": "67ac1f7851c7f3b53ffc4e1b"
    },
    "publishedAt": "2025-02-11T23:11:49.993Z",
    "title": "Auditing Prompt Caching in Language Model APIs",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07776.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6042
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.05932",
      "authors": [
        {
          "_id": "67ac4356401012b81050022a",
          "user": {
            "_id": "67ac430c4ab9207cc227d23f",
            "avatarUrl": "/avatars/59c499cc191e28a66ae917963c28ffb3.svg",
            "isPro": false,
            "fullname": "Tenglong Liu",
            "user": "LTL07",
            "type": "user"
          },
          "name": "Tenglong Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-12T09:15:09.627Z",
          "hidden": false
        },
        {
          "_id": "67ac4356401012b81050022b",
          "name": "Jianxiong Li",
          "hidden": false
        },
        {
          "_id": "67ac4356401012b81050022c",
          "name": "Yinan Zheng",
          "hidden": false
        },
        {
          "_id": "67ac4356401012b81050022d",
          "name": "Haoyi Niu",
          "hidden": false
        },
        {
          "_id": "67ac4356401012b81050022e",
          "name": "Yixing Lan",
          "hidden": false
        },
        {
          "_id": "67ac4356401012b81050022f",
          "name": "Xin Xu",
          "hidden": false
        },
        {
          "_id": "67ac4356401012b810500230",
          "name": "Xianyuan Zhan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-09T15:22:38.000Z",
      "title": "Dans l'espace des paramètres, combiner les technologies d'expansion et d'extension",
      "summary": "La humanité a acquis la capacité de récupérer des compétences connues pour résoudre de nouveaux problèmes et développer de nouvelles technologies dans le processus de résolution. Ce paradigme a étendu largement dans le développement de systèmes qui évoluent automatiquement les nouveaux problèmes. Cependant, les méthodes précédentes présentaient des limitations en termes d'efficacité d'entraînement lors de l'expansion des technologies, ce qui empêchait que les compétences connues ne soient pas complètement exploitées pour l'apprentissage de nouvelles tâches. Dans cet article, nous proposons un nouveau cadre de travail appelé PSEC (Paramètres d'Amélioration et de Combinaison). Ce cadre permet la gestion de bibliothèques de technologies, facilitant ainsi la résolution efficace de problèmes nouveaux et l'évolution des capacités des agents. Les bibliothèques PSEC permettent la configuration de portefeuilles d'évolution progressive des composants de base des technologies, en utilisant des ajustements microparamétriques. Cette structure combine des modules LoRA pour combiner directement les technologies et exploiter l'information de comparaison entre technologies pour programmer de nouvelles technologies de manière efficace. En se basant sur cela, nous proposons l'idée de modules contextuels qui peuvent être activés dynamiquement pour traiter des tâches nouvelles de manière conjointe. À travers la combinaison d'applications telles que multi-tâches, shift dynamique et shift continu, PSEC a démontré sa capacité d'utiliser efficacement les compétences connues pour résoudre de nouveaux problèmes et d'étendre les bibliothèques de technologies pour évoluer les capacités. Les résultats obtenus sur D4RL, DSRL benchmark et DeepMind Control Suite confirment que PSEC est capable de répandre et d'évoluer les capacités des agents grâce à l'utilisation efficace des compétences connues. Site web du projet : https://ltlhuuu.github.io/PSEC/",
      "upvotes": 0,
      "discussionId": "67ac435b401012b8105003dc"
    },
    "publishedAt": "2025-02-12T04:53:50.325Z",
    "title": "Skill Expansion and Composition in Parameter Space",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05932.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67ac430c4ab9207cc227d23f",
      "avatarUrl": "/avatars/59c499cc191e28a66ae917963c28ffb3.svg",
      "fullname": "Tenglong Liu",
      "name": "LTL07",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]