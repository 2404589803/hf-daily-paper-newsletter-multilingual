[
  {
    "paper": {
      "id": "2507.01951",
      "authors": [
        {
          "_id": "6868daac213f123a1f88b9c8",
          "name": "Zixiao Wang",
          "hidden": false
        },
        {
          "_id": "6868daac213f123a1f88b9c9",
          "name": "Yuxin Wang",
          "hidden": false
        },
        {
          "_id": "6868daac213f123a1f88b9ca",
          "name": "Xiaorui Wang",
          "hidden": false
        },
        {
          "_id": "6868daac213f123a1f88b9cb",
          "name": "Mengting Xing",
          "hidden": false
        },
        {
          "_id": "6868daac213f123a1f88b9cc",
          "name": "Jie Gao",
          "hidden": false
        },
        {
          "_id": "6868daac213f123a1f88b9cd",
          "name": "Jianjun Xu",
          "hidden": false
        },
        {
          "_id": "6868daac213f123a1f88b9ce",
          "name": "Guangcan Liu",
          "hidden": false
        },
        {
          "_id": "6868daac213f123a1f88b9cf",
          "name": "Chenhui Jin",
          "hidden": false
        },
        {
          "_id": "6868daac213f123a1f88b9d0",
          "name": "Zhuo Wang",
          "hidden": false
        },
        {
          "_id": "6868daac213f123a1f88b9d1",
          "name": "Shengzhuo Zhang",
          "hidden": false
        },
        {
          "_id": "6868daac213f123a1f88b9d2",
          "name": "Hongtao Xie",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/638700c723da90491eb72722/mRIN68aSkxejrT4UqCaKz.jpeg"
      ],
      "publishedAt": "2025-07-02T17:58:01.000Z",
      "submittedOnDailyAt": "2025-07-14T00:42:27.924Z",
      "title": "Modèle de Génération de Réflexions par Escalage de Temps de Test",
      "submittedOnDailyBy": {
        "_id": "638700c723da90491eb72722",
        "avatarUrl": "/avatars/6dcddca5c31121b60e45aab0816e11be.svg",
        "isPro": false,
        "fullname": "Yuxin Wang",
        "user": "wangyuxin87",
        "type": "user"
      },
      "summary": "Introduisons MetaStone-S1, le premier modèle génératif réflechi pour OpenAI o3, qui atteint ses performances en utilisant le Modèle de Répétition de Processus de Récompense (SPRM). Le SPRM partage un réseau de base, utilise des têtes spécifiques aux tâches pour la prédiction du prochain token et l'évaluation du score de processus, et intègre les modèles de récompense de politique et de processus (PRM) dans un seul boucle de rétroaction unifiée, réduisant les paramètres du PRM de plus de 99%, rendant-il un modèle efficace adapté à l'inférence. MetaStone-S1, équipé du SPRM, est naturel et bien adapté au scaling en temps de test (TTS). Nous fournissons ces efforts pour trois raisons : court, moyen et long durée de pensée contrôlée. De plus, pour clarifier la relation entre le calcul global de la pensée et la performance du TTS, nous avons expérimentalement construit un émetteur de scala. Les résultats montrent que notre MetaStone-S1 atteint des performances comparables à celles de la série OpenAI-o3-mini avec seulement 32B paramètres. Pour soutenir la communauté de recherche, nous rendons MetaStone-S1 disponible sous forme d'ouvrage source sur GitHub (https://github.com/MetaStone-AI/MetaStone-S1).",
      "upvotes": 58,
      "discussionId": "6868daac213f123a1f88b9d3",
      "githubRepo": "https://github.com/MetaStone-AI/MetaStone-S1",
      "ai_summary": "MetaStone-S1, a reflective generative model using a self-supervised process reward model, achieves efficient reasoning and scalable performance with fewer parameters compared to existing models.",
      "ai_keywords": [
        "reflective generative model",
        "self-supervised process reward model",
        "backbone network",
        "task-specific heads",
        "policy model",
        "process reward model",
        "test time scaling",
        "controllable thinking length",
        "scaling law",
        "total thinking computation"
      ],
      "githubStars": 41
    },
    "publishedAt": "2025-07-02T13:58:01.000Z",
    "title": "Test-Time Scaling with Reflective Generative Model",
    "summary": "We introduce our first reflective generative model MetaStone-S1, which\nobtains OpenAI o3's performance via the self-supervised process reward model\n(SPRM). Through sharing the backbone network and using task-specific heads for\nnext token prediction and process scoring respectively, SPRM successfully\nintegrates the policy model and process reward model(PRM) into a unified\ninterface without extra process annotation, reducing over 99% PRM parameters\nfor efficient reasoning. Equipped with SPRM, MetaStone-S1 is naturally suitable\nfor test time scaling (TTS), and we provide three reasoning effort modes (low,\nmedium, and high), based on the controllable thinking length. Moreover, we\nempirically establish a scaling law that reveals the relationship between total\nthinking computation and TTS performance. Experiments demonstrate that our\nMetaStone-S1 achieves comparable performance to OpenAI-o3-mini's series with\nonly 32B parameter size. To support the research community, we have\nopen-sourced MetaStone-S1 at https://github.com/MetaStone-AI/MetaStone-S1.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/638700c723da90491eb72722/mRIN68aSkxejrT4UqCaKz.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.01951.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "638700c723da90491eb72722",
      "avatarUrl": "/avatars/6dcddca5c31121b60e45aab0816e11be.svg",
      "fullname": "Yuxin Wang",
      "name": "wangyuxin87",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.08776",
      "authors": [
        {
          "_id": "68745e0f257d4f0435370288",
          "name": "Zhengqing Wang",
          "hidden": false
        },
        {
          "_id": "68745e0f257d4f0435370289",
          "name": "Yuefan Wu",
          "hidden": false
        },
        {
          "_id": "68745e0f257d4f043537028a",
          "name": "Jiacheng Chen",
          "hidden": false
        },
        {
          "_id": "68745e0f257d4f043537028b",
          "name": "Fuyang Zhang",
          "hidden": false
        },
        {
          "_id": "68745e0f257d4f043537028c",
          "name": "Yasutaka Furukawa",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64d97c5bfd0b55d501ba00cf/xQVrjtR_Sb4SOz-GghYdp.mp4"
      ],
      "publishedAt": "2025-07-11T17:38:52.000Z",
      "submittedOnDailyAt": "2025-07-14T00:07:38.676Z",
      "title": "CLiFT : Explorez l'efficacité en calcul et l'adaptabilité des tokens de champ de lumière léger.",
      "submittedOnDailyBy": {
        "_id": "64d97c5bfd0b55d501ba00cf",
        "avatarUrl": "/avatars/47505f2a573acea7176a96f538226ecb.svg",
        "isPro": false,
        "fullname": "Zhengqing Wang",
        "user": "EricW123456",
        "type": "user"
      },
      "summary": "Cet article propose la technique des « Tokens de Compression de Panneau (CLiFTs) » et utilise cette dernière pour maintenir la riche information d'apparence et de géométrie du panneau. Les CLiFTs permettent un rendement informatique efficace grâce à la compression des tokens et, en utilisant la même réseau d'entraînement, il est possible de modifier la quantité de tokens du panneau pour rendre des nouvelles vues. Concrètement, un ensemble d'images est fourni, les images sont tokenisées à partir de la perspective de plusieurs caméras. Le méthode K-means dans l'espace potentiel est utilisée pour sélectionner les tokens qui représentent des ensembles de couleurs réduites comme centres de clusters. Les « compresseurs » de multiples points compressent toute l'information des tokens dans les centres de clusters, construisant ainsi les CLiFTs. Pendant le test, la vision cible et le bucket de calcul (nombre de CLiFTs) sont fournis, les tokens les plus proches de la vision cible sont regroupés et de nouvelles vues sont synthétisées par un processus de rendement informatique efficace. Les expériences avec les ensembles de données RealEstate10K et DL3DV prouvent quantitativement et qualitativement notre approche, atteignant une amélioration de la qualité du rendement, de la quantité de données, de la qualité du rendement et de la vitesse de rendement.",
      "upvotes": 38,
      "discussionId": "68745e10257d4f043537028d",
      "projectPage": "https://clift-nvs.github.io/",
      "githubRepo": "https://github.com/eric-zqwang/CLiFT",
      "ai_summary": "A neural rendering method uses compressed light-field tokens to efficiently represent scenes and render novel views with varying compute budgets.",
      "ai_keywords": [
        "neural rendering",
        "compressed light-field tokens",
        "CLiFTs",
        "multi-view encoder",
        "latent-space K-means",
        "condenser",
        "compute-adaptive renderer",
        "RealEstate10K",
        "DL3DV datasets"
      ],
      "githubStars": 7
    },
    "publishedAt": "2025-07-11T13:38:52.000Z",
    "title": "CLiFT: Compressive Light-Field Tokens for Compute-Efficient and Adaptive\n  Neural Rendering",
    "summary": "This paper proposes a neural rendering approach that represents a scene as\n\"compressed light-field tokens (CLiFTs)\", retaining rich appearance and\ngeometric information of a scene. CLiFT enables compute-efficient rendering by\ncompressed tokens, while being capable of changing the number of tokens to\nrepresent a scene or render a novel view with one trained network. Concretely,\ngiven a set of images, multi-view encoder tokenizes the images with the camera\nposes. Latent-space K-means selects a reduced set of rays as cluster centroids\nusing the tokens. The multi-view ``condenser'' compresses the information of\nall the tokens into the centroid tokens to construct CLiFTs. At test time,\ngiven a target view and a compute budget (i.e., the number of CLiFTs), the\nsystem collects the specified number of nearby tokens and synthesizes a novel\nview using a compute-adaptive renderer. Extensive experiments on RealEstate10K\nand DL3DV datasets quantitatively and qualitatively validate our approach,\nachieving significant data reduction with comparable rendering quality and the\nhighest overall rendering score, while providing trade-offs of data size,\nrendering quality, and rendering speed.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64d97c5bfd0b55d501ba00cf/xQVrjtR_Sb4SOz-GghYdp.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.08776.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64d97c5bfd0b55d501ba00cf",
      "avatarUrl": "/avatars/47505f2a573acea7176a96f538226ecb.svg",
      "fullname": "Zhengqing Wang",
      "name": "EricW123456",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.08800",
      "authors": [
        {
          "_id": "6874615e257d4f043537028f",
          "name": "Luke Rivard",
          "hidden": false
        },
        {
          "_id": "6874615e257d4f0435370290",
          "name": "Sun Sun",
          "hidden": false
        },
        {
          "_id": "6874615e257d4f0435370291",
          "name": "Hongyu Guo",
          "hidden": false
        },
        {
          "_id": "6874615e257d4f0435370292",
          "name": "Wenhu Chen",
          "hidden": false
        },
        {
          "_id": "6874615e257d4f0435370293",
          "name": "Yuntian Deng",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63081e15a670ed10f9d44229/8bwvqBSzlNYlfcdn-pVWu.mp4"
      ],
      "publishedAt": "2025-07-11T17:59:40.000Z",
      "submittedOnDailyAt": "2025-07-14T01:10:17.755Z",
      "title": "NeuralOS : Recherche sur la Simulation de Systèmes d'Exploitation en Utilisant le Modèle de Générateur Neuronal",
      "submittedOnDailyBy": {
        "_id": "63081e15a670ed10f9d44229",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63081e15a670ed10f9d44229/w1b9uq-9774bMMgJbSPsS.jpeg",
        "isPro": true,
        "fullname": "Yuntian Deng",
        "user": "yuntian-deng",
        "type": "user"
      },
      "summary": "Nueralos, un cadre de travail de réseau de neurones est présenté. Ce cadre utilise directement l'interface utilisateur graphique (GUI) du système d'exploitation pour prédire mouvements de souris, clics et événements de clavier, entre autres, en réponse aux entrées de l'utilisateur. Nueralos combine une réseau de neurones récurrent (RNN) qui suive l'état de la machine avec un modèle de rendu basé sur des arbres. Le modèle est entraîné avec des ensembles de données d'interface utilisateur, qui comprennent tant les interactions réelles générées par l'IA que les interactions générées de manière aléatoire. Les expérimentations montrent que Nueralos peut rendre avec succès des séquences réelles de GUI, reconnaître précisément les interactions avec la souris et prédire avec confiance les transitions d'état comme le démarrage d'applications. Cependant, modéliser avec précision la structure micro des interactions de clavier est un défi, mais Nueralos se développe en tant qu'interface neuronale générable adaptable à l'interface humain-ordinateur.",
      "upvotes": 23,
      "discussionId": "6874615e257d4f0435370294",
      "projectPage": "https://neural-os.com/",
      "githubRepo": "https://github.com/yuntian-group/neural-os",
      "ai_summary": "NeuralOS uses a combination of RNNs and diffusion-based rendering to simulate OS GUIs by predicting screen frames from user inputs, demonstrating realistic GUI rendering and state transitions.",
      "ai_keywords": [
        "recurrent neural network",
        "RNN",
        "diffusion-based neural renderer",
        "GUI",
        "user inputs",
        "mouse interactions",
        "keyboard events",
        "state transitions",
        "application launches"
      ],
      "githubStars": 1
    },
    "publishedAt": "2025-07-11T13:59:40.000Z",
    "title": "NeuralOS: Towards Simulating Operating Systems via Neural Generative\n  Models",
    "summary": "We introduce NeuralOS, a neural framework that simulates graphical user\ninterfaces (GUIs) of operating systems by directly predicting screen frames in\nresponse to user inputs such as mouse movements, clicks, and keyboard events.\nNeuralOS combines a recurrent neural network (RNN), which tracks computer\nstate, with a diffusion-based neural renderer that generates screen images. The\nmodel is trained on a large-scale dataset of Ubuntu XFCE recordings, which\ninclude both randomly generated interactions and realistic interactions\nproduced by AI agents. Experiments show that NeuralOS successfully renders\nrealistic GUI sequences, accurately captures mouse interactions, and reliably\npredicts state transitions like application launches. Although modeling\nfine-grained keyboard interactions precisely remains challenging, NeuralOS\noffers a step toward creating fully adaptive, generative neural interfaces for\nfuture human-computer interaction systems.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63081e15a670ed10f9d44229/8bwvqBSzlNYlfcdn-pVWu.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.08800.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "63081e15a670ed10f9d44229",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63081e15a670ed10f9d44229/w1b9uq-9774bMMgJbSPsS.jpeg",
      "fullname": "Yuntian Deng",
      "name": "yuntian-deng",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 245
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.05397",
      "authors": [
        {
          "_id": "6874a1ff257d4f0435370344",
          "name": "Pengfei Zhou",
          "hidden": false
        },
        {
          "_id": "6874a1ff257d4f0435370345",
          "name": "Jie Xia",
          "hidden": false
        },
        {
          "_id": "6874a1ff257d4f0435370346",
          "name": "Xiaopeng Peng",
          "hidden": false
        },
        {
          "_id": "6874a1ff257d4f0435370347",
          "name": "Wangbo Zhao",
          "hidden": false
        },
        {
          "_id": "6874a1ff257d4f0435370348",
          "name": "Zilong Ye",
          "hidden": false
        },
        {
          "_id": "6874a1ff257d4f0435370349",
          "name": "Zekai Li",
          "hidden": false
        },
        {
          "_id": "6874a1ff257d4f043537034a",
          "name": "Suorong Yang",
          "hidden": false
        },
        {
          "_id": "6874a1ff257d4f043537034b",
          "name": "Jiadong Pan",
          "hidden": false
        },
        {
          "_id": "6874a1ff257d4f043537034c",
          "name": "Yuanxiang Chen",
          "hidden": false
        },
        {
          "_id": "6874a1ff257d4f043537034d",
          "name": "Ziqiao Wang",
          "hidden": false
        },
        {
          "_id": "6874a1ff257d4f043537034e",
          "name": "Kai Wang",
          "hidden": false
        },
        {
          "_id": "6874a1ff257d4f043537034f",
          "name": "Qian Zheng",
          "hidden": false
        },
        {
          "_id": "6874a1ff257d4f0435370350",
          "name": "Xiaojun Chang",
          "hidden": false
        },
        {
          "_id": "6874a1ff257d4f0435370351",
          "name": "Gang Pan",
          "hidden": false
        },
        {
          "_id": "6874a1ff257d4f0435370352",
          "name": "Shurong Dong",
          "hidden": false
        },
        {
          "_id": "6874a1ff257d4f0435370353",
          "name": "Kaipeng Zhang",
          "hidden": false
        },
        {
          "_id": "6874a1ff257d4f0435370354",
          "name": "Yang You",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/65f1713552c38a91e0a445e8/KhRuhMNBeK8P8MfGydldo.png"
      ],
      "publishedAt": "2025-07-07T18:31:50.000Z",
      "submittedOnDailyAt": "2025-07-14T04:53:28.591Z",
      "title": "\"Réseau de Neurones Drive Image Editing\"",
      "submittedOnDailyBy": {
        "_id": "65f1713552c38a91e0a445e8",
        "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
        "isPro": false,
        "fullname": "kaipeng",
        "user": "kpzhang996",
        "type": "user"
      },
      "summary": "L'édition d'images traditionnellement reposait sur la dépendance de écrans tactiles, ce qui impliquait des coûts de travail élevés et limitait son accès à des personnes ayant des limitations de contrôle tactile ou de langage. Récemment, le développement des Interfaces Cerveau-Ordinateur (BCI) et des modèles génératifs a permis de proposer LoongX, un méthode d'accès à l'édition d'images sans mains. LoongX utilise un modèle de diffusion mis à jour avec 23 928 paires d'éditions d'images pour comprendre l'intention de l'utilisateur à travers des signaux comme les graphes électroencéphalographiques (EEG), la spectrographie des infrarouges fonctionnels (fNIRS), les potenciogrammes (PPG) et les signaux de mouvement cranien. Pour gérer efficacement la diversité de ces signaux, LoongX combine deux modules clés : le module d'échelle des états spatiaux (CS3) codifie des caractéristiques spécifiques des modèles d'information, tandis que le module de fusion du pont dynamique (DGF) intègre ces caractéristiques dans un espace latin. Cet espace latin est édité par un ajustement micro d'un transformateur de diffusion (DiT) pour que la sémantique de l'édition d'images corresponde à l'intention de l'utilisateur. De plus, un entraînement de contraction est utilisé pour entraîner l'encodeur avec des données préalables et assurer que l'état cognitif et la sémantique de la nature du langage correspondent à l'intention de l'utilisateur. Les expériences étendues montrent que LoongX atteint des performances comparables aux méthodes textuelles, comme CLIP-T (0,2588 vs 0,2549). Ces résultats démontrent que les modèles génératifs neuronaux permettent une édition d'images intuitivement accessible et ouvrent de nouvelles perspectives dans la technologie des conteneurs cognitifs. Les ensembles de données et le code seront publiés pour encourager futures recherches et le développement de ce nouveau domaine.",
      "upvotes": 16,
      "discussionId": "6874a200257d4f0435370355",
      "projectPage": "https://loongx1.github.io/",
      "githubRepo": "https://github.com/LanceZPF/loongx",
      "ai_summary": "LoongX uses multimodal neurophysiological signals and diffusion models for hands-free image editing, achieving performance comparable to text-driven methods and outperforming them when combined with speech.",
      "ai_keywords": [
        "diffusion models",
        "electroencephalography (EEG)",
        "functional near-infrared spectroscopy (fNIRS)",
        "photoplethysmography (PPG)",
        "head motion signals",
        "cross-scale state space (CS3) module",
        "dynamic gated fusion (DGF) module",
        "diffusion transformer (DiT)",
        "contrastive learning"
      ],
      "githubStars": 3
    },
    "publishedAt": "2025-07-07T14:31:50.000Z",
    "title": "Neural-Driven Image Editing",
    "summary": "Traditional image editing typically relies on manual prompting, making it\nlabor-intensive and inaccessible to individuals with limited motor control or\nlanguage abilities. Leveraging recent advances in brain-computer interfaces\n(BCIs) and generative models, we propose LoongX, a hands-free image editing\napproach driven by multimodal neurophysiological signals. LoongX utilizes\nstate-of-the-art diffusion models trained on a comprehensive dataset of 23,928\nimage editing pairs, each paired with synchronized electroencephalography\n(EEG), functional near-infrared spectroscopy (fNIRS), photoplethysmography\n(PPG), and head motion signals that capture user intent. To effectively address\nthe heterogeneity of these signals, LoongX integrates two key modules. The\ncross-scale state space (CS3) module encodes informative modality-specific\nfeatures. The dynamic gated fusion (DGF) module further aggregates these\nfeatures into a unified latent space, which is then aligned with edit semantics\nvia fine-tuning on a diffusion transformer (DiT). Additionally, we pre-train\nthe encoders using contrastive learning to align cognitive states with semantic\nintentions from embedded natural language. Extensive experiments demonstrate\nthat LoongX achieves performance comparable to text-driven methods (CLIP-I:\n0.6605 vs. 0.6558; DINO: 0.4812 vs. 0.4636) and outperforms them when neural\nsignals are combined with speech (CLIP-T: 0.2588 vs. 0.2549). These results\nhighlight the promise of neural-driven generative models in enabling\naccessible, intuitive image editing and open new directions for\ncognitive-driven creative technologies. Datasets and code will be released to\nsupport future work and foster progress in this emerging area.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/65f1713552c38a91e0a445e8/KhRuhMNBeK8P8MfGydldo.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.05397.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65f1713552c38a91e0a445e8",
      "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
      "fullname": "kaipeng",
      "name": "kpzhang996",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.08799",
      "authors": [
        {
          "_id": "6874ac1e257d4f0435370389",
          "name": "Max Belitsky",
          "hidden": false
        },
        {
          "_id": "6874ac1e257d4f043537038a",
          "name": "Dawid J. Kopiczko",
          "hidden": false
        },
        {
          "_id": "6874ac1e257d4f043537038b",
          "name": "Michael Dorkenwald",
          "hidden": false
        },
        {
          "_id": "6874ac1e257d4f043537038c",
          "name": "M. Jehanzeb Mirza",
          "hidden": false
        },
        {
          "_id": "6874ac1e257d4f043537038d",
          "name": "Cees G. M. Snoek",
          "hidden": false
        },
        {
          "_id": "6874ac1e257d4f043537038e",
          "name": "Yuki M. Asano",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/637d21239a5217b88b7549c3/w5UPBRgGM_-9_ELxgOBlL.png"
      ],
      "publishedAt": "2025-07-11T17:59:36.000Z",
      "submittedOnDailyAt": "2025-07-14T05:40:13.268Z",
      "title": "KV Cache Steering pour Induire le Raisonnement dans des Petits Modèles de Langue",
      "submittedOnDailyBy": {
        "_id": "637d21239a5217b88b7549c3",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637d21239a5217b88b7549c3/LrIGPiva5VGVZG87rTAJz.jpeg",
        "isPro": false,
        "fullname": "Yuki Asano",
        "user": "yukimasano",
        "type": "user"
      },
      "summary": "Nous proposons l'orientation par cache. C'est un méthode légère pour orienter les modèles de langage, qui applique une interruption directe dans un cache de clé-valeur pour orienter le modèle de manière non intrusive. Pour tester l'efficacité de l'orientation par cache, nous appliquons cette technique à un petit modèle de langage pour induire des pensées en chaîne. Notre approche utilise les traces d'inférence générées par GPT-4o pour construire des vecteurs d'orientation, ce qui permet d'orienter le comportement du modèle de manière plus claire et multi-niveau, sans nécessiter d'ajustements micro ou de modifications de prompts. Les résultats expérimentaux dans différents cadres d'évaluation d'inférence montrent que l'orientation par cache améliore à la fois la structure qualitative et le rendement quantitatif du modèle d'inférence. En comparaison avec les technologies d'orientation des activations qui nécessitent une interruption continue, une seule orientation par cache offre des avantages significatifs en termes de stabilité des hyperparamètres, d'efficacité dans les temps d'inférence et de facilité d'intégration. Par conséquent, elle peut être considérée comme une solution plus robuste et pratique pour la génération contrôlée.",
      "upvotes": 15,
      "discussionId": "6874ac1e257d4f043537038f",
      "ai_summary": "Cache steering improves reasoning in language models through a single intervention in the key-value cache, enhancing both reasoning structure and task performance.",
      "ai_keywords": [
        "cache steering",
        "key-value cache",
        "chain-of-thought reasoning",
        "GPT-4o",
        "steering vectors",
        "multi-step reasoning",
        "activation steering",
        "hyperparameter stability",
        "inference-time efficiency",
        "ease of integration",
        "controlled generation"
      ]
    },
    "publishedAt": "2025-07-11T13:59:36.000Z",
    "title": "KV Cache Steering for Inducing Reasoning in Small Language Models",
    "summary": "We propose cache steering, a lightweight method for implicit steering of\nlanguage models via a one-shot intervention applied directly to the key-value\ncache. To validate its effectiveness, we apply cache steering to induce\nchain-of-thought reasoning in small language models. Our approach leverages\nGPT-4o-generated reasoning traces to construct steering vectors that shift\nmodel behavior toward more explicit, multi-step reasoning without fine-tuning\nor prompt modifications. Experimental evaluations on diverse reasoning\nbenchmarks demonstrate that cache steering improves both the qualitative\nstructure of model reasoning and quantitative task performance. Compared to\nprior activation steering techniques that require continuous interventions, our\none-shot cache steering offers substantial advantages in terms of\nhyperparameter stability, inference-time efficiency, and ease of integration,\nmaking it a more robust and practical solution for controlled generation.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/637d21239a5217b88b7549c3/w5UPBRgGM_-9_ELxgOBlL.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.08799.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "637d21239a5217b88b7549c3",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637d21239a5217b88b7549c3/LrIGPiva5VGVZG87rTAJz.jpeg",
      "fullname": "Yuki Asano",
      "name": "yukimasano",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.08801",
      "authors": [
        {
          "_id": "68746fc3257d4f04353702ce",
          "name": "Hangjie Yuan",
          "hidden": false
        },
        {
          "_id": "68746fc3257d4f04353702cf",
          "name": "Weihua Chen",
          "hidden": false
        },
        {
          "_id": "68746fc3257d4f04353702d0",
          "name": "Jun Cen",
          "hidden": false
        },
        {
          "_id": "68746fc3257d4f04353702d1",
          "name": "Hu Yu",
          "hidden": false
        },
        {
          "_id": "68746fc3257d4f04353702d2",
          "name": "Jingyun Liang",
          "hidden": false
        },
        {
          "_id": "68746fc3257d4f04353702d3",
          "name": "Shuning Chang",
          "hidden": false
        },
        {
          "_id": "68746fc3257d4f04353702d4",
          "name": "Zhihui Lin",
          "hidden": false
        },
        {
          "_id": "68746fc3257d4f04353702d5",
          "name": "Tao Feng",
          "hidden": false
        },
        {
          "_id": "68746fc3257d4f04353702d6",
          "name": "Pengwei Liu",
          "hidden": false
        },
        {
          "_id": "68746fc3257d4f04353702d7",
          "name": "Jiazheng Xing",
          "hidden": false
        },
        {
          "_id": "68746fc3257d4f04353702d8",
          "name": "Hao Luo",
          "hidden": false
        },
        {
          "_id": "68746fc3257d4f04353702d9",
          "name": "Jiasheng Tang",
          "hidden": false
        },
        {
          "_id": "68746fc3257d4f04353702da",
          "name": "Fan Wang",
          "hidden": false
        },
        {
          "_id": "68746fc3257d4f04353702db",
          "name": "Yi Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-11T17:59:42.000Z",
      "submittedOnDailyAt": "2025-07-14T01:18:10.056Z",
      "title": "Lumos-1 : Perspective de la génération automatique de vidéos de récupération dans les modèles de unification",
      "submittedOnDailyBy": {
        "_id": "649d54b314afbb10ce2a9eeb",
        "avatarUrl": "/avatars/15c325d8c2273ff63569f23015e98486.svg",
        "isPro": false,
        "fullname": "Hangjie Yuan",
        "user": "JacobYuan",
        "type": "user"
      },
      "summary": "Automatiquement, les modèles de langage grands (LLMs) ont unifié plusieurs tâches de langage et ont déclenché des débuts précoces dans la génération automatique de vidéos. Bien que les générateurs automatiques de vidéos actuels s'éloignent de l'architecture standard des LLMs, ils dépendent d'un grand encodeur de texte externe et l'interprétation des tokens reste un problème. Dans cet article, nous présentons Lumos-1, un générateur automatique de vidéos qui maintient l'architecture des LLMs avec un minimum de modifications. Pour introduire la corrélation espace-temporelle dans les LLMs, nous avons reconnu l'effet de RoPE 3D et nous sommes diagnostiqué le spectre de fréquences non équilibré. En conséquence, nous proposons MM-RoPE. MM-RoPE maintient le RoPE original du texte tout en fournissant un spectre de fréquences étendu et des informations de position 3D pour plusieurs modélisations. De plus, Lumos-1 adopte la relation de dépendance des tokens qui respecte la causalité temporelle. En se basant sur cette relation, nous avons identifié le problème d'inégalité dans la perte de frames due à la répétitivité de l'information spatiale et nous proposons AR-DF. AR-DF utilise un tapis temporel lors de l'entraînement et évite la perte de qualité en s'assurant que la politique de tapis soit compatible lors de l'inférence. En utilisant des techniques d'entraînement efficaces en mémoire, Lumos-1 a atteint des performances comparables à EMU3 dans GenEval, à COSMOS-Video2World dans VBench-I2V et à OpenSoraPlan dans VBench-T2V. Le code et le modèle sont disponibles sur https://github.com/alibaba-damo-academy/Lumos.",
      "upvotes": 13,
      "discussionId": "68746fc3257d4f04353702dc",
      "githubRepo": "https://github.com/alibaba-damo-academy/Lumos",
      "ai_summary": "Lumos-1 is an autoregressive video generator that uses a modified LLM architecture with MM-RoPE and AR-DF to address spatiotemporal correlation and frame-wise loss imbalance, achieving competitive performance with fewer resources.",
      "ai_keywords": [
        "autoregressive large language models",
        "LLMs",
        "autoregressive video generation",
        "3D RoPE",
        "MM-RoPE",
        "token dependency strategy",
        "intra-frame bidirectionality",
        "inter-frame temporal causality",
        "frame-wise loss imbalance",
        "Autoregressive Discrete Diffusion Forcing",
        "AR-DF",
        "temporal tube masking",
        "GenEval",
        "COSMOS-Video2World",
        "VBench-I2V",
        "OpenSoraPlan",
        "VBench-T2V"
      ],
      "githubStars": 26
    },
    "publishedAt": "2025-07-11T13:59:42.000Z",
    "title": "Lumos-1: On Autoregressive Video Generation from a Unified Model\n  Perspective",
    "summary": "Autoregressive large language models (LLMs) have unified a vast range of\nlanguage tasks, inspiring preliminary efforts in autoregressive video\ngeneration. Existing autoregressive video generators either diverge from\nstandard LLM architectures, depend on bulky external text encoders, or incur\nprohibitive latency due to next-token decoding. In this paper, we introduce\nLumos-1, an autoregressive video generator that retains the LLM architecture\nwith minimal architectural modifications. To inject spatiotemporal correlations\nin LLMs, we identify the efficacy of incorporating 3D RoPE and diagnose its\nimbalanced frequency spectrum ranges. Therefore, we propose MM-RoPE, a RoPE\nscheme that preserves the original textual RoPE while providing comprehensive\nfrequency spectra and scaled 3D positions for modeling multimodal\nspatiotemporal data. Moreover, Lumos-1 resorts to a token dependency strategy\nthat obeys intra-frame bidirectionality and inter-frame temporal causality.\nBased on this dependency strategy, we identify the issue of frame-wise loss\nimbalance caused by spatial information redundancy and solve it by proposing\nAutoregressive Discrete Diffusion Forcing (AR-DF). AR-DF introduces temporal\ntube masking during training with a compatible inference-time masking policy to\navoid quality degradation. By using memory-efficient training techniques, we\npre-train Lumos-1 on only 48 GPUs, achieving performance comparable to EMU3 on\nGenEval, COSMOS-Video2World on VBench-I2V, and OpenSoraPlan on VBench-T2V. Code\nand models are available at https://github.com/alibaba-damo-academy/Lumos.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.08801.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "649d54b314afbb10ce2a9eeb",
      "avatarUrl": "/avatars/15c325d8c2273ff63569f23015e98486.svg",
      "fullname": "Hangjie Yuan",
      "name": "JacobYuan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.08772",
      "authors": [
        {
          "_id": "68746c95257d4f04353702b7",
          "name": "Shaocong Dong",
          "hidden": false
        },
        {
          "_id": "68746c95257d4f04353702b8",
          "name": "Lihe Ding",
          "hidden": false
        },
        {
          "_id": "68746c95257d4f04353702b9",
          "name": "Xiao Chen",
          "hidden": false
        },
        {
          "_id": "68746c95257d4f04353702ba",
          "name": "Yaokun Li",
          "hidden": false
        },
        {
          "_id": "68746c95257d4f04353702bb",
          "name": "Yuxin Wang",
          "hidden": false
        },
        {
          "_id": "68746c95257d4f04353702bc",
          "name": "Yucheng Wang",
          "hidden": false
        },
        {
          "_id": "68746c95257d4f04353702bd",
          "name": "Qi Wang",
          "hidden": false
        },
        {
          "_id": "68746c95257d4f04353702be",
          "name": "Jaehyeok Kim",
          "hidden": false
        },
        {
          "_id": "68746c95257d4f04353702bf",
          "name": "Chenjian Gao",
          "hidden": false
        },
        {
          "_id": "68746c95257d4f04353702c0",
          "name": "Zhanpeng Huang",
          "hidden": false
        },
        {
          "_id": "68746c95257d4f04353702c1",
          "name": "Zibin Wang",
          "hidden": false
        },
        {
          "_id": "68746c95257d4f04353702c2",
          "name": "Tianfan Xue",
          "hidden": false
        },
        {
          "_id": "68746c95257d4f04353702c3",
          "name": "Dan Xu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-11T17:33:18.000Z",
      "submittedOnDailyAt": "2025-07-14T01:05:21.741Z",
      "title": "Onnéot Mood : Génération 3D Partielle à Connaissance Contextuelle",
      "submittedOnDailyBy": {
        "_id": "63ae91af2314b93f9e6dde42",
        "avatarUrl": "/avatars/792ce138cbee85b8754fdcec7fb1ff52.svg",
        "isPro": false,
        "fullname": "Shaocong Dong",
        "user": "dscdyc",
        "type": "user"
      },
      "summary": "La technologie récente de génération 3D a transformé en un cadre de diffusion du potentiel spécifique de 3D, en se basant sur des informations géométriques inférées à partir de données réelles. Malgré ces avancées, trois limites principales persistent dans la génération 3D : 1) une représentation unique de potentiel ne capture pas la complexité de multiples géométries, ce qui conduit à une perte de détails ; 2) le codage global perd l'indépendance partielle et la relation entre parties nécessaires pour le design ; 3) la structure de conditionnement global ne possède pas de micro-connexité. Nous proposons un cadre de diffusion pour le reconnaissance partielle qui modélise le flux de travail de design 3D, décompose les objets 3D en représentations potentielles partielles par rapport au contexte, et permet la génération multi-partie distribuée. Ce paradigme offre trois avantages : i) réduit la complexité par l'encodage partiel ; ii) permet de modéliser des relations partielles explicites ; iii) soutient l'attribution de conditions au niveau partiel. De plus, en utilisant des réseaux neuronaux connexes, nous avons développé une stratégie de guidage échangé pour optimiser un modèle différentiel pré-entraîné par rapport au bruit potentiel commun dans les parties. Nous construisons un nouveau jeu de données 3D partielle, permettant un entraînement à grande échelle grâce à la division automatique et aux annotations humaines confirmées dans Partverse-Objaverse. Dans des expériences extensives, CoPart a démontré des performances élevées en édition partielle, génération d'objets architecturaux et configuration de scénarios sans précédent avec micro-connexité.",
      "upvotes": 9,
      "discussionId": "68746c96257d4f04353702c4",
      "projectPage": "https://hkdsc.github.io/project/copart/",
      "githubRepo": "https://github.com/hkdsc/copart",
      "ai_summary": "A part-aware diffusion framework, CoPart, enhances 3D generation by decomposing objects into contextual parts, improving complexity handling, relationship modeling, and part-level conditioning.",
      "ai_keywords": [
        "latent diffusion frameworks",
        "geometric priors",
        "single-latent representations",
        "holistic latent coding",
        "part independence",
        "interrelationships",
        "compositional design",
        "global conditioning mechanisms",
        "fine-grained controllability",
        "human 3D design workflows",
        "part-aware diffusion framework",
        "contextual part latents",
        "coherent multi-part generation",
        "encoding complexity",
        "part relationship modeling",
        "part-level conditioning",
        "mutual guidance strategy",
        "joint part latent denoising",
        "geometric coherence",
        "foundation model priors",
        "Partverse",
        "Objaverse",
        "automated mesh segmentation",
        "human-verified annotations",
        "part-level editing",
        "articulated object generation",
        "scene composition"
      ],
      "githubStars": 38
    },
    "publishedAt": "2025-07-11T13:33:18.000Z",
    "title": "From One to More: Contextual Part Latents for 3D Generation",
    "summary": "Recent advances in 3D generation have transitioned from multi-view 2D\nrendering approaches to 3D-native latent diffusion frameworks that exploit\ngeometric priors in ground truth data. Despite progress, three key limitations\npersist: (1) Single-latent representations fail to capture complex multi-part\ngeometries, causing detail degradation; (2) Holistic latent coding neglects\npart independence and interrelationships critical for compositional design; (3)\nGlobal conditioning mechanisms lack fine-grained controllability. Inspired by\nhuman 3D design workflows, we propose CoPart - a part-aware diffusion framework\nthat decomposes 3D objects into contextual part latents for coherent multi-part\ngeneration. This paradigm offers three advantages: i) Reduces encoding\ncomplexity through part decomposition; ii) Enables explicit part relationship\nmodeling; iii) Supports part-level conditioning. We further develop a mutual\nguidance strategy to fine-tune pre-trained diffusion models for joint part\nlatent denoising, ensuring both geometric coherence and foundation model\npriors. To enable large-scale training, we construct Partverse - a novel 3D\npart dataset derived from Objaverse through automated mesh segmentation and\nhuman-verified annotations. Extensive experiments demonstrate CoPart's superior\ncapabilities in part-level editing, articulated object generation, and scene\ncomposition with unprecedented controllability.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.08772.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "63ae91af2314b93f9e6dde42",
      "avatarUrl": "/avatars/792ce138cbee85b8754fdcec7fb1ff52.svg",
      "fullname": "Shaocong Dong",
      "name": "dscdyc",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.08794",
      "authors": [
        {
          "_id": "68747a58257d4f04353702f9",
          "name": "Yulai Zhao",
          "hidden": false
        },
        {
          "_id": "68747a58257d4f04353702fa",
          "name": "Haolin Liu",
          "hidden": false
        },
        {
          "_id": "68747a58257d4f04353702fb",
          "name": "Dian Yu",
          "hidden": false
        },
        {
          "_id": "68747a58257d4f04353702fc",
          "name": "S. Y. Kung",
          "hidden": false
        },
        {
          "_id": "68747a58257d4f04353702fd",
          "name": "Haitao Mi",
          "hidden": false
        },
        {
          "_id": "68747a58257d4f04353702fe",
          "name": "Dong Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-11T17:55:22.000Z",
      "submittedOnDailyAt": "2025-07-14T02:04:25.605Z",
      "title": "1 Token pour tromper le LLM-as-Juge",
      "submittedOnDailyBy": {
        "_id": "62d58fd53bf5e059f7cc3245",
        "avatarUrl": "/avatars/7a4f3ee4a37245f67efd26749d66a706.svg",
        "isPro": false,
        "fullname": "Dian Yu",
        "user": "yudian",
        "type": "user"
      },
      "summary": "Le modèle de récompense généré (ou LLMs-as-judges) est un modèle qui utilise de grands modèles de langage (LLMs) pour évaluer la qualité des réponses, et a été amélioré grâce à l'application d'un apprentissage par renforcement (RLVR) qui inclut des récompenses sûres. Il est préférable à des métriques basées sur des règles, surtout pour des tâches complexes de théorie de la raison qui comprennent des sorties libres. Dans ce paradigme, généralement, l'LLM compare les réponses candidates avec des références factuelles pour attribuer une récompense binaire qui montre la précision. À différence d'une tâche semblable simple, le modèle de récompense généré est vulnérable à des actions superficielles : symboles non mots (par exemple : « : » ou « . ») et processus de raisonnement comme « le processus de pensée : » ou « je résoudrai ce problème de manière séquentielle : » conduisent souvent à des récompenses incorrectes. Cette vulnérabilité s'étend dans une large gamme de modèles de LLMs, de jeux de données et de formats d'exportation, et représente une menace grave pour les paradigmes d'algorithmes clés basés sur des modèles de récompense (par exemple : sampling de rejet, optimisation de préférences, RLVR). Pour atténuer ce problème, nous avons introduit des stratégies simples et efficaces d'expansion des données et nous avons entraîné un nouveau modèle de récompense généré pour créer un modèle avec une grande robustesse. Notre découverte souligne la nécessité urgente de méthodes d'évaluation basées sur des LLMs plus fiables. Nous publions sur https://huggingface.co/sarosavo/Master-RM et https://huggingface.co/datasets/sarosavo/Master-RM un modèle de récompense robuste et généralisable et ses données d'expansion.",
      "upvotes": 7,
      "discussionId": "68747a58257d4f04353702ff",
      "ai_summary": "Generative reward models using LLMs are vulnerable to superficial manipulations but can be improved with data augmentation strategies.",
      "ai_keywords": [
        "generative reward models",
        "LLMs-as-judges",
        "large language models",
        "reinforcement learning with verifiable rewards",
        "RLVR",
        "binary reward",
        "rejection sampling",
        "preference optimization"
      ]
    },
    "publishedAt": "2025-07-11T13:55:22.000Z",
    "title": "One Token to Fool LLM-as-a-Judge",
    "summary": "Generative reward models (also known as LLMs-as-judges), which use large\nlanguage models (LLMs) to evaluate answer quality, are increasingly adopted in\nreinforcement learning with verifiable rewards (RLVR). They are often preferred\nover rigid rule-based metrics, especially for complex reasoning tasks involving\nfree-form outputs. In this paradigm, an LLM is typically prompted to compare a\ncandidate answer against a ground-truth reference and assign a binary reward\nindicating correctness. Despite the seeming simplicity of this comparison task,\nwe find that generative reward models exhibit surprising vulnerabilities to\nsuperficial manipulations: non-word symbols (e.g., \":\" or \".\") or reasoning\nopeners like \"Thought process:\" and \"Let's solve this problem step by step.\"\ncan often lead to false positive rewards. We demonstrate that this weakness is\nwidespread across LLMs, datasets, and prompt formats, posing a serious threat\nfor core algorithmic paradigms that rely on generative reward models, such as\nrejection sampling, preference optimization, and RLVR. To mitigate this issue,\nwe introduce a simple yet effective data augmentation strategy and train a new\ngenerative reward model with substantially improved robustness. Our findings\nhighlight the urgent need for more reliable LLM-based evaluation methods. We\nrelease our robust, general-domain reward model and its synthetic training data\nat https://huggingface.co/sarosavo/Master-RM and\nhttps://huggingface.co/datasets/sarosavo/Master-RM.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.08794.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62d58fd53bf5e059f7cc3245",
      "avatarUrl": "/avatars/7a4f3ee4a37245f67efd26749d66a706.svg",
      "fullname": "Dian Yu",
      "name": "yudian",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.08441",
      "authors": [
        {
          "_id": "6874b80f257d4f04353703a8",
          "name": "Anlin Zheng",
          "hidden": false
        },
        {
          "_id": "6874b80f257d4f04353703a9",
          "name": "Xin Wen",
          "hidden": false
        },
        {
          "_id": "6874b80f257d4f04353703aa",
          "name": "Xuanyang Zhang",
          "hidden": false
        },
        {
          "_id": "6874b80f257d4f04353703ab",
          "name": "Chuofan Ma",
          "hidden": false
        },
        {
          "_id": "6874b80f257d4f04353703ac",
          "name": "Tiancai Wang",
          "hidden": false
        },
        {
          "_id": "6874b80f257d4f04353703ad",
          "name": "Gang Yu",
          "hidden": false
        },
        {
          "_id": "6874b80f257d4f04353703ae",
          "name": "Xiangyu Zhang",
          "hidden": false
        },
        {
          "_id": "6874b80f257d4f04353703af",
          "name": "Xiaojuan Qi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-11T09:32:45.000Z",
      "submittedOnDailyAt": "2025-07-14T06:26:25.095Z",
      "title": "Le point de vue sur le traitement de la Vision Fund Model comme un bon tokenisateur visuel pour la génération automatique d'images",
      "submittedOnDailyBy": {
        "_id": "63483629ac5172169929da0e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1665676793089-noauth.jpeg",
        "isPro": false,
        "fullname": "Xin Wen",
        "user": "xwen99",
        "type": "user"
      },
      "summary": "Nous avons révisé une nouvelle direction qui utilise une forte expression d'un modèle de fondation de vision et de texte pour construire un tokenisateur directement sur le modèle. En particulier, ce domaine a été peu exploré. Concrètement, nous utilisons un modèle de fondation de vision et de texte libre comme l'encodeur d'un tokenisateur. Pour améliorer l'efficacité de ce modèle, nous avons introduit deux composants principaux : 1. Un cadre de réduction par zones pour réduire la redondance des caractéristiques prédites sur une grille 2D. 2. Un objet de reconstruction contextuel qui ajuste l'output du tokenisateur à la représentation du modèle de fondation et maintient la dépendance contextuelle. Sur la base de cette architecture, le tokenisateur d'images proposé, VFMTok, améliore significativement la qualité de la reconstruction et de la génération d'images, augmente l'efficience du tokenisage et promeut la génération automatique de reconstruction. De plus, il atteint un gFID de 2.07 sur le benchmark d'ImageNet, accélère la convergence du modèle dans un tiers et permet la synthèse de haute qualité avec des conditions de classe. Le code est publiquement libéré et offre des avantages à la communauté.",
      "upvotes": 4,
      "discussionId": "6874b80f257d4f04353703b0",
      "ai_summary": "A novel image tokenizer built on pre-trained vision foundation models improves image reconstruction, generation quality, and token efficiency, enhancing autoregressive generation and class-conditional synthesis.",
      "ai_keywords": [
        "pre-trained vision foundation models",
        "image tokenizer",
        "region-adaptive quantization framework",
        "semantic reconstruction objective",
        "VFMTok",
        "gFID",
        "autoregressive generation",
        "class-conditional synthesis",
        "classifier-free guidance"
      ]
    },
    "publishedAt": "2025-07-11T05:32:45.000Z",
    "title": "Vision Foundation Models as Effective Visual Tokenizers for\n  Autoregressive Image Generation",
    "summary": "Leveraging the powerful representations of pre-trained vision foundation\nmodels -- traditionally used for visual comprehension -- we explore a novel\ndirection: building an image tokenizer directly atop such models, a largely\nunderexplored area. Specifically, we employ a frozen vision foundation model as\nthe encoder of our tokenizer. To enhance its effectiveness, we introduce two\nkey components: (1) a region-adaptive quantization framework that reduces\nredundancy in the pre-trained features on regular 2D grids, and (2) a semantic\nreconstruction objective that aligns the tokenizer's outputs with the\nfoundation model's representations to preserve semantic fidelity. Based on\nthese designs, our proposed image tokenizer, VFMTok, achieves substantial\nimprovements in image reconstruction and generation quality, while also\nenhancing token efficiency. It further boosts autoregressive (AR) generation --\nachieving a gFID of 2.07 on ImageNet benchmarks, while accelerating model\nconvergence by three times, and enabling high-fidelity class-conditional\nsynthesis without the need for classifier-free guidance (CFG). The code will be\nreleased publicly to benefit the community.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.08441.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63483629ac5172169929da0e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1665676793089-noauth.jpeg",
      "fullname": "Xin Wen",
      "name": "xwen99",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.06952",
      "authors": [
        {
          "_id": "68749c03257d4f043537033e",
          "name": "Keyon Vafa",
          "hidden": false
        },
        {
          "_id": "68749c03257d4f043537033f",
          "name": "Peter G. Chang",
          "hidden": false
        },
        {
          "_id": "68749c03257d4f0435370340",
          "name": "Ashesh Rambachan",
          "hidden": false
        },
        {
          "_id": "68749c03257d4f0435370341",
          "name": "Sendhil Mullainathan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-09T15:36:15.000Z",
      "submittedOnDailyAt": "2025-07-14T04:27:13.989Z",
      "title": "Que se concentra le modèle de base ? Nous explorons le modèle mondial en utilisant l'inclinaison d'inférence.",
      "submittedOnDailyBy": {
        "_id": "64d98ef7a4839890b25eb78b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64d98ef7a4839890b25eb78b/215-CSVLl81z6CAq0ECWU.jpeg",
        "isPro": true,
        "fullname": "Fangyuan Yu",
        "user": "Ksgk-fy",
        "type": "user"
      },
      "summary": "Le modèle de base est basé sur une compréhension profonde du domaine de la prédiction de séquences. Cela ressemble à la façon dont la prédiction du mouvement des planètes dans l'univers solaire est liée au découverte de la mécanique de Newton. Cependant, évaluer si le modèle comprend la structure essentielle exactement est un défi. Nous avons développé des techniques pour évaluer le modèle de base, qui explorent comment le modèle s'adapte aux ensembles de données synthétiques générées à partir du modèle mondial établi. Ces techniques évaluent si le modèle de base a un indice de bras qui coïncide avec le modèle mondial, ce qui est fait par la méthode d'évaluation de l'indice de bras. Bien que le modèle de base ait démontré des résultats exceptionnels dans des tâches d'entraînement dans plusieurs domaines, il a été découvert qu'il n'avait pas développé des tests d'indice de bras pour le modèle mondial lorsqu'il s'adapte à de nouvelles tâches. En particulier, les modèles entraînés sur des projets de routes ont montré un comportement cohérent en n'appliquant pas la mécanique de Newton dans de nouvelles tâches physiques. De plus, ces modèles ont développé des heuristiques pour des tâches spécifiques, mais ont montré qu'ils ne pouvaient pas généraliser leurs comportements.",
      "upvotes": 4,
      "discussionId": "68749c03257d4f0435370342",
      "ai_summary": "Foundation models, despite excelling in training tasks, often fail to generalize to new tasks due to task-specific heuristics rather than capturing underlying world models.",
      "ai_keywords": [
        "sequence prediction",
        "foundation models",
        "inductive bias",
        "synthetic datasets",
        "world model",
        "inductive bias probe",
        "Newtonian mechanics",
        "task-specific heuristics",
        "generalization"
      ]
    },
    "publishedAt": "2025-07-09T11:36:15.000Z",
    "title": "What Has a Foundation Model Found? Using Inductive Bias to Probe for\n  World Models",
    "summary": "Foundation models are premised on the idea that sequence prediction can\nuncover deeper domain understanding, much like how Kepler's predictions of\nplanetary motion later led to the discovery of Newtonian mechanics. However,\nevaluating whether these models truly capture deeper structure remains a\nchallenge. We develop a technique for evaluating foundation models that\nexamines how they adapt to synthetic datasets generated from some postulated\nworld model. Our technique measures whether the foundation model's inductive\nbias aligns with the world model, and so we refer to it as an inductive bias\nprobe. Across multiple domains, we find that foundation models can excel at\ntheir training tasks yet fail to develop inductive biases towards the\nunderlying world model when adapted to new tasks. We particularly find that\nfoundation models trained on orbital trajectories consistently fail to apply\nNewtonian mechanics when adapted to new physics tasks. Further analysis reveals\nthat these models behave as if they develop task-specific heuristics that fail\nto generalize.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.06952.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64d98ef7a4839890b25eb78b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64d98ef7a4839890b25eb78b/215-CSVLl81z6CAq0ECWU.jpeg",
      "fullname": "Fangyuan Yu",
      "name": "Ksgk-fy",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 15
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.08771",
      "authors": [
        {
          "_id": "68747a17257d4f04353702ef",
          "name": "Chenyang Song",
          "hidden": false
        },
        {
          "_id": "68747a17257d4f04353702f0",
          "name": "Weilin Zhao",
          "hidden": false
        },
        {
          "_id": "68747a17257d4f04353702f1",
          "name": "Xu Han",
          "hidden": false
        },
        {
          "_id": "68747a17257d4f04353702f2",
          "name": "Chaojun Xiao",
          "hidden": false
        },
        {
          "_id": "68747a17257d4f04353702f3",
          "name": "Yingfa Chen",
          "hidden": false
        },
        {
          "_id": "68747a17257d4f04353702f4",
          "name": "Yuxuan Li",
          "hidden": false
        },
        {
          "_id": "68747a17257d4f04353702f5",
          "name": "Zhiyuan Liu",
          "hidden": false
        },
        {
          "_id": "68747a17257d4f04353702f6",
          "name": "Maosong Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-11T17:28:56.000Z",
      "submittedOnDailyAt": "2025-07-14T03:24:41.458Z",
      "title": "BlockFFN : Fonction d'Activation Spatiale avec Spécialités Hybrides Orientées vers le Mixture of Experts adapté pour l'Amélioration de la Section de la Terminaison par l'Utilisation de la Sparseté de l'Activation au Niveau des Chunks",
      "submittedOnDailyBy": {
        "_id": "64c09684e56520a63d35ec87",
        "avatarUrl": "/avatars/86a338e8d122a66a94143bbb9bf3ebf8.svg",
        "isPro": false,
        "fullname": "Chenyang Song",
        "user": "Raincleared",
        "type": "user"
      },
      "summary": "Pour réduire le chargement informatique des modèles de langage naturel (LLMs), des architectures avec une rareté des activations ont été soulignées, notamment la Mix of Experts (MoE). Cependant, la manque d'invariance différentielle et de flexibilité dans la MoE des modèles de base a affecté le rendement. De plus, il a été démontré que chaque token nécessite seulement activer un petit nombre de paramètres, ce qui révèle une faible rareté des activations dans l'architecture. Cela indique une forte proportion de paramètres activés sur plusieurs tokens continus. Ce modèle de rareté n'est pas adapté pour l'accélération sous ressources limitées (par exemple : des dispositifs de point final) ni pour les principales technologies d'accélération (par exemple : l'interprétation de l'inférence). Pour résoudre ces problèmes, nous introduisons une nouvelle architecture MoE, BlockFFN, et proposons des méthodes efficaces d'entraînement et de batch. En particulier, grâce à la combinaison de l'activation ReLU et de RMSNorm, nous réussissons à réaliser une route différenciable et flexible. De plus, nous concevons des objectifs d'entraînement pour promouvoir la rareté des activations au niveau du token (TLS) et au niveau du tour de tokens (CLS), et nous adaptons BlockFFN pour cela. Enfin, nous implémentons le premier accélérateur valide qui combine la rareté des activations et l'interprétation de l'inférence. Les résultats expérimentaux montrent que BlockFFN dépasse les autres lignes basées sur la MoE en termes de rendement, atteignant plus de 80% de TLS et plus de 70% de CLS pour 8 tokens. Notre accélérateur a réalisé un accroissement de vitesse de 3,67 fois sur des dispositifs de point final. Tout le code et les checkpoints sont disponibles (https://github.com/thunlp/BlockFFN).",
      "upvotes": 2,
      "discussionId": "68747a17257d4f04353702f7",
      "githubRepo": "https://github.com/thunlp/BlockFFN",
      "githubStars": 3
    },
    "publishedAt": "2025-07-11T13:28:56.000Z",
    "title": "BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with\n  Chunk-Level Activation Sparsity",
    "summary": "To alleviate the computational burden of large language models (LLMs),\narchitectures with activation sparsity, represented by mixture-of-experts\n(MoE), have attracted increasing attention. However, the non-differentiable and\ninflexible routing of vanilla MoE hurts model performance. Moreover, while each\ntoken activates only a few parameters, these sparsely-activated architectures\nexhibit low chunk-level sparsity, indicating that the union of multiple\nconsecutive tokens activates a large ratio of parameters. Such a sparsity\npattern is unfriendly for acceleration under low-resource conditions (e.g.,\nend-side devices) and incompatible with mainstream acceleration techniques\n(e.g., speculative decoding). To address these challenges, we introduce a novel\nMoE architecture, BlockFFN, as well as its efficient training and deployment\ntechniques. Specifically, we use a router integrating ReLU activation and\nRMSNorm for differentiable and flexible routing. Next, to promote both\ntoken-level sparsity (TLS) and chunk-level sparsity (CLS), CLS-aware training\nobjectives are designed, making BlockFFN more acceleration-friendly. Finally,\nwe implement efficient acceleration kernels, combining activation sparsity and\nspeculative decoding for the first time. The experimental results demonstrate\nthe superior performance of BlockFFN over other MoE baselines, achieving over\n80% TLS and 70% 8-token CLS. Our kernels achieve up to 3.67times speedup on\nreal end-side devices than dense models. All codes and checkpoints are\navailable publicly (https://github.com/thunlp/BlockFFN).",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.08771.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64c09684e56520a63d35ec87",
      "avatarUrl": "/avatars/86a338e8d122a66a94143bbb9bf3ebf8.svg",
      "fullname": "Chenyang Song",
      "name": "Raincleared",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.07151",
      "authors": [
        {
          "_id": "68707d75c8391850d6097823",
          "user": {
            "_id": "66b34647a29e5c00011d34c3",
            "avatarUrl": "/avatars/ae39f9b84f9379423fa3a8509fbcc94e.svg",
            "isPro": false,
            "fullname": "Zongmeng Zhang",
            "user": "ustc-zhangzm",
            "type": "user"
          },
          "name": "Zongmeng Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-11T08:00:24.241Z",
          "hidden": false
        },
        {
          "_id": "68707d75c8391850d6097824",
          "name": "Wengang Zhou",
          "hidden": false
        },
        {
          "_id": "68707d75c8391850d6097825",
          "name": "Jie Zhao",
          "hidden": false
        },
        {
          "_id": "68707d75c8391850d6097826",
          "name": "Houqiang Li",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/66b34647a29e5c00011d34c3/Zj-2S7r3SlJnKipepoQxm.png"
      ],
      "publishedAt": "2025-07-09T11:18:38.000Z",
      "submittedOnDailyAt": "2025-07-14T01:45:01.699Z",
      "title": "Modèle fort de langage bilingue pour l'étude de la branche rouge du modèle de flux",
      "submittedOnDailyBy": {
        "_id": "66b34647a29e5c00011d34c3",
        "avatarUrl": "/avatars/ae39f9b84f9379423fa3a8509fbcc94e.svg",
        "isPro": false,
        "fullname": "Zongmeng Zhang",
        "user": "ustc-zhangzm",
        "type": "user"
      },
      "summary": "Le modèle de langage multimodal de DeepMood (MLLM) montre des capacités exceptionnelles dans des tâches visuelles, mais il est tendance à produire des hallucinations facilement dans des situations réelles. Dans cet article, nous nous concentrons sur le conflit entre la réponse du modèle et l'entrée, en étudiant le phénomène de hallucination dans le MLLM. Bien que les études précédentes aient ciblé le conflit entre la réponse et l'entrée, nous nous intéressons au conflit spécifique des entrées qui provoquent directement la hallucination. Nous définissons formellement le conflit du modèle et construisons le jeu de données Modalidade Multimodal de Conflit (MMMC) pour simuler ce phénomène dans des tâches visuelles. Nous proposons trois méthodes : l'ingénierie des prompts, les ajustements normaux et l'apprentissage par refonte, avec l'objectif de mitiger la hallucination causée par le conflit du modèle. Nous allongeons le jeu de données MMMC pour effectuer des expériences et analysons les avantages et inconvénients de ces méthodes. Nos résultats montrent que l'apprentissage par refonte est le plus efficace pour mitiger la hallucination sous conflits du modèle, tandis que l'ajustement normal montre un rendement désirable. Notre recherche clarifie le conflit du modèle qui provoque des hallucinations et fournit une compréhension plus profonde de la robustesse du MLLM.",
      "upvotes": 2,
      "discussionId": "68707d76c8391850d6097827",
      "projectPage": "https://github.com/zmzhang2000/MMMC",
      "githubRepo": "https://github.com/zmzhang2000/MMMC",
      "ai_summary": "Investigation of modality conflict in multimodal large language models reveals its role in causing hallucinations, with reinforcement learning emerging as the most effective mitigation strategy.",
      "ai_keywords": [
        "multimodal large language models",
        "vision-language tasks",
        "hallucinations",
        "modality conflict",
        "prompt engineering",
        "supervised fine-tuning",
        "reinforcement learning",
        "Multimodal Modality Conflict (MMMC)"
      ],
      "githubStars": 1
    },
    "publishedAt": "2025-07-09T07:18:38.000Z",
    "title": "Robust Multimodal Large Language Models Against Modality Conflict",
    "summary": "Despite the impressive capabilities of multimodal large language models\n(MLLMs) in vision-language tasks, they are prone to hallucinations in\nreal-world scenarios. This paper investigates the hallucination phenomenon in\nMLLMs from the perspective of modality conflict. Unlike existing works focusing\non the conflicts between model responses and inputs, we study the inherent\nconflicts in inputs from different modalities that place MLLMs in a dilemma and\ndirectly lead to hallucinations. We formally define the modality conflict and\nconstruct a dataset named Multimodal Modality Conflict (MMMC) to simulate this\nphenomenon in vision-language tasks. Three methods based on prompt engineering,\nsupervised fine-tuning, and reinforcement learning are proposed to alleviate\nthe hallucination caused by modality conflict. Extensive experiments are\nconducted on the MMMC dataset to analyze the merits and demerits of these\nmethods. Our results show that the reinforcement learning method achieves the\nbest performance in mitigating the hallucination under modality conflict, while\nthe supervised fine-tuning method shows promising and stable performance. Our\nwork sheds light on the unnoticed modality conflict that leads to\nhallucinations and provides more insights into the robustness of MLLMs.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/66b34647a29e5c00011d34c3/Zj-2S7r3SlJnKipepoQxm.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.07151.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66b34647a29e5c00011d34c3",
      "avatarUrl": "/avatars/ae39f9b84f9379423fa3a8509fbcc94e.svg",
      "fullname": "Zongmeng Zhang",
      "name": "ustc-zhangzm",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]