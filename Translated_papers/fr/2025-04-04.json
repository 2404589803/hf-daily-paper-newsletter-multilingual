[
  {
    "paper": {
      "id": "2504.01990",
      "authors": [
        {
          "_id": "67ef8723d325fe100f36107e",
          "user": {
            "_id": "654a97282d2fcd6bf2851173",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/654a97282d2fcd6bf2851173/9zXf940gr4WNt4e-oOt4k.png",
            "isPro": false,
            "fullname": "Bang Liu",
            "user": "Bang-UdeM-Mila",
            "type": "user"
          },
          "name": "Bang Liu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-04T07:15:51.456Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f36107f",
          "user": {
            "_id": "65af5f8f3db2280ece7fac79",
            "avatarUrl": "/avatars/66d88b2d744c8d00e11d39a55ab86c2e.svg",
            "isPro": false,
            "fullname": "Xin-Feng Li",
            "user": "xinfeng1i",
            "type": "user"
          },
          "name": "Xinfeng Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:26:45.785Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361080",
          "user": {
            "_id": "64b78a39954ae43365984448",
            "avatarUrl": "/avatars/6de9d4bf320a69eca6b758e718ee116c.svg",
            "isPro": false,
            "fullname": "Zhang",
            "user": "Peiyan",
            "type": "user"
          },
          "name": "Jiayi Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:28:49.444Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361081",
          "user": {
            "_id": "64324bb3034ecbefddd99863",
            "avatarUrl": "/avatars/3b8cdc2066251999a3a7e6d5565dceb5.svg",
            "isPro": false,
            "fullname": "Jinlin Wang",
            "user": "JinlinW",
            "type": "user"
          },
          "name": "Jinlin Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:27:02.727Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361082",
          "name": "Tanjin He",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361083",
          "name": "Sirui Hong",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361084",
          "name": "Hongzhang Liu",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361085",
          "name": "Shaokun Zhang",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361086",
          "user": {
            "_id": "5fc0b2b61160c47d1d438568",
            "avatarUrl": "/avatars/90beea6b452c662d579197dbf592423a.svg",
            "isPro": false,
            "fullname": "Kaitao Song",
            "user": "KaitaoSong",
            "type": "user"
          },
          "name": "Kaitao Song",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:27:47.151Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361087",
          "user": {
            "_id": "64c090a9f613170e7be93d2f",
            "avatarUrl": "/avatars/ccbdf444e1f2386d2281e8e42059ebb0.svg",
            "isPro": false,
            "fullname": "KunlunZhu",
            "user": "KunlunZhu",
            "type": "user"
          },
          "name": "Kunlun Zhu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:28:03.582Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361088",
          "user": {
            "_id": "6664783b0ab8b63cbb4a3156",
            "avatarUrl": "/avatars/71859e6f76c157191bd2e968061f08b0.svg",
            "isPro": false,
            "fullname": "cyh",
            "user": "chengyuheng",
            "type": "user"
          },
          "name": "Yuheng Cheng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:28:14.543Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361089",
          "user": {
            "_id": "62bb1e0f3ff437e49a3088e5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62bb1e0f3ff437e49a3088e5/bcUQmH8tKfI6DIWH9IcYp.jpeg",
            "isPro": true,
            "fullname": "Suyuchen Wang",
            "user": "sheryc",
            "type": "user"
          },
          "name": "Suyuchen Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:28:21.351Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f36108a",
          "user": {
            "_id": "655c092183186f133f959108",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/WMVgGhjQbVJXK9eh4EuT9.jpeg",
            "isPro": false,
            "fullname": "Xiaoqiang Wang",
            "user": "qindomitable",
            "type": "user"
          },
          "name": "Xiaoqiang Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:28:26.707Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f36108b",
          "name": "Yuyu Luo",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f36108c",
          "user": {
            "_id": "648ee5fe9ae7cc4fcffa9aef",
            "avatarUrl": "/avatars/9bcc5eb91452c1360b9a0a4f9def8af8.svg",
            "isPro": false,
            "fullname": "Haibo Jin",
            "user": "Nick233",
            "type": "user"
          },
          "name": "Haibo Jin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:28:40.177Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f36108d",
          "user": {
            "_id": "64b78a39954ae43365984448",
            "avatarUrl": "/avatars/6de9d4bf320a69eca6b758e718ee116c.svg",
            "isPro": false,
            "fullname": "Zhang",
            "user": "Peiyan",
            "type": "user"
          },
          "name": "Peiyan Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:29:02.679Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f36108e",
          "user": {
            "_id": "66197a8afeb55cbe39e50ae8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/pcSS1TsCoHxRcAEkcMNm0.png",
            "isPro": false,
            "fullname": "Ollie Liu",
            "user": "oliu-io",
            "type": "user"
          },
          "name": "Ollie Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:29:10.114Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f36108f",
          "name": "Jiaqi Chen",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361090",
          "user": {
            "_id": "6719d581a6cad13741b8bc7f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6719d581a6cad13741b8bc7f/w4EttqfXRgWZJc6HpYOS9.jpeg",
            "isPro": false,
            "fullname": "Huan Zhang",
            "user": "huanzhang12",
            "type": "user"
          },
          "name": "Huan Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:29:24.614Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361091",
          "user": {
            "_id": "640dc84b474aa6f89554d518",
            "avatarUrl": "/avatars/64f47f76d97c5e91b7ab8380bcada61c.svg",
            "isPro": false,
            "fullname": "Zhaoyang Yu",
            "user": "MoshiQAQ",
            "type": "user"
          },
          "name": "Zhaoyang Yu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:29:38.969Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361092",
          "name": "Haochen Shi",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361093",
          "name": "Boyan Li",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361094",
          "name": "Dekun Wu",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361095",
          "user": {
            "_id": "6402e8fb06c715b93407442d",
            "avatarUrl": "/avatars/12b67f0632be5a53b56d8a68586a7f98.svg",
            "isPro": false,
            "fullname": "Fengwei Teng",
            "user": "leavendough",
            "type": "user"
          },
          "name": "Fengwei Teng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:30:09.492Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361096",
          "name": "Xiaojun Jia",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361097",
          "name": "Jiawei Xu",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361098",
          "name": "Jinyu Xiang",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361099",
          "name": "Yizhang Lin",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f36109a",
          "name": "Tianming Liu",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f36109b",
          "name": "Tongliang Liu",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f36109c",
          "name": "Yu Su",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f36109d",
          "name": "Huan Sun",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f36109e",
          "user": {
            "_id": "66a8fa5fd909c30167f1f5cd",
            "avatarUrl": "/avatars/c9b26d5b2dd78bed9661df429012fd97.svg",
            "isPro": false,
            "fullname": "Glen Berseth",
            "user": "gberseth",
            "type": "user"
          },
          "name": "Glen Berseth",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:30:19.433Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f36109f",
          "name": "Jianyun Nie",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f3610a0",
          "name": "Ian Foster",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f3610a1",
          "name": "Logan Ward",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f3610a2",
          "name": "Qingyun Wu",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f3610a3",
          "user": {
            "_id": "5e7e595230dc073f817a2bb5",
            "avatarUrl": "/avatars/d5ff36e45555d9e169cf56c845736444.svg",
            "isPro": false,
            "fullname": "Yu Gu",
            "user": "entslscheia",
            "type": "user"
          },
          "name": "Yu Gu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:50:40.603Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f3610a4",
          "user": {
            "_id": "64403daae44f30a72323e4ca",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64403daae44f30a72323e4ca/skJ9h0pdNfmE4VbQL8xDR.png",
            "isPro": false,
            "fullname": "mingchen zhuge",
            "user": "tjpxiaoming",
            "type": "user"
          },
          "name": "Mingchen Zhuge",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:26:28.011Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f3610a5",
          "name": "Xiangru Tang",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f3610a6",
          "name": "Haohan Wang",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f3610a7",
          "name": "Jiaxuan You",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f3610a8",
          "name": "Chi Wang",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f3610a9",
          "user": {
            "_id": "670918b02806bda07e44780c",
            "avatarUrl": "/avatars/c08ba5048d9e911ef488862e8869792f.svg",
            "isPro": false,
            "fullname": "Jian Pei",
            "user": "StrawHat2333",
            "type": "user"
          },
          "name": "Jian Pei",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:36:21.496Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f3610aa",
          "name": "Qiang Yang",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f3610ab",
          "user": {
            "_id": "6342829eb9454d65a2e7a4c4",
            "avatarUrl": "/avatars/4438abdf189dbe26a52948800d79a7c5.svg",
            "isPro": false,
            "fullname": "Xiaoliang Qi",
            "user": "phynics",
            "type": "user"
          },
          "name": "Xiaoliang Qi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:30:59.175Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f3610ac",
          "name": "Chenglin Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-31T18:00:29.000Z",
      "submittedOnDailyAt": "2025-04-04T05:46:58.338Z",
      "title": "L'évolution des agents de base et les problèmes : évolution vers un système coopératif et sûr inspiré par le cerveau",
      "submittedOnDailyBy": {
        "_id": "64403daae44f30a72323e4ca",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64403daae44f30a72323e4ca/skJ9h0pdNfmE4VbQL8xDR.png",
        "isPro": false,
        "fullname": "mingchen zhuge",
        "user": "tjpxiaoming",
        "type": "user"
      },
      "summary": "Le début de l'émergence des LLM a détourné des changements innovants dans le domaine de l'intelligence artificielle, offrant des opportunités pour développer des intelligences artificielles de haut rendement capables de réaliser des actions transformatrices dans diverses zones, ainsi que pour développer des théories logiques complexes et renforcer la perception. En supposant que ces intelligences pourront avoir un impact plus grand sur la recherche et les applications pratiques de l'intelligence artificielle, le design, l'évaluation et l'amélioration continue des IAs ont devenu des défis complexes et multidimensionnels. Dans cette recherche, on propose de configurer une intelligence artificielle modulaire basée sur une architecture similaire à celle de la fonction de la mémoire humaine, intégrant des principes de la science cognitive, de la neuroscience et de la recherche informatique. On fournit une description détaillée du design, de l'évaluation et de l'amélioration continue de cette intelligence artificielle. Notre travail est divisé en quatre parties interrelées. Premièrement, on examine la base modulaire de l'intelligence artificielle, cartographiant des modules de la cognition, de la perception et de l'action correspondant aux fonctions du cerveau humain, et on identifie des éléments clés comme la mémoire, le modélisation du monde, le traitement des récompenses et les systèmes émotionnels. Deuxièmement, on discute des structures évolutives automatiques et du processus d'adaptation, en investiguant comment les IAs peuvent évaluer automatiquement leurs habiletés et s'adapter aux environnements dynamiques, atteignant un apprentissage continu par le paradigme d'optimisation automatique. Cela comprend le développement de nouvelles stratégies d'optimisation dirigées par les LLM. Troisièmement, on étudie des systèmes de multiples IAs coopératives et évolutives, en explorant l'intelligence collective générée par l'échange, la collaboration et les structures sociales des IAs, et on établit une relation parallèle avec la dynamique de la société humaine. Finalement, on discute les responsabilités fondamentales pour la construction de systèmes IA sûrs, éthiques et bénéfiques, en mettant en avant des stratégies pratiques pour atténuer les menaces internes et externes, assurer des accords éthiques, renforcer la robustesse et la confiance.",
      "upvotes": 55,
      "discussionId": "67ef8727d325fe100f3611aa",
      "githubRepo": "https://github.com/FoundationAgents/awesome-foundation-agents"
    },
    "publishedAt": "2025-03-31T14:00:29.000Z",
    "title": "Advances and Challenges in Foundation Agents: From Brain-Inspired\n  Intelligence to Evolutionary, Collaborative, and Safe Systems",
    "summary": "The advent of large language models (LLMs) has catalyzed a transformative\nshift in artificial intelligence, paving the way for advanced intelligent\nagents capable of sophisticated reasoning, robust perception, and versatile\naction across diverse domains. As these agents increasingly drive AI research\nand practical applications, their design, evaluation, and continuous\nimprovement present intricate, multifaceted challenges. This survey provides a\ncomprehensive overview, framing intelligent agents within a modular,\nbrain-inspired architecture that integrates principles from cognitive science,\nneuroscience, and computational research. We structure our exploration into\nfour interconnected parts. First, we delve into the modular foundation of\nintelligent agents, systematically mapping their cognitive, perceptual, and\noperational modules onto analogous human brain functionalities, and elucidating\ncore components such as memory, world modeling, reward processing, and\nemotion-like systems. Second, we discuss self-enhancement and adaptive\nevolution mechanisms, exploring how agents autonomously refine their\ncapabilities, adapt to dynamic environments, and achieve continual learning\nthrough automated optimization paradigms, including emerging AutoML and\nLLM-driven optimization strategies. Third, we examine collaborative and\nevolutionary multi-agent systems, investigating the collective intelligence\nemerging from agent interactions, cooperation, and societal structures,\nhighlighting parallels to human social dynamics. Finally, we address the\ncritical imperative of building safe, secure, and beneficial AI systems,\nemphasizing intrinsic and extrinsic security threats, ethical alignment,\nrobustness, and practical mitigation strategies necessary for trustworthy\nreal-world deployment.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.01990.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64403daae44f30a72323e4ca",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64403daae44f30a72323e4ca/skJ9h0pdNfmE4VbQL8xDR.png",
      "fullname": "mingchen zhuge",
      "name": "tjpxiaoming",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.02826",
      "authors": [
        {
          "_id": "67ef4be0985aa66b67021ddc",
          "user": {
            "_id": "6530e62f536dbca918e71c3e",
            "avatarUrl": "/avatars/efc93bc767e561c6c6d429f65c23382d.svg",
            "isPro": false,
            "fullname": "Xiangyu Z",
            "user": "PhoenixZ",
            "type": "user"
          },
          "name": "Xiangyu Zhao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:09:07.389Z",
          "hidden": false
        },
        {
          "_id": "67ef4be0985aa66b67021ddd",
          "user": {
            "_id": "6710be3e6d1b33cf24417e38",
            "avatarUrl": "/avatars/f60bc9a67bb58f5997cbcc28cb93c079.svg",
            "isPro": false,
            "fullname": "zpy",
            "user": "zpy777",
            "type": "user"
          },
          "name": "Peiyuan Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:09:11.816Z",
          "hidden": false
        },
        {
          "_id": "67ef4be0985aa66b67021dde",
          "user": {
            "_id": "662516d72419feed62fb3a0a",
            "avatarUrl": "/avatars/24c4157829e70a4e346aa984885aa5ad.svg",
            "isPro": false,
            "fullname": "Dian",
            "user": "KexianTang",
            "type": "user"
          },
          "name": "Kexian Tang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:21:18.584Z",
          "hidden": false
        },
        {
          "_id": "67ef4be0985aa66b67021ddf",
          "name": "Hao Li",
          "hidden": false
        },
        {
          "_id": "67ef4be0985aa66b67021de0",
          "name": "Zicheng Zhang",
          "hidden": false
        },
        {
          "_id": "67ef4be0985aa66b67021de1",
          "user": {
            "_id": "65535b125413c1a54e6fb243",
            "avatarUrl": "/avatars/03bcf1d58865f5406aff49a415e78bdc.svg",
            "isPro": false,
            "fullname": "Guangtao Zhai",
            "user": "GTZhai",
            "type": "user"
          },
          "name": "Guangtao Zhai",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:22:23.057Z",
          "hidden": false
        },
        {
          "_id": "67ef4be0985aa66b67021de2",
          "user": {
            "_id": "667289f903c802764985d8c6",
            "avatarUrl": "/avatars/916befcbf0e52ce56be49617f31c7bb2.svg",
            "isPro": false,
            "fullname": "Junchi Yan",
            "user": "Rethinker",
            "type": "user"
          },
          "name": "Junchi Yan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:22:30.311Z",
          "hidden": false
        },
        {
          "_id": "67ef4be0985aa66b67021de3",
          "name": "Hua Yang",
          "hidden": false
        },
        {
          "_id": "67ef4be0985aa66b67021de4",
          "user": {
            "_id": "648e77184cae4f6921dbb382",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/648e77184cae4f6921dbb382/zAAJRvOStC0wZplqVWrk_.jpeg",
            "isPro": false,
            "fullname": "Xue Yang",
            "user": "yangxue",
            "type": "user"
          },
          "name": "Xue Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:09:09.267Z",
          "hidden": false
        },
        {
          "_id": "67ef4be0985aa66b67021de5",
          "user": {
            "_id": "63ee1379190ddd6214efd73a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676546883247-noauth.png",
            "isPro": false,
            "fullname": "HAODONG DUAN",
            "user": "KennyUTC",
            "type": "user"
          },
          "name": "Haodong Duan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:22:37.146Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63ee1379190ddd6214efd73a/PQFr3_6S3BeUSNb79jMSO.png",
        "https://cdn-uploads.huggingface.co/production/uploads/63ee1379190ddd6214efd73a/0rGBBJT_JcUPBk5cE_Te0.png"
      ],
      "publishedAt": "2025-04-03T17:59:56.000Z",
      "submittedOnDailyAt": "2025-04-04T01:35:35.280Z",
      "title": "Pixels après l'imagination : un cadre de référence pour l'édition visuelle basée sur des preuves.",
      "submittedOnDailyBy": {
        "_id": "63ee1379190ddd6214efd73a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676546883247-noauth.png",
        "isPro": false,
        "fullname": "HAODONG DUAN",
        "user": "KennyUTC",
        "type": "user"
      },
      "summary": "Les grands modèles multimodal (MMMs) évoluent dans le domaine de la compréhension visuelle et de la génération, mais dans l'ensemble du domaine de l'édition visuelle, ils encourent encore des défis tels que la suivi d'instructions complexes, la maintenance de la cohérence externe et le soutien à des formats d'entrée flexibles. Pour combler ces lacunes, nous présentons RISEBench, un cadre d'évaluation. RISEBench met l'accent sur les principes temporels, causaux, spatiaux et logiques. Il fournit des cas de test de qualité élevée pour chaque catégorie et propose un cadre pour évaluer la raison des instructions, la cohérence externe et la possibilité visuelle, en utilisant un approche de jury de jury humaine et de modèles MMM-as-jury. Selon les résultats des expérimentations, GPT-4o-Native dépasse significativement les autres modèles ouverts et propriétaires, mais reconnaît également des difficultés dans le domaine des principes logiques, ce qui indique une recherche insuffisante dans cette régime. Grâce aux efforts initiaux, RISEBench fournit une base de connaissances fondamentales sur l'édition visuelle basée sur la logique et stimule des futures recherches. Bien que nous soit encore à l'origine, nous sommes déterminés à élargir et à améliorer continuément le cadre d'évaluation pour soutenir une évaluation plus stricte et échelonnable dans les futurs systèmes multimodal. Les codes et les données sont disponibles sur https://github.com/PhoenixZ810/RISEBench.",
      "upvotes": 40,
      "discussionId": "67ef4be4985aa66b67021ef2",
      "githubRepo": "https://github.com/PhoenixZ810/RISEBench",
      "ai_keywords": [
        "Large Multi-modality Models (LMMs)",
        "General Visual Editing",
        "Temporal Reasoning",
        "Causal Reasoning",
        "Spatial Reasoning",
        "Logical Reasoning",
        "RISEBench",
        "Instruction Reasoning",
        "Appearance Consistency",
        "Visual Plausibility",
        "GPT-4o-Native",
        "multimodal systems"
      ]
    },
    "publishedAt": "2025-04-03T13:59:56.000Z",
    "title": "Envisioning Beyond the Pixels: Benchmarking Reasoning-Informed Visual\n  Editing",
    "summary": "Large Multi-modality Models (LMMs) have made significant progress in visual\nunderstanding and generation, but they still face challenges in General Visual\nEditing, particularly in following complex instructions, preserving appearance\nconsistency, and supporting flexible input formats. To address this gap, we\nintroduce RISEBench, the first benchmark for evaluating Reasoning-Informed\nviSual Editing (RISE). RISEBench focuses on four key reasoning types: Temporal,\nCausal, Spatial, and Logical Reasoning. We curate high-quality test cases for\neach category and propose an evaluation framework that assesses Instruction\nReasoning, Appearance Consistency, and Visual Plausibility with both human\njudges and an LMM-as-a-judge approach. Our experiments reveal that while\nGPT-4o-Native significantly outperforms other open-source and proprietary\nmodels, even this state-of-the-art system struggles with logical reasoning\ntasks, highlighting an area that remains underexplored. As an initial effort,\nRISEBench aims to provide foundational insights into reasoning-aware visual\nediting and to catalyze future research. Though still in its early stages, we\nare committed to continuously expanding and refining the benchmark to support\nmore comprehensive, reliable, and scalable evaluations of next-generation\nmultimodal systems. Our code and data will be released at\nhttps://github.com/PhoenixZ810/RISEBench.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63ee1379190ddd6214efd73a/PQFr3_6S3BeUSNb79jMSO.png",
      "https://cdn-uploads.huggingface.co/production/uploads/63ee1379190ddd6214efd73a/0rGBBJT_JcUPBk5cE_Te0.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02826.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63ee1379190ddd6214efd73a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676546883247-noauth.png",
      "fullname": "HAODONG DUAN",
      "name": "KennyUTC",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 24
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.02782",
      "authors": [
        {
          "_id": "67ef502ce803d818f00e1b94",
          "name": "Zhiyuan Yan",
          "hidden": false
        },
        {
          "_id": "67ef502ce803d818f00e1b95",
          "user": {
            "_id": "66978ee0b8656f6506b4acb2",
            "avatarUrl": "/avatars/298acb8222e189fce4368985ee5374a1.svg",
            "isPro": false,
            "fullname": "Junyan Ye",
            "user": "Yejy53",
            "type": "user"
          },
          "name": "Junyan Ye",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:17:10.032Z",
          "hidden": false
        },
        {
          "_id": "67ef502ce803d818f00e1b96",
          "user": {
            "_id": "66d5b56c77a026c3d2086a79",
            "avatarUrl": "/avatars/45da07fd82fd455955faa05b27a6393f.svg",
            "isPro": false,
            "fullname": "Weijia Li",
            "user": "liweijia",
            "type": "user"
          },
          "name": "Weijia Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:16:44.819Z",
          "hidden": false
        },
        {
          "_id": "67ef502ce803d818f00e1b97",
          "user": {
            "_id": "6487e158f675b4a7867f45fa",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6487e158f675b4a7867f45fa/J0sls6zZ682o-SH7iQs7B.jpeg",
            "isPro": false,
            "fullname": "Zilong Huang",
            "user": "SereinH",
            "type": "user"
          },
          "name": "Zilong Huang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:08:56.501Z",
          "hidden": false
        },
        {
          "_id": "67ef502ce803d818f00e1b98",
          "user": {
            "_id": "63468720dd6d90d82ccf3450",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
            "isPro": false,
            "fullname": "YSH",
            "user": "BestWishYsh",
            "type": "user"
          },
          "name": "Shenghai Yuan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:09:05.246Z",
          "hidden": false
        },
        {
          "_id": "67ef502ce803d818f00e1b99",
          "user": {
            "_id": "67ef53656c7ba428e7c2e605",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/fFVJTpMRKF15Bf63yZEG_.png",
            "isPro": false,
            "fullname": "He",
            "user": "shawnxyh",
            "type": "user"
          },
          "name": "Xiangyang He",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:09:02.573Z",
          "hidden": false
        },
        {
          "_id": "67ef502ce803d818f00e1b9a",
          "user": {
            "_id": "6459a47e4fe72fae522b4fc9",
            "avatarUrl": "/avatars/a4139f8e348081e45b28dd99d96908d3.svg",
            "isPro": false,
            "fullname": "Kaiqing.Lin",
            "user": "lin6123",
            "type": "user"
          },
          "name": "Kaiqing Lin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:16:28.452Z",
          "hidden": false
        },
        {
          "_id": "67ef502ce803d818f00e1b9b",
          "user": {
            "_id": "670ddb69d6ac6394419d88c5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/XxnGNaX3FWug4aiZVjg93.png",
            "isPro": false,
            "fullname": "Jun He",
            "user": "JunHe0915",
            "type": "user"
          },
          "name": "Jun He",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:08:59.877Z",
          "hidden": false
        },
        {
          "_id": "67ef502ce803d818f00e1b9c",
          "user": {
            "_id": "63f9fca8d4349b157a109eec",
            "avatarUrl": "/avatars/fa1f2ae7972d7cde99dab178136ccbb0.svg",
            "isPro": false,
            "fullname": "Conghui He",
            "user": "conghui",
            "type": "user"
          },
          "name": "Conghui He",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:17:18.123Z",
          "hidden": false
        },
        {
          "_id": "67ef502ce803d818f00e1b9d",
          "user": {
            "_id": "66135a5e50350afe76beebce",
            "avatarUrl": "/avatars/370a4b83949355feb050c2cb0425c264.svg",
            "isPro": false,
            "fullname": "yl2488",
            "user": "yl2488",
            "type": "user"
          },
          "name": "Li Yuan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:08:40.281Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-03T17:23:16.000Z",
      "submittedOnDailyAt": "2025-04-04T01:51:34.697Z",
      "title": "GPT-ImgEval : Étapes détaillées d'évaluation pour la génération d'images par GPT4o",
      "submittedOnDailyBy": {
        "_id": "63468720dd6d90d82ccf3450",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
        "isPro": false,
        "fullname": "YSH",
        "user": "BestWishYsh",
        "type": "user"
      },
      "summary": "Récemment, le développement du modèle GPT4o de OpenAI a démontré des capacités exceptionnelles dans la génération et l'édition d'images, obtenant des résultats très intéressants dans la communauté. Dans ce rapport technique, un premier benchmark d'évaluation de projections appelé GPT-ImgEval est utilisé pour évaluer de manière quantitative et sincère le rendement de GPT-4o dans trois aspects importants : génération d'images, édition et synthèse de sens basée sur le monde. Dans ces trois tâches, GPT-4o montre un excellent rendement, surpassant considérablement les méthodes existantes en termes de contrôle de la génération et de qualité de la sortie, et en particulier, démontre une capacité d'inférence de connaissance. De plus, un approche basée sur des modèles de classification est proposée pour explorer l'architecture potentielle du modèle à partir des données générées par GPT-4o, et les résultats des expériences montrent que le modèle a une structure combinant l'auto-régression (AR) et des têtes basées sur la ramification. Des limites concrètes observées dans la génération d'images de GPT-4o ont également été identifiées et visualisées, et des feedback synthétiques ont été fournis. De plus, une comparaison de étude sur l'édition d'images itérative entre GPT-4o et Gemini 2.0 Flash a été réalisée, et l'impact sur la sécurité des résultats de GPT-4o a été discuté. Les codes et ensembles de données utilisés dans l'évaluation de GPT-4o peuvent être obtenus sur https://github.com/PicoTrex/GPT-ImgEval.",
      "upvotes": 28,
      "discussionId": "67ef502fe803d818f00e1c70",
      "githubRepo": "https://github.com/PicoTrex/GPT-ImgEval",
      "ai_keywords": [
        "auto-regressive (AR)",
        "diffusion-based head",
        "VAR-like architectures",
        "multi-round image editing",
        "image forensic models"
      ]
    },
    "publishedAt": "2025-04-03T13:23:16.000Z",
    "title": "GPT-ImgEval: A Comprehensive Benchmark for Diagnosing GPT4o in Image\n  Generation",
    "summary": "The recent breakthroughs in OpenAI's GPT4o model have demonstrated\nsurprisingly good capabilities in image generation and editing, resulting in\nsignificant excitement in the community. This technical report presents the\nfirst-look evaluation benchmark (named GPT-ImgEval), quantitatively and\nqualitatively diagnosing GPT-4o's performance across three critical dimensions:\n(1) generation quality, (2) editing proficiency, and (3) world\nknowledge-informed semantic synthesis. Across all three tasks, GPT-4o\ndemonstrates strong performance, significantly surpassing existing methods in\nboth image generation control and output quality, while also showcasing\nexceptional knowledge reasoning capabilities. Furthermore, based on the\nGPT-4o's generated data, we propose a classification-model-based approach to\ninvestigate the underlying architecture of GPT-4o, where our empirical results\nsuggest the model consists of an auto-regressive (AR) combined with a\ndiffusion-based head for image decoding, rather than the VAR-like\narchitectures. We also provide a complete speculation on GPT-4o's overall\narchitecture. In addition, we conduct a series of analyses to identify and\nvisualize GPT-4o's specific limitations and the synthetic artifacts commonly\nobserved in its image generation. We also present a comparative study of\nmulti-round image editing between GPT-4o and Gemini 2.0 Flash, and discuss the\nsafety implications of GPT-4o's outputs, particularly their detectability by\nexisting image forensic models. We hope that our work can offer valuable\ninsight and provide a reliable benchmark to guide future research, foster\nreproducibility, and accelerate innovation in the field of image generation and\nbeyond. The codes and datasets used for evaluating GPT-4o can be found at\nhttps://github.com/PicoTrex/GPT-ImgEval.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02782.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63468720dd6d90d82ccf3450",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
      "fullname": "YSH",
      "name": "BestWishYsh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 36
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.02587",
      "authors": [
        {
          "_id": "67ef3f9804be7fba0c882738",
          "user": {
            "_id": "633fc70529b5a95f6e15a6b7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/633fc70529b5a95f6e15a6b7/Fzh7wWuqU-fBbzdupOUtF.jpeg",
            "isPro": false,
            "fullname": "Yan Ma",
            "user": "ManTle",
            "type": "user"
          },
          "name": "Yan Ma",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:17:53.820Z",
          "hidden": false
        },
        {
          "_id": "67ef3f9804be7fba0c882739",
          "user": {
            "_id": "64b370fe6d953e7c75ede314",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b370fe6d953e7c75ede314/RdP2q3hGXWE4E2zfSv0KU.png",
            "isPro": false,
            "fullname": "Steffi Chern",
            "user": "steffichern",
            "type": "user"
          },
          "name": "Steffi Chern",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:09:14.660Z",
          "hidden": false
        },
        {
          "_id": "67ef3f9804be7fba0c88273a",
          "user": {
            "_id": "642e4d4d6748dd4f8eeb7732",
            "avatarUrl": "/avatars/fd911e9143d1a7aedd21a7d611543fcc.svg",
            "isPro": false,
            "fullname": "Xuyang Shen",
            "user": "Ryan1122",
            "type": "user"
          },
          "name": "Xuyang Shen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:17:40.397Z",
          "hidden": false
        },
        {
          "_id": "67ef3f9804be7fba0c88273b",
          "user": {
            "_id": "64c525e4d68946edad6c7067",
            "avatarUrl": "/avatars/1b108661634af602717a4ab4b66a151f.svg",
            "isPro": false,
            "fullname": "Yiran Zhong",
            "user": "IanZhong",
            "type": "user"
          },
          "name": "Yiran Zhong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:09:16.707Z",
          "hidden": false
        },
        {
          "_id": "67ef3f9804be7fba0c88273c",
          "user": {
            "_id": "6144a0c4ff1146bbd84d9865",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1661715958139-6144a0c4ff1146bbd84d9865.png",
            "isPro": false,
            "fullname": "Pengfei Liu",
            "user": "Pengfei",
            "type": "user"
          },
          "name": "Pengfei Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:17:34.472Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-03T13:53:28.000Z",
      "submittedOnDailyAt": "2025-04-04T00:42:23.044Z",
      "title": "Réfléchir à l'échelle de RL appliquée aux modèles de langage visuel : un cadre transparent de travail de bout en bout et des scripts d'évaluation détaillés",
      "submittedOnDailyBy": {
        "_id": "633fc70529b5a95f6e15a6b7",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/633fc70529b5a95f6e15a6b7/Fzh7wWuqU-fBbzdupOUtF.jpeg",
        "isPro": false,
        "fullname": "Yan Ma",
        "user": "ManTle",
        "type": "user"
      },
      "summary": "L'apprentissage par renforcement (RL) a récemment acquis une notoriété comme une puissante possibilité pour améliorer le rendement des modèles de langue de grande échelle, et il est en développement pour son application dans les modèles de langue visuelle (VLMs). Cependant, l'application du RL dans les VLMs actuels se base principalement sur des architectures complexes qui comprometent la reproductibilité et l'accessibilité, et la manque de protocoles d'évaluation standard a difficile la comparaison des résultats et l'interprétation des dynamiques d'apprentissage. Dans cet article, nous proposons un simple et fonctionnel pipeline de 4 étapes pour l'RL dans les VLMs, offrant un cadre transparent vérifié dans différents modèles et ensembles de données pour standardiser les protocoles d'évaluation et proposer des techniques d'évaluation standards pour la dynamique d'apprentissage et le comportement réflexif. Dans des expériences distribuées sur des tâches de raisonnement visuel, nous avons découvert les principaux résultats suivants : la longueur de la réponse est sensible à l'aléa, la réflexion a une corrélation avec la longueur de l'output, et le RL montre un rendement plus constant en généralisation sur des données de haute qualité par rapport à l'adaptation par fine-tuning (SFT). Ces résultats et le cadre proposé se concentrent sur établir une ligne de base reproductible et soutenir une large collaboration dans l'étude des VLMs basées sur l'RL.",
      "upvotes": 19,
      "discussionId": "67ef3f9904be7fba0c882772",
      "ai_keywords": [
        "reinforcement learning",
        "reasoning capabilities",
        "large language models",
        "vision-language models",
        "reproducibility",
        "accessibility",
        "standardized evaluation protocols",
        "transparent framework",
        "four-step pipeline",
        "training dynamics",
        "reflective behaviors",
        "visual reasoning tasks",
        "response length",
        "reflection",
        "supervised fine-tuning"
      ]
    },
    "publishedAt": "2025-04-03T09:53:28.000Z",
    "title": "Rethinking RL Scaling for Vision Language Models: A Transparent,\n  From-Scratch Framework and Comprehensive Evaluation Scheme",
    "summary": "Reinforcement learning (RL) has recently shown strong potential in improving\nthe reasoning capabilities of large language models and is now being actively\nextended to vision-language models (VLMs). However, existing RL applications in\nVLMs often rely on heavily engineered frameworks that hinder reproducibility\nand accessibility, while lacking standardized evaluation protocols, making it\ndifficult to compare results or interpret training dynamics. This work\nintroduces a transparent, from-scratch framework for RL in VLMs, offering a\nminimal yet functional four-step pipeline validated across multiple models and\ndatasets. In addition, a standardized evaluation scheme is proposed to assess\ntraining dynamics and reflective behaviors. Extensive experiments on visual\nreasoning tasks uncover key empirical findings: response length is sensitive to\nrandom seeds, reflection correlates with output length, and RL consistently\noutperforms supervised fine-tuning (SFT) in generalization, even with\nhigh-quality data. These findings, together with the proposed framework, aim to\nestablish a reproducible baseline and support broader engagement in RL-based\nVLM research.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02587.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "633fc70529b5a95f6e15a6b7",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/633fc70529b5a95f6e15a6b7/Fzh7wWuqU-fBbzdupOUtF.jpeg",
      "fullname": "Yan Ma",
      "name": "ManTle",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.02436",
      "authors": [
        {
          "_id": "67ef3dfae8b932ae7a832950",
          "user": {
            "_id": "617ba1820e4237bd1731b867",
            "avatarUrl": "/avatars/f9de06363e64bddd7dc977e96e85df8a.svg",
            "isPro": false,
            "fullname": "zhengcong fei",
            "user": "onion",
            "type": "user"
          },
          "name": "Zhengcong Fei",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:19:16.548Z",
          "hidden": false
        },
        {
          "_id": "67ef3dfae8b932ae7a832951",
          "user": {
            "_id": "65dc3a850af7e21ba40e939f",
            "avatarUrl": "/avatars/e129c64617675edd05d4317d39604318.svg",
            "isPro": false,
            "fullname": "Li",
            "user": "Debang",
            "type": "user"
          },
          "name": "Debang Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:19:27.042Z",
          "hidden": false
        },
        {
          "_id": "67ef3dfae8b932ae7a832952",
          "user": {
            "_id": "65bef422fdb8d33cefeaccc3",
            "avatarUrl": "/avatars/d40b0d7dda21fa1a68c291d11bc357ec.svg",
            "isPro": false,
            "fullname": "Qiu Di",
            "user": "diqiu7",
            "type": "user"
          },
          "name": "Di Qiu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:19:41.458Z",
          "hidden": false
        },
        {
          "_id": "67ef3dfae8b932ae7a832953",
          "name": "Jiahua Wang",
          "hidden": false
        },
        {
          "_id": "67ef3dfae8b932ae7a832954",
          "name": "Yikun Dou",
          "hidden": false
        },
        {
          "_id": "67ef3dfae8b932ae7a832955",
          "user": {
            "_id": "62e0f1314db2175cd270ad08",
            "avatarUrl": "/avatars/1d3d6af6c63557f4abf0484e028fa942.svg",
            "isPro": false,
            "fullname": "Rui Wang",
            "user": "ruiwang",
            "type": "user"
          },
          "name": "Rui Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:20:11.206Z",
          "hidden": false
        },
        {
          "_id": "67ef3dfae8b932ae7a832956",
          "user": {
            "_id": "666a674967c686801acf25bb",
            "avatarUrl": "/avatars/c1f3edd63fd378dfb555e6413a966932.svg",
            "isPro": false,
            "fullname": "jingtao xu",
            "user": "raul678",
            "type": "user"
          },
          "name": "Jingtao Xu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:20:20.880Z",
          "hidden": false
        },
        {
          "_id": "67ef3dfae8b932ae7a832957",
          "user": {
            "_id": "634672bfb7b4e71c7f45360f",
            "avatarUrl": "/avatars/4b646fc3e271be90b9ec619d42ce3e99.svg",
            "isPro": false,
            "fullname": "Fan Mingyuan",
            "user": "MichaelFan",
            "type": "user"
          },
          "name": "Mingyuan Fan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:20:32.597Z",
          "hidden": false
        },
        {
          "_id": "67ef3dfae8b932ae7a832958",
          "name": "Guibin Chen",
          "hidden": false
        },
        {
          "_id": "67ef3dfae8b932ae7a832959",
          "name": "Yang Li",
          "hidden": false
        },
        {
          "_id": "67ef3dfae8b932ae7a83295a",
          "name": "Yahui Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-03T09:50:50.000Z",
      "submittedOnDailyAt": "2025-04-04T00:33:57.000Z",
      "title": "SkyReels-A2 : SkyReels-A2 : SkyReels-A2 : Transformateur de diffusion cinématographique pour créer tout.",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Dans cet article, nous présentons SkyReels-A2, un cadre de création de vidéos avec contrôle. Ce cadre permet de combiner n'importe quel élément visuel (par exemple, personnages, objets, fonds) dans un vidéo à travers un programme de chaînes de caractères, en maintenant une précise concordance avec les images de référence. Ce travail est connu sous le nom d'Élément-à-Vidéo (E2V) et présente principalement les problèmes de maintenir l'authenticité des éléments, garantir la continuité des images et atteindre un résultat naturel. Pour aborder ces problèmes, nous concevons d'abord une pipeline de données détaillée pour construire des tuples de programme de chaînes de caractères-images de référence-vidéos pour l'entraînement du modèle. Ensuite, nous proposons un modèle de cartographie commun entre images et texte, et nous introduisons des représentations multi-élémentaires dans le processus de génération pour équilibrer la concordance spécifique des éléments, la continuité globale et la correspondance du contexte. De plus, nous optimisons la pipeline d'inférence pour une plus grande vitesse et stabilité de sortie. De plus, nous présentons A2 Bench, un cadre de validation probant, avec l'objectif de évaluer le système. Les expérimentations montrent que notre cadre de travail peut générer des vidéos de différentes qualités et contrôler précisément les éléments. SkyReels-A2 est le premier modèle commercial ouvert en création d'E2V, et est valorisé bien comparé aux modèles commerciaux fermés développés. Nous espérons que SkyReels-A2 dépasse les limites de la possibilité de contrôle dans la génération de vidéos dans des applications créatives telles que les drames ou les entreprises virtuelles.",
      "upvotes": 17,
      "discussionId": "67ef3dfee8b932ae7a832a97",
      "ai_keywords": [
        "elements-to-video (E2V)",
        "image-text joint embedding model",
        "prompt-reference-video triplets",
        "generative process",
        "multi-element representations",
        "strict consistency",
        "coherent composition",
        "natural outputs",
        "output stability",
        "A2 Bench (benchmark)",
        "high-quality videos",
        "precise element control",
        "open-source commercial grade model"
      ]
    },
    "publishedAt": "2025-04-03T05:50:50.000Z",
    "title": "SkyReels-A2: Compose Anything in Video Diffusion Transformers",
    "summary": "This paper presents SkyReels-A2, a controllable video generation framework\ncapable of assembling arbitrary visual elements (e.g., characters, objects,\nbackgrounds) into synthesized videos based on textual prompts while maintaining\nstrict consistency with reference images for each element. We term this task\nelements-to-video (E2V), whose primary challenges lie in preserving the\nfidelity of each reference element, ensuring coherent composition of the scene,\nand achieving natural outputs. To address these, we first design a\ncomprehensive data pipeline to construct prompt-reference-video triplets for\nmodel training. Next, we propose a novel image-text joint embedding model to\ninject multi-element representations into the generative process, balancing\nelement-specific consistency with global coherence and text alignment. We also\noptimize the inference pipeline for both speed and output stability. Moreover,\nwe introduce a carefully curated benchmark for systematic evaluation, i.e, A2\nBench. Experiments demonstrate that our framework can generate diverse,\nhigh-quality videos with precise element control. SkyReels-A2 is the first\nopen-source commercial grade model for the generation of E2V, performing\nfavorably against advanced closed-source commercial models. We anticipate\nSkyReels-A2 will advance creative applications such as drama and virtual\ne-commerce, pushing the boundaries of controllable video generation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02436.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6573
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.00502",
      "authors": [
        {
          "_id": "67ef72898667ee5c99026d16",
          "user": {
            "_id": "67014d33126f9ab39fc52481",
            "avatarUrl": "/avatars/60d1f791e7f3201ce1aef72e9216ff78.svg",
            "isPro": false,
            "fullname": "Qianhao Yuan",
            "user": "yuanqianhao",
            "type": "user"
          },
          "name": "Qianhao Yuan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:23:28.149Z",
          "hidden": false
        },
        {
          "_id": "67ef72898667ee5c99026d17",
          "name": "Qingyu Zhang",
          "hidden": false
        },
        {
          "_id": "67ef72898667ee5c99026d18",
          "name": "Yanjiang Liu",
          "hidden": false
        },
        {
          "_id": "67ef72898667ee5c99026d19",
          "user": {
            "_id": "654c7fbe6b51714c2a6ff590",
            "avatarUrl": "/avatars/db217415c56730872b9a807f3afb4e5b.svg",
            "isPro": false,
            "fullname": "Jiawei Chen",
            "user": "chenjiawei-icip",
            "type": "user"
          },
          "name": "Jiawei Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:08:04.134Z",
          "hidden": false
        },
        {
          "_id": "67ef72898667ee5c99026d1a",
          "user": {
            "_id": "6216496a9b34d2fb49144599",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6216496a9b34d2fb49144599/41CKA_h1Ffj3RzVabSAkm.jpeg",
            "isPro": false,
            "fullname": "Yaojie Lu",
            "user": "luyaojie",
            "type": "user"
          },
          "name": "Yaojie Lu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:24:09.511Z",
          "hidden": false
        },
        {
          "_id": "67ef72898667ee5c99026d1b",
          "user": {
            "_id": "6711c702f858a456b4b9f3a4",
            "avatarUrl": "/avatars/178e9567c3111ab22717c3c0dd003a6a.svg",
            "isPro": false,
            "fullname": "Hongyu  Lin",
            "user": "sanmusunrise",
            "type": "user"
          },
          "name": "Hongyu Lin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:24:15.188Z",
          "hidden": false
        },
        {
          "_id": "67ef72898667ee5c99026d1c",
          "name": "Jia Zheng",
          "hidden": false
        },
        {
          "_id": "67ef72898667ee5c99026d1d",
          "user": {
            "_id": "65e99a77e71555ed193609cf",
            "avatarUrl": "/avatars/38ceb127883944677665da967d17dd18.svg",
            "isPro": false,
            "fullname": "Xianpei Han",
            "user": "xphan",
            "type": "user"
          },
          "name": "Xianpei Han",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:24:23.046Z",
          "hidden": false
        },
        {
          "_id": "67ef72898667ee5c99026d1e",
          "name": "Le Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-01T07:47:55.000Z",
      "submittedOnDailyAt": "2025-04-04T04:19:46.946Z",
      "title": "Descapes d'images et tokens de refroidissement non spécifiquement efficaces pour des modèles de langage multi-modal efficaces",
      "submittedOnDailyBy": {
        "_id": "67014d33126f9ab39fc52481",
        "avatarUrl": "/avatars/60d1f791e7f3201ce1aef72e9216ff78.svg",
        "isPro": false,
        "fullname": "Qianhao Yuan",
        "user": "yuanqianhao",
        "type": "user"
      },
      "summary": "Les modèles de langage grand de Dialogue Multimodal (MLLMs) souvent subissent des coûts de calcul élevés en raison de leur grande échelle et de l'utilisation de tokens visuels étendus. Dans cet article, nous introduisons un nouveau métrique appelé « Contribution de la Couche (LC) » pour quantifier l'impact de la transformation d'une couche sur les tokens visuels et contextuels. Le calcul de LC est effectué en mesurant la variance entre le modèle et la transformation d'une couche qui supprime les tokens spécifiques. Les tests de fatigue montrent clairement que de nombreuses couches de MLLMs contribuent peu au traitement des tokens visuels. Sur la base de ces observations, nous proposons un méthode appelé « ShortV » qui utilise LC pour identifier les couches inutiles et bloquer l'actualisation des tokens visuels dans ces couches, sans nécessité d'entraînement supplémentaire. Les résultats des expériences montrent que ShortV peut bloquer l'actualisation des tokens visuels dans environ 60% des couches de MLLM, réduisant significativement les coûts de calcul associés. Par exemple, dans le modèle LLaVA-NeXT-13B, il est possible de réduire les FLOPs d'un 50% tout en maintenant la qualité du rendement. Le code est disponible sur https://github.com/icip-cas/ShortV.",
      "upvotes": 11,
      "discussionId": "67ef728a8667ee5c99026d69",
      "githubRepo": "https://github.com/icip-cas/ShortV",
      "ai_keywords": [
        "Multimodal Large Language Models (MLLMs)",
        "Layer Contribution (LC)",
        "visual tokens",
        "transformations",
        "layer-wise redundancy",
        "model output",
        "divergence",
        "ineffective layers",
        "training-free method",
        "visual token updates",
        "computational costs",
        "FLOPs",
        "LLaVA-NeXT-13B"
      ]
    },
    "publishedAt": "2025-04-01T03:47:55.000Z",
    "title": "ShortV: Efficient Multimodal Large Language Models by Freezing Visual\n  Tokens in Ineffective Layers",
    "summary": "Multimodal Large Language Models (MLLMs) suffer from high computational costs\ndue to their massive size and the large number of visual tokens. In this paper,\nwe investigate layer-wise redundancy in MLLMs by introducing a novel metric,\nLayer Contribution (LC), which quantifies the impact of a layer's\ntransformations on visual and text tokens, respectively. The calculation of LC\ninvolves measuring the divergence in model output that results from removing\nthe layer's transformations on the specified tokens. Our pilot experiment\nreveals that many layers of MLLMs exhibit minimal contribution during the\nprocessing of visual tokens. Motivated by this observation, we propose ShortV,\na training-free method that leverages LC to identify ineffective layers, and\nfreezes visual token updates in these layers. Experiments show that ShortV can\nfreeze visual token in approximately 60\\% of the MLLM layers, thereby\ndramatically reducing computational costs related to updating visual tokens.\nFor example, it achieves a 50\\% reduction in FLOPs on LLaVA-NeXT-13B while\nmaintaining superior performance. The code will be publicly available at\nhttps://github.com/icip-cas/ShortV",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00502.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67014d33126f9ab39fc52481",
      "avatarUrl": "/avatars/60d1f791e7f3201ce1aef72e9216ff78.svg",
      "fullname": "Qianhao Yuan",
      "name": "yuanqianhao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.02542",
      "authors": [
        {
          "_id": "67ef3773ac0c701df7fd98aa",
          "user": {
            "_id": "6264a7dfc39850dc093eb68a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1650763566575-noauth.png",
            "isPro": false,
            "fullname": "Fa-Ting Hong",
            "user": "HarlanHong",
            "type": "user"
          },
          "name": "Fa-Ting Hong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:09:38.641Z",
          "hidden": false
        },
        {
          "_id": "67ef3773ac0c701df7fd98ab",
          "user": {
            "_id": "6481523b3fb124fc9850afed",
            "avatarUrl": "/avatars/ddde178c88713662800aafd2343647a4.svg",
            "isPro": false,
            "fullname": "Zunnan Xu",
            "user": "xuzn",
            "type": "user"
          },
          "name": "Zunnan Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:09:23.733Z",
          "hidden": false
        },
        {
          "_id": "67ef3773ac0c701df7fd98ac",
          "name": "Zixiang Zhou",
          "hidden": false
        },
        {
          "_id": "67ef3773ac0c701df7fd98ad",
          "name": "Jun Zhou",
          "hidden": false
        },
        {
          "_id": "67ef3773ac0c701df7fd98ae",
          "name": "Xiu Li",
          "hidden": false
        },
        {
          "_id": "67ef3773ac0c701df7fd98af",
          "name": "Qin Lin",
          "hidden": false
        },
        {
          "_id": "67ef3773ac0c701df7fd98b0",
          "name": "Qinglin Lu",
          "hidden": false
        },
        {
          "_id": "67ef3773ac0c701df7fd98b1",
          "user": {
            "_id": "66feab48651e00e22f33222e",
            "avatarUrl": "/avatars/7344377e2c796c7ec85194bb2fc78521.svg",
            "isPro": false,
            "fullname": "Dan Xu",
            "user": "danxuhk",
            "type": "user"
          },
          "name": "Dan Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:09:20.987Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/66feab48651e00e22f33222e/VOMaQBXsDjs1295R5NVOh.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/66feab48651e00e22f33222e/tksGfiG1zEDZ4QUg4IorF.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/66feab48651e00e22f33222e/aUp8tmNM2pdPYCa3A3Noh.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/66feab48651e00e22f33222e/MpCNSPEP6OSjZR_0m70PP.mp4"
      ],
      "publishedAt": "2025-04-03T12:44:41.000Z",
      "submittedOnDailyAt": "2025-04-04T01:37:45.934Z",
      "title": "Modélisation vidéo avec contrôle par états cachés, états de sélection spatiale masqués, modélisation de tokens naturels avec tête de génération",
      "submittedOnDailyBy": {
        "_id": "66feab48651e00e22f33222e",
        "avatarUrl": "/avatars/7344377e2c796c7ec85194bb2fc78521.svg",
        "isPro": false,
        "fullname": "Dan Xu",
        "user": "danxuhk",
        "type": "user"
      },
      "summary": "La synthèse de têtes en mouvement est cruciale pour la conversion vidéo-à-vidéo et l'interaction humain-ordinateur, mais les méthodes actuelles souvent sont limitées par leur contrôle basé sur un seul modèle principal, ce qui rend difficile leur application pratique. Dans ce contexte, nous présentons ACTalker. ACTalker est un cadre de travail pour la génération de vidéos qui soutient tant un contrôle multi-signal que un contrôle uni-signal. Pour le contrôle multi-signal, nous avons conçu une structure mamba parallèle, permettant à chaque section d'utiliser un signal de contrôle différent pour manipuler spécifiquement des régions faciales. Nous avons appliqué une structure de porte dans chaque section pour offrir un contrôle flexible dans la génération de vidéos. Pour garantir des associations naturelles dans le temps et l'espace, nous avons utilisé la structure mamba, ce qui permet à chaque section de manipuler des caractères de tokens sur les deux axes. De plus, nous avons introduit la stratégie de drop de masque pour permettre à chaque signal de contrôle de contrôler indépendamment les régions faciales correspondantes dans la structure mamba, évitant ainsi les conflits. À travers les résultats expérimentaux, nous avons démontré que notre méthode génère des vidéos de visages naturels en mouvement avec plusieurs signaux de contrôle, et que les couches mamba peuvent intégrer de manière continue et sans conflits plusieurs modèles de contrôle.",
      "upvotes": 8,
      "discussionId": "67ef3775ac0c701df7fd994c",
      "projectPage": "https://harlanhong.github.io/publications/actalker/index.html",
      "githubRepo": "https://github.com/harlanhong/ACTalker",
      "ai_keywords": [
        "ACTalker",
        "video diffusion framework",
        "multi-signals control",
        "parallel mamba structure",
        "driving signals",
        "gate mechanism",
        "temporal coordination",
        "spatial coordination",
        "feature tokens",
        "mask-drop strategy",
        "facial videos",
        "multiple driving modalities"
      ]
    },
    "publishedAt": "2025-04-03T08:44:41.000Z",
    "title": "Audio-visual Controlled Video Diffusion with Masked Selective State\n  Spaces Modeling for Natural Talking Head Generation",
    "summary": "Talking head synthesis is vital for virtual avatars and human-computer\ninteraction. However, most existing methods are typically limited to accepting\ncontrol from a single primary modality, restricting their practical utility. To\nthis end, we introduce ACTalker, an end-to-end video diffusion\nframework that supports both multi-signals control and single-signal control\nfor talking head video generation. For multiple control, we design a parallel\nmamba structure with multiple branches, each utilizing a separate driving\nsignal to control specific facial regions. A gate mechanism is applied across\nall branches, providing flexible control over video generation. To ensure\nnatural coordination of the controlled video both temporally and spatially, we\nemploy the mamba structure, which enables driving signals to manipulate feature\ntokens across both dimensions in each branch. Additionally, we introduce a\nmask-drop strategy that allows each driving signal to independently control its\ncorresponding facial region within the mamba structure, preventing control\nconflicts. Experimental results demonstrate that our method produces\nnatural-looking facial videos driven by diverse signals and that the mamba\nlayer seamlessly integrates multiple driving modalities without conflict.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/66feab48651e00e22f33222e/VOMaQBXsDjs1295R5NVOh.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/66feab48651e00e22f33222e/tksGfiG1zEDZ4QUg4IorF.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/66feab48651e00e22f33222e/aUp8tmNM2pdPYCa3A3Noh.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/66feab48651e00e22f33222e/MpCNSPEP6OSjZR_0m70PP.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02542.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66feab48651e00e22f33222e",
      "avatarUrl": "/avatars/7344377e2c796c7ec85194bb2fc78521.svg",
      "fullname": "Dan Xu",
      "name": "danxuhk",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.02507",
      "authors": [
        {
          "_id": "67ef5a3d4417508df8d99dad",
          "user": {
            "_id": "62cd4b03c5cc157be82f0b56",
            "avatarUrl": "/avatars/351e963c1c763d507ae78cbcd62966a3.svg",
            "isPro": false,
            "fullname": "Abhay kumar",
            "user": "akanyaani",
            "type": "user"
          },
          "name": "Abhay Kumar",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:08:18.512Z",
          "hidden": false
        },
        {
          "_id": "67ef5a3d4417508df8d99dae",
          "user": {
            "_id": "6071c4b270e11b30cfcfd7a3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6071c4b270e11b30cfcfd7a3/-1ekCBzSTpqxkkul0bgmI.jpeg",
            "isPro": false,
            "fullname": "Louis Owen",
            "user": "louisowen6",
            "type": "user"
          },
          "name": "Louis Owen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:08:21.051Z",
          "hidden": false
        },
        {
          "_id": "67ef5a3d4417508df8d99daf",
          "user": {
            "_id": "645a0d3dd6648853107c5fdc",
            "avatarUrl": "/avatars/1e3b6a4f5ce81a707ba7cbdf81631091.svg",
            "isPro": false,
            "fullname": "Nilabhra Roy Chowdhury",
            "user": "nilabhra",
            "type": "user"
          },
          "name": "Nilabhra Roy Chowdhury",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:08:12.764Z",
          "hidden": false
        },
        {
          "_id": "67ef5a3d4417508df8d99db0",
          "user": {
            "_id": "65e4be59e8b017ee1310a1b6",
            "avatarUrl": "/avatars/c3f7cdf5d0859cb80bfb2b970a675dfa.svg",
            "isPro": false,
            "fullname": "Fabian",
            "user": "gueraf",
            "type": "user"
          },
          "name": "Fabian Güra",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:08:16.132Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-03T11:41:55.000Z",
      "submittedOnDailyAt": "2025-04-04T02:34:36.631Z",
      "title": "ZClip : Adaptation de l'stress de parole dans l'apprentissage préalable de modèles de LLM",
      "submittedOnDailyBy": {
        "_id": "6071c4b270e11b30cfcfd7a3",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6071c4b270e11b30cfcfd7a3/-1ekCBzSTpqxkkul0bgmI.jpeg",
        "isPro": false,
        "fullname": "Louis Owen",
        "user": "louisowen6",
        "type": "user"
      },
      "summary": "Les grands modèles de langue (LLMs) en train de se former rencontrent plusieurs problèmes tels que l'instabilité des gradients et les pics de perte. Ces phénomènes peuvent provoquer des catastrophes de gradients, nécessitant la récupération de points de contrôle de coût élevé et le saut de batchs de données. Les techniques de coupe du gradient basées sur des seuils fixes ou des heuristiques ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques, ne peuvent pas résoudre efficacement ces problèmes. Ces techniques, basées sur des seuils fixes ou des heuristiques,",
      "upvotes": 7,
      "discussionId": "67ef5a3e4417508df8d99dfc",
      "githubRepo": "https://github.com/bluorion-com/ZClip/",
      "ai_keywords": [
        "large language models (LLMs)",
        "gradient instability",
        "loss spikes",
        "catastrophic divergence",
        "checkpoint restoration",
        "data batch skipping",
        "traditional gradient clipping techniques",
        "norm-based methods",
        "adaptive gradient clipping",
        "clipping threshold",
        "statistical properties of gradient norms",
        "z-score-based anomaly detection",
        "malignant loss spikes",
        "convergence"
      ]
    },
    "publishedAt": "2025-04-03T07:41:55.000Z",
    "title": "ZClip: Adaptive Spike Mitigation for LLM Pre-Training",
    "summary": "Training large language models (LLMs) presents numerous challenges, including\ngradient instability and loss spikes. These phenomena can lead to catastrophic\ndivergence, requiring costly checkpoint restoration and data batch skipping.\nTraditional gradient clipping techniques, such as constant or norm-based\nmethods, fail to address these issues effectively due to their reliance on\nfixed thresholds or heuristics, leading to inefficient learning and requiring\nfrequent manual intervention. In this work, we propose ZClip, an adaptive\ngradient clipping algorithm that dynamically adjusts the clipping threshold\nbased on statistical properties of gradient norms over time. Unlike prior\nreactive strategies, ZClip proactively adapts to training dynamics without\nmaking any prior assumptions on the scale and the temporal evolution of\ngradient norms. At its core, it leverages z-score-based anomaly detection to\nidentify and mitigate large gradient spikes, preventing malignant loss spikes\nwhile not interfering with convergence otherwise. Our code is available at:\nhttps://github.com/bluorion-com/ZClip.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02507.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6071c4b270e11b30cfcfd7a3",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6071c4b270e11b30cfcfd7a3/-1ekCBzSTpqxkkul0bgmI.jpeg",
      "fullname": "Louis Owen",
      "name": "louisowen6",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.02398",
      "authors": [
        {
          "_id": "67ef63b5e8b932ae7a8d3043",
          "user": {
            "_id": "66b9bc2dacdbc1d0b39c3b50",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/hwR0pVfP_E8XjimXIxDOU.jpeg",
            "isPro": false,
            "fullname": "Gallil Maimon",
            "user": "gallilmaimon",
            "type": "user"
          },
          "name": "Gallil Maimon",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:08:06.509Z",
          "hidden": false
        },
        {
          "_id": "67ef63b5e8b932ae7a8d3044",
          "user": {
            "_id": "6547411a9295970f878aa52e",
            "avatarUrl": "/avatars/6e240f0add27bf1a6c04a9618eccdf83.svg",
            "isPro": false,
            "fullname": "Michael Hassid",
            "user": "hassid",
            "type": "user"
          },
          "name": "Michael Hassid",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:24:39.934Z",
          "hidden": false
        },
        {
          "_id": "67ef63b5e8b932ae7a8d3045",
          "user": {
            "_id": "64b7b7b38ba7d6c922d753d6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b7b7b38ba7d6c922d753d6/rt0thjYa84VZHy1BEcW4p.jpeg",
            "isPro": false,
            "fullname": "Amit Roth",
            "user": "MajoRoth",
            "type": "user"
          },
          "name": "Amit Roth",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:24:48.483Z",
          "hidden": false
        },
        {
          "_id": "67ef63b5e8b932ae7a8d3046",
          "user": {
            "_id": "6481e135578646b5c2386728",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6481e135578646b5c2386728/SPva4iNw0pORiCXD45cx9.jpeg",
            "isPro": false,
            "fullname": "Yossi Adi",
            "user": "adiyoss",
            "type": "user"
          },
          "name": "Yossi Adi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:24:54.851Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/66b9bc2dacdbc1d0b39c3b50/wqUq-bT-DvKoNybPX46uL.png"
      ],
      "publishedAt": "2025-04-03T08:46:56.000Z",
      "submittedOnDailyAt": "2025-04-04T03:52:15.607Z",
      "title": "Intralíver de la Speech - Analyse de l'Échelonnage de Modèles de Langage Textuel",
      "submittedOnDailyBy": {
        "_id": "66b9bc2dacdbc1d0b39c3b50",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/hwR0pVfP_E8XjimXIxDOU.jpeg",
        "isPro": false,
        "fullname": "Gallil Maimon",
        "user": "gallilmaimon",
        "type": "user"
      },
      "summary": "Actuellement, l'analyse d'échelle de modèles de langage (SLM) décrit des joueurs sombres. Ils prédisent que les SLM nécessiteront une grande quantité de calculs et de données par rapport aux documents, ce qui soulève des doutes sur la possibilité d'entraîner des SLMs de haute qualité. Cependant, les SLMs modernes sont généralement initialisés par un méthode qui croise le langage et le texte, à partir de modèles pré-entraînés TextLM, et sont conçus pour permettre la transmission de connaissances. Cela soulève la question de savoir si un SLM sans interruptions peut être échelonné de manière plus efficace qu'un SLM sans texte. Cet article résout ces problèmes de manière rigoureuse et montre des résultats positifs. Nous avons effectué une analyse d'échelle d'un SLM sans interruptions, entraîné des dizaines de modèles et analysé les tendances d'échelle. Dans cette configuration, nous avons découvert que les SLMs peuvent être échelonnés de manière plus efficace en fonction de la quantité de calculs. De plus, nos résultats montrent que l'échelle est dynamiquement différente d'une SLM sans texte, ce qui indique qu'il est nécessaire de gérer la quantité de calculs lors de l'expansion du taille du modèle. Nous avons également étudié le rôle des données synthétiques et des familles de modèles TextLM, en essayant de développer ces potentiels. Les résultats montrent que nos modèles d'échelle atteignent des performances similaires à celles des leaders, comme le modèle d'évaluation de langage, en utilisant un minimum de calculs et de données. Les modèles, échantillons et données sont disponibles - https://pages.cs.huji.ac.il/adiyoss-lab/sims.",
      "upvotes": 7,
      "discussionId": "67ef63b6e8b932ae7a8d306d",
      "projectPage": "https://pages.cs.huji.ac.il/adiyoss-lab/sims/",
      "githubRepo": "https://github.com/slp-rl/slamkit",
      "ai_keywords": [
        "Speech Language Model (SLM)",
        "TextLMs",
        "speech-text interleaving",
        "scaling analysis",
        "compute",
        "knowledge transfer",
        "textless-SLMs",
        "scaling trends",
        "scaling-dynamics",
        "training tokens",
        "synthetic data",
        "model families",
        "speech semantic metrics"
      ]
    },
    "publishedAt": "2025-04-03T04:46:56.000Z",
    "title": "Scaling Analysis of Interleaved Speech-Text Language Models",
    "summary": "Existing Speech Language Model (SLM) scaling analysis paints a bleak picture.\nThey predict that SLMs require much more compute and data compared to text,\nleading some to question the feasibility of training high-quality SLMs.\nHowever, modern SLMs are often initialised from pre-trained TextLMs using\nspeech-text interleaving to allow knowledge transfer. This raises the question\n- Do interleaved SLMs scale more efficiently than textless-SLMs? In this paper\nwe answer a resounding, yes! We conduct scaling analysis of interleaved SLMs by\ntraining several dozen and analysing the scaling trends. We see that under this\nsetup SLMs scale more efficiently with compute. Additionally, our results\nindicate that the scaling-dynamics are significantly different than\ntextless-SLMs, suggesting one should allocate notably more of the compute\nbudget for increasing model size over training tokens. We also study the role\nof synthetic data and TextLM model families in unlocking this potential.\nResults suggest, that our scaled up model achieves comparable performance with\nleading models on speech semantic metrics while using less compute and data\nthan other approaches. We open source models, samples, and data -\nhttps://pages.cs.huji.ac.il/adiyoss-lab/sims.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/66b9bc2dacdbc1d0b39c3b50/wqUq-bT-DvKoNybPX46uL.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02398.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66b9bc2dacdbc1d0b39c3b50",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/hwR0pVfP_E8XjimXIxDOU.jpeg",
      "fullname": "Gallil Maimon",
      "name": "gallilmaimon",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.02012",
      "authors": [
        {
          "_id": "67ef5af0724d484dd41afe5c",
          "user": {
            "_id": "66189b980da4c017c401fb5d",
            "avatarUrl": "/avatars/d5184741e4a333435022bcb9a4a9b9d8.svg",
            "isPro": false,
            "fullname": "soro bedio",
            "user": "bedio",
            "type": "user"
          },
          "name": "Soro Bedionita",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:51:20.621Z",
          "hidden": false
        },
        {
          "_id": "67ef5af0724d484dd41afe5d",
          "name": "Bruno Andreis",
          "hidden": false
        },
        {
          "_id": "67ef5af0724d484dd41afe5e",
          "name": "Song Chong",
          "hidden": false
        },
        {
          "_id": "67ef5af0724d484dd41afe5f",
          "name": "Sung Ju Hwang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-02T05:50:19.000Z",
      "submittedOnDailyAt": "2025-04-04T02:38:11.321Z",
      "title": "Instructions de Guide pour la Création des Paramètres du Réseau Aggregate Network du Norlive",
      "submittedOnDailyBy": {
        "_id": "66189b980da4c017c401fb5d",
        "avatarUrl": "/avatars/d5184741e4a333435022bcb9a4a9b9d8.svg",
        "isPro": false,
        "fullname": "soro bedio",
        "user": "bedio",
        "type": "user"
      },
      "summary": "Selon les règles d'apprentissage des Turkers, la génération de paramètres de réseau de neurones dans des conditions spécifiques est cruciale pour l'adaptabilité du modèle et le développement du apprentissage de motifs. Les méthodes actuelles, en particulier celles basées sur des réseaux neuronaux profonds, présentent des limitations en termes d'échelle vers des architectures plus grandes, une inflexibilité à gérer différentes profondeurs de réseaux et la séparation de la génération de paramètres qui détruit la collaboration entre couches indirectes. Dans cette étude, nous proposons un cadre automatique de rétroaction uniforme pour la génération de paramètres, nommé IGPG (Génération de Paramètres de Résolution de Paramètres), qui s'applique à différentes tâches et architectures. IGPG utilise VQ-VAE et modèles de rétroaction automatique pour générer des paramètres de réseau de neurones en fonction d'instructions de tâche, de jeux de données et de détails d'architecture. Il fonctionne de manière automatique de rétroaction en créant des poids de la réseau comme tokens. IGPG assure la collaboration entre couches indirectes et permet une adaptation efficace entre le modèle et les jeux de données. En opérant au niveau de tokens, IGPG capture effectivement la distribution complexe de paramètres réduits à partir de modèles bien entraînés. Des expériences diffusées sur des jeux de données visuels montrent que IGPG intègre des modèles bien entraînés dans un seul cadre de génération flexible, obtenant des résultats compétitifs ou supérieurs aux méthodes de pointe. En particulier, en termes d'échelle et d'efficacité pour des architectures plus grandes, IGPG met en avant la possibilité d'être un instrument puissant pour la recherche de poids entraînés, la sélection de modèles et le rapide ajustement de tâches spécifiques.",
      "upvotes": 5,
      "discussionId": "67ef5af1724d484dd41afef3",
      "ai_keywords": [
        "diffusion models",
        "IGPG (Instruction Guided Parameter Generation)",
        "VQ-VAE",
        "autoregressive framework",
        "token level",
        "parameter synthesis",
        "inter-layer coherence",
        "vision datasets",
        "pretrained models",
        "pretrained weight retrieval",
        "model selection",
        "task-specific fine-tuning"
      ]
    },
    "publishedAt": "2025-04-02T01:50:19.000Z",
    "title": "Instruction-Guided Autoregressive Neural Network Parameter Generation",
    "summary": "Learning to generate neural network parameters conditioned on task\ndescriptions and architecture specifications is pivotal for advancing model\nadaptability and transfer learning. Existing methods especially those based on\ndiffusion models suffer from limited scalability to large architectures,\nrigidity in handling varying network depths, and disjointed parameter\ngeneration that undermines inter-layer coherence. In this work, we propose IGPG\n(Instruction Guided Parameter Generation), an autoregressive framework that\nunifies parameter synthesis across diverse tasks and architectures. IGPG\nleverages a VQ-VAE and an autoregressive model to generate neural network\nparameters, conditioned on task instructions, dataset, and architecture\ndetails. By autoregressively generating neural network weights' tokens, IGPG\nensures inter-layer coherence and enables efficient adaptation across models\nand datasets. Operating at the token level, IGPG effectively captures complex\nparameter distributions aggregated from a broad spectrum of pretrained models.\nExtensive experiments on multiple vision datasets demonstrate that IGPG\nconsolidates diverse pretrained models into a single, flexible generative\nframework. The synthesized parameters achieve competitive or superior\nperformance relative to state-of-the-art methods, especially in terms of\nscalability and efficiency when applied to large architectures. These results\nunderscore ICPG potential as a powerful tool for pretrained weight retrieval,\nmodel selection, and rapid task-specific fine-tuning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02012.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66189b980da4c017c401fb5d",
      "avatarUrl": "/avatars/d5184741e4a333435022bcb9a4a9b9d8.svg",
      "fullname": "soro bedio",
      "name": "bedio",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.02119",
      "authors": [
        {
          "_id": "67ef41e7efcb0a2fbfbb6a32",
          "user": {
            "_id": "670826649e319cca029ff240",
            "avatarUrl": "/avatars/6d12b3abf75f714d75d1775d88885345.svg",
            "isPro": false,
            "fullname": "rtfvbhkuj",
            "user": "wwdd7718",
            "type": "user"
          },
          "name": "Wang Wei",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-04T02:20:24.253Z",
          "hidden": false
        },
        {
          "_id": "67ef41e7efcb0a2fbfbb6a33",
          "user": {
            "_id": "66e4e50a52356419c4a1ad14",
            "avatarUrl": "/avatars/4be3ce17671785cbe7126b9c1141478b.svg",
            "isPro": false,
            "fullname": "Tiankai Yang",
            "user": "tiankaiy",
            "type": "user"
          },
          "name": "Tiankai Yang",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-04-04T02:25:15.747Z",
          "hidden": false
        },
        {
          "_id": "67ef41e7efcb0a2fbfbb6a34",
          "name": "Hongjie Chen",
          "hidden": false
        },
        {
          "_id": "67ef41e7efcb0a2fbfbb6a35",
          "user": {
            "_id": "62a3ab83e4dd6252344d27cd",
            "avatarUrl": "/avatars/7ca8510f70a58dc207b104240e30c35c.svg",
            "isPro": false,
            "fullname": "Ryan A. Rossi",
            "user": "ryanrossi",
            "type": "user"
          },
          "name": "Ryan A. Rossi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:23:05.421Z",
          "hidden": false
        },
        {
          "_id": "67ef41e7efcb0a2fbfbb6a36",
          "name": "Yue Zhao",
          "hidden": false
        },
        {
          "_id": "67ef41e7efcb0a2fbfbb6a37",
          "user": {
            "_id": "62c5947524171688a9feb992",
            "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
            "isPro": false,
            "fullname": "Franck Dernoncourt",
            "user": "Franck-Dernoncourt",
            "type": "user"
          },
          "name": "Franck Dernoncourt",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-04-04T02:34:51.212Z",
          "hidden": false
        },
        {
          "_id": "67ef41e7efcb0a2fbfbb6a38",
          "name": "Hoda Eldardiry",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-02T20:33:27.000Z",
      "submittedOnDailyAt": "2025-04-04T00:50:35.167Z",
      "title": "Méthode Efficace pour la Sélection de Modèles en Prévision de Séries Temporelles Utilisant des LLMs",
      "submittedOnDailyBy": {
        "_id": "62c5947524171688a9feb992",
        "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
        "isPro": false,
        "fullname": "Franck Dernoncourt",
        "user": "Franck-Dernoncourt",
        "type": "user"
      },
      "summary": "La sélection de modèles est l'un des étapes importantes dans la prédiction de séquences temporelles, et jusqu'à présent, une évaluation très stricte du rendement pour différents ensembles de données a été nécessaire. L'approche de l'apprentissage par méta-apprentissage a essayé d'automatiser ce processus, mais généralement dépend de la construction préalable de métriques de rendement, ce qui est coûteux. Dans cette étude, nous proposons d'utiliser des Grands Modèles de Langue (LLMs) comme alternatives légères pour la sélection de modèles. Notre méthode utilise les connaissances et les capacités d'inférence propres aux LLMs pour éliminer la nécessité de métriques de rendement explicites. À travers d'expériences étendues avec LLaMA, GPT et Gemini, nous démontrons que notre approche dépasse les méthodes traditionnelles de l'apprentissage par méta-apprentissage et de l'approche heuristique, et réduit significativement le fardeau informatique. Ces résultats soulignent la possibilité que les LLMs puissent être efficaces dans la sélection de modèles pour la prédiction de séquences temporelles.",
      "upvotes": 4,
      "discussionId": "67ef41e8efcb0a2fbfbb6a93",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "model selection",
        "time series forecasting",
        "meta-learning approaches",
        "pre-constructed performance matrices",
        "reasoning capabilities",
        "experiments",
        "LLaMA",
        "GPT",
        "Gemini",
        "heuristic baselines",
        "computational overhead"
      ]
    },
    "publishedAt": "2025-04-02T16:33:27.000Z",
    "title": "Efficient Model Selection for Time Series Forecasting via LLMs",
    "summary": "Model selection is a critical step in time series forecasting, traditionally\nrequiring extensive performance evaluations across various datasets.\nMeta-learning approaches aim to automate this process, but they typically\ndepend on pre-constructed performance matrices, which are costly to build. In\nthis work, we propose to leverage Large Language Models (LLMs) as a lightweight\nalternative for model selection. Our method eliminates the need for explicit\nperformance matrices by utilizing the inherent knowledge and reasoning\ncapabilities of LLMs. Through extensive experiments with LLaMA, GPT and Gemini,\nwe demonstrate that our approach outperforms traditional meta-learning\ntechniques and heuristic baselines, while significantly reducing computational\noverhead. These findings underscore the potential of LLMs in efficient model\nselection for time series forecasting.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02119.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62c5947524171688a9feb992",
      "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
      "fullname": "Franck Dernoncourt",
      "name": "Franck-Dernoncourt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.00891",
      "authors": [
        {
          "_id": "67ef62342a18e60aeee0ea02",
          "name": "Jian Zhao",
          "hidden": false
        },
        {
          "_id": "67ef62342a18e60aeee0ea03",
          "user": {
            "_id": "667187ba9ab144eb3ac43a1b",
            "avatarUrl": "/avatars/db5558aa1c5160b9aee8b58573271959.svg",
            "isPro": false,
            "fullname": "Runze Liu",
            "user": "RyanLiu112",
            "type": "user"
          },
          "name": "Runze Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:08:09.923Z",
          "hidden": false
        },
        {
          "_id": "67ef62342a18e60aeee0ea04",
          "user": {
            "_id": "60bc94cd85a3ab33829b6211",
            "avatarUrl": "/avatars/b57d36c7577fbbb42ea5b963eef4144a.svg",
            "isPro": false,
            "fullname": "Kaiyan Zhang",
            "user": "iseesaw",
            "type": "user"
          },
          "name": "Kaiyan Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:25:22.709Z",
          "hidden": false
        },
        {
          "_id": "67ef62342a18e60aeee0ea05",
          "name": "Zhimu Zhou",
          "hidden": false
        },
        {
          "_id": "67ef62342a18e60aeee0ea06",
          "user": {
            "_id": "67ab05fe4c6ca2d5db4c0c52",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/QpGUNDkeuKjX71s2GXlXF.png",
            "isPro": false,
            "fullname": "Junqi Gao",
            "user": "ChetKao",
            "type": "user"
          },
          "name": "Junqi Gao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:25:38.624Z",
          "hidden": false
        },
        {
          "_id": "67ef62342a18e60aeee0ea07",
          "name": "Dong Li",
          "hidden": false
        },
        {
          "_id": "67ef62342a18e60aeee0ea08",
          "user": {
            "_id": "6562db314e8918182da42706",
            "avatarUrl": "/avatars/b113bbbb496bf4dac254f0e840f08e10.svg",
            "isPro": false,
            "fullname": "Jiafei Lyu",
            "user": "dmux",
            "type": "user"
          },
          "name": "Jiafei Lyu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:25:45.394Z",
          "hidden": false
        },
        {
          "_id": "67ef62342a18e60aeee0ea09",
          "user": {
            "_id": "65b34c5785b6c2144807db37",
            "avatarUrl": "/avatars/4c1cb03cda250d4ec760ebf7815a3bce.svg",
            "isPro": false,
            "fullname": "Qianzhouyi",
            "user": "Saputello",
            "type": "user"
          },
          "name": "Zhouyi Qian",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:26:00.763Z",
          "hidden": false
        },
        {
          "_id": "67ef62342a18e60aeee0ea0a",
          "user": {
            "_id": "645d9c3058f9ee315148116d",
            "avatarUrl": "/avatars/165e18f27b5a50738bf1d22857118478.svg",
            "isPro": false,
            "fullname": "Biqing Qi",
            "user": "jackqi7",
            "type": "user"
          },
          "name": "Biqing Qi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:26:06.517Z",
          "hidden": false
        },
        {
          "_id": "67ef62342a18e60aeee0ea0b",
          "name": "Xiu Li",
          "hidden": false
        },
        {
          "_id": "67ef62342a18e60aeee0ea0c",
          "user": {
            "_id": "669f614b59adf5b56e05bce3",
            "avatarUrl": "/avatars/ffd4189efbceb0e63a03db273065a44b.svg",
            "isPro": false,
            "fullname": "BowenZhou",
            "user": "bowenZhou",
            "type": "user"
          },
          "name": "Bowen Zhou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:26:14.312Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-01T15:21:05.000Z",
      "submittedOnDailyAt": "2025-04-04T03:13:15.991Z",
      "title": "GenPRM : Théorie de la Compensation des Processus pour Augmenter la Quantité de Calculs dans la Test de la Théorie des Rapports Génératifs",
      "submittedOnDailyBy": {
        "_id": "667187ba9ab144eb3ac43a1b",
        "avatarUrl": "/avatars/db5558aa1c5160b9aee8b58573271959.svg",
        "isPro": false,
        "fullname": "Runze Liu",
        "user": "RyanLiu112",
        "type": "user"
      },
      "summary": "Récemment, il a été suggéré que l'utilisation de modèles de récompense de processus (PRMs) comme vérificateurs pour améliorer le rendement des grands modèles de langage (LLMs) soit approprié. Cependant, les PRMs actuels font face à trois problèmes importants : 1) les limitations dans le contrôle et la capacité de généralisation du processus, 2) la dépendance sur la prédiction scalaire sans exploiter la capacité générative des LLMs, et 3) la difficulté de réduire la quantité de calculs lors du test. Dans cet article, nous présentons un modèle de récompense de processus génératif (GenPRM) qui vérifie la logique de la chaîne de pensée (CoT) et le code, et fournit une évaluation de chaque étape logique, permettant ainsi d'obtenir des étiquettes de contrôle du processus et des données de raisonnement de haute qualité. Nous proposons l'Évaluation de Progrès Relative (RPE) et un cadre de synthèse de raisons pour montrer que GenPRM, en utilisant 23K données d'entraînement dans le jeu de données MATH, dépasse considérablement les PRMs existants. En fonction de la scalabilité du test, le GenPRM de 1,5B dépasse GPT-4o, tandis que le GenPRM de 7B dépasse Qwen2,5-Math-PRM-72B sur ProcessBench. De plus, GenPRM montre une forte capacité en tant que modèle d'évaluation pour l'entraînement de modèles de politique. Cet article recommande un nouveau paradigme pour combler la lacune entre PRMs et modèles d'évaluation, et les codes, modèles et données sont disponibles sur https://ryanliu112.github.io/GenPRM.",
      "upvotes": 4,
      "discussionId": "67ef62352a18e60aeee0ea4b",
      "projectPage": "https://ryanliu112.github.io/GenPRM",
      "githubRepo": "https://github.com/RyanLiu112/GenPRM",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "Process Reward Models (PRMs)",
        "Chain-of-Thought (CoT) reasoning",
        "Relative Progress Estimation (RPE)",
        "ProcessBench",
        "MATH dataset",
        "GPT-4",
        "Qwen2.5-Math-PRM-72B",
        "critic model",
        "policy model refinement"
      ]
    },
    "publishedAt": "2025-04-01T11:21:05.000Z",
    "title": "GenPRM: Scaling Test-Time Compute of Process Reward Models via\n  Generative Reasoning",
    "summary": "Recent advancements in Large Language Models (LLMs) have shown that it is\npromising to utilize Process Reward Models (PRMs) as verifiers to enhance the\nperformance of LLMs. However, current PRMs face three key challenges: (1)\nlimited process supervision and generalization capabilities, (2) dependence on\nscalar value prediction without leveraging the generative abilities of LLMs,\nand (3) inability to scale the test-time compute of PRMs. In this work, we\nintroduce GenPRM, a generative process reward model that performs explicit\nChain-of-Thought (CoT) reasoning with code verification before providing\njudgment for each reasoning step. To obtain high-quality process supervision\nlabels and rationale data, we propose Relative Progress Estimation (RPE) and a\nrationale synthesis framework that incorporates code verification. Experimental\nresults on ProcessBench and several mathematical reasoning tasks show that\nGenPRM significantly outperforms prior PRMs with only 23K training data from\nMATH dataset. Through test-time scaling, a 1.5B GenPRM outperforms GPT-4o, and\na 7B GenPRM surpasses Qwen2.5-Math-PRM-72B on ProcessBench. Additionally,\nGenPRM demonstrates strong abilities to serve as a critic model for policy\nmodel refinement. This work establishes a new paradigm for process supervision\nthat bridges the gap between PRMs and critic models in LLMs. Our code, model,\nand data will be available in https://ryanliu112.github.io/GenPRM.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00891.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "667187ba9ab144eb3ac43a1b",
      "avatarUrl": "/avatars/db5558aa1c5160b9aee8b58573271959.svg",
      "fullname": "Runze Liu",
      "name": "RyanLiu112",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.22444",
      "authors": [
        {
          "_id": "67ef33a4456bcf30fa95b2f1",
          "user": {
            "_id": "655fb8a122ce47e5fa491c72",
            "avatarUrl": "/avatars/d320fe777649b68fbd1372865a2f4def.svg",
            "isPro": false,
            "fullname": "Pengsong Zhang",
            "user": "universea",
            "type": "user"
          },
          "name": "Pengsong Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:09:42.152Z",
          "hidden": false
        },
        {
          "_id": "67ef33a4456bcf30fa95b2f2",
          "name": "Heng Zhang",
          "hidden": false
        },
        {
          "_id": "67ef33a4456bcf30fa95b2f3",
          "name": "Huazhe Xu",
          "hidden": false
        },
        {
          "_id": "67ef33a4456bcf30fa95b2f4",
          "name": "Renjun Xu",
          "hidden": false
        },
        {
          "_id": "67ef33a4456bcf30fa95b2f5",
          "name": "Zhenting Wang",
          "hidden": false
        },
        {
          "_id": "67ef33a4456bcf30fa95b2f6",
          "name": "Cong Wang",
          "hidden": false
        },
        {
          "_id": "67ef33a4456bcf30fa95b2f7",
          "name": "Animesh Garg",
          "hidden": false
        },
        {
          "_id": "67ef33a4456bcf30fa95b2f8",
          "name": "Zhibin Li",
          "hidden": false
        },
        {
          "_id": "67ef33a4456bcf30fa95b2f9",
          "name": "Arash Ajoudani",
          "hidden": false
        },
        {
          "_id": "67ef33a4456bcf30fa95b2fa",
          "name": "Xinyu Liu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/655fb8a122ce47e5fa491c72/d-38XQKRw3vnyFhzZFTEd.jpeg"
      ],
      "publishedAt": "2025-03-28T14:00:27.000Z",
      "submittedOnDailyAt": "2025-04-04T06:29:27.852Z",
      "title": "La loi de l'échelle des découvertes scientifiques réalisées par l'IA et les scientifiques de la robotique",
      "submittedOnDailyBy": {
        "_id": "655fb8a122ce47e5fa491c72",
        "avatarUrl": "/avatars/d320fe777649b68fbd1372865a2f4def.svg",
        "isPro": false,
        "fullname": "Pengsong Zhang",
        "user": "universea",
        "type": "user"
      },
      "summary": "La recherche scientifique est attendue d'avancer rapidement grâce à la haute technologie des robots et de l'intelligence artificielle. Actuellement, la recherche scientifique consomme beaucoup de temps et de ressources dans des expériences manuelles, et l'intégration des connaissances dans diverses domaines scientifiques nécessite des connaissances qui dépassent le domaine d'un seul chercheur. Ici, nous imaginons le concept d'un chercheur général automatique (CGA). Ce concept vise à automatiser complètement le cycle de recherche par la combinaison efficace de l'IA et de robots abstraits. Ce système favorise l'interaction physique et visuelle et soutient l'intégration des connaissances dans diverses domaines scientifiques. Ces technologies peuvent étendre directement toutes les étapes de la recherche (recherche de la littérature, génération d'hypothèses, expériences, rédaction de rapports de recherche) et, avec l'introduction de la réflexion interne et de la rétroaction externe, peuvent réduire significativement le temps et les ressources nécessaires pour une découverte scientifique. Un chercheur virtuel d'IA peut évoluer vers un chercheur robotique basé sur l'IA générale, et le CGA offre une possibilité globale. On espère que ces systèmes automatiques seront intégrés plus strictement dans les processus de recherche, et on suppose que les découvertes scientifiques peuvent atteindre un nouveau niveau, en considérant la création et l'évolution des connaissances depuis une nouvelle perspective. L'adaptation aux environnements extrêmes des robots abstraits et l'effet de niveau de liberté due à l'augmentation actif du savoir scientifique vise à dépasser constamment les frontières physiques et intellectuelles.",
      "upvotes": 4,
      "discussionId": "67ef33a5456bcf30fa95b35e",
      "githubRepo": "https://github.com/openags/Awesome-AI-Scientist-Papers"
    },
    "publishedAt": "2025-03-28T10:00:27.000Z",
    "title": "Scaling Laws in Scientific Discovery with AI and Robot Scientists",
    "summary": "Scientific discovery is poised for rapid advancement through advanced\nrobotics and artificial intelligence. Current scientific practices face\nsubstantial limitations as manual experimentation remains time-consuming and\nresource-intensive, while multidisciplinary research demands knowledge\nintegration beyond individual researchers' expertise boundaries. Here, we\nenvision an autonomous generalist scientist (AGS) concept combines agentic AI\nand embodied robotics to automate the entire research lifecycle. This system\ncould dynamically interact with both physical and virtual environments while\nfacilitating the integration of knowledge across diverse scientific\ndisciplines. By deploying these technologies throughout every research stage --\nspanning literature review, hypothesis generation, experimentation, and\nmanuscript writing -- and incorporating internal reflection alongside external\nfeedback, this system aims to significantly reduce the time and resources\nneeded for scientific discovery. Building on the evolution from virtual AI\nscientists to versatile generalist AI-based robot scientists, AGS promises\ngroundbreaking potential. As these autonomous systems become increasingly\nintegrated into the research process, we hypothesize that scientific discovery\nmight adhere to new scaling laws, potentially shaped by the number and\ncapabilities of these autonomous systems, offering novel perspectives on how\nknowledge is generated and evolves. The adaptability of embodied robots to\nextreme environments, paired with the flywheel effect of accumulating\nscientific knowledge, holds the promise of continually pushing beyond both\nphysical and intellectual frontiers.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/655fb8a122ce47e5fa491c72/d-38XQKRw3vnyFhzZFTEd.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.22444.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "655fb8a122ce47e5fa491c72",
      "avatarUrl": "/avatars/d320fe777649b68fbd1372865a2f4def.svg",
      "fullname": "Pengsong Zhang",
      "name": "universea",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.01871",
      "authors": [
        {
          "_id": "67eea9e5117231f8bb04402b",
          "user": {
            "_id": "65d0c00b0954f06e472909f4",
            "avatarUrl": "/avatars/7dd76d922b781ed9895c7f4e62fefd9c.svg",
            "isPro": false,
            "fullname": "tom bush",
            "user": "tuphs",
            "type": "user"
          },
          "name": "Thomas Bush",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-03T19:20:34.085Z",
          "hidden": false
        },
        {
          "_id": "67eea9e5117231f8bb04402c",
          "name": "Stephen Chung",
          "hidden": false
        },
        {
          "_id": "67eea9e5117231f8bb04402d",
          "name": "Usman Anwar",
          "hidden": false
        },
        {
          "_id": "67eea9e5117231f8bb04402e",
          "user": {
            "_id": "645ecd18f0f92653b9f33d4e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645ecd18f0f92653b9f33d4e/nHDMWtM9ZHrji0c4Y4XW1.jpeg",
            "isPro": false,
            "fullname": "Adrià Garriga-Alonso",
            "user": "agaralon",
            "type": "user"
          },
          "name": "Adrià Garriga-Alonso",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-03T15:31:53.577Z",
          "hidden": false
        },
        {
          "_id": "67eea9e5117231f8bb04402f",
          "name": "David Krueger",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-02T16:24:23.000Z",
      "submittedOnDailyAt": "2025-04-04T07:00:29.802Z",
      "title": "Retrospective Planning in Model-Free Reinforcement Learning",
      "submittedOnDailyBy": {
        "_id": "65d0c00b0954f06e472909f4",
        "avatarUrl": "/avatars/7dd76d922b781ed9895c7f4e62fefd9c.svg",
        "isPro": false,
        "fullname": "tom bush",
        "user": "tuphs",
        "type": "user"
      },
      "summary": "Voici la traduction en français :\n\nCi-dessous est fourni un modèle d'apprentissage par renforcement sans modèle qui effectue l'apprentissage de plans basé sur des preuves mécaniques. Ce méthode a été appliquée dans le jeu de la machine \"SoCoBaN\" pour promouvoir l'explicabilité basée sur des concepts dans les agents sans modèle. En particulier, le DRC (Guez et al. (2019)), un agent sans modèle du genre introduit, utilise des représentations de concepts appris pour la configuration de plans internes, prédisant les effets à long terme de l'environnement et influençant la décision d'actions. Notre méthode est composée de trois étapes : (1) détection de concepts liés aux plans, (2) recherche de la formation de plans au sein de la représentation de l'agent, et (3) confirmation que les plans trouvés (au sein de la représentation de l'agent) ont un impact causal sur les actions de l'agent. De plus, la découverte de ces plans permet d'identifier des capacités telles que le rendement planifié et d'obtenir des avantages en termes de calcul supplémentaire lors des tests. Enfin, une analyse qualitative est effectuée sur l'algorithme de plans appris par l'agent, et une forte similitude est observée avec des explorations parallèles bidirectionnelles. Ces résultats permettent de mieux comprendre la structure interne des comportements planifiés de l'agent et sont cruciales pour comprendre des phénomènes tels que la capacité de planification et d'inférence dans l'apprentissage par renforcement des LLM récents.",
      "upvotes": 3,
      "discussionId": "67eea9e9117231f8bb044167",
      "ai_keywords": [
        "model-free reinforcement learning",
        "concept-based interpretability",
        "Sokoban",
        "DRC",
        "learned concept representations",
        "plan formation",
        "causal effect",
        "parallelized bidirectional search",
        "LLMs",
        "emergent planning",
        "reasoning capabilities"
      ]
    },
    "publishedAt": "2025-04-02T12:24:23.000Z",
    "title": "Interpreting Emergent Planning in Model-Free Reinforcement Learning",
    "summary": "We present the first mechanistic evidence that model-free reinforcement\nlearning agents can learn to plan. This is achieved by applying a methodology\nbased on concept-based interpretability to a model-free agent in Sokoban -- a\ncommonly used benchmark for studying planning. Specifically, we demonstrate\nthat DRC, a generic model-free agent introduced by Guez et al. (2019), uses\nlearned concept representations to internally formulate plans that both predict\nthe long-term effects of actions on the environment and influence action\nselection. Our methodology involves: (1) probing for planning-relevant\nconcepts, (2) investigating plan formation within the agent's representations,\nand (3) verifying that discovered plans (in the agent's representations) have a\ncausal effect on the agent's behavior through interventions. We also show that\nthe emergence of these plans coincides with the emergence of a planning-like\nproperty: the ability to benefit from additional test-time compute. Finally, we\nperform a qualitative analysis of the planning algorithm learned by the agent\nand discover a strong resemblance to parallelized bidirectional search. Our\nfindings advance understanding of the internal mechanisms underlying planning\nbehavior in agents, which is important given the recent trend of emergent\nplanning and reasoning capabilities in LLMs through RL",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.01871.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65d0c00b0954f06e472909f4",
      "avatarUrl": "/avatars/7dd76d922b781ed9895c7f4e62fefd9c.svg",
      "fullname": "tom bush",
      "name": "tuphs",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.02821",
      "authors": [
        {
          "_id": "67ef9a8885ea9d1d7db3fdb5",
          "name": "Mateusz Pach",
          "hidden": false
        },
        {
          "_id": "67ef9a8885ea9d1d7db3fdb6",
          "name": "Shyamgopal Karthik",
          "hidden": false
        },
        {
          "_id": "67ef9a8885ea9d1d7db3fdb7",
          "name": "Quentin Bouniot",
          "hidden": false
        },
        {
          "_id": "67ef9a8885ea9d1d7db3fdb8",
          "name": "Serge Belongie",
          "hidden": false
        },
        {
          "_id": "67ef9a8885ea9d1d7db3fdb9",
          "name": "Zeynep Akata",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-03T17:58:35.000Z",
      "submittedOnDailyAt": "2025-04-04T07:08:47.568Z",
      "title": "L'Encodeur Spas apprend des caractéristiques d'un seul sens dans les modèles de langage visuel.",
      "submittedOnDailyBy": {
        "_id": "6254599b6e36fe62e141c8f9",
        "avatarUrl": "/avatars/08d6b68b92c7bfd0a8be022ba9f2f289.svg",
        "isPro": false,
        "fullname": "Shyamgopal Karthik",
        "user": "shyamgopal",
        "type": "user"
      },
      "summary": "Les Autoencodeurs Spatiaux (AEs) ont démontré être utiles pour améliorer l'interprétabilité et le contrôle des Modèles de Langage Massifs (LLMs) récemment. Dans cet article, l'application des AEs est étendue aux Modèles Vision-Langage (VLMs) et un cadre détaillé approprié est proposé pour évaluer l'unicité de la représentation visuelle. Les résultats des expérimentations montrent que les AEs entraînés sur les VLMs améliorent significativement l'unicité de la représentation de chaque neurone et produisent une représentation stratifiée qui correspond à celle définie par les experts (par exemple, la classification de iNaturalist). En particulier, il est montré que l'application des AEs à l'encodeur visuel de CLIP permet de manipuler directement les résultats de modèles multi-phasiques comme LLaVA. Ces résultats soulignent l'utilité et l'efficacité des AEs comme une forme pratique et efficace d'améliorer l'interprétabilité et le contrôle des VLMs.",
      "upvotes": 2,
      "discussionId": "67ef9a8985ea9d1d7db3fe20",
      "ai_keywords": [
        "Sparse Autoencoders (SAEs)",
        "Large Language Models (LLMs)",
        "Vision-Language Models (VLMs)",
        "CLIP",
        "monosemanticity",
        "vision representations",
        "hierarchical representations",
        "iNaturalist taxonomy",
        "multimodal LLMs",
        "LLaVA",
        "unsupervised approach"
      ]
    },
    "publishedAt": "2025-04-03T13:58:35.000Z",
    "title": "Sparse Autoencoders Learn Monosemantic Features in Vision-Language\n  Models",
    "summary": "Sparse Autoencoders (SAEs) have recently been shown to enhance\ninterpretability and steerability in Large Language Models (LLMs). In this\nwork, we extend the application of SAEs to Vision-Language Models (VLMs), such\nas CLIP, and introduce a comprehensive framework for evaluating monosemanticity\nin vision representations. Our experimental results reveal that SAEs trained on\nVLMs significantly enhance the monosemanticity of individual neurons while also\nexhibiting hierarchical representations that align well with expert-defined\nstructures (e.g., iNaturalist taxonomy). Most notably, we demonstrate that\napplying SAEs to intervene on a CLIP vision encoder, directly steer output from\nmultimodal LLMs (e.g., LLaVA) without any modifications to the underlying\nmodel. These findings emphasize the practicality and efficacy of SAEs as an\nunsupervised approach for enhancing both the interpretability and control of\nVLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02821.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6254599b6e36fe62e141c8f9",
      "avatarUrl": "/avatars/08d6b68b92c7bfd0a8be022ba9f2f289.svg",
      "fullname": "Shyamgopal Karthik",
      "name": "shyamgopal",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  }
]