[
  {
    "paper": {
      "id": "2505.22617",
      "authors": [
        {
          "_id": "6837cd8fc537d91527323667",
          "user": {
            "_id": "650eba9555dc1e841746f132",
            "avatarUrl": "/avatars/af6f5ee78f161d25ec0afc45d2def8eb.svg",
            "isPro": false,
            "fullname": "Ganqu Cui",
            "user": "ganqu",
            "type": "user"
          },
          "name": "Ganqu Cui",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:42:09.467Z",
          "hidden": false
        },
        {
          "_id": "6837cd8fc537d91527323668",
          "user": {
            "_id": "66e3f8fb5d97b5bb46923444",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/DW806I00-00oQAYvD4ocQ.png",
            "isPro": false,
            "fullname": "Yuchen Zhang",
            "user": "YucZhang2003",
            "type": "user"
          },
          "name": "Yuchen Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:42:14.207Z",
          "hidden": false
        },
        {
          "_id": "6837cd8fc537d91527323669",
          "user": {
            "_id": "65352acb7139c5dd8d9a8590",
            "avatarUrl": "/avatars/e2ff22b596aee45cdfb8f68dc15572f9.svg",
            "isPro": false,
            "fullname": "JiachengChen",
            "user": "JC-Chen",
            "type": "user"
          },
          "name": "Jiacheng Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T09:40:20.736Z",
          "hidden": false
        },
        {
          "_id": "6837cd8fc537d9152732366a",
          "name": "Lifan Yuan",
          "hidden": false
        },
        {
          "_id": "6837cd8fc537d9152732366b",
          "name": "Zhi Wang",
          "hidden": false
        },
        {
          "_id": "6837cd8fc537d9152732366c",
          "user": {
            "_id": "622474f38dc6b0b64f5e903d",
            "avatarUrl": "/avatars/d6b60a014277a8ec7d564163c5f644aa.svg",
            "isPro": false,
            "fullname": "Yuxin Zuo",
            "user": "yuxinzuo",
            "type": "user"
          },
          "name": "Yuxin Zuo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:42:12.022Z",
          "hidden": true
        },
        {
          "_id": "6837cd8fc537d9152732366d",
          "user": {
            "_id": "662f638ba9891e43cc4c5125",
            "avatarUrl": "/avatars/77c22de5511f9b85d98ec75fb0b5e9be.svg",
            "isPro": false,
            "fullname": "Li Haozhan",
            "user": "Haozhan72",
            "type": "user"
          },
          "name": "Haozhan Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T09:40:22.720Z",
          "hidden": false
        },
        {
          "_id": "6837cd8fc537d9152732366e",
          "name": "Yuchen Fan",
          "hidden": false
        },
        {
          "_id": "6837cd8fc537d9152732366f",
          "name": "Huayu Chen",
          "hidden": false
        },
        {
          "_id": "6837cd8fc537d91527323670",
          "name": "Weize Chen",
          "hidden": false
        },
        {
          "_id": "6837cd8fc537d91527323671",
          "name": "Zhiyuan Liu",
          "hidden": false
        },
        {
          "_id": "6837cd8fc537d91527323672",
          "name": "Hao Peng",
          "hidden": false
        },
        {
          "_id": "6837cd8fc537d91527323673",
          "name": "Lei Bai",
          "hidden": false
        },
        {
          "_id": "6837cd8fc537d91527323674",
          "name": "Wanli Ouyang",
          "hidden": false
        },
        {
          "_id": "6837cd8fc537d91527323675",
          "name": "Yu Cheng",
          "hidden": false
        },
        {
          "_id": "6837cd8fc537d91527323676",
          "name": "Bowen Zhou",
          "hidden": false
        },
        {
          "_id": "6837cd8fc537d91527323677",
          "user": {
            "_id": "60cf4bcb1ce3775ebb86e5d5",
            "avatarUrl": "/avatars/12bcd18d215abf91f297f93007733148.svg",
            "isPro": false,
            "fullname": "Ning Ding",
            "user": "stingning",
            "type": "user"
          },
          "name": "Ning Ding",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:42:16.809Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T17:38:45.000Z",
      "submittedOnDailyAt": "2025-05-29T01:38:57.501Z",
      "title": "Inférence de modèles de langage en utilisant des structures d'entraînement par renforcement d'entropie",
      "submittedOnDailyBy": {
        "_id": "650eba9555dc1e841746f132",
        "avatarUrl": "/avatars/af6f5ee78f161d25ec0afc45d2def8eb.svg",
        "isPro": false,
        "fullname": "Ganqu Cui",
        "user": "ganqu",
        "type": "user"
      },
      "summary": "Cet article vise à surmonter la rupture de l'entropie de la politique en réalisant une logique de grands modèles de langue (LLMs) à travers l'échelle de l'apprentissage par renforcement (RL). Ce phénomène peut être observé de manière constante dans des expériences de RL à grande échelle, où l'entropie de la politique diminue rapidement au cours des premiers stades d'apprentissage, et la capacité d'exploration diminue en même temps que le rendement de la politique s'atténue. En fait, une équation expérimentale a été établie entre l'entropie H et le rendement R, R = -a * e^H + b. Cette loi expérimentale offre des pistes significatives et permet de prédire les limites du rendement de la politique lorsque l'entropie est insuffisante, car le rendement est remplacé par l'entropie. Lorsque H=0, R=-a+b. Notre découverte montre que pour gérer l'entropie, il est nécessaire d'écaler les calculs pour maintenir une exploration continue. Cela a conduit à étudier la dynamique de l'entropie tant théoriquement que expérimentalement. Nos calculs montrent que la variance des probabilités d'action et la variance des logistiques restent positives au cours de l'apprentissage, ce qui explique mieux la diminution de l'entropie de la politique. Pour comprendre la structure dynamique de l'entropie, nous considérons limiter l'actualisation des tokens de forte variance pour contrôler l'entropie. Deux techniques simples et efficaces sont proposées : Clip-Cov et KL-Cov. Clip-Cov applique une copie aux tokens de forte variance et applique une pénalité KL. Les expériences montrent que ces techniques favorisent l'exploration et permettent à des politiques de sortir de la rupture de l'entropie, atteignant un rendement meilleur en temps.",
      "upvotes": 64,
      "discussionId": "6837cd90c537d9152732369d",
      "githubRepo": "https://github.com/PRIME-RL/Entropy-Mechanism-of-RL",
      "ai_summary": "Entropy dynamics in reinforcement learning with large language models are investigated to prevent policy entropy collapse and improve exploration.",
      "ai_keywords": [
        "policy entropy",
        "reinforcement learning",
        "LLMs",
        "entropy intervention",
        "transformation equation",
        "policy performance",
        "entropy dynamics",
        "covariance",
        "action probability",
        "logits",
        "advantage",
        "Policy Gradient",
        "Clip-Cov",
        "KL-Cov"
      ]
    },
    "publishedAt": "2025-05-28T13:38:45.000Z",
    "title": "The Entropy Mechanism of Reinforcement Learning for Reasoning Language\n  Models",
    "summary": "This paper aims to overcome a major obstacle in scaling RL for reasoning with\nLLMs, namely the collapse of policy entropy. Such phenomenon is consistently\nobserved across vast RL runs without entropy intervention, where the policy\nentropy dropped sharply at the early training stage, this diminished\nexploratory ability is always accompanied with the saturation of policy\nperformance. In practice, we establish a transformation equation R=-a*e^H+b\nbetween entropy H and downstream performance R. This empirical law strongly\nindicates that, the policy performance is traded from policy entropy, thus\nbottlenecked by its exhaustion, and the ceiling is fully predictable H=0,\nR=-a+b. Our finding necessitates entropy management for continuous exploration\ntoward scaling compute for RL. To this end, we investigate entropy dynamics\nboth theoretically and empirically. Our derivation highlights that, the change\nin policy entropy is driven by the covariance between action probability and\nthe change in logits, which is proportional to its advantage when using Policy\nGradient-like algorithms. Empirical study shows that, the values of covariance\nterm and entropy differences matched exactly, supporting the theoretical\nconclusion. Moreover, the covariance term stays mostly positive throughout\ntraining, further explaining why policy entropy would decrease monotonically.\nThrough understanding the mechanism behind entropy dynamics, we motivate to\ncontrol entropy by restricting the update of high-covariance tokens.\nSpecifically, we propose two simple yet effective techniques, namely Clip-Cov\nand KL-Cov, which clip and apply KL penalty to tokens with high covariances\nrespectively. Experiments show that these methods encourage exploration, thus\nhelping policy escape entropy collapse and achieve better downstream\nperformance.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22617.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "650eba9555dc1e841746f132",
      "avatarUrl": "/avatars/af6f5ee78f161d25ec0afc45d2def8eb.svg",
      "fullname": "Ganqu Cui",
      "name": "ganqu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 18
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.21600",
      "authors": [
        {
          "_id": "6837bc9c9937bcb69885799c",
          "user": {
            "_id": "6445fd9ba56444c355dcbcba",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6445fd9ba56444c355dcbcba/NCyRCD-MK-yA0_qY6I2y0.png",
            "isPro": false,
            "fullname": "Tianyu Fu",
            "user": "fuvty",
            "type": "user"
          },
          "name": "Tianyu Fu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:42:43.123Z",
          "hidden": false
        },
        {
          "_id": "6837bc9c9937bcb69885799d",
          "name": "Yi Ge",
          "hidden": false
        },
        {
          "_id": "6837bc9c9937bcb69885799e",
          "user": {
            "_id": "66954ebfbcd81f395e9dca37",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66954ebfbcd81f395e9dca37/0C3m5YdxyXuK7dJBu4AdL.png",
            "isPro": false,
            "fullname": "Yichen You",
            "user": "youyc22",
            "type": "user"
          },
          "name": "Yichen You",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:42:40.790Z",
          "hidden": false
        },
        {
          "_id": "6837bc9c9937bcb69885799f",
          "name": "Enshu Liu",
          "hidden": false
        },
        {
          "_id": "6837bc9c9937bcb6988579a0",
          "name": "Zhihang Yuan",
          "hidden": false
        },
        {
          "_id": "6837bc9c9937bcb6988579a1",
          "name": "Guohao Dai",
          "hidden": false
        },
        {
          "_id": "6837bc9c9937bcb6988579a2",
          "name": "Shengen Yan",
          "hidden": false
        },
        {
          "_id": "6837bc9c9937bcb6988579a3",
          "name": "Huazhong Yang",
          "hidden": false
        },
        {
          "_id": "6837bc9c9937bcb6988579a4",
          "name": "Yu Wang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6445fd9ba56444c355dcbcba/edz42EmKaJTdax2mDUmhh.mp4"
      ],
      "publishedAt": "2025-05-27T16:57:20.000Z",
      "submittedOnDailyAt": "2025-05-29T00:18:54.118Z",
      "title": "R2R : Modèle de routage de tokens pour explorer des trajets efficaces pour d'autres raisons",
      "submittedOnDailyBy": {
        "_id": "6445fd9ba56444c355dcbcba",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6445fd9ba56444c355dcbcba/NCyRCD-MK-yA0_qY6I2y0.png",
        "isPro": false,
        "fullname": "Tianyu Fu",
        "user": "fuvty",
        "type": "user"
      },
      "summary": "Les modèles de langage grands (LLMs) atteignent une capacité d'intelligence artificielle impressionnante, mais leur introduction présente des défis importants en raison des coûts élevés de l'overcharge d'inférence. Cependant, les petits modèles de langage déterminés (SLMs) ne peuvent pas suivre le chemin de l'intelligence artificielle des LLMs, ce qui implique un déclin dans leur performance. Heureusement, la différence dans le chemin d'intelligence artificielle entre LLMs et SLMs se produit seulement dans les petits pourcentages de tokens. De nombreux tokens générés montrent des différences neutres, comme des variations minimales dans l'expression ou de semblance. Pour aborder ces différences, nous avons introduit un nouveau méthode de routage de tokens appelé **Roads to Rome (R2R)**. Ce méthode attribue aux SLMs les tokens qui ne sont pas significatifs pour la route, permettant la sélection des LLMs lorsque cela est nécessaire. De plus, le processus automatique de test de données reconnaît les tokens qui indiquent des différences et génère des étiquettes de routage au niveau de token pour créer des données pour l'entraînement d'un routier léger. Le R2R combine les modèles R1-1.5B et R1-32B de la famille DeepSeek pour évaluer des scénarios de tests difficiles en mathématiques, programmation et réponses à des questions. Avec un taille moyen de paramètres actifs de 5.6B, le R2R a atteint un rendement moyen de R1-7B à un facteur de 1.6, dépassant le rendement de R1-14B. En comparaison avec R1-32B, il offre une amélioration de 2.8 fois dans la vitesse de traitement du travail, ainsi qu'un prototype de scalabilité du temps de test. Le code est disponible sur https://github.com/thu-nics/R2R.",
      "upvotes": 47,
      "discussionId": "6837bc9d9937bcb6988579d1",
      "projectPage": "https://fuvty.github.io/R2R_Project_Page/",
      "githubRepo": "https://github.com/thu-nics/R2R",
      "ai_summary": "Roads to Rome (R2R) selectively utilizes large language models for critical reasoning tasks to enhance efficiency and performance in lightweight models.",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "Small Language Models (SLMs)",
        "token routing",
        "neural token routing",
        "token divergence",
        "token generation",
        "automatic data generation pipeline",
        "token-level routing labels",
        "R1-1.5B",
        "R1-32B",
        "math benchmarks",
        "coding benchmarks",
        "QA benchmarks",
        "parameter size",
        "activated parameters",
        "test-time scaling efficiency",
        "pareto frontier"
      ]
    },
    "publishedAt": "2025-05-27T12:57:20.000Z",
    "title": "R2R: Efficiently Navigating Divergent Reasoning Paths with Small-Large\n  Model Token Routing",
    "summary": "Large Language Models (LLMs) achieve impressive reasoning capabilities at the\ncost of substantial inference overhead, posing substantial deployment\nchallenges. Although distilled Small Language Models (SLMs) significantly\nenhance efficiency, their performance suffers as they fail to follow LLMs'\nreasoning paths. Luckily, we reveal that only a small fraction of tokens\ngenuinely diverge reasoning paths between LLMs and SLMs. Most generated tokens\nare either identical or exhibit neutral differences, such as minor variations\nin abbreviations or expressions. Leveraging this insight, we introduce **Roads\nto Rome (R2R)**, a neural token routing method that selectively utilizes LLMs\nonly for these critical, path-divergent tokens, while leaving the majority of\ntoken generation to the SLM. We also develop an automatic data generation\npipeline that identifies divergent tokens and generates token-level routing\nlabels to train the lightweight router. We apply R2R to combine R1-1.5B and\nR1-32B models from the DeepSeek family, and evaluate on challenging math,\ncoding, and QA benchmarks. With an average activated parameter size of 5.6B,\nR2R surpasses the average accuracy of R1-7B by 1.6x, outperforming even the\nR1-14B model. Compared to R1-32B, it delivers a 2.8x wall-clock speedup with\ncomparable performance, advancing the Pareto frontier of test-time scaling\nefficiency. Our code is available at https://github.com/thu-nics/R2R.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6445fd9ba56444c355dcbcba/edz42EmKaJTdax2mDUmhh.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21600.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6445fd9ba56444c355dcbcba",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6445fd9ba56444c355dcbcba/NCyRCD-MK-yA0_qY6I2y0.png",
      "fullname": "Tianyu Fu",
      "name": "fuvty",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.22312",
      "authors": [
        {
          "_id": "6837c342cd1601f5bd670255",
          "name": "Jujie He",
          "hidden": false
        },
        {
          "_id": "6837c342cd1601f5bd670256",
          "user": {
            "_id": "65643645b09c0b9ece1b8f0e",
            "avatarUrl": "/avatars/d5197103b6e92f765bfda7ed2cc8d53e.svg",
            "isPro": false,
            "fullname": "Jiacai Liu",
            "user": "skydownacai",
            "type": "user"
          },
          "name": "Jiacai Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:42:28.696Z",
          "hidden": false
        },
        {
          "_id": "6837c342cd1601f5bd670257",
          "user": {
            "_id": "658229ef5f6d83438257fce5",
            "avatarUrl": "/avatars/b4417de9a338e95dc69cc547a46348e8.svg",
            "isPro": false,
            "fullname": "Chris (Yuhao) Liu",
            "user": "chrisliu298",
            "type": "user"
          },
          "name": "Chris Yuhao Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:42:32.117Z",
          "hidden": false
        },
        {
          "_id": "6837c342cd1601f5bd670258",
          "name": "Rui Yan",
          "hidden": false
        },
        {
          "_id": "6837c342cd1601f5bd670259",
          "name": "Chaojie Wang",
          "hidden": false
        },
        {
          "_id": "6837c342cd1601f5bd67025a",
          "name": "Peng Cheng",
          "hidden": false
        },
        {
          "_id": "6837c342cd1601f5bd67025b",
          "name": "Xiaoyu Zhang",
          "hidden": false
        },
        {
          "_id": "6837c342cd1601f5bd67025c",
          "name": "Fuxiang Zhang",
          "hidden": false
        },
        {
          "_id": "6837c342cd1601f5bd67025d",
          "name": "Jiacheng Xu",
          "hidden": false
        },
        {
          "_id": "6837c342cd1601f5bd67025e",
          "name": "Wei Shen",
          "hidden": false
        },
        {
          "_id": "6837c342cd1601f5bd67025f",
          "name": "Siyuan Li",
          "hidden": false
        },
        {
          "_id": "6837c342cd1601f5bd670260",
          "name": "Liang Zeng",
          "hidden": false
        },
        {
          "_id": "6837c342cd1601f5bd670261",
          "name": "Tianwen Wei",
          "hidden": false
        },
        {
          "_id": "6837c342cd1601f5bd670262",
          "name": "Cheng Cheng",
          "hidden": false
        },
        {
          "_id": "6837c342cd1601f5bd670263",
          "name": "Bo An",
          "hidden": false
        },
        {
          "_id": "6837c342cd1601f5bd670264",
          "name": "Yang Liu",
          "hidden": false
        },
        {
          "_id": "6837c342cd1601f5bd670265",
          "name": "Yahui Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T12:56:04.000Z",
      "submittedOnDailyAt": "2025-05-29T00:48:05.741Z",
      "title": "Skywork Open Reasoner 1 Rapport Technique",
      "submittedOnDailyBy": {
        "_id": "658229ef5f6d83438257fce5",
        "avatarUrl": "/avatars/b4417de9a338e95dc69cc547a46348e8.svg",
        "isPro": false,
        "fullname": "Chris (Yuhao) Liu",
        "user": "chrisliu298",
        "type": "user"
      },
      "summary": "Le succès de DeepSeek-R1 indique que l'apprentissage par renforcement (RL) joue un rôle crucial dans l'amélioration des capacités de raisonnement des grands modèles de langage (LLMs). Dans ce travail, nous proposons une implémentation efficace et scalable de RL pour les modèles de Chain-of-Thought (CoT) à long terme. Notre approche basée sur la série de modèles DeepSeek-R1-Distill a atteint un amélioration significative en termes d'efficacité, augmentant le pourcentage moyen de succès sur AIME24, AIME25 et LiveCodeBench de 57,8% à 72,8% (un amélioration de 15,0%) pour des modèles de 32B et de 43,6% à 57,5% (un amélioration de 13,9%) pour des modèles de 7B. Le modèle Skywork-OR1-32B a dépassé DeepSeek-R1 et Qwen3-32B sur les indicateurs AIME24 et AIME25, et a obtenu des résultats compétitifs sur LiveCodeBench. Les modèles Skywork-OR1-7B et Skywork-OR1-Math-7B ont démontré des capacités de raisonnement compétitives par rapport à des modèles de même taille. Nous avons effectué des tests exhaustifs sur les composants clés du flux d'apprentissage et démontré leur efficacité. De plus, nous avons étudié en détail le phénomène de la chute de l'entropie et identifié les causes qui affectent sa dynamique, démontrant que l'inhibition d'une chute anticipée de l'entropie est cruciale pour l'amélioration des résultats. Pour soutenir la communauté de recherche, nous fournissons des modèles de poids, du code d'apprentissage et des ensembles de données d'apprentissage complètement ouverts.",
      "upvotes": 39,
      "discussionId": "6837c344cd1601f5bd6702dd",
      "githubRepo": "https://github.com/SkyworkAI/Skywork-OR1",
      "ai_summary": "Skywork-OR1 is a reinforcement learning approach for long Chain-of-Thought models that improves accuracy over DeepSeek-R1 across various benchmarks by addressing entropy collapse.",
      "ai_keywords": [
        "reinforcement learning",
        "LLMs",
        "Chain-of-Thought",
        "Skywork-OR1",
        "DeepSeek-R1-Distill",
        "AIME24",
        "AIME25",
        "LiveCodeBench",
        "entropy collapse",
        "ablation studies"
      ]
    },
    "publishedAt": "2025-05-28T08:56:04.000Z",
    "title": "Skywork Open Reasoner 1 Technical Report",
    "summary": "The success of DeepSeek-R1 underscores the significant role of reinforcement\nlearning (RL) in enhancing the reasoning capabilities of large language models\n(LLMs). In this work, we present Skywork-OR1, an effective and scalable RL\nimplementation for long Chain-of-Thought (CoT) models. Building on the\nDeepSeek-R1-Distill model series, our RL approach achieves notable performance\ngains, increasing average accuracy across AIME24, AIME25, and LiveCodeBench\nfrom 57.8% to 72.8% (+15.0%) for the 32B model and from 43.6% to 57.5% (+13.9%)\nfor the 7B model. Our Skywork-OR1-32B model surpasses both DeepSeek-R1 and\nQwen3-32B on the AIME24 and AIME25 benchmarks, while achieving comparable\nresults on LiveCodeBench. The Skywork-OR1-7B and Skywork-OR1-Math-7B models\ndemonstrate competitive reasoning capabilities among models of similar size. We\nperform comprehensive ablation studies on the core components of our training\npipeline to validate their effectiveness. Additionally, we thoroughly\ninvestigate the phenomenon of entropy collapse, identify key factors affecting\nentropy dynamics, and demonstrate that mitigating premature entropy collapse is\ncritical for improved test performance. To support community research, we fully\nopen-source our model weights, training code, and training datasets.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22312.png",
    "numComments": 4,
    "submittedBy": {
      "_id": "658229ef5f6d83438257fce5",
      "avatarUrl": "/avatars/b4417de9a338e95dc69cc547a46348e8.svg",
      "fullname": "Chris (Yuhao) Liu",
      "name": "chrisliu298",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.22651",
      "authors": [
        {
          "_id": "6837ffdd1bfb4a669ad6de09",
          "user": {
            "_id": "662678dfdd43e904ef1dcd03",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/662678dfdd43e904ef1dcd03/9bbnrnsRV3ylSuZXwcSNv.jpeg",
            "isPro": false,
            "fullname": "Yi Ding",
            "user": "Tuwhy",
            "type": "user"
          },
          "name": "Yi Ding",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:41:19.811Z",
          "hidden": false
        },
        {
          "_id": "6837ffdd1bfb4a669ad6de0a",
          "name": "Ruqi Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T17:58:03.000Z",
      "submittedOnDailyAt": "2025-05-29T06:18:44.515Z",
      "title": "Shellrock: Logique d'Auto-Correction dans les Modèles de Langue Visuo-Sémantiques",
      "submittedOnDailyBy": {
        "_id": "662678dfdd43e904ef1dcd03",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/662678dfdd43e904ef1dcd03/9bbnrnsRV3ylSuZXwcSNv.jpeg",
        "isPro": false,
        "fullname": "Yi Ding",
        "user": "Tuwhy",
        "type": "user"
      },
      "summary": "Les VLMs de vision et raisonnement, comme les mots longs, montrent un comportement attendu dans des tâches multimodales complexes. Cependant, ils rencontrent principalement les problèmes suivants : sont extrêmement sensibles aux erreurs logiques, nécessitent beaucoup de données standardisées ou de données de validation précises, et sont difficiles à généraliser dans des domaines spécifiques. Pour résoudre ces limitations, nous avons adopté la régulation automatique des VLMs de raisonnement comme stratégie et nous avons essayé d'améliorer leur performance. Tout d'abord, nous avons effectué un analyse détaillée de la capacité de régulation automatique des VLMs de raisonnement et nous avons identifié leurs principales déficiences. Sur la base de cette analyse, nous avons introduit un cadre d'entraînement pour la régulation et l'amélioration automatique appelé Sherlock. Sherlock intègre des objets de régulation au niveau de la séquence, des méthodes de construction de données basées sur des motifs visuels et l'introduction de beta dynamique. Le modèle a été entraîné sur 20 000 données standardisées aléatoirement, puis a continué avec l'amélioration automatique, en excluant les superviseurs externes. Le modèle Sherlock construit pour Llama3.2-Vision-11B a obtenu des résultats impressionnants dans le cadre de 8 banques de marque, avec une précision moyenne de la génération directe qui a augmenté de 64,1 à 65,4 après la régulation automatique. Il dépasse LLaVA-CoT (63,2), Mulberry (63,9) et LlamaV-o1 (63,4), et utilise plus de 20% des données utilisées pour ces modèles.",
      "upvotes": 38,
      "discussionId": "6837ffdf1bfb4a669ad6de71",
      "projectPage": "https://dripnowhy.github.io/Sherlock/",
      "githubRepo": "https://github.com/DripNowhy/Sherlock",
      "ai_summary": "Sherlock, a self-correction and self-improvement framework for reasoning vision-language models, enhances accuracy across benchmarks using limited annotated data.",
      "ai_keywords": [
        "vision-language models",
        "self-correction",
        "trajectory-level self-correction",
        "preference data",
        "visual perturbation",
        "dynamic beta",
        "Llama3.2-Vision-11B",
        "LLaVA-CoT",
        "Mulberry",
        "LlamaV-o1"
      ]
    },
    "publishedAt": "2025-05-28T13:58:03.000Z",
    "title": "Sherlock: Self-Correcting Reasoning in Vision-Language Models",
    "summary": "Reasoning Vision-Language Models (VLMs) have shown promising performance on\ncomplex multimodal tasks. However, they still face significant challenges: they\nare highly sensitive to reasoning errors, require large volumes of annotated\ndata or accurate verifiers, and struggle to generalize beyond specific domains.\nTo address these limitations, we explore self-correction as a strategy to\nenhance reasoning VLMs. We first conduct an in-depth analysis of reasoning\nVLMs' self-correction abilities and identify key gaps. Based on our findings,\nwe introduce Sherlock, a self-correction and self-improvement training\nframework. Sherlock introduces a trajectory-level self-correction objective, a\npreference data construction method based on visual perturbation, and a dynamic\nbeta for preference tuning. Once the model acquires self-correction\ncapabilities using only 20k randomly sampled annotated data, it continues to\nself-improve without external supervision. Built on the Llama3.2-Vision-11B\nmodel, Sherlock achieves remarkable results across eight benchmarks, reaching\nan average accuracy of 64.1 with direct generation and 65.4 after\nself-correction. It outperforms LLaVA-CoT (63.2), Mulberry (63.9), and\nLlamaV-o1 (63.4) while using less than 20% of the annotated data.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22651.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "662678dfdd43e904ef1dcd03",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/662678dfdd43e904ef1dcd03/9bbnrnsRV3ylSuZXwcSNv.jpeg",
      "fullname": "Yi Ding",
      "name": "Tuwhy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.22453",
      "authors": [
        {
          "_id": "6837c318a4e378954486e45d",
          "user": {
            "_id": "64a16b1aeacb4b50ba1c889d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a16b1aeacb4b50ba1c889d/EhWoBGL6LlFGIVTUsp2F4.jpeg",
            "isPro": false,
            "fullname": "Lai Wei",
            "user": "WaltonFuture",
            "type": "user"
          },
          "name": "Lai Wei",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:42:35.145Z",
          "hidden": false
        },
        {
          "_id": "6837c318a4e378954486e45e",
          "name": "Yuting Li",
          "hidden": false
        },
        {
          "_id": "6837c318a4e378954486e45f",
          "name": "Chen Wang",
          "hidden": false
        },
        {
          "_id": "6837c318a4e378954486e460",
          "user": {
            "_id": "65e095da35ad8b2fe8c80e71",
            "avatarUrl": "/avatars/225d4be6411dcec2460047ea1a88a5c3.svg",
            "isPro": false,
            "fullname": "Weiran Huang",
            "user": "weiranhuang",
            "type": "user"
          },
          "name": "Yue Wang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-29T02:14:48.838Z",
          "hidden": false
        },
        {
          "_id": "6837c318a4e378954486e461",
          "name": "Linghe Kong",
          "hidden": false
        },
        {
          "_id": "6837c318a4e378954486e462",
          "user": {
            "_id": "65e095da35ad8b2fe8c80e71",
            "avatarUrl": "/avatars/225d4be6411dcec2460047ea1a88a5c3.svg",
            "isPro": false,
            "fullname": "Weiran Huang",
            "user": "weiranhuang",
            "type": "user"
          },
          "name": "Weiran Huang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-29T02:14:48.838Z",
          "hidden": false
        },
        {
          "_id": "6837c318a4e378954486e463",
          "name": "Lichao Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T15:11:16.000Z",
      "submittedOnDailyAt": "2025-05-29T00:45:28.570Z",
      "title": "Inférence d'un modèle de grande maîtrise multimodal en utilisant l'apprentissage sans points de vérification avec GRPO",
      "submittedOnDailyBy": {
        "_id": "64a16b1aeacb4b50ba1c889d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a16b1aeacb4b50ba1c889d/EhWoBGL6LlFGIVTUsp2F4.jpeg",
        "isPro": false,
        "fullname": "Lai Wei",
        "user": "WaltonFuture",
        "type": "user"
      },
      "summary": "En la phase d'Hodirain, l'amélioration des modèles de langage multimodal (MLLMs) se fait généralement par des ajustements de micro-réseaux (SFT) ou l'apprentissage par renforcement (RL). Cependant, ces méthodes traditionnelles nécessitent de données multimodales étiquetées de manière manuelle et coûteuses, ce qui n'est pas durable à long terme. D'autre part, les efforts récents se concentrent sur la revue de la phase d'Hodirain et sa méthodologie est complexe et répétitive. Dans cet article, nous explorons pour la première fois un méthode permettant une amélioration continue sans nécessité de normalisation externe en utilisant l'algorithme d'apprentissage par renforcement en ligne GRPO (Algorithme d'Apprentissage par Renforcement en Ligne stable et scalable). Nous proposons le cadre MM-UPT (Cadre de Training Multimodal Non-patterné simple et efficace), qui remplace les signaux de récompense existants par une stratégie d'auto-récompense basée sur le vote, basée sur GRPO. Les résultats des expériences montrent que MM-UPT améliore significativement la capacité de compréhension de Qwen2.5-VL-7B (par exemple, de 66.3% à 72.9% sur MathVista et de 62.9% à 68.7% sur We-Math), et qu'il peut être appliqué dans des situations où les étiquettes réelles ne sont pas disponibles, en utilisant uniquement des ensembles de données standards. MM-UPT dépasse les normes récentes d'Hodirain et approche les résultats d'un GRPO traditionnel. De plus, nous démontrons qu'il peut améliorer son performance en utilisant des questions synthétiques générées par le propre modèle MLLM, présentant une approche prometteuse pour une amélioration continue et autonome scalable. En général, MM-UPT offre un nouveau paradigme pour l'amélioration continue et autonome des MLLMs sans la nécessité de normalisation externe. Le code est disponible sur https://github.com/waltonfuture/MM-UPT.",
      "upvotes": 29,
      "discussionId": "6837c318a4e378954486e48a",
      "projectPage": "https://github.com/waltonfuture/MM-UPT",
      "githubRepo": "https://github.com/waltonfuture/MM-UPT",
      "ai_summary": "MM-UPT, a framework employing GRPO and self-rewarding, enhances multi-modal LLMs through unsupervised continual learning, showing performance improvements without manual annotations.",
      "ai_keywords": [
        "GRPO",
        "MM-UPT",
        "reinforcement learning",
        "unsupervised post-training",
        "multi-modal large language models",
        "self-rewarding mechanism",
        "majority voting",
        "synthetic questions",
        "MathVista",
        "We-Math"
      ]
    },
    "publishedAt": "2025-05-28T11:11:16.000Z",
    "title": "Unsupervised Post-Training for Multi-Modal LLM Reasoning via GRPO",
    "summary": "Improving Multi-modal Large Language Models (MLLMs) in the post-training\nstage typically relies on supervised fine-tuning (SFT) or reinforcement\nlearning (RL). However, these supervised methods require expensive and manually\nannotated multi-modal data--an ultimately unsustainable resource. While recent\nefforts have explored unsupervised post-training, their methods are complex and\ndifficult to iterate. In this work, we are the first to investigate the use of\nGRPO, a stable and scalable online RL algorithm, for enabling continual\nself-improvement without any external supervision. We propose MM-UPT, a simple\nyet effective framework for unsupervised post-training of MLLMs. MM-UPT builds\nupon GRPO, replacing traditional reward signals with a self-rewarding mechanism\nbased on majority voting over multiple sampled responses. Our experiments\ndemonstrate that MM-UPT significantly improves the reasoning ability of\nQwen2.5-VL-7B (e.g., 66.3 %rightarrow72.9 % on MathVista, 62.9\n%rightarrow68.7 % on We-Math), using standard dataset without ground truth\nlabels. MM-UPT also outperforms prior unsupervised baselines and even\napproaches the results of supervised GRPO. Furthermore, we show that\nincorporating synthetic questions, generated solely by MLLM itself, can boost\nperformance as well, highlighting a promising approach for scalable\nself-improvement. Overall, MM-UPT offers a new paradigm for continual,\nautonomous enhancement of MLLMs in the absence of external supervision. Our\ncode is available at https://github.com/waltonfuture/MM-UPT.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22453.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64a16b1aeacb4b50ba1c889d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a16b1aeacb4b50ba1c889d/EhWoBGL6LlFGIVTUsp2F4.jpeg",
      "fullname": "Lai Wei",
      "name": "WaltonFuture",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.21136",
      "authors": [
        {
          "_id": "6837c91ec790885f338b8f27",
          "user": {
            "_id": "66c0a08bac74db25de8427ec",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c0a08bac74db25de8427ec/9D-piDBZqSt6KNkHImmkv.jpeg",
            "isPro": false,
            "fullname": "Jintao Zhang",
            "user": "jt-zhang",
            "type": "user"
          },
          "name": "Jintao Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:42:22.525Z",
          "hidden": false
        },
        {
          "_id": "6837c91ec790885f338b8f28",
          "name": "Xiaoming Xu",
          "hidden": false
        },
        {
          "_id": "6837c91ec790885f338b8f29",
          "name": "Jia Wei",
          "hidden": false
        },
        {
          "_id": "6837c91ec790885f338b8f2a",
          "name": "Haofeng Huang",
          "hidden": false
        },
        {
          "_id": "6837c91ec790885f338b8f2b",
          "name": "Pengle Zhang",
          "hidden": false
        },
        {
          "_id": "6837c91ec790885f338b8f2c",
          "name": "Chendong Xiang",
          "hidden": false
        },
        {
          "_id": "6837c91ec790885f338b8f2d",
          "name": "Jun Zhu",
          "hidden": false
        },
        {
          "_id": "6837c91ec790885f338b8f2e",
          "name": "Jianfei Chen",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/66c0a08bac74db25de8427ec/e7t1XOditivMjvbyyouoc.png"
      ],
      "publishedAt": "2025-05-27T12:50:36.000Z",
      "submittedOnDailyAt": "2025-05-29T01:12:49.349Z",
      "title": "SageAttention2++ : Implémentation Plus Efficace de SageAttention2",
      "submittedOnDailyBy": {
        "_id": "66c0a08bac74db25de8427ec",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c0a08bac74db25de8427ec/9D-piDBZqSt6KNkHImmkv.jpeg",
        "isPro": false,
        "fullname": "Jintao Zhang",
        "user": "jt-zhang",
        "type": "user"
      },
      "summary": "L'efficacité de SageAttention2 est cruciale et résulte du fait que la quantité de calculs de temps augmente de manière quadratique avec la longueur de la séquence. SageAttention2 aborde ce problème en améliorant la vitesse de la multiplication de matrices (Matmul) en utilisant des compteurs. De plus, il propose l'utilisation de commandes rapides de multiplication de matrices en FP8 pour rendre SageAttention2 encore plus rapide. Ces commandes sont deux fois plus rapides que la multiplication de matrices en FP8 utilisée dans SageAttention2, et les résultats expérimentaux montrent que SageAttention2++ est 3,9 fois plus rapide que FlashAttention, tout en maintenant la même précision d'attention. Cela signifie une vitesse efficace pour les modèles de génération de langage, d'images et de vidéos, sans perte de performance mesurée à l'écran. Le code est disponible sur https://github.com/thu-ml/SageAttention.",
      "upvotes": 28,
      "discussionId": "6837c923c790885f338b90e5",
      "projectPage": "https://github.com/thu-ml/SageAttention",
      "githubRepo": "https://github.com/thu-ml/SageAttention",
      "ai_summary": "SageAttention2++ improves attention efficiency by using FP8 Matmul in FP16, achieving a 3.9x speedup over FlashAttention without losing accuracy.",
      "ai_keywords": [
        "attention",
        "time complexity",
        "sequence length",
        "quantization",
        "matrix multiplications",
        "Matmul",
        "FP8",
        "FP16",
        "SageAttention2",
        "SageAttention2++",
        "FlashAttention",
        "image generation",
        "video generation"
      ]
    },
    "publishedAt": "2025-05-27T08:50:36.000Z",
    "title": "SageAttention2++: A More Efficient Implementation of SageAttention2",
    "summary": "The efficiency of attention is critical because its time complexity grows\nquadratically with sequence length. SageAttention2 addresses this by utilizing\nquantization to accelerate matrix multiplications (Matmul) in attention. To\nfurther accelerate SageAttention2, we propose to utilize the faster instruction\nof FP8 Matmul accumulated in FP16. The instruction is 2x faster than the FP8\nMatmul used in SageAttention2. Our experiments show that SageAttention2++\nachieves a 3.9x speedup over FlashAttention while maintaining the same\nattention accuracy as SageAttention2. This means SageAttention2++ effectively\naccelerates various models, including those for language, image, and video\ngeneration, with negligible end-to-end metrics loss. The code will be available\nat https://github.com/thu-ml/SageAttention.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/66c0a08bac74db25de8427ec/e7t1XOditivMjvbyyouoc.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21136.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66c0a08bac74db25de8427ec",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c0a08bac74db25de8427ec/9D-piDBZqSt6KNkHImmkv.jpeg",
      "fullname": "Jintao Zhang",
      "name": "jt-zhang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.22334",
      "authors": [
        {
          "_id": "6837c360b127cae8a0b36e85",
          "user": {
            "_id": "64a16b1aeacb4b50ba1c889d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a16b1aeacb4b50ba1c889d/EhWoBGL6LlFGIVTUsp2F4.jpeg",
            "isPro": false,
            "fullname": "Lai Wei",
            "user": "WaltonFuture",
            "type": "user"
          },
          "name": "Lai Wei",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:42:26.705Z",
          "hidden": false
        },
        {
          "_id": "6837c360b127cae8a0b36e86",
          "name": "Yuting Li",
          "hidden": false
        },
        {
          "_id": "6837c360b127cae8a0b36e87",
          "name": "Kaipeng Zheng",
          "hidden": false
        },
        {
          "_id": "6837c360b127cae8a0b36e88",
          "name": "Chen Wang",
          "hidden": false
        },
        {
          "_id": "6837c360b127cae8a0b36e89",
          "user": {
            "_id": "65e095da35ad8b2fe8c80e71",
            "avatarUrl": "/avatars/225d4be6411dcec2460047ea1a88a5c3.svg",
            "isPro": false,
            "fullname": "Weiran Huang",
            "user": "weiranhuang",
            "type": "user"
          },
          "name": "Yue Wang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-29T02:16:03.908Z",
          "hidden": false
        },
        {
          "_id": "6837c360b127cae8a0b36e8a",
          "name": "Linghe Kong",
          "hidden": false
        },
        {
          "_id": "6837c360b127cae8a0b36e8b",
          "name": "Lichao Sun",
          "hidden": false
        },
        {
          "_id": "6837c360b127cae8a0b36e8c",
          "user": {
            "_id": "65e095da35ad8b2fe8c80e71",
            "avatarUrl": "/avatars/225d4be6411dcec2460047ea1a88a5c3.svg",
            "isPro": false,
            "fullname": "Weiran Huang",
            "user": "weiranhuang",
            "type": "user"
          },
          "name": "Weiran Huang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-29T02:16:03.908Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T13:21:38.000Z",
      "submittedOnDailyAt": "2025-05-29T00:46:20.111Z",
      "title": "Le développement de la logique multimodale à l'aide de l'apprentissage par renforcement au Royaume-Uni du Nord",
      "submittedOnDailyBy": {
        "_id": "64a16b1aeacb4b50ba1c889d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a16b1aeacb4b50ba1c889d/EhWoBGL6LlFGIVTUsp2F4.jpeg",
        "isPro": false,
        "fullname": "Lai Wei",
        "user": "WaltonFuture",
        "type": "user"
      },
      "summary": "Le développement récent de grands modèles de langage (LLMs) a montré une capacité impressionnante de pensée continu, ce qui permet clairement d'identifier le rôle crucial de l'apprentissage par refonte (RL) dans cette évolution. Les modèles représentant des patterns de \"moments mémorisés\" s'adaptent automatiquement, ce qui est généralement expliqué par la caractéristique épisodique de l'RL, mais ces patterns existent également dans de nombreux modèles avant l'entraînement par RL, et ne sont pas nécessairement strictement liés à l'amélioration logique. En se basant sur cette observation, nous proposons une série de recherches détaillées pour améliorer la capacité logique des MLLMs : 1. Nous utilisons un entraînement supervisé avec un début froid (SFT) pour générer des patterns de pensée continus structurés. 2. Nous améliorons la capacité avec l'apprentissage par refonte utilisant GRPO. Les expériences détaillées sur différents benchmarks de logique difficile pour les MLLMs montrent que l'utilisation de SFT ou RL seuls ne dépasse pas le méthode qui combine les deux. Enfin, le modèle résultant atteint les meilleurs rendements dans les MLLMs ouverts, constituant le leader dans les deux tailles de modèle : 3B et 7B. En particulier, le modèle 7B a considérablement amélioré par rapport au base (par exemple, de 66,3% à 73,4% sur MathVista et de 62,9% à 70,4% sur We-Math), tandis que le modèle 3B concurrence avec les rendements de plusieurs modèles 7B. Cette recherche fournit des guides pratiques pour la construction de MLLMs améliorés. Le code est disponible sur https://github.com/waltonfuture/RL-with-Cold-Start.",
      "upvotes": 25,
      "discussionId": "6837c363b127cae8a0b36f6f",
      "projectPage": "https://github.com/waltonfuture/RL-with-Cold-Start",
      "githubRepo": "https://github.com/waltonfuture/RL-with-Cold-Start",
      "ai_summary": "A two-stage approach combining supervised fine-tuning and reinforcement learning enhances multimodal reasoning in large language models, achieving state-of-the-art performance on benchmarks.",
      "ai_keywords": [
        "large language models",
        "LLMs",
        "multimodal LLMs",
        "MLLMs",
        "reinforcement learning",
        "RL",
        "chain-of-thought reasoning",
        "GRPO",
        "supervised fine-tuning",
        "SFT",
        "multimodal reasoning",
        "MathVista",
        "We-Math"
      ]
    },
    "publishedAt": "2025-05-28T09:21:38.000Z",
    "title": "Advancing Multimodal Reasoning via Reinforcement Learning with Cold\n  Start",
    "summary": "Recent advancements in large language models (LLMs) have demonstrated\nimpressive chain-of-thought reasoning capabilities, with reinforcement learning\n(RL) playing a crucial role in this progress. While \"aha moment\"\npatterns--where models exhibit self-correction through reflection--are often\nattributed to emergent properties from RL, we first demonstrate that these\npatterns exist in multimodal LLMs (MLLMs) prior to RL training but may not\nnecessarily correlate with improved reasoning performance. Building on these\ninsights, we present a comprehensive study on enhancing multimodal reasoning\nthrough a two-stage approach: (1) supervised fine-tuning (SFT) as a cold start\nwith structured chain-of-thought reasoning patterns, followed by (2)\nreinforcement learning via GRPO to further refine these capabilities. Our\nextensive experiments show that this combined approach consistently outperforms\nboth SFT-only and RL-only methods across challenging multimodal reasoning\nbenchmarks. The resulting models achieve state-of-the-art performance among\nopen-source MLLMs at both 3B and 7B scales, with our 7B model showing\nsubstantial improvements over base models (e.g., 66.3 %rightarrow73.4 % on\nMathVista, 62.9 %rightarrow70.4 % on We-Math) and our 3B model achieving\nperformance competitive with several 7B models. Overall, this work provides\npractical guidance for building advanced multimodal reasoning models. Our code\nis available at https://github.com/waltonfuture/RL-with-Cold-Start.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22334.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64a16b1aeacb4b50ba1c889d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a16b1aeacb4b50ba1c889d/EhWoBGL6LlFGIVTUsp2F4.jpeg",
      "fullname": "Lai Wei",
      "name": "WaltonFuture",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.22457",
      "authors": [
        {
          "_id": "68380912e9c1608de91e23f3",
          "name": "Haonan Wang",
          "hidden": false
        },
        {
          "_id": "68380912e9c1608de91e23f4",
          "name": "Hongfu Liu",
          "hidden": false
        },
        {
          "_id": "68380912e9c1608de91e23f5",
          "name": "Xiangyan Liu",
          "hidden": false
        },
        {
          "_id": "68380912e9c1608de91e23f6",
          "name": "Chao Du",
          "hidden": false
        },
        {
          "_id": "68380912e9c1608de91e23f7",
          "name": "Kenji Kawaguchi",
          "hidden": false
        },
        {
          "_id": "68380912e9c1608de91e23f8",
          "name": "Ye Wang",
          "hidden": false
        },
        {
          "_id": "68380912e9c1608de91e23f9",
          "name": "Tianyu Pang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T15:13:34.000Z",
      "submittedOnDailyAt": "2025-05-29T05:44:17.072Z",
      "title": "Ce sont les prévisions des événements prochains pour encourager le vidéo.",
      "submittedOnDailyBy": {
        "_id": "63d91b6d255ef6add20e1b38",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1675921369867-63d91b6d255ef6add20e1b38.jpeg",
        "isPro": false,
        "fullname": "Tianyu Pang",
        "user": "P2333",
        "type": "user"
      },
      "summary": "La prédiction des tokens suivante est une tâche de formation de base qui permet aux modèles de langage grands (LLM) de faire des inférences. Cependant, comment devraient-ils être formés les modèles d'apprentissage de langage multimodal (MLLM) dotés de capacités temporelles pour traiter des entrées vidéo ? Actuellement, les réponses des clients vidéo et d'autres éléments dépendent souvent de l'annotation humaine ou de MLLMs puissants. En contraste, les vidéos sont principalement utilisées pour combiner des inférences temporelles et de l'information spatiale. Pour aborder ces problèmes spatiaux, on propose la Prédiction d'Événements Futurs (NEP). L'NEP est une tâche de formation qui utilise des segments de vidéo futurs comme signaux riches et automatiquement observés pour favoriser des inférences temporelles. Les vidéos sont divisées en segments antérieurs et postérieurs, et le MLLM prédit un résumé des événements qui peuvent se produire dans les segments futurs à partir des segments passés. De cette manière, le modèle est incité à faire des inférences temporelles pour compléter la tâche. Pour soutenir cet objectif, on crée le jeu de données V1-33K. Ce jeu de données comprend 33 000 segments de vidéo extraits automatiquement et enregistre différentes scènes de vie réelle. De plus, on élargit le champ des stratégies d'entraînement d'instructions vidéo pour explorer l'effet des inférences temporelles. Pour évaluer le progrès, on introduit FutureBench, un méthode pour évaluer la concordance de la prédiction d'événements futurs non vus avant. Les expériences montrent que l'NEP fournit un modèle d'entraînement efficace et adapté pour favoriser les inférences temporelles dans les MLLM.",
      "upvotes": 23,
      "discussionId": "68380913e9c1608de91e2430",
      "githubRepo": "https://github.com/sail-sg/Video-Next-Event-Prediction",
      "ai_summary": "Next-event prediction (NEP) is proposed as a learning task to enable MLLMs to reason temporally over video inputs, using future video segments as a self-supervised signal.",
      "ai_keywords": [
        "next-token prediction",
        "next-event prediction (NEP)",
        "LLMs",
        "MLLMs",
        "video question answering",
        "video captioning",
        "temporal reasoning",
        "future video segments",
        "past frames",
        "video segments",
        "V1-33K",
        "video instruction-tuning strategies",
        "FutureBench"
      ]
    },
    "publishedAt": "2025-05-28T11:13:34.000Z",
    "title": "Fostering Video Reasoning via Next-Event Prediction",
    "summary": "Next-token prediction serves as the foundational learning task enabling\nreasoning in LLMs. But what should the learning task be when aiming to equip\nMLLMs with temporal reasoning capabilities over video inputs? Existing tasks\nsuch as video question answering often rely on annotations from humans or much\nstronger MLLMs, while video captioning tends to entangle temporal reasoning\nwith spatial information. To address this gap, we propose next-event prediction\n(NEP), a learning task that harnesses future video segments as a rich,\nself-supervised signal to foster temporal reasoning. We segment each video into\npast and future frames: the MLLM takes the past frames as input and predicts a\nsummary of events derived from the future frames, thereby encouraging the model\nto reason temporally in order to complete the task. To support this task, we\ncurate V1-33K, a dataset comprising 33,000 automatically extracted video\nsegments spanning diverse real-world scenarios. We further explore a range of\nvideo instruction-tuning strategies to study their effects on temporal\nreasoning. To evaluate progress, we introduce FutureBench to assess coherence\nin predicting unseen future events. Experiments validate that NEP offers a\nscalable and effective training paradigm for fostering temporal reasoning in\nMLLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22457.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63d91b6d255ef6add20e1b38",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1675921369867-63d91b6d255ef6add20e1b38.jpeg",
      "fullname": "Tianyu Pang",
      "name": "P2333",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.19253",
      "authors": [
        {
          "_id": "6837bc8d0b39c9653de4d06f",
          "user": {
            "_id": "6244451c9fdefb55a0b900cc",
            "avatarUrl": "/avatars/ca2b46ddb5d905501d827920582b5438.svg",
            "isPro": false,
            "fullname": "Joao Coelho",
            "user": "jmvcoelho",
            "type": "user"
          },
          "name": "João Coelho",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:42:45.049Z",
          "hidden": false
        },
        {
          "_id": "6837bc8d0b39c9653de4d070",
          "name": "Jingjie Ning",
          "hidden": false
        },
        {
          "_id": "6837bc8d0b39c9653de4d071",
          "name": "Jingyuan He",
          "hidden": false
        },
        {
          "_id": "6837bc8d0b39c9653de4d072",
          "name": "Kangrui Mao",
          "hidden": false
        },
        {
          "_id": "6837bc8d0b39c9653de4d073",
          "name": "Abhijay Paladugu",
          "hidden": false
        },
        {
          "_id": "6837bc8d0b39c9653de4d074",
          "name": "Pranav Setlur",
          "hidden": false
        },
        {
          "_id": "6837bc8d0b39c9653de4d075",
          "name": "Jiahe Jin",
          "hidden": false
        },
        {
          "_id": "6837bc8d0b39c9653de4d076",
          "name": "Jamie Callan",
          "hidden": false
        },
        {
          "_id": "6837bc8d0b39c9653de4d077",
          "name": "João Magalhães",
          "hidden": false
        },
        {
          "_id": "6837bc8d0b39c9653de4d078",
          "name": "Bruno Martins",
          "hidden": false
        },
        {
          "_id": "6837bc8d0b39c9653de4d079",
          "name": "Chenyan Xiong",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-25T18:16:13.000Z",
      "submittedOnDailyAt": "2025-05-29T00:28:31.472Z",
      "title": "DeepResearchGym : Libre, transparente, et avec reproductibilité dans le sandbox d'évaluation de recherche profonde",
      "submittedOnDailyBy": {
        "_id": "6135eeeb5bc6ecdf86b60f0d",
        "avatarUrl": "/avatars/43cedcf20ab6b0801a662787400e1384.svg",
        "isPro": false,
        "fullname": "Shi Yu",
        "user": "yushi",
        "type": "user"
      },
      "summary": "Les systèmes de recherche profonde sont une classe de méthodes de recherche d'information émergentes qui génèrent des rapports détaillés et riches en preuves pour des requêtes complexes. Cependant, de nombreux cadres de travail actuels dépendent d'APIs de recherche commerciales dynamiques et subissent des problèmes supplémentaires de reproductibilité et de transparence, ce qui les rendent coûteuses. Pour résoudre ces limitations, nous présentons DeepResearchGym, un sandbox de code open qui combine une API de recherche garantissant la reproductibilité avec un protocole d'évaluation strict pour effectuer des benchmarks de systèmes de recherche profonde. L'API utilise la plus récente technologie de recherche dense et de l'ANN en disque pour indexer des grands corpus de web publics tels que ClueWeb22 et FineWeb, atteignant un coût de mise à jour inférieur à celles des APIs commerciales populaires, assurant la stabilité de la classification des documents et permettant leur utilisation gratuite pour des fins d'investigation. Pour évaluer les résultats des systèmes de recherche profonde, nous avons étendu les métriques d'évaluation automatique grâce à l'évaluation avec un modèle de langue (LLM-as-a-judge) et développé le benchmark Researchy Questions pour évaluer l'adéquation à l'information du utilisateur, la précision de la recherche et la qualité du rapport. Les résultats des expérimentations montrent que les systèmes combinés avec DeepResearchGym atteignent le même rendement que ceux utilisant des APIs commerciales, et la position de rendement coïncide avec les métriques d'évaluation. De plus, les études d'évaluation humaine ont confirmé que le protocole d'évaluation automatique est en accord avec les préférences humaines et que le cadre de travail peut aider les systèmes de recherche profonde à effectuer des évaluations contrôlées. Les codes et documentations de l'API sont disponibles sur https://www.deepresearchgym.ai.",
      "upvotes": 17,
      "discussionId": "6837bc8d0b39c9653de4d0a6",
      "projectPage": "https://www.deepresearchgym.ai",
      "ai_summary": "DeepResearchGym provides an open-source evaluation framework for deep research systems using a reproducible search API and LLM-as-a-judge assessments.",
      "ai_keywords": [
        "agentic information retrieval",
        "deep research systems",
        "search API",
        "reproducibility",
        "transparency",
        "open-source sandbox",
        "ClueWeb22",
        "FineWeb",
        "dense retriever",
        "approximate nearest neighbor search",
        "DiskANN",
        "Researchy Questions benchmark",
        "LLM-as-a-judge",
        "retrieval faithfulness",
        "report quality",
        "human evaluation"
      ]
    },
    "publishedAt": "2025-05-25T14:16:13.000Z",
    "title": "DeepResearchGym: A Free, Transparent, and Reproducible Evaluation\n  Sandbox for Deep Research",
    "summary": "Deep research systems represent an emerging class of agentic information\nretrieval methods that generate comprehensive and well-supported reports to\ncomplex queries. However, most existing frameworks rely on dynamic commercial\nsearch APIs, which pose reproducibility and transparency challenges in addition\nto their cost. To address these limitations, we introduce DeepResearchGym, an\nopen-source sandbox that combines a reproducible search API with a rigorous\nevaluation protocol for benchmarking deep research systems. The API indexes\nlarge-scale public web corpora, namely ClueWeb22 and FineWeb, using a\nstate-of-the-art dense retriever and approximate nearest neighbor search via\nDiskANN. It achieves lower latency than popular commercial APIs while ensuring\nstable document rankings across runs, and is freely available for research use.\nTo evaluate deep research systems' outputs, we extend the Researchy Questions\nbenchmark with automatic metrics through LLM-as-a-judge assessments to measure\nalignment with users' information needs, retrieval faithfulness, and report\nquality. Experimental results show that systems integrated with DeepResearchGym\nachieve performance comparable to those using commercial APIs, with performance\nrankings remaining consistent across evaluation metrics. A human evaluation\nstudy further confirms that our automatic protocol aligns with human\npreferences, validating the framework's ability to help support controlled\nassessment of deep research systems. Our code and API documentation are\navailable at https://www.deepresearchgym.ai.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19253.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6135eeeb5bc6ecdf86b60f0d",
      "avatarUrl": "/avatars/43cedcf20ab6b0801a662787400e1384.svg",
      "fullname": "Shi Yu",
      "name": "yushi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.21925",
      "authors": [
        {
          "_id": "6837c23acce400abe6f18790",
          "user": {
            "_id": "60747cbf3ea03830676542b5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60747cbf3ea03830676542b5/wGr1Jzz520JM9nZ-UcLyb.png",
            "isPro": false,
            "fullname": "Chong Zeng",
            "user": "NCJ",
            "type": "user"
          },
          "name": "Chong Zeng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:42:38.730Z",
          "hidden": false
        },
        {
          "_id": "6837c23acce400abe6f18791",
          "user": {
            "_id": "63299011bdb6242b42b77f57",
            "avatarUrl": "/avatars/056aec97eb3c10d3b63eb13238e1d2a4.svg",
            "isPro": false,
            "fullname": "doyleconan",
            "user": "doyleconan",
            "type": "user"
          },
          "name": "Yue Dong",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-29T02:11:08.981Z",
          "hidden": false
        },
        {
          "_id": "6837c23acce400abe6f18792",
          "name": "Pieter Peers",
          "hidden": false
        },
        {
          "_id": "6837c23acce400abe6f18793",
          "name": "Hongzhi Wu",
          "hidden": false
        },
        {
          "_id": "6837c23acce400abe6f18794",
          "name": "Xin Tong",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/60747cbf3ea03830676542b5/DuGYODs8k7DhVHb78BmCZ.jpeg"
      ],
      "publishedAt": "2025-05-28T03:20:46.000Z",
      "submittedOnDailyAt": "2025-05-29T00:45:14.960Z",
      "title": "RenderFormer : Triangles de Correspondance Basé sur le Transformer pour la Renormalisation et l'Illumination Globale",
      "submittedOnDailyBy": {
        "_id": "60747cbf3ea03830676542b5",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60747cbf3ea03830676542b5/wGr1Jzz520JM9nZ-UcLyb.png",
        "isPro": false,
        "fullname": "Chong Zeng",
        "user": "NCJ",
        "type": "user"
      },
      "summary": "RenderFormer est une nouvelle réseau neuronal de rendu qui effectue la représentation de scènes basée sur des triangles pour rendre des images directement. Cette réseau permet d'obtenir un effet d'illumination global complet, ce qui signifie qu'il n'est pas nécessaire d'entraîner ou d'ajuster chaque scène individuellement. Sa architecture est basée sur la transformation de colonnes de tokens représentant des triangles avec des propriétés de réflexion en colonnes de tokens représentant de petits motifs de pixels. RenderFormer utilise un flux de travail en deux étapes : dans l'étape indépendante, on modélise la propagation de la lumière des triangles vers d'autres triangles, tandis que dans l'étape dépendante, on transforme les tokens des triangles obtenus dans l'étape indépendante en valeurs de pixels représentant les lignes de lumière. Les deux étapes sont entraînées sous une architecture transformée avec des restrictions minimales préalables. On évalue la capacité et le rendement de RenderFormer dans des scènes de complexité variable en termes de forme et de propagation de la lumière.",
      "upvotes": 15,
      "discussionId": "6837c23ccce400abe6f18812",
      "projectPage": "https://microsoft.github.io/renderformer/",
      "githubRepo": "https://github.com/microsoft/renderformer",
      "ai_summary": "RenderFormer is a transformer-based neural rendering pipeline that renders images from triangle representations without per-scene training and with full global illumination effects.",
      "ai_keywords": [
        "neural rendering pipeline",
        "global illumination effects",
        "sequence-to-sequence transformation",
        "tokens",
        "reflectance properties",
        "pixel patches",
        "transformer architecture",
        "view-independent stage",
        "view-dependent stage",
        "triangle-to-triangle light transport",
        "ray bundles"
      ]
    },
    "publishedAt": "2025-05-27T23:20:46.000Z",
    "title": "RenderFormer: Transformer-based Neural Rendering of Triangle Meshes with\n  Global Illumination",
    "summary": "We present RenderFormer, a neural rendering pipeline that directly renders an\nimage from a triangle-based representation of a scene with full global\nillumination effects and that does not require per-scene training or\nfine-tuning. Instead of taking a physics-centric approach to rendering, we\nformulate rendering as a sequence-to-sequence transformation where a sequence\nof tokens representing triangles with reflectance properties is converted to a\nsequence of output tokens representing small patches of pixels. RenderFormer\nfollows a two stage pipeline: a view-independent stage that models\ntriangle-to-triangle light transport, and a view-dependent stage that\ntransforms a token representing a bundle of rays to the corresponding pixel\nvalues guided by the triangle-sequence from the view-independent stage. Both\nstages are based on the transformer architecture and are learned with minimal\nprior constraints. We demonstrate and evaluate RenderFormer on scenes with\nvarying complexity in shape and light transport.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/60747cbf3ea03830676542b5/DuGYODs8k7DhVHb78BmCZ.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21925.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60747cbf3ea03830676542b5",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60747cbf3ea03830676542b5/wGr1Jzz520JM9nZ-UcLyb.png",
      "fullname": "Chong Zeng",
      "name": "NCJ",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.18600",
      "authors": [
        {
          "_id": "6837fe7664391bba7e477747",
          "name": "Bryan Sangwoo Kim",
          "hidden": false
        },
        {
          "_id": "6837fe7664391bba7e477748",
          "name": "Jeongsol Kim",
          "hidden": false
        },
        {
          "_id": "6837fe7664391bba7e477749",
          "name": "Jong Chul Ye",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-24T08:50:08.000Z",
      "submittedOnDailyAt": "2025-05-29T05:07:53.112Z",
      "title": "Chain-of-Zoom : Résultat à l'extrême de la hauteur de résolution par une échelle automatique de régression et un ajustement d'intérêt",
      "submittedOnDailyBy": {
        "_id": "6628efe14e1fa854f48d3a28",
        "avatarUrl": "/avatars/aa5421149a07a82b5c2a25978f9b6926.svg",
        "isPro": false,
        "fullname": "Sangwoo Kim",
        "user": "bryanswkim",
        "type": "user"
      },
      "summary": "Les modèles de haute résolution d'une image unique (SISR) modernes offrent des résultats réalistes dans un intervalle de facteurs d'échelle entraînés, mais s'effacent lorsqu'ils sont amplifiés à une échelle plus grande. Pour résoudre ce problème d'échelle, on utilise la Chain-of-Zoom (CoZ). La CoZ décompose le SISR en utilisant une connexion auto-régressive intermédiaire qui contient des informations de plusieurs échelles, basée sur un Prompt qui inclut des informations de plusieurs échelles. La CoZ réutilise le modèle principal de SR et divise le problème en sous-problèmes permettant de calculer des probabilités conditionnelles, atteignant des résolutions élevées sans nécessiter d'entraînement supplémentaire. En raison du déclin de la qualité visuelle à des échelles élevées, des Prompts incluant des informations de plusieurs échelles générés par un modèle de langage visuel (VLM) sont ajoutés à chaque étape d'amplification. Ce processus d'extraction de Prompts est aligné avec la dynamique de texte guidée par des préférences humaines en utilisant la Policy Optimization de Reward Generalisé (GRPO) et un VLM de bord. Les expériences montrent que l'implémentation d'un modèle de SR standard de 4x dans la CoZ permet d'atteindre une amplification de plus de 256x avec une qualité visuelle élevée et une fidélité élevée. Page du projet : https://bryanswkim.github.io/chain-of-zoom/ .",
      "upvotes": 15,
      "discussionId": "6837fe7864391bba7e47779b",
      "projectPage": "https://bryanswkim.github.io/chain-of-zoom/",
      "githubRepo": "https://github.com/bryanswkim/Chain-of-Zoom",
      "ai_summary": "Chain-of-Zoom (CoZ) enhances single-image super-resolution models by using an autoregressive chain of intermediate scale-states and multi-scale-aware prompts to achieve extreme magnifications with high quality.",
      "ai_keywords": [
        "single-image super-resolution",
        "Chain-of-Zoom",
        "autoregressive chain",
        "multi-scale-aware prompts",
        "backbone SR model",
        "diffusion SR model",
        "prompt extractor",
        "Generalized Reward Policy Optimization",
        "critic VLM"
      ]
    },
    "publishedAt": "2025-05-24T04:50:08.000Z",
    "title": "Chain-of-Zoom: Extreme Super-Resolution via Scale Autoregression and\n  Preference Alignment",
    "summary": "Modern single-image super-resolution (SISR) models deliver photo-realistic\nresults at the scale factors on which they are trained, but collapse when asked\nto magnify far beyond that regime. We address this scalability bottleneck with\nChain-of-Zoom (CoZ), a model-agnostic framework that factorizes SISR into an\nautoregressive chain of intermediate scale-states with multi-scale-aware\nprompts. CoZ repeatedly re-uses a backbone SR model, decomposing the\nconditional probability into tractable sub-problems to achieve extreme\nresolutions without additional training. Because visual cues diminish at high\nmagnifications, we augment each zoom step with multi-scale-aware text prompts\ngenerated by a vision-language model (VLM). The prompt extractor itself is\nfine-tuned using Generalized Reward Policy Optimization (GRPO) with a critic\nVLM, aligning text guidance towards human preference. Experiments show that a\nstandard 4x diffusion SR model wrapped in CoZ attains beyond 256x enlargement\nwith high perceptual quality and fidelity. Project Page:\nhttps://bryanswkim.github.io/chain-of-zoom/ .",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18600.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6628efe14e1fa854f48d3a28",
      "avatarUrl": "/avatars/aa5421149a07a82b5c2a25978f9b6926.svg",
      "fullname": "Sangwoo Kim",
      "name": "bryanswkim",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.19075",
      "authors": [
        {
          "_id": "6837fc484d14d7c8800e8b9c",
          "user": {
            "_id": "64c3732de6c3860fba66ceb0",
            "avatarUrl": "/avatars/785783ca08687923053eac641326281f.svg",
            "isPro": false,
            "fullname": "JaeminKim",
            "user": "kjm981995",
            "type": "user"
          },
          "name": "Jaemin Kim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:41:35.589Z",
          "hidden": false
        },
        {
          "_id": "6837fc484d14d7c8800e8b9d",
          "name": "Hangeol Chang",
          "hidden": false
        },
        {
          "_id": "6837fc484d14d7c8800e8b9e",
          "name": "Hyunmin Hwang",
          "hidden": false
        },
        {
          "_id": "6837fc484d14d7c8800e8b9f",
          "name": "Choonghan Kim",
          "hidden": false
        },
        {
          "_id": "6837fc484d14d7c8800e8ba0",
          "name": "Jong Chul Ye",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-25T10:19:10.000Z",
      "submittedOnDailyAt": "2025-05-29T04:49:39.731Z",
      "title": "Universal Reservoir : Utilise un seul réservoir combinable plug-and-play pour les LLMs gelées.",
      "submittedOnDailyBy": {
        "_id": "64c3732de6c3860fba66ceb0",
        "avatarUrl": "/avatars/785783ca08687923053eac641326281f.svg",
        "isPro": false,
        "fullname": "JaeminKim",
        "user": "kjm981995",
        "type": "user"
      },
      "summary": "Les modèles de langage grands (LLMs) montrent des capacités générales exceptionnelles, mais leur amélioration dans le domaine logique est associée à une charge de ressources informatiques importante, et peut même entraîner des pertes dans leur généralisation. Le méthode de Fine-Tuning d'Efficience de Paramètres (PEFT) a été proposée comme une alternative plus respectueuse des ressources informatiques, mais nécessite un entraînement supplémentaire pour chaque modèle LLM, dépendant de sa architecture. Pour résoudre ces problèmes, nous proposons le Universal Reasoner (UniR). UniR est un module logique léger, combinable, pluggable et playable. Ce module peut être combiné avec n'importe quel modèle LLM sans restrictions pour lui conférer des habiletés logiques spécifiques. En particulier, UniR est un module logique spécialisé qui est entraîné indépendamment en utilisant des compensations réservées, transformant des signaux de niveau de processus en guides de niveau de token. Après l'entraînement, UniR est simplement ajouté à la sortie logistique du modèle LLM, permettant sa combinaison avec n'importe quel modèle LLM sans restrictions. Cette structure supplémentaire facilite la combinaison de modules : des modules de UniR entraînés sur des tâches différentes peuvent être appliqués ensemble en sommant les sorties logistiques pour aborder des problèmes logiques complexes. Les résultats des expériences en logique mathématique et en traduction automatique montrent que UniR dépasse significativement les méthodes de réentraînement des baselines existants en utilisant le modèle Llama3.2. De plus, UniR montre une forte généralisation, démontrant que des modules logiques entraînés sur de petits modèles peuvent guider efficacement des modèles plus grands. Cela représente une solution efficace, adaptable et puissante pour améliorer la logique des LLMs, évitant de nuire à leurs capacités principales. Le code est disponible sur https://github.com/hangeol/UniR.",
      "upvotes": 14,
      "discussionId": "6837fc494d14d7c8800e8be6",
      "ai_summary": "UniR, a lightweight reasoning module, enhances Large Language Models with specialized reasoning abilities through modular composition, improving performance and generalization at lower computational costs.",
      "ai_keywords": [
        "Large Language Models",
        "Parameter-Efficient Fine-Tuning",
        "Universal Reasoner",
        "trajectory-level signals",
        "token-level guidance",
        "additive structure",
        "modular composition",
        "Llama3.2",
        "mathematical reasoning",
        "machine translation",
        "cost-efficient",
        "adaptable",
        "robust"
      ]
    },
    "publishedAt": "2025-05-25T06:19:10.000Z",
    "title": "Universal Reasoner: A Single, Composable Plug-and-Play Reasoner for\n  Frozen LLMs",
    "summary": "Large Language Models (LLMs) have demonstrated remarkable general\ncapabilities, but enhancing skills such as reasoning often demands substantial\ncomputational resources and may compromise their generalization. While\nParameter-Efficient Fine-Tuning (PEFT) methods offer a more resource-conscious\nalternative, they typically requires retraining for each LLM backbone due to\narchitectural dependencies. To address these challenges, here we propose\nUniversal Reasoner (UniR) - a single, lightweight, composable, and\nplug-and-play reasoning module that can be used with any frozen LLM to endow it\nwith specialized reasoning capabilities. Specifically, UniR decomposes the\nreward into a standalone reasoning module that is trained independently using\npredefined rewards, effectively translating trajectory-level signals into\ntoken-level guidance. Once trained, UniR can be combined with any frozen LLM at\ninference time by simply adding its output logits to those of the LLM backbone.\nThis additive structure naturally enables modular composition: multiple UniR\nmodules trained for different tasks can be jointly applied by summing their\nlogits, enabling complex reasoning via composition. Experimental results on\nmathematical reasoning and machine translation tasks show that UniR\nsignificantly outperforms existing baseline fine-tuning methods using the\nLlama3.2 model. Furthermore, UniR demonstrates strong weak-to-strong\ngeneralization: reasoning modules trained on smaller models effectively guide\nmuch larger LLMs. This makes UniR a cost-efficient, adaptable, and robust\nsolution for enhancing reasoning in LLMs without compromising their core\ncapabilities. Code is open-sourced at https://github.com/hangeol/UniR",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19075.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64c3732de6c3860fba66ceb0",
      "avatarUrl": "/avatars/785783ca08687923053eac641326281f.svg",
      "fullname": "JaeminKim",
      "name": "kjm981995",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.21887",
      "authors": [
        {
          "_id": "6837e3400aa18c6f96fe4876",
          "user": {
            "_id": "656864e12d73834278a8dea7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg",
            "isPro": true,
            "fullname": "Ahmed Heakl",
            "user": "ahmedheakl",
            "type": "user"
          },
          "name": "Ahmed Heakl",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:41:49.456Z",
          "hidden": false
        },
        {
          "_id": "6837e3400aa18c6f96fe4877",
          "name": "Yahia Salaheldin Shaaban",
          "hidden": false
        },
        {
          "_id": "6837e3400aa18c6f96fe4878",
          "name": "Martin Takac",
          "hidden": false
        },
        {
          "_id": "6837e3400aa18c6f96fe4879",
          "name": "Salem Lahlou",
          "hidden": false
        },
        {
          "_id": "6837e3400aa18c6f96fe487a",
          "name": "Zangir Iklassov",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/Xa51WJrOIV-xl7A-Ry4t2.png",
        "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/eS_bEDm2o2WAbn3YPS2XR.png",
        "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/jaG4WahYyrQGaalZObNew.png",
        "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/mMKY5jn0SCJPxxR8_mssx.png"
      ],
      "publishedAt": "2025-05-28T02:03:31.000Z",
      "submittedOnDailyAt": "2025-05-29T03:02:06.399Z",
      "title": "SVRPBench : Benchmark Pratique pour le Problème du Routage de Voies Aléatoires",
      "submittedOnDailyBy": {
        "_id": "656864e12d73834278a8dea7",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg",
        "isPro": true,
        "fullname": "Ahmed Heakl",
        "user": "ahmedheakl",
        "type": "user"
      },
      "summary": "Le processus de gestion des routes sûres est central dans la logistique réelle, et la plupart des benchmarks supposent des environnements statiques et idéaux. Nous présentons SVRPBench, le premier benchmark ouvert pour comprendre la précision élevée du processus de gestion des routes à l'échelle urbaine. Expérience avec plus de 500 instances et plus de 1000 cranes, qui modélisent des fenêtres de temps expérimentales basées sur un trafic dépendant du temps, des retards logarithmiques, des pensées aléatoires, des cranes résidentiels et commerciaux. Notre processus génère des scénarios divers et inclut une riche variété de configurations et de contraintes. À travers les résultats du benchmark, même les solveurs RL les plus avancés, comme POMO et AM, subissent un baisse de rendement de 20% ou plus en raison de variations distribuées, tandis que les méthodes classiques et méta-hybrides sont robustes. Nous libérons le jeu de données et le système d'évaluation pour faciliter la recherche expérimentale. SVRPBench défie la communauté à concevoir des solveurs généralisables au-delà des hypothèses synthétiques.",
      "upvotes": 13,
      "discussionId": "6837e3410aa18c6f96fe48b8",
      "githubRepo": "https://github.com/yehias21/vrp-benchmarks",
      "ai_summary": "SVRPBench introduces a new benchmark for vehicle routing under uncertainty, simulating realistic urban conditions and highlighting the limitations of state-of-the-art RL solvers.",
      "ai_keywords": [
        "vehicle routing",
        "SVRPBench",
        "time-dependent congestion",
        "log-normal delays",
        "probabilistic accidents",
        "multi-depot",
        "multi-vehicle",
        "state-of-the-art RL solvers",
        "POMO",
        "AM",
        "distributional shift",
        "classical methods",
        "metaheuristic methods"
      ]
    },
    "publishedAt": "2025-05-27T22:03:31.000Z",
    "title": "SVRPBench: A Realistic Benchmark for Stochastic Vehicle Routing Problem",
    "summary": "Robust routing under uncertainty is central to real-world logistics, yet most\nbenchmarks assume static, idealized settings. We present SVRPBench, the first\nopen benchmark to capture high-fidelity stochastic dynamics in vehicle routing\nat urban scale. Spanning more than 500 instances with up to 1000 customers, it\nsimulates realistic delivery conditions: time-dependent congestion, log-normal\ndelays, probabilistic accidents, and empirically grounded time windows for\nresidential and commercial clients. Our pipeline generates diverse,\nconstraint-rich scenarios, including multi-depot and multi-vehicle setups.\nBenchmarking reveals that state-of-the-art RL solvers like POMO and AM degrade\nby over 20% under distributional shift, while classical and metaheuristic\nmethods remain robust. To enable reproducible research, we release the dataset\nand evaluation suite. SVRPBench challenges the community to design solvers that\ngeneralize beyond synthetic assumptions and adapt to real-world uncertainty.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/Xa51WJrOIV-xl7A-Ry4t2.png",
      "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/eS_bEDm2o2WAbn3YPS2XR.png",
      "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/jaG4WahYyrQGaalZObNew.png",
      "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/mMKY5jn0SCJPxxR8_mssx.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21887.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "656864e12d73834278a8dea7",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg",
      "fullname": "Ahmed Heakl",
      "name": "ahmedheakl",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 39
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.22129",
      "authors": [
        {
          "_id": "6837dae6e9b21653755a05d4",
          "user": {
            "_id": "64c71a5647418a0a59e5c7cb",
            "avatarUrl": "/avatars/a99ab24c0c19b1399d2e6795fb9d7000.svg",
            "isPro": false,
            "fullname": "Jinhong Ni",
            "user": "mcleanie",
            "type": "user"
          },
          "name": "Jinhong Ni",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:42:03.476Z",
          "hidden": false
        },
        {
          "_id": "6837dae6e9b21653755a05d5",
          "user": {
            "_id": "65434daa5a36a8774d0e2271",
            "avatarUrl": "/avatars/abc3ddec72072121130d581e32cd9045.svg",
            "isPro": false,
            "fullname": "Allen Zhang",
            "user": "allencbzhang",
            "type": "user"
          },
          "name": "Chang-Bin Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:41:59.930Z",
          "hidden": false
        },
        {
          "_id": "6837dae6e9b21653755a05d6",
          "name": "Qiang Zhang",
          "hidden": false
        },
        {
          "_id": "6837dae6e9b21653755a05d7",
          "name": "Jing Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T08:54:04.000Z",
      "submittedOnDailyAt": "2025-05-29T02:38:27.338Z",
      "title": "Quels sont les facteurs essentiels pour la génération stable d'un panorama de 360 degrés ?",
      "submittedOnDailyBy": {
        "_id": "65434daa5a36a8774d0e2271",
        "avatarUrl": "/avatars/abc3ddec72072121130d581e32cd9045.svg",
        "isPro": false,
        "fullname": "Allen Zhang",
        "user": "allencbzhang",
        "type": "user"
      },
      "summary": "Récemment, un étude a commencé qui applique l'abondance de modèles de diffusion d'images à la génération d'images panoramiques 360°, comme Stable Diffusion. Les études précédentes ont démontré l'efficacité des techniques générales de diffusion de faible performance, en utilisant des modèles de diffusion entraînés précédemment pour générer des images panoramiques. Cependant, il existe des doutes sur la grande différence d'angles de capture et l'aire entre les images panoramiques, ainsi que les succès expérimentaux de ces études. Nous avons étudié que les composants entrainables sont finalement entraînés sur des données panoramiques et montrent des comportements différents, et que cette diffusion cache une fonction interne qui utilise le savoir préalablement entraîné au sein du modèle de diffusion. Notre analyse résume : 1) les modules d'attention et les matrices de requête et clé sont responsables de l'information commun partagée entre l'aire panoramique et l'angle de capture, et ne sont pas liés à la génération panoramique ; 2) les matrices de valeurs et de poids de sortie se spécialisent à appliquer le savoir préalablement entraîné dans l'aire panoramique, et jouent un rôle plus important dans l'entraînement final de la génération panoramique. Nous proposons un simple cadre expérimental pour tester ces observations et appelons ce cadre UniPano. Notre objectif est de créer un excellent référent pour futures recherches. UniPano dépasse les méthodes existantes et, en comparaison avec l'approche double des études précédentes, réduit significativement l'utilisation de la mémoire et le temps d'entraînement, tout en pouvant également être appliqué à la génération de panoramas de haute résolution. Le code est disponible.",
      "upvotes": 12,
      "discussionId": "6837daece9b21653755a0791",
      "ai_summary": "Analysis of fine-tuning diffusion models for panoramic image generation reveals distinct roles of attention module matrices and introduces UniPano, a memory-efficient and speed-enhanced baseline framework.",
      "ai_keywords": [
        "text-to-image diffusion models",
        "Stable Diffusion",
        "low-rank adaptation",
        "pre-trained diffusion models",
        "attention modules",
        "query matrices",
        "key matrices",
        "value matrices",
        "output weight matrices",
        "panoramic image generation",
        "domain gap",
        "common information",
        "pre-trained knowledge",
        "UniPano",
        "end-to-end panorama generation",
        "memory usage",
        "training time"
      ]
    },
    "publishedAt": "2025-05-28T04:54:04.000Z",
    "title": "What Makes for Text to 360-degree Panorama Generation with Stable\n  Diffusion?",
    "summary": "Recent prosperity of text-to-image diffusion models, e.g. Stable Diffusion,\nhas stimulated research to adapt them to 360-degree panorama generation. Prior\nwork has demonstrated the feasibility of using conventional low-rank adaptation\ntechniques on pre-trained diffusion models to generate panoramic images.\nHowever, the substantial domain gap between perspective and panoramic images\nraises questions about the underlying mechanisms enabling this empirical\nsuccess. We hypothesize and examine that the trainable counterparts exhibit\ndistinct behaviors when fine-tuned on panoramic data, and such an adaptation\nconceals some intrinsic mechanism to leverage the prior knowledge within the\npre-trained diffusion models. Our analysis reveals the following: 1) the query\nand key matrices in the attention modules are responsible for common\ninformation that can be shared between the panoramic and perspective domains,\nthus are less relevant to panorama generation; and 2) the value and output\nweight matrices specialize in adapting pre-trained knowledge to the panoramic\ndomain, playing a more critical role during fine-tuning for panorama\ngeneration. We empirically verify these insights by introducing a simple\nframework called UniPano, with the objective of establishing an elegant\nbaseline for future research. UniPano not only outperforms existing methods but\nalso significantly reduces memory usage and training time compared to prior\ndual-branch approaches, making it scalable for end-to-end panorama generation\nwith higher resolution. The code will be released.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22129.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65434daa5a36a8774d0e2271",
      "avatarUrl": "/avatars/abc3ddec72072121130d581e32cd9045.svg",
      "fullname": "Allen Zhang",
      "name": "allencbzhang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.20411",
      "authors": [
        {
          "_id": "683735aff42cc8a1d260e677",
          "user": {
            "_id": "654e5e094319c75e3e1b6cbc",
            "avatarUrl": "/avatars/a8889036fa38f80f2d45aea8d1471395.svg",
            "isPro": false,
            "fullname": "Ibragim",
            "user": "ibragim-bad",
            "type": "user"
          },
          "name": "Ibragim Badertdinov",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T09:40:24.705Z",
          "hidden": false
        },
        {
          "_id": "683735aff42cc8a1d260e678",
          "user": {
            "_id": "644e9ffcd6001776ed77d874",
            "avatarUrl": "/avatars/b93e02caf929679b7e9bc589eed0b689.svg",
            "isPro": false,
            "fullname": "Alexander",
            "user": "djalexj",
            "type": "user"
          },
          "name": "Alexander Golubev",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-28T19:13:38.030Z",
          "hidden": false
        },
        {
          "_id": "683735aff42cc8a1d260e679",
          "name": "Maksim Nekrashevich",
          "hidden": false
        },
        {
          "_id": "683735aff42cc8a1d260e67a",
          "name": "Anton Shevtsov",
          "hidden": false
        },
        {
          "_id": "683735aff42cc8a1d260e67b",
          "user": {
            "_id": "65e48cb3a4e46e644ec1277d",
            "avatarUrl": "/avatars/dd2bf04a6f81bf0a0892080af5d485b2.svg",
            "isPro": false,
            "fullname": "Simon Karasik",
            "user": "sbkarasik",
            "type": "user"
          },
          "name": "Simon Karasik",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T09:40:26.644Z",
          "hidden": false
        },
        {
          "_id": "683735aff42cc8a1d260e67c",
          "name": "Andrei Andriushchenko",
          "hidden": false
        },
        {
          "_id": "683735aff42cc8a1d260e67d",
          "name": "Maria Trofimova",
          "hidden": false
        },
        {
          "_id": "683735aff42cc8a1d260e67e",
          "name": "Daria Litvintseva",
          "hidden": false
        },
        {
          "_id": "683735aff42cc8a1d260e67f",
          "name": "Boris Yangel",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-26T18:01:00.000Z",
      "submittedOnDailyAt": "2025-05-29T06:59:04.261Z",
      "title": "SWE-rebench : Pipelina pour automatiser la statistique des tâches et l'évaluation de la décontamination en ingénierie logicielle",
      "submittedOnDailyBy": {
        "_id": "644e9ffcd6001776ed77d874",
        "avatarUrl": "/avatars/b93e02caf929679b7e9bc589eed0b689.svg",
        "isPro": false,
        "fullname": "Alexander",
        "user": "djalexj",
        "type": "user"
      },
      "summary": "Les agents basés sur des modèles de langage grands (LLM) montrent de plus en plus de capacités variées dans les tâches de développement de logiciel (SWE). Cependant, leur développement fait face à deux problèmes importants. Le premier est l'insuffisance de données d'entraînement de haute qualité, en particulier, des données qui reflètent des scénarios réels de SWE où l'agent interagit avec l'environnement de développement, exécute du code et modifie son comportement en fonction des résultats de ces exercices. Les ensembles de données actuels sont limités à un ensemble de tâches de génération de code ou de petites modifications interactives, ce qui limite leur échelle et diversité. Le second problème est la manque de nouvelles tâches interactives, ce qui affecte significativement l'évaluation des modèles qui s'améliorent rapidement. Les benchmarks statiques se dégradent rapidement en raison de problèmes de conteneurs. Pour résoudre ces limitations, nous avons introduit une nouvelle, automatisée et échellable chaîne de travail pour extraire de manière continue des tâches interactives de différents dépôts de GitHub. Cette chaîne de travail a permis la construction du jeu de données public SWE-rebench. Ce jeu de données comprend plus de 21 000 tâches interactives de SWE basées sur Python et est conçu pour l'apprentissage par renforcement d'agents de SWE. De plus, grâce à l'utilisation du méthode SWE-rebench pour la récolte de nouvelles tâches, nous avons construit un benchmark qui ne se voit pas affecté par des problèmes de conteneurs. Dans ce benchmark, les résultats de chaque LLM sont comparés à ceux de SWE-bench Verified, ce qui démontre la possibilité que le rendement des modèles d'ingénierie soit élargi en raison de problèmes de conteneurs.",
      "upvotes": 11,
      "discussionId": "683735b0f42cc8a1d260e69f",
      "ai_summary": "A novel pipeline extracts real-world, interactive software engineering tasks from GitHub to create SWE-rebench, improving the evaluation of reinforcement learning models in SWE.",
      "ai_keywords": [
        "LLM-based agents",
        "reinforcement learning",
        "software engineering tasks",
        "GitHub repositories",
        "SWE-rebench",
        "contamination-free benchmark",
        "SWE-bench Verified"
      ]
    },
    "publishedAt": "2025-05-26T14:01:00.000Z",
    "title": "SWE-rebench: An Automated Pipeline for Task Collection and\n  Decontaminated Evaluation of Software Engineering Agents",
    "summary": "LLM-based agents have shown promising capabilities in a growing range of\nsoftware engineering (SWE) tasks. However, advancing this field faces two\ncritical challenges. First, high-quality training data is scarce, especially\ndata that reflects real-world SWE scenarios, where agents must interact with\ndevelopment environments, execute code and adapt behavior based on the outcomes\nof their actions. Existing datasets are either limited to one-shot code\ngeneration or comprise small, manually curated collections of interactive\ntasks, lacking both scale and diversity. Second, the lack of fresh interactive\nSWE tasks affects evaluation of rapidly improving models, as static benchmarks\nquickly become outdated due to contamination issues. To address these\nlimitations, we introduce a novel, automated, and scalable pipeline to\ncontinuously extract real-world interactive SWE tasks from diverse GitHub\nrepositories. Using this pipeline, we construct SWE-rebench, a public dataset\ncomprising over 21,000 interactive Python-based SWE tasks, suitable for\nreinforcement learning of SWE agents at scale. Additionally, we use continuous\nsupply of fresh tasks collected using SWE-rebench methodology to build a\ncontamination-free benchmark for agentic software engineering. We compare\nresults of various LLMs on this benchmark to results on SWE-bench Verified and\nshow that performance of some language models might be inflated due to\ncontamination issues.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20411.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "644e9ffcd6001776ed77d874",
      "avatarUrl": "/avatars/b93e02caf929679b7e9bc589eed0b689.svg",
      "fullname": "Alexander",
      "name": "djalexj",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.22648",
      "authors": [
        {
          "_id": "6837c03cbbee677da73e6034",
          "user": {
            "_id": "644a4fbc2166258fccc664bc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
            "isPro": false,
            "fullname": "Jialong Wu",
            "user": "callanwu",
            "type": "user"
          },
          "name": "Jialong Wu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-29T02:02:37.069Z",
          "hidden": false
        },
        {
          "_id": "6837c03cbbee677da73e6035",
          "name": "Baixuan Li",
          "hidden": false
        },
        {
          "_id": "6837c03cbbee677da73e6036",
          "name": "Runnan Fang",
          "hidden": false
        },
        {
          "_id": "6837c03cbbee677da73e6037",
          "name": "Wenbiao Yin",
          "hidden": false
        },
        {
          "_id": "6837c03cbbee677da73e6038",
          "name": "Liwen Zhang",
          "hidden": false
        },
        {
          "_id": "6837c03cbbee677da73e6039",
          "name": "Zhengwei Tao",
          "hidden": false
        },
        {
          "_id": "6837c03cbbee677da73e603a",
          "name": "Dingchu Zhang",
          "hidden": false
        },
        {
          "_id": "6837c03cbbee677da73e603b",
          "name": "Zekun Xi",
          "hidden": false
        },
        {
          "_id": "6837c03cbbee677da73e603c",
          "name": "Yong Jiang",
          "hidden": false
        },
        {
          "_id": "6837c03cbbee677da73e603d",
          "name": "Pengjun Xie",
          "hidden": false
        },
        {
          "_id": "6837c03cbbee677da73e603e",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "6837c03cbbee677da73e603f",
          "name": "Jingren Zhou",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/644a4fbc2166258fccc664bc/vhAmZAlJqekE6vLcVjWtO.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/644a4fbc2166258fccc664bc/RXQVwE9PRmBzxCURRiKAU.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/644a4fbc2166258fccc664bc/woFRNvRqdUXKnAHpRH2eM.mp4"
      ],
      "publishedAt": "2025-05-28T17:57:07.000Z",
      "submittedOnDailyAt": "2025-05-29T00:34:30.750Z",
      "title": "Webdir: Adresse pour l'exploration automatique de l'information",
      "submittedOnDailyBy": {
        "_id": "644a4fbc2166258fccc664bc",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
        "isPro": false,
        "fullname": "Jialong Wu",
        "user": "callanwu",
        "type": "user"
      },
      "summary": "Pour résoudre des problèmes complexes réels, il est nécessaire d'explorer des informations concrètes et de faire des inférences en étapes. Le développement de systèmes d'agents comme Deep Research a mis en avant la possibilité d'une recherche en étapes automatisées. Dans cet article, à partir d'une perspective centrée sur les données et le processus d'entraînement, nous proposons une série de paradigmes pour la construction d'agents de recherche d'information sur des dispositifs utilisateurs. Notre approche comprend quatre étapes principales : 1) la construction d'un broker de données, 2) l'échantillonnage de requêtes, 3) l'entraînement initial efficace, et 4) l'apprentissage par renforcement pour l'expansion. Ce cadre a été implémenté dans WebDancer, un agent web basé sur ReAct. Les expérimentations sur les benchmarks de renseignements difficiles comme GAIA et WebWalkerQA ont montré la puissance de WebDancer et ont clairement démontré l'effet de notre paradigme d'entraînement. Une analyse supplémentaire pour l'entraînement de l'agent fournit des idées et des clés d'action et de système qui peuvent aider au développement de modèles d'agents plus efficaces. Le code et le démo sont disponibles sur https://github.com/Alibaba-NLP/WebAgent.",
      "upvotes": 10,
      "discussionId": "6837c03dbbee677da73e607f",
      "githubRepo": "https://github.com/Alibaba-NLP/WebAgent",
      "ai_summary": "The paper proposes a framework for building end-to-end agentic information seeking agents through a combination of data construction, trajectory sampling, supervised fine-tuning, and reinforcement learning, showcasing its effectiveness on information seeking benchmarks.",
      "ai_keywords": [
        "browsing data construction",
        "trajectories sampling",
        "supervised fine-tuning",
        "reinforcement learning",
        "WebDancer",
        "GAIA",
        "WebWalkerQA"
      ]
    },
    "publishedAt": "2025-05-28T13:57:07.000Z",
    "title": "WebDancer: Towards Autonomous Information Seeking Agency",
    "summary": "Addressing intricate real-world problems necessitates in-depth information\nseeking and multi-step reasoning. Recent progress in agentic systems,\nexemplified by Deep Research, underscores the potential for autonomous\nmulti-step research. In this work, we present a cohesive paradigm for building\nend-to-end agentic information seeking agents from a data-centric and\ntraining-stage perspective. Our approach consists of four key stages: (1)\nbrowsing data construction, (2) trajectories sampling, (3) supervised\nfine-tuning for effective cold start, and (4) reinforcement learning for\nenhanced generalisation. We instantiate this framework in a web agent based on\nthe ReAct, WebDancer. Empirical evaluations on the challenging information\nseeking benchmarks, GAIA and WebWalkerQA, demonstrate the strong performance of\nWebDancer, achieving considerable results and highlighting the efficacy of our\ntraining paradigm. Further analysis of agent training provides valuable\ninsights and actionable, systematic pathways for developing more capable\nagentic models. The codes and demo will be released in\nhttps://github.com/Alibaba-NLP/WebAgent.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/644a4fbc2166258fccc664bc/vhAmZAlJqekE6vLcVjWtO.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/644a4fbc2166258fccc664bc/RXQVwE9PRmBzxCURRiKAU.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/644a4fbc2166258fccc664bc/woFRNvRqdUXKnAHpRH2eM.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22648.png",
    "numComments": 4,
    "submittedBy": {
      "_id": "644a4fbc2166258fccc664bc",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
      "fullname": "Jialong Wu",
      "name": "callanwu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.19187",
      "authors": [
        {
          "_id": "6837210455e9bab4e9c302b1",
          "user": {
            "_id": "6002c316698168af3bb9f4a6",
            "avatarUrl": "/avatars/1028860b59951be04e94b126b0985dc0.svg",
            "isPro": false,
            "fullname": "yangxiao",
            "user": "YangXiao-nlp",
            "type": "user"
          },
          "name": "Yang Xiao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-28T19:13:52.090Z",
          "hidden": false
        },
        {
          "_id": "6837210455e9bab4e9c302b2",
          "name": "Jiashuo Wang",
          "hidden": false
        },
        {
          "_id": "6837210455e9bab4e9c302b3",
          "name": "Ruifeng Yuan",
          "hidden": false
        },
        {
          "_id": "6837210455e9bab4e9c302b4",
          "name": "Chunpu Xu",
          "hidden": false
        },
        {
          "_id": "6837210455e9bab4e9c302b5",
          "name": "Kaishuai Xu",
          "hidden": false
        },
        {
          "_id": "6837210455e9bab4e9c302b6",
          "name": "Wenjie Li",
          "hidden": false
        },
        {
          "_id": "6837210455e9bab4e9c302b7",
          "name": "Pengfei Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-25T15:17:57.000Z",
      "submittedOnDailyAt": "2025-05-29T01:05:08.605Z",
      "title": "LIMOPro : Développement de la Théorie Logique pour la Programmation Efficace et Efficace du Horaire de Tests de Temps",
      "submittedOnDailyBy": {
        "_id": "6002c316698168af3bb9f4a6",
        "avatarUrl": "/avatars/1028860b59951be04e94b126b0985dc0.svg",
        "isPro": false,
        "fullname": "yangxiao",
        "user": "YangXiao-nlp",
        "type": "user"
      },
      "summary": "Les modèles de langage grands (LLMs) montrent une capacité logique et computationnelle surprenante lorsqu'ils utilisent des données de \"Chain of Thought\" (CoT) pendant l'entraînement final. Cependant, ces chaînes logiques comprennent beaucoup d'éléments de longues phrases, surtout si on parle d'un processus évolutif de résolution (chemin principal pour résoudre un problème) et d'éléments fonctionnels comme le processus de vérification, la recherche de solutions alternatives et la correction d'erreurs. Bien que le processus évolutif soit important, les éléments fonctionnels augmentent considérablement la charge de calcul lors du processus d'inférence.\n\nNous introduisons un cadre structuré pour évaluer de manière quantitative l'importance de chaque étape logique dans la prédiction des réponses, avec l'objectif d'améliorer l'efficacité de l'entraînement. Ce cadre, connu sous le nom de PIR (Réfinement de l'Importance basé sur la Pérplexité), permet de maintenir les composants évolutifs de la logique tout en éliminant de manière sélective des étapes fonctionnelles de faible importance, générant des données d'entraînement optimisées. Les modèles entraînés avec PIR montrent des résultats meilleurs sur des échelles de test, avec une réduction du consommation de calcul (-3% à -41%) et un amélioration de la précision (+0.9% à +6.6%). Ce effet a été observé dans des tests logiques difficiles comme AIME, AMC et GPQA Diamond. Notre approche démontre une forte généralisation sur des différents tailles de modèle, sources de données et niveaux de token, offrant une solution utile pour l'application pratique des LLMs logiques.",
      "upvotes": 10,
      "discussionId": "6837210555e9bab4e9c302f2",
      "ai_summary": "A framework called PIR refines the importance of reasoning steps in large language models by pruning low-importance functional elements, leading to more concise reasoning chains with improved accuracy and reduced computational demands.",
      "ai_keywords": [
        "large language models",
        "chain-of-thought",
        "large reasoning models",
        "progressive reasoning",
        "functional elements",
        "perplexity-based importance refinement",
        "token usage",
        "reasoning benchmarks",
        "AIME",
        "AMC",
        "GPQA Diamond"
      ]
    },
    "publishedAt": "2025-05-25T11:17:57.000Z",
    "title": "LIMOPro: Reasoning Refinement for Efficient and Effective Test-time\n  Scaling",
    "summary": "Large language models (LLMs) have demonstrated remarkable reasoning\ncapabilities through test-time scaling approaches, particularly when fine-tuned\nwith chain-of-thought (CoT) data distilled from more powerful large reasoning\nmodels (LRMs). However, these reasoning chains often contain verbose elements\nthat mirror human problem-solving, categorized as progressive reasoning (the\nessential solution development path) and functional elements (verification\nprocesses, alternative solution approaches, and error corrections). While\nprogressive reasoning is crucial, the functional elements significantly\nincrease computational demands during test-time inference. We introduce PIR\n(Perplexity-based Importance Refinement), a principled framework that\nquantitatively evaluates the importance of each reasoning step based on its\nimpact on answer prediction confidence. PIR systematically identifies and\nselectively prunes only low-importance functional steps while preserving\nprogressive reasoning components, creating optimized training data that\nmaintains the integrity of the core solution path while reducing verbosity.\nModels fine-tuned on PIR-optimized data exhibit superior test-time scaling\nproperties, generating more concise reasoning chains while achieving improved\naccuracy (+0.9\\% to +6.6\\%) with significantly reduced token usage (-3\\% to\n-41\\%) across challenging reasoning benchmarks (AIME, AMC, and GPQA Diamond).\nOur approach demonstrates strong generalizability across different model sizes,\ndata sources, and token budgets, offering a practical solution for deploying\nreasoning-capable LLMs in scenarios where efficient test-time scaling, response\ntime, and computational efficiency are valuable constraints.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19187.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6002c316698168af3bb9f4a6",
      "avatarUrl": "/avatars/1028860b59951be04e94b126b0985dc0.svg",
      "fullname": "yangxiao",
      "name": "YangXiao-nlp",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.17663",
      "authors": [
        {
          "_id": "6833c6ff97966d18e7b995b0",
          "user": {
            "_id": "6002c316698168af3bb9f4a6",
            "avatarUrl": "/avatars/1028860b59951be04e94b126b0985dc0.svg",
            "isPro": false,
            "fullname": "yangxiao",
            "user": "YangXiao-nlp",
            "type": "user"
          },
          "name": "Yang Xiao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-28T19:14:41.639Z",
          "hidden": false
        },
        {
          "_id": "6833c6ff97966d18e7b995b1",
          "name": "Jiashuo Wang",
          "hidden": false
        },
        {
          "_id": "6833c6ff97966d18e7b995b2",
          "name": "Qiancheng Xu",
          "hidden": false
        },
        {
          "_id": "6833c6ff97966d18e7b995b3",
          "name": "Changhe Song",
          "hidden": false
        },
        {
          "_id": "6833c6ff97966d18e7b995b4",
          "name": "Chunpu Xu",
          "hidden": false
        },
        {
          "_id": "6833c6ff97966d18e7b995b5",
          "name": "Yi Cheng",
          "hidden": false
        },
        {
          "_id": "6833c6ff97966d18e7b995b6",
          "name": "Wenjie Li",
          "hidden": false
        },
        {
          "_id": "6833c6ff97966d18e7b995b7",
          "name": "Pengfei Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T09:27:40.000Z",
      "submittedOnDailyAt": "2025-05-29T01:05:57.079Z",
      "title": "Théorie du Cœur Dynamique : Évaluation de l'Adaptation des Modèles de Langue de Haut Niveau à l'Évolution du Temps des États Humains",
      "submittedOnDailyBy": {
        "_id": "6002c316698168af3bb9f4a6",
        "avatarUrl": "/avatars/1028860b59951be04e94b126b0985dc0.svg",
        "isPro": false,
        "fullname": "yangxiao",
        "user": "YangXiao-nlp",
        "type": "user"
      },
      "summary": "Avec l'augmentation de la participation des LLM dans l'échange interactif entre les êtres humains et l'IA, l'évaluation de la capacité de la Théorie de la Mémoire (ToM) a acquis une importance. En particulier, la capacité de suivre l'évolution de l'état de la mémoire est cruciale. Actuellement, les normes se concentrent sur l'évaluation des compétences de base de la ToM, mais principalement sur des états mentaux statiques, ignorant la réalité de l'interaction sociale dans le temps. Un nouveau standard appelé DynToM est proposé. Ce standard a été conçu spécifiquement pour évaluer la capacité des LLM à suivre l'évolution de l'état de la mémoire et à le relier dans des scénarios temporels. À travers un cadre de travail systématique de 4 étapes, 1 100 contextes sociaux ont été générés, ceux-ci ont été inclus dans 5 500 scénarios et 78 100 questions. Ceux-ci ont été validés pour réalisme et qualité. L'évaluation détaillée des 10 meilleurs LLMs a révélé que le rendement moyen était de 44,7% inférieur à celui des humains, et que la capacité de suivre et d'expliquer l'état de la mémoire a diminué notablement. Ces différences de rendement révèlent les limitations fondamentales des LLMs dans la modélisation des caractéristiques dynamiques de l'état de la mémoire humaine.",
      "upvotes": 10,
      "discussionId": "6833c70097966d18e7b99616",
      "ai_summary": "The DynToM benchmark evaluates LLMs' ability to track and understand the temporal progression of mental states, revealing significant gaps compared to human performance.",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "Theory of Mind (ToM)",
        "DynToM",
        "social contexts",
        "mental states",
        "temporal progression",
        "evaluation framework",
        "state-of-the-art LLMs",
        "performance gap",
        "dynamic mental states"
      ]
    },
    "publishedAt": "2025-05-23T05:27:40.000Z",
    "title": "Towards Dynamic Theory of Mind: Evaluating LLM Adaptation to Temporal\n  Evolution of Human States",
    "summary": "As Large Language Models (LLMs) increasingly participate in human-AI\ninteractions, evaluating their Theory of Mind (ToM) capabilities - particularly\ntheir ability to track dynamic mental states - becomes crucial. While existing\nbenchmarks assess basic ToM abilities, they predominantly focus on static\nsnapshots of mental states, overlooking the temporal evolution that\ncharacterizes real-world social interactions. We present DynToM, a\nnovel benchmark specifically designed to evaluate LLMs' ability to understand\nand track the temporal progression of mental states across interconnected\nscenarios. Through a systematic four-step framework, we generate 1,100 social\ncontexts encompassing 5,500 scenarios and 78,100 questions, each validated for\nrealism and quality. Our comprehensive evaluation of ten state-of-the-art LLMs\nreveals that their average performance underperforms humans by 44.7\\%, with\nperformance degrading significantly when tracking and reasoning about the shift\nof mental states. This performance gap highlights fundamental limitations in\ncurrent LLMs' ability to model the dynamic nature of human mental states.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17663.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6002c316698168af3bb9f4a6",
      "avatarUrl": "/avatars/1028860b59951be04e94b126b0985dc0.svg",
      "fullname": "yangxiao",
      "name": "YangXiao-nlp",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.22019",
      "authors": [
        {
          "_id": "6837ed297d00cf0a04677bc1",
          "user": {
            "_id": "657429d833e5a4bf5b278615",
            "avatarUrl": "/avatars/ed7e28c1b9a7bed1cad864c992cdcc69.svg",
            "isPro": false,
            "fullname": "QiuchenWang",
            "user": "autumncc",
            "type": "user"
          },
          "name": "Qiuchen Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T09:40:19.039Z",
          "hidden": false
        },
        {
          "_id": "6837ed297d00cf0a04677bc2",
          "name": "Ruixue Ding",
          "hidden": false
        },
        {
          "_id": "6837ed297d00cf0a04677bc3",
          "user": {
            "_id": "665d652e0f35c005de892108",
            "avatarUrl": "/avatars/240bebdc7fdc6d50719c65de0e3cf1cd.svg",
            "isPro": false,
            "fullname": "Yu Zeng",
            "user": "YuZeng260",
            "type": "user"
          },
          "name": "Yu Zeng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:41:42.906Z",
          "hidden": false
        },
        {
          "_id": "6837ed297d00cf0a04677bc4",
          "name": "Zehui Chen",
          "hidden": false
        },
        {
          "_id": "6837ed297d00cf0a04677bc5",
          "user": {
            "_id": "64b02ec0e5000ae8a572ced5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b02ec0e5000ae8a572ced5/6ifLntBU2ICQK7SW8WxKU.png",
            "isPro": false,
            "fullname": "Lin Chen",
            "user": "Lin-Chen",
            "type": "user"
          },
          "name": "Lin Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:41:44.983Z",
          "hidden": false
        },
        {
          "_id": "6837ed297d00cf0a04677bc6",
          "name": "Shihang Wang",
          "hidden": false
        },
        {
          "_id": "6837ed297d00cf0a04677bc7",
          "name": "Pengjun Xie",
          "hidden": false
        },
        {
          "_id": "6837ed297d00cf0a04677bc8",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "6837ed297d00cf0a04677bc9",
          "name": "Feng Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T06:30:51.000Z",
      "submittedOnDailyAt": "2025-05-29T03:46:05.539Z",
      "title": "VRAG-RL : Méthode d'amélioration de la compréhension de l'information visuelle par un RAG basé sur la vision, renforcée par un apprentissage par renforcement via une inférence itérative",
      "submittedOnDailyBy": {
        "_id": "64b02ec0e5000ae8a572ced5",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b02ec0e5000ae8a572ced5/6ifLntBU2ICQK7SW8WxKU.png",
        "isPro": false,
        "fullname": "Lin Chen",
        "user": "Lin-Chen",
        "type": "user"
      },
      "summary": "Une recherche efficace et logique d'informations larges associées à des images, ainsi que sa compréhension, constitue un défi pour les méthodes RAG. Les méthodes traditionnelles basées sur le texte ne peuvent pas traiter des informations liées aux images. D'autre part, les approches actuelles basées sur les images pour RAG sont limitées par des lignes de production fixes et présentent des difficultés pour exécuter une logique efficacement en raison de la faiblesse des capacités de base des modèles. Nous avons démontré que la RL peut avoir un impact bénéfique sur la logique des modèles, pourquoi nous présentons un nouveau cadre de RL VRAG-RL pour réaliser des logiques complexes. Ce cadre permet que les VLMs interagissent avec un moteur de recherche, en utilisant des tokens de reconnaissance d'images pour auto-échantillonner une logique une ou plusieurs fois, et se perfectionnent de manière continue en fonction de ces échantillons. Notre approche met en évidence les principales limites de la RL dans le domaine de RAG : (i) les approches RAG multi-modèle traditionnelles incluent seulement des images dans le contexte, ce qui résulte en une assignation insuffisante de tokens de logique et ne dépasse pas le reconnaissance propre des images ; (ii) lorsque un modèle interagit avec un moteur de recherche, il ne peut pas exprimer clairement ses demandes, ce qui empêche de rechercher des informations pertinentes, ce qui rend impossible d'atteindre le meilleur rendement. Pour résoudre ces défis, nous définissons un espace d'action en appliquant un carton de tuiles aux entrées associées aux images et incluons des actions comme le coupage et l'échelle, permettant au modèle de récupérer des informations depuis une perspective visuelle de la bande centrale jusqu'à la bande de pointe. De plus, pour réduire la distance entre les questions des utilisateurs et le modèle de recherche, nous utilisons une compensation simple et efficace, en intégrant la récréation des requêtes et le rendement de la recherche avec une compensation basée sur le modèle. VRAG-RL est conçu pour optimiser des tâches de RAG avec des stratégies de RL spécifiques et a pour objectif que le modèle s'adapte aux applications en réalité. Le code est disponible sur https://github.com/Alibaba-NLP/VRAG.",
      "upvotes": 8,
      "discussionId": "6837ed297d00cf0a04677bf5",
      "githubRepo": "https://github.com/Alibaba-NLP/VRAG",
      "ai_summary": "VRAG-RL, a reinforcement learning framework, enhances reasoning and visual information handling in RAG methods by integrating visual perception tokens and employing specialized action spaces and rewards.",
      "ai_keywords": [
        "reinforcement learning",
        "VRAG-RL",
        "VLMs",
        "search engines",
        "visually rich information",
        "reasoning trajectories",
        "visual perception tokens",
        "action space",
        "query rewriting",
        "retrieval performance",
        "model-based reward"
      ]
    },
    "publishedAt": "2025-05-28T02:30:51.000Z",
    "title": "VRAG-RL: Empower Vision-Perception-Based RAG for Visually Rich\n  Information Understanding via Iterative Reasoning with Reinforcement Learning",
    "summary": "Effectively retrieving, reasoning and understanding visually rich information\nremains a challenge for RAG methods. Traditional text-based methods cannot\nhandle visual-related information. On the other hand, current vision-based RAG\napproaches are often limited by fixed pipelines and frequently struggle to\nreason effectively due to the insufficient activation of the fundamental\ncapabilities of models. As RL has been proven to be beneficial for model\nreasoning, we introduce VRAG-RL, a novel RL framework tailored for complex\nreasoning across visually rich information. With this framework, VLMs interact\nwith search engines, autonomously sampling single-turn or multi-turn reasoning\ntrajectories with the help of visual perception tokens and undergoing continual\noptimization based on these samples. Our approach highlights key limitations of\nRL in RAG domains: (i) Prior Multi-modal RAG approaches tend to merely\nincorporate images into the context, leading to insufficient reasoning token\nallocation and neglecting visual-specific perception; and (ii) When models\ninteract with search engines, their queries often fail to retrieve relevant\ninformation due to the inability to articulate requirements, thereby leading to\nsuboptimal performance. To address these challenges, we define an action space\ntailored for visually rich inputs, with actions including cropping and scaling,\nallowing the model to gather information from a coarse-to-fine perspective.\nFurthermore, to bridge the gap between users' original inquiries and the\nretriever, we employ a simple yet effective reward that integrates query\nrewriting and retrieval performance with a model-based reward. Our VRAG-RL\noptimizes VLMs for RAG tasks using specially designed RL strategies, aligning\nthe model with real-world applications. The code is available at\nhttps://github.com/Alibaba-NLP/VRAG{https://github.com/Alibaba-NLP/VRAG}.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22019.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64b02ec0e5000ae8a572ced5",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b02ec0e5000ae8a572ced5/6ifLntBU2ICQK7SW8WxKU.png",
      "fullname": "Lin Chen",
      "name": "Lin-Chen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 89
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.22232",
      "authors": [
        {
          "_id": "683815574d9866c160e88670",
          "name": "Mehdi Ali",
          "hidden": false
        },
        {
          "_id": "683815574d9866c160e88671",
          "user": {
            "_id": "62fa1d95e8c9c532aa75331c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62fa1d95e8c9c532aa75331c/WFfk_n8gOj845pSkfdazA.jpeg",
            "isPro": false,
            "fullname": "Manuel Brack",
            "user": "mbrack",
            "type": "user"
          },
          "name": "Manuel Brack",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T09:40:14.826Z",
          "hidden": false
        },
        {
          "_id": "683815574d9866c160e88672",
          "name": "Max Lübbering",
          "hidden": false
        },
        {
          "_id": "683815574d9866c160e88673",
          "name": "Elias Wendt",
          "hidden": false
        },
        {
          "_id": "683815574d9866c160e88674",
          "name": "Abbas Goher Khan",
          "hidden": false
        },
        {
          "_id": "683815574d9866c160e88675",
          "name": "Richard Rutmann",
          "hidden": false
        },
        {
          "_id": "683815574d9866c160e88676",
          "name": "Alex Jude",
          "hidden": false
        },
        {
          "_id": "683815574d9866c160e88677",
          "name": "Maurice Kraus",
          "hidden": false
        },
        {
          "_id": "683815574d9866c160e88678",
          "name": "Alexander Arno Weber",
          "hidden": false
        },
        {
          "_id": "683815574d9866c160e88679",
          "name": "Felix Stollenwerk",
          "hidden": false
        },
        {
          "_id": "683815574d9866c160e8867a",
          "name": "David Kaczér",
          "hidden": false
        },
        {
          "_id": "683815574d9866c160e8867b",
          "name": "Florian Mai",
          "hidden": false
        },
        {
          "_id": "683815574d9866c160e8867c",
          "name": "Lucie Flek",
          "hidden": false
        },
        {
          "_id": "683815574d9866c160e8867d",
          "name": "Rafet Sifa",
          "hidden": false
        },
        {
          "_id": "683815574d9866c160e8867e",
          "name": "Nicolas Flores-Herr",
          "hidden": false
        },
        {
          "_id": "683815574d9866c160e8867f",
          "name": "Joachim Köhler",
          "hidden": false
        },
        {
          "_id": "683815574d9866c160e88680",
          "name": "Patrick Schramowski",
          "hidden": false
        },
        {
          "_id": "683815574d9866c160e88681",
          "name": "Michael Fromm",
          "hidden": false
        },
        {
          "_id": "683815574d9866c160e88682",
          "name": "Kristian Kersting",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T11:06:54.000Z",
      "submittedOnDailyAt": "2025-05-29T07:38:07.888Z",
      "title": "Qualité du langage : accès multilingue pour l'entraînement\nFiltrage des données avec des modèles de langage",
      "submittedOnDailyBy": {
        "_id": "62fa1d95e8c9c532aa75331c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62fa1d95e8c9c532aa75331c/WFfk_n8gOj845pSkfdazA.jpeg",
        "isPro": false,
        "fullname": "Manuel Brack",
        "user": "mbrack",
        "type": "user"
      },
      "summary": "Les données d'entraînement de haute qualité pour plusieurs langues sont essentielles pour l'entraînement préalable efficace de modèles de langage grands (LLMs). Cependant, l'utilisation de jeux de données de langues ouvertes appropriées est limitée. Les jeux de données leaders actuels se basent principalement sur des méthodes de filtrage heuristique, ce qui limite leur capacité de transformation entre langues et leur scalabilité. Dans ce contexte, nous présentons une méthodologie systématique appelée JQL. Cette méthodologie se concentre sur l'efficacité et la diversité des données multilingues, avec l'objectif de réduire significativement la charge de calcul. JQL applique des attributs à des modèles légers de tagging basés sur les capacités de notes des LLMs. Ces modèles montrent un comportement fort en langues non vues et dans différents systèmes de caractères lors de l'entraînement, démontrant une amélioration du rendement multilingue et interlingue. Après une évaluation expérimentale avec 35 langues, JQL dépasse notablement les méthodes de filtrage heuristique comme Fineweb2. En particulier, JQL améliore la qualité de l'entraînement des modèles et augmente la densité des données. Notre travail offre une outil pratique et des ressources précieuses, avec l'objectif de élever les normes des données multilingues.",
      "upvotes": 7,
      "discussionId": "683815594d9866c160e88708",
      "projectPage": "https://huggingface.co/spaces/Jackal-AI/JQL",
      "githubRepo": "https://github.com/JQL-AI/JQL-Annotation-Pipeline",
      "ai_summary": "JQL systematically curates high-quality multilingual training data using pretrained multilingual embeddings, outperforming heuristic methods and improving downstream model training across diverse languages.",
      "ai_keywords": [
        "pretraining",
        "large language models",
        "multilingual datasets",
        "heuristic filtering methods",
        "JQL",
        "lightweight annotators",
        "multilingual embeddings",
        "cross-lingual transferability",
        "annotation pipeline",
        "data retention rates",
        "multilingual data curation"
      ]
    },
    "publishedAt": "2025-05-28T07:06:54.000Z",
    "title": "Judging Quality Across Languages: A Multilingual Approach to Pretraining\n  Data Filtering with Language Models",
    "summary": "High-quality multilingual training data is essential for effectively\npretraining large language models (LLMs). Yet, the availability of suitable\nopen-source multilingual datasets remains limited. Existing state-of-the-art\ndatasets mostly rely on heuristic filtering methods, restricting both their\ncross-lingual transferability and scalability. Here, we introduce JQL, a\nsystematic approach that efficiently curates diverse and high-quality\nmultilingual data at scale while significantly reducing computational demands.\nJQL distills LLMs' annotation capabilities into lightweight annotators based on\npretrained multilingual embeddings. These models exhibit robust multilingual\nand cross-lingual performance, even for languages and scripts unseen during\ntraining. Evaluated empirically across 35 languages, the resulting annotation\npipeline substantially outperforms current heuristic filtering methods like\nFineweb2. JQL notably enhances downstream model training quality and increases\ndata retention rates. Our research provides practical insights and valuable\nresources for multilingual data curation, raising the standards of multilingual\ndataset development.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22232.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62fa1d95e8c9c532aa75331c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62fa1d95e8c9c532aa75331c/WFfk_n8gOj845pSkfdazA.jpeg",
      "fullname": "Manuel Brack",
      "name": "mbrack",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 15
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.22613",
      "authors": [
        {
          "_id": "6837d79d4d9866c160d8f43b",
          "name": "Yuchi Wang",
          "hidden": false
        },
        {
          "_id": "6837d79d4d9866c160d8f43c",
          "name": "Yishuo Cai",
          "hidden": false
        },
        {
          "_id": "6837d79d4d9866c160d8f43d",
          "name": "Shuhuai Ren",
          "hidden": false
        },
        {
          "_id": "6837d79d4d9866c160d8f43e",
          "name": "Sihan Yang",
          "hidden": false
        },
        {
          "_id": "6837d79d4d9866c160d8f43f",
          "name": "Linli Yao",
          "hidden": false
        },
        {
          "_id": "6837d79d4d9866c160d8f440",
          "name": "Yuanxin Liu",
          "hidden": false
        },
        {
          "_id": "6837d79d4d9866c160d8f441",
          "name": "Yuanxing Zhang",
          "hidden": false
        },
        {
          "_id": "6837d79d4d9866c160d8f442",
          "name": "Pengfei Wan",
          "hidden": false
        },
        {
          "_id": "6837d79d4d9866c160d8f443",
          "name": "Xu Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T17:29:34.000Z",
      "submittedOnDailyAt": "2025-05-29T02:20:00.209Z",
      "title": "Rico : Pour améliorer la précision et la complétude de la récolte d'images, on utilise la reconstruction visuelle.",
      "submittedOnDailyBy": {
        "_id": "622842e296588dd1a2594746",
        "avatarUrl": "/avatars/b96d0b49e4cff25d83fefc67b7cd1076.svg",
        "isPro": false,
        "fullname": "wangyuchi",
        "user": "YuchiWang",
        "type": "user"
      },
      "summary": "Le ricapiting d'images est largement utilisé pour améliorer la qualité dans divers modèles et tâches. Actuellement, les méthodes de ricapiting dépendent généralement de modèles de langage multimodal (MLLM) robustes pour améliorer le contexte, mais sont insuffisantes dans les détails minimaux et détaillés, ce qui provoque des incertitudes et des incomplétudes. Pour résoudre ces limitations, nous proposons un nouveau cadre de travail pour améliorer le ricapiting basé sur la reconstruction d'images. Spécifiquement, nous utilisons des modèles de contexte pour reconstruire l'image du contexte, et nous reconnaissons les différences entre l'image originale et la reconstruite via un MLLM pour améliorer le ricapiting. Ce processus se fait de manière itérative et aide à créer des descriptions fiables et détaillées. Pour réduire les coûts computationnels supplémentaires, nous introduisons un ricapoche-Flash qui apprend à générer des captions comme un ricapoche, en utilisant DPO. Les expériences étendues montrent que notre approche améliore significativement la précision et la détaillée du ricapiting, améliorant d'au moins 10% dans les modèles de référence sur CapsBench et CompreCap. Le code est disponible sur la URL suivante.\nhttps://github.com/wangyuchi369/RICO",
      "upvotes": 5,
      "discussionId": "6837d79e4d9866c160d8f471",
      "githubRepo": "https://github.com/wangyuchi369/RICO",
      "ai_summary": "A novel iterative framework, RICO, improves image caption accuracy by using visual reconstruction and a text-to-image model to refine discrepancies, while RICO-Flash enhances efficiency using DPO.",
      "ai_keywords": [
        "multimodal large language models",
        "text-to-image model",
        "DPO",
        "CapsBench",
        "CompreCap"
      ]
    },
    "publishedAt": "2025-05-28T13:29:34.000Z",
    "title": "RICO: Improving Accuracy and Completeness in Image Recaptioning via\n  Visual Reconstruction",
    "summary": "Image recaptioning is widely used to generate training datasets with enhanced\nquality for various multimodal tasks. Existing recaptioning methods typically\nrely on powerful multimodal large language models (MLLMs) to enhance textual\ndescriptions, but often suffer from inaccuracies due to hallucinations and\nincompleteness caused by missing fine-grained details. To address these\nlimitations, we propose RICO, a novel framework that refines captions through\nvisual reconstruction. Specifically, we leverage a text-to-image model to\nreconstruct a caption into a reference image, and prompt an MLLM to identify\ndiscrepancies between the original and reconstructed images to refine the\ncaption. This process is performed iteratively, further progressively promoting\nthe generation of more faithful and comprehensive descriptions. To mitigate the\nadditional computational cost induced by the iterative process, we introduce\nRICO-Flash, which learns to generate captions like RICO using DPO. Extensive\nexperiments demonstrate that our approach significantly improves caption\naccuracy and completeness, outperforms most baselines by approximately 10% on\nboth CapsBench and CompreCap. Code released at\nhttps://github.com/wangyuchi369/RICO.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22613.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "622842e296588dd1a2594746",
      "avatarUrl": "/avatars/b96d0b49e4cff25d83fefc67b7cd1076.svg",
      "fullname": "wangyuchi",
      "name": "YuchiWang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.22525",
      "authors": [
        {
          "_id": "6837da438f680552f7b86b28",
          "user": {
            "_id": "64bb5f9d8e051085bace4d1e",
            "avatarUrl": "/avatars/15ccbb78c6131dfe46b7a9d8e7d1a31f.svg",
            "isPro": true,
            "fullname": "Ethan Chern",
            "user": "ethanchern",
            "type": "user"
          },
          "name": "Ethan Chern",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:42:05.613Z",
          "hidden": false
        },
        {
          "_id": "6837da438f680552f7b86b29",
          "name": "Zhulin Hu",
          "hidden": false
        },
        {
          "_id": "6837da438f680552f7b86b2a",
          "name": "Steffi Chern",
          "hidden": false
        },
        {
          "_id": "6837da438f680552f7b86b2b",
          "name": "Siqi Kou",
          "hidden": false
        },
        {
          "_id": "6837da438f680552f7b86b2c",
          "name": "Jiadi Su",
          "hidden": false
        },
        {
          "_id": "6837da438f680552f7b86b2d",
          "name": "Yan Ma",
          "hidden": false
        },
        {
          "_id": "6837da438f680552f7b86b2e",
          "name": "Zhijie Deng",
          "hidden": false
        },
        {
          "_id": "6837da438f680552f7b86b2f",
          "name": "Pengfei Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T16:12:45.000Z",
      "submittedOnDailyAt": "2025-05-29T02:24:51.876Z",
      "title": "Création d'images pour penser",
      "submittedOnDailyBy": {
        "_id": "64bb5f9d8e051085bace4d1e",
        "avatarUrl": "/avatars/15ccbb78c6131dfe46b7a9d8e7d1a31f.svg",
        "isPro": true,
        "fullname": "Ethan Chern",
        "user": "ethanchern",
        "type": "user"
      },
      "summary": "Nous présentons un nouveau paradigme appelé \"Penser avec des images générées\". Ce paradigme implique un changement fondamental dans l'interaction entre la logique visuelle et les grands modèles multimodalités (LMMs). Cela se réalise lorsque les modèles combinent le texte et les modalités visuelles de manière naturelle, générant de manière spontanée les processus visuels intermédiaires. Actuellement, la logique visuelle des LMMs est limitée à traiter des images fixes fournies par l'utilisateur ou à suivre une chaîne de raisonnement basée sur le texte (CoT). \"Penser avec des images générées\" ouvre une nouvelle dimension de capacité cognitive pour les modèles, permettant qu'ils évaluent leurs propres hypothèses visuelles et améliorent leur logique principale. Nous avons testé l'efficacité de cette approche par deux mécanismes complémentaires : (1) la génération visuelle basée sur des sous-objectifs visuels intermédiaires, où les composants managables sont détaillés et générés pas à pas pour traiter des tâches visuelles complexes, et (2) la génération visuelle basée sur l'auto-évaluation, où des hypothèses visuelles initiales sont générées, analysées par une logique textuelle et améliorées selon l'évaluation propre. Les résultats des benchmarks visuels montrent une amélioration significative par rapport aux méthodes de référence, atteignant un accroissement relatif de 50% (de 38% à 57%) dans le traitement de scénarios multi-objets complexes. Notre approche permet aux modèles d'IA de participer à l'imagination visuelle humaine, connue pour sa créativité, analyse et stratégie, ainsi qu'à l'amélioration itérative, comme dans l'exploration de nouvelles structures protéiques, le design spatial d'architectes, la reconstruction de lieux criminels et l'imagination de jeux stratégiques de basketball. Disponible à l'adresse https://github.com/GAIR-NLP/thinking-with-generated-images.",
      "upvotes": 5,
      "discussionId": "6837da468f680552f7b86bb2",
      "githubRepo": "https://github.com/GAIR-NLP/thinking-with-generated-images",
      "ai_summary": "Thinking with Generated Images allows large multimodal models to generate and critique intermediate visual steps, enhancing visual reasoning capabilities and achieving significant improvements in complex scenarios.",
      "ai_keywords": [
        "LMMs",
        "visual reasoning",
        "chain-of-thought",
        "vision generation",
        "intermediate visual subgoals",
        "self-critique",
        "multis-object scenarios",
        "biochemists",
        "architects",
        "forensic analysts",
        "basketball players"
      ]
    },
    "publishedAt": "2025-05-28T12:12:45.000Z",
    "title": "Thinking with Generated Images",
    "summary": "We present Thinking with Generated Images, a novel paradigm that\nfundamentally transforms how large multimodal models (LMMs) engage with visual\nreasoning by enabling them to natively think across text and vision modalities\nthrough spontaneous generation of intermediate visual thinking steps. Current\nvisual reasoning with LMMs is constrained to either processing fixed\nuser-provided images or reasoning solely through text-based chain-of-thought\n(CoT). Thinking with Generated Images unlocks a new dimension of cognitive\ncapability where models can actively construct intermediate visual thoughts,\ncritique their own visual hypotheses, and refine them as integral components of\ntheir reasoning process. We demonstrate the effectiveness of our approach\nthrough two complementary mechanisms: (1) vision generation with intermediate\nvisual subgoals, where models decompose complex visual tasks into manageable\ncomponents that are generated and integrated progressively, and (2) vision\ngeneration with self-critique, where models generate an initial visual\nhypothesis, analyze its shortcomings through textual reasoning, and produce\nrefined outputs based on their own critiques. Our experiments on vision\ngeneration benchmarks show substantial improvements over baseline approaches,\nwith our models achieving up to 50% (from 38% to 57%) relative improvement in\nhandling complex multi-object scenarios. From biochemists exploring novel\nprotein structures, and architects iterating on spatial designs, to forensic\nanalysts reconstructing crime scenes, and basketball players envisioning\nstrategic plays, our approach enables AI models to engage in the kind of visual\nimagination and iterative refinement that characterizes human creative,\nanalytical, and strategic thinking. We release our open-source suite at\nhttps://github.com/GAIR-NLP/thinking-with-generated-images.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22525.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64bb5f9d8e051085bace4d1e",
      "avatarUrl": "/avatars/15ccbb78c6131dfe46b7a9d8e7d1a31f.svg",
      "fullname": "Ethan Chern",
      "name": "ethanchern",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.22523",
      "authors": [
        {
          "_id": "6837c1cb80fc90ca2d9e8153",
          "name": "Junwen Chen",
          "hidden": false
        },
        {
          "_id": "6837c1cb80fc90ca2d9e8154",
          "name": "Heyang Jiang",
          "hidden": false
        },
        {
          "_id": "6837c1cb80fc90ca2d9e8155",
          "name": "Yanbin Wang",
          "hidden": false
        },
        {
          "_id": "6837c1cb80fc90ca2d9e8156",
          "name": "Keming Wu",
          "hidden": false
        },
        {
          "_id": "6837c1cb80fc90ca2d9e8157",
          "name": "Ji Li",
          "hidden": false
        },
        {
          "_id": "6837c1cb80fc90ca2d9e8158",
          "name": "Chao Zhang",
          "hidden": false
        },
        {
          "_id": "6837c1cb80fc90ca2d9e8159",
          "name": "Keiji Yanai",
          "hidden": false
        },
        {
          "_id": "6837c1cb80fc90ca2d9e815a",
          "name": "Dong Chen",
          "hidden": false
        },
        {
          "_id": "6837c1cb80fc90ca2d9e815b",
          "name": "Yuhui Yuan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T16:09:33.000Z",
      "submittedOnDailyAt": "2025-05-29T00:40:52.849Z",
      "title": "PrismLayers : PrismLayers est un modèle génératif basé sur des données d'images transparentes de haute qualité ouvertes.",
      "submittedOnDailyBy": {
        "_id": "631f108bb45367a05fe74260",
        "avatarUrl": "/avatars/c20da6800b643728062712d9a2771648.svg",
        "isPro": false,
        "fullname": "Researcher",
        "user": "YuanYuhui",
        "type": "user"
      },
      "summary": "Générer des images de haute qualité et multi-transparentes à partir de textes de hinto permet un nouveau niveau de contrôle créatif, permettant aux utilisateurs de modifier chaque couche comme si elles étaient des sorties d'un modèle de langage. Cependant, l'absence de grands ensembles de données de haute qualité multi-transparentes a retardé le développement des modèles de génération multi-couche par rapport aux modèles traditionnels de texte à image. Dans cet article, nous avons abordé ce défi de la manière suivante : (i) nous publions le premier ensemble de données PrismLayers (PrismLayersPro) ouvert et de haute précision, qui comprend 200K (20K) images multi-transparentes et alpha mattes précis; (ii) nous introduisons une pipeline de synthèse sans entraînement qui utilise des modèles de diffusion disponibles pour générer ces images selon la nécessité; (iii) nous proposons un modèle de génération multi-couche ART+ puissant, qui a un style de conception comparable aux modèles de génération de texte à image modernes. Nos contributions technologiques clés sont : LayerFLUX est adepte de générer des images de haute qualité et des alpha mattes précis, tandis que MultiLayerFLUX combine les résultats de LayerFLUX basés sur des étiquetages sémantiques pour créer des images complètes. Pour garantir une meilleure qualité, nous appliquons un processus strict de filtrage pour éliminer les artefacts et les inconsistences sémantiques, puis nous sélectionnons manuellement. Les modèles ART les plus avancés sont ajustés aux données synthétisées de PrismLayersPro pour obtenir ART+, qui, dans une étude de tête à tête avec 60% de participants, dépasse l'original ART et ressemble en qualité visuelle aux images générées par FLUX.1-[dev]. Nous espérons que notre travail contribue à la construction d'un solide dataset d'images multi-transparentes et encourage la recherche et les applications nécessaires pour des images multi-couche précises, éditables et visuellement attrayantes.",
      "upvotes": 4,
      "discussionId": "6837c1d180fc90ca2d9e82bc",
      "ai_summary": "The work introduces a dataset and model for generating high-quality, multi-layer transparent images using diffusion models and a novel synthesis pipeline.",
      "ai_keywords": [
        "PrismLayers",
        "diffusion models",
        "LayerFLUX",
        "MultiLayerFLUX",
        "alpha mattes",
        "semantic layout",
        "user study",
        "ART model",
        "FLUX.1-[dev]"
      ]
    },
    "publishedAt": "2025-05-28T12:09:33.000Z",
    "title": "PrismLayers: Open Data for High-Quality Multi-Layer Transparent Image\n  Generative Models",
    "summary": "Generating high-quality, multi-layer transparent images from text prompts can\nunlock a new level of creative control, allowing users to edit each layer as\neffortlessly as editing text outputs from LLMs. However, the development of\nmulti-layer generative models lags behind that of conventional text-to-image\nmodels due to the absence of a large, high-quality corpus of multi-layer\ntransparent data. In this paper, we address this fundamental challenge by: (i)\nreleasing the first open, ultra-high-fidelity PrismLayers (PrismLayersPro)\ndataset of 200K (20K) multilayer transparent images with accurate alpha mattes,\n(ii) introducing a trainingfree synthesis pipeline that generates such data on\ndemand using off-the-shelf diffusion models, and (iii) delivering a strong,\nopen-source multi-layer generation model, ART+, which matches the aesthetics of\nmodern text-to-image generation models. The key technical contributions\ninclude: LayerFLUX, which excels at generating high-quality single transparent\nlayers with accurate alpha mattes, and MultiLayerFLUX, which composes multiple\nLayerFLUX outputs into complete images, guided by human-annotated semantic\nlayout. To ensure higher quality, we apply a rigorous filtering stage to remove\nartifacts and semantic mismatches, followed by human selection. Fine-tuning the\nstate-of-the-art ART model on our synthetic PrismLayersPro yields ART+, which\noutperforms the original ART in 60% of head-to-head user study comparisons and\neven matches the visual quality of images generated by the FLUX.1-[dev] model.\nWe anticipate that our work will establish a solid dataset foundation for the\nmulti-layer transparent image generation task, enabling research and\napplications that require precise, editable, and visually compelling layered\nimagery.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22523.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "631f108bb45367a05fe74260",
      "avatarUrl": "/avatars/c20da6800b643728062712d9a2771648.svg",
      "fullname": "Researcher",
      "name": "YuanYuhui",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.22338",
      "authors": [
        {
          "_id": "6837c79576eac3fa930de19b",
          "name": "Hanyang Wang",
          "hidden": false
        },
        {
          "_id": "6837c79576eac3fa930de19c",
          "name": "Lu Wang",
          "hidden": false
        },
        {
          "_id": "6837c79576eac3fa930de19d",
          "name": "Chaoyun Zhang",
          "hidden": false
        },
        {
          "_id": "6837c79576eac3fa930de19e",
          "name": "Tianjun Mao",
          "hidden": false
        },
        {
          "_id": "6837c79576eac3fa930de19f",
          "name": "Si Qin",
          "hidden": false
        },
        {
          "_id": "6837c79576eac3fa930de1a0",
          "name": "Qingwei Lin",
          "hidden": false
        },
        {
          "_id": "6837c79576eac3fa930de1a1",
          "name": "Saravan Rajmohan",
          "hidden": false
        },
        {
          "_id": "6837c79576eac3fa930de1a2",
          "name": "Dongmei Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T13:23:49.000Z",
      "submittedOnDailyAt": "2025-05-29T01:04:31.053Z",
      "title": "Text2Grad: Apprentissage par Renforcement avec Feedback Naturel",
      "submittedOnDailyBy": {
        "_id": "654dbac9938fbf1e696be8aa",
        "avatarUrl": "/avatars/b3c4035c48169c1bfb04a439fce3499f.svg",
        "isPro": false,
        "fullname": "Chaoyun Zhang",
        "user": "vyokky",
        "type": "user"
      },
      "summary": "La RLHF traditionnelle optimise un modèle de langue en utilisant une récompense à une échelle simple et en masquant les raisons concrètes de succès ou de défaillance, ainsi que l'apprentissage se fait de manière lente. Récemment, des méthodes ont été étudiées pour ajouter des évaluations bibliographiques basées sur des prompts et des réflexions à l'apprentissage par renforcement (RL), améliorant ainsi sa compréhensibilité sans modifier les paramètres du modèle. Dans notre travail, nous proposons un paradigme RL où la rétroaction linguistique est transformée en une pente large pour améliorer directement les parties erronées de la politique du modèle. Ainsi, la rétroaction devient précise et le nœud global disparaît. Text2Grad est composé de trois composants : (1) un système de rétroaction de commentaires de haute qualité, (2) un modèle de récompense détaillé, et (3) un optimiseur de politique à une échelle large. En résumé, dans des domaines comme la génération de code, la résolution de problèmes, Text2Grad fournit une précision accrue dans les métriques de tâche et une meilleure compréhensibilité par rapport à un RL basé sur des récompenses à une échelle simple ou un RL qui utilise uniquement des prompts. Nos résultats montrent que convertir la rétroaction en langue en pente peut être une forte signale pour l'optimisation de politiques spécifiques. Notre code est disponible sur https://github.com/microsoft/Text2Grad.",
      "upvotes": 4,
      "discussionId": "6837c79576eac3fa930de1dd",
      "ai_summary": "Text2Grad converts human textual feedback into span-level gradients to optimize language models precisely and efficiently.",
      "ai_keywords": [
        "RLHF",
        "reinforcement-learning",
        "free-form textual feedback",
        "span-level gradients",
        "token spans",
        "differentiable reward signals",
        "gradient updates",
        "span-level policy optimizer",
        "fine-grained reward model",
        "feedback-annotation pipeline"
      ]
    },
    "publishedAt": "2025-05-28T09:23:49.000Z",
    "title": "Text2Grad: Reinforcement Learning from Natural Language Feedback",
    "summary": "Traditional RLHF optimizes language models with coarse, scalar rewards that\nmask the fine-grained reasons behind success or failure, leading to slow and\nopaque learning. Recent work augments RL with textual critiques through\nprompting or reflection, improving interpretability but leaving model\nparameters untouched. We introduce Text2Grad, a reinforcement-learning paradigm\nthat turns free-form textual feedback into span-level gradients. Given human\n(or programmatic) critiques, Text2Grad aligns each feedback phrase with the\nrelevant token spans, converts these alignments into differentiable reward\nsignals, and performs gradient updates that directly refine the offending\nportions of the model's policy. This yields precise, feedback-conditioned\nadjustments instead of global nudges. Text2Grad is realized through three\ncomponents: (1) a high-quality feedback-annotation pipeline that pairs\ncritiques with token spans; (2) a fine-grained reward model that predicts\nspan-level reward on answer while generating explanatory critiques; and (3) a\nspan-level policy optimizer that back-propagates natural-language gradients.\nAcross summarization, code generation, and question answering, Text2Grad\nconsistently surpasses scalar-reward RL and prompt-only baselines, providing\nboth higher task metrics and richer interpretability. Our results demonstrate\nthat natural-language feedback, when converted to gradients, is a powerful\nsignal for fine-grained policy optimization. The code for our method is\navailable at https://github.com/microsoft/Text2Grad",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22338.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "654dbac9938fbf1e696be8aa",
      "avatarUrl": "/avatars/b3c4035c48169c1bfb04a439fce3499f.svg",
      "fullname": "Chaoyun Zhang",
      "name": "vyokky",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.22203",
      "authors": [
        {
          "_id": "6837dcc41448b8bf0c91fa30",
          "user": {
            "_id": "6462def82a83863b97c0611e",
            "avatarUrl": "/avatars/c03e9cc7d75b0266fcc56ecb6ee62148.svg",
            "isPro": false,
            "fullname": "Yuzhen Huang",
            "user": "yuzhen17",
            "type": "user"
          },
          "name": "Yuzhen Huang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:41:53.737Z",
          "hidden": false
        },
        {
          "_id": "6837dcc41448b8bf0c91fa31",
          "name": "Weihao Zeng",
          "hidden": false
        },
        {
          "_id": "6837dcc41448b8bf0c91fa32",
          "name": "Xingshan Zeng",
          "hidden": false
        },
        {
          "_id": "6837dcc41448b8bf0c91fa33",
          "name": "Qi Zhu",
          "hidden": false
        },
        {
          "_id": "6837dcc41448b8bf0c91fa34",
          "name": "Junxian He",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T10:28:41.000Z",
      "submittedOnDailyAt": "2025-05-29T02:37:12.694Z",
      "title": "Règles ou modèle de données de validation -- Étude de cas en mathématique d'inférence",
      "submittedOnDailyBy": {
        "_id": "6462def82a83863b97c0611e",
        "avatarUrl": "/avatars/c03e9cc7d75b0266fcc56ecb6ee62148.svg",
        "isPro": false,
        "fullname": "Yuzhen Huang",
        "user": "yuzhen17",
        "type": "user"
      },
      "summary": "Une validation de haute confiance est essentielle pour le succès de l'apprentissage par renforcement (Apprentissage par Renforcement, AR), ce qui est fondamental pour des modèles logiques d'une grande échelle comme DeepSeek-R1. C'est l'un des méthodes fondamentaux dans le développement de modèles logiques mathématiques. Dans les domaines complexes de modèles logiques mathématiques, les validateurs basés sur des normes ont été largement utilisés pour l'entraînement de modèles logiques robustes. Cependant, la confiance dans ces validateurs et leur impact sur le processus d'entraînement d'apprentissage par renforcement n'a pas été entièrement compris. Dans cette étude, nous travaillons avec des modèles logiques mathématiques comme cas d'étude et nous analysons de manière détaillée deux aspects : l'évaluation dynamique et l'entraînement d'apprentissage par renforcement avec différents validateurs. Tout d'abord, nous constatons que les validateurs ouverts ont des difficultés à reconnaître des réponses équivalentes sous différentes formes dans plusieurs ensembles de données mathématiques, ce qui conduit à l'apparition de validateurs hautement sensibles visuellement. Cette limitation a un impact négatif sur le rendement de l'entraînement d'apprentissage par renforcement, et cet effet devient plus clair lorsque le modèle de politique est renforcé. Ensuite, nous examinons la validation de versions de modèles comme solution possible. Dans l'évaluation dynamique, les validateurs de versions de modèles atteignent une précision élevée de reconnaissance, mais sont très vulnérables à la malclassification de motifs dans l'analyse de l'évolution et dans les résultats de l'entraînement d'apprentissage par renforcement. Cette vulnérabilité peut augmenter artificiellement la récompense dans l'optimisation du modèle de politique. Nos résultats se concentrent sur les risques inhérents aux validateurs de normes et aux de versions de modèles, et fournissent des conseils précieux pour le développement de systèmes de récompense en apprentissage par renforcement.",
      "upvotes": 4,
      "discussionId": "6837dcc51448b8bf0c91fa54",
      "ai_summary": "The study examines the effectiveness and reliability of rule-based and model-based verifiers in reinforcement learning with verifiable reward, highlighting limitations and vulnerabilities in their use for mathematical reasoning tasks.",
      "ai_keywords": [
        "reinforcement learning",
        "verifiable reward",
        "DeepSeek-R1",
        "mathematical reasoning",
        "rule-based verifiers",
        "reward systems",
        "model-based verifiers",
        "false negatives",
        "false positives"
      ]
    },
    "publishedAt": "2025-05-28T06:28:41.000Z",
    "title": "Pitfalls of Rule- and Model-based Verifiers -- A Case Study on\n  Mathematical Reasoning",
    "summary": "Trustworthy verifiers are essential for the success of reinforcement learning\nwith verifiable reward (RLVR), which is the core methodology behind various\nlarge reasoning models such as DeepSeek-R1. In complex domains like\nmathematical reasoning, rule-based verifiers have been widely adopted in\nprevious works to train strong reasoning models. However, the reliability of\nthese verifiers and their impact on the RL training process remain poorly\nunderstood. In this work, we take mathematical reasoning as a case study and\nconduct a comprehensive analysis of various verifiers in both static evaluation\nand RL training scenarios. First, we find that current open-source rule-based\nverifiers often fail to recognize equivalent answers presented in different\nformats across multiple commonly used mathematical datasets, resulting in\nnon-negligible false negative rates. This limitation adversely affects RL\ntraining performance and becomes more pronounced as the policy model gets\nstronger. Subsequently, we investigate model-based verifiers as a potential\nsolution to address these limitations. While the static evaluation shows that\nmodel-based verifiers achieve significantly higher verification accuracy,\nfurther analysis and RL training results imply that they are highly susceptible\nto hacking, where they misclassify certain patterns in responses as correct\n(i.e., false positives). This vulnerability is exploited during policy model\noptimization, leading to artificially inflated rewards. Our findings underscore\nthe unique risks inherent to both rule-based and model-based verifiers, aiming\nto offer valuable insights to develop more robust reward systems in\nreinforcement learning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22203.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6462def82a83863b97c0611e",
      "avatarUrl": "/avatars/c03e9cc7d75b0266fcc56ecb6ee62148.svg",
      "fullname": "Yuzhen Huang",
      "name": "yuzhen17",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.22202",
      "authors": [
        {
          "_id": "6837eff312d1f7a138bd09b3",
          "name": "Hyeonbin Hwang",
          "hidden": false
        },
        {
          "_id": "6837eff312d1f7a138bd09b4",
          "name": "Byeongguk Jeon",
          "hidden": false
        },
        {
          "_id": "6837eff312d1f7a138bd09b5",
          "name": "Seungone Kim",
          "hidden": false
        },
        {
          "_id": "6837eff312d1f7a138bd09b6",
          "name": "Jiyeon Kim",
          "hidden": false
        },
        {
          "_id": "6837eff312d1f7a138bd09b7",
          "name": "Hoyeon Chang",
          "hidden": false
        },
        {
          "_id": "6837eff312d1f7a138bd09b8",
          "name": "Sohee Yang",
          "hidden": false
        },
        {
          "_id": "6837eff312d1f7a138bd09b9",
          "name": "Seungpil Won",
          "hidden": false
        },
        {
          "_id": "6837eff312d1f7a138bd09ba",
          "name": "Dohaeng Lee",
          "hidden": false
        },
        {
          "_id": "6837eff312d1f7a138bd09bb",
          "name": "Youbin Ahn",
          "hidden": false
        },
        {
          "_id": "6837eff312d1f7a138bd09bc",
          "name": "Minjoon Seo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T10:28:35.000Z",
      "submittedOnDailyAt": "2025-05-29T04:00:37.847Z",
      "title": "Bien sûr, voici la traduction des textes en français, en maintenant une professionnalité et une précision :\n\n1. \"S'il vous plaît, je prédire et traduirai chaque phrase.\"\n   - \"S'il vous plaît, je prédirai et traduirai chaque phrase.\"\n\n2. \"Retournez cette phrase.\"\n   - \"Retournez cette phrase.\"\n\n3. \"N'ajoutez aucune explication ou texte supplémentaire, seulement le résultat de la traduction.\"\n   - \"N'ajoutez aucune explication ou texte supplémentaire, seulement le résultat de la traduction.\"\n\nN'ajoutez aucune explication ou texte supplémentaire, seulement le résultat de la traduction.",
      "submittedOnDailyBy": {
        "_id": "647eaaf61a1fcad2fdc5d1ef",
        "avatarUrl": "/avatars/596d7a61d6e6227d38b661210a32fed0.svg",
        "isPro": false,
        "fullname": "Hyeonbin Hwang ",
        "user": "hbin0701",
        "type": "user"
      },
      "summary": "Les modèles automatiques de langue de régression (LM) génèrent un token à la fois, tandis que la logique humaine utilise une plus grande abstraition aux niveaux plus élevés d'énoncés, propositions et concepts. Ce contraste soulève une question cruciale : peuvent-ils les LM réaliser de la logique dans des unités structurées de sens ? Dans cette étude, on cherche à savoir si on peut cartographier ces logiques abstraites dans des espaces logiques. On propose un cadre qui applique un LM qui prédit des tokens au niveau de la prédiction continue, en utilisant un cartographage automatique de régression. On examine deux paradigmes de cartographie inspirés par l'apprentissage de représentations classiques : 1) le cartographage sémantique, qui utilise une codification automatique pour préserver un sens superficiel ; et 2) le cartographage contextuel, qui codifie la structure prédite par la prédiction de la prochaine phrase. On évalue deux modes d'inférence : le mode séparé interprète le cartographage prédit comme une chaîne de caractères pour le recodér ; le mode continu exécute une logique complète dans l'espace de cartographie, obtenant une efficacité. Dans quatre domaines : mathématiques, logique, connaissance générale et planification, le cartographage contextuel de l'inférence continue montre une performance compétitive avec la Chain-of-Thought (CoT) et réduit en moyenne le temps d'inférence à la moitié. De plus, il montre des débuts de scalabilité et d'adaptabilité modulaire. Enfin, on introduit le diagnostic SentenceLens pour visualiser le potentiel de trafic et interpréter l'état du modèle en langage interprétable. Ces résultats montrent que les LM entraînés peuvent être efficacement transformés en logiques abstraites et structurées dans un espace de cartographie.",
      "upvotes": 4,
      "discussionId": "6837eff412d1f7a138bd0a3e",
      "ai_summary": "Pretrained language models can adapt to operate in sentence space, reason over structured semantic units, and achieve competitive performance with Chain-of-Thought while reducing inference-time FLOPs.",
      "ai_keywords": [
        "autoregressive language models",
        "semantic embeddings",
        "contextual embeddings",
        "next-sentence prediction",
        "Chain-of-Thought",
        "SentenceLens"
      ]
    },
    "publishedAt": "2025-05-28T06:28:35.000Z",
    "title": "Let's Predict Sentence by Sentence",
    "summary": "Autoregressive language models (LMs) generate one token at a time, yet human\nreasoning operates over higher-level abstractions - sentences, propositions,\nand concepts. This contrast raises a central question- Can LMs likewise learn\nto reason over structured semantic units rather than raw token sequences? In\nthis work, we investigate whether pretrained LMs can be lifted into such\nabstract reasoning spaces by building on their learned representations. We\npresent a framework that adapts a pretrained token-level LM to operate in\nsentence space by autoregressively predicting continuous embeddings of next\nsentences. We explore two embedding paradigms inspired by classical\nrepresentation learning: 1) semantic embeddings, learned via autoencoding to\npreserve surface meaning; and 2) contextual embeddings, trained via\nnext-sentence prediction to encode anticipatory structure. We evaluate both\nunder two inference regimes: Discretized, which decodes each predicted\nembedding into text before re-encoding; and Continuous, which reasons entirely\nin embedding space for improved efficiency. Across four domains - mathematics,\nlogic, commonsense, and planning - contextual embeddings under continuous\ninference show competitive performance with Chain-of-Thought (CoT) while\nreducing inference-time FLOPs on average by half. We also present early signs\nof scalability and modular adaptation. Finally, to visualize latent\ntrajectories, we introduce SentenceLens, a diagnostic tool that decodes\nintermediate model states into interpretable sentences. Together, our results\nindicate that pretrained LMs can effectively transition to abstract, structured\nreasoning within latent embedding spaces.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22202.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "647eaaf61a1fcad2fdc5d1ef",
      "avatarUrl": "/avatars/596d7a61d6e6227d38b661210a32fed0.svg",
      "fullname": "Hyeonbin Hwang ",
      "name": "hbin0701",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.21876",
      "authors": [
        {
          "_id": "6837d80bf42b2aacfc26c460",
          "name": "Zun Wang",
          "hidden": false
        },
        {
          "_id": "6837d80bf42b2aacfc26c461",
          "name": "Jaemin Cho",
          "hidden": false
        },
        {
          "_id": "6837d80bf42b2aacfc26c462",
          "name": "Jialu Li",
          "hidden": false
        },
        {
          "_id": "6837d80bf42b2aacfc26c463",
          "name": "Han Lin",
          "hidden": false
        },
        {
          "_id": "6837d80bf42b2aacfc26c464",
          "user": {
            "_id": "652066649004117947e46ed6",
            "avatarUrl": "/avatars/972c97df6f26d2c3d6ce71ec579984bb.svg",
            "isPro": false,
            "fullname": "Jaehong Yoon",
            "user": "jaehong31",
            "type": "user"
          },
          "name": "Jaehong Yoon",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:42:07.590Z",
          "hidden": false
        },
        {
          "_id": "6837d80bf42b2aacfc26c465",
          "name": "Yue Zhang",
          "hidden": false
        },
        {
          "_id": "6837d80bf42b2aacfc26c466",
          "name": "Mohit Bansal",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T01:45:26.000Z",
      "submittedOnDailyAt": "2025-05-29T02:15:48.194Z",
      "title": "EPiC : Utiliser les directives de contrôle vidéo pour entraîner un contrôle efficace de la caméra vidéo",
      "submittedOnDailyBy": {
        "_id": "5ffe32d8942cf3533d364449",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654821969191-5ffe32d8942cf3533d364449.jpeg",
        "isPro": false,
        "fullname": "Jaemin Cho",
        "user": "j-min",
        "type": "user"
      },
      "summary": "Récemment, les méthodes de contrôle de caméras 3D dans les modèles de vidéo dépendance (VDM) génèrent des vidéos anticipées à partir de points estimés en fonction de trajectoires de caméras marquées, et souvent guident le modèle de dépendance avec des lignes structurantes. Cependant, les erreurs inhérentes à l'estimation de points génèrent des incertitudes dans les vidéos anticipées. De plus, l'analyse nécessaire des trajectoires de caméras marquées augmente les demandes en ressources. Pour résoudre ces limitations, nous présentons un cadre d'apprentissage de contrôle de caméras efficace et précis, appelé EPiC, qui construit des vidéos anticipées de haute qualité automatiquement sans la nécessité de marquer des trajectoires de caméras coûteuses. Spécifiquement, il se base sur le premier frame de vidéo pour appliquer une masque à la source de vidéo, générant ainsi des vidéos anticipées de haute qualité. Cette approche assure une bonne alignement et permet la création de paires d'entraînement (I2V) de vidéos sans la nécessité de marquer des trajectoires de caméras. De plus, nous présentons un module conditionnel léger Anchor-ControlNet, qui intègre des guides de vidéo anticipées dans un modèle de dépendance pré-entraîné, en utilisant moins de 1% des paramètres du modèle principal. Ce cadre ne nécessite pas de modifications dans le modèle principal de dépendance général, réduisant significativement les paramètres, les étapes d'entraînement et la quantité de données nécessaires, ce qui permet un entraînement efficace. Par conséquent, EPiC atteint les meilleurs résultats dans les tâches de contrôle de caméras I2V, comme RealEstate10K et MiraData, démontrant des capacités précises, robustes et solides dans le contrôle de caméras. En particulier, EPiC généralise fortement les vidéos anticipées générées à partir de points, permettant un contrôle de caméras basé sur des informations 3D précises et montrant une forte généralisation de 0 seed dans des scénarios vidéo à vidéo.",
      "upvotes": 4,
      "discussionId": "6837d810f42b2aacfc26c5ec",
      "projectPage": "https://zunwang1.github.io/Epic",
      "githubRepo": "https://github.com/wz0919/EPiC",
      "ai_summary": "EPiC is a framework for efficient 3D camera control in video diffusion models that constructs high-quality anchor videos through first-frame visibility masking, integrates them using a lightweight ControlNet module, and achieves state-of-the-art performance on I2V tasks with minimal resources.",
      "ai_keywords": [
        "anchor videos",
        "point cloud estimation",
        "camera trajectories",
        "diffusion models",
        "first-frame visibility",
        "EPiC",
        "ControlNet",
        "I2V training pairs",
        "rendering misalignments",
        "RealEstate10K",
        "MiraData"
      ]
    },
    "publishedAt": "2025-05-27T21:45:26.000Z",
    "title": "EPiC: Efficient Video Camera Control Learning with Precise Anchor-Video\n  Guidance",
    "summary": "Recent approaches on 3D camera control in video diffusion models (VDMs) often\ncreate anchor videos to guide diffusion models as a structured prior by\nrendering from estimated point clouds following annotated camera trajectories.\nHowever, errors inherent in point cloud estimation often lead to inaccurate\nanchor videos. Moreover, the requirement for extensive camera trajectory\nannotations further increases resource demands. To address these limitations,\nwe introduce EPiC, an efficient and precise camera control learning framework\nthat automatically constructs high-quality anchor videos without expensive\ncamera trajectory annotations. Concretely, we create highly precise anchor\nvideos for training by masking source videos based on first-frame visibility.\nThis approach ensures high alignment, eliminates the need for camera trajectory\nannotations, and thus can be readily applied to any in-the-wild video to\ngenerate image-to-video (I2V) training pairs. Furthermore, we introduce\nAnchor-ControlNet, a lightweight conditioning module that integrates anchor\nvideo guidance in visible regions to pretrained VDMs, with less than 1% of\nbackbone model parameters. By combining the proposed anchor video data and\nControlNet module, EPiC achieves efficient training with substantially fewer\nparameters, training steps, and less data, without requiring modifications to\nthe diffusion model backbone typically needed to mitigate rendering\nmisalignments. Although being trained on masking-based anchor videos, our\nmethod generalizes robustly to anchor videos made with point clouds during\ninference, enabling precise 3D-informed camera control. EPiC achieves SOTA\nperformance on RealEstate10K and MiraData for I2V camera control task,\ndemonstrating precise and robust camera control ability both quantitatively and\nqualitatively. Notably, EPiC also exhibits strong zero-shot generalization to\nvideo-to-video scenarios.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21876.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5ffe32d8942cf3533d364449",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654821969191-5ffe32d8942cf3533d364449.jpeg",
      "fullname": "Jaemin Cho",
      "name": "j-min",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.18700",
      "authors": [
        {
          "_id": "6837cc29bbee677da741aba7",
          "name": "Chun Wang",
          "hidden": false
        },
        {
          "_id": "6837cc29bbee677da741aba8",
          "name": "Xiaoran Pan",
          "hidden": false
        },
        {
          "_id": "6837cc29bbee677da741aba9",
          "name": "Zihao Pan",
          "hidden": false
        },
        {
          "_id": "6837cc29bbee677da741abaa",
          "name": "Haofan Wang",
          "hidden": false
        },
        {
          "_id": "6837cc29bbee677da741abab",
          "name": "Yiren Song",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-24T13:48:57.000Z",
      "submittedOnDailyAt": "2025-05-29T01:25:51.194Z",
      "title": "GRE Suite : Ajuste d'un modèle de langue visuolinguistique et renforcement de la chaîne de raisonnement pour l'inférence géographique basée sur l'estimation de la localisation.",
      "submittedOnDailyBy": {
        "_id": "64311a95034ecbefddd141ef",
        "avatarUrl": "/avatars/b6dc5ca373bedbaa368208517954c375.svg",
        "isPro": true,
        "fullname": "Yiren Song",
        "user": "yiren98",
        "type": "user"
      },
      "summary": "Le développement récent des modèles de langage visuel radio (VLMs) a démontré des résultats exceptionnels dans des tâches de logique visuelle. Cependant, le reconnaissance de la position présente des problèmes caractéristiques. Il est nécessaire d'extraire diverses informations visuelles des images et de les intégrer avec des connaissances du monde extérieur pour effectuer des logiques systématiques. Les approches actuelles pour le reconnaissance de la position manquent d'une forte structure logique et d'explicabilité. Pour résoudre ces limitations, on propose un nouveau cadre qui étend les VLMs à une chaîne logique structurée. Ce cadre renforce l'inférence de position interprétable. Le cadre GRE (Geological Reasoning Evaluation) est développé de manière systématique dans trois éléments principaux : dataset, modèle et benchmark. Tout d'abord, on présente GRE30K, un dataset de haute qualité de reconnaissance de position et d'analyse de contexte conçu pour favoriser la recherche en analyse visuelle et contextuelle. Ensuite, on présente le modèle GRE, qui utilise des stratégies logiques à plusieurs niveaux pour inférer des attributs du scénario, des détails de la position et des caractéristiques sémantiques, identifiant des zones potentielles de position avec une grande précision et améliorant ainsi la précision du reconnaissance de la position. Enfin, on établit le Geo Reason Evaluation Benchmark (GREval-Bench), un cadre d'évaluation détaillé pour évaluer la capacité de reconnaissance de la position dans des scénarios urbains, naturels et de marques. Les résultats des expérimentations montrent que GRE dépasse significativement les méthodes actuelles dans tous les aspects de la tâche de reconnaissance de la position. On souligne l'effet des VLMs explicables sur l'inférence complexe de la position. Le code et les données sont disponibles sur https://github.com/Thorin215/GRE.",
      "upvotes": 3,
      "discussionId": "6837cc2abbee677da741abf5",
      "ai_summary": "The GRE Suite enhances Visual Language Models with structured reasoning chains, improving geo-localization tasks through a multi-stage strategy and comprehensive evaluation benchmark.",
      "ai_keywords": [
        "Visual Language Models",
        "geo-localization",
        "reasoning chains",
        "GRE30K",
        "GRE model",
        "GREval-Bench",
        "multi-stage reasoning",
        "scene attributes",
        "local details",
        "semantic features",
        "coarse-grained localization",
        "fine-grained localization"
      ]
    },
    "publishedAt": "2025-05-24T09:48:57.000Z",
    "title": "GRE Suite: Geo-localization Inference via Fine-Tuned Vision-Language\n  Models and Enhanced Reasoning Chains",
    "summary": "Recent advances in Visual Language Models (VLMs) have demonstrated\nexceptional performance in visual reasoning tasks. However, geo-localization\npresents unique challenges, requiring the extraction of multigranular visual\ncues from images and their integration with external world knowledge for\nsystematic reasoning. Current approaches to geo-localization tasks often lack\nrobust reasoning mechanisms and explainability, limiting their effectiveness.\nTo address these limitations, we propose the Geo Reason Enhancement (GRE)\nSuite, a novel framework that augments VLMs with structured reasoning chains\nfor accurate and interpretable location inference. The GRE Suite is\nsystematically developed across three key dimensions: dataset, model, and\nbenchmark. First, we introduce GRE30K, a high-quality geo-localization\nreasoning dataset designed to facilitate fine-grained visual and contextual\nanalysis. Next, we present the GRE model, which employs a multi-stage reasoning\nstrategy to progressively infer scene attributes, local details, and semantic\nfeatures, thereby narrowing down potential geographic regions with enhanced\nprecision. Finally, we construct the Geo Reason Evaluation Benchmark\n(GREval-Bench), a comprehensive evaluation framework that assesses VLMs across\ndiverse urban, natural, and landmark scenes to measure both coarse-grained\n(e.g., country, continent) and fine-grained (e.g., city, street) localization\nperformance. Experimental results demonstrate that GRE significantly\noutperforms existing methods across all granularities of geo-localization\ntasks, underscoring the efficacy of reasoning-augmented VLMs in complex\ngeographic inference. Code and data will be released at\nhttps://github.com/Thorin215/GRE.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18700.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64311a95034ecbefddd141ef",
      "avatarUrl": "/avatars/b6dc5ca373bedbaa368208517954c375.svg",
      "fullname": "Yiren Song",
      "name": "yiren98",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 15
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.17870",
      "authors": [
        {
          "_id": "6837f049023315653be65a88",
          "user": {
            "_id": "640f32f6ef5c6dcac8b094bd",
            "avatarUrl": "/avatars/89b95837666ad696fe1f10808e4619b0.svg",
            "isPro": false,
            "fullname": "Shaina Raza",
            "user": "Shainarazavi",
            "type": "user"
          },
          "name": "Shaina Raza",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-29T05:27:38.248Z",
          "hidden": false
        },
        {
          "_id": "6837f049023315653be65a89",
          "name": "Rizwan Qureshi",
          "hidden": false
        },
        {
          "_id": "6837f049023315653be65a8a",
          "name": "Marcelo Lotif",
          "hidden": false
        },
        {
          "_id": "6837f049023315653be65a8b",
          "user": {
            "_id": "63a4754927f1f64ed7238dac",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
            "isPro": false,
            "fullname": "Aman Chadha",
            "user": "amanchadha",
            "type": "user"
          },
          "name": "Aman Chadha",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:41:40.928Z",
          "hidden": false
        },
        {
          "_id": "6837f049023315653be65a8c",
          "name": "Deval Pandya",
          "hidden": false
        },
        {
          "_id": "6837f049023315653be65a8d",
          "name": "Christos Emmanouilidis",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T13:20:23.000Z",
      "submittedOnDailyAt": "2025-05-29T04:08:04.627Z",
      "title": "Les modèles doivent également disposer d'une qualité de vie similaire à celle humaine : le modèle de la communication du peuple élimine les mensonges.",
      "submittedOnDailyBy": {
        "_id": "63a4754927f1f64ed7238dac",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
        "isPro": false,
        "fullname": "Aman Chadha",
        "user": "amanchadha",
        "type": "user"
      },
      "summary": "Les modèles d'IA génératif apprennent et récapitulent fréquemment des informations fausses incluses dans l'ensemble d'entraînement. Cet article suppose que, de manière similaire à l'immunisation biologique, le contact contrôlé avec des virus débiles contribue à la construction de l'immunité, et présente que les modèles d'IA effectuent des ajustements basiques en basant sur de petits et séparés ensembles de faits faux étiquetés explicitement, ce qui est appelé \"vaccin\". Ces faits faux sont injectés régulièrement lors de l'ajustement, renforçant la capacité du modèle à maintenir la précision face aux données d'entrée, reconnaître et rejeter des déclarations erronées. Un cas d'étude spécifique montre que les modèles immunisés produisent significativement moins de faits faux que les modèles de référence. Selon notre connaissance, cela est un premier cadre d'entraînement, montrant que les faits faux vérifiés sont traités comme des \"vaccins\" standardisés et que le modèle se renforce avec futurs faits faux sans dépendre de la déformation des données d'entrée ou de signaux de rétroaction humaine générale. De plus, nous expliquons les normes éthiques et les contrôles de gestion pour l'utilisation sécurisée de données fausses. L'immunisation du modèle offre un cadre anticipé pour faire face à la fidélité des systèmes d'IA.",
      "upvotes": 3,
      "discussionId": "6837f04a023315653be65ac6",
      "ai_summary": "A generative AI model is fine-tuned with labeled falsehoods to reduce misinformation generation, analogous to biological immunization.",
      "ai_keywords": [
        "generative AI models",
        "misinformation",
        "fine-tuning",
        "labeled falsehoods",
        "immunization",
        "fact-checked falsehoods",
        "supervised vaccine"
      ]
    },
    "publishedAt": "2025-05-23T09:20:23.000Z",
    "title": "Just as Humans Need Vaccines, So Do Models: Model Immunization to Combat\n  Falsehoods",
    "summary": "Generative AI models often learn and reproduce false information present in\ntheir training corpora. This position paper argues that, analogous to\nbiological immunization, where controlled exposure to a weakened pathogen\nbuilds immunity, AI models should be fine tuned on small, quarantined sets of\nexplicitly labeled falsehoods as a \"vaccine\" against misinformation. These\ncurated false examples are periodically injected during finetuning,\nstrengthening the model ability to recognize and reject misleading claims while\npreserving accuracy on truthful inputs. An illustrative case study shows that\nimmunized models generate substantially less misinformation than baselines. To\nour knowledge, this is the first training framework that treats fact checked\nfalsehoods themselves as a supervised vaccine, rather than relying on input\nperturbations or generic human feedback signals, to harden models against\nfuture misinformation. We also outline ethical safeguards and governance\ncontrols to ensure the safe use of false data. Model immunization offers a\nproactive paradigm for aligning AI systems with factuality.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17870.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a4754927f1f64ed7238dac",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
      "fullname": "Aman Chadha",
      "name": "amanchadha",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.21191",
      "authors": [
        {
          "_id": "6837eb38f09a146728a4b80f",
          "name": "Junyan Zhang",
          "hidden": false
        },
        {
          "_id": "6837eb38f09a146728a4b810",
          "name": "Yubo Gao",
          "hidden": false
        },
        {
          "_id": "6837eb38f09a146728a4b811",
          "name": "Yibo Yan",
          "hidden": false
        },
        {
          "_id": "6837eb38f09a146728a4b812",
          "name": "Jungang Li",
          "hidden": false
        },
        {
          "_id": "6837eb38f09a146728a4b813",
          "name": "Zhaorui Hou",
          "hidden": false
        },
        {
          "_id": "6837eb38f09a146728a4b814",
          "name": "Sicheng Tao",
          "hidden": false
        },
        {
          "_id": "6837eb38f09a146728a4b815",
          "name": "Shuliang Liu",
          "hidden": false
        },
        {
          "_id": "6837eb38f09a146728a4b816",
          "name": "Song Dai",
          "hidden": false
        },
        {
          "_id": "6837eb38f09a146728a4b817",
          "name": "Yonghua Hei",
          "hidden": false
        },
        {
          "_id": "6837eb38f09a146728a4b818",
          "name": "Junzhuo Li",
          "hidden": false
        },
        {
          "_id": "6837eb38f09a146728a4b819",
          "name": "Xuming Hu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-27T13:40:28.000Z",
      "submittedOnDailyAt": "2025-05-29T03:36:16.801Z",
      "title": "Instruction-Respondent Neuron and Experts Published: An Analytical Framework for Analyzing the Capacity of Instruction Following in LLMs",
      "submittedOnDailyBy": {
        "_id": "64b76528fdb702b3d8641514",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b76528fdb702b3d8641514/wCbEtpPEHRjS4sZSVK1gK.png",
        "isPro": false,
        "fullname": "Jungang Li",
        "user": "Jungang",
        "type": "user"
      },
      "summary": "La finetuning des LLMs a considérablement augmenté leur capacité à agir selon des instructions, bien que l'architecture informatique qui pousse cette amélioration soit encore mal compris. Dans cette étude, les composants rares (neurones ou experts de l'architecture Mixture-of-Experts (MoE)) spécialisés dans les instructions sont séparés et analysés, en étudiant systématiquement comment les calculs dans les LLMs sont réconfigures. En particulier, un ensemble de données d'instructions plus adapté est présenté, comprenant six catégories différentes appelées HexaInst, et un nouveau cadre d'analyse appelé SPARCOM est proposé. Ce cadre comprend trois contributions principales : 1) des méthodes pour identifier ces composants rares, 2) l'évaluation de leur généralité fonctionnelle et de leurs caractéristiques uniques, et 3) une comparaison systématique de leurs changements. Les expérimentations montrent l'importance fonctionnelle et unique de ces composants lors de l'exécution d'instructions. En découvrant la relation entre la finetuning et la structure de calcul rare, une ingénierie plus profonde pour l'intégration des comportements des LLMs selon des instructions sera possible, permettant ainsi l'offre d'une communauté de LLMs fiables.",
      "upvotes": 2,
      "discussionId": "6837eb39f09a146728a4b872",
      "ai_summary": "The study investigates the role of sparse computational components in the instruction-following capabilities of Large Language Models through systematic analysis and introduces HexaInst and SPARCOM for better understanding.",
      "ai_keywords": [
        "Large Language Models",
        "fine-tuning",
        "instruction-following",
        "HexaInst",
        "SPARCOM",
        "sparse components",
        "neurons",
        "Mixture-of-Experts",
        "instruction execution",
        "computational adaptations"
      ]
    },
    "publishedAt": "2025-05-27T09:40:28.000Z",
    "title": "Unveiling Instruction-Specific Neurons & Experts: An Analytical\n  Framework for LLM's Instruction-Following Capabilities",
    "summary": "The finetuning of Large Language Models (LLMs) has significantly advanced\ntheir instruction-following capabilities, yet the underlying computational\nmechanisms driving these improvements remain poorly understood. This study\nsystematically examines how fine-tuning reconfigures LLM computations by\nisolating and analyzing instruction-specific sparse components, i.e., neurons\nin dense models and both neurons and experts in Mixture-of-Experts (MoE)\narchitectures. In particular, we introduce HexaInst, a carefully curated and\nbalanced instructional dataset spanning six distinct categories, and propose\nSPARCOM, a novel analytical framework comprising three key contributions: (1) a\nmethod for identifying these sparse components, (2) an evaluation of their\nfunctional generality and uniqueness, and (3) a systematic comparison of their\nalterations. Through experiments, we demonstrate functional generality,\nuniqueness, and the critical role of these components in instruction execution.\nBy elucidating the relationship between fine-tuning-induced adaptations and\nsparse computational substrates, this work provides deeper insights into how\nLLMs internalize instruction-following behavior for the trustworthy LLM\ncommunity.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21191.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "64b76528fdb702b3d8641514",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b76528fdb702b3d8641514/wCbEtpPEHRjS4sZSVK1gK.png",
      "fullname": "Jungang Li",
      "name": "Jungang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.17507",
      "authors": [
        {
          "_id": "6833f24ed5c438959f7decf9",
          "user": {
            "_id": "619ef3f253061ce00477b09e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/619ef3f253061ce00477b09e/FknZhgQhV2_3aTqIKVsTo.jpeg",
            "isPro": false,
            "fullname": "Qiaosheng Chen",
            "user": "cqsss",
            "type": "user"
          },
          "name": "Qiaosheng Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-28T09:01:00.198Z",
          "hidden": false
        },
        {
          "_id": "6833f24ed5c438959f7decfa",
          "name": "Kaijia Huang",
          "hidden": false
        },
        {
          "_id": "6833f24ed5c438959f7decfb",
          "name": "Xiao Zhou",
          "hidden": false
        },
        {
          "_id": "6833f24ed5c438959f7decfc",
          "name": "Weiqing Luo",
          "hidden": false
        },
        {
          "_id": "6833f24ed5c438959f7decfd",
          "name": "Yuanning Cui",
          "hidden": false
        },
        {
          "_id": "6833f24ed5c438959f7decfe",
          "name": "Gong Cheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T06:00:20.000Z",
      "submittedOnDailyAt": "2025-05-29T00:53:38.693Z",
      "title": "Recommandation, Classification et Evaluation Basée sur des Points de Repère de la Réseau de Face de la Main",
      "submittedOnDailyBy": {
        "_id": "619ef3f253061ce00477b09e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/619ef3f253061ce00477b09e/FknZhgQhV2_3aTqIKVsTo.jpeg",
        "isPro": false,
        "fullname": "Qiaosheng Chen",
        "user": "cqsss",
        "type": "user"
      },
      "summary": "Le rapide croissance des ressources d'apprentissage automatique (ML) ouvertes a contribué à accélérer la recherche en Intelligence Relacionada (IR). Cependant, des plateformes telles que Hugging Face ne utilisent pas explicitement des représentations structurées, limitant ainsi des fonctions avancées de recherche et d'analyse telles que le suivi de l'évolution des modèles et la recommandation de ensembles de données liés. Dans ce contexte, un premier grand graphe de connaissances, HuggingKG, a été construit par la communauté de Hugging Face. Ce graphe comprend 2,6 millions de nœuds et 6,2 millions d'arêtes, et explore des relations propres à chaque domaine et des caractéristiques lexicales riches. Cela permet le développement d'un benchmark multi- tâche appelé HuggingBench, qui inclut des tâches comme la recommandation de ressources, la classification de classes et le suivi de l'évolution des modèles. Nos expériences révèlent les caractéristiques de HuggingKG et des tâches générées à partir de lui. Les deux ressources sont accessibles publiquement, ce qui nous espérons encourager le développement de la recherche en comparaison et gestion de ressources ouvertes.",
      "upvotes": 2,
      "discussionId": "6833f24ed5c438959f7ded31",
      "githubRepo": "https://github.com/nju-websoft/HuggingBench",
      "ai_summary": "HuggingKG, a large-scale knowledge graph, enhances open source ML resource management by enabling advanced queries and analyses via HuggingBench.",
      "ai_keywords": [
        "knowledge graph",
        "resource recommendation",
        "classification",
        "tracing",
        "multi-task benchmark"
      ]
    },
    "publishedAt": "2025-05-23T02:00:20.000Z",
    "title": "Benchmarking Recommendation, Classification, and Tracing Based on\n  Hugging Face Knowledge Graph",
    "summary": "The rapid growth of open source machine learning (ML) resources, such as\nmodels and datasets, has accelerated IR research. However, existing platforms\nlike Hugging Face do not explicitly utilize structured representations,\nlimiting advanced queries and analyses such as tracing model evolution and\nrecommending relevant datasets. To fill the gap, we construct HuggingKG, the\nfirst large-scale knowledge graph built from the Hugging Face community for ML\nresource management. With 2.6 million nodes and 6.2 million edges, HuggingKG\ncaptures domain-specific relations and rich textual attributes. It enables us\nto further present HuggingBench, a multi-task benchmark with three novel test\ncollections for IR tasks including resource recommendation, classification, and\ntracing. Our experiments reveal unique characteristics of HuggingKG and the\nderived tasks. Both resources are publicly available, expected to advance\nresearch in open source resource sharing and management.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17507.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "619ef3f253061ce00477b09e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/619ef3f253061ce00477b09e/FknZhgQhV2_3aTqIKVsTo.jpeg",
      "fullname": "Qiaosheng Chen",
      "name": "cqsss",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.15813",
      "authors": [
        {
          "_id": "68380974717461677df17514",
          "name": "Muquan Yu",
          "hidden": false
        },
        {
          "_id": "68380974717461677df17515",
          "name": "Mu Nan",
          "hidden": false
        },
        {
          "_id": "68380974717461677df17516",
          "name": "Hossein Adeli",
          "hidden": false
        },
        {
          "_id": "68380974717461677df17517",
          "name": "Jacob S. Prince",
          "hidden": false
        },
        {
          "_id": "68380974717461677df17518",
          "name": "John A. Pyles",
          "hidden": false
        },
        {
          "_id": "68380974717461677df17519",
          "name": "Leila Wehbe",
          "hidden": false
        },
        {
          "_id": "68380974717461677df1751a",
          "name": "Margaret M. Henderson",
          "hidden": false
        },
        {
          "_id": "68380974717461677df1751b",
          "name": "Michael J. Tarr",
          "hidden": false
        },
        {
          "_id": "68380974717461677df1751c",
          "user": {
            "_id": "64b6ce23dbbd1f2cdb624d56",
            "avatarUrl": "/avatars/022738ce8f76fa9545b3e363d7264b53.svg",
            "isPro": false,
            "fullname": "Andrew Luo",
            "user": "aluo-x",
            "type": "user"
          },
          "name": "Andrew F. Luo",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-29T07:16:44.495Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64b6ce23dbbd1f2cdb624d56/vCXz4Xv4AgHNNIC1S8aYf.png"
      ],
      "publishedAt": "2025-05-21T17:59:41.000Z",
      "submittedOnDailyAt": "2025-05-29T05:45:43.922Z",
      "title": "Apprentissage méta pour le modèle de transformeur de contexte de la vision humaine à haut niveau",
      "submittedOnDailyBy": {
        "_id": "64b6ce23dbbd1f2cdb624d56",
        "avatarUrl": "/avatars/022738ce8f76fa9545b3e363d7264b53.svg",
        "isPro": false,
        "fullname": "Andrew Luo",
        "user": "aluo-x",
        "type": "user"
      },
      "summary": "Les réseaux neuronaux pré-entraînés sur des ensembles de données de fin d'entraînement ont des moments qui surprennent la coincidence expressive avec les réponses neuronales humaines, mais le modélisation de modèles de calcul d'images basés sur des patrons visuels nécessite de grands ensembles de données d'IRM à chaque niveau. Ces données sont coûteuses et consommantes de temps, ce qui limite la capacité de généralisation de l'encodeur face à de nouveaux participants ou stimuli. BraInCoRL ne fait pas d'ajustements supplémentaires pour de nouveaux participants ou stimuli, mais utilise l'apprentissage explicatif pour prédire les réactions neuronales de chaque cellule. En utilisant l'architecture de Transformer, BraInCoRL attribue des conditions qui peuvent être adaptées à la variance des stimuli visuels, permettant l'apprentissage d'un biais d'indices pour plusieurs participants. Pendant le période d'entraînement, BraInCoRL optimise le modèle spécifiquement pour l'apprentissage explicatif, générant directement des modèles des activations visuelles de chaque cellule sous des conditions de caractéristiques de l'image et d'activation des cellules, ce qui permet un meilleur rendement. BraInCoRL peut ajuster le design de l'encodeur aux cellules pour de nouvelles images complètement différentes, montrant un biais de mise à l'échelle fort dans les tests. Le modèle peut généraliser complètement de nouveaux ensembles de données d'IRM visuelles en utilisant des paramètres d'IRM et des participants différents. De plus, BraInCoRL promeut l'interprétabilité des signaux neuronaux de la perception visuelle. Enfin, BraInCoRL permet le cartographage explicatif de la sélection de cellules à partir de questions naturelles.",
      "upvotes": 2,
      "discussionId": "68380977717461677df17638",
      "ai_summary": "BraInCoRL employs a transformer-based in-context learning approach to model higher visual cortex neural responses with few-shot examples, demonstrating superior performance and generalizability across new subjects, stimuli, and datasets.",
      "ai_keywords": [
        "functional representations",
        "higher visual cortex",
        "artificial neural networks",
        "fMRI datasets",
        "in-context learning",
        "transformer architecture",
        "inductive bias",
        "voxelwise neural responses",
        "image features",
        "voxel activations",
        "test-time scaling",
        "interpretability",
        "natural language queries",
        "voxel selectivity"
      ]
    },
    "publishedAt": "2025-05-21T13:59:41.000Z",
    "title": "Meta-Learning an In-Context Transformer Model of Human Higher Visual\n  Cortex",
    "summary": "Understanding functional representations within higher visual cortex is a\nfundamental question in computational neuroscience. While artificial neural\nnetworks pretrained on large-scale datasets exhibit striking representational\nalignment with human neural responses, learning image-computable models of\nvisual cortex relies on individual-level, large-scale fMRI datasets. The\nnecessity for expensive, time-intensive, and often impractical data acquisition\nlimits the generalizability of encoders to new subjects and stimuli. BraInCoRL\nuses in-context learning to predict voxelwise neural responses from few-shot\nexamples without any additional finetuning for novel subjects and stimuli. We\nleverage a transformer architecture that can flexibly condition on a variable\nnumber of in-context image stimuli, learning an inductive bias over multiple\nsubjects. During training, we explicitly optimize the model for in-context\nlearning. By jointly conditioning on image features and voxel activations, our\nmodel learns to directly generate better performing voxelwise models of higher\nvisual cortex. We demonstrate that BraInCoRL consistently outperforms existing\nvoxelwise encoder designs in a low-data regime when evaluated on entirely novel\nimages, while also exhibiting strong test-time scaling behavior. The model also\ngeneralizes to an entirely new visual fMRI dataset, which uses different\nsubjects and fMRI data acquisition parameters. Further, BraInCoRL facilitates\nbetter interpretability of neural signals in higher visual cortex by attending\nto semantically relevant stimuli. Finally, we show that our framework enables\ninterpretable mappings from natural language queries to voxel selectivity.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64b6ce23dbbd1f2cdb624d56/vCXz4Xv4AgHNNIC1S8aYf.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15813.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b6ce23dbbd1f2cdb624d56",
      "avatarUrl": "/avatars/022738ce8f76fa9545b3e363d7264b53.svg",
      "fullname": "Andrew Luo",
      "name": "aluo-x",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.12667",
      "authors": [
        {
          "_id": "682dd41740c6417d995087de",
          "user": {
            "_id": "648dca31385b84261811505d",
            "avatarUrl": "/avatars/dfd124e3b5ffed8b0d3f429be0f7bdc0.svg",
            "isPro": false,
            "fullname": "Zihan Su",
            "user": "Sugewud",
            "type": "user"
          },
          "name": "Zihan Su",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-28T16:17:54.970Z",
          "hidden": false
        },
        {
          "_id": "682dd41740c6417d995087df",
          "name": "Xuerui Qiu",
          "hidden": false
        },
        {
          "_id": "682dd41740c6417d995087e0",
          "name": "Hongbin Xu",
          "hidden": false
        },
        {
          "_id": "682dd41740c6417d995087e1",
          "name": "Tangyu Jiang",
          "hidden": false
        },
        {
          "_id": "682dd41740c6417d995087e2",
          "name": "Junhao Zhuang",
          "hidden": false
        },
        {
          "_id": "682dd41740c6417d995087e3",
          "name": "Chun Yuan",
          "hidden": false
        },
        {
          "_id": "682dd41740c6417d995087e4",
          "name": "Ming Li",
          "hidden": false
        },
        {
          "_id": "682dd41740c6417d995087e5",
          "name": "Shengfeng He",
          "hidden": false
        },
        {
          "_id": "682dd41740c6417d995087e6",
          "name": "Fei Richard Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-19T03:31:31.000Z",
      "submittedOnDailyAt": "2025-05-29T01:30:41.124Z",
      "title": "Safe-Sora : Séquences de vidéos générées à partir de texte en utilisant un marqueur graphique sécurisé.",
      "submittedOnDailyBy": {
        "_id": "648dca31385b84261811505d",
        "avatarUrl": "/avatars/dfd124e3b5ffed8b0d3f429be0f7bdc0.svg",
        "isPro": false,
        "fullname": "Zihan Su",
        "user": "Sugewud",
        "type": "user"
      },
      "summary": "Le croissance explosive des modèles de génération vidéo a entraîné une augmentation de la nécessité de la protection des droits d'auteur fiable pour le contenu généré à l'aide de l'IA. Malgré la popularité de la synthèse d'images, le générable n'a pas été largement examiné dans le contexte de la génération de vidéos. Pour combler cette lacune, nous proposons Safe-Sora, qui est le premier marqueur graphique directement intégré dans le processus de génération de vidéos. Nous observons la relation entre la similitude visuelle du marquage et son performance basée sur le contenu sur-écrit et sur-écrit, introduisant une structure d'ajustement adaptatif de la méthode à des détails. En particulier, les images de marque sont divisées en patches et sont attribuées aux frames de vidéo les plus similaires visuellement, et sont localisées plus spatialement dans les mêmes. Pour permettre la fonctionnalité espace-temporelle des patches de marque entre les frames de vidéo, nous avons développé une architecture de carte d'expansion d'onde 3D, préparant une nouvelle stratégie de sampling espace-temporel local. Cela ouvre une nouvelle voie pour la protection des droits d'auteur efficace et robuste, modélisant de manière efficace les dépendances à long terme. C'est la première tentative d'application de modèles de état à des marqueurs, ouvrant de nouvelles voies pour la protection des droits d'auteur efficace et robuste. Les expériences détaillées ont atteint les meilleurs résultats en termes de qualité de vidéo, précision et robustesse du marquage, contribuant de manière significative à notre proposition. Avec la publication, nous publions notre code.",
      "upvotes": 2,
      "discussionId": "682dd41840c6417d99508847",
      "projectPage": "https://sugewud.github.io/Safe-Sora-project/",
      "githubRepo": "https://github.com/Sugewud/Safe-Sora",
      "ai_summary": "Safe-Sora embeds invisible watermarks into AI-generated videos using a hierarchical adaptive matching mechanism and a 3D wavelet transform-enhanced Mamba architecture, achieving top performance in video quality, watermark fidelity, and robustness.",
      "ai_keywords": [
        "generative watermarking",
        "hierarchical coarse-to-fine adaptive matching",
        "3D wavelet transform",
        "Mamba architecture",
        "spatiotemporal local scanning",
        "state space models",
        "watermark embedding",
        "watermark retrieval"
      ]
    },
    "publishedAt": "2025-05-18T23:31:31.000Z",
    "title": "Safe-Sora: Safe Text-to-Video Generation via Graphical Watermarking",
    "summary": "The explosive growth of generative video models has amplified the demand for\nreliable copyright preservation of AI-generated content. Despite its popularity\nin image synthesis, invisible generative watermarking remains largely\nunderexplored in video generation. To address this gap, we propose Safe-Sora,\nthe first framework to embed graphical watermarks directly into the video\ngeneration process. Motivated by the observation that watermarking performance\nis closely tied to the visual similarity between the watermark and cover\ncontent, we introduce a hierarchical coarse-to-fine adaptive matching\nmechanism. Specifically, the watermark image is divided into patches, each\nassigned to the most visually similar video frame, and further localized to the\noptimal spatial region for seamless embedding. To enable spatiotemporal fusion\nof watermark patches across video frames, we develop a 3D wavelet\ntransform-enhanced Mamba architecture with a novel spatiotemporal local\nscanning strategy, effectively modeling long-range dependencies during\nwatermark embedding and retrieval. To the best of our knowledge, this is the\nfirst attempt to apply state space models to watermarking, opening new avenues\nfor efficient and robust watermark protection. Extensive experiments\ndemonstrate that Safe-Sora achieves state-of-the-art performance in terms of\nvideo quality, watermark fidelity, and robustness, which is largely attributed\nto our proposals. We will release our code upon publication.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.12667.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "648dca31385b84261811505d",
      "avatarUrl": "/avatars/dfd124e3b5ffed8b0d3f429be0f7bdc0.svg",
      "fullname": "Zihan Su",
      "name": "Sugewud",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.22645",
      "authors": [
        {
          "_id": "6837dbed1233747046da00f5",
          "name": "Hanjia Lyu",
          "hidden": false
        },
        {
          "_id": "6837dbed1233747046da00f6",
          "name": "Jiebo Luo",
          "hidden": false
        },
        {
          "_id": "6837dbed1233747046da00f7",
          "name": "Jian Kang",
          "hidden": false
        },
        {
          "_id": "6837dbed1233747046da00f8",
          "name": "Allison Koenecke",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T17:56:49.000Z",
      "submittedOnDailyAt": "2025-05-29T02:33:57.876Z",
      "title": "Caractérisation Bayésienne : Répertoire pour les Modèles de Langue de Grande Taille en Chinois Classique et en Chinois Traditionnel",
      "submittedOnDailyBy": {
        "_id": "64c939307dba66c3a7e4d215",
        "avatarUrl": "/avatars/0b662cc1799525188476f3e6e1f97d29.svg",
        "isPro": false,
        "fullname": "BruceLyu",
        "user": "brucelyu",
        "type": "user"
      },
      "summary": "La capacité des modèles de langage de grande taille (LLMs) a été étudiée tant pour le katakana simple que pour le katakana standard, mais il n'est pas encore connu si ces modèles présentent des performances différentes dans les deux langues écrites. Cette compréhension est cruciale. La différence de qualité des réponses des LLMs peut nuire aux décisions concernant l'utilisation de ces modèles dans des domaines comme l'éducation ou l'emploi, perpetuant le dégât symbolique infligé aux cultures associées à ces langues écrites et amplifiant les pertes dans les communautés touchées. Pour explorer ces différences, nous avons conçu deux tâches d'évaluation reflétant des scénarios réels : la sélection de termes locaux (encourageant les LLMs à nommer des choses différentes en katakana simple et katakana standard) et la sélection de noms locaux (encourageant les LLMs à choisir des personnes pour l'emploi à partir de listes de noms en katakana simple et katakana standard). Dans les deux tâches, nous avons évalué le comportement de 11 services avancés de LLMs commerciaux et de modèles open-source. Ces modèles ont été entraînés principalement en anglais, katakana simple ou katakana standard. Notre analyse montre que les réponses des LLMs sont sujets à des biais qui dépendent du contexte et de la langue écrite. Dans la tâche de sélection de termes locaux, la plupart des LLMs ont préféré des réponses en katakana simple, tandis que dans la tâche de sélection de noms locaux, ils ont préféré des noms en katakana standard. Ces différences peuvent ressortir du fait de la représentation dans les données d'entraînement, de la préférence de la langue écrite et des différences dans le tokenisage de katakana simple et katakana standard. Ces résultats démontrent la nécessité d'une analyse plus approfondie des biais des LLMs, et nous fournissons un ensemble de données open-source pour évaluer le comportement des LLMs dans les variantes de chinois, ce qui promet de stimuler la recherche dans ce domaine. (https://github.com/brucelyu17/SC-TC-Bench)",
      "upvotes": 1,
      "discussionId": "6837dbee1233747046da0125",
      "githubRepo": "https://github.com/brucelyu17/SC-TC-Bench",
      "ai_summary": "Research examines LLM performance biases between Simplified and Traditional Chinese in regional term and name choice tasks, attributing differences to training data and tokenization.",
      "ai_keywords": [
        "Large Language Models",
        "LLMs",
        "LLM-facilitated decision-making",
        "regional term choice",
        "regional name choice",
        "open-sourced benchmark dataset",
        "SC-TC-Bench"
      ]
    },
    "publishedAt": "2025-05-28T13:56:49.000Z",
    "title": "Characterizing Bias: Benchmarking Large Language Models in Simplified\n  versus Traditional Chinese",
    "summary": "While the capabilities of Large Language Models (LLMs) have been studied in\nboth Simplified and Traditional Chinese, it is yet unclear whether LLMs exhibit\ndifferential performance when prompted in these two variants of written\nChinese. This understanding is critical, as disparities in the quality of LLM\nresponses can perpetuate representational harms by ignoring the different\ncultural contexts underlying Simplified versus Traditional Chinese, and can\nexacerbate downstream harms in LLM-facilitated decision-making in domains such\nas education or hiring. To investigate potential LLM performance disparities,\nwe design two benchmark tasks that reflect real-world scenarios: regional term\nchoice (prompting the LLM to name a described item which is referred to\ndifferently in Mainland China and Taiwan), and regional name choice (prompting\nthe LLM to choose who to hire from a list of names in both Simplified and\nTraditional Chinese). For both tasks, we audit the performance of 11 leading\ncommercial LLM services and open-sourced models -- spanning those primarily\ntrained on English, Simplified Chinese, or Traditional Chinese. Our analyses\nindicate that biases in LLM responses are dependent on both the task and\nprompting language: while most LLMs disproportionately favored Simplified\nChinese responses in the regional term choice task, they surprisingly favored\nTraditional Chinese names in the regional name choice task. We find that these\ndisparities may arise from differences in training data representation, written\ncharacter preferences, and tokenization of Simplified and Traditional Chinese.\nThese findings highlight the need for further analysis of LLM biases; as such,\nwe provide an open-sourced benchmark dataset to foster reproducible evaluations\nof future LLM behavior across Chinese language variants\n(https://github.com/brucelyu17/SC-TC-Bench).",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22645.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64c939307dba66c3a7e4d215",
      "avatarUrl": "/avatars/0b662cc1799525188476f3e6e1f97d29.svg",
      "fullname": "BruceLyu",
      "name": "brucelyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.21960",
      "authors": [
        {
          "_id": "6837dd74ec10479b9605da15",
          "user": {
            "_id": "637e1cf4f09bf2498c543a73",
            "avatarUrl": "/avatars/3c0e4e15602a384839bee0c23029f58a.svg",
            "isPro": false,
            "fullname": "Senmao Li",
            "user": "senmaonk",
            "type": "user"
          },
          "name": "Senmao Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:41:51.659Z",
          "hidden": false
        },
        {
          "_id": "6837dd74ec10479b9605da16",
          "name": "Lei Wang",
          "hidden": false
        },
        {
          "_id": "6837dd74ec10479b9605da17",
          "name": "Kai Wang",
          "hidden": false
        },
        {
          "_id": "6837dd74ec10479b9605da18",
          "name": "Tao Liu",
          "hidden": false
        },
        {
          "_id": "6837dd74ec10479b9605da19",
          "name": "Jiehang Xie",
          "hidden": false
        },
        {
          "_id": "6837dd74ec10479b9605da1a",
          "name": "Joost van de Weijer",
          "hidden": false
        },
        {
          "_id": "6837dd74ec10479b9605da1b",
          "name": "Fahad Shahbaz Khan",
          "hidden": false
        },
        {
          "_id": "6837dd74ec10479b9605da1c",
          "name": "Shiqi Yang",
          "hidden": false
        },
        {
          "_id": "6837dd74ec10479b9605da1d",
          "name": "Yaxing Wang",
          "hidden": false
        },
        {
          "_id": "6837dd74ec10479b9605da1e",
          "name": "Jian Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T04:23:22.000Z",
      "submittedOnDailyAt": "2025-05-29T02:44:09.323Z",
      "title": "Titre : Ticket d'Origine : Acceptation du Modèle de Diffusion de Texte à Images en Utilisant un Encodeur Unifié Indépendant du Temps\n\nRésumé : Utilise un modèle de diffusion pour transformer du texte en images, où l'encodeur est unifié et indépendant du temps.",
      "submittedOnDailyBy": {
        "_id": "637e1cf4f09bf2498c543a73",
        "avatarUrl": "/avatars/3c0e4e15602a384839bee0c23029f58a.svg",
        "isPro": false,
        "fullname": "Senmao Li",
        "user": "senmaonk",
        "type": "user"
      },
      "summary": "Le modèle de diffusion pour texte à image (T2I) a réalisé une révolution impressionnante dans le modèle génératif, mais il existe un équilibre entre la vitesse d'inférence et la qualité de l'image, ainsi que des problèmes liés au traitement efficace de la machine. Les modèles T2I combinés existants peuvent générer des images de haute qualité en peu de pas de débit, mais présentent des faiblesses en termes de diversité et de qualité, surtout dans les modèles mono-passage. Dans notre analyse, nous avons confirmé l'existence de calculs inutiles dans l'encodeur UNet. Notre résultat montre que le décodeur dans les modèles de diffusion T2I capture une information large et claire de sens, tandis que l'encodeur peut être partagé entre différentes phases temporelles et entre différents décodeurs. En fonction de ces résultats, nous introduisons le premier encodeur temporellement indépendant sans boucle (TiUE) dans la structure UNet comme modèle étudiant. TiUE adopte une approche de génération d'images sans boucle et utilise une réseau neuronal en un seul pas pour la combinaison de la diffusion, permettant ainsi de partager les caractéristiques de l'encodeur sur plusieurs phases temporelles et de rendre possible le débit parallèle, ce qui signifie une réduction significative de la complexité du temps d'inférence. De plus, nous ajoutons un terme de divergence de Kullback-Leibler pour normaliser la prédiction du bruit et améliorer la réalité visuelle et la diversité des images générées. Les résultats des tests montrent que TiUE dépasse les meilleurs résultats, générant des résultats plus divers et réalistes tout en maintenant une efficacité calculelle.",
      "upvotes": 1,
      "discussionId": "6837dd78ec10479b9605db06",
      "githubRepo": "https://github.com/sen-mao/Loopfree",
      "ai_summary": "Time-independent Unified Encoder TiUE reduces inference time and improves diversity and quality in Text-to-Image diffusion models by sharing encoder features across decoder time steps.",
      "ai_keywords": [
        "Text-to-Image diffusion models",
        "inference speed",
        "image quality",
        "distilled T2I models",
        "UNet encoders",
        "decoders",
        "semantic information",
        "Time-independent Unified Encoder TiUE",
        "loop-free image generation",
        "one-pass scheme",
        "parallel sampling",
        "KL divergence",
        "perceptual realism",
        "LCM",
        "SD-Turbo",
        "SwiftBrushv2"
      ]
    },
    "publishedAt": "2025-05-28T00:23:22.000Z",
    "title": "One-Way Ticket:Time-Independent Unified Encoder for Distilling\n  Text-to-Image Diffusion Models",
    "summary": "Text-to-Image (T2I) diffusion models have made remarkable advancements in\ngenerative modeling; however, they face a trade-off between inference speed and\nimage quality, posing challenges for efficient deployment. Existing distilled\nT2I models can generate high-fidelity images with fewer sampling steps, but\noften struggle with diversity and quality, especially in one-step models. From\nour analysis, we observe redundant computations in the UNet encoders. Our\nfindings suggest that, for T2I diffusion models, decoders are more adept at\ncapturing richer and more explicit semantic information, while encoders can be\neffectively shared across decoders from diverse time steps. Based on these\nobservations, we introduce the first Time-independent Unified Encoder TiUE for\nthe student model UNet architecture, which is a loop-free image generation\napproach for distilling T2I diffusion models. Using a one-pass scheme, TiUE\nshares encoder features across multiple decoder time steps, enabling parallel\nsampling and significantly reducing inference time complexity. In addition, we\nincorporate a KL divergence term to regularize noise prediction, which enhances\nthe perceptual realism and diversity of the generated images. Experimental\nresults demonstrate that TiUE outperforms state-of-the-art methods, including\nLCM, SD-Turbo, and SwiftBrushv2, producing more diverse and realistic results\nwhile maintaining the computational efficiency.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21960.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "637e1cf4f09bf2498c543a73",
      "avatarUrl": "/avatars/3c0e4e15602a384839bee0c23029f58a.svg",
      "fullname": "Senmao Li",
      "name": "senmaonk",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.20715",
      "authors": [
        {
          "_id": "6837eef0e237f02cd3c87963",
          "name": "Fuwen Luo",
          "hidden": false
        },
        {
          "_id": "6837eef0e237f02cd3c87964",
          "name": "Shengfeng Lou",
          "hidden": false
        },
        {
          "_id": "6837eef0e237f02cd3c87965",
          "name": "Chi Chen",
          "hidden": false
        },
        {
          "_id": "6837eef0e237f02cd3c87966",
          "name": "Ziyue Wang",
          "hidden": false
        },
        {
          "_id": "6837eef0e237f02cd3c87967",
          "name": "Chenliang Li",
          "hidden": false
        },
        {
          "_id": "6837eef0e237f02cd3c87968",
          "name": "Weizhou Shen",
          "hidden": false
        },
        {
          "_id": "6837eef0e237f02cd3c87969",
          "name": "Jiyue Guo",
          "hidden": false
        },
        {
          "_id": "6837eef0e237f02cd3c8796a",
          "name": "Peng Li",
          "hidden": false
        },
        {
          "_id": "6837eef0e237f02cd3c8796b",
          "name": "Ming Yan",
          "hidden": false
        },
        {
          "_id": "6837eef0e237f02cd3c8796c",
          "name": "Ji Zhang",
          "hidden": false
        },
        {
          "_id": "6837eef0e237f02cd3c8796d",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "6837eef0e237f02cd3c8796e",
          "name": "Yang Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-27T04:50:07.000Z",
      "submittedOnDailyAt": "2025-05-29T03:57:26.118Z",
      "title": "MUSEG : Intérêt pour les marques de temps à travers la différence de segments multiples pour renforcer la compréhension temporelle du vidéo.",
      "submittedOnDailyBy": {
        "_id": "642086ed290342c5df85662d",
        "avatarUrl": "/avatars/915a4d7b89455ae97b8544c79286ddf8.svg",
        "isPro": false,
        "fullname": "Chi Chen",
        "user": "carboncoo",
        "type": "user"
      },
      "summary": "La compréhension de la séquence temporelle est cruciale pour expliquer des événements dans des images dans les modèles multimodales de langage (MLLMs). Malgré les progrès récents dans la compréhension des images, les MLLMs actuels ont des difficultés avec des causes temporelles subtiles. L'apprentissage par renforcement (RL) a été exploré récemment pour résoudre ce problème, mais les méthodes actuelles de RL ont des effets limités. Dans cette étude, un nouvel approche basée sur le RL appelé MUSEG est proposée, qui introduit un flux de graphes multiniveaux reliés aux étapes temporelles pour améliorer la compréhension temporelle. MUSEG favorise une explication plus détaillée des causes temporelles en agrégant des images et des questions. Pour encourager un apprentissage efficace, des récompenses par graphes temporels sont fournies de manière statistique, et un approche d'apprentissage par RL adaptatif est conçue. Des tests extensives ont été réalisés sur des graphes temporels et des tâches d'images temporelles de questions et de réponses, démontrant que MUSEG est significativement supérieur aux méthodes actuelles et montre une bonne généralisation dans divers scénarios de compréhension temporelle. Ce projet peut être consulté à l'adresse https://github.com/THUNLP-MT/MUSEG.",
      "upvotes": 1,
      "discussionId": "6837eef1e237f02cd3c8798d",
      "githubRepo": "https://github.com/THUNLP-MT/MUSEG",
      "ai_summary": "MUSEG, an RL-based method with timestamp-aware multi-segment grounding, significantly enhances the temporal understanding of large language models by improving alignment with video segments and demonstrating superior performance in temporal reasoning tasks.",
      "ai_keywords": [
        "reinforcement learning",
        "timestamp-aware multi-segment grounding",
        "temporal understanding",
        "MUSEG",
        "phased rewards",
        "temporal grounding",
        "time-sensitive video QA"
      ]
    },
    "publishedAt": "2025-05-27T00:50:07.000Z",
    "title": "MUSEG: Reinforcing Video Temporal Understanding via Timestamp-Aware\n  Multi-Segment Grounding",
    "summary": "Video temporal understanding is crucial for multimodal large language models\n(MLLMs) to reason over events in videos. Despite recent advances in general\nvideo understanding, current MLLMs still struggle with fine-grained temporal\nreasoning. While reinforcement learning (RL) has been explored to address this\nissue recently, existing RL approaches remain limited in effectiveness. In this\nwork, we propose MUSEG, a novel RL-based method that enhances temporal\nunderstanding by introducing timestamp-aware multi-segment grounding. MUSEG\nenables MLLMs to align queries with multiple relevant video segments, promoting\nmore comprehensive temporal reasoning. To facilitate effective learning, we\ndesign a customized RL training recipe with phased rewards that progressively\nguides the model toward temporally grounded reasoning. Extensive experiments on\ntemporal grounding and time-sensitive video QA tasks demonstrate that MUSEG\nsignificantly outperforms existing methods and generalizes well across diverse\ntemporal understanding scenarios. View our project at\nhttps://github.com/THUNLP-MT/MUSEG.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20715.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642086ed290342c5df85662d",
      "avatarUrl": "/avatars/915a4d7b89455ae97b8544c79286ddf8.svg",
      "fullname": "Chi Chen",
      "name": "carboncoo",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.21060",
      "authors": [
        {
          "_id": "683818841902f641cc669774",
          "name": "Peng Wang",
          "hidden": false
        },
        {
          "_id": "683818841902f641cc669775",
          "name": "Xiang Liu",
          "hidden": false
        },
        {
          "_id": "683818841902f641cc669776",
          "name": "Peidong Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-27T11:47:15.000Z",
      "submittedOnDailyAt": "2025-05-29T06:49:42.295Z",
      "title": "\"3R Style : Modélisation de styles 3D en temps réel pour n'importe quel scénario et style\"",
      "submittedOnDailyBy": {
        "_id": "5f1158120c833276f61f1a84",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
        "isPro": false,
        "fullname": "Niels Rogge",
        "user": "nielsr",
        "type": "user"
      },
      "summary": "Créer rapidement des scènes 3D stylisées et maintenir la cohérence multi-point tout en coincidant avec l'image stylisée est une tâche importante. Les méthodes de stylisation 3D les plus avancées actuellement utilisent des optimisations de temps d'essai avec haute performance, et transmettent des caractéristiques artistiques aux représentations 3D entraînées, ce qui nécessite des images d'entrée avec des poses denses. En contraste, nous présentons un nouvel approche qui utilise le développement récent des modèles de reconstruction dirigée pour effectuer le stylisage 3D directement en moins de 1 seconde, en utilisant des images de scènes de vue d'espace sans pose et des images de style arbitraire. Pour résoudre la connexion inhérente entre la reconstruction et le stylisage, nous introduisons une architecture de division qui sépare la modélisation structurale et la tomographie de la surface, évitant que les tendances de stylisage déforment la structure de la scène 3D. De plus, nous appliquons une perte d'identification pour promouvoir l'entraînement préalable du modèle de stylisation à travers de nouvelles tâches de synthèse visuelle. Cette stratégie permet d'adapter le stylisage tout en maintenant la capacité de reconstruction originale. Des évaluations détaillées à l'intérieur et à l'extérieur du jeu de données montrent que notre approche améliore l'intégration du style et de la face de la scène, ainsi que la cohérence multi-point et l'efficacité par rapport aux méthodes actuelles.",
      "upvotes": 0,
      "discussionId": "6838188b1902f641cc669947",
      "ai_summary": "A novel feed-forward model achieves fast 3D stylization using sparse view images, maintaining multi-view consistency and high-quality style transfer while retaining reconstruction accuracy.",
      "ai_keywords": [
        "feed-forward reconstruction models",
        "3D stylization",
        "dense posed input images",
        "unposed sparse-view scene images",
        "branched architecture",
        "structure modeling",
        "appearance shading",
        "identity loss",
        "novel view synthesis",
        "in-domain datasets",
        "out-of-domain datasets",
        "multi-view consistency"
      ]
    },
    "publishedAt": "2025-05-27T07:47:15.000Z",
    "title": "Styl3R: Instant 3D Stylized Reconstruction for Arbitrary Scenes and\n  Styles",
    "summary": "Stylizing 3D scenes instantly while maintaining multi-view consistency and\nfaithfully resembling a style image remains a significant challenge. Current\nstate-of-the-art 3D stylization methods typically involve computationally\nintensive test-time optimization to transfer artistic features into a\npretrained 3D representation, often requiring dense posed input images. In\ncontrast, leveraging recent advances in feed-forward reconstruction models, we\ndemonstrate a novel approach to achieve direct 3D stylization in less than a\nsecond using unposed sparse-view scene images and an arbitrary style image. To\naddress the inherent decoupling between reconstruction and stylization, we\nintroduce a branched architecture that separates structure modeling and\nappearance shading, effectively preventing stylistic transfer from distorting\nthe underlying 3D scene structure. Furthermore, we adapt an identity loss to\nfacilitate pre-training our stylization model through the novel view synthesis\ntask. This strategy also allows our model to retain its original reconstruction\ncapabilities while being fine-tuned for stylization. Comprehensive evaluations,\nusing both in-domain and out-of-domain datasets, demonstrate that our approach\nproduces high-quality stylized 3D content that achieve a superior blend of\nstyle and scene appearance, while also outperforming existing methods in terms\nof multi-view consistency and efficiency.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21060.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5f1158120c833276f61f1a84",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
      "fullname": "Niels Rogge",
      "name": "nielsr",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 874
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.21582",
      "authors": [
        {
          "_id": "68380c860fb1ddbe91ba0bf9",
          "user": {
            "_id": "631f5035c6b20f03c823c4ba",
            "avatarUrl": "/avatars/5557b07bc7bd76f5752b5cf3d8e8f22f.svg",
            "isPro": false,
            "fullname": "Christopher Knievel",
            "user": "CKnievel",
            "type": "user"
          },
          "name": "Christopher Knievel",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:41:12.986Z",
          "hidden": false
        },
        {
          "_id": "68380c860fb1ddbe91ba0bfa",
          "name": "Alexander Bernhardt",
          "hidden": false
        },
        {
          "_id": "68380c860fb1ddbe91ba0bfb",
          "name": "Christian Bernhardt",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-27T10:07:05.000Z",
      "submittedOnDailyAt": "2025-05-29T06:18:15.842Z",
      "title": "AITEE - Tameur des Étrangers pour Ingénierie Électrique",
      "submittedOnDailyBy": {
        "_id": "631f5035c6b20f03c823c4ba",
        "avatarUrl": "/avatars/5557b07bc7bd76f5752b5cf3d8e8f22f.svg",
        "isPro": false,
        "fullname": "Christopher Knievel",
        "user": "CKnievel",
        "type": "user"
      },
      "summary": "La combinaison d'un système de tutorat intelligent et de modèles de langage à grande échelle est un approche prometteuse pour satisfaire les diverses nécessités des étudiants et promouvoir l'apprentissage autonome. Bien que les modèles de langage à grande échelle aient une bonne compréhension des concepts de base de l'électronique, leur capacité à résoudre des questions spécifiques sur les circuits électriques n'est pas suffisante. Dans ce travail, nous présentons le système de tutorat basé sur des agents spécifiques pour l'électronique, AITEE, qui accompagne les étudiants pendant le processus d'apprentissage, offrant un soutien personnalisé et promouvant l'apprentissage autonome. AITEE supporte la rédaction manuelle et les circuits digitaux, permettant une interaction naturelle avec les étudiants. Un nouveau méthode de mesure de similarité basée sur les graphes utilise un approche de génération de suggestions pour rechercher des contextes appropriés à partir de matériaux de cours. De plus, la simulation parallèle améliore la précision dans l'application des méthodes de résolution. Le système met en œuvre un dialogue cyclique et encourage l'autonomie de l'apprentissage à travers des questions guidées. Une évaluation expérimentale montre que AITEE dépasse significativement les approches basées sur des connaissances spécifiques, et montre également un rendement notable avec des modèles de langage de taille moyenne. Nos résultats soulignent la possibilité de fournir un environnement d'apprentissage efficace, personnalisé et scalable, grâce à un système de tutorat basé sur des agents.",
      "upvotes": 0,
      "discussionId": "68380c870fb1ddbe91ba0c55",
      "ai_summary": "An agent-based tutoring system for electrical engineering enhances learning through natural circuit interaction, context-retrieving generation, and guided questioning, demonstrating superior performance compared to baseline methods.",
      "ai_keywords": [
        "agent-based tutoring system",
        "large language models",
        "electrical circuits",
        "adapted circuit reconstruction",
        "graph-based similarity measure",
        "retrieval augmented generation",
        "parallel Spice simulation",
        "Socratic dialogue"
      ]
    },
    "publishedAt": "2025-05-27T06:07:05.000Z",
    "title": "AITEE -- Agentic Tutor for Electrical Engineering",
    "summary": "Intelligent tutoring systems combined with large language models offer a\npromising approach to address students' diverse needs and promote\nself-efficacious learning. While large language models possess good\nfoundational knowledge of electrical engineering basics, they remain\ninsufficiently capable of addressing specific questions about electrical\ncircuits. In this paper, we present AITEE, an agent-based tutoring system for\nelectrical engineering designed to accompany students throughout their learning\nprocess, offer individualized support, and promote self-directed learning.\nAITEE supports both hand-drawn and digital circuits through an adapted circuit\nreconstruction process, enabling natural interaction with students. Our novel\ngraph-based similarity measure identifies relevant context from lecture\nmaterials through a retrieval augmented generation approach, while parallel\nSpice simulation further enhances accuracy in applying solution methodologies.\nThe system implements a Socratic dialogue to foster learner autonomy through\nguided questioning. Experimental evaluations demonstrate that AITEE\nsignificantly outperforms baseline approaches in domain-specific knowledge\napplication, with even medium-sized LLM models showing acceptable performance.\nOur results highlight the potential of agentic tutors to deliver scalable,\npersonalized, and effective learning environments for electrical engineering\neducation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21582.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "631f5035c6b20f03c823c4ba",
      "avatarUrl": "/avatars/5557b07bc7bd76f5752b5cf3d8e8f22f.svg",
      "fullname": "Christopher Knievel",
      "name": "CKnievel",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.18149",
      "authors": [
        {
          "_id": "6837e51d05c81fd7d7d1962e",
          "user": {
            "_id": "63ca499104c97982831127ec",
            "avatarUrl": "/avatars/816a962c62c805adf7789fa8e398b01e.svg",
            "isPro": false,
            "fullname": "Aradhye Agarwal",
            "user": "aradhye",
            "type": "user"
          },
          "name": "Aradhye Agarwal",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:41:47.337Z",
          "hidden": false
        },
        {
          "_id": "6837e51d05c81fd7d7d1962f",
          "name": "Ayan Sengupta",
          "hidden": false
        },
        {
          "_id": "6837e51d05c81fd7d7d19630",
          "name": "Tanmoy Chakraborty",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T17:57:43.000Z",
      "submittedOnDailyAt": "2025-05-29T06:15:54.434Z",
      "title": "Début de la recherche : Accélération efficace du temps de test pour les modèles de langage grands",
      "submittedOnDailyBy": {
        "_id": "63ca499104c97982831127ec",
        "avatarUrl": "/avatars/816a962c62c805adf7789fa8e398b01e.svg",
        "isPro": false,
        "fullname": "Aradhye Agarwal",
        "user": "aradhye",
        "type": "user"
      },
      "summary": "L'accélération du temps de test (TTS) est un méthode potentielle pour améliorer l'efficacité des modèles de langage grands en temps d'inférence par la dynamique de l'attribution de la quantité de calculs. Les méthodes existantes de TTS sont efficaces, mais nécessitent de longues étapes de décodage, la génération de nombreux exemples, et un augmentation de la quantité de tokens et du temps d'inférence. Nous avons constaté que les résultats fréquemment obtenus avec des traces courtes sont très précis, mais moins précis que ceux des traces longues. En fonction de ce constat, nous avons introduit une stratégie de décodage parallèle sans entraînement appelée First Finish Search (FFS). FFS commence avec n échantillons indépendants et retourne la première qui est terminée. FFS a évalué, en utilisant 4 modèles logiques (DeepSeek-R1, R1-Distill-Qwen-32B, QwQ-32B, Phi-4-Reasoning-Plus) et 4 ensembles de données (AIME24, AIME25-I, AIME25-II, GPQA Diamond), la décodage simple, la recherche de faisceau, la vote et la suppression de la séquence la plus probable. Dans DeepSeek-R1, FFS a atteint un 82.23% de précision sur l'ensemble AIME, améliorant le rendement individuel d'un 15% et montrant un comportement similaire à a4-mini de OpenAI. Analyse théorique : explique que l'on peut arrêter au plus court tracé pour obtenir une réponse précise et identifie les conditions où l'arrêt tôt n'est pas optimal. La beauté et la simplicité de FFS révèlent que une stratégie simple de TTS peut démontrer un comportement notable et révéler le potentiel d'une approximation simple en temps d'inférence.",
      "upvotes": 0,
      "discussionId": "6837e51d05c81fd7d7d19659",
      "ai_summary": "First Finish Search improves accuracy in large language models by stopping inference at the first completed sample, significantly outperforming other decoding strategies in reasoning tasks.",
      "ai_keywords": [
        "Test-time scaling",
        "TTS",
        "dynamic allocation",
        "inference",
        "reasoning tasks",
        "First Finish Search",
        "FFS",
        "parallel decoding",
        "decoding strategies",
        "beam search",
        "majority voting",
        "budget forcing",
        "accuracy",
        "performance",
        "inference latency",
        "early stopping"
      ]
    },
    "publishedAt": "2025-05-23T13:57:43.000Z",
    "title": "First Finish Search: Efficient Test-Time Scaling in Large Language\n  Models",
    "summary": "Test-time scaling (TTS), which involves dynamic allocation of compute during\ninference, offers a promising way to improve reasoning in large language\nmodels. While existing TTS methods work well, they often rely on long decoding\npaths or require a large number of samples to be generated, increasing the\ntoken usage and inference latency. We observe the surprising fact that for\nreasoning tasks, shorter traces are much more likely to be correct than longer\nones. Motivated by this, we introduce First Finish Search (FFS), a\ntraining-free parallel decoding strategy that launches n independent samples\nand returns as soon as any one completes. We evaluate FFS alongside simple\ndecoding, beam search, majority voting, and budget forcing on four reasoning\nmodels (DeepSeek-R1, R1-Distill-Qwen-32B, QwQ-32B and Phi-4-Reasoning-Plus) and\nacross four datasets (AIME24, AIME25-I, AIME25-II and GPQA Diamond). With\nDeepSeek-R1, FFS achieves 82.23% accuracy on the AIME datasets, a 15%\nimprovement over DeepSeek-R1's standalone accuracy, nearly matching OpenAI's\no4-mini performance. Our theoretical analysis explains why stopping at the\nshortest trace is likely to yield a correct answer and identifies the\nconditions under which early stopping may be suboptimal. The elegance and\nsimplicity of FFS demonstrate that straightforward TTS strategies can perform\nremarkably well, revealing the untapped potential of simple approaches at\ninference time.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18149.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63ca499104c97982831127ec",
      "avatarUrl": "/avatars/816a962c62c805adf7789fa8e398b01e.svg",
      "fullname": "Aradhye Agarwal",
      "name": "aradhye",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  }
]