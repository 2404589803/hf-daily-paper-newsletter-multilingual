[
  {
    "paper": {
      "id": "2502.20730",
      "authors": [
        {
          "_id": "67c514aba3d873e41624a082",
          "user": {
            "_id": "63664c8fa2abcdf2fd6425ed",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63664c8fa2abcdf2fd6425ed/IywpB0DXZ_twkmZmVSCCD.jpeg",
            "isPro": false,
            "fullname": "Li Zhuoqun",
            "user": "lzq2021",
            "type": "user"
          },
          "name": "Zhuoqun Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-03T08:07:26.218Z",
          "hidden": false
        },
        {
          "_id": "67c514aba3d873e41624a083",
          "user": {
            "_id": "64a4ceda9a90f701134189b7",
            "avatarUrl": "/avatars/859a189c5d2ae2fcb9aa2d79104fbfe7.svg",
            "isPro": false,
            "fullname": "Haiyang Yu",
            "user": "yhycai",
            "type": "user"
          },
          "name": "Haiyang Yu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-03T09:31:12.493Z",
          "hidden": false
        },
        {
          "_id": "67c514aba3d873e41624a084",
          "user": {
            "_id": "63ef664304b0e373992a2633",
            "avatarUrl": "/avatars/cba554ff88bd8b68ae51bea8ee991d13.svg",
            "isPro": false,
            "fullname": "Xuanang Chen",
            "user": "xuanang",
            "type": "user"
          },
          "name": "Xuanang Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-03T09:29:31.384Z",
          "hidden": false
        },
        {
          "_id": "67c514aba3d873e41624a085",
          "user": {
            "_id": "6711c702f858a456b4b9f3a4",
            "avatarUrl": "/avatars/178e9567c3111ab22717c3c0dd003a6a.svg",
            "isPro": false,
            "fullname": "Hongyu  Lin",
            "user": "sanmusunrise",
            "type": "user"
          },
          "name": "Hongyu Lin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-03T09:28:09.791Z",
          "hidden": false
        },
        {
          "_id": "67c514aba3d873e41624a086",
          "user": {
            "_id": "6216496a9b34d2fb49144599",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6216496a9b34d2fb49144599/41CKA_h1Ffj3RzVabSAkm.jpeg",
            "isPro": false,
            "fullname": "Yaojie Lu",
            "user": "luyaojie",
            "type": "user"
          },
          "name": "Yaojie Lu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-03T09:29:38.957Z",
          "hidden": false
        },
        {
          "_id": "67c514aba3d873e41624a087",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "67c514aba3d873e41624a088",
          "user": {
            "_id": "65e99a77e71555ed193609cf",
            "avatarUrl": "/avatars/38ceb127883944677665da967d17dd18.svg",
            "isPro": false,
            "fullname": "Xianpei Han",
            "user": "xphan",
            "type": "user"
          },
          "name": "Xianpei Han",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-03T09:29:51.007Z",
          "hidden": false
        },
        {
          "_id": "67c514aba3d873e41624a089",
          "user": {
            "_id": "66641b2fd8e1e34bc621e688",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66641b2fd8e1e34bc621e688/csPETwnx2zCIHSWi9uAi-.png",
            "isPro": false,
            "fullname": "Yongbin Li",
            "user": "Yongbin-Li",
            "type": "user"
          },
          "name": "Yongbin Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-03T09:29:57.561Z",
          "hidden": false
        },
        {
          "_id": "67c514aba3d873e41624a08a",
          "name": "Le Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-28T05:23:10.000Z",
      "title": "DeepSolution : Amélioration du design de solutions techniques d'ingénierie complexes grâce à l'exploration arborée et à deux formes de pensée",
      "summary": "La conception de solutions pour les problèmes complexes d'ingénierie est cruciale dans les activités productives humaines. Cependant, les recherches dans le domaine de la génération avec révision (Review-Augmented Generation, RAG) passées n'ont pas suffisamment abordé les tâches liées à la conception de solutions complexes d'ingénierie. Pour combler ce manque, nous présentons SolutionBench, un nouveau cadre de référence pour évaluer la capacité à générer des solutions complètes possibles pour les problèmes d'ingénierie. De plus, pour encourager le développement de la conception de solutions complexes d'ingénierie, nous proposons un nouveau système appelé SolutionRAG, qui utilise l'exploration d'arbres et des structures de pensée bidimensionnelles. A travers des résultats expérimentaux amplifiés, SolutionRAG atteint le meilleur rendement actuel (SOTA) sur SolutionBench et montre clairement son potentiel pour automatiser et améliorer la confiance dans la conception de solutions complexes d'ingénierie dans des applications globales.",
      "upvotes": 11,
      "discussionId": "67c514aca3d873e41624a10b"
    },
    "publishedAt": "2025-03-02T21:35:24.437Z",
    "title": "DeepSolution: Boosting Complex Engineering Solution Design via Tree-based Exploration and Bi-point Thinking",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63664c8fa2abcdf2fd6425ed/y_kT4GP3xgm-5RdguMNV7.png",
      "https://cdn-uploads.huggingface.co/production/uploads/63664c8fa2abcdf2fd6425ed/wDAS_USsxsVHbin1I5CEe.png",
      "https://cdn-uploads.huggingface.co/production/uploads/63664c8fa2abcdf2fd6425ed/4lJgWp9V8pm4vDBUH4I5n.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.20730.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "63664c8fa2abcdf2fd6425ed",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63664c8fa2abcdf2fd6425ed/IywpB0DXZ_twkmZmVSCCD.jpeg",
      "fullname": "Li Zhuoqun",
      "name": "lzq2021",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.18600",
      "authors": [
        {
          "_id": "67c0a8058589d8ecb79d472b",
          "user": {
            "_id": "6594b1bb57a556fbe162915e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6594b1bb57a556fbe162915e/WuYxqbbvaJaT-xsk5KhoT.jpeg",
            "isPro": false,
            "fullname": "Silei Xu",
            "user": "sileixu",
            "type": "user"
          },
          "name": "Silei Xu",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-02-27T18:01:14.543Z",
          "hidden": false
        },
        {
          "_id": "67c0a8058589d8ecb79d472c",
          "name": "Wenhao Xie",
          "hidden": false
        },
        {
          "_id": "67c0a8058589d8ecb79d472d",
          "name": "Lingxiao Zhao",
          "hidden": false
        },
        {
          "_id": "67c0a8058589d8ecb79d472e",
          "user": {
            "_id": "5efd09cf49ed724c8a135868",
            "avatarUrl": "/avatars/af12bc94657979677a9f26183f0c9727.svg",
            "isPro": false,
            "fullname": "Pengcheng He",
            "user": "DeBERTa",
            "type": "user"
          },
          "name": "Pengcheng He",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-03T09:30:43.479Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-25T19:36:06.000Z",
      "title": "Une rapide guide pour penser rapidement avec un plan de bord court",
      "summary": "Les modèles de langage grands (LLMs) montrent des résultats exceptionnels dans la résolution de problèmes de raisonnement complexe grâce à des structures comme l'entraînement de la chaîne de pensée (CoT). Cependant, l'humanité souvent préfère des stratégies plus efficaces : écrire des pensées intermédiaires résumées et extraire de ces dernières seulement l'information essentielle. Dans cet article, nous proposons un nouveau paradigme appelé \"Chaîne de Suppression (CoD)\" qui inspire le processus cognitif humain, permettant aux LLMs d'émettre une information minimale de raisonnement intermédiaire tout en résolvant des tâches. Ainsi, la bureaucratie est réduite et les points de vue importants sont mis en avant, atteignant des résultats précis comparables à ceux de CoT, tout en réduisant significativement les coûts et la latence dans les tâches de raisonnement complexe, en utilisant seulement 7,6% de tokens.",
      "upvotes": 6,
      "discussionId": "67c0a8078589d8ecb79d47ed"
    },
    "publishedAt": "2025-03-03T02:35:09.967Z",
    "title": "Chain of Draft: Thinking Faster by Writing Less",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.18600.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "63da3d7ae697e5898cb86854",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1675246771355-noauth.jpeg",
      "fullname": "Talha Rüzgar Akkuş",
      "name": "Q-bert",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 85
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.18017",
      "authors": [
        {
          "_id": "67bef5a6070ec160042d99f4",
          "user": {
            "_id": "657429d833e5a4bf5b278615",
            "avatarUrl": "/avatars/ed7e28c1b9a7bed1cad864c992cdcc69.svg",
            "isPro": false,
            "fullname": "QiuchenWang",
            "user": "autumncc",
            "type": "user"
          },
          "name": "Qiuchen Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-28T12:15:57.850Z",
          "hidden": false
        },
        {
          "_id": "67bef5a6070ec160042d99f5",
          "name": "Ruixue Ding",
          "hidden": false
        },
        {
          "_id": "67bef5a6070ec160042d99f6",
          "user": {
            "_id": "64892d31cbda0d1cdb956897",
            "avatarUrl": "/avatars/3cdafe03a8295124636347d15a099aaf.svg",
            "isPro": false,
            "fullname": "Zehui Chen",
            "user": "lovesnowbest",
            "type": "user"
          },
          "name": "Zehui Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-03T09:32:18.129Z",
          "hidden": false
        },
        {
          "_id": "67bef5a6070ec160042d99f7",
          "user": {
            "_id": "65351cbe6141b3927afaed17",
            "avatarUrl": "/avatars/5abf5f2c4ab329e63a7f45c15c9dfb93.svg",
            "isPro": false,
            "fullname": "weiqi wu",
            "user": "vickywu",
            "type": "user"
          },
          "name": "Weiqi Wu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-03T09:32:12.075Z",
          "hidden": false
        },
        {
          "_id": "67bef5a6070ec160042d99f8",
          "user": {
            "_id": "62e8efb14210d3fe69eacb42",
            "avatarUrl": "/avatars/2feadd75274bf353b910f4679ef72b39.svg",
            "isPro": false,
            "fullname": "Shihang Wang",
            "user": "shihang",
            "type": "user"
          },
          "name": "Shihang Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-03T09:32:05.679Z",
          "hidden": false
        },
        {
          "_id": "67bef5a6070ec160042d99f9",
          "user": {
            "_id": "63a091e42fabbbb89991f5ce",
            "avatarUrl": "/avatars/d55485b06461764c36c9edf9d6e8892c.svg",
            "isPro": false,
            "fullname": "pengjun xie",
            "user": "xpjandy",
            "type": "user"
          },
          "name": "Pengjun Xie",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-03T09:31:59.813Z",
          "hidden": false
        },
        {
          "_id": "67bef5a6070ec160042d99fa",
          "name": "Feng Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-25T09:26:12.000Z",
      "title": "ViDoRAG : Agente de raisonnement itératif dynamique pour l'expansion et la génération de la recherche de documents visuels",
      "summary": "Comprendre des informations dans des documents riches en visuels est un problème important dans l'approche traditionnelle de la génération augmentée par recherche (RAG). Les benchmarks actuels se concentrent principalement sur la réponse à des questions basées sur des images, ignorant les problèmes fondamentaux de recherche efficace, compréhension et recherche de raisons dans des documents visuellement complexes. Pour combler ces lacunes, on a introduit le nouveau dataset ViDoSeek, offrant des méthodes d'évaluation appropriées pour des documents riches en visuels qui nécessitent des raisons complexes. Ainsi, on a identifié les principales limitations de l'approche actuelle de RAG : (i) les méthodes de recherche visuelles simples ont des difficultés à intégrer efficacement des caractéristiques visuelles et textuelles, et (ii) les approches actuelles ne souvent pas suffisamment attribuent de tokens de raisons, ce qui limite leur efficacité. Pour résoudre ces problèmes, on propose ViDoRAG, un nouveau cadre de RAG multi-agente. ViDoRAG nécessite des raisons complexes pour des documents riches en visuels et utilise un modèle hybride basé sur le Modèle de mélange de Gaussien (GMM) pour traiter efficacement des recherches multi-modèle. Pour améliorer la capacité de raisonnement du modèle, on introduit un flux de travail itératif d'agents qui inclut l'exploration, le résumé et la réflexion, et on fournit un cadre pour échelonner le temps de test dans le domaine de RAG. Les expériences étendues sur ViDoSeek montrent l'efficacité et la capacité de généralisation de notre approche. En particulier, ViDoRAG a montré un effet compétitif avec les méthodes actuelles sur le benchmark ViDoSeek, améliorant les résultats d'un 10% ou plus.",
      "upvotes": 4,
      "discussionId": "67bef5a7070ec160042d9a65"
    },
    "publishedAt": "2025-03-02T22:22:01.895Z",
    "title": "ViDoRAG: Visual Document Retrieval-Augmented Generation via Dynamic Iterative Reasoning Agents",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.18017.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "657429d833e5a4bf5b278615",
      "avatarUrl": "/avatars/ed7e28c1b9a7bed1cad864c992cdcc69.svg",
      "fullname": "QiuchenWang",
      "name": "autumncc",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.20545",
      "authors": [
        {
          "_id": "67c51b459d5807d6674b3d3c",
          "name": "Kechen Li",
          "hidden": false
        },
        {
          "_id": "67c51b459d5807d6674b3d3d",
          "name": "Wenqi Zhu",
          "hidden": false
        },
        {
          "_id": "67c51b459d5807d6674b3d3e",
          "name": "Coralia Cartis",
          "hidden": false
        },
        {
          "_id": "67c51b459d5807d6674b3d3f",
          "user": {
            "_id": "64bb61e876a6e2efcc728e22",
            "avatarUrl": "/avatars/b0ed1c9f13fd1f2c99d202155001e39b.svg",
            "isPro": false,
            "fullname": "Tianbo Ji",
            "user": "jitianbo",
            "type": "user"
          },
          "name": "Tianbo Ji",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-03T09:35:49.782Z",
          "hidden": false
        },
        {
          "_id": "67c51b459d5807d6674b3d40",
          "name": "Shiwei Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-27T21:41:43.000Z",
      "title": "SoS1: O1 y R1-Like Reasoning LLMs son responsables de résoudre des problèmes de sommes de carrés.",
      "summary": "Les modèles de langage grand (LLMs) atteignent une excellente capacité cognitif humaine dans diverses tâches, mais leur capacité à résoudre des problèmes mathématiques de manière rigoureuse reste un défi ouvert. Dans cette étude, on aborde des problèmes de calcul de base mais complexes, notamment la détermination si un polynôme multivariable est positif ou non. Ce problème est directement lié à la 17ème question de Hilbert et joue un rôle crucial dans l'optimisation globale des polynômes, étant utilisé dans de nombreuses domaines.\n\nTout d'abord, on présente le dataset SoS-1K, un ensemble de données refiné et structuré. Ce dataset inclut des guides logiques conçus par des experts basés sur cinq critères d'évaluation de difficulté progressive. En utilisant ce dataset, on a évalué les plus récents modèles de LLMs. Sans guides logiques structurés, tous les modèles dépassent la base aléatoire de 50% seulement légèrement. Cependant, les guides logiques de haute qualité améliorent significativement la précision, augmentant le rendement de 81%. De plus, grâce à un ajustement de 4 heures sur SoS-1K, le modèle SoS-7B, avec 7B paramètres, dépasse la précision de DeepSeek-V3 et GPT-4o-mini, nécessitant respectivement seulement 1,8% et 5% du temps de calcul.\n\nLes résultats de cette étude montrent que les modèles de LLMs peuvent dépasser les limites des guides logiques mathématiques et montrent la possibilité de résoudre des problèmes NP-hard.",
      "upvotes": 2,
      "discussionId": "67c51b469d5807d6674b3d88"
    },
    "publishedAt": "2025-03-02T22:00:31.796Z",
    "title": "SoS1: O1 and R1-Like Reasoning LLMs are Sum-of-Square Solvers",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.20545.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6260
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.20396",
      "authors": [
        {
          "_id": "67c51d36c830dcb76bbb5994",
          "name": "Toru Lin",
          "hidden": false
        },
        {
          "_id": "67c51d36c830dcb76bbb5995",
          "name": "Kartik Sachdev",
          "hidden": false
        },
        {
          "_id": "67c51d36c830dcb76bbb5996",
          "name": "Linxi Fan",
          "hidden": false
        },
        {
          "_id": "67c51d36c830dcb76bbb5997",
          "user": {
            "_id": "65369a95605a07338de78ab0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/sGFjOjLT2akN-sn5beVWL.jpeg",
            "isPro": false,
            "fullname": "Jitendra Malik ",
            "user": "jitendra1995",
            "type": "user"
          },
          "name": "Jitendra Malik",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-03T09:36:34.177Z",
          "hidden": false
        },
        {
          "_id": "67c51d36c830dcb76bbb5998",
          "name": "Yuke Zhu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-27T18:59:52.000Z",
      "title": "Simulation of a visual detector based on articulated body models using reinforcement learning in a real mobile robot",
      "summary": "L'apprentissage par référence est capable de réaliser des résultats souhaités dans divers domaines de problèmes, mais le succès dans la manipulation de robotique spécialisée est limité. Dans cette recherche, nous explorons les principales difficultés dans l'application de l'apprentissage par référence pour résoudre des tâches de travail impliquant un contact abondant avec la structure corporelle d'un jouet. Nous présentons une nouvelle méthodologie pour réaliser des compléments cognitifs et nous testons son efficacité. Notre contribution principale comprend un module d'ajuste automatique qui s'ajuste aux environnements de simulation, un conception de récompenses généralisées pour simplifier l'obtention de récompenses à long terme dans des tâches de travail avec contact abondant, un processus d'apprentissage des attaques en différents pas qui améliore l'efficacité de l'exploration et maintient le rendement dans le monde réel à partir de la simulation, et une représentation mixte d'objets rares et denses pour concrétiser les erreurs cognitives depuis la simulation. Nous démontrons les résultats souhaités dans trois tâches de travail spécialisées dans les jouets et effectuons un étude des méthodes utilisées. Notre travail fournit une approche réussie pour l'apprentissage par référence de la simulation au monde réel, atteignant une forte généralisation et de hauts rendements sans la nécessité de guidance humaine.",
      "upvotes": 1,
      "discussionId": "67c51d39c830dcb76bbb5a1f"
    },
    "publishedAt": "2025-03-02T22:08:44.891Z",
    "title": "Sim-to-Real Reinforcement Learning for Vision-Based Dexterous Manipulation on Humanoids",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.20396.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6260
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.20811",
      "authors": [
        {
          "_id": "67c51c198d02783fa3a6249d",
          "name": "Xiao Wang",
          "hidden": false
        },
        {
          "_id": "67c51c198d02783fa3a6249e",
          "name": "Jingyun Hua",
          "hidden": false
        },
        {
          "_id": "67c51c198d02783fa3a6249f",
          "user": {
            "_id": "675a69699e086bd6250a36ef",
            "avatarUrl": "/avatars/95c72e3975d1a37f8655a2fe629746ec.svg",
            "isPro": false,
            "fullname": "Weihong Lin",
            "user": "lwher1996",
            "type": "user"
          },
          "name": "Weihong Lin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-03T09:42:30.547Z",
          "hidden": false
        },
        {
          "_id": "67c51c198d02783fa3a624a0",
          "name": "Yuanxing Zhang",
          "hidden": false
        },
        {
          "_id": "67c51c198d02783fa3a624a1",
          "name": "Fuzheng Zhang",
          "hidden": false
        },
        {
          "_id": "67c51c198d02783fa3a624a2",
          "name": "Jianlong Wu",
          "hidden": false
        },
        {
          "_id": "67c51c198d02783fa3a624a3",
          "name": "Di Zhang",
          "hidden": false
        },
        {
          "_id": "67c51c198d02783fa3a624a4",
          "name": "Liqiang Nie",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-28T07:53:40.000Z",
      "title": "HAIC : Utilisation d'images améliorées pour améliorer la compréhension et la génération d'actions humaines dans un modèle de langage multimodal",
      "summary": "Les modèles de langue multimodal (MLLMs) ont réalisé un grand progrès dans la compréhension des films. Cependant, le rendement de ces modèles dans la compréhension du comportement humain est limité par la rareté de données de haute qualité. Pour aborder ce problème, nous avons introduit un processus de description de données en deux étapes. Tout d'abord, nous avons conceu une stratégie pour collecter des films qui incluent clairement le comportement humain depuis le web. Ensuite, nous avons utilisé des caractéristiques humaines pour différencier les individus et décrire leurs actions et interactions avec des détails temporels dans un format de sous-titres standard. Ce processus nous a permis de créer deux ensembles de données : HAICTrain et HAICBench. HAICTrain a été généré avec Gemini-Pro et est composé de 126K paires de films-sous-titres pour des fins d'entraînement. Par contre, HAICBench inclut 500 paires de films-sous-titres générés automatiquement et 1,400 paires de questions et réponses, offrant une évaluation détaillée de la compréhension du comportement humain. Les résultats des tests montrent que l'entraînement avec HAICTrain améliore significativement la capacité de compréhension humaine dans quatre référentiels et améliore les résultats de génération de films. HAICTrain et HAICBench sont disponibles sur https://huggingface.co/datasets/KuaishouHAIC/HAIC.",
      "upvotes": 1,
      "discussionId": "67c51c1b8d02783fa3a62543"
    },
    "publishedAt": "2025-03-02T22:04:15.087Z",
    "title": "HAIC: Improving Human Action Understanding and Generation with Better Captions for Multi-modal Large Language Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.20811.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6260
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.20583",
      "authors": [
        {
          "_id": "67c516998d02783fa3a52dc8",
          "user": {
            "_id": "6304ac1a412a1b9d381ca378",
            "avatarUrl": "/avatars/f4724eb5afc2a3b0e61e6da7bfa7be27.svg",
            "isPro": false,
            "fullname": "Keisuke Kamahori",
            "user": "kamahori",
            "type": "user"
          },
          "name": "Keisuke Kamahori",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-03T08:07:02.986Z",
          "hidden": false
        },
        {
          "_id": "67c516998d02783fa3a52dc9",
          "user": {
            "_id": "62908273c740ebb981a6dba4",
            "avatarUrl": "/avatars/465f50369c367b07670f5209c83d65f2.svg",
            "isPro": false,
            "fullname": "Jungo Kasai",
            "user": "jungok",
            "type": "user"
          },
          "name": "Jungo Kasai",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-03T09:43:49.097Z",
          "hidden": false
        },
        {
          "_id": "67c516998d02783fa3a52dca",
          "user": {
            "_id": "628c26a8b80bb09700d6af86",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1653352051245-noauth.jpeg",
            "isPro": false,
            "fullname": "Noriyuki Kojima",
            "user": "kojimano",
            "type": "user"
          },
          "name": "Noriyuki Kojima",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-03T09:43:56.698Z",
          "hidden": false
        },
        {
          "_id": "67c516998d02783fa3a52dcb",
          "user": {
            "_id": "654132fe5a9a913c6c870e79",
            "avatarUrl": "/avatars/2f6807eddef1929c571977e9af35f952.svg",
            "isPro": false,
            "fullname": "Baris Kasikci",
            "user": "kasikci",
            "type": "user"
          },
          "name": "Baris Kasikci",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-03T09:44:04.084Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-27T22:52:21.000Z",
      "title": "LiteASR : Utilisation d'approximations de faible qualité pour un reconnaissance vocale efficace",
      "summary": "Dans les modèles modernes de reconnaissance automatique du langage (RAL), comme Whisper de OpenAI, des architectures profondes encoder-decoder sont utilisées. Ces encodeurs jouent un rôle crucial pour le traitement efficace, fonctionnant avec de grandes quantités de calculs. Nous présentons LiteASR, une technique de compression de faible rendement pour l'encodeur d'un RAL. Cette méthodologie permet de réduire les coûts d'inférence tout en maintenant la précision de lecture. Notre approche est basée sur la propriété forte de faible rendement des activations intermédiaires : nous appliquons l'analyse en composantes principales (PCA) sur de petits ensembles de données d'apprentissage, et nous approximons la transformation linéaire comme une séquence de multiplications de matrices de faible rendement, ce qui permet d'optimiser l'attention sur les symboles. Les résultats d'évaluation montrent que notre méthode peut réduire la taille de l'encodeur de Whisper large-v3 d'au-delà de 50%, tout en maintenant ou même améliorant la précision de lecture, ce qui établit une nouvelle frontière de Pareto entre efficacité et rendement. Le code de LiteASR est disponible sur GitHub : https://github.com/efeslab/LiteASR.",
      "upvotes": 1,
      "discussionId": "67c516998d02783fa3a52dfd"
    },
    "publishedAt": "2025-03-02T21:48:46.577Z",
    "title": "LiteASR: Efficient Automatic Speech Recognition with Low-Rank Approximation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.20583.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6304ac1a412a1b9d381ca378",
      "avatarUrl": "/avatars/f4724eb5afc2a3b0e61e6da7bfa7be27.svg",
      "fullname": "Keisuke Kamahori",
      "name": "kamahori",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.19577",
      "authors": [
        {
          "_id": "67c42356054ae6d1c760b643",
          "user": {
            "_id": "66588b6fd22637bfab498709",
            "avatarUrl": "/avatars/9007f0d3b078bd6193912a5359107f24.svg",
            "isPro": false,
            "fullname": "Hugues Turbé",
            "user": "hturbe",
            "type": "user"
          },
          "name": "Hugues Turbé",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-02T20:15:04.391Z",
          "hidden": false
        },
        {
          "_id": "67c42356054ae6d1c760b644",
          "name": "Mina Bjelogrlic",
          "hidden": false
        },
        {
          "_id": "67c42356054ae6d1c760b645",
          "name": "Gianmarco Mengaldo",
          "hidden": false
        },
        {
          "_id": "67c42356054ae6d1c760b646",
          "name": "Christian Lovis",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-26T21:40:30.000Z",
      "title": "Bonjour ! Voici la traduction en français :\n\n\"Bonjour ! Voici la traduction en français du document en anglais spécifié.\"",
      "summary": "Les modèles de la Fondation Visuelle (VFMs) célèbrent des performances élevées avec la dernière technologie et se popularisent. Cependant, l'explicabilité est cruciale. Dans ce contexte, les modèles autoexplicatifs (SEM) visent à décomposer la prédiction en un ensemble de poids de concepts interprétables et à fournir un classifieur de classes explicables. Des preuves de la manque d'explicabilité ont été présentées dans des études récentes. Dans ce travail, un approche nommée ProtoFM est proposée, qui combine VFMs avec une nouvelle architecture et des objectifs de formation spécifiques. Avec cette approche, un petit ensemble de poids (environ 1M paramètres) est entraîné sur VFMs, ce qui offre une solution efficace et explicable. Les résultats d'évaluation montrent que notre approche dépasse les modèles actuels sur une large gamme d'indicateurs d'explicabilité, tout en atteignant un rendement relatif dans la classification de classes. Le code est disponible sur https://github.com/hturbe/proto-fm.",
      "upvotes": 0,
      "discussionId": "67c4235c054ae6d1c760b806"
    },
    "publishedAt": "2025-03-03T04:21:42.563Z",
    "title": "Tell me why: Visual foundation models as self-explainable classifiers",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/66588b6fd22637bfab498709/4VG_eDtZKZ4kj1AdG_P14.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.19577.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66588b6fd22637bfab498709",
      "avatarUrl": "/avatars/9007f0d3b078bd6193912a5359107f24.svg",
      "fullname": "Hugues Turbé",
      "name": "hturbe",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]