[
  {
    "paper": {
      "id": "2505.09388",
      "authors": [
        {
          "_id": "68299e3128752b51372d31ea",
          "user": {
            "_id": "62088594a5943c8a8fc94560",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1644733028938-62088594a5943c8a8fc94560.png",
            "isPro": false,
            "fullname": "An Yang",
            "user": "yangapku",
            "type": "user"
          },
          "name": "An Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-19T06:43:00.733Z",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d31eb",
          "user": {
            "_id": "6799128b9da39716ab1ebd95",
            "avatarUrl": "/avatars/677d8ae2087137134c3f0e58f4cf769f.svg",
            "isPro": false,
            "fullname": "Anfeng Li",
            "user": "laf070810",
            "type": "user"
          },
          "name": "Anfeng Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:15:44.771Z",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d31ec",
          "user": {
            "_id": "64b0a77df12b47366663884c",
            "avatarUrl": "/avatars/a212ea862abb5966060e439dd0e7656f.svg",
            "isPro": false,
            "fullname": "Baosong Yang",
            "user": "Baosong",
            "type": "user"
          },
          "name": "Baosong Yang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:15:37.853Z",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d31ed",
          "user": {
            "_id": "64b93578ee257c3a4cfceed1",
            "avatarUrl": "/avatars/e6188562254f75a09b4048b800860016.svg",
            "isPro": false,
            "fullname": "Beichen Zhang",
            "user": "BeichenZhang",
            "type": "user"
          },
          "name": "Beichen Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:16:13.672Z",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d31ee",
          "user": {
            "_id": "61e4c4ca1ab24785ac11ba69",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61e4c4ca1ab24785ac11ba69/1Q1zhhyGSJ9RJG9MzwxVv.jpeg",
            "isPro": false,
            "fullname": "Binyuan Hui",
            "user": "huybery",
            "type": "user"
          },
          "name": "Binyuan Hui",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:16:22.151Z",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d31ef",
          "name": "Bo Zheng",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d31f0",
          "user": {
            "_id": "6583ab7983a9e1460c67d876",
            "avatarUrl": "/avatars/74400bc448c3f07e23a4cd53d68a6af7.svg",
            "isPro": false,
            "fullname": "bowen",
            "user": "bowenYu",
            "type": "user"
          },
          "name": "Bowen Yu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:16:31.453Z",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d31f1",
          "name": "Chang Gao",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d31f2",
          "name": "Chengen Huang",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d31f3",
          "name": "Chenxu Lv",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d31f4",
          "user": {
            "_id": "610b70452719facd4ea85e28",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/610b70452719facd4ea85e28/S7nMy7D0Rxq0VIVblhYDG.jpeg",
            "isPro": false,
            "fullname": "Chujie Zheng",
            "user": "chujiezheng",
            "type": "user"
          },
          "name": "Chujie Zheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-19T06:43:04.798Z",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d31f5",
          "user": {
            "_id": "6434d4989bd5a84b5dd0b0f5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6434d4989bd5a84b5dd0b0f5/0Elf9qbfG9Hkgypm9pTGm.jpeg",
            "isPro": false,
            "fullname": "Dayiheng Liu",
            "user": "Losin94",
            "type": "user"
          },
          "name": "Dayiheng Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:17:32.677Z",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d31f6",
          "name": "Fan Zhou",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d31f7",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d31f8",
          "name": "Feng Hu",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d31f9",
          "name": "Hao Ge",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d31fa",
          "user": {
            "_id": "6436618aeef1f55654a9f458",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6436618aeef1f55654a9f458/OvxGtuDg2GAFG9As-2hzW.jpeg",
            "isPro": false,
            "fullname": "Haoran Wei",
            "user": "HaoranWei",
            "type": "user"
          },
          "name": "Haoran Wei",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:17:56.110Z",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d31fb",
          "name": "Huan Lin",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d31fc",
          "user": {
            "_id": "63281d05ac205d01918b5fc7",
            "avatarUrl": "/avatars/fc3e0f7285bb2869a92670f764dfc535.svg",
            "isPro": false,
            "fullname": "Jialong Tang",
            "user": "Jialong",
            "type": "user"
          },
          "name": "Jialong Tang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:18:16.959Z",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d31fd",
          "name": "Jian Yang",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d31fe",
          "user": {
            "_id": "654bead777401b47e6424f88",
            "avatarUrl": "/avatars/7bcbdbb051c93b004f0dc3ad36c4a0ce.svg",
            "isPro": false,
            "fullname": "Jianhong Tu",
            "user": "ToviTu",
            "type": "user"
          },
          "name": "Jianhong Tu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:18:30.045Z",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d31ff",
          "name": "Jianwei Zhang",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3200",
          "name": "Jianxin Yang",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3201",
          "name": "Jiaxi Yang",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3202",
          "name": "Jing Zhou",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3203",
          "user": {
            "_id": "602f88f5e8149a962412a667",
            "avatarUrl": "/avatars/b78f0e583df8e5d5e3365934fe5f4900.svg",
            "isPro": false,
            "fullname": "Zhou",
            "user": "Jingren",
            "type": "user"
          },
          "name": "Jingren Zhou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:20:51.253Z",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3204",
          "name": "Junyang Lin",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3205",
          "name": "Kai Dang",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3206",
          "name": "Keqin Bao",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3207",
          "name": "Kexin Yang",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3208",
          "name": "Le Yu",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3209",
          "name": "Lianghao Deng",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d320a",
          "name": "Mei Li",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d320b",
          "user": {
            "_id": "5f8946925d083370c711f296",
            "avatarUrl": "/avatars/14246aae3b1f8b7ad050f8ff2c8b260e.svg",
            "isPro": false,
            "fullname": "Mingfeng Xue",
            "user": "mingfengxue",
            "type": "user"
          },
          "name": "Mingfeng Xue",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:21:56.048Z",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d320c",
          "name": "Mingze Li",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d320d",
          "name": "Pei Zhang",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d320e",
          "user": {
            "_id": "62f220ccee7d7af44979efc7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f220ccee7d7af44979efc7/RImNglMumGCpAKB5gin6k.jpeg",
            "isPro": false,
            "fullname": "Peng Wang",
            "user": "ZJUPeng",
            "type": "user"
          },
          "name": "Peng Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-19T06:43:02.813Z",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d320f",
          "name": "Qin Zhu",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3210",
          "name": "Rui Men",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3211",
          "user": {
            "_id": "6629ed94aabce1b25c3db90c",
            "avatarUrl": "/avatars/cbc39db81c8e8f950d3bd2c2e03f71c8.svg",
            "isPro": false,
            "fullname": "Ruize Gao",
            "user": "gaoruize",
            "type": "user"
          },
          "name": "Ruize Gao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:21:46.295Z",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3212",
          "name": "Shixuan Liu",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3213",
          "name": "Shuang Luo",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3214",
          "name": "Tianhao Li",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3215",
          "name": "Tianyi Tang",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3216",
          "name": "Wenbiao Yin",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3217",
          "name": "Xingzhang Ren",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3218",
          "name": "Xinyu Wang",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3219",
          "name": "Xinyu Zhang",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d321a",
          "name": "Xuancheng Ren",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d321b",
          "name": "Yang Fan",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d321c",
          "name": "Yang Su",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d321d",
          "name": "Yichang Zhang",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d321e",
          "name": "Yinger Zhang",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d321f",
          "name": "Yu Wan",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3220",
          "user": {
            "_id": "666aacfb918ba11c7c598194",
            "avatarUrl": "/avatars/45bee8f1fdbdd256ee47d25e4bf01a7a.svg",
            "isPro": false,
            "fullname": "Yuqiong Liu",
            "user": "lyq333",
            "type": "user"
          },
          "name": "Yuqiong Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:20:06.363Z",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3221",
          "name": "Zekun Wang",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3222",
          "user": {
            "_id": "672c25ca8cfb61188128eb6f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/FJWy9Tt7UQmu9KcTOx3Rt.png",
            "isPro": false,
            "fullname": "Zeyu Cui",
            "user": "misakamage",
            "type": "user"
          },
          "name": "Zeyu Cui",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:19:43.843Z",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3223",
          "name": "Zhenru Zhang",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3224",
          "name": "Zhipeng Zhou",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3225",
          "user": {
            "_id": "647ccbd6e07cf9bb2d485244",
            "avatarUrl": "/avatars/e8915abaff04f6762247e196b7cf84df.svg",
            "isPro": false,
            "fullname": "Zihan Qiu",
            "user": "QwQZh",
            "type": "user"
          },
          "name": "Zihan Qiu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:18:58.545Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-14T13:41:34.000Z",
      "submittedOnDailyAt": "2025-05-19T01:23:20.310Z",
      "title": "Rapport Technique Qwen3\n\n(Veuillez noter que, bien que vous ayiez demandé seulement le résultat de la traduction, pour assurer la profondeur et la précision, je fournirai une traduction complète, maintenant la structure et le format du texte original.)\n\nRapport Technique Qwen3\n\nLe Rapport Technique Qwen3 détaille précisément les technologies clés et le rendement du modèle Qwen3, avec l'objectif de fournir des informations utiles aux utilisateurs et aux chercheurs. Ce rapport inclut le contexte du développement, les éléments techniques, l'évaluation du rendement et les perspectives futures de Qwen3, aidant ainsi aux utilisateurs et aux chercheurs à mieux comprendre le développement et les orientations de la technologie de l'intelligence artificielle la plus récente.",
      "submittedOnDailyBy": {
        "_id": "610b70452719facd4ea85e28",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/610b70452719facd4ea85e28/S7nMy7D0Rxq0VIVblhYDG.jpeg",
        "isPro": false,
        "fullname": "Chujie Zheng",
        "user": "chujiezheng",
        "type": "user"
      },
      "summary": "Dans ce travail, nous présentons Qwen3, la dernière version du célèbre modèle de langue Qwen. Qwen3 a été conçu pour améliorer le rendement, l'efficacité et les capacités multilingues d'une série de grands modèles de langue (LLMs). La série Qwen3 comprend des architectures tant denses que de type Mixture-of-Expert (MoE), avec un nombre de paramètres allant de 0,6 millions à 235 millions. L'une des innovations les plus importantes de Qwen3 est l'intégration d'un mode de mémoire pour l'inférence de cas complexes et d'un mode sans mémoire qui répond rapidement en fonction du contexte, ce qui permet à la série Qwen3 d'être un seul cadre de travail pour des modèles optimisés pour le chat (comme GPT-4o) et des modèles d'inférence spécialisés (comme QwQ-32B), évitant la nécessité d'échanger entre différents modèles. De plus, Qwen3 permet un échange dynamique de modes selon les demandes de l'utilisateur ou les templates de chat, et lors de l'inférence, assigne de manière adaptative les ressources de la réseau de calcul, maintenant un équilibre entre la complexité de la tâche, la latence et le rendement. De plus, Qwen3 exploite le savoir du modèle principal pour réduire significativement les ressources de réseau de calcul nécessaires pour la construction de modèles petits, ce qui permet d'améliorer leur rendement. Selon les évaluations expérimentales, Qwen3 obtient les meilleurs résultats dans différents cadres de référence, comme la génération de code, l'inférence mathématique et les tâches d'agent, et concurrence avec des modèles plus grands ou spécialisés. En comparaison avec Qwen2.5, Qwen3 a étendu le support des langues de 29 à 119, améliorant la compréhension et la génération critique multilingues, ce qui a augmenté son accessibilité mondiale. Tous les modèles de Qwen3 sont disponibles sous la licence Apache 2.0 de logiciel libre, avec l'objectif de promouvoir la reproductibilité et le développement dirigé par la communauté.",
      "upvotes": 70,
      "discussionId": "68299e3228752b51372d325f",
      "projectPage": "https://qwenlm.github.io/blog/qwen3/",
      "githubRepo": "https://github.com/QwenLM/Qwen3",
      "ai_keywords": [
        "large language models (LLMs)",
        "Mixture-of-Expert (MoE) architectures",
        "thinking mode",
        "non-thinking mode",
        "chat-optimized models",
        "dedicated reasoning models",
        "thinking budget mechanism",
        "computational resources adaptively",
        "inference",
        "latency",
        "performance",
        "code generation",
        "mathematical reasoning",
        "agent tasks",
        "multilingual support",
        "cross-lingual understanding",
        "generation capabilities"
      ]
    },
    "publishedAt": "2025-05-14T09:41:34.000Z",
    "title": "Qwen3 Technical Report",
    "summary": "In this work, we present Qwen3, the latest version of the Qwen model family.\nQwen3 comprises a series of large language models (LLMs) designed to advance\nperformance, efficiency, and multilingual capabilities. The Qwen3 series\nincludes models of both dense and Mixture-of-Expert (MoE) architectures, with\nparameter scales ranging from 0.6 to 235 billion. A key innovation in Qwen3 is\nthe integration of thinking mode (for complex, multi-step reasoning) and\nnon-thinking mode (for rapid, context-driven responses) into a unified\nframework. This eliminates the need to switch between different models--such as\nchat-optimized models (e.g., GPT-4o) and dedicated reasoning models (e.g.,\nQwQ-32B)--and enables dynamic mode switching based on user queries or chat\ntemplates. Meanwhile, Qwen3 introduces a thinking budget mechanism, allowing\nusers to allocate computational resources adaptively during inference, thereby\nbalancing latency and performance based on task complexity. Moreover, by\nleveraging the knowledge from the flagship models, we significantly reduce the\ncomputational resources required to build smaller-scale models, while ensuring\ntheir highly competitive performance. Empirical evaluations demonstrate that\nQwen3 achieves state-of-the-art results across diverse benchmarks, including\ntasks in code generation, mathematical reasoning, agent tasks, etc.,\ncompetitive against larger MoE models and proprietary models. Compared to its\npredecessor Qwen2.5, Qwen3 expands multilingual support from 29 to 119\nlanguages and dialects, enhancing global accessibility through improved\ncross-lingual understanding and generation capabilities. To facilitate\nreproducibility and community-driven research and development, all Qwen3 models\nare publicly accessible under Apache 2.0.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.09388.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "610b70452719facd4ea85e28",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/610b70452719facd4ea85e28/S7nMy7D0Rxq0VIVblhYDG.jpeg",
      "fullname": "Chujie Zheng",
      "name": "chujiezheng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 37
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.10610",
      "authors": [
        {
          "_id": "682adaf581c740ab4aabc5a3",
          "name": "Zhaowei Wang",
          "hidden": false
        },
        {
          "_id": "682adaf581c740ab4aabc5a4",
          "name": "Wenhao Yu",
          "hidden": false
        },
        {
          "_id": "682adaf581c740ab4aabc5a5",
          "name": "Xiyu Ren",
          "hidden": false
        },
        {
          "_id": "682adaf581c740ab4aabc5a6",
          "name": "Jipeng Zhang",
          "hidden": false
        },
        {
          "_id": "682adaf581c740ab4aabc5a7",
          "name": "Yu Zhao",
          "hidden": false
        },
        {
          "_id": "682adaf581c740ab4aabc5a8",
          "name": "Rohit Saxena",
          "hidden": false
        },
        {
          "_id": "682adaf581c740ab4aabc5a9",
          "name": "Liang Cheng",
          "hidden": false
        },
        {
          "_id": "682adaf581c740ab4aabc5aa",
          "name": "Ginny Wong",
          "hidden": false
        },
        {
          "_id": "682adaf581c740ab4aabc5ab",
          "name": "Simon See",
          "hidden": false
        },
        {
          "_id": "682adaf581c740ab4aabc5ac",
          "name": "Pasquale Minervini",
          "hidden": false
        },
        {
          "_id": "682adaf581c740ab4aabc5ad",
          "name": "Yangqiu Song",
          "hidden": false
        },
        {
          "_id": "682adaf581c740ab4aabc5ae",
          "name": "Mark Steedman",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-15T17:52:54.000Z",
      "submittedOnDailyAt": "2025-05-19T08:37:50.522Z",
      "title": "MMLongBench : Indice d'Évaluation pour Modèles de Langage Vision-Vision de Contexte Long\nIndice pour évaluer de manière efficace et détaillée les modèles de langage vision-vision de contexte long.",
      "submittedOnDailyBy": {
        "_id": "657ccbf2869d5bb0e53b482f",
        "avatarUrl": "/avatars/2eae5a10bdc14814a04d9f255f16de6b.svg",
        "isPro": false,
        "fullname": "Rohit Saxena",
        "user": "rohitsaxena",
        "type": "user"
      },
      "summary": "Avec le rapide expansion du contexte de la langue de commerce, les modèles de langue de commerce à grand contexte (LCVLMs) ont évolué, permettant de traiter en une seule exécution des centaines d'images et de tokens de texte échangés. Dans cet article, nous présentons MMLongBench, une collection de tâches de langue de commerce à grand contexte, conçue pour couvrir une large gamme de tâches et évaluer de manière efficace et insightful les LCVLMs. MMLongBench comprend 5 types de tâches et 13,331 cas, abordant une large gamme d'images naturelles et synthétiques. Pour évaluer la réaction du modèle par rapport à la longueur des entrées, nous fournissons des échantillons en utilisant le programme de tokenisation cosmo et en combinant des blocs de commerce et des tokens de texte dans 5 longueurs d'entrée standard (8K-128K tokens). Nous effectuons une évaluation détaillée de 46 modèles de LCVLMs, tant propriétaires que ouverts, et nous effectuons un analyse exhaustive des capacités de grand contexte des modèles actuels. Les résultats révèlent : i) le rendement dans une tâche est un peu représentatif de la capacité de grand contexte générale ; ii) tant les modèles propriétaires que les ouverts ont des limites dans les tâches de grand contexte, avec des espaces pour des améliorations futures ; iii) les modèles avec une forte capacité cognitive tendent à avoir de meilleurs résultats dans le grand contexte. Grâce à sa large couverture de tâches, diversité d'images et contrôle stricte de longueur, MMLongBench fournit une base essentielle pour le diagnostic et le progrès des prochaines générations de LCVLMs.",
      "upvotes": 14,
      "discussionId": "682adaf681c740ab4aabc5e2",
      "ai_keywords": [
        "long-context vision-language models (LCVLMs)",
        "MMLongBench",
        "Visual RAG",
        "Many-Shot ICL",
        "vision patches",
        "cross-modal tokenization scheme",
        "long-context vision-language tasks",
        "reasoning ability"
      ]
    },
    "publishedAt": "2025-05-15T13:52:54.000Z",
    "title": "MMLongBench: Benchmarking Long-Context Vision-Language Models\n  Effectively and Thoroughly",
    "summary": "The rapid extension of context windows in large vision-language models has\ngiven rise to long-context vision-language models (LCVLMs), which are capable\nof handling hundreds of images with interleaved text tokens in a single forward\npass. In this work, we introduce MMLongBench, the first benchmark covering a\ndiverse set of long-context vision-language tasks, to evaluate LCVLMs\neffectively and thoroughly. MMLongBench is composed of 13,331 examples spanning\nfive different categories of downstream tasks, such as Visual RAG and Many-Shot\nICL. It also provides broad coverage of image types, including various natural\nand synthetic images. To assess the robustness of the models to different input\nlengths, all examples are delivered at five standardized input lengths (8K-128K\ntokens) via a cross-modal tokenization scheme that combines vision patches and\ntext tokens. Through a thorough benchmarking of 46 closed-source and\nopen-source LCVLMs, we provide a comprehensive analysis of the current models'\nvision-language long-context ability. Our results show that: i) performance on\na single task is a weak proxy for overall long-context capability; ii) both\nclosed-source and open-source models face challenges in long-context\nvision-language tasks, indicating substantial room for future improvement; iii)\nmodels with stronger reasoning ability tend to exhibit better long-context\nperformance. By offering wide task coverage, various image types, and rigorous\nlength control, MMLongBench provides the missing foundation for diagnosing and\nadvancing the next generation of LCVLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.10610.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "657ccbf2869d5bb0e53b482f",
      "avatarUrl": "/avatars/2eae5a10bdc14814a04d9f255f16de6b.svg",
      "fullname": "Rohit Saxena",
      "name": "rohitsaxena",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.11409",
      "authors": [
        {
          "_id": "682abb7984695084c1a48eab",
          "name": "Yi Xu",
          "hidden": false
        },
        {
          "_id": "682abb7984695084c1a48eac",
          "name": "Chengzu Li",
          "hidden": false
        },
        {
          "_id": "682abb7984695084c1a48ead",
          "user": {
            "_id": "62b279e92375526ae51a537b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b279e92375526ae51a537b/U2DxDscDjQ6kWh-jMn0IG.jpeg",
            "isPro": false,
            "fullname": "Han Zhou",
            "user": "hzhouml",
            "type": "user"
          },
          "name": "Han Zhou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-19T06:42:16.276Z",
          "hidden": false
        },
        {
          "_id": "682abb7984695084c1a48eae",
          "user": {
            "_id": "65bf213f8467e2a3d6374d4b",
            "avatarUrl": "/avatars/0194cdba95d7a4c01fbbdd505e384a3d.svg",
            "isPro": false,
            "fullname": "X Wan",
            "user": "masonxw",
            "type": "user"
          },
          "name": "Xingchen Wan",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-19T05:02:52.536Z",
          "hidden": false
        },
        {
          "_id": "682abb7984695084c1a48eaf",
          "user": {
            "_id": "63920dfac47e36ddeb8f1864",
            "avatarUrl": "/avatars/c36cbf7b084d62368312e5c9292e4260.svg",
            "isPro": false,
            "fullname": "Caiqi Zhang",
            "user": "caiqizh",
            "type": "user"
          },
          "name": "Caiqi Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:23:48.005Z",
          "hidden": false
        },
        {
          "_id": "682abb7984695084c1a48eb0",
          "user": {
            "_id": "617a6284941993035fbaf299",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1635410461794-noauth.jpeg",
            "isPro": false,
            "fullname": "Anna Korhonen",
            "user": "akorhonen",
            "type": "user"
          },
          "name": "Anna Korhonen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:23:42.059Z",
          "hidden": false
        },
        {
          "_id": "682abb7984695084c1a48eb1",
          "user": {
            "_id": "6273e70dc8d55dd434bd8e52",
            "avatarUrl": "/avatars/3483eeda218e95b1eb00c3dc63c7d000.svg",
            "isPro": false,
            "fullname": "Ivan Vulić",
            "user": "ivulic",
            "type": "user"
          },
          "name": "Ivan Vulić",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:23:36.111Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/62b279e92375526ae51a537b/VYeWx-h6G2brVuuu-Wg5i.png"
      ],
      "publishedAt": "2025-05-16T16:17:22.000Z",
      "submittedOnDailyAt": "2025-05-19T03:37:48.826Z",
      "title": "Vision de design : pense uniquement à des images",
      "submittedOnDailyBy": {
        "_id": "62b279e92375526ae51a537b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b279e92375526ae51a537b/U2DxDscDjQ6kWh-jMn0IG.jpeg",
        "isPro": false,
        "fullname": "Han Zhou",
        "user": "hzhouml",
        "type": "user"
      },
      "summary": "Récemment, le développement de modèles de langage grands (LLMs) et son expansion multimodales (MLLMs) a considérablement amélioré la perception des machines dans diverses tâches. Cependant, ces modèles dépendent généralement principalement de l'information textuelle pour exprimer et structurer l'intelligence, surtout lorsqu'il s'agit d'informations visuelles. Dans cet article, nous argumentons que les modèles qui représentent et structurent l'intelligence de manière plus naturelle et efficace ne dépendent pas uniquement de la texture, et sont particulièrement importants dans des tâches qui incluent des informations spatiales et géométriques. En nous basant sur cette hypothèse, nous proposons un nouveau paradigme : 'Planification Visuelle'. Dans ce paradigme, des plans peuvent être réalisés en utilisant simples représentations visuelles sans dépendre du texte. Dans ce paradigme, les plans sont réalisés de manière itérative, représentant l'inférence en séquences d'images dans le domaine visuel, comme si une personne pouvait esboir ou visualiser des actions futures. Nous présentons un nouveau cadre d'apprentissage par renforcement 'Planification Visuelle par Apprentissage par Renforcement (VPRL)', basé sur des modèles de vision à long terme (VLMs) et de programmation de réseau (GRPO). Ce cadre d'apprentissage par renforcement atteint des améliorations significatives dans des tâches de navigation visuelle représentatives (FrozenLake, Maze, MiniBehavior). Notre paradigme de planification visuelle est supérieur à toute variante de planification basée uniquement sur le texte dans des environnements spatiaux, où la logique de l'intelligence est exécutée. Nos résultats montrent que la planification visuelle peut remplacer la logique basée sur le langage. Nous ouvrons une nouvelle voie dans des tâches qui nécessitent une inférence intuitive basée sur les images.",
      "upvotes": 10,
      "discussionId": "682abb7c84695084c1a48fb4",
      "githubRepo": "https://github.com/yix8/VisualPlanning",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "multimodal extensions (MLLMs)",
        "machine reasoning",
        "visual information",
        "Visual Planning",
        "purely visual representations",
        "sequences of images",
        "step-by-step inference",
        "Visual Planning via Reinforcement Learning (VPRL)",
        "GRPO",
        "post-training large vision models",
        "planning",
        "visual navigation tasks",
        "FrozenLake",
        "Maze",
        "MiniBehavior",
        "text-only space",
        "intuitive, image-based inference"
      ]
    },
    "publishedAt": "2025-05-16T12:17:22.000Z",
    "title": "Visual Planning: Let's Think Only with Images",
    "summary": "Recent advancements in Large Language Models (LLMs) and their multimodal\nextensions (MLLMs) have substantially enhanced machine reasoning across diverse\ntasks. However, these models predominantly rely on pure text as the medium for\nboth expressing and structuring reasoning, even when visual information is\npresent. In this work, we argue that language may not always be the most\nnatural or effective modality for reasoning, particularly in tasks involving\nspatial and geometrical information. Motivated by this, we propose a new\nparadigm, Visual Planning, which enables planning through purely visual\nrepresentations, independent of text. In this paradigm, planning is executed\nvia sequences of images that encode step-by-step inference in the visual\ndomain, akin to how humans sketch or visualize future actions. We introduce a\nnovel reinforcement learning framework, Visual Planning via Reinforcement\nLearning (VPRL), empowered by GRPO for post-training large vision models,\nleading to substantial improvements in planning in a selection of\nrepresentative visual navigation tasks, FrozenLake, Maze, and MiniBehavior. Our\nvisual planning paradigm outperforms all other planning variants that conduct\nreasoning in the text-only space. Our results establish Visual Planning as a\nviable and promising alternative to language-based reasoning, opening new\navenues for tasks that benefit from intuitive, image-based inference.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62b279e92375526ae51a537b/VYeWx-h6G2brVuuu-Wg5i.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.11409.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62b279e92375526ae51a537b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b279e92375526ae51a537b/U2DxDscDjQ6kWh-jMn0IG.jpeg",
      "fullname": "Han Zhou",
      "name": "hzhouml",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.07675",
      "authors": [
        {
          "_id": "6829dcab0daa5ccc817e6ec8",
          "name": "Seongjae Kang",
          "hidden": false
        },
        {
          "_id": "6829dcab0daa5ccc817e6ec9",
          "user": {
            "_id": "64f000769e7770db74d44bba",
            "avatarUrl": "/avatars/d015820380ffb823b1b35df64dcd3457.svg",
            "isPro": false,
            "fullname": "Dong-Bok Lee",
            "user": "dongboklee",
            "type": "user"
          },
          "name": "Dong Bok Lee",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-19T06:42:58.152Z",
          "hidden": false
        },
        {
          "_id": "6829dcab0daa5ccc817e6eca",
          "name": "Hyungjoon Jang",
          "hidden": false
        },
        {
          "_id": "6829dcab0daa5ccc817e6ecb",
          "name": "Sung Ju Hwang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-12T15:39:51.000Z",
      "submittedOnDailyAt": "2025-05-19T06:17:24.942Z",
      "title": "Réaliser une simple collecte de connaissances en utilisant le Modèle de Vision avec Étiquettes et optimiser avec Double Head.",
      "submittedOnDailyBy": {
        "_id": "64f000769e7770db74d44bba",
        "avatarUrl": "/avatars/d015820380ffb823b1b35df64dcd3457.svg",
        "isPro": false,
        "fullname": "Dong-Bok Lee",
        "user": "dongboklee",
        "type": "user"
      },
      "summary": "Vision-language models (VLMs) ont réussi à obtenir des résultats impressionnants sur diverses tâches en utilisant des informations contextuelles riches à travers des données standardisées minimales. Cependant, l'implémentation de modèles à cette échelle est particulièrement difficile dans des environnements avec des limitations en ressources. La technique de distillation de connaissances (KD) est une solution établie pour aborder ces problèmes. Cependant, les méthodes récentes de KD pour VLMs ont augmenté l'accumulation de charge de calcul et la complexité de l'optimisation grâce à des apprentissages multi-étapes ou des ajustements supplémentaires. Dans cet article, nous proposons un cadre de distillation de connaissances simple et efficace pour transmettre les connaissances de modèles spécialisés pour des tâches vers des modèles compressibles dans les VLMs, proposant le \\texttt{D}ual-\\texttt{H}ead \\texttt{O}ptimization (DHO). Spécifiquement, nous introduisons une tête de prédiction supplémentaire qui est entraînée indépendamment des données étiquetées et des prédictions du professeur, et nous proposons de combiner sa sortie linéairement lors de l'inférence. DHO normalise et réduit le conflit de gradients entre les signaux combinés, permettant un apprentissage de caractéristiques plus efficace que la ligne basée sur la distillation de connaissances d'une seule tête. Les résultats d'expériences larges montrent que DHO suit la ligne de base sur plusieurs ensembles de données. En particulier, nous atteignons un nouveau rendement sur ImageNet, améliorant la précision de 3% avec 1% de données étiquetées et de 0.1% avec 10% de données étiquetées, en utilisant peu de paramètres.",
      "upvotes": 8,
      "discussionId": "6829dcad0daa5ccc817e6f40",
      "ai_keywords": [
        "Vision-language models (VLMs)",
        "knowledge distillation (KD)",
        "dual prediction heads",
        "gradient conflicts",
        "feature learning",
        "semi-supervised settings",
        "state-of-the-art performance",
        "ImageNet",
        "accuracy"
      ]
    },
    "publishedAt": "2025-05-12T11:39:51.000Z",
    "title": "Simple Semi-supervised Knowledge Distillation from Vision-Language\n  Models via texttt{D}ual-texttt{H}ead\n  texttt{O}ptimization",
    "summary": "Vision-language models (VLMs) have achieved remarkable success across diverse\ntasks by leveraging rich textual information with minimal labeled data.\nHowever, deploying such large models remains challenging, particularly in\nresource-constrained environments. Knowledge distillation (KD) offers a\nwell-established solution to this problem; however, recent KD approaches from\nVLMs often involve multi-stage training or additional tuning, increasing\ncomputational overhead and optimization complexity. In this paper, we propose\ntexttt{D}ual-texttt{H}ead\ntexttt{O}ptimization (texttt{DHO}) -- a simple yet\neffective KD framework that transfers knowledge from VLMs to compact,\ntask-specific models in semi-supervised settings. Specifically, we introduce\ndual prediction heads that independently learn from labeled data and teacher\npredictions, and propose to linearly combine their outputs during inference. We\nobserve that DHO mitigates gradient conflicts between supervised and\ndistillation signals, enabling more effective feature learning than single-head\nKD baselines. As a result, extensive experiments show that DHO\nconsistently outperforms baselines across multiple domains and fine-grained\ndatasets. Notably, on ImageNet, it achieves state-of-the-art performance,\nimproving accuracy by 3% and 0.1% with 1% and 10% labeled data, respectively,\nwhile using fewer parameters.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.07675.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64f000769e7770db74d44bba",
      "avatarUrl": "/avatars/d015820380ffb823b1b35df64dcd3457.svg",
      "fullname": "Dong-Bok Lee",
      "name": "dongboklee",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.11107",
      "authors": [
        {
          "_id": "682ad96cdc6d7453624831b9",
          "user": {
            "_id": "6213410828005421265b27d3",
            "avatarUrl": "/avatars/930ac20daf640ca31fab713bf00c3268.svg",
            "isPro": false,
            "fullname": "許湛然",
            "user": "Splend1dchan",
            "type": "user"
          },
          "name": "Chan-Jan Hsu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-19T07:23:15.798Z",
          "hidden": false
        },
        {
          "_id": "682ad96cdc6d7453624831ba",
          "name": "Davide Buffelli",
          "hidden": false
        },
        {
          "_id": "682ad96cdc6d7453624831bb",
          "name": "Jamie McGowan",
          "hidden": false
        },
        {
          "_id": "682ad96cdc6d7453624831bc",
          "name": "Feng-Ting Liao",
          "hidden": false
        },
        {
          "_id": "682ad96cdc6d7453624831bd",
          "name": "Yi-Chang Chen",
          "hidden": false
        },
        {
          "_id": "682ad96cdc6d7453624831be",
          "name": "Sattar Vakili",
          "hidden": false
        },
        {
          "_id": "682ad96cdc6d7453624831bf",
          "name": "Da-shan Shiu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-16T10:40:35.000Z",
      "submittedOnDailyAt": "2025-05-19T05:58:53.531Z",
      "title": "Groupe Signiping : Coopération d'Intelligence Artificielle Logique parallèle au niveau de Tokens",
      "submittedOnDailyBy": {
        "_id": "6213410828005421265b27d3",
        "avatarUrl": "/avatars/930ac20daf640ca31fab713bf00c3268.svg",
        "isPro": false,
        "fullname": "許湛然",
        "user": "Splend1dchan",
        "type": "user"
      },
      "summary": "Récemment, le développement de grands modèles de langue (LLMs) a démontré la capacité de faire des raisonnements logiques à travers des séquences de pensées autogénérées. Les groupes de raisonnement logique, en grande majorité, peuvent améliorer la qualité des résultats individuels vers un raisonnement logique plus commun. Cependant, ces groupes souvent interagissent de manière alternée, en augmentant la rétroaction pour améliorer la qualité. Dans cet article, nous proposons un seul LLM appelé Group Think. Ce modèle fonctionne comme un groupe de multiples axes de raisonnement logique (synchronisation) parallèles. En partageant la progression partielle de génération d'autres axes, Group Think adapte dynamiquement les trajectoires logiques des autres axes au niveau de tokens, introduisant de nouveaux patrons de raisonnement parallèles. Par exemple, si un axe de raisonnement détecte qu'un fil de raisonnement peut continuer dans une position plus appropriée, il peut changer de génération en milieu. La collaboration au niveau de tokens permet que Group Think réduise les raisonnements répétitifs, améliore la qualité tout en atteignant une rétroaction très faible. De plus, grâce à son parallélisme, il peut utiliser efficacement les ressources de calcul disponibles et est particulièrement adapté à l'inférence en nuage. Dans cet article, nous proposons un changement simple et généralisable pour que tous les LLMs puissent effectuer Group Think sur des GPUs locales. De plus, nous présentons une stratégie d'évaluation pour benchmarker la rétroaction des raisonnements logiques et nous montrons expérimentalement comment Group Think améliore la rétroaction en utilisant un LLM ouvert et non entraîné explicitement. Cette recherche indique que les futurs LLMs montreront des comportements de collaboration plus complexes et efficaces, orientant-se vers la génération de qualité supérieure.",
      "upvotes": 7,
      "discussionId": "682ad96ddc6d7453624831f3",
      "ai_keywords": [
        "large language models (LLMs)",
        "reasoning through self-generated chains of thought",
        "reasoning agents",
        "turn-based manner",
        "Group Think",
        "concurrent reasoning agents",
        "think ers",
        "shared visibility",
        "reasoning trajectories",
        "token level",
        "reasoning thread",
        "fine-grained, token-level collaboration",
        "redundant reasoning",
        "edge inference",
        "modification",
        "LLMs",
        "local GPU",
        "evaluation strategy",
        "reasoning latency"
      ]
    },
    "publishedAt": "2025-05-16T06:40:35.000Z",
    "title": "Group Think: Multiple Concurrent Reasoning Agents Collaborating at Token\n  Level Granularity",
    "summary": "Recent advances in large language models (LLMs) have demonstrated the power\nof reasoning through self-generated chains of thought. Multiple reasoning\nagents can collaborate to raise joint reasoning quality above individual\noutcomes. However, such agents typically interact in a turn-based manner,\ntrading increased latency for improved quality. In this paper, we propose Group\nThink--a single LLM that acts as multiple concurrent reasoning agents, or\nthinkers. With shared visibility into each other's partial generation progress,\nGroup Think introduces a new concurrent-reasoning paradigm in which multiple\nreasoning trajectories adapt dynamically to one another at the token level. For\nexample, a reasoning thread may shift its generation mid-sentence upon\ndetecting that another thread is better positioned to continue. This\nfine-grained, token-level collaboration enables Group Think to reduce redundant\nreasoning and improve quality while achieving significantly lower latency.\nMoreover, its concurrent nature allows for efficient utilization of idle\ncomputational resources, making it especially suitable for edge inference,\nwhere very small batch size often underutilizes local~GPUs. We give a simple\nand generalizable modification that enables any existing LLM to perform Group\nThink on a local GPU. We also present an evaluation strategy to benchmark\nreasoning latency and empirically demonstrate latency improvements using\nopen-source LLMs that were not explicitly trained for Group Think. We hope this\nwork paves the way for future LLMs to exhibit more sophisticated and more\nefficient collaborative behavior for higher quality generation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.11107.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6213410828005421265b27d3",
      "avatarUrl": "/avatars/930ac20daf640ca31fab713bf00c3268.svg",
      "fullname": "許湛然",
      "name": "Splend1dchan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.11427",
      "authors": [
        {
          "_id": "682ad9809506a7e45a93be00",
          "user": {
            "_id": "6318e7a2acffc70bd4e057ec",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6318e7a2acffc70bd4e057ec/2m3XSbNLwv7Kmo8qfWq3L.jpeg",
            "isPro": false,
            "fullname": "Adrian Robert Minut",
            "user": "adrianrob",
            "type": "user"
          },
          "name": "Adrian Robert Minut",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:25:22.059Z",
          "hidden": false
        },
        {
          "_id": "682ad9809506a7e45a93be01",
          "user": {
            "_id": "63ab16a6d7ee953f604ecd52",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ab16a6d7ee953f604ecd52/ujylOpczHKxU6Kfr-jGVr.png",
            "isPro": false,
            "fullname": "Tommaso Mencattini",
            "user": "tmencatt",
            "type": "user"
          },
          "name": "Tommaso Mencattini",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-19T07:22:21.898Z",
          "hidden": false
        },
        {
          "_id": "682ad9809506a7e45a93be02",
          "user": {
            "_id": "5e8ef1f14957053f606489e6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1635502086699-5e8ef1f14957053f606489e6.jpeg",
            "isPro": false,
            "fullname": "Andrea Santilli",
            "user": "teelinsan",
            "type": "user"
          },
          "name": "Andrea Santilli",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-19T07:22:26.518Z",
          "hidden": false
        },
        {
          "_id": "682ad9809506a7e45a93be03",
          "user": {
            "_id": "64256584daa3502ee3570b86",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64256584daa3502ee3570b86/kui0eb59S5aTUeZIjawUj.jpeg",
            "isPro": false,
            "fullname": "Donato Crisostomi",
            "user": "crisostomi",
            "type": "user"
          },
          "name": "Donato Crisostomi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:25:28.737Z",
          "hidden": false
        },
        {
          "_id": "682ad9809506a7e45a93be04",
          "user": {
            "_id": "652681664e066bf73f8e2bd1",
            "avatarUrl": "/avatars/084dec4765d9996d74901b8df95ec35f.svg",
            "isPro": false,
            "fullname": "Emanuele Rodola'",
            "user": "erodola",
            "type": "user"
          },
          "name": "Emanuele Rodolà",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:25:35.566Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-16T16:43:23.000Z",
      "submittedOnDailyAt": "2025-05-19T05:45:27.421Z",
      "title": "Merginet : Modèle d'évolution simple pour l'intégration de outils Merginet",
      "submittedOnDailyBy": {
        "_id": "5e8ef1f14957053f606489e6",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1635502086699-5e8ef1f14957053f606489e6.jpeg",
        "isPro": false,
        "fullname": "Andrea Santilli",
        "user": "teelinsan",
        "type": "user"
      },
      "summary": "Le modèle peut être intégré. Les fonctions d'un modèle existant peuvent être intégrées dans un nouveau modèle. Il n'est pas nécessaire de le ré-entraîner de nouveau. Cela réduit les coûts et soutient les bibliothèques utilisant des processeurs graphiques pour les consommateurs, améliorant l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela améliore l'efficacité. Cela",
      "upvotes": 6,
      "discussionId": "682ad9819506a7e45a93be38",
      "githubRepo": "https://github.com/tommasomncttn/mergenetic",
      "ai_keywords": [
        "model merging",
        "evolutionary algorithms",
        "Mergenetic",
        "fitness estimators",
        "evaluation costs"
      ]
    },
    "publishedAt": "2025-05-16T12:43:23.000Z",
    "title": "Mergenetic: a Simple Evolutionary Model Merging Library",
    "summary": "Model merging allows combining the capabilities of existing models into a new\none - post hoc, without additional training. This has made it increasingly\npopular thanks to its low cost and the availability of libraries that support\nmerging on consumer GPUs. Recent work shows that pairing merging with\nevolutionary algorithms can boost performance, but no framework currently\nsupports flexible experimentation with such strategies in language models. We\nintroduce Mergenetic, an open-source library for evolutionary model merging.\nMergenetic enables easy composition of merging methods and evolutionary\nalgorithms while incorporating lightweight fitness estimators to reduce\nevaluation costs. We describe its design and demonstrate that Mergenetic\nproduces competitive results across tasks and languages using modest hardware.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.11427.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5e8ef1f14957053f606489e6",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1635502086699-5e8ef1f14957053f606489e6.jpeg",
      "fullname": "Andrea Santilli",
      "name": "teelinsan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.10962",
      "authors": [
        {
          "_id": "682ab4fe7a9f1a7ec9779dd6",
          "user": {
            "_id": "62ffa3f8311cad266f9af236",
            "avatarUrl": "/avatars/4c88cb518e000a475f8381573f21aa7f.svg",
            "isPro": false,
            "fullname": "Zhenwen Liang",
            "user": "invokerliang",
            "type": "user"
          },
          "name": "Zhenwen Liang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:24:26.692Z",
          "hidden": false
        },
        {
          "_id": "682ab4fe7a9f1a7ec9779dd7",
          "user": {
            "_id": "64c94eddcb2f1bf0e7db5a4d",
            "avatarUrl": "/avatars/f7e2532d3c85d5e5b5a02c579ea68c3a.svg",
            "isPro": false,
            "fullname": "Linfeng Song",
            "user": "freesunshine0316",
            "type": "user"
          },
          "name": "Linfeng Song",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:24:45.999Z",
          "hidden": false
        },
        {
          "_id": "682ab4fe7a9f1a7ec9779dd8",
          "name": "Yang Li",
          "hidden": false
        },
        {
          "_id": "682ab4fe7a9f1a7ec9779dd9",
          "name": "Tao Yang",
          "hidden": false
        },
        {
          "_id": "682ab4fe7a9f1a7ec9779dda",
          "name": "Feng Zhang",
          "hidden": false
        },
        {
          "_id": "682ab4fe7a9f1a7ec9779ddb",
          "user": {
            "_id": "65147a1426fbd558dbd08f1b",
            "avatarUrl": "/avatars/86574ee2d5c22e940be1c4e50be88675.svg",
            "isPro": false,
            "fullname": "Haitao Mi",
            "user": "haitaominlp",
            "type": "user"
          },
          "name": "Haitao Mi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:24:56.781Z",
          "hidden": false
        },
        {
          "_id": "682ab4fe7a9f1a7ec9779ddc",
          "name": "Dong Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-16T07:56:03.000Z",
      "submittedOnDailyAt": "2025-05-19T03:06:11.065Z",
      "title": "MPS-Prover : Exécution de tests de démonstration d'ordonnancement par étapes à l'aide de recherche et de création de données dans Multi-Person Shop",
      "submittedOnDailyBy": {
        "_id": "64c94eddcb2f1bf0e7db5a4d",
        "avatarUrl": "/avatars/f7e2532d3c85d5e5b5a02c579ea68c3a.svg",
        "isPro": false,
        "fullname": "Linfeng Song",
        "user": "freesunshine0316",
        "type": "user"
      },
      "summary": "Automatic Theorem Proving (ATP) est un défi complexe pour l'intelligence artificielle dans les langages formels, qui requiert des inférences logiques strictes et un grand espace de recherche. De plus, les modèles de langage à grande échelle (LLMs) ont démontré un rendement intelligent, mais actuellement, les étapes de test de raisonnement peuvent conduire à des stratégies de test inadéquates par des guides de recherche biaisés, ce qui diminue l'efficacité. Dans cet article, nous présentons un nouveau système de ATP basé sur des étapes de test, le Multi-Prospective Proof Prover (MPS-Prover), conçu pour surmonter ces limitations. MPS-Prover introduit deux innovations clés : une stratégie d'édition des données d'entraînement postérieure qui réduit approximativement 40% des données d'entraînement redondantes sans perdre d'efficacité, et une combinaison d'un modèle d'évaluation entraîné et de règles heuristiques stratégiquement conçues pour sélectionner diverses stratégies de recherche, éviter des états inutiles et améliorer la robustesse de la recherche. Dans des évaluations larges, MPS-Prover atteint les meilleurs rendements sur des benchmarks difficiles comme miniF2F et ProofNet, surpassant les modèles existants de 7B paramètres. De plus, l'analyse montre que MPS-Prover génère des tests de raisonnement plus courts et divers comparativement aux méthodes de test basées sur des étapes et sur tout le processus, ce qui caractérise son efficacité et son efficacité. Notre recherche offre un cadre solide et un analyse détaillée pour le développement de capacités logiques formelles basées sur des modèles de langage à grande échelle et le développement d'un meilleur prover de raisonnement.",
      "upvotes": 5,
      "discussionId": "682ab4ff7a9f1a7ec9779e71",
      "ai_keywords": [
        "Automated Theorem Proving (ATP)",
        "large language models (LLMs)",
        "biased search guidance",
        "Multi-Perspective Search Prover (MPS-Prover)",
        "post-training data curation strategy",
        "multi-perspective tree search mechanism",
        "learned critic model",
        "heuristic rules",
        "tactic selection",
        "search robustness",
        "miniF2F",
        "ProofNet",
        "state-of-the-art performance",
        "formal reasoning"
      ]
    },
    "publishedAt": "2025-05-16T03:56:03.000Z",
    "title": "MPS-Prover: Advancing Stepwise Theorem Proving by Multi-Perspective\n  Search and Data Curation",
    "summary": "Automated Theorem Proving (ATP) in formal languages remains a formidable\nchallenge in AI, demanding rigorous logical deduction and navigating vast\nsearch spaces. While large language models (LLMs) have shown promising\nperformance, existing stepwise provers often suffer from biased search\nguidance, leading to inefficiencies and suboptimal proof strategies. This paper\nintroduces the Multi-Perspective Search Prover (MPS-Prover), a novel stepwise\nATP system designed to overcome these limitations. MPS-Prover incorporates two\nkey innovations: a highly effective post-training data curation strategy that\nprunes approximately 40% of redundant training data without sacrificing\nperformance, and a multi-perspective tree search mechanism. This search\nintegrates a learned critic model with strategically designed heuristic rules\nto diversify tactic selection, prevent getting trapped in unproductive states,\nand enhance search robustness. Extensive evaluations demonstrate that\nMPS-Prover achieves state-of-the-art performance on multiple challenging\nbenchmarks, including miniF2F and ProofNet, outperforming prior 7B parameter\nmodels. Furthermore, our analyses reveal that MPS-Prover generates\nsignificantly shorter and more diverse proofs compared to existing stepwise and\nwhole-proof methods, highlighting its efficiency and efficacy. Our work\nadvances the capabilities of LLM-based formal reasoning and offers a robust\nframework and a comprehensive analysis for developing more powerful theorem\nprovers.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.10962.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64c94eddcb2f1bf0e7db5a4d",
      "avatarUrl": "/avatars/f7e2532d3c85d5e5b5a02c579ea68c3a.svg",
      "fullname": "Linfeng Song",
      "name": "freesunshine0316",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.10518",
      "authors": [
        {
          "_id": "68271c682f2e31ef0667bfaf",
          "user": {
            "_id": "668e4d1b446c8736208d99e1",
            "avatarUrl": "/avatars/dbe10f3b181e789d98b9b6bde4f711b2.svg",
            "isPro": false,
            "fullname": "Anastasios Gerontopoulos",
            "user": "nasos10",
            "type": "user"
          },
          "name": "Anastasios Gerontopoulos",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-18T19:39:25.735Z",
          "hidden": false
        },
        {
          "_id": "68271c682f2e31ef0667bfb0",
          "name": "Spyros Gidaris",
          "hidden": false
        },
        {
          "_id": "68271c682f2e31ef0667bfb1",
          "name": "Nikos Komodakis",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/668e4d1b446c8736208d99e1/VMNrdGj9BjgRO8fMsagaH.png"
      ],
      "publishedAt": "2025-05-15T17:25:03.000Z",
      "submittedOnDailyAt": "2025-05-19T07:50:27.978Z",
      "title": "Multi-Token Prediction Needs Registers",
      "submittedOnDailyBy": {
        "_id": "668e4d1b446c8736208d99e1",
        "avatarUrl": "/avatars/dbe10f3b181e789d98b9b6bde4f711b2.svg",
        "isPro": false,
        "fullname": "Anastasios Gerontopoulos",
        "user": "nasos10",
        "type": "user"
      },
      "summary": "La prédiction de Datok a apparu comme un objectif potentiel pour améliorer l'entraînement préalable de modèles de langage, mais son avantage n'est pas constant avec d'autres configurations et ne s'étend pas. Dans cet article, nous proposons un approche simple et efficace pour la prédiction de Datok, où des tokens de registre sont insérés dans la séquence d'entrée. Comparé aux méthodes existantes, MuToR offre les principales avantages suivants : le nombre de paramètres supplémentaires est minimal, aucun changement structurel n'est requis - ce qui garantit la compatibilité avec des modèles de langage entraînés précédemment - et il est développé selon l'objectif d'entraînement précédent du token suivant, ce qui est particulièrement adapté pour le but d'entraînement avec rétroalimentation de sous-ensembles. De plus, il supporte naturellement la Prédiction Scalable Holizon. Dans divers cas, nous démontrons l'efficacité et la diversité de MuToR dans des tâches de génération complexes dans le domaine de la langue et de la vision, y compris l'entraînement avec rétroalimentation de sous-ensembles et l'entraînement efficace de paramètres (PEFT). Le code est disponible sur la URL suivante : https://github.com/nasosger/MuToR.",
      "upvotes": 3,
      "discussionId": "68271c692f2e31ef0667bff6",
      "githubRepo": "https://github.com/nasosger/MuToR",
      "ai_keywords": [
        "register tokens",
        "multi-token prediction",
        "next-token pretraining",
        "parameter-efficient fine-tuning (PEFT)",
        "generative tasks"
      ]
    },
    "publishedAt": "2025-05-15T13:25:03.000Z",
    "title": "Multi-Token Prediction Needs Registers",
    "summary": "Multi-token prediction has emerged as a promising objective for improving\nlanguage model pretraining, but its benefits have not consistently generalized\nto other settings such as fine-tuning. In this paper, we propose MuToR, a\nsimple and effective approach to multi-token prediction that interleaves\nlearnable register tokens into the input sequence, each tasked with predicting\nfuture targets. Compared to existing methods, MuToR offers several key\nadvantages: it introduces only a negligible number of additional parameters,\nrequires no architectural changes--ensuring compatibility with off-the-shelf\npretrained language models--and remains aligned with the next-token pretraining\nobjective, making it especially well-suited for supervised fine-tuning.\nMoreover, it naturally supports scalable prediction horizons. We demonstrate\nthe effectiveness and versatility of MuToR across a range of use cases,\nincluding supervised fine-tuning, parameter-efficient fine-tuning (PEFT), and\npretraining, on challenging generative tasks in both language and vision\ndomains. Our code will be available at: https://github.com/nasosger/MuToR.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/668e4d1b446c8736208d99e1/VMNrdGj9BjgRO8fMsagaH.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.10518.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "668e4d1b446c8736208d99e1",
      "avatarUrl": "/avatars/dbe10f3b181e789d98b9b6bde4f711b2.svg",
      "fullname": "Anastasios Gerontopoulos",
      "name": "nasos10",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.11152",
      "authors": [
        {
          "_id": "682a9a3f5e6f0c59f4d8a0e5",
          "user": {
            "_id": "65601c6ee23401f82005e361",
            "avatarUrl": "/avatars/e9fc24bd8c5afd8b07a2f42765d44a7d.svg",
            "isPro": false,
            "fullname": "Daniel Sungho Jung",
            "user": "dqj5182",
            "type": "user"
          },
          "name": "Daniel Sungho Jung",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-19T06:42:43.502Z",
          "hidden": false
        },
        {
          "_id": "682a9a3f5e6f0c59f4d8a0e6",
          "user": {
            "_id": "656056b21392aa3beb5de0bd",
            "avatarUrl": "/avatars/07f25b750ef308d65f2e6c82506e7816.svg",
            "isPro": false,
            "fullname": "Kyoung Mu  Lee ",
            "user": "kyoungmu",
            "type": "user"
          },
          "name": "Kyoung Mu Lee",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:25:48.640Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-16T11:54:25.000Z",
      "submittedOnDailyAt": "2025-05-19T01:11:35.713Z",
      "title": "DENSIT HAND estime des points de contact lors de l'entraînement avec des données déséquilibrées.",
      "submittedOnDailyBy": {
        "_id": "65601c6ee23401f82005e361",
        "avatarUrl": "/avatars/e9fc24bd8c5afd8b07a2f42765d44a7d.svg",
        "isPro": false,
        "fullname": "Daniel Sungho Jung",
        "user": "dqj5182",
        "type": "user"
      },
      "summary": "Les mains jouent un rôle important dans l'interaction humaine, et comprendre la contact de les mains avec le monde est crucial pour une compréhension intégrale de leurs fonctions. Récemment, le nombre de jeux de données d'interaction de les mains, qui incluent des objets, d'autres mains, des scènes et le corps, a augmenté. En raison de l'importance de ce travail et de l'augmentation de données de haute qualité, il a été démontré que des méthodes efficaces pour estimer la densité de la contact de les mains n'ont pas encore été explorées. Pour estimer la densité de la contact de les mains, deux problèmes principaux sont identifiés : premièrement, il existe une grande disparité de classes dans les jeux de données de contact, car de nombreux exemples n'ont pas de contact ; deuxièmement, il y a un problème d'imbalance dans la fréquence de contact aux extrémités de les mains, ce qui difficile la généralisation du contact dans d'autres zones. Pour résoudre ces problèmes, nous proposons un cadre d'apprentissage pour estimer la densité de contact de les mains (HACO) à partir de données imbalancées. Pour aborder le problème de disparité de classes, nous introduisons un méthode d'échantillonnage équilibré et effectuons des échantillonnages dans différents groupes pour représenter adéquatement les statistiques de contact et de non-contact. De plus, pour résoudre le problème d'imbalance spectral, nous proposons la perte d'équilibre de classes à l'échelle de la fréquence maximale (VCB), qui assigne des poids spécifiques à la fréquence de contact dans le jeu de données et ajuste la contribution de la perte en fonction de la distribution spatiale du contact. Enfin, en utilisant de grands jeux de données de contact de les mains, nous pouvons apprendre à estimer la densité de contact de les mains sans considérer les problèmes de disparité de classes et spectral. Le code est disponible pour télécharger.",
      "upvotes": 2,
      "discussionId": "682a9a405e6f0c59f4d8a125",
      "projectPage": "https://haco-release.github.io/",
      "githubRepo": "https://github.com/dqj5182/HACO_RELEASE",
      "ai_keywords": [
        "dense hand contact estimation",
        "class imbalance issue",
        "spatial imbalance issue",
        "finger tips",
        "balanced contact sampling",
        "vertex-level class-balanced (VCB) loss",
        "contact distribution",
        "contact frequency"
      ]
    },
    "publishedAt": "2025-05-16T07:54:25.000Z",
    "title": "Learning Dense Hand Contact Estimation from Imbalanced Data",
    "summary": "Hands are essential to human interaction, and understanding contact between\nhands and the world can promote comprehensive understanding of their function.\nRecently, there have been growing number of hand interaction datasets that\ncover interaction with object, other hand, scene, and body. Despite the\nsignificance of the task and increasing high-quality data, how to effectively\nlearn dense hand contact estimation remains largely underexplored. There are\ntwo major challenges for learning dense hand contact estimation. First, there\nexists class imbalance issue from hand contact datasets where majority of\nsamples are not in contact. Second, hand contact datasets contain spatial\nimbalance issue with most of hand contact exhibited in finger tips, resulting\nin challenges for generalization towards contacts in other hand regions. To\ntackle these issues, we present a framework that learns dense HAnd COntact\nestimation (HACO) from imbalanced data. To resolve the class imbalance issue,\nwe introduce balanced contact sampling, which builds and samples from multiple\nsampling groups that fairly represent diverse contact statistics for both\ncontact and non-contact samples. Moreover, to address the spatial imbalance\nissue, we propose vertex-level class-balanced (VCB) loss, which incorporates\nspatially varying contact distribution by separately reweighting loss\ncontribution of each vertex based on its contact frequency across dataset. As a\nresult, we effectively learn to predict dense hand contact estimation with\nlarge-scale hand contact data without suffering from class and spatial\nimbalance issue. The codes will be released.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.11152.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65601c6ee23401f82005e361",
      "avatarUrl": "/avatars/e9fc24bd8c5afd8b07a2f42765d44a7d.svg",
      "fullname": "Daniel Sungho Jung",
      "name": "dqj5182",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.11049",
      "authors": [
        {
          "_id": "682af4241286a7273c5bfd09",
          "name": "Yue Liu",
          "hidden": false
        },
        {
          "_id": "682af4241286a7273c5bfd0a",
          "name": "Shengfang Zhai",
          "hidden": false
        },
        {
          "_id": "682af4241286a7273c5bfd0b",
          "name": "Mingzhe Du",
          "hidden": false
        },
        {
          "_id": "682af4241286a7273c5bfd0c",
          "name": "Yulin Chen",
          "hidden": false
        },
        {
          "_id": "682af4241286a7273c5bfd0d",
          "name": "Tri Cao",
          "hidden": false
        },
        {
          "_id": "682af4241286a7273c5bfd0e",
          "name": "Hongcheng Gao",
          "hidden": false
        },
        {
          "_id": "682af4241286a7273c5bfd0f",
          "name": "Cheng Wang",
          "hidden": false
        },
        {
          "_id": "682af4241286a7273c5bfd10",
          "name": "Xinfeng Li",
          "hidden": false
        },
        {
          "_id": "682af4241286a7273c5bfd11",
          "name": "Kun Wang",
          "hidden": false
        },
        {
          "_id": "682af4241286a7273c5bfd12",
          "name": "Junfeng Fang",
          "hidden": false
        },
        {
          "_id": "682af4241286a7273c5bfd13",
          "name": "Jiaheng Zhang",
          "hidden": false
        },
        {
          "_id": "682af4241286a7273c5bfd14",
          "name": "Bryan Hooi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-16T09:46:10.000Z",
      "submittedOnDailyAt": "2025-05-19T07:36:35.140Z",
      "title": "GuardReasoner-VL : Protège les VLMs avec une logique améliorée.",
      "submittedOnDailyBy": {
        "_id": "6650c77a74664a42ddfb9187",
        "avatarUrl": "/avatars/92001bbe0ae9b14309730316b639cede.svg",
        "isPro": false,
        "fullname": "yueliu1999",
        "user": "yueliu1999",
        "type": "user"
      },
      "summary": "Pour améliorer la sécurité des VLMs, cet article présente un nouveau modèle de protection basé sur des preuves appelé \"GuardReasoner-VL\". L'idée clé est de recommander que le modèle de protection fournisse une explication avant de prendre des décisions via RL en ligne. Tout d'abord, un corpus de raisons avec 123K échantillons et 631K étapes de raisonnement appelé \"GuardReasoner-VLTrain\" est construit. Ensuite, la capacité de raisonnement du modèle est initialisée par SFT. De plus, la capacité de raisonnement du modèle est développée et évoluée via RL en ligne. Spécifiquement, la diversité et la difficulté des échantillons sont augmentées par la réduction d'échantillons par rejet basée sur des données avec conscience de sécurité et l'affirmation de données. De plus, des paramètres de clip dynamique sont utilisés pour promouvoir l'exploration dans les étapes initiales et équilibrer le développement dans les étapes suivantes. Pour équilibrer le rendement et l'efficacité en tokens, un risque de sécurité basé sur la précision, le format et le coût des tokens est conçu. Les expériences extensives démontrent l'excellence du modèle et dépassent le score F1 moyen de 19,27%. Les données, le code et les modèles (3B/7B) de GuardReasoner-VL sont disponibles sur https://github.com/yueliu1999/GuardReasoner-VL.",
      "upvotes": 2,
      "discussionId": "682af42c1286a7273c5bfed9",
      "ai_keywords": [
        "GuardReasoner-VL",
        "online RL",
        "GuardReasoner-VLTrain",
        "reasoning corpus",
        "SFT",
        "rejection sampling",
        "data augmentation",
        "safety-aware data concatenation",
        "dynamic clipping parameter",
        "length-aware safety reward",
        "F1 score"
      ]
    },
    "publishedAt": "2025-05-16T05:46:10.000Z",
    "title": "GuardReasoner-VL: Safeguarding VLMs via Reinforced Reasoning",
    "summary": "To enhance the safety of VLMs, this paper introduces a novel reasoning-based\nVLM guard model dubbed GuardReasoner-VL. The core idea is to incentivize the\nguard model to deliberatively reason before making moderation decisions via\nonline RL. First, we construct GuardReasoner-VLTrain, a reasoning corpus with\n123K samples and 631K reasoning steps, spanning text, image, and text-image\ninputs. Then, based on it, we cold-start our model's reasoning ability via SFT.\nIn addition, we further enhance reasoning regarding moderation through online\nRL. Concretely, to enhance diversity and difficulty of samples, we conduct\nrejection sampling followed by data augmentation via the proposed safety-aware\ndata concatenation. Besides, we use a dynamic clipping parameter to encourage\nexploration in early stages and exploitation in later stages. To balance\nperformance and token efficiency, we design a length-aware safety reward that\nintegrates accuracy, format, and token cost. Extensive experiments demonstrate\nthe superiority of our model. Remarkably, it surpasses the runner-up by 19.27%\nF1 score on average. We release data, code, and models (3B/7B) of\nGuardReasoner-VL at https://github.com/yueliu1999/GuardReasoner-VL/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.11049.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "6650c77a74664a42ddfb9187",
      "avatarUrl": "/avatars/92001bbe0ae9b14309730316b639cede.svg",
      "fullname": "yueliu1999",
      "name": "yueliu1999",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.11140",
      "authors": [
        {
          "_id": "682ad417500638b80a43471d",
          "user": {
            "_id": "60d33fbbd7b174177faabd4f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60d33fbbd7b174177faabd4f/pfyv_xj2B2m2N4F4sT9zJ.jpeg",
            "isPro": true,
            "fullname": "Mike Zhang",
            "user": "jjzha",
            "type": "user"
          },
          "name": "Mike Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-19T07:23:04.053Z",
          "hidden": false
        },
        {
          "_id": "682ad417500638b80a43471e",
          "user": {
            "_id": "678fa79005ae7fe48d03ba47",
            "avatarUrl": "/avatars/a78ab2b37fa3e18ace783f6f71f5a361.svg",
            "isPro": false,
            "fullname": "Johannes Bjerva",
            "user": "bjerva",
            "type": "user"
          },
          "name": "Johannes Bjerva",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:25:58.827Z",
          "hidden": false
        },
        {
          "_id": "682ad417500638b80a43471f",
          "user": {
            "_id": "60ed4c56abab3c2620df8ac8",
            "avatarUrl": "/avatars/ad5508c1c94a96f6d1290e4735e81b73.svg",
            "isPro": false,
            "fullname": "Russa Biswas",
            "user": "rubis",
            "type": "user"
          },
          "name": "Russa Biswas",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:26:04.749Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-16T11:39:33.000Z",
      "submittedOnDailyAt": "2025-05-19T05:25:57.460Z",
      "title": "Scaling regularization peut améliorer la réalité de grands modèles de langue.",
      "submittedOnDailyBy": {
        "_id": "60d33fbbd7b174177faabd4f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60d33fbbd7b174177faabd4f/pfyv_xj2B2m2N4F4sT9zJ.jpeg",
        "isPro": true,
        "fullname": "Mike Zhang",
        "user": "jjzha",
        "type": "user"
      },
      "summary": "Récemment, les études sur la capacité logique des modèles de grands langages (LLM) ont montré des améliorations significatives dans les tâches mathématiques logiques en utilisant des processus de pensée et de calcul plus longs. Cependant, il n'a pas encore été démontré que les cas logiques longs améliorent automatiquement la précision de la vérité, surtout dans des contextes mathématiques. Dans cette étude, nous examinons la logique des modèles de grands langages (LLM) dans des scénarios de réponse à des questions (QA) dans des domaines complexes ouverts. Tout d'abord, nous présentons brièvement les travaux logiques de modèles complexes (QwQ-32B et DeepSeek-R1-671B), et nous ajustons des modèles variés allant des petits basés sur Qwen2.5 jusqu'à des architectures grandes. Nous introduisons des étapes de travaux logiques depuis des graphes de connaissance pour enrichir les travaux logiques. La configuration expérimentale comprend 4 approches de référence et 6 modèles d'ajustement d'instructions, avec 22.6K questions. Nous avons effectué 168 expériences et analysés approximativement 1,7 millions de travaux logiques. Nos résultats montrent que un petit modèle logique peut améliorer significativement la précision de la vérité dans un seul expérience, et que ce modèle ajusté à des instructions est supérieur. De plus, notre analyse montre que l'échelle dans le calcul et les tokens pendant la preuve améliorent positivement la précision de la vérité dans un intervalle de 2 à 8%, démontrant clairement l'effet de l'échelle sur l'amélioration de la précision de la vérité dans les tâches de QA dans des domaines ouverts. Tous les artefacts des expériences ont été évolués pour améliorer la précision de la vérité.",
      "upvotes": 1,
      "discussionId": "682ad418500638b80a434770",
      "githubRepo": "https://github.com/jjzha/fs1",
      "ai_keywords": [
        "large language model (LLM)",
        "reasoning capabilities",
        "mathematical reasoning",
        "length thinking process",
        "computational resources",
        "inference",
        "complex open-domain question-answering (QA)",
        "reasoning traces",
        "reasoning models",
        "QwQ-32B",
        "DeepSeek-R1-671B",
        "instruction-tuned variants",
        "Qwen2.5",
        "knowledge graphs",
        "paths",
        "reasoning traces",
        "baseline approaches",
        "instruction-tuned models",
        "benchmark",
        "datasets",
        "experimental runs",
        "factual accuracy",
        "test-time compute",
        "token budgets",
        "test-time scaling",
        "reasoning accuracy"
      ]
    },
    "publishedAt": "2025-05-16T07:39:33.000Z",
    "title": "Scaling Reasoning can Improve Factuality in Large Language Models",
    "summary": "Recent studies on large language model (LLM) reasoning capabilities have\ndemonstrated promising improvements in model performance by leveraging a\nlengthy thinking process and additional computational resources during\ninference, primarily in tasks involving mathematical reasoning (Muennighoff et\nal., 2025). However, it remains uncertain if longer reasoning chains inherently\nenhance factual accuracy, particularly beyond mathematical contexts. In this\nwork, we thoroughly examine LLM reasoning within complex open-domain\nquestion-answering (QA) scenarios. We initially distill reasoning traces from\nadvanced, large-scale reasoning models (QwQ-32B and DeepSeek-R1-671B), then\nfine-tune a variety of models ranging from smaller, instruction-tuned variants\nto larger architectures based on Qwen2.5. To enrich reasoning traces, we\nintroduce factual information from knowledge graphs in the form of paths into\nour reasoning traces. Our experimental setup includes four baseline approaches\nand six different instruction-tuned models evaluated across a benchmark of six\ndatasets, encompassing over 22.6K questions. Overall, we carry out 168\nexperimental runs and analyze approximately 1.7 million reasoning traces. Our\nfindings indicate that, within a single run, smaller reasoning models achieve\nnoticeable improvements in factual accuracy compared to their original\ninstruction-tuned counterparts. Moreover, our analysis demonstrates that adding\ntest-time compute and token budgets factual accuracy consistently improves by\n2-8%, further confirming the effectiveness of test-time scaling for enhancing\nperformance and consequently improving reasoning accuracy in open-domain QA\ntasks. We release all the experimental artifacts for further research.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.11140.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60d33fbbd7b174177faabd4f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60d33fbbd7b174177faabd4f/pfyv_xj2B2m2N4F4sT9zJ.jpeg",
      "fullname": "Mike Zhang",
      "name": "jjzha",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 56
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.11011",
      "authors": [
        {
          "_id": "682ae098730bd40a0755f87c",
          "name": "Darija Barak",
          "hidden": false
        },
        {
          "_id": "682ae098730bd40a0755f87d",
          "name": "Miguel Costa-Gomes",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-16T09:01:09.000Z",
      "submittedOnDailyAt": "2025-05-19T06:12:42.874Z",
      "title": "L'humanité attend de la LLM de jeux stratégiques un raisonnement et une collaboration.",
      "submittedOnDailyBy": {
        "_id": "6475c2794766357252e69e9f",
        "avatarUrl": "/avatars/db428715dfd2239df2aeaaff1282323f.svg",
        "isPro": false,
        "fullname": "i",
        "user": "iliashum",
        "type": "user"
      },
      "summary": "Durant le processus où les LLMs sont intégrés dans l'interaction sociale et économique, il est nécessaire d'avoir un profond compréhension de la façon dont les êtres humains peuvent se confronter aux LLMs comme adversaires. De plus, nous présentons les résultats des premiers expériences contrôlées de compensation des coûts qui permettent d'observer des différences dans le comportement entre les êtres humains et les LLMs dans la compétition de diversité et de beauté. Cette expérience utilise des participants identiques pour comparer les comportements individuels. Dans ce contexte, les êtres humains choisissent des nombres très bas en comparaison avec les LLMs lors des luttes contre eux. Cette évolution contribue principalement à l'augmentation du nombre de sélections de \"zero\" Nash-equilibrium. Cette évolution montre une collaboration étonnante entre la stratégie humaine et la stratégie des LLMs, expliquant comment cette collaboration fonctionne. Nos résultats fournissent une base fondamentale pour le jeu de sélections dans l'interaction entre les êtres humains et les LLMs, clairement démontrent l'inégalité entre le comportement et la croyance humaines, et ont un sens important pour le design structurel de systèmes hybrides de l'humain et des LLMs.",
      "upvotes": 1,
      "discussionId": "682ae099730bd40a0755f8b9",
      "ai_keywords": [
        "p-beauty contest",
        "Nash-equilibrium choices",
        "strategic reasoning ability",
        "reasoning ability",
        "propensity towards cooperation",
        "mechanism design"
      ]
    },
    "publishedAt": "2025-05-16T05:01:09.000Z",
    "title": "Humans expect rationality and cooperation from LLM opponents in\n  strategic games",
    "summary": "As Large Language Models (LLMs) integrate into our social and economic\ninteractions, we need to deepen our understanding of how humans respond to LLMs\nopponents in strategic settings. We present the results of the first controlled\nmonetarily-incentivised laboratory experiment looking at differences in human\nbehaviour in a multi-player p-beauty contest against other humans and LLMs. We\nuse a within-subject design in order to compare behaviour at the individual\nlevel. We show that, in this environment, human subjects choose significantly\nlower numbers when playing against LLMs than humans, which is mainly driven by\nthe increased prevalence of `zero' Nash-equilibrium choices. This shift is\nmainly driven by subjects with high strategic reasoning ability. Subjects who\nplay the zero Nash-equilibrium choice motivate their strategy by appealing to\nperceived LLM's reasoning ability and, unexpectedly, propensity towards\ncooperation. Our findings provide foundational insights into the multi-player\nhuman-LLM interaction in simultaneous choice games, uncover heterogeneities in\nboth subjects' behaviour and beliefs about LLM's play when playing against\nthem, and suggest important implications for mechanism design in mixed\nhuman-LLM systems.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.11011.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6475c2794766357252e69e9f",
      "avatarUrl": "/avatars/db428715dfd2239df2aeaaff1282323f.svg",
      "fullname": "i",
      "name": "iliashum",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  }
]