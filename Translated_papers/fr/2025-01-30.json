[
  {
    "paper": {
      "id": "2501.17703",
      "authors": [
        {
          "_id": "679ae76cf211c66bd702f5d5",
          "user": {
            "_id": "636a35eff8d9af4aea181608",
            "avatarUrl": "/avatars/d9c5cf3491243d1f2b1c5df1873ee8e7.svg",
            "isPro": false,
            "fullname": "yubo",
            "user": "ubowang",
            "type": "user"
          },
          "name": "Yubo Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-01-30T08:39:49.375Z",
          "hidden": false
        },
        {
          "_id": "679ae76cf211c66bd702f5d6",
          "name": "Xiang Yue",
          "hidden": false
        },
        {
          "_id": "679ae76cf211c66bd702f5d7",
          "user": {
            "_id": "6313a86154e6e5d9f0f94e04",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
            "isPro": false,
            "fullname": "Wenhu Chen",
            "user": "wenhu",
            "type": "user"
          },
          "name": "Wenhu Chen",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-01-30T02:43:59.302Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-29T15:20:30.000Z",
      "title": "Critique de l'ajustement final : Apprendre à critiquer est plus efficace que d'apprendre les erreurs.",
      "summary": "Supervised Fine-Tuning (SFT) est utilisé généralement pour entraîner des modèles de langage via des réponses annotées qui modélisent les instructions données. Dans cet article, ce paradigme est défié et une stratégie appelée Critique Fine-Tuning (CFT) est proposée. Cette méthodologie se concentre sur l'apprentissage du modèle à fournir des réponses précises, plutôt que de simplement mimétiser des réponses avec du bruit. CFT est reliée au processus d'apprentissage humain, mettant l'accent sur le pensée critique et promouvant un analyse plus profonde et une compréhension plus complexe de caractéristiques précédemment erronées, obtenues par le SFT standard. Pour démontrer les avantages de CFT, un ensemble de données de 50K échantillons dans WebInstruct a été construit, en utilisant GPT-4o comme modèle d'instructeur (input=[question; réponse avec bruit], output=évaluateur) pour générer des évaluations. L'entraînement avec CFT sur cet ensemble de données a considérablement amélioré des modèles de base comme Qwen2.5, Qwen2.5-Math et DeepSeek-Math dans 6 cadres de tests de mathématiques, avec un augmentation probabiliste de 4 à 10% par rapport au SFT. De plus, cette amélioration a été étendue aux ensembles de données comme MetaMath et NuminaMath. Spécifiquement, le modèle Qwen2.5-Math-CFT a dépassé des modèles puissants entraînés avec presque 2M échantillons, et a dépassé Qwen2.5-Math-Instruct. Les résultats négatifs indiquent que CFT montre la robustesse des sources de réponses avec bruit et du modèle d'évaluateur de l'instructeur. De ces résultats, une méthode efficace de amélioration de la logique des modèles de langage par entraînement critique est proposée.",
      "upvotes": 12,
      "discussionId": "679ae770f211c66bd702f697"
    },
    "publishedAt": "2025-01-29T21:51:11.227Z",
    "title": "Critique Fine-Tuning: Learning to Critique is More Effective than Learning to Imitate",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.17703.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "636a35eff8d9af4aea181608",
      "avatarUrl": "/avatars/d9c5cf3491243d1f2b1c5df1873ee8e7.svg",
      "fullname": "yubo",
      "name": "ubowang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2501.14334",
      "authors": [
        {
          "_id": "679a7546805383520ce065af",
          "user": {
            "_id": "644156da1a80f6d83cb1667c",
            "avatarUrl": "/avatars/106d30a576b0fb58118ac4333b17260b.svg",
            "isPro": false,
            "fullname": "Clement Desroches",
            "user": "clementdesroches",
            "type": "user"
          },
          "name": "Clément Desroches",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-01-29T21:06:17.418Z",
          "hidden": false
        },
        {
          "_id": "679a7546805383520ce065b0",
          "user": {
            "_id": "66221f6295e8f09a668f07f0",
            "avatarUrl": "/avatars/f7c943996c814630ab5dcfaaaba01a83.svg",
            "isPro": false,
            "fullname": "Martin Chauvin",
            "user": "Neyri56",
            "type": "user"
          },
          "name": "Martin Chauvin",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-01-30T09:38:17.235Z",
          "hidden": false
        },
        {
          "_id": "679a7546805383520ce065b1",
          "name": "Louis Ladan",
          "hidden": false
        },
        {
          "_id": "679a7546805383520ce065b2",
          "name": "Caroline Vateau",
          "hidden": false
        },
        {
          "_id": "679a7546805383520ce065b3",
          "name": "Simon Gosset",
          "hidden": false
        },
        {
          "_id": "679a7546805383520ce065b4",
          "name": "Philippe Cordier",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-24T08:58:49.000Z",
      "title": "Exploration de l'escalade durable de l'IA : étude de la prédiction de l'impact environnemental de l'IA sur l'entreprise",
      "summary": "La rapide expansion de l'Intelligence Artificielle (IA), en particulier des modèles de langage à grande échelle (LLMs), a soulevé des préoccupations concernant son impact environnemental. Ce problème s'est manifesté plus dans la production de matériaux durs que dans l'émission de CO2, incluant tout le cycle de vie. La nature opaque des principaux fournisseurs empêche aux entreprises d'évaluer l'impact environnemental lié à l'IA et d'atteindre des objectifs de zéro, ce qui représente un obstacle significatif.\n\nDans cet article, nous proposons une méthodologie pour évaluer l'impact environnemental des offres d'IA des entreprises, confirmant qu'il n'est pas nécessaire de connaître des connaissances spécialisées en Analyse de Cycle de Vie (LCA) pour fournir des solutions pratiques. Les résultats montrent que les modèles d'IA à grande échelle consomment 4600 fois plus d'énergie que les modèles traditionnels. Notre approche de modélisation considère la variation du mélange énergétique selon les scénarios de l'IPCC, l'augmentation de l'utilisation de l'IA et l'amélioration de l'efficacité des dispositifs de calcul pour prédire le consommateur d'énergie de l'IA jusqu'au 2030. Dans le scénario de forte introduction, l'introduction générale d'IA de génération et d'agents intelligents peut augmenter le consommateur d'énergie de l'IA d'un facteur de 24,4.\n\nPour réduire l'impact environnemental de l'IA de génération jusqu'au 2030, il est nécessaire un effort coopératif tout au long de la chaîne de valeur de l'IA. Les mesures indépendantes telles que l'efficacité des matériaux, l'efficacité du modèle ou l'amélioration de la réseau ne sont pas suffisantes. Nous demandons la normalisation des cadres d'évaluation environnementale, la plus grande transparence des propriétaires de la chaîne de valeur et l'introduction de métriques de \"compensation environnementale\", soutenant que cela est nécessaire pour aligner le développement de l'IA avec des objectifs de zéro.",
      "upvotes": 11,
      "discussionId": "679a7548805383520ce065f5"
    },
    "publishedAt": "2025-01-30T03:05:08.789Z",
    "title": "Exploring the sustainable scaling of AI dilemma: A projective study of corporations' AI environmental impacts",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.14334.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "644156da1a80f6d83cb1667c",
      "avatarUrl": "/avatars/106d30a576b0fb58118ac4333b17260b.svg",
      "fullname": "Clement Desroches",
      "name": "clementdesroches",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2501.17749",
      "authors": [
        {
          "_id": "679ae5eab898ac90bf4480b6",
          "user": {
            "_id": "657b3a44de028a439ea2ed9d",
            "avatarUrl": "/avatars/9f05e8eb6809a0ce1b50cd1fc9b5a044.svg",
            "isPro": false,
            "fullname": "Aitor Arrieta",
            "user": "aitorarrieta",
            "type": "user"
          },
          "name": "Aitor Arrieta",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-01-30T08:45:20.561Z",
          "hidden": false
        },
        {
          "_id": "679ae5eab898ac90bf4480b7",
          "name": "Miriam Ugarte",
          "hidden": false
        },
        {
          "_id": "679ae5eab898ac90bf4480b8",
          "user": {
            "_id": "65001514f322f9156663f096",
            "avatarUrl": "/avatars/e8712f60d4e8b7c70ac02c532ad547ef.svg",
            "isPro": false,
            "fullname": "Pablo Valle",
            "user": "pablovalle",
            "type": "user"
          },
          "name": "Pablo Valle",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-01-30T09:39:30.629Z",
          "hidden": false
        },
        {
          "_id": "679ae5eab898ac90bf4480b9",
          "user": {
            "_id": "63527de67e4cc3135fd16651",
            "avatarUrl": "/avatars/5eb8076d448d0b6746e256c24e1440e0.svg",
            "isPro": false,
            "fullname": "José Antonio Parejo Maestre",
            "user": "japarejo",
            "type": "user"
          },
          "name": "José Antonio Parejo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-01-30T09:39:06.958Z",
          "hidden": false
        },
        {
          "_id": "679ae5eab898ac90bf4480ba",
          "user": {
            "_id": "6790d642a1863df579840ae3",
            "avatarUrl": "/avatars/a10a6f4af327c1bb67513c56d7f84820.svg",
            "isPro": false,
            "fullname": "Sergio Segura",
            "user": "ssegura",
            "type": "user"
          },
          "name": "Sergio Segura",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-01-30T02:37:35.516Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-29T16:36:53.000Z",
      "title": "OpenAI's o3-mini Initial External Safety Verification: Insights from Pre-Deployment Evaluation",
      "summary": "Les modèles de langue grand (LLMs) occupent une part importante de notre vie quotidienne. Cependant, ils sont accompagnés de risques qui peuvent endommager la vie privée, perpetuer des préjugés et diffuser des informations anormales. Ces risques soulignent la nécessité de structures de sécurité robustes, de directives éthiques et de tests détaillés pour la fonctionnalité responsable. La sécurité des LLMs est une caractéristique importante qui est garantie avant que le modèle fonctionne et soit accessible aux utilisateurs. Dans cet article, nous présentons l'expérience des tests de sécurité externes réalisés par les chercheurs de l'Université de Montréal et de l'Université de Seville sur le nouveau modèle de LLM d'OpenAI, o3-mini. Spécifiquement, nous utilisons notre outil ASTRAL pour générer automatiquement et systématiquement des prompts insecures et évaluer différentes catégories de sécurité des LLMs. Pour o3-mini beta, nous avons confirmé directement les tests insecures classifiés par ASTRAL et avons identifié 87 instances de comportement insecure réels. Cet article met en lumière les contributions et les découvertes importantes de la vérification externe d'OpenAI au cours de la phase de fonctionnalité de son dernier modèle de LLM.",
      "upvotes": 7,
      "discussionId": "679ae5f0b898ac90bf44826c"
    },
    "publishedAt": "2025-01-29T21:38:42.464Z",
    "title": "Early External Safety Testing of OpenAI's o3-mini: Insights from the Pre-Deployment Evaluation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.17749.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5860
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.17433",
      "authors": [
        {
          "_id": "679b1319f87b99a2a7c41e36",
          "user": {
            "_id": "67325283b318faa97f7ae5f7",
            "avatarUrl": "/avatars/2f83452768148b323c540c43ad695ee6.svg",
            "isPro": false,
            "fullname": "TianshengHuang",
            "user": "TianshengHuang",
            "type": "user"
          },
          "name": "Tiansheng Huang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-01-30T08:39:47.548Z",
          "hidden": false
        },
        {
          "_id": "679b1319f87b99a2a7c41e37",
          "user": {
            "_id": "6539cab119c3ef6679794706",
            "avatarUrl": "/avatars/a88691ff5a547c7a1384edcc615c8209.svg",
            "isPro": false,
            "fullname": "Sihao Hu",
            "user": "SihaoHu",
            "type": "user"
          },
          "name": "Sihao Hu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-01-30T09:39:58.723Z",
          "hidden": false
        },
        {
          "_id": "679b1319f87b99a2a7c41e38",
          "user": {
            "_id": "647615b995a4dc98e58c24f2",
            "avatarUrl": "/avatars/7f73999246526c1aef4d019d5f5595ad.svg",
            "isPro": false,
            "fullname": "Fatih Ilhan",
            "user": "tawreos",
            "type": "user"
          },
          "name": "Fatih Ilhan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-01-30T09:40:06.004Z",
          "hidden": false
        },
        {
          "_id": "679b1319f87b99a2a7c41e39",
          "user": {
            "_id": "65aae89948c718a57434db6f",
            "avatarUrl": "/avatars/6c0fae8dafad9b9265098a9bc3bfc102.svg",
            "isPro": false,
            "fullname": "selim tekin",
            "user": "sftekin25",
            "type": "user"
          },
          "name": "Selim Furkan Tekin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-01-30T09:40:16.339Z",
          "hidden": false
        },
        {
          "_id": "679b1319f87b99a2a7c41e3a",
          "user": {
            "_id": "65c998005e17dbeaf147db84",
            "avatarUrl": "/avatars/6fb47b1e095971b93ff7dcd10369f926.svg",
            "isPro": false,
            "fullname": "Ling Liu",
            "user": "ling1119",
            "type": "user"
          },
          "name": "Ling Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-01-30T09:40:37.075Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-29T06:24:58.000Z",
      "title": "Virus : Ataque micro-optimisé nocif passant le modèle de Gated-Gail pour des modèles de langage grands",
      "summary": "Selon des études récentes, les modèles de langage grands (LLMs) sont vulnérables à des attaques dangereuses de micro-ajustements : le modèle perd sa capacité à s'adapter à la sécurité après avoir été ajusté avec des échantillons dangereux. Pour réduire le risque, une ligne de protection est généralement utilisée pour filtrer les échantillons dangereux avant l'ajustement micro. Dans cet article, un nouvel approche de rotation est conçue pour mettre en évidence les limites d'une seule ligne de protection qui dépend du filtrage des données. Le méthode d'attaque proposée, l'optimisation des données dangereuses par Virus, ne présente pas un taux de faux négatifs de 100% dans la sensibilité du système de protection micro-ajusté, tout en atteignant un bon rendement d'attaque. Enfin, la principale message transmis dans cet article est que de ignorer les limites des lignes de protection face aux attaques dangereuses de micro-ajustements est dangereux. Cela est dû au fait que les problèmes de sécurité propres aux LLMs pré-entraînés ne peuvent pas être résolus par ces lignes de protection. Le code est disponible sur GitHub : https://github.com/git-disl/Virus.",
      "upvotes": 2,
      "discussionId": "679b131bf87b99a2a7c41ede"
    },
    "publishedAt": "2025-01-30T01:30:18.013Z",
    "title": "Virus: Harmful Fine-tuning Attack for Large Language Models Bypassing Guardrail Moderation",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/67325283b318faa97f7ae5f7/1hJo5gEfGEXAwYB5a6yWY.png",
      "https://cdn-uploads.huggingface.co/production/uploads/67325283b318faa97f7ae5f7/8SaMXA1izw5vcfwtU2Nhj.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.17433.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "67325283b318faa97f7ae5f7",
      "avatarUrl": "/avatars/2f83452768148b323c540c43ad695ee6.svg",
      "fullname": "TianshengHuang",
      "name": "TianshengHuang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2501.17195",
      "authors": [
        {
          "_id": "679ae7655c55250b48483742",
          "name": "Andrei Alexandru",
          "hidden": false
        },
        {
          "_id": "679ae7655c55250b48483743",
          "user": {
            "_id": "66e184e86048d62cd8fb4e52",
            "avatarUrl": "/avatars/dc459c692fe9fce0911fa1229df0aeee.svg",
            "isPro": false,
            "fullname": "Antonia Calvi",
            "user": "NinaCalvi",
            "type": "user"
          },
          "name": "Antonia Calvi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-01-30T09:40:54.827Z",
          "hidden": false
        },
        {
          "_id": "679ae7655c55250b48483744",
          "name": "Henry Broomfield",
          "hidden": false
        },
        {
          "_id": "679ae7655c55250b48483745",
          "name": "Jackson Golden",
          "hidden": false
        },
        {
          "_id": "679ae7655c55250b48483746",
          "name": "Kyle Dai",
          "hidden": false
        },
        {
          "_id": "679ae7655c55250b48483747",
          "name": "Mathias Leys",
          "hidden": false
        },
        {
          "_id": "679ae7655c55250b48483748",
          "name": "Maurice Burger",
          "hidden": false
        },
        {
          "_id": "679ae7655c55250b48483749",
          "name": "Max Bartolo",
          "hidden": false
        },
        {
          "_id": "679ae7655c55250b4848374a",
          "name": "Roman Engeler",
          "hidden": false
        },
        {
          "_id": "679ae7655c55250b4848374b",
          "name": "Sashank Pisupati",
          "hidden": false
        },
        {
          "_id": "679ae7655c55250b4848374c",
          "name": "Toby Drane",
          "hidden": false
        },
        {
          "_id": "679ae7655c55250b4848374d",
          "name": "Young Sun Park",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-27T15:09:08.000Z",
      "title": "Atla Selene Mini : Modèle d'évaluation générale",
      "summary": "Atla Selene Mini se présente comme le modèle de langage le plus avancé pour son taille, offrant la capacité de reconnaissance de langage (SLMJ). Selene Mini a évalué son rendement sur 11 référentiels, démontrant être un évaluateur général qui dépasse SLMJ et GPT-4o-mini. Il dépasse dans une large gamme d'évaluations, y compris des scores absolus, la classification des classes et deux tâches de préférence. Dans RewardBench, il a enregistré le meilleur score d'un modèle de génération de 8B, dépassant GPT-4o et un évaluateur professionnel. Pour assurer la qualité, des filtres et l'élimination de jeux de données ont été utilisés pour garantir un haut niveau de qualité. Le modèle a été entraîné en combinant l'optimisation directe des préférences (DPO) et l'ajustement micro d'apprentissage par observation (SFT), ce qui le transforme en un évaluateur hautement testable. Ensemble avec les jeux de données de financement et de santé, l'évaluation des experts et l'accord de 0 shot ont été significativement améliorés, démontrant une résistance aux changements dans le format des tests. À partir des résultats initiaux, Selene Mini a été évaluée comme l'évaluateur le plus élevé dans des évaluations réelles. Les poids du modèle sont disponibles sur HuggingFace (https://hf.co/AtlaAI/Selene-1-Mini-Llama-3.1-8B) et Ollama, et on invite une large communauté à participer.",
      "upvotes": 2,
      "discussionId": "679ae76b5c55250b484838e0"
    },
    "publishedAt": "2025-01-29T21:44:37.041Z",
    "title": "Atla Selene Mini: A General Purpose Evaluation Model",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.17195.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5860
    },
    "isAuthorParticipating": false
  }
]