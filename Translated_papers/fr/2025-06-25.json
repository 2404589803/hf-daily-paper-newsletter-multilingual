[
  {
    "paper": {
      "id": "2506.19851",
      "authors": [
        {
          "_id": "685b5a46d2ee4fac76521dce",
          "user": {
            "_id": "6375d136dee28348a9c63cbf",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6375d136dee28348a9c63cbf/gK465HBrQWIOZ-qtHb-Vh.jpeg",
            "isPro": false,
            "fullname": "zehuan-huang",
            "user": "huanngzh",
            "type": "user"
          },
          "name": "Zehuan Huang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-25T08:19:21.031Z",
          "hidden": false
        },
        {
          "_id": "685b5a46d2ee4fac76521dcf",
          "user": {
            "_id": "65240d0ca801972b6eb12ed8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65240d0ca801972b6eb12ed8/hl2RAssBperb5JlgOIDvw.jpeg",
            "isPro": false,
            "fullname": "Haoran Feng",
            "user": "fenghora",
            "type": "user"
          },
          "name": "Haoran Feng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-25T09:19:31.409Z",
          "hidden": false
        },
        {
          "_id": "685b5a46d2ee4fac76521dd0",
          "user": {
            "_id": "63a41cb584a6a25c65bd8316",
            "avatarUrl": "/avatars/1d474831c320c7f9ca9e6d88f68acc06.svg",
            "isPro": false,
            "fullname": "Yangtian Sun",
            "user": "Yang-Tian",
            "type": "user"
          },
          "name": "Yangtian Sun",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-25T09:19:29.490Z",
          "hidden": false
        },
        {
          "_id": "685b5a46d2ee4fac76521dd1",
          "name": "Yuanchen Guo",
          "hidden": false
        },
        {
          "_id": "685b5a46d2ee4fac76521dd2",
          "user": {
            "_id": "638066faf022c8a5803f7eb8",
            "avatarUrl": "/avatars/4cfd699c3f6c5461b12b7dc5e3fe183d.svg",
            "isPro": false,
            "fullname": "Yanpei Cao",
            "user": "pookiefoof",
            "type": "user"
          },
          "name": "Yanpei Cao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-25T08:21:27.467Z",
          "hidden": false
        },
        {
          "_id": "685b5a46d2ee4fac76521dd3",
          "user": {
            "_id": "65b722dbe02a17f0f8d1cc6b",
            "avatarUrl": "/avatars/65f20601ef9b8ebfdddadd737f9153d6.svg",
            "isPro": false,
            "fullname": "Lu Sheng",
            "user": "lsheng2024",
            "type": "user"
          },
          "name": "Lu Sheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-25T09:19:33.186Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64a96a375a69e2ca889abdff/ttfXuQOJQmOqnO-_-F1op.mp4"
      ],
      "publishedAt": "2025-06-24T17:59:58.000Z",
      "submittedOnDailyAt": "2025-06-25T01:12:40.364Z",
      "title": "アニマX : Vidéo de mouvement des microorganismes en 3D - Modèle de Différenciation des Positions (Modèle PoseDIF)",
      "submittedOnDailyBy": {
        "_id": "64a96a375a69e2ca889abdff",
        "avatarUrl": "/avatars/f288c66ace09d907f132a79a740a3701.svg",
        "isPro": false,
        "fullname": "fanhongxing",
        "user": "fanhongxing",
        "type": "user"
      },
      "summary": "AnimaX est un cadre de création d'animation 3D en profondeur qui combine le méthode de synthèse de mouvements en 3D avec la structure contrôlable de l'animation basée sur des squelettes. Les méthodes de synthèse de mouvements existantes sont limitées aux squelettes fixes ou nécessitent un coût élevé dans l'espace de déformation de haute dimension. En revanche, AnimaX transmet efficacement le savoir des mouvements basé sur des vidéos aux zones 3D et soutient les artisans avec différents squelettes. Dans notre approche, les mouvements 3D sont représentés sur des cartes de positions 2D multi-angulaires et multi-frame, et les vidéos et poses continus peuvent être binairisées à l'aide de l'apprentissage de modèles et de prompts basés sur des chaînes de caractères. De plus, des codifications d'alignement spatio-temporelles sont introduites pour garantir la correspondance entre séquences de vidéos et poses, en utilisant des codifications de localisation partagées et des modèles de reconnaissance. Ainsi, le contrôle des vidéos peut être transmis efficacement à la tâche de génération de mouvements. De plus, ces séquences de poses multi-angulaires sont triangulées et transformées en positions d'articulations 3D, qui sont ensuite transformées en animation de surface par la cinématique inverse. AnimaX, entraîné avec un nouveau jeu de données (160 000 séquences de props), obtient les résultats les plus avancés en généralisation, précision et efficacité sur VBench, offrant une solution scalable pour la création d'animation 3D sans catégorie. La page du projet est disponible sur https://anima-x.github.io/.",
      "upvotes": 30,
      "discussionId": "685b5a47d2ee4fac76521dd4",
      "projectPage": "https://anima-x.github.io/",
      "githubRepo": "https://github.com/anima-x/anima-x",
      "ai_summary": "AnimaX creates multi-skeleton 3D animations by blending video diffusion model priors with skeleton-based control, using joint video-pose diffusion and shared positional encodings.",
      "ai_keywords": [
        "feed-forward 3D animation framework",
        "video diffusion models",
        "skeleton-based animation",
        "motion synthesis",
        "high-dimensional deformation spaces",
        "2D pose maps",
        "joint video-pose diffusion",
        "template renderings",
        "textual motion prompt",
        "shared positional encodings",
        "modality-aware embeddings",
        "spatial-temporal alignment",
        "inverse kinematics",
        "VBench",
        "category-agnostic 3D animation"
      ],
      "githubStars": 34
    },
    "publishedAt": "2025-06-24T13:59:58.000Z",
    "title": "AnimaX: Animating the Inanimate in 3D with Joint Video-Pose Diffusion\n  Models",
    "summary": "We present AnimaX, a feed-forward 3D animation framework that bridges the\nmotion priors of video diffusion models with the controllable structure of\nskeleton-based animation. Traditional motion synthesis methods are either\nrestricted to fixed skeletal topologies or require costly optimization in\nhigh-dimensional deformation spaces. In contrast, AnimaX effectively transfers\nvideo-based motion knowledge to the 3D domain, supporting diverse articulated\nmeshes with arbitrary skeletons. Our method represents 3D motion as multi-view,\nmulti-frame 2D pose maps, and enables joint video-pose diffusion conditioned on\ntemplate renderings and a textual motion prompt. We introduce shared positional\nencodings and modality-aware embeddings to ensure spatial-temporal alignment\nbetween video and pose sequences, effectively transferring video priors to\nmotion generation task. The resulting multi-view pose sequences are\ntriangulated into 3D joint positions and converted into mesh animation via\ninverse kinematics. Trained on a newly curated dataset of 160,000 rigged\nsequences, AnimaX achieves state-of-the-art results on VBench in\ngeneralization, motion fidelity, and efficiency, offering a scalable solution\nfor category-agnostic 3D animation. Project page:\nhttps://anima-x.github.io/{https://anima-x.github.io/}.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64a96a375a69e2ca889abdff/ttfXuQOJQmOqnO-_-F1op.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.19851.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64a96a375a69e2ca889abdff",
      "avatarUrl": "/avatars/f288c66ace09d907f132a79a740a3701.svg",
      "fullname": "fanhongxing",
      "name": "fanhongxing",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.16141",
      "authors": [
        {
          "_id": "6858b1fac0c8e29df8ea3c18",
          "name": "Yi Chen",
          "hidden": false
        },
        {
          "_id": "6858b1fac0c8e29df8ea3c19",
          "name": "Yuying Ge",
          "hidden": false
        },
        {
          "_id": "6858b1fac0c8e29df8ea3c1a",
          "name": "Rui Wang",
          "hidden": false
        },
        {
          "_id": "6858b1fac0c8e29df8ea3c1b",
          "name": "Yixiao Ge",
          "hidden": false
        },
        {
          "_id": "6858b1fac0c8e29df8ea3c1c",
          "name": "Junhao Cheng",
          "hidden": false
        },
        {
          "_id": "6858b1fac0c8e29df8ea3c1d",
          "name": "Ying Shan",
          "hidden": false
        },
        {
          "_id": "6858b1fac0c8e29df8ea3c1e",
          "name": "Xihui Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-19T08:49:13.000Z",
      "submittedOnDailyAt": "2025-06-25T01:50:33.428Z",
      "title": "GRPO-CARE : Apprentissage par Référence Orienté à la Cohérence pour l'Inférence Multi-Modulaire",
      "submittedOnDailyBy": {
        "_id": "60d045c4778bafd0fbcfa3f5",
        "avatarUrl": "/avatars/0cc0c2739c1934430ea09df7e9668c80.svg",
        "isPro": false,
        "fullname": "Yi Chen",
        "user": "ChenYi99",
        "type": "user"
      },
      "summary": "Récemment, l'approche d'apprentissage par renforcement appelée Global Reward with Observation Policy Optimization (GRPO) a favorisé le développement de la raison d'être dans le traitement du langage par des modèles de grands langages (LLMs), mais cette approche n'a pas été appliquée aux modèles de langages multimodales (MLLMs). Pour résoudre la manque d'évaluations strictes des méthodes d'entraînement de MLLMs, l'on a introduit le benchmark SEED-Bench-R1. Ce benchmark exige un reconnaissance équilibrée d'images réalistes complexes et une série logique de raisons, et fournit un ensemble de données d'entraînement important pour évaluer la généralisation sur trois niveaux : distribution interne, entre environnements et entre tâches. En utilisant SEED-Bench-R1, le GRPO standard a amélioré la précision des réponses mais a diminué la concordance entre les raisons logiques et les réponses, avec un pourcentage de concordance de 57,9 %. Cela est dû au fait que le GRPO se concentre sur la réponse finale, encourage des chemins courts et limite l'exploration par des pénalités strictes de KL. En réponse à cela, on propose le cadre d'apprentissage par renforcement GRPO-CARE, qui optimise à la fois la précision des réponses et la concordance des raisons logiques, sans nécessité de supervision explicite. GRPO-CARE introduit deux types de récompenses : (1) une récompense basique pour la précision de la réponse et (2) un bonus d'amélioration adaptatif pour la concordance des raisons logiques. Ce bonus est calculé en utilisant un modèle de référence qui est développé progressivement et compare des pairs de modèles. Cette structure double amplifie la récompense pour des chemins logiques précis. En remplaçant la pénalité de KL par ce bonus adaptatif, GRPO-CARE dépasse le GRPO standard sur SEED-Bench-R1, atteignant un amélioration de 6,7 % du rendement et un amélioration de 24,5 % de la concordance sur les niveaux les plus difficiles d'évaluation. De plus, il montre une forte transfert et améliore le rendement du modèle sur différents benchmarks de compréhension d'images. Notre étude fournit un benchmark conçu de manière systématique et un cadre d'entraînement postérieur qui peut être généralisé, contribuant au développement de MLLMs analytiques et robustes.",
      "upvotes": 22,
      "discussionId": "6858b1fac0c8e29df8ea3c1f",
      "githubRepo": "https://github.com/TencentARC/GRPO-CARE",
      "ai_summary": "GRPO-CARE, a reinforcement learning framework optimizing for consistency and correctness, outperforms standard GRPO on a new video understanding benchmark, SEED-Bench-R1, improving both performance and logical coherence in multimodal large language models.",
      "ai_keywords": [
        "reinforcement learning",
        "outcome-supervised GRPO",
        "Chain-of-Thought reasoning",
        "large language models",
        "multimodal large language models",
        "SEED-Bench-R1",
        "in-distribution",
        "cross-environment",
        "cross-environment-task",
        "logical coherence",
        "reasoning steps",
        "answer accuracy",
        "reward signals",
        "shortcuts",
        "KL penalties",
        "exploration",
        "consistency-aware RL framework",
        "two-tiered reward",
        "reasoning-to-answer likelihood",
        "adaptive consistency bonus",
        "video understanding benchmarks",
        "transferability",
        "interpretable models",
        "robust models"
      ],
      "githubStars": 19
    },
    "publishedAt": "2025-06-19T04:49:13.000Z",
    "title": "GRPO-CARE: Consistency-Aware Reinforcement Learning for Multimodal\n  Reasoning",
    "summary": "Recent reinforcement learning approaches, such as outcome-supervised GRPO,\nhave advanced Chain-of-Thought reasoning in large language models (LLMs), yet\ntheir adaptation to multimodal LLMs (MLLMs) is unexplored. To address the lack\nof rigorous evaluation for MLLM post-training methods, we introduce\nSEED-Bench-R1, a benchmark with complex real-world videos requiring balanced\nperception and reasoning. It offers a large training set and evaluates\ngeneralization across three escalating challenges: in-distribution,\ncross-environment, and cross-environment-task scenarios. Using SEED-Bench-R1,\nwe find that standard GRPO, while improving answer accuracy, often reduces\nlogical coherence between reasoning steps and answers, with only a 57.9%\nconsistency rate. This stems from reward signals focusing solely on final\nanswers, encouraging shortcuts, and strict KL penalties limiting exploration.To\naddress this, we propose GRPO-CARE, a consistency-aware RL framework optimizing\nboth answer correctness and reasoning coherence without explicit supervision.\nGRPO-CARE introduces a two-tiered reward: (1) a base reward for answer\ncorrectness, and (2) an adaptive consistency bonus, computed by comparing the\nmodel's reasoning-to-answer likelihood (via a slowly-evolving reference model)\nagainst group peers.This dual mechanism amplifies rewards for reasoning paths\nthat are both correct and logically consistent. Replacing KL penalties with\nthis adaptive bonus, GRPO-CARE outperforms standard GRPO on SEED-Bench-R1,\nachieving a 6.7% performance gain on the hardest evaluation level and a 24.5%\nimprovement in consistency. It also shows strong transferability, improving\nmodel performance across diverse video understanding benchmarks. Our work\ncontributes a systematically designed benchmark and a generalizable\npost-training framework, advancing the development of more interpretable and\nrobust MLLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.16141.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60d045c4778bafd0fbcfa3f5",
      "avatarUrl": "/avatars/0cc0c2739c1934430ea09df7e9668c80.svg",
      "fullname": "Yi Chen",
      "name": "ChenYi99",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.19848",
      "authors": [
        {
          "_id": "685b7cc2d2ee4fac76521e83",
          "name": "Long Xing",
          "hidden": false
        },
        {
          "_id": "685b7cc2d2ee4fac76521e84",
          "user": {
            "_id": "656f1b21b075b63c90ba02ee",
            "avatarUrl": "/avatars/d6856815ef06261394178161e4d511b4.svg",
            "isPro": false,
            "fullname": "Huang Qidong",
            "user": "shikiw",
            "type": "user"
          },
          "name": "Qidong Huang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-25T08:07:23.757Z",
          "hidden": false
        },
        {
          "_id": "685b7cc2d2ee4fac76521e85",
          "name": "Xiaoyi Dong",
          "hidden": false
        },
        {
          "_id": "685b7cc2d2ee4fac76521e86",
          "name": "Pan Zhang",
          "hidden": false
        },
        {
          "_id": "685b7cc2d2ee4fac76521e87",
          "user": {
            "_id": "63859cf3b2906edaf83af9f0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63859cf3b2906edaf83af9f0/mCgynEtoXdILvzyoPexii.png",
            "isPro": false,
            "fullname": "Yuhang Zang",
            "user": "yuhangzang",
            "type": "user"
          },
          "name": "Yuhang Zang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-25T08:07:21.449Z",
          "hidden": false
        },
        {
          "_id": "685b7cc2d2ee4fac76521e88",
          "name": "Yuhang Cao",
          "hidden": false
        },
        {
          "_id": "685b7cc2d2ee4fac76521e89",
          "name": "Jinsong Li",
          "hidden": false
        },
        {
          "_id": "685b7cc2d2ee4fac76521e8a",
          "name": "Shuangrui Ding",
          "hidden": false
        },
        {
          "_id": "685b7cc2d2ee4fac76521e8b",
          "name": "Weiming Zhang",
          "hidden": false
        },
        {
          "_id": "685b7cc2d2ee4fac76521e8c",
          "name": "Nenghai Yu",
          "hidden": false
        },
        {
          "_id": "685b7cc2d2ee4fac76521e8d",
          "name": "Jiaqi Wang",
          "hidden": false
        },
        {
          "_id": "685b7cc2d2ee4fac76521e8e",
          "name": "Feng Wu",
          "hidden": false
        },
        {
          "_id": "685b7cc2d2ee4fac76521e8f",
          "name": "Dahua Lin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-24T17:59:55.000Z",
      "submittedOnDailyAt": "2025-06-25T03:07:04.508Z",
      "title": "Escalado Captcha : Correction du biais dans un module double pour des captchas d'images échelonnées en inférence",
      "submittedOnDailyBy": {
        "_id": "64b4eec4faa3181a5eab9c46",
        "avatarUrl": "/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg",
        "isPro": true,
        "fullname": "Jiaqi Wang",
        "user": "myownskyW7",
        "type": "user"
      },
      "summary": "Dans cet article, nous présentons ScaleCap, une étape de captage d'images inférentielle et scalable, et expliquons comment des captures détaillées d'images sont générées. L'un des principaux problèmes de la captage d'images de haute qualité est l'inclinaison propre des LVLM (Modèles de Vision et Langue Grands) : l'inclinaison de la diversité peut causer un déséquilibre dans la qualité des descriptions, car certains éléments reçoivent une description détaillée tandis que d'autres ne le font pas ; l'inclinaison linguistique peut générer des descriptions distordues pour des objets qui n'existent pas dans le langage. Pour résoudre ces problèmes, ScaleCap propose une étape de captage d'images qui ne nécessite pas un dispositif scalable et qui peut enrichir et ajuster les captures de manière continue, bien que cela augmente les coûts d'inférence. Spécifiquement, il propose deux nouveaux composants : réponses à des questions heuristiques et évaluation contextuelle de phrases. La première génère et répond aux questions liées au contenu de l'image, ce qui permet d'introduire de l'information dans les captures d'images de manière progressive. La seconde utilise une évaluation contextuelle en ligne de phrases pour identifier et éliminer les distorsions causées par l'inclinaison linguistique. Bien que les coûts d'inférence augmentent, ScaleCap détecte de plus en plus de détails visuels, génère des captures plus précises, équilibrées et riches en information. Les expériences larges ont montré les effets de ScaleCap. Nous avons expliqué 450K images avec ScaleCap, et après son utilisation dans l'entraînement précédent des LVLM, nous avons observé un améliorament continu sur 11 référentiels largement utilisés. De plus, ScaleCap effectue deux tâches supplémentaires : remplacer les images par des captures et reconstruire les images à partir des captures pour évaluer la couverture significative, montrant ainsi la richesse et la fidélité des captures générées. Le code est disponible sur https://github.com/Cooperx521/ScaleCap.",
      "upvotes": 19,
      "discussionId": "685b7cc2d2ee4fac76521e90",
      "githubRepo": "https://github.com/Cooperx521/ScaleCap",
      "ai_summary": "ScaleCap enhances image captioning by iteratively enriching and calibrating captions using heuristic question answering and contrastive sentence rating, addressing multimodal and linguistic biases to improve accuracy, balance, and informativeness.",
      "ai_keywords": [
        "LVLMs",
        "multimodal bias",
        "linguistic bias",
        "heuristic question answering",
        "contrastive sentence rating",
        "VQA task"
      ],
      "githubStars": 22
    },
    "publishedAt": "2025-06-24T13:59:55.000Z",
    "title": "ScaleCap: Inference-Time Scalable Image Captioning via Dual-Modality\n  Debiasing",
    "summary": "This paper presents ScaleCap, an inference-time scalable image captioning\nstrategy that generates comprehensive and detailed image captions. The key\nchallenges of high-quality image captioning lie in the inherent biases of\nLVLMs: multimodal bias resulting in imbalanced descriptive granularity,\noffering detailed accounts of some elements while merely skimming over others;\nlinguistic bias leading to hallucinated descriptions of non-existent objects.\nTo address these issues, we propose a scalable debiased captioning strategy,\nwhich continuously enriches and calibrates the caption with increased inference\nbudget. Specifically, we propose two novel components: heuristic question\nanswering and contrastive sentence rating. The former generates\ncontent-specific questions based on the image and answers them to progressively\ninject relevant information into the caption. The latter employs sentence-level\noffline contrastive decoding to effectively identify and eliminate\nhallucinations caused by linguistic biases. With increased inference cost, more\nheuristic questions are raised by ScaleCap to progressively capture additional\nvisual details, generating captions that are more accurate, balanced, and\ninformative. Extensive modality alignment experiments demonstrate the\neffectiveness of ScaleCap. Annotating 450K images with ScaleCap and using them\nfor LVLM pretraining leads to consistent performance gains across 11 widely\nused benchmarks. Furthermore, ScaleCap showcases superb richness and fidelity\nof generated captions with two additional tasks: replacing images with captions\nin VQA task, and reconstructing images from captions to assess semantic\ncoverage. Code is available at https://github.com/Cooperx521/ScaleCap.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.19848.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b4eec4faa3181a5eab9c46",
      "avatarUrl": "/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg",
      "fullname": "Jiaqi Wang",
      "name": "myownskyW7",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 20
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.19290",
      "authors": [
        {
          "_id": "685b6640d2ee4fac76521e42",
          "user": {
            "_id": "6621efe1a6eec3ad03e38759",
            "avatarUrl": "/avatars/c35acce69f244ec0833dffd53eedf6a3.svg",
            "isPro": false,
            "fullname": "Liang Zeng",
            "user": "zengliangcs",
            "type": "user"
          },
          "name": "Liang Zeng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-25T08:08:30.529Z",
          "hidden": false
        },
        {
          "_id": "685b6640d2ee4fac76521e43",
          "user": {
            "_id": "612cfc6e1f69b222aacf831b",
            "avatarUrl": "/avatars/b6c7d15ebc7b5dd4b56620bfab324c77.svg",
            "isPro": false,
            "fullname": "lycfight",
            "user": "lycfight",
            "type": "user"
          },
          "name": "Yongcong Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-25T08:08:28.275Z",
          "hidden": false
        },
        {
          "_id": "685b6640d2ee4fac76521e44",
          "name": "Yuzhen Xiao",
          "hidden": false
        },
        {
          "_id": "685b6640d2ee4fac76521e45",
          "name": "Changshi Li",
          "hidden": false
        },
        {
          "_id": "685b6640d2ee4fac76521e46",
          "user": {
            "_id": "658229ef5f6d83438257fce5",
            "avatarUrl": "/avatars/b4417de9a338e95dc69cc547a46348e8.svg",
            "isPro": false,
            "fullname": "Chris (Yuhao) Liu",
            "user": "chrisliu298",
            "type": "user"
          },
          "name": "Chris Yuhao Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-25T08:08:22.815Z",
          "hidden": false
        },
        {
          "_id": "685b6640d2ee4fac76521e47",
          "name": "Rui Yan",
          "hidden": false
        },
        {
          "_id": "685b6640d2ee4fac76521e48",
          "name": "Tianwen Wei",
          "hidden": false
        },
        {
          "_id": "685b6640d2ee4fac76521e49",
          "name": "Jujie He",
          "hidden": false
        },
        {
          "_id": "685b6640d2ee4fac76521e4a",
          "name": "Xuchen Song",
          "hidden": false
        },
        {
          "_id": "685b6640d2ee4fac76521e4b",
          "name": "Yang Liu",
          "hidden": false
        },
        {
          "_id": "685b6640d2ee4fac76521e4c",
          "name": "Yahui Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-24T03:53:36.000Z",
      "submittedOnDailyAt": "2025-06-25T01:35:02.603Z",
      "title": "Skywork-SWE : Expliquez le processus d'échelle des données dans le développement de logiciels en utilisant des modèles de langage de haut niveau (LLMs).",
      "submittedOnDailyBy": {
        "_id": "6621efe1a6eec3ad03e38759",
        "avatarUrl": "/avatars/c35acce69f244ec0833dffd53eedf6a3.svg",
        "isPro": false,
        "fullname": "Liang Zeng",
        "user": "zengliangcs",
        "type": "user"
      },
      "summary": "Le Software Engineering (SWE) a acquis une importance croissante en tant que terrain de test pour les prochaines générations d'agents de LLM, avec deux compétences inhérentes nécessaires : la résolution de problèmes complexes en séquence (par exemple, interactions de plus de 50 tours) et la résolution de relations de contexte à long terme (par exemple, plus de 32,000 tokens). Cependant, le processus de préparation de données dans SWE peut nécessiter beaucoup de temps, car dépend de la filtration de fichiers de code et de la configuration d'environnements d'exécution spécifiques, ce qui peut sembler plus simple à comprendre qu'il est. En conséquence, les ensembles de données actuels sont presque exclusivement limités à des milliers d'instances de GitHub.\n\nDans ces conditions, nous proposons un processus d'automatisation progressive pour élargir systématiquement la dimension et la diversité des ensembles de données de SWE. Notre ensemble de données comprend 10,169 instances de tâches de Python en réalité, provenant de 2,531 repositories différents de GitHub, chacune spécifiée en nature et utilisée pour la validation de tests unitaires automatiques à travers des images d'environnements d'exécution. Nous avons préparé avec précaution un ensemble de données de validation d'exécution réussie de plus de 8,000 instances dans notre ensemble de données de SWE proposé. En ajustant le modèle Skywork-SWE avec cet ensemble de données, le rendement en matière de développement de logiciel d'un LLM s'améliore continuément avec l'augmentation de la quantité de données, sans laisser de traces de sur-ajustement. En particulier, notre modèle Skywork-SWE a atteint une précision de 38,0% sur le benchmark SWE-bench Verified, établissant un nouveau niveau d'excellence (SOTA) dans le cadre de OpenHands pour l'agent Qwen2.5-Coder-32B. De plus, l'introduction de la technologie d'échelle temporelle de tests a amélioré l'efficacité, dépassant les résultats SOTA de modèles de 32B de paramètres et atteignant une précision de 47,0%. Nous publions le checkpoint du modèle Skywork-SWE-32B et nous cherchons à accélérer futurs études.",
      "upvotes": 18,
      "discussionId": "685b6641d2ee4fac76521e4d",
      "projectPage": "https://quixotic-sting-239.notion.site/eb17f379610040ceb54da5d5d24065bd",
      "ai_summary": "An automated data-curation pipeline for software engineering improves large language model performance on SWE tasks, achieving state-of-the-art results with and without test-time scaling techniques.",
      "ai_keywords": [
        "LLM agents",
        "iterative problem-solving",
        "long-context dependency resolution",
        "code file filtering",
        "unit tests",
        "runtime environments",
        "data-curation pipeline",
        "software engineering capabilities",
        "Skywork-SWE model",
        "SWE-bench Verified",
        "pass@1 accuracy",
        "OpenHands agent framework",
        "test-time scaling techniques",
        "parameter-efficient fine-tuning"
      ]
    },
    "publishedAt": "2025-06-23T23:53:36.000Z",
    "title": "Skywork-SWE: Unveiling Data Scaling Laws for Software Engineering in\n  LLMs",
    "summary": "Software engineering (SWE) has recently emerged as a crucial testbed for\nnext-generation LLM agents, demanding inherent capabilities in two critical\ndimensions: sustained iterative problem-solving (e.g., >50 interaction rounds)\nand long-context dependency resolution (e.g., >32k tokens). However, the data\ncuration process in SWE remains notoriously time-consuming, as it heavily\nrelies on manual annotation for code file filtering and the setup of dedicated\nruntime environments to execute and validate unit tests. Consequently, most\nexisting datasets are limited to only a few thousand GitHub-sourced instances.\nTo this end, we propose an incremental, automated data-curation pipeline that\nsystematically scales both the volume and diversity of SWE datasets. Our\ndataset comprises 10,169 real-world Python task instances from 2,531 distinct\nGitHub repositories, each accompanied by a task specified in natural language\nand a dedicated runtime-environment image for automated unit-test validation.\nWe have carefully curated over 8,000 successfully runtime-validated training\ntrajectories from our proposed SWE dataset. When fine-tuning the Skywork-SWE\nmodel on these trajectories, we uncover a striking data scaling phenomenon: the\ntrained model's performance for software engineering capabilities in LLMs\ncontinues to improve as the data size increases, showing no signs of\nsaturation. Notably, our Skywork-SWE model achieves 38.0% pass@1 accuracy on\nthe SWE-bench Verified benchmark without using verifiers or multiple rollouts,\nestablishing a new state-of-the-art (SOTA) among the Qwen2.5-Coder-32B-based\nLLMs built on the OpenHands agent framework. Furthermore, with the\nincorporation of test-time scaling techniques, the performance further improves\nto 47.0% accuracy, surpassing the previous SOTA results for sub-32B parameter\nmodels. We release the Skywork-SWE-32B model checkpoint to accelerate future\nresearch.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.19290.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6621efe1a6eec3ad03e38759",
      "avatarUrl": "/avatars/c35acce69f244ec0833dffd53eedf6a3.svg",
      "fullname": "Liang Zeng",
      "name": "zengliangcs",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.18701",
      "authors": [
        {
          "_id": "685a14da0e4ad7e21975854d",
          "user": {
            "_id": "63aed0e7f873109b112dbb1b",
            "avatarUrl": "/avatars/0c07beb1ef7287128528b165d18371f6.svg",
            "isPro": false,
            "fullname": "Yifan Zhang",
            "user": "Vanint",
            "type": "user"
          },
          "name": "Yifan Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-25T08:09:21.431Z",
          "hidden": false
        },
        {
          "_id": "685a14da0e4ad7e21975854e",
          "name": "Chunli Peng",
          "hidden": false
        },
        {
          "_id": "685a14da0e4ad7e21975854f",
          "name": "Boyang Wang",
          "hidden": false
        },
        {
          "_id": "685a14da0e4ad7e219758550",
          "name": "Puyi Wang",
          "hidden": false
        },
        {
          "_id": "685a14da0e4ad7e219758551",
          "name": "Qingcheng Zhu",
          "hidden": false
        },
        {
          "_id": "685a14da0e4ad7e219758552",
          "name": "Fei Kang",
          "hidden": false
        },
        {
          "_id": "685a14da0e4ad7e219758553",
          "name": "Biao Jiang",
          "hidden": false
        },
        {
          "_id": "685a14da0e4ad7e219758554",
          "name": "Zedong Gao",
          "hidden": false
        },
        {
          "_id": "685a14da0e4ad7e219758555",
          "name": "Eric Li",
          "hidden": false
        },
        {
          "_id": "685a14da0e4ad7e219758556",
          "name": "Yang Liu",
          "hidden": false
        },
        {
          "_id": "685a14da0e4ad7e219758557",
          "name": "Yahui Zhou",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63aed0e7f873109b112dbb1b/2AbnZSPpuvFOad20XamiG.mp4"
      ],
      "publishedAt": "2025-06-23T14:40:49.000Z",
      "submittedOnDailyAt": "2025-06-25T07:50:07.299Z",
      "title": "Matrix-Game : Modèle de Monde Interactif Fondamental",
      "submittedOnDailyBy": {
        "_id": "63aed0e7f873109b112dbb1b",
        "avatarUrl": "/avatars/0c07beb1ef7287128528b165d18371f6.svg",
        "isPro": false,
        "fullname": "Yifan Zhang",
        "user": "Vanint",
        "type": "user"
      },
      "summary": "Matrix-Game est un modèle basé sur des mondes interactifs pour la génération de mondes de jeu contrôlables. Matrix-Game effectue une grande échelle d'entraînement non-étiqueté pour comprendre l'environnement, puis entraîne avec des étiquettes d'actions pour la génération de vidéos interactives. Pour cela, un ensemble de données détaillé Matrix-Game-MC a été sélectionné. Ces données comprennent plus de 2,700 heures de vidéos de jeux non-étiquetées et plus de 1,000 heures de clips étiquetés de haute qualité, ainsi que des annotations d'actions de clavier et souris. Notre modèle adopte le modèle de génération de monde à partir d'images contrôlables, d'actions utilisateur et de contexte d'action. Avec plus de 1,700 millions de paramètres, Matrix-Game permet un contrôle précis des actions des personnages et du mouvement de la caméra, tout en maintenant une qualité visuelle élevée et une cohérence temporelle. Pour évaluer le rendement, un cadre de référence intégré appelé GameWorld Score a été développé, qui évalue la qualité visuelle, la qualité temporelle, la possibilité de contrôle des actions et la compréhension des lois physiques dans la génération de vidéos. Par des expériences distribuées, Matrix-Game concorde et dépasse dans toutes les métriques les modèles de jeux open-source du passé semaine, comme Oasis et MineWorld. En particulier, un effet très fort a été observé sur la possibilité de contrôle et la cohérence physique. Les évaluations par des personnes non informées ont confirmé l'excellence de Matrix-Game et ont souligné sa capacité à générer des vidéos visuellement réalistes et précisément contrôlables dans différents scénarios de jeu. Les poids du modèle Matrix-Game et le cadre de référence GameWorld Score sont publiés pour des futures recherches sur la génération de mondes à partir d'images interactives.",
      "upvotes": 18,
      "discussionId": "685a14da0e4ad7e219758558",
      "projectPage": "https://matrix-game-homepage.github.io",
      "githubRepo": "https://github.com/SkyworkAI/Matrix-Game",
      "ai_summary": "Matrix-Game, a controllable game world generation model trained in a two-stage process, outperforms existing models by producing high-quality, action-controllable, and physically consistent Minecraft world videos.",
      "ai_keywords": [
        "Matrix-Game",
        "interactive world foundation model",
        "large-scale unlabeled pretraining",
        "action-labeled training",
        "contrrollable image-to-world generation",
        "Matrix-Game-MC",
        "motion context",
        "character actions",
        "camera movements",
        "visual quality",
        "temporal coherence",
        "GameWorld Score",
        "double-blind human evaluations",
        "interactive image-to-world generation",
        "Oasis",
        "MineWorld",
        "perceptually realistic"
      ],
      "githubStars": 744
    },
    "publishedAt": "2025-06-23T10:40:49.000Z",
    "title": "Matrix-Game: Interactive World Foundation Model",
    "summary": "We introduce Matrix-Game, an interactive world foundation model for\ncontrollable game world generation. Matrix-Game is trained using a two-stage\npipeline that first performs large-scale unlabeled pretraining for environment\nunderstanding, followed by action-labeled training for interactive video\ngeneration. To support this, we curate Matrix-Game-MC, a comprehensive\nMinecraft dataset comprising over 2,700 hours of unlabeled gameplay video clips\nand over 1,000 hours of high-quality labeled clips with fine-grained keyboard\nand mouse action annotations. Our model adopts a controllable image-to-world\ngeneration paradigm, conditioned on a reference image, motion context, and user\nactions. With over 17 billion parameters, Matrix-Game enables precise control\nover character actions and camera movements, while maintaining high visual\nquality and temporal coherence. To evaluate performance, we develop GameWorld\nScore, a unified benchmark measuring visual quality, temporal quality, action\ncontrollability, and physical rule understanding for Minecraft world\ngeneration. Extensive experiments show that Matrix-Game consistently\noutperforms prior open-source Minecraft world models (including Oasis and\nMineWorld) across all metrics, with particularly strong gains in\ncontrollability and physical consistency. Double-blind human evaluations\nfurther confirm the superiority of Matrix-Game, highlighting its ability to\ngenerate perceptually realistic and precisely controllable videos across\ndiverse game scenarios. To facilitate future research on interactive\nimage-to-world generation, we will open-source the Matrix-Game model weights\nand the GameWorld Score benchmark at https://github.com/SkyworkAI/Matrix-Game.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63aed0e7f873109b112dbb1b/2AbnZSPpuvFOad20XamiG.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18701.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63aed0e7f873109b112dbb1b",
      "avatarUrl": "/avatars/0c07beb1ef7287128528b165d18371f6.svg",
      "fullname": "Yifan Zhang",
      "name": "Vanint",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.18951",
      "authors": [
        {
          "_id": "685ba757d2ee4fac76521f47",
          "name": "Jinyang Li",
          "hidden": false
        },
        {
          "_id": "685ba757d2ee4fac76521f48",
          "user": {
            "_id": "653693cb8ee17cfd44eed8ce",
            "avatarUrl": "/avatars/82be2428bec4e06c0a15a27647b9b8aa.svg",
            "isPro": false,
            "fullname": "Xiaolong Li",
            "user": "xia01ongLi",
            "type": "user"
          },
          "name": "Xiaolong Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-25T08:07:05.354Z",
          "hidden": false
        },
        {
          "_id": "685ba757d2ee4fac76521f49",
          "name": "Ge Qu",
          "hidden": false
        },
        {
          "_id": "685ba757d2ee4fac76521f4a",
          "name": "Per Jacobsson",
          "hidden": false
        },
        {
          "_id": "685ba757d2ee4fac76521f4b",
          "name": "Bowen Qin",
          "hidden": false
        },
        {
          "_id": "685ba757d2ee4fac76521f4c",
          "name": "Binyuan Hui",
          "hidden": false
        },
        {
          "_id": "685ba757d2ee4fac76521f4d",
          "name": "Shuzheng Si",
          "hidden": false
        },
        {
          "_id": "685ba757d2ee4fac76521f4e",
          "name": "Nan Huo",
          "hidden": false
        },
        {
          "_id": "685ba757d2ee4fac76521f4f",
          "user": {
            "_id": "63a3eb8af460e4379b5991e7",
            "avatarUrl": "/avatars/7564a048d8496cac38d689178d90a8f9.svg",
            "isPro": false,
            "fullname": "Xiaohan Xu",
            "user": "Tebmer",
            "type": "user"
          },
          "name": "Xiaohan Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-25T09:19:04.596Z",
          "hidden": false
        },
        {
          "_id": "685ba757d2ee4fac76521f50",
          "name": "Yue Zhang",
          "hidden": false
        },
        {
          "_id": "685ba757d2ee4fac76521f51",
          "name": "Ziwei Tang",
          "hidden": false
        },
        {
          "_id": "685ba757d2ee4fac76521f52",
          "name": "Yuanshuai Li",
          "hidden": false
        },
        {
          "_id": "685ba757d2ee4fac76521f53",
          "name": "Florensia Widjaja",
          "hidden": false
        },
        {
          "_id": "685ba757d2ee4fac76521f54",
          "name": "Xintong Zhu",
          "hidden": false
        },
        {
          "_id": "685ba757d2ee4fac76521f55",
          "name": "Feige Zhou",
          "hidden": false
        },
        {
          "_id": "685ba757d2ee4fac76521f56",
          "name": "Yongfeng Huang",
          "hidden": false
        },
        {
          "_id": "685ba757d2ee4fac76521f57",
          "name": "Yannis Papakonstantinou",
          "hidden": false
        },
        {
          "_id": "685ba757d2ee4fac76521f58",
          "name": "Fatma Ozcan",
          "hidden": false
        },
        {
          "_id": "685ba757d2ee4fac76521f59",
          "name": "Chenhao Ma",
          "hidden": false
        },
        {
          "_id": "685ba757d2ee4fac76521f5a",
          "name": "Reynold Cheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-23T09:41:37.000Z",
      "submittedOnDailyAt": "2025-06-25T06:09:19.766Z",
      "title": "SWE-SQL : Comment trouver les étapes par étapes pour résoudre les problèmes SQL des utilisateurs dans des projets réels",
      "submittedOnDailyBy": {
        "_id": "6419435385030eca6ac94701",
        "avatarUrl": "/avatars/c49ff1991739de49ec98c8310ab21e46.svg",
        "isPro": false,
        "fullname": "Ge Qu",
        "user": "gq2138",
        "type": "user"
      },
      "summary": "La résolution de problèmes complexes de SQL reste un grand défi dans les applications de bases de données réelles. Les grands modèles de langage actuels (LLMs) dépassent dans la traduction de texte en SQL, mais n'ont pas été évalués avec rigueur dans le travail de débogage de problèmes SQL. Pour corriger cela, nous avons extrait des problèmes réels et nous avons construit un nouveau cadre de référence pour la débogage de problèmes SQL appelé BIRD-CRITIC, qui comprend 530 tâches critiques SQL récréatives (BIRD-CRITIC-PG) et 570 tâches multi-répertoires (BIRD-CRITIC-Multi). L'évaluation de base met en évidence la complexité de ces tâches, et le modèle leader O3-Mini a atteint un succès de 38.87% sur BIRD-CRITIC-PG et de 33.33% sur BIRD-CRITIC-Multi. De plus, le développement de modèles open-source pour des tâches de bases de données, en respectant la confidentialité des données et en soutenant le développement local, est crucial. Par conséquent, nous présentons Sql-fIX-Gym, un environnement d'entraînement pour améliorer la capacité des modèles open-source à la débogage de problèmes SQL. Cet environnement génère automatiquement des ensembles de données de problèmes à partir de SQL précis en utilisant la stratégie SQL-Rewind. Cependant, la méthode finale d'entraînement des modèles corrects dans de grands canaux d'audience n'a pas été étudiée. De plus, nous proposons le f-Plan Boosting pour extraire des plans de débogage d'haut niveau à partir de solutions SQL, ce qui augmente le succès de l'entraînement de 73.7%. Ces composants ont été intégrés pour construire un agent open-source appelé Bird-Fixer, basé sur Qwen-2.5-Coder-14B. Bird-Fixer a atteint un succès de 38.11% sur BIRD-CRITIC-PG et de 29.65% sur BIRD-CRITIC-Multi, ouvrant de manière significative la démocratisation de la capacité de débogage de SQL complexe, dépassant Claude-3.7-Sonnet et GPT-4.1. Le tableau des résultats et les codes source sont disponibles sur https://bird-critic.github.io/.",
      "upvotes": 10,
      "discussionId": "685ba758d2ee4fac76521f5b",
      "ai_summary": "A new benchmark and training environment for debugging SQL issues using advanced open-source models significantly improves their performance compared to proprietary solutions.",
      "ai_keywords": [
        "BIRD-CRITIC",
        "BIRD-CRITIC-PG",
        "BIRD-CRITIC-Multi",
        "PostgreSQL",
        "Six-Gym (Sql-fIX-Gym)",
        "SQL-Rewind",
        "f-Plan Boosting",
        "Bird-Fixer",
        "Qwen-2.5-Coder-14B",
        "Claude-3.7-Sonnet",
        "GPT-4.1"
      ]
    },
    "publishedAt": "2025-06-23T05:41:37.000Z",
    "title": "SWE-SQL: Illuminating LLM Pathways to Solve User SQL Issues in\n  Real-World Applications",
    "summary": "Resolution of complex SQL issues persists as a significant bottleneck in\nreal-world database applications. Current Large Language Models (LLMs), while\nadept at text-to-SQL translation, have not been rigorously evaluated on the\nmore challenging task of debugging SQL issues. To address this gap, we\nintroduce BIRD-CRITIC, a new SQL issue debugging benchmark comprising 530\nPostgreSQL tasks (BIRD-CRITIC-PG) and 570 multi-dialect tasks\n(BIRD-CRITIC-Multi), distilled from authentic user issues and replayed within\nnew environments to facilitate rigorous evaluation. Baseline evaluations\nunderscore the task's complexity, with the leading reasoning model O3-Mini\nachieving only 38.87% success rate on BIRD-CRITIC-PG and 33.33% on\nBIRD-CRITIC-Multi. Meanwhile, advancing open-source models for database tasks\nis crucial for empowering local development while safeguarding data privacy.\nTherefore, we present Six-Gym (Sql-fIX-Gym), a training environment for\nelevating open-source model capabilities for SQL issue debugging. This\nenvironment leverages SQL-Rewind strategy, which automatically generates\nexecutable issue-solution datasets by reverse-engineering issues from verified\nSQLs. However, popular trajectory-based fine-tuning methods do not explore\nsubstantial supervisory signals. We further propose f-Plan Boosting, which\nextracts high-level debugging plans from SQL solutions, enabling teacher LLMs\nto produce 73.7% more successful trajectories for training. We integrate these\ncomponents into an open-source agent, Bird-Fixer. Based on Qwen-2.5-Coder-14B,\nBird-Fixer achieves 38.11% success rate on BIRD-CRITIC-PG and 29.65% on\nBIRD-CRITIC-Multi, surpassing leading proprietary models such as\nClaude-3.7-Sonnet and GPT-4.1, marking a significant step toward democratizing\nsophisticated SQL-debugging capabilities. The leaderboard and source code are\navailable: https://bird-critic.github.io/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18951.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6419435385030eca6ac94701",
      "avatarUrl": "/avatars/c49ff1991739de49ec98c8310ab21e46.svg",
      "fullname": "Ge Qu",
      "name": "gq2138",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.19767",
      "authors": [
        {
          "_id": "685b5791d2ee4fac76521dc2",
          "user": {
            "_id": "670aa09d35918e99fe7ff6b1",
            "avatarUrl": "/avatars/5cbea2284165191e96544bacf2bfb50f.svg",
            "isPro": false,
            "fullname": "Yuqian Fu",
            "user": "Yuqian-Fu",
            "type": "user"
          },
          "name": "Yuqian Fu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-25T08:08:54.115Z",
          "hidden": false
        },
        {
          "_id": "685b5791d2ee4fac76521dc3",
          "name": "Tinghong Chen",
          "hidden": false
        },
        {
          "_id": "685b5791d2ee4fac76521dc4",
          "name": "Jiajun Chai",
          "hidden": false
        },
        {
          "_id": "685b5791d2ee4fac76521dc5",
          "name": "Xihuai Wang",
          "hidden": false
        },
        {
          "_id": "685b5791d2ee4fac76521dc6",
          "user": {
            "_id": "66e14f4142ceed655c731966",
            "avatarUrl": "/avatars/d708562349913b89c8c4cf384628f82a.svg",
            "isPro": false,
            "fullname": "SONGJUN TU",
            "user": "SONGJUNTU",
            "type": "user"
          },
          "name": "Songjun Tu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-25T08:08:52.066Z",
          "hidden": false
        },
        {
          "_id": "685b5791d2ee4fac76521dc7",
          "name": "Guojun Yin",
          "hidden": false
        },
        {
          "_id": "685b5791d2ee4fac76521dc8",
          "name": "Wei Lin",
          "hidden": false
        },
        {
          "_id": "685b5791d2ee4fac76521dc9",
          "name": "Qichao Zhang",
          "hidden": false
        },
        {
          "_id": "685b5791d2ee4fac76521dca",
          "name": "Yuanheng Zhu",
          "hidden": false
        },
        {
          "_id": "685b5791d2ee4fac76521dcb",
          "name": "Dongbin Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-24T16:31:37.000Z",
      "submittedOnDailyAt": "2025-06-25T01:33:02.160Z",
      "title": "SRFT : Sujet et simulation de rétroaction inclus dans un méthode à une étape",
      "submittedOnDailyBy": {
        "_id": "66e14f4142ceed655c731966",
        "avatarUrl": "/avatars/d708562349913b89c8c4cf384628f82a.svg",
        "isPro": false,
        "fullname": "SONGJUN TU",
        "user": "SONGJUNTU",
        "type": "user"
      },
      "summary": "Les grands modèles de langue (LLMs) ont réalisé un progrès impressionnant dans les tâches, mais l'intégration optimale des ajustements de fins supervisés (SFT) et l'apprentissage par reforcement (RL) est un problème fondamental. Il est nécessaire d'effectuer un analyse détaillée de la distribution des tokens, des dynamiques d'apprentissage, des structures d'intégration, à travers une perspective historique, pour révéler les relations entre ces paradigmes et leur impact sur l'efficacité et la robustesse des modèles. Cette analyse doit aborder tant l'optimisation de la structure d'entraînement que l'amélioration de l'efficacité de l'apprentissage, ainsi que l'intégration des connaissances historiques et l'influence de la structure d'entraînement sur la capacité du modèle à aborder de nouvelles tâches. De plus, il faut explorer comment la combinaison de ces approches peut améliorer la capacité de généralisation et la précision des modèles dans des contextes nouveaux et complexes.",
      "upvotes": 7,
      "discussionId": "685b5792d2ee4fac76521dcc",
      "projectPage": "https://anonymous.4open.science/w/SRFT2025",
      "ai_summary": " Supervised Reinforcement Fine-Tuning (SRFT) integrates Supervised Fine-Tuning and Reinforcement Learning through entropy-aware weighting to achieve high accuracy in language model optimization.",
      "ai_keywords": [
        "Large language models",
        "Supervised Fine-Tuning",
        "Reinforcement Learning",
        "token distributions",
        "learning dynamics",
        "entropy",
        "Supervised Reinforcement Fine-Tuning"
      ]
    },
    "publishedAt": "2025-06-24T12:31:37.000Z",
    "title": "SRFT: A Single-Stage Method with Supervised and Reinforcement\n  Fine-Tuning for Reasoning",
    "summary": "Large language models (LLMs) have achieved remarkable progress in reasoning\ntasks, yet the optimal integration of Supervised Fine-Tuning (SFT) and\nReinforcement Learning (RL) remains a fundamental challenge. Through\ncomprehensive analysis of token distributions, learning dynamics, and\nintegration mechanisms from entropy-based perspectives, we reveal key\ndifferences between these paradigms: SFT induces coarse-grained global changes\nto LLM policy distributions, while RL performs fine-grained selective\noptimizations, with entropy serving as a critical indicator of training\neffectiveness. Building on these observations, we propose Supervised\nReinforcement Fine-Tuning (SRFT), a single-stage method that unifies both\nfine-tuning paradigms through entropy-aware weighting mechanisms. Our approach\nsimultaneously applies SFT and RL to directly optimize the LLM using\ndemonstrations and self-exploration rollouts rather than through two-stage\nsequential methods. Extensive experiments show that SRFT achieves 59.1% average\naccuracy, outperforming zero-RL methods by 9.0% on five mathematical reasoning\nbenchmarks and 10.9% on three out-of-distribution benchmarks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.19767.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66e14f4142ceed655c731966",
      "avatarUrl": "/avatars/d708562349913b89c8c4cf384628f82a.svg",
      "fullname": "SONGJUN TU",
      "name": "SONGJUNTU",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.19838",
      "authors": [
        {
          "_id": "685b5e05d2ee4fac76521ddd",
          "name": "Liangbin Xie",
          "hidden": false
        },
        {
          "_id": "685b5e05d2ee4fac76521dde",
          "name": "Yu Li",
          "hidden": false
        },
        {
          "_id": "685b5e05d2ee4fac76521ddf",
          "name": "Shian Du",
          "hidden": false
        },
        {
          "_id": "685b5e05d2ee4fac76521de0",
          "name": "Menghan Xia",
          "hidden": false
        },
        {
          "_id": "685b5e05d2ee4fac76521de1",
          "name": "Xintao Wang",
          "hidden": false
        },
        {
          "_id": "685b5e05d2ee4fac76521de2",
          "name": "Fanghua Yu",
          "hidden": false
        },
        {
          "_id": "685b5e05d2ee4fac76521de3",
          "name": "Ziyan Chen",
          "hidden": false
        },
        {
          "_id": "685b5e05d2ee4fac76521de4",
          "name": "Pengfei Wan",
          "hidden": false
        },
        {
          "_id": "685b5e05d2ee4fac76521de5",
          "name": "Jiantao Zhou",
          "hidden": false
        },
        {
          "_id": "685b5e05d2ee4fac76521de6",
          "name": "Chao Dong",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-24T17:57:26.000Z",
      "submittedOnDailyAt": "2025-06-25T00:55:41.694Z",
      "title": "SimpleGVR: Résolution de Video Supérieure de Base Line Basique de la Connexion de Séquences Potentielles",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": true,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Les modèles de diffusion ont apparu comme un avancé avancé dans la génération de vidéos efficaces. Cependant, à mesure que les attentes des utilisateurs se concentrent sur la sortie à haute résolution, dépendre uniquement sur la diffusion a montré ses limites. Une approche attendue divise le processus en deux étapes : la génération de contenu sémantique et la synthèse de détails. La première étape utilise un modèle basique riche en calculs mais à faible résolution, tandis que la seconde étape met en œuvre une haute résolution de sortie en utilisant un modèle VSR (Résolution Vidéo Supérieure) léger. Dans cette étude, on explore les principes clés du design de modèles VSR continus, qui n'ont pas encore été étudiés. Tout d'abord, on propose deux stratégies de malice pour générer des paires d'entraînement qui améliorent la représentation des caractéristiques de la sortie du modèle basique, aidant à aligner le VSR avec le générateur supérieur. Ensuite, on effectue un analyse systématique de la stratégie d'échantillonnage dans les étapes temporelles et l'impact de l'ajout de bruit aux données à faible résolution (LR), fournissant des insights importants sur le comportement du VSR. Ces résultats ont un impact direct sur l'architecture et l'entraînement. Enfin, on effectue des entraînements efficaces et des inférences en utilisant des unités de temps courtes et une attention locale dispersée, réduisant significativement la surcharge de calcul. Les expériences de diffusion montrent que notre cadre dépasse les méthodes existantes, et les études de dévancement confirment l'effet de chaque choix de design. Notre étude fournit une base simple et efficace, offrant des pratiques réelles pour le développement de futurs systèmes de synthèse continus efficaces.",
      "upvotes": 6,
      "discussionId": "685b5e05d2ee4fac76521de7",
      "ai_summary": "Researchers propose design principles for cascaded video super-resolution models to improve high-resolution video generation by introduces degradation strategies, timestep sampling, noise augmentation, and interleaving temporal units with sparse local attention.",
      "ai_keywords": [
        "latent diffusion models",
        "video generation",
        "cascaded video super-resolution",
        "VSR",
        "degradation strategies",
        "timestep sampling",
        "noise augmentation",
        "interleaving temporal unit",
        "sparse local attention"
      ]
    },
    "publishedAt": "2025-06-24T13:57:26.000Z",
    "title": "SimpleGVR: A Simple Baseline for Latent-Cascaded Video Super-Resolution",
    "summary": "Latent diffusion models have emerged as a leading paradigm for efficient\nvideo generation. However, as user expectations shift toward higher-resolution\noutputs, relying solely on latent computation becomes inadequate. A promising\napproach involves decoupling the process into two stages: semantic content\ngeneration and detail synthesis. The former employs a computationally intensive\nbase model at lower resolutions, while the latter leverages a lightweight\ncascaded video super-resolution (VSR) model to achieve high-resolution output.\nIn this work, we focus on studying key design principles for latter cascaded\nVSR models, which are underexplored currently. First, we propose two\ndegradation strategies to generate training pairs that better mimic the output\ncharacteristics of the base model, ensuring alignment between the VSR model and\nits upstream generator. Second, we provide critical insights into VSR model\nbehavior through systematic analysis of (1) timestep sampling strategies, (2)\nnoise augmentation effects on low-resolution (LR) inputs. These findings\ndirectly inform our architectural and training innovations. Finally, we\nintroduce interleaving temporal unit and sparse local attention to achieve\nefficient training and inference, drastically reducing computational overhead.\nExtensive experiments demonstrate the superiority of our framework over\nexisting methods, with ablation studies confirming the efficacy of each design\nchoice. Our work establishes a simple yet effective baseline for cascaded video\nsuper-resolution generation, offering practical insights to guide future\nadvancements in efficient cascaded synthesis systems.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.19838.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": true,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7183
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.19794",
      "authors": [
        {
          "_id": "685b75d0d2ee4fac76521e70",
          "name": "Yuqi Zhu",
          "hidden": false
        },
        {
          "_id": "685b75d0d2ee4fac76521e71",
          "name": "Yi Zhong",
          "hidden": false
        },
        {
          "_id": "685b75d0d2ee4fac76521e72",
          "name": "Jintian Zhang",
          "hidden": false
        },
        {
          "_id": "685b75d0d2ee4fac76521e73",
          "name": "Ziheng Zhang",
          "hidden": false
        },
        {
          "_id": "685b75d0d2ee4fac76521e74",
          "name": "Shuofei Qiao",
          "hidden": false
        },
        {
          "_id": "685b75d0d2ee4fac76521e75",
          "name": "Yujie Luo",
          "hidden": false
        },
        {
          "_id": "685b75d0d2ee4fac76521e76",
          "name": "Lun Du",
          "hidden": false
        },
        {
          "_id": "685b75d0d2ee4fac76521e77",
          "name": "Da Zheng",
          "hidden": false
        },
        {
          "_id": "685b75d0d2ee4fac76521e78",
          "name": "Huajun Chen",
          "hidden": false
        },
        {
          "_id": "685b75d0d2ee4fac76521e79",
          "name": "Ningyu Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-24T17:04:23.000Z",
      "submittedOnDailyAt": "2025-06-25T02:37:00.536Z",
      "title": "Pourquoi les modèles de langage ouverts (LLMs) en confrontent-ils à des problèmes dans l'analyse de données ? Recherche expérimentale systématique",
      "submittedOnDailyBy": {
        "_id": "620b3bbb0668e435407c8d0a",
        "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
        "isPro": false,
        "fullname": "Ningyu Zhang",
        "user": "Ningyu",
        "type": "user"
      },
      "summary": "Les langages de programmation grands (LLMs) ont de nombreuses possibilités pour l'automatisation des tâches d'analyse de données, cependant, dans ces analyses raisonnables, les modèles d'open-source ont des limitations significatives. Dans cette étude, on examine des stratégies pour améliorer la capacité d'analyse de données des modèles d'open-source. On construit différents ensembles de données de test pratiques pour évaluer les modèles de trois manières : compréhension des données, génération de code et planification stratégique. Dans l'analyse, on a trouvé trois principales découvertes : (1) la qualité de la planification stratégique est un facteur décisif pour le rendement du modèle ; (2) le design de l'interaction et la complexité de la tâche affectent significativement la capacité raisonnable ; (3) la qualité des données a un impact plus grand que la diversité pour atteindre un rendement optimal. En utilisant ces découvertes, on développe des méthodes de synthèse de données et on améliore considérablement les capacités analytiques raisonnables des modèles d'open-source.",
      "upvotes": 6,
      "discussionId": "685b75d1d2ee4fac76521e7a",
      "ai_summary": "Enhancements to open-source large language models' data analysis capabilities through strategic planning, interaction design, and data quality improvements were identified and applied.",
      "ai_keywords": [
        "Large Language Models",
        "data analysis",
        "data understanding",
        "code generation",
        "strategic planning",
        "interaction design",
        "task complexity",
        "data quality",
        "data synthesis methodology"
      ]
    },
    "publishedAt": "2025-06-24T13:04:23.000Z",
    "title": "Why Do Open-Source LLMs Struggle with Data Analysis? A Systematic\n  Empirical Study",
    "summary": "Large Language Models (LLMs) hold promise in automating data analysis tasks,\nyet open-source models face significant limitations in these kinds of\nreasoning-intensive scenarios. In this work, we investigate strategies to\nenhance the data analysis capabilities of open-source LLMs. By curating a seed\ndataset of diverse, realistic scenarios, we evaluate models across three\ndimensions: data understanding, code generation, and strategic planning. Our\nanalysis reveals three key findings: (1) Strategic planning quality serves as\nthe primary determinant of model performance; (2) Interaction design and task\ncomplexity significantly influence reasoning capabilities; (3) Data quality\ndemonstrates a greater impact than diversity in achieving optimal performance.\nWe leverage these insights to develop a data synthesis methodology,\ndemonstrating significant improvements in open-source LLMs' analytical\nreasoning capabilities.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.19794.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "620b3bbb0668e435407c8d0a",
      "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
      "fullname": "Ningyu Zhang",
      "name": "Ningyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 24
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.19713",
      "authors": [
        {
          "_id": "685b9a5dd2ee4fac76521ecc",
          "user": {
            "_id": "63b4b02a103617b0a5b0ee2e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b4b02a103617b0a5b0ee2e/LasBC-pCnPJ6XuCt6qqcp.jpeg",
            "isPro": false,
            "fullname": "Seyedmorteza Sadat",
            "user": "msadat97",
            "type": "user"
          },
          "name": "Seyedmorteza Sadat",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-25T08:07:14.606Z",
          "hidden": false
        },
        {
          "_id": "685b9a5dd2ee4fac76521ecd",
          "name": "Tobias Vontobel",
          "hidden": false
        },
        {
          "_id": "685b9a5dd2ee4fac76521ece",
          "name": "Farnood Salehi",
          "hidden": false
        },
        {
          "_id": "685b9a5dd2ee4fac76521ecf",
          "name": "Romann M. Weber",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-24T15:19:42.000Z",
      "submittedOnDailyAt": "2025-06-25T05:16:23.771Z",
      "title": "Les guides de fréquence dans ce domaine permettent une échantillonnage de haute qualité à une échelle réduite.",
      "submittedOnDailyBy": {
        "_id": "63b4b02a103617b0a5b0ee2e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b4b02a103617b0a5b0ee2e/LasBC-pCnPJ6XuCt6qqcp.jpeg",
        "isPro": false,
        "fullname": "Seyedmorteza Sadat",
        "user": "msadat97",
        "type": "user"
      },
      "summary": "La Classification des Fréquences (CFG) fonctionne comme une composante importante des modèles de différenciation conditionnelle modernes. Il est connu qu'elle est très efficace en pratique, mais la structure qui améliore la qualité, le détail et l'attention au prompt de CFG n'a pas été complètement compris. Nous avons analysé l'impact de CFG dans le domaine des fréquences, démontrant que les fréquences basses et élevées ont des influences différentes. Spécifiquement, les guides de fréquences basses dominent la structure globale et l'attention au prompt, tandis que les guides de fréquences élevées améliorent principalement la fidélité visuelle. Cependant, la CFG standard applique la même échelle à toutes les fréquences, ce qui provoque une saturation et une diminution de la diversité à grandes échelles, et une perte de qualité visuelle à petites échelles. En se basant sur cette perspective, nous proposons la Guide des Fréquences Séparées (FDG). La FDG divise la CFG en composantes de fréquences basses et élevées, et applique une guide de fréquences séparées à chaque composante. L'objectif de la FDG est d'améliorer la qualité des images avec une échelle de guide basse et d'éviter les défauts de l'échelle de guide élevée. À travers une large gamme d'expérimentations avec différents ensembles de données et modèles, nous avons démontré que la FDG améliore la fidélité des échantillons, maintient la diversité et améliore les indices de fidélité (FID) et de rappel (recall) plus que la CFG. Nous avons établi notre méthode comme un plug-in et plateforme pour la CFG standard.",
      "upvotes": 6,
      "discussionId": "685b9a5ed2ee4fac76521ed0",
      "ai_summary": "Frequency-decoupled guidance (FDG) enhances image quality and diversity by separately controlling low- and high-frequency guidance components in diffusion models, outperforming standard classifier-free guidance.",
      "ai_keywords": [
        "classifier-free guidance",
        "conditional diffusion models",
        "frequency domain",
        "low-frequency guidance",
        "high-frequency guidance",
        "frequency-decoupled guidance",
        "FID",
        "recall"
      ]
    },
    "publishedAt": "2025-06-24T11:19:42.000Z",
    "title": "Guidance in the Frequency Domain Enables High-Fidelity Sampling at Low\n  CFG Scales",
    "summary": "Classifier-free guidance (CFG) has become an essential component of modern\nconditional diffusion models. Although highly effective in practice, the\nunderlying mechanisms by which CFG enhances quality, detail, and prompt\nalignment are not fully understood. We present a novel perspective on CFG by\nanalyzing its effects in the frequency domain, showing that low and high\nfrequencies have distinct impacts on generation quality. Specifically,\nlow-frequency guidance governs global structure and condition alignment, while\nhigh-frequency guidance mainly enhances visual fidelity. However, applying a\nuniform scale across all frequencies -- as is done in standard CFG -- leads to\noversaturation and reduced diversity at high scales and degraded visual quality\nat low scales. Based on these insights, we propose frequency-decoupled guidance\n(FDG), an effective approach that decomposes CFG into low- and high-frequency\ncomponents and applies separate guidance strengths to each component. FDG\nimproves image quality at low guidance scales and avoids the drawbacks of high\nCFG scales by design. Through extensive experiments across multiple datasets\nand models, we demonstrate that FDG consistently enhances sample fidelity while\npreserving diversity, leading to improved FID and recall compared to CFG,\nestablishing our method as a plug-and-play alternative to standard\nclassifier-free guidance.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.19713.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63b4b02a103617b0a5b0ee2e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b4b02a103617b0a5b0ee2e/LasBC-pCnPJ6XuCt6qqcp.jpeg",
      "fullname": "Seyedmorteza Sadat",
      "name": "msadat97",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.18843",
      "authors": [
        {
          "_id": "685a06460e4ad7e2197584c0",
          "user": {
            "_id": "6179f36a2a4e9edab3a95798",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6179f36a2a4e9edab3a95798/0mmFY5lFzPC5k6_GSdmYQ.jpeg",
            "isPro": false,
            "fullname": "Heng-Jui Chang",
            "user": "vectominist",
            "type": "user"
          },
          "name": "Heng-Jui Chang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-24T08:08:49.104Z",
          "hidden": false
        },
        {
          "_id": "685a06460e4ad7e2197584c1",
          "user": {
            "_id": "67d301cbba86f5d66eb73d7c",
            "avatarUrl": "/avatars/8546bbd2145c16d4be5675624516b649.svg",
            "isPro": false,
            "fullname": "Saurabhchand Bhati",
            "user": "saurabhati",
            "type": "user"
          },
          "name": "Saurabhchand Bhati",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-25T08:09:27.586Z",
          "hidden": false
        },
        {
          "_id": "685a06460e4ad7e2197584c2",
          "name": "James Glass",
          "hidden": false
        },
        {
          "_id": "685a06460e4ad7e2197584c3",
          "name": "Alexander H. Liu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6179f36a2a4e9edab3a95798/QPPxw2hyPglQF_q3uWSyk.png"
      ],
      "publishedAt": "2025-06-23T17:02:00.000Z",
      "submittedOnDailyAt": "2025-06-25T02:21:09.630Z",
      "title": "USAD : Récapitulation d'expériences à travers le langage général et les expressions vocales",
      "submittedOnDailyBy": {
        "_id": "6179f36a2a4e9edab3a95798",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6179f36a2a4e9edab3a95798/0mmFY5lFzPC5k6_GSdmYQ.jpeg",
        "isPro": false,
        "fullname": "Heng-Jui Chang",
        "user": "vectominist",
        "type": "user"
      },
      "summary": "L'apprentissage autosupervisé (SSL) a un impact innovant sur les représentations de la voix, mais les modèles sont généralement spécialisés dans des tâches de langage ou non verbales, et se concentrent sur chaque tâche. Dans cet article, nous présentons une méthodologie pour l'apprentissage de représentations de la voix qui intègre de manière unifiée différents types de voix linguistiques, acoustiques et musicales, nommée Universal Speech and Audio Distillation (USAD). USAD utilise un décrochage efficace par couches à partir de modèles SSL spécialisés pour entraîner des modèles élèves avec des ensembles de données de la voix pratiques. USAD montre un comportement fort dans différents cadres de référence et ensembles de données, obtenant des résultats exceptionnels dans des tâches de traitement du langage, d'étiquetage de la voix, de classification acoustique, entre autres, et atteignant les limites supérieures dans les cadres de référence SUPERB et HEAR.",
      "upvotes": 6,
      "discussionId": "685a06470e4ad7e2197584c4",
      "ai_summary": "USAD integrates diverse audio types using efficient layer-to-layer distillation from domain-specific models, achieving competitive performance across various benchmarks with a single encoder.",
      "ai_keywords": [
        "self-supervised learning",
        "universal speech and audio distillation",
        "domain-specific models",
        "layer-to-layer distillation",
        "frame and instance-level speech processing",
        "audio tagging",
        "sound classification",
        "encoder",
        "SUPERB benchmarks",
        "HEAR benchmarks"
      ]
    },
    "publishedAt": "2025-06-23T13:02:00.000Z",
    "title": "USAD: Universal Speech and Audio Representation via Distillation",
    "summary": "Self-supervised learning (SSL) has revolutionized audio representations, yet\nmodels often remain domain-specific, focusing on either speech or non-speech\ntasks. In this work, we present Universal Speech and Audio Distillation (USAD),\na unified approach to audio representation learning that integrates diverse\naudio types - speech, sound, and music - into a single model. USAD employs\nefficient layer-to-layer distillation from domain-specific SSL models to train\na student on a comprehensive audio dataset. USAD offers competitive performance\nacross various benchmarks and datasets, including frame and instance-level\nspeech processing tasks, audio tagging, and sound classification, achieving\nnear state-of-the-art results with a single encoder on SUPERB and HEAR\nbenchmarks.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6179f36a2a4e9edab3a95798/QPPxw2hyPglQF_q3uWSyk.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18843.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6179f36a2a4e9edab3a95798",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6179f36a2a4e9edab3a95798/0mmFY5lFzPC5k6_GSdmYQ.jpeg",
      "fullname": "Heng-Jui Chang",
      "name": "vectominist",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.19807",
      "authors": [
        {
          "_id": "685b75edd2ee4fac76521e7c",
          "name": "Baochang Ren",
          "hidden": false
        },
        {
          "_id": "685b75edd2ee4fac76521e7d",
          "name": "Shuofei Qiao",
          "hidden": false
        },
        {
          "_id": "685b75edd2ee4fac76521e7e",
          "name": "Wenhao Yu",
          "hidden": false
        },
        {
          "_id": "685b75edd2ee4fac76521e7f",
          "name": "Huajun Chen",
          "hidden": false
        },
        {
          "_id": "685b75edd2ee4fac76521e80",
          "name": "Ningyu Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-24T17:17:17.000Z",
      "submittedOnDailyAt": "2025-06-25T02:37:40.331Z",
      "title": "KnowRL : Apprentissage par Renforcement avec Connaissance pour l'Exploration de la Vérité",
      "submittedOnDailyBy": {
        "_id": "620b3bbb0668e435407c8d0a",
        "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
        "isPro": false,
        "fullname": "Ningyu Zhang",
        "user": "Ningyu",
        "type": "user"
      },
      "summary": "Les modèles de langue générale (LLMs), en particulier les modèles lents de pensée, ne peuvent pas reconnaître exactement les limites de la connaissance lorsqu'ils expliquent des raisons, ce qui provoque une confusion stricte et de nombreux cas de sortie de contenu incorrect. L'apprentissage par renforcement (RL) peut améliorer la capacité de raisonnement complexe, mais la structure de récompense liée aux objectifs manque de suffisamment de survie réelle et favorise la confusion. Pour résoudre les hauts niveaux de confusion des modèles lents de pensée, nous proposons l'apprentissage connu (KnowRL). KnowRL intègre la récompense de la réalité dans le processus d'apprentissage RL, ce qui conduit le modèle à penser lentement en fonction de la réalité et aide à reconnaître les limites de la connaissance. Pendant le processus d'apprentissage RL, le modèle apprend et internalise des stratégies de raisonnement basées sur la réalité, grâce à des entrées de données factuelles. En fournissant une récompense directe pour agir selon la réalité au cours du processus de raisonnement, KnowRL encourage un processus de pensée fiable. Les résultats des expériences avec 3 jeux de données d'évaluation de confusion et 2 jeux de données d'évaluation de raisonnement montrent que KnowRL réduit effectivement la confusion des modèles lents de pensée et maintient leur forte capacité de raisonnement originale. Le code est disponible sur : https://github.com/zjunlp/KnowRL.",
      "upvotes": 4,
      "discussionId": "685b75edd2ee4fac76521e81",
      "ai_summary": "KnowRL, a knowledge-enhanced reinforcement learning approach, reduces hallucinations in slow-thinking large language models by incorporating factuality rewards based on knowledge verification during training.",
      "ai_keywords": [
        "Large Language Models",
        "slow-thinking models",
        "hallucination",
        "Reinforcement Learning",
        "KnowRL",
        "factuality reward",
        "knowledge verification",
        "reasoning",
        "reasoning capabilities"
      ]
    },
    "publishedAt": "2025-06-24T13:17:17.000Z",
    "title": "KnowRL: Exploring Knowledgeable Reinforcement Learning for Factuality",
    "summary": "Large Language Models (LLMs), particularly slow-thinking models, often\nexhibit severe hallucination, outputting incorrect content due to an inability\nto accurately recognize knowledge boundaries during reasoning. While\nReinforcement Learning (RL) can enhance complex reasoning abilities, its\noutcome-oriented reward mechanism often lacks factual supervision over the\nthinking process, further exacerbating the hallucination problem. To address\nthe high hallucination in slow-thinking models, we propose Knowledge-enhanced\nRL, KnowRL. KnowRL guides models to perform fact-based slow thinking by\nintegrating a factuality reward, based on knowledge verification, into the RL\ntraining process, helping them recognize their knowledge boundaries. KnowRL\nguides models to perform fact-based slow thinking by integrating a factuality\nreward, based on knowledge verification, into the RL training process, helping\nthem recognize their knowledge boundaries. This targeted factual input during\nRL training enables the model to learn and internalize fact-based reasoning\nstrategies. By directly rewarding adherence to facts within the reasoning\nsteps, KnowRL fosters a more reliable thinking process. Experimental results on\nthree hallucination evaluation datasets and two reasoning evaluation datasets\ndemonstrate that KnowRL effectively mitigates hallucinations in slow-thinking\nmodels while maintaining their original strong reasoning capabilities. Our code\nis available at https://github.com/zjunlp/KnowRL.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.19807.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "620b3bbb0668e435407c8d0a",
      "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
      "fullname": "Ningyu Zhang",
      "name": "Ningyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 24
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.17612",
      "authors": [
        {
          "_id": "685b7538d2ee4fac76521e63",
          "user": {
            "_id": "64ecb174f22081b4ac7ca397",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ecb174f22081b4ac7ca397/PiAPtD_rbuhGOqfE6ZSIu.jpeg",
            "isPro": true,
            "fullname": "Yunlong Lin",
            "user": "LYL1015",
            "type": "user"
          },
          "name": "Yunlong Lin",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-25T08:07:25.557Z",
          "hidden": false
        },
        {
          "_id": "685b7538d2ee4fac76521e64",
          "name": "Zixu Lin",
          "hidden": false
        },
        {
          "_id": "685b7538d2ee4fac76521e65",
          "name": "Kunjie Lin",
          "hidden": false
        },
        {
          "_id": "685b7538d2ee4fac76521e66",
          "name": "Jinbin Bai",
          "hidden": false
        },
        {
          "_id": "685b7538d2ee4fac76521e67",
          "name": "Panwang Pan",
          "hidden": false
        },
        {
          "_id": "685b7538d2ee4fac76521e68",
          "name": "Chenxin Li",
          "hidden": false
        },
        {
          "_id": "685b7538d2ee4fac76521e69",
          "name": "Haoyu Chen",
          "hidden": false
        },
        {
          "_id": "685b7538d2ee4fac76521e6a",
          "name": "Zhongdao Wang",
          "hidden": false
        },
        {
          "_id": "685b7538d2ee4fac76521e6b",
          "name": "Xinghao Ding",
          "hidden": false
        },
        {
          "_id": "685b7538d2ee4fac76521e6c",
          "name": "Wenbo Li",
          "hidden": false
        },
        {
          "_id": "685b7538d2ee4fac76521e6d",
          "name": "Shuicheng Yan",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64ecb174f22081b4ac7ca397/PCTU9EbA4XUz8eKeLbxVC.gif"
      ],
      "publishedAt": "2025-06-21T06:36:00.000Z",
      "submittedOnDailyAt": "2025-06-25T02:43:05.885Z",
      "title": "Jabize Art : Libération de la création artistique humaine par l'agent de traitement de photos intelligents",
      "submittedOnDailyBy": {
        "_id": "64ecb174f22081b4ac7ca397",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ecb174f22081b4ac7ca397/PiAPtD_rbuhGOqfE6ZSIu.jpeg",
        "isPro": true,
        "fullname": "Yunlong Lin",
        "user": "LYL1015",
        "type": "user"
      },
      "summary": "L'édition de photos a acquis un rôle important comme composant des narrations visuelles modernes. Les utilisateurs peuvent exprimer leur sens esthétique et créativité. Des outils professionnels tels que Adobe Lightroom, qui disposent de fonctionnalités puissantes, nécessitent cependant des connaissances approfondies et un effort manuel. D'autre part, les solutions basées sur l'IA actuelles offrent une automatisation, mais avec des limitations dans l'ajustement et la généralité, ce qui n'est pas adapté pour l'édition de différents utilisateurs. Pour atténuer ces différences, on présente JarvisArt, un agent qui exécute un modèle de langage multimodal (MLLM) conçu pour collaborer de manière stratégique avec plus de 200 outils d'édition à l'intérieur de Lightroom. JarvisArt s'améliore à travers deux étapes d'entraînement : dans la première, la logique et l'utilisation des outils sont ajustées via un processus de rétroaction de la Chine de la Théorie de la Chaîne de Pensée (Chain-of-Thought), et dans la seconde, la décision d'édition et la perfection des outils sont améliorées par la Politique d'Optimisation Relative pour Retroque (GRPO-R). De plus, il propose un protocole Agent-to-Lightroom pour favoriser une intégration sans restrictions entre l'agent et Lightroom. Dans l'évaluation du rendement, un nouveau benchmark appelé MMArt-Bench a été développé, dans lequel JarvisArt a démontré une interface agréable pour l'utilisateur, une grande généralité et un contrôle micro des ajustements globaux et régionaux, ouvrant de nouvelles perspectives dans l'édition de photos. En particulier, il a amélioré les résultats de MMArt-Bench et GPT-4o d'au-delà de 60%, tout en maintenant la capacité de suivre les mêmes instructions. Le site web du projet est disponible sur https://jarvisart.vercel.app/.",
      "upvotes": 4,
      "discussionId": "685b7539d2ee4fac76521e6e",
      "projectPage": "https://jarvisart.vercel.app/",
      "githubRepo": "https://github.com/LYL1015/JarvisArt",
      "ai_summary": "JarvisArt, an MLLM-driven agent, achieves superior photo retouching by understanding user intent and coordinating multiple retouching tools in Lightroom, outperforming GPT-4o on a novel benchmark.",
      "ai_keywords": [
        "multi-modal large language model",
        "Chain-of-Thought supervised fine-tuning",
        "Group Relative Policy Optimization",
        "Agent-to-Lightroom Protocol",
        "MMArt-Bench",
        "global adjustments",
        "local adjustments",
        "content fidelity",
        "instruction-following capabilities"
      ],
      "githubStars": 3
    },
    "publishedAt": "2025-06-21T02:36:00.000Z",
    "title": "JarvisArt: Liberating Human Artistic Creativity via an Intelligent Photo\n  Retouching Agent",
    "summary": "Photo retouching has become integral to contemporary visual storytelling,\nenabling users to capture aesthetics and express creativity. While professional\ntools such as Adobe Lightroom offer powerful capabilities, they demand\nsubstantial expertise and manual effort. In contrast, existing AI-based\nsolutions provide automation but often suffer from limited adjustability and\npoor generalization, failing to meet diverse and personalized editing needs. To\nbridge this gap, we introduce JarvisArt, a multi-modal large language model\n(MLLM)-driven agent that understands user intent, mimics the reasoning process\nof professional artists, and intelligently coordinates over 200 retouching\ntools within Lightroom. JarvisArt undergoes a two-stage training process: an\ninitial Chain-of-Thought supervised fine-tuning to establish basic reasoning\nand tool-use skills, followed by Group Relative Policy Optimization for\nRetouching (GRPO-R) to further enhance its decision-making and tool\nproficiency. We also propose the Agent-to-Lightroom Protocol to facilitate\nseamless integration with Lightroom. To evaluate performance, we develop\nMMArt-Bench, a novel benchmark constructed from real-world user edits.\nJarvisArt demonstrates user-friendly interaction, superior generalization, and\nfine-grained control over both global and local adjustments, paving a new\navenue for intelligent photo retouching. Notably, it outperforms GPT-4o with a\n60% improvement in average pixel-level metrics on MMArt-Bench for content\nfidelity, while maintaining comparable instruction-following capabilities.\nProject Page: https://jarvisart.vercel.app/.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64ecb174f22081b4ac7ca397/PCTU9EbA4XUz8eKeLbxVC.gif"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.17612.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64ecb174f22081b4ac7ca397",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ecb174f22081b4ac7ca397/PiAPtD_rbuhGOqfE6ZSIu.jpeg",
      "fullname": "Yunlong Lin",
      "name": "LYL1015",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.19850",
      "authors": [
        {
          "_id": "685b63c2d2ee4fac76521dee",
          "name": "Yuqi Wang",
          "hidden": false
        },
        {
          "_id": "685b63c2d2ee4fac76521def",
          "name": "Xinghang Li",
          "hidden": false
        },
        {
          "_id": "685b63c2d2ee4fac76521df0",
          "name": "Wenxuan Wang",
          "hidden": false
        },
        {
          "_id": "685b63c2d2ee4fac76521df1",
          "name": "Junbo Zhang",
          "hidden": false
        },
        {
          "_id": "685b63c2d2ee4fac76521df2",
          "name": "Yingyan Li",
          "hidden": false
        },
        {
          "_id": "685b63c2d2ee4fac76521df3",
          "name": "Yuntao Chen",
          "hidden": false
        },
        {
          "_id": "685b63c2d2ee4fac76521df4",
          "name": "Xinlong Wang",
          "hidden": false
        },
        {
          "_id": "685b63c2d2ee4fac76521df5",
          "name": "Zhaoxiang Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-24T17:59:57.000Z",
      "submittedOnDailyAt": "2025-06-25T06:01:30.093Z",
      "title": "Modèle de Langage d'Action de Vision Intégrée",
      "submittedOnDailyBy": {
        "_id": "649fe21d59c1ae90dbfacf91",
        "avatarUrl": "/avatars/7f77fa77113e80cb45406927e2386387.svg",
        "isPro": false,
        "fullname": "Wang Yuqi",
        "user": "Yuqi1997",
        "type": "user"
      },
      "summary": "Les modèles de vision-langue-action (VLAs) se concentrent sur la possibilité de pousser les actions de mains mécaniques. Cependant, les méthodes précédentes ont principalement utilisé la capacité générale de compréhension des modèles de langue et de vision (VLMs) pour générer des signaux d'action, laissant de côté la structure temporelle et causale riche qui est incluse dans les observations visuelles. Dans cet article, nous proposons le modèle VLA unifié et continu, nommé UniVLA. Ce modèle modélise les signaux visuels, linguistiques et d'action comme séquences de tokens dispersés de manière automatique et récursive. Cette configuration facilite particulièrement l'apprentissage de tâches flexibles multiformes sur de grands ensembles de données vidéo. Après avoir été entraîné, UniVLA applique une modélisation du monde et promeut une transition efficace des politiques pour comprendre la dynamique causale dans les vidéos. Notre approche a obtenu de nouveaux résultats de distance minimale dans des cadres d'évaluation simulés largement utilisés comme CALVIN, LIBERO et Simplenv-Bridge, surpassant considérablement les méthodes précédentes. Par exemple, UniVLA a atteint un pourcentage moyen de succès de 95,5% sur le benchmark LIBERO, surpassant pi0-FAST avec un 85,5%. De plus, nous avons démontré une large gamme d'applications en action réelle et autonome dans le monde réel.",
      "upvotes": 3,
      "discussionId": "685b63c3d2ee4fac76521df6",
      "ai_summary": "UniVLA is a multimodal VLA model that autoregressively processes vision, language, and action as token sequences, incorporating world modeling for effective long-horizon policy learning and achieving state-of-the-art results across simulation and real-world benchmarks.",
      "ai_keywords": [
        "vision-language-action models",
        "VLAs",
        "vision-language models",
        "VLMs",
        "autoregressive models",
        "discrete token sequences",
        "multimodal tasks learning",
        "world modeling",
        "causal dynamics",
        "policy learning",
        "simulation benchmarks",
        "CALVIN",
        "LIBERO",
        "Simplenv-Bridge",
        "ALOHA manipulation",
        "autonomous driving"
      ]
    },
    "publishedAt": "2025-06-24T13:59:57.000Z",
    "title": "Unified Vision-Language-Action Model",
    "summary": "Vision-language-action models (VLAs) have garnered significant attention for\ntheir potential in advancing robotic manipulation. However, previous approaches\npredominantly rely on the general comprehension capabilities of vision-language\nmodels (VLMs) to generate action signals, often overlooking the rich temporal\nand causal structure embedded in visual observations. In this paper, we present\nUniVLA, a unified and native multimodal VLA model that autoregressively models\nvision, language, and action signals as discrete token sequences. This\nformulation enables flexible multimodal tasks learning, particularly from\nlarge-scale video data. By incorporating world modeling during post-training,\nUniVLA captures causal dynamics from videos, facilitating effective transfer to\ndownstream policy learning--especially for long-horizon tasks. Our approach\nsets new state-of-the-art results across several widely used simulation\nbenchmarks, including CALVIN, LIBERO, and Simplenv-Bridge, significantly\nsurpassing previous methods. For example, UniVLA achieves 95.5% average success\nrate on LIBERO benchmark, surpassing pi0-FAST's 85.5%. We further demonstrate\nits broad applicability on real-world ALOHA manipulation and autonomous\ndriving.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.19850.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "649fe21d59c1ae90dbfacf91",
      "avatarUrl": "/avatars/7f77fa77113e80cb45406927e2386387.svg",
      "fullname": "Wang Yuqi",
      "name": "Yuqi1997",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.14012",
      "authors": [
        {
          "_id": "685b863bd2ee4fac76521e92",
          "user": {
            "_id": "655efd24afee0e00788bb589",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/655efd24afee0e00788bb589/22guLxIWNybbJR3jI-c4w.jpeg",
            "isPro": false,
            "fullname": "Amr Mohamed",
            "user": "amr-mohamed",
            "type": "user"
          },
          "name": "Amr Mohamed",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-25T08:07:19.266Z",
          "hidden": false
        },
        {
          "_id": "685b863bd2ee4fac76521e93",
          "name": "Yang Zhang",
          "hidden": false
        },
        {
          "_id": "685b863bd2ee4fac76521e94",
          "name": "Michalis Vazirgiannis",
          "hidden": false
        },
        {
          "_id": "685b863bd2ee4fac76521e95",
          "user": {
            "_id": "6087e598e2b7cc3a117b0dc5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6087e598e2b7cc3a117b0dc5/Ctz_W-uo1gOQRBHXalD1P.png",
            "isPro": false,
            "fullname": "Guokan Shang",
            "user": "guokan-shang",
            "type": "user"
          },
          "name": "Guokan Shang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-25T08:07:16.772Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-16T21:19:27.000Z",
      "submittedOnDailyAt": "2025-06-25T03:51:26.828Z",
      "title": "Micks est confus : Évaluation de la compréhension du texte de switch de code.",
      "submittedOnDailyBy": {
        "_id": "655efd24afee0e00788bb589",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/655efd24afee0e00788bb589/22guLxIWNybbJR3jI-c4w.jpeg",
        "isPro": false,
        "fullname": "Amr Mohamed",
        "user": "amr-mohamed",
        "type": "user"
      },
      "summary": "Le code de changement (CSW) est un comportement intérieur d'un décodeur qui permet d'échanger deux ou plusieurs langues. Ce phénomène est largement observé dans des sociétés multilingues et a évolué naturellement dans la communication quotidienne, où plusieurs langues sont utilisées dans les contenus en ligne. En conséquence, les modèles de langage à grande échelle (LLMs) ont acquis un rôle central dans le traitement et la génération de contenu, mais ils reçoivent souvent des entrées qui sont CSW. Par conséquent, il est crucial de comprendre comment les LLMs traitent et interprètent ces textes mélangés de langues. Dans cet article, une version CSW des cadres de référence de raisonnement et compréhension est générée, et la compréhension de CSW dans les LLMs est évaluée de manière systématique. Quand des tokens étrangers affectent le texte en anglais, on peut observer des dommages, bien que le redirectionnement en anglais dans un autre langage améliore la compréhension. Bien que l'entrée puisse renvoyer des résultats mélangés, la fine-tuning montre un chemin plus stable pour inhiber les dommages, permettant ainsi un gestionnement efficace de CSW.",
      "upvotes": 3,
      "discussionId": "685b863bd2ee4fac76521e96",
      "ai_summary": "LLMs' comprehension and reasoning skills are evaluated under code-switching conditions, revealing that embedding English into other languages can improve understanding, while prompts and fine-tuning affect degradation mitigation differently.",
      "ai_keywords": [
        "Large Language Models",
        "code-switching",
        "CSW",
        "reasoning benchmarks",
        "comprehension benchmarks",
        "foreign tokens",
        "embedding",
        "fine-tuning"
      ]
    },
    "publishedAt": "2025-06-16T17:19:27.000Z",
    "title": "Lost in the Mix: Evaluating LLM Understanding of Code-Switched Text",
    "summary": "Code-switching (CSW) is the act of alternating between two or more languages\nwithin a single discourse. This phenomenon is widespread in multilingual\ncommunities, and increasingly prevalent in online content, where users\nnaturally mix languages in everyday communication. As a result, Large Language\nModels (LLMs), now central to content processing and generation, are frequently\nexposed to code-switched inputs. Given their widespread use, it is crucial to\nunderstand how LLMs process and reason about such mixed-language text. This\npaper presents a systematic evaluation of LLM comprehension under\ncode-switching by generating CSW variants of established reasoning and\ncomprehension benchmarks. While degradation is evident when foreign tokens\ndisrupt English textx2013even under linguistic\nconstraintsx2013embedding English into other languages often\nimproves comprehension. Though prompting yields mixed results, fine-tuning\noffers a more stable path to degradation mitigation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14012.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "655efd24afee0e00788bb589",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/655efd24afee0e00788bb589/22guLxIWNybbJR3jI-c4w.jpeg",
      "fullname": "Amr Mohamed",
      "name": "amr-mohamed",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": true
  }
]