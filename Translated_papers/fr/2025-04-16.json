[
  {
    "paper": {
      "id": "2504.08672",
      "authors": [
        {
          "_id": "67fcb7294a92187863e805ee",
          "user": {
            "_id": "64e6cf78ecce34cb442dc889",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e6cf78ecce34cb442dc889/qVZFiUEpBpSkmH8SQeinm.jpeg",
            "isPro": false,
            "fullname": "Fangzhi Xu",
            "user": "xufangzhi",
            "type": "user"
          },
          "name": "Fangzhi Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-14T09:46:16.537Z",
          "hidden": false
        },
        {
          "_id": "67fcb7294a92187863e805ef",
          "name": "Hang Yan",
          "hidden": false
        },
        {
          "_id": "67fcb7294a92187863e805f0",
          "name": "Chang Ma",
          "hidden": false
        },
        {
          "_id": "67fcb7294a92187863e805f1",
          "name": "Haiteng Zhao",
          "hidden": false
        },
        {
          "_id": "67fcb7294a92187863e805f2",
          "user": {
            "_id": "6064a0eeb1703ddba0d458b9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1617207525789-noauth.png",
            "isPro": false,
            "fullname": "Qiushi",
            "user": "QiushiSun",
            "type": "user"
          },
          "name": "Qiushi Sun",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-16T09:24:08.107Z",
          "hidden": false
        },
        {
          "_id": "67fcb7294a92187863e805f3",
          "name": "Kanzhi Cheng",
          "hidden": false
        },
        {
          "_id": "67fcb7294a92187863e805f4",
          "name": "Junxian He",
          "hidden": false
        },
        {
          "_id": "67fcb7294a92187863e805f5",
          "name": "Jun Liu",
          "hidden": false
        },
        {
          "_id": "67fcb7294a92187863e805f6",
          "name": "Zhiyong Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-11T16:26:23.000Z",
      "submittedOnDailyAt": "2025-04-16T05:46:28.754Z",
      "title": "Genio : Généralisé et un cadre d'apprentissage automatique complet sans machinerie. De plus, il est adapté aux raisonnements avancés.",
      "submittedOnDailyBy": {
        "_id": "64e6cf78ecce34cb442dc889",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e6cf78ecce34cb442dc889/qVZFiUEpBpSkmH8SQeinm.jpeg",
        "isPro": false,
        "fullname": "Fangzhi Xu",
        "user": "xufangzhi",
        "type": "user"
      },
      "summary": "Le développement de la théorie des LLM a réveillé un grand intérêt. Cependant, la technologie actuelle d'entraînement postérieur est limitée en termes de résultats sous-optimaux.",
      "upvotes": 35,
      "discussionId": "67fcb72a4a92187863e8061b",
      "projectPage": "https://github.com/xufangzhi/Genius",
      "githubRepo": "https://github.com/xufangzhi/Genius",
      "ai_keywords": [
        "self-training framework",
        "Genius",
        "stepwise foresight re-sampling strategy",
        "advantage-calibrated optimization (ACO) loss function"
      ]
    },
    "publishedAt": "2025-04-11T12:26:23.000Z",
    "title": "Genius: A Generalizable and Purely Unsupervised Self-Training Framework\n  For Advanced Reasoning",
    "summary": "Advancing LLM reasoning skills has captivated wide interest. However, current\npost-training techniques rely heavily on supervisory signals, such as outcome\nsupervision or auxiliary reward models, which face the problem of scalability\nand high annotation costs. This motivates us to enhance LLM reasoning without\nthe need for external supervision. We introduce a generalizable and purely\nunsupervised self-training framework, named Genius. Without external auxiliary,\nGenius requires to seek the optimal response sequence in a stepwise manner and\noptimize the LLM. To explore the potential steps and exploit the optimal ones,\nGenius introduces a stepwise foresight re-sampling strategy to sample and\nestimate the step value by simulating future outcomes. Further, we recognize\nthat the unsupervised setting inevitably induces the intrinsic noise and\nuncertainty. To provide a robust optimization, we propose an\nadvantage-calibrated optimization (ACO) loss function to mitigate estimation\ninconsistencies. Combining these techniques together, Genius provides an\nadvanced initial step towards self-improve LLM reasoning with general queries\nand without supervision, revolutionizing reasoning scaling laws given the vast\navailability of general queries. The code will be released at\nhttps://github.com/xufangzhi/Genius.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08672.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64e6cf78ecce34cb442dc889",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e6cf78ecce34cb442dc889/qVZFiUEpBpSkmH8SQeinm.jpeg",
      "fullname": "Fangzhi Xu",
      "name": "xufangzhi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.10481",
      "authors": [
        {
          "_id": "67fdc1b41d1bc292f7b9358e",
          "user": {
            "_id": "64e18e9ec20c27fcc8df384e",
            "avatarUrl": "/avatars/64ef866b9fa385efcefb34ea76b76802.svg",
            "isPro": false,
            "fullname": "Ding Chen",
            "user": "Hush-cd",
            "type": "user"
          },
          "name": "Ding Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-15T07:54:22.449Z",
          "hidden": false
        },
        {
          "_id": "67fdc1b41d1bc292f7b9358f",
          "user": {
            "_id": "6455ff584095c967f9a847bb",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6455ff584095c967f9a847bb/A5wjtWsudC73fLVmgASBr.jpeg",
            "isPro": false,
            "fullname": "Qingchen Yu",
            "user": "Duguce",
            "type": "user"
          },
          "name": "Qingchen Yu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-16T09:23:45.275Z",
          "hidden": false
        },
        {
          "_id": "67fdc1b41d1bc292f7b93590",
          "name": "Pengyuan Wang",
          "hidden": false
        },
        {
          "_id": "67fdc1b41d1bc292f7b93591",
          "name": "Wentao Zhang",
          "hidden": false
        },
        {
          "_id": "67fdc1b41d1bc292f7b93592",
          "name": "Bo Tang",
          "hidden": false
        },
        {
          "_id": "67fdc1b41d1bc292f7b93593",
          "name": "Feiyu Xiong",
          "hidden": false
        },
        {
          "_id": "67fdc1b41d1bc292f7b93594",
          "name": "Xinchi Li",
          "hidden": false
        },
        {
          "_id": "67fdc1b41d1bc292f7b93595",
          "name": "Minchuan Yang",
          "hidden": false
        },
        {
          "_id": "67fdc1b41d1bc292f7b93596",
          "name": "Zhiyu Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-14T17:59:36.000Z",
      "submittedOnDailyAt": "2025-04-16T00:53:50.942Z",
      "title": "xVerify : Outil de Vérification des Réponses pour Évaluer des Modèles de Raisonnement Efficaces",
      "submittedOnDailyBy": {
        "_id": "64e18e9ec20c27fcc8df384e",
        "avatarUrl": "/avatars/64ef866b9fa385efcefb34ea76b76802.svg",
        "isPro": false,
        "fullname": "Ding Chen",
        "user": "Hush-cd",
        "type": "user"
      },
      "summary": "Avec l'actualisation du modèle o1, des modèles d'inférence ont apparu qui appliquent des étapes de temps et génèrent des réponses qui incluent des inférences complexes, des étapes intermédiaires et une autoperception. Ces modèles sont insuffisamment évalués par les méthodes existantes car ils ne peuvent pas déterminer si la sortie du modèle est exactement la réponse correcte, ni extraire la réponse finale de réponses longues et complexes. Pour résoudre ces problèmes, on propose une fonction de vérification de réponses efficace pour l'évaluation de modèles d'inférence appelée xVerify. xVerify montre une capacité forte en évaluation d'égalité et est conçu pour déterminer effectivement si les réponses générées par des modèles d'inférence sont égales à la réponse correcte pour diverses questions. Pour l'entraînement et l'évaluation de xVerify, des paires de questions et réponses générées dans différents datasets, ainsi qu'un ensemble de données spécifique pour l'évaluation de modèles d'inférence appelé VAR, ont été utilisées. La précision des labeleurs a été garantie par des processus de notes de réunions divers. Des modèles de xVerify de différents tailles ont été entraînés sur le dataset VAR. Dans les expériences d'évaluation avec des ensembles de test et de généralisation, tous les modèles de xVerify ont dépassé le F1 score et la précision de 95%. En particulier, la version de taille minimale, xVerify-0.5B-I, a dépassé tous les méthodes d'évaluation, sauf GPT-4o, tandis que xVerify-3B-Ib a dépassé GPT-4o en termes de performance globale. Ces résultats démontrent l'efficacité et la capacité de généralisation de xVerify.",
      "upvotes": 28,
      "discussionId": "67fdc1b51d1bc292f7b935e8",
      "githubRepo": "https://github.com/IAAR-Shanghai/xVerify",
      "ai_keywords": [
        "reasoning models",
        "o1 model",
        "slow thinking strategies",
        "complex reasoning",
        "intermediate steps",
        "self-reflection",
        "evaluation methods",
        "LLM output",
        "reference answer",
        "final answer",
        "xVerify",
        "equivalence judgment",
        "VAR dataset",
        "multi-round annotation process",
        "F1 scores",
        "xVerify-0.5B-I",
        "xVerify-3B-Ib",
        "GPT-4o"
      ]
    },
    "publishedAt": "2025-04-14T13:59:36.000Z",
    "title": "xVerify: Efficient Answer Verifier for Reasoning Model Evaluations",
    "summary": "With the release of the o1 model by OpenAI, reasoning models adopting slow\nthinking strategies have gradually emerged. As the responses generated by such\nmodels often include complex reasoning, intermediate steps, and\nself-reflection, existing evaluation methods are often inadequate. They\nstruggle to determine whether the LLM output is truly equivalent to the\nreference answer, and also have difficulty identifying and extracting the final\nanswer from long, complex responses. To address this issue, we propose xVerify,\nan efficient answer verifier for reasoning model evaluations. xVerify\ndemonstrates strong capability in equivalence judgment, enabling it to\neffectively determine whether the answers produced by reasoning models are\nequivalent to reference answers across various types of objective questions. To\ntrain and evaluate xVerify, we construct the VAR dataset by collecting\nquestion-answer pairs generated by multiple LLMs across various datasets,\nleveraging multiple reasoning models and challenging evaluation sets designed\nspecifically for reasoning model assessment. A multi-round annotation process\nis employed to ensure label accuracy. Based on the VAR dataset, we train\nmultiple xVerify models of different scales. In evaluation experiments\nconducted on both the test set and generalization set, all xVerify models\nachieve overall F1 scores and accuracy exceeding 95\\%. Notably, the smallest\nvariant, xVerify-0.5B-I, outperforms all evaluation methods except GPT-4o,\nwhile xVerify-3B-Ib surpasses GPT-4o in overall performance. These results\nvalidate the effectiveness and generalizability of xVerify.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10481.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64e18e9ec20c27fcc8df384e",
      "avatarUrl": "/avatars/64ef866b9fa385efcefb34ea76b76802.svg",
      "fullname": "Ding Chen",
      "name": "Hush-cd",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.10465",
      "authors": [
        {
          "_id": "67ff26c3414c03ebc1d42529",
          "name": "Tao Zhang",
          "hidden": false
        },
        {
          "_id": "67ff26c3414c03ebc1d4252a",
          "user": {
            "_id": "63958b4414513eaf9029ebf1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/U1g5H071pWRswGAG9UTpo.png",
            "isPro": false,
            "fullname": "Xiangtai Li",
            "user": "LXT",
            "type": "user"
          },
          "name": "Xiangtai Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-16T08:42:53.598Z",
          "hidden": false
        },
        {
          "_id": "67ff26c3414c03ebc1d4252b",
          "name": "Zilong Huang",
          "hidden": false
        },
        {
          "_id": "67ff26c3414c03ebc1d4252c",
          "name": "Yanwei Li",
          "hidden": false
        },
        {
          "_id": "67ff26c3414c03ebc1d4252d",
          "name": "Weixian Lei",
          "hidden": false
        },
        {
          "_id": "67ff26c3414c03ebc1d4252e",
          "name": "Xueqing Deng",
          "hidden": false
        },
        {
          "_id": "67ff26c3414c03ebc1d4252f",
          "name": "Shihao Chen",
          "hidden": false
        },
        {
          "_id": "67ff26c3414c03ebc1d42530",
          "name": "Shunping Ji",
          "hidden": false
        },
        {
          "_id": "67ff26c3414c03ebc1d42531",
          "name": "Jiashi Feng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-14T17:52:22.000Z",
      "submittedOnDailyAt": "2025-04-16T02:11:29.898Z",
      "title": "Pixel-SAIL : Un Transformer unique pour atteindre la compréhension basée sur les pixels",
      "submittedOnDailyBy": {
        "_id": "63958b4414513eaf9029ebf1",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/U1g5H071pWRswGAG9UTpo.png",
        "isPro": false,
        "fullname": "Xiangtai Li",
        "user": "LXT",
        "type": "user"
      },
      "summary": "Les modèles de langage multimodal de haute dimension (MLLMs) atteignent un rendement impressionnant dans les tâches de compréhension au niveau de pixels. Cependant, tous les études dépendent de fonctions supplémentaires comme l'encodeur visuel CLIP et la segmentation spéciale, ce qui limite l'expansion du modèle. Dans ce travail, nous explorons un MLLM simplifié à haute dimension sans introduire de nouvelles fonctions. Ce travail est basé sur le concept récent d'un seul transformer comme modèle de vision-langage unifié (SAIL). Dans cette recherche, nous utilisons des transformers pour entraîner des tokens visuels et textuels ensemble. Pixel-SAIL présente un seul transformer pour les tâches de MLLM au niveau de pixels. Spécifiquement, nous proposons trois améliorations techniques sur le baseline. Premièrement, nous concevons un module d'upsampling entrainable pour raffiner les caractéristiques des tokens visuels. Ensuite, nous proposons une stratégie d'injection de prédictions visuelles pour que le transformer unique puisse comprendre les entrées de prédictions visuelles et tirer parti de la fusion initiale des embeddings de prédictions visuelles et de tokens visuels. Finalement, nous introduisons une stratégie d'apprentissage expérimental pour améliorer la capacité d'extraction de caractéristiques précises du segmentateur visuel. De plus, nous présentons le benchmark détaillé de compréhension de pixels (PerBench) collecté par la méthode manuelle. Ce benchmark inclut trois tâches : explication détaillée d'objets, réponse à des questions basées sur des prédictions visuelles et segmentation de références de texte visuel. Nous effectuons des expériences larges pour les quatre références de segmentation, un benchmark de prédictions visuelles et notre PerBench, montrant que notre Pixel-SAIL obtient des résultats meilleurs ou moins mauvais comparés à un système plus simple. Les codes et modèles sont disponibles sur https://github.com/magic-research/Sa2VA.",
      "upvotes": 21,
      "discussionId": "67ff26c6414c03ebc1d425de",
      "ai_keywords": [
        "multimodal large language models (MLLMs)",
        "pixel-level understanding",
        "vision encoder (CLIP)",
        "segmentation experts",
        "single transformer as a unified vision-language model (SAIL)",
        "pixel-wise MLLM tasks",
        "learnable upsampling module",
        "visual prompt injection",
        "visual prompt embeddings",
        "vision expert distillation",
        "pixel understanding benchmark (PerBench)",
        "detailed object description",
        "visual prompt-based question answering",
        "visual-text referring segmentation",
        "referring segmentation benchmarks",
        "visual prompt benchmark"
      ]
    },
    "publishedAt": "2025-04-14T13:52:22.000Z",
    "title": "Pixel-SAIL: Single Transformer For Pixel-Grounded Understanding",
    "summary": "Multimodal Large Language Models (MLLMs) achieve remarkable performance for\nfine-grained pixel-level understanding tasks. However, all the works rely\nheavily on extra components, such as vision encoder (CLIP), segmentation\nexperts, leading to high system complexity and limiting model scaling. In this\nwork, our goal is to explore a highly simplified MLLM without introducing extra\ncomponents. Our work is motivated by the recent works on Single trAnsformer as\na unified vIsion-Language Model (SAIL) design, where these works jointly learn\nvision tokens and text tokens in transformers. We present Pixel-SAIL, a single\ntransformer for pixel-wise MLLM tasks. In particular, we present three\ntechnical improvements on the plain baseline. First, we design a learnable\nupsampling module to refine visual token features. Secondly, we propose a novel\nvisual prompt injection strategy to enable the single transformer to understand\nvisual prompt inputs and benefit from the early fusion of visual prompt\nembeddings and vision tokens. Thirdly, we introduce a vision expert\ndistillation strategy to efficiently enhance the single transformer's\nfine-grained feature extraction capability. In addition, we have collected a\ncomprehensive pixel understanding benchmark (PerBench), using a manual check.\nIt includes three tasks: detailed object description, visual prompt-based\nquestion answering, and visual-text referring segmentation. Extensive\nexperiments on four referring segmentation benchmarks, one visual prompt\nbenchmark, and our PerBench show that our Pixel-SAIL achieves comparable or\neven better results with a much simpler pipeline. Code and model will be\nreleased at https://github.com/magic-research/Sa2VA.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10465.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "63958b4414513eaf9029ebf1",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/U1g5H071pWRswGAG9UTpo.png",
      "fullname": "Xiangtai Li",
      "name": "LXT",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.10337",
      "authors": [
        {
          "_id": "67fddae99a03686367721718",
          "user": {
            "_id": "6471a24381ded91f253ceb1c",
            "avatarUrl": "/avatars/31d447068fe1fd4200ab5d08ab31eed4.svg",
            "isPro": false,
            "fullname": "Wesley Shi",
            "user": "WesleyShi",
            "type": "user"
          },
          "name": "Wenlei Shi",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-04-15T06:43:40.277Z",
          "hidden": false
        },
        {
          "_id": "67fddae99a03686367721719",
          "name": "Xing Jin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-14T15:46:33.000Z",
      "submittedOnDailyAt": "2025-04-16T00:52:23.733Z",
      "title": "Haymider : Essai d'escalade pour la validation du reconnaissance de la génération",
      "submittedOnDailyBy": {
        "_id": "6471a24381ded91f253ceb1c",
        "avatarUrl": "/avatars/31d447068fe1fd4200ab5d08ab31eed4.svg",
        "isPro": false,
        "fullname": "Wesley Shi",
        "user": "WesleyShi",
        "type": "user"
      },
      "summary": "Le système AI est capable de générer et de maintenir des connaissances, ainsi que de vérifier ces connaissances. Les dernières recherches sur la raisonnement de chaîne de pensée (Chain-of-Thought) ont montré que les modèles de langage grands (LLM) ont un grand potentiel pour résoudre des problèmes compétitifs, mais leur capacité de vérification est faible et n'a pas été suffisamment explorée. Dans cet article, nous proposons Heimdall, un modèle de LLM qui effectue la vérification de la raisonnement de chaîne de pensée. Heimdall peut évaluer précisément la précision des solutions. En utilisant l'apprentissage par renforcement comme approche principale, Hemdall a amélioré la précision de la vérification dans des problèmes mathématiques compétitifs, augmentant la précision de 62,5% à 94,5%. En appliquant des techniques de ré-sampling pour l'échelle, la précision a atteint 97,5%. Selon l'évaluation humaine, Heimdall montre une capacité impressionnante de généralisation et détecte correctement de nombreux problèmes de démonstrations mathématiques difficiles qui n'ont pas été inclus lors de l'entraînement. De plus, nous proposons la vérification pessimiste (Pessimistic Verification), qui élargit les fonctions de Heimdall pour l'échelle de la résolution de problèmes. La vérification pessimiste évalue les solutions fournies par un modèle de solution et sélectionne celles avec la moindre confiance. Lorsque DeepSeek-R1-Distill-Qwen-32B a été utilisé comme modèle de solution, la vérification pessimiste a amélioré la précision de la résolution de AIME2025 de 54,2% à 70,0%, et avec 16 fois plus de boucles de calcul, a atteint 83,3%. Avec l'utilisation d'un modèle de solution puissant comme 2.5 Pro, la précision a atteint 93,0%. Enfin, nous avons construit un prototype d'un système automatique de découverte de connaissances. Dans ce système, la première source propose le problème, la seconde fournit la solution et la troisième la vérification. Nous avons utilisé le premier composant de la plateforme de synthèse de données NuminaMath et Heimdall a détecté efficacement les problèmes dans le jeu de données, révélant que presque la moitié des données nécessitent une correction. Cela reflète profondément les récentes recherches de NuminaMath sur la réduction des données.",
      "upvotes": 21,
      "discussionId": "67fddaea9a03686367721776",
      "ai_keywords": [
        "Chain-of-Thought reasoning",
        "LLMs (Large Language Models)",
        "Heimdall",
        "long CoT verification",
        "pure reinforcement learning",
        "synthetic math problems",
        "human evaluation",
        "generalization capabilities",
        "Pessimistic Verification",
        "DeepSeek-R1-Distill-Qwen-32B",
        "AIME2025",
        "Gemini 2.5 Pro",
        "solution accuracy",
        "automatic knowledge discovery system",
        "ternary system",
        "NuminaMath",
        "data synthesis",
        "data records",
        "flawed data"
      ]
    },
    "publishedAt": "2025-04-14T11:46:33.000Z",
    "title": "Heimdall: test-time scaling on the generative verification",
    "summary": "An AI system can create and maintain knowledge only to the extent that it can\nverify that knowledge itself. Recent work on long Chain-of-Thought reasoning\nhas demonstrated great potential of LLMs on solving competitive problems, but\ntheir verification ability remains to be weak and not sufficiently\ninvestigated. In this paper, we propose Heimdall, the long CoT verification LLM\nthat can accurately judge the correctness of solutions. With pure reinforcement\nlearning, we boost the verification accuracy from 62.5% to 94.5% on competitive\nmath problems. By scaling with repeated sampling, the accuracy further\nincreases to 97.5%. Through human evaluation, Heimdall demonstrates impressive\ngeneralization capabilities, successfully detecting most issues in challenging\nmath proofs, the type of which is not included during training. Furthermore, we\npropose Pessimistic Verification to extend the functionality of Heimdall to\nscaling up the problem solving. It calls Heimdall to judge the solutions from a\nsolver model and based on the pessimistic principle, selects the most likely\ncorrect solution with the least uncertainty. Taking\nDeepSeek-R1-Distill-Qwen-32B as the solver model, Pessimistic Verification\nimproves the solution accuracy on AIME2025 from 54.2% to 70.0% with 16x compute\nbudget and to 83.3% with more compute budget. With the stronger solver Gemini\n2.5 Pro, the score reaches 93.0%. Finally, we prototype an automatic knowledge\ndiscovery system, a ternary system where one poses questions, another provides\nsolutions, and the third verifies the solutions. Using the data synthesis work\nNuminaMath for the first two components, Heimdall effectively identifies\nproblematic records within the dataset and reveals that nearly half of the data\nis flawed, which interestingly aligns with the recent ablation studies from\nNuminaMath.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10337.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6471a24381ded91f253ceb1c",
      "avatarUrl": "/avatars/31d447068fe1fd4200ab5d08ab31eed4.svg",
      "fullname": "Wesley Shi",
      "name": "WesleyShi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.11346",
      "authors": [
        {
          "_id": "67ff18961dc5d56fdd6ca724",
          "name": "Yu Gao",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca725",
          "name": "Lixue Gong",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca726",
          "name": "Qiushan Guo",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca727",
          "name": "Xiaoxia Hou",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca728",
          "name": "Zhichao Lai",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca729",
          "name": "Fanshi Li",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca72a",
          "name": "Liang Li",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca72b",
          "name": "Xiaochen Lian",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca72c",
          "name": "Chao Liao",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca72d",
          "name": "Liyang Liu",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca72e",
          "name": "Wei Liu",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca72f",
          "name": "Yichun Shi",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca730",
          "name": "Shiqi Sun",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca731",
          "name": "Yu Tian",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca732",
          "name": "Zhi Tian",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca733",
          "name": "Peng Wang",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca734",
          "name": "Rui Wang",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca735",
          "name": "Xuanda Wang",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca736",
          "name": "Xun Wang",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca737",
          "name": "Ye Wang",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca738",
          "name": "Guofeng Wu",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca739",
          "name": "Jie Wu",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca73a",
          "name": "Xin Xia",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca73b",
          "name": "Xuefeng Xiao",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca73c",
          "name": "Zhonghua Zhai",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca73d",
          "name": "Xinyu Zhang",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca73e",
          "name": "Qi Zhang",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca73f",
          "name": "Yuwei Zhang",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca740",
          "name": "Shijia Zhao",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca741",
          "name": "Jianchao Yang",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca742",
          "name": "Weilin Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-15T16:19:07.000Z",
      "submittedOnDailyAt": "2025-04-16T01:10:39.295Z",
      "title": "Certainly! Voici la traduction du texte fourni en français, en maintenant un ton professionnel et précis :\n\n**Rapport Technique de la Technologie Seedream 3.0**",
      "submittedOnDailyBy": {
        "_id": "63468720dd6d90d82ccf3450",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
        "isPro": false,
        "fullname": "YSH",
        "user": "BestWishYsh",
        "type": "user"
      },
      "summary": "Zongshimeng 3.0 est un modèle de génération d'images bilingue haute qualité en chinois et anglais. Pour résoudre les problèmes existants dans Zongshimeng 2.0, le projet a réalisé une série d'améliorations techniques sur tout le processus, allant de la construction des données à l'implémentation du modèle. En particulier, des problèmes tels que la réponse à des motifs complexes, la génération de typographies détaillées, la qualité inadéquate de l'image et les limites de résolution de l'image ont été abordés. Le développement de Zongshimeng 3.0 s'appuie sur l'amélioration de tout le processus, de la construction des données à l'implémentation du modèle. Dans la couche des données, des paradigmes d'apprentissage pour les défauts et un cadre de sampling de données collaboratif à double axe ont été utilisés pour doubler le jeu de données. De plus, lors de l'apprentissage préalable, des méthodes efficaces comme l'apprentissage mixte de langues, RoPE de modalités croisées, perte de correspondance de représentations et sampling de pas de temps pour la gamme ont été employées. Dans la phase d'apprentissage profond, un SFT avec des commentaires artistiques variés et un modèle de récompense basé sur VLM étendu ont été utilisés pour générer des sorties qui s'adaptent aux préférences humaines. De plus, Zongshimeng 3.0 a développé un nouveau modèle d'accélération. En utilisant une valeur attendue de bruit constante et un sampling de pas de temps pour l'importance, il a été possible de maintenir la qualité de l'image tout en augmentant la vitesse de 4 ou 8 fois. Zongshimeng 3.0 montre une amélioration claire par rapport à Zongshimeng 2.0, en particulier en termes de rendition de textes chinois complexes, ce qui est crucial pour la génération de typographie. De plus, il fournit une sortie à haute résolution, permettant la génération d'images de grande qualité.",
      "upvotes": 20,
      "discussionId": "67ff189c1dc5d56fdd6ca8e0",
      "projectPage": "https://team.doubao.com/zh/tech/seedream3_0",
      "ai_keywords": [
        "mixed-resolution training",
        "cross-modality RoPE",
        "representation alignment loss",
        "resolution-aware timestep sampling",
        "SFT (Supervised Fine-Tuning)",
        "VLM (Vision Language Model)",
        "consistent noise expectation",
        "importance-aware timestep sampling"
      ]
    },
    "publishedAt": "2025-04-15T12:19:07.000Z",
    "title": "Seedream 3.0 Technical Report",
    "summary": "We present Seedream 3.0, a high-performance Chinese-English bilingual image\ngeneration foundation model. We develop several technical improvements to\naddress existing challenges in Seedream 2.0, including alignment with\ncomplicated prompts, fine-grained typography generation, suboptimal visual\naesthetics and fidelity, and limited image resolutions. Specifically, the\nadvancements of Seedream 3.0 stem from improvements across the entire pipeline,\nfrom data construction to model deployment. At the data stratum, we double the\ndataset using a defect-aware training paradigm and a dual-axis collaborative\ndata-sampling framework. Furthermore, we adopt several effective techniques\nsuch as mixed-resolution training, cross-modality RoPE, representation\nalignment loss, and resolution-aware timestep sampling in the pre-training\nphase. During the post-training stage, we utilize diversified aesthetic\ncaptions in SFT, and a VLM-based reward model with scaling, thereby achieving\noutputs that well align with human preferences. Furthermore, Seedream 3.0\npioneers a novel acceleration paradigm. By employing consistent noise\nexpectation and importance-aware timestep sampling, we achieve a 4 to 8 times\nspeedup while maintaining image quality. Seedream 3.0 demonstrates significant\nimprovements over Seedream 2.0: it enhances overall capabilities, in particular\nfor text-rendering in complicated Chinese characters which is important to\nprofessional typography generation. In addition, it provides native\nhigh-resolution output (up to 2K), allowing it to generate images with high\nvisual quality.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11346.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63468720dd6d90d82ccf3450",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
      "fullname": "YSH",
      "name": "BestWishYsh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 47
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.11442",
      "authors": [
        {
          "_id": "67ff1387e1bfbb6bdd79ab72",
          "user": {
            "_id": "628b671f8fb67b90658613f7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1653303027789-noauth.jpeg",
            "isPro": false,
            "fullname": "Leon Guertler",
            "user": "LeonGuertler",
            "type": "user"
          },
          "name": "Leon Guertler",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-16T08:43:02.481Z",
          "hidden": false
        },
        {
          "_id": "67ff1387e1bfbb6bdd79ab73",
          "user": {
            "_id": "653879fbf5f5016df355d010",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653879fbf5f5016df355d010/hU3mrTOw3DQ8auUGoQceW.jpeg",
            "isPro": false,
            "fullname": "Bobby Cheng",
            "user": "bobbycxy",
            "type": "user"
          },
          "name": "Bobby Cheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-16T08:43:06.026Z",
          "hidden": false
        },
        {
          "_id": "67ff1387e1bfbb6bdd79ab74",
          "user": {
            "_id": "636681feaa6a4af6073ba73e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/636681feaa6a4af6073ba73e/u_0moYNu6as9Sszp-ej95.png",
            "isPro": true,
            "fullname": "Simon Yu",
            "user": "simonycl",
            "type": "user"
          },
          "name": "Simon Yu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-16T08:43:08.368Z",
          "hidden": false
        },
        {
          "_id": "67ff1387e1bfbb6bdd79ab75",
          "user": {
            "_id": "635e3a76106f984574c36409",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1667120725800-635e3a76106f984574c36409.png",
            "isPro": false,
            "fullname": "Bo Liu",
            "user": "Benjamin-eecs",
            "type": "user"
          },
          "name": "Bo Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-16T08:46:16.265Z",
          "hidden": false
        },
        {
          "_id": "67ff1387e1bfbb6bdd79ab76",
          "name": "Leshem Choshen",
          "hidden": false
        },
        {
          "_id": "67ff1387e1bfbb6bdd79ab77",
          "name": "Cheston Tan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-15T17:55:20.000Z",
      "submittedOnDailyAt": "2025-04-16T01:06:25.215Z",
      "title": "TextArena\n\nCette traduction est du contenu de TextArena traduit en français. TextArena est une plateforme pour traiter et traduire des textes. Cette plateforme peut traduire des textes en différents langues selon les demandes des utilisateurs. TextArena analyse le texte des utilisateurs, fournit des résultats de traduction précis et offre des fonctions avancées de traduction qui permettent de comprendre et d'appliquer différents langages et règles grammaticales. TextArena fournit une variété d'outils et de fonctions nécessaires pour la traduction, aidant à rendre les tâches de traduction des utilisateurs efficaces et précises.",
      "submittedOnDailyBy": {
        "_id": "635e3a76106f984574c36409",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1667120725800-635e3a76106f984574c36409.png",
        "isPro": false,
        "fullname": "Bo Liu",
        "user": "Benjamin-eecs",
        "type": "user"
      },
      "summary": "TextArena est un ensemble de jeux de texte compétitifs ouverts-source pour l'entraînement et l'évaluation des actions d'agents dans des modèles de langage de grande taille (LLMs). Il comprend plus de 57 environnements uniques (y compris des configurations pour un seul joueur, des joueurs de route et des multi-joueurs), permettant aux humains ou à d'autres modèles fournis d'évaluer facilement les capacités du modèle via un système de jeu en ligne. Il offre des scores en temps réel de TrueSkill. Il est connu que les benchmarks traditionnels tendent à manquer dans l'évaluation des compétences sociales dynamiques (par exemple, Negajo, Mindsink, Desgin), et TextArena peut résoudre ces lacunes. Il est conçu sur la base de recherche, communauté et extensibilité, en prioritisant l'ajout de nouveaux jeux, des changements de cadre, des tests de modèles, des jeux entre modèles et l'entraînement de modèles. Pour obtenir une description détaillée, y compris les environnements, les jeux, le classement et des exemples, visitez https://github.com/LeonGuertler/TextArena et https://www.textarena.ai/.",
      "upvotes": 18,
      "discussionId": "67ff1388e1bfbb6bdd79abbe",
      "projectPage": "https://textarena.ai/",
      "githubRepo": "https://github.com/LeonGuertler/TextArena",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "TrueSkill scores",
        "negotiation",
        "theory of mind",
        "deception",
        "dynamic social skills"
      ]
    },
    "publishedAt": "2025-04-15T13:55:20.000Z",
    "title": "TextArena",
    "summary": "TextArena is an open-source collection of competitive text-based games for\ntraining and evaluation of agentic behavior in Large Language Models (LLMs). It\nspans 57+ unique environments (including single-player, two-player, and\nmulti-player setups) and allows for easy evaluation of model capabilities via\nan online-play system (against humans and other submitted models) with\nreal-time TrueSkill scores. Traditional benchmarks rarely assess dynamic social\nskills such as negotiation, theory of mind, and deception, creating a gap that\nTextArena addresses. Designed with research, community and extensibility in\nmind, TextArena emphasizes ease of adding new games, adapting the framework,\ntesting models, playing against the models, and training models. Detailed\ndocumentation of environments, games, leaderboard, and examples are available\non https://github.com/LeonGuertler/TextArena and https://www.textarena.ai/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11442.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "635e3a76106f984574c36409",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1667120725800-635e3a76106f984574c36409.png",
      "fullname": "Bo Liu",
      "name": "Benjamin-eecs",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.10766",
      "authors": [
        {
          "_id": "67ff114a3026f8abc4bf7e43",
          "name": "Ming Li",
          "hidden": false
        },
        {
          "_id": "67ff114a3026f8abc4bf7e44",
          "name": "Yanhong Li",
          "hidden": false
        },
        {
          "_id": "67ff114a3026f8abc4bf7e45",
          "name": "Ziyue Li",
          "hidden": false
        },
        {
          "_id": "67ff114a3026f8abc4bf7e46",
          "user": {
            "_id": "647f5af5b0e96764589f3b2a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
            "isPro": false,
            "fullname": "Tianyi Zhou",
            "user": "zhoutianyi",
            "type": "user"
          },
          "name": "Tianyi Zhou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-16T08:46:19.086Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-14T23:53:47.000Z",
      "submittedOnDailyAt": "2025-04-16T00:40:29.697Z",
      "title": "Les données d'instruction et les données de raison de ce que l'impact est après l'entraînement : qualité des données à travers chaque couche",
      "submittedOnDailyBy": {
        "_id": "647f5af5b0e96764589f3b2a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
        "isPro": false,
        "fullname": "Tianyi Zhou",
        "user": "zhoutianyi",
        "type": "user"
      },
      "summary": "L'apprentissage en arrière des modèles de langage grands (LLMs) évolue vers des problèmes de logique complexes orientés vers des guides, mais la compréhension de la manière dont différents données dynamiquement affectent l'ajustement est principalement explorée. Dans cet article, nous proposons un analyse spectrale des gradients par couches basée sur des données de guides et de logique de qualité faible ou forte résultant de l'apprentissage en arrière des LLMs. Notre analyse montre que les indicateurs préliminaires constitués d'évaluations de données (par exemple, IFD, InsTag, Difficulté, Reward) peuvent être interprétés en termes des caractéristiques du spectre des valeurs propres calculés à partir de la décomposition des valeurs propres (SVD). En particulier, les données de haute qualité montrent généralement une norme nucléaire plus faible et un classement effectif plus élevé, ce qui démontre que le classement effectif a une meilleure robustesse et une meilleure résolution pour détecter de petites différences de qualité, comme la norme nucléaire. Par exemple, les données logiques montrent un classement effectif significativement plus élevé que les données de guides, et ont une structure de gradient plus riche pour gérer des problèmes complexes. Les expériences montrent également que les modèles de la même famille partagent des patrons de gradients similaires, tandis que les modèles de différente famille montrent des différences significatives. Cette recherche fournit une vision cohérente sur l'impact de la qualité des données sur l'apprentissage, clairement démontre l'interaction entre la qualité des données et la stabilité de l'apprentissage, et offre une nouvelle perspective pour le développement de stratégies d'exploration de données en apprentissage en arrière.",
      "upvotes": 16,
      "discussionId": "67ff11503026f8abc4bf7fed",
      "githubRepo": "https://github.com/MingLiiii/Gradient_Unified",
      "ai_keywords": [
        "spectral analysis",
        "layer-wise gradients",
        "low/high-quality instruction",
        "reasoning data",
        "IFD",
        "InsTag",
        "Difficulty",
        "Reward",
        "singular value decomposition (SVD)",
        "nuclear norms",
        "effective ranks",
        "gradient structures",
        "training stability"
      ]
    },
    "publishedAt": "2025-04-14T19:53:47.000Z",
    "title": "How Instruction and Reasoning Data shape Post-Training: Data Quality\n  through the Lens of Layer-wise Gradients",
    "summary": "As the post-training of large language models (LLMs) advances from\ninstruction-following to complex reasoning tasks, understanding how different\ndata affect finetuning dynamics remains largely unexplored. In this paper, we\npresent a spectral analysis of layer-wise gradients induced by low/high-quality\ninstruction and reasoning data for LLM post-training. Our analysis reveals that\nwidely-studied metrics for data evaluation, e.g., IFD, InsTag, Difficulty, and\nReward, can be explained and unified by spectral properties computed from\ngradients' singular value decomposition (SVD). Specifically, higher-quality\ndata are usually associated with lower nuclear norms and higher effective\nranks. Notably, effective rank exhibits better robustness and resolution than\nnuclear norm in capturing subtle quality differences. For example, reasoning\ndata achieves substantially higher effective ranks than instruction data,\nimplying richer gradient structures on more complex tasks. Our experiments also\nhighlight that models within the same family share similar gradient patterns\nregardless of their sizes, whereas different model families diverge\nsignificantly. Providing a unified view on the effects of data quality across\ninstruction and reasoning data, this work illuminates the interplay between\ndata quality and training stability, shedding novel insights into developing\nbetter data exploration strategies for post-training.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10766.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "647f5af5b0e96764589f3b2a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
      "fullname": "Tianyi Zhou",
      "name": "zhoutianyi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.10462",
      "authors": [
        {
          "_id": "67ff2aa6a0346c2e622afdb2",
          "name": "Weixian Lei",
          "hidden": false
        },
        {
          "_id": "67ff2aa6a0346c2e622afdb3",
          "name": "Jiacong Wang",
          "hidden": false
        },
        {
          "_id": "67ff2aa6a0346c2e622afdb4",
          "name": "Haochen Wang",
          "hidden": false
        },
        {
          "_id": "67ff2aa6a0346c2e622afdb5",
          "user": {
            "_id": "63958b4414513eaf9029ebf1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/U1g5H071pWRswGAG9UTpo.png",
            "isPro": false,
            "fullname": "Xiangtai Li",
            "user": "LXT",
            "type": "user"
          },
          "name": "Xiangtai Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-16T08:42:51.715Z",
          "hidden": false
        },
        {
          "_id": "67ff2aa6a0346c2e622afdb6",
          "name": "Jun Hao Liew",
          "hidden": false
        },
        {
          "_id": "67ff2aa6a0346c2e622afdb7",
          "name": "Jiashi Feng",
          "hidden": false
        },
        {
          "_id": "67ff2aa6a0346c2e622afdb8",
          "name": "Zilong Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-14T17:50:20.000Z",
      "submittedOnDailyAt": "2025-04-16T02:27:49.065Z",
      "title": "La simplicité de l'scalabilité : un analyse expérimentale de l'apprentissage de visions et de run-grays réalisé à travers un seul Transformer.",
      "submittedOnDailyBy": {
        "_id": "63958b4414513eaf9029ebf1",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/U1g5H071pWRswGAG9UTpo.png",
        "isPro": false,
        "fullname": "Xiangtai Li",
        "user": "LXT",
        "type": "user"
      },
      "summary": "Dans cet article, la codification des pixels et l'interprétation du langage sont intégrées dans une architecture unique à travers un grand modèle de transformateurs monomodaux unifiés, SAIL. Au contraire des modèles modularisés comme ceux des modèles actuels, SAIL ne nécessite pas un encodeur visuel indépendant d'un modèle de transformateurs visuels (ViT) préalablement entraîné, offrant ainsi une architecture de conception plus minimale. Au lieu d'introduire de nouveaux composants architecturaux, SAIL applique une structure d'attention mixte et un encoding de position monomodaux adaptés aux caractéristiques des modèles visuels et textuels. Les caractéristiques de SAIL, telles que sa scalabilité, le modèle de flux de l'information multicapa et sa capacité de représentation visuelle, sont comparées systématiquement à ceux des modèles modularisés de MLLM. En augmentant à la fois les données d'entraînement et la taille du modèle, SAIL atteint un rendement comparable à ceux des modèles modularisés de MLLM. En particulier, l'élimination des composants entraînés précédemment de ViT améliore la scalabilité de SAIL et modifie considérablement le modèle de flux de l'information multicapa. De plus, SAIL montre une forte capacité de représentation visuelle, obtenant des résultats similaires à ceux de ViT-22B dans des tâches visuelles comme la segmentation sémantique. Les codes et les modèles sont disponibles sur https://github.com/bytedance/SAIL.",
      "upvotes": 12,
      "discussionId": "67ff2aa7a0346c2e622afe08",
      "ai_keywords": [
        "single transformer",
        "unified multimodal large language model (MLLM)",
        "raw pixel encoding",
        "language decoding",
        "vision transformer (ViT)",
        "mix-attention mechanisms",
        "multimodal positional encodings",
        "scalability",
        "cross-modal information flow patterns",
        "visual representation capabilities",
        "semantic segmentation",
        "ViT-22B"
      ]
    },
    "publishedAt": "2025-04-14T13:50:20.000Z",
    "title": "The Scalability of Simplicity: Empirical Analysis of Vision-Language\n  Learning with a Single Transformer",
    "summary": "This paper introduces SAIL, a single transformer unified multimodal large\nlanguage model (MLLM) that integrates raw pixel encoding and language decoding\nwithin a singular architecture. Unlike existing modular MLLMs, which rely on a\npre-trained vision transformer (ViT), SAIL eliminates the need for a separate\nvision encoder, presenting a more minimalist architecture design. Instead of\nintroducing novel architectural components, SAIL adapts mix-attention\nmechanisms and multimodal positional encodings to better align with the\ndistinct characteristics of visual and textual modalities. We systematically\ncompare SAIL's properties-including scalability, cross-modal information flow\npatterns, and visual representation capabilities-with those of modular MLLMs.\nBy scaling both training data and model size, SAIL achieves performance\ncomparable to modular MLLMs. Notably, the removal of pretrained ViT components\nenhances SAIL's scalability and results in significantly different cross-modal\ninformation flow patterns. Moreover, SAIL demonstrates strong visual\nrepresentation capabilities, achieving results on par with ViT-22B in vision\ntasks such as semantic segmentation. Code and models are available at\nhttps://github.com/bytedance/SAIL.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10462.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "63958b4414513eaf9029ebf1",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/U1g5H071pWRswGAG9UTpo.png",
      "fullname": "Xiangtai Li",
      "name": "LXT",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.10559",
      "authors": [
        {
          "_id": "67ff1df03b42083b37219456",
          "name": "Keyu Duan",
          "hidden": false
        },
        {
          "_id": "67ff1df03b42083b37219457",
          "name": "Zichen Liu",
          "hidden": false
        },
        {
          "_id": "67ff1df03b42083b37219458",
          "name": "Xin Mao",
          "hidden": false
        },
        {
          "_id": "67ff1df03b42083b37219459",
          "name": "Tianyu Pang",
          "hidden": false
        },
        {
          "_id": "67ff1df03b42083b3721945a",
          "name": "Changyu Chen",
          "hidden": false
        },
        {
          "_id": "67ff1df03b42083b3721945b",
          "name": "Qiguang Chen",
          "hidden": false
        },
        {
          "_id": "67ff1df03b42083b3721945c",
          "name": "Michael Qizhe Shieh",
          "hidden": false
        },
        {
          "_id": "67ff1df03b42083b3721945d",
          "user": {
            "_id": "6214e4ee1e35c843d42d1f88",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6214e4ee1e35c843d42d1f88/fj-9wuIdPhvogh3BrcXTB.jpeg",
            "isPro": true,
            "fullname": "Longxu Dou",
            "user": "dreamerdeo",
            "type": "user"
          },
          "name": "Longxu Dou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-16T08:43:00.444Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-14T14:53:56.000Z",
      "submittedOnDailyAt": "2025-04-16T01:34:03.069Z",
      "title": "Méthode pour atteindre l'entraînement de modèles de récompense de processus efficaces par apprentissage chimique actif",
      "submittedOnDailyBy": {
        "_id": "6214e4ee1e35c843d42d1f88",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6214e4ee1e35c843d42d1f88/fj-9wuIdPhvogh3BrcXTB.jpeg",
        "isPro": true,
        "fullname": "Longxu Dou",
        "user": "dreamerdeo",
        "type": "user"
      },
      "summary": "Le modèle de récompense par processus (PRMs) fournit des sous-réponses statistiques dans les étapes pour les modèles de langage grands (LLMs), mais présente des défis à l'échelle de l'explication du jeu de données d'entraînement tant pour les humains que pour les LLMs. Pour surmonter cette limitation, nous proposons un approche d'apprentissage actif. ActPRM sélectionne de manière active les échantillons les plus incertains pour réduire les coûts de marquage. Pendant l'entraînement, ActPRM évalue l'incertitude post-propagation et laisse seuls les données à haut risque basé sur cette incertitude. Ensuite, un modèle d'inférence avec des coûts associés plus élevés est utilisé pour marquer ces données. La perte par rapport à la marque est calculée et les poids d'ActPRM sont mis à jour. Comparé à l'ajustement micro de la théorie de la probabilité, ActPRM réduit de 50% les explications dans un environnement d'apprentissage actif basé sur des ensembles (POOL), tout en présentant un rendement relativement meilleur. En dépassant l'efficacité des explications, nous utilisons ActPRM pour filtrer un chargeur de données logiques mathématiques de plus de 1M, laissant seulement le 60% des données. Ensuite, l'entraînement basé sur ce jeu de données sélectionné permet au PRM des nouvelles technologies plus récentes (SOTA) d'atteindre les niveaux de ProcessBench (75.0%) et PRMBench (65.5%).",
      "upvotes": 8,
      "discussionId": "67ff1df23b42083b372194a8",
      "githubRepo": "https://github.com/sail-sg/ActivePRM",
      "ai_keywords": [
        "active learning",
        "ActPRM",
        "uncertainty estimation",
        "labeling costs",
        "vanilla fine-tuning",
        "pool-based active learning",
        "annotation efficiency",
        "math reasoning trajectories",
        "state-of-the-art (SOTA)",
        "ProcessBench",
        "PRMBench"
      ]
    },
    "publishedAt": "2025-04-14T10:53:56.000Z",
    "title": "Efficient Process Reward Model Training via Active Learning",
    "summary": "Process Reward Models (PRMs) provide step-level supervision to large language\nmodels (LLMs), but scaling up training data annotation remains challenging for\nboth humans and LLMs. To address this limitation, we propose an active learning\napproach, ActPRM, which proactively selects the most uncertain samples for\ntraining, substantially reducing labeling costs. During training, we use the\nPRM to estimate uncertainty after the forward pass, retaining only highly\nuncertain data. A capable yet costly reasoning model then labels this data.\nThen we compute the loss with respect to the labels and update the PRM's\nweights. We compare ActPRM vs. vanilla fine-tuning, on a pool-based active\nlearning setting, demonstrating that ActPRM reduces 50% annotation, but\nachieving the comparable or even better performance. Beyond annotation\nefficiency, we further advance the actively trained PRM by filtering over 1M+\nmath reasoning trajectories with ActPRM, retaining 60% of the data. A\nsubsequent training on this selected dataset yields a new state-of-the-art\n(SOTA) PRM on ProcessBench (75.0%) and PRMBench (65.5%) compared with same\nsized models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10559.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6214e4ee1e35c843d42d1f88",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6214e4ee1e35c843d42d1f88/fj-9wuIdPhvogh3BrcXTB.jpeg",
      "fullname": "Longxu Dou",
      "name": "dreamerdeo",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 19
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.11427",
      "authors": [
        {
          "_id": "67ff1cc4372d6790b1b7da90",
          "name": "Yanrui Bin",
          "hidden": false
        },
        {
          "_id": "67ff1cc4372d6790b1b7da91",
          "name": "Wenbo Hu",
          "hidden": false
        },
        {
          "_id": "67ff1cc4372d6790b1b7da92",
          "name": "Haoyuan Wang",
          "hidden": false
        },
        {
          "_id": "67ff1cc4372d6790b1b7da93",
          "name": "Xinya Chen",
          "hidden": false
        },
        {
          "_id": "67ff1cc4372d6790b1b7da94",
          "name": "Bing Wang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/657a7458afbb0117ba15c59f/dKoipt1ASgjiyDt33avVt.mp4"
      ],
      "publishedAt": "2025-04-15T17:39:07.000Z",
      "submittedOnDailyAt": "2025-04-16T01:28:48.790Z",
      "title": "NormalCrafter : Apprentissage des vecteurs de rythme pour synchroniser les séquences temporelles dans les films\nProjet dispersé vers le haut",
      "submittedOnDailyBy": {
        "_id": "657a7458afbb0117ba15c59f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/657a7458afbb0117ba15c59f/8_iwTS1UG_mKnfylFbLsY.jpeg",
        "isPro": false,
        "fullname": "Wenbo Hu",
        "user": "wbhu-tc",
        "type": "user"
      },
      "summary": "La prévision de la surface Normal est essentielle pour de nombreuses applications de la vision par ordinateur. Bien que beaucoup d'efforts aient été déployés pour des scénarios d'images statiques, garantir la cohérence temporelle dans la prévision de Normal à partir de vidéo est un défi complexe. Au lieu de rajouter des éléments temporels à des méthodes existantes, nous présentons NormalCrafter, qui exploite les prédictions temporelles intrinsèques au modèle de diffusion vidéo. Nous proposons la Régularisation des Caractéristiques Sémantiques (SFR) pour garantir une prévision de Normal de haute qualité tout au long de la séquence, en recommandant que le modèle se concentre sur les caractéristiques significatives spécifiques de chaque image. De plus, nous introduisons un protocole d'apprentissage en deux étapes qui utilise à la fois l'espace potentiel et l'espace de pixels, avec l'objectif de maintenir la précision spatiale tout en maintenant un contexte temporel à long terme. Une évaluation large montre l'effet de notre méthode et montre que, à partir de multiples vidéos, notre approche génère des séquences de Normal temporellement cohérentes avec des détails complexes, démontrant son efficacité.",
      "upvotes": 5,
      "discussionId": "67ff1cc5372d6790b1b7daee",
      "projectPage": "https://normalcrafter.github.io/",
      "githubRepo": "https://github.com/Binyr/NormalCrafter",
      "ai_keywords": [
        "video diffusion models",
        "Semantic Feature Regularization (SFR)",
        "latent space",
        "pixel space",
        "temporal coherence",
        "spatial accuracy",
        "long temporal context",
        "temporally consistent",
        "intricate details"
      ]
    },
    "publishedAt": "2025-04-15T13:39:07.000Z",
    "title": "NormalCrafter: Learning Temporally Consistent Normals from Video\n  Diffusion Priors",
    "summary": "Surface normal estimation serves as a cornerstone for a spectrum of computer\nvision applications. While numerous efforts have been devoted to static image\nscenarios, ensuring temporal coherence in video-based normal estimation remains\na formidable challenge. Instead of merely augmenting existing methods with\ntemporal components, we present NormalCrafter to leverage the inherent temporal\npriors of video diffusion models. To secure high-fidelity normal estimation\nacross sequences, we propose Semantic Feature Regularization (SFR), which\naligns diffusion features with semantic cues, encouraging the model to\nconcentrate on the intrinsic semantics of the scene. Moreover, we introduce a\ntwo-stage training protocol that leverages both latent and pixel space learning\nto preserve spatial accuracy while maintaining long temporal context. Extensive\nevaluations demonstrate the efficacy of our method, showcasing a superior\nperformance in generating temporally consistent normal sequences with intricate\ndetails from diverse videos.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/657a7458afbb0117ba15c59f/dKoipt1ASgjiyDt33avVt.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11427.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "657a7458afbb0117ba15c59f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/657a7458afbb0117ba15c59f/8_iwTS1UG_mKnfylFbLsY.jpeg",
      "fullname": "Wenbo Hu",
      "name": "wbhu-tc",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.11343",
      "authors": [
        {
          "_id": "67ff2d4a86e7ad2b4bea1349",
          "name": "Wei Xiong",
          "hidden": false
        },
        {
          "_id": "67ff2d4a86e7ad2b4bea134a",
          "name": "Jiarui Yao",
          "hidden": false
        },
        {
          "_id": "67ff2d4a86e7ad2b4bea134b",
          "name": "Yuhui Xu",
          "hidden": false
        },
        {
          "_id": "67ff2d4a86e7ad2b4bea134c",
          "name": "Bo Pang",
          "hidden": false
        },
        {
          "_id": "67ff2d4a86e7ad2b4bea134d",
          "name": "Lei Wang",
          "hidden": false
        },
        {
          "_id": "67ff2d4a86e7ad2b4bea134e",
          "name": "Doyen Sahoo",
          "hidden": false
        },
        {
          "_id": "67ff2d4a86e7ad2b4bea134f",
          "name": "Junnan Li",
          "hidden": false
        },
        {
          "_id": "67ff2d4a86e7ad2b4bea1350",
          "name": "Nan Jiang",
          "hidden": false
        },
        {
          "_id": "67ff2d4a86e7ad2b4bea1351",
          "name": "Tong Zhang",
          "hidden": false
        },
        {
          "_id": "67ff2d4a86e7ad2b4bea1352",
          "name": "Caiming Xiong",
          "hidden": false
        },
        {
          "_id": "67ff2d4a86e7ad2b4bea1353",
          "name": "Hanze Dong",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-15T16:15:02.000Z",
      "submittedOnDailyAt": "2025-04-16T02:39:25.160Z",
      "title": "Un approche minimaliste dans l'inférence de modèles de langage grands : raffinement expérimental à partir du sampling de rejet",
      "submittedOnDailyBy": {
        "_id": "643e59806db6ba8c5ee123f3",
        "avatarUrl": "/avatars/4052f2a250107f43b3634c3ee3cc30a1.svg",
        "isPro": false,
        "fullname": "Wei Xiong",
        "user": "weqweasdas",
        "type": "user"
      },
      "summary": "L'apprentissage par renforcement (RL) a acquis une position principale comme approche principale pour ajuster des grands modèles de langage (LLMs) pour faire face à des tâches complexes de raisonnement. Récemment, des méthodes comme GRPO ont montré des succès expérimentaux dans l'entraînement de modèles tels que DeepSeek-R1, bien que leur compréhension des effets soit encore incipiente. Dans cet article, GRPO est réévalué et ses composants essentiels sont analysés à partir de la perspective des algorithmes d'apprentissage par renforcement comme le gradient de politiques. De manière surprenante, le simple RAFT, basé sur des échantillons de rejet, atteint des rendements comparables à GRPO et PPO, car il est entraîné uniquement avec des échantillons qui reçoivent des récompenses positives. N'est pas efficace de normalisation, mais l'un des principaux avantages de GRPO est d'éliminer complètement les réponses erronées aux questions. D'une telle perspective, une version minimale d'expansion du gradient de politiques qui filtre autant de réponses complètement erronées que correctes, appelé Reinforce-Rej, est proposée. Reinforce-Rej améliore l'efficacité et la stabilité de la divergence de Kullback-Leibler (KL), et est utilisée comme une alternative efficace et légère par rapport aux algorithmes RL complexes. RAFT est adopté comme une base de référence interprétable et forte, et il est argumenté en faveur d'un design plus principiant au lieu d'utiliser des échantillons négatifs de manière indifférente. Nos résultats fournissent des orientations pour la recherche future sur le traitement post-compensatoire des LLMs.",
      "upvotes": 4,
      "discussionId": "67ff2d4b86e7ad2b4bea1381",
      "ai_keywords": [
        "GRPO",
        "DeepSeek-R1",
        "reinforcement learning (RL)",
        "fine-tuning",
        "large language models (LLMs)",
        "complex reasoning tasks",
        "RAFT",
        "positively rewarded samples",
        "policy gradient",
        "KL efficiency"
      ]
    },
    "publishedAt": "2025-04-15T12:15:02.000Z",
    "title": "A Minimalist Approach to LLM Reasoning: from Rejection Sampling to\n  Reinforce",
    "summary": "Reinforcement learning (RL) has become a prevailing approach for fine-tuning\nlarge language models (LLMs) on complex reasoning tasks. Among recent methods,\nGRPO stands out for its empirical success in training models such as\nDeepSeek-R1, yet the sources of its effectiveness remain poorly understood. In\nthis work, we revisit GRPO from a reinforce-like algorithm perspective and\nanalyze its core components. Surprisingly, we find that a simple rejection\nsampling baseline, RAFT, which trains only on positively rewarded samples,\nyields competitive performance than GRPO and PPO. Our ablation studies reveal\nthat GRPO's main advantage arises from discarding prompts with entirely\nincorrect responses, rather than from its reward normalization. Motivated by\nthis insight, we propose Reinforce-Rej, a minimal extension of policy gradient\nthat filters both entirely incorrect and entirely correct samples.\nReinforce-Rej improves KL efficiency and stability, serving as a lightweight\nyet effective alternative to more complex RL algorithms. We advocate RAFT as a\nrobust and interpretable baseline, and suggest that future advances should\nfocus on more principled designs for incorporating negative samples, rather\nthan relying on them indiscriminately. Our findings provide guidance for future\nwork in reward-based LLM post-training.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11343.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "643e59806db6ba8c5ee123f3",
      "avatarUrl": "/avatars/4052f2a250107f43b3634c3ee3cc30a1.svg",
      "fullname": "Wei Xiong",
      "name": "weqweasdas",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 18
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.10188",
      "authors": [
        {
          "_id": "67fe602166c0e8f3c2df22a9",
          "user": {
            "_id": "649d59cec6b4fdd84ebe0d47",
            "avatarUrl": "/avatars/a070e15659c0686fdfc69e559f3d6493.svg",
            "isPro": false,
            "fullname": "Deyuan Liu",
            "user": "SempraETY",
            "type": "user"
          },
          "name": "Deyuan Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-15T16:46:56.270Z",
          "hidden": false
        },
        {
          "_id": "67fe602166c0e8f3c2df22aa",
          "name": "Peng Sun",
          "hidden": false
        },
        {
          "_id": "67fe602166c0e8f3c2df22ab",
          "name": "Xufeng Li",
          "hidden": false
        },
        {
          "_id": "67fe602166c0e8f3c2df22ac",
          "name": "Tao Lin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-14T12:43:17.000Z",
      "submittedOnDailyAt": "2025-04-16T03:03:14.152Z",
      "title": "Utilisation de représentations internes par des modèles génératifs efficacesment entraînés",
      "submittedOnDailyBy": {
        "_id": "649d59cec6b4fdd84ebe0d47",
        "avatarUrl": "/avatars/a070e15659c0686fdfc69e559f3d6493.svg",
        "isPro": false,
        "fullname": "Deyuan Liu",
        "user": "SempraETY",
        "type": "user"
      },
      "summary": "Les modèles de diffusion montrent un excellent rendement dans la génération de données de haute dimension, bien que des lacunes soient identifiées en termes d'efficacité d'entraînement et de qualité de la représentation lorsqu'ils sont comparés aux méthodes de reconnaissance automatique. Nous avons identifié les principales lacunes : l'insuffisance de l'utilisation de représentations de haute qualité et significatives pendant l'entraînement, qui retarde particulièrement la vitesse de convergence. À travers un analyse systématique, nous avons découvert que principalement dans les régions de traitement de représentation, les couches initiales apprennent des patrons de signification et d'architecture jusqu'à ce que la génération soit complète. Pour aborder ces lacunes, nous proposons le Warmup de Représentation Embarquée (ERW). L'ERW initialise les modules ERW avec des représentations de haute qualité et préalablement entraînées, similairement à un WARMUP. Ce WARMUP aide à initier l'entraînement des représentations de manière à minimiser l'inhibition et à accélérer la convergence, améliorant ainsi le rendement. Selon des analyses théoriques, l'effet de l'ERW est confirmé par l'intégration précise de certains caps de la réseau neuronal dans la zone de traitement de représentation. De plus, l'ERW réellement améliore la qualité des représentations, ce qui est démontré par une accélération réelle de la vitesse de convergence : expérimentalement, comparé au méthode optimale actuelle REPA, l'ERW accélère l'entraînement de 40 fois. Le code est disponible sur https://github.com/LINs-lab/ERW.",
      "upvotes": 4,
      "discussionId": "67fe602266c0e8f3c2df2334",
      "projectPage": "https://lins-lab.github.io/ERW/",
      "githubRepo": "https://github.com/LINs-lab/ERW",
      "ai_keywords": [
        "diffusion models",
        "high-dimensional data",
        "self-supervised methods",
        "high-quality representations",
        "semantic representations",
        "structural pattern learning",
        "Embedded Representation Warmup (ERW)",
        "warmup",
        "early layers",
        "representation processing region",
        "pretrained representations",
        "convergence",
        "training convergence",
        "representation quality",
        "REPA"
      ]
    },
    "publishedAt": "2025-04-14T08:43:17.000Z",
    "title": "Efficient Generative Model Training via Embedded Representation Warmup",
    "summary": "Diffusion models excel at generating high-dimensional data but fall short in\ntraining efficiency and representation quality compared to self-supervised\nmethods. We identify a key bottleneck: the underutilization of high-quality,\nsemantically rich representations during training notably slows down\nconvergence. Our systematic analysis reveals a critical representation\nprocessing region -- primarily in the early layers -- where semantic and\nstructural pattern learning takes place before generation can occur. To address\nthis, we propose Embedded Representation Warmup (ERW), a plug-and-play\nframework where in the first stage we get the ERW module serves as a warmup\nthat initializes the early layers of the diffusion model with high-quality,\npretrained representations. This warmup minimizes the burden of learning\nrepresentations from scratch, thereby accelerating convergence and boosting\nperformance. Our theoretical analysis demonstrates that ERW's efficacy depends\non its precise integration into specific neural network layers -- termed the\nrepresentation processing region -- where the model primarily processes and\ntransforms feature representations for later generation. We further establish\nthat ERW not only accelerates training convergence but also enhances\nrepresentation quality: empirically, our method achieves a 40times\nacceleration in training speed compared to REPA, the current state-of-the-art\nmethods. Code is available at https://github.com/LINs-lab/ERW.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10188.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "649d59cec6b4fdd84ebe0d47",
      "avatarUrl": "/avatars/a070e15659c0686fdfc69e559f3d6493.svg",
      "fullname": "Deyuan Liu",
      "name": "SempraETY",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.11447",
      "authors": [
        {
          "_id": "67ff1026f8afab940cc23f88",
          "name": "An Zhaol",
          "hidden": false
        },
        {
          "_id": "67ff1026f8afab940cc23f89",
          "name": "Shengyuan Zhang",
          "hidden": false
        },
        {
          "_id": "67ff1026f8afab940cc23f8a",
          "name": "Ling Yang",
          "hidden": false
        },
        {
          "_id": "67ff1026f8afab940cc23f8b",
          "name": "Zejian Li",
          "hidden": false
        },
        {
          "_id": "67ff1026f8afab940cc23f8c",
          "name": "Jiale Wu",
          "hidden": false
        },
        {
          "_id": "67ff1026f8afab940cc23f8d",
          "name": "Haoran Xu",
          "hidden": false
        },
        {
          "_id": "67ff1026f8afab940cc23f8e",
          "name": "AnYang Wei",
          "hidden": false
        },
        {
          "_id": "67ff1026f8afab940cc23f8f",
          "name": "Perry Pengyun GU Lingyun Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-15T17:57:13.000Z",
      "submittedOnDailyAt": "2025-04-16T00:35:02.754Z",
      "title": "La réalisation efficace d'un LiDAR 3D à bureau par optimisation directe des préférences en utilisant la Diffusion Déshydratation",
      "submittedOnDailyBy": {
        "_id": "63943c882b9483beb473ec25",
        "avatarUrl": "/avatars/abd2aae43e68c34770159c15a01c8297.svg",
        "isPro": false,
        "fullname": "Shengyuan Zhang",
        "user": "SYZhang0805",
        "type": "user"
      },
      "summary": "L'utilisation de modèles de dispersion à la fin d'un cycle de LiDAR est limitée par la vitesse lente de prise de mesures de la dispersion. La vitesse de prise de mesures est réduite pour économiser des points, mais cela entraîne une perte de performance. Après l'entraînement avec l'optimisation directe des politiques (DPO), le rendement est amélioré en utilisant des données de préférence. Dans cet article, nous proposons un nouveau cadre d'économie de dispersion appelé \"Distillation-DPO\", qui dispose d'une configuration de préférence. Tout d'abord, des cycles de LiDAR complets sont générés avec des combinaisons de bruits initiaux différents. Ensuite, des paires de échantillons de gagnants et de perdants sont établies en utilisant une mesure d'évaluation de LiDAR comme préférence. Cette configuration est raisonnable et efficace, car de nombreux indicateurs de LiDAR contiennent de l'information mais ne peuvent pas être optimisés directement. De plus, \"Distillation-DPO\" optimise le modèle étudiant en utilisant la différence de fonctions de score entre le modèle enseignant et l'étudiant. Ce processus est répété jusqu'à la convergence. Des expériences larges comparant le modèle de dispersion de LiDAR complet à l'état de limite avec \"Distillation-DPO\" montrent que le dernier réalise des cycles de meilleure qualité et accélère la vitesse de terminaison plus de cinq fois. Notre méthode vise à appliquer l'apprentissage par préférence pour l'économie. Notre code est disponible sur https://github.com/happyw1nd/DistillationDPO.",
      "upvotes": 3,
      "discussionId": "67ff1027f8afab940cc23fd4",
      "ai_keywords": [
        "diffusion models",
        "LiDAR scene completion",
        "score distillation",
        "direct policy optimization (DPO)",
        "preference alignment",
        "student model",
        "paired completion scenes",
        "LiDAR scene evaluation metrics",
        "winning and losing sample pairs",
        "score functions",
        "preference learning"
      ]
    },
    "publishedAt": "2025-04-15T13:57:13.000Z",
    "title": "Diffusion Distillation With Direct Preference Optimization For Efficient\n  3D LiDAR Scene Completion",
    "summary": "The application of diffusion models in 3D LiDAR scene completion is limited\ndue to diffusion's slow sampling speed. Score distillation accelerates\ndiffusion sampling but with performance degradation, while post-training with\ndirect policy optimization (DPO) boosts performance using preference data. This\npaper proposes Distillation-DPO, a novel diffusion distillation framework for\nLiDAR scene completion with preference aligment. First, the student model\ngenerates paired completion scenes with different initial noises. Second, using\nLiDAR scene evaluation metrics as preference, we construct winning and losing\nsample pairs. Such construction is reasonable, since most LiDAR scene metrics\nare informative but non-differentiable to be optimized directly. Third,\nDistillation-DPO optimizes the student model by exploiting the difference in\nscore functions between the teacher and student models on the paired completion\nscenes. Such procedure is repeated until convergence. Extensive experiments\ndemonstrate that, compared to state-of-the-art LiDAR scene completion diffusion\nmodels, Distillation-DPO achieves higher-quality scene completion while\naccelerating the completion speed by more than 5-fold. Our method is the first\nto explore adopting preference learning in distillation to the best of our\nknowledge and provide insights into preference-aligned distillation. Our code\nis public available on https://github.com/happyw1nd/DistillationDPO.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11447.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63943c882b9483beb473ec25",
      "avatarUrl": "/avatars/abd2aae43e68c34770159c15a01c8297.svg",
      "fullname": "Shengyuan Zhang",
      "name": "SYZhang0805",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.11326",
      "authors": [
        {
          "_id": "67ff25b765b52d1b69c1f6c1",
          "user": {
            "_id": "67ff29ecbf6889a333c69c7a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/fYIJFtF0uciB-1wb0lmTY.png",
            "isPro": false,
            "fullname": "Henghui Ding",
            "user": "HenghuiDing",
            "type": "user"
          },
          "name": "Henghui Ding",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-16T03:55:17.825Z",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6c2",
          "name": "Chang Liu",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6c3",
          "name": "Nikhila Ravi",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6c4",
          "name": "Shuting He",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6c5",
          "name": "Yunchao Wei",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6c6",
          "name": "Song Bai",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6c7",
          "name": "Philip Torr",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6c8",
          "name": "Kehuan Song",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6c9",
          "name": "Xinglin Xie",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6ca",
          "name": "Kexin Zhang",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6cb",
          "name": "Licheng Jiao",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6cc",
          "name": "Lingling Li",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6cd",
          "name": "Shuyuan Yang",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6ce",
          "name": "Xuqiang Cao",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6cf",
          "name": "Linnan Zhao",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6d0",
          "name": "Jiaxuan Zhao",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6d1",
          "name": "Fang Liu",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6d2",
          "name": "Mengjiao Wang",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6d3",
          "name": "Junpei Zhang",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6d4",
          "name": "Xu Liu",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6d5",
          "name": "Yuting Yang",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6d6",
          "name": "Mengru Ma",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6d7",
          "name": "Hao Fang",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6d8",
          "name": "Runmin Cong",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6d9",
          "name": "Xiankai Lu",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6da",
          "name": "Zhiyang Che",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6db",
          "name": "Wei Zhan",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6dc",
          "name": "Tianming Liang",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6dd",
          "name": "Haichao Jiang",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6de",
          "name": "Wei-Shi Zheng",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6df",
          "name": "Jian-Fang Hu",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6e0",
          "name": "Haobo Yuan",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6e1",
          "user": {
            "_id": "63958b4414513eaf9029ebf1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/U1g5H071pWRswGAG9UTpo.png",
            "isPro": false,
            "fullname": "Xiangtai Li",
            "user": "LXT",
            "type": "user"
          },
          "name": "Xiangtai Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-16T08:42:58.429Z",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6e2",
          "name": "Tao Zhang",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6e3",
          "name": "Lu Qi",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6e4",
          "name": "Ming-Hsuan Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-15T16:02:47.000Z",
      "submittedOnDailyAt": "2025-04-16T02:26:49.281Z",
      "title": "PVUW 2025 Rapport de Défi : Progrès dans la Compréhension du Vidéo de la Vie Naturelle au Niveau des Pixels",
      "submittedOnDailyBy": {
        "_id": "67ff29ecbf6889a333c69c7a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/fYIJFtF0uciB-1wb0lmTY.png",
        "isPro": false,
        "fullname": "Henghui Ding",
        "user": "HenghuiDing",
        "type": "user"
      },
      "summary": "Ce rapport fournit une description détaillée des quatre concours de Compréhension du niveau pixel de vidéo dans le monde (PVUW) qui seront organisés en conjonction avec la CVPR de 2025. Ce rapport résume les résultats des concours et contient des informations sur la façon de participer et les directions de recherche futures. Les concours présentent deux phases. Une se concentre sur la segmentation d'objets dans des vidéos de panneaux complexes, appelée MOSE, tandis que l'autre travaille sur la segmentation de vidéo basée sur le langage, guidée par le mouvement. Les deux phases présentent de nouveaux et plus difficiles ensembles de données pour refléter plus précisément le monde réel. A travers des évaluations et des analyses détaillées, les concours fournissent une guidance précieuse sur les technologies de segmentation de vidéos complexes et les tendances émergentes. Pour obtenir plus d'informations, consultez https://pvuw.github.io/.",
      "upvotes": 3,
      "discussionId": "67ff25b865b52d1b69c1f736"
    },
    "publishedAt": "2025-04-15T12:02:47.000Z",
    "title": "PVUW 2025 Challenge Report: Advances in Pixel-level Understanding of\n  Complex Videos in the Wild",
    "summary": "This report provides a comprehensive overview of the 4th Pixel-level Video\nUnderstanding in the Wild (PVUW) Challenge, held in conjunction with CVPR 2025.\nIt summarizes the challenge outcomes, participating methodologies, and future\nresearch directions. The challenge features two tracks: MOSE, which focuses on\ncomplex scene video object segmentation, and MeViS, which targets\nmotion-guided, language-based video segmentation. Both tracks introduce new,\nmore challenging datasets designed to better reflect real-world scenarios.\nThrough detailed evaluation and analysis, the challenge offers valuable\ninsights into the current state-of-the-art and emerging trends in complex video\nsegmentation. More information can be found on the workshop website:\nhttps://pvuw.github.io/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11326.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67ff29ecbf6889a333c69c7a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/fYIJFtF0uciB-1wb0lmTY.png",
      "fullname": "Henghui Ding",
      "name": "HenghuiDing",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.06949",
      "authors": [
        {
          "_id": "67ff12ea58ed263257af79b5",
          "name": "Zhixuan Lin",
          "hidden": false
        },
        {
          "_id": "67ff12ea58ed263257af79b6",
          "name": "Johan Obando-Ceron",
          "hidden": false
        },
        {
          "_id": "67ff12ea58ed263257af79b7",
          "user": {
            "_id": "66906c4e37eadb9c577984d3",
            "avatarUrl": "/avatars/b81765472942fdf94c0ee885ca62df2d.svg",
            "isPro": false,
            "fullname": "Owen He",
            "user": "littleowen",
            "type": "user"
          },
          "name": "Xu Owen He",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-16T02:16:11.020Z",
          "hidden": false
        },
        {
          "_id": "67ff12ea58ed263257af79b8",
          "name": "Aaron Courville",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-09T14:57:55.000Z",
      "submittedOnDailyAt": "2025-04-16T01:28:49.703Z",
      "title": "Adaptive Computation Pruning pour le Transformer de la Mauvaise Mémoire",
      "submittedOnDailyBy": {
        "_id": "6694cc1009326cb83f2d11bb",
        "avatarUrl": "/avatars/1ddaaed70a16ac475a9404848aef5d48.svg",
        "isPro": false,
        "fullname": "Zhixuan Lin",
        "user": "zhixuan-lin",
        "type": "user"
      },
      "summary": "Le Forgetting Transformer (FoX), récemment proposé, intègre un gate d'oubli dans la softmax attention, démontrant un rendement constant par rapport au Transformer standard basé sur RoPE. En particulier, de nombreux chevaux d'attention de FoX ont un oubli rapide, et le sortie à chaque étape temporelle dépend principalement du contexte local. En fonction de cette observation, nous proposons l'Adaptive Computation Pruning (ACP) pour FoX. L'ACP supprime de manière dynamique les calculs liés à une forte diminution de la dépendance entre entrée et sortie grâce à un gate d'oubli. L'ACP utilise un seuil dynamique de réduction pour rendre les poids d'attention réduits invisibles au niveau micro. L'ACP est appliquée lors de l'entraînement préalable de modèles de langage, réduisant approximativement 70% des FLOP de la softmax attention pour différents tailles de modèle et longueurs de contexte. On peut obtenir améliorations de 10% à 35% sur les étapes de traduction de tâches d'entraînement. De plus, les longueurs de contexte plus longues peuvent obtenir des réductions plus importantes de coût de calcul. Cette amélioration de vitesse est mise en œuvre de manière non affectée par une perte de rendement. De plus, nous avons effectué diverses analyses, telles que la revue des patrons de réduction et l'analyse de la distribution de réduction de FLOP dans différents chevaux d'attention, offrant une compréhension profonde du méthode. Le code est disponible sur https://github.com/zhixuan-lin/arctic-fox.",
      "upvotes": 3,
      "discussionId": "67ff12eb58ed263257af79fc",
      "ai_keywords": [
        "Forgetting Transformer (FoX)",
        "forget gate",
        "softmax attention",
        "RoPE-based Transformer",
        "Adaptive Computation Pruning (ACP)",
        "input-output dependencies",
        "pruning threshold",
        "FLOPs",
        "training throughput",
        "pruning patterns"
      ]
    },
    "publishedAt": "2025-04-09T10:57:55.000Z",
    "title": "Adaptive Computation Pruning for the Forgetting Transformer",
    "summary": "The recently proposed Forgetting Transformer (FoX) incorporates a forget gate\ninto softmax attention and has shown consistently better or on-par performance\ncompared to the standard RoPE-based Transformer. Notably, many attention heads\nin FoX tend to forget quickly, causing their output at each timestep to rely\nprimarily on the local context. Based on this observation, we propose Adaptive\nComputation Pruning (ACP) for FoX, a method that dynamically prunes\ncomputations involving input-output dependencies that are strongly decayed by\nthe forget gate. This is achieved using a dynamically set pruning threshold\nthat ensures that the pruned attention weights remain negligible. We apply ACP\nto language model pretraining with FoX and show it consistently reduces the\nnumber of FLOPs in softmax attention by around 70% across different model sizes\nand context lengths, resulting in a roughly 10% to 35% improvement in training\nthroughput. Furthermore, longer context lengths yield greater computational\nsavings. All these speed improvements are achieved without any performance\ndegradation. We also perform several analyses to provide deeper insights into\nour method, such as examining the pruning patterns and analyzing the\ndistribution of FLOP savings across different attention heads. Our code is\navailable at https://github.com/zhixuan-lin/arctic-fox.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.06949.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6694cc1009326cb83f2d11bb",
      "avatarUrl": "/avatars/1ddaaed70a16ac475a9404848aef5d48.svg",
      "fullname": "Zhixuan Lin",
      "name": "zhixuan-lin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.11456",
      "authors": [
        {
          "_id": "67ff79b3d68757d92e9c168e",
          "name": "Zhiwei He",
          "hidden": false
        },
        {
          "_id": "67ff79b3d68757d92e9c168f",
          "name": "Tian Liang",
          "hidden": false
        },
        {
          "_id": "67ff79b3d68757d92e9c1690",
          "name": "Jiahao Xu",
          "hidden": false
        },
        {
          "_id": "67ff79b3d68757d92e9c1691",
          "name": "Qiuzhi Liu",
          "hidden": false
        },
        {
          "_id": "67ff79b3d68757d92e9c1692",
          "name": "Xingyu Chen",
          "hidden": false
        },
        {
          "_id": "67ff79b3d68757d92e9c1693",
          "name": "Yue Wang",
          "hidden": false
        },
        {
          "_id": "67ff79b3d68757d92e9c1694",
          "name": "Linfeng Song",
          "hidden": false
        },
        {
          "_id": "67ff79b3d68757d92e9c1695",
          "name": "Dian Yu",
          "hidden": false
        },
        {
          "_id": "67ff79b3d68757d92e9c1696",
          "name": "Zhenwen Liang",
          "hidden": false
        },
        {
          "_id": "67ff79b3d68757d92e9c1697",
          "name": "Wenxuan Wang",
          "hidden": false
        },
        {
          "_id": "67ff79b3d68757d92e9c1698",
          "name": "Zhuosheng Zhang",
          "hidden": false
        },
        {
          "_id": "67ff79b3d68757d92e9c1699",
          "name": "Rui Wang",
          "hidden": false
        },
        {
          "_id": "67ff79b3d68757d92e9c169a",
          "name": "Zhaopeng Tu",
          "hidden": false
        },
        {
          "_id": "67ff79b3d68757d92e9c169b",
          "name": "Haitao Mi",
          "hidden": false
        },
        {
          "_id": "67ff79b3d68757d92e9c169c",
          "name": "Dong Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-15T17:59:51.000Z",
      "submittedOnDailyAt": "2025-04-16T08:09:13.393Z",
      "title": "DeepMath-103K : Un ensemble de données mathématiques à grande échelle, complexe, non contaminé et vérifiable, qui favorise le développement de la logique.",
      "submittedOnDailyBy": {
        "_id": "60107b385ac3e86b3ea4fc34",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1627505688463-60107b385ac3e86b3ea4fc34.jpeg",
        "isPro": true,
        "fullname": "Daniel van Strien",
        "user": "davanstrien",
        "type": "user"
      },
      "summary": "La capacité mathématique et logique complexes est un important cadre de référence pour l'intelligence artificielle. L'application de l'apprentissage par renforcement (RL) dans des modèles de langage grands (LLM) a démontré des résultats désirables, mais son développement est limité par la rareté de grands ensembles de données, l'insuffisance de ces derniers pour être adaptés à l'apprentissage par renforcement et la manque de formats de réponses probablement valides, en plus de la contamination causée par les cadres d'évaluation. Pour surmonter ces limitations, nous présentons DeepMath-103K. DeepMath-103K est un nouveau ensemble de données d'environ 103K problèmes mathématiques conçu spécialement pour entraîner des modèles de haute logique en utilisant RL. Cet ensemble de données a été ajusté avec précision grâce à l'analyse de données de sources, l'élimination de la contamination dans plusieurs cadres de référence et la filtration des niveaux de difficulté élevée (principalement les niveaux 5-9). Chaque problème comprend une réponse finale probablement valide, ce qui permet l'apprentissage basé sur des règles et inclut trois solutions générées par R1 différents, ce qui l'adapte à différents paradigmes d'entraînement (par exemple, apprentissage par observation et sauts). Il étend le domaine des thèmes mathématiques et encourage le développement de logique généralisable. Les modèles entraînés avec DeepMath-103K montrent des améliorations significatives sur les benchmarks mathématiques difficiles et ont été testés. DeepMath-103K a été lancé publiquement avec l'objectif de favoriser le développement d'une communauté contribuant à la construction de systèmes de logique artificielle plus puissants : https://github.com/zwhe99/DeepMath.",
      "upvotes": 2,
      "discussionId": "67ff79b4d68757d92e9c16e1",
      "githubRepo": "https://github.com/zwhe99/DeepMath",
      "ai_keywords": [
        "reinforcement learning",
        "large-scale dataset",
        "mathematical problems",
        "training data",
        "verifiable answer formats",
        "decontamination",
        "benchmark",
        "curriculum learning",
        "rule-based RL",
        "supervised fine-tuning",
        "distillation",
        "generalizable reasoning",
        "AI reasoning systems"
      ]
    },
    "publishedAt": "2025-04-15T13:59:51.000Z",
    "title": "DeepMath-103K: A Large-Scale, Challenging, Decontaminated, and\n  Verifiable Mathematical Dataset for Advancing Reasoning",
    "summary": "The capacity for complex mathematical reasoning is a key benchmark for\nartificial intelligence. While reinforcement learning (RL) applied to LLMs\nshows promise, progress is significantly hindered by the lack of large-scale\ntraining data that is sufficiently challenging, possesses verifiable answer\nformats suitable for RL, and is free from contamination with evaluation\nbenchmarks. To address these limitations, we introduce DeepMath-103K, a new,\nlarge-scale dataset comprising approximately 103K mathematical problems,\nspecifically designed to train advanced reasoning models via RL. DeepMath-103K\nis curated through a rigorous pipeline involving source analysis, stringent\ndecontamination against numerous benchmarks, and filtering for high difficulty\n(primarily Levels 5-9), significantly exceeding existing open resources in\nchallenge. Each problem includes a verifiable final answer, enabling rule-based\nRL, and three distinct R1-generated solutions suitable for diverse training\nparadigms like supervised fine-tuning or distillation. Spanning a wide range of\nmathematical topics, DeepMath-103K promotes the development of generalizable\nreasoning. We demonstrate that models trained on DeepMath-103K achieve\nsignificant improvements on challenging mathematical benchmarks, validating its\neffectiveness. We release DeepMath-103K publicly to facilitate community\nprogress in building more capable AI reasoning systems:\nhttps://github.com/zwhe99/DeepMath.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11456.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60107b385ac3e86b3ea4fc34",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1627505688463-60107b385ac3e86b3ea4fc34.jpeg",
      "fullname": "Daniel van Strien",
      "name": "davanstrien",
      "type": "user",
      "isPro": true,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 586
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.11001",
      "authors": [
        {
          "_id": "67ff4bdb1dc5d56fdd7a1bc4",
          "user": {
            "_id": "62d7b2339b629105a5d6888a",
            "avatarUrl": "/avatars/c3f164fde6b8f9a671890e08ce8a3e75.svg",
            "isPro": false,
            "fullname": "Alan Dao",
            "user": "alandao",
            "type": "user"
          },
          "name": "Alan Dao",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-16T06:19:08.903Z",
          "hidden": false
        },
        {
          "_id": "67ff4bdb1dc5d56fdd7a1bc5",
          "name": "Thinh Le",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/62d7b2339b629105a5d6888a/s2uxHvFhyqjBXMY5wkDLy.mp4"
      ],
      "publishedAt": "2025-04-15T09:18:21.000Z",
      "submittedOnDailyAt": "2025-04-16T04:49:41.555Z",
      "title": "ReZero : Méthode d'intention de réduire la capacité de recherche d'un modèle de langage génératif (LLM) en utilisant le monomisisme",
      "submittedOnDailyBy": {
        "_id": "62d7b2339b629105a5d6888a",
        "avatarUrl": "/avatars/c3f164fde6b8f9a671890e08ce8a3e75.svg",
        "isPro": false,
        "fullname": "Alan Dao",
        "user": "alandao",
        "type": "user"
      },
      "summary": "La RAG (Retrieval-Augmented Generation) est utilisée pour améliorer le rendement des modèles de langage grands (LLM) dans des tâches de regroupement de connaissances, mais dépend sensiblement de la qualité des premiers mots-clés de recherche. Les méthodes actuelles utilisent généralement l'apprentissage par renforcement (RL) pour se concentrer sur la configuration des mots-clés de recherche ou sur les résultats de la recherche, mais ne promeut pas explicitement la recherche de nouveaux mots-clés après un échec initial de recherche. Nous présentons un nouveau cadre d'RL appelé ReZero (Réduisant l'apprentissage sans supervision). ReZero attribue une récompense directe pour la recherche de nouveaux mots-clés après un échec initial, induisant l'LLM à explorer des mots-clés alternatifs. ReZero atteint un rendement de précision de 46,88% plus que le standard de 25%, améliorant la robustesse de l'LLM dans des scénarios complexes de recherche d'information où les premiers mots-clés de recherche ne sont pas suffisants.",
      "upvotes": 2,
      "discussionId": "67ff4bdc1dc5d56fdd7a1c36",
      "githubRepo": "https://github.com/menloresearch/ReZero",
      "ai_keywords": [
        "Retrieval-Augmented Generation (RAG)",
        "Large Language Model (LLM)",
        "knowledge-intensive tasks",
        "Reinforcement Learning (RL)",
        "query formulation",
        "reasoning over results",
        "ReZero (Retry-Zero)",
        "persistence",
        "search query",
        "unsuccessful attempt",
        "alternative queries",
        "robustness",
        "information-seeking scenarios"
      ]
    },
    "publishedAt": "2025-04-15T05:18:21.000Z",
    "title": "ReZero: Enhancing LLM search ability by trying one-more-time",
    "summary": "Retrieval-Augmented Generation (RAG) improves Large Language Model (LLM)\nperformance on knowledge-intensive tasks but depends heavily on initial search\nquery quality. Current methods, often using Reinforcement Learning (RL),\ntypically focus on query formulation or reasoning over results, without\nexplicitly encouraging persistence after a failed search. We introduce ReZero\n(Retry-Zero), a novel RL framework that directly rewards the act of retrying a\nsearch query following an initial unsuccessful attempt. This incentivizes the\nLLM to explore alternative queries rather than prematurely halting. ReZero\ndemonstrates significant improvement, achieving 46.88% accuracy compared to a\n25% baseline. By rewarding persistence, ReZero enhances LLM robustness in\ncomplex information-seeking scenarios where initial queries may prove\ninsufficient.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62d7b2339b629105a5d6888a/s2uxHvFhyqjBXMY5wkDLy.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11001.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62d7b2339b629105a5d6888a",
      "avatarUrl": "/avatars/c3f164fde6b8f9a671890e08ce8a3e75.svg",
      "fullname": "Alan Dao",
      "name": "alandao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.10049",
      "authors": [
        {
          "_id": "67ff5b335cf0fe153845d1c9",
          "user": {
            "_id": "60d35154d7b174177faabd55",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1645712223620-60d35154d7b174177faabd55.jpeg",
            "isPro": false,
            "fullname": "Théo Gigant",
            "user": "gigant",
            "type": "user"
          },
          "name": "Théo Gigant",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-16T08:42:49.462Z",
          "hidden": false
        },
        {
          "_id": "67ff5b335cf0fe153845d1ca",
          "name": "Camille Guinaudeau",
          "hidden": false
        },
        {
          "_id": "67ff5b335cf0fe153845d1cb",
          "name": "Frédéric Dufaux",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-14T09:55:01.000Z",
      "submittedOnDailyAt": "2025-04-16T05:55:17.253Z",
      "title": "Résumé de la représentation multimodale et structurale sur l'impact de la modalité et de la structure",
      "submittedOnDailyBy": {
        "_id": "60d35154d7b174177faabd55",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1645712223620-60d35154d7b174177faabd55.jpeg",
        "isPro": false,
        "fullname": "Théo Gigant",
        "user": "gigant",
        "type": "user"
      },
      "summary": "Les modèles Vision-Language Models (VLMs) peuvent traiter plusieurs formes d'informations visuelles et textuelles : texte, images, combinaisons de texte et images, ou environ 1 heure de vidéo. Dans cette étude, une analyse qualitative et quantitative détaillée est effectuée sur l'auto-résumé de présentations multimodales en utilisant les VLMs. Dans ces expériences, une stratégie appropriée est proposée pour générer des résumés riches de documents multimodales en utilisant les VLMs. De plus, il est démontré que c'est bénéfique de comparer les feuilles de slide extraites de vidéos de streaming avec les vidéos originales et que les représentations structurées obtenues à partir de combinaisons de feuilles de slide et de texte offrent le meilleur rendement. Enfin, des conseils sont proposés pour réfléchir et améliorer les caractéristiques de l'interaction croisée entre différentes modalités de présentations.",
      "upvotes": 2,
      "discussionId": "67ff5b355cf0fe153845d215",
      "ai_keywords": [
        "Vision-Language Models (VLMs)",
        "automatic summarization",
        "multimodal presentations",
        "text-heavy multimodal documents",
        "input-length budgets",
        "slides",
        "video stream",
        "raw video",
        "structured representation",
        "interleaved slides",
        "transcript",
        "cross-modal interactions"
      ]
    },
    "publishedAt": "2025-04-14T05:55:01.000Z",
    "title": "Summarization of Multimodal Presentations with Vision-Language Models:\n  Study of the Effect of Modalities and Structure",
    "summary": "Vision-Language Models (VLMs) can process visual and textual information in\nmultiple formats: texts, images, interleaved texts and images, or even\nhour-long videos. In this work, we conduct fine-grained quantitative and\nqualitative analyses of automatic summarization of multimodal presentations\nusing VLMs with various representations as input. From these experiments, we\nsuggest cost-effective strategies for generating summaries from text-heavy\nmultimodal documents under different input-length budgets using VLMs. We show\nthat slides extracted from the video stream can be beneficially used as input\nagainst the raw video, and that a structured representation from interleaved\nslides and transcript provides the best performance. Finally, we reflect and\ncomment on the nature of cross-modal interactions in multimodal presentations\nand share suggestions to improve the capabilities of VLMs to understand\ndocuments of this nature.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10049.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60d35154d7b174177faabd55",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1645712223620-60d35154d7b174177faabd55.jpeg",
      "fullname": "Théo Gigant",
      "name": "gigant",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 32
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.08846",
      "authors": [
        {
          "_id": "67ff25f33026f8abc4c4a10d",
          "name": "Mostafa Faghih Shojaei",
          "hidden": false
        },
        {
          "_id": "67ff25f33026f8abc4c4a10e",
          "name": "Rahul Gulati",
          "hidden": false
        },
        {
          "_id": "67ff25f33026f8abc4c4a10f",
          "name": "Benjamin A. Jasperson",
          "hidden": false
        },
        {
          "_id": "67ff25f33026f8abc4c4a110",
          "name": "Shangshang Wang",
          "hidden": false
        },
        {
          "_id": "67ff25f33026f8abc4c4a111",
          "user": {
            "_id": "6729342804227d5ea3b283c1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/_GPZiRH3LA39QleNtRqYH.png",
            "isPro": false,
            "fullname": "Simone Cimolato",
            "user": "simocimolato",
            "type": "user"
          },
          "name": "Simone Cimolato",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-16T08:42:55.794Z",
          "hidden": false
        },
        {
          "_id": "67ff25f33026f8abc4c4a112",
          "user": {
            "_id": "672ad19141a93b8e140e8689",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/VRMw7_PIE5QoD_eWP0hOp.png",
            "isPro": false,
            "fullname": "Dangli Cao",
            "user": "Dinzhenzhenzhu",
            "type": "user"
          },
          "name": "Dangli Cao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-16T09:23:14.535Z",
          "hidden": false
        },
        {
          "_id": "67ff25f33026f8abc4c4a113",
          "name": "Willie Neiswanger",
          "hidden": false
        },
        {
          "_id": "67ff25f33026f8abc4c4a114",
          "user": {
            "_id": "67859d73e670c62966ba5767",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Mx3fE-YPsjsxpYsBh_DDn.png",
            "isPro": false,
            "fullname": "Krishna Garikipati",
            "user": "garikipati",
            "type": "user"
          },
          "name": "Krishna Garikipati",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-04-16T05:32:49.978Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-11T01:26:34.000Z",
      "submittedOnDailyAt": "2025-04-16T07:21:37.722Z",
      "title": "AI Universidad : Plateforme d'Intégration Alimentaire pour la Classe de Science dans Classland",
      "submittedOnDailyBy": {
        "_id": "6729342804227d5ea3b283c1",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/_GPZiRH3LA39QleNtRqYH.png",
        "isPro": false,
        "fullname": "Simone Cimolato",
        "user": "simocimolato",
        "type": "user"
      },
      "summary": "AI Universidad (AI-U) propose un cadre flexible pour la distribution de contenu des cours en utilisant l'IA, en s'adaptant aux pratiques d'enseignement des professeurs. L'approche est que AI-U utilise la technologie de RAG (Recherche d'Information pour la Génération de Contenu) pour des fins d'apprentissage pour ajuster les grands modèles de langage (LLM) et générer des réponses qui s'adaptent aux vidéos de cours, aux notes et aux livres. Pour un cours de FEM (Méthode des Éléments Finis) de haut niveau, il propose un scénario d'étude qui inclut la construction systématique de données d'entraînement, l'adaptation d'un modèle de LLM ouvert par Adaptation de Bas Rang (LoRA), et l'optimisation des réponses par RAG. La combinaison de la similarité cosinus, des évaluations basées sur des LLM et des révisions par des experts démontre une forte adéquation des contenus du cours. De plus, un prototype d'application web \"https://my-ai-university.com\" a été développé pour relier des sections spécifiques de contenu du cours et des vidéos d'accès ouverts avec des étiquettes de temps, renforçant l'approche basée sur le suivi. Le modèle d'expert dépasse la similarité cosinus avec le référent dans 86% des cas de test et l'évaluateur de LLM a montré un rendement supérieur au modèle de base Llama 3.2 dans presque 4 sur 5 tests. AI-U offre une approche scalable pour l'éducation de la discrimination de l'IA, promouvant son introduction dans l'enseignement supérieur. Voici le cadre pour un cours de FEM, mais c'est un exemple spécifique dans un contexte plus large de l'adaptation de LLM pour des contenus de recherche scientifique.",
      "upvotes": 2,
      "discussionId": "67ff25f43026f8abc4c4a16c",
      "projectPage": "https://my-ai-university.com",
      "githubRepo": "https://github.com/my-ai-university/finite-element-method",
      "ai_keywords": [
        "large language model (LLM)",
        "retrieval-augmented generation (RAG)",
        "finite-element-method (FEM)",
        "Low-Rank Adaptation (LoRA)",
        "RAG-based synthesis",
        "cosine similarity",
        "LLM-based assessment",
        "traceability",
        "base Llama 3.2 model"
      ]
    },
    "publishedAt": "2025-04-10T21:26:34.000Z",
    "title": "AI-University: An LLM-based platform for instructional alignment to\n  scientific classrooms",
    "summary": "We introduce AI University (AI-U), a flexible framework for AI-driven course\ncontent delivery that adapts to instructors' teaching styles. At its core, AI-U\nfine-tunes a large language model (LLM) with retrieval-augmented generation\n(RAG) to generate instructor-aligned responses from lecture videos, notes, and\ntextbooks. Using a graduate-level finite-element-method (FEM) course as a case\nstudy, we present a scalable pipeline to systematically construct training\ndata, fine-tune an open-source LLM with Low-Rank Adaptation (LoRA), and\noptimize its responses through RAG-based synthesis. Our evaluation - combining\ncosine similarity, LLM-based assessment, and expert review - demonstrates\nstrong alignment with course materials. We also have developed a prototype web\napplication, available at https://my-ai-university.com, that enhances\ntraceability by linking AI-generated responses to specific sections of the\nrelevant course material and time-stamped instances of the open-access video\nlectures. Our expert model is found to have greater cosine similarity with a\nreference on 86% of test cases. An LLM judge also found our expert model to\noutperform the base Llama 3.2 model approximately four times out of five. AI-U\noffers a scalable approach to AI-assisted education, paving the way for broader\nadoption in higher education. Here, our framework has been presented in the\nsetting of a class on FEM - a subject that is central to training PhD and\nMaster students in engineering science. However, this setting is a particular\ninstance of a broader context: fine-tuning LLMs to research content in science.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08846.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6729342804227d5ea3b283c1",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/_GPZiRH3LA39QleNtRqYH.png",
      "fullname": "Simone Cimolato",
      "name": "simocimolato",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.09454",
      "authors": [
        {
          "_id": "67ff6f8c661b74d0050d2774",
          "name": "Weinan Jia",
          "hidden": false
        },
        {
          "_id": "67ff6f8c661b74d0050d2775",
          "name": "Mengqi Huang",
          "hidden": false
        },
        {
          "_id": "67ff6f8c661b74d0050d2776",
          "name": "Nan Chen",
          "hidden": false
        },
        {
          "_id": "67ff6f8c661b74d0050d2777",
          "name": "Lei Zhang",
          "hidden": false
        },
        {
          "_id": "67ff6f8c661b74d0050d2778",
          "name": "Zhendong Mao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-13T06:33:28.000Z",
      "submittedOnDailyAt": "2025-04-16T07:21:45.180Z",
      "title": "D^2iT: Bibliothèque de Distribution Dynamique de Canaux Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Suivis Su",
      "submittedOnDailyBy": {
        "_id": "630636bcd37ce67e0e4d1d42",
        "avatarUrl": "/avatars/c3ba695d339eaaf9e810bfa0d9a7689a.svg",
        "isPro": false,
        "fullname": "Mengqi Huang",
        "user": "CoreloneH",
        "type": "user"
      },
      "summary": "Le modèle DiPiFusion est reconnu pour sa capacité à générer des images de haute qualité. Basé sur le rendement excellent et la scalabilité de l'architecture DiT Transformer, bien que dans le processus de DiPiFusion une compression fixe soit appliquée dans différentes zones de l'image, ignorant ainsi la densité d'information naturelle et variée qui peut exister dans ces zones. Cependant, une compression très élevée limite la qualité locale des images, tandis qu'une compression très faible augmente la complexité du calcul, détruit la cohérence globale et affecte la qualité des images finales. Pour résoudre ces limitations, nous proposons un nouveau cadre de travail à deux étapes qui reconnaissent l'importance de différentes zones et compressent de manière dynamique pour améliorer l'efficience et l'efficacité de la génération d'images. Tout d'abord, le VAE Dynamique (DVAE) utilise des taux de sampling différents pour encoder différentes zones de l'image, générant des codes potentiels plus précis et naturels. Ensuite, le DiT Transformer Dynamique (D^2iT) combine un nouvel approche avec le Transformer dynamique et le Transformer de contenu dynamique pour prédire une mélange de bruits qui incluent les \"core-els\" (zones lisses avec peu de codes potentiels) et les \"finit-els\" (zones détaillées avec beaucoup de codes potentiels), générant ainsi des images. La combinaison du bruit prédit de grande taille et la définition des zones détaillées permet d'atteindre une intégration de la cohérence globale et de la qualité locale des images. Par des expériences détaillées sur différentes tâches de génération, nous avons vérifié l'efficacité de notre approche. Le code est disponible sur https://github.com/jiawn-creator/Dynamic-DiT.",
      "upvotes": 1,
      "discussionId": "67ff6f8f661b74d0050d28a9",
      "ai_keywords": [
        "Diffusion models",
        "Diffusion Transformer (DiT)",
        "compression",
        "image regions",
        "information densities",
        "local realism",
        "computational complexity",
        "global consistency",
        "generated images",
        "Dynamic VAE (DVAE)",
        "hierarchical encoder",
        "downsampling rates",
        "latent codes",
        "Dynamic Diffusion Transformer (D$^2$iT)",
        "multi-grained noise",
        "coarse-grained",
        "fine-grained",
        "Dynamic Grain Transformer",
        "Dynamic Content Transformer",
        "rough prediction",
        "detailed regions correction"
      ]
    },
    "publishedAt": "2025-04-13T02:33:28.000Z",
    "title": "D^2iT: Dynamic Diffusion Transformer for Accurate Image Generation",
    "summary": "Diffusion models are widely recognized for their ability to generate\nhigh-fidelity images. Despite the excellent performance and scalability of the\nDiffusion Transformer (DiT) architecture, it applies fixed compression across\ndifferent image regions during the diffusion process, disregarding the\nnaturally varying information densities present in these regions. However,\nlarge compression leads to limited local realism, while small compression\nincreases computational complexity and compromises global consistency,\nultimately impacting the quality of generated images. To address these\nlimitations, we propose dynamically compressing different image regions by\nrecognizing the importance of different regions, and introduce a novel\ntwo-stage framework designed to enhance the effectiveness and efficiency of\nimage generation: (1) Dynamic VAE (DVAE) at first stage employs a hierarchical\nencoder to encode different image regions at different downsampling rates,\ntailored to their specific information densities, thereby providing more\naccurate and natural latent codes for the diffusion process. (2) Dynamic\nDiffusion Transformer (D^2iT) at second stage generates images by predicting\nmulti-grained noise, consisting of coarse-grained (less latent code in smooth\nregions) and fine-grained (more latent codes in detailed regions), through an\nnovel combination of the Dynamic Grain Transformer and the Dynamic Content\nTransformer. The strategy of combining rough prediction of noise with detailed\nregions correction achieves a unification of global consistency and local\nrealism. Comprehensive experiments on various generation tasks validate the\neffectiveness of our approach. Code will be released at\nhttps://github.com/jiawn-creator/Dynamic-DiT.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.09454.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "630636bcd37ce67e0e4d1d42",
      "avatarUrl": "/avatars/c3ba695d339eaaf9e810bfa0d9a7689a.svg",
      "fullname": "Mengqi Huang",
      "name": "CoreloneH",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  }
]