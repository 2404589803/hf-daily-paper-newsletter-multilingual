[
  {
    "paper": {
      "id": "2502.12152",
      "authors": [
        {
          "_id": "67b41ed52867282b4eb37ce4",
          "name": "Xialin He",
          "hidden": false
        },
        {
          "_id": "67b41ed52867282b4eb37ce5",
          "user": {
            "_id": "6201fc5d91d53938a6432fbf",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6201fc5d91d53938a6432fbf/VLs8ZYaZrop4KBpZn53fH.jpeg",
            "isPro": false,
            "fullname": "Runpei Dong",
            "user": "RunpeiDong",
            "type": "user"
          },
          "name": "Runpei Dong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-18T09:31:13.178Z",
          "hidden": false
        },
        {
          "_id": "67b41ed52867282b4eb37ce6",
          "name": "Zixuan Chen",
          "hidden": false
        },
        {
          "_id": "67b41ed52867282b4eb37ce7",
          "name": "Saurabh Gupta",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-17T18:59:06.000Z",
      "title": "Apprentissage de Politiques pour le Déplacement dans les Robots Humanoïdes de la Réalité",
      "summary": "Automatiquement, le polikarya est une condition importante à considérer pour les robots de petits corps de confiance. En raison des multiples structures et des difficultés de fonctionnement sur diverses surfaces, la conception d'un contrôleur manuel est complexe. Dans cet article, un cadre d'apprentissage est développé pour créer un contrôleur permettant au robot de petits corps de se déplacer doucement sur différentes structures et surfaces. Une différence avec les précédents essais de mouvement du robot de petits corps est que les mouvements doux incluent des patrons de contact complexes, ce qui nécessite une modélisation géométrique des collisions et une description de récompenses rares. Pour aborder ces défis, un approche à deux étapes est employée. La première étape se concentre sur la recherche d'une trajectoire douce et efficace sous contraintes de stabilité et de vitesse/charge, tandis que la deuxième étape améliore l'action trouvée pour la rendre réellement utilisable (c'est-à-dire, douce et progressive) et plus résistante aux changements initiaux de la structure et de la surface. Nous montrons que cette innovation permet aux robots de petits corps de type G1 de réaliser deux états doux dans le monde réel : a) l'état doux sur la face et b) l'état doux en dessous de la face, les deux plans, flexibles et testés sur des surfaces douces et inclinées (par exemple, des surfaces glissantes et des champs de vision). Cela représente le premier expérimentation réussie d'une politique pour que un robot de petits corps de taille humaine se déplace doucement dans le monde réel. Page du projet : https://humanoid-getup.github.io/",
      "upvotes": 23,
      "discussionId": "67b41edb2867282b4eb37ddf"
    },
    "publishedAt": "2025-02-18T00:49:53.124Z",
    "title": "Learning Getting-Up Policies for Real-World Humanoid Robots",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6201fc5d91d53938a6432fbf/x35BuXOhc6ubukxLfiVzt.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.12152.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6201fc5d91d53938a6432fbf",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6201fc5d91d53938a6432fbf/VLs8ZYaZrop4KBpZn53fH.jpeg",
      "fullname": "Runpei Dong",
      "name": "RunpeiDong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.11190",
      "authors": [
        {
          "_id": "67b420dfb2528c023491f455",
          "name": "Haoming Xu",
          "hidden": false
        },
        {
          "_id": "67b420dfb2528c023491f456",
          "name": "Ningyuan Zhao",
          "hidden": false
        },
        {
          "_id": "67b420dfb2528c023491f457",
          "name": "Liming Yang",
          "hidden": false
        },
        {
          "_id": "67b420dfb2528c023491f458",
          "name": "Sendong Zhao",
          "hidden": false
        },
        {
          "_id": "67b420dfb2528c023491f459",
          "name": "Shumin Deng",
          "hidden": false
        },
        {
          "_id": "67b420dfb2528c023491f45a",
          "name": "Mengru Wang",
          "hidden": false
        },
        {
          "_id": "67b420dfb2528c023491f45b",
          "name": "Bryan Hooi",
          "hidden": false
        },
        {
          "_id": "67b420dfb2528c023491f45c",
          "name": "Nay Oo",
          "hidden": false
        },
        {
          "_id": "67b420dfb2528c023491f45d",
          "name": "Huajun Chen",
          "hidden": false
        },
        {
          "_id": "67b420dfb2528c023491f45e",
          "user": {
            "_id": "620b3bbb0668e435407c8d0a",
            "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
            "isPro": false,
            "fullname": "Ningyu Zhang",
            "user": "Ningyu",
            "type": "user"
          },
          "name": "Ningyu Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-18T09:31:11.243Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-16T16:31:00.000Z",
      "title": "ReLearn : Apprentissage par oubli à travers des modèles de langage grands.",
      "summary": "Les méthodes d'oubli dans les modèles de langage grands actuels sont réalisées par l'optimisation de la rétro-replication pour réduire la probabilité des tokens cibles. Cependant, ce paradigme détruit la prédiction des tokens suivants et diminue le rendement et la cohérence grammaticale du modèle. De plus, les indicateurs d'évaluation actuels exigent un oubli contextuel excessif et évaluent peu le flux et la relation de la réponse. Pour résoudre ces problèmes, nous proposons ReLearn. ReLearn inclut une pipeline d'expansion de données et d'ajustement fin pour réaliser un oubli efficace, ainsi qu'un cadre d'évaluation détaillé. Ce cadre introduit la Taux de Retenue du Savoir (KFR) et le Ratio de Retenue du Savoir (KRR) pour évaluer la conservation du savoir, et la Note de Langue (LS) pour évaluer la qualité de la génération. Nos expérimentations montrent que ReLearn réussit à oublier les objectifs tout en maintenant une haute qualité dans les résultats. Une analyse structurale renforce que ReLearn maintient ces capacités fondamentales, détruisant ainsi la cohérence contextuelle que l'optimisation de la rétro-replication détruit. Le code est disponible sur https://github.com/zjunlp/unlearn.",
      "upvotes": 11,
      "discussionId": "67b420e2b2528c023491f506"
    },
    "publishedAt": "2025-02-18T00:58:24.094Z",
    "title": "ReLearn: Unlearning via Learning for Large Language Models",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/620b3bbb0668e435407c8d0a/A4YB7t6hDVty6QrvLN0a7.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.11190.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "620b3bbb0668e435407c8d0a",
      "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
      "fullname": "Ningyu Zhang",
      "name": "Ningyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 17
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.12115",
      "authors": [
        {
          "_id": "67b41a72a38d04cc6148d80e",
          "name": "Samuel Miserendino",
          "hidden": false
        },
        {
          "_id": "67b41a72a38d04cc6148d80f",
          "name": "Michele Wang",
          "hidden": false
        },
        {
          "_id": "67b41a72a38d04cc6148d810",
          "name": "Tejal Patwardhan",
          "hidden": false
        },
        {
          "_id": "67b41a72a38d04cc6148d811",
          "name": "Johannes Heidecke",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-17T18:41:16.000Z",
      "title": "SWE-Lancer : Les FRONTEND LLMs pourront-ils gagner un million de dollars en tant que professionnels de logiciels indépendants ?",
      "summary": "Nous introduisons SWE-Lancer. Cette norme comprend plus de 1,400 tâches gratuites d'ingénierie logicielle disponibles sur Upwork. La récompense totale réelle de cette collection de tâches est de un million de dollars. SWE-Lancer est composé de tâches d'ingénierie indépendante et de tâches de gestion. Les tâches indépendantes comprennent un large éventail d'activités, allant du correction de 50 erreurs à l'implémentation de fonctions avec un montant de 32,000 dollars. Les tâches de gestion se concentrent sur la sélection entre propositions techniques d'implémentation. Les tâches indépendantes sont évaluées par des ingénieurs expérimentés en logiciel qui ont passé par trois tests de testing end-to-end. Les décisions de gestion sont évaluées en les comparant aux choix des gestionnaires d'ingénierie employés originalement. Le rendement du modèle a été évalué et il a été découvert que le modèle frontier ne peut pas résoudre la majorité des tâches. Pour les futures recherches, nous publions en code open-source l'image Docker intégrée et la division de l'évaluation publique de SWE-Lancer diamant (https://github.com/openai/SWELancer-Benchmark). Nous cartographions le rendement du modèle à la valuation économique et soutenons l'expansion de la recherche sur l'impact économique du développement de modèles d'IA avec SWE-Lancer.",
      "upvotes": 9,
      "discussionId": "67b41a74a38d04cc6148d84b"
    },
    "publishedAt": "2025-02-18T00:28:31.293Z",
    "title": "SWE-Lancer: Can Frontier LLMs Earn $1 Million from Real-World Freelance Software Engineering?",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.12115.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6128
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.12148",
      "authors": [
        {
          "_id": "67b40c8cdb88dfd19ab917f3",
          "name": "Ling Yang",
          "hidden": false
        },
        {
          "_id": "67b40c8cdb88dfd19ab917f4",
          "user": {
            "_id": "653e5d31ffd60206c8b64bb5",
            "avatarUrl": "/avatars/5076795722ec1f9e031654f301d30e8f.svg",
            "isPro": false,
            "fullname": "Xinchen Zhang",
            "user": "comin",
            "type": "user"
          },
          "name": "Xinchen Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-18T09:31:31.841Z",
          "hidden": false
        },
        {
          "_id": "67b40c8cdb88dfd19ab917f5",
          "name": "Ye Tian",
          "hidden": false
        },
        {
          "_id": "67b40c8cdb88dfd19ab917f6",
          "name": "Chenming Shang",
          "hidden": false
        },
        {
          "_id": "67b40c8cdb88dfd19ab917f7",
          "name": "Minghao Xu",
          "hidden": false
        },
        {
          "_id": "67b40c8cdb88dfd19ab917f8",
          "name": "Wentao Zhang",
          "hidden": false
        },
        {
          "_id": "67b40c8cdb88dfd19ab917f9",
          "name": "Bin Cui",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-17T18:57:51.000Z",
      "title": "\"Herimas Flow : Le méthode d'atteindre la distance infinie entre compréhension et génération\"",
      "summary": "Le grand succès des modèles de grammaire a conduit à un développement clair des modèles de langage multimodal de haut niveau (MLLMs). Des modèles comme Show-o, Transfusion, Emu3 ont réalisé un développement impressionnant, attirant une attention particulière sur la compréhension et la génération d'images. Tout d'abord, il a été découvert que la capacité de compréhension des MLLMs est généralement beaucoup plus forte que sa capacité de génération. D'une telle perspective, on propose un simple et général cadre appelé HermesFlow. Ce cadre vise à faciliter la connexion entre la compréhension et la génération. En particulier, en utilisant le même ensemble de données, on extrait des informations avec la même inclinaison tant pour la compréhension que pour la génération, et on ajuste la compréhension et la génération multimodal appropriéement en utilisant Pair-DPO et des jeux d'auto-entraînement avec des données de la même inclinaison. Les expériences étendues montrent clairement l'avantage de notre approche par rapport aux méthodes existantes, en particulier dans la réduction du gap entre la compréhension et la génération multimodal. Ces résultats révèlent la possibilité que HermesFlow puisse être un cadre général pour l'ajustement des prochains modèles fondamentaux de langage multimodal. Code : https://github.com/Gen-Verse/HermesFlow",
      "upvotes": 9,
      "discussionId": "67b40c8edb88dfd19ab9183f"
    },
    "publishedAt": "2025-02-17T23:29:29.396Z",
    "title": "HermesFlow: Seamlessly Closing the Gap in Multimodal Understanding and Generation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.12148.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "653e5d31ffd60206c8b64bb5",
      "avatarUrl": "/avatars/5076795722ec1f9e031654f301d30e8f.svg",
      "fullname": "Xinchen Zhang",
      "name": "comin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 13
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.11167",
      "authors": [
        {
          "_id": "67b4221bbc387d2eda6f8637",
          "user": {
            "_id": "650267e7e751d03da933a24a",
            "avatarUrl": "/avatars/f047a047d1de304cd97027463541bdf3.svg",
            "isPro": false,
            "fullname": "Bohan22",
            "user": "Bohan22",
            "type": "user"
          },
          "name": "Bohan Lyu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-18T09:31:06.388Z",
          "hidden": false
        },
        {
          "_id": "67b4221bbc387d2eda6f8638",
          "name": "Siqiao Huang",
          "hidden": false
        },
        {
          "_id": "67b4221bbc387d2eda6f8639",
          "user": {
            "_id": "67286718746a95c09d04cb1d",
            "avatarUrl": "/avatars/317efa8459cca08c2ff56c3ab116e15c.svg",
            "isPro": false,
            "fullname": "Zichen Liang",
            "user": "zcliang22",
            "type": "user"
          },
          "name": "Zichen Liang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-18T09:31:08.469Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-16T15:38:19.000Z",
      "title": "SURGE : La possibilité de faire fonctionner les fonctions d'exécution de code générales des modèles de langage à grande échelle",
      "summary": "Les grands modèles de langue (LLMs) montrent une excellente capacité dans des tâches liées au code, comme la compréhension et la génération de code. Cependant, une recherche systématique sur leur potentiel en tant qu'aidants généraux pour l'exécution de code a été insuffisante. Pour aborder cette question, nous présentons SURGE, un cadre de référence détaillé. SURGE est constitué de 8 points clés : tâches de programmation multilingues, problèmes de programmation de niveau de concours, analyse de codes au niveau de la base de données, calcul scientifique de haut coût, algorithmes de grande complexité en temps, analyse de codes avec des erreurs, programmes dépendant d'un compilateur ou d'un environnement spécifique d'exécution, et la vérification de démonstrations mathématiques formelles. Les modèles de langage de code ouverts et propriétaires sont évalués dans SURGE, et des études de scalabilité sont menées en analysant l'impact du taille du modèle et du jeu de données d'entraînement. De plus, les erreurs de prédiction du modèle sont classifiées et des domaines potentiellement améliorables sont explorés. Nos résultats montrent que les LLMs peuvent prédire des résultats d'exécution de code dans certaines situations. Cependant, des lacunes dans leur fonctionnalité en tant qu'exécutants généraux de code ont été identifiées. Cette recherche offre une perspective empirique sur la possibilité de remplacer les exécutants de code par les LLMs. Les codes et datasets sont disponibles sur https://github.com/Imbernoulli/SURGE.",
      "upvotes": 7,
      "discussionId": "67b4221ebc387d2eda6f8717"
    },
    "publishedAt": "2025-02-18T01:01:24.331Z",
    "title": "SURGE: On the Potential of Large Language Models as General-Purpose Surrogate Code Executors",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.11167.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "650267e7e751d03da933a24a",
      "avatarUrl": "/avatars/f047a047d1de304cd97027463541bdf3.svg",
      "fullname": "Bohan22",
      "name": "Bohan22",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.12146",
      "authors": [
        {
          "_id": "67b40ce4d3c5f50aa9b71df5",
          "name": "Ye Tian",
          "hidden": false
        },
        {
          "_id": "67b40ce4d3c5f50aa9b71df6",
          "name": "Ling Yang",
          "hidden": false
        },
        {
          "_id": "67b40ce4d3c5f50aa9b71df7",
          "user": {
            "_id": "653e5d31ffd60206c8b64bb5",
            "avatarUrl": "/avatars/5076795722ec1f9e031654f301d30e8f.svg",
            "isPro": false,
            "fullname": "Xinchen Zhang",
            "user": "comin",
            "type": "user"
          },
          "name": "Xinchen Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-18T09:31:29.697Z",
          "hidden": false
        },
        {
          "_id": "67b40ce4d3c5f50aa9b71df8",
          "name": "Yunhai Tong",
          "hidden": false
        },
        {
          "_id": "67b40ce4d3c5f50aa9b71df9",
          "name": "Mengdi Wang",
          "hidden": false
        },
        {
          "_id": "67b40ce4d3c5f50aa9b71dfa",
          "name": "Bin Cui",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-17T18:57:26.000Z",
      "title": "Diffusion Sharpening : Réglage du bruit dans la diffusion des modèles de diffusion",
      "summary": "Je propose la technique de Diffusion-Sharpening. Cette approche vise à optimiser le projet de l'échantillonnage et à améliorer la disposition en bas. Les méthodes actuelles de réglage micro basées sur l'apprentissage par renforcement se concentrent sur un seul pas d'entraînement, surpassant la disposition au niveau du projet, mais les derniers méthodes d'optimisation de projets d'échantillonnage souffrent d'un coût d'inférence NFE excessif. La Diffusion-Sharpening utilise un cadre de mise en œuvre pas à pas pour choisir le meilleur projet pendant l'entraînement et utilise la rétroaction de récompenses pour surmonter les coûts d'inférence. Notre méthode montre une efficacité d'inférence optimale sans nécessité d'ajouts supplémentaires de NFE, démontrant une efficacité d'apprentissage et un rapide convergence. Les expériences prolongées montrent des résultats supérieurs aux méthodes de réglage micro basées sur l'apprentissage par renforcement (par exemple, Diffusion-DPO) et aux méthodes d'optimisation de projets d'échantillonnage (par exemple, amplification de l'inférence) à travers des métriques variées, comme le démarrage de texte, la capacité de configuration et les préférences humaines. Code : https://github.com/Gen-Verse/Diffusion-Sharpening",
      "upvotes": 6,
      "discussionId": "67b40ce8d3c5f50aa9b71f9a"
    },
    "publishedAt": "2025-02-17T23:30:53.097Z",
    "title": "Diffusion-Sharpening: Fine-tuning Diffusion Models with Denoising Trajectory Sharpening",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.12146.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "653e5d31ffd60206c8b64bb5",
      "avatarUrl": "/avatars/5076795722ec1f9e031654f301d30e8f.svg",
      "fullname": "Xinchen Zhang",
      "name": "comin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 13
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.10458",
      "authors": [
        {
          "_id": "67b3ea0f4dd7ea0538ce589d",
          "user": {
            "_id": "6354bda206d707b33249c4c2",
            "avatarUrl": "/avatars/bbd9f76274ac52214df92084d50bc7b5.svg",
            "isPro": false,
            "fullname": "Zhenxing Mi",
            "user": "Mifucius",
            "type": "user"
          },
          "name": "Zhenxing Mi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-18T09:31:52.837Z",
          "hidden": false
        },
        {
          "_id": "67b3ea0f4dd7ea0538ce589e",
          "name": "Kuan-Chieh Wang",
          "hidden": false
        },
        {
          "_id": "67b3ea0f4dd7ea0538ce589f",
          "name": "Guocheng Qian",
          "hidden": false
        },
        {
          "_id": "67b3ea0f4dd7ea0538ce58a0",
          "name": "Hanrong Ye",
          "hidden": false
        },
        {
          "_id": "67b3ea0f4dd7ea0538ce58a1",
          "name": "Runtao Liu",
          "hidden": false
        },
        {
          "_id": "67b3ea0f4dd7ea0538ce58a2",
          "name": "Sergey Tulyakov",
          "hidden": false
        },
        {
          "_id": "67b3ea0f4dd7ea0538ce58a3",
          "name": "Kfir Aberman",
          "hidden": false
        },
        {
          "_id": "67b3ea0f4dd7ea0538ce58a4",
          "name": "Dan Xu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-12T05:30:08.000Z",
      "title": "「Récolle et diffuse ça : théorie des raisons pour des cas individuels dans les modèles de diffusion」",
      "summary": "Dans cet article, un nouveau paradigme d'alignement appelé ThinkDiff est présenté. Ce paradigme intègre les forces des modèles de langage visuel (VLMs) pour fournir aux modèles de diffusion d'images à partir de texte une variété d'entendements de contexte et de capacités d'inférence. Les méthodes actuelles d'ajustement pour les différents types de diffusion se concentrent principalement sur la reconstruction de pixels, mais pas sur l'inférence de contexte, ce qui est limité par la complexité et la restriction des ensembles de données d'inférence. ThinkDiff aborde ces défis en utilisant l'entraînement visuel comme tâche fausse et en alignant les décodificateurs de VLMs et de grands modèles de langage (LLM). Cette tâche fausse permet au décodificateur de LLM d'observer que l'espace de caractéristiques d'entrée est similaire à l'espace de codification de LLM, ce qui facilite l'alignement des VLMs et des décodificateurs de diffusion. Cela nécessite moins d'entraînement complexe ou d'ensembles de données, permettant que ThinkDiff développe efficacement des habiletés de compréhension, d'inférence et de constitution. Les expériences dans le cadre de CoBSAT ont amélioré significativement la précision, de 19,2% à 46,3%, et il a été possible d'entraîner sur 4 GPUs A100 pendant 5 heures. De plus, ThinkDiff montre des résultats exceptionnels dans la configuration d'images et de textes de manière logiquement cohérente. Page du projet : https://mizhenxing.github.io/ThinkDiff.",
      "upvotes": 5,
      "discussionId": "67b3ea124dd7ea0538ce592d"
    },
    "publishedAt": "2025-02-18T04:33:41.120Z",
    "title": "I Think, Therefore I Diffuse: Enabling Multimodal In-Context Reasoning in Diffusion Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.10458.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6354bda206d707b33249c4c2",
      "avatarUrl": "/avatars/bbd9f76274ac52214df92084d50bc7b5.svg",
      "fullname": "Zhenxing Mi",
      "name": "Mifucius",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.11196",
      "authors": [
        {
          "_id": "67b42223c2fe54b8d43efed6",
          "name": "Yixin Ou",
          "hidden": false
        },
        {
          "_id": "67b42223c2fe54b8d43efed7",
          "name": "Yunzhi Yao",
          "hidden": false
        },
        {
          "_id": "67b42223c2fe54b8d43efed8",
          "user": {
            "_id": "620b3bbb0668e435407c8d0a",
            "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
            "isPro": false,
            "fullname": "Ningyu Zhang",
            "user": "Ningyu",
            "type": "user"
          },
          "name": "Ningyu Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-18T09:31:04.227Z",
          "hidden": false
        },
        {
          "_id": "67b42223c2fe54b8d43efed9",
          "name": "Hui Jin",
          "hidden": false
        },
        {
          "_id": "67b42223c2fe54b8d43efeda",
          "name": "Jiacheng Sun",
          "hidden": false
        },
        {
          "_id": "67b42223c2fe54b8d43efedb",
          "name": "Shumin Deng",
          "hidden": false
        },
        {
          "_id": "67b42223c2fe54b8d43efedc",
          "name": "Zhenguo Li",
          "hidden": false
        },
        {
          "_id": "67b42223c2fe54b8d43efedd",
          "name": "Huajun Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-16T16:55:43.000Z",
      "title": "LLM se demande comment obtient-il de nouveaux savoirs ? À travers la perspective du cycle de connaissance de l'apprentissage continu prédictif.",
      "summary": "Les grands modèles de langue (MLLs) ont montré une pertinence importante dans la capacité de prendre en compte de nouveaux savoirs pour des tâches de connaissances largement denses. En particulier, il est difficile de comprendre comment les connaissances entraînées sont structuralement intégrées dans la computation neuronale. Nous avons abordé ce problème dans l'évolution des circuits de connaissance, identifiant des sous-graphes computationnels qui favorisent l'accueil et le traitement de connaissances. A travers un analyse systématique de l'évolution des circuits, nous avons obtenu les résultats clés suivants : 1) l'acquisition de nouveaux savoirs est influencée par la connexion avec les connaissances existantes ; 2) l'évolution des circuits de connaissance montre différentes étapes de formation à l'optimisation ; 3) l'évolution des circuits de connaissance suive des motifs profonds. Ces points fournissent une compréhension théorique de la structure de l'acquisition de nouveaux savoirs dans les MLLs et ont le potentiel d'améliorer la stratégie d'entraînement prédictif continu, ce qui peut améliorer le rendement du modèle. Les codes et les données sont disponibles sur https://github.com/zjunlp/DynamicKnowledgeCircuits.",
      "upvotes": 5,
      "discussionId": "67b42225c2fe54b8d43eff9b"
    },
    "publishedAt": "2025-02-18T01:02:25.236Z",
    "title": "How Do LLMs Acquire New Knowledge? A Knowledge Circuits Perspective on Continual Pre-Training",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/620b3bbb0668e435407c8d0a/_LGnwvwslWc3YDIirfOKS.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.11196.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "620b3bbb0668e435407c8d0a",
      "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
      "fullname": "Ningyu Zhang",
      "name": "Ningyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 17
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.11438",
      "authors": [
        {
          "_id": "67b406993d0f54ab381594f5",
          "name": "Jimin Lee",
          "hidden": false
        },
        {
          "_id": "67b406993d0f54ab381594f6",
          "name": "Ingeol Baek",
          "hidden": false
        },
        {
          "_id": "67b406993d0f54ab381594f7",
          "name": "Byeongjeong Kim",
          "hidden": false
        },
        {
          "_id": "67b406993d0f54ab381594f8",
          "name": "Hwanhee Lee",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-17T04:52:24.000Z",
      "title": "SAFE-SQL : Apprentissage par rétro-correction à l'intérieur du format de texte grâce à l'automatisation du sampling de pixels à partir du texte.",
      "summary": "Text-to-SQL vise à convertir des questions en langage nature en requêtes SQL exécutables. Dans les méthodes précédentes, des techniques comme la sélection d'exemples similaires et la structure de masque ont été utilisées pour rechercher des exemples similaires et guider un grand modèle de langage (LLM) pour montrer un rendement fort. Cependant, dans des scénarios réels et très populaires, des difficultés se posent lorsque ces exemples n'existent pas. Pour surmonter ces limites, nous proposons un nouveau cadre de travail \"Self-Augmentation in-context learning with Fine-grained Example selection for Text-to-SQL (SAFE-SQL)\" pour améliorer la génération de SQL dans le cadre Text-to-SQL. SAFE-SQL génère et filtre automatiquement des exemples pour construire des exemples d'apprentissage en contexte de haute qualité. En utilisant ces exemples, SAFE-SQL dépasse les cadres de Text-to-SQL précédents de 0-shot et few-shot et améliore la précision d'exécution. En particulier, notre approche fournit une amélioration supplémentaire dans les scénarios où le simple méthode échoue avec une forte probabilité et les exemples sont presque impossibles à trouver.",
      "upvotes": 5,
      "discussionId": "67b4069a3d0f54ab38159520"
    },
    "publishedAt": "2025-02-17T23:06:03.562Z",
    "title": "SAFE-SQL: Self-Augmented In-Context Learning with Fine-grained Example Selection for Text-to-SQL",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.11438.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63f6f245e94ed998c46316df",
      "avatarUrl": "/avatars/9c0ec8682d4a85b96d2180602b1bbe6c.svg",
      "fullname": "ingeolbaek",
      "name": "ingeol",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.09061",
      "authors": [
        {
          "_id": "67b401de3995f28d45c212d6",
          "name": "Debangshu Banerjee",
          "hidden": false
        },
        {
          "_id": "67b401de3995f28d45c212d7",
          "name": "Tarun Suresh",
          "hidden": false
        },
        {
          "_id": "67b401de3995f28d45c212d8",
          "name": "Shubham Ugare",
          "hidden": false
        },
        {
          "_id": "67b401de3995f28d45c212d9",
          "name": "Sasa Misailovic",
          "hidden": false
        },
        {
          "_id": "67b401de3995f28d45c212da",
          "name": "Gagandeep Singh",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-13T08:23:42.000Z",
      "title": "CRANE : Inférence basée sur la génération de LMs avec contraintes",
      "summary": "LLM doit générer des sorties qui maintiennent la syntaxe et la précision sémantique dans des tâches telles que la génération de code, la logique mathématique symbolique et d'autres. Forcer la génération d'un LLM à suivre des langages formels peut réduire sa capacité de pensée logique, car cela est un phénomène courant dans les expériences. Dans cette étude, on explique d'abord théoriquement pourquoi limiter la sortie d'un LLM à un langage très restreint peut diminuer sa capacité de pensée logique. Ensuite, on propose un approche qui garantit la syntaxe et la précision sémantique tout en maintenant la capacité de pensée logique. Sur la base de ces théories, on propose un algorithme de décodage restreint qui renforce le pensée logique, nommé CRANE. CRANE équilibre la précision de la génération restreinte avec la flexibilité de la génération non restreinte, et améliore de 10% la précision sur les benchmarks difficiles de logique symbolique tels que GSM-symbolic et FOLIO dans divers codes ouverts et benchmarks.",
      "upvotes": 4,
      "discussionId": "67b401e03995f28d45c21354"
    },
    "publishedAt": "2025-02-17T22:43:51.555Z",
    "title": "CRANE: Reasoning with constrained LLM generation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.09061.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65e7bb35e5e78134ab049942",
      "avatarUrl": "/avatars/3c0972f0d59e51ebb5c218ee736d4458.svg",
      "fullname": "Tarun Suresh",
      "name": "tarsur909",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.11275",
      "authors": [
        {
          "_id": "67b3fa2862838a378b21860d",
          "name": "Letian Peng",
          "hidden": false
        },
        {
          "_id": "67b3fa2862838a378b21860e",
          "name": "Zilong Wang",
          "hidden": false
        },
        {
          "_id": "67b3fa2862838a378b21860f",
          "name": "Feng Yao",
          "hidden": false
        },
        {
          "_id": "67b3fa2862838a378b218610",
          "name": "Jingbo Shang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-16T21:32:20.000Z",
      "title": "Cooker : L'ancienn'habitat des grandes réseaux de calcul qui a donné naissance aux libres de IE LLM",
      "summary": "Les données de haute qualité et de forte demande, c'est-à-dire les textes de modèle pour l'entraînement préalable et les annotations après l'entraînement, sont préparées avec soin et favorisent le développement de grands modèles de langage (LLMs). De plus, dans l'extraction d'information (IE), les séquences avec des étiquettes BIO comme celles des données d'entraînement précédentes sont difficiles à étendre. Nous montrons comment les modèles d'IE peuvent agir libres de ressources de données de LLM. Concrètement, nous reconstruisons la prédiction de texte suivante. En particulier, le paradigme d'extraction de texte proposé (NTE) transforme 102,6M de données d'extraction à partir de données d'entraînement précédentes et de données après l'entraînement pour entraîner différents modèles d'IE comme Cuckoo. Dans des scénarios avec peu d'exemples, Cuckoo obtient des résultats meilleurs que les modèles d'IE basés sur des commandes complexes, comparés aux modèles d'IE entraînés précédemment. Naturellement, Cuckoo évolue avec l'amélioration de la préparation des données de LLM, bénéficiant ainsi de l'amélioration de la chaîne d'entraînement de LLM. Aucun effort manuel supplémentaire n'est nécessaire.",
      "upvotes": 4,
      "discussionId": "67b3fa2962838a378b21867b"
    },
    "publishedAt": "2025-02-17T22:10:49.900Z",
    "title": "Cuckoo: An IE Free Rider Hatched by Massive Nutrition in LLM's Nest",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.11275.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64323dd503d81fa4d26deaf9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64323dd503d81fa4d26deaf9/x3ES8VXEZJljxDWvFWaAf.png",
      "fullname": "Letian Peng",
      "name": "KomeijiForce",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.11901",
      "authors": [
        {
          "_id": "67b3f8cc1bfe04e82830b752",
          "name": "Dylan Zhang",
          "hidden": false
        },
        {
          "_id": "67b3f8cc1bfe04e82830b753",
          "name": "Justin Wang",
          "hidden": false
        },
        {
          "_id": "67b3f8cc1bfe04e82830b754",
          "name": "Tianran Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-17T15:24:11.000Z",
      "title": "Le développement d'un programmeur spécialisé dans la mise en œuvre de tests avec le 64% de la position en réponse à la manque de données.",
      "summary": "Les modèles actuels de langage (LM) rencontrent des problèmes en raison de la pénurie de données dans la programmation par tests, ce qui se réduit à deux points principaux.",
      "upvotes": 3,
      "discussionId": "67b3f8cd1bfe04e82830b77f"
    },
    "publishedAt": "2025-02-17T22:05:54.047Z",
    "title": "Building A Proof-Oriented Programmer That Is 64% Better Than GPT-4o Under Data Scarsity",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.11901.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642b8add48f67b6f21d4eb20",
      "avatarUrl": "/avatars/f15025b39248daa19a18e6ccb2eaaa0c.svg",
      "fullname": "Dylan",
      "name": "shizhuo2",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.12054",
      "authors": [
        {
          "_id": "67b44a6888813676da9f8239",
          "name": "Xinyu Zhang",
          "hidden": false
        },
        {
          "_id": "67b44a6888813676da9f823a",
          "name": "Yuxuan Dong",
          "hidden": false
        },
        {
          "_id": "67b44a6888813676da9f823b",
          "name": "Yanrui Wu",
          "hidden": false
        },
        {
          "_id": "67b44a6888813676da9f823c",
          "name": "Jiaxing Huang",
          "hidden": false
        },
        {
          "_id": "67b44a6888813676da9f823d",
          "user": {
            "_id": "6602548a68d519ed324b47c5",
            "avatarUrl": "/avatars/5ab411f87440cc2a98c7a1c6a3ed5548.svg",
            "isPro": false,
            "fullname": "ChengyouJia",
            "user": "ChengyouJia",
            "type": "user"
          },
          "name": "Chengyou Jia",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-18T09:30:47.313Z",
          "hidden": false
        },
        {
          "_id": "67b44a6888813676da9f823e",
          "name": "Basura Fernando",
          "hidden": false
        },
        {
          "_id": "67b44a6888813676da9f823f",
          "name": "Mike Zheng Shou",
          "hidden": false
        },
        {
          "_id": "67b44a6888813676da9f8240",
          "name": "Lingling Zhang",
          "hidden": false
        },
        {
          "_id": "67b44a6888813676da9f8241",
          "name": "Jun Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-17T17:24:14.000Z",
      "title": "Física : Base physique du raisonnement fondamental",
      "summary": "Les modèles de langage de haut niveau montrent des capacités spécialisées en mathématiques ou en inférence logique, mais les évaluations actuelles dépassent la logique physique. La logique physique est un travail complexe qui exige des théorèmes et des restrictions physiques. Nous proposons un cadre d'évaluation appelé \"PhysReason\", qui comprend 1 200 problèmes combinant des problèmes basés sur le savoir (25%) et des problèmes basés sur la logique (75%). Les problèmes basés sur la logique peuvent être classifiés en trois niveaux de difficulté : facile, intermédiaire et difficile. En particulier, les problèmes nécessitent, en moyenne, 8,1 étapes pour leur résolution, tandis que les plus difficiles nécessitent 15,6 étapes, reflétant la complexité de la logique physique. Nous proposons un cadre d'évaluation automatique pour les réponses physiques qui inclut des évaluations détaillées du niveau de réponse et de la configuration. Des modèles exceptionnels comme Deepseek-R1, Gemini-2.0-Flash-Thinking et o3-mini-high atteignent moins de 60% sur l'évaluation du niveau de réponse, et leur performance décroît de problèmes de connaissance (75,11%) à problèmes difficiles (31,95%). À travers l'évaluation du niveau d'étapes, nous avons identifié quatre mots-clés clés : application de théorèmes physiques, compréhension de processus physiques, calculs et analyse de conditions physiques. Ces résultats établissent le rôle de \"PhysReason\" comme un nouveau cadre intégral d'évaluation pour la capacité logique physique. Notre code et nos données sont disponibles sur https://dxzxy12138.github.io/PhysReason.",
      "upvotes": 2,
      "discussionId": "67b44a6988813676da9f82d0"
    },
    "publishedAt": "2025-02-18T03:53:47.570Z",
    "title": "PhysReason: A Comprehensive Benchmark towards Physics-Based Reasoning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.12054.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6602548a68d519ed324b47c5",
      "avatarUrl": "/avatars/5ab411f87440cc2a98c7a1c6a3ed5548.svg",
      "fullname": "ChengyouJia",
      "name": "ChengyouJia",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.11330",
      "authors": [
        {
          "_id": "67b42c5632929e97a92dee90",
          "name": "Minbyul Jeong",
          "hidden": false
        },
        {
          "_id": "67b42c5632929e97a92dee91",
          "name": "Jungho Cho",
          "hidden": false
        },
        {
          "_id": "67b42c5632929e97a92dee92",
          "name": "Minsoo Khang",
          "hidden": false
        },
        {
          "_id": "67b42c5632929e97a92dee93",
          "name": "Dawoon Jung",
          "hidden": false
        },
        {
          "_id": "67b42c5632929e97a92dee94",
          "name": "Teakgyu Hong",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-17T01:05:31.000Z",
      "title": "Utilisation de modèles open-source pour la génération de messages de système basée sur les préférences de l'utilisateur",
      "summary": "Les messages de système jouent un rôle crucial dans l'interaction avec des modèles de langage grand échelle (LLM). Ces messages sont utilisés pour initier des conversations, permettent aux utilisateurs de réaliser des tâches spécifiques, déterminent les actions et incluent des informations de contexte. De plus, ils peuvent spécifier le format de sortie et l'étiquette de communication. Cette diversité est ajoutée parce que les données publiques, quasi inexistantes, sont limitées par des restrictions strictes de licence. Automatiquement étiquetter les messages de système dans les données publiques pour que les utilisateurs les génèrent de manière appropriée est un processus qui nécessite de significatifs ressources. Pour résoudre ces problèmes, notre étude a introduit une pipeline pour générer des messages de système appelée \"SysGen\". Cette pipeline permet de générer des messages de système à partir de jeux de données qui ne comprennent pas de messages de système, et en utilisant des messages de système générés à partir de jeux de données de caractéristiques sous-ensemble, obtenir des réponses plus adéquates des assistants. L'entraînement avec des données de SysGen a considérablement amélioré la concordance entre les réponses du modèle, les messages de système et les instructions de l'utilisateur sur les benchmarks multifacets, et a eu un impact minimal sur d'autres benchmarks non vus (par exemple, Open LLM Leaderboard). Notre analyse qualitative souligne l'importance de différents messages de système pour garantir une meilleure adaptation dans différents contextes.",
      "upvotes": 2,
      "discussionId": "67b42c5732929e97a92deed7"
    },
    "publishedAt": "2025-02-18T01:45:36.359Z",
    "title": "System Message Generation for User Preferences using Open-Source Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.11330.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64587be872b60ae7a3817858",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64587be872b60ae7a3817858/BbdOOxOCEzWTvEpkWp8MM.png",
      "fullname": "Minbyul Jeong",
      "name": "Minbyul",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.09083",
      "authors": [
        {
          "_id": "67b30726d4665a0448e6436d",
          "user": {
            "_id": "6698cffdb2ebada9f4a7e7d7",
            "avatarUrl": "/avatars/e66d946c14595d3b008185f2be8d2f57.svg",
            "isPro": false,
            "fullname": "Greta Warren",
            "user": "gretawarren",
            "type": "user"
          },
          "name": "Greta Warren",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-18T09:32:34.585Z",
          "hidden": false
        },
        {
          "_id": "67b30726d4665a0448e6436e",
          "name": "Irina Shklovski",
          "hidden": false
        },
        {
          "_id": "67b30726d4665a0448e6436f",
          "name": "Isabelle Augenstein",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-13T08:56:25.000Z",
      "title": "「Montre comment fonctionne : Problèmes de vérification automatique de faits expliquables」",
      "summary": "Les langages de programmation et l'IA générative ont largement étendu leur portée sur les médias en ligne, ce qui a entraîné une augmentation de la sécurité et de la complexité, et une nécessité d'un système efficace de vérification de la vérité. En raison des caractéristiques complexes de la vérification de la vérité, il est crucial que les systèmes de vérification de la vérité fournissent des explications aux vérificateurs de la vérité pour qu'ils puissent réviser les résultats. Cependant, ces explications ne sont pas claires sur la manière dont elles peuvent être mieux intégrées dans la fonction de décision et le processus logique des vérificateurs de la vérité. À travers des entretiens non structurés avec des experts en vérification de la vérité, nous avons pu identifier les points suivants qui nécessitent une amélioration : (i) la façon dont les vérificateurs de la vérité évaluent l'évidence et prennent des décisions, expliquant le processus ; (ii) comment les vérificateurs de la vérité utilisent réellement les outils automatiques ; (iii) les explications que les systèmes de vérification de la vérité doivent fournir aux vérificateurs de la vérité. Ces résultats montrent la nécessité d'améliorer les explications et établissent des critères importants pour les explications de la vérification de la vérité itérative, montrant comment les raisons du modèle peuvent être modifiées, se référer à des preuves spécifiques, et montrer comment les incertitudes et les lacunes d'information sont des caractéristiques particulières de la vérification de la vérité.",
      "upvotes": 1,
      "discussionId": "67b30727d4665a0448e6438d"
    },
    "publishedAt": "2025-02-18T04:37:21.573Z",
    "title": "Show Me the Work: Fact-Checkers' Requirements for Explainable Automated Fact-Checking",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6698cffdb2ebada9f4a7e7d7/55xAEeg9Xsk87DXHTH9gM.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.09083.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6698cffdb2ebada9f4a7e7d7",
      "avatarUrl": "/avatars/e66d946c14595d3b008185f2be8d2f57.svg",
      "fullname": "Greta Warren",
      "name": "gretawarren",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.12135",
      "authors": [
        {
          "_id": "67b4028237db78705fb256e1",
          "user": {
            "_id": "64fb31a34c8924c4fe7498bc",
            "avatarUrl": "/avatars/6c8e4a66e1b8b3c786a4000210089392.svg",
            "isPro": false,
            "fullname": "Chaoyue Song",
            "user": "chaoyue7",
            "type": "user"
          },
          "name": "Chaoyue Song",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-18T09:31:40.771Z",
          "hidden": false
        },
        {
          "_id": "67b4028237db78705fb256e2",
          "name": "Jianfeng Zhang",
          "hidden": false
        },
        {
          "_id": "67b4028237db78705fb256e3",
          "name": "Xiu Li",
          "hidden": false
        },
        {
          "_id": "67b4028237db78705fb256e4",
          "name": "Fan Yang",
          "hidden": false
        },
        {
          "_id": "67b4028237db78705fb256e5",
          "name": "Yiwen Chen",
          "hidden": false
        },
        {
          "_id": "67b4028237db78705fb256e6",
          "name": "Zhongcong Xu",
          "hidden": false
        },
        {
          "_id": "67b4028237db78705fb256e7",
          "name": "Jun Hao Liew",
          "hidden": false
        },
        {
          "_id": "67b4028237db78705fb256e8",
          "name": "Xiaoyang Guo",
          "hidden": false
        },
        {
          "_id": "67b4028237db78705fb256e9",
          "name": "Fayao Liu",
          "hidden": false
        },
        {
          "_id": "67b4028237db78705fb256ea",
          "name": "Jiashi Feng",
          "hidden": false
        },
        {
          "_id": "67b4028237db78705fb256eb",
          "name": "Guosheng Lin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-17T18:53:27.000Z",
      "title": "MagicArticulate : Il est en train de préparer l'architecture du modèle 3D.",
      "summary": "Le croissance explosive du contenu 3D a entraîné une augmentation de la demande pour convertir des modèles 3D statiques en articulations dynamiques de manière automatique. Les approches traditionnelles nécessitent plus de temps et de travail en raison de l'annotation automatique et de la manque de cadres de référence à grande échelle, ce qui a empêché le développement de solutions basées sur l'apprentissage. Dans cet article, nous proposons le cadre efficace MagicArticulate pour effectuer la conversion automatique de modèles 3D statiques en versions avec articulations dynamiques. Nos principales contributions sont trois : premièrement, nous proposons le cadre de référence Articulation-XL, qui sélectionne des modèles 3D avec plus de 33 000 annotations de haute qualité d'articulation ; deuxièmement, nous proposons un nouveau méthode de génération de scènes pour aborder le problème du modélisation séquentiel, en utilisant un moteur de conversion collaborative automatique pour gérer la variation dans le nombre de squelettes ou d'articulations et les relations spécifiques entre modèles 3D ; et finalement, nous utilisons un processus de dispersion fonctionnelle qui inclut des calculs de distance géométrique pour prédire les poids de texturisation. Les expériences extensives montrent que MagicArticulate dépasse significativement les méthodes actuelles, permettant la réalisation de hautes qualités d'articulation et la création d'animations pratiques. Page du projet : https://chaoyuesong.github.io/MagicArticulate.",
      "upvotes": 1,
      "discussionId": "67b4028437db78705fb25726"
    },
    "publishedAt": "2025-02-18T04:34:15.786Z",
    "title": "MagicArticulate: Make Your 3D Models Articulation-Ready",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.12135.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64fb31a34c8924c4fe7498bc",
      "avatarUrl": "/avatars/6c8e4a66e1b8b3c786a4000210089392.svg",
      "fullname": "Chaoyue Song",
      "name": "chaoyue7",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.11831",
      "authors": [
        {
          "_id": "67b450cf315f7b69956df3d6",
          "name": "Quentin Garrido",
          "hidden": false
        },
        {
          "_id": "67b450cf315f7b69956df3d7",
          "name": "Nicolas Ballas",
          "hidden": false
        },
        {
          "_id": "67b450cf315f7b69956df3d8",
          "name": "Mahmoud Assran",
          "hidden": false
        },
        {
          "_id": "67b450cf315f7b69956df3d9",
          "name": "Adrien Bardes",
          "hidden": false
        },
        {
          "_id": "67b450cf315f7b69956df3da",
          "name": "Laurent Najman",
          "hidden": false
        },
        {
          "_id": "67b450cf315f7b69956df3db",
          "name": "Michael Rabbat",
          "hidden": false
        },
        {
          "_id": "67b450cf315f7b69956df3dc",
          "name": "Emmanuel Dupoux",
          "hidden": false
        },
        {
          "_id": "67b450cf315f7b69956df3dd",
          "name": "Yann LeCun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-17T14:27:14.000Z",
      "title": "L'intuition physique se développe à travers l'apprentissage préalable de règles naturelles dans les vidéos de la nature.",
      "summary": "Nous étudions l'apparition du reconnaissance physique intuitive en utilisant un modèle de réseau de neurones profond général pour prédire des régions cachées dans des films naturels. Un modèle qui prédit des résultats dans un espace de représentation appris via le cadre de travail de détruire les attentes a découvert la compréhension des caractéristiques physiques intuitives comme l'identité et la cohérence de forme d'objets. D'un autre côté, des modèles de langage à différentes échelles qui cherchent des raisons en passant par des prédictions d'images et de texte atteignent des rendements presque similaires. En comparant ces architectures, il est souligné que l'apprentissage d'espaces de représentation abstraits, tout en prédisant des codifications, permet d'obtenir le reconnaissance physique intuitive. Il est également constaté que des modèles entraînés sur des films personnels atteignent un rendement supérieur à celui de la chance. Cela soulève l'idée que le savoir essentiel (un système génétique dépendant pour comprendre le monde) est nécessaire pour comprendre le reconnaissance physique intuitive.",
      "upvotes": 1,
      "discussionId": "67b450d0315f7b69956df3f9"
    },
    "publishedAt": "2025-02-18T04:20:25.916Z",
    "title": "Intuitive physics understanding emerges from self-supervised pretraining on natural videos",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.11831.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5f1158120c833276f61f1a84",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
      "fullname": "Niels Rogge",
      "name": "nielsr",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 763
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.11085",
      "authors": [
        {
          "_id": "67b44f44620ae0bad17d6699",
          "name": "Yasir Ghunaim",
          "hidden": false
        },
        {
          "_id": "67b44f44620ae0bad17d669a",
          "user": {
            "_id": "642b51385bf2355d02a23d15",
            "avatarUrl": "/avatars/87985347643b2647555f2453fa4d94fb.svg",
            "isPro": true,
            "fullname": "Hasan Abed Al Kader Hammoud",
            "user": "hammh0a",
            "type": "user"
          },
          "name": "Hasan Abed Al Kader Hammoud",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-18T09:30:43.057Z",
          "hidden": false
        },
        {
          "_id": "67b44f44620ae0bad17d669b",
          "name": "Bernard Ghanem",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-16T11:46:23.000Z",
      "title": "Prédiction des caractéristiques d'atomes par apprentissage préalable pour optimiser l'efficacité des données",
      "summary": "Cet article défie le dernier paradigme pour prédire les propriétés des matériaux atomiques. Ce paradigme est lié au développement, au taille et à la quantité de données et aux ressources informatiques. Nous montrons que, en choisissant un ensemble de données approprié, on peut dépasser l'apprentissage précédent à grande échelle, réduisant les coûts de calcul par un 1/24. Nous appliquons un nouvel indice basé sur la Fréchet Inception Distance de la vision par ordinateur, appelé CSI (Indice de Similarité Chimique), aux graphes moléculaires pour quantifier la concordance entre ensembles de données d'apprentissage précédent et tâches ultérieures. En sélectionnant l'ensemble de données la plus pertinente, le modèle d'apprentissage précédent réalisé sur de petits ensembles de données peut dépasser expérimentalement les modèles d'apprentissage précédent sur des ensembles de données mixtes grands (par exemple, JMP). D'autre part, l'ajout de données non pertinentes peut dégrader le rendement du modèle. Notre résultat montre que la question de l'apprentissage précédent dans la prédiction des propriétés des matériaux atomiques est plus valeureuse que la quantité de données.",
      "upvotes": 1,
      "discussionId": "67b44f45620ae0bad17d66b0"
    },
    "publishedAt": "2025-02-18T04:16:28.219Z",
    "title": "Towards Data-Efficient Pretraining for Atomic Property Prediction",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/642b51385bf2355d02a23d15/bLvTbh56AkUmcmRst8mT3.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.11085.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "642b51385bf2355d02a23d15",
      "avatarUrl": "/avatars/87985347643b2647555f2453fa4d94fb.svg",
      "fullname": "Hasan Abed Al Kader Hammoud",
      "name": "hammh0a",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.11775",
      "authors": [
        {
          "_id": "67b4147f7721b4fe4d2bd466",
          "name": "Guangzhi Sun",
          "hidden": false
        },
        {
          "_id": "67b4147f7721b4fe4d2bd467",
          "name": "Yudong Yang",
          "hidden": false
        },
        {
          "_id": "67b4147f7721b4fe4d2bd468",
          "name": "Jimin Zhuang",
          "hidden": false
        },
        {
          "_id": "67b4147f7721b4fe4d2bd469",
          "name": "Changli Tang",
          "hidden": false
        },
        {
          "_id": "67b4147f7721b4fe4d2bd46a",
          "name": "Yixuan Li",
          "hidden": false
        },
        {
          "_id": "67b4147f7721b4fe4d2bd46b",
          "name": "Wei Li",
          "hidden": false
        },
        {
          "_id": "67b4147f7721b4fe4d2bd46c",
          "name": "Zejun MA",
          "hidden": false
        },
        {
          "_id": "67b4147f7721b4fe4d2bd46d",
          "name": "Chao Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-17T13:07:40.000Z",
      "title": "video-SALMONN-o1 : Motivation forte pour le modèle audio-visuel multilingue",
      "summary": "Le progrès de l'optimisation du raisonnement dans les modèles de grands langages (LLMs) a considérablement augmenté les capacités de ces modèles, mais actuellement, les efforts se concentrent sur la résolution de problèmes mathématiques et l'entrée graphique visuelle, sans accorder beaucoup d'attention aux larges applications de la compréhension générale des vidéos. Dans ce travail, nous proposons le premier modèle de langage open-source (open-source) qui renforce le raisonnement dans des tâches de compréhension de vidéos générales. Il est nommé video-SALMONN-o1. Pour améliorer sa capacité de raisonnement, nous avons développé un ensemble de données qui possède des caractéristiques de résolution pas à pas, y compris des vidéos de voix complexes. De plus, nous proposons un processus d'optimisation directe des préférences de pas (pDPO) qui utilise une sélection relativement stricte de pas, et nous avons mis en place un modèle de récompense du niveau de pas qui est efficace à l'adaptation à différentes entrées. Nous introduisons également RivaBench, un cadre d'évaluation de compréhension de vidéos qui renforce le raisonnement, qui contient plus de 4 000 paires de questions et réponses de haute qualité et éditées par des experts, abordant des scénarios comme des comédies de théâtre, des discours académiques et la détection de vidéos synthétiques. Video-SALMONN-o1 a atteint un accroissement de précision de 3 à 8% dans différents cadres d'évaluation de raisonnement de vidéos, comparé à LLaVA-OneVision. De plus, pDPO a atteint un accroissement de 6 à 8% dans le modèle d'ajustement des sous-objets de RivaBench. L'amélioration du raisonnement a permis que video-SALMONN-o1 effectue la capacité de détection de vidéos synthétiques en 0 shot.",
      "upvotes": 1,
      "discussionId": "67b414827721b4fe4d2bd534"
    },
    "publishedAt": "2025-02-18T00:06:55.671Z",
    "title": "video-SALMONN-o1: Reasoning-enhanced Audio-visual Large Language Model",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.11775.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6128
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.11098",
      "authors": [
        {
          "_id": "67b411e45e634139c0d86a1e",
          "name": "Zhao Wang",
          "hidden": false
        },
        {
          "_id": "67b411e45e634139c0d86a1f",
          "name": "Sota Moriyama",
          "hidden": false
        },
        {
          "_id": "67b411e45e634139c0d86a20",
          "name": "Wei-Yao Wang",
          "hidden": false
        },
        {
          "_id": "67b411e45e634139c0d86a21",
          "name": "Briti Gangopadhyay",
          "hidden": false
        },
        {
          "_id": "67b411e45e634139c0d86a22",
          "name": "Shingo Takamatsu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-16T12:26:58.000Z",
      "title": "Structuralement réalisé et agissant stratégiquement : Le Système Multi-Agent du Cadre de Collaborative de LLM",
      "summary": "Récemment, on attend qu'un développement significatif se produise dans les systèmes d'autorité basés sur des modèles de langage profond (LLM-MA), mais des problèmes graves persistent dans la communication et l'amélioration entre les auteurs pour des tâches complexes. Dans cet article, nous proposons un nouveau cadre de travail appelé \"Talk Structurally, Act Hierarchically (TalkHier)\", qui comprend un protocole de communication structuré pour faciliter la communication efficace dans des tâches complexes, ainsi qu'un système d'amélioration hiérarchique pour aborder des problèmes tels que des sorties négatives, des fausses représentations et les préjugés. TalkHier dépasse les modèles de LLM et les systèmes de travail basés sur un seul auteur (par exemple, ReAct, GPT4o) ainsi que les modèles d'autorité ouverts (par exemple, AgentVerse), incluant également des modèles d'échelle d'inférence pour les systèmes d'autorité basés sur des LLM (OpenAI-o1). Cela permet aux systèmes d'autorité basés sur des LLM (LLM-MA) d'atteindre un nouveau niveau de performance efficace, adaptable et collaboratif dans différentes tâches. Les résultats de ce travail ouvrent des voies pour le développement de nouveaux cadres d'autorité plus efficaces, adaptables et collaboratifs. Le code est disponible sur https://github.com/sony/talkhier.",
      "upvotes": 1,
      "discussionId": "67b411e55e634139c0d86a4c"
    },
    "publishedAt": "2025-02-17T23:51:50.821Z",
    "title": "Talk Structurally, Act Hierarchically: A Collaborative Framework for LLM Multi-Agent Systems",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.11098.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6128
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.10454",
      "authors": [
        {
          "_id": "67b40e56bffd44cc85976ecd",
          "name": "Yinghui Li",
          "hidden": false
        },
        {
          "_id": "67b40e56bffd44cc85976ece",
          "name": "Jiayi Kuang",
          "hidden": false
        },
        {
          "_id": "67b40e56bffd44cc85976ecf",
          "name": "Haojing Huang",
          "hidden": false
        },
        {
          "_id": "67b40e56bffd44cc85976ed0",
          "name": "Zhikun Xu",
          "hidden": false
        },
        {
          "_id": "67b40e56bffd44cc85976ed1",
          "name": "Xinnian Liang",
          "hidden": false
        },
        {
          "_id": "67b40e56bffd44cc85976ed2",
          "name": "Yi Yu",
          "hidden": false
        },
        {
          "_id": "67b40e56bffd44cc85976ed3",
          "name": "Wenlian Lu",
          "hidden": false
        },
        {
          "_id": "67b40e56bffd44cc85976ed4",
          "name": "Yangning Li",
          "hidden": false
        },
        {
          "_id": "67b40e56bffd44cc85976ed5",
          "name": "Xiaoyu Tan",
          "hidden": false
        },
        {
          "_id": "67b40e56bffd44cc85976ed6",
          "name": "Chao Qu",
          "hidden": false
        },
        {
          "_id": "67b40e56bffd44cc85976ed7",
          "name": "Ying Shen",
          "hidden": false
        },
        {
          "_id": "67b40e56bffd44cc85976ed8",
          "name": "Hai-Tao Zheng",
          "hidden": false
        },
        {
          "_id": "67b40e56bffd44cc85976ed9",
          "name": "Philip S. Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-12T02:01:10.000Z",
      "title": "Un exemple montre que de nombreux concepts sont connus ! Dans la logique conceptuelle des LLM en mathématiques, des contre-exemples sont présentés.",
      "summary": "La utilisation de modèles de langage grands (LLMs) pour la génération de démonstrations est l'un des thèmes fondamentaux de la recherche sur les LLMs. La capacité des LLMs actuels à réaliser des démonstrations dépend sensiblement de leur expérience avec des processus de démonstration liés lors de l'entraînement. Cette dépendance limite la compréhension profonde des concepts liés aux théorèmes mathématiques. Dans l'éducation mathématique généralement utilisée par les humains, existe un méthode éducative de \"démonstration par exception\", et notre recherche vise à améliorer la capacité de logique mathématique et de démonstration des LLMs en utilisant des exceptions. Concrètement, nous avons créé directement un haut niveau de benchmark de mathématiques universitaires de qualité élevée appelé CounterMATH, ce qui nous permet de demander aux LLMs d'utiliser des exceptions pour réaliser et comprendre les démonstrations. De plus, nous avons développé un cadre de données scientifique et des objectifs pour obtenir des données d'entraînement automatique pour le développement du modèle. Les expériences étendues et l'analyse détaillée montrent que CounterMATH est difficile et révèlent que les capacités de démonstration en utilisant des exceptions dans les modèles de LLMs comme OpenAI ne sont pas suffisantes. De plus, la révision lors du processus d'entraînement montre l'importance de renforcer la capacité logique conceptuelle en utilisant des exceptions dans la mémoire mathématique complète des LLMs. Nous sommes convaincus que notre recherche offre une nouvelle perspective à la communauté des LLMs en mathématiques.",
      "upvotes": 1,
      "discussionId": "67b40e57bffd44cc85976f0e"
    },
    "publishedAt": "2025-02-17T23:37:16.770Z",
    "title": "One Example Shown, Many Concepts Known! Counterexample-Driven Conceptual Reasoning in Mathematical LLMs",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.10454.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6128
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.11574",
      "authors": [
        {
          "_id": "67b435c29e5685b308a8edac",
          "user": {
            "_id": "65bcbc01d6d0ffbceb8b2e6e",
            "avatarUrl": "/avatars/73edb2d6b7b11208439ac88b365079e8.svg",
            "isPro": false,
            "fullname": "Johan Boye",
            "user": "jboye",
            "type": "user"
          },
          "name": "Johan Boye",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-02-18T07:24:50.956Z",
          "hidden": false
        },
        {
          "_id": "67b435c29e5685b308a8edad",
          "user": {
            "_id": "6033e34a9aa44495c80dd043",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1614079701740-6033e34a9aa44495c80dd043.jpeg",
            "isPro": false,
            "fullname": "Birger Moell",
            "user": "birgermoell",
            "type": "user"
          },
          "name": "Birger Moell",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-18T09:30:49.328Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-17T09:07:32.000Z",
      "title": "Le échec des grands modèles de langue et les raisons mathématiques",
      "summary": "Cet article utilise 50 nouveaux problèmes de niveau secondaire pour explorer la capacité d'inférence mathématique des modèles de langage grands (LLMs). Dans les études précédentes, l'accent était mis sur la précision des réponses, mais dans ce travail, nous avons examiné rigoureusement tant la réponse finale que les étapes pour atteindre la solution, identifiant des erreurs dans l'inférence. Nous avons évalué 8 modèles plus récents (Mixtral, Llama, Gemini, GPT-4o, la version o1 de OpenAI et d'autres modèles), et nous avons constaté que les nouveaux modèles comme o3-mini et deepseek-r1 atteignent de hautes précisions, mais tous présentent des erreurs dans l'inférence spatiale, la planification stratégique et le calcul, parfois générant des réponses correctes basées sur une logique erronée. Les failles générales comprennent des suppositions inutiles, un excessive dépendance sur des motifs numériques et le difficile traduction d'intuitions physiques et de étapes mathématiques. L'analyse manuelle montre que les modèles ont des difficultés dans les problèmes nécessitant une inférence multiniveau et des connaissances du monde réel, et qu'à la mesure de leur base de connaissances mathématiques, il est plus difficile d'adapter à ces problèmes. Nos résultats soulignent l'importance de l'évaluation du raisonnement et alertent contre la surévaluation de la capacité de résolution de problèmes des LLMs. Cette étude révèle les déficiences résiduelles dans la capacité de généralisation des LLMs et souligne la nécessité d'améliorations structurelles et spécifiques dans le traitement des limites.",
      "upvotes": 0,
      "discussionId": "67b435c29e5685b308a8edf1"
    },
    "publishedAt": "2025-02-18T02:26:18.856Z",
    "title": "Large Language Models and Mathematical Reasoning Failures",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.11574.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6033e34a9aa44495c80dd043",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1614079701740-6033e34a9aa44495c80dd043.jpeg",
      "fullname": "Birger Moell",
      "name": "birgermoell",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 36
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.11578",
      "authors": [
        {
          "_id": "67b435475bff5f34c1ebee1b",
          "user": {
            "_id": "6033e34a9aa44495c80dd043",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1614079701740-6033e34a9aa44495c80dd043.jpeg",
            "isPro": false,
            "fullname": "Birger Moell",
            "user": "birgermoell",
            "type": "user"
          },
          "name": "Birger Moell",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-18T09:30:52.639Z",
          "hidden": false
        },
        {
          "_id": "67b435475bff5f34c1ebee1c",
          "user": {
            "_id": "65bcbc01d6d0ffbceb8b2e6e",
            "avatarUrl": "/avatars/73edb2d6b7b11208439ac88b365079e8.svg",
            "isPro": false,
            "fullname": "Johan Boye",
            "user": "jboye",
            "type": "user"
          },
          "name": "Johan Boye",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-02-18T07:22:48.554Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-17T09:09:58.000Z",
      "title": "La mesure de la complexité du langage est utilisée comme un proxy pour éliminer le bruit dans l'évaluation du rendement des modèles de langage grands.",
      "summary": "Les modèles de langage grands (LLMs) ont réalisé des progrès importants dans la génération de texte naturel, mais présentent des problèmes dans les tâches qui exigent une précision élevée, comme les calculs ou les analyses structurales. Dans cet article, en calculant la pondération de la difficulté de lecture LIX et la distance de dépendance moyenne (ADD), nous étudions le rendement des plus récents LLMs en fonction de la complexité linguistique. En utilisant des articles de niveau scolaire et universitaire suédois, nous évaluons la capacité des modèles à calculer la pondération LIX et à analyser la dépendance, en les comparant aux normes existantes. Nos résultats montrent que tous les modèles présentent des capacités différentes dans ces tâches, avec ChatGPT-o1-mini atteignant la meilleure précision et la meilleure précision dans le calcul de LIX et l'analyse de dépendance. De plus, nous constatons une forte corrélation significative entre la précision du calcul de LIX du modèle et le rendement général sur le benchmark Massive Multitask Language Understanding (MMLU) (-0.875, p 0.026, N=6). Ces résultats démontrent que la capacité de mesurer la complexité linguistique peut agir comme un représentant 0 shot qui inclut du bruit dans l'évaluation générale des LLMs. De plus, ils fournissent des méthodes utiles pour évaluer les modèles lorsque des grandes collections de données de benchmark sont disponibles.",
      "upvotes": 0,
      "discussionId": "67b435485bff5f34c1ebee52"
    },
    "publishedAt": "2025-02-18T02:23:29.869Z",
    "title": "Language Complexity Measurement as a Noisy Zero-Shot Proxy for Evaluating LLM Performance",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.11578.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6033e34a9aa44495c80dd043",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1614079701740-6033e34a9aa44495c80dd043.jpeg",
      "fullname": "Birger Moell",
      "name": "birgermoell",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 36
    },
    "isAuthorParticipating": true
  }
]