[
  {
    "paper": {
      "id": "2505.16938",
      "authors": [
        {
          "_id": "682fe3a565bac3ec3556fc6c",
          "name": "NovelSeek Team",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc6d",
          "name": "Bo Zhang",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc6e",
          "name": "Shiyang Feng",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc6f",
          "name": "Xiangchao Yan",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc70",
          "name": "Jiakang Yuan",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc71",
          "name": "Zhiyin Yu",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc72",
          "name": "Xiaohan He",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc73",
          "name": "Songtao Huang",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc74",
          "name": "Shaowei Hou",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc75",
          "name": "Zheng Nie",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc76",
          "name": "Zhilong Wang",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc77",
          "name": "Jinyao Liu",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc78",
          "name": "Runmin Ma",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc79",
          "name": "Tianshuo Peng",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc7a",
          "name": "Peng Ye",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc7b",
          "name": "Dongzhan Zhou",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc7c",
          "name": "Shufei Zhang",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc7d",
          "name": "Xiaosong Wang",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc7e",
          "name": "Yilan Zhang",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc7f",
          "name": "Meng Li",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc80",
          "name": "Zhongying Tu",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc81",
          "name": "Xiangyu Yue",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc82",
          "name": "Wangli Ouyang",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc83",
          "name": "Bowen Zhou",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc84",
          "name": "Lei Bai",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T17:27:43.000Z",
      "submittedOnDailyAt": "2025-05-23T01:25:34.477Z",
      "title": "Nubelsh : Agente scientifique qui divise - Construction d'un système fermé de l'hypothèse à la vérification de la même",
      "submittedOnDailyBy": {
        "_id": "643dfd235aafbdca3a5792c0",
        "avatarUrl": "/avatars/ce8553cf5936012c692e08054ee27937.svg",
        "isPro": false,
        "fullname": "Bo Zhang",
        "user": "BoZhang",
        "type": "user"
      },
      "summary": "L'intelligence artificielle (IA) accélère les changements dans le paradigme de la recherche scientifique et renforce son efficacité, en guidant l'innovation. On présente le Nueral Sketch. Nueral Sketch est un cadre de travail intégré et fermé multi-agent qui effectue des traductions automatiques dans diverses domaines de la recherche scientifique, permettant aux chercheurs de résoudre des problèmes complexes avec une rapidité et une précision sans précédent. Nueral Sketch se distingue par trois principales avantages : 1) L'échelle : il montre diversité dans 12 tâches de recherche scientifique et génère des innovations pour améliorer le rendement des codes de référence. 2) L'interactivité : il fournit une interaction entre le feedback des experts humains depuis des terminals automatisés et de multiples agents, facilitant l'intégration continue des connaissances des experts dans une conversation. 3) L'efficacité : il réduit significativement l'effort humain et offre des gains de rendement surprenants dans diverses domaines scientifiques. Par exemple, la prédiction de la production de réactions a augmenté de 27,6% à 35,4% en 12 heures, la prédiction de l'activité des enzymes de 0,52 à 0,79 en 4 heures, et la segmentation sémantique 2D de 78,8% à 81,0% en 30 heures.",
      "upvotes": 70,
      "discussionId": "682fe3a865bac3ec3556fd21"
    },
    "publishedAt": "2025-05-22T13:27:43.000Z",
    "title": "NovelSeek: When Agent Becomes the Scientist -- Building Closed-Loop\n  System from Hypothesis to Verification",
    "summary": "Artificial Intelligence (AI) is accelerating the transformation of scientific\nresearch paradigms, not only enhancing research efficiency but also driving\ninnovation. We introduce NovelSeek, a unified closed-loop multi-agent framework\nto conduct Autonomous Scientific Research (ASR) across various scientific\nresearch fields, enabling researchers to tackle complicated problems in these\nfields with unprecedented speed and precision. NovelSeek highlights three key\nadvantages: 1) Scalability: NovelSeek has demonstrated its versatility across\n12 scientific research tasks, capable of generating innovative ideas to enhance\nthe performance of baseline code. 2) Interactivity: NovelSeek provides an\ninterface for human expert feedback and multi-agent interaction in automated\nend-to-end processes, allowing for the seamless integration of domain expert\nknowledge. 3) Efficiency: NovelSeek has achieved promising performance gains in\nseveral scientific fields with significantly less time cost compared to human\nefforts. For instance, in reaction yield prediction, it increased from 27.6% to\n35.4% in just 12 hours; in enhancer activity prediction, accuracy rose from\n0.52 to 0.79 with only 4 hours of processing; and in 2D semantic segmentation,\nprecision advanced from 78.8% to 81.0% in a mere 30 hours.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16938.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "643dfd235aafbdca3a5792c0",
      "avatarUrl": "/avatars/ce8553cf5936012c692e08054ee27937.svg",
      "fullname": "Bo Zhang",
      "name": "BoZhang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.16410",
      "authors": [
        {
          "_id": "682fd6045e83dc325675312b",
          "name": "Guanting Dong",
          "hidden": false
        },
        {
          "_id": "682fd6045e83dc325675312c",
          "name": "Yifei Chen",
          "hidden": false
        },
        {
          "_id": "682fd6045e83dc325675312d",
          "name": "Xiaoxi Li",
          "hidden": false
        },
        {
          "_id": "682fd6045e83dc325675312e",
          "name": "Jiajie Jin",
          "hidden": false
        },
        {
          "_id": "682fd6045e83dc325675312f",
          "name": "Hongjin Qian",
          "hidden": false
        },
        {
          "_id": "682fd6045e83dc3256753130",
          "name": "Yutao Zhu",
          "hidden": false
        },
        {
          "_id": "682fd6045e83dc3256753131",
          "name": "Hangyu Mao",
          "hidden": false
        },
        {
          "_id": "682fd6045e83dc3256753132",
          "name": "Guorui Zhou",
          "hidden": false
        },
        {
          "_id": "682fd6045e83dc3256753133",
          "name": "Zhicheng Dou",
          "hidden": false
        },
        {
          "_id": "682fd6045e83dc3256753134",
          "name": "Ji-Rong Wen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T09:00:19.000Z",
      "submittedOnDailyAt": "2025-05-23T00:31:41.669Z",
      "title": "Tool-Star : Apprentissage par répétition pour le soutien de vérificateurs de fonctions multifonctionnelles dans les modèles de langage en mode aveugle",
      "submittedOnDailyBy": {
        "_id": "61cd4b833dd34ba1985e0753",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61cd4b833dd34ba1985e0753/BfHfrwotoMESpXZOHiIe4.png",
        "isPro": false,
        "fullname": "KABI",
        "user": "dongguanting",
        "type": "user"
      },
      "summary": "Récemment, les Modèles de Langue de Longue Portée (LLMs) ont démontré des habiletés logiques impressionnantes à travers de grands scénarios d'Apprentissage par Renforcement (RL). Cependant, la collaboration efficace de plusieurs outils et le renforcement de la logique par RL dans les LLMs représentent un défi ouvert. Dans cet article, nous présentons un cadre de travail basé sur le RL appelé Tool-Star, dont l'objectif est que les LLMs puissent appeler automatiquement diverses outils externes à différentes étapes de la logique. Tool-Star intègre six outils et adopte un design systématique pour la synthèse et l'entraînement de données. Pour aborder la pénurie de données d'utilisation d'outils, nous proposons un système de synthèse de données logiques qui inclut des outils généraux, en combinant l'utilisation de Prompts et de Hints pour générer des processus d'utilisation d'outils automatiques et scalables. De plus, nous utilisons des processus de normalisation de qualité et de classification de niveaux de difficulté pour éliminer les échantillons de faible qualité et organiser le jeu de données de manière croissante en fonction de la difficulté. Nous proposons un cadre d'entraînement en deux étapes, incluant un ajustement froid et une logique de récompense heuristique pour l'auto-évaluation de plusieurs outils. Les analyses expérimentales sur 10 ou plus benchmarks logiques difficiles démontrent l'efficacité et l'efficience de Tool-Star. Le code est disponible sur : https://github.com/dongguanting/Tool-Star.",
      "upvotes": 37,
      "discussionId": "682fd6055e83dc3256753187",
      "projectPage": "https://github.com/dongguanting/Tool-Star/",
      "githubRepo": "https://github.com/dongguanting/Tool-Star/",
      "ai_summary": "Tool-Star, an RL-based framework, enables LLMs to autonomously use multiple tools for stepwise reasoning, leveraging data synthesis and hierarchical reward design.",
      "ai_keywords": [
        "large language models",
        "LLMs",
        "large-scale reinforcement learning",
        "RL",
        "multi-tool collaborative reasoning",
        "tool-use data",
        "tool-integrated reasoning",
        "tool-invocation feedback",
        "multi-tool self-critic",
        "hierarchical reward design"
      ]
    },
    "publishedAt": "2025-05-22T05:00:19.000Z",
    "title": "Tool-Star: Empowering LLM-Brained Multi-Tool Reasoner via Reinforcement\n  Learning",
    "summary": "Recently, large language models (LLMs) have shown remarkable reasoning\ncapabilities via large-scale reinforcement learning (RL). However, leveraging\nthe RL algorithm to empower effective multi-tool collaborative reasoning in\nLLMs remains an open challenge. In this paper, we introduce Tool-Star, an\nRL-based framework designed to empower LLMs to autonomously invoke multiple\nexternal tools during stepwise reasoning. Tool-Star integrates six types of\ntools and incorporates systematic designs in both data synthesis and training.\nTo address the scarcity of tool-use data, we propose a general tool-integrated\nreasoning data synthesis pipeline, which combines tool-integrated prompting\nwith hint-based sampling to automatically and scalably generate tool-use\ntrajectories. A subsequent quality normalization and difficulty-aware\nclassification process filters out low-quality samples and organizes the\ndataset from easy to hard. Furthermore, we propose a two-stage training\nframework to enhance multi-tool collaborative reasoning by: (1) cold-start\nfine-tuning, which guides LLMs to explore reasoning patterns via\ntool-invocation feedback; and (2) a multi-tool self-critic RL algorithm with\nhierarchical reward design, which reinforces reward understanding and promotes\neffective tool collaboration. Experimental analyses on over 10 challenging\nreasoning benchmarks highlight the effectiveness and efficiency of Tool-Star.\nThe code is available at https://github.com/dongguanting/Tool-Star.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16410.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "61cd4b833dd34ba1985e0753",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61cd4b833dd34ba1985e0753/BfHfrwotoMESpXZOHiIe4.png",
      "fullname": "KABI",
      "name": "dongguanting",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 22
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.14810",
      "authors": [
        {
          "_id": "682ea2b450671dc82688b8ad",
          "user": {
            "_id": "640ad17a1ee054d66a74783e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640ad17a1ee054d66a74783e/u0PjIkyC-9HkGzEyUQ7JN.jpeg",
            "isPro": false,
            "fullname": "Tingchen Fu",
            "user": "TingchenFu",
            "type": "user"
          },
          "name": "Tingchen Fu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:15:44.217Z",
          "hidden": false
        },
        {
          "_id": "682ea2b450671dc82688b8ae",
          "name": "Jiawei Gu",
          "hidden": false
        },
        {
          "_id": "682ea2b450671dc82688b8af",
          "user": {
            "_id": "63f3502a520c14618925825a",
            "avatarUrl": "/avatars/e986a2a6625e7be6890616a417f908d2.svg",
            "isPro": false,
            "fullname": "Yafu Li",
            "user": "yaful",
            "type": "user"
          },
          "name": "Yafu Li",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-22T04:06:13.396Z",
          "hidden": false
        },
        {
          "_id": "682ea2b450671dc82688b8b0",
          "name": "Xiaoye Qu",
          "hidden": false
        },
        {
          "_id": "682ea2b450671dc82688b8b1",
          "name": "Yu Cheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-20T18:18:01.000Z",
      "submittedOnDailyAt": "2025-05-23T00:49:30.349Z",
      "title": "Échelon de la région, perte de contrôle : évaluation de la fidélité aux instructions dans les modèles logiques à grande échelle",
      "submittedOnDailyBy": {
        "_id": "64cb54da1af278541d663708",
        "avatarUrl": "/avatars/c44507cc92bb2e83154bad31b90ce6dd.svg",
        "isPro": false,
        "fullname": "Xiaoye Qu",
        "user": "Xiaoye08",
        "type": "user"
      },
      "summary": "Suivre les instructions est essentiel pour ajuster les modèles de langage grands (LLMs) aux intentions du utilisateur. Les modèles les plus récents ont démontré un excellent rendement dans les problèmes mathématiques complexes, mais leur capacité à suivre les instructions en langage nature n'a pas encore été suffisamment étudiée. Dans cet article, nous présentons \"MathIF\", un cadre de référence spécialisé pour évaluer la capacité à suivre les instructions dans des tâches de raisonnement mathématique. Les analyses expérimentales ont clairement révélé que maintenir la contrôlabilité (contrôlabilité) lors de l'amélioration de la capacité de raisonnement est un défi. De plus, les modèles à forte capacité de raisonnement tendent à rencontrer des difficultés à suivre les instructions du utilisateur. Limiter la longueur des séquences ou entraîner les modèles par apprentissage par renforcement en raisonnement peut diminuer le rendement de la suivi des instructions, surtout lorsque les séquences de texte sont longues. De plus, la récupération partielle du suivi des instructions avec des interférences simples peut coûter la capacité de raisonnement, révélant une contraposition fondamentale dans le paradigme actuel d'entraînement des LLMs et soulignant la nécessité de modèles de raisonnement pour suivre les instructions. Les codes et les données sont disponibles sur https://github.com/TingchenFu/MathIF.",
      "upvotes": 37,
      "discussionId": "682ea2b550671dc82688b8e2",
      "githubRepo": "https://github.com/TingchenFu/MathIF",
      "ai_summary": "An empirical analysis of MathIF identifies a tension between enhancing reasoning capacity and maintaining instruction adherence in large language models.",
      "ai_keywords": [
        "instruction-following",
        "reasoning-oriented models",
        "benchmarks",
        "chains-of-thought",
        "reinforcement learning",
        "instruction adherence"
      ]
    },
    "publishedAt": "2025-05-20T14:18:01.000Z",
    "title": "Scaling Reasoning, Losing Control: Evaluating Instruction Following in\n  Large Reasoning Models",
    "summary": "Instruction-following is essential for aligning large language models (LLMs)\nwith user intent. While recent reasoning-oriented models exhibit impressive\nperformance on complex mathematical problems, their ability to adhere to\nnatural language instructions remains underexplored. In this work, we introduce\nMathIF, a dedicated benchmark for evaluating instruction-following in\nmathematical reasoning tasks. Our empirical analysis reveals a consistent\ntension between scaling up reasoning capacity and maintaining controllability,\nas models that reason more effectively often struggle to comply with user\ndirectives. We find that models tuned on distilled long chains-of-thought or\ntrained with reasoning-oriented reinforcement learning often degrade in\ninstruction adherence, especially when generation length increases.\nFurthermore, we show that even simple interventions can partially recover\nobedience, though at the cost of reasoning performance. These findings\nhighlight a fundamental tension in current LLM training paradigms and motivate\nthe need for more instruction-aware reasoning models. We release the code and\ndata at https://github.com/TingchenFu/MathIF.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14810.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64cb54da1af278541d663708",
      "avatarUrl": "/avatars/c44507cc92bb2e83154bad31b90ce6dd.svg",
      "fullname": "Xiaoye Qu",
      "name": "Xiaoye08",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.16707",
      "authors": [
        {
          "_id": "682fdd77e3102e71872d9b00",
          "name": "Yongliang Wu",
          "hidden": false
        },
        {
          "_id": "682fdd77e3102e71872d9b01",
          "name": "Zonghui Li",
          "hidden": false
        },
        {
          "_id": "682fdd77e3102e71872d9b02",
          "name": "Xinting Hu",
          "hidden": false
        },
        {
          "_id": "682fdd77e3102e71872d9b03",
          "name": "Xinyu Ye",
          "hidden": false
        },
        {
          "_id": "682fdd77e3102e71872d9b04",
          "name": "Xianfang Zeng",
          "hidden": false
        },
        {
          "_id": "682fdd77e3102e71872d9b05",
          "name": "Gang Yu",
          "hidden": false
        },
        {
          "_id": "682fdd77e3102e71872d9b06",
          "name": "Wenbo Zhu",
          "hidden": false
        },
        {
          "_id": "682fdd77e3102e71872d9b07",
          "name": "Bernt Schiele",
          "hidden": false
        },
        {
          "_id": "682fdd77e3102e71872d9b08",
          "name": "Ming-Hsuan Yang",
          "hidden": false
        },
        {
          "_id": "682fdd77e3102e71872d9b09",
          "name": "Xu Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T14:08:59.000Z",
      "submittedOnDailyAt": "2025-05-23T00:59:23.402Z",
      "title": "KRIS-Bench : Marceau d'Évaluation pour Modèles d'Édition d'Images Intelligentes de l'Avenir",
      "submittedOnDailyBy": {
        "_id": "66f6bc97980d52c75c300511",
        "avatarUrl": "/avatars/f7c23c4b09701580b533212ec9b6e306.svg",
        "isPro": false,
        "fullname": "Yongliang",
        "user": "Liang0223",
        "type": "user"
      },
      "summary": "Le développement récent des modèles génératifs multimodal a réalisé des avancées significatives dans l'édition d'images selon des indications. Cependant, ces modèles ne doivent pas seulement créer des sorties visuellement adéquates ; ils doivent également exécuter des tâches d'édition logique basées sur des connaissances. Cet article présente KRIS-Bench, un cadre de référence pour évaluer les modèles basés sur des connaissances logiques dans les systèmes d'édition d'images. KRIS-Bench, inspiré par des théories éducatives, classe les tâches d'édition en trois types de connaissances : factuelles, conceptuelles et processuelles. En se basant sur ces noms techniques, un total de 22 tâches représentatives sur 7 dimensions logiques ont été conçues, et une collection de 1,267 instances étiquetées de haute qualité a été lancée. Un nouveau métrique est proposé pour évaluer la raisonnabilité du savoir et des suggestions de connaissances sont ajustées pour améliorer l'étude humaine. Les résultats des expériences avec les 10 modèles les plus avancés montrent clairement une grande variabilité dans le rendement logique, soulignant la nécessité d'un cadre de référence centré sur les connaissances et favorisant le développement de systèmes d'édition d'images basés sur le cerveau.",
      "upvotes": 33,
      "discussionId": "682fdd79e3102e71872d9b79",
      "projectPage": "https://yongliang-wu.github.io/kris_bench_project_page/",
      "githubRepo": "https://github.com/mercurystraw/Kris_Bench",
      "ai_summary": "KRIS-Bench assesses generative models' knowledge-based reasoning in image editing through a taxonomy of editing tasks and a Knowledge Plausibility metric.",
      "ai_keywords": [
        "multi-modal generative models",
        "instruction-based image editing",
        "knowledge-based reasoning",
        "KRIS-Bench",
        "cognitive assessment",
        "foundational knowledge types",
        "Factual",
        "Conceptual",
        "Procedural",
        "reasoning dimensions",
        "Knowledge Plausibility metric"
      ]
    },
    "publishedAt": "2025-05-22T10:08:59.000Z",
    "title": "KRIS-Bench: Benchmarking Next-Level Intelligent Image Editing Models",
    "summary": "Recent advances in multi-modal generative models have enabled significant\nprogress in instruction-based image editing. However, while these models\nproduce visually plausible outputs, their capacity for knowledge-based\nreasoning editing tasks remains under-explored. In this paper, we introduce\nKRIS-Bench (Knowledge-based Reasoning in Image-editing Systems Benchmark), a\ndiagnostic benchmark designed to assess models through a cognitively informed\nlens. Drawing from educational theory, KRIS-Bench categorizes editing tasks\nacross three foundational knowledge types: Factual, Conceptual, and Procedural.\nBased on this taxonomy, we design 22 representative tasks spanning 7 reasoning\ndimensions and release 1,267 high-quality annotated editing instances. To\nsupport fine-grained evaluation, we propose a comprehensive protocol that\nincorporates a novel Knowledge Plausibility metric, enhanced by knowledge hints\nand calibrated through human studies. Empirical results on 10 state-of-the-art\nmodels reveal significant gaps in reasoning performance, highlighting the need\nfor knowledge-centric benchmarks to advance the development of intelligent\nimage editing systems.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16707.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66f6bc97980d52c75c300511",
      "avatarUrl": "/avatars/f7c23c4b09701580b533212ec9b6e306.svg",
      "fullname": "Yongliang",
      "name": "Liang0223",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.15966",
      "authors": [
        {
          "_id": "682fe6bd5f80e910085b5116",
          "name": "Alex Su",
          "hidden": false
        },
        {
          "_id": "682fe6bd5f80e910085b5117",
          "name": "Haozhe Wang",
          "hidden": false
        },
        {
          "_id": "682fe6bd5f80e910085b5118",
          "name": "Weimin Ren",
          "hidden": false
        },
        {
          "_id": "682fe6bd5f80e910085b5119",
          "name": "Fangzhen Lin",
          "hidden": false
        },
        {
          "_id": "682fe6bd5f80e910085b511a",
          "user": {
            "_id": "6313a86154e6e5d9f0f94e04",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
            "isPro": false,
            "fullname": "Wenhu Chen",
            "user": "wenhu",
            "type": "user"
          },
          "name": "Wenhu Chen",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-23T03:08:46.964Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6313a86154e6e5d9f0f94e04/pPkAUFC7KKDS7Q9fhZKmM.png"
      ],
      "publishedAt": "2025-05-21T19:35:08.000Z",
      "submittedOnDailyAt": "2025-05-23T01:39:51.922Z",
      "title": "Pixel Reasoner : Appuie la théorie de la raison dans l'espace de pixels en utilisant la neurone de coupure de Kaical.",
      "submittedOnDailyBy": {
        "_id": "6313a86154e6e5d9f0f94e04",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
        "isPro": false,
        "fullname": "Wenhu Chen",
        "user": "wenhu",
        "type": "user"
      },
      "summary": "La pensée rationnelle en chaîne a apporté des améliorations significatives au rendement des grands modèles de langue (LLMs) dans diverses domaines. Cependant, ce processus d'inférence se développe uniquement dans l'espace de texte, ce qui limite l'efficacité dans des tâches de densité visuelle. Pour résoudre cette limitation, nous proposons le concept d'inférence dans l'espace de pixels. Dans ce nouveau cadre, les modèles de langue visuelle (VLMs) acquièrent des opérations d'inférence visuelles, comme l'amplification et la sélection de cadres. Ces opérations permettent aux VLMs de vérifier et de poser des questions sur des tests visuels directement, ce qui améliore la précision de l'inférence dans des tâches visuelles. De plus, l'introduction de cette capacité d'inférence dans l'espace de pixels aux VLMs soulève des défis clairs, tels que l'inégalité initiale de capacité du modèle et la résistance à de nouvelles opérations dans l'espace de pixels. Pour aborder ces défis, nous utilisons un méthode d'apprentissage en deux étapes. Dans la première étape, un ajustement guidé par des trajectoires d'inférence synthétique est effectué pour que le modèle se familiarise avec de nouvelles opérations visuelles. Dans la deuxième étape, l'apprentissage par renforcement (RL) utilise un plan de récompense poussé par la curiosité pour équilibrer l'exploration entre l'inférence dans l'espace de pixels et dans le texte. Grâce à ces opérations visuelles, les VLMs peuvent interagir avec des entrées visuelles complexes et, par exemple, collecter des informations activement dans des images ou des vidéos riches en contenu. Nous avons démontré que ces méthodologies améliorent significativement le rendement des VLMs dans différents critères de tests d'inférence visuelle. Notre modèle de 700 millions de paramètres, \\model, a atteint une précision de 84% sur les critères V*, de 74% sur TallyQA-Complex et de 84% sur InfographicsVQA, ce qui représente la précision la plus élevée à ce jour d'un modèle open. Ces résultats soulignent l'importance de l'inférence dans l'espace de pixels et l'efficacité de notre cadre.",
      "upvotes": 25,
      "discussionId": "682fe6bf5f80e910085b51ae",
      "ai_summary": "Introducing pixel-space reasoning in Vision-Language Models (VLMs) through visual operations like zoom-in and select-frame enhances their performance on visual tasks.",
      "ai_keywords": [
        "Vision-Language Models",
        "VLMs",
        "pixel-space reasoning",
        "visual reasoning operations",
        "zoom-in",
        "select-frame",
        "reinforcement learning",
        "RL",
        "curiosity-driven reward scheme",
        "V* bench",
        "TallyQA-Complex",
        "InfographicsVQA"
      ]
    },
    "publishedAt": "2025-05-21T15:35:08.000Z",
    "title": "Pixel Reasoner: Incentivizing Pixel-Space Reasoning with\n  Curiosity-Driven Reinforcement Learning",
    "summary": "Chain-of-thought reasoning has significantly improved the performance of\nLarge Language Models (LLMs) across various domains. However, this reasoning\nprocess has been confined exclusively to textual space, limiting its\neffectiveness in visually intensive tasks. To address this limitation, we\nintroduce the concept of reasoning in the pixel-space. Within this novel\nframework, Vision-Language Models (VLMs) are equipped with a suite of visual\nreasoning operations, such as zoom-in and select-frame. These operations enable\nVLMs to directly inspect, interrogate, and infer from visual evidences, thereby\nenhancing reasoning fidelity for visual tasks. Cultivating such pixel-space\nreasoning capabilities in VLMs presents notable challenges, including the\nmodel's initially imbalanced competence and its reluctance to adopt the newly\nintroduced pixel-space operations. We address these challenges through a\ntwo-phase training approach. The first phase employs instruction tuning on\nsynthesized reasoning traces to familiarize the model with the novel visual\noperations. Following this, a reinforcement learning (RL) phase leverages a\ncuriosity-driven reward scheme to balance exploration between pixel-space\nreasoning and textual reasoning. With these visual operations, VLMs can\ninteract with complex visual inputs, such as information-rich images or videos\nto proactively gather necessary information. We demonstrate that this approach\nsignificantly improves VLM performance across diverse visual reasoning\nbenchmarks. Our 7B model, \\model, achieves 84\\% on V* bench, 74\\% on\nTallyQA-Complex, and 84\\% on InfographicsVQA, marking the highest accuracy\nachieved by any open-source model to date. These results highlight the\nimportance of pixel-space reasoning and the effectiveness of our framework.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6313a86154e6e5d9f0f94e04/pPkAUFC7KKDS7Q9fhZKmM.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15966.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6313a86154e6e5d9f0f94e04",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
      "fullname": "Wenhu Chen",
      "name": "wenhu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 38
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.16175",
      "authors": [
        {
          "_id": "682fd91a1ffb93faf139d288",
          "name": "Benjamin Schneider",
          "hidden": false
        },
        {
          "_id": "682fd91a1ffb93faf139d289",
          "name": "Dongfu Jiang",
          "hidden": false
        },
        {
          "_id": "682fd91a1ffb93faf139d28a",
          "name": "Chao Du",
          "hidden": false
        },
        {
          "_id": "682fd91a1ffb93faf139d28b",
          "name": "Tianyu Pang",
          "hidden": false
        },
        {
          "_id": "682fd91a1ffb93faf139d28c",
          "name": "Wenhu Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T03:26:50.000Z",
      "submittedOnDailyAt": "2025-05-23T00:43:01.292Z",
      "title": "Bien sûr ! Voici la traduction en français :\n\n**VIDEO RAPIDE : Compréhension en Temps Réel de Vidéos de Longue Durée par des Algorithmes de Système**\n**Code Sheep**",
      "submittedOnDailyBy": {
        "_id": "62567c86d444a9b5a0ec51c1",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62567c86d444a9b5a0ec51c1/1vXJf2uGztPcXpkwyTBr6.png",
        "isPro": false,
        "fullname": "Dongfu Jiang",
        "user": "DongfuJiang",
        "type": "user"
      },
      "summary": "La compréhension des vidéos a émergé comme une compétence importante dans des applications réelles. Par exemple, elle est utilisée dans le vidéoédition, les résumés de réunions, l'analyse des cours d'éducation et les émissions sportives, entre d'autres domaines. Cependant, dans les modèles de langage vidéo (LLM), deux problèmes principaux limitent leur exécution : le fort consommateur de calculs et l'impossibilité d'exécution. 1) Le décodage séquentiel de vidéo, lors du processus de conversion du flux de bytes de vidéo en images RGB, peut prendre environ un minute pour un vidéo d'une heure. 2) Pour l'inférence de LLM, il est nécessaire de charger anticipé la mémoire du GPU pour 100,000 tokens, ce qui augmente le temps de chargement et l'utilisation de la mémoire. Pour résoudre ces problèmes, on propose le système algorithmique QuickVideo, qui vise à accélérer significativement la compréhension des vidéos et à soutenir des applications en temps réel. QuickVideo est constitué de trois innovations clés : QuickDecoder, qui divise la vidéo en intervalles correspondant à des images clés, et améliore la vitesse de 2 à 3 fois grâce à un décodage CPU parallèle ; QuickPrefill, qui utilise un caché de KV pour une lecture anticipée efficace de la mémoire, permettant de soutenir plus d'images avec moins de mémoire GPU ; et la combinaison du décodage de vidéo CPU et de l'inférence sur GPU par superposition temporelle. Ces composants réduisent approximativement le temps d'inférence à un minute pour une vidéo longue et permettent une compréhension de vidéo de haute qualité scalable, même sur des dispositifs avec des ressources matérielles limitées. Les tests montrent que QuickVideo démontre une capacité de scalabilité en temps et en fréquence d'échantillonnage, et est capable de traiter des vidéos de longue durée de manière pratique.",
      "upvotes": 24,
      "discussionId": "682fd91b1ffb93faf139d2d0",
      "ai_summary": "QuickVideo accelerates long-video understanding by combining a parallelized video decoder, memory-efficient prefilling, and overlapping video decoding with inference, enabling real-time performance.",
      "ai_keywords": [
        "QuickDecoder",
        "parallelized CPU-based video decoder",
        "keyframe-aligned intervals",
        "QuickPrefill",
        "memory-efficient prefilling",
        "KV-cache pruning",
        "overlapping scheme"
      ]
    },
    "publishedAt": "2025-05-21T23:26:50.000Z",
    "title": "QuickVideo: Real-Time Long Video Understanding with System Algorithm\n  Co-Design",
    "summary": "Long-video understanding has emerged as a crucial capability in real-world\napplications such as video surveillance, meeting summarization, educational\nlecture analysis, and sports broadcasting. However, it remains computationally\nprohibitive for VideoLLMs, primarily due to two bottlenecks: 1) sequential\nvideo decoding, the process of converting the raw bit stream to RGB frames can\ntake up to a minute for hour-long video inputs, and 2) costly prefilling of up\nto several million tokens for LLM inference, resulting in high latency and\nmemory use. To address these challenges, we propose QuickVideo, a\nsystem-algorithm co-design that substantially accelerates long-video\nunderstanding to support real-time downstream applications. It comprises three\nkey innovations: QuickDecoder, a parallelized CPU-based video decoder that\nachieves 2-3 times speedup by splitting videos into keyframe-aligned intervals\nprocessed concurrently; QuickPrefill, a memory-efficient prefilling method\nusing KV-cache pruning to support more frames with less GPU memory; and an\noverlapping scheme that overlaps CPU video decoding with GPU inference.\nTogether, these components infernece time reduce by a minute on long video\ninputs, enabling scalable, high-quality video understanding even on limited\nhardware. Experiments show that QuickVideo generalizes across durations and\nsampling rates, making long video processing feasible in practice.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16175.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62567c86d444a9b5a0ec51c1",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62567c86d444a9b5a0ec51c1/1vXJf2uGztPcXpkwyTBr6.png",
      "fullname": "Dongfu Jiang",
      "name": "DongfuJiang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 22
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.17022",
      "authors": [
        {
          "_id": "682ffa9a6e906040a3bb7160",
          "name": "Chengqi Duan",
          "hidden": false
        },
        {
          "_id": "682ffa9a6e906040a3bb7161",
          "name": "Rongyao Fang",
          "hidden": false
        },
        {
          "_id": "682ffa9a6e906040a3bb7162",
          "name": "Yuqing Wang",
          "hidden": false
        },
        {
          "_id": "682ffa9a6e906040a3bb7163",
          "name": "Kun Wang",
          "hidden": false
        },
        {
          "_id": "682ffa9a6e906040a3bb7164",
          "name": "Linjiang Huang",
          "hidden": false
        },
        {
          "_id": "682ffa9a6e906040a3bb7165",
          "name": "Xingyu Zeng",
          "hidden": false
        },
        {
          "_id": "682ffa9a6e906040a3bb7166",
          "name": "Hongsheng Li",
          "hidden": false
        },
        {
          "_id": "682ffa9a6e906040a3bb7167",
          "name": "Xihui Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T17:59:58.000Z",
      "submittedOnDailyAt": "2025-05-23T03:06:29.578Z",
      "title": "GoT-R1 : La libération de la capacité théorique de génération de vision pour MLLM est réalisée par l'apprentissage par répétition.",
      "submittedOnDailyBy": {
        "_id": "64a2b496e2e19de17db7de65",
        "avatarUrl": "/avatars/241448ca487833d6cc5d57bb1fdb6ee5.svg",
        "isPro": false,
        "fullname": "Duan Chengqi",
        "user": "gogoduan",
        "type": "user"
      },
      "summary": "Les modèles de génération visuelle ont connu un développement impressionnant pour générer des images réalistes à partir de prompts textuels, mais lorsqu'ils traitent de prompts complexes qui spécifient plusieurs objets et relations spatiales et propriétés précises, ils rencontrent des difficultés. Pour traiter efficacement ces prompts, il est nécessaire une logique claire tant dans le contenu littéraire que dans la disposition spatiale. Nous présentons un cadre d'entraînement pour renforcer la logique dans la génération visuelle, ce qui améliore la génération d'images. Ce cadre est basé sur l'approche de la chaîne de pensée pour la génération (CoT), permettant une logique littéraire et spatiale dans les prompts complexes. Pour y parvenir, nous proposons un cadre de récompenses multi-score en deux étapes utilisant MLLM pour évaluer le processus de logique et le résultat final. De cette manière, on peut soutenir efficacement toute la chaîne de génération. Le système de récompenses évalue de manière intégrée la cohérence littéraire, la précision spatiale et la qualité visuelle. Les résultats des expérimentations montrent une amélioration notable dans les tâches de composition qui incluent des relations spatiales et des propriétés précises, comme observé sur le benchmark T2I-CompBench. GoT-R1 a réussi notablement à la capacité de logique complexe, permettant que le processus de génération visuel se dirige vers la création d'images de haut niveau. Nous publions le code et les modèles pré-entraînés sur https://github.com/gogoduan/GoT-R1 pour encourager futures recherches.",
      "upvotes": 21,
      "discussionId": "682ffa9b6e906040a3bb71ba",
      "githubRepo": "https://github.com/gogoduan/GoT-R1",
      "ai_summary": "GoT-R1 enhances visual generation by using reinforcement learning to improve semantic-spatial reasoning, outperforming existing models on complex compositional tasks.",
      "ai_keywords": [
        "reinforcement learning",
        "Generation Chain-of-Thought",
        "MLLMs",
        "dual-stage multi-dimensional reward framework",
        "semantic alignment",
        "spatial accuracy",
        "visual quality",
        "T2I-CompBench"
      ]
    },
    "publishedAt": "2025-05-22T13:59:58.000Z",
    "title": "GoT-R1: Unleashing Reasoning Capability of MLLM for Visual Generation\n  with Reinforcement Learning",
    "summary": "Visual generation models have made remarkable progress in creating realistic\nimages from text prompts, yet struggle with complex prompts that specify\nmultiple objects with precise spatial relationships and attributes. Effective\nhandling of such prompts requires explicit reasoning about the semantic content\nand spatial layout. We present GoT-R1, a framework that applies reinforcement\nlearning to enhance semantic-spatial reasoning in visual generation. Building\nupon the Generation Chain-of-Thought approach, GoT-R1 enables models to\nautonomously discover effective reasoning strategies beyond predefined\ntemplates through carefully designed reinforcement learning. To achieve this,\nwe propose a dual-stage multi-dimensional reward framework that leverages MLLMs\nto evaluate both the reasoning process and final output, enabling effective\nsupervision across the entire generation pipeline. The reward system assesses\nsemantic alignment, spatial accuracy, and visual quality in a unified approach.\nExperimental results demonstrate significant improvements on T2I-CompBench\nbenchmark, particularly in compositional tasks involving precise spatial\nrelationships and attribute binding. GoT-R1 advances the state-of-the-art in\nimage generation by successfully transferring sophisticated reasoning\ncapabilities to the visual generation domain. To facilitate future research, we\nmake our code and pretrained models publicly available at\nhttps://github.com/gogoduan/GoT-R1.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17022.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64a2b496e2e19de17db7de65",
      "avatarUrl": "/avatars/241448ca487833d6cc5d57bb1fdb6ee5.svg",
      "fullname": "Duan Chengqi",
      "name": "gogoduan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.16933",
      "authors": [
        {
          "_id": "682fe37bb998c9f79463b563",
          "name": "Zebin You",
          "hidden": false
        },
        {
          "_id": "682fe37bb998c9f79463b564",
          "name": "Shen Nie",
          "hidden": false
        },
        {
          "_id": "682fe37bb998c9f79463b565",
          "name": "Xiaolu Zhang",
          "hidden": false
        },
        {
          "_id": "682fe37bb998c9f79463b566",
          "name": "Jun Hu",
          "hidden": false
        },
        {
          "_id": "682fe37bb998c9f79463b567",
          "name": "Jun Zhou",
          "hidden": false
        },
        {
          "_id": "682fe37bb998c9f79463b568",
          "name": "Zhiwu Lu",
          "hidden": false
        },
        {
          "_id": "682fe37bb998c9f79463b569",
          "name": "Ji-Rong Wen",
          "hidden": false
        },
        {
          "_id": "682fe37bb998c9f79463b56a",
          "name": "Chongxuan Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T17:23:26.000Z",
      "submittedOnDailyAt": "2025-05-23T01:26:20.847Z",
      "title": "LLaDA-V : Modèle de Diffusion de Langages de Grande Échelle et d'Entraînement Visuel des Instructions",
      "submittedOnDailyBy": {
        "_id": "624f909eac5dd186b01ac3f5",
        "avatarUrl": "/avatars/0aafdb1cbb492fda52a0303031cc6c14.svg",
        "isPro": false,
        "fullname": "Zebin You",
        "user": "yyyou",
        "type": "user"
      },
      "summary": "Dans cette étude, on présente LLaDA-V, un modèle multimodal et de langage basé sur la distribution complète. Ce modèle se distingue en s'écartant du paradigme auto-régressif dominant dans les approches actuelles du multimodal, en intégrant l'instruction visuelle d'entraînement et le modèle de distribution avec masque. Basé sur LLaDA (un modèle représentatif de distribution de langage), LLaDA-V inclut un encodeur visuel et un ensemble de connecteurs MLP, projetant des caractéristiques visuelles dans l'espace d'embedding linguistique pour faciliter un alignement efficace des modèles multimodal. À travers une recherche expérimentale, nous avons obtenu les résultats suivants intéressants : premièrement, LLaDA-V montre des résultats acceptables en comparaison avec des modèles de langage comme LLaMA3-8B et Qwen2-7B dans des tâches simples de caractères, mais meilleurs en termes de performance multimodal. Avec des données d'instruction équivalentes, LLaDA-V présente une compétence élevée dans des tâches multimodales par rapport à LLaMA3-V et est adapté aux échelles de données. De plus, elle réduit la différence de performance avec Qwen2-VL, démontrant l'efficacité de cette architecture. Deuxièmement, en comparant la combinaison actuelle de l'auto-régressif et de la distribution complète de MLLM, LLaDA-V atteint les meilleurs résultats en compréhension multimodale. Nos résultats montrent que les modèles de distribution de langage sont appropriés dans des contextes multimodal et devraient être considérés dans futures recherches. La page du projet et le code sont disponibles sur l'URL fournie.",
      "upvotes": 20,
      "discussionId": "682fe37cb998c9f79463b5ae",
      "ai_summary": "A diffusion-based Multimodal Large Language Model (LLaDA-V) with integrated visual instruction tuning performs competitively on multimodal tasks and outperforms existing models in multimodal understanding.",
      "ai_keywords": [
        "diffusion-based",
        "Multimodal Large Language Model (MLLM)",
        "visual instruction tuning",
        "masked diffusion models",
        "autoregressive paradigms",
        "vision encoder",
        "MLP connector",
        "language embedding space",
        "multimodal performance",
        "LLaDA",
        "LLaMA3-8B",
        "Qwen2-7B",
        "LLaMA3-V",
        "Qwen2-VL",
        "multimodal understanding",
        "hybrid autoregressive-diffusion"
      ]
    },
    "publishedAt": "2025-05-22T13:23:26.000Z",
    "title": "LLaDA-V: Large Language Diffusion Models with Visual Instruction Tuning",
    "summary": "In this work, we introduce LLaDA-V, a purely diffusion-based Multimodal Large\nLanguage Model (MLLM) that integrates visual instruction tuning with masked\ndiffusion models, representing a departure from the autoregressive paradigms\ndominant in current multimodal approaches. Built upon LLaDA, a representative\nlarge language diffusion model, LLaDA-V incorporates a vision encoder and MLP\nconnector that projects visual features into the language embedding space,\nenabling effective multimodal alignment. Our empirical investigation reveals\nseveral intriguing results: First, LLaDA-V demonstrates promising multimodal\nperformance despite its language model being weaker on purely textual tasks\nthan counterparts like LLaMA3-8B and Qwen2-7B. When trained on the same\ninstruction data, LLaDA-V is highly competitive to LLaMA3-V across multimodal\ntasks with better data scalability. It also narrows the performance gap to\nQwen2-VL, suggesting the effectiveness of its architecture for multimodal\ntasks. Second, LLaDA-V achieves state-of-the-art performance in multimodal\nunderstanding compared to existing hybrid autoregressive-diffusion and purely\ndiffusion-based MLLMs. Our findings suggest that large language diffusion\nmodels show promise in multimodal contexts and warrant further investigation in\nfuture research. Project page and codes:\nhttps://ml-gsai.github.io/LLaDA-V-demo/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16933.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "624f909eac5dd186b01ac3f5",
      "avatarUrl": "/avatars/0aafdb1cbb492fda52a0303031cc6c14.svg",
      "fullname": "Zebin You",
      "name": "yyyou",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.16925",
      "authors": [
        {
          "_id": "68302d01b85e3ed6a61e6476",
          "user": {
            "_id": "67a33f0e36fccbd55d6e8f7f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67a33f0e36fccbd55d6e8f7f/o_2UU5bxtnPmPdKo-jg7x.png",
            "isPro": false,
            "fullname": "Igor Udovichenko",
            "user": "i-udovichenko",
            "type": "user"
          },
          "name": "Igor Udovichenko",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-23T08:16:34.263Z",
          "hidden": false
        },
        {
          "_id": "68302d01b85e3ed6a61e6477",
          "name": "Olivier Croissant",
          "hidden": false
        },
        {
          "_id": "68302d01b85e3ed6a61e6478",
          "name": "Anita Toleutaeva",
          "hidden": false
        },
        {
          "_id": "68302d01b85e3ed6a61e6479",
          "name": "Evgeny Burnaev",
          "hidden": false
        },
        {
          "_id": "68302d01b85e3ed6a61e647a",
          "name": "Alexander Korotin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T17:18:07.000Z",
      "submittedOnDailyAt": "2025-05-23T06:41:43.892Z",
      "title": "Risque d'évasion de l'apprentissage par renforcement avec la perte d'état épisoscopique",
      "submittedOnDailyBy": {
        "_id": "67a33f0e36fccbd55d6e8f7f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67a33f0e36fccbd55d6e8f7f/o_2UU5bxtnPmPdKo-jg7x.png",
        "isPro": false,
        "fullname": "Igor Udovichenko",
        "user": "i-udovichenko",
        "type": "user"
      },
      "summary": "L'apprentissage par récompense de déviation de risque est appliqué dans de nombreux domaines de haut risque. Au contraire du réapprentissage par récompense classique, l'objectif n'est pas de maximiser l'utilité mais de minimiser le risque en sélectionnant des politiques, et parfois permet temporairement l'utilité attendue. Cette tendance peut être modélisée par la théorie de l'utilité. Nous nous concentrons sur un cas spécial de la fonction d'utilité exponentielle, de laquelle nous obtenons l'équation de Bellman et pouvons appliquer des algorithmes d'apprentissage par récompense sans modifications significatives. Cependant, ces méthodes souffrent d'instabilité numérique en raison de la nécessité de calculs exponentiels. Pour résoudre ce problème, nous introduisons une fonction de perte numériquement stable et mathématiquement justifiée basée sur la divergence d'Itakura-Saito dans l'apprentissage des fonctions de valeur d'état et d'action. Elle a été évaluée théoriquement et expérimentalement en comparaison avec ses alternatives. Dans la section des expériences, plusieurs scénarios financiers ont été examinés, y compris certains où une solution analytique était connue, démontrant que la fonction de perte proposée est supérieure aux alternatives.",
      "upvotes": 16,
      "discussionId": "68302d02b85e3ed6a61e64db",
      "ai_summary": "Proposed Itakura-Saito divergence-based loss function enhances numerical stability in risk-averse reinforcement learning using exponential utility functions.",
      "ai_keywords": [
        "reinforcement learning",
        "risk-averse",
        "utility theory",
        "exponential utility function",
        "Bellman equations",
        "numerical instability",
        "Itakura-Saito divergence",
        "state-value functions",
        "action-value functions"
      ]
    },
    "publishedAt": "2025-05-22T13:18:07.000Z",
    "title": "Risk-Averse Reinforcement Learning with Itakura-Saito Loss",
    "summary": "Risk-averse reinforcement learning finds application in various high-stakes\nfields. Unlike classical reinforcement learning, which aims to maximize\nexpected returns, risk-averse agents choose policies that minimize risk,\noccasionally sacrificing expected value. These preferences can be framed\nthrough utility theory. We focus on the specific case of the exponential\nutility function, where we can derive the Bellman equations and employ various\nreinforcement learning algorithms with few modifications. However, these\nmethods suffer from numerical instability due to the need for exponent\ncomputation throughout the process. To address this, we introduce a numerically\nstable and mathematically sound loss function based on the Itakura-Saito\ndivergence for learning state-value and action-value functions. We evaluate our\nproposed loss function against established alternatives, both theoretically and\nempirically. In the experimental section, we explore multiple financial\nscenarios, some with known analytical solutions, and show that our loss\nfunction outperforms the alternatives.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16925.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67a33f0e36fccbd55d6e8f7f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67a33f0e36fccbd55d6e8f7f/o_2UU5bxtnPmPdKo-jg7x.png",
      "fullname": "Igor Udovichenko",
      "name": "i-udovichenko",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.15270",
      "authors": [
        {
          "_id": "682e907a24b2bb08885b94dc",
          "user": {
            "_id": "682e8e6d007cd8c2f2cd0afd",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/az0B5USOPlLR3vq872JeN.png",
            "isPro": false,
            "fullname": "Chenyu Zheng",
            "user": "ChenyuZheng",
            "type": "user"
          },
          "name": "Chenyu Zheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:16:10.939Z",
          "hidden": false
        },
        {
          "_id": "682e907a24b2bb08885b94dd",
          "name": "Xinyu Zhang",
          "hidden": false
        },
        {
          "_id": "682e907a24b2bb08885b94de",
          "name": "Rongzhen Wang",
          "hidden": false
        },
        {
          "_id": "682e907a24b2bb08885b94df",
          "name": "Wei Huang",
          "hidden": false
        },
        {
          "_id": "682e907a24b2bb08885b94e0",
          "name": "Zhi Tian",
          "hidden": false
        },
        {
          "_id": "682e907a24b2bb08885b94e1",
          "name": "Weilin Huang",
          "hidden": false
        },
        {
          "_id": "682e907a24b2bb08885b94e2",
          "name": "Jun Zhu",
          "hidden": false
        },
        {
          "_id": "682e907a24b2bb08885b94e3",
          "name": "Chongxuan Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T08:49:03.000Z",
      "submittedOnDailyAt": "2025-05-23T00:59:19.168Z",
      "title": "Utilisant le μP pour élargir de manière efficace les Transformers de Diffusion",
      "submittedOnDailyBy": {
        "_id": "682e8e6d007cd8c2f2cd0afd",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/az0B5USOPlLR3vq872JeN.png",
        "isPro": false,
        "fullname": "Chenyu Zheng",
        "user": "ChenyuZheng",
        "type": "user"
      },
      "summary": "Les Transformers de Diffusion apparaissent comme la base des modèles générateurs visuels, cependant, leur scalabilité est limitée par le coût élevé associé à l'ajustement des paramètres à grande échelle. Récemment, la Maximal Update Parametrization (muP) a été proposée par la bergevent frange-mineur, permettant un ajustement de paramètres stable depuis des modèles petits jusqu'à des modèles de langage grands, réduisant significativement les coûts d'ajustement. Cependant, on ne sait pas si la version de muP de la bergevent frange-mineur peut être appliquée aux Transformers de Diffusion. Dans cet article, la muP est généralisée pour les Transformers de Diffusion et son efficacité est testée à travers des expériences à grande échelle. Tout d'abord, on démontre rigoureusement que les modèles principaux de Transformers de Diffusion (DiT, U-ViT, PixArt-alpha, MMDiT) avec muP coïncident avec la muP de la bergevent frange-mineur et que les méthodes existantes de muP peuvent être directement appliquées. Ces résultats sont utilisés pour tester la forte applicabilité de l'ajustement de paramètres de DiT-muP. En particulier, on démontre que DiT-XL-2-muP converge plus rapidement en apprentissage que l'original DiT-XL-2, atteignant un rendement 2,9 fois plus rapide. Enfin, on vérifie l'efficacité de la muP dans la génération d'images à partir du texte pour PixArt-alpha et MMDiT. PixArt-alpha passe de 0,04B à 0,61B, tandis que MMDiT passe de 0,18B à 18B. Les deux modèles avec muP dépassent les modèles de référence avec un coût d'ajustement réduit. Il suffit de 5,5% du coût d'un seul entraînement pour PixArt-alpha et 3% du coût d'un expert pour MMDiT-18B. Ces résultats établissent la muP comme un cadre rationnel et efficace pour la scalabilité des Transformers de Diffusion.",
      "upvotes": 16,
      "discussionId": "682e907b24b2bb08885b952c",
      "projectPage": "https://github.com/ML-GSAI/Scaling-Diffusion-Transformers-muP",
      "githubRepo": "https://github.com/ML-GSAI/Scaling-Diffusion-Transformers-muP",
      "ai_summary": "Maximal Update Parametrization (μP) is extended to diffusion Transformers, demonstrating efficient hyperparameter transferability and reduced tuning costs across various models and tasks.",
      "ai_keywords": [
        "Diffusion Transformers",
        "Maximal Update Parametrization",
        "μP",
        "hyperparameter tuning",
        "DiT",
        "U-ViT",
        "PixArt-α",
        "MMDiT",
        "text-to-image generation",
        "transfer learning",
        "convergence",
        "training run"
      ]
    },
    "publishedAt": "2025-05-21T04:49:03.000Z",
    "title": "Scaling Diffusion Transformers Efficiently via μP",
    "summary": "Diffusion Transformers have emerged as the foundation for vision generative\nmodels, but their scalability is limited by the high cost of hyperparameter\n(HP) tuning at large scales. Recently, Maximal Update Parametrization (muP)\nwas proposed for vanilla Transformers, which enables stable HP transfer from\nsmall to large language models, and dramatically reduces tuning costs. However,\nit remains unclear whether muP of vanilla Transformers extends to diffusion\nTransformers, which differ architecturally and objectively. In this work, we\ngeneralize standard muP to diffusion Transformers and validate its\neffectiveness through large-scale experiments. First, we rigorously prove that\nmuP of mainstream diffusion Transformers, including DiT, U-ViT,\nPixArt-alpha, and MMDiT, aligns with that of the vanilla Transformer,\nenabling the direct application of existing muP methodologies. Leveraging\nthis result, we systematically demonstrate that DiT-muP enjoys robust HP\ntransferability. Notably, DiT-XL-2-muP with transferred learning rate\nachieves 2.9 times faster convergence than the original DiT-XL-2. Finally, we\nvalidate the effectiveness of muP on text-to-image generation by scaling\nPixArt-alpha from 0.04B to 0.61B and MMDiT from 0.18B to 18B. In both cases,\nmodels under muP outperform their respective baselines while requiring small\ntuning cost, only 5.5% of one training run for PixArt-alpha and 3% of\nconsumption by human experts for MMDiT-18B. These results establish muP as a\nprincipled and efficient framework for scaling diffusion Transformers.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15270.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "682e8e6d007cd8c2f2cd0afd",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/az0B5USOPlLR3vq872JeN.png",
      "fullname": "Chenyu Zheng",
      "name": "ChenyuZheng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.14604",
      "authors": [
        {
          "_id": "682f34b52b9fdc24ae9de371",
          "name": "Haoran Zhao",
          "hidden": false
        },
        {
          "_id": "682f34b52b9fdc24ae9de372",
          "name": "Yuchen Yan",
          "hidden": false
        },
        {
          "_id": "682f34b52b9fdc24ae9de373",
          "name": "Yongliang Shen",
          "hidden": false
        },
        {
          "_id": "682f34b52b9fdc24ae9de374",
          "name": "Haolei Xu",
          "hidden": false
        },
        {
          "_id": "682f34b52b9fdc24ae9de375",
          "name": "Wenqi Zhang",
          "hidden": false
        },
        {
          "_id": "682f34b52b9fdc24ae9de376",
          "name": "Kaitao Song",
          "hidden": false
        },
        {
          "_id": "682f34b52b9fdc24ae9de377",
          "name": "Jian Shao",
          "hidden": false
        },
        {
          "_id": "682f34b52b9fdc24ae9de378",
          "name": "Weiming Lu",
          "hidden": false
        },
        {
          "_id": "682f34b52b9fdc24ae9de379",
          "name": "Jun Xiao",
          "hidden": false
        },
        {
          "_id": "682f34b52b9fdc24ae9de37a",
          "name": "Yueting Zhuang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-20T16:53:40.000Z",
      "submittedOnDailyAt": "2025-05-23T03:36:02.584Z",
      "title": "Les modèles de langage grands (LLMs) proposent de réduire la pression de pensée excessive grâce à leurs propres limitations, permettant ainsi qu'ils fonctionnent de manière plus libre.",
      "submittedOnDailyBy": {
        "_id": "5e1058e9fcf41d740b69966d",
        "avatarUrl": "/avatars/ce74839ba871f2b54313a670a233ba82.svg",
        "isPro": false,
        "fullname": "Yongliang Shen",
        "user": "tricktreat",
        "type": "user"
      },
      "summary": "Les modèles logiques de discours (LRMs), tels que OpenAI ou1 et DeepSeek-R1, sont des exemples. Ces modèles augmentent la longueur de la pensée pour atteindre des performances exceptionnelles dans diverses tâches et améliorent significativement leur capacité de raisonnement. Cependant, cette amélioration de performance est affectée par un accroissement significatif de raisons redondantes et par des surchargements computationnelles et des problèmes de suppositions excessives dans le processus de génération. La plupart des méthodes actuelles dépendent d'une intervention externe pour résoudre ce problème de suppositions excessives. Cependant, ces méthodologies généralement dépendent de la régulation externe. Dans cet article, nous proposons un nouveau cadre qui permet aux modèles de réguler automatiquement le processus de raisonnement. Ce cadre évite la dépendance à la régulation externe et permet aux modèles de contrôler le processus de raisonnement de manière autonome pour résoudre le problème de suppositions excessives. En utilisant des critères de mesure pour identifier les suppositions excessives basées sur des réponses standard, nous concevons un méthode systématique pour identifier les raisons redondantes et nous identifions et nous entraînons des signaux d'entraînement pour quantifier précisément les étapes innecessaires dans le tracé de raisonnement. Avec cette base, nous développons une stratégie complète pour la construction de données avec des raisons adaptatives et introduisons une structure innovante de \"frein\" pour que le modèle apprenne à s'arrêter au moment adéquat, sans nécessité de restrictions. Les expériences sur les benchmarks mathématiques (AIME, AMC, MATH500, GSM8K) montrent que ce méthode réduit le consommation de tokens d'au moins 60 % et maintient une précision relativement élevée sans restrictions.",
      "upvotes": 16,
      "discussionId": "682f34b62b9fdc24ae9de3be",
      "ai_summary": "A novel Self-Braking Tuning framework reduces overthinking and unnecessary computational overhead in large reasoning models by enabling the model to self-regulate its reasoning process.",
      "ai_keywords": [
        "large reasoning models",
        "self-braking tuning",
        "overthinking",
        "reasoning capabilities",
        "mathematical benchmarks",
        "adaptive reasoning lengths",
        "braking prompt mechanism"
      ]
    },
    "publishedAt": "2025-05-20T12:53:40.000Z",
    "title": "Let LLMs Break Free from Overthinking via Self-Braking Tuning",
    "summary": "Large reasoning models (LRMs), such as OpenAI o1 and DeepSeek-R1, have\nsignificantly enhanced their reasoning capabilities by generating longer chains\nof thought, demonstrating outstanding performance across a variety of tasks.\nHowever, this performance gain comes at the cost of a substantial increase in\nredundant reasoning during the generation process, leading to high\ncomputational overhead and exacerbating the issue of overthinking. Although\nnumerous existing approaches aim to address the problem of overthinking, they\noften rely on external interventions. In this paper, we propose a novel\nframework, Self-Braking Tuning (SBT), which tackles overthinking from the\nperspective of allowing the model to regulate its own reasoning process, thus\neliminating the reliance on external control mechanisms. We construct a set of\noverthinking identification metrics based on standard answers and design a\nsystematic method to detect redundant reasoning. This method accurately\nidentifies unnecessary steps within the reasoning trajectory and generates\ntraining signals for learning self-regulation behaviors. Building on this\nfoundation, we develop a complete strategy for constructing data with adaptive\nreasoning lengths and introduce an innovative braking prompt mechanism that\nenables the model to naturally learn when to terminate reasoning at an\nappropriate point. Experiments across mathematical benchmarks (AIME, AMC,\nMATH500, GSM8K) demonstrate that our method reduces token consumption by up to\n60% while maintaining comparable accuracy to unconstrained models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14604.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5e1058e9fcf41d740b69966d",
      "avatarUrl": "/avatars/ce74839ba871f2b54313a670a233ba82.svg",
      "fullname": "Yongliang Shen",
      "name": "tricktreat",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 23
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.16916",
      "authors": [
        {
          "_id": "682fdcfc2c98b5e99660561e",
          "name": "Xuankun Rong",
          "hidden": false
        },
        {
          "_id": "682fdcfc2c98b5e99660561f",
          "name": "Wenke Huang",
          "hidden": false
        },
        {
          "_id": "682fdcfc2c98b5e996605620",
          "name": "Jian Liang",
          "hidden": false
        },
        {
          "_id": "682fdcfc2c98b5e996605621",
          "name": "Jinhe Bi",
          "hidden": false
        },
        {
          "_id": "682fdcfc2c98b5e996605622",
          "name": "Xun Xiao",
          "hidden": false
        },
        {
          "_id": "682fdcfc2c98b5e996605623",
          "name": "Yiming Li",
          "hidden": false
        },
        {
          "_id": "682fdcfc2c98b5e996605624",
          "name": "Bo Du",
          "hidden": false
        },
        {
          "_id": "682fdcfc2c98b5e996605625",
          "name": "Mang Ye",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T17:11:58.000Z",
      "submittedOnDailyAt": "2025-05-23T00:58:25.177Z",
      "title": "Post-processing de multi-modele avec pistes externes",
      "submittedOnDailyBy": {
        "_id": "66c014820836dd7a55be3fde",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/5OuqAxEFq1Ny2CHbl9HOm.jpeg",
        "isPro": false,
        "fullname": "Xuankun Rong",
        "user": "XuankunRong",
        "type": "user"
      },
      "summary": "Les modèles de langage multimodal de DeepMind (MLLMs) sont utilisés pour des tâches générales en utilisant des ensembles de données fournis par l'utilisateur dans la configuration de FINCH (Fine-Tuning as a Service). Cette flexibilité nécessite un minimum d'effort pour insérer des backdoors dans les MLLMs, ce qui augmente les risques de sécurité. Dans cet article, nous observons que les backdoors Trigger génèrent une concentration atypique d'attention dans des zones de la réseau qui ne devraient pas recevoir de attention et détruisent le traitement multimodal. En se basant sur cette observation, nous proposons un cadre de filtrage de données \"Believe Your Eyes (BYE)\" qui utilise des motifs d'entropie d'attention pour identifier et filtrer les échantillons avec des backdoors. BYE fonctionne par trois étapes : 1) Extraction de cartes d'attention en utilisant des modèles de FINCH, 2) Calcul de scores d'entropie et profilage de couches sensibles par séparation de modalités, et 3) Clustering non-subjonctif pour éliminer les échantillons suspects. A différence de d'autres mécanismes de protection, BYE ne nécessite pas de nettoyage subjonctif, d'étiquettes auxiliaires ou de modifications du modèle. La validité de BYE a été vérifiée dans des expériences avec divers ensembles de données, modèles et types de backdoors : il maintient le rendement des tâches propres tout en atteignant un pourcentage d'échec d'attaque proche de 0, offrant ainsi une solution généralisable et puissante pour atténuer le risque de backdoors dans les MLLMs.",
      "upvotes": 14,
      "discussionId": "682fdcfd2c98b5e99660567b",
      "ai_summary": "A novel defense framework, Believe Your Eyes (BYE), identifies and filters backdoor samples in fine-tuned multimodal large language models by analyzing attention entropy patterns, preventing trigger activation without requiring additional labels or model changes.",
      "ai_keywords": [
        "Multimodal Large Language Models",
        "fine-tuning-as-a-service",
        "FTaaS",
        "backdoors",
        "attention collapse",
        "attention entropy",
        "self-supervised",
        "bimodal separation",
        "unsupervised clustering",
        "clean-task performance"
      ]
    },
    "publishedAt": "2025-05-22T13:11:58.000Z",
    "title": "Backdoor Cleaning without External Guidance in MLLM Fine-tuning",
    "summary": "Multimodal Large Language Models (MLLMs) are increasingly deployed in\nfine-tuning-as-a-service (FTaaS) settings, where user-submitted datasets adapt\ngeneral-purpose models to downstream tasks. This flexibility, however,\nintroduces serious security risks, as malicious fine-tuning can implant\nbackdoors into MLLMs with minimal effort. In this paper, we observe that\nbackdoor triggers systematically disrupt cross-modal processing by causing\nabnormal attention concentration on non-semantic regions--a phenomenon we term\nattention collapse. Based on this insight, we propose Believe Your Eyes (BYE),\na data filtering framework that leverages attention entropy patterns as\nself-supervised signals to identify and filter backdoor samples. BYE operates\nvia a three-stage pipeline: (1) extracting attention maps using the fine-tuned\nmodel, (2) computing entropy scores and profiling sensitive layers via bimodal\nseparation, and (3) performing unsupervised clustering to remove suspicious\nsamples. Unlike prior defenses, BYE equires no clean supervision, auxiliary\nlabels, or model modifications. Extensive experiments across various datasets,\nmodels, and diverse trigger types validate BYE's effectiveness: it achieves\nnear-zero attack success rates while maintaining clean-task performance,\noffering a robust and generalizable solution against backdoor threats in MLLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16916.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66c014820836dd7a55be3fde",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/5OuqAxEFq1Ny2CHbl9HOm.jpeg",
      "fullname": "Xuankun Rong",
      "name": "XuankunRong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.14684",
      "authors": [
        {
          "_id": "682f474c9ee0bb0cc953b885",
          "name": "Haolei Xu",
          "hidden": false
        },
        {
          "_id": "682f474c9ee0bb0cc953b886",
          "name": "Yuchen Yan",
          "hidden": false
        },
        {
          "_id": "682f474c9ee0bb0cc953b887",
          "name": "Yongliang Shen",
          "hidden": false
        },
        {
          "_id": "682f474c9ee0bb0cc953b888",
          "name": "Wenqi Zhang",
          "hidden": false
        },
        {
          "_id": "682f474c9ee0bb0cc953b889",
          "name": "Guiyang Hou",
          "hidden": false
        },
        {
          "_id": "682f474c9ee0bb0cc953b88a",
          "name": "Shengpei Jiang",
          "hidden": false
        },
        {
          "_id": "682f474c9ee0bb0cc953b88b",
          "name": "Kaitao Song",
          "hidden": false
        },
        {
          "_id": "682f474c9ee0bb0cc953b88c",
          "name": "Weiming Lu",
          "hidden": false
        },
        {
          "_id": "682f474c9ee0bb0cc953b88d",
          "name": "Jun Xiao",
          "hidden": false
        },
        {
          "_id": "682f474c9ee0bb0cc953b88e",
          "name": "Yueting Zhuang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-20T17:59:31.000Z",
      "submittedOnDailyAt": "2025-05-23T03:43:43.765Z",
      "title": "Fermer l'espace pour créer un saut de pensée qui améliore le Tuning de la Chaîne de Pensée.",
      "submittedOnDailyBy": {
        "_id": "64098738342c26884c792c93",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64098738342c26884c792c93/SxBUd-wLrl-PjQsrVYJte.jpeg",
        "isPro": false,
        "fullname": "Yuchen Yan",
        "user": "yanyc",
        "type": "user"
      },
      "summary": "Les modèles de langage grands (LLMs) ont réalisé des progrès impressionnants dans des tâches mathématiques en utilisant la logique de CoT (Chain-of-Thought). Cependant, les ensembles de données de CoT actuels de mathématiques permettent aux experts de sauter des étapes intermédiaires en utilisant des \"sauts de pensée\" (Thought Leaps), ce qui a un impact négatif sur l'apprentissage et la généralisation du modèle. Nous proposons la tâche de CoT Thought Leap Bridge, avec l'objectif de détecter automatiquement ces sauts et de générer des étapes de logique intermédiaires insuffisantes pour restaurer la complétude et la cohérence de la CoT. Pour promouvoir cela, nous basons notre travail sur l'ensemble de données structuré ScaleQuestMath et construisons un ensemble de données d'entraînement spécialisé, entraînant le CoT-Bridge. À travers des expériences détaillées dans des cadres de référence de logique mathématique, nous observons que un modèle fin avec les données ajustées surpasse les modèles entraînés avec l'ensemble original, avec une amélioration de 5,87% sur NuminaMath. Notre approche optimise efficacement la combinaison de données (+3,02%), offre un meilleur point de départ pour l'apprentissage par réflexion et fonctionne en concert avec les méthodes d'optimisation existantes, en utilisant des modules de plug-in et play-in. De plus, le CoT-Bridge améliore la généralisation dans des tâches de logique mathématique dans des domaines de perturbation et renforce la complétude de la logique, démontrant une large gamme de bénéfices.",
      "upvotes": 14,
      "discussionId": "682f474d9ee0bb0cc953b8c7",
      "projectPage": "https://zju-real.github.io/CoT-Bridge/",
      "githubRepo": "https://github.com/ZJU-REAL/Mind-the-Gap",
      "ai_summary": "A model for detecting and generating missing intermediate steps in mathematical Chain-of-Thought reasoning improves performance and generalization on mathematical and logical reasoning tasks.",
      "ai_keywords": [
        "Chain-of-Thought",
        "reasoning",
        "thought leaps",
        "ScaleQM+",
        "ScaleQuestMath",
        "NuminaMath",
        "distilled data",
        "reinforcement learning",
        "generalization",
        "logical reasoning tasks"
      ]
    },
    "publishedAt": "2025-05-20T13:59:31.000Z",
    "title": "Mind the Gap: Bridging Thought Leap for Improved Chain-of-Thought Tuning",
    "summary": "Large language models (LLMs) have achieved remarkable progress on\nmathematical tasks through Chain-of-Thought (CoT) reasoning. However, existing\nmathematical CoT datasets often suffer from Thought Leaps due to experts\nomitting intermediate steps, which negatively impacts model learning and\ngeneralization. We propose the CoT Thought Leap Bridge Task, which aims to\nautomatically detect leaps and generate missing intermediate reasoning steps to\nrestore the completeness and coherence of CoT. To facilitate this, we\nconstructed a specialized training dataset called ScaleQM+, based on the\nstructured ScaleQuestMath dataset, and trained CoT-Bridge to bridge thought\nleaps. Through comprehensive experiments on mathematical reasoning benchmarks,\nwe demonstrate that models fine-tuned on bridged datasets consistently\noutperform those trained on original datasets, with improvements of up to\n+5.87% on NuminaMath. Our approach effectively enhances distilled data (+3.02%)\nand provides better starting points for reinforcement learning (+3.1%),\nfunctioning as a plug-and-play module compatible with existing optimization\ntechniques. Furthermore, CoT-Bridge demonstrate improved generalization to\nout-of-domain logical reasoning tasks, confirming that enhancing reasoning\ncompleteness yields broadly applicable benefits.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14684.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64098738342c26884c792c93",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64098738342c26884c792c93/SxBUd-wLrl-PjQsrVYJte.jpeg",
      "fullname": "Yuchen Yan",
      "name": "yanyc",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.16990",
      "authors": [
        {
          "_id": "682fdd034640a9db4d1cc04d",
          "name": "Runpeng Yu",
          "hidden": false
        },
        {
          "_id": "682fdd034640a9db4d1cc04e",
          "name": "Xinyin Ma",
          "hidden": false
        },
        {
          "_id": "682fdd034640a9db4d1cc04f",
          "name": "Xinchao Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T17:55:04.000Z",
      "submittedOnDailyAt": "2025-05-23T01:04:28.755Z",
      "title": "DINPL : Vérification du Modèle de Dispersion Distribuée et du Modèle de Langue en Parallélisme",
      "submittedOnDailyBy": {
        "_id": "635364b3c41f548fe39db945",
        "avatarUrl": "/avatars/ad1916bbfabca0b6651c8eabacc5eba8.svg",
        "isPro": false,
        "fullname": "Runpeng Yu",
        "user": "rp-yu",
        "type": "user"
      },
      "summary": "Dans cette étude, on propose le premier modèle de langue de DMLLM (Modèle de Langue de Distribution de Diépension) appelé Dimple. L'approche unique d'entraînement de la distribution de diépension rencontre des problèmes graves tels que l'instabilité de l'entraînement, le rendement optimal et les biais de longueur. Pour faire face à ces problèmes, un nouveau schéma d'entraînement a été conçu qui combine un premier pas automatique de régression et un second pas de diépension. Avec cette approche, le modèle Dimple-7B a été construit et entraîné en utilisant une pipeline d'entraînement similaire à LLaVA-NEXT. Dimple-7B a montré un rendement supérieur à LLaVA-NEXT d'un 3,9%, démontrant que DMLLM peut atteindre le même rendement que les modèles de régression automatique. Pour améliorer l'efficacité de l'inférence, on propose un schéma de décodage appelé \"décodage en confiance\", qui ajuste dynamiquement la quantité de tokens générés à chaque étape pour réduire significativement la quantité d'itérations de génération. Contrairement aux modèles de régression automatique, où la quantité d'itérations bidirectionnelles est égale à la longueur de la réponse, le décodage en confiance peut réduire la quantité d'itérations nécessaires dans Dimple jusqu'à trois fois la longueur de la réponse. De plus, la technologie de prédiction des modèles de régression automatique a été ré-implémentée pour éviter que cela affecte significativement le rendement sur de nombreux tests d'évaluation, offrant un accélération de 1,5 à 7 fois. De plus, une structure de profil permet de contrôler avec précision les réponses de Dimple, ce qui permet la génération de réponses structurées basées sur des commandes ou sur des programmations de chaînes, ainsi qu'un contrôle micro de la forme et de la longueur des réponses. En résumé, cette étude démontre la possibilité et les forces de l'DMLLM, ainsi que l'amélioration de l'efficacité de l'inférence et le contrôle des réponses. Le code et le modèle sont disponibles sur https://github.com/yu-rp/Dimple.",
      "upvotes": 12,
      "discussionId": "682fdd044640a9db4d1cc0a1",
      "githubRepo": "https://github.com/yu-rp/Dimple",
      "ai_summary": "Dimple, a Discrete Diffusion Multimodal Large Language Model, achieves performance comparable to autoregressive models through a hybrid training approach and enhances inference efficiency with confident decoding and structure priors.",
      "ai_keywords": [
        "Discrete Diffusion Multimodal Large Language Model",
        "DMLLM",
        "autoregressive phase",
        "diffusion phase",
        "confident decoding",
        "prefilling technique",
        "structure priors"
      ]
    },
    "publishedAt": "2025-05-22T13:55:04.000Z",
    "title": "Dimple: Discrete Diffusion Multimodal Large Language Model with Parallel\n  Decoding",
    "summary": "In this work, we propose Dimple, the first Discrete Diffusion Multimodal\nLarge Language Model (DMLLM). We observe that training with a purely discrete\ndiffusion approach leads to significant training instability, suboptimal\nperformance, and severe length bias issues. To address these challenges, we\ndesign a novel training paradigm that combines an initial autoregressive phase\nwith a subsequent diffusion phase. This approach yields the Dimple-7B model,\ntrained on the same dataset and using a similar training pipeline as\nLLaVA-NEXT. Dimple-7B ultimately surpasses LLaVA-NEXT in performance by 3.9%,\ndemonstrating that DMLLM can achieve performance comparable to that of\nautoregressive models. To improve inference efficiency, we propose a decoding\nstrategy termed confident decoding, which dynamically adjusts the number of\ntokens generated at each step, significantly reducing the number of generation\niterations. In autoregressive models, the number of forward iterations during\ngeneration equals the response length. With confident decoding, however, the\nnumber of iterations needed by Dimple is even only text{response\nlength}{3}. We also re-implement the prefilling technique in autoregressive\nmodels and demonstrate that it does not significantly impact performance on\nmost benchmark evaluations, while offering a speedup of 1.5x to 7x.\nAdditionally, we explore Dimple's capability to precisely control its response\nusing structure priors. These priors enable structured responses in a manner\ndistinct from instruction-based or chain-of-thought prompting, and allow\nfine-grained control over response format and length, which is difficult to\nachieve in autoregressive models. Overall, this work validates the feasibility\nand advantages of DMLLM and enhances its inference efficiency and\ncontrollability. Code and models are available at\nhttps://github.com/yu-rp/Dimple.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16990.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "635364b3c41f548fe39db945",
      "avatarUrl": "/avatars/ad1916bbfabca0b6651c8eabacc5eba8.svg",
      "fullname": "Runpeng Yu",
      "name": "rp-yu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.16864",
      "authors": [
        {
          "_id": "682fe14abafb480b9595da32",
          "name": "Yuechen Zhang",
          "hidden": false
        },
        {
          "_id": "682fe14abafb480b9595da33",
          "name": "Jinbo Xing",
          "hidden": false
        },
        {
          "_id": "682fe14abafb480b9595da34",
          "name": "Bin Xia",
          "hidden": false
        },
        {
          "_id": "682fe14abafb480b9595da35",
          "name": "Shaoteng Liu",
          "hidden": false
        },
        {
          "_id": "682fe14abafb480b9595da36",
          "name": "Bohao Peng",
          "hidden": false
        },
        {
          "_id": "682fe14abafb480b9595da37",
          "name": "Xin Tao",
          "hidden": false
        },
        {
          "_id": "682fe14abafb480b9595da38",
          "name": "Pengfei Wan",
          "hidden": false
        },
        {
          "_id": "682fe14abafb480b9595da39",
          "name": "Eric Lo",
          "hidden": false
        },
        {
          "_id": "682fe14abafb480b9595da3a",
          "name": "Jiaya Jia",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T16:21:32.000Z",
      "submittedOnDailyAt": "2025-05-23T01:22:45.264Z",
      "title": "Méthodes efficaces de génération de vidéos pour l'apprentissage non supervisé : méthode de séparation dynamique de tokens",
      "submittedOnDailyBy": {
        "_id": "6418554a0956be7233a1023e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6418554a0956be7233a1023e/9EKN0GoOpcDbvBDmAQEJf.png",
        "isPro": false,
        "fullname": "zhang yuechen",
        "user": "julianjuaner",
        "type": "user"
      },
      "summary": "Le modèle Transformer de diffusion vidéo (DiT) présente une qualité de génération impressionnante, mais sa mise en œuvre pratique est strictement limitée par des exigences computationnelles élevées. Cette inefficacité s'explique par deux problèmes principaux : la complexité quadratique de l'attention automatique par rapport à la longueur des tokens et les caractéristiques multi-échelles des modèles de diffusion. Pour résoudre ces limitations, nous proposons une architecture innovante d'inférence appelée \"Jenga\". Cette architecture combine la séparation dynamique de l'attention et la génération évolutive de la résolution. Notre approche est basée sur deux principaux insights : (1) au premier pas de débruitage, il n'est pas nécessaire d'avoir des potentiels de haute résolution, et (2) dans les pas suivants, une attention dense n'est pas nécessaire. Jenga introduit une structure d'attention par blocs qui sélectionne l'interaction entre les tokens associés de manière dynamique en utilisant une courbe de moulage dans l'espace 3D, et augmente progressivement la résolution potentielle pendant le processus de génération. Grâce aux résultats expérimentaux, Jenga atteint un accroissement significatif de la vitesse pour de nombreux modèles de diffusion vidéo récents, tout en maintenant une qualité de génération relativement élevée (accroissement de vitesse de 8,83 sur VBench avec un déclin de rendement de 0,01%). Comme un plat de fromage et de pain, Jenga permet aux modèles qui ne nécessitent pas de réentraînement de réduire drastiquement leur temps d'inférence et facilite la génération de vidéo de haute qualité sur des matériels modernes. Code : https://github.com/dvlab-research/Jenga",
      "upvotes": 11,
      "discussionId": "682fe14ebafb480b9595db1c",
      "projectPage": "https://julianjuaner.github.io/projects/jenga/",
      "githubRepo": "https://github.com/dvlab-research/Jenga",
      "ai_summary": "Jenga, a novel inference pipeline for video Diffusion Transformer models, combines dynamic attention carving and progressive resolution generation to significantly speed up video generation while maintaining high quality.",
      "ai_keywords": [
        "video Diffusion Transformer (DiT)",
        "self-attention",
        "diffusion models",
        "dynamic attention carving",
        "progressive resolution generation",
        "block-wise attention",
        "3D space-filling curves"
      ]
    },
    "publishedAt": "2025-05-22T12:21:32.000Z",
    "title": "Training-Free Efficient Video Generation via Dynamic Token Carving",
    "summary": "Despite the remarkable generation quality of video Diffusion Transformer\n(DiT) models, their practical deployment is severely hindered by extensive\ncomputational requirements. This inefficiency stems from two key challenges:\nthe quadratic complexity of self-attention with respect to token length and the\nmulti-step nature of diffusion models. To address these limitations, we present\nJenga, a novel inference pipeline that combines dynamic attention carving with\nprogressive resolution generation. Our approach leverages two key insights: (1)\nearly denoising steps do not require high-resolution latents, and (2) later\nsteps do not require dense attention. Jenga introduces a block-wise attention\nmechanism that dynamically selects relevant token interactions using 3D\nspace-filling curves, alongside a progressive resolution strategy that\ngradually increases latent resolution during generation. Experimental results\ndemonstrate that Jenga achieves substantial speedups across multiple\nstate-of-the-art video diffusion models while maintaining comparable generation\nquality (8.83times speedup with 0.01\\% performance drop on VBench). As a\nplug-and-play solution, Jenga enables practical, high-quality video generation\non modern hardware by reducing inference time from minutes to seconds --\nwithout requiring model retraining. Code:\nhttps://github.com/dvlab-research/Jenga",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16864.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6418554a0956be7233a1023e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6418554a0956be7233a1023e/9EKN0GoOpcDbvBDmAQEJf.png",
      "fullname": "zhang yuechen",
      "name": "julianjuaner",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.16181",
      "authors": [
        {
          "_id": "682fddbd2b4a4d1ce53c5afb",
          "user": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "name": "Mohammad Reza Taesiri",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-23T03:18:33.876Z",
          "hidden": false
        },
        {
          "_id": "682fddbd2b4a4d1ce53c5afc",
          "user": {
            "_id": "62c5947524171688a9feb992",
            "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
            "isPro": false,
            "fullname": "Franck Dernoncourt",
            "user": "Franck-Dernoncourt",
            "type": "user"
          },
          "name": "Brandon Collins",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-23T02:32:10.307Z",
          "hidden": false
        },
        {
          "_id": "682fddbd2b4a4d1ce53c5afd",
          "user": {
            "_id": "668c8e8c142f9b26a49f03cc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/668c8e8c142f9b26a49f03cc/YNmPCrlsi6iwSeNfh1iID.png",
            "isPro": false,
            "fullname": "Logan Bolton",
            "user": "loganbolton",
            "type": "user"
          },
          "name": "Logan Bolton",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-23T02:33:14.731Z",
          "hidden": false
        },
        {
          "_id": "682fddbd2b4a4d1ce53c5afe",
          "name": "Viet Dac Lai",
          "hidden": false
        },
        {
          "_id": "682fddbd2b4a4d1ce53c5aff",
          "name": "Franck Dernoncourt",
          "hidden": false
        },
        {
          "_id": "682fddbd2b4a4d1ce53c5b00",
          "name": "Trung Bui",
          "hidden": false
        },
        {
          "_id": "682fddbd2b4a4d1ce53c5b01",
          "name": "Anh Totti Nguyen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T03:35:15.000Z",
      "submittedOnDailyAt": "2025-05-23T01:00:45.248Z",
      "title": "**Comprendu. Voici la traduction en français :**\n\n**Comprendre les capacités de l'intelligence artificielle générative dans le travail d'édition d'images quotidiennes**\n\n**Note :** Bien que aucune explication ou texte supplémentaire ne soit demandé, une version de traduction plus détaillée a été fournie pour s'adapter aux habitudes de lecture des lecteurs coréens.",
      "submittedOnDailyBy": {
        "_id": "62c5947524171688a9feb992",
        "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
        "isPro": false,
        "fullname": "Franck Dernoncourt",
        "user": "Franck-Dernoncourt",
        "type": "user"
      },
      "summary": "Les AI générés (GenAI) ont un grand potentiel pour automatiser les tâches d'édition d'images quotidiennes, et ce potentiel a été souligné encore plus après la publication de GPT-4o le 25 mars 2025. Cependant, quels sont les thèmes d'édition les plus demandés ? Quels types de tâches d'édition (par exemple, l'élimination ou l'estilization d'un thème) souhaitent les utilisateurs ? Préfèrent-ils des éditions précises et prévisibles ou des éditions de haut niveau de créativité ? Ai-t-il l'intérêt de comprendre les caractéristiques des demandes réelles et des éditions adaptatives réalisées par des professionnels de la photographie libres, pour améliorer les éditeurs basés sur l'IA et déterminer quels types de demandes peuvent être traitées avec succès par un éditeur d'IA ? Cet article a analysé 83k demandes et 305k éditions de PSR-wizard recueillies dans la communauté Reddit au cours des 12 dernières années (de 2013 à 2025) pour étudier ces thèmes de manière unique. Selon des évaluations humaines, les meilleurs éditeurs d'IA (y compris GPT-4o, Gemini-2.0-Flash et SeedEdit) peuvent gérer environ 33% des demandes. Intéressamment, les éditeurs d'IA réalisent des éditions plus précises dans les tâches ouvertes que dans les tâches à faible niveau de créativité. Souvent, les éditeurs d'IA trouvent difficile à maintenir l'identité des personnages et des animaux, ce qui les rend moins sollicités. D'autre part, les juges de modèles de langue (VLM, par exemple, o1) montrent une tendance plus marquée vers l'éditeur d'IA. Les codes et exemples de qualité peuvent être trouvés sur https://psrdataset.github.io.",
      "upvotes": 11,
      "discussionId": "682fddc32b4a4d1ce53c5c60",
      "projectPage": "https://psrdataset.github.io",
      "ai_summary": "Analysis of 83k image editing requests reveals that AI editors, including GPT-4o, struggle with low-creativity tasks and precise editing, while performing better on open-ended tasks, and human and VLM judges differ in their preferences for AI versus human edits.",
      "ai_keywords": [
        "GPT-4o",
        "Gemini-2.0-Flash",
        "SeedEdit",
        "VLM judges"
      ]
    },
    "publishedAt": "2025-05-21T23:35:15.000Z",
    "title": "Understanding Generative AI Capabilities in Everyday Image Editing Tasks",
    "summary": "Generative AI (GenAI) holds significant promise for automating everyday image\nediting tasks, especially following the recent release of GPT-4o on March 25,\n2025. However, what subjects do people most often want edited? What kinds of\nediting actions do they want to perform (e.g., removing or stylizing the\nsubject)? Do people prefer precise edits with predictable outcomes or highly\ncreative ones? By understanding the characteristics of real-world requests and\nthe corresponding edits made by freelance photo-editing wizards, can we draw\nlessons for improving AI-based editors and determine which types of requests\ncan currently be handled successfully by AI editors? In this paper, we present\na unique study addressing these questions by analyzing 83k requests from the\npast 12 years (2013-2025) on the Reddit community, which collected 305k\nPSR-wizard edits. According to human ratings, approximately only 33% of\nrequests can be fulfilled by the best AI editors (including GPT-4o,\nGemini-2.0-Flash, SeedEdit). Interestingly, AI editors perform worse on\nlow-creativity requests that require precise editing than on more open-ended\ntasks. They often struggle to preserve the identity of people and animals, and\nfrequently make non-requested touch-ups. On the other side of the table, VLM\njudges (e.g., o1) perform differently from human judges and may prefer AI edits\nmore than human edits. Code and qualitative examples are available at:\nhttps://psrdataset.github.io",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16181.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62c5947524171688a9feb992",
      "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
      "fullname": "Franck Dernoncourt",
      "name": "Franck-Dernoncourt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.15952",
      "authors": [
        {
          "_id": "682fe833f39f561d1d8cd5d1",
          "user": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "name": "Mohammad Reza Taesiri",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-23T03:18:36.072Z",
          "hidden": false
        },
        {
          "_id": "682fe833f39f561d1d8cd5d2",
          "name": "Abhijay Ghildyal",
          "hidden": false
        },
        {
          "_id": "682fe833f39f561d1d8cd5d3",
          "name": "Saman Zadtootaghaj",
          "hidden": false
        },
        {
          "_id": "682fe833f39f561d1d8cd5d4",
          "name": "Nabajeet Barman",
          "hidden": false
        },
        {
          "_id": "682fe833f39f561d1d8cd5d5",
          "user": {
            "_id": "644feede17b6189cda58575d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/WVo1Ah7xmHEeBOUQpkYgS.png",
            "isPro": false,
            "fullname": "Cor-Paul",
            "user": "corpaul",
            "type": "user"
          },
          "name": "Cor-Paul Bezemer",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-23T03:15:06.040Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T19:08:38.000Z",
      "submittedOnDailyAt": "2025-05-23T01:45:28.315Z",
      "title": "VideoGameQA-Bench : Évaluation de la Garantie de Qualité du Modèle Visuel de Langage des Jeux Vidéo",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Les jeux vidéo actuellement génèrent les revenus les plus élevés dans le secteur de l'industrie de l'entertainment et l'optimisation du processus de développement de jeux est essentielle pour le développement continu de l'industrie. Le développement récent des modèles de langage visuel (VLMs) a démontré la possibilité d'automatiser et d'améliorer différentes zones du développement de jeux, surtout dans le domaine de la garantie de qualité (QA), où les opportunités d'automatisation actuelles sont limitées. Pour évaluer précisément le rendement des VLMs dans les tâches de QA de jeux et évaluer leur efficacité dans le traitement de scénarios réels, il est nécessaire d'un cadre de référence standard. Les cadres de référence actuels ne répondent pas aux spécificités de ce domaine, par conséquent, il est présenté VideoGameQA-Bench, un cadre de référence détaillé qui couvre une large gamme d'activités de QA de jeux. Ce cadre de référence inclut des tests visuels unitaires, des tests de réinitialisation visuelle, des tâches de noeuds de la pile d'exécution, la détection de surprises, des rapports d'erreurs dans les images et les vidéos des jeux, entre d'autres activités de QA de jeux. Les codes et les données sont disponibles sur la URL suivante : https://asgaardlab.github.io/videogameqa-bench/",
      "upvotes": 11,
      "discussionId": "682fe83af39f561d1d8cd7e5",
      "projectPage": "https://asgaardlab.github.io/videogameqa-bench/",
      "ai_summary": "A benchmark called VideoGameQA-Bench is introduced to assess Vision-Language Models in video game quality assurance tasks.",
      "ai_keywords": [
        "Vision-Language Models",
        "VideoGameQA-Bench",
        "visual unit testing",
        "visual regression testing",
        "needle-in-a-haystack tasks",
        "glitch detection",
        "bug report generation"
      ]
    },
    "publishedAt": "2025-05-21T15:08:38.000Z",
    "title": "VideoGameQA-Bench: Evaluating Vision-Language Models for Video Game\n  Quality Assurance",
    "summary": "With video games now generating the highest revenues in the entertainment\nindustry, optimizing game development workflows has become essential for the\nsector's sustained growth. Recent advancements in Vision-Language Models (VLMs)\noffer considerable potential to automate and enhance various aspects of game\ndevelopment, particularly Quality Assurance (QA), which remains one of the\nindustry's most labor-intensive processes with limited automation options. To\naccurately evaluate the performance of VLMs in video game QA tasks and\ndetermine their effectiveness in handling real-world scenarios, there is a\nclear need for standardized benchmarks, as existing benchmarks are insufficient\nto address the specific requirements of this domain. To bridge this gap, we\nintroduce VideoGameQA-Bench, a comprehensive benchmark that covers a wide array\nof game QA activities, including visual unit testing, visual regression\ntesting, needle-in-a-haystack tasks, glitch detection, and bug report\ngeneration for both images and videos of various games. Code and data are\navailable at: https://asgaardlab.github.io/videogameqa-bench/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15952.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 83
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.17018",
      "authors": [
        {
          "_id": "682fe2b865bac3ec3556c016",
          "name": "Kaixuan Fan",
          "hidden": false
        },
        {
          "_id": "682fe2b865bac3ec3556c017",
          "name": "Kaituo Feng",
          "hidden": false
        },
        {
          "_id": "682fe2b865bac3ec3556c018",
          "name": "Haoming Lyu",
          "hidden": false
        },
        {
          "_id": "682fe2b865bac3ec3556c019",
          "name": "Dongzhan Zhou",
          "hidden": false
        },
        {
          "_id": "682fe2b865bac3ec3556c01a",
          "name": "Xiangyu Yue",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T17:59:53.000Z",
      "submittedOnDailyAt": "2025-05-23T01:28:23.338Z",
      "title": "SophiaVL-R1: Considérations pour la récompense pour renforcer les calculs de modèles de compréhension du texte",
      "submittedOnDailyBy": {
        "_id": "67079840a9bcb7459b8d2a46",
        "avatarUrl": "/avatars/32466863c5554f20cb2775b138832ac3.svg",
        "isPro": false,
        "fullname": "Kaituo Feng",
        "user": "KaituoFeng",
        "type": "user"
      },
      "summary": "Récemment, les progrès ont démontré le succès de la forte capacité logique des modèles de langage multimodal (MLLMs) grâce au apprentissage par renforcement basé sur des règles (RL). Cependant, ce paradigme présente une limitation dans l'observation du processus de pensée qui conduit à la déduction des résultats finaux, ce qui peut limiter la capacité des modèles pour apprendre des stratégies logiques optimales. Pour compléter cette lacune, nous proposons SophiaVL-R1, un essai de l'introduction de signaux de récompense dans le processus de pensée dans ce paradigme. Pour y parvenir, nous avons d'abord entraîné un modèle de récompense du processus de pensée et évalué la qualité du processus de pensée du modèle entier. La récompense du processus de pensée peut être moins fiable pour certains exemples, pourquoi nous proposons le méthode Trust-GRPO et assignons des poids de confiance lors de l'entraînement. Ces poids sont calculés en comparant la récompense du processus de pensée avec des réponses correctes et incorrectes, ce qui aide à atténuer l'impact potentiel des récompenses du processus de pensée non fiables. De plus, nous avons conçu une stratégie d'entraînement de relaxation pour réduire progressivement la récompense du processus de pensée au fur et à mesure du temps, ce qui permet au modèle d'adapter à une dépendance plus grande des récompenses des résultats basées sur des règles précises lors des phases d'entraînement ultérieures. Les résultats des tests montrent que notre SophiaVL-R1 dépasse une série de modèles logiques sur différents benchmarks comme MathVisita et MMMU, démontrant une forte capacité logique et généralisation. En particulier, notre SophiaVL-R1-7B dépasse LLaVA-OneVision-72B, qui a 10 fois plus de paramètres, en termes de capacité logique et généralisation. Tout le code, les modèles et les ensembles de données sont disponibles sur https://github.com/kxfan2002/SophiaVL-R1.",
      "upvotes": 10,
      "discussionId": "682fe2b965bac3ec3556c066",
      "projectPage": "https://github.com/kxfan2002/SophiaVL-R1",
      "githubRepo": "https://github.com/kxfan2002/SophiaVL-R1",
      "ai_summary": "An enhanced multimodal language model incorporates thinking process rewards to improve reasoning and generalization, achieving superior performance on benchmarks compared to larger models.",
      "ai_keywords": [
        "multimodal large language models",
        "rule-based reinforcement learning",
        "reward signals",
        "thinking reward model",
        "Trust-GRPO method",
        "thinking reward comparison",
        "annealing training strategy",
        "reasoning MLLMs",
        "MathVisita",
        "MMMU"
      ]
    },
    "publishedAt": "2025-05-22T13:59:53.000Z",
    "title": "SophiaVL-R1: Reinforcing MLLMs Reasoning with Thinking Reward",
    "summary": "Recent advances have shown success in eliciting strong reasoning abilities in\nmultimodal large language models (MLLMs) through rule-based reinforcement\nlearning (RL) with outcome rewards. However, this paradigm typically lacks\nsupervision over the thinking process leading to the final outcome.As a result,\nthe model may learn sub-optimal reasoning strategies, which can hinder its\ngeneralization ability. In light of this, we propose SophiaVL-R1, as an attempt\nto add reward signals for the thinking process in this paradigm. To achieve\nthis, we first train a thinking reward model that evaluates the quality of the\nentire thinking process. Given that the thinking reward may be unreliable for\ncertain samples due to reward hacking, we propose the Trust-GRPO method, which\nassigns a trustworthiness weight to the thinking reward during training. This\nweight is computed based on the thinking reward comparison of responses leading\nto correct answers versus incorrect answers, helping to mitigate the impact of\npotentially unreliable thinking rewards. Moreover, we design an annealing\ntraining strategy that gradually reduces the thinking reward over time,\nallowing the model to rely more on the accurate rule-based outcome reward in\nlater training stages. Experiments show that our SophiaVL-R1 surpasses a series\nof reasoning MLLMs on various benchmarks (e.g., MathVisita, MMMU),\ndemonstrating strong reasoning and generalization capabilities. Notably, our\nSophiaVL-R1-7B even outperforms LLaVA-OneVision-72B on most benchmarks, despite\nthe latter having 10 times more parameters. All code, models, and datasets are\nmade publicly available at https://github.com/kxfan2002/SophiaVL-R1.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17018.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67079840a9bcb7459b8d2a46",
      "avatarUrl": "/avatars/32466863c5554f20cb2775b138832ac3.svg",
      "fullname": "Kaituo Feng",
      "name": "KaituoFeng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.17012",
      "authors": [
        {
          "_id": "682fde942f8f73559fcbc5da",
          "name": "Haoning Wu",
          "hidden": false
        },
        {
          "_id": "682fde942f8f73559fcbc5db",
          "name": "Xiao Huang",
          "hidden": false
        },
        {
          "_id": "682fde942f8f73559fcbc5dc",
          "name": "Yaohui Chen",
          "hidden": false
        },
        {
          "_id": "682fde942f8f73559fcbc5dd",
          "name": "Ya Zhang",
          "hidden": false
        },
        {
          "_id": "682fde942f8f73559fcbc5de",
          "name": "Yanfeng Wang",
          "hidden": false
        },
        {
          "_id": "682fde942f8f73559fcbc5df",
          "name": "Weidi Xie",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/632c7a0d1d303f5f9acf01b8/3GQcp-q-BBj5bX_DRmB6S.jpeg"
      ],
      "publishedAt": "2025-05-22T17:59:03.000Z",
      "submittedOnDailyAt": "2025-05-23T01:12:41.090Z",
      "title": "Score Spectral : Une méthode d'évaluation uniforme de la compréhension du spectre de Damoclès",
      "submittedOnDailyBy": {
        "_id": "632c7a0d1d303f5f9acf01b8",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/632c7a0d1d303f5f9acf01b8/T010IFuCp6UaOeIyWhbCk.jpeg",
        "isPro": false,
        "fullname": "Haoning Wu",
        "user": "haoningwu",
        "type": "user"
      },
      "summary": "Les modèles de langage multimodal de diffusion (MLLMs) ont réalisé un succès impressionnant dans les tâches de réponse à des questions, mais leur capacité à comprendre les espaces a été peu étudiée. Cet article étudie principalement les problèmes suivants : ont-ils actuellement les MLLMs la capacité de comprendre et de comprendre les espaces 3D ? Concrètement, cet article présente les contributions suivantes : (i) nous introduisons VGBench, un standard spécialement conçu pour évaluer la perception géométrique visuelle des MLLMs, qui comprend divers aspects tels que l'estimation de la position et du mouvement de la caméra ; (ii) nous proposons SpatialScore, le plus large et diversifié standard de compréhension spatiale multimodal jusqu'à présent, qui intègre des données de 11 datasets différents par rapport à VGBench ; ce standard inclut 28 000 échantillons et aborde diverses tâches de compréhension spatiale, modalités et formats de questions et réponses, ainsi qu'un sous-ensemble défisant appelé SpatialScore-Hard ; (iii) nous développons SpatialAgent, un nouveau système d'agent multimodal multiagent qui combine 9 outils professionnels et soutient le modèle d'inférence Plan-Execute et ReAct ; (iv) nous effectuons une évaluation large qui révèle les défis continus dans l'inférence spatiale et démontre l'efficacité de SpatialAgent. Je crois que SpatialScore pourrait devenir un standard strict pour l'avancement prochain des MLLMs.",
      "upvotes": 9,
      "discussionId": "682fde952f8f73559fcbc616",
      "projectPage": "https://haoningwu3639.github.io/SpatialScore/",
      "githubRepo": "https://github.com/haoningwu3639/SpatialScore/",
      "ai_summary": "SpatialScore benchmarks multimodal large language models for 3D spatial understanding, revealing challenges and showcasing the effectiveness of SpatialAgent with specialized tools.",
      "ai_keywords": [
        "Multimodal large language models",
        "MLLMs",
        "VGBench",
        "SpatialScore",
        "spatial understanding",
        "visual geometry perception",
        "camera pose",
        "motion estimation",
        "multi-agent system",
        "SpatialAgent",
        "Plan-Execute",
        "ReAct reasoning paradigms"
      ]
    },
    "publishedAt": "2025-05-22T13:59:03.000Z",
    "title": "SpatialScore: Towards Unified Evaluation for Multimodal Spatial\n  Understanding",
    "summary": "Multimodal large language models (MLLMs) have achieved impressive success in\nquestion-answering tasks, yet their capabilities for spatial understanding are\nless explored. This work investigates a critical question: do existing MLLMs\npossess 3D spatial perception and understanding abilities? Concretely, we make\nthe following contributions in this paper: (i) we introduce VGBench, a\nbenchmark specifically designed to assess MLLMs for visual geometry perception,\ne.g., camera pose and motion estimation; (ii) we propose SpatialScore, the most\ncomprehensive and diverse multimodal spatial understanding benchmark to date,\nintegrating VGBench with relevant data from the other 11 existing datasets.\nThis benchmark comprises 28K samples across various spatial understanding\ntasks, modalities, and QA formats, along with a carefully curated challenging\nsubset, SpatialScore-Hard; (iii) we develop SpatialAgent, a novel multi-agent\nsystem incorporating 9 specialized tools for spatial understanding, supporting\nboth Plan-Execute and ReAct reasoning paradigms; (iv) we conduct extensive\nevaluations to reveal persistent challenges in spatial reasoning while\ndemonstrating the effectiveness of SpatialAgent. We believe SpatialScore will\noffer valuable insights and serve as a rigorous benchmark for the next\nevolution of MLLMs.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/632c7a0d1d303f5f9acf01b8/3GQcp-q-BBj5bX_DRmB6S.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17012.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "632c7a0d1d303f5f9acf01b8",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/632c7a0d1d303f5f9acf01b8/T010IFuCp6UaOeIyWhbCk.jpeg",
      "fullname": "Haoning Wu",
      "name": "haoningwu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.16839",
      "authors": [
        {
          "_id": "682fd5758d2fd6fc7cd5c9f7",
          "name": "Shufan Li",
          "hidden": false
        },
        {
          "_id": "682fd5758d2fd6fc7cd5c9f8",
          "name": "Konstantinos Kallidromitis",
          "hidden": false
        },
        {
          "_id": "682fd5758d2fd6fc7cd5c9f9",
          "name": "Hritik Bansal",
          "hidden": false
        },
        {
          "_id": "682fd5758d2fd6fc7cd5c9fa",
          "name": "Akash Gokul",
          "hidden": false
        },
        {
          "_id": "682fd5758d2fd6fc7cd5c9fb",
          "name": "Yusuke Kato",
          "hidden": false
        },
        {
          "_id": "682fd5758d2fd6fc7cd5c9fc",
          "name": "Kazuki Kozuka",
          "hidden": false
        },
        {
          "_id": "682fd5758d2fd6fc7cd5c9fd",
          "name": "Jason Kuen",
          "hidden": false
        },
        {
          "_id": "682fd5758d2fd6fc7cd5c9fe",
          "name": "Zhe Lin",
          "hidden": false
        },
        {
          "_id": "682fd5758d2fd6fc7cd5c9ff",
          "name": "Kai-Wei Chang",
          "hidden": false
        },
        {
          "_id": "682fd5758d2fd6fc7cd5ca00",
          "name": "Aditya Grover",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6310531914aa81e1044363ed/85JcNXnpZ6f0XvO4vZ5_s.gif"
      ],
      "publishedAt": "2025-05-22T16:07:12.000Z",
      "submittedOnDailyAt": "2025-05-23T00:30:19.437Z",
      "title": "LaViDa : Modèle de diffusion grand pour la compréhension multimodale",
      "submittedOnDailyBy": {
        "_id": "6310531914aa81e1044363ed",
        "avatarUrl": "/avatars/ae7767e591cb7199ea2f62d2db89fc7f.svg",
        "isPro": false,
        "fullname": "Shufan Li",
        "user": "jacklishufan",
        "type": "user"
      },
      "summary": "Les modèles de langue similaires au monde actuel (VLMs) peuvent résoudre plusieurs tâches. Dans des scénarios très populaires, les VLMs nécessitent des caractéristiques telles que l'inférence rapide et le contrôle de la génération (par exemple, limiter la sortie à un format spécifique). Cependant, les modèles VLMs de récupération automatique (AR) comme LLaVA ont des difficultés dans ces aspects. Les modèles de description discrète (DMs) permettent une interprétation parallèle, ce qui facilite l'inférence rapide, et utilisent le contexte pour contrôler la génération. Bien que les DMs soient efficaces seulement dans des environnements de langue, leur capacité à gérer des tâches multiformes a été peu étudiée. Nous présentons une famille de VLMs basée sur les DMs. Nous ajoutons un encodeur visuel aux DMs et combinons la partie combinée avec un cycle de commandes pour construire LaViDa. Pour faire face à ces défis, LaViDa introduit de nouvelles technologies telles que les masques d'interpolation, le cache de préfixe KV et le déplacement temporel. Les expériences montrent que LaViDa présente un excellent rendement sur les benchmarks comme MMMU par rapport aux VLMs de AR, et offre les avantages uniques des DMs. Le code et les modèles sont disponibles dans la version de caméra.",
      "upvotes": 9,
      "discussionId": "682fd5768d2fd6fc7cd5ca3c",
      "projectPage": " https://homepage.jackli.org/projects/lavida/index.html",
      "githubRepo": "https://github.com/jacklishufan/LaViDa",
      "ai_summary": "LaViDa, a family of vision-language models built on discrete diffusion models, offers competitive performance on multimodal benchmarks with advantages in speed, controllability, and bidirectional reasoning.",
      "ai_keywords": [
        "autoregressive (AR) VLMs",
        "discrete diffusion models (DMs)",
        "parallel decoding",
        "bidirectional context",
        "text-infilling",
        "multimodal instruction following",
        "complementary masking",
        "prefix KV cache",
        "timestep shifting",
        "MMMU",
        "COCO captioning",
        "Constrained Poem Completion",
        "Open-LLaVa-Next-8B",
        "CIDEr"
      ]
    },
    "publishedAt": "2025-05-22T12:07:12.000Z",
    "title": "LaViDa: A Large Diffusion Language Model for Multimodal Understanding",
    "summary": "Modern Vision-Language Models (VLMs) can solve a wide range of tasks\nrequiring visual reasoning. In real-world scenarios, desirable properties for\nVLMs include fast inference and controllable generation (e.g., constraining\noutputs to adhere to a desired format). However, existing autoregressive (AR)\nVLMs like LLaVA struggle in these aspects. Discrete diffusion models (DMs)\noffer a promising alternative, enabling parallel decoding for faster inference\nand bidirectional context for controllable generation through text-infilling.\nWhile effective in language-only settings, DMs' potential for multimodal tasks\nis underexplored. We introduce LaViDa, a family of VLMs built on DMs. We build\nLaViDa by equipping DMs with a vision encoder and jointly fine-tune the\ncombined parts for multimodal instruction following. To address challenges\nencountered, LaViDa incorporates novel techniques such as complementary masking\nfor effective training, prefix KV cache for efficient inference, and timestep\nshifting for high-quality sampling. Experiments show that LaViDa achieves\ncompetitive or superior performance to AR VLMs on multi-modal benchmarks such\nas MMMU, while offering unique advantages of DMs, including flexible\nspeed-quality tradeoff, controllability, and bidirectional reasoning. On COCO\ncaptioning, LaViDa surpasses Open-LLaVa-Next-8B by +4.1 CIDEr with 1.92x\nspeedup. On bidirectional tasks, it achieves +59% improvement on Constrained\nPoem Completion. These results demonstrate LaViDa as a strong alternative to AR\nVLMs. Code and models will be released in the camera-ready version.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6310531914aa81e1044363ed/85JcNXnpZ6f0XvO4vZ5_s.gif"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16839.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6310531914aa81e1044363ed",
      "avatarUrl": "/avatars/ae7767e591cb7199ea2f62d2db89fc7f.svg",
      "fullname": "Shufan Li",
      "name": "jacklishufan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.14625",
      "authors": [
        {
          "_id": "682fdb318df2d5446a1cf30b",
          "name": "Zhangchen Xu",
          "hidden": false
        },
        {
          "_id": "682fdb318df2d5446a1cf30c",
          "name": "Yuetai Li",
          "hidden": false
        },
        {
          "_id": "682fdb318df2d5446a1cf30d",
          "name": "Fengqing Jiang",
          "hidden": false
        },
        {
          "_id": "682fdb318df2d5446a1cf30e",
          "name": "Bhaskar Ramasubramanian",
          "hidden": false
        },
        {
          "_id": "682fdb318df2d5446a1cf30f",
          "name": "Luyao Niu",
          "hidden": false
        },
        {
          "_id": "682fdb318df2d5446a1cf310",
          "name": "Bill Yuchen Lin",
          "hidden": false
        },
        {
          "_id": "682fdb318df2d5446a1cf311",
          "name": "Radha Poovendran",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-20T17:16:44.000Z",
      "submittedOnDailyAt": "2025-05-23T00:50:16.785Z",
      "title": "TinyV : La réduction du surapprentissage dans la vérification contribue au meilleur rendement de la RL dans les LLM.",
      "submittedOnDailyBy": {
        "_id": "653df1323479e9ebbe3eb6cc",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653df1323479e9ebbe3eb6cc/K_g-r1iMRNKj99LXPuYF3.jpeg",
        "isPro": true,
        "fullname": "Zhangchen Xu",
        "user": "zhangchenxu",
        "type": "user"
      },
      "summary": "L'apprentissage par renforcement (RL) a acquis une position dominante en tant qu'outil puissant pour améliorer la capacité logique de grands modèles de langage (LLMs) en utilisant des signaux de récompense. Cependant, le succès du RL dépend de la confiance dans ces signaux de récompense, qui sont fournies par les vérificateurs. Dans cet article, on expose et analyse les problèmes larges des \"négatifs faux\" (false negatives) dans lesquels les vérificateurs rejettent des réponses correctes du modèle. Une étude détaillée sur le jeu de données Big-Math-RL-Verified révèle que plus de 38% des réponses générées par le modèle sont rejetées par les vérificateurs, qui ne peuvent pas reconnaître la réponse correcte. Cette \"négation fausse\" est démontrée tant expérimentalement qu'théoriquement, et elle indique un gradient informatif qui est perdu par le modèle, détériorant son entraînement et ralentissant sa convergence. Pour atténuer cela, on propose un vérificateur basé sur un modèle de langage appelé tinyV. Ce vérificateur renforce les méthodes basées sur les règles actuelles, reconnaît dynamiquement les possibles \"négatifs faux\" et récupère des réponses correctes, permettant ainsi une estimation plus précise de la récompense. L'intégration de TinyV dans plusieurs marqueurs de logique mathématique a augmenté la taux d'acceptation d'au-delà de 10% et accéléré la convergence. Notre résultat souligne l'importance de résoudre les \"négatifs faux\" des vérificateurs et fournit une approche utile pour améliorer l'apprentissage par renforcement des LLMs. Notre code est disponible sur https://github.com/uw-nsl/TinyV.",
      "upvotes": 9,
      "discussionId": "682fdb328df2d5446a1cf377",
      "githubRepo": "https://github.com/uw-nsl/TinyV",
      "ai_summary": "TinyV, a lightweight LLM-based verifier, improves RL training of large language models by addressing false negatives from existing rule-based verifiers, enhancing reward accuracy and convergence speed.",
      "ai_keywords": [
        "Reinforcement Learning",
        "large language models",
        "policies",
        "reward signals",
        "verifiers",
        "false negatives",
        "Big-Math-RL-Verified dataset",
        "gradient signals",
        "convergence",
        "TinyV",
        "rule-based methods",
        "math-reasoning benchmarks",
        "pass rates"
      ]
    },
    "publishedAt": "2025-05-20T13:16:44.000Z",
    "title": "TinyV: Reducing False Negatives in Verification Improves RL for LLM\n  Reasoning",
    "summary": "Reinforcement Learning (RL) has become a powerful tool for enhancing the\nreasoning abilities of large language models (LLMs) by optimizing their\npolicies with reward signals. Yet, RL's success relies on the reliability of\nrewards, which are provided by verifiers. In this paper, we expose and analyze\na widespread problem--false negatives--where verifiers wrongly reject correct\nmodel outputs. Our in-depth study of the Big-Math-RL-Verified dataset reveals\nthat over 38% of model-generated responses suffer from false negatives, where\nthe verifier fails to recognize correct answers. We show, both empirically and\ntheoretically, that these false negatives severely impair RL training by\ndepriving the model of informative gradient signals and slowing convergence. To\nmitigate this, we propose tinyV, a lightweight LLM-based verifier that augments\nexisting rule-based methods, which dynamically identifies potential false\nnegatives and recovers valid responses to produce more accurate reward\nestimates. Across multiple math-reasoning benchmarks, integrating TinyV boosts\npass rates by up to 10% and accelerates convergence relative to the baseline.\nOur findings highlight the critical importance of addressing verifier false\nnegatives and offer a practical approach to improve RL-based fine-tuning of\nLLMs. Our code is available at https://github.com/uw-nsl/TinyV.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14625.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "653df1323479e9ebbe3eb6cc",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653df1323479e9ebbe3eb6cc/K_g-r1iMRNKj99LXPuYF3.jpeg",
      "fullname": "Zhangchen Xu",
      "name": "zhangchenxu",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 15
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.16151",
      "authors": [
        {
          "_id": "682fd61601208348fffaa62e",
          "name": "Hongchen Wei",
          "hidden": false
        },
        {
          "_id": "682fd61601208348fffaa62f",
          "name": "Zhenzhong Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T02:51:12.000Z",
      "submittedOnDailyAt": "2025-05-23T00:34:39.659Z",
      "title": "Théorie de l'intelligence non entraînée et la fonction de rétroaction en MLLM",
      "submittedOnDailyBy": {
        "_id": "63f96e99ade090bc87bc2f81",
        "avatarUrl": "/avatars/0dd0807e5b2cec011e97c8d6a3c61bae.svg",
        "isPro": false,
        "fullname": "hcwei",
        "user": "hcwei",
        "type": "user"
      },
      "summary": "Le développement récent des Reasoning LLMs (comme DeepSeek-R1 et OpenAI-o1) a démontré une capacité d'inférence attrayante basée sur l'apprentissage par renforcement. Cependant, l'extension de ces capacités aux MLLMs (Multimodal Large Language Models) a été difficile en raison des coûts élevés de retraitement et de la rareté de jeux de données multimodales de haute qualité et vérifiables. Dans cet article, nous présentons le modèle FRANK. Ce modèle offre une inférence et des capacités de réflexion sans nécessiter d'actualisations ∇ ni de stratégies supplémentaires, et il est un MLLM similaire à r1 mais avec des coûts d'entraînement nuls. Notre principale idée est de séparer l'inférence et la justification dans les couches décoditrices du MLLM. En particulier, nous observons que les couches décoditrices superficielles distribuent une grande attention sur les tokens visuels, tandis que les couches profondes se concentrent sur le sens littéral. Cette observation fournit une stratégie progressive pour combiner les poids d'un MLLM avec un modèle spécialisé en justifications, promouvant l'intégration de la capacité de justification dans les couches profondes et la préservation de la vision dans les couches superficielles par une structure de fusion de type \"tir à fer à coups\". Les expériences sur des benchmarks difficiles de l'inférence multimodal montrent les effets de notre approche. Dans le benchmark MMMU, notre modèle FRANK-38B atteint une précision de 69.2, surpassant considérablement un modèle de référence fort comme InternVL2.5-38B de +5.3 et également les modèles de profil comme le GPT-4o. Pour plus d'informations, consultez notre site web de projet à la suivante URL : http://iip.whu.edu.cn/frank/index.html",
      "upvotes": 6,
      "discussionId": "682fd61701208348fffaa654",
      "ai_summary": "The FRANK Model enhances multimodal LLMs with reasoning and reflection abilities without retraining, using a hierarchical weight merging approach that merges visual-pretrained and reasoning-specialized models.",
      "ai_keywords": [
        "Reasoning LLMs",
        "Multimodal LLMs (MLLMs)",
        "FRANK Model",
        "reinforcement learning",
        "multimodal reasoning datasets",
        "hierarchical weight merging",
        "Taylor-derived closed-form fusion mechanism",
        "MMMU benchmark",
        "visual tokens",
        "textual semantics",
        "deep decoder layers",
        "shallow decoder layers"
      ]
    },
    "publishedAt": "2025-05-21T22:51:12.000Z",
    "title": "Training-Free Reasoning and Reflection in MLLMs",
    "summary": "Recent advances in Reasoning LLMs (e.g., DeepSeek-R1 and OpenAI-o1) have\nshowcased impressive reasoning capabilities via reinforcement learning.\nHowever, extending these capabilities to Multimodal LLMs (MLLMs) is hampered by\nthe prohibitive costs of retraining and the scarcity of high-quality,\nverifiable multimodal reasoning datasets. This paper introduces FRANK Model, a\ntraining-FRee ANd r1-liKe MLLM that imbues off-the-shelf MLLMs with reasoning\nand reflection abilities, without any gradient updates or extra supervision.\nOur key insight is to decouple perception and reasoning across MLLM decoder\nlayers. Specifically, we observe that compared to the deeper decoder layers,\nthe shallow decoder layers allocate more attention to visual tokens, while the\ndeeper decoder layers concentrate on textual semantics. This observation\nmotivates a hierarchical weight merging approach that combines a\nvisual-pretrained MLLM with a reasoning-specialized LLM. To this end, we\npropose a layer-wise, Taylor-derived closed-form fusion mechanism that\nintegrates reasoning capacity into deep decoder layers while preserving visual\ngrounding in shallow decoder layers. Extensive experiments on challenging\nmultimodal reasoning benchmarks demonstrate the effectiveness of our approach.\nOn the MMMU benchmark, our model FRANK-38B achieves an accuracy of 69.2,\noutperforming the strongest baseline InternVL2.5-38B by +5.3, and even\nsurpasses the proprietary GPT-4o model. Our project homepage is at:\nhttp://iip.whu.edu.cn/frank/index.html",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16151.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "63f96e99ade090bc87bc2f81",
      "avatarUrl": "/avatars/0dd0807e5b2cec011e97c8d6a3c61bae.svg",
      "fullname": "hcwei",
      "name": "hcwei",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.15879",
      "authors": [
        {
          "_id": "682fecc3fd3719dbe6fbb84b",
          "name": "Yue Fan",
          "hidden": false
        },
        {
          "_id": "682fecc3fd3719dbe6fbb84c",
          "name": "Xuehai He",
          "hidden": false
        },
        {
          "_id": "682fecc3fd3719dbe6fbb84d",
          "name": "Diji Yang",
          "hidden": false
        },
        {
          "_id": "682fecc3fd3719dbe6fbb84e",
          "name": "Kaizhi Zheng",
          "hidden": false
        },
        {
          "_id": "682fecc3fd3719dbe6fbb84f",
          "name": "Ching-Chen Kuo",
          "hidden": false
        },
        {
          "_id": "682fecc3fd3719dbe6fbb850",
          "name": "Yuting Zheng",
          "hidden": false
        },
        {
          "_id": "682fecc3fd3719dbe6fbb851",
          "name": "Sravana Jyothi Narayanaraju",
          "hidden": false
        },
        {
          "_id": "682fecc3fd3719dbe6fbb852",
          "name": "Xinze Guan",
          "hidden": false
        },
        {
          "_id": "682fecc3fd3719dbe6fbb853",
          "name": "Xin Eric Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T17:54:49.000Z",
      "submittedOnDailyAt": "2025-05-23T02:05:02.804Z",
      "title": "GRIT : Méthode de direction de MLLM imaginée avec des images",
      "submittedOnDailyBy": {
        "_id": "64679a226192d39142245e5e",
        "avatarUrl": "/avatars/05abee0b6317f100923936ca2099e9eb.svg",
        "isPro": false,
        "fullname": "Xin Eric Wang",
        "user": "xw-eric",
        "type": "user"
      },
      "summary": "Recentes études ont montré l'efficacité de la construction de modèles explicatifs en utilisant l'apprentissage par renforcement (RL). Cependant, le développement de mécanismes pour expliquer des tâches de langue visuelle est en cours, et les modèles de raisonnement visuel en code ouverts actuels génèrent généralement des contenus de raisonnement dans un simple langage naturel, ce qui limite l'intégration explicite d'information visuelle. Cela restreint la capacité à générer une séquence claire de raisonnements. En réponse à cette problématique, nous proposons l'introduction d'une base de raisonnement basique (GRIT) qui utilise des images et du texte. GRIT offre un nouvel approche pour l'entraînement de modèles MLLM qui pensent avec des images. GRIT introduit un paradigme de raisonnement fondamental et se concentre sur le fait que le modèle génère des séquences de raisonnements qui croisent le langage naturel et les coordonnées de la boîte de délimitation, qui spécifient les régions de l'image que le modèle référence lors du processus de raisonnement. De plus, GRIT utilise un approche d'apprentissage par renforcement basée sur l'algorithme GRPO, connu sous le nom de GRPO-GR. GRPO-GR utilise une recompense forte qui se concentre sur la précision de la réponse finale et sur le format de sortie de la raisonnement basique, ce qui permet de tester la séquence de raisonnements sans nécessiter des étiquettes de la boîte de délimitation explicites. Avec cette base, GRIT atteint une efficacité élevée dans l'utilisation des données, nécessitant seulement 20 tuples d'images-questions-réponses dans les ensembles de données actuels. Les évaluations détaillées montrent que GRIT entraîne efficacement les modèles MLLM pour générer des séquences de raisonnements et intègre de manière satisfaisante la capacité de raisonnement et de raisonnement basique.",
      "upvotes": 6,
      "discussionId": "682fecc4fd3719dbe6fbb8ac",
      "projectPage": "https://grounded-reasoning.github.io",
      "githubRepo": "https://github.com/eric-ai-lab/GRIT",
      "ai_summary": "A novel method called GRIT enhances visual reasoning in MLLMs by generating reasoning chains that integrate both natural language and bounding box coordinates, guided by a reinforcement learning approach for high data efficiency.",
      "ai_keywords": [
        "Reinforcement Learning",
        "RL",
        "MLLMs",
        "reasoning chains",
        "interleave natural language",
        "bounding box coordinates",
        "GRPO-GR",
        "GRPO",
        "grounded reasoning output",
        "reasoning and grounding abilities"
      ]
    },
    "publishedAt": "2025-05-21T13:54:49.000Z",
    "title": "GRIT: Teaching MLLMs to Think with Images",
    "summary": "Recent studies have demonstrated the efficacy of using Reinforcement Learning\n(RL) in building reasoning models that articulate chains of thoughts prior to\nproducing final answers. However, despite ongoing advances that aim at enabling\nreasoning for vision-language tasks, existing open-source visual reasoning\nmodels typically generate reasoning content with pure natural language, lacking\nexplicit integration of visual information. This limits their ability to\nproduce clearly articulated and visually grounded reasoning chains. To this\nend, we propose Grounded Reasoning with Images and Texts (GRIT), a novel method\nfor training MLLMs to think with images. GRIT introduces a grounded reasoning\nparadigm, in which models generate reasoning chains that interleave natural\nlanguage and explicit bounding box coordinates. These coordinates point to\nregions of the input image that the model consults during its reasoning\nprocess. Additionally, GRIT is equipped with a reinforcement learning approach,\nGRPO-GR, built upon the GRPO algorithm. GRPO-GR employs robust rewards focused\non the final answer accuracy and format of the grounded reasoning output, which\neliminates the need for data with reasoning chain annotations or explicit\nbounding box labels. As a result, GRIT achieves exceptional data efficiency,\nrequiring as few as 20 image-question-answer triplets from existing datasets.\nComprehensive evaluations demonstrate that GRIT effectively trains MLLMs to\nproduce coherent and visually grounded reasoning chains, showing a successful\nunification of reasoning and grounding abilities.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15879.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64679a226192d39142245e5e",
      "avatarUrl": "/avatars/05abee0b6317f100923936ca2099e9eb.svg",
      "fullname": "Xin Eric Wang",
      "name": "xw-eric",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.16944",
      "authors": [
        {
          "_id": "683023848d2fd6fc7ce9b99a",
          "name": "Yunjia Qi",
          "hidden": false
        },
        {
          "_id": "683023848d2fd6fc7ce9b99b",
          "name": "Hao Peng",
          "hidden": false
        },
        {
          "_id": "683023848d2fd6fc7ce9b99c",
          "name": "Xiaozhi Wang",
          "hidden": false
        },
        {
          "_id": "683023848d2fd6fc7ce9b99d",
          "name": "Amy Xin",
          "hidden": false
        },
        {
          "_id": "683023848d2fd6fc7ce9b99e",
          "name": "Youfeng Liu",
          "hidden": false
        },
        {
          "_id": "683023848d2fd6fc7ce9b99f",
          "name": "Bin Xu",
          "hidden": false
        },
        {
          "_id": "683023848d2fd6fc7ce9b9a0",
          "name": "Lei Hou",
          "hidden": false
        },
        {
          "_id": "683023848d2fd6fc7ce9b9a1",
          "name": "Juanzi Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T17:31:10.000Z",
      "submittedOnDailyAt": "2025-05-23T05:58:53.468Z",
      "title": "AGENTIF : Scénario de test de conformité aux instructions pour modèles de langage à grande échelle",
      "submittedOnDailyBy": {
        "_id": "6556cf2cee35f7d8bcf13bb3",
        "avatarUrl": "/avatars/0a945054cb4732c2ee7a5502c42628bf.svg",
        "isPro": false,
        "fullname": "Qi Yunjia",
        "user": "Kikkk",
        "type": "user"
      },
      "summary": "Les modèles de langage grands (LLMs) ont démontré des capacités élevées pour le développement de systèmes d'agents efficaces et réalistes. Le progrès de la recherche vise à développer des agents basés sur les LLMs qui répondent aux besoins disponibles, mais des nouveaux défis ont été rencontrés. Les scénarios d'agents incluent des systèmes de commandes longs et complexes, ainsi que des spécifications détaillées d'outils. L'adaptation aux instructions est cruciale dans l'utilisation des systèmes d'agents, mais la recherche sur la capacité des LLMs à suivre ces instructions de manière fiable est insuffisante. Dans cet article, nous présentons \"AgentIF\", le premier benchmark pour évaluer de manière systématique la capacité de suivi d'instructions des LLMs dans des scénarios d'agents. AgentIF comporte trois caractéristiques principales : 1. Il a été construit à partir de 50 systèmes d'agents réels. 2. Les commandes sont longues, avec une longueur moyenne de plus de 1,723 caractères et un maximum de 15,630 caractères. 3. Elles sont complexes, avec un nombre moyen de 11,9 restrictions par instruction et plusieurs types de restrictions, comme spécifications d'outils et conditions. Pour la construction de AgentIF, 707 instructions annotées par humains de 50 tâches d'agents ont été collectées. Les restrictions liées à chaque instruction ont été annotées et des critères d'évaluation ont été inclus, comprenant des évaluations basées sur du code, basées sur les LLMs et hybrides de code-LLMs. En utilisant AgentIF, les progrès actuels des LLMs ont été évalués systématiquement. Les modèles actuels montrent un rendement faible, en particulier dans le traitement de structures de restrictions complexes et de spécifications d'outils. De plus, des analyses d'erreurs et des expériences analytiques sur la longueur des commandes et des restrictions méta ont été réalisées, fournissant des découvertes sur les modes de faiblesse des LLMs actuels. L'objectif est de publier le code et les données pour soutenir futures recherches.",
      "upvotes": 5,
      "discussionId": "683023858d2fd6fc7ce9b9e4",
      "githubRepo": "https://github.com/THU-KEG/AgentIF",
      "ai_summary": "A new benchmark, AgentIF, evaluates Large Language Models' ability to follow complex instructions in realistic agentic scenarios, revealing performance limitations in handling constraints and tool specifications.",
      "ai_keywords": [
        "AgentIF",
        "Large Language Models (LLMs)",
        "agentic applications",
        "system prompts",
        "tool specifications",
        "instruction following",
        "constraint structures",
        "error analysis"
      ]
    },
    "publishedAt": "2025-05-22T13:31:10.000Z",
    "title": "AGENTIF: Benchmarking Instruction Following of Large Language Models in\n  Agentic Scenarios",
    "summary": "Large Language Models (LLMs) have demonstrated advanced capabilities in\nreal-world agentic applications. Growing research efforts aim to develop\nLLM-based agents to address practical demands, introducing a new challenge:\nagentic scenarios often involve lengthy instructions with complex constraints,\nsuch as extended system prompts and detailed tool specifications. While\nadherence to such instructions is crucial for agentic applications, whether\nLLMs can reliably follow them remains underexplored. In this paper, we\nintroduce AgentIF, the first benchmark for systematically evaluating LLM\ninstruction following ability in agentic scenarios. AgentIF features three key\ncharacteristics: (1) Realistic, constructed from 50 real-world agentic\napplications. (2) Long, averaging 1,723 words with a maximum of 15,630 words.\n(3) Complex, averaging 11.9 constraints per instruction, covering diverse\nconstraint types, such as tool specifications and condition constraints. To\nconstruct AgentIF, we collect 707 human-annotated instructions across 50\nagentic tasks from industrial application agents and open-source agentic\nsystems. For each instruction, we annotate the associated constraints and\ncorresponding evaluation metrics, including code-based evaluation, LLM-based\nevaluation, and hybrid code-LLM evaluation. We use AgentIF to systematically\nevaluate existing advanced LLMs. We observe that current models generally\nperform poorly, especially in handling complex constraint structures and tool\nspecifications. We further conduct error analysis and analytical experiments on\ninstruction length and meta constraints, providing some findings about the\nfailure modes of existing LLMs. We have released the code and data to\nfacilitate future research.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16944.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6556cf2cee35f7d8bcf13bb3",
      "avatarUrl": "/avatars/0a945054cb4732c2ee7a5502c42628bf.svg",
      "fullname": "Qi Yunjia",
      "name": "Kikkk",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.16854",
      "authors": [
        {
          "_id": "682fdd8691757629e1d58e16",
          "name": "Jiaqi Wang",
          "hidden": false
        },
        {
          "_id": "682fdd8691757629e1d58e17",
          "name": "Kevin Qinghong Lin",
          "hidden": false
        },
        {
          "_id": "682fdd8691757629e1d58e18",
          "name": "James Cheng",
          "hidden": false
        },
        {
          "_id": "682fdd8691757629e1d58e19",
          "name": "Mike Zheng Shou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T16:13:29.000Z",
      "submittedOnDailyAt": "2025-05-23T01:09:25.197Z",
      "title": "Modèle Vision-linguistique utilisant l'apprentissage Renos pour la Théorie des Raisons Sélectives",
      "submittedOnDailyBy": {
        "_id": "64440be5af034cdfd69ca3a7",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64440be5af034cdfd69ca3a7/qmx24QiDFT29vleCxL9TX.jpeg",
        "isPro": false,
        "fullname": "Qinghong (Kevin) Lin",
        "user": "KevinQHLin",
        "type": "user"
      },
      "summary": "La formation par récompense (RL) a démontré être une stratégie efficace pour améliorer la logique des modèles de langage visuel (VLMs). Le Groupe Relatively Optimal (GRPO) est un méthode récente et importante qui promeut la génération complète de traces logiques avant que le modèle réponde, avec l'objectif d'optimiser l'efficience de la logique en termes de nombre de tokens et de coûts de calcul. Il a été étudié comment modéliser le processus de pensée humaine, apprenant à éviter la logique dans les problèmes simples et à penser avec précaution seulement quand nécessaire. Il a également été cherché comment les VLMs pourraient décider dès le départ si la logique est nécessaire. Pour atteindre ceci, on propose TON (Think or Not), qui est une stratégie d'apprentissage en deux étapes : (i) une étape de fine-tuning normalisé (SFT) qui inclut l'opération 'thought dropout', introduisant un format think-or-not pour que les traces logiques puissent être remplacées par des pensées aléatoires, fournissant un refroidissement sélectif de la logique, et (ii) dans l'étape de GRPO, on décide si le modèle peut penser librement et on cherche à maximiser les récompenses des résultats liés à la tâche. Les résultats des expériences montrent que TON peut réduire de 90% le temps de terminaison par rapport à GRPO, améliorant l'efficience sans perdre la qualité. Par des évaluations sur différents niveaux de difficulté de tâches visuelles linguistiques, on a confirmé expérimentalement que les modèles apprennent à sauter progressivement les étapes logiques inutiles durant l'apprentissage. Ces résultats éclairent le chemin vers un approche d'apprentissage par récompense qui oriente les patrons logiques de manière similaire à celle humaine. Le code est disponible sur https://github.com/kokolerk/TON.",
      "upvotes": 5,
      "discussionId": "682fdd8791757629e1d58e77",
      "projectPage": "https://huggingface.co/collections/kolerk/ton-682ad9038395c21e228a645b",
      "githubRepo": "https://github.com/kokolerk/TON",
      "ai_summary": "TON, a two-stage training strategy combining supervised fine-tuning with thought dropout and Group Relative Policy Optimization, reduces unnecessary reasoning steps in vision-language models without sacrificing performance.",
      "ai_keywords": [
        "Reinforcement Learning (RL)",
        "vision-language models (VLMs)",
        "Group Relative Policy Optimization (GRPO)",
        "thought dropout",
        "selective reasoning"
      ]
    },
    "publishedAt": "2025-05-22T12:13:29.000Z",
    "title": "Think or Not? Selective Reasoning via Reinforcement Learning for\n  Vision-Language Models",
    "summary": "Reinforcement Learning (RL) has proven to be an effective post-training\nstrategy for enhancing reasoning in vision-language models (VLMs). Group\nRelative Policy Optimization (GRPO) is a recent prominent method that\nencourages models to generate complete reasoning traces before answering,\nleading to increased token usage and computational cost. Inspired by the\nhuman-like thinking process-where people skip reasoning for easy questions but\nthink carefully when needed-we explore how to enable VLMs to first decide when\nreasoning is necessary. To realize this, we propose TON, a two-stage training\nstrategy: (i) a supervised fine-tuning (SFT) stage with a simple yet effective\n'thought dropout' operation, where reasoning traces are randomly replaced with\nempty thoughts. This introduces a think-or-not format that serves as a cold\nstart for selective reasoning; (ii) a GRPO stage that enables the model to\nfreely explore when to think or not, while maximizing task-aware outcome\nrewards. Experimental results show that TON can reduce the completion length by\nup to 90% compared to vanilla GRPO, without sacrificing performance or even\nimproving it. Further evaluations across diverse vision-language tasks-covering\na range of reasoning difficulties under both 3B and 7B models-consistently\nreveal that the model progressively learns to bypass unnecessary reasoning\nsteps as training advances. These findings shed light on the path toward\nhuman-like reasoning patterns in reinforcement learning approaches. Our code is\navailable at https://github.com/kokolerk/TON.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16854.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64440be5af034cdfd69ca3a7",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64440be5af034cdfd69ca3a7/qmx24QiDFT29vleCxL9TX.jpeg",
      "fullname": "Qinghong (Kevin) Lin",
      "name": "KevinQHLin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 31
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.16400",
      "authors": [
        {
          "_id": "682fe6644640a9db4d1f31d9",
          "name": "Yang Chen",
          "hidden": false
        },
        {
          "_id": "682fe6644640a9db4d1f31da",
          "user": {
            "_id": "67d75b0117c2acac528f47b6",
            "avatarUrl": "/avatars/619aacd1a619aab64de3499ac3ee2229.svg",
            "isPro": false,
            "fullname": "Zhuolin Yang",
            "user": "zhuoliny",
            "type": "user"
          },
          "name": "Zhuolin Yang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-23T03:07:17.257Z",
          "hidden": false
        },
        {
          "_id": "682fe6644640a9db4d1f31db",
          "name": "Zihan Liu",
          "hidden": false
        },
        {
          "_id": "682fe6644640a9db4d1f31dc",
          "name": "Chankyu Lee",
          "hidden": false
        },
        {
          "_id": "682fe6644640a9db4d1f31dd",
          "name": "Peng Xu",
          "hidden": false
        },
        {
          "_id": "682fe6644640a9db4d1f31de",
          "name": "Mohammad Shoeybi",
          "hidden": false
        },
        {
          "_id": "682fe6644640a9db4d1f31df",
          "name": "Bryan Catanzaro",
          "hidden": false
        },
        {
          "_id": "682fe6644640a9db4d1f31e0",
          "user": {
            "_id": "663ee43bfeeb49803537da98",
            "avatarUrl": "/avatars/17c3e9c435cc36fb04b4589e6176a243.svg",
            "isPro": false,
            "fullname": "Wei Ping",
            "user": "wping",
            "type": "user"
          },
          "name": "Wei Ping",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-23T03:07:17.257Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T08:50:47.000Z",
      "submittedOnDailyAt": "2025-05-23T01:38:02.331Z",
      "title": "AceReason-Nemotron : Compréhension de la Mathématiques et du Code par l'Apprentissage Renor",
      "submittedOnDailyBy": {
        "_id": "62bc9d90e81dfd65cced9316",
        "avatarUrl": "/avatars/05df14cd1fdbc7d6a80d2960a05a94f0.svg",
        "isPro": false,
        "fullname": "Yang Chen",
        "user": "ychenNLP",
        "type": "user"
      },
      "summary": "Dans le développement de modèles logiques pour l'apprentissage par renforcement (RL) à grande échelle récent, la détection d'entraînement reste incertaine. Les détails d'implémentation des modèles de pointe, tels que DeepSeek-R1, le recupération de données et la détection d'entraînement RL, sont généralement omis. De plus, des études récentes ont montré que la traduction est plus efficace que l'RL pour des modèles petits. Dans ce travail, nous montrons que l'RL à grande échelle peut significativement améliorer la capacité logique de petits ou de modèles de taille intermédiaire, et qu'il peut atteindre des résultats meilleurs que ceux des modèles basés sur la traduction plus avancées. Nous construisons le processus d'entraînement de RL, effectuons des tests de dispersion et proposons un approche simple et efficace. Nous entraînons d'abord avec un chien qui n'a que des mathématiques, puis avec un chien qui n'a que du code. En particulier, l'RL avec des mathématiques montre seulement qu'il peut améliorer significativement les marqueurs de mathématiques de modèles forts de traduction (par exemple, +14.6%/+17.2% sur AIME 2025 pour des modèles de 7B/14B) et les tâches de logique du code (par exemple, +6.8%/+5.8% sur LiveCodeBench pour des modèles de 7B/14B). De plus, l'entraînement de RL avec du code étendu montre qu'il peut améliorer le rendement des marqueurs du code sans nécessité de résultats mathématiques ou de désaccations. Nous développons un puissant pipeline de recupération de données pour collecter des chiens difficiles, incluant des réponses de haute qualité et des cas de test, ce qui permet de tester l'RL basé sur les tests et de détecter l'entraînement RL dans les deux domaines. Enfin, nous établissons le rétroaction des principales expériences et incluons l'effet de l'amélioration de l'apprentissage de la chaîne de réponses croissante et de la stabilisation de l'actualisation de paramètres en ligne. L'RL développe les capacités logiques fondamentales acquises lors de l'apprentissage précédent et de la normalisation (par exemple, la traduction), dépasse les limites de la capacité logique du modèle et peut résoudre des problèmes qui n'avaient pas été résolus précédemment.",
      "upvotes": 5,
      "discussionId": "682fe6654640a9db4d1f3229",
      "ai_summary": "Large-scale reinforcement learning enhances reasoning capabilities in small and mid-sized models more effectively than distillation, achieving superior results in both math and code benchmarks.",
      "ai_keywords": [
        "reinforcement learning",
        "RL",
        "DeepSeek-R1",
        "data curation",
        "distillation",
        "math-only prompts",
        "code-only prompts",
        "AIME 2025",
        "LiveCodeBench",
        "curriculum learning",
        "on-policy parameter updates"
      ]
    },
    "publishedAt": "2025-05-22T04:50:47.000Z",
    "title": "AceReason-Nemotron: Advancing Math and Code Reasoning through\n  Reinforcement Learning",
    "summary": "Despite recent progress in large-scale reinforcement learning (RL) for\nreasoning, the training recipe for building high-performing reasoning models\nremains elusive. Key implementation details of frontier models, such as\nDeepSeek-R1, including data curation strategies and RL training recipe, are\noften omitted. Moreover, recent research indicates distillation remains more\neffective than RL for smaller models. In this work, we demonstrate that\nlarge-scale RL can significantly enhance the reasoning capabilities of strong,\nsmall- and mid-sized models, achieving results that surpass those of\nstate-of-the-art distillation-based models. We systematically study the RL\ntraining process through extensive ablations and propose a simple yet effective\napproach: first training on math-only prompts, then on code-only prompts.\nNotably, we find that math-only RL not only significantly enhances the\nperformance of strong distilled models on math benchmarks (e.g., +14.6% /\n+17.2% on AIME 2025 for the 7B / 14B models), but also code reasoning tasks\n(e.g., +6.8% / +5.8% on LiveCodeBench for the 7B / 14B models). In addition,\nextended code-only RL iterations further improve performance on code benchmarks\nwith minimal or no degradation in math results. We develop a robust data\ncuration pipeline to collect challenging prompts with high-quality, verifiable\nanswers and test cases to enable verification-based RL across both domains.\nFinally, we identify key experimental insights, including curriculum learning\nwith progressively increasing response lengths and the stabilizing effect of\non-policy parameter updates. We find that RL not only elicits the foundational\nreasoning capabilities acquired during pretraining and supervised fine-tuning\n(e.g., distillation), but also pushes the limits of the model's reasoning\nability, enabling it to solve problems that were previously unsolvable.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16400.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62bc9d90e81dfd65cced9316",
      "avatarUrl": "/avatars/05df14cd1fdbc7d6a80d2960a05a94f0.svg",
      "fullname": "Yang Chen",
      "name": "ychenNLP",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.16192",
      "authors": [
        {
          "_id": "68302db3b73ef22aebdce9c2",
          "name": "Chaoya Jiang",
          "hidden": false
        },
        {
          "_id": "68302db3b73ef22aebdce9c3",
          "name": "Yongrui Heng",
          "hidden": false
        },
        {
          "_id": "68302db3b73ef22aebdce9c4",
          "name": "Wei Ye",
          "hidden": false
        },
        {
          "_id": "68302db3b73ef22aebdce9c5",
          "name": "Han Yang",
          "hidden": false
        },
        {
          "_id": "68302db3b73ef22aebdce9c6",
          "name": "Haiyang Xu",
          "hidden": false
        },
        {
          "_id": "68302db3b73ef22aebdce9c7",
          "name": "Ming Yan",
          "hidden": false
        },
        {
          "_id": "68302db3b73ef22aebdce9c8",
          "name": "Ji Zhang",
          "hidden": false
        },
        {
          "_id": "68302db3b73ef22aebdce9c9",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "68302db3b73ef22aebdce9ca",
          "name": "Shikun Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T03:50:13.000Z",
      "submittedOnDailyAt": "2025-05-23T06:42:34.763Z",
      "title": "VLM-R^3 : Reconnaissance d'aires, théorie et amélioration par l'expansion de multiples modèles continus.",
      "submittedOnDailyBy": {
        "_id": "645b10e80c73ea27d13f7aca",
        "avatarUrl": "/avatars/95e565306472a15067440b5b43e07a6f.svg",
        "isPro": false,
        "fullname": "xuhaiyang",
        "user": "xhyandwyy",
        "type": "user"
      },
      "summary": "Récemment, les modèles de langue et de vision basés sur la raison (MLLM) ont atteint un certain niveau de succès dans la génération de longues phrases. Cependant, dans des tâches complexes, il est nécessaire de traiter dynamiquement les zones visuelles des images et de les réanalyser, et d'ajuster précisément la raison au contexte, ce qui est difficile avec des preuves visuelles. Nous présentons le cadre de travail VLM-R^3 (Visual Language Model with Region Recognition and Reasoning) pour les modèles de langue visuelle, qui aborde le reconnaissance de régions et la raison. Ce cadre fournit aux MLLM les capacités suivantes : (i) déterminer si il est nécessaire d'obtenir plus d'informations visuelles, (ii) décider où dans l'image la raison doit être appliquée, et (iii) relier les contenus des sous-images liés dans une séquence de raisons continue. L'essence de notre méthode est la sélection de régions informatives, la configuration de transformations appropriées (par exemple, extraction, expansion) et l'optimisation de politiques de renforcement basées sur des conditions de régions (R-GRPO) pour intégrer ces résultats dans les étapes ultérieures de raisonnement. Pour initier ces politiques, nous avons construit un corpus VLIR (Visual Language Interactive Reasoning) basé sur des dossiers et au niveau des étapes du contexte de raisonnement, qui inclut des interactions visuelles interactives. Ce corpus, en extension de tests comme MathVista, ScienceQA et d'autres benchmarks, a permis que VLM-R^3 atteigne un nouveau niveau de performance en 0 shot et avec des configurations de caractéristiques, en particulier en montrant de grands progrès dans les problèmes qui nécessitent des raisons spatiales complexes et des détails visuels précis.",
      "upvotes": 4,
      "discussionId": "68302db5b73ef22aebdcea32",
      "ai_summary": "VLM-R3 enhances multi-modal language models with region recognition and reasoning, achieving state-of-the-art performance on visual question answering tasks through region-conditioned reinforcement policy optimization.",
      "ai_keywords": [
        "VLM-R3",
        "Region-Conditioned Reinforcement Policy Optimization (R-GRPO)",
        "Visuo-Lingual Interleaved Rationale (VLIR) corpus",
        "MathVista",
        "ScienceQA"
      ]
    },
    "publishedAt": "2025-05-21T23:50:13.000Z",
    "title": "VLM-R^3: Region Recognition, Reasoning, and Refinement for Enhanced\n  Multimodal Chain-of-Thought",
    "summary": "Recently, reasoning-based MLLMs have achieved a degree of success in\ngenerating long-form textual reasoning chains. However, they still struggle\nwith complex tasks that necessitate dynamic and iterative focusing on and\nrevisiting of visual regions to achieve precise grounding of textual reasoning\nin visual evidence. We introduce VLM-R^3 (Visual\nLanguage Model with Region Recognition and\nReasoning), a framework that equips an MLLM with the ability to (i)\ndecide when additional visual evidence is needed, (ii) determine\nwhere to ground within the image, and (iii) seamlessly weave the\nrelevant sub-image content back into an interleaved chain-of-thought. The core\nof our method is Region-Conditioned Reinforcement Policy Optimization\n(R-GRPO), a training paradigm that rewards the model for selecting informative\nregions, formulating appropriate transformations (e.g.\\ crop, zoom), and\nintegrating the resulting visual context into subsequent reasoning steps. To\nbootstrap this policy, we compile a modest but carefully curated Visuo-Lingual\nInterleaved Rationale (VLIR) corpus that provides step-level supervision on\nregion selection and textual justification. Extensive experiments on MathVista,\nScienceQA, and other benchmarks show that VLM-R^3 sets a new state of the art\nin zero-shot and few-shot settings, with the largest gains appearing on\nquestions demanding subtle spatial reasoning or fine-grained visual cue\nextraction.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16192.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645b10e80c73ea27d13f7aca",
      "avatarUrl": "/avatars/95e565306472a15067440b5b43e07a6f.svg",
      "fullname": "xuhaiyang",
      "name": "xhyandwyy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.15963",
      "authors": [
        {
          "_id": "682fdcc0087ea62f1663df96",
          "name": "Shujun Liu",
          "hidden": false
        },
        {
          "_id": "682fdcc0087ea62f1663df97",
          "name": "Siyuan Wang",
          "hidden": false
        },
        {
          "_id": "682fdcc0087ea62f1663df98",
          "name": "Zejun Li",
          "hidden": false
        },
        {
          "_id": "682fdcc0087ea62f1663df99",
          "name": "Jianxiang Wang",
          "hidden": false
        },
        {
          "_id": "682fdcc0087ea62f1663df9a",
          "name": "Cheng Zeng",
          "hidden": false
        },
        {
          "_id": "682fdcc0087ea62f1663df9b",
          "name": "Zhongyu Wei",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T19:26:09.000Z",
      "submittedOnDailyAt": "2025-05-23T00:57:50.603Z",
      "title": "OViP : Langage de Vision Online de Préférences d'Apprentissage",
      "submittedOnDailyBy": {
        "_id": "6534c1fc23e0af0e0d7e8ebd",
        "avatarUrl": "/avatars/d65237d82aea19328f28b5a0cc93b271.svg",
        "isPro": false,
        "fullname": "Siyuan Wang",
        "user": "Siyuanyuan",
        "type": "user"
      },
      "summary": "Les modèles de langue visuolinguistique (LVLMs) sont vulnérables à la découverte et à la génération de contenu qui ne correspond pas à l'entrée visuelle. Les derniers approches proposent l'optimisation préférentielle directe (DPO) pour atténuer la découverte, mais ils utilisent souvent des échantillons négatifs prédéfinis ou édités de manière aléatoire, ce qui ne reflète pas efficacement les erreurs du modèle et limite l'efficacité de l'entraînement. Dans cette étude, nous proposons un cadre d'entraînement de préférence visuolinguistique en ligne (OViP) qui construit des données d'entraînement dynamiques basées sur les sorties que le modèle a trouvées. En identifiant les différences significatives entre pairs de réponses et en utilisant des modèles de diffusion pour synthétiser des images négatives, OViP génère des signaux de sous-objets liés en temps réel. Cette forme d'entraînement actif permet d'ajuster de manière adaptative le langage et la préférence visuelle. De plus, les protocoles d'évaluation sont améliorés pour améliorer la compréhension de la découverte et le compromis entre expressivité et découverte. Dans des expériences de découverte et de benchmark généraux, OViP montre son efficacité à réduire la découverte tout en maintenant la capacité de diversité essentielle.",
      "upvotes": 4,
      "discussionId": "682fdcc1087ea62f1663dfcb",
      "ai_summary": "OViP dynamically generates contrastive training data using a diffusion model to reduce hallucinations in large vision-language models while maintaining their multi-modal capabilities.",
      "ai_keywords": [
        "large vision-language models",
        "hallucination",
        "multi-modal Direct Preference Optimization",
        "OViP",
        "diffusion model",
        "contrastive training",
        "semantic differences",
        "failure-driven training"
      ]
    },
    "publishedAt": "2025-05-21T15:26:09.000Z",
    "title": "OViP: Online Vision-Language Preference Learning",
    "summary": "Large vision-language models (LVLMs) remain vulnerable to hallucination,\noften generating content misaligned with visual inputs. While recent approaches\nadvance multi-modal Direct Preference Optimization (DPO) to mitigate\nhallucination, they typically rely on predefined or randomly edited negative\nsamples that fail to reflect actual model errors, limiting training efficacy.\nIn this work, we propose an Online Vision-language Preference Learning (OViP)\nframework that dynamically constructs contrastive training data based on the\nmodel's own hallucinated outputs. By identifying semantic differences between\nsampled response pairs and synthesizing negative images using a diffusion\nmodel, OViP generates more relevant supervision signals in real time. This\nfailure-driven training enables adaptive alignment of both textual and visual\npreferences. Moreover, we refine existing evaluation protocols to better\ncapture the trade-off between hallucination suppression and expressiveness.\nExperiments on hallucination and general benchmarks demonstrate that OViP\neffectively reduces hallucinations while preserving core multi-modal\ncapabilities.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15963.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6534c1fc23e0af0e0d7e8ebd",
      "avatarUrl": "/avatars/d65237d82aea19328f28b5a0cc93b271.svg",
      "fullname": "Siyuan Wang",
      "name": "Siyuanyuan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.11711",
      "authors": [
        {
          "_id": "682e0d9540c6417d9962227a",
          "user": {
            "_id": "6255a34d7dacca56ac2b04e4",
            "avatarUrl": "/avatars/3e7751aa6ef7c880e3e36ac995c9a191.svg",
            "isPro": false,
            "fullname": "sagnik mukherjee",
            "user": "sagnikM",
            "type": "user"
          },
          "name": "Sagnik Mukherjee",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:17:26.467Z",
          "hidden": false
        },
        {
          "_id": "682e0d9540c6417d9962227b",
          "name": "Lifan Yuan",
          "hidden": false
        },
        {
          "_id": "682e0d9540c6417d9962227c",
          "name": "Dilek Hakkani-Tur",
          "hidden": false
        },
        {
          "_id": "682e0d9540c6417d9962227d",
          "name": "Hao Peng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-16T21:42:28.000Z",
      "submittedOnDailyAt": "2025-05-23T00:08:00.344Z",
      "title": "En l'apprentissage par récompense, on ajuste petites sous-réseaux de grands modèles de langue.",
      "submittedOnDailyBy": {
        "_id": "6255a34d7dacca56ac2b04e4",
        "avatarUrl": "/avatars/3e7751aa6ef7c880e3e36ac995c9a191.svg",
        "isPro": false,
        "fullname": "sagnik mukherjee",
        "user": "sagnikM",
        "type": "user"
      },
      "summary": "L'apprentissage par renforcement (RL) apporte une augmentation significative de la qualité des tâches de modèles de langage grands (LLMs) et de la concordance avec les valeurs humaines. Cependant, cette augmentation est obtenue en mettant à jour seulement un petit pourcentage de paramètres, entre 5% et 30%, sans modifier effectivement les autres. Ce phénomène est appelée sparsité de l'actualisation des paramètres en RL et est observé dans 10 familles différentes de LLMs et 7 algorithmes de RL largement utilisés (comme PPO, GRPO, DPO). Cette sparsité peut être due à la normalisation explicite de la sparsité ou aux contraintes architecturales. La précision est rétablie uniquement grâce à l'ajustement de sous-réseaux, permettant d'atteindre un rendement comparable à l'ajustement complet de la réseau. Les seeds aléatoires, les données d'entraînement et les réseaux de RL dépassent les attentes. L'analyse montre que cette sparsité implique des actualisations approximativement spars en toutes les matrices de paramètres, au lieu d'actualisations dans des couches spécifiques. De plus, la majorité des actualisations de paramètres sont des actualisations approximativement complètes dans toutes les dimensions, et les actualisations de RL mettent à jour un petit ensemble de paramètres qui couvrent approximativement toutes les dimensions possibles. Cette sparsité suggère que l'apprentissage de données proches de la distribution des politiques et la création de modèles similaires à des modèles entraînés précédemment (comme la normalisation KL et le clipping de gradients) ont un impact limité.",
      "upvotes": 4,
      "discussionId": "682e0d9540c6417d996222d7",
      "ai_summary": "Reinforcement learning improves large language models with minimal parameter updates, affecting only a small subnetwork without explicit sparsity techniques.",
      "ai_keywords": [
        "reinforcement learning",
        "large language models",
        "parameter update sparsity",
        "PPO",
        "GRPO",
        "DPO",
        "policy distribution",
        "KL regularization",
        "gradient clipping"
      ]
    },
    "publishedAt": "2025-05-16T17:42:28.000Z",
    "title": "Reinforcement Learning Finetunes Small Subnetworks in Large Language\n  Models",
    "summary": "Reinforcement learning (RL) yields substantial improvements in large language\nmodels (LLMs) downstream task performance and alignment with human values.\nSurprisingly, such large gains result from updating only a small subnetwork\ncomprising just 5 percent to 30 percent of the parameters, with the rest\neffectively unchanged. We refer to this phenomenon as parameter update sparsity\ninduced by RL. It is observed across all 7 widely used RL algorithms (e.g.,\nPPO, GRPO, DPO) and all 10 LLMs from different families in our experiments.\nThis sparsity is intrinsic and occurs without any explicit sparsity promoting\nregularizations or architectural constraints. Finetuning the subnetwork alone\nrecovers the test accuracy, and, remarkably, produces a model nearly identical\nto the one obtained via full finetuning. The subnetworks from different random\nseeds, training data, and even RL algorithms show substantially greater overlap\nthan expected by chance. Our analysis suggests that this sparsity is not due to\nupdating only a subset of layers, instead, nearly all parameter matrices\nreceive similarly sparse updates. Moreover, the updates to almost all parameter\nmatrices are nearly full-rank, suggesting RL updates a small subset of\nparameters that nevertheless span almost the full subspaces that the parameter\nmatrices can represent. We conjecture that the this update sparsity can be\nprimarily attributed to training on data that is near the policy distribution,\ntechniques that encourage the policy to remain close to the pretrained model,\nsuch as the KL regularization and gradient clipping, have limited impact.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.11711.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6255a34d7dacca56ac2b04e4",
      "avatarUrl": "/avatars/3e7751aa6ef7c880e3e36ac995c9a191.svg",
      "fullname": "sagnik mukherjee",
      "name": "sagnikM",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.16186",
      "authors": [
        {
          "_id": "683005352b4a4d1ce546568b",
          "name": "Kaiwen Zhou",
          "hidden": false
        },
        {
          "_id": "683005352b4a4d1ce546568c",
          "name": "Xuandong Zhao",
          "hidden": false
        },
        {
          "_id": "683005352b4a4d1ce546568d",
          "name": "Gaowen Liu",
          "hidden": false
        },
        {
          "_id": "683005352b4a4d1ce546568e",
          "name": "Jayanth Srinivasa",
          "hidden": false
        },
        {
          "_id": "683005352b4a4d1ce546568f",
          "name": "Aosong Feng",
          "hidden": false
        },
        {
          "_id": "683005352b4a4d1ce5465690",
          "name": "Dawn Song",
          "hidden": false
        },
        {
          "_id": "683005352b4a4d1ce5465691",
          "name": "Xin Eric Wang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64679a226192d39142245e5e/Qu6Z3hnjamfkpCZ9DuRkT.png"
      ],
      "publishedAt": "2025-05-22T03:46:03.000Z",
      "submittedOnDailyAt": "2025-05-23T03:51:00.196Z",
      "title": "SafeKey : Renforce les moments critiques d'insights d'insta pour des raisons de sécurité.",
      "submittedOnDailyBy": {
        "_id": "64679a226192d39142245e5e",
        "avatarUrl": "/avatars/05abee0b6317f100923936ca2099e9eb.svg",
        "isPro": false,
        "fullname": "Xin Eric Wang",
        "user": "xw-eric",
        "type": "user"
      },
      "summary": "Les modèles d'inférence à grande échelle (MGE) introduisent un nouveau paradigme dans l'ère actuelle, offrant des réponses avec des raisons explicites et atteignant un amélioration impressionnante dans des tâches complexes. Cependant, ils présentent un risque élevé de sécurité face aux consultations perjudiciales et aux attaques adversaires. Les dernières tentatives pour améliorer la sécurité des MGE, comme l'ajustement micro avec contrôle de normes (SFT), augmentent la sécurité, mais réduisent la capacité des modèles à répondre à des consultations qui n'avaient pas encore été rencontrées. On explore en profondeur la génération des MGE, identifie les moments de 'ah!' qui activent la sécurité et reconnaît que ces moments sont clés pour la génération de réponses sûres. Ce 'moment d'ah!' apparaît généralement sous forme de 'mots clés' dans le processus de compréhension de la consultation et indique si le modèle peut suivre la consultation de manière sûre. Sur cette perspective, on propose SafeKey, un méthode qui comprend deux objectifs complémentaires pour améliorer l'activation de la sécurité dans les mots clés : 1) renforcer les signaux de sécurité dans les représentations internes du modèle avant les mots clés avec un 'en-tête de sécurité double pas', et 2) améliorer la compréhension de la consultation avec le modèle de masques de consultation. Les expérimentations dans divers cadres de référence de sécurité montrent que notre méthode améliore significativement la capacité d'expansion de sécurité face aux attaques de 'réverser les freins' et aux consultations perjudiciales hors distribution, réduisant le taux moyen de dommages à 9,6% et maintenant la capacité générale. L'analyse montre comment SafeKey améliore la sécurité, modifiant l'attention interne et améliorant la qualité des représentations cachées.",
      "upvotes": 3,
      "discussionId": "683005362b4a4d1ce54656b1",
      "ai_summary": "SafeKey enhances the safety of large reasoning models by focusing on activating a safety aha moment in the key sentence through dual-path safety head and query-mask modeling, thereby improving generalization to harmful prompts.",
      "ai_keywords": [
        "Large Reasoning Models",
        "LRMs",
        "explicit reasoning",
        "safety risks",
        "adversarial attacks",
        "supervised fine-tuning",
        "SFT",
        "safety aha moment",
        "key sentence",
        "query understanding process",
        "Dual-Path Safety Head",
        "Query-Mask Modeling",
        "safety generalization",
        "jailbreak attacks",
        "out-of-distribution harmful prompts",
        "average harmfulness rate",
        "hidden representations"
      ]
    },
    "publishedAt": "2025-05-21T23:46:03.000Z",
    "title": "SafeKey: Amplifying Aha-Moment Insights for Safety Reasoning",
    "summary": "Large Reasoning Models (LRMs) introduce a new generation paradigm of\nexplicitly reasoning before answering, leading to remarkable improvements in\ncomplex tasks. However, they pose great safety risks against harmful queries\nand adversarial attacks. While recent mainstream safety efforts on LRMs,\nsupervised fine-tuning (SFT), improve safety performance, we find that\nSFT-aligned models struggle to generalize to unseen jailbreak prompts. After\nthorough investigation of LRMs' generation, we identify a safety aha moment\nthat can activate safety reasoning and lead to a safe response. This aha moment\ntypically appears in the `key sentence', which follows models' query\nunderstanding process and can indicate whether the model will proceed safely.\nBased on these insights, we propose SafeKey, including two complementary\nobjectives to better activate the safety aha moment in the key sentence: (1) a\nDual-Path Safety Head to enhance the safety signal in the model's internal\nrepresentations before the key sentence, and (2) a Query-Mask Modeling\nobjective to improve the models' attention on its query understanding, which\nhas important safety hints. Experiments across multiple safety benchmarks\ndemonstrate that our methods significantly improve safety generalization to a\nwide range of jailbreak attacks and out-of-distribution harmful prompts,\nlowering the average harmfulness rate by 9.6\\%, while maintaining general\nabilities. Our analysis reveals how SafeKey enhances safety by reshaping\ninternal attention and improving the quality of hidden representations.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64679a226192d39142245e5e/Qu6Z3hnjamfkpCZ9DuRkT.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16186.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64679a226192d39142245e5e",
      "avatarUrl": "/avatars/05abee0b6317f100923936ca2099e9eb.svg",
      "fullname": "Xin Eric Wang",
      "name": "xw-eric",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.15517",
      "authors": [
        {
          "_id": "682e8bc5b38184d0edcd1671",
          "user": {
            "_id": "66d2af23f040611f7cea1b1b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66d2af23f040611f7cea1b1b/vCToO_XgrLD0nfR8rzjZd.jpeg",
            "isPro": false,
            "fullname": "Kaiyuan Eric Chen",
            "user": "keplerccc",
            "type": "user"
          },
          "name": "Kaiyuan Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:16:21.981Z",
          "hidden": false
        },
        {
          "_id": "682e8bc5b38184d0edcd1672",
          "name": "Shuangyu Xie",
          "hidden": false
        },
        {
          "_id": "682e8bc5b38184d0edcd1673",
          "name": "Zehan Ma",
          "hidden": false
        },
        {
          "_id": "682e8bc5b38184d0edcd1674",
          "name": "Ken Goldberg",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/66d2af23f040611f7cea1b1b/f5gpZzVb5pLF6IXMt0_xZ.png"
      ],
      "publishedAt": "2025-05-21T13:42:52.000Z",
      "submittedOnDailyAt": "2025-05-23T03:39:07.197Z",
      "title": "Robo2VLM : Ensemble de données de manipulation de robots en dehors pour réponses visuelles aux questions",
      "submittedOnDailyBy": {
        "_id": "66d2af23f040611f7cea1b1b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66d2af23f040611f7cea1b1b/vCToO_XgrLD0nfR8rzjZd.jpeg",
        "isPro": false,
        "fullname": "Kaiyuan Eric Chen",
        "user": "keplerccc",
        "type": "user"
      },
      "summary": "La Vision Langue Longue-Mot (VLM) obtient des connaissances de la réalité et des capacités logiques générales à partir de corpus d'image-texte à l'échelle internet. Ces modèles renforcent la compréhension des scénarios et la planification des tâches, ainsi que soutiennent les politiques visuelles motrices basées sur les données de trajets de robots. Nous examinons un approche opposée - renforçant et évaluant les VLMs avec des données de trajets de robots riches, réels et divers. Dans cet article, nous présentons Robo2VLM, un cadre de travail pour générer des ensembles de données de réponses à des questions visuelles (VQA) pour les VLMs. Lorsqu'une personne fournit des données de trajets de robots téléopérés, Robo2VLM obtient des données réelles de capteurs non visuels et non explicatifs. Avec ces capteurs, les données de trajets de robots sont divisées en séquences d'étapes. À chaque étape, Robo2VLM comprend le scénario et l'interaction, identifiant le robot, le but de la tâche et les caractéristiques 3D des objets cibles. En utilisant ces caractéristiques, des templates de questions VQA logiques basés sur l'espace et l'interaction sont générés. Robo2VLM-1 fournit un ensemble de données à partir de grandes quantités de données de trajets de robots. Cet ensemble comprend 176k données de trajets de robots réels, 463 types de scénarios et 3,396 types de tâches de mouvement du robot, enregistrant 684,710 questions. Les résultats montrent que Robo2VLM-1 benchmarke et améliore les capacités logiques spatiales et d'interaction des VLMs.",
      "upvotes": 2,
      "discussionId": "682e8bc6b38184d0edcd16bc",
      "githubRepo": "https://github.com/KeplerC/robo2VLM",
      "ai_summary": "Robo2VLM, a framework for generating Visual Question Answering datasets using robot trajectory data, enhances and evaluates Vision-Language Models by leveraging sensory modalities and 3D property understanding.",
      "ai_keywords": [
        "Visual-Language Models",
        "Visual Question Answering",
        "VQA",
        "robot trajectory data",
        "end-effector pose",
        "gripper aperture",
        "force sensing",
        "manipulation phases",
        "3D properties",
        "task goal",
        "target object",
        "spatial reasoning",
        "goal-conditioned reasoning",
        "interaction reasoning",
        "Robo2VLM-1"
      ]
    },
    "publishedAt": "2025-05-21T09:42:52.000Z",
    "title": "Robo2VLM: Visual Question Answering from Large-Scale In-the-Wild Robot\n  Manipulation Datasets",
    "summary": "Vision-Language Models (VLMs) acquire real-world knowledge and general\nreasoning ability through Internet-scale image-text corpora. They can augment\nrobotic systems with scene understanding and task planning, and assist\nvisuomotor policies that are trained on robot trajectory data. We explore the\nreverse paradigm - using rich, real, multi-modal robot trajectory data to\nenhance and evaluate VLMs. In this paper, we present Robo2VLM, a Visual\nQuestion Answering (VQA) dataset generation framework for VLMs. Given a human\ntele-operated robot trajectory, Robo2VLM derives ground-truth from non-visual\nand non-descriptive sensory modalities, such as end-effector pose, gripper\naperture, and force sensing. Based on these modalities, it segments the robot\ntrajectory into a sequence of manipulation phases. At each phase, Robo2VLM uses\nscene and interaction understanding to identify 3D properties of the robot,\ntask goal, and the target object. The properties are used to generate\nrepresentative VQA queries - images with textural multiple-choice questions -\nbased on spatial, goal-conditioned, and interaction reasoning question\ntemplates. We curate Robo2VLM-1, a large-scale in-the-wild dataset with 684,710\nquestions covering 463 distinct scenes and 3,396 robotic manipulation tasks\nfrom 176k real robot trajectories. Results suggest that Robo2VLM-1 can\nbenchmark and improve VLM capabilities in spatial and interaction reasoning.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/66d2af23f040611f7cea1b1b/f5gpZzVb5pLF6IXMt0_xZ.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15517.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66d2af23f040611f7cea1b1b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66d2af23f040611f7cea1b1b/vCToO_XgrLD0nfR8rzjZd.jpeg",
      "fullname": "Kaiyuan Eric Chen",
      "name": "keplerccc",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.17019",
      "authors": [
        {
          "_id": "683041f868160a3c0e525cae",
          "name": "Chenhao Zhang",
          "hidden": false
        },
        {
          "_id": "683041f868160a3c0e525caf",
          "name": "Yazhe Niu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T17:59:53.000Z",
      "submittedOnDailyAt": "2025-05-23T08:11:12.176Z",
      "title": "\"Laissez que les Androides Rêvent d'Ovinos Électriques : Un Cadre de Compréhension et de Réaction aux Implications des Images Similaires à Celles Humaines\"",
      "submittedOnDailyBy": {
        "_id": "647daf00cfca67bc50f9a99f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647daf00cfca67bc50f9a99f/8Snmk1V6lZ8ecdTW3POfm.jpeg",
        "isPro": false,
        "fullname": "Chenhao(Leo) Zhang",
        "user": "MING-ZCH",
        "type": "user"
      },
      "summary": "Comprendre la métaphore des images est un problème important dans les systèmes d'IA, car les modèles actuels ont des difficultés à comprendre les significations culturelles, émotionnelles et contextuelles qui se trouvent dans le contenu des images. Les modèles de langage multimodal (MLLMs) montrent un excellent rendement dans des tâches de base telles que la Réponse à des Questions Visuelles (VQA), mais leur compréhension des significations des images est limitée par des défauts contextuels, ce qui inspire le propos d'un nouveau cadre appelé \"Let Androids Dream (LAD)\". LAD utilise un cadre à trois étapes pour aborder ces défauts : (1) la conversion de l'information visuelle en une représentation de texte multiniveau appelée \"Perception\", (2) la résolution indirecte des défauts et l'intégration de connaissances contextuelles lors de l'étape appelée \"Recherche\", et (3) la génération explicite de la signification contextuelle des images lors de l'étape appelée \"Raisonnement\". Le cadre utilisant le modèle GPT-4o-mini léger a démontré un rendement supérieur à 15+MLLM sur le benchmark de signification des images en anglais, a montré un grand améliorament sur le benchmark chinois, a montré un rendement comparable à celui de GPT-4o pour les questions à plusieurs réponses, et a amélioré de 36,7% pour les questions ouvertes. Ce projet fournit de nouvelles directives sur comment l'IA peut comprendre le sens des images, et est disponible pour l'utilisation publique (https://github.com/MING-ZCH/Let-Androids-Dream-of-Electric-Sheep).",
      "upvotes": 1,
      "discussionId": "683041f968160a3c0e525cf2",
      "githubRepo": "https://github.com/MING-ZCH/Let-Androids-Dream-of-Electric-Sheep",
      "ai_summary": "LAD, a three-stage framework using GPT-4o-mini, achieves state-of-the-art performance in image implication understanding and reasoning tasks across different languages and question types.",
      "ai_keywords": [
        "Visual Question Answering (VQA)",
        "image implication understanding",
        "reasoning",
        "multimodal large language models (MLLMs)",
        "cross-domain knowledge",
        "context-alignment",
        "Multiple-Choice Question (MCQ)",
        "Open-Style Question (OSQ)"
      ]
    },
    "publishedAt": "2025-05-22T13:59:53.000Z",
    "title": "Let Androids Dream of Electric Sheep: A Human-like Image Implication\n  Understanding and Reasoning Framework",
    "summary": "Metaphorical comprehension in images remains a critical challenge for AI\nsystems, as existing models struggle to grasp the nuanced cultural, emotional,\nand contextual implications embedded in visual content. While multimodal large\nlanguage models (MLLMs) excel in basic Visual Question Answer (VQA) tasks, they\nstruggle with a fundamental limitation on image implication tasks: contextual\ngaps that obscure the relationships between different visual elements and their\nabstract meanings. Inspired by the human cognitive process, we propose Let\nAndroids Dream (LAD), a novel framework for image implication understanding and\nreasoning. LAD addresses contextual missing through the three-stage framework:\n(1) Perception: converting visual information into rich and multi-level textual\nrepresentations, (2) Search: iteratively searching and integrating cross-domain\nknowledge to resolve ambiguity, and (3) Reasoning: generating context-alignment\nimage implication via explicit reasoning. Our framework with the lightweight\nGPT-4o-mini model achieves SOTA performance compared to 15+ MLLMs on English\nimage implication benchmark and a huge improvement on Chinese benchmark,\nperforming comparable with the GPT-4o model on Multiple-Choice Question (MCQ)\nand outperforms 36.7% on Open-Style Question (OSQ). Additionally, our work\nprovides new insights into how AI can more effectively interpret image\nimplications, advancing the field of vision-language reasoning and human-AI\ninteraction. Our project is publicly available at\nhttps://github.com/MING-ZCH/Let-Androids-Dream-of-Electric-Sheep.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17019.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "647daf00cfca67bc50f9a99f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647daf00cfca67bc50f9a99f/8Snmk1V6lZ8ecdTW3POfm.jpeg",
      "fullname": "Chenhao(Leo) Zhang",
      "name": "MING-ZCH",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.16612",
      "authors": [
        {
          "_id": "6830410d37efd0e958fcaf9c",
          "name": "Daniel Scalena",
          "hidden": false
        },
        {
          "_id": "6830410d37efd0e958fcaf9d",
          "user": {
            "_id": "5e7749883d77a72421292d07",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5e7749883d77a72421292d07/M4AmBReZk_otxCIG3o0bL.jpeg",
            "isPro": false,
            "fullname": "Gabriele Sarti",
            "user": "gsarti",
            "type": "user"
          },
          "name": "Gabriele Sarti",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-23T09:35:27.088Z",
          "hidden": false
        },
        {
          "_id": "6830410d37efd0e958fcaf9e",
          "name": "Arianna Bisazza",
          "hidden": false
        },
        {
          "_id": "6830410d37efd0e958fcaf9f",
          "name": "Elisabetta Fersini",
          "hidden": false
        },
        {
          "_id": "6830410d37efd0e958fcafa0",
          "name": "Malvina Nissim",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/5e7749883d77a72421292d07/NW7x7zlN8SRZybdNR_ad1.png",
        "https://cdn-uploads.huggingface.co/production/uploads/5e7749883d77a72421292d07/Y5HbXIQzjGZjHUu99IGGT.png"
      ],
      "publishedAt": "2025-05-22T12:47:16.000Z",
      "submittedOnDailyAt": "2025-05-23T08:07:20.310Z",
      "title": "Ce texte est déjà écrit en coréen, donc aucune traduction n'est nécessaire.",
      "submittedOnDailyBy": {
        "_id": "5e7749883d77a72421292d07",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5e7749883d77a72421292d07/M4AmBReZk_otxCIG3o0bL.jpeg",
        "isPro": false,
        "fullname": "Gabriele Sarti",
        "user": "gsarti",
        "type": "user"
      },
      "summary": "Les systèmes de traduction basés sur des modèles de langage grands de haute qualité (LLMs) ont facilité la production de traductions de sites qui reflètent des théories d'esthétique. Cependant, dans des cas où les exigences d'esthétique sont imprécises ou difficiles à transmettre, ces systèmes rencontrent des défis. Nous avons examiné diverses stratégies pour la portée et l'individualisation de la génération de LLMs, nous nous concentrant sur des domaines de traduction littéraire complexes. Nous présentons des stratégies et des interventions dans l'inférence pour contrôler la génération de modèles pour des styles individuels, et nous proposons un cadre relativement autonome en utilisant des concepts potentiels extraits de codificateurs rares. Nos résultats montrent que le contrôle renforce l'individualisation tout en maintenant la qualité de la traduction. De plus, nous avons examiné l'impact de l'influence de la représentation des LLMs et nous constatons que les couches de modèles liées à l'individualisation reçoivent la même influence, ce qui indique qu'ils fonctionnent sous la même structure.",
      "upvotes": 1,
      "discussionId": "6830410d37efd0e958fcafd5",
      "ai_summary": "Strategies including prompting and contrastive frameworks using latent concepts from sparse autoencoders effectively personalize LLM translations in low-resource settings while maintaining quality.",
      "ai_keywords": [
        "large language models",
        "LLMs",
        "literary translation",
        "prompting strategies",
        "inference-time interventions",
        "steering",
        "contrastive framework",
        "latent concepts",
        "sparse autoencoders",
        "personalization properties",
        "translation quality",
        "multi-shot prompting"
      ]
    },
    "publishedAt": "2025-05-22T08:47:16.000Z",
    "title": "Steering Large Language Models for Machine Translation Personalization",
    "summary": "High-quality machine translation systems based on large language models\n(LLMs) have simplified the production of personalized translations reflecting\nspecific stylistic constraints. However, these systems still struggle in\nsettings where stylistic requirements are less explicit and might be harder to\nconvey via prompting. We explore various strategies for personalizing\nLLM-generated translations in low-resource settings, focusing on the\nchallenging literary translation domain. We explore prompting strategies and\ninference-time interventions for steering model generations towards a\npersonalized style, and propose a contrastive framework exploiting latent\nconcepts extracted from sparse autoencoders to identify salient personalization\nproperties. Our results show that steering achieves strong personalization\nwhile preserving translation quality. We further examine the impact of steering\non LLM representations, finding model layers with a relevant impact for\npersonalization are impacted similarly by multi-shot prompting and our steering\nmethod, suggesting similar mechanism at play.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/5e7749883d77a72421292d07/NW7x7zlN8SRZybdNR_ad1.png",
      "https://cdn-uploads.huggingface.co/production/uploads/5e7749883d77a72421292d07/Y5HbXIQzjGZjHUu99IGGT.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16612.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5e7749883d77a72421292d07",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5e7749883d77a72421292d07/M4AmBReZk_otxCIG3o0bL.jpeg",
      "fullname": "Gabriele Sarti",
      "name": "gsarti",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 224
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.16170",
      "authors": [
        {
          "_id": "68301340cc0d12cd873342e1",
          "user": {
            "_id": "65c0de12efbb14b39c97f78e",
            "avatarUrl": "/avatars/18485f79427a35bd9e19f71b67c88dce.svg",
            "isPro": false,
            "fullname": "Yuqing Yang",
            "user": "ayyyq",
            "type": "user"
          },
          "name": "Yuqing Yang",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-23T06:35:23.129Z",
          "hidden": false
        },
        {
          "_id": "68301340cc0d12cd873342e2",
          "name": "Robin Jia",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T03:16:00.000Z",
      "submittedOnDailyAt": "2025-05-23T05:10:31.730Z",
      "title": "Modèle de Confiance et le rôle de la rétroaction - \"Comment les modèles de LLMs reconnaissent-ils les erreurs ?\"",
      "submittedOnDailyBy": {
        "_id": "65c0de12efbb14b39c97f78e",
        "avatarUrl": "/avatars/18485f79427a35bd9e19f71b67c88dce.svg",
        "isPro": false,
        "fullname": "Yuqing Yang",
        "user": "ayyyq",
        "type": "user"
      },
      "summary": "Les modèles de langage grands (LLMs) soulèvent la question de savoir si ils peuvent reconnaître des erreurs lorsqu'ils sont plus informés que ce qu'ils devraient être. Dans cette étude, nous définissons l'action de reconnaître des erreurs dans des réponses générées précédemment comme \"reculer\" et nous cherchons à comprendre dans quelles situations les LLMs le font. Tout d'abord, nous construisons un ensemble de données propre pour évaluer si les modèles reculent des erreurs qui contredisent leur connaissance paramétrique. Les LLMs ont la capacité de reculer, mais cela ne se produit pas fréquemment. Le reculer montre qu'il est lié à la confiance interne du modèle, et que les erreurs qui se confient à la réponse ne sont pas retirées. Des expériences dirigées montrent la relation causale entre la confiance interne et le reculer, et encouragent les modèles à vérifier leur réponse lorsqu'ils ne la confient pas, montrant comment cela peut changer leur comportement. Enfin, une petite régulation de superviseur montre que les modèles peuvent apprendre un connaissance interne plus précise, ce qui améliore considérablement la capacité de reculer. Les codes et ensembles de données sont disponibles sur https://github.com/ayyyq/llm-retraction.",
      "upvotes": 1,
      "discussionId": "68301341cc0d12cd87334304",
      "githubRepo": "https://github.com/ayyyq/llm-retraction",
      "ai_summary": "LLMs rarely retract incorrect answers they believe to be factually correct, but supervised fine-tuning can improve their retraction performance by refining their internal beliefs.",
      "ai_keywords": [
        "retraction",
        "model-specific datasets",
        "parametric knowledge",
        "internal belief",
        "self-verification",
        "attention behavior",
        "supervised fine-tuning"
      ]
    },
    "publishedAt": "2025-05-21T23:16:00.000Z",
    "title": "When Do LLMs Admit Their Mistakes? Understanding the Role of Model\n  Belief in Retraction",
    "summary": "Can large language models (LLMs) admit their mistakes when they should know\nbetter? In this work, we define the behavior of acknowledging errors in\npreviously generated answers as \"retraction\" and aim to understand when and why\nLLMs choose to retract. We first construct model-specific datasets to evaluate\nwhether a model will retract an incorrect answer that contradicts its own\nparametric knowledge. While LLMs are capable of retraction, they do so only\ninfrequently. We demonstrate that retraction is closely tied to previously\nidentified indicators of models' internal belief: models fail to retract wrong\nanswers that they \"believe\" to be factually correct. Steering experiments\nfurther demonstrate that internal belief causally influences model retraction.\nIn particular, when the model does not believe its answer, this not only\nencourages the model to attempt to verify the answer, but also alters attention\nbehavior during self-verification. Finally, we demonstrate that simple\nsupervised fine-tuning significantly improves retraction performance by helping\nthe model learn more accurate internal beliefs. Code and datasets are available\non https://github.com/ayyyq/llm-retraction.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16170.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65c0de12efbb14b39c97f78e",
      "avatarUrl": "/avatars/18485f79427a35bd9e19f71b67c88dce.svg",
      "fullname": "Yuqing Yang",
      "name": "ayyyq",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.16088",
      "authors": [
        {
          "_id": "68302befc518550cc0c2e505",
          "name": "Gagan Bhatia",
          "hidden": false
        },
        {
          "_id": "68302befc518550cc0c2e506",
          "name": "Maxime Peyrard",
          "hidden": false
        },
        {
          "_id": "68302befc518550cc0c2e507",
          "name": "Wei Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T00:06:29.000Z",
      "submittedOnDailyAt": "2025-05-23T06:35:22.205Z",
      "title": "Data Frame Work : Verrouillage de Tokenisateur Cache à l'Inférence Temporelle",
      "submittedOnDailyBy": {
        "_id": "60394599033b61166496163b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1614366097007-noauth.jpeg",
        "isPro": false,
        "fullname": "Gagan Bhatia",
        "user": "gagan3012",
        "type": "user"
      },
      "summary": "Le tokenizer moderne BPE divise souvent les dates en fragments sans sens. Par exemple, 20250312 est divisé en 202,503,12, ce qui augmente le nombre de tokens et cache une forte structure logique temporelle. Dans cette étude, nous présentons (1) des indicateurs simples et interprétables, appelés pourcentage de fragments de date, qui évaluent comment les composants de la date sont maintenus dans un multi-digit. (2) Nous mettons à jour DateAugBench, offrant un système contenant 6500 cas qui abordent trois tâches temporelles : la résolution de dates basée sur le contexte, les puzzles d'invariance de format et les opérations de dates historiques, modernes et futures. (3) À travers l'analyse des tests par couche et de l'attention causale, nous découvrons la structure abstraite des données que les modèles de langage grands utilisent pour combiner les composants du mois, du jour et de l'année pour effectuer la logique temporelle. Les résultats des expérimentations montrent que l'excès de fragmentation réduit la précision dans les dates non observées, comme les historiques et les futures, d'un 10%. De plus, la structure abstraite des données pour traiter les fragments de date se développe rapidement lorsque le taille du modèle augmente. Enfin, nous observons le passage de l'inférence où les modèles de LLM combinent les fragments, ce qui diffère généralement de la manière dont l'humanité l'interprète (année → mois → jour).",
      "upvotes": 1,
      "discussionId": "68302bf0c518550cc0c2e52c",
      "ai_summary": "New DateAugBench benchmarks reveal how modern tokenizers fragment dates, impacting the accuracy of temporal reasoning in large language models, which compensate for fragmentation more effectively as they grow larger.",
      "ai_keywords": [
        "BPE tokenizers",
        "date fragmentation ratio",
        "DateAugBench",
        "temporal reasoning tasks",
        "context-based date resolution",
        "format-invariance puzzles",
        "date arithmetic",
        "layer-wise probing",
        "causal attention-hop analyses",
        "date-abstraction mechanism",
        "large language models",
        "historical dates",
        "futuristic dates"
      ]
    },
    "publishedAt": "2025-05-21T20:06:29.000Z",
    "title": "Date Fragments: A Hidden Bottleneck of Tokenization for Temporal\n  Reasoning",
    "summary": "Modern BPE tokenizers often split calendar dates into meaningless fragments,\ne.g., 20250312 rightarrow 202, 503, 12, inflating token counts and obscuring\nthe inherent structure needed for robust temporal reasoning. In this work, we\n(1) introduce a simple yet interpretable metric, termed date fragmentation\nratio, that measures how faithfully a tokenizer preserves multi-digit date\ncomponents; (2) release DateAugBench, a suite of 6500 examples spanning three\ntemporal reasoning tasks: context-based date resolution, format-invariance\npuzzles, and date arithmetic across historical, contemporary, and future\nregimes; and (3) through layer-wise probing and causal attention-hop analyses,\nuncover an emergent date-abstraction mechanism whereby large language models\nstitch together the fragments of month, day, and year components for temporal\nreasoning. Our experiments show that excessive fragmentation correlates with\naccuracy drops of up to 10 points on uncommon dates like historical and\nfuturistic dates. Further, we find that the larger the model, the faster the\nemergent date abstraction that heals date fragments is accomplished. Lastly, we\nobserve a reasoning path that LLMs follow to assemble date fragments, typically\ndiffering from human interpretation (year rightarrow month rightarrow\nday).",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16088.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60394599033b61166496163b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1614366097007-noauth.jpeg",
      "fullname": "Gagan Bhatia",
      "name": "gagan3012",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 23
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.15865",
      "authors": [
        {
          "_id": "68301a21694f7a58a32919a8",
          "name": "Ingeol Baek",
          "hidden": false
        },
        {
          "_id": "68301a21694f7a58a32919a9",
          "name": "Hwan Chang",
          "hidden": false
        },
        {
          "_id": "68301a21694f7a58a32919aa",
          "name": "Sunghyun Ryu",
          "hidden": false
        },
        {
          "_id": "68301a21694f7a58a32919ab",
          "name": "Hwanhee Lee",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T10:53:41.000Z",
      "submittedOnDailyAt": "2025-05-23T05:19:58.152Z",
      "title": "Le modèle de langue visuelle explique comment le modèle reconnaît le texte dans les images et souligne clairement le rôle caractéristique du chapitre de Reconnaissance Optique de Caractères (OCR).",
      "submittedOnDailyBy": {
        "_id": "63f6f245e94ed998c46316df",
        "avatarUrl": "/avatars/9c0ec8682d4a85b96d2180602b1bbe6c.svg",
        "isPro": false,
        "fullname": "ingeolbaek",
        "user": "ingeol",
        "type": "user"
      },
      "summary": "Les modèles de langue et de vision à longue portée (LVLMs) ont connu un progrès clair, mais des erreurs persistent encore dans l'interprétation et la détection et l'interprétation de l'information contextuelle dans les images. Dans cet article, nous revoyons ces LVLMs et identifions des têtes spécifiques responsables de reconnaître des caractères dans les images. Ces têtes seront appelées \"têtes OCR\". Les résultats de ces têtes sont les suivants : 1) Elles ne sont pas rares : en contraste avec les têtes de recherche précédentes, de nombreuses têtes sont activées pour extraire des informations contextuelles des images. 2) Elles sont différentes en termes de qualité : les têtes OCR montrent une faible similitude avec les têtes de recherche générales. 3) Elles sont activées dynamiquement : la fréquence d'activation de ces têtes est fortement corrélée avec le score OCR. Ces résultats ont été appliqués à la technique de \"Chain-of-Thought\" (CoT) pour OCR et pour les têtes de recherche générales, et ont été validés par la masquage de ces têtes dans des tâches ultérieures. De plus, un améliorament de l'efficacité a été confirmé en réorganisant les valeurs des tokens synchroniseurs à l'intérieur des têtes OCR. Ces observations fournissent une compréhension plus profonde de la structure interne utilisée par les LVLMs pour traiter l'information de caractères incorporés dans les images.",
      "upvotes": 1,
      "discussionId": "68301a22694f7a58a32919ef",
      "ai_summary": "The study identifies and analyzes OCR Heads within Large Vision Language Models, revealing their unique activation patterns and roles in interpreting text within images.",
      "ai_keywords": [
        "Large Vision Language Models",
        "LVLMs",
        "Optical Character Recognition Head",
        "OCR Head",
        "retrieval heads",
        "Chain-of-Thought",
        "CoT",
        "sink-token values"
      ]
    },
    "publishedAt": "2025-05-21T06:53:41.000Z",
    "title": "How Do Large Vision-Language Models See Text in Image? Unveiling the\n  Distinctive Role of OCR Heads",
    "summary": "Despite significant advancements in Large Vision Language Models (LVLMs), a\ngap remains, particularly regarding their interpretability and how they locate\nand interpret textual information within images. In this paper, we explore\nvarious LVLMs to identify the specific heads responsible for recognizing text\nfrom images, which we term the Optical Character Recognition Head (OCR Head).\nOur findings regarding these heads are as follows: (1) Less Sparse: Unlike\nprevious retrieval heads, a large number of heads are activated to extract\ntextual information from images. (2) Qualitatively Distinct: OCR heads possess\nproperties that differ significantly from general retrieval heads, exhibiting\nlow similarity in their characteristics. (3) Statically Activated: The\nfrequency of activation for these heads closely aligns with their OCR scores.\nWe validate our findings in downstream tasks by applying Chain-of-Thought (CoT)\nto both OCR and conventional retrieval heads and by masking these heads. We\nalso demonstrate that redistributing sink-token values within the OCR heads\nimproves performance. These insights provide a deeper understanding of the\ninternal mechanisms LVLMs employ in processing embedded textual information in\nimages.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15865.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63f6f245e94ed998c46316df",
      "avatarUrl": "/avatars/9c0ec8682d4a85b96d2180602b1bbe6c.svg",
      "fullname": "ingeolbaek",
      "name": "ingeol",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.14395",
      "authors": [
        {
          "_id": "682db49f167398cff979ec27",
          "user": {
            "_id": "654f3cca8cc59d5b490b805b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/654f3cca8cc59d5b490b805b/WAZZjN4q8VGp3VS2lAWyy.png",
            "isPro": false,
            "fullname": "Seyoung Song",
            "user": "seyoungsong",
            "type": "user"
          },
          "name": "Seyoung Song",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-21T12:27:01.867Z",
          "hidden": false
        },
        {
          "_id": "682db49f167398cff979ec28",
          "name": "Seogyeong Jeong",
          "hidden": false
        },
        {
          "_id": "682db49f167398cff979ec29",
          "name": "Eunsu Kim",
          "hidden": false
        },
        {
          "_id": "682db49f167398cff979ec2a",
          "name": "Jiho Jin",
          "hidden": false
        },
        {
          "_id": "682db49f167398cff979ec2b",
          "name": "Dongkwan Kim",
          "hidden": false
        },
        {
          "_id": "682db49f167398cff979ec2c",
          "name": "Jay Shin",
          "hidden": false
        },
        {
          "_id": "682db49f167398cff979ec2d",
          "user": {
            "_id": "60e0251ea9b5d8282481f2b7",
            "avatarUrl": "/avatars/43441373af054a6184c22097bfeb97e4.svg",
            "isPro": false,
            "fullname": "Alice Oh",
            "user": "aliceoh",
            "type": "user"
          },
          "name": "Alice Oh",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-21T19:06:44.285Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/654f3cca8cc59d5b490b805b/cxsyLLbll1-fv-itFCzp_.png"
      ],
      "publishedAt": "2025-05-20T14:14:00.000Z",
      "submittedOnDailyAt": "2025-05-23T07:02:40.832Z",
      "title": "MUG-Eval : Marceau pour l'Évaluation Virtuelle de la Génération Multilingue\n  Compétences Disponibles pour Chaque Langue",
      "submittedOnDailyBy": {
        "_id": "654f3cca8cc59d5b490b805b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/654f3cca8cc59d5b490b805b/WAZZjN4q8VGp3VS2lAWyy.png",
        "isPro": false,
        "fullname": "Seyoung Song",
        "user": "seyoungsong",
        "type": "user"
      },
      "summary": "La évaluation de la capacité à générer du texte dans les modèles de langage grands (LLMs) est un défi, surtout dans les langues à faible ressource où les méthodes d'évaluation directes sont limitées. Nous proposons un nouveau cadre de référence appelé MUG-Eval. Ce cadre de référence transforme les benchmarks existants en tâches de dialogue pour évaluer la capacité à générer du texte des LLMs par langue. En particulier, ces tâches de dialogue sont conçues pour faciliter une communication efficace dans la langue cible. De plus, le succès dans ces tâches est utilisé comme indicateur de succès dans la génération du dialogue. Notre approche offre deux principales avantages. Une est que ce n'est pas dépendant de tools NLP spécifiques par langue ou de jeux de données étiquetés. La seconde est qu'il ne dépend pas des juges basés sur des modèles de langage grands (LLMs-as-judges). Par conséquent, la qualité de l'évaluation ne diminue pas en dehors du domaine des langues à haute ressource. Nous évaluons 8 modèles de langage grands dans un ensemble de 30 langues, qui incluent des catégories de haut, moyen et faible ressource. Nous démontrons que MUG-Eval a une forte corrélation avec les benchmarks existants et permet la comparaison de langues et de modèles. Notre cadre de référence offre une solution efficace en termes de ressources pour l'évaluation de la génération par langue et peut être étendu à des milliers de langues.",
      "upvotes": 1,
      "discussionId": "682db4a0167398cff979ec67",
      "ai_summary": "MUG-Eval assesses LLMs' multilingual generation by transforming benchmarks into conversational tasks, offering a language-independent and NLP tool-free method that correlates well with established benchmarks.",
      "ai_keywords": [
        "large language models",
        "LLMs",
        "multilingual generation",
        "conversational tasks",
        "task success rate",
        "low-resource languages",
        "high-resource languages",
        "NLP tools",
        "annotated datasets",
        "MUG-Eval",
        "standardized comparisons"
      ]
    },
    "publishedAt": "2025-05-20T10:14:00.000Z",
    "title": "MUG-Eval: A Proxy Evaluation Framework for Multilingual Generation\n  Capabilities in Any Language",
    "summary": "Evaluating text generation capabilities of large language models (LLMs) is\nchallenging, particularly for low-resource languages where methods for direct\nassessment are scarce. We propose MUG-Eval, a novel framework that evaluates\nLLMs' multilingual generation capabilities by transforming existing benchmarks\ninto conversational tasks and measuring the LLMs' accuracies on those tasks. We\nspecifically designed these conversational tasks to require effective\ncommunication in the target language. Then, we simply use task success rate as\na proxy of successful conversation generation. Our approach offers two key\nadvantages: it is independent of language-specific NLP tools or annotated\ndatasets, which are limited for most languages, and it does not rely on\nLLMs-as-judges, whose evaluation quality degrades outside a few high-resource\nlanguages. We evaluate 8 LLMs across 30 languages spanning high, mid, and\nlow-resource categories, and we find that MUG-Eval correlates strongly with\nestablished benchmarks (r > 0.75) while enabling standardized comparisons\nacross languages and models. Our framework provides a robust and\nresource-efficient solution for evaluating multilingual generation that can be\nextended to thousands of languages.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/654f3cca8cc59d5b490b805b/cxsyLLbll1-fv-itFCzp_.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14395.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "654f3cca8cc59d5b490b805b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/654f3cca8cc59d5b490b805b/WAZZjN4q8VGp3VS2lAWyy.png",
      "fullname": "Seyoung Song",
      "name": "seyoungsong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.16048",
      "authors": [
        {
          "_id": "68302100d260f25aad14b1c7",
          "user": {
            "_id": "62a1e17591f85abff79c2cdf",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654776114657-noauth.jpeg",
            "isPro": false,
            "fullname": "Philipp Siedler",
            "user": "philippds",
            "type": "user"
          },
          "name": "Philipp D. Siedler",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-23T07:26:08.014Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T22:00:20.000Z",
      "submittedOnDailyAt": "2025-05-23T05:49:26.287Z",
      "title": "SPhyR : Description physique spatiale du benchmark de la distribution de la matière",
      "submittedOnDailyBy": {
        "_id": "62a1e17591f85abff79c2cdf",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654776114657-noauth.jpeg",
        "isPro": false,
        "fullname": "Philipp Siedler",
        "user": "philippds",
        "type": "user"
      },
      "summary": "Nous présentons un nouveau ensemble de données basé sur l'optimisation topologique. L'optimisation topologique est un méthode qui calcule la distribution optimale de matériaux dans un espace de conception, sous des conditions de charges et de supports spécifiques. Dans cet ensemble de données, nous fournissons aux modèles de langage à grande échelle (LLM) des conditions frontières bidimensionnelles, des forces appliquées et des supports, pour expliquer pourquoi une distribution optimale de matériaux est obtenue. Cet ensemble de données comprend diverses tâches, allant du remplissage d'aires masquées dans des structures jusqu'à la prédiction de distributions complètes de matériaux. Pour résoudre ces tâches, il est nécessaire de comprendre le flux de forces sous les contraintes données, expliquer pourquoi cela ne dépend pas de simulations ou de modèles physiques clairs. Ainsi, la tâche est d'expliquer la stabilité de la structure et l'organisation de l'espace. Notre ensemble de données vise à évaluer la capacité de raisonnement spatial et physique dans un environnement bidimensionnel, offrant une perspective complémentaire aux référentiels linguistiques et logiques traditionnels.",
      "upvotes": 0,
      "discussionId": "68302101d260f25aad14b215",
      "githubRepo": "https://github.com/philippds/SPhyR",
      "ai_summary": "A dataset benchmarks spatial and physical reasoning of LLMs using topology optimization tasks without simulation tools.",
      "ai_keywords": [
        "Large Language Models (LLM)",
        "topology optimization",
        "material distribution",
        "structural stability",
        "spatial reasoning",
        "physical reasoning",
        "boundary conditions",
        "applied forces",
        "supports"
      ]
    },
    "publishedAt": "2025-05-21T18:00:20.000Z",
    "title": "SPhyR: Spatial-Physical Reasoning Benchmark on Material Distribution",
    "summary": "We introduce a novel dataset designed to benchmark the physical and spatial\nreasoning capabilities of Large Language Models (LLM) based on topology\noptimization, a method for computing optimal material distributions within a\ndesign space under prescribed loads and supports. In this dataset, LLMs are\nprovided with conditions such as 2D boundary, applied forces and supports, and\nmust reason about the resulting optimal material distribution. The dataset\nincludes a variety of tasks, ranging from filling in masked regions within\npartial structures to predicting complete material distributions. Solving these\ntasks requires understanding the flow of forces and the required material\ndistribution under given constraints, without access to simulation tools or\nexplicit physical models, challenging models to reason about structural\nstability and spatial organization. Our dataset targets the evaluation of\nspatial and physical reasoning abilities in 2D settings, offering a\ncomplementary perspective to traditional language and logic benchmarks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16048.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62a1e17591f85abff79c2cdf",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654776114657-noauth.jpeg",
      "fullname": "Philipp Siedler",
      "name": "philippds",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  }
]