[
  {
    "paper": {
      "id": "2506.15675",
      "authors": [
        {
          "_id": "6853946599bf39f9665c79e0",
          "name": "Zhen Li",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79e1",
          "name": "Chuanhao Li",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79e2",
          "name": "Xiaofeng Mao",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79e3",
          "name": "Shaoheng Lin",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79e4",
          "name": "Ming Li",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79e5",
          "name": "Shitian Zhao",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79e6",
          "name": "Zhaopan Xu",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79e7",
          "name": "Xinyue Li",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79e8",
          "name": "Yukang Feng",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79e9",
          "name": "Jianwen Sun",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79ea",
          "name": "Zizhen Li",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79eb",
          "name": "Fanrui Zhang",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79ec",
          "name": "Jiaxin Ai",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79ed",
          "name": "Zhixiang Wang",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79ee",
          "name": "Yuwei Wu",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79ef",
          "name": "Tong He",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79f0",
          "name": "Jiangmiao Pang",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79f1",
          "name": "Yu Qiao",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79f2",
          "name": "Yunde Jia",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79f3",
          "user": {
            "_id": "63527f4e7d071f23d085ad45",
            "avatarUrl": "/avatars/99a51adef5673b3ac1a8c02eb47759c4.svg",
            "isPro": false,
            "fullname": "KAIPENG ZHANG",
            "user": "kpzhang",
            "type": "user"
          },
          "name": "Kaipeng Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-19T10:09:38.100Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/65f1713552c38a91e0a445e8/GFrjhPvZnILgeTd_w2kvc.mp4"
      ],
      "publishedAt": "2025-06-18T17:57:06.000Z",
      "submittedOnDailyAt": "2025-06-19T03:16:13.113Z",
      "title": "World : Dataset pour l'Exploration Vidéo du Monde",
      "submittedOnDailyBy": {
        "_id": "65f1713552c38a91e0a445e8",
        "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
        "isPro": false,
        "fullname": "kaipeng",
        "user": "kpzhang996",
        "type": "user"
      },
      "summary": "La technologie de génération vidéo a connu un développement impressionnant et doit devenir la base pour explorer le monde interactif. Cependant, les ensembles de données vidéo actuels ne sont pas adaptés pour l'entraînement de l'exploration du monde. Ceux-ci présentent des limitations telles que des lieux limités, des temps courts, des scènes statiques et une manque d'explications sur l'exploration et le monde. Dans cet article, nous présentons le premier ensemble de vidéo à point de vue premier d'un monde (Sekai), de haute qualité, qui signifie \"monde\". Cet ensemble de données a collecté plus de 5 000 heures de vidéo à partir de la perspective de caminants et de pilotes (FPV et UVA) dans plus de 750 villes de plus de 100 pays et régions, incluant une description complète de l'exploration du monde. Des outils efficaces et efficaces ont été développés pour la collecte, le pré-traitement et l'annotation basés sur des descriptions de l'ensemble de données. Les expériences montrant la qualité de l'ensemble de données ont utilisé une partie de ceux-ci pour entraîner des modèles d'exploration du monde interactif appelés \"YUME\" (qui signifie \"rêve\" en japonais). On pense que Sekai aidera à la génération de vidéo et à l'exploration du monde, et promettra des applications précieuses.",
      "upvotes": 26,
      "discussionId": "6853946599bf39f9665c79f4",
      "projectPage": "https://lixsp11.github.io/sekai-project/",
      "githubRepo": "https://github.com/Lixsp11/sekai-codebase",
      "ai_summary": "Sekai, a worldwide video dataset with comprehensive annotations, is introduced to support world exploration applications, enhancing video generation models.",
      "ai_keywords": [
        "first-person view",
        "worldwide video dataset",
        "rich annotations",
        "FPV",
        "UVA",
        "video collection",
        "pre-processing",
        "camera trajectories",
        "interactive video world exploration model"
      ]
    },
    "publishedAt": "2025-06-18T13:57:06.000Z",
    "title": "Sekai: A Video Dataset towards World Exploration",
    "summary": "Video generation techniques have made remarkable progress, promising to be\nthe foundation of interactive world exploration. However, existing video\ngeneration datasets are not well-suited for world exploration training as they\nsuffer from some limitations: limited locations, short duration, static scenes,\nand a lack of annotations about exploration and the world. In this paper, we\nintroduce Sekai (meaning ``world'' in Japanese), a high-quality first-person\nview worldwide video dataset with rich annotations for world exploration. It\nconsists of over 5,000 hours of walking or drone view (FPV and UVA) videos from\nover 100 countries and regions across 750 cities. We develop an efficient and\neffective toolbox to collect, pre-process and annotate videos with location,\nscene, weather, crowd density, captions, and camera trajectories. Experiments\ndemonstrate the quality of the dataset. And, we use a subset to train an\ninteractive video world exploration model, named YUME (meaning ``dream'' in\nJapanese). We believe Sekai will benefit the area of video generation and world\nexploration, and motivate valuable applications.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/65f1713552c38a91e0a445e8/GFrjhPvZnILgeTd_w2kvc.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.15675.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65f1713552c38a91e0a445e8",
      "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
      "fullname": "kaipeng",
      "name": "kpzhang996",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.15681",
      "authors": [
        {
          "_id": "68536fc899bf39f9665c7961",
          "user": {
            "_id": "657152eb12f162153b50ec9d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/657152eb12f162153b50ec9d/qnldHP35PclV0pDz_05q8.jpeg",
            "isPro": false,
            "fullname": "Byung-Kwan Lee",
            "user": "BK-Lee",
            "type": "user"
          },
          "name": "Byung-Kwan Lee",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-19T09:10:25.415Z",
          "hidden": false
        },
        {
          "_id": "68536fc899bf39f9665c7962",
          "user": {
            "_id": "65b33e5f7cd0069ad648c4e8",
            "avatarUrl": "/avatars/1a746ea535cffa92ea08006e05ea414a.svg",
            "isPro": false,
            "fullname": "Ryo Hachiuma",
            "user": "rhachiuma",
            "type": "user"
          },
          "name": "Ryo Hachiuma",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-19T02:02:49.546Z",
          "hidden": false
        },
        {
          "_id": "68536fc899bf39f9665c7963",
          "name": "Yong Man Ro",
          "hidden": false
        },
        {
          "_id": "68536fc899bf39f9665c7964",
          "name": "Yu-Chiang Frank Wang",
          "hidden": false
        },
        {
          "_id": "68536fc899bf39f9665c7965",
          "name": "Yueh-Hua Wu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/657152eb12f162153b50ec9d/2xNUivkkqkJWJLWtIPNvb.mp4"
      ],
      "publishedAt": "2025-06-18T17:59:49.000Z",
      "submittedOnDailyAt": "2025-06-19T00:36:10.331Z",
      "title": "GenRecal : Génération de modèles de vision-langue avec ajustement vers le plus petit côté après redimensionnement",
      "submittedOnDailyBy": {
        "_id": "657152eb12f162153b50ec9d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/657152eb12f162153b50ec9d/qnldHP35PclV0pDz_05q8.jpeg",
        "isPro": false,
        "fullname": "Byung-Kwan Lee",
        "user": "BK-Lee",
        "type": "user"
      },
      "summary": "Le développement récent des modèles de langage visuel (VLMs) a permis d'atteindre des performances similaires à ceux des systèmes fermés comme GPT-4V, en utilisant de grands modèles de langage (LLMs). Cependant, le fonctionnement de ces modèles dans des environnements réels, surtout sur des dispositifs limités en ressources, a été compliqué par les exigences de calcul importantes. Cela a conduit à s'intéresser à réduire de manière efficace les VLMs à grande échelle vers des VLMs à petite échelle. La diversité structurale des VLMs est un problème important dans ce contexte. Ces modèles sont construits sur différents LLMs et présentent des changements dans le taille de mot, la division des mots et l'ordre des indices de mots, ainsi que des types de tâches différents. Pour résoudre les limitations spécifiques d'un type de VLM, nous présentons GenRecal. GenRecal est un nouveau cadre de conception général qui inclut un ré-calibrateur pour ajuster et adapter les représentations de caractéristiques entre différents VLMs. GenRecal a considérablement amélioré la performance dans des benchmarks difficiles grâce à de nombreux expérimentations, et a finalement montré de surpasser les grands VLMs ouverts et fermés.",
      "upvotes": 13,
      "discussionId": "68536fc899bf39f9665c7966",
      "projectPage": "https://byungkwanlee.github.io/GenRecal-page/",
      "ai_summary": "GenRecal, a novel distillation framework, improves performance of vision-language models by aligning feature representations across different architectures.",
      "ai_keywords": [
        "vision-language models",
        "large language models",
        "distillation framework",
        "GenerRecal",
        "recalibration",
        "feature representations",
        "heterogeneous VLMs"
      ]
    },
    "publishedAt": "2025-06-18T13:59:49.000Z",
    "title": "GenRecal: Generation after Recalibration from Large to Small\n  Vision-Language Models",
    "summary": "Recent advancements in vision-language models (VLMs) have leveraged large\nlanguage models (LLMs) to achieve performance on par with closed-source systems\nlike GPT-4V. However, deploying these models in real-world scenarios,\nparticularly on resource-constrained devices, remains challenging due to their\nsubstantial computational demands. This has spurred interest in distilling\nknowledge from large VLMs into smaller, more efficient counterparts. A key\nchallenge arises here from the diversity of VLM architectures, which are built\non different LLMs and employ varying token types-differing in vocabulary size,\ntoken splits, and token index ordering. To address this challenge of limitation\nto a specific VLM type, we present Generation after Recalibration (GenRecal), a\nnovel, general-purpose distillation framework for VLMs. GenRecal incorporates a\nRecalibrator that aligns and adapts feature representations between\nheterogeneous VLMs, enabling effective knowledge transfer across different\ntypes of VLMs. Through extensive experiments on multiple challenging\nbenchmarks, we demonstrate that GenRecal significantly improves baseline\nperformances, eventually outperforming large-scale open- and closed-source\nVLMs.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/657152eb12f162153b50ec9d/2xNUivkkqkJWJLWtIPNvb.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.15681.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "657152eb12f162153b50ec9d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/657152eb12f162153b50ec9d/qnldHP35PclV0pDz_05q8.jpeg",
      "fullname": "Byung-Kwan Lee",
      "name": "BK-Lee",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 56
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.15677",
      "authors": [
        {
          "_id": "68536b2399bf39f9665c794c",
          "name": "Yining Hong",
          "hidden": false
        },
        {
          "_id": "68536b2399bf39f9665c794d",
          "name": "Rui Sun",
          "hidden": false
        },
        {
          "_id": "68536b2399bf39f9665c794e",
          "name": "Bingxuan Li",
          "hidden": false
        },
        {
          "_id": "68536b2399bf39f9665c794f",
          "name": "Xingcheng Yao",
          "hidden": false
        },
        {
          "_id": "68536b2399bf39f9665c7950",
          "name": "Maxine Wu",
          "hidden": false
        },
        {
          "_id": "68536b2399bf39f9665c7951",
          "name": "Alexander Chien",
          "hidden": false
        },
        {
          "_id": "68536b2399bf39f9665c7952",
          "name": "Da Yin",
          "hidden": false
        },
        {
          "_id": "68536b2399bf39f9665c7953",
          "name": "Ying Nian Wu",
          "hidden": false
        },
        {
          "_id": "68536b2399bf39f9665c7954",
          "name": "Zhecan James Wang",
          "hidden": false
        },
        {
          "_id": "68536b2399bf39f9665c7955",
          "name": "Kai-Wei Chang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-18T17:58:17.000Z",
      "submittedOnDailyAt": "2025-06-19T00:14:22.240Z",
      "title": "Intelligence Artificielle Intégrée qui Connecte le Monde Physique et Numérique",
      "submittedOnDailyBy": {
        "_id": "6431b64df76c34519e93d1ba",
        "avatarUrl": "/avatars/ea577762b6b4798f87a7a3f1d53d082c.svg",
        "isPro": true,
        "fullname": "Yining Hong",
        "user": "evelynhong",
        "type": "user"
      },
      "summary": "Les agents actuels d'IA sont principalement codés : cela inclut la capacité de rechercher et d'expliquer logiquement de grandes quantités d'informations et de connaissances numériques disponibles en ligne, ainsi que la capacité de reconnaître, planifier et agir pour interagir avec le monde physique. Cependant, il est rare qu'ils puissent effectuer ces deux tâches simultanément. Cette séparation limite leur capacité à intégrer l'intelligence physique et numérique pour résoudre des tâches. Par exemple, préparer un plat de cuisine en fonction de recettes en ligne, naviguer en utilisant des données dynamiques de cartes, ou interpréter des indications de réalité augmentée à partir de données web. Nous introduisons un nouveau paradigme qui naturalement relie les connaissances expérimentales et les raisons de vendeurs. Pour mettre en œuvre ce concept, nous avons d'abord développé une plateforme de simulation intégrée qui est fortement associée à des environnements 3D réalistes d'intérieurs et d'extérieurs, ainsi qu'à une interface fonctionnelle de vendeurs. Avec cette plateforme, nous construisons et lanchons diverses tâches telles que la cuisine, la navigation, le commerce, le tourisme et la localisation. Ces tâches nécessitent l'intégration des deux domaines physique et numérique, ce qui nécessite une évaluation systématique de l'intelligence dans des zones croisées. Les résultats des expériences montrent une grande différence de rendement entre les systèmes d'IA les plus avancés et les capacités humaines, révélant à la fois des problèmes et des opportunités au point de rencontre du reconnaissance expérimentale et de l'accès au savoir de vendeurs. Tous les datasets, les codes et les sites web sont disponibles sur notre page de projet (https://embodied-web-agent.github.io/).",
      "upvotes": 7,
      "discussionId": "68536b2399bf39f9665c7956",
      "ai_summary": "Embodied Web Agents integrate physical interaction and web-scale reasoning to assess cross-domain intelligence in a novel benchmark environment.",
      "ai_keywords": [
        "Embodied Web Agents",
        "task environments",
        "simulation platform",
        "3D indoor and outdoor environments",
        "functional web interfaces",
        "Embodied Web Agents Benchmark",
        "systematic assessment",
        "cross-domain intelligence",
        "embodied cognition"
      ]
    },
    "publishedAt": "2025-06-18T13:58:17.000Z",
    "title": "Embodied Web Agents: Bridging Physical-Digital Realms for Integrated\n  Agent Intelligence",
    "summary": "AI agents today are mostly siloed - they either retrieve and reason over vast\namount of digital information and knowledge obtained online; or interact with\nthe physical world through embodied perception, planning and action - but\nrarely both. This separation limits their ability to solve tasks that require\nintegrated physical and digital intelligence, such as cooking from online\nrecipes, navigating with dynamic map data, or interpreting real-world landmarks\nusing web knowledge. We introduce Embodied Web Agents, a novel paradigm for AI\nagents that fluidly bridge embodiment and web-scale reasoning. To\noperationalize this concept, we first develop the Embodied Web Agents task\nenvironments, a unified simulation platform that tightly integrates realistic\n3D indoor and outdoor environments with functional web interfaces. Building\nupon this platform, we construct and release the Embodied Web Agents Benchmark,\nwhich encompasses a diverse suite of tasks including cooking, navigation,\nshopping, tourism, and geolocation - all requiring coordinated reasoning across\nphysical and digital realms for systematic assessment of cross-domain\nintelligence. Experimental results reveal significant performance gaps between\nstate-of-the-art AI systems and human capabilities, establishing both\nchallenges and opportunities at the intersection of embodied cognition and\nweb-scale knowledge access. All datasets, codes and websites are publicly\navailable at our project page https://embodied-web-agent.github.io/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.15677.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6431b64df76c34519e93d1ba",
      "avatarUrl": "/avatars/ea577762b6b4798f87a7a3f1d53d082c.svg",
      "fullname": "Yining Hong",
      "name": "evelynhong",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.15068",
      "authors": [
        {
          "_id": "68536bf399bf39f9665c7958",
          "name": "Zongxia Li",
          "hidden": false
        },
        {
          "_id": "68536bf399bf39f9665c7959",
          "name": "Yapei Chang",
          "hidden": false
        },
        {
          "_id": "68536bf399bf39f9665c795a",
          "name": "Yuhang Zhou",
          "hidden": false
        },
        {
          "_id": "68536bf399bf39f9665c795b",
          "name": "Xiyang Wu",
          "hidden": false
        },
        {
          "_id": "68536bf399bf39f9665c795c",
          "name": "Zichao Liang",
          "hidden": false
        },
        {
          "_id": "68536bf399bf39f9665c795d",
          "name": "Yoo Yeon Sung",
          "hidden": false
        },
        {
          "_id": "68536bf399bf39f9665c795e",
          "name": "Jordan Lee Boyd-Graber",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-18T02:16:53.000Z",
      "submittedOnDailyAt": "2025-06-19T04:04:25.155Z",
      "title": "Intérêt pour la Sémantique et la configuration de récompenses, ainsi que des réponses pour l'entraînement R1 en mode ouvert pour l'apprentissage autonome.",
      "submittedOnDailyBy": {
        "_id": "64ea62f918d79efd533c93fe",
        "avatarUrl": "/avatars/9985a789ce11b788de2cba12adfb72fc.svg",
        "isPro": false,
        "fullname": "Xiyang Wu",
        "user": "wuxiyang",
        "type": "user"
      },
      "summary": "L'évaluation de longues phrases dans des terminals ouverts est difficile. Cela est dû à la difficulté de distinguer clairement entre des sorties bonnes et mauvaises. Les méthodes d'évaluation actuelles mal valorisent principalement les éléments de consistance, de style et de pertinence, ainsi que de la manière de se biaser en fonction des données d'entraînement. Pour ces raisons, l'évaluation de longues phrases dans des terminals ouverts est un problème d'insuffisance de recherche. Pour résoudre ce problème, nous proposons PrefBERT. PrefBERT est un modèle de pondération pour évaluer la génération de longues phrases dans des terminals ouverts en GRPO, offrant différentes récompenses pour des sorties bonnes et mauvaises pour guider l'apprentissage. PrefBERT a été entraîné avec deux ensembles de données de réponses avec des styles de longues phrases différents et de qualité d'évaluation Likert. De cette manière, PrefBERT fournit une rétroaction significative plus importante que Local ou BERTScore, et soutient efficacement GRPO. Grâce à l'évaluation comme un juge (LLM-as-a-judge), évaluation humaine et analyse qualitative, PrefBERT a été entraîné pour diverses phrases et longueurs, offrant une rétroaction stable et vérifiable nécessaire pour GRPO, même dans des cas longs. L'évaluation humaine a confirmé que l'utilisation de PrefBERT comme signal de récompense permet d'obtenir des réponses bien ajustées aux préférences humaines. Le code est disponible sur https://github.com/zli12321/long_form_rl.",
      "upvotes": 7,
      "discussionId": "68536bf399bf39f9665c795f",
      "ai_summary": "PrefBERT, a scoring model, improves open-ended long-form generation by providing better semantic reward feedback than traditional metrics.",
      "ai_keywords": [
        "PrefBERT",
        "GRPO",
        "multi-sentence responses",
        "paragraph-length responses",
        "Likert-rated quality",
        "LLM-as-a-judge",
        "human ratings",
        "qualitative analysis",
        "verifiable rewards"
      ]
    },
    "publishedAt": "2025-06-17T22:16:53.000Z",
    "title": "Semantically-Aware Rewards for Open-Ended R1 Training in Free-Form\n  Generation",
    "summary": "Evaluating open-ended long-form generation is challenging because it is hard\nto define what clearly separates good from bad outputs. Existing methods often\nmiss key aspects like coherence, style, or relevance, or are biased by\npretraining data, making open-ended long-form evaluation an underexplored\nproblem. To address this gap, we propose PrefBERT, a scoring model for\nevaluating open-ended long-form generation in GRPO and guiding its training\nwith distinct rewards for good and bad outputs. Trained on two response\nevaluation datasets with diverse long-form styles and Likert-rated quality,\nPrefBERT effectively supports GRPO by offering better semantic reward feedback\nthan traditional metrics ROUGE-L and BERTScore do. Through comprehensive\nevaluations, including LLM-as-a-judge, human ratings, and qualitative analysis,\nwe show that PrefBERT, trained on multi-sentence and paragraph-length\nresponses, remains reliable across varied long passages and aligns well with\nthe verifiable rewards GRPO needs. Human evaluations confirm that using\nPrefBERT as the reward signal to train policy models yields responses better\naligned with human preferences than those trained with traditional metrics. Our\ncode is available at https://github.com/zli12321/long_form_rl.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.15068.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ea62f918d79efd533c93fe",
      "avatarUrl": "/avatars/9985a789ce11b788de2cba12adfb72fc.svg",
      "fullname": "Xiyang Wu",
      "name": "wuxiyang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.15569",
      "authors": [
        {
          "_id": "68537ca999bf39f9665c799a",
          "name": "Chengye Wang",
          "hidden": false
        },
        {
          "_id": "68537ca999bf39f9665c799b",
          "name": "Yifei Shen",
          "hidden": false
        },
        {
          "_id": "68537ca999bf39f9665c799c",
          "name": "Zexi Kuang",
          "hidden": false
        },
        {
          "_id": "68537ca999bf39f9665c799d",
          "name": "Arman Cohan",
          "hidden": false
        },
        {
          "_id": "68537ca999bf39f9665c799e",
          "user": {
            "_id": "62f662bcc58915315c4eccea",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
            "isPro": true,
            "fullname": "Yilun Zhao",
            "user": "yilunzhao",
            "type": "user"
          },
          "name": "Yilun Zhao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-19T09:10:23.732Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-18T15:43:26.000Z",
      "submittedOnDailyAt": "2025-06-19T01:28:40.934Z",
      "title": "SciVer: Modèle d'Évaluation de la Structure Double de la Proteste Scientifique",
      "submittedOnDailyBy": {
        "_id": "62f662bcc58915315c4eccea",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
        "isPro": true,
        "fullname": "Yilun Zhao",
        "user": "yilunzhao",
        "type": "user"
      },
      "summary": "SciVer est le premier benchmark conçu pour évaluer la capacité de vérifier des arguments scientifiques dans le contexte scientifique. SciVer comprend 3 000 annotations d'experts sur 1 113 articles et contient 4 sous-ensembles communs de logique pour la vérification d'arguments scientifiques. Il est accompagné d'une preuve auxiliaire annotée par des experts pour faciliter l'évaluation de chaque exemple. Les rendements de 21 modèles de pointe, y compris o4-mini, Gemini-2.5-Flash, Llama-3.2-Vision et Qwen2.5-VL, sont évalués. On observe une grande différence de performance entre les modèles et les experts humains dans SciVer. L'analyse détaillée de la génération d'agressions de recherche (RAG) et des erreurs évaluées par des personnes humaines permet d'identifier des limitations importantes des modèles open-source, fournissant des insights clés sur l'compréhension des modèles et le développement de la logique dans le travail avec des articles scientifiques multi-modèles.",
      "upvotes": 6,
      "discussionId": "68537ca999bf39f9665c799f",
      "githubRepo": "https://github.com/QDRhhhh/SciVer",
      "ai_summary": "A benchmark named SciVer evaluates multimodal foundation models' claim verification capabilities within scientific contexts, revealing performance gaps and limitations in current models.",
      "ai_keywords": [
        "retrieval-augmented generation (RAG)"
      ]
    },
    "publishedAt": "2025-06-18T11:43:26.000Z",
    "title": "SciVer: Evaluating Foundation Models for Multimodal Scientific Claim\n  Verification",
    "summary": "We introduce SciVer, the first benchmark specifically designed to evaluate\nthe ability of foundation models to verify claims within a multimodal\nscientific context. SciVer consists of 3,000 expert-annotated examples over\n1,113 scientific papers, covering four subsets, each representing a common\nreasoning type in multimodal scientific claim verification. To enable\nfine-grained evaluation, each example includes expert-annotated supporting\nevidence. We assess the performance of 21 state-of-the-art multimodal\nfoundation models, including o4-mini, Gemini-2.5-Flash, Llama-3.2-Vision, and\nQwen2.5-VL. Our experiment reveals a substantial performance gap between these\nmodels and human experts on SciVer. Through an in-depth analysis of\nretrieval-augmented generation (RAG), and human-conducted error evaluations, we\nidentify critical limitations in current open-source models, offering key\ninsights to advance models' comprehension and reasoning in multimodal\nscientific literature tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.15569.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62f662bcc58915315c4eccea",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
      "fullname": "Yilun Zhao",
      "name": "yilunzhao",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 13
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.15050",
      "authors": [
        {
          "_id": "68539c6199bf39f9665c79f6",
          "name": "Tiantian Fan",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c79f7",
          "name": "Lingjun Liu",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c79f8",
          "name": "Yu Yue",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c79f9",
          "name": "Jiaze Chen",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c79fa",
          "name": "Chengyi Wang",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c79fb",
          "name": "Qiying Yu",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c79fc",
          "name": "Chi Zhang",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c79fd",
          "name": "Zhiqi Lin",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c79fe",
          "name": "Ruofei Zhu",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c79ff",
          "name": "Yufeng Yuan",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c7a00",
          "name": "Xiaochen Zuo",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c7a01",
          "name": "Bole Ma",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c7a02",
          "name": "Mofan Zhang",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c7a03",
          "name": "Gaohong Liu",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c7a04",
          "name": "Ru Zhang",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c7a05",
          "name": "Haotian Zhou",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c7a06",
          "name": "Cong Xie",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c7a07",
          "name": "Ruidong Zhu",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c7a08",
          "name": "Zhi Zhang",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c7a09",
          "name": "Xin Liu",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c7a0a",
          "name": "Mingxuan Wang",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c7a0b",
          "name": "Lin Yan",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c7a0c",
          "name": "Yonghui Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-18T01:21:38.000Z",
      "submittedOnDailyAt": "2025-06-19T03:43:49.620Z",
      "title": "Truncated Proximal Policy Optimization",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Récemment, modèles de décroissance de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de la racine de",
      "upvotes": 4,
      "discussionId": "68539c6199bf39f9665c7a0d",
      "ai_summary": "T-PPO, an extension of PPO, improves training efficiency for Large Language Models by optimizing policy updates and utilizing hardware resources more effectively.",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "chains-of-thought (CoT)",
        "reinforcement learning (RL)",
        "Proximal Policy Optimization (PPO)",
        "Truncated Proximal Policy Optimization (T-PPO)",
        "Extended Generalized Advantage Estimation (EGAE)",
        "advantage estimation",
        "policy and value models",
        "independent optimization",
        "prompt and truncated tokens",
        "AIME 2024",
        "base model"
      ]
    },
    "publishedAt": "2025-06-17T21:21:38.000Z",
    "title": "Truncated Proximal Policy Optimization",
    "summary": "Recently, test-time scaling Large Language Models (LLMs) have demonstrated\nexceptional reasoning capabilities across scientific and professional tasks by\ngenerating long chains-of-thought (CoT). As a crucial component for developing\nthese reasoning models, reinforcement learning (RL), exemplified by Proximal\nPolicy Optimization (PPO) and its variants, allows models to learn through\ntrial and error. However, PPO can be time-consuming due to its inherent\non-policy nature, which is further exacerbated by increasing response lengths.\nIn this work, we propose Truncated Proximal Policy Optimization (T-PPO), a\nnovel extension to PPO that improves training efficiency by streamlining policy\nupdate and length-restricted response generation. T-PPO mitigates the issue of\nlow hardware utilization, an inherent drawback of fully synchronized\nlong-generation procedures, where resources often sit idle during the waiting\nperiods for complete rollouts. Our contributions are two-folds. First, we\npropose Extended Generalized Advantage Estimation (EGAE) for advantage\nestimation derived from incomplete responses while maintaining the integrity of\npolicy learning. Second, we devise a computationally optimized mechanism that\nallows for the independent optimization of the policy and value models. By\nselectively filtering prompt and truncated tokens, this mechanism reduces\nredundant computations and accelerates the training process without sacrificing\nconvergence performance. We demonstrate the effectiveness and efficacy of T-PPO\non AIME 2024 with a 32B base model. The experimental results show that T-PPO\nimproves the training efficiency of reasoning LLMs by up to 2.5x and\noutperforms its existing competitors.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.15050.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 88
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.06279",
      "authors": [
        {
          "_id": "6850e0285e07650ecce890f3",
          "user": {
            "_id": "637f22d27119bd030dfd4af8",
            "avatarUrl": "/avatars/55c468c40ad3bd3218d086759fb1be3c.svg",
            "isPro": false,
            "fullname": "Shi Liu",
            "user": "CLLBJ16",
            "type": "user"
          },
          "name": "Shi Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-18T12:16:48.071Z",
          "hidden": false
        },
        {
          "_id": "6850e0285e07650ecce890f4",
          "user": {
            "_id": "63e4562f9db5da2dc1f3b520",
            "avatarUrl": "/avatars/f4eecf1396b05e1c72436e7026d85cef.svg",
            "isPro": false,
            "fullname": "Weijie Su",
            "user": "jackroos",
            "type": "user"
          },
          "name": "Weijie Su",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-19T09:10:33.557Z",
          "hidden": false
        },
        {
          "_id": "6850e0285e07650ecce890f5",
          "name": "Xizhou Zhu",
          "hidden": false
        },
        {
          "_id": "6850e0285e07650ecce890f6",
          "name": "Wenhai Wang",
          "hidden": false
        },
        {
          "_id": "6850e0285e07650ecce890f7",
          "name": "Jifeng Dai",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-06T17:59:06.000Z",
      "submittedOnDailyAt": "2025-06-19T00:24:25.743Z",
      "title": "CoMemo : Les LVLMs nécessitent un contexte d'image et une mémoire d'image.",
      "submittedOnDailyBy": {
        "_id": "637f22d27119bd030dfd4af8",
        "avatarUrl": "/avatars/55c468c40ad3bd3218d086759fb1be3c.svg",
        "isPro": false,
        "fullname": "Shi Liu",
        "user": "CLLBJ16",
        "type": "user"
      },
      "summary": "Récemment, les grands modèles de langue visuelle et linguistique (LVLM) ont développé un paradigme principal basé sur des modèles de langue grand (LLM), centré sur la synchronisation de caractéristiques visuelles et des représentations de LLM. Cependant, ces LVLM tendent à ne pas avoir une structure de traitement multiforme optimisée. Tout d'abord, les LVLM montrent deux distributions qui influent sur l'attribution d'attention et peuvent négliger l'information visuelle à travers le contenu visuel lorsque le contexte s'étend. De plus, la simple technique d'encodage de position simple échoue à maintenir les relations structurales bidimensionnelles lors du traitement d'images haute résolution dynamique. Pour résoudre ces limitations, nous proposons la structure duale CoMemo. Cette structure combine des images de contexte et des mémoires d'images. CoMemo réduit effectivement l'ignorance de l'information visuelle. De plus, nous introduisons RoPE-DHR, une nouvelle technique d'encodage de position qui utilise une agression de position basée sur des images superficielles pour atténuer la décélération distantielle lors de séquences longues, tout en maintenant la perception dans l'espace bidimensionnel. Dans des évaluations sur 7 référentiels (compréhension de contexte étendu, inférence d'images multi-échelle, tests de réponse à des questions visuelles, etc.), CoMemo montre un rendement supérieur par rapport aux structures traditionnelles de LVLM. Pour obtenir plus d'informations sur le projet, visitez la page du projet sur https://lalbj.github.io/projects/CoMemo/.",
      "upvotes": 4,
      "discussionId": "6850e0295e07650ecce890f8",
      "projectPage": "https://lalbj.github.io/projects/CoMemo/",
      "githubRepo": "https://github.com/LALBJ/CoMemo",
      "ai_summary": "CoMemo addresses visual information neglect and spatial awareness in multimodal processing by using a dual-path architecture and a novel positional encoding mechanism.",
      "ai_keywords": [
        "Large Vision-Language Models",
        "Large Language Models",
        "multimodal processing",
        "bimodal distribution",
        "attention allocation",
        "middle visual content",
        "positional encoding",
        "visual processing",
        "image Memory path",
        "RoPE-DHR",
        "positional aggregation",
        "2D structural relationships",
        "spatial awareness",
        "remote decay",
        "long-context comprehension",
        "multi-image reasoning",
        "visual question answering"
      ]
    },
    "publishedAt": "2025-06-06T13:59:06.000Z",
    "title": "CoMemo: LVLMs Need Image Context with Image Memory",
    "summary": "Recent advancements in Large Vision-Language Models built upon Large Language\nModels have established aligning visual features with LLM representations as\nthe dominant paradigm. However, inherited LLM architectural designs introduce\nsuboptimal characteristics for multimodal processing. First, LVLMs exhibit a\nbimodal distribution in attention allocation, leading to the progressive\nneglect of middle visual content as context expands. Second, conventional\npositional encoding schemes fail to preserve vital 2D structural relationships\nwhen processing dynamic high-resolution images. To address these limitations,\nwe propose CoMemo - a dual-path architecture that combines a Context image path\nwith an image Memory path for visual processing, effectively alleviating visual\ninformation neglect. Additionally, we introduce RoPE-DHR, a novel positional\nencoding mechanism that employs thumbnail-based positional aggregation to\nmaintain 2D spatial awareness while mitigating remote decay in extended\nsequences. Evaluations across seven benchmarks,including long-context\ncomprehension, multi-image reasoning, and visual question answering,\ndemonstrate CoMemo's superior performance compared to conventional LVLM\narchitectures. Project page is available at\nhttps://lalbj.github.io/projects/CoMemo/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.06279.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "637f22d27119bd030dfd4af8",
      "avatarUrl": "/avatars/55c468c40ad3bd3218d086759fb1be3c.svg",
      "fullname": "Shi Liu",
      "name": "CLLBJ16",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.15672",
      "authors": [
        {
          "_id": "6853bbcc99bf39f9665c7a50",
          "name": "Yao Zhang",
          "hidden": false
        },
        {
          "_id": "6853bbcc99bf39f9665c7a51",
          "name": "Chenyang Lin",
          "hidden": false
        },
        {
          "_id": "6853bbcc99bf39f9665c7a52",
          "name": "Shijie Tang",
          "hidden": false
        },
        {
          "_id": "6853bbcc99bf39f9665c7a53",
          "name": "Haokun Chen",
          "hidden": false
        },
        {
          "_id": "6853bbcc99bf39f9665c7a54",
          "name": "Shijie Zhou",
          "hidden": false
        },
        {
          "_id": "6853bbcc99bf39f9665c7a55",
          "name": "Yunpu Ma",
          "hidden": false
        },
        {
          "_id": "6853bbcc99bf39f9665c7a56",
          "name": "Volker Tresp",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-18T17:54:55.000Z",
      "submittedOnDailyAt": "2025-06-19T06:26:51.897Z",
      "title": "Création d'un système entièrement automatisé agile basé sur la Sewerum Intelligence",
      "submittedOnDailyBy": {
        "_id": "648cbea3dee03837c823cbf2",
        "avatarUrl": "/avatars/3f8c36436a5cbff2948df099ae604418.svg",
        "isPro": false,
        "fullname": "Shuo Chen",
        "user": "ShuoChen99",
        "type": "user"
      },
      "summary": "Le rapide développement des modèles de langue généralisés a impulsé le progrès des systèmes de sortie pour la décision, la collaboration et l'exécution de tâches. Cependant, les cadres actuels de création de ces systèmes de sortie ne disposent pas d'autonomie complète et limitent l'adaptabilité et l'échellabilité en raison de la pénurie de l'écriture, de la génération de sorties, de l'optimisation automatique et de la manque de collaboration. Nous proposons le cadre SwarmAgentic pour atteindre la création d'un système de sorties entièrement automatisé. Ce cadre effectue la construction de systèmes de sorties de l'écriture à la sortie, exécutant un processus d'optimisation conjoint qui dépend des fonctions de sorties et de la collaboration. Pour permettre une exploration efficace de la structure du système, SwarmAgentic maintient un groupe de systèmes candidats, les met à jour par rétroaction et inspiration de l'Optimisation de Particules (PSO). Notre méthode est évaluée sur six tâches réelles, ouvertes et exploratoires, démontrant des niveaux élevés de planification, de collaboration au niveau du système et de créativité. Avec seulement la description des tâches et des fonctions objectifs, SwarmAgentic dépasse tous les baselines et atteint un amélioration relative de +261,8% sur le benchmark TravelPlanner, soulignant l'effet de l'automatisation complète sur des tâches non structurées. Ce cadre montre un pas important dans le design de systèmes de sorties avec interchangeabilité et autonomie, connectant le savoir collectif à la génération de sorties de système. Notre code est disponible sur https://yaoz720.github.io/SwarmAgentic/.",
      "upvotes": 2,
      "discussionId": "6853bbcc99bf39f9665c7a57",
      "ai_summary": "SwarmAgentic is a framework for automated agentic system generation that optimize agent functionality and collaboration through language-driven exploration, outperforming existing baselines in unconstrained tasks.",
      "ai_keywords": [
        "Large Language Models",
        "agentic systems",
        "from-scratch agent generation",
        "self-optimizing agent functionality",
        "collaboration",
        "Particle Swarm Optimization (PSO)",
        "TravelPlanner benchmark",
        "system-level coordination",
        "creative reasoning"
      ]
    },
    "publishedAt": "2025-06-18T13:54:55.000Z",
    "title": "SwarmAgentic: Towards Fully Automated Agentic System Generation via\n  Swarm Intelligence",
    "summary": "The rapid progress of Large Language Models has advanced agentic systems in\ndecision-making, coordination, and task execution. Yet, existing agentic system\ngeneration frameworks lack full autonomy, missing from-scratch agent\ngeneration, self-optimizing agent functionality, and collaboration, limiting\nadaptability and scalability. We propose SwarmAgentic, a framework for fully\nautomated agentic system generation that constructs agentic systems from\nscratch and jointly optimizes agent functionality and collaboration as\ninterdependent components through language-driven exploration. To enable\nefficient search over system-level structures, SwarmAgentic maintains a\npopulation of candidate systems and evolves them via feedback-guided updates,\ndrawing inspiration from Particle Swarm Optimization (PSO). We evaluate our\nmethod on six real-world, open-ended, and exploratory tasks involving\nhigh-level planning, system-level coordination, and creative reasoning. Given\nonly a task description and an objective function, SwarmAgentic outperforms all\nbaselines, achieving a +261.8% relative improvement over ADAS on the\nTravelPlanner benchmark, highlighting the effectiveness of full automation in\nstructurally unconstrained tasks. This framework marks a significant step\ntoward scalable and autonomous agentic system design, bridging swarm\nintelligence with fully automated system multi-agent generation. Our code is\npublicly released at https://yaoz720.github.io/SwarmAgentic/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.15672.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "648cbea3dee03837c823cbf2",
      "avatarUrl": "/avatars/3f8c36436a5cbff2948df099ae604418.svg",
      "fullname": "Shuo Chen",
      "name": "ShuoChen99",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.14435",
      "authors": [
        {
          "_id": "68537b2a99bf39f9665c7990",
          "name": "Hongyu Wang",
          "hidden": false
        },
        {
          "_id": "68537b2a99bf39f9665c7991",
          "name": "Jiayu Xu",
          "hidden": false
        },
        {
          "_id": "68537b2a99bf39f9665c7992",
          "name": "Ruiping Wang",
          "hidden": false
        },
        {
          "_id": "68537b2a99bf39f9665c7993",
          "name": "Yan Feng",
          "hidden": false
        },
        {
          "_id": "68537b2a99bf39f9665c7994",
          "name": "Yitao Zhai",
          "hidden": false
        },
        {
          "_id": "68537b2a99bf39f9665c7995",
          "name": "Peng Pei",
          "hidden": false
        },
        {
          "_id": "68537b2a99bf39f9665c7996",
          "name": "Xunliang Cai",
          "hidden": false
        },
        {
          "_id": "68537b2a99bf39f9665c7997",
          "name": "Xilin Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-17T11:53:49.000Z",
      "submittedOnDailyAt": "2025-06-19T01:22:11.793Z",
      "title": "MoTE : Modèle Mixte d'Experts Triadique pour une Mémoire Efficace dans les Grands Modèles Multimodales",
      "submittedOnDailyBy": {
        "_id": "63f71771d36951307fcb4dcd",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f71771d36951307fcb4dcd/1c-GSQmSSuDXyVWjxZFy_.jpeg",
        "isPro": false,
        "fullname": "Hongyu Wang",
        "user": "hongyuw",
        "type": "user"
      },
      "summary": "Les grands modèles de MoEs (Mixture of Experts) augmentent leur taille et améliorent leur performance en fixant les paramètres actifs. Cependant, dans des études antérieures, les experts de précision totale étaient principalement utilisés lors de l'échelle à distance. Ceux-ci ont montré des performances élevées sur des tâches de fin de fiche, mais ont augmenté considérablement le nombre d'experts dans l'impression de mémoire et ont présenté des problèmes significatifs lors de l'introduction de dispositifs de bord. Dans cet article, nous proposons MoTE (Mixture of Ternary Experts), un approche échellable et efficace en mémoire pour l'entraînement avec contrôle de densité. Au lieu d'entraîner des experts de haute précision, nous augmentons les experts de faible précision et les utilisons lors de l'échelle à distance. Spécifiquement, nous partageons un FFN pré-entraîné comme expert commun et entraînons des experts ternaires avec trois valeurs {-1, 0, 1}. Les expériences larges montrent que notre approche démontre un rendement comparable au baseline MoE-LLaVA basé sur la précision totale, mais avec des impressions de mémoire plus petites. De plus, notre approche est compatible avec des méthodes de réduction postérieure et améliore considérablement lorsque les contraintes de mémoire sont réduites. En maintenant le même imprimé de mémoire par expert (3.4GB), la combinaison de réduction postérieure et MoTE a augmenté la précision moyenne sur des tâches de fin de fiche d'un 4.3%, démontrant son efficacité sur des dispositifs avec des contraintes de mémoire.",
      "upvotes": 2,
      "discussionId": "68537b2b99bf39f9665c7998",
      "ai_summary": "MoTE, a scalable and memory-efficient method, improves Mixture-of-Experts models using low-precision ternary experts, enhancing performance and reducing memory footprint for deployment on edge devices.",
      "ai_keywords": [
        "Mixture-of-Experts",
        "MoEs",
        "sparse up-cycling",
        "low-precision",
        "ternary experts",
        "shared expert",
        "FFN",
        "pre-trained",
        "post-training quantization",
        "memory-constrained",
        "end tasks"
      ]
    },
    "publishedAt": "2025-06-17T07:53:49.000Z",
    "title": "MoTE: Mixture of Ternary Experts for Memory-efficient Large Multimodal\n  Models",
    "summary": "Large multimodal Mixture-of-Experts (MoEs) effectively scale the model size\nto boost performance while maintaining fixed active parameters. However,\nprevious works primarily utilized full-precision experts during sparse\nup-cycling. Despite they show superior performance on end tasks, the large\namount of experts introduces higher memory footprint, which poses significant\nchallenges for the deployment on edge devices. In this work, we propose MoTE, a\nscalable and memory-efficient approach to train Mixture-of-Ternary-Experts\nmodels from dense checkpoint. Instead of training fewer high-precision experts,\nwe propose to train more low-precision experts during up-cycling. Specifically,\nwe use the pre-trained FFN as a shared expert and train ternary routed experts\nwith parameters in {-1, 0, 1}. Extensive experiments show that our approach has\npromising scaling trend along model size. MoTE achieves comparable performance\nto full-precision baseline MoE-LLaVA while offering lower memory footprint.\nFurthermore, our approach is compatible with post-training quantization methods\nand the advantage further amplifies when memory-constraint goes lower. Given\nthe same amount of expert memory footprint of 3.4GB and combined with\npost-training quantization, MoTE outperforms MoE-LLaVA by a gain of 4.3%\naverage accuracy on end tasks, demonstrating its effectiveness and potential\nfor memory-constrained devices.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14435.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63f71771d36951307fcb4dcd",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f71771d36951307fcb4dcd/1c-GSQmSSuDXyVWjxZFy_.jpeg",
      "fullname": "Hongyu Wang",
      "name": "hongyuw",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 18
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.14866",
      "authors": [
        {
          "_id": "6853db4199bf39f9665c7ae5",
          "name": "Thomas Kuntz",
          "hidden": false
        },
        {
          "_id": "6853db4199bf39f9665c7ae6",
          "name": "Agatha Duzan",
          "hidden": false
        },
        {
          "_id": "6853db4199bf39f9665c7ae7",
          "name": "Hao Zhao",
          "hidden": false
        },
        {
          "_id": "6853db4199bf39f9665c7ae8",
          "name": "Francesco Croce",
          "hidden": false
        },
        {
          "_id": "6853db4199bf39f9665c7ae9",
          "name": "Zico Kolter",
          "hidden": false
        },
        {
          "_id": "6853db4199bf39f9665c7aea",
          "name": "Nicolas Flammarion",
          "hidden": false
        },
        {
          "_id": "6853db4199bf39f9665c7aeb",
          "name": "Maksym Andriushchenko",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-17T17:59:31.000Z",
      "submittedOnDailyAt": "2025-06-19T08:15:06.478Z",
      "title": "OS-Harm : Marqueur de sécurité pour mesurer la sécurité des agents d'utilisation de l'ordinateur",
      "submittedOnDailyBy": {
        "_id": "64c225f0129617dbaba5ae88",
        "avatarUrl": "/avatars/b12db433cd6b37c8e5299e575bdf98f9.svg",
        "isPro": false,
        "fullname": "Maksym Andriushchenko",
        "user": "MaksymAndriushchenko",
        "type": "user"
      },
      "summary": "Les agents de gestion de ordinateurs sont des agents basés sur des modèles de langage de haut niveau qui interagissent directement avec l'interface graphique utilisateur grâce au traitement de captures de écran ou d'arbres d'accès. Ces systèmes commencent à se développer, mais leur sécurité a été significativement compromise, ce qui rend nécessaire d'évaluer leur potentiel comportement nocif. Pour aborder ce thème, nous présentons un nouveau cadre de référence appelé OS-Harm. OS-Harm est construit dans l'environnement OSWorld et valide les modèles dans trois catégories : utilisation non-positive des utilisateurs intentionnels, attaques par injection de prompts et comportement non-positive du modèle. Pour cela, on crée 150 tâches et on exige des interactions avec des agents de sites web (clients de courriel, éditeurs de code, navigateurs, etc.). De plus, on propose un système de checkboxes automatiques pour évaluer la précision et la sécurité, atteignant des niveaux élevés de concordance avec des analyses humaines (score F1 de 0.76 et 0.79). OS-Harm évalue la sécurité de modèles avancés comme o4-mini, Claude 3.7 Sonnet et Gemini 2.5 Pro, soulignant que tous répondent directement à des demandes d'utilisation non-positive intentionnelle, étant relativement vulnérables à l'injection de prompts dynamiques et parfois montrant des comportements instables. Le cadre de référence OS-Harm est disponible sur https://github.com/tml-epfl/os-harm.",
      "upvotes": 1,
      "discussionId": "6853db4199bf39f9665c7aec",
      "githubRepo": "https://github.com/tml-epfl/os-harm",
      "ai_summary": "A new benchmark called OS-Harm measures the safety of computer use agents interacting with GUIs, evaluating their susceptibility to misuse, prompt injection attacks, and misbehavior across various safety violations and applications.",
      "ai_keywords": [
        "LLM-based agents",
        "OS-Harm",
        "OSWorld environment",
        "deliberate user misuse",
        "prompt injection attacks",
        "model misbehavior",
        "harassment",
        "copyright infringement",
        "disinformation",
        "data exfiltration",
        "automated judge",
        "F1 score",
        "GUI"
      ]
    },
    "publishedAt": "2025-06-17T13:59:31.000Z",
    "title": "OS-Harm: A Benchmark for Measuring Safety of Computer Use Agents",
    "summary": "Computer use agents are LLM-based agents that can directly interact with a\ngraphical user interface, by processing screenshots or accessibility trees.\nWhile these systems are gaining popularity, their safety has been largely\noverlooked, despite the fact that evaluating and understanding their potential\nfor harmful behavior is essential for widespread adoption. To address this gap,\nwe introduce OS-Harm, a new benchmark for measuring safety of computer use\nagents. OS-Harm is built on top of the OSWorld environment and aims to test\nmodels across three categories of harm: deliberate user misuse, prompt\ninjection attacks, and model misbehavior. To cover these cases, we create 150\ntasks that span several types of safety violations (harassment, copyright\ninfringement, disinformation, data exfiltration, etc.) and require the agent to\ninteract with a variety of OS applications (email client, code editor, browser,\netc.). Moreover, we propose an automated judge to evaluate both accuracy and\nsafety of agents that achieves high agreement with human annotations (0.76 and\n0.79 F1 score). We evaluate computer use agents based on a range of frontier\nmodels - such as o4-mini, Claude 3.7 Sonnet, Gemini 2.5 Pro - and provide\ninsights into their safety. In particular, all models tend to directly comply\nwith many deliberate misuse queries, are relatively vulnerable to static prompt\ninjections, and occasionally perform unsafe actions. The OS-Harm benchmark is\navailable at https://github.com/tml-epfl/os-harm.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14866.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64c225f0129617dbaba5ae88",
      "avatarUrl": "/avatars/b12db433cd6b37c8e5299e575bdf98f9.svg",
      "fullname": "Maksym Andriushchenko",
      "name": "MaksymAndriushchenko",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.14824",
      "authors": [
        {
          "_id": "6853c3af99bf39f9665c7a89",
          "name": "Yao Zhang",
          "hidden": false
        },
        {
          "_id": "6853c3af99bf39f9665c7a8a",
          "name": "Hewei Gao",
          "hidden": false
        },
        {
          "_id": "6853c3af99bf39f9665c7a8b",
          "name": "Haokun Chen",
          "hidden": false
        },
        {
          "_id": "6853c3af99bf39f9665c7a8c",
          "name": "Weiguo Li",
          "hidden": false
        },
        {
          "_id": "6853c3af99bf39f9665c7a8d",
          "name": "Yunpu Ma",
          "hidden": false
        },
        {
          "_id": "6853c3af99bf39f9665c7a8e",
          "name": "Volker Tresp",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-12T17:50:50.000Z",
      "submittedOnDailyAt": "2025-06-19T06:31:06.085Z",
      "title": "FedNano : Apprentissage Fédéré Léger pour Modèles Pré-entraînés - Progrès en Amélioration",
      "submittedOnDailyBy": {
        "_id": "648cbea3dee03837c823cbf2",
        "avatarUrl": "/avatars/3f8c36436a5cbff2948df099ae604418.svg",
        "isPro": false,
        "fullname": "Shuo Chen",
        "user": "ShuoChen99",
        "type": "user"
      },
      "summary": "Les modèles multimodal (MLLMs) montrent un excellent rendement dans des tâches complexes comme l'inférence de modèles multimodal et la recherche croisée. Cependant, dans des environnements réels, leur fonctionnement est souvent limité par la distribution des données multimodales et les hauts exigences de confidentialité, ce qui empêche leur utilisation efficace. L'apprentissage federé (FL) permet l'apprentissage collectif du modèle sans centraliser les données, offrant une solution au problème. Cependant, l'implémentation de FL dans les MLLMs présente des défis tels que des demandes computationnelles élevées, des limites de capacité des clients, des coûts élevés de communication et des problèmes dérivés des données hétérogènes. Les méthodes actuelles de FL supposent que l'apprentissage de la réseau neuronal complet est effectué sur le côté du client, une hypothèse qui est détruite en raison du grand taille des MLLMs et des exigences de communication. Pour résoudre ces limitations, on propose FedNano, le premier cadre de FL. FedNano centralise le modèle de mémoire de longue durée (LLM) sur le côté du serveur et introduit un module appelé NanoEdge sur le côté du client, conçu pour répondre aux besoins spécifiques. NanoEdge utilise un encodeur propre du modèle, un connecteur et un adaptateur de faible rang pour éliminer la nécessité d'apprentissage de la réseau neuronal sur le côté du client, réduisant de 95% la mémoire du client et limitant les coûts de communication des paramètres du modèle à moins de 0,01%. En transmettant uniquement le NanoAdapter, FedNano aborde les défis des données hétérogènes et des limites de ressources, maintenant la confidentialité et équilibre le taille du modèle et la possibilité d'apprentissage federé, offrant la possibilité d'un système AI distribué multimodal. Les expériences montrent que FedNano dépasse les limites des méthodes de FL existantes, équilibre le taille du modèle et la possibilité d'apprentissage federé, et offre la possibilité d'un système AI distribué multimodal.",
      "upvotes": 1,
      "discussionId": "6853c3af99bf39f9665c7a8f",
      "ai_summary": "FedNano is a federated learning framework that centralizes large language models on servers and uses NanoEdge modules for client-specific adaptation, addressing scalability and privacy issues.",
      "ai_keywords": [
        "Multimodal Large Language Models",
        "Federated Learning",
        "NanoEdge",
        "modality-specific encoders",
        "connectors",
        "NanoAdapters",
        "low-rank adaptation",
        "client-specific adaptation",
        "compact NanoAdapter updates",
        "decentralized multimodal AI systems"
      ]
    },
    "publishedAt": "2025-06-12T13:50:50.000Z",
    "title": "FedNano: Toward Lightweight Federated Tuning for Pretrained Multimodal\n  Large Language Models",
    "summary": "Multimodal Large Language Models (MLLMs) excel in tasks like multimodal\nreasoning and cross-modal retrieval but face deployment challenges in\nreal-world scenarios due to distributed multimodal data and strict privacy\nrequirements. Federated Learning (FL) offers a solution by enabling\ncollaborative model training without centralizing data. However, realizing FL\nfor MLLMs presents significant challenges, including high computational\ndemands, limited client capacity, substantial communication costs, and\nheterogeneous client data. Existing FL methods assume client-side deployment of\nfull models, an assumption that breaks down for large-scale MLLMs due to their\nmassive size and communication demands. To address these limitations, we\npropose FedNano, the first FL framework that centralizes the LLM on the server\nwhile introducing NanoEdge, a lightweight module for client-specific\nadaptation. NanoEdge employs modality-specific encoders, connectors, and\ntrainable NanoAdapters with low-rank adaptation. This design eliminates the\nneed to deploy LLM on clients, reducing client-side storage by 95%, and\nlimiting communication overhead to only 0.01% of the model parameters. By\ntransmitting only compact NanoAdapter updates, FedNano handles heterogeneous\nclient data and resource constraints while preserving privacy. Experiments\ndemonstrate that FedNano outperforms prior FL baselines, bridging the gap\nbetween MLLM scale and FL feasibility, and enabling scalable, decentralized\nmultimodal AI systems.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14824.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "648cbea3dee03837c823cbf2",
      "avatarUrl": "/avatars/3f8c36436a5cbff2948df099ae604418.svg",
      "fullname": "Shuo Chen",
      "name": "ShuoChen99",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  }
]