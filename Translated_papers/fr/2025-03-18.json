[
  {
    "paper": {
      "id": "2503.06053",
      "authors": [
        {
          "_id": "67cfd2d7bc539099da9ebecb",
          "name": "Runze Zhang",
          "hidden": false
        },
        {
          "_id": "67cfd2d7bc539099da9ebecc",
          "user": {
            "_id": "6474a63f7d131daf633d10f2",
            "avatarUrl": "/avatars/5e5d1ce5731987a810448835a1a69c91.svg",
            "isPro": false,
            "fullname": "GeorgeDu",
            "user": "georgedu",
            "type": "user"
          },
          "name": "Guoguang Du",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-17T08:45:36.478Z",
          "hidden": false
        },
        {
          "_id": "67cfd2d7bc539099da9ebecd",
          "user": {
            "_id": "66b01dc4e48856bb718f2ba8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66b01dc4e48856bb718f2ba8/MHZyIDd3BEtsnH9M2BvzM.jpeg",
            "isPro": false,
            "fullname": "Xiaochuan Li",
            "user": "lixiaochuan",
            "type": "user"
          },
          "name": "Xiaochuan Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-17T08:45:39.724Z",
          "hidden": false
        },
        {
          "_id": "67cfd2d7bc539099da9ebece",
          "name": "Qi Jia",
          "hidden": false
        },
        {
          "_id": "67cfd2d7bc539099da9ebecf",
          "name": "Liang Jin",
          "hidden": false
        },
        {
          "_id": "67cfd2d7bc539099da9ebed0",
          "user": {
            "_id": "66f67725cdcb9a4eaef04027",
            "avatarUrl": "/avatars/fb5f4b467cc4d73e129fa9aa60ef344d.svg",
            "isPro": false,
            "fullname": "Ellen Liu",
            "user": "EllenAP",
            "type": "user"
          },
          "name": "Lu Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-17T08:45:32.476Z",
          "hidden": false
        },
        {
          "_id": "67cfd2d7bc539099da9ebed1",
          "name": "Jingjing Wang",
          "hidden": false
        },
        {
          "_id": "67cfd2d7bc539099da9ebed2",
          "user": {
            "_id": "6297889a64501abb8d002c6b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/-tfSq05d4nkLkU_E-N75e.png",
            "isPro": false,
            "fullname": "Cong Xu",
            "user": "NeilXu",
            "type": "user"
          },
          "name": "Cong Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-18T09:16:48.232Z",
          "hidden": false
        },
        {
          "_id": "67cfd2d7bc539099da9ebed3",
          "name": "Zhenhua Guo",
          "hidden": false
        },
        {
          "_id": "67cfd2d7bc539099da9ebed4",
          "name": "Yaqian Zhao",
          "hidden": false
        },
        {
          "_id": "67cfd2d7bc539099da9ebed5",
          "name": "Xiaoli Gong",
          "hidden": false
        },
        {
          "_id": "67cfd2d7bc539099da9ebed6",
          "name": "Rengang Li",
          "hidden": false
        },
        {
          "_id": "67cfd2d7bc539099da9ebed7",
          "name": "Baoyu Fan",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/66b01dc4e48856bb718f2ba8/_R9279HPwaWKB34BDJmAv.png"
      ],
      "publishedAt": "2025-03-08T04:37:38.000Z",
      "submittedOnDailyAt": "2025-03-18T05:40:31.378Z",
      "title": "\"Supprimer Rizubervido : Données et méthodes d'accès pour explorer la cohérence spatio-temporelle\"",
      "submittedOnDailyBy": {
        "_id": "66b01dc4e48856bb718f2ba8",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66b01dc4e48856bb718f2ba8/MHZyIDd3BEtsnH9M2BvzM.jpeg",
        "isPro": false,
        "fullname": "Xiaochuan Li",
        "user": "lixiaochuan",
        "type": "user"
      },
      "summary": "La cohérence temporelle est un thème important dans le domaine de la génération de vidéos. Un segment de vidéo généré correctement doit maintenir une cohérence visuelle et assurer la cohérence entre les objets et les scènes, en garantissant également la possibilité de flottage et d'interactions. Dans les études précédentes, notamment dans les projets ouverts, l'accent était mis sur la maintenance d'une cohérence temporelle ou spatiale, ou une combinaison de base de ces deux. Par exemple, on expliquait le mouvement de la caméra après un prompt, mais sans limiter les résultats du mouvement. Cependant, le mouvement de la caméra peut ajouter de nouveaux objets à la scène ou supprimer des objets existants, ce qui peut affecter les objets précédents. En particulier, le mouvement de la caméra est un facteur complexe dans les vidéos nécessitant plusieurs interactions entre flottages. Dans cet article, nous présentons et examinons une cohérence temporelle intégrée qui considère la simplification entre le processus de flottage et la technologie de la caméra, ainsi que l'impact à long terme sur la génération ultérieure. Cette étude couvre une large gamme de perspectives, de la construction de jeux de données à l'élaboration de modèles. Tout d'abord, nous avons construit le jeu de données DropletVideo-10M, qui comprend 10 millions de vidéos comportant des mouvements de caméra et des actions d'objets. Chaque vidéo est expliquée avec une moyenne de 206 captions, détaillant différents mouvements de caméra et le processus de flottage. Ensuite, nous avons développé le modèle DropletVideo pour maintenir la cohérence temporelle dans la génération de vidéos. Le jeu de données et le modèle DropletVideo peuvent être accédés via https://dropletx.github.io.",
      "upvotes": 47,
      "discussionId": "67cfd2debc539099da9ec061",
      "ai_keywords": [
        "spatio-temporal consistency",
        "video generation",
        "plot plausibility",
        "visual consistency",
        "objects",
        "scenes",
        "viewpoints",
        "camera movement",
        "prompt",
        "narrative",
        "plot progression",
        "camera techniques",
        "long-term impact",
        "dataset construction",
        "DropletVideo-10M dataset",
        "dynamic camera motion",
        "object actions",
        "caption",
        "DropletVideo model",
        "spatio-temporal coherence"
      ]
    },
    "publishedAt": "2025-03-07T23:37:38.000Z",
    "title": "DropletVideo: A Dataset and Approach to Explore Integral Spatio-Temporal\n  Consistent Video Generation",
    "summary": "Spatio-temporal consistency is a critical research topic in video generation.\nA qualified generated video segment must ensure plot plausibility and coherence\nwhile maintaining visual consistency of objects and scenes across varying\nviewpoints. Prior research, especially in open-source projects, primarily\nfocuses on either temporal or spatial consistency, or their basic combination,\nsuch as appending a description of a camera movement after a prompt without\nconstraining the outcomes of this movement. However, camera movement may\nintroduce new objects to the scene or eliminate existing ones, thereby\noverlaying and affecting the preceding narrative. Especially in videos with\nnumerous camera movements, the interplay between multiple plots becomes\nincreasingly complex. This paper introduces and examines integral\nspatio-temporal consistency, considering the synergy between plot progression\nand camera techniques, and the long-term impact of prior content on subsequent\ngeneration. Our research encompasses dataset construction through to the\ndevelopment of the model. Initially, we constructed a DropletVideo-10M dataset,\nwhich comprises 10 million videos featuring dynamic camera motion and object\nactions. Each video is annotated with an average caption of 206 words,\ndetailing various camera movements and plot developments. Following this, we\ndeveloped and trained the DropletVideo model, which excels in preserving\nspatio-temporal coherence during video generation. The DropletVideo dataset and\nmodel are accessible at https://dropletx.github.io.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/66b01dc4e48856bb718f2ba8/_R9279HPwaWKB34BDJmAv.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.06053.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66b01dc4e48856bb718f2ba8",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66b01dc4e48856bb718f2ba8/MHZyIDd3BEtsnH9M2BvzM.jpeg",
      "fullname": "Xiaochuan Li",
      "name": "lixiaochuan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.12533",
      "authors": [
        {
          "_id": "67d8eadc045f869fea1ce3f2",
          "name": "Haoqi Yuan",
          "hidden": false
        },
        {
          "_id": "67d8eadc045f869fea1ce3f3",
          "name": "Yu Bai",
          "hidden": false
        },
        {
          "_id": "67d8eadc045f869fea1ce3f4",
          "user": {
            "_id": "67d92b2218de6ef86c60f7d4",
            "avatarUrl": "/avatars/9758522c99bc38bc7b60845eff8bf8d7.svg",
            "isPro": false,
            "fullname": "Yuhui Fu",
            "user": "fuyh",
            "type": "user"
          },
          "name": "Yuhui Fu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:17:46.443Z",
          "hidden": false
        },
        {
          "_id": "67d8eadc045f869fea1ce3f5",
          "name": "Bohan Zhou",
          "hidden": false
        },
        {
          "_id": "67d8eadc045f869fea1ce3f6",
          "user": {
            "_id": "6655b86e607894ea80d74910",
            "avatarUrl": "/avatars/663c0135c903c9c127fe1b8d8aaf279c.svg",
            "isPro": false,
            "fullname": "yicheng feng",
            "user": "takenpeanut",
            "type": "user"
          },
          "name": "Yicheng Feng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:17:31.771Z",
          "hidden": false
        },
        {
          "_id": "67d8eadc045f869fea1ce3f7",
          "user": {
            "_id": "653238fdcd5377e9adee0c41",
            "avatarUrl": "/avatars/78aea70cde6ab0050c7e18b5e148075c.svg",
            "isPro": false,
            "fullname": "Xinrun Xu",
            "user": "SherryXu",
            "type": "user"
          },
          "name": "Xinrun Xu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:17:05.200Z",
          "hidden": false
        },
        {
          "_id": "67d8eadc045f869fea1ce3f8",
          "name": "Yi Zhan",
          "hidden": false
        },
        {
          "_id": "67d8eadc045f869fea1ce3f9",
          "user": {
            "_id": "61e52be53d6dbb1da842316a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61e52be53d6dbb1da842316a/gx0WGPcOCClXPymoKglc4.jpeg",
            "isPro": false,
            "fullname": "Börje Karlsson",
            "user": "tellarin",
            "type": "user"
          },
          "name": "Börje F. Karlsson",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-18T08:07:34.005Z",
          "hidden": false
        },
        {
          "_id": "67d8eadc045f869fea1ce3fa",
          "user": {
            "_id": "67d905c0e27ba28109384f5c",
            "avatarUrl": "/avatars/26712594ac9d43c8d1a3e75e36b5df16.svg",
            "isPro": false,
            "fullname": "Zongqing Lu",
            "user": "chungtsing",
            "type": "user"
          },
          "name": "Zongqing Lu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:16:58.409Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-16T14:53:53.000Z",
      "submittedOnDailyAt": "2025-03-18T02:11:08.263Z",
      "title": "Agent de robot humanoïde : Introduction des modèles de vision et de technologie modulaire",
      "submittedOnDailyBy": {
        "_id": "61e52be53d6dbb1da842316a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61e52be53d6dbb1da842316a/gx0WGPcOCClXPymoKglc4.jpeg",
        "isPro": false,
        "fullname": "Börje Karlsson",
        "user": "tellarin",
        "type": "user"
      },
      "summary": "L'objectif final de la recherche en robots autonomes humainoïdes est le développement d'un robot autonome humainoïde capable de réaliser un rendement humain dans des tâches concrètes de la réalité. Les derniers progrès ont atteint un avancement clair dans le développement de compétences de bas niveau chez les robots humainoïdes grâce au FM (Modèle de Base), mais l'intégration directe de ces composants peut conduire à l'accumulation d'erreurs et à une perte d'efficacité dans les tâches à long terme en raison des différences de retards entre les modules. Dans ce contexte, l'objectif est d'améliorer cette situation grâce au cadre heuristique agrégateur \"Being-0\", qui intègre le FM et une bibliothèque de compétences modulaires. Le FM charge des tâches cognitives de haut niveau comme la compréhension d'ordres, la planification de tâches et la raison, tandis que la bibliothèque de compétences fournit des compétences de bas niveau de contrôle, des mouvements stables et des manipulations flexibles. Pour combler le gap entre ces niveaux, on propose un nouveau module Connector. Ce module comporte un modèle de langage visuel-langage (VLM) léger, renforce les capacités de visualisation du FM, traduit des plans basés sur le langage en commandes de compétences pouvant être déployées et manipulées de manière dynamique, et améliore la taux de réussite des tâches. Being-0 permet que tous les composants, à l'exception du FM, soient implémentés sur des dispositifs de calcul virtuel de faible coût, transformant la personne de planification en un robot humainoïde efficace grâce à des outils de visualisation et un VLM actif. Les expériences distribuées dans des environnements intérieurs de grande taille ont démontré que Being-0 peut résoudre des tâches complexes à long terme, en mettant en œuvre des sous-tâches difficiles de navigation et de manipulation. Pour plus de détails, consultez https://beingbeyond.github.io/being-0.",
      "upvotes": 35,
      "discussionId": "67d8eadd045f869fea1ce44a",
      "projectPage": "https://beingbeyond.github.io/Being-0/",
      "ai_keywords": [
        "Foundation Models (FMs)",
        "modular skill library",
        "high-level cognitive tasks",
        "instruction understanding",
        "task planning",
        "reasoning",
        "stable locomotion",
        "dexterous manipulation",
        "low-level control",
        "Connector module",
        "lightweight vision-language model (VLM",
        "embodied capabilities",
        "language-based plans",
        "actionable skill commands",
        "dynamic coordination",
        "full-sized humanoid robot",
        "dexterous hands",
        "active vision",
        "complex, long-horizon tasks",
        "challenging navigation",
        "manipulation subtasks"
      ]
    },
    "publishedAt": "2025-03-16T10:53:53.000Z",
    "title": "Being-0: A Humanoid Robotic Agent with Vision-Language Models and\n  Modular Skills",
    "summary": "Building autonomous robotic agents capable of achieving human-level\nperformance in real-world embodied tasks is an ultimate goal in humanoid robot\nresearch. Recent advances have made significant progress in high-level\ncognition with Foundation Models (FMs) and low-level skill development for\nhumanoid robots. However, directly combining these components often results in\npoor robustness and efficiency due to compounding errors in long-horizon tasks\nand the varied latency of different modules. We introduce Being-0, a\nhierarchical agent framework that integrates an FM with a modular skill\nlibrary. The FM handles high-level cognitive tasks such as instruction\nunderstanding, task planning, and reasoning, while the skill library provides\nstable locomotion and dexterous manipulation for low-level control. To bridge\nthe gap between these levels, we propose a novel Connector module, powered by a\nlightweight vision-language model (VLM). The Connector enhances the FM's\nembodied capabilities by translating language-based plans into actionable skill\ncommands and dynamically coordinating locomotion and manipulation to improve\ntask success. With all components, except the FM, deployable on low-cost\nonboard computation devices, Being-0 achieves efficient, real-time performance\non a full-sized humanoid robot equipped with dexterous hands and active vision.\nExtensive experiments in large indoor environments demonstrate Being-0's\neffectiveness in solving complex, long-horizon tasks that require challenging\nnavigation and manipulation subtasks. For further details and videos, visit\nhttps://beingbeyond.github.io/being-0.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12533.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "61e52be53d6dbb1da842316a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61e52be53d6dbb1da842316a/gx0WGPcOCClXPymoKglc4.jpeg",
      "fullname": "Börje Karlsson",
      "name": "tellarin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 24
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.12885",
      "authors": [
        {
          "_id": "67d8e23afa59a8b15a9057e8",
          "user": {
            "_id": "65eaa1e2b11eeb516a973508",
            "avatarUrl": "/avatars/beecd135bb940fdc02406f9063b3fa67.svg",
            "isPro": false,
            "fullname": "Dewei Zhou",
            "user": "limuloo1999",
            "type": "user"
          },
          "name": "Dewei Zhou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:20:39.038Z",
          "hidden": false
        },
        {
          "_id": "67d8e23afa59a8b15a9057e9",
          "user": {
            "_id": "64551bc2c9c0dcc8c2484cf6",
            "avatarUrl": "/avatars/0d1ed4f4502f6f54ac6ba071e4c9a220.svg",
            "isPro": false,
            "fullname": "Mingwei Li",
            "user": "aiJojosh",
            "type": "user"
          },
          "name": "Mingwei Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:20:46.810Z",
          "hidden": false
        },
        {
          "_id": "67d8e23afa59a8b15a9057ea",
          "user": {
            "_id": "619bf9b3cbedb87e1a92fb3b",
            "avatarUrl": "/avatars/ee280db0232e21416c948ab9a9a2344e.svg",
            "isPro": false,
            "fullname": "Zongxin Yang",
            "user": "z-x-yang",
            "type": "user"
          },
          "name": "Zongxin Yang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:20:52.869Z",
          "hidden": false
        },
        {
          "_id": "67d8e23afa59a8b15a9057eb",
          "name": "Yi Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-17T07:30:16.000Z",
      "submittedOnDailyAt": "2025-03-18T01:33:30.593Z",
      "title": "DREAM-RENDER : Technologie permettant de contrôler les propriétés dynamiques d'un objet de texte de grande échelle vers une image.",
      "submittedOnDailyBy": {
        "_id": "65eaa1e2b11eeb516a973508",
        "avatarUrl": "/avatars/beecd135bb940fdc02406f9063b3fa67.svg",
        "isPro": false,
        "fullname": "Dewei Zhou",
        "user": "limuloo1999",
        "type": "user"
      },
      "summary": "Selon les conditions de dessin, les méthodes de génération de profondeur ou selon les conditions de Canny montrent une capacité impressionnante pour la synthèse d'images précises. Cependant, les modèles actuels ont des difficultés à contrôler avec précision le contenu de plusieurs instances (ou zones). Parmi les plus avancés, FLUX et 3DIS ont identifié des problèmes comme la perte de propriétés entre instances et ont limité le contrôle de l'utilisateur. Pour résoudre ces problèmes, nous présentons DreamRenderer, un approche d'apprentissage limité basée sur le modèle FLUX. DreamRenderer permet à l'utilisateur de contrôler le contenu de chaque instance via des Bounding Box ou des masques, tout en maintenant l'harmonie visuelle de l'ensemble. Nous proposons deux innovations : 1) un pont d'images pour combiner des propriétés complexes du contexte et 2) la combinaison de propriétés d'image strictes dans les couches de Vitral. A travers l'analyse de FLUX, nous identifions les joueurs clés responsables du rendu des propriétés d'instances et nous appliquons des ensembles stricts de propriétés d'image uniquement dans les couches de Vitral, tout en effectuant une fusion douce dans les autres couches. Cette approche permet une précision contrôlée tout en maintenant la qualité de l'image. Selon les évaluations sur les marques de référence COCO-POS et COCO-MIG, DreamRenderer améliore la taux de succès d'images de 17,7% par rapport à FLUX et améliore le rendement des modèles de conception d'images de 26,8% par rapport à GLIGEN et 3DIS. Page du projet : https://limuloo.github.io/DreamRenderer/",
      "upvotes": 24,
      "discussionId": "67d8e23cfa59a8b15a9058ba",
      "projectPage": "https://limuloo.github.io/DreamRenderer/",
      "githubRepo": "https://github.com/limuloo/DreamRenderer",
      "ai_keywords": [
        "Bridge Image Tokens",
        "Hard Text Attribute Binding",
        "Replicated image tokens",
        "T5 text embeddings",
        "Joint Attention",
        "Hard Image Attribute Binding",
        "Vital layers",
        "Soft binding",
        "Image Success Ratio",
        "COCO-POS",
        "COCO-MIG",
        "layout-to-image models",
        "GLIGEN",
        "3DIS"
      ]
    },
    "publishedAt": "2025-03-17T03:30:16.000Z",
    "title": "DreamRenderer: Taming Multi-Instance Attribute Control in Large-Scale\n  Text-to-Image Models",
    "summary": "Image-conditioned generation methods, such as depth- and canny-conditioned\napproaches, have demonstrated remarkable abilities for precise image synthesis.\nHowever, existing models still struggle to accurately control the content of\nmultiple instances (or regions). Even state-of-the-art models like FLUX and\n3DIS face challenges, such as attribute leakage between instances, which limits\nuser control. To address these issues, we introduce DreamRenderer, a\ntraining-free approach built upon the FLUX model. DreamRenderer enables users\nto control the content of each instance via bounding boxes or masks, while\nensuring overall visual harmony. We propose two key innovations: 1) Bridge\nImage Tokens for Hard Text Attribute Binding, which uses replicated image\ntokens as bridge tokens to ensure that T5 text embeddings, pre-trained solely\non text data, bind the correct visual attributes for each instance during Joint\nAttention; 2) Hard Image Attribute Binding applied only to vital layers.\nThrough our analysis of FLUX, we identify the critical layers responsible for\ninstance attribute rendering and apply Hard Image Attribute Binding only in\nthese layers, using soft binding in the others. This approach ensures precise\ncontrol while preserving image quality. Evaluations on the COCO-POS and\nCOCO-MIG benchmarks demonstrate that DreamRenderer improves the Image Success\nRatio by 17.7% over FLUX and enhances the performance of layout-to-image models\nlike GLIGEN and 3DIS by up to 26.8%. Project Page:\nhttps://limuloo.github.io/DreamRenderer/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12885.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "65eaa1e2b11eeb516a973508",
      "avatarUrl": "/avatars/beecd135bb940fdc02406f9063b3fa67.svg",
      "fullname": "Dewei Zhou",
      "name": "limuloo1999",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.13327",
      "authors": [
        {
          "_id": "67d8e00f0922c3dc8866520c",
          "user": {
            "_id": "640d704c8036cc2142299c19",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640d704c8036cc2142299c19/Wt9AslcVxWOSSc11epk8l.jpeg",
            "isPro": false,
            "fullname": "Lan Chen",
            "user": "Orannue",
            "type": "user"
          },
          "name": "Lan Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:21:18.982Z",
          "hidden": false
        },
        {
          "_id": "67d8e00f0922c3dc8866520d",
          "name": "Qi Mao",
          "hidden": false
        },
        {
          "_id": "67d8e00f0922c3dc8866520e",
          "user": {
            "_id": "63021630a35b21bd8a53305a",
            "avatarUrl": "/avatars/7a7e8b39749eda61e57d8a1908726558.svg",
            "isPro": true,
            "fullname": "Gu Yuchao",
            "user": "guyuchao",
            "type": "user"
          },
          "name": "Yuchao Gu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:21:44.702Z",
          "hidden": false
        },
        {
          "_id": "67d8e00f0922c3dc8866520f",
          "user": {
            "_id": "661ab3da2b14565c7acccf5c",
            "avatarUrl": "/avatars/fa4fc03664803e02aede4d4c3d50b393.svg",
            "isPro": false,
            "fullname": "Mike Zheng Shou",
            "user": "AnalMom",
            "type": "user"
          },
          "name": "Mike Zheng Shou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:21:29.754Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-17T16:04:44.000Z",
      "submittedOnDailyAt": "2025-03-18T01:26:49.605Z",
      "title": "Apprentissage de la relation visuelle du contexte en édition d'images",
      "submittedOnDailyBy": {
        "_id": "640d704c8036cc2142299c19",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640d704c8036cc2142299c19/Wt9AslcVxWOSSc11epk8l.jpeg",
        "isPro": false,
        "fullname": "Lan Chen",
        "user": "Orannue",
        "type": "user"
      },
      "summary": "Les nouveaux ajustements \"Edit Transfer\" permettent à un modèle d'apprendre à transformer des images de nouvelles questions à partir d'un exemple de source et de cible unique. Les méthodes basées sur le contexte dépassent significativement des tâches en traitant le contexte, mais rencontrent des difficultés avec les détails géométriques de haute précision (comme des changements d'attitude et de point de vue). L'édition basée sur le but se concentre généralement sur le style ou l'apparence, sans atteindre des transformations non rigides. \"Edit Transfer\" apprend des transformations éditables explicites à partir de paires de source et de cible uniques, ce qui défie les restrictions de référence tant en contexte que dans l'apparence. En s'appuyant sur l'apprentissage de contexte dans des modèles de langage grands, nous proposons de construire un modèle d'image dans des contextes de DiT et un schéma d'apprentissage de relations visuelles. Nous combinons des images éditées et des questions dans un composant de 4 pages pour détecter des transformations spatiales complexes avec une faible quantité d'exemples, en appliquant un ajustement LoRA léger. Avec seulement 42 échantillons d'entraînement, \"Edit Transfer\" dépasse considérablement les méthodes de TIE et RIE leaders en échelles non rigides, démontrant des effets d'apprentissage visuel de peu d'échantillons.",
      "upvotes": 19,
      "discussionId": "67d8e0100922c3dc88665285",
      "projectPage": "https://cuc-mipg.github.io/EditTransfer.github.io",
      "githubRepo": "https://github.com/CUC-MIPG/Edit-Transfer",
      "ai_keywords": [
        "Edit Transfer",
        "visual relation in-context learning",
        "DiT-based text-to-image model",
        "four-panel composite",
        "LoRA fine-tuning",
        "few-shot visual relation learning",
        "non-rigid transformations",
        "TIE methods",
        "RIE methods"
      ]
    },
    "publishedAt": "2025-03-17T12:04:44.000Z",
    "title": "Edit Transfer: Learning Image Editing via Vision In-Context Relations",
    "summary": "We introduce a new setting, Edit Transfer, where a model learns a\ntransformation from just a single source-target example and applies it to a new\nquery image. While text-based methods excel at semantic manipulations through\ntextual prompts, they often struggle with precise geometric details (e.g.,\nposes and viewpoint changes). Reference-based editing, on the other hand,\ntypically focuses on style or appearance and fails at non-rigid\ntransformations. By explicitly learning the editing transformation from a\nsource-target pair, Edit Transfer mitigates the limitations of both text-only\nand appearance-centric references. Drawing inspiration from in-context learning\nin large language models, we propose a visual relation in-context learning\nparadigm, building upon a DiT-based text-to-image model. We arrange the edited\nexample and the query image into a unified four-panel composite, then apply\nlightweight LoRA fine-tuning to capture complex spatial transformations from\nminimal examples. Despite using only 42 training samples, Edit Transfer\nsubstantially outperforms state-of-the-art TIE and RIE methods on diverse\nnon-rigid scenarios, demonstrating the effectiveness of few-shot visual\nrelation learning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13327.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "640d704c8036cc2142299c19",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640d704c8036cc2142299c19/Wt9AslcVxWOSSc11epk8l.jpeg",
      "fullname": "Lan Chen",
      "name": "Orannue",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.12590",
      "authors": [
        {
          "_id": "67d8de85f7809eea577c4805",
          "name": "Haoran Feng",
          "hidden": false
        },
        {
          "_id": "67d8de85f7809eea577c4806",
          "user": {
            "_id": "6375d136dee28348a9c63cbf",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6375d136dee28348a9c63cbf/gK465HBrQWIOZ-qtHb-Vh.jpeg",
            "isPro": false,
            "fullname": "zehuan-huang",
            "user": "huanngzh",
            "type": "user"
          },
          "name": "Zehuan Huang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-18T08:08:06.131Z",
          "hidden": false
        },
        {
          "_id": "67d8de85f7809eea577c4807",
          "name": "Lin Li",
          "hidden": false
        },
        {
          "_id": "67d8de85f7809eea577c4808",
          "user": {
            "_id": "674ded8ee50d988a4b9e108b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/8oQITwlb7AB8LeIJjooYc.png",
            "isPro": false,
            "fullname": "Hairong Lv",
            "user": "lvhairong",
            "type": "user"
          },
          "name": "Hairong Lv",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:22:23.294Z",
          "hidden": false
        },
        {
          "_id": "67d8de85f7809eea577c4809",
          "name": "Lu Sheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-16T17:51:16.000Z",
      "submittedOnDailyAt": "2025-03-18T01:18:31.307Z",
      "title": "Dans le Transformateur de Diffusion, tous les éléments peuvent être individualisés de manière automatique et gratuitement.",
      "submittedOnDailyBy": {
        "_id": "6375d136dee28348a9c63cbf",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6375d136dee28348a9c63cbf/gK465HBrQWIOZ-qtHb-Vh.jpeg",
        "isPro": false,
        "fullname": "zehuan-huang",
        "user": "huanngzh",
        "type": "user"
      },
      "summary": "La génération d'images personnalisées vise la création d'images avec des concepts personnalisés et des fonctions d'édition flexibles. Les méthodes récentes qui ne requièrent pas d'entraînement ont montré une efficacité informatique plus élevée par rapport aux méthodes basées sur l'entraînement, mais présentent des problèmes dans la maintenance de l'identité, l'applicabilité et la compatibilité avec les Transformers de Diffusion (DiT). Dans cet article, on découvre le potentiel non développé du DiT et on montre que remplaçant les tokens de bruit de référence par 0, il est possible de réaliser des reconfigurations thématiques de 0 shot. Cette technique simple et efficace d'entraînement résout divers scénarios allant des images personnalisées aux éditions. En se basant sur cette découverte, on propose un cadre sans entraînement \"Personalize Anything\" qui utilise le DiT pour réaliser la génération d'images personnalisées. 1) Remplaçant les tokens de l'étape temporelle adaptative introduite dans les étapes initiales pour forcer la concordance du thème et normalisant dans les étapes suivantes pour améliorer la flexibilité. 2) Utilisant Patch Ferritic pour augmenter la diversité structurale. Notre méthode soutient la génération orientée batch, la personnalisation de multiples thèmes et l'édition de masse contrôlée. L'évaluation montre le meilleur rendement en maintenance de l'identité et de la diversité. Notre étude offre de nouveaux insights sur le DiT et fournit un modèle pratique pour la personnalisation efficace.",
      "upvotes": 18,
      "discussionId": "67d8de89f7809eea577c4930",
      "projectPage": "https://fenghora.github.io/Personalize-Anything-Page/",
      "githubRepo": "https://github.com/fenghora/personalize-anything",
      "ai_keywords": [
        "diffusion transformers (DiTs)",
        "denoising tokens",
        "zero-shot subject reconstruction",
        "timestep-adaptive token replacement",
        "early-stage injection",
        "late-stage regularization",
        "patch perturbation strategies",
        "layout-guided generation",
        "multi-subject personalization",
        "mask-controlled editing",
        "identity preservation",
        "versatility"
      ]
    },
    "publishedAt": "2025-03-16T13:51:16.000Z",
    "title": "Personalize Anything for Free with Diffusion Transformer",
    "summary": "Personalized image generation aims to produce images of user-specified\nconcepts while enabling flexible editing. Recent training-free approaches,\nwhile exhibit higher computational efficiency than training-based methods,\nstruggle with identity preservation, applicability, and compatibility with\ndiffusion transformers (DiTs). In this paper, we uncover the untapped potential\nof DiT, where simply replacing denoising tokens with those of a reference\nsubject achieves zero-shot subject reconstruction. This simple yet effective\nfeature injection technique unlocks diverse scenarios, from personalization to\nimage editing. Building upon this observation, we propose Personalize\nAnything, a training-free framework that achieves personalized image\ngeneration in DiT through: 1) timestep-adaptive token replacement that enforces\nsubject consistency via early-stage injection and enhances flexibility through\nlate-stage regularization, and 2) patch perturbation strategies to boost\nstructural diversity. Our method seamlessly supports layout-guided generation,\nmulti-subject personalization, and mask-controlled editing. Evaluations\ndemonstrate state-of-the-art performance in identity preservation and\nversatility. Our work establishes new insights into DiTs while delivering a\npractical paradigm for efficient personalization.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12590.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6375d136dee28348a9c63cbf",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6375d136dee28348a9c63cbf/gK465HBrQWIOZ-qtHb-Vh.jpeg",
      "fullname": "zehuan-huang",
      "name": "huanngzh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 24
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.13435",
      "authors": [
        {
          "_id": "67d8dd0b924be985c277c8f6",
          "user": {
            "_id": "64fde4e252e82dd432b74ce9",
            "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
            "isPro": false,
            "fullname": "Ling Yang",
            "user": "Lingaaaaaaa",
            "type": "user"
          },
          "name": "Ling Yang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:22:51.251Z",
          "hidden": false
        },
        {
          "_id": "67d8dd0b924be985c277c8f7",
          "user": {
            "_id": "6708920aeae29d1cd41a703b",
            "avatarUrl": "/avatars/922427a86523b0aa810412fd2d75f88e.svg",
            "isPro": false,
            "fullname": "kaixin zhu",
            "user": "czkk566",
            "type": "user"
          },
          "name": "Kaixin Zhu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-18T09:16:44.192Z",
          "hidden": false
        },
        {
          "_id": "67d8dd0b924be985c277c8f8",
          "user": {
            "_id": "670880950e79a8b46f7ff9dd",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/670880950e79a8b46f7ff9dd/hA1TLhwlQblkFsq8wLrkB.jpeg",
            "isPro": false,
            "fullname": "Juanxi Tian",
            "user": "Juanxi",
            "type": "user"
          },
          "name": "Juanxi Tian",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-18T08:08:12.128Z",
          "hidden": false
        },
        {
          "_id": "67d8dd0b924be985c277c8f9",
          "user": {
            "_id": "6671214c92412fd4640714eb",
            "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg",
            "isPro": false,
            "fullname": "bohan zeng",
            "user": "zbhpku",
            "type": "user"
          },
          "name": "Bohan Zeng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-18T08:08:15.654Z",
          "hidden": false
        },
        {
          "_id": "67d8dd0b924be985c277c8fa",
          "user": {
            "_id": "64a2a8127adb12be606ec33e",
            "avatarUrl": "/avatars/f85dc39a23727a4d50f8a5f5a3865b0d.svg",
            "isPro": false,
            "fullname": "Mingbao Lin",
            "user": "mingbao",
            "type": "user"
          },
          "name": "Mingbao Lin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:22:58.991Z",
          "hidden": false
        },
        {
          "_id": "67d8dd0b924be985c277c8fb",
          "name": "Hongjuan Pei",
          "hidden": false
        },
        {
          "_id": "67d8dd0b924be985c277c8fc",
          "name": "Wentao Zhang",
          "hidden": false
        },
        {
          "_id": "67d8dd0b924be985c277c8fd",
          "name": "Shuicheng Yan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-17T17:58:18.000Z",
      "submittedOnDailyAt": "2025-03-18T01:44:02.731Z",
      "title": "Extensa reconstruction 4D : on peut réaliser une reconstruction de grande qualité sur un large éventail de mouvements et de scènes.",
      "submittedOnDailyBy": {
        "_id": "64fde4e252e82dd432b74ce9",
        "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
        "isPro": false,
        "fullname": "Ling Yang",
        "user": "Lingaaaaaaa",
        "type": "user"
      },
      "summary": "La rapide évolution de la technologie de reconstruction 3D a conduit à un développement similaire dans la recherche de reconstruction 4D. Les méthodes actuelles de reconstruction 4D peuvent générer des scénarios de haute qualité, mais présentent des problèmes dans la capture de données vidéo multi-angles et les benchmarks de reconstruction 4D se concentrent principalement sur des actions limitées (par exemple, le ballet) dans des scénarios restreints. Dans des scénarios réels, beaucoup d'entre eux contiennent de grands mouvements spatiaux, ce qui souligne clairement les limites des jeux de données de reconstruction 4D actuels. De plus, les méthodes de reconstruction 4D actuelles dépendent de champs de déformation pour évaluer la dynamique des objets 3D, mais ces champs de déformation ont des difficultés avec de grands mouvements spatiaux, ce qui limite la capacité de reconstruction de haute qualité de scénarios 4D qui incluent de grands mouvements spatiaux. Dans cet article, nous nous concentrons sur la reconstruction de scénarios 4D qui incluent de grands mouvements spatiaux et proposons un nouveau benchmark de reconstruction 4D appelé \"WideRange4D\". Ce benchmark est composé de données de scénarios 4D riches qui incluent de grands changements spatiaux, ce qui permet une évaluation plus détaillée de la capacité de création des méthodes de reconstruction 4D. De plus, nous présentons un nouveau méthode de reconstruction 4D appelé \"Progress4D\", qui montre sa capacité à générer des résultats de haute qualité dans des tâches de reconstruction de scénarios 4D complexes de manière stable. Des expériences de comparaison qualitative et quantitative sont réalisées sur WideRange4D et nous montrent que notre Progress4D dépasse les méthodes de reconstruction 4D les plus avancées actuelles. Projet : https://github.com/Gen-Verse/WideRange4D",
      "upvotes": 14,
      "discussionId": "67d8dd0d924be985c277c998",
      "projectPage": "https://huggingface.co/datasets/Gen-Verse/WideRange4D",
      "githubRepo": "https://github.com/Gen-Verse/WideRange4D",
      "ai_keywords": [
        "deformation fields",
        "4D reconstruction",
        "scene reconstruction",
        "multi-view video",
        "spatial movements",
        "3D objects",
        "4D scene data",
        "generation capabilities",
        "4D generation methods",
        "WideRange4D",
        "Progress4D"
      ]
    },
    "publishedAt": "2025-03-17T13:58:18.000Z",
    "title": "WideRange4D: Enabling High-Quality 4D Reconstruction with Wide-Range\n  Movements and Scenes",
    "summary": "With the rapid development of 3D reconstruction technology, research in 4D\nreconstruction is also advancing, existing 4D reconstruction methods can\ngenerate high-quality 4D scenes. However, due to the challenges in acquiring\nmulti-view video data, the current 4D reconstruction benchmarks mainly display\nactions performed in place, such as dancing, within limited scenarios. In\npractical scenarios, many scenes involve wide-range spatial movements,\nhighlighting the limitations of existing 4D reconstruction datasets.\nAdditionally, existing 4D reconstruction methods rely on deformation fields to\nestimate the dynamics of 3D objects, but deformation fields struggle with\nwide-range spatial movements, which limits the ability to achieve high-quality\n4D scene reconstruction with wide-range spatial movements. In this paper, we\nfocus on 4D scene reconstruction with significant object spatial movements and\npropose a novel 4D reconstruction benchmark, WideRange4D. This benchmark\nincludes rich 4D scene data with large spatial variations, allowing for a more\ncomprehensive evaluation of the generation capabilities of 4D generation\nmethods. Furthermore, we introduce a new 4D reconstruction method, Progress4D,\nwhich generates stable and high-quality 4D results across various complex 4D\nscene reconstruction tasks. We conduct both quantitative and qualitative\ncomparison experiments on WideRange4D, showing that our Progress4D outperforms\nexisting state-of-the-art 4D reconstruction methods. Project:\nhttps://github.com/Gen-Verse/WideRange4D",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13435.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64fde4e252e82dd432b74ce9",
      "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
      "fullname": "Ling Yang",
      "name": "Lingaaaaaaa",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.13434",
      "authors": [
        {
          "_id": "67d916b500030726e0df2a67",
          "user": {
            "_id": "6362801380c1a705a6ea54ac",
            "avatarUrl": "/avatars/041ad5abf9be42e336938f51ebb8746c.svg",
            "isPro": false,
            "fullname": "Yaowei Li",
            "user": "Yw22",
            "type": "user"
          },
          "name": "Yaowei Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:23:48.215Z",
          "hidden": false
        },
        {
          "_id": "67d916b500030726e0df2a68",
          "user": {
            "_id": "66837d3c48edefb453b0640a",
            "avatarUrl": "/avatars/b16385eaa612578728e2c6460a76b38f.svg",
            "isPro": false,
            "fullname": "Lingen Li",
            "user": "l-li",
            "type": "user"
          },
          "name": "Lingen Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:23:54.498Z",
          "hidden": false
        },
        {
          "_id": "67d916b500030726e0df2a69",
          "user": {
            "_id": "658409ceca19ccf6d9989add",
            "avatarUrl": "/avatars/3ac1dbd25e52435185babdeb3da28875.svg",
            "isPro": false,
            "fullname": "Zhaoyang Zhang",
            "user": "ZyZcuhk",
            "type": "user"
          },
          "name": "Zhaoyang Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-18T08:07:22.583Z",
          "hidden": false
        },
        {
          "_id": "67d916b500030726e0df2a6a",
          "name": "Xiaoyu Li",
          "hidden": false
        },
        {
          "_id": "67d916b500030726e0df2a6b",
          "user": {
            "_id": "6422b973ef9e8971003cdd22",
            "avatarUrl": "/avatars/8564a2e984e2e79e46d90cc9c35e5773.svg",
            "isPro": false,
            "fullname": "Guangzhi Wang",
            "user": "daoyuan98",
            "type": "user"
          },
          "name": "Guangzhi Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:24:18.787Z",
          "hidden": false
        },
        {
          "_id": "67d916b500030726e0df2a6c",
          "user": {
            "_id": "653eb3bd4a52f10eaf72fbaf",
            "avatarUrl": "/avatars/b525482b61c6f6054bf44bbc3113c29f.svg",
            "isPro": false,
            "fullname": "Hongxiang Li",
            "user": "HongxiangLi",
            "type": "user"
          },
          "name": "Hongxiang Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:24:25.700Z",
          "hidden": false
        },
        {
          "_id": "67d916b500030726e0df2a6d",
          "user": {
            "_id": "63184c517ca1b876d99b7e0e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63184c517ca1b876d99b7e0e/b-qDExoeJuDXK0cJBZKnz.jpeg",
            "isPro": false,
            "fullname": "Xiaodong Cun",
            "user": "vinthony",
            "type": "user"
          },
          "name": "Xiaodong Cun",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:24:32.118Z",
          "hidden": false
        },
        {
          "_id": "67d916b500030726e0df2a6e",
          "user": {
            "_id": "63ca3ddc04c979828310bfcb",
            "avatarUrl": "/avatars/615e0d8622950b4408b40d550f02a894.svg",
            "isPro": false,
            "fullname": "Ying Shan",
            "user": "yshan2u",
            "type": "user"
          },
          "name": "Ying Shan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:24:39.725Z",
          "hidden": false
        },
        {
          "_id": "67d916b500030726e0df2a6f",
          "name": "Yuexian Zou",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/658409ceca19ccf6d9989add/fhyGe83BiDhDf9a71kUyP.mp4"
      ],
      "publishedAt": "2025-03-17T17:58:05.000Z",
      "submittedOnDailyAt": "2025-03-18T05:20:30.708Z",
      "title": "BlobCtrl : Gestion uniforme de la génération et de l'édition d'images à l'échelle des éléments et offre un cadre flexible.",
      "submittedOnDailyBy": {
        "_id": "658409ceca19ccf6d9989add",
        "avatarUrl": "/avatars/3ac1dbd25e52435185babdeb3da28875.svg",
        "isPro": false,
        "fullname": "Zhaoyang Zhang",
        "user": "ZyZcuhk",
        "type": "user"
      },
      "summary": "Les travaux de visualisation au niveau d'éléments sont essentiels dans le développement de contenu numérique, mais les méthodes basées sur l'expansion actuelle présentent des déficits en termes de précision et de flexibilité lorsqu'elles sont comparées aux outils traditionnels. Dans cet article, nous présentons un cadre de travail qui intègre la génération et l'édition au niveau d'éléments en utilisant des représentations blob basées sur des probabilités, appelé BlobCtrl. En utilisant des blob comme base de visualisation, notre approche permet une séparation adéquate de l'information sur la position spatiale, le contenu littéraire et l'identité, ainsi que la manipulation précise au niveau d'éléments. Les principaux contributeurs sont : 1) l'intégration continue de fond et de traînée par une structure double de brancheur utilisant des fusions de caractéristiques hiérarchiques ; 2) un paradigme d'apprentissage autosupervisé qui s'adapte à l'expansion de données et aux fonctions de pondération ; 3) une stratégie de dropout pour équilibrer la rétroaction et la diversité. Pour les futures recherches, nous présentons BlobData, qui soutient l'entraînement à grande échelle, et BlobBench, qui permet des évaluations systématiques. Les résultats des expérimentations montrent que BlobCtrl présente des résultats exceptionnels dans les tâches de manipulation d'éléments au niveau de composants. De plus, il montre la possibilité d'être une solution pratique et efficace pour la création de contenu visualisé avec précision et flexibilité. Page du projet : https://liyaowei-stu.github.io/project/BlobCtrl/",
      "upvotes": 12,
      "discussionId": "67d916bc00030726e0df2c3e",
      "projectPage": "https://liyaowei-stu.github.io/project/BlobCtrl/",
      "githubRepo": "https://github.com/TencentARC/BlobCtrl",
      "ai_keywords": [
        "probabilistic blob-based representation",
        "dual-branch diffusion architecture",
        "hierarchical feature fusion",
        "self-supervised training paradigm",
        "tailored data augmentation",
        "score functions",
        "controllable dropout strategies",
        "BlobData",
        "BlobBench"
      ]
    },
    "publishedAt": "2025-03-17T13:58:05.000Z",
    "title": "BlobCtrl: A Unified and Flexible Framework for Element-level Image\n  Generation and Editing",
    "summary": "Element-level visual manipulation is essential in digital content creation,\nbut current diffusion-based methods lack the precision and flexibility of\ntraditional tools. In this work, we introduce BlobCtrl, a framework that\nunifies element-level generation and editing using a probabilistic blob-based\nrepresentation. By employing blobs as visual primitives, our approach\neffectively decouples and represents spatial location, semantic content, and\nidentity information, enabling precise element-level manipulation. Our key\ncontributions include: 1) a dual-branch diffusion architecture with\nhierarchical feature fusion for seamless foreground-background integration; 2)\na self-supervised training paradigm with tailored data augmentation and score\nfunctions; and 3) controllable dropout strategies to balance fidelity and\ndiversity. To support further research, we introduce BlobData for large-scale\ntraining and BlobBench for systematic evaluation. Experiments show that\nBlobCtrl excels in various element-level manipulation tasks while maintaining\ncomputational efficiency, offering a practical solution for precise and\nflexible visual content creation. Project page:\nhttps://liyaowei-stu.github.io/project/BlobCtrl/",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/658409ceca19ccf6d9989add/fhyGe83BiDhDf9a71kUyP.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13434.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "658409ceca19ccf6d9989add",
      "avatarUrl": "/avatars/3ac1dbd25e52435185babdeb3da28875.svg",
      "fullname": "Zhaoyang Zhang",
      "name": "ZyZcuhk",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.11751",
      "authors": [
        {
          "_id": "67d8e861fa59a8b15a921052",
          "user": {
            "_id": "6351712b40dffad651f128c7",
            "avatarUrl": "/avatars/87708c86c1baef548ef556f5d32dca71.svg",
            "isPro": false,
            "fullname": "Zhaofeng Wu",
            "user": "ZhaofengWu",
            "type": "user"
          },
          "name": "Zhaofeng Wu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:27:29.372Z",
          "hidden": false
        },
        {
          "_id": "67d8e861fa59a8b15a921053",
          "user": {
            "_id": "621e9388345a1d9ab65391c3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/621e9388345a1d9ab65391c3/RxurNzyAWJOUdgeSHQi1R.jpeg",
            "isPro": false,
            "fullname": "Michihiro Yasunaga",
            "user": "michiyasunaga",
            "type": "user"
          },
          "name": "Michihiro Yasunaga",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:27:35.318Z",
          "hidden": false
        },
        {
          "_id": "67d8e861fa59a8b15a921054",
          "name": "Andrew Cohen",
          "hidden": false
        },
        {
          "_id": "67d8e861fa59a8b15a921055",
          "name": "Yoon Kim",
          "hidden": false
        },
        {
          "_id": "67d8e861fa59a8b15a921056",
          "name": "Asli Celikyilmaz",
          "hidden": false
        },
        {
          "_id": "67d8e861fa59a8b15a921057",
          "user": {
            "_id": "660f0fd377a1e2509aa5a679",
            "avatarUrl": "/avatars/e04ef05bed0bf6cefdc7e3e39674e2f9.svg",
            "isPro": false,
            "fullname": "Marjan Ghazvininejad",
            "user": "mghazvininejad",
            "type": "user"
          },
          "name": "Marjan Ghazvininejad",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:28:15.386Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-14T17:59:41.000Z",
      "submittedOnDailyAt": "2025-03-18T01:59:35.073Z",
      "title": "reWordBench : Évaluation de la robustesse des modèles de récompense en utilisant des entrées transformées et amélioration",
      "submittedOnDailyBy": {
        "_id": "6351712b40dffad651f128c7",
        "avatarUrl": "/avatars/87708c86c1baef548ef556f5d32dca71.svg",
        "isPro": false,
        "fullname": "Zhaofeng Wu",
        "user": "ZhaofengWu",
        "type": "user"
      },
      "summary": "Le modèle de correction est une fonction standard dans le NLP moderne. Il s'agit d'un évaluateur de texte scalable et constitué de multiples composants essentiels, comme des recettes d'alignement et des algorithmes d'inférence. Cependant, les modèles de correction récents ont montré des améliorations sur les marqueurs standards, mais certains ont été affectés par le sur-ajustement, ce qui peut obstaculer la compréhension de leurs capacités fondamentales. Dans cet article, nous étudions la robustesse et le degré de sur-ajustement des modèles de correction. Nous construisons **reWordBench** et transformons de manière systématique les entrées des modèles de correction pour maintenir leur sens et l'ordre. Nous démontrons que les modèles de correction les plus récents subissent une perte considérable de performance avec seulement quelques transformations d'entrée, parfois même sous une précision inférieure à la aléatoire. Pour améliorer la robustesse des modèles de correction, nous proposons de les entraîner pour attribuer clairement des scores à des phrases transformées de manière significative. Ce cadre d'approche améliore également la robustesse face à d'autres transformations. Par exemple, notre modèle de correction robuste peut réduire la perte dans le sous-ensemble Chat Hard de RewardBench d'environ la moitié. De plus, lorsqu'il est utilisé avec l'alignement, notre modèle de correction robuste montre une plus grande efficacité, génère des sorties de haute qualité et dépasse les modèles d'entraînement standard dans 59% des cas.",
      "upvotes": 12,
      "discussionId": "67d8e866fa59a8b15a92117c",
      "ai_keywords": [
        "reward models",
        "NLP",
        "text evaluator",
        "alignment",
        "inference-time algorithms",
        "overfitting",
        "reWordBench",
        "meaning-preserving",
        "ranking-preserving",
        "state-of-the-art",
        "performance degradation",
        "below-random accuracy",
        "brittleness",
        "paraphrases",
        "robust reward model",
        "Chat Hard subset",
        "RewardBench",
        "utility",
        "higher-quality outputs"
      ]
    },
    "publishedAt": "2025-03-14T13:59:41.000Z",
    "title": "reWordBench: Benchmarking and Improving the Robustness of Reward Models\n  with Transformed Inputs",
    "summary": "Reward models have become a staple in modern NLP, serving as not only a\nscalable text evaluator, but also an indispensable component in many alignment\nrecipes and inference-time algorithms. However, while recent reward models\nincrease performance on standard benchmarks, this may partly be due to\noverfitting effects, which would confound an understanding of their true\ncapability. In this work, we scrutinize the robustness of reward models and the\nextent of such overfitting. We build **reWordBench**, which systematically\ntransforms reward model inputs in meaning- or ranking-preserving ways. We show\nthat state-of-the-art reward models suffer from substantial performance\ndegradation even with minor input transformations, sometimes dropping to\nsignificantly below-random accuracy, suggesting brittleness. To improve reward\nmodel robustness, we propose to explicitly train them to assign similar scores\nto paraphrases, and find that this approach also improves robustness to other\ndistinct kinds of transformations. For example, our robust reward model reduces\nsuch degradation by roughly half for the Chat Hard subset in RewardBench.\nFurthermore, when used in alignment, our robust reward models demonstrate\nbetter utility and lead to higher-quality outputs, winning in up to 59% of\ninstances against a standardly trained RM.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.11751.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6351712b40dffad651f128c7",
      "avatarUrl": "/avatars/87708c86c1baef548ef556f5d32dca71.svg",
      "fullname": "Zhaofeng Wu",
      "name": "ZhaofengWu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.13399",
      "authors": [
        {
          "_id": "67d8d99a0983992037cdf33f",
          "user": {
            "_id": "650871aeb44445e9b3625c7b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/650871aeb44445e9b3625c7b/mtx3EnkuNF4z29IosnhaQ.png",
            "isPro": false,
            "fullname": "James Burgess",
            "user": "jmhb",
            "type": "user"
          },
          "name": "James Burgess",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-18T08:08:18.287Z",
          "hidden": false
        },
        {
          "_id": "67d8d99a0983992037cdf340",
          "name": "Jeffrey J Nirschl",
          "hidden": false
        },
        {
          "_id": "67d8d99a0983992037cdf341",
          "name": "Laura Bravo-Sánchez",
          "hidden": false
        },
        {
          "_id": "67d8d99a0983992037cdf342",
          "name": "Alejandro Lozano",
          "hidden": false
        },
        {
          "_id": "67d8d99a0983992037cdf343",
          "name": "Sanket Rajan Gupte",
          "hidden": false
        },
        {
          "_id": "67d8d99a0983992037cdf344",
          "name": "Jesus G. Galaz-Montoya",
          "hidden": false
        },
        {
          "_id": "67d8d99a0983992037cdf345",
          "name": "Yuhui Zhang",
          "hidden": false
        },
        {
          "_id": "67d8d99a0983992037cdf346",
          "user": {
            "_id": "666aa5183263a8feca6b7003",
            "avatarUrl": "/avatars/6ac4d52e8abea0df9f83da408502c076.svg",
            "isPro": false,
            "fullname": "Yuchang Su",
            "user": "suyc21",
            "type": "user"
          },
          "name": "Yuchang Su",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:27:03.753Z",
          "hidden": false
        },
        {
          "_id": "67d8d99a0983992037cdf347",
          "name": "Disha Bhowmik",
          "hidden": false
        },
        {
          "_id": "67d8d99a0983992037cdf348",
          "name": "Zachary Coman",
          "hidden": false
        },
        {
          "_id": "67d8d99a0983992037cdf349",
          "name": "Sarina M. Hasan",
          "hidden": false
        },
        {
          "_id": "67d8d99a0983992037cdf34a",
          "name": "Alexandra Johannesson",
          "hidden": false
        },
        {
          "_id": "67d8d99a0983992037cdf34b",
          "name": "William D. Leineweber",
          "hidden": false
        },
        {
          "_id": "67d8d99a0983992037cdf34c",
          "name": "Malvika G Nair",
          "hidden": false
        },
        {
          "_id": "67d8d99a0983992037cdf34d",
          "name": "Ridhi Yarlagadda",
          "hidden": false
        },
        {
          "_id": "67d8d99a0983992037cdf34e",
          "name": "Connor Zuraski",
          "hidden": false
        },
        {
          "_id": "67d8d99a0983992037cdf34f",
          "name": "Wah Chiu",
          "hidden": false
        },
        {
          "_id": "67d8d99a0983992037cdf350",
          "user": {
            "_id": "655b8dbb83186f133f7f8a98",
            "avatarUrl": "/avatars/15f41d0efb3a59a2e389cdb5338e0c1e.svg",
            "isPro": false,
            "fullname": "Sarah Cohen",
            "user": "shcohen",
            "type": "user"
          },
          "name": "Sarah Cohen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:26:24.760Z",
          "hidden": false
        },
        {
          "_id": "67d8d99a0983992037cdf351",
          "name": "Jan N. Hansen",
          "hidden": false
        },
        {
          "_id": "67d8d99a0983992037cdf352",
          "name": "Manuel D Leonetti",
          "hidden": false
        },
        {
          "_id": "67d8d99a0983992037cdf353",
          "user": {
            "_id": "64fc5c1cc45dd732acc2ec48",
            "avatarUrl": "/avatars/b1f072cbfec014f1a054d4a433cff93c.svg",
            "isPro": false,
            "fullname": "Chad Liu",
            "user": "chadliu",
            "type": "user"
          },
          "name": "Chad Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:25:42.249Z",
          "hidden": false
        },
        {
          "_id": "67d8d99a0983992037cdf354",
          "user": {
            "_id": "678af263320331c7e008f842",
            "avatarUrl": "/avatars/e2cde80f018f3dd278000270fdbc104d.svg",
            "isPro": false,
            "fullname": "emma lundberg",
            "user": "lundbergemma",
            "type": "user"
          },
          "name": "Emma Lundberg",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:25:33.946Z",
          "hidden": false
        },
        {
          "_id": "67d8d99a0983992037cdf355",
          "name": "Serena Yeung-Levy",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-17T17:33:10.000Z",
      "submittedOnDailyAt": "2025-03-18T01:06:54.667Z",
      "title": "MicroVQA : Benchmark de Microstructures de Science Visionnelle pour la Théorie de la Structure Multimodale",
      "submittedOnDailyBy": {
        "_id": "650871aeb44445e9b3625c7b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/650871aeb44445e9b3625c7b/mtx3EnkuNF4z29IosnhaQ.png",
        "isPro": false,
        "fullname": "James Burgess",
        "user": "jmhb",
        "type": "user"
      },
      "summary": "La recherche scientifique exige une complexe inférence sur des données multimodales, un défi particulièrement courant en biologie. Bien que des avancées récentes aient été réalisées dans les modèles de langage multimodal (MLLMs), les tests de référence actuels pour l'inférence multimodal se concentrent sur des difficultés universitaires, et les tests de référence au niveau de la recherche mettent en avant un faible niveau de sensibilité, ne couvrant pas les complexes inférences multimodales nécessaires pour la recherche scientifique. Pour combler cette lacune, nous présentons MicroVQA. Ceci est un test de référence de questions et réponses visuelles (VQA) qui évalue trois capacités d'inférence cruciales dans le processus de travail de recherche. MicroVQA comprend 1,042 questions de choix multiple (MCQs) sélectionnées avec soin par des experts en biologie de différentes modalités d'optique microscopique, représentant des exemples de pratiques scientifiques réelles. En construisant le test de référence, nous avons découvert que les méthodes standards pour la génération de MCQs peuvent conduire à un apprentissage du langage, nous proposons une nouvelle pipeline en deux étapes : premièrement, structurer des paires de questions-réponses en MCQs avec l'aide d'hints de modèles de langage optimisés, et secondement, éliminer cet apprentissage du langage par `RefineBot`. En effectuant le test de référence avec les MLLMs les plus avancés, nous avons obtenu un rendement maximal de 53%. Bien que les modèles plus petits de langage de langage soient légèrement affectés, cela démontre que l'inférence basée sur le langage est plus défique que l'inférence multimodale. Nous avons vérifié que le rendement peut être amélioré en fine-tunant des articles scientifiques. Selon l'analyse des erreurs de raisonnement en chaîne, les erreurs de sensibilité sont les plus communes, suivies par les erreurs de connaissance et ensuite par les erreurs de généralisation excessive. Ces observations mettent en lumière les défis de l'inférence multimodale scientifique et démontrent que MicroVQA est une source précieuse pour la recherche biomédicale dirigée par l'intelligence artificielle. MicroVQA peut être obtenue à l'adresse https://huggingface.co/datasets/jmhb/microvqa et le site web du projet se trouve à https://jmhb0.github.io/microvqa.",
      "upvotes": 10,
      "discussionId": "67d8d99f0983992037cdf47e",
      "projectPage": "https://jmhb0.github.io/microvqa/",
      "githubRepo": "https://github.com/jmhb0/microvqa",
      "ai_keywords": [
        "multimodal large language models",
        "visual-question answering (VQA)",
        "multiple-choice questions (MCQs)",
        "biology experts",
        "microscopy modalities",
        "standard MCQ generation methods",
        "optimized LLM prompt",
        "agent-based `RefineBot'",
        "chain-of-thought responses",
        "perception errors",
        "knowledge errors",
        "overgeneralization errors"
      ]
    },
    "publishedAt": "2025-03-17T13:33:10.000Z",
    "title": "MicroVQA: A Multimodal Reasoning Benchmark for Microscopy-Based\n  Scientific Research",
    "summary": "Scientific research demands sophisticated reasoning over multimodal data, a\nchallenge especially prevalent in biology. Despite recent advances in\nmultimodal large language models (MLLMs) for AI-assisted research, existing\nmultimodal reasoning benchmarks only target up to college-level difficulty,\nwhile research-level benchmarks emphasize lower-level perception, falling short\nof the complex multimodal reasoning needed for scientific discovery. To bridge\nthis gap, we introduce MicroVQA, a visual-question answering (VQA) benchmark\ndesigned to assess three reasoning capabilities vital in research workflows:\nexpert image understanding, hypothesis generation, and experiment proposal.\nMicroVQA consists of 1,042 multiple-choice questions (MCQs) curated by biology\nexperts across diverse microscopy modalities, ensuring VQA samples represent\nreal scientific practice. In constructing the benchmark, we find that standard\nMCQ generation methods induce language shortcuts, motivating a new two-stage\npipeline: an optimized LLM prompt structures question-answer pairs into MCQs;\nthen, an agent-based `RefineBot' updates them to remove shortcuts. Benchmarking\non state-of-the-art MLLMs reveal a peak performance of 53\\%; models with\nsmaller LLMs only slightly underperform top models, suggesting that\nlanguage-based reasoning is less challenging than multimodal reasoning; and\ntuning with scientific articles enhances performance. Expert analysis of\nchain-of-thought responses shows that perception errors are the most frequent,\nfollowed by knowledge errors and then overgeneralization errors. These insights\nhighlight the challenges in multimodal scientific reasoning, showing MicroVQA\nis a valuable resource advancing AI-driven biomedical research. MicroVQA is\navailable at https://huggingface.co/datasets/jmhb/microvqa, and project page at\nhttps://jmhb0.github.io/microvqa.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13399.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "650871aeb44445e9b3625c7b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/650871aeb44445e9b3625c7b/mtx3EnkuNF4z29IosnhaQ.png",
      "fullname": "James Burgess",
      "name": "jmhb",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.12605",
      "authors": [
        {
          "_id": "67d939f6fa59a8b15aa931a8",
          "user": {
            "_id": "64ff369d9abcc85a5519b33e",
            "avatarUrl": "/avatars/4b99cdaf5f970d930b196eddf1e5e499.svg",
            "isPro": false,
            "fullname": "Yaoting Wang",
            "user": "Gh0stAR",
            "type": "user"
          },
          "name": "Yaoting Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:29:33.845Z",
          "hidden": false
        },
        {
          "_id": "67d939f6fa59a8b15aa931a9",
          "user": {
            "_id": "64c139d867eff857ea51caa8",
            "avatarUrl": "/avatars/4b7b3f41c2e2cfa21dd43bbac6e081ae.svg",
            "isPro": false,
            "fullname": "Shengqiong Wu",
            "user": "ChocoWu",
            "type": "user"
          },
          "name": "Shengqiong Wu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:29:41.386Z",
          "hidden": false
        },
        {
          "_id": "67d939f6fa59a8b15aa931aa",
          "name": "Yuecheng Zhang",
          "hidden": false
        },
        {
          "_id": "67d939f6fa59a8b15aa931ab",
          "name": "William Wang",
          "hidden": false
        },
        {
          "_id": "67d939f6fa59a8b15aa931ac",
          "user": {
            "_id": "62ab1ac1d48b4d8b048a3473",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656826685333-62ab1ac1d48b4d8b048a3473.png",
            "isPro": false,
            "fullname": "Ziwei Liu",
            "user": "liuziwei7",
            "type": "user"
          },
          "name": "Ziwei Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:30:17.710Z",
          "hidden": false
        },
        {
          "_id": "67d939f6fa59a8b15aa931ad",
          "name": "Jiebo Luo",
          "hidden": false
        },
        {
          "_id": "67d939f6fa59a8b15aa931ae",
          "user": {
            "_id": "647773a1168cb428e00e9a8f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647773a1168cb428e00e9a8f/NiRR3ScY6Plzjibfwy1hC.jpeg",
            "isPro": false,
            "fullname": "Hao Fei",
            "user": "scofield7419",
            "type": "user"
          },
          "name": "Hao Fei",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-18T09:30:49.867Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64ff369d9abcc85a5519b33e/YhPHIT3BwUWkXcOT1r1LJ.png",
        "https://cdn-uploads.huggingface.co/production/uploads/64ff369d9abcc85a5519b33e/7SvBogepNT-uNYcY5dAgv.png",
        "https://cdn-uploads.huggingface.co/production/uploads/64ff369d9abcc85a5519b33e/VqwQO1NsLKrjaKEBZxXUb.png"
      ],
      "publishedAt": "2025-03-16T18:39:13.000Z",
      "submittedOnDailyAt": "2025-03-18T07:53:47.429Z",
      "title": "Damodar Core Obsidian Implementation : Enquête Complète",
      "submittedOnDailyBy": {
        "_id": "64ff369d9abcc85a5519b33e",
        "avatarUrl": "/avatars/4b99cdaf5f970d930b196eddf1e5e499.svg",
        "isPro": false,
        "fullname": "Yaoting Wang",
        "user": "Gh0stAR",
        "type": "user"
      },
      "summary": "MCoT (Chain of Thought Multi-Type) est un approche qui exploite les avantages des processus qui se développent de manière humaine, pas à pas, et qui peuvent être étendus à différents contextes. En particulier, il a récemment attiré l'attention par son intégration avec des modèles de langage multi-type (MLLMs). Elle est efficace dans des domaines tels que la robotique, la médecine, la conduite autonome et la génération multi-type.",
      "upvotes": 8,
      "discussionId": "67d939f7fa59a8b15aa9322a",
      "projectPage": "https://github.com/yaotingwangofficial/Awesome-MCoT",
      "githubRepo": "https://github.com/yaotingwangofficial/Awesome-MCoT",
      "ai_keywords": [
        "chain-of-thought (CoT) reasoning",
        "multimodal CoT (MCoT) reasoning",
        "multimodal large language models (MLLMs)",
        "image",
        "video",
        "speech",
        "audio",
        "3D",
        "structured data",
        "robotics",
        "healthcare",
        "autonomous driving",
        "multimodal generation",
        "multimodal AGI"
      ]
    },
    "publishedAt": "2025-03-16T14:39:13.000Z",
    "title": "Multimodal Chain-of-Thought Reasoning: A Comprehensive Survey",
    "summary": "By extending the advantage of chain-of-thought (CoT) reasoning in human-like\nstep-by-step processes to multimodal contexts, multimodal CoT (MCoT) reasoning\nhas recently garnered significant research attention, especially in the\nintegration with multimodal large language models (MLLMs). Existing MCoT\nstudies design various methodologies and innovative reasoning paradigms to\naddress the unique challenges of image, video, speech, audio, 3D, and\nstructured data across different modalities, achieving extensive success in\napplications such as robotics, healthcare, autonomous driving, and multimodal\ngeneration. However, MCoT still presents distinct challenges and opportunities\nthat require further focus to ensure consistent thriving in this field, where,\nunfortunately, an up-to-date review of this domain is lacking. To bridge this\ngap, we present the first systematic survey of MCoT reasoning, elucidating the\nrelevant foundational concepts and definitions. We offer a comprehensive\ntaxonomy and an in-depth analysis of current methodologies from diverse\nperspectives across various application scenarios. Furthermore, we provide\ninsights into existing challenges and future research directions, aiming to\nfoster innovation toward multimodal AGI.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64ff369d9abcc85a5519b33e/YhPHIT3BwUWkXcOT1r1LJ.png",
      "https://cdn-uploads.huggingface.co/production/uploads/64ff369d9abcc85a5519b33e/7SvBogepNT-uNYcY5dAgv.png",
      "https://cdn-uploads.huggingface.co/production/uploads/64ff369d9abcc85a5519b33e/VqwQO1NsLKrjaKEBZxXUb.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12605.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ff369d9abcc85a5519b33e",
      "avatarUrl": "/avatars/4b99cdaf5f970d930b196eddf1e5e499.svg",
      "fullname": "Yaoting Wang",
      "name": "Gh0stAR",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.12937",
      "authors": [
        {
          "_id": "67d8eb0c18de6ef86c4eb457",
          "name": "Jingyi Zhang",
          "hidden": false
        },
        {
          "_id": "67d8eb0c18de6ef86c4eb458",
          "user": {
            "_id": "65237910b80dc49ba03a96d9",
            "avatarUrl": "/avatars/9d81c4c8fb2d597079e8dd9d9b79a8d8.svg",
            "isPro": false,
            "fullname": "jiaxing",
            "user": "huangjiaxing",
            "type": "user"
          },
          "name": "Jiaxing Huang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:31:01.731Z",
          "hidden": false
        },
        {
          "_id": "67d8eb0c18de6ef86c4eb459",
          "user": {
            "_id": "6590e03454f8826173ed5ee6",
            "avatarUrl": "/avatars/b2fbaaf444e1e53c5e914cd42a41389a.svg",
            "isPro": false,
            "fullname": "Huanjin Yao",
            "user": "HuanjinYao",
            "type": "user"
          },
          "name": "Huanjin Yao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:31:08.069Z",
          "hidden": false
        },
        {
          "_id": "67d8eb0c18de6ef86c4eb45a",
          "user": {
            "_id": "6713afea187a20dc579e121b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6713afea187a20dc579e121b/ELxVQLVF9ifuT-TCWPK22.jpeg",
            "isPro": false,
            "fullname": "Shunyu Liu",
            "user": "liushunyu",
            "type": "user"
          },
          "name": "Shunyu Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:31:15.587Z",
          "hidden": false
        },
        {
          "_id": "67d8eb0c18de6ef86c4eb45b",
          "user": {
            "_id": "6274a9620d11b4f675085fbc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1651812606924-noauth.jpeg",
            "isPro": false,
            "fullname": "Xikun Zhang",
            "user": "Xikun",
            "type": "user"
          },
          "name": "Xikun Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:31:21.950Z",
          "hidden": false
        },
        {
          "_id": "67d8eb0c18de6ef86c4eb45c",
          "name": "Shijian Lu",
          "hidden": false
        },
        {
          "_id": "67d8eb0c18de6ef86c4eb45d",
          "name": "Dacheng Tao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-17T08:51:44.000Z",
      "submittedOnDailyAt": "2025-03-18T02:10:10.429Z",
      "title": "R1-VL : Méthode d'optimisation de politiques collectives relatives aux groupes pour l'apprentissage de la logique en utilisant le modèle de langage de Damo",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Les derniers études ont amélioré la capacité d'inférence des modèles logiques à base de micro-ajustements en utilisant des données d'inférence de contenu de haute qualité (MLLMs), mais ils souvent se concentrent sur simplement mimétiser les étapes d'inférence qui fonctionnent généralement bien, sans comprendre en totalité ce qui ne fonctionne pas. Dans cet article, nous dépassons l'approche indirecte de la capacité d'inférence des MLLMs et nous nous concentrons sur l'amélioration de cette capacité. Pour cela, nous avons conçu un nouveau cadre d'apprentissage en ligne d'efficacité de réentraînement appelé \"Step-wise Group Relative Policy Optimization (StepGRPO)\". Ce cadre permet aux MLLMs d'améliorer leur capacité d'inférence grâce à une compensation efficace et dense en étapes. Spécifiquement, StepGRPO introduit deux nouvelles règles de compensation basées sur l'inférence : \"Step-wise Reasoning Accuracy Reward (StepRAR)\" et \"Step-wise Reasoning Validity Reward (StepRVR)\". StepRAR fournit une compensation par un méthode flexible d'ajustement de pas clés, y compris des étapes intermédiaires dans le pas d'inférence, tandis que StepRVR offre une compensation par la cohérence logique du processus d'inférence et la complexité et la stratégie d'évaluation logique. Grâce au StepGRPO, nous avons introduit une série de MLLMs avec une excellente capacité d'inférence en étapes, comme R1-VL. Les expériences sur 8 benchmarks étendus ont démontré la performance exceptionnelle de notre méthode.",
      "upvotes": 7,
      "discussionId": "67d8eb0d18de6ef86c4eb4aa",
      "ai_keywords": [
        "Step-wise Group Relative Policy Optimization (StepGRPO)",
        "online reinforcement learning",
        "Step-wise Reasoning Accuracy Reward (StepRAR)",
        "Step-wise Reasoning Validity Reward (StepRVR)",
        "soft key-step matching",
        "reasoning completeness",
        "logic evaluation",
        "R1-VL",
        "step-by-step reasoning"
      ]
    },
    "publishedAt": "2025-03-17T04:51:44.000Z",
    "title": "R1-VL: Learning to Reason with Multimodal Large Language Models via\n  Step-wise Group Relative Policy Optimization",
    "summary": "Recent studies generally enhance MLLMs' reasoning capabilities via supervised\nfine-tuning on high-quality chain-of-thought reasoning data, which often leads\nmodels to merely imitate successful reasoning paths without understanding what\nthe wrong reasoning paths are. In this work, we aim to enhance the MLLMs'\nreasoning ability beyond passively imitating positive reasoning paths. To this\nend, we design Step-wise Group Relative Policy Optimization (StepGRPO), a new\nonline reinforcement learning framework that enables MLLMs to self-improve\nreasoning ability via simple, effective and dense step-wise rewarding.\nSpecifically, StepGRPO introduces two novel rule-based reasoning rewards:\nStep-wise Reasoning Accuracy Reward (StepRAR) and Step-wise Reasoning Validity\nReward (StepRVR). StepRAR rewards the reasoning paths that contain necessary\nintermediate reasoning steps via a soft key-step matching technique, while\nStepRAR rewards reasoning paths that follow a well-structured and logically\nconsistent reasoning process through a reasoning completeness and logic\nevaluation strategy. With the proposed StepGRPO, we introduce R1-VL, a series\nof MLLMs with outstanding capabilities in step-by-step reasoning. Extensive\nexperiments over 8 benchmarks demonstrate the superiority of our methods.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12937.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6390
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.13444",
      "authors": [
        {
          "_id": "67d8eeb17e184aa2954d19f4",
          "name": "Ye Liu",
          "hidden": false
        },
        {
          "_id": "67d8eeb17e184aa2954d19f5",
          "user": {
            "_id": "64440be5af034cdfd69ca3a7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64440be5af034cdfd69ca3a7/qmx24QiDFT29vleCxL9TX.jpeg",
            "isPro": true,
            "fullname": "Qinghong (Kevin) Lin",
            "user": "KevinQHLin",
            "type": "user"
          },
          "name": "Kevin Qinghong Lin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:28:47.081Z",
          "hidden": false
        },
        {
          "_id": "67d8eeb17e184aa2954d19f6",
          "name": "Chang Wen Chen",
          "hidden": false
        },
        {
          "_id": "67d8eeb17e184aa2954d19f7",
          "user": {
            "_id": "661ab3da2b14565c7acccf5c",
            "avatarUrl": "/avatars/fa4fc03664803e02aede4d4c3d50b393.svg",
            "isPro": false,
            "fullname": "Mike Zheng Shou",
            "user": "AnalMom",
            "type": "user"
          },
          "name": "Mike Zheng Shou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:29:07.309Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-17T17:59:33.000Z",
      "submittedOnDailyAt": "2025-03-18T02:25:58.731Z",
      "title": "VideoMind : Vidéo de longue durée et logique du modèle LoRA chaîne-de-fil",
      "submittedOnDailyBy": {
        "_id": "64440be5af034cdfd69ca3a7",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64440be5af034cdfd69ca3a7/qmx24QiDFT29vleCxL9TX.jpeg",
        "isPro": true,
        "fullname": "Qinghong (Kevin) Lin",
        "user": "KevinQHLin",
        "type": "user"
      },
      "summary": "Videos, avec leur dimension temporelle unique, exigent une compréhension précise et fondée, où les réponses sont directement liées à des preuves visuelles et interprétables. Malgré les avancées significatives dans les capacités de raisonnement des Grands Modèles de Langue, le raisonnement multimodal, en particulier pour les vidéos, reste peu exploré. Dans ce travail, nous présentons VideoMind, un nouvel agent de langage et vidéo conçu pour la compréhension temporelle des vidéos. VideoMind intègre deux innovations clés : (i) nous identifions les capacités essentielles pour le raisonnement temporel des vidéos et développons un flux de travail agent-centré, incluant un planificateur pour coordonner différents rôles, un localiseur temporel, un vérificateur pour évaluer la précision des intervalles temporels et une répondre pour la réponse aux questions. (ii) Pour intégrer efficacement ces différents rôles, nous proposons une stratégie de chaîne de LoRA novatrice, permettant une transition de rôles sans poids grâce à des adaptateurs LoRA légers, évitant ainsi un surchargement de multiples modèles, en équilibreant ainsi l'efficacité et la flexibilité. Des expériences extensives sur 14 marqueurs publics ont montré que notre agent atteint un rendement de pointe dans diverses tâches de compréhension de vidéos, y compris 3 en réponse à des questions de vidéo fondées, 6 en localisation temporelle de vidéo et 5 en réponse à des questions de vidéo générales, soulignant son efficacité dans l'avancement de l'agent de vidéo et du raisonnement temporel à long terme.",
      "upvotes": 6,
      "discussionId": "67d8eeb37e184aa2954d1a39",
      "ai_keywords": [
        "VideoMind",
        "temporal-grounded video understanding",
        "role-based agentic workflow",
        "planner",
        "grounder",
        "temporale localization",
        "verifier",
        "temporal interval accuracy",
        "answerer",
        "question-answering",
        "Chain-of-LoRA",
        "LoRA adaptors",
        "grounded video question-answering",
        "video temporal grounding",
        "general video question-answering",
        "temporal reasoning"
      ]
    },
    "publishedAt": "2025-03-17T13:59:33.000Z",
    "title": "VideoMind: A Chain-of-LoRA Agent for Long Video Reasoning",
    "summary": "Videos, with their unique temporal dimension, demand precise grounded\nunderstanding, where answers are directly linked to visual, interpretable\nevidence. Despite significant breakthroughs in reasoning capabilities within\nLarge Language Models, multi-modal reasoning - especially for videos - remains\nunexplored. In this work, we introduce VideoMind, a novel video-language agent\ndesigned for temporal-grounded video understanding. VideoMind incorporates two\nkey innovations: (i) We identify essential capabilities for video temporal\nreasoning and develop a role-based agentic workflow, including a planner for\ncoordinating different roles, a grounder for temporal localization, a verifier\nto assess temporal interval accuracy, and an answerer for question-answering.\n(ii) To efficiently integrate these diverse roles, we propose a novel\nChain-of-LoRA strategy, enabling seamless role-switching via lightweight LoRA\nadaptors while avoiding the overhead of multiple models, thus balancing\nefficiency and flexibility. Extensive experiments on 14 public benchmarks\ndemonstrate that our agent achieves state-of-the-art performance on diverse\nvideo understanding tasks, including 3 on grounded video question-answering, 6\non video temporal grounding, and 5 on general video question-answering,\nunderscoring its effectiveness in advancing video agent and long-form temporal\nreasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13444.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64440be5af034cdfd69ca3a7",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64440be5af034cdfd69ca3a7/qmx24QiDFT29vleCxL9TX.jpeg",
      "fullname": "Qinghong (Kevin) Lin",
      "name": "KevinQHLin",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isMod": false,
      "followerCount": 21
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.13070",
      "authors": [
        {
          "_id": "67d8fbf641d31cc626e4d7b9",
          "user": {
            "_id": "65f7e6856bd4bac5b6a4ecc3",
            "avatarUrl": "/avatars/9c0c48310fb5e99c19700316a0ab531e.svg",
            "isPro": false,
            "fullname": "Yihong Luo",
            "user": "Luo-Yihong",
            "type": "user"
          },
          "name": "Yihong Luo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-18T08:07:30.236Z",
          "hidden": false
        },
        {
          "_id": "67d8fbf641d31cc626e4d7ba",
          "user": {
            "_id": "636a40faa6f948c4f0c62ae5",
            "avatarUrl": "/avatars/30c35b194ba84d6e274df30e91a8cc45.svg",
            "isPro": false,
            "fullname": "Tianyang Hu",
            "user": "whatlegequ",
            "type": "user"
          },
          "name": "Tianyang Hu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:33:01.489Z",
          "hidden": false
        },
        {
          "_id": "67d8fbf641d31cc626e4d7bb",
          "name": "Weijian Luo",
          "hidden": false
        },
        {
          "_id": "67d8fbf641d31cc626e4d7bc",
          "name": "Kenji Kawaguchi",
          "hidden": false
        },
        {
          "_id": "67d8fbf641d31cc626e4d7bd",
          "name": "Jing Tang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-17T11:21:43.000Z",
      "submittedOnDailyAt": "2025-03-18T03:24:24.806Z",
      "title": "La compensation est suffisante pour générer des images de haute qualité de photos rapidement.",
      "submittedOnDailyBy": {
        "_id": "65f7e6856bd4bac5b6a4ecc3",
        "avatarUrl": "/avatars/9c0c48310fb5e99c19700316a0ab531e.svg",
        "isPro": false,
        "fullname": "Yihong Luo",
        "user": "Luo-Yihong",
        "type": "user"
      },
      "summary": "La complexité des images générées et leur adaptation aux textes complexes et aux préférences humaines constituent un problème essentiel dans le contenu généré par l'intelligence artificielle (AIGC). Pour améliorer la possibilité de contrôle et la confiance dans les modèles transformant le texte en images, une excellente solution a été développée avec l'application d'un déterroir distribué avec des récompenses renforcées. Cela a entraîné une modification fondamentale du paradigme de la génération, où la récompense devient le moteur principal, car les conditions sont spécifiées et les signaux de récompense sont renforcés. D'un autre côté, il est considéré que le déterroir distribué est un ajustement excessif. Pour tester ces hypothèses, un nouvel approche de génération conditionnelle appelé R0 est proposée. R0 montre la dominance de la récompense dans des conditions complexes et propose une nouvelle perspective sur la génération d'images, traitant l'optimisation comme un problème dans l'espace de données sans confiancer dans le déterroir distribué. En utilisant R0, on peut entraîner des modèles de génération d'images à partir de textes complexes en un seul pas, en basant sur le design unique des paramètres du générateur et sur des théories d'ajustement appropriées. De cette manière, R0 défie le processus de post-traitement de la diversité et les perspectives traditionnelles de la génération conditionnelle, démontrant la dominance de la récompense dans des conditions complexes. Nos résultats de recherche contribuent au développement d'un paradigme de génération axé sur la personne et la récompense dans le domaine plus large de l'AIGC. Le code est disponible sur https://github.com/Luo-Yihong/R0.",
      "upvotes": 5,
      "discussionId": "67d8fbf841d31cc626e4d812",
      "githubRepo": "https://github.com/Luo-Yihong/R0",
      "ai_keywords": [
        "reward-enhanced diffusion distillation",
        "diffusion losses",
        "R0",
        "regularized reward maximization",
        "optimization problem in data space",
        "compositional rewards",
        "generator parameterization",
        "state-of-the-art few-step text-to-image generative models",
        "diffusion post-training",
        "human-centric generation",
        "reward-centric generation paradigms"
      ]
    },
    "publishedAt": "2025-03-17T07:21:43.000Z",
    "title": "Rewards Are Enough for Fast Photo-Realistic Text-to-image Generation",
    "summary": "Aligning generated images to complicated text prompts and human preferences\nis a central challenge in Artificial Intelligence-Generated Content (AIGC).\nWith reward-enhanced diffusion distillation emerging as a promising approach\nthat boosts controllability and fidelity of text-to-image models, we identify a\nfundamental paradigm shift: as conditions become more specific and reward\nsignals stronger, the rewards themselves become the dominant force in\ngeneration. In contrast, the diffusion losses serve as an overly expensive form\nof regularization. To thoroughly validate our hypothesis, we introduce R0, a\nnovel conditional generation approach via regularized reward maximization.\nInstead of relying on tricky diffusion distillation losses, R0 proposes a new\nperspective that treats image generations as an optimization problem in data\nspace which aims to search for valid images that have high compositional\nrewards. By innovative designs of the generator parameterization and proper\nregularization techniques, we train state-of-the-art few-step text-to-image\ngenerative models with R0 at scales. Our results challenge the conventional\nwisdom of diffusion post-training and conditional generation by demonstrating\nthat rewards play a dominant role in scenarios with complex conditions. We hope\nour findings can contribute to further research into human-centric and\nreward-centric generation paradigms across the broader field of AIGC. Code is\navailable at https://github.com/Luo-Yihong/R0.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13070.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65f7e6856bd4bac5b6a4ecc3",
      "avatarUrl": "/avatars/9c0c48310fb5e99c19700316a0ab531e.svg",
      "fullname": "Yihong Luo",
      "name": "Luo-Yihong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.11495",
      "authors": [
        {
          "_id": "67d8ca56e94f1237cb3ba3ca",
          "user": {
            "_id": "667ee096b0fad0fdee319ed4",
            "avatarUrl": "/avatars/d9df687e8522d47f7fcefe40fd9b575b.svg",
            "isPro": false,
            "fullname": "Zixu Cheng",
            "user": "Cade921",
            "type": "user"
          },
          "name": "Zixu Cheng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:31:58.589Z",
          "hidden": false
        },
        {
          "_id": "67d8ca56e94f1237cb3ba3cb",
          "user": {
            "_id": "65e1b6e9501590df0173cbd3",
            "avatarUrl": "/avatars/a73e2139700e23eff455734c99cef5ba.svg",
            "isPro": false,
            "fullname": "Jian Hu",
            "user": "lwpyh",
            "type": "user"
          },
          "name": "Jian Hu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-18T08:08:20.643Z",
          "hidden": false
        },
        {
          "_id": "67d8ca56e94f1237cb3ba3cc",
          "name": "Ziquan Liu",
          "hidden": false
        },
        {
          "_id": "67d8ca56e94f1237cb3ba3cd",
          "user": {
            "_id": "635f8ed47c05eb9f59963d3a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/635f8ed47c05eb9f59963d3a/uQf4p9N9pSaFy87Wg9v4k.jpeg",
            "isPro": false,
            "fullname": "ChenyangSi",
            "user": "ChenyangSi",
            "type": "user"
          },
          "name": "Chenyang Si",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:32:21.924Z",
          "hidden": false
        },
        {
          "_id": "67d8ca56e94f1237cb3ba3ce",
          "name": "Wei Li",
          "hidden": false
        },
        {
          "_id": "67d8ca56e94f1237cb3ba3cf",
          "name": "Shaogang Gong",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/65e1b6e9501590df0173cbd3/A2JrOm6FBAq5keCRRdVMK.png"
      ],
      "publishedAt": "2025-03-14T15:21:44.000Z",
      "submittedOnDailyAt": "2025-03-18T01:11:08.161Z",
      "title": "V-STaR : Benchmark de LLMs d'Images pour l'Espace Spectral-Espace-Temporel-Causalité",
      "submittedOnDailyBy": {
        "_id": "65e1b6e9501590df0173cbd3",
        "avatarUrl": "/avatars/a73e2139700e23eff455734c99cef5ba.svg",
        "isPro": false,
        "fullname": "Jian Hu",
        "user": "lwpyh",
        "type": "user"
      },
      "summary": "La humanité comprend la cause des films à travers la logique spatial-temporelle séquentielle. Tout d'abord, un cadre relatif (\"à quel moment\") est spécifié, puis l'analyse de la relation spatiale des objets clés (\"où se trouvent-ils\") est effectuée, et enfin, cette relation est utilisée pour faire des inférences (\"ce qui se passe ou ce qui est fait\"). Cependant, la nature des grands modèles de langage vidéo (Video-LLMs) dans les films est incertaine quant à leur capacité à trouver des causes par la logique spatial-temporelle séquentielle. Les évaluations actuelles des Video-LLMs se concentrent principalement sur l'existence d'objets, dépassant ainsi la logique relationnelle. Par conséquent, il est difficile d'évaluer si les modèles réellement comprennent les interactions entre les objets (actions/événements) dans les films ou si ils génèrent des réponses basées sur la \"mémoire\" de ces interactions. Cet article présente le benchmark logique spatial-temporelle des films (V-STaR) pour résoudre ces limitations. L'idée principale est de décomposer la compréhension des films en tâches logiques spatial-temporelles inverses (RSTR), évaluant simultanément l'existence d'objets, le moment des événements et leurs localisations, et comprendre la logique de raisonnement de chaîne (CoT) sous-jacente. Pour évaluer ce processus, un ensemble de données est construit qui extrait le traitement logique spatial-temporelle des Video-LLMs. Ces données sont générées automatiquement à travers un processus de rétroaction dans un travail de feedback core basé sur GPT-4, qui mime la cognition humaine et inclut des chaînes explicites de raisonnement. Dans 14 expériences avec des Video-LLMs, notre V-STaR a clairement démontré la nécessité et la robustesse d'une logique spatial-temporelle cohérente par rapport aux Video-LLMs actuels.",
      "upvotes": 5,
      "discussionId": "67d8ca59e94f1237cb3ba47c",
      "ai_keywords": [
        "Video Large Language Models (Video-LLMs)",
        "Reverse Spatio-Temporal Reasoning (RSTR)",
        "Chain-of-thought (CoT)",
        "GPT-4",
        "Video Spatio-Temporal Reasoning (V-STaR)",
        "CoT questions",
        "CoT logic",
        "human cognition"
      ]
    },
    "publishedAt": "2025-03-14T11:21:44.000Z",
    "title": "V-STaR: Benchmarking Video-LLMs on Video Spatio-Temporal Reasoning",
    "summary": "Human processes video reasoning in a sequential spatio-temporal reasoning\nlogic, we first identify the relevant frames (\"when\") and then analyse the\nspatial relationships (\"where\") between key objects, and finally leverage these\nrelationships to draw inferences (\"what\"). However, can Video Large Language\nModels (Video-LLMs) also \"reason through a sequential spatio-temporal logic\" in\nvideos? Existing Video-LLM benchmarks primarily focus on assessing object\npresence, neglecting relational reasoning. Consequently, it is difficult to\nmeasure whether a model truly comprehends object interactions (actions/events)\nin videos or merely relies on pre-trained \"memory\" of co-occurrences as biases\nin generating answers. In this work, we introduce a Video Spatio-Temporal\nReasoning (V-STaR) benchmark to address these shortcomings. The key idea is to\ndecompose video understanding into a Reverse Spatio-Temporal Reasoning (RSTR)\ntask that simultaneously evaluates what objects are present, when events occur,\nand where they are located while capturing the underlying Chain-of-thought\n(CoT) logic. To support this evaluation, we construct a dataset to elicit the\nspatial-temporal reasoning process of Video-LLMs. It contains coarse-to-fine\nCoT questions generated by a semi-automated GPT-4-powered pipeline, embedding\nexplicit reasoning chains to mimic human cognition. Experiments from 14\nVideo-LLMs on our V-STaR reveal significant gaps between current Video-LLMs and\nthe needs for robust and consistent spatio-temporal reasoning.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/65e1b6e9501590df0173cbd3/A2JrOm6FBAq5keCRRdVMK.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.11495.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65e1b6e9501590df0173cbd3",
      "avatarUrl": "/avatars/a73e2139700e23eff455734c99cef5ba.svg",
      "fullname": "Jian Hu",
      "name": "lwpyh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.11412",
      "authors": [
        {
          "_id": "67d8f8b77f61dda9ea6512b7",
          "name": "Shiyuan Yang",
          "hidden": false
        },
        {
          "_id": "67d8f8b77f61dda9ea6512b8",
          "user": {
            "_id": "678f49878af7a399877b87c0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/3IHnM4AKbIW_wmL15wdZf.png",
            "isPro": false,
            "fullname": "GuZheng",
            "user": "GuZheng",
            "type": "user"
          },
          "name": "Zheng Gu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:33:49.096Z",
          "hidden": false
        },
        {
          "_id": "67d8f8b77f61dda9ea6512b9",
          "user": {
            "_id": "64560a2aaaaf85a98fa9a4b9",
            "avatarUrl": "/avatars/e81e21f353baf48f0d91bf29ad200eea.svg",
            "isPro": false,
            "fullname": "Liang Hou",
            "user": "lianghou",
            "type": "user"
          },
          "name": "Liang Hou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:33:57.501Z",
          "hidden": false
        },
        {
          "_id": "67d8f8b77f61dda9ea6512ba",
          "name": "Xin Tao",
          "hidden": false
        },
        {
          "_id": "67d8f8b77f61dda9ea6512bb",
          "user": {
            "_id": "662f93942510ef5735d7ad00",
            "avatarUrl": "/avatars/dc9486db75869ce902d0a638eea126bd.svg",
            "isPro": false,
            "fullname": "magicwpf",
            "user": "magicwpf",
            "type": "user"
          },
          "name": "Pengfei Wan",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-03-18T04:42:06.437Z",
          "hidden": false
        },
        {
          "_id": "67d8f8b77f61dda9ea6512bc",
          "user": {
            "_id": "67be7cd21616162fc336cb44",
            "avatarUrl": "/avatars/e58cc3c2d1484419222a5ccfc11f5c48.svg",
            "isPro": false,
            "fullname": "Xiaodong Chen",
            "user": "XiaodongChen",
            "type": "user"
          },
          "name": "Xiaodong Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:34:15.858Z",
          "hidden": false
        },
        {
          "_id": "67d8f8b77f61dda9ea6512bd",
          "user": {
            "_id": "65e77726767bfc7d109c45bf",
            "avatarUrl": "/avatars/24e68c86e06055ea1209598ba49ce8b9.svg",
            "isPro": false,
            "fullname": "Jing Liao",
            "user": "CeciliaJL",
            "type": "user"
          },
          "name": "Jing Liao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:34:23.653Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63316d499e3604f3f17f5d89/jV8ETjZh0NTDoFBhSNv8I.mp4"
      ],
      "publishedAt": "2025-03-14T13:54:10.000Z",
      "submittedOnDailyAt": "2025-03-18T03:20:01.050Z",
      "title": "MTV-Inpaint : Inpaintage de Vidéos Longues Multi-Tâche",
      "submittedOnDailyBy": {
        "_id": "63316d499e3604f3f17f5d89",
        "avatarUrl": "/avatars/c7e0c8bdd7e598276cfd133d5222c5e8.svg",
        "isPro": false,
        "fullname": "catfood",
        "user": "ysy31415926",
        "type": "user"
      },
      "summary": "Video inpainting incluit la modification de zones spécifiques dans un vidéo et garantit une cohérence spatiale et temporelle. Les méthodes actuelles se concentrent principalement sur la complétion spatiale (compléter des zones vides), mais ont une capacité limitée pour insérer de nouveaux objets de manière contrôlée dans l'espace. Malgré cela, le développement récent de modèles de diffusion de texte à vidéo (T2V) a connecté l'inpainting vidéo guidée par texte. Cependant, appliquer directement un modèle T2V pour l'inpainting est complexe en raison de l'intégration de deux tâches (compléter et insérer), de la possibilité de contrôle de l'entrée, des limitations pour traiter des vidéos longues et de la flexibilité d'application. Pour résoudre ces problèmes, nous proposons MTV-Inpaint, un cadre intégré qui aborde à la fois la complétion spatiale et l'insertion de nouveaux objets dans un vidéo. Pour intégrer ces tâches, nous avons conçu une structure d'attention spatiale dans la U-Net de diffusion T2V de manière duale, ce qui permet de combiner la différence entre complétion spatiale et insertion d'objets. MTV-Inpaint inclut également un mode d'inpainting image à vidéo (I2V) guidée par texte, offrant plusieurs options de contrôle. De plus, nous présentons une chaîne de travail en deux étapes qui combine l'inpainting de cadres clés et la propagation de cadres indirects, ce qui permet de traiter des vidéos longues de centaines de cadres efficacement. Les expériences approfondies montrent que MTV-Inpaint présente les meilleurs résultats pour les deux tâches, ainsi qu'une large gamme d'applications extensibles, comme l'inpainting multi-mode, l'édition et l'élimination d'objets, la création de flammes d'objets dans les images et le traitement de vidéos longues. La page du projet est disponible sur https://mtv-inpaint.github.io/.",
      "upvotes": 5,
      "discussionId": "67d8f8bf7f61dda9ea6514a7",
      "projectPage": "https://mtv-inpaint.github.io/",
      "ai_keywords": [
        "text-to-video (T2V) diffusion models",
        "text-guided video inpainting",
        "dual-branch spatial attention mechanism",
        "T2V diffusion U-Net",
        "multimodal control",
        "image-to-video (I2V) inpainting mode",
        "keyframe inpainting",
        "in-between frame propagation",
        "multi-modal inpainting",
        "object editing",
        "object removal",
        "image object brush"
      ]
    },
    "publishedAt": "2025-03-14T09:54:10.000Z",
    "title": "MTV-Inpaint: Multi-Task Long Video Inpainting",
    "summary": "Video inpainting involves modifying local regions within a video, ensuring\nspatial and temporal consistency. Most existing methods focus primarily on\nscene completion (i.e., filling missing regions) and lack the capability to\ninsert new objects into a scene in a controllable manner. Fortunately, recent\nadvancements in text-to-video (T2V) diffusion models pave the way for\ntext-guided video inpainting. However, directly adapting T2V models for\ninpainting remains limited in unifying completion and insertion tasks, lacks\ninput controllability, and struggles with long videos, thereby restricting\ntheir applicability and flexibility. To address these challenges, we propose\nMTV-Inpaint, a unified multi-task video inpainting framework capable of\nhandling both traditional scene completion and novel object insertion tasks. To\nunify these distinct tasks, we design a dual-branch spatial attention mechanism\nin the T2V diffusion U-Net, enabling seamless integration of scene completion\nand object insertion within a single framework. In addition to textual\nguidance, MTV-Inpaint supports multimodal control by integrating various image\ninpainting models through our proposed image-to-video (I2V) inpainting mode.\nAdditionally, we propose a two-stage pipeline that combines keyframe inpainting\nwith in-between frame propagation, enabling MTV-Inpaint to effectively handle\nlong videos with hundreds of frames. Extensive experiments demonstrate that\nMTV-Inpaint achieves state-of-the-art performance in both scene completion and\nobject insertion tasks. Furthermore, it demonstrates versatility in derived\napplications such as multi-modal inpainting, object editing, removal, image\nobject brush, and the ability to handle long videos. Project page:\nhttps://mtv-inpaint.github.io/.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63316d499e3604f3f17f5d89/jV8ETjZh0NTDoFBhSNv8I.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.11412.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63316d499e3604f3f17f5d89",
      "avatarUrl": "/avatars/c7e0c8bdd7e598276cfd133d5222c5e8.svg",
      "fullname": "catfood",
      "name": "ysy31415926",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.10704",
      "authors": [
        {
          "_id": "67d7eba831dd5b46c3e6fdcb",
          "user": {
            "_id": "633c2310c0fb6fd232f0accf",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/633c2310c0fb6fd232f0accf/AnvEvxMtd-BpSZbaoA4ha.jpeg",
            "isPro": false,
            "fullname": "Wang Jing",
            "user": "k-nick",
            "type": "user"
          },
          "name": "Jing Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-18T08:09:01.169Z",
          "hidden": false
        },
        {
          "_id": "67d7eba831dd5b46c3e6fdcc",
          "user": {
            "_id": "64b8c1a995bd42c7707f7918",
            "avatarUrl": "/avatars/08c2929f8f150ecd6f8e5a06c4cb9034.svg",
            "isPro": false,
            "fullname": "Fengzhuo Zhang",
            "user": "Fengzhuo",
            "type": "user"
          },
          "name": "Fengzhuo Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:34:37.712Z",
          "hidden": false
        },
        {
          "_id": "67d7eba831dd5b46c3e6fdcd",
          "user": {
            "_id": "67aa01782183876b1ec5760f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/hZd1iBn_2yjHVoavcXPQo.png",
            "isPro": false,
            "fullname": "xiaolili",
            "user": "xiaolili",
            "type": "user"
          },
          "name": "Xiaoli Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:34:47.006Z",
          "hidden": false
        },
        {
          "_id": "67d7eba831dd5b46c3e6fdce",
          "name": "Vincent Y. F. Tan",
          "hidden": false
        },
        {
          "_id": "67d7eba831dd5b46c3e6fdcf",
          "user": {
            "_id": "661a4a556fb488fa078c60aa",
            "avatarUrl": "/avatars/c77401fa9c6d2db896b4a337bb3f8add.svg",
            "isPro": false,
            "fullname": "Tianyu Pang",
            "user": "TIanyupang",
            "type": "user"
          },
          "name": "Tianyu Pang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:35:17.732Z",
          "hidden": false
        },
        {
          "_id": "67d7eba831dd5b46c3e6fdd0",
          "user": {
            "_id": "632407c892e07e3ca20aca28",
            "avatarUrl": "/avatars/23b51b37b12b51a0947f687d1de4d3b5.svg",
            "isPro": false,
            "fullname": "Chao Du",
            "user": "duchao",
            "type": "user"
          },
          "name": "Chao Du",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:35:29.239Z",
          "hidden": false
        },
        {
          "_id": "67d7eba831dd5b46c3e6fdd1",
          "user": {
            "_id": "664aab898fa42b4fe70ebf52",
            "avatarUrl": "/avatars/a38455fd17bbc74ce3111f2c3da9aa59.svg",
            "isPro": false,
            "fullname": "Aixin Sun",
            "user": "aixinsun",
            "type": "user"
          },
          "name": "Aixin Sun",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:35:35.567Z",
          "hidden": false
        },
        {
          "_id": "67d7eba831dd5b46c3e6fdd2",
          "user": {
            "_id": "6397873ec0b27f432db8693f",
            "avatarUrl": "/avatars/1db65fe55002ad5c137c4a59bbcd239d.svg",
            "isPro": false,
            "fullname": "Zhuoran Yang",
            "user": "zhuoran",
            "type": "user"
          },
          "name": "Zhuoran Yang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-17T09:30:18.764Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-12T15:32:44.000Z",
      "submittedOnDailyAt": "2025-03-18T03:10:46.254Z",
      "title": "Modèle automatique de rétroaction pour l'analyse d'erreurs dans les émetteurs vidéo : un seul cadre intégré",
      "submittedOnDailyBy": {
        "_id": "633c2310c0fb6fd232f0accf",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/633c2310c0fb6fd232f0accf/AnvEvxMtd-BpSZbaoA4ha.jpeg",
        "isPro": false,
        "fullname": "Wang Jing",
        "user": "k-nick",
        "type": "user"
      },
      "summary": "Varios modèles de diffusion vidéo rétroactionnelle automatique (ARVDM) ont réussi à obtenir un succès notable dans la génération de vidéos de longue durée réalistes. Cependant, l'analyse théorique de ces modèles est insuffisante. Dans cet article, une base théorique de ces modèles est développée et utilisée pour améliorer le rendement des modèles existants. Tout d'abord, un cadre intégré de ARVDM, Meta-ARVDM, qui intègre tous les méthodes existantes pour ARVDM, est développé. Grâce à Meta-ARVDM, la variation de Kullback-Leibler (KL) entre le vidéo généré par Meta-ARVDM et le vidéo réel est analysée. L'analyse révèle deux phénomènes importants uniques de ARVDM : l'accumulation d'erreurs et le bloquage de la mémoire. Des résultats théoriques d'impossibilité sont calculés, montrant que le phénomène du bloquage de la mémoire ne peut pas être évité. Pour atténuer le bloquage de la mémoire, différentes structures de réseaux qui utilisent explicitement les frames passées sont conçues. De plus, la mitigation du bloquage de la mémoire et l'efficacité de l'inférence sont significativement améliorées en compressant les frames. Les résultats des expériences réalisées sur DMLab et Minecraft prouvent l'effet des méthodes proposées dans cet article. De plus, des ondes d'erreurs accumulées et du bloquage de la mémoire sont présentées.",
      "upvotes": 4,
      "discussionId": "67d7ebaa31dd5b46c3e6fe5a",
      "projectPage": "https://sail-sg.github.io/AR-Video-Diffusion",
      "ai_keywords": [
        "Auto-Regressive Video Diffusion Models",
        "Meta-ARVDM",
        "KL-divergence",
        "error accumulation",
        "memory bottleneck",
        "information-theoretic impossibility result",
        "network structures",
        "frame compression",
        "DMLab",
        "Minecraft",
        "Pareto-frontier"
      ]
    },
    "publishedAt": "2025-03-12T11:32:44.000Z",
    "title": "Error Analyses of Auto-Regressive Video Diffusion Models: A Unified\n  Framework",
    "summary": "A variety of Auto-Regressive Video Diffusion Models (ARVDM) have achieved\nremarkable successes in generating realistic long-form videos. However,\ntheoretical analyses of these models remain scant. In this work, we develop\ntheoretical underpinnings for these models and use our insights to improve the\nperformance of existing models. We first develop Meta-ARVDM, a unified\nframework of ARVDMs that subsumes most existing methods. Using Meta-ARVDM, we\nanalyze the KL-divergence between the videos generated by Meta-ARVDM and the\ntrue videos. Our analysis uncovers two important phenomena inherent to ARVDM --\nerror accumulation and memory bottleneck. By deriving an information-theoretic\nimpossibility result, we show that the memory bottleneck phenomenon cannot be\navoided. To mitigate the memory bottleneck, we design various network\nstructures to explicitly use more past frames. We also achieve a significantly\nimproved trade-off between the mitigation of the memory bottleneck and the\ninference efficiency by compressing the frames. Experimental results on DMLab\nand Minecraft validate the efficacy of our methods. Our experiments also\ndemonstrate a Pareto-frontier between the error accumulation and memory\nbottleneck across different methods.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10704.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "633c2310c0fb6fd232f0accf",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/633c2310c0fb6fd232f0accf/AnvEvxMtd-BpSZbaoA4ha.jpeg",
      "fullname": "Wang Jing",
      "name": "k-nick",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.10719",
      "authors": [
        {
          "_id": "67d91dadb533888991ade4e1",
          "user": {
            "_id": "672c6f3d4c1e2de12c6f174e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/mv-8GX6OBBpJLzVvyPYbz.png",
            "isPro": false,
            "fullname": "Yehang Zhang",
            "user": "Buzz-lightyear",
            "type": "user"
          },
          "name": "Yehang Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-18T08:07:19.937Z",
          "hidden": false
        },
        {
          "_id": "67d91dadb533888991ade4e2",
          "user": {
            "_id": "64b4ab62eec33e27dcd733b5",
            "avatarUrl": "/avatars/0a9bf220c9a5efe7279f9b287b087d36.svg",
            "isPro": false,
            "fullname": "Xinli XU",
            "user": "Xxlbigbrother",
            "type": "user"
          },
          "name": "Xinli Xu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:35:50.412Z",
          "hidden": false
        },
        {
          "_id": "67d91dadb533888991ade4e3",
          "name": "Xiaojie Xu",
          "hidden": false
        },
        {
          "_id": "67d91dadb533888991ade4e4",
          "name": "Li Liu",
          "hidden": false
        },
        {
          "_id": "67d91dadb533888991ade4e5",
          "user": {
            "_id": "655cba1d87b67834000590e8",
            "avatarUrl": "/avatars/3bd43b7c9351f65b8f38f4c8237a0146.svg",
            "isPro": false,
            "fullname": "Yingcong Chen",
            "user": "yingcongchen",
            "type": "user"
          },
          "name": "Yingcong Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:36:19.820Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-13T07:58:23.000Z",
      "submittedOnDailyAt": "2025-03-18T06:41:21.358Z",
      "title": "\"Dans la synthèse d'audios de longue durée, l'utilisation efficace des techniques de traitement audio est essentielle pour améliorer la qualité du son, créer de nouveaux sons et adapter l'audio à différents contextes. Ces techniques peuvent inclure le traitement numérique, l'analyse des signaux et la génération de sons. L'objectif est de créer des audios plus clairs, plus doux et qui résonnent de manière plus naturelle, améliorant l'expérience de l'auditeur. De plus, la synthèse d'audio peut être utilisée pour créer des sons spéciaux, remplacer ou améliorer des parties du son original, et adapter l'audio à différents environnements ou appareils. Cela nécessite un solide connaissance de la théorie de l'audio, une expérience pratique et l'utilisation de outils de traitement audio avancés.\"",
      "submittedOnDailyBy": {
        "_id": "672c6f3d4c1e2de12c6f174e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/mv-8GX6OBBpJLzVvyPYbz.png",
        "isPro": false,
        "fullname": "Yehang Zhang",
        "user": "Buzz-lightyear",
        "type": "user"
      },
      "summary": "La synthèse de voix à partir de vidéo est une technologie qui génère des voix synchronisées avec le contenu visuel, ce qui signifie qu'elle peut significativement améliorer la participation et la narration dans les films et les médias interactifs. Cependant, la traduction de voix à partir de vidéo pour des contenus longs (par exemple, des films) reste un problème qui n'a pas encore été résolu, notamment la dynamique du sens, l'asynchronie temporelle et la manque de données spécifiques. Les méthodes existantes fonctionnent bien pour des vidéos courtes, mais échouent lorsqu'il s'agit de contenus longs, en raison de la manque de division de la synthèse et de la cohérence spectrale. Nous proposons un nouveau cadre de travail multi-agent appelé \"LVAS-Agent\", qui modélise le flux de travail d'une traduction professionnelle. Notre approche se décompose en quatre étapes : division du scénario, génération de scénarios, conception de sons et synthèse de voix. Les innovations principales comprennent l'utilisation d'un mécanisme de révision et de modification pour la précision et l'ajustement temporel et significatif des scénarios/scripts. Pour évaluer le système de manière systématique, nous présentons le premier benchmark \"LVAS-Bench\", qui inclut plusieurs vidéos longues éditées par 207 professionnels. Les expériences montrent un meilleur ajustement sonore et visuel que les méthodes de référence. Page du projet : https://lvas-agent.github.io",
      "upvotes": 2,
      "discussionId": "67d91dafb533888991ade557",
      "projectPage": "https://lvas-agent.github.io/",
      "ai_keywords": [
        "scene segmentation",
        "script generation",
        "sound design",
        "audio synthesis",
        "discussion-correction mechanism",
        "generation-retrieval loop",
        "temporal-semantic alignment",
        "LVAS-Agent",
        "LVAS-Bench",
        "audio-visual alignment"
      ]
    },
    "publishedAt": "2025-03-13T03:58:23.000Z",
    "title": "Long-Video Audio Synthesis with Multi-Agent Collaboration",
    "summary": "Video-to-audio synthesis, which generates synchronized audio for visual\ncontent, critically enhances viewer immersion and narrative coherence in film\nand interactive media. However, video-to-audio dubbing for long-form content\nremains an unsolved challenge due to dynamic semantic shifts, temporal\nmisalignment, and the absence of dedicated datasets. While existing methods\nexcel in short videos, they falter in long scenarios (e.g., movies) due to\nfragmented synthesis and inadequate cross-scene consistency. We propose\nLVAS-Agent, a novel multi-agent framework that emulates professional dubbing\nworkflows through collaborative role specialization. Our approach decomposes\nlong-video synthesis into four steps including scene segmentation, script\ngeneration, sound design and audio synthesis. Central innovations include a\ndiscussion-correction mechanism for scene/script refinement and a\ngeneration-retrieval loop for temporal-semantic alignment. To enable systematic\nevaluation, we introduce LVAS-Bench, the first benchmark with 207\nprofessionally curated long videos spanning diverse scenarios. Experiments\ndemonstrate superior audio-visual alignment over baseline methods. Project\npage: https://lvas-agent.github.io",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10719.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "672c6f3d4c1e2de12c6f174e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/mv-8GX6OBBpJLzVvyPYbz.png",
      "fullname": "Yehang Zhang",
      "name": "Buzz-lightyear",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.13369",
      "authors": [
        {
          "_id": "67d8e38e5fde3b1be16874e4",
          "user": {
            "_id": "64b214c4f4361a032002cdcf",
            "avatarUrl": "/avatars/3188a4edec4d6c945a0ed9f36050a03c.svg",
            "isPro": false,
            "fullname": "Andrew Wan Ju Kang",
            "user": "soarhigh",
            "type": "user"
          },
          "name": "Wan Ju Kang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-18T08:08:01.967Z",
          "hidden": false
        },
        {
          "_id": "67d8e38e5fde3b1be16874e5",
          "user": {
            "_id": "628e3b87a2cb9819d4391ba6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1653488512816-noauth.jpeg",
            "isPro": false,
            "fullname": "Eunki Kim",
            "user": "eunkey",
            "type": "user"
          },
          "name": "Eunki Kim",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:36:36.532Z",
          "hidden": false
        },
        {
          "_id": "67d8e38e5fde3b1be16874e6",
          "user": {
            "_id": "65cccd8c80d3c4b865d3b262",
            "avatarUrl": "/avatars/e6f6d8f06dd54e1e7b6d686835a9c075.svg",
            "isPro": false,
            "fullname": "Na Min An",
            "user": "namin0202",
            "type": "user"
          },
          "name": "Na Min An",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:36:43.821Z",
          "hidden": false
        },
        {
          "_id": "67d8e38e5fde3b1be16874e7",
          "user": {
            "_id": "62f2638d04674e28535d40f8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1672818467177-62f2638d04674e28535d40f8.png",
            "isPro": false,
            "fullname": "Sangryul Kim",
            "user": "sangryul",
            "type": "user"
          },
          "name": "Sangryul Kim",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:36:49.867Z",
          "hidden": false
        },
        {
          "_id": "67d8e38e5fde3b1be16874e8",
          "user": {
            "_id": "6639807fc9648d06126d7ec4",
            "avatarUrl": "/avatars/d996b5102ae9092da6db5f44f5142b54.svg",
            "isPro": false,
            "fullname": "haemin choi",
            "user": "hammnii",
            "type": "user"
          },
          "name": "Haemin Choi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:36:55.951Z",
          "hidden": false
        },
        {
          "_id": "67d8e38e5fde3b1be16874e9",
          "name": "Ki Hoon Kwak",
          "hidden": false
        },
        {
          "_id": "67d8e38e5fde3b1be16874ea",
          "name": "James Thorne",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-17T16:52:46.000Z",
      "submittedOnDailyAt": "2025-03-18T06:53:48.028Z",
      "title": "Signal de : Utilisation de rétroaction visuelle pour le développement d'un ensemble de données d'explications de diagrammes correspondant à BLV",
      "submittedOnDailyBy": {
        "_id": "64b214c4f4361a032002cdcf",
        "avatarUrl": "/avatars/3188a4edec4d6c945a0ed9f36050a03c.svg",
        "isPro": false,
        "fullname": "Andrew Wan Ju Kang",
        "user": "soarhigh",
        "type": "user"
      },
      "summary": "En général, les demandes et les capacités visuelles des Groupes d'Analyse et des Utilisateurs Finaux sont différentes. La génération de explications détaillées d'images pour des utilisateurs tels que les aveugles et ceux avec une vision réduite (BLV) constitue une zone complexe. Les Groupes d'Analyse avec une vision sont capables de clairement expliquer le contenu visuel, mais selon des recherches précédentes, créer directement ces explications est coûteux, biaisée et peu conforme aux normes des BLV. Dans cette étude, nous n'exigeons pas que les personnes avec une vision évaluent les explications des images ni qu'elles les créent. Le Modèle de Langage et Vision (VLM) est guidé par des hypothèses potentielles et réalise plusieurs étapes d'inférence pour évaluer les explications des images générées. L'évaluation par des personnes avec une vision peut être efficace et utile pour des professionnels de l'éducation spécialisés dans l'enseignement d'étudiants avec une incapacité visuelle, comme les BLV. On publie Sightation, un ensemble de données de 5k explications d'images et 137k échantillons, utilisé pour l'entraînement dans des tâches de reconnaissance, de recherche, de réponse à des questions et de logique, et on montre également la possibilité de fine-tuning.",
      "upvotes": 1,
      "discussionId": "67d8e3905fde3b1be168759f",
      "projectPage": "https://huggingface.co/Sightation",
      "ai_keywords": [
        "vision-language models (VLM)",
        "latent supervision",
        "multi-pass inference",
        "diagram description datasets",
        "fine-tuning"
      ]
    },
    "publishedAt": "2025-03-17T12:52:46.000Z",
    "title": "Sightation Counts: Leveraging Sighted User Feedback in Building a\n  BLV-aligned Dataset of Diagram Descriptions",
    "summary": "Often, the needs and visual abilities differ between the annotator group and\nthe end user group. Generating detailed diagram descriptions for blind and\nlow-vision (BLV) users is one such challenging domain. Sighted annotators could\ndescribe visuals with ease, but existing studies have shown that direct\ngenerations by them are costly, bias-prone, and somewhat lacking by BLV\nstandards. In this study, we ask sighted individuals to assess -- rather than\nproduce -- diagram descriptions generated by vision-language models (VLM) that\nhave been guided with latent supervision via a multi-pass inference. The\nsighted assessments prove effective and useful to professional educators who\nare themselves BLV and teach visually impaired learners. We release Sightation,\na collection of diagram description datasets spanning 5k diagrams and 137k\nsamples for completion, preference, retrieval, question answering, and\nreasoning training purposes and demonstrate their fine-tuning potential in\nvarious downstream tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13369.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b214c4f4361a032002cdcf",
      "avatarUrl": "/avatars/3188a4edec4d6c945a0ed9f36050a03c.svg",
      "fullname": "Andrew Wan Ju Kang",
      "name": "soarhigh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.12530",
      "authors": [
        {
          "_id": "67d8f0fa53f713733d6c6b1c",
          "user": {
            "_id": "6737d99b728a96aa64a2b00a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/p3vMX0xkvL_4IpWwyURLM.png",
            "isPro": false,
            "fullname": "Hunter Sawyer",
            "user": "HTSawyer",
            "type": "user"
          },
          "name": "Hunter Sawyer",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:37:16.399Z",
          "hidden": false
        },
        {
          "_id": "67d8f0fa53f713733d6c6b1d",
          "user": {
            "_id": "63c19eb3a0ffa3857eae2efa",
            "avatarUrl": "/avatars/35b06ca092f615a6d11ee99683d0376a.svg",
            "isPro": false,
            "fullname": "Jesse Roberts",
            "user": "JesseTNRoberts",
            "type": "user"
          },
          "name": "Jesse Roberts",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-18T04:05:16.739Z",
          "hidden": false
        },
        {
          "_id": "67d8f0fa53f713733d6c6b1e",
          "user": {
            "_id": "675ec603e4d6d0e820ad9d3f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/ruPDRCZlcbDhZ6zp4xi_P.png",
            "isPro": false,
            "fullname": "Olzhas",
            "user": "KyleMoore",
            "type": "user"
          },
          "name": "Kyle Moore",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:37:25.148Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-16T14:50:54.000Z",
      "submittedOnDailyAt": "2025-03-18T02:37:23.781Z",
      "title": "Vision et Langage Modèles (Modèles Vision Langage)",
      "submittedOnDailyBy": {
        "_id": "63c19eb3a0ffa3857eae2efa",
        "avatarUrl": "/avatars/35b06ca092f615a6d11ee99683d0376a.svg",
        "isPro": false,
        "fullname": "Jesse Roberts",
        "user": "JesseTNRoberts",
        "type": "user"
      },
      "summary": "Le domaine de la psychologie a reconnu pendant longtemps la classification basique des stimuli visuels en relation avec l'étiquetage présenté par Rosch en 1976. Cette classification basique inclut des tâches de langage visuel comme l'apprentissage, et augmente la densité d'information et la compréhension humaine. Dans cet article, nous explorons la classification basique dans deux modèles de langage de gestion d'entreprise (VLMs) récemment publiés. Nous avons constaté que Llama 3.2 Vision Instruct (11B) et Molmo 7B-D préfèrent une classification basique qui correspond à le comportement humain, montrant des effets biologiques et non biologiques, ainsi que des variations dans la classification basique des experts existants. Ces résultats montrent que les VLMs apprennent des comportements cognitifs humains à partir de données entraînées.",
      "upvotes": 1,
      "discussionId": "67d8f0fc53f713733d6c6b89",
      "ai_keywords": [
        "vision-language models (VLMs)",
        "basic level categorization",
        "biological versus non-biological basic level effects",
        "expert basic level shift",
        "cognitive categorization behaviors"
      ]
    },
    "publishedAt": "2025-03-16T10:50:54.000Z",
    "title": "Basic Category Usage in Vision Language Models",
    "summary": "The field of psychology has long recognized a basic level of categorization\nthat humans use when labeling visual stimuli, a term coined by Rosch in 1976.\nThis level of categorization has been found to be used most frequently, to have\nhigher information density, and to aid in visual language tasks with priming in\nhumans. Here, we investigate basic level categorization in two recently\nreleased, open-source vision-language models (VLMs). This paper demonstrates\nthat Llama 3.2 Vision Instruct (11B) and Molmo 7B-D both prefer basic level\ncategorization consistent with human behavior. Moreover, the models'\npreferences are consistent with nuanced human behaviors like the biological\nversus non-biological basic level effects and the well established expert basic\nlevel shift, further suggesting that VLMs acquire cognitive categorization\nbehaviors from the human data on which they are trained.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12530.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63c19eb3a0ffa3857eae2efa",
      "avatarUrl": "/avatars/35b06ca092f615a6d11ee99683d0376a.svg",
      "fullname": "Jesse Roberts",
      "name": "JesseTNRoberts",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.12528",
      "authors": [
        {
          "_id": "67d8f044f8b0e148f60cef0d",
          "name": "Kyle Moore",
          "hidden": false
        },
        {
          "_id": "67d8f044f8b0e148f60cef0e",
          "user": {
            "_id": "63c19eb3a0ffa3857eae2efa",
            "avatarUrl": "/avatars/35b06ca092f615a6d11ee99683d0376a.svg",
            "isPro": false,
            "fullname": "Jesse Roberts",
            "user": "JesseTNRoberts",
            "type": "user"
          },
          "name": "Jesse Roberts",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-18T04:02:14.260Z",
          "hidden": false
        },
        {
          "_id": "67d8f044f8b0e148f60cef0f",
          "name": "Daryl Watson",
          "hidden": false
        },
        {
          "_id": "67d8f044f8b0e148f60cef10",
          "name": "Pamela Wisniewski",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-16T14:45:43.000Z",
      "submittedOnDailyAt": "2025-03-18T02:34:14.582Z",
      "title": "Investigation de l'Incertitude dans les Modèles de Langue de Réponse Humaine",
      "submittedOnDailyBy": {
        "_id": "63c19eb3a0ffa3857eae2efa",
        "avatarUrl": "/avatars/35b06ca092f615a6d11ee99683d0376a.svg",
        "isPro": false,
        "fullname": "Jesse Roberts",
        "user": "JesseTNRoberts",
        "type": "user"
      },
      "summary": "Les derniers études se concentrent sur la quantification de l'incertitude des modèles de langage à grande échelle et sur l'ajustement du contrôle du modèle et la relation de confiance avec l'utilisateur. Les études précédentes ont été axées sur des méthodes de mesure théoriquement fondées pour l'incertitude et sur des méthodes de mesure qui reflètent le comportement moyen du modèle. Dans cette étude, nous avons investigué divers méthodes de mesure de l'incertitude et nous avons cherché à déterminer des méthodes de mesure liées à l'incertitude au niveau de groupe d'utilisateurs. Nous avons constaté que les mesures bayésiennes et les variantes de la mesure d'entropie (entropie top-k) correspondent au comportement de l'utilisateur selon le taille du modèle. De plus, certains méthodes fortes ont montré une diminution de la similitude avec l'utilisateur selon le taille du modèle, mais nous avons confirmé que en combinant divers méthodes de mesure de l'incertitude par des régressions linéaires multiples, il est possible de réduire la dépendance du taille du modèle et d'augmenter la réponse de l'utilisateur.",
      "upvotes": 1,
      "discussionId": "67d8f046f8b0e148f60cef95",
      "ai_keywords": [
        "Bayesian measures",
        "entropy measures",
        "top-k entropy",
        "human-similarity",
        "human-alignment",
        "multiple linear regression"
      ]
    },
    "publishedAt": "2025-03-16T10:45:43.000Z",
    "title": "Investigating Human-Aligned Large Language Model Uncertainty",
    "summary": "Recent work has sought to quantify large language model uncertainty to\nfacilitate model control and modulate user trust. Previous works focus on\nmeasures of uncertainty that are theoretically grounded or reflect the average\novert behavior of the model. In this work, we investigate a variety of\nuncertainty measures, in order to identify measures that correlate with human\ngroup-level uncertainty. We find that Bayesian measures and a variation on\nentropy measures, top-k entropy, tend to agree with human behavior as a\nfunction of model size. We find that some strong measures decrease in\nhuman-similarity with model size, but, by multiple linear regression, we find\nthat combining multiple uncertainty measures provide comparable human-alignment\nwith reduced size-dependency.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12528.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63c19eb3a0ffa3857eae2efa",
      "avatarUrl": "/avatars/35b06ca092f615a6d11ee99683d0376a.svg",
      "fullname": "Jesse Roberts",
      "name": "JesseTNRoberts",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]