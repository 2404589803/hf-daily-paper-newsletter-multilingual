[
  {
    "paper": {
      "id": "2507.01949",
      "authors": [
        {
          "_id": "6865e6218c83dab5f72d1e47",
          "name": "Kwai Keye Team",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e48",
          "name": "Biao Yang",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e49",
          "name": "Bin Wen",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e4a",
          "name": "Changyi Liu",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e4b",
          "name": "Chenglong Chu",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e4c",
          "name": "Chengru Song",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e4d",
          "name": "Chongling Rao",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e4e",
          "name": "Chuan Yi",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e4f",
          "name": "Da Li",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e50",
          "name": "Dunju Zang",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e51",
          "name": "Fan Yang",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e52",
          "name": "Guorui Zhou",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e53",
          "name": "Hao Peng",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e54",
          "name": "Haojie Ding",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e55",
          "name": "Jiaming Huang",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e56",
          "user": {
            "_id": "65802e81b701ff85a37caba8",
            "avatarUrl": "/avatars/b2e9726893caa7e62aad83b1d02e5b41.svg",
            "isPro": false,
            "fullname": "jiangxia cao",
            "user": "caojiangxia",
            "type": "user"
          },
          "name": "Jiangxia Cao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-03T07:16:44.694Z",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e57",
          "name": "Jiankang Chen",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e58",
          "user": {
            "_id": "61540338e5b9ae6774201e58",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61540338e5b9ae6774201e58/p_minqil1sdiqg5wEVxT5.jpeg",
            "isPro": false,
            "fullname": "jingyun",
            "user": "hjy",
            "type": "user"
          },
          "name": "Jingyun Hua",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-03T07:15:01.253Z",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e59",
          "name": "Jin Ouyang",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e5a",
          "name": "Kaibing Chen",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e5b",
          "name": "Kaiyu Jiang",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e5c",
          "name": "Kaiyu Tang",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e5d",
          "name": "Kun Gai",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e5e",
          "name": "Shengnan Zhang",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e5f",
          "name": "Siyang Mao",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e60",
          "name": "Sui Huang",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e61",
          "name": "Tianke Zhang",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e62",
          "user": {
            "_id": "68652063e29f1407b58da60f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/hTgS65zsKRPxELjSMDSNm.png",
            "isPro": false,
            "fullname": "tingting gao",
            "user": "TinaGao",
            "type": "user"
          },
          "name": "Tingting Gao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-03T07:47:43.679Z",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e63",
          "name": "Wei Chen",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e64",
          "user": {
            "_id": "65423daba385933e812516d5",
            "avatarUrl": "/avatars/d18b85b4206ab5905ef5bc95622dff3e.svg",
            "isPro": false,
            "fullname": "wei yuan",
            "user": "yw95",
            "type": "user"
          },
          "name": "Wei Yuan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-03T07:16:49.219Z",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e65",
          "name": "Xiangyu Wu",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e66",
          "user": {
            "_id": "64a4dba8fe950993d2d89113",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a4dba8fe950993d2d89113/yukb9NNeVspIX7eFTreUq.jpeg",
            "isPro": false,
            "fullname": "Xiao Hu",
            "user": "huxiao09",
            "type": "user"
          },
          "name": "Xiao Hu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-03T07:16:42.172Z",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e67",
          "user": {
            "_id": "673597cfc2424474d12ca58c",
            "avatarUrl": "/avatars/f748f19619b07838a66bc419a7a6db9d.svg",
            "isPro": false,
            "fullname": "xingyulu",
            "user": "Xingyulu47",
            "type": "user"
          },
          "name": "Xingyu Lu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-03T07:16:46.882Z",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e68",
          "name": "Yang Zhou",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e69",
          "user": {
            "_id": "623d8ca4c29adf5ef6175615",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623d8ca4c29adf5ef6175615/q7lHao7UPwU1u7YLSP56m.jpeg",
            "isPro": false,
            "fullname": "Yi-Fan Zhang",
            "user": "yifanzhang114",
            "type": "user"
          },
          "name": "Yi-Fan Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-03T07:15:06.461Z",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e6a",
          "name": "Yiping Yang",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e6b",
          "name": "Yulong Chen",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e6c",
          "name": "Zhenhua Wu",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e6d",
          "name": "Zhenyu Li",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e6e",
          "user": {
            "_id": "641948b7d13ffa40812eb239",
            "avatarUrl": "/avatars/65a0262fea6907bec48ddc1d966742da.svg",
            "isPro": false,
            "fullname": "Zhixin Ling",
            "user": "NamingIsTroublesome",
            "type": "user"
          },
          "name": "Zhixin Ling",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-03T07:15:03.454Z",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e6f",
          "name": "Ziming Li",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e70",
          "name": "Dehua Ma",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e71",
          "name": "Di Xu",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e72",
          "name": "Haixuan Gao",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e73",
          "name": "Hang Li",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e74",
          "name": "Jiawei Guo",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e75",
          "name": "Jing Wang",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e76",
          "name": "Lejian Ren",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e77",
          "name": "Muhao Wei",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e78",
          "name": "Qianqian Wang",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e79",
          "name": "Qigen Hu",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e7a",
          "name": "Shiyao Wang",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e7b",
          "name": "Tao Yu",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e7c",
          "name": "Xinchen Luo",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e7d",
          "name": "Yan Li",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e7e",
          "name": "Yiming Liang",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e7f",
          "name": "Yuhang Hu",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e80",
          "name": "Zeyi Lu",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e81",
          "name": "Zhuoran Yang",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e82",
          "name": "Zixing Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-02T17:57:28.000Z",
      "submittedOnDailyAt": "2025-07-03T00:40:58.759Z",
      "title": "Rapport Technique de Kwai Keye-VL\n\nLe Rapport Technique de Kwai Keye-VL fournit des informations détaillées sur la technologie Kwai Keye-VL, expliquant son fondement essentiel, ses fonctionnalités et des exemples d'applications réelles. Ce rapport est une source fondamentale pour comprendre le développement et les perspectives futures de la technologie Kwai Keye-VL, offrant un analyse approfondie qui peut être appliquée dans divers secteurs.",
      "submittedOnDailyBy": {
        "_id": "623d8ca4c29adf5ef6175615",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623d8ca4c29adf5ef6175615/q7lHao7UPwU1u7YLSP56m.jpeg",
        "isPro": false,
        "fullname": "Yi-Fan Zhang",
        "user": "yifanzhang114",
        "type": "user"
      },
      "summary": "Les MLLMs (Modèles de Langue de Vision de Marche) ont démontré des capacités impressionnantes pour des images statiques, mais leur compréhension de vidéos courtes riches en information est insuffisante. Pour combler ce vide, nous présentons Kwai Keye-VL. Kwai Keye-VL est un modèle marche modal avec 8 milliards de paramètres, maintenant un leadership dans la compréhension de vidéos courtes, tout en possédant une forte capacité en langage visuel général. L'amélioration de Keye-VL repose sur deux piliers clés : une grande quantité de données de haute qualité et un processus d'entraînement unique. Le ensemble de données dépasse les 6000 milliards de tokens, avec un accentus sur les vidéos. Le processus d'entraînement comprend 4 étapes d'entraînement préalable et 2 étapes d'entraînement postérieure. Dans la première étape postérieure, les capacités basiques et la capacité de suivi d'instructions sont améliorées, tandis que dans la deuxième, l'accent est mis sur la génération de logique. Dans cette dernière, un ensemble de données \"Cold Start\" avec 5 modes (\"Pensée\", \"Non Pensée\", \"Automatique\", \"Pensée et Peinture\", et haute qualité de vidéos) est utilisé, ce qui est une innovation cruciale pour que le modèle apprenne quand et comment faire de la logique. Ces processus, conjointement avec l'apprentissage par renforcement (RL) et le passage d'alignement, améliorent les habiletés de logique et modifient le comportement idéal du modèle pour son amélioration continue. Pour tester l'efficacité de son approche, Keye-VL a atteint les meilleurs résultats dans les marques de tests de vidéos publiques et maintient une forte compétitivité dans les tâches basées sur les images (Figure 1). De plus, un nouveau cadre de référence KC-MMBench a été développé pour fournir un cadre de référence approprié pour les scénarios réels de vidéos courtes, clairement montrant la supériorité de Keye-VL.",
      "upvotes": 85,
      "discussionId": "6865e6218c83dab5f72d1e83",
      "projectPage": "https://kwai-keye.github.io/",
      "githubRepo": "https://github.com/Kwai-Keye/Keye",
      "githubStars": 365
    },
    "publishedAt": "2025-07-02T13:57:28.000Z",
    "title": "Kwai Keye-VL Technical Report",
    "summary": "While Multimodal Large Language Models (MLLMs) demonstrate remarkable\ncapabilities on static images, they often fall short in comprehending dynamic,\ninformation-dense short-form videos, a dominant medium in today's digital\nlandscape. To bridge this gap, we introduce Kwai Keye-VL, an\n8-billion-parameter multimodal foundation model engineered for leading-edge\nperformance in short-video understanding while maintaining robust\ngeneral-purpose vision-language abilities. The development of Keye-VL rests on\ntwo core pillars: a massive, high-quality dataset exceeding 600 billion tokens\nwith a strong emphasis on video, and an innovative training recipe. This recipe\nfeatures a four-stage pre-training process for solid vision-language alignment,\nfollowed by a meticulous two-phase post-training process. The first\npost-training stage enhances foundational capabilities like instruction\nfollowing, while the second phase focuses on stimulating advanced reasoning. In\nthis second phase, a key innovation is our five-mode ``cold-start'' data\nmixture, which includes ``thinking'', ``non-thinking'', ``auto-think'', ``think\nwith image'', and high-quality video data. This mixture teaches the model to\ndecide when and how to reason. Subsequent reinforcement learning (RL) and\nalignment steps further enhance these reasoning capabilities and correct\nabnormal model behaviors, such as repetitive outputs. To validate our approach,\nwe conduct extensive evaluations, showing that Keye-VL achieves\nstate-of-the-art results on public video benchmarks and remains highly\ncompetitive on general image-based tasks (Figure 1). Furthermore, we develop\nand release the KC-MMBench, a new benchmark tailored for real-world\nshort-video scenarios, where Keye-VL shows a significant advantage.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.01949.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "623d8ca4c29adf5ef6175615",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623d8ca4c29adf5ef6175615/q7lHao7UPwU1u7YLSP56m.jpeg",
      "fullname": "Yi-Fan Zhang",
      "name": "yifanzhang114",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.01945",
      "authors": [
        {
          "_id": "6865e4b88c83dab5f72d1e41",
          "user": {
            "_id": "6629d7c9fa14eaccf07d8633",
            "avatarUrl": "/avatars/dceb2f6c804c583adf15a3536c8c995b.svg",
            "isPro": false,
            "fullname": "Nan Chen",
            "user": "CNcreator0331",
            "type": "user"
          },
          "name": "Nan Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-03T07:16:51.437Z",
          "hidden": false
        },
        {
          "_id": "6865e4b88c83dab5f72d1e42",
          "name": "Mengqi Huang",
          "hidden": false
        },
        {
          "_id": "6865e4b88c83dab5f72d1e43",
          "name": "Yihao Meng",
          "hidden": false
        },
        {
          "_id": "6865e4b88c83dab5f72d1e44",
          "name": "Zhendong Mao",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6629d7c9fa14eaccf07d8633/0WDfsnDIDJ9hzI6iJym9L.mp4"
      ],
      "publishedAt": "2025-07-02T17:55:50.000Z",
      "submittedOnDailyAt": "2025-07-03T00:57:17.831Z",
      "title": "Animation de l'écran : Génération d'animation de l'écran en utilisant la mémoire globale locale dynamique",
      "submittedOnDailyBy": {
        "_id": "6629d7c9fa14eaccf07d8633",
        "avatarUrl": "/avatars/dceb2f6c804c583adf15a3536c8c995b.svg",
        "isPro": false,
        "fullname": "Nan Chen",
        "user": "CNcreator0331",
        "type": "user"
      },
      "summary": "La colorisation d'animation est un aspect important dans l'industrie de la production d'animation réelle. Il nécessite une grande quantité de coûts laboraux pour la colorisation à long terme des animations. Par conséquent, il a une grande valeur de recherche l'automatisation de la colorisation à long terme basée sur des modèles de génération vidéo. Les études précédentes sont limitées à la colorisation à court terme. Ces études introduisent des motifs locaux pour atteindre des mouvements lisses entre segments locaux par fusion de caractéristiques répétées. Cependant, les motifs locaux ignorent l'information globale et ne peuvent pas maintenir la cohérence de couleur à long terme. Dans cette étude, on soutient que une cohérence idéale de couleur à long terme peut être atteinte par un paradigme dynamique global-local. Concrètement, nous proposons un nouveau cadre de travail appelé LongAnimation, qui est constitué de SketchDiT, Dynamic Global-Local Memory (DGLM) et Reward de Cohérence de Couleur. SketchDiT capture des caractéristiques de référence hybrides pour l'intégration et soutient le module DGLM. Le module DGLM utilise des modèles de compréhension vidéo à long terme pour compresser dynamiquement les caractéristiques historiques globales et adapter les caractéristiques de génération actuelles. Le Reward de Cohérence de Couleur est introduit pour raffiner la cohérence de couleur. Pendant l'inférence, on propose la fusion de la cohérence de couleur pour lisser la consistance de couleur. Les expériences étendues pour des animations à court terme (14 frame) et à long terme (moyenne de 500 frames) montrent que LongAnimation maintient la cohérence de couleur à court et à long terme dans le travail de colorisation d'animations publiques. Le code peut être trouvé sur https://cn-makers.github.io/long_animation_web/.",
      "upvotes": 52,
      "discussionId": "6865e4b88c83dab5f72d1e45",
      "projectPage": "https://cn-makers.github.io/long_animation_web/",
      "githubRepo": "https://github.com/CN-makers/LongAnimation",
      "githubStars": 65
    },
    "publishedAt": "2025-07-02T13:55:50.000Z",
    "title": "LongAnimation: Long Animation Generation with Dynamic Global-Local\n  Memory",
    "summary": "Animation colorization is a crucial part of real animation industry\nproduction. Long animation colorization has high labor costs. Therefore,\nautomated long animation colorization based on the video generation model has\nsignificant research value. Existing studies are limited to short-term\ncolorization. These studies adopt a local paradigm, fusing overlapping features\nto achieve smooth transitions between local segments. However, the local\nparadigm neglects global information, failing to maintain long-term color\nconsistency. In this study, we argue that ideal long-term color consistency can\nbe achieved through a dynamic global-local paradigm, i.e., dynamically\nextracting global color-consistent features relevant to the current generation.\nSpecifically, we propose LongAnimation, a novel framework, which mainly\nincludes a SketchDiT, a Dynamic Global-Local Memory (DGLM), and a Color\nConsistency Reward. The SketchDiT captures hybrid reference features to support\nthe DGLM module. The DGLM module employs a long video understanding model to\ndynamically compress global historical features and adaptively fuse them with\nthe current generation features. To refine the color consistency, we introduce\na Color Consistency Reward. During inference, we propose a color consistency\nfusion to smooth the video segment transition. Extensive experiments on both\nshort-term (14 frames) and long-term (average 500 frames) animations show the\neffectiveness of LongAnimation in maintaining short-term and long-term color\nconsistency for open-domain animation colorization task. The code can be found\nat https://cn-makers.github.io/long_animation_web/.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6629d7c9fa14eaccf07d8633/0WDfsnDIDJ9hzI6iJym9L.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.01945.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6629d7c9fa14eaccf07d8633",
      "avatarUrl": "/avatars/dceb2f6c804c583adf15a3536c8c995b.svg",
      "fullname": "Nan Chen",
      "name": "CNcreator0331",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.01634",
      "authors": [
        {
          "_id": "6865e04b8c83dab5f72d1e2d",
          "user": {
            "_id": "66ef2611fcc1c455f8dce832",
            "avatarUrl": "/avatars/c73ef2dfcd1e6ec8414a31226ad38e3b.svg",
            "isPro": false,
            "fullname": "Boyuan Sun",
            "user": "BBBBCHAN",
            "type": "user"
          },
          "name": "Boyuan Sun",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-03T07:16:54.412Z",
          "hidden": false
        },
        {
          "_id": "6865e04b8c83dab5f72d1e2e",
          "name": "Modi Jin",
          "hidden": false
        },
        {
          "_id": "6865e04b8c83dab5f72d1e2f",
          "name": "Bowen Yin",
          "hidden": false
        },
        {
          "_id": "6865e04b8c83dab5f72d1e30",
          "name": "Qibin Hou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-02T12:05:57.000Z",
      "submittedOnDailyAt": "2025-07-03T00:16:17.577Z",
      "title": "Dans n'importe quelle situation, quelque chose peut être abordé de manière profonde.",
      "submittedOnDailyBy": {
        "_id": "66ef2611fcc1c455f8dce832",
        "avatarUrl": "/avatars/c73ef2dfcd1e6ec8414a31226ad38e3b.svg",
        "isPro": false,
        "fullname": "Boyuan Sun",
        "user": "BBBBCHAN",
        "type": "user"
      },
      "summary": "DepthAnything àn Toute Condition (DepthAnything-AC) est un modèle de base d'estimation de profondeur qui gère diverses conditions environnementales. Les modèles de base existants montrent un excellent rendement dans les lieux courants, mais dans des environnements complexes ouverts, ils souvent font face à des changements d'illumination, des climats hostiles et des phénomènes exceptionnels causés par les capteurs. Pour résoudre les problèmes de manque de données et de génération de labels virtuels de haute qualité à partir d'images endommagées, on propose un paradigme de normalisation manuelle consistante. De plus, on propose une restriction spatiale et on explicite que le modèle apprend des relations relatives au niveau des patches, ce qui permet de rendre les frontières plus claires et les détails plus précis. A travers les résultats des expériences, on a démontré la capacité de DepthAnything-AC en 0 shot, et on a présenté des résultats dans différents cadres de référence, y compris des environnements hostiles réels, des cadres de référence synthétiques de dégâts et des cadres de référence généraux.\n\nPage du projet : https://ghost233lism.github.io/depthanything-AC-page\nCode : https://github.com/HVision-NKU/DepthAnythingAC",
      "upvotes": 27,
      "discussionId": "6865e04b8c83dab5f72d1e31",
      "projectPage": "https://ghost233lism.github.io/depthanything-AC-page/",
      "githubRepo": "https://github.com/HVision-NKU/DepthAnythingAC",
      "githubStars": 91
    },
    "publishedAt": "2025-07-02T08:05:57.000Z",
    "title": "Depth Anything at Any Condition",
    "summary": "We present Depth Anything at Any Condition (DepthAnything-AC), a foundation\nmonocular depth estimation (MDE) model capable of handling diverse\nenvironmental conditions. Previous foundation MDE models achieve impressive\nperformance across general scenes but not perform well in complex open-world\nenvironments that involve challenging conditions, such as illumination\nvariations, adverse weather, and sensor-induced distortions. To overcome the\nchallenges of data scarcity and the inability of generating high-quality\npseudo-labels from corrupted images, we propose an unsupervised consistency\nregularization finetuning paradigm that requires only a relatively small amount\nof unlabeled data. Furthermore, we propose the Spatial Distance Constraint to\nexplicitly enforce the model to learn patch-level relative relationships,\nresulting in clearer semantic boundaries and more accurate details.\nExperimental results demonstrate the zero-shot capabilities of DepthAnything-AC\nacross diverse benchmarks, including real-world adverse weather benchmarks,\nsynthetic corruption benchmarks, and general benchmarks.\n  Project Page: https://ghost233lism.github.io/depthanything-AC-page\n  Code: https://github.com/HVision-NKU/DepthAnythingAC",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.01634.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66ef2611fcc1c455f8dce832",
      "avatarUrl": "/avatars/c73ef2dfcd1e6ec8414a31226ad38e3b.svg",
      "fullname": "Boyuan Sun",
      "name": "BBBBCHAN",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.01925",
      "authors": [
        {
          "_id": "686600cf8c83dab5f72d1ed0",
          "name": "Yifan Zhong",
          "hidden": false
        },
        {
          "_id": "686600cf8c83dab5f72d1ed1",
          "name": "Fengshuo Bai",
          "hidden": false
        },
        {
          "_id": "686600cf8c83dab5f72d1ed2",
          "user": {
            "_id": "6578459d62d3ac1817ed79fe",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6578459d62d3ac1817ed79fe/AXDJuwLUoEOb4Fj3U0Xxo.jpeg",
            "isPro": false,
            "fullname": "Shaofei Cai",
            "user": "phython96",
            "type": "user"
          },
          "name": "Shaofei Cai",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-03T07:14:49.923Z",
          "hidden": false
        },
        {
          "_id": "686600cf8c83dab5f72d1ed3",
          "user": {
            "_id": "66ee5cf1801ea45d7a44a542",
            "avatarUrl": "/avatars/04bafbcbf1aea3920a79bddbd1a18f42.svg",
            "isPro": false,
            "fullname": "XUCHUAN HUANG",
            "user": "Feernnn",
            "type": "user"
          },
          "name": "Xuchuan Huang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-03T07:14:52.141Z",
          "hidden": false
        },
        {
          "_id": "686600cf8c83dab5f72d1ed4",
          "name": "Zhang Chen",
          "hidden": false
        },
        {
          "_id": "686600cf8c83dab5f72d1ed5",
          "name": "Xiaowei Zhang",
          "hidden": false
        },
        {
          "_id": "686600cf8c83dab5f72d1ed6",
          "name": "Yuanfei Wang",
          "hidden": false
        },
        {
          "_id": "686600cf8c83dab5f72d1ed7",
          "name": "Shaoyang Guo",
          "hidden": false
        },
        {
          "_id": "686600cf8c83dab5f72d1ed8",
          "name": "Tianrui Guan",
          "hidden": false
        },
        {
          "_id": "686600cf8c83dab5f72d1ed9",
          "name": "Ka Nam Lui",
          "hidden": false
        },
        {
          "_id": "686600cf8c83dab5f72d1eda",
          "name": "Zhiquan Qi",
          "hidden": false
        },
        {
          "_id": "686600cf8c83dab5f72d1edb",
          "name": "Yitao Liang",
          "hidden": false
        },
        {
          "_id": "686600cf8c83dab5f72d1edc",
          "name": "Yuanpei Chen",
          "hidden": false
        },
        {
          "_id": "686600cf8c83dab5f72d1edd",
          "name": "Yaodong Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-02T17:34:52.000Z",
      "submittedOnDailyAt": "2025-07-03T03:39:33.625Z",
      "title": "Vision de Module Action Rune : Du point de vue de Toki Ninja",
      "submittedOnDailyBy": {
        "_id": "655d9f43b5da99edaf3f2f81",
        "avatarUrl": "/avatars/c7225b3ed54d099a4fd87682427fb5bf.svg",
        "isPro": false,
        "fullname": "Yifan Zhong",
        "user": "Yifan-Zhong",
        "type": "user"
      },
      "summary": "L'avancement spectaculaire des modèles de base de vision et de langage est entraînant un impact significatif sur la compréhension, le raisonnement et la génération, impulsé la prolifération des modèles Vision-Langue-Action (VLA) vers la compréhension de la réalité physique. Les VLA actuels présentent différents approches, mais nous avons découvert qu'ils peuvent être unifiés sous un seul cadre : les entrées de vision et de langage sont traitées de manière continue par les modules VLA, générant une séquence de tokens d'action qui incluent de l'information de plus en plus concrète et actionnable, finalement créant des actions exécutables. De plus, la décision principale de conception des modèles VLA a clairement identifié la composition des tokens d'action. Ceux-ci sont classifiés en explications linguistiques, code, AFFORDANCE, routes, états de but, représentations potentielles, actions simples, raisons et classification. Cependant, la faible compréhension détaillée des tokens d'action empêche un progrès efficace dans le développement des VLA et cache les directions potentielles futures. Par conséquent, cette recherche vise à classer les études existantes à partir de la perspective de la tokenisation d'actions, à extraire les forces et les limites de chaque token et à identifier des domaines avec des possibilités d'amélioration. Grâce à cette évaluation systématique et d'analyse, nous examinons l'évolution générale des VLA d'une perspective synthétique, révélant des directions potentielles qui n'ont pas été investiguées mais qui ont du potentiel, offrant des orientations pour futures recherches et attendant que l'on approche l'intelligence générale.",
      "upvotes": 13,
      "discussionId": "686600cf8c83dab5f72d1ede",
      "githubRepo": "https://github.com/Psi-Robot/Awesome-VLA-Papers",
      "githubStars": 21
    },
    "publishedAt": "2025-07-02T13:34:52.000Z",
    "title": "A Survey on Vision-Language-Action Models: An Action Tokenization\n  Perspective",
    "summary": "The remarkable advancements of vision and language foundation models in\nmultimodal understanding, reasoning, and generation has sparked growing efforts\nto extend such intelligence to the physical world, fueling the flourishing of\nvision-language-action (VLA) models. Despite seemingly diverse approaches, we\nobserve that current VLA models can be unified under a single framework: vision\nand language inputs are processed by a series of VLA modules, producing a chain\nof action tokens that progressively encode more grounded and\nactionable information, ultimately generating executable actions. We further\ndetermine that the primary design choice distinguishing VLA models lies in how\naction tokens are formulated, which can be categorized into language\ndescription, code, affordance, trajectory, goal state, latent representation,\nraw action, and reasoning. However, there remains a lack of comprehensive\nunderstanding regarding action tokens, significantly impeding effective VLA\ndevelopment and obscuring future directions. Therefore, this survey aims to\ncategorize and interpret existing VLA research through the lens of action\ntokenization, distill the strengths and limitations of each token type, and\nidentify areas for improvement. Through this systematic review and analysis, we\noffer a synthesized outlook on the broader evolution of VLA models, highlight\nunderexplored yet promising directions, and contribute guidance for future\nresearch, hoping to bring the field closer to general-purpose intelligence.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.01925.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "655d9f43b5da99edaf3f2f81",
      "avatarUrl": "/avatars/c7225b3ed54d099a4fd87682427fb5bf.svg",
      "fullname": "Yifan Zhong",
      "name": "Yifan-Zhong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.01953",
      "authors": [
        {
          "_id": "686601648c83dab5f72d1ee0",
          "name": "Yukang Cao",
          "hidden": false
        },
        {
          "_id": "686601648c83dab5f72d1ee1",
          "name": "Chenyang Si",
          "hidden": false
        },
        {
          "_id": "686601648c83dab5f72d1ee2",
          "name": "Jinghao Wang",
          "hidden": false
        },
        {
          "_id": "686601648c83dab5f72d1ee3",
          "name": "Ziwei Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-02T17:58:20.000Z",
      "submittedOnDailyAt": "2025-07-03T02:40:23.949Z",
      "title": "FreeMorph : Technologie de déformation d'images étendues sans ajustements en utilisant des bandes de forme",
      "submittedOnDailyBy": {
        "_id": "63a07c3ab5515dccd40fdb71",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a07c3ab5515dccd40fdb71/ly3pwhjWVge25LAeVgriV.png",
        "isPro": false,
        "fullname": "Yukang Cao",
        "user": "yukangcao",
        "type": "user"
      },
      "summary": "FreeMorph est un des méthodes de morphologie d'images initiales sans contraintes d'ajustement, capable de gérer différentes sémantiques et dispositions. Les méthodes actuelles nécessitent des ajustements vers des modèles de diffusion préalablement entraînés, ce qui les limite en termes de temps et de sémantique. Cependant, FreeMorph dépasse ces limites, sans nécessiter d'entraînement par projet et offrant des morphologies d'images de haute qualité. Les méthodes sans ajustement présentent des problèmes en raison de la non-linéarité du processus multi-échelle de diffusion et des biais hérités de modèles préalablement entraînés. Dans cet article, nous présentons deux innovations clés pour résoudre ces problèmes et introduire FreeMorph. 1) Nous proposons un interrupteur génératif qui accepte des guides explicites des images d'entrée, améliore le module d'autonomie pour corriger les pertes de reconnaissance et assure des opportunités dirigées tout au long de la séquence générée. 2) Nous introduisons des changements progressifs qui combinent les modules d'autonomie obtenus de chaque image d'entrée, réalisant des opportunités contrôlées et cohérentes pour les deux images. Selon des évaluations larges, FreeMorph dépasse les méthodes actuelles et a innové dans la morphologie d'images, offrant des innovations entre 10 et 50 fois plus rapides.",
      "upvotes": 9,
      "discussionId": "686601658c83dab5f72d1ee4",
      "projectPage": "https://yukangcao.github.io/FreeMorph/",
      "githubRepo": "https://github.com/yukangcao/FreeMorph",
      "githubStars": 7
    },
    "publishedAt": "2025-07-02T13:58:20.000Z",
    "title": "FreeMorph: Tuning-Free Generalized Image Morphing with Diffusion Model",
    "summary": "We present FreeMorph, the first tuning-free method for image morphing that\naccommodates inputs with different semantics or layouts. Unlike existing\nmethods that rely on finetuning pre-trained diffusion models and are limited by\ntime constraints and semantic/layout discrepancies, FreeMorph delivers\nhigh-fidelity image morphing without requiring per-instance training. Despite\ntheir efficiency and potential, tuning-free methods face challenges in\nmaintaining high-quality results due to the non-linear nature of the multi-step\ndenoising process and biases inherited from the pre-trained diffusion model. In\nthis paper, we introduce FreeMorph to address these challenges by integrating\ntwo key innovations. 1) We first propose a guidance-aware spherical\ninterpolation design that incorporates explicit guidance from the input images\nby modifying the self-attention modules, thereby addressing identity loss and\nensuring directional transitions throughout the generated sequence. 2) We\nfurther introduce a step-oriented variation trend that blends self-attention\nmodules derived from each input image to achieve controlled and consistent\ntransitions that respect both inputs. Our extensive evaluations demonstrate\nthat FreeMorph outperforms existing methods, being 10x ~ 50x faster and\nestablishing a new state-of-the-art for image morphing.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.01953.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a07c3ab5515dccd40fdb71",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a07c3ab5515dccd40fdb71/ly3pwhjWVge25LAeVgriV.png",
      "fullname": "Yukang Cao",
      "name": "yukangcao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.01957",
      "authors": [
        {
          "_id": "686633d28c83dab5f72d1f39",
          "name": "Zhuoyang Zhang",
          "hidden": false
        },
        {
          "_id": "686633d28c83dab5f72d1f3a",
          "name": "Luke J. Huang",
          "hidden": false
        },
        {
          "_id": "686633d28c83dab5f72d1f3b",
          "name": "Chengyue Wu",
          "hidden": false
        },
        {
          "_id": "686633d28c83dab5f72d1f3c",
          "name": "Shang Yang",
          "hidden": false
        },
        {
          "_id": "686633d28c83dab5f72d1f3d",
          "name": "Kelly Peng",
          "hidden": false
        },
        {
          "_id": "686633d28c83dab5f72d1f3e",
          "name": "Yao Lu",
          "hidden": false
        },
        {
          "_id": "686633d28c83dab5f72d1f3f",
          "name": "Song Han",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-02T17:59:23.000Z",
      "submittedOnDailyAt": "2025-07-03T06:13:07.255Z",
      "title": "Génération d'images par des chaînes de mots automatiquement grâce à des méthodes de décision parallèles efficaces basées sur l'information locale.",
      "submittedOnDailyBy": {
        "_id": "650e6ab08f3228d807707735",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/650e6ab08f3228d807707735/yFo6aLuyGH06t9yG8AOp7.png",
        "isPro": false,
        "fullname": "Zhuoyang Zhang",
        "user": "zhuoyang20",
        "type": "user"
      },
      "summary": "Introducing la Décomposition en Parallèle Loca (LPD) pour les intéressés par Rocket, et accélérant la génération automatique de images réduites. La génération automatique traditionnelle de images réduites est basée sur le traitement en mémoire et la prédiction de patch suivant, ce qui entraîne une latence élevée. Des recherches précédentes ont essayé de paralléliser la prédiction de patch suivant en passant à la prédiction de multiples patchs pour accélérer le traitement, mais ont atteint une parallélisation limitée. Pour atteindre une parallélisation élevée tout en maintenant la qualité de la génération, nous introduisons deux technologies clés : (1) Modélisation Automatique de Reduction en Parallèle Flexible, une nouvelle architecture qui permet un ordre de génération arbitraire et un degré de parallélisation variable. En utilisant des tokens de requête de position apprenables, elle guide la génération à la position cible tout en garantissant la visibilité mutuelle des tokens générés, atteignant une décomposition parallèle cohérente. (2) Ordre de Génération Intéressant pour Rocket, forme de groupes et minimise les dépendances intra-groupe, maximise l'accentuation contextuelle pour améliorer la qualité de la génération. En conséquence, dans la génération conditionnelle sur ImageNet, nous réduisons le nombre d'étapes de génération de 256 à 20 (résolution 256×256) et de 1024 à 48 (résolution 512×512), atteignant une latence 3,4 fois plus faible par rapport aux modèles automatiques de réduction parallélisés précédents tout en maintenant la qualité.",
      "upvotes": 7,
      "discussionId": "686633d28c83dab5f72d1f40"
    },
    "publishedAt": "2025-07-02T13:59:23.000Z",
    "title": "Locality-aware Parallel Decoding for Efficient Autoregressive Image\n  Generation",
    "summary": "We present Locality-aware Parallel Decoding (LPD) to accelerate\nautoregressive image generation. Traditional autoregressive image generation\nrelies on next-patch prediction, a memory-bound process that leads to high\nlatency. Existing works have tried to parallelize next-patch prediction by\nshifting to multi-patch prediction to accelerate the process, but only achieved\nlimited parallelization. To achieve high parallelization while maintaining\ngeneration quality, we introduce two key techniques: (1) Flexible Parallelized\nAutoregressive Modeling, a novel architecture that enables arbitrary generation\nordering and degrees of parallelization. It uses learnable position query\ntokens to guide generation at target positions while ensuring mutual visibility\namong concurrently generated tokens for consistent parallel decoding. (2)\nLocality-aware Generation Ordering, a novel schedule that forms groups to\nminimize intra-group dependencies and maximize contextual support, enhancing\ngeneration quality. With these designs, we reduce the generation steps from 256\nto 20 (256times256 res.) and 1024 to 48 (512times512 res.) without\ncompromising quality on the ImageNet class-conditional generation, and\nachieving at least 3.4times lower latency than previous parallelized\nautoregressive models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.01957.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "650e6ab08f3228d807707735",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/650e6ab08f3228d807707735/yFo6aLuyGH06t9yG8AOp7.png",
      "fullname": "Zhuoyang Zhang",
      "name": "zhuoyang20",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.23552",
      "authors": [
        {
          "_id": "6865e0148c83dab5f72d1e26",
          "name": "Mingi Kwon",
          "hidden": false
        },
        {
          "_id": "6865e0148c83dab5f72d1e27",
          "user": {
            "_id": "631074d895c34b95407945f0",
            "avatarUrl": "/avatars/699baf06ec818650dec5752aca87c5b4.svg",
            "isPro": false,
            "fullname": "Joonghyuk Shin",
            "user": "alex4727",
            "type": "user"
          },
          "name": "Joonghyuk Shin",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-03T07:16:56.605Z",
          "hidden": false
        },
        {
          "_id": "6865e0148c83dab5f72d1e28",
          "name": "Jaeseok Jung",
          "hidden": false
        },
        {
          "_id": "6865e0148c83dab5f72d1e29",
          "name": "Jaesik Park",
          "hidden": false
        },
        {
          "_id": "6865e0148c83dab5f72d1e2a",
          "name": "Youngjung Uh",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-30T06:51:40.000Z",
      "submittedOnDailyAt": "2025-07-03T00:15:16.476Z",
      "title": "JAM-Flow : Régulation de Flux pour la Synthèse d'Audio-Motion Combinée",
      "submittedOnDailyBy": {
        "_id": "631074d895c34b95407945f0",
        "avatarUrl": "/avatars/699baf06ec818650dec5752aca87c5b4.svg",
        "isPro": false,
        "fullname": "Joonghyuk Shin",
        "user": "alex4727",
        "type": "user"
      },
      "summary": "L'lien intrinsèque entre le mouvement physique et le son a été généralement ignoré dans le modèle génératif, étant un sujet microscopique. Dans cet article, nous présentons un ensemble de cadres de travail qui synthétisent simultanément des phénomènes physiques et des sons, configurables selon des conditions. Notre approche utilise le matching de flux et une nouvelle architecture de Transformer Multi-Modal Diffusion (MM-DiT), combinant des modules spécifiques de Motion-DiT et Audio-DiT. Ceux-ci sont unis par des couches d'attention communes optionnelles, permettant une interaction efficace entre les modalités tout en maintenant leurs propres caractéristiques. Le JAM-Flow, entraîné pour le type PENCINTA, supporte une large gamme d'entrées de conditions, y compris du texte, des sons de référence et des mouvements de référence, ce qui permet la génération de tableaux de tête synchronisés à partir du texte et l'animation de sons de mouvement. JAM-Flow fournit une solution pratique pour la synthèse d'hybrides de sons et représente un grand avancé dans la modélisation générative multimodal. Page du projet : https://joonghyuk.com/jamflow-web",
      "upvotes": 3,
      "discussionId": "6865e0148c83dab5f72d1e2b"
    },
    "publishedAt": "2025-06-30T02:51:40.000Z",
    "title": "JAM-Flow: Joint Audio-Motion Synthesis with Flow Matching",
    "summary": "The intrinsic link between facial motion and speech is often overlooked in\ngenerative modeling, where talking head synthesis and text-to-speech (TTS) are\ntypically addressed as separate tasks. This paper introduces JAM-Flow, a\nunified framework to simultaneously synthesize and condition on both facial\nmotion and speech. Our approach leverages flow matching and a novel Multi-Modal\nDiffusion Transformer (MM-DiT) architecture, integrating specialized Motion-DiT\nand Audio-DiT modules. These are coupled via selective joint attention layers\nand incorporate key architectural choices, such as temporally aligned\npositional embeddings and localized joint attention masking, to enable\neffective cross-modal interaction while preserving modality-specific strengths.\nTrained with an inpainting-style objective, JAM-Flow supports a wide array of\nconditioning inputs-including text, reference audio, and reference\nmotion-facilitating tasks such as synchronized talking head generation from\ntext, audio-driven animation, and much more, within a single, coherent model.\nJAM-Flow significantly advances multi-modal generative modeling by providing a\npractical solution for holistic audio-visual synthesis. project page:\nhttps://joonghyuk.com/jamflow-web",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.23552.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "631074d895c34b95407945f0",
      "avatarUrl": "/avatars/699baf06ec818650dec5752aca87c5b4.svg",
      "fullname": "Joonghyuk Shin",
      "name": "alex4727",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.22868",
      "authors": [
        {
          "_id": "68637f0d588cea0da970c95e",
          "user": {
            "_id": "6719de3235b6494469ab69f6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/67l_hybjE-E2nH8_EyBuO.png",
            "isPro": false,
            "fullname": "Junsung Lee",
            "user": "jslee525",
            "type": "user"
          },
          "name": "Junsung Lee",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-03T07:32:55.680Z",
          "hidden": false
        },
        {
          "_id": "68637f0d588cea0da970c95f",
          "name": "Junoh Kang",
          "hidden": false
        },
        {
          "_id": "68637f0d588cea0da970c960",
          "name": "Bohyung Han",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-28T12:36:19.000Z",
      "submittedOnDailyAt": "2025-07-03T06:08:50.749Z",
      "title": "STR-Match : Coincidence de la ponctuation de la relation espace-temps (pour l'édition de vidéos sans nécessiter d'entraînement)",
      "submittedOnDailyBy": {
        "_id": "6719de3235b6494469ab69f6",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/67l_hybjE-E2nH8_EyBuO.png",
        "isPro": false,
        "fullname": "Junsung Lee",
        "user": "jslee525",
        "type": "user"
      },
      "summary": "Les méthodes d'édition vidéo réalisées selon le guide précédent présentaient des limitations telles que la discontinuité temporelle, la distorsion du mouvement, en particulier les transformations dans des zones spécifiques. Ces limitations expliquent la manque de modélisation des relations spatio-temporelles entre les pixels. Pour résoudre ces problèmes, on propose l'algorithme d'édition vidéo \"STR-Match\". Ce méthode utilise un nouveau \"STR score\" pour guider la planification potentielle et générer des vidéos visuellement agréables et cohérentes spatio-temporellement. Ce score utilise des modules d'attention en 2D spatial et 1D temporel pour convertir le texte en un modèle de diffusion vidéo (T2V), ce qui permet de comprendre les relations spatio-temporelles entre les pixels de frames adjacentes. Pour éviter les coûts de calcul élevés de la structure d'attention 3D, ce score est efficace. La combinaison du cadre de travail de planification potentielle et de la masque potentielle dans STR-Match génère des vidéos temporellement cohérentes et visuellement fiables, en maintenant un haut rendement face à de grandes transformations dans des zones. Cette efficacité est démontrée par des expériences détaillées qui montrent un performance supérieure en termes de qualité visuelle et de cohérence spatio-temporelle par rapport aux méthodes actuelles.",
      "upvotes": 3,
      "discussionId": "68637f0e588cea0da970c961",
      "projectPage": "https://jslee525.github.io/str-match",
      "githubRepo": "https://github.com/jslee525/STR-Match_official",
      "ai_summary": "STR-Match uses latent optimization and a novel STR score to produce spatiotemporally coherent and visually appealing edited videos by leveraging 2D spatial and 1D temporal attention in T2V diffusion models.",
      "ai_keywords": [
        "T2V diffusion models",
        "latent optimization",
        "spatiotemporal pixel relevance",
        "latent mask",
        "2D spatial attention",
        "1D temporal modules"
      ],
      "githubStars": 2
    },
    "publishedAt": "2025-06-28T08:36:19.000Z",
    "title": "STR-Match: Matching SpatioTemporal Relevance Score for Training-Free\n  Video Editing",
    "summary": "Previous text-guided video editing methods often suffer from temporal\ninconsistency, motion distortion, and-most notably-limited domain\ntransformation. We attribute these limitations to insufficient modeling of\nspatiotemporal pixel relevance during the editing process. To address this, we\npropose STR-Match, a training-free video editing algorithm that produces\nvisually appealing and spatiotemporally coherent videos through latent\noptimization guided by our novel STR score. The score captures spatiotemporal\npixel relevance across adjacent frames by leveraging 2D spatial attention and\n1D temporal modules in text-to-video (T2V) diffusion models, without the\noverhead of computationally expensive 3D attention mechanisms. Integrated into\na latent optimization framework with a latent mask, STR-Match generates\ntemporally consistent and visually faithful videos, maintaining strong\nperformance even under significant domain transformations while preserving key\nvisual attributes of the source. Extensive experiments demonstrate that\nSTR-Match consistently outperforms existing methods in both visual quality and\nspatiotemporal consistency.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.22868.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6719de3235b6494469ab69f6",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/67l_hybjE-E2nH8_EyBuO.png",
      "fullname": "Junsung Lee",
      "name": "jslee525",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]