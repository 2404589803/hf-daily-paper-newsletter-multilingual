[
  {
    "paper": {
      "id": "2506.09513",
      "authors": [
        {
          "_id": "684b8dbd3b733ba33368701b",
          "user": {
            "_id": "6723079ad1306fe9c76a1d29",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/b4BNPCeZs59MKxo1qmT6r.png",
            "isPro": false,
            "fullname": "Yu Sun",
            "user": "YuSun-AI",
            "type": "user"
          },
          "name": "Yu Sun",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-13T02:32:30.652Z",
          "hidden": false
        },
        {
          "_id": "684b8dbd3b733ba33368701c",
          "name": "Xingyu Qian",
          "hidden": false
        },
        {
          "_id": "684b8dbd3b733ba33368701d",
          "name": "Weiwen Xu",
          "hidden": false
        },
        {
          "_id": "684b8dbd3b733ba33368701e",
          "user": {
            "_id": "64b7cd74ff6d81ae297feded",
            "avatarUrl": "/avatars/880fbc96cc093f5e901ce84f32a1d21d.svg",
            "isPro": false,
            "fullname": "ZHANG HAO",
            "user": "26hzhang",
            "type": "user"
          },
          "name": "Hao Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:39:43.056Z",
          "hidden": false
        },
        {
          "_id": "684b8dbd3b733ba33368701f",
          "name": "Chenghao Xiao",
          "hidden": false
        },
        {
          "_id": "684b8dbd3b733ba333687020",
          "name": "Long Li",
          "hidden": false
        },
        {
          "_id": "684b8dbd3b733ba333687021",
          "user": {
            "_id": "642eecbf9b2484d7d8526781",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642eecbf9b2484d7d8526781/4IvGbd66s49Wx5pZyZGHA.png",
            "isPro": false,
            "fullname": "Yu Rong",
            "user": "Swrooy",
            "type": "user"
          },
          "name": "Yu Rong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:39:40.908Z",
          "hidden": false
        },
        {
          "_id": "684b8dbd3b733ba333687022",
          "name": "Wenbing Huang",
          "hidden": false
        },
        {
          "_id": "684b8dbd3b733ba333687023",
          "name": "Qifeng Bai",
          "hidden": false
        },
        {
          "_id": "684b8dbd3b733ba333687024",
          "name": "Tingyang Xu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-11T08:36:55.000Z",
      "submittedOnDailyAt": "2025-06-13T01:06:46.741Z",
      "title": "ReasonMed : 370K Dataset de Données pour la Création d'Agents en Recherche sur le Développement de la Logique Médicale",
      "submittedOnDailyBy": {
        "_id": "6723079ad1306fe9c76a1d29",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/b4BNPCeZs59MKxo1qmT6r.png",
        "isPro": false,
        "fullname": "Yu Sun",
        "user": "YuSun-AI",
        "type": "user"
      },
      "summary": "Les modèles de langage grands basés sur la logique (LLMs) montrent des résultats exceptionnels en mathématiques et en programmation, mais ont une capacité insuffisante pour répondre à des questions médicales de type connaissance dense. Pour aborder ce défi, nous présentons la ReasonMed, le plus grand ensemble de données de raisons médicales. La ReasonMed comprend 1,7 millions de pas de raisons initiales, organisés dans 370.000 exemples de haute qualité. La ReasonMed identifie et renforce les pas de raisons à risque d'erreur marqués par les évaluateurs, désignant un processus d'erreur-Finn pour corriger ces pas et construisant un processus de recherche et d'amélioration efficace. En utilisant la ReasonMed, nous avons effectué une recherche systématique sur le meilleur processus d'entraînement de modèles de raisons médicales et avons découvert un processus de fine-tuning optimal qui combine des raisons détaillées de type Chain-of-Thought (CoT) et des résumés concis. Avec ce processus, nous avons entraîné la ReasonMed-7B et avons établi un nouveau standard de test pour les modèles de 10B, surpassant le meilleur modèle de la semaine précédente de 4,17% et la LLaMA3.1-70B de 4,60% sur la PubMedQA.",
      "upvotes": 46,
      "discussionId": "684b8dbe3b733ba333687025",
      "githubRepo": "https://github.com/YuSun-Work/ReasonMed",
      "ai_summary": "ReasonMed, a large medical reasoning dataset, enhances the accuracy of medical question answering models by combining detailed reasoning paths with concise summaries, setting new benchmarks for model performance.",
      "ai_keywords": [
        "reasoning-based large language models",
        "LLMs",
        "medical question answering",
        "ReasonMed",
        "multi-agent verification",
        "Error Refiner",
        "Chain-of-Thought",
        "CoT reasoning",
        "ReasonMed-7B",
        "PubMedQA"
      ]
    },
    "publishedAt": "2025-06-11T04:36:55.000Z",
    "title": "ReasonMed: A 370K Multi-Agent Generated Dataset for Advancing Medical\n  Reasoning",
    "summary": "Though reasoning-based large language models (LLMs) have excelled in\nmathematics and programming, their capabilities in knowledge-intensive medical\nquestion answering remain underexplored. To address this, we introduce\nReasonMed, the largest medical reasoning dataset, comprising 370k high-quality\nexamples distilled from 1.7 million initial reasoning paths generated by\nvarious LLMs. ReasonMed is constructed through a multi-agent\nverification and refinement process, where we design an Error Refiner\nto enhance the reasoning paths by identifying and correcting error-prone steps\nflagged by a verifier. Leveraging ReasonMed, we systematically investigate best\npractices for training medical reasoning models and find that combining\ndetailed Chain-of-Thought (CoT) reasoning with concise answer summaries yields\nthe most effective fine-tuning strategy. Based on this strategy, we train\nReasonMed-7B, which sets a new benchmark for sub-10B models, outperforming the\nprior best by 4.17\\% and even exceeding LLaMA3.1-70B on PubMedQA by 4.60\\%.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09513.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6723079ad1306fe9c76a1d29",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/b4BNPCeZs59MKxo1qmT6r.png",
      "fullname": "Yu Sun",
      "name": "YuSun-AI",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.10954",
      "authors": [
        {
          "_id": "684b7ea83b733ba333686f8a",
          "name": "Lianghong Guo",
          "hidden": false
        },
        {
          "_id": "684b7ea83b733ba333686f8b",
          "name": "Yanlin Wang",
          "hidden": false
        },
        {
          "_id": "684b7ea83b733ba333686f8c",
          "name": "Caihua Li",
          "hidden": false
        },
        {
          "_id": "684b7ea83b733ba333686f8d",
          "name": "Pengyu Yang",
          "hidden": false
        },
        {
          "_id": "684b7ea83b733ba333686f8e",
          "name": "Jiachi Chen",
          "hidden": false
        },
        {
          "_id": "684b7ea83b733ba333686f8f",
          "user": {
            "_id": "6355473d525beaee688b7ba1",
            "avatarUrl": "/avatars/0a0f0acc65829c6d864440444c580698.svg",
            "isPro": false,
            "fullname": "Wei Tao",
            "user": "itaowe",
            "type": "user"
          },
          "name": "Wei Tao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:39:58.091Z",
          "hidden": false
        },
        {
          "_id": "684b7ea83b733ba333686f90",
          "name": "Yingtian Zou",
          "hidden": false
        },
        {
          "_id": "684b7ea83b733ba333686f91",
          "name": "Duyu Tang",
          "hidden": false
        },
        {
          "_id": "684b7ea83b733ba333686f92",
          "name": "Zibin Zheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-12T17:54:17.000Z",
      "submittedOnDailyAt": "2025-06-13T00:07:20.052Z",
      "title": "SWE-Factory : Fábrica Automatique de Données de Solution de Problèmes et de Benchmark d'Évaluation",
      "submittedOnDailyBy": {
        "_id": "6355473d525beaee688b7ba1",
        "avatarUrl": "/avatars/0a0f0acc65829c6d864440444c580698.svg",
        "isPro": false,
        "fullname": "Wei Tao",
        "user": "itaowe",
        "type": "user"
      },
      "summary": "La construction de grands ensembles de données pour des tâches de résolution de problèmes sur GitHub est cruciale pour l'entraînement et l'évaluation des capacités de développement de logiciel de modèles de langage grands (LLMs). Cependant, les procédures traditionnelles de génération de ces benchmarks sont particulièrement difficiles lors des étapes de configuration de l'environnement d'évaluation, de l'évaluation des résultats de tests et de la vérification d'instances de tâches, ce qui nécessite beaucoup d'effort. Dans cet article, nous proposons SWE-Factory, une chaîne d'opérations d'automatisation pour aborder ces défis. Cette chaîne d'opérations combine trois composants d'automatisation essentiels conçus pour résoudre ces problèmes. Premier, SWE-Builder, un système d'agents multi-agents, automatise l'environnement d'évaluation et utilise des mémoires d'environnement spécialisées pour améliorer l'efficacité de la collaboration avec quatre agents spécialisés. Ensuite, nous introduisons un méthode d'évaluation basée sur du code étendu et standardisé, évitant la nécessité de manuellement écrire les paramètres de configuration. Finalement, nous automatisons le processus de vérification d'instances de tâches en utilisant des signaux de code étendu de haute confiance. Les résultats des expériences avec 671 problèmes dans quatre langages de programmation montrent que notre chaîne d'opérations est efficace dans la construction d'instances valides de tâches. Par exemple, en utilisant GPT-4.1-mini, 269 instances valides ont été construites à un coût de 0.045 instances par unité, et en utilisant Gemini-2.5-flash, un rendement similaire a été atteint à un coût de 0.024 instances par unité, le plus bas. De plus, l'évaluation basée sur du code étendu a atteint une précision de 100% par rapport à la vérification manuelle, et l'automatisation du processus de vérification de deux étapes (fail2pass) a atteint une précision de 0.92 et une reproductibilité de 1.00. Nous espérons que cette chaîne d'opérations d'automatisation accélère la collecte de grands ensembles de données de résolution de problèmes sur GitHub et les aide à leur entraînement et évaluation. Notre code et ensemble de données sont disponibles sur https://github.com/DeepSoftwareAnalytics/swe-factory.",
      "upvotes": 28,
      "discussionId": "684b7ea83b733ba333686f93",
      "githubRepo": "https://github.com/DeepSoftwareAnalytics/swe-factory",
      "ai_summary": "A pipeline named SWE-Factory automates the creation and validation of GitHub issue resolution datasets for training and evaluating Large Language Models, using SWE-Builder for environment setup, exit-code-based grading, and automated fail2pass validation.",
      "ai_keywords": [
        "Large Language Models",
        "LLMs",
        "SWE-Factory",
        "SWE-Builder",
        "multi-agent system",
        "environment memory pool",
        "exit-code-based grading",
        "automated fail2pass validation",
        "GPT-4.1-mini",
        "Gemini-2.5-flash",
        "precision",
        "recall"
      ]
    },
    "publishedAt": "2025-06-12T13:54:17.000Z",
    "title": "SWE-Factory: Your Automated Factory for Issue Resolution Training Data\n  and Evaluation Benchmarks",
    "summary": "Constructing large-scale datasets for the GitHub issue resolution task is\ncrucial for both training and evaluating the software engineering capabilities\nof Large Language Models (LLMs). However, the traditional process for creating\nsuch benchmarks is notoriously challenging and labor-intensive, particularly in\nthe stages of setting up evaluation environments, grading test outcomes, and\nvalidating task instances. In this paper, we propose SWE-Factory, an automated\npipeline designed to address these challenges. To tackle these issues, our\npipeline integrates three core automated components. First, we introduce\nSWE-Builder, a multi-agent system that automates evaluation environment\nconstruction, which employs four specialized agents that work in a\ncollaborative, iterative loop and leverages an environment memory pool to\nenhance efficiency. Second, we introduce a standardized, exit-code-based\ngrading method that eliminates the need for manually writing custom parsers.\nFinally, we automate the fail2pass validation process using these reliable exit\ncode signals. Experiments on 671 issues across four programming languages show\nthat our pipeline can effectively construct valid task instances; for example,\nwith GPT-4.1-mini, our SWE-Builder constructs 269 valid instances at 0.045 per\ninstance, while with Gemini-2.5-flash, it achieves comparable performance at\nthe lowest cost of 0.024 per instance. We also demonstrate that our\nexit-code-based grading achieves 100% accuracy compared to manual inspection,\nand our automated fail2pass validation reaches a precision of 0.92 and a recall\nof 1.00. We hope our automated pipeline will accelerate the collection of\nlarge-scale, high-quality GitHub issue resolution datasets for both training\nand evaluation. Our code and datasets are released at\nhttps://github.com/DeepSoftwareAnalytics/swe-factory.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10954.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6355473d525beaee688b7ba1",
      "avatarUrl": "/avatars/0a0f0acc65829c6d864440444c580698.svg",
      "fullname": "Wei Tao",
      "name": "itaowe",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.09993",
      "authors": [
        {
          "_id": "684ae204dbd21a9cc27b0fba",
          "user": {
            "_id": "66012e9c9e1cf5eb41ee0c4c",
            "avatarUrl": "/avatars/b07240cb86315b9e33d14677e02e4024.svg",
            "isPro": false,
            "fullname": "Jaewon Min",
            "user": "Min-Jaewon",
            "type": "user"
          },
          "name": "Jaewon Min",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:41:25.024Z",
          "hidden": false
        },
        {
          "_id": "684ae204dbd21a9cc27b0fbb",
          "user": {
            "_id": "65ec3449a69aaabb431db0da",
            "avatarUrl": "/avatars/d7b507be0175a61a8fc21176eea45001.svg",
            "isPro": false,
            "fullname": "Jin Hyeon Kim",
            "user": "jinlovespho",
            "type": "user"
          },
          "name": "Jin Hyeon Kim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:41:22.776Z",
          "hidden": false
        },
        {
          "_id": "684ae204dbd21a9cc27b0fbc",
          "user": {
            "_id": "6752b6315281c3cae4b0783f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/xmcyVEl2xBhk3G5_7dmpz.png",
            "isPro": false,
            "fullname": "Paul Hyunbin Cho",
            "user": "paulcho98",
            "type": "user"
          },
          "name": "Paul Hyunbin Cho",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:41:20.327Z",
          "hidden": false
        },
        {
          "_id": "684ae204dbd21a9cc27b0fbd",
          "name": "Jaeeun Lee",
          "hidden": false
        },
        {
          "_id": "684ae204dbd21a9cc27b0fbe",
          "name": "Jihye Park",
          "hidden": false
        },
        {
          "_id": "684ae204dbd21a9cc27b0fbf",
          "name": "Minkyu Park",
          "hidden": false
        },
        {
          "_id": "684ae204dbd21a9cc27b0fc0",
          "name": "Sangpil Kim",
          "hidden": false
        },
        {
          "_id": "684ae204dbd21a9cc27b0fc1",
          "name": "Hyunhee Park",
          "hidden": false
        },
        {
          "_id": "684ae204dbd21a9cc27b0fc2",
          "name": "Seungryong Kim",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-11T17:59:46.000Z",
      "submittedOnDailyAt": "2025-06-13T00:32:01.285Z",
      "title": "Liste des Images Intéressantes Utilisant le Modèle de Diffusion",
      "submittedOnDailyBy": {
        "_id": "66012e9c9e1cf5eb41ee0c4c",
        "avatarUrl": "/avatars/b07240cb86315b9e33d14677e02e4024.svg",
        "isPro": false,
        "fullname": "Jaewon Min",
        "user": "Min-Jaewon",
        "type": "user"
      },
      "summary": "L'objectif de la raffination d'images est de récupérer des images détériorées. Cependant, les méthodes de raffination basées sur la diffusion actuelles ont réussi de manière significative à raffiner des images naturelles, mais ont rencontré des difficultés pour reconstruire de manière précise les zones de texte dans des images détériorées. Ces méthodes génèrent des motifs similaires à ceux de texte, ce qui est connu sous le nom de \"bruits de texte\" et est considéré comme incorrect. Dans cet article, nous présentons l'Image de Texte Intéressant (TAIR) pour la raffination d'images. C'est une nouvelle tâche de raffination qui récupère à la fois le contenu visuel et la précision du texte. Pour résoudre ce problème, nous proposons SA-Text, qui est un cadre de référence de haute qualité avec 100K images de scène, où les différentes instances de texte ont été annotées de manière dense et complexe. De plus, nous proposons TeReDiff, un cadre de diffusion multi-tâche qui intègre les caractéristiques internes du modèle de diffusion dans un module de filtrage de texte, ce qui permet à deux composants de tirer parti des avantages lors de l'entraînement conjoint. De cette manière, des représentations riches de texte sont extraites et utilisées comme prompts dans les étapes subséquentes de desdenoise. Les expériences étendues montrent que notre approche dépasse les méthodes de raffination les plus avancées de manière constante et a montré un impact significatif sur la précision de la reconnaissance de texte. Pour plus d'informations, consultez le site web du projet : https://cvlab-kaist.github.io/TAIR/",
      "upvotes": 28,
      "discussionId": "684ae204dbd21a9cc27b0fc5",
      "projectPage": "https://cvlab-kaist.github.io/TAIR/",
      "githubRepo": "https://github.com/cvlab-kaist/TAIR",
      "ai_summary": "The proposed Text-Aware Image Restoration (TAIR) system integrates a multi-task diffusion framework with a text-spotting module to enhance both image recovery and textual fidelity, outperforming existing diffusion-based methods.",
      "ai_keywords": [
        "diffusion-based restoration",
        "text-image hallucination",
        "Text-Aware Image Restoration (TAIR)",
        "SA-Text",
        "multi-task diffusion framework",
        "TeReDiff",
        "text-spotting module",
        "text recognition accuracy"
      ]
    },
    "publishedAt": "2025-06-11T13:59:46.000Z",
    "title": "Text-Aware Image Restoration with Diffusion Models",
    "summary": "Image restoration aims to recover degraded images. However, existing\ndiffusion-based restoration methods, despite great success in natural image\nrestoration, often struggle to faithfully reconstruct textual regions in\ndegraded images. Those methods frequently generate plausible but incorrect\ntext-like patterns, a phenomenon we refer to as text-image hallucination. In\nthis paper, we introduce Text-Aware Image Restoration (TAIR), a novel\nrestoration task that requires the simultaneous recovery of visual contents and\ntextual fidelity. To tackle this task, we present SA-Text, a large-scale\nbenchmark of 100K high-quality scene images densely annotated with diverse and\ncomplex text instances. Furthermore, we propose a multi-task diffusion\nframework, called TeReDiff, that integrates internal features from diffusion\nmodels into a text-spotting module, enabling both components to benefit from\njoint training. This allows for the extraction of rich text representations,\nwhich are utilized as prompts in subsequent denoising steps. Extensive\nexperiments demonstrate that our approach consistently outperforms\nstate-of-the-art restoration methods, achieving significant gains in text\nrecognition accuracy. See our project page: https://cvlab-kaist.github.io/TAIR/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09993.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66012e9c9e1cf5eb41ee0c4c",
      "avatarUrl": "/avatars/b07240cb86315b9e33d14677e02e4024.svg",
      "fullname": "Jaewon Min",
      "name": "Min-Jaewon",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.10857",
      "authors": [
        {
          "_id": "684b817e3b733ba333686f95",
          "user": {
            "_id": "64b89a14cf14c2fabe96664c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/tEd3fBjMcEubF4plqzcUz.jpeg",
            "isPro": false,
            "fullname": "Jiashuo Yu",
            "user": "awojustin",
            "type": "user"
          },
          "name": "Jiashuo Yu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:39:55.618Z",
          "hidden": false
        },
        {
          "_id": "684b817e3b733ba333686f96",
          "name": "Yue Wu",
          "hidden": false
        },
        {
          "_id": "684b817e3b733ba333686f97",
          "name": "Meng Chu",
          "hidden": false
        },
        {
          "_id": "684b817e3b733ba333686f98",
          "name": "Zhifei Ren",
          "hidden": false
        },
        {
          "_id": "684b817e3b733ba333686f99",
          "name": "Zizheng Huang",
          "hidden": false
        },
        {
          "_id": "684b817e3b733ba333686f9a",
          "user": {
            "_id": "64c9beb2904317f42de06dd8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c9beb2904317f42de06dd8/he3rxfyzfwEd1vLuK6_o2.jpeg",
            "isPro": false,
            "fullname": "Pei Chu",
            "user": "chupei",
            "type": "user"
          },
          "name": "Pei Chu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:39:51.884Z",
          "hidden": false
        },
        {
          "_id": "684b817e3b733ba333686f9b",
          "name": "Ruijie Zhang",
          "hidden": false
        },
        {
          "_id": "684b817e3b733ba333686f9c",
          "name": "Yinan He",
          "hidden": false
        },
        {
          "_id": "684b817e3b733ba333686f9d",
          "name": "Qirui Li",
          "hidden": false
        },
        {
          "_id": "684b817e3b733ba333686f9e",
          "user": {
            "_id": "64acbbd51aee69ece03c6c0c",
            "avatarUrl": "/avatars/604df1cabc5faeda55022ae4c1997e56.svg",
            "isPro": false,
            "fullname": "Songze Li",
            "user": "LarryLee",
            "type": "user"
          },
          "name": "Songze Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:39:53.776Z",
          "hidden": false
        },
        {
          "_id": "684b817e3b733ba333686f9f",
          "name": "Zhenxiang Li",
          "hidden": false
        },
        {
          "_id": "684b817e3b733ba333686fa0",
          "name": "Zhongying Tu",
          "hidden": false
        },
        {
          "_id": "684b817e3b733ba333686fa1",
          "name": "Conghui He",
          "hidden": false
        },
        {
          "_id": "684b817e3b733ba333686fa2",
          "name": "Yu Qiao",
          "hidden": false
        },
        {
          "_id": "684b817e3b733ba333686fa3",
          "name": "Yali Wang",
          "hidden": false
        },
        {
          "_id": "684b817e3b733ba333686fa4",
          "name": "Yi Wang",
          "hidden": false
        },
        {
          "_id": "684b817e3b733ba333686fa5",
          "name": "Limin Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-12T16:17:17.000Z",
      "submittedOnDailyAt": "2025-06-13T00:10:47.082Z",
      "title": "VRBench : Indicateur de performance pour des vidéos longues de plusieurs stades appropriés",
      "submittedOnDailyBy": {
        "_id": "64b89a14cf14c2fabe96664c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/tEd3fBjMcEubF4plqzcUz.jpeg",
        "isPro": false,
        "fullname": "Jiashuo Yu",
        "user": "awojustin",
        "type": "user"
      },
      "summary": "VRBench est le premier large-scale benchmark de rendu neuronal pour évaluer la capacité d'inférence multi-niveau de grands modèles. Actuellement, il résout les déficiences dans l'inférence temporelle et la justification des procédures lors des évaluations. Il comprend 1,010 longs vidéos, 9,468 paires de questions et réponses multi-niveau avec des étiquettes de labelisation humaine et 30,292 tracés temporels de justification. Ces vidéos ont été filtrées par un processus de filtrage multi-niveau pour prioriser la cohérence du scénario. Il développe un cadre de collaboration entre l'humain et l'IA, et génère un cours de justification unique qui nécessite plusieurs étapes temporelles et comprend 7 types de justifications (par exemple, explication d'événements, histoire cachée). VRBench concevoit un processus d'évaluation multi-niveau pour évaluer à la fois les résultats et les processus. Il propose un métrique de score de guides de LLM multi-niveau pour évaluer de manière structurée la qualité des cours de justification, non seulement la réponse finale MCQ. Il valide 12 modèles de LLM et 16 de VLM sur VRBench, et effectue des analyses détaillées pour fournir des conseils précieux dans la zone de l'inférence multi-niveau.",
      "upvotes": 23,
      "discussionId": "684b817e3b733ba333686fa6",
      "projectPage": "https://vrbench.github.io/",
      "githubRepo": "https://github.com/OpenGVLab/VRBench",
      "ai_summary": "VRBench is a long narrative video benchmark designed to evaluate models' multi-step reasoning and procedural validity through human-labeled question-answering pairs and a human-AI collaborative framework with a multi-phase evaluation pipeline.",
      "ai_keywords": [
        "VRBench",
        "multi-step reasoning",
        "temporal reasoning",
        "procedural validity",
        "long videos",
        "human-labeled",
        "multi-step question-answering",
        "expert inter-rater reviewing",
        "coherent reasoning chains",
        "event attribution",
        "implicit inference",
        "multi-phase evaluation",
        "progress-level LLM-guided scoring metric",
        "LLMs",
        "VLMs"
      ]
    },
    "publishedAt": "2025-06-12T12:17:17.000Z",
    "title": "VRBench: A Benchmark for Multi-Step Reasoning in Long Narrative Videos",
    "summary": "We present VRBench, the first long narrative video benchmark crafted for\nevaluating large models' multi-step reasoning capabilities, addressing\nlimitations in existing evaluations that overlook temporal reasoning and\nprocedural validity. It comprises 1,010 long videos (with an average duration\nof 1.6 hours), along with 9,468 human-labeled multi-step question-answering\npairs and 30,292 reasoning steps with timestamps. These videos are curated via\na multi-stage filtering process including expert inter-rater reviewing to\nprioritize plot coherence. We develop a human-AI collaborative framework that\ngenerates coherent reasoning chains, each requiring multiple temporally\ngrounded steps, spanning seven types (e.g., event attribution, implicit\ninference). VRBench designs a multi-phase evaluation pipeline that assesses\nmodels at both the outcome and process levels. Apart from the MCQs for the\nfinal results, we propose a progress-level LLM-guided scoring metric to\nevaluate the quality of the reasoning chain from multiple dimensions\ncomprehensively. Through extensive evaluations of 12 LLMs and 16 VLMs on\nVRBench, we undertake a thorough analysis and provide valuable insights that\nadvance the field of multi-step reasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10857.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b89a14cf14c2fabe96664c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/tEd3fBjMcEubF4plqzcUz.jpeg",
      "fullname": "Jiashuo Yu",
      "name": "awojustin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.10540",
      "authors": [
        {
          "_id": "684bad683b733ba3336870b6",
          "user": {
            "_id": "652fb8bcc9dd2692a25ef2e3",
            "avatarUrl": "/avatars/461e6cc1c3441cde18192b080b0b8576.svg",
            "isPro": false,
            "fullname": "Haoyuan Shi",
            "user": "MrSunshy",
            "type": "user"
          },
          "name": "Haoyuan Shi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:38:52.713Z",
          "hidden": false
        },
        {
          "_id": "684bad683b733ba3336870b7",
          "user": {
            "_id": "62fdb01bc1588e1d4c6c1a7c",
            "avatarUrl": "/avatars/bd03085995b1c34e0ac8a845cf2c4e83.svg",
            "isPro": false,
            "fullname": "Yunxin Li",
            "user": "YunxinLi",
            "type": "user"
          },
          "name": "Yunxin Li",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-13T04:47:39.539Z",
          "hidden": false
        },
        {
          "_id": "684bad683b733ba3336870b8",
          "name": "Xinyu Chen",
          "hidden": false
        },
        {
          "_id": "684bad683b733ba3336870b9",
          "name": "Longyue Wang",
          "hidden": false
        },
        {
          "_id": "684bad683b733ba3336870ba",
          "name": "Baotian Hu",
          "hidden": false
        },
        {
          "_id": "684bad683b733ba3336870bb",
          "name": "Min Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-12T10:06:21.000Z",
      "submittedOnDailyAt": "2025-06-13T03:26:17.710Z",
      "title": "AniMaker : Automatisation de l'histoire et de la création de clips par une multi-agente d'animation de MCTS Droid",
      "submittedOnDailyBy": {
        "_id": "62fdb01bc1588e1d4c6c1a7c",
        "avatarUrl": "/avatars/bd03085995b1c34e0ac8a845cf2c4e83.svg",
        "isPro": false,
        "fullname": "Yunxin Li",
        "user": "YunxinLi",
        "type": "user"
      },
      "summary": "Bien que le développement rapide des modèles de TV ne cesse de surprendre, la combinaison de nombreux scénarios et personnages pour créer des histoires continues est un défi. Les méthodes actuelles transforment les cadres préfabriqués en clips de longueur fixe, générant des problèmes de continuité et de pagination dans les neulages continus. De plus, l'incertitude interne des modèles de TV peut significativement affecter la cohérence logique et la continuité visuelle de l'animation complète. Pour surmonter ces obstacles, nous présentons AniMaker. AniMaker est un cadre de travail multi-agent qui permet une génération efficace de multiples clips et la sélection de clips liés à l'histoire. Ce cadre de travail est composé d'un agent directeur pour la génération de scénarios, d'un agent photographique pour la création de clips, d'un agent d'évaluation et d'un agent de postproduction pour l'édition et les révisions de voix. L'approche clé de AniMaker sont les deux composants technologiques : MCTS-Gen, une stratégie d'exploration des espaces candidats basée sur la Recherche de Trees de Monte Carlo (MCTS) pour générer des clips de haute qualité, et AniEval, un cadre de travail spécialisé dans l'évaluation des animations continues, qui considère le contexte des clips précédents et suivants pour évaluer des aspects tels que la cohérence narrative, la complétude des actions et les caractéristiques uniques de l'animation. Les expériences, réalisées avec VBench et le cadre de travail AniEval proposé, montrent un améliorament significatif dans l'efficacité de la génération de multiples clips et ont démontré que la production d'animation de histoire générée par l'IA approche les normes de production.",
      "upvotes": 23,
      "discussionId": "684bad683b733ba3336870bc",
      "ai_summary": "AniMaker, a multi-agent framework using MCTS-Gen and AniEval, generates coherent storytelling videos from text input, outperforming existing models with better quality and efficiency.",
      "ai_keywords": [
        "multi-agent framework",
        "Director Agent",
        "Photography Agent",
        "Reviewer Agent",
        "Post-Production Agent",
        "Monte Carlo Tree Search (MCTS)",
        "AniEval",
        "VBench",
        "action completion",
        "story-level consistency",
        "animation-specific features"
      ]
    },
    "publishedAt": "2025-06-12T06:06:21.000Z",
    "title": "AniMaker: Automated Multi-Agent Animated Storytelling with MCTS-Driven\n  Clip Generation",
    "summary": "Despite rapid advancements in video generation models, generating coherent\nstorytelling videos that span multiple scenes and characters remains\nchallenging. Current methods often rigidly convert pre-generated keyframes into\nfixed-length clips, resulting in disjointed narratives and pacing issues.\nFurthermore, the inherent instability of video generation models means that\neven a single low-quality clip can significantly degrade the entire output\nanimation's logical coherence and visual continuity. To overcome these\nobstacles, we introduce AniMaker, a multi-agent framework enabling efficient\nmulti-candidate clip generation and storytelling-aware clip selection, thus\ncreating globally consistent and story-coherent animation solely from text\ninput. The framework is structured around specialized agents, including the\nDirector Agent for storyboard generation, the Photography Agent for video clip\ngeneration, the Reviewer Agent for evaluation, and the Post-Production Agent\nfor editing and voiceover. Central to AniMaker's approach are two key technical\ncomponents: MCTS-Gen in Photography Agent, an efficient Monte Carlo Tree Search\n(MCTS)-inspired strategy that intelligently navigates the candidate space to\ngenerate high-potential clips while optimizing resource usage; and AniEval in\nReviewer Agent, the first framework specifically designed for multi-shot\nanimation evaluation, which assesses critical aspects such as story-level\nconsistency, action completion, and animation-specific features by considering\neach clip in the context of its preceding and succeeding clips. Experiments\ndemonstrate that AniMaker achieves superior quality as measured by popular\nmetrics including VBench and our proposed AniEval framework, while\nsignificantly improving the efficiency of multi-candidate generation, pushing\nAI-generated storytelling animation closer to production standards.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10540.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "62fdb01bc1588e1d4c6c1a7c",
      "avatarUrl": "/avatars/bd03085995b1c34e0ac8a845cf2c4e83.svg",
      "fullname": "Yunxin Li",
      "name": "YunxinLi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.10952",
      "authors": [
        {
          "_id": "684b96403b733ba33368703a",
          "user": {
            "_id": "65e808ed7c10574cc3f8e363",
            "avatarUrl": "/avatars/ed10759d354e271bfc15afd946b66b4a.svg",
            "isPro": false,
            "fullname": "zhangmozhi",
            "user": "mzzhang",
            "type": "user"
          },
          "name": "Mozhi Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:39:33.791Z",
          "hidden": false
        },
        {
          "_id": "684b96403b733ba33368703b",
          "user": {
            "_id": "6718fc605e14ff6b94a7109f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6718fc605e14ff6b94a7109f/uz2O2F_qbY3wefpY548qS.jpeg",
            "isPro": false,
            "fullname": "Howe Tissue",
            "user": "Howe77",
            "type": "user"
          },
          "name": "Howe Tissue",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:39:31.965Z",
          "hidden": false
        },
        {
          "_id": "684b96403b733ba33368703c",
          "name": "Lu Wang",
          "hidden": false
        },
        {
          "_id": "684b96403b733ba33368703d",
          "name": "Xipeng Qiu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-12T17:53:51.000Z",
      "submittedOnDailyAt": "2025-06-13T01:43:47.223Z",
      "title": "Vectores de domaine 2 : Vectorisation de jeux de données qui cherchent une mixte optimale de données sans entraînement",
      "submittedOnDailyBy": {
        "_id": "6718fc605e14ff6b94a7109f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6718fc605e14ff6b94a7109f/uz2O2F_qbY3wefpY548qS.jpeg",
        "isPro": false,
        "fullname": "Howe Tissue",
        "user": "Howe77",
        "type": "user"
      },
      "summary": "Domain2Vec est un nouvel approche qui utilise la combinaison linéaire de méta-domaines (meta-domains), un concept nouveau, pour décomposer des ensembles de données. Cette approche est conçue pour explorer les caractéristiques cachées des ensembles de données. Domain2Vec utilise un classifieur de classes qui décompose un ensemble de données donné en vecteurs de domaines correspondant à un tableau de méta-domaines, tout en maintenant le tableau de méta-domaines. Ces vecteurs de domaines peuvent être utilisés efficacement pour identifier la meilleure combinaison de données et améliorer le rendement des modèles pré-entraînés de langage (LM), sans nécessité d'entraînement supplémentaire, en se basant sur la distribution assumée des données (DA^2). Cette distribution assumée montre que une concordance plus élevée dans la distribution des données entre l'ensemble d'entraînement et l'ensemble de validation conduit à une perte de validation plus faible. De plus, Domain2Vec peut modéliser de manière indiscriminante la relation entre vecteurs de domaines et le rendement de LM, améliorant significativement l'efficacité et la scalabilité des méthodes précédentes. Les expériences larges montrent que Domain2Vec peut trouver des combinaisons de données qui améliorent le rendement des tâches de bas niveau avec un minimum de chargement informatique. En particulier, avec une combinaison de Pile-CC, le pourcentage de calcul nécessaire pour l'entraînement est réduit à 51,5 %, atteignant la même perte de validation et améliorant en moyenne le rendement des tâches de bas niveau d'un 2,83 %.",
      "upvotes": 14,
      "discussionId": "684b96413b733ba33368703e",
      "ai_summary": "Domain2Vec decomposes datasets into meta-domains to optimize language model pretraining and downstream performance with reduced computational cost.",
      "ai_keywords": [
        "Domain2Vec",
        "meta-domains",
        "domain vector",
        "distribution alignment assumption",
        "DA²",
        "language model",
        "pretraining",
        "downstream task performance",
        "Pile-CC",
        "The Pile dataset"
      ]
    },
    "publishedAt": "2025-06-12T13:53:51.000Z",
    "title": "Domain2Vec: Vectorizing Datasets to Find the Optimal Data Mixture\n  without Training",
    "summary": "We introduce~Domain2Vec, a novel approach that decomposes any\ndataset into a linear combination of several meta-domains, a new concept\ndesigned to capture the key underlying features of datasets.\nDomain2Vec maintains a vocabulary of meta-domains and uses a\nclassifier to decompose any given dataset into a domain vector that corresponds\nto a distribution over this vocabulary. These domain vectors enable the\nidentification of the optimal data mixture for language model (LM) pretraining\nin a training-free manner under the \\textbf{Distribution\nAlignment Assumption} (DA^{2}), which suggests that when\nthe data distributions of the training set and the validation set are better\naligned, a lower validation loss is achieved. Moreover, Domain2vec can\nbe seamlessly integrated into previous works to model the relationship between\ndomain vectors and LM performance, greatly enhancing the efficiency and\nscalability of previous methods. Extensive experiments demonstrate that\nDomain2Vec helps find the data mixture that enhances downstream task\nperformance with minimal computational overhead. Specifically,\nDomain2Vec achieves the same validation loss on Pile-CC using only\n51.5% of the computation required when training on the original mixture of\nThe Pile dataset. Under equivalent compute budget, Domain2Vec improves\ndownstream performance by an average of 2.83%.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10952.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6718fc605e14ff6b94a7109f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6718fc605e14ff6b94a7109f/uz2O2F_qbY3wefpY548qS.jpeg",
      "fullname": "Howe Tissue",
      "name": "Howe77",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.10357",
      "authors": [
        {
          "_id": "684b86bf3b733ba333686fbe",
          "name": "Zaijing Li",
          "hidden": false
        },
        {
          "_id": "684b86bf3b733ba333686fbf",
          "name": "Yuquan Xie",
          "hidden": false
        },
        {
          "_id": "684b86bf3b733ba333686fc0",
          "name": "Rui Shao",
          "hidden": false
        },
        {
          "_id": "684b86bf3b733ba333686fc1",
          "name": "Gongwei Chen",
          "hidden": false
        },
        {
          "_id": "684b86bf3b733ba333686fc2",
          "name": "Weili Guan",
          "hidden": false
        },
        {
          "_id": "684b86bf3b733ba333686fc3",
          "name": "Dongmei Jiang",
          "hidden": false
        },
        {
          "_id": "684b86bf3b733ba333686fc4",
          "name": "Liqiang Nie",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-12T05:29:40.000Z",
      "submittedOnDailyAt": "2025-06-13T00:37:48.793Z",
      "title": "Optimus-3 : Utilise un explorateur de tâches échelonnables pour un agent microsoft assembler multi-type manuel généralement",
      "submittedOnDailyBy": {
        "_id": "66b45fe75d0ac130d7d82764",
        "avatarUrl": "/avatars/09253f41f82c533b36199f82620cd075.svg",
        "isPro": false,
        "fullname": "Zaijing Li",
        "user": "dawn0815",
        "type": "user"
      },
      "summary": "Récemment, les modèles de langage multimodal (MLLM) basés sur des agents ont réalisé des progrès impressionnants dans plusieurs domaines. Cependant, la construction d'agents généraux possédant des fonctions communes telles que le reconnaissance visuelle, la planification, l'action, la fondation et la réflexion dans des environnements ouverts tels que les jeux d'état ou Minecraft est un problème complexe : il y a une pénurie de données spécifiques, des interférences entre des tâches différentes et une diversité visuelle dans les environnements ouverts. Dans cet article, sont proposées trois contributions principales pour résoudre ces problèmes. 1) On propose une pipeline de génération de données pour fournir des données de haute qualité et extensibles pour le développement des agents. 2) Pour atténuer les interférences entre des tâches, on introduit une architecture Mixture-of-Experts (MoE) en utilisant un routage à l'échelle des tâches. 3) On développe une approche de Reinforcement Learning Augmenté avec une raisonnement multimodal pour améliorer la capacité d'inférence visuelle de l'agent dans Minecraft. Sur la base de ces innovations, on présente Optimus-3, un agent général pour Minecraft. À travers des résultats expérimentaux étendus, on montre que Optimus-3 dépasse les agents généraux de MLLM et les meilleurs agents actuels dans différentes tâches dans l'environnement de Minecraft. Page du projet : https://cybertronagent.github.io/Optimus-3.github.io/",
      "upvotes": 12,
      "discussionId": "684b86bf3b733ba333686fc5",
      "projectPage": "https://cybertronagent.github.io/Optimus-3.github.io/",
      "githubRepo": "https://github.com/JiuTian-VL/Optimus-3",
      "ai_summary": "Optimus-3, a multimodal large language model agent, uses knowledge-enhanced data generation, a Mixture-of-Experts architecture, and multimodal reasoning-augmented reinforcement learning to achieve superior performance across various tasks in Minecraft.",
      "ai_keywords": [
        "multimodal large language models",
        "knowledge-enhanced data generation",
        "Mixture-of-Experts",
        "task-level routing",
        "multimodal reasoning-augmented reinforcement learning"
      ]
    },
    "publishedAt": "2025-06-12T01:29:40.000Z",
    "title": "Optimus-3: Towards Generalist Multimodal Minecraft Agents with Scalable\n  Task Experts",
    "summary": "Recently, agents based on multimodal large language models (MLLMs) have\nachieved remarkable progress across various domains. However, building a\ngeneralist agent with capabilities such as perception, planning, action,\ngrounding, and reflection in open-world environments like Minecraft remains\nchallenges: insufficient domain-specific data, interference among heterogeneous\ntasks, and visual diversity in open-world settings. In this paper, we address\nthese challenges through three key contributions. 1) We propose a\nknowledge-enhanced data generation pipeline to provide scalable and\nhigh-quality training data for agent development. 2) To mitigate interference\namong heterogeneous tasks, we introduce a Mixture-of-Experts (MoE) architecture\nwith task-level routing. 3) We develop a Multimodal Reasoning-Augmented\nReinforcement Learning approach to enhance the agent's reasoning ability for\nvisual diversity in Minecraft. Built upon these innovations, we present\nOptimus-3, a general-purpose agent for Minecraft. Extensive experimental\nresults demonstrate that Optimus-3 surpasses both generalist multimodal large\nlanguage models and existing state-of-the-art agents across a wide range of\ntasks in the Minecraft environment. Project page:\nhttps://cybertronagent.github.io/Optimus-3.github.io/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10357.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66b45fe75d0ac130d7d82764",
      "avatarUrl": "/avatars/09253f41f82c533b36199f82620cd075.svg",
      "fullname": "Zaijing Li",
      "name": "dawn0815",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.10974",
      "authors": [
        {
          "_id": "684b8e193b733ba333687028",
          "user": {
            "_id": "6241749cf80bd930bd99f3dd",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669210243382-6241749cf80bd930bd99f3dd.jpeg",
            "isPro": false,
            "fullname": "Ou Yixin",
            "user": "OE-Heart",
            "type": "user"
          },
          "name": "Yixin Ou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:39:36.148Z",
          "hidden": false
        },
        {
          "_id": "684b8e193b733ba333687029",
          "name": "Yujie Luo",
          "hidden": false
        },
        {
          "_id": "684b8e193b733ba33368702a",
          "name": "Jingsheng Zheng",
          "hidden": false
        },
        {
          "_id": "684b8e193b733ba33368702b",
          "name": "Lanning Wei",
          "hidden": false
        },
        {
          "_id": "684b8e193b733ba33368702c",
          "name": "Shuofei Qiao",
          "hidden": false
        },
        {
          "_id": "684b8e193b733ba33368702d",
          "name": "Jintian Zhang",
          "hidden": false
        },
        {
          "_id": "684b8e193b733ba33368702e",
          "name": "Da Zheng",
          "hidden": false
        },
        {
          "_id": "684b8e193b733ba33368702f",
          "name": "Huajun Chen",
          "hidden": false
        },
        {
          "_id": "684b8e193b733ba333687030",
          "user": {
            "_id": "620b3bbb0668e435407c8d0a",
            "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
            "isPro": false,
            "fullname": "Ningyu Zhang",
            "user": "Ningyu",
            "type": "user"
          },
          "name": "Ningyu Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:39:37.984Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-12T17:59:32.000Z",
      "submittedOnDailyAt": "2025-06-13T03:44:21.173Z",
      "title": "AutoMind : Agente adaptatif de connaissance pour la science des données automatique",
      "submittedOnDailyBy": {
        "_id": "620b3bbb0668e435407c8d0a",
        "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
        "isPro": false,
        "fullname": "Ningyu Zhang",
        "user": "Ningyu",
        "type": "user"
      },
      "summary": "Les modèles de langage grand (LLM) ont démontré un potentiel pour résoudre des problèmes de données scientifiques réels. Les agents de données scientifiques dirigés par LLM promettent d'automatiser toutes les couches d'apprentissage automatique, mais leur efficacité est limitée. Les cadres actuels dépendent de flux de travail stricts et de stratégies de codification immuables, montrant un excellent rendement sur des problèmes relativement simples mais ne captant pas le savoir empirique humain pour des tâches complexes innovantes. Dans cette étude, nous présentons \"AutoMind\", un cadre d'agent automatisé de connaissance qui surpasse ces limites grâce à trois innovations clés : (1) basé sur le savoir des experts, (2) algorithmes de recherche d'arbre de connaissance de l'agent, et (3) stratégies de codification automatiques. Évalués dans deux cadres de tests de données scientifiques automatiques, AutoMind a démontré un comportement supérieur par rapport aux limites avancées. Les analyses supplémentaires ont confirmé l'excellence en validité, efficacité et qualité des solutions qualitatives, établissant clairement que AutoMind joue un rôle approprié et puissant en tant que étape pour la données scientifiques complètement automatiques.",
      "upvotes": 10,
      "discussionId": "684b8e193b733ba333687031",
      "ai_summary": "AutoMind, a flexible and knowledgeable LLM-agent framework, improves automated data science through expert knowledge integration, strategic solution exploration, and adaptive coding, outperforming existing systems.",
      "ai_keywords": [
        "LLM",
        "data science agents",
        "machine learning pipeline",
        "expert knowledge base",
        "agentic knowledgeable tree search",
        "self-adaptive coding strategy"
      ]
    },
    "publishedAt": "2025-06-12T13:59:32.000Z",
    "title": "AutoMind: Adaptive Knowledgeable Agent for Automated Data Science",
    "summary": "Large Language Model (LLM) agents have shown great potential in addressing\nreal-world data science problems. LLM-driven data science agents promise to\nautomate the entire machine learning pipeline, yet their real-world\neffectiveness remains limited. Existing frameworks depend on rigid, pre-defined\nworkflows and inflexible coding strategies; consequently, they excel only on\nrelatively simple, classical problems and fail to capture the empirical\nexpertise that human practitioners bring to complex, innovative tasks. In this\nwork, we introduce AutoMind, an adaptive, knowledgeable LLM-agent framework\nthat overcomes these deficiencies through three key advances: (1) a curated\nexpert knowledge base that grounds the agent in domain expert knowledge, (2) an\nagentic knowledgeable tree search algorithm that strategically explores\npossible solutions, and (3) a self-adaptive coding strategy that dynamically\ntailors code generation to task complexity. Evaluations on two automated data\nscience benchmarks demonstrate that AutoMind delivers superior performance\nversus state-of-the-art baselines. Additional analyses confirm favorable\neffectiveness, efficiency, and qualitative solution quality, highlighting\nAutoMind as an efficient and robust step toward fully automated data science.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10974.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "620b3bbb0668e435407c8d0a",
      "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
      "fullname": "Ningyu Zhang",
      "name": "Ningyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 23
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.10960",
      "authors": [
        {
          "_id": "684bb33a3b733ba3336870c5",
          "name": "Kangwei Liu",
          "hidden": false
        },
        {
          "_id": "684bb33a3b733ba3336870c6",
          "name": "Siyuan Cheng",
          "hidden": false
        },
        {
          "_id": "684bb33a3b733ba3336870c7",
          "name": "Bozhong Tian",
          "hidden": false
        },
        {
          "_id": "684bb33a3b733ba3336870c8",
          "name": "Xiaozhuan Liang",
          "hidden": false
        },
        {
          "_id": "684bb33a3b733ba3336870c9",
          "name": "Yuyang Yin",
          "hidden": false
        },
        {
          "_id": "684bb33a3b733ba3336870ca",
          "name": "Meng Han",
          "hidden": false
        },
        {
          "_id": "684bb33a3b733ba3336870cb",
          "user": {
            "_id": "620b3bbb0668e435407c8d0a",
            "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
            "isPro": false,
            "fullname": "Ningyu Zhang",
            "user": "Ningyu",
            "type": "user"
          },
          "name": "Ningyu Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:38:50.549Z",
          "hidden": false
        },
        {
          "_id": "684bb33a3b733ba3336870cc",
          "name": "Bryan Hooi",
          "hidden": false
        },
        {
          "_id": "684bb33a3b733ba3336870cd",
          "user": {
            "_id": "635113fdcba4ff2e81cb236e",
            "avatarUrl": "/avatars/f80df906b722b4901debce9baa867073.svg",
            "isPro": false,
            "fullname": "chen",
            "user": "Jasonchen123",
            "type": "user"
          },
          "name": "Xi Chen",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-13T05:12:27.526Z",
          "hidden": false
        },
        {
          "_id": "684bb33a3b733ba3336870ce",
          "name": "Shumin Deng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-12T17:57:05.000Z",
      "submittedOnDailyAt": "2025-06-13T03:43:12.055Z",
      "title": "Chine : Benchmark pour la détection de contenu nuisible",
      "submittedOnDailyBy": {
        "_id": "620b3bbb0668e435407c8d0a",
        "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
        "isPro": false,
        "fullname": "Ningyu Zhang",
        "user": "Ningyu",
        "type": "user"
      },
      "summary": "Les modèles de langage grands (LLMs) ont été automatisés pour la détection de contenus préjudiciables. Ils aident les développeurs à reconnaître les violations de politiques et à améliorer l'efficacité et la précision générale de la révision de contenu, bien que les ressources actuelles de détection de contenus préjudiciables se concentrent principalement sur l'anglais, et les ensembles de données en chinois sont rares et limités. Nous présentons un benchmark professionnellement expliqué qui comprend six catégories représentatives, construit avec des données réelles. Ce processus d'explication permet aux LLMs d'obtenir des connaissances spécialisées claires qui les aideront à la détection de contenus préjudiciables en chinois. De plus, nous proposons une ligne de connaissance additive basée sur des règles humaines et sur le connaissance potentielle obtenue des LLMs, ce qui permettra aux petits modèles d'atteindre un rendement comparable aux plus avancés. Les codes et les données sont disponibles sur https://github.com/zjunlp/ChineseHarm-bench.",
      "upvotes": 9,
      "discussionId": "684bb33a3b733ba3336870cf",
      "ai_summary": "A benchmark for Chinese harmful content detection is introduced, along with a knowledge-augmented model that enhances efficiency and accuracy using human-annotated rules and LLMs.",
      "ai_keywords": [
        "large language models",
        "LLMs",
        "harmful content detection",
        "knowledge-augmented baseline",
        "annotation process",
        "knowledge rule base",
        "Chinese datasets"
      ]
    },
    "publishedAt": "2025-06-12T13:57:05.000Z",
    "title": "ChineseHarm-Bench: A Chinese Harmful Content Detection Benchmark",
    "summary": "Large language models (LLMs) have been increasingly applied to automated\nharmful content detection tasks, assisting moderators in identifying policy\nviolations and improving the overall efficiency and accuracy of content review.\nHowever, existing resources for harmful content detection are predominantly\nfocused on English, with Chinese datasets remaining scarce and often limited in\nscope. We present a comprehensive, professionally annotated benchmark for\nChinese content harm detection, which covers six representative categories and\nis constructed entirely from real-world data. Our annotation process further\nyields a knowledge rule base that provides explicit expert knowledge to assist\nLLMs in Chinese harmful content detection. In addition, we propose a\nknowledge-augmented baseline that integrates both human-annotated knowledge\nrules and implicit knowledge from large language models, enabling smaller\nmodels to achieve performance comparable to state-of-the-art LLMs. Code and\ndata are available at https://github.com/zjunlp/ChineseHarm-bench.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10960.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "620b3bbb0668e435407c8d0a",
      "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
      "fullname": "Ningyu Zhang",
      "name": "Ningyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 23
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.10821",
      "authors": [
        {
          "_id": "684b91c73b733ba333687033",
          "name": "Huaying Yuan",
          "hidden": false
        },
        {
          "_id": "684b91c73b733ba333687034",
          "name": "Zheng Liu",
          "hidden": false
        },
        {
          "_id": "684b91c73b733ba333687035",
          "name": "Junjie Zhou",
          "hidden": false
        },
        {
          "_id": "684b91c73b733ba333687036",
          "name": "Ji-Rong Wen",
          "hidden": false
        },
        {
          "_id": "684b91c73b733ba333687037",
          "name": "Zhicheng Dou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-12T15:39:10.000Z",
      "submittedOnDailyAt": "2025-06-13T01:20:51.837Z",
      "title": "VideoDeepResearch : Utilisation d'une outil hors la route pour comprendre des vidéos longues",
      "submittedOnDailyBy": {
        "_id": "66d916a7b86f0d569aa19b60",
        "avatarUrl": "/avatars/2537cee66afecc2d999e05b01c78d319.svg",
        "isPro": false,
        "fullname": "huaying Yuan",
        "user": "avery00",
        "type": "user"
      },
      "summary": "La Compréhension de Vidéo à Long Terme (LVU) est un problème important pour les modèles de langage multimodal (MLLMs) actuels. Pour résoudre ce problème, il est généralement nécessaire un contexte d'extension, une forte capacité de reconnaissance visuelle et un modèle MLLM avec des connaissances spécialisées. Dans cet article, nous défions cette croyance commune et présentons un nouveau cadre d'agent appelé VideoDeepResearch. Notre approche combine un modèle de langage grand basé sur le texte unique (LRM) et un ensemble de modules de tools multimodal. Ce conjoint comprend un réflecteur multimodal et un reconnaissance visuelle reconnue, ce qui permet son utilisation pratique. Pour chaque tâche de LVU, le système construit des stratégies de résolution de problèmes à travers la logique et accède sélectivement au contenu visuel nécessaire pour utiliser les outils. Des expériences extensives ont été réalisées dans les populaires cadres d'évaluation LVU, comme MLVU (Test), LVBench et LongVideoBench. VideoDeepResearch a atteint des améliorations notables par rapport aux modèles MLLM actuels, avec des taux d'amélioration de 9,6%, 6,6% et 3,9% pour chacun des cadres d'évaluation. Ces résultats révèlent la possibilité que le système d'agents surpasse les problèmes critiques de LVU.",
      "upvotes": 9,
      "discussionId": "684b91c73b733ba333687038",
      "ai_summary": "VideoDeepResearch, a text-only reasoning model with modular tools, surpasses existing baselines in long video understanding tasks without extending context windows or enhancing visual perception capabilities.",
      "ai_keywords": [
        "long video understanding",
        "multi-modal large language models",
        "VideoDeepResearch",
        "text-only large reasoning model",
        "multimodal retrievers",
        "visual perceivers",
        "MLVU",
        "Video-MME",
        "LVBench",
        "LongVideoBench",
        "agentic systems"
      ]
    },
    "publishedAt": "2025-06-12T11:39:10.000Z",
    "title": "VideoDeepResearch: Long Video Understanding With Agentic Tool Using",
    "summary": "Long video understanding (LVU) presents a significant challenge for current\nmulti-modal large language models (MLLMs) due to the task's inherent complexity\nand context window constraint. It is widely assumed that addressing LVU tasks\nrequires foundation MLLMs with extended context windows, strong visual\nperception capabilities, and proficient domain expertise. In this work, we\nchallenge this common belief by introducing VideoDeepResearch, a novel agentic\nframework for long video understanding. Our approach relies solely on a\ntext-only large reasoning model (LRM) combined with a modular multi-modal\ntoolkit, including multimodal retrievers and visual perceivers, all of which\nare readily available in practice. For each LVU task, the system formulates a\nproblem-solving strategy through reasoning, while selectively accessing and\nutilizing essential video content via tool using. We conduct extensive\nexperiments on popular LVU benchmarks, including MLVU, Video-MME, and LVBench.\nOur results demonstrate that VideoDeepResearch achieves substantial\nimprovements over existing MLLM baselines, surpassing the previous\nstate-of-the-art by 9.6%, 6.6%, and 3.9% on MLVU (test), LVBench, and\nLongVideoBench, respectively. These findings highlight the promise of agentic\nsystems in overcoming key challenges in LVU problems.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10821.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66d916a7b86f0d569aa19b60",
      "avatarUrl": "/avatars/2537cee66afecc2d999e05b01c78d319.svg",
      "fullname": "huaying Yuan",
      "name": "avery00",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.10741",
      "authors": [
        {
          "_id": "684b881f3b733ba333686fd4",
          "user": {
            "_id": "64966691990b342dcc9fccb5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64966691990b342dcc9fccb5/tQSrE3MkBeakk5QYfgHSo.jpeg",
            "isPro": false,
            "fullname": "sixiang chen",
            "user": "Ephemeral182",
            "type": "user"
          },
          "name": "SiXiang Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:39:47.414Z",
          "hidden": false
        },
        {
          "_id": "684b881f3b733ba333686fd5",
          "name": "Jianyu Lai",
          "hidden": false
        },
        {
          "_id": "684b881f3b733ba333686fd6",
          "name": "Jialin Gao",
          "hidden": false
        },
        {
          "_id": "684b881f3b733ba333686fd7",
          "name": "Tian Ye",
          "hidden": false
        },
        {
          "_id": "684b881f3b733ba333686fd8",
          "name": "Haoyu Chen",
          "hidden": false
        },
        {
          "_id": "684b881f3b733ba333686fd9",
          "name": "Hengyu Shi",
          "hidden": false
        },
        {
          "_id": "684b881f3b733ba333686fda",
          "name": "Shitong Shao",
          "hidden": false
        },
        {
          "_id": "684b881f3b733ba333686fdb",
          "name": "Yunlong Lin",
          "hidden": false
        },
        {
          "_id": "684b881f3b733ba333686fdc",
          "name": "Song Fei",
          "hidden": false
        },
        {
          "_id": "684b881f3b733ba333686fdd",
          "name": "Zhaohu Xing",
          "hidden": false
        },
        {
          "_id": "684b881f3b733ba333686fde",
          "name": "Yeying Jin",
          "hidden": false
        },
        {
          "_id": "684b881f3b733ba333686fdf",
          "name": "Junfeng Luo",
          "hidden": false
        },
        {
          "_id": "684b881f3b733ba333686fe0",
          "name": "Xiaoming Wei",
          "hidden": false
        },
        {
          "_id": "684b881f3b733ba333686fe1",
          "name": "Lei Zhu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-12T14:28:12.000Z",
      "submittedOnDailyAt": "2025-06-13T04:30:38.214Z",
      "title": "PosterCraft : Révision sur la génération de posters d'art de haute qualité dans un cadre de travail unifié",
      "submittedOnDailyBy": {
        "_id": "66015e8aa4d296af07de538e",
        "avatarUrl": "/avatars/a1295c631cc2646282c545859975ce4c.svg",
        "isPro": false,
        "fullname": "Ye",
        "user": "Owen777",
        "type": "user"
      },
      "summary": "La génération de posters n'est pas seulement un défi de design graphique, mais une tâche plus complexe : nécessite un design d'images précises de texte, un ajustement infini de contenu artistique abstrait, une esthétique de design attrayante et une harmonie visuelle. Pour aborder ces défis, nous proposons PosterCraft, un cadre intégré qui permet aux modèles d'explorer et de générer des designs visuellement attrayants et cohérents sans avoir à faire partie d'un système modulaire ou avec des layouts prédéfinis. PosterCraft optimise la création de posters de haute qualité grâce à un flux de travail strict : (i) optimisation des images de texte avec le nouveau ensemble de données Text-Render-2M ; (ii) fine-tuning pour des zones spécifiques avec HQ-Poster100K ; (iii) optimisation du style artistique du texte ; et (iv) refinement de la communication visuelle. Chaque étape est soutenue par des formulaires de design de données adaptés à leurs besoins spécifiques, permettant une forte puissance d'entraînement, sauf pour des changements dans la complexité de l'architecture. PosterCraft, évalué par des multiples expériences, dépasse significativement la ligne ouverte et approche la qualité des systèmes commerciaux les plus avancés en termes de précision de rendu, cohérence du design et attractivité visuelle. Notre code, nos modèles et nos ensembles de données sont disponibles sur notre page du projet : https://ephemeral182.github.io/PosterCraft",
      "upvotes": 9,
      "discussionId": "684b881f3b733ba333686fe2",
      "projectPage": "https://ephemeral182.github.io/PosterCraft/",
      "githubRepo": "https://github.com/Ephemeral182/PosterCraft",
      "ai_summary": "PosterCraft improves aesthetic poster generation through a unified, modular pipeline with enhanced text rendering, region-aware fine-tuning, aesthetic reinforcement learning, and joint vision-language refinement.",
      "ai_keywords": [
        "text-rendering optimization",
        "Text-Render-2M",
        "region-aware supervised fine-tuning",
        "HQ-Poster100K",
        "aesthetic-text-reinforcement learning",
        "best-of-n preference optimization",
        "joint vision-language feedback refinement"
      ]
    },
    "publishedAt": "2025-06-12T10:28:12.000Z",
    "title": "PosterCraft: Rethinking High-Quality Aesthetic Poster Generation in a\n  Unified Framework",
    "summary": "Generating aesthetic posters is more challenging than simple design images:\nit requires not only precise text rendering but also the seamless integration\nof abstract artistic content, striking layouts, and overall stylistic harmony.\nTo address this, we propose PosterCraft, a unified framework that abandons\nprior modular pipelines and rigid, predefined layouts, allowing the model to\nfreely explore coherent, visually compelling compositions. PosterCraft employs\na carefully designed, cascaded workflow to optimize the generation of\nhigh-aesthetic posters: (i) large-scale text-rendering optimization on our\nnewly introduced Text-Render-2M dataset; (ii) region-aware supervised\nfine-tuning on HQ-Poster100K; (iii) aesthetic-text-reinforcement learning via\nbest-of-n preference optimization; and (iv) joint vision-language feedback\nrefinement. Each stage is supported by a fully automated data-construction\npipeline tailored to its specific needs, enabling robust training without\ncomplex architectural modifications. Evaluated on multiple experiments,\nPosterCraft significantly outperforms open-source baselines in rendering\naccuracy, layout coherence, and overall visual appeal-approaching the quality\nof SOTA commercial systems. Our code, models, and datasets can be found in the\nProject page: https://ephemeral182.github.io/PosterCraft",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10741.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "66015e8aa4d296af07de538e",
      "avatarUrl": "/avatars/a1295c631cc2646282c545859975ce4c.svg",
      "fullname": "Ye",
      "name": "Owen777",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.10890",
      "authors": [
        {
          "_id": "684b8b533b733ba333686fe4",
          "user": {
            "_id": "62bc1adacaf01b9bec398547",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656494729797-noauth.png",
            "isPro": false,
            "fullname": "Zhao Zhang",
            "user": "zbrl",
            "type": "user"
          },
          "name": "Zhao Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:39:45.153Z",
          "hidden": false
        },
        {
          "_id": "684b8b533b733ba333686fe5",
          "name": "Yutao Cheng",
          "hidden": false
        },
        {
          "_id": "684b8b533b733ba333686fe6",
          "user": {
            "_id": "6669a0cc9f28880b31d7c4ef",
            "avatarUrl": "/avatars/bd66a6f68a9af2bf7ee40510579e57fe.svg",
            "isPro": false,
            "fullname": "dexiang hong",
            "user": "hxxxl",
            "type": "user"
          },
          "name": "Dexiang Hong",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-13T02:22:14.202Z",
          "hidden": false
        },
        {
          "_id": "684b8b533b733ba333686fe7",
          "user": {
            "_id": "63fd7279ed9eead590fd02ed",
            "avatarUrl": "/avatars/4cf6f005069412ee87ed07cd81500f1e.svg",
            "isPro": false,
            "fullname": "YangMaoke",
            "user": "YangMaoke",
            "type": "user"
          },
          "name": "Maoke Yang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-13T02:22:14.202Z",
          "hidden": false
        },
        {
          "_id": "684b8b533b733ba333686fe8",
          "user": {
            "_id": "6436619ead9b9147de287a24",
            "avatarUrl": "/avatars/180c43c79e552dd345636a47db80e3e9.svg",
            "isPro": false,
            "fullname": "ShiLayne",
            "user": "ShiLayne",
            "type": "user"
          },
          "name": "Gonglei Shi",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-06-13T02:23:52.345Z",
          "hidden": true
        },
        {
          "_id": "684b8b533b733ba333686fe9",
          "name": "Lei Ma",
          "hidden": false
        },
        {
          "_id": "684b8b533b733ba333686fea",
          "name": "Hui Zhang",
          "hidden": false
        },
        {
          "_id": "684b8b533b733ba333686feb",
          "name": "Jie Shao",
          "hidden": false
        },
        {
          "_id": "684b8b533b733ba333686fec",
          "name": "Xinglong Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-12T16:54:39.000Z",
      "submittedOnDailyAt": "2025-06-13T00:55:02.473Z",
      "title": "CreatiPoster : Outil pour l'édition et le contrôle de la génération de dessins graphiques multi-couche.",
      "submittedOnDailyBy": {
        "_id": "62bc1adacaf01b9bec398547",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656494729797-noauth.png",
        "isPro": false,
        "fullname": "Zhao Zhang",
        "user": "zbrl",
        "type": "user"
      },
      "summary": "Le design graphique joue un rôle important tant sur les aspects commerciaux que personnels, mais la création de compositions graphiques de haute qualité, éditables et élégantes est particulièrement défique pour les débutants, car nécessite du temps et une grande technique. Les outils actuels d'IA automatisent une partie du flux de travail, mais il est difficile pour les utilisateurs d'inclure précisément leurs ressources et de maintenir la possibilité d'édition tout en atteignant une beauté visuelle professionnelle. Dans les systèmes commerciaux, des outils comme Canva Magic Design basés sur de grandes bibliothèques de modèles sont utiles, mais pas efficaces pour réutiliser. Dans cet article, nous présentons le cadre CreatiPoster. Ce cadre génère des compositions multi-couche éditables à partir d'indications naturelles ou de ressources sélectionnées. Les modèles de protocole et de couleur RGBA génèrent des fichiers JSON spécifiques qui détaillent précisément la position, la structure, le contenu et l'esthétique de chaque couche de texte ou de ressource. En outre, il inclut un simple fond de fond. Ensuite, un modèle de fond conditionnel s'appuie sur cette couche de fond pour synthétiser un fond cohérent. CreatiPoster établit un cadre de référence sur le marché de l'automatisation graphique, montrant une approche avancée et patentable, ainsi qu'une accessibilité ouverte. Il publie un corpus de 100,000 dessins multi-couche sans droits de propriété pour promouvoir le feedback. CreatiPoster soutient diverses applications, comme édition de fond, superposition de texte, redimensionnement responsable, multilinguisme et animation de poster, et encourage la démocratisation du design graphique grâce à l'aide de l'IA. Page du projet : https://github.com/graphic-design-ai/creatiposter",
      "upvotes": 7,
      "discussionId": "684b8b533b733ba333686fed",
      "githubRepo": "https://github.com/graphic-design-ai/creatiposter",
      "ai_summary": "CreatiPoster generates high-quality, editable, and customizable graphic compositions from text or assets, outperforming existing tools and templates.",
      "ai_keywords": [
        "RGBA large multimodal model",
        "JSON specification",
        "conditional background model",
        "automated metrics",
        "graphic-design generation",
        "multi-layer designs",
        "AI-assisted graphic design"
      ]
    },
    "publishedAt": "2025-06-12T12:54:39.000Z",
    "title": "CreatiPoster: Towards Editable and Controllable Multi-Layer Graphic\n  Design Generation",
    "summary": "Graphic design plays a crucial role in both commercial and personal contexts,\nyet creating high-quality, editable, and aesthetically pleasing graphic\ncompositions remains a time-consuming and skill-intensive task, especially for\nbeginners. Current AI tools automate parts of the workflow, but struggle to\naccurately incorporate user-supplied assets, maintain editability, and achieve\nprofessional visual appeal. Commercial systems, like Canva Magic Design, rely\non vast template libraries, which are impractical for replicate. In this paper,\nwe introduce CreatiPoster, a framework that generates editable, multi-layer\ncompositions from optional natural-language instructions or assets. A protocol\nmodel, an RGBA large multimodal model, first produces a JSON specification\ndetailing every layer (text or asset) with precise layout, hierarchy, content\nand style, plus a concise background prompt. A conditional background model\nthen synthesizes a coherent background conditioned on this rendered foreground\nlayers. We construct a benchmark with automated metrics for graphic-design\ngeneration and show that CreatiPoster surpasses leading open-source approaches\nand proprietary commercial systems. To catalyze further research, we release a\ncopyright-free corpus of 100,000 multi-layer designs. CreatiPoster supports\ndiverse applications such as canvas editing, text overlay, responsive resizing,\nmultilingual adaptation, and animated posters, advancing the democratization of\nAI-assisted graphic design. Project homepage:\nhttps://github.com/graphic-design-ai/creatiposter",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10890.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62bc1adacaf01b9bec398547",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656494729797-noauth.png",
      "fullname": "Zhao Zhang",
      "name": "zbrl",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.09967",
      "authors": [
        {
          "_id": "684ae1eedbd21a9cc27b0f10",
          "user": {
            "_id": "67469d6a8407f929491dce06",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67469d6a8407f929491dce06/7vd7zyApmT4rXe58gqmG1.png",
            "isPro": true,
            "fullname": "Shangshang Wang",
            "user": "upup-ashton-wang",
            "type": "user"
          },
          "name": "Shangshang Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:41:31.479Z",
          "hidden": false
        },
        {
          "_id": "684ae1eedbd21a9cc27b0f11",
          "name": "Julian Asilis",
          "hidden": false
        },
        {
          "_id": "684ae1eedbd21a9cc27b0f12",
          "name": "Ömer Faruk Akgül",
          "hidden": false
        },
        {
          "_id": "684ae1eedbd21a9cc27b0f13",
          "name": "Enes Burak Bilgin",
          "hidden": false
        },
        {
          "_id": "684ae1eedbd21a9cc27b0f14",
          "name": "Ollie Liu",
          "hidden": false
        },
        {
          "_id": "684ae1eedbd21a9cc27b0f15",
          "user": {
            "_id": "63c8454e46421a2efe82709d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63c8454e46421a2efe82709d/3BcSk4KOwAgWHEPVtsAV3.png",
            "isPro": true,
            "fullname": "Deqing Fu",
            "user": "deqing",
            "type": "user"
          },
          "name": "Deqing Fu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:41:29.284Z",
          "hidden": false
        },
        {
          "_id": "684ae1eedbd21a9cc27b0f16",
          "user": {
            "_id": "644bf65522d211df6444a7f4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644bf65522d211df6444a7f4/k_ddZdQDg2fzhwjI1EXyx.jpeg",
            "isPro": false,
            "fullname": "Willie Neiswanger",
            "user": "willieneis",
            "type": "user"
          },
          "name": "Willie Neiswanger",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:41:27.301Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-11T17:44:01.000Z",
      "submittedOnDailyAt": "2025-06-13T02:39:37.215Z",
      "title": "Resa : Une théorie de la transparence est générée avec les modèles de MoMo à travers les SAEs.",
      "submittedOnDailyBy": {
        "_id": "67469d6a8407f929491dce06",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67469d6a8407f929491dce06/7vd7zyApmT4rXe58gqmG1.png",
        "isPro": true,
        "fullname": "Shangshang Wang",
        "user": "upup-ashton-wang",
        "type": "user"
      },
      "summary": "Nous explorons si il est possible d'extraire une forte théorie logique efficacement du modèle de langue en utilisant les représentations potentielles du modèle. La réponse à cette question est Resa. Resa est une famille de modèles logiques de 150M unités qui utilisent un nouveau procédé efficace d'ajustement de codificateurs automatiques spars (SAE-Tuning). Ce méthode est réalisée de la manière suivante : d'abord, un SAE est entraîné pour identifier les capacités logiques du modèle source, et ensuite, l'SAE entraîné est utilisé pour guider le processus d'entraînement standard de surchargage pour extraire les capacités logiques au modèle cible. Cette technique est réalisée avec des données de réponse à des questions validées sans empreintes logiques. En particulier, lorsque cette technique est appliquée à un modèle basé sur une base spécifique avant d'entraîner le modèle de RL, SAE-Tuning maintient le rendement logique du modèle d'entraînement de RL en plus de 97%, réduisant le coût d'entraînement de plus de 2000 fois et diminuant le temps d'entraînement à environ 1 dollar et 20 minutes. De plus, ce méthode peut être appliquée à des modèles d'entraînement de RL légers en 1 heure avec 2 GPUs, atteignant un Rendement Pass@1 de 43,33% sur AIME24 et un 90% sur AMC23, avec un rendement logique presque identique sans augmenter significativement les coûts. Une observation notable est que les capacités logiques extraites d'un SAE sont potentiellement modulaires et généralisables. La généralisation signifie que les capacités extraites améliorent le rendement dans de plus grands corpus répétés. La modularité signifie que les capacités extraites dans Qwen ou Qwen-Math peuvent être ajoutées à un modèle R1-Distill sans nécessiter de retrainer, obtenant un bénéfice relativement grand. Les expériences étendues démontrent ces observations, et tous les artefacts sont complètement disponibles sous une licence de code open.",
      "upvotes": 6,
      "discussionId": "684ae1eedbd21a9cc27b0f17",
      "projectPage": "https://shangshangwang.notion.site/resa",
      "githubRepo": "https://github.com/shangshang-wang/Resa",
      "ai_summary": "SAE-Tuning efficiently elicits strong reasoning in language models by leveraging sparse autoencoders, enabling cost-effective performance gains without extensive retraining.",
      "ai_keywords": [
        "sparse autoencoder tuning",
        "SAE-Tuning",
        "reasoning models",
        "verification",
        "sparse autoencoders",
        "supervised fine-tuning",
        "RL post-training",
        "Pass@1",
        "AIME24",
        "AMC23",
        "generality",
        "modularity",
        "R1-Distill",
        "Qwen",
        "Qwen-Math"
      ]
    },
    "publishedAt": "2025-06-11T13:44:01.000Z",
    "title": "Resa: Transparent Reasoning Models via SAEs",
    "summary": "How cost-effectively can we elicit strong reasoning in language models by\nleveraging their underlying representations? We answer this question with Resa,\na family of 1.5B reasoning models trained via a novel and efficient sparse\nautoencoder tuning (SAE-Tuning) procedure. This method first trains an SAE to\ncapture reasoning abilities from a source model, and then uses the trained SAE\nto guide a standard supervised fine-tuning process to elicit such abilities in\na target model, all using verified question-answer data without any reasoning\ntraces. Notably, when applied to certain base models before further RL\npost-training, SAE-Tuning retains >97% of its RL-trained counterpart's\nreasoning performance while reducing training costs by >2000x to roughly \\1\nand training time by >450x to around 20 minutes. Furthermore, when applied to\nlightly RL-trained models (e.g., within 1 hour on 2 GPUs), it enables reasoning\nperformance such as 43.33% Pass@1 on AIME24 and 90% Pass@1 on AMC23 for only\naround 1 additional cost. Surprisingly, the reasoning abilities extracted via\nSAEs are potentially both generalizable and modular. Generality means abilities\nextracted from one dataset still elevate performance on a larger and\noverlapping corpus. Modularity means abilities extracted from Qwen or Qwen-Math\ncan be attached to the R1-Distill model at test time, without any retraining,\nand yield comparable gains. Extensive ablations validate these findings and all\nartifacts are fully open-sourced.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09967.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67469d6a8407f929491dce06",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67469d6a8407f929491dce06/7vd7zyApmT4rXe58gqmG1.png",
      "fullname": "Shangshang Wang",
      "name": "upup-ashton-wang",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.10910",
      "authors": [
        {
          "_id": "684bbe273b733ba3336870ed",
          "name": "Mistral-AI",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba3336870ef",
          "name": "Abhinav Rastogi",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba3336870f0",
          "name": "Albert Q. Jiang",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba3336870f1",
          "name": "Andy Lo",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba3336870f2",
          "name": "Gabrielle Berrada",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba3336870f3",
          "name": "Guillaume Lample",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba3336870f4",
          "name": "Jason Rute",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba3336870f5",
          "name": "Joep Barmentlo",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba3336870f6",
          "name": "Karmesh Yadav",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba3336870f7",
          "name": "Kartik Khandelwal",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba3336870f8",
          "name": "Khyathi Raghavi Chandu",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba3336870f9",
          "name": "Léonard Blier",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba3336870fa",
          "name": "Lucile Saulnier",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba3336870fb",
          "name": "Matthieu Dinot",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba3336870fc",
          "name": "Maxime Darrin",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba3336870fd",
          "name": "Neha Gupta",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba3336870fe",
          "name": "Roman Soletskyi",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba3336870ff",
          "name": "Sagar Vaze",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687100",
          "name": "Teven Le Scao",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687101",
          "name": "Yihan Wang",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687102",
          "name": "Adam Yang",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687103",
          "name": "Alexander H. Liu",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687104",
          "name": "Alexandre Sablayrolles",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687105",
          "name": "Amélie Héliou",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687106",
          "name": "Amélie Martin",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687107",
          "name": "Andy Ehrenberg",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687108",
          "name": "Anmol Agarwal",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687109",
          "name": "Antoine Roux",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368710a",
          "name": "Arthur Darcet",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368710b",
          "name": "Arthur Mensch",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368710c",
          "name": "Baptiste Bout",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368710d",
          "name": "Baptiste Rozière",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368710e",
          "name": "Baudouin De Monicault",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368710f",
          "name": "Chris Bamford",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687110",
          "name": "Christian Wallenwein",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687111",
          "name": "Christophe Renaudin",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687112",
          "name": "Clémence Lanfranchi",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687113",
          "name": "Darius Dabert",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687114",
          "name": "Devon Mizelle",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687115",
          "name": "Diego de las Casas",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687116",
          "name": "Elliot Chane-Sane",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687117",
          "name": "Emilien Fugier",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687118",
          "name": "Emma Bou Hanna",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687119",
          "name": "Gauthier Delerce",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368711a",
          "name": "Gauthier Guinet",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368711b",
          "name": "Georgii Novikov",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368711c",
          "name": "Guillaume Martin",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368711d",
          "name": "Himanshu Jaju",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368711e",
          "name": "Jan Ludziejewski",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368711f",
          "name": "Jean-Hadrien Chabran",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687120",
          "name": "Jean-Malo Delignon",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687121",
          "name": "Joachim Studnia",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687122",
          "name": "Jonas Amar",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687123",
          "name": "Josselin Somerville Roberts",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687124",
          "name": "Julien Denize",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687125",
          "name": "Karan Saxena",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687126",
          "name": "Kush Jain",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687127",
          "name": "Lingxiao Zhao",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687128",
          "name": "Louis Martin",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687129",
          "name": "Luyu Gao",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368712a",
          "name": "Lélio Renard Lavaud",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368712b",
          "name": "Marie Pellat",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368712c",
          "name": "Mathilde Guillaumin",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368712d",
          "name": "Mathis Felardos",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368712e",
          "name": "Maximilian Augustin",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368712f",
          "name": "Mickaël Seznec",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687130",
          "name": "Nikhil Raghuraman",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687131",
          "name": "Olivier Duchenne",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687132",
          "name": "Patricia Wang",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687133",
          "name": "Patrick von Platen",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687134",
          "name": "Patryk Saffer",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687135",
          "name": "Paul Jacob",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687136",
          "name": "Paul Wambergue",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687137",
          "name": "Paula Kurylowicz",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687138",
          "name": "Pavankumar Reddy Muddireddy",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687139",
          "name": "Philomène Chagniot",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368713a",
          "name": "Pierre Stock",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368713b",
          "name": "Pravesh Agrawal",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368713c",
          "name": "Romain Sauvestre",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368713d",
          "name": "Rémi Delacourt",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368713e",
          "name": "Sanchit Gandhi",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368713f",
          "name": "Sandeep Subramanian",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687140",
          "name": "Shashwat Dalal",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687141",
          "name": "Siddharth Gandhi",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687142",
          "name": "Soham Ghosh",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687143",
          "name": "Srijan Mishra",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687144",
          "name": "Sumukh Aithal",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687145",
          "name": "Szymon Antoniak",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687146",
          "name": "Thibault Schueller",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687147",
          "name": "Thibaut Lavril",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687148",
          "name": "Thomas Robert",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687149",
          "name": "Thomas Wang",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368714a",
          "name": "Timothée Lacroix",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368714b",
          "name": "Valeriia Nemychnikova",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368714c",
          "name": "Victor Paltz",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368714d",
          "name": "Virgile Richard",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368714e",
          "name": "Wen-Ding Li",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368714f",
          "name": "William Marshall",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687150",
          "name": "Xuanyu Zhang",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687151",
          "name": "Yunhao Tang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-12T17:22:37.000Z",
      "submittedOnDailyAt": "2025-06-13T04:29:39.974Z",
      "title": "Majesté",
      "submittedOnDailyBy": {
        "_id": "5e6a3d4ea9afd5125d9ec064",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1584020801691-noauth.jpeg",
        "isPro": true,
        "fullname": "Stefan Schweter",
        "user": "stefan-it",
        "type": "user"
      },
      "summary": "Introducing le modèle d'inférence initial de Mistral et notre pipeline d'apprentissage par renforcement (RL) scalable. Contrairement aux implémentations existantes et aux modèles précédents qui reposent sur une chaîne d'outils RL dérivés de leurs propres modèles, nous adoptons une approche simple qui dépend uniquement de nos modèles et de notre infrastructure. Plus spécifiquement, nous présentons une pile pour explorer les limites de l'apprentissage RL simple pour les grands modèles de langage, nous démontrons une approche simple forcée pour la raisonnement du modèle et nous constatons que l'RL basé uniquement sur les données documentaires maintient les capacités du point de départ. Nous découvrons que l'RL sur les données documentaires peut maintenir ou améliorer la compréhension, suivre des instructions, faire des appels de fonction et plus encore. Nous présentons des modèles entraînés sur la raisonnement de Magistral Medium et Mistral Medium 3, et nous publions Magistral Small (licence Apache 2.0). Magistral Small est un modèle plus fort qui commence à partir de données incluses dans Magistral Medium.",
      "upvotes": 5,
      "discussionId": "684bbe283b733ba333687152",
      "ai_summary": "Magistral, a scalable reinforcement learning pipeline, demonstrates that RL can enhance multimodal understanding and instruction following in large language models without requiring existing RL traces.",
      "ai_keywords": [
        "reinforcement learning",
        "RL",
        "LLMs",
        "multimodal understanding",
        "instruction following",
        "function calling",
        "cold-start data"
      ]
    },
    "publishedAt": "2025-06-12T13:22:37.000Z",
    "title": "Magistral",
    "summary": "We introduce Magistral, Mistral's first reasoning model and our own scalable\nreinforcement learning (RL) pipeline. Instead of relying on existing\nimplementations and RL traces distilled from prior models, we follow a ground\nup approach, relying solely on our own models and infrastructure. Notably, we\ndemonstrate a stack that enabled us to explore the limits of pure RL training\nof LLMs, present a simple method to force the reasoning language of the model,\nand show that RL on text data alone maintains most of the initial checkpoint's\ncapabilities. We find that RL on text maintains or improves multimodal\nunderstanding, instruction following and function calling. We present Magistral\nMedium, trained for reasoning on top of Mistral Medium 3 with RL alone, and we\nopen-source Magistral Small (Apache 2.0) which further includes cold-start data\nfrom Magistral Medium.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10910.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5e6a3d4ea9afd5125d9ec064",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1584020801691-noauth.jpeg",
      "fullname": "Stefan Schweter",
      "name": "stefan-it",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2746
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.09344",
      "authors": [
        {
          "_id": "684ae277dbd21a9cc27b118d",
          "name": "Inclusion AI",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b118e",
          "name": "Biao Gong",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b118f",
          "name": "Cheng Zou",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b1190",
          "name": "Chuanyang Zheng",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b1191",
          "name": "Chunluan Zhou",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b1192",
          "name": "Canxiang Yan",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b1193",
          "name": "Chunxiang Jin",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b1194",
          "name": "Chunjie Shen",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b1195",
          "name": "Dandan Zheng",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b1196",
          "name": "Fudong Wang",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b1197",
          "name": "Furong Xu",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b1198",
          "name": "GuangMing Yao",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b1199",
          "name": "Jun Zhou",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b119a",
          "name": "Jingdong Chen",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b119b",
          "name": "Jianxin Sun",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b119c",
          "name": "Jiajia Liu",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b119d",
          "name": "Jianjiang Zhu",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b119e",
          "name": "Jun Peng",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b119f",
          "name": "Kaixiang Ji",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11a0",
          "name": "Kaiyou Song",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11a1",
          "name": "Kaimeng Ren",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11a2",
          "name": "Libin Wang",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11a3",
          "name": "Lixiang Ru",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11a4",
          "name": "Lele Xie",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11a5",
          "name": "Longhua Tan",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11a6",
          "name": "Lyuxin Xue",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11a7",
          "name": "Lan Wang",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11a8",
          "name": "Mochen Bai",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11a9",
          "name": "Ning Gao",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11aa",
          "name": "Pei Chen",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11ab",
          "name": "Qingpei Guo",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11ac",
          "name": "Qinglong Zhang",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11ad",
          "name": "Qiang Xu",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11ae",
          "name": "Rui Liu",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11af",
          "name": "Ruijie Xiong",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11b0",
          "name": "Sirui Gao",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11b1",
          "name": "Tinghao Liu",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11b2",
          "name": "Taisong Li",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11b3",
          "name": "Weilong Chai",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11b4",
          "name": "Xinyu Xiao",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11b5",
          "name": "Xiaomei Wang",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11b6",
          "name": "Xiaoxue Chen",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11b7",
          "name": "Xiao Lu",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11b8",
          "name": "Xiaoyu Li",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11b9",
          "name": "Xingning Dong",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11ba",
          "name": "Xuzheng Yu",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11bb",
          "name": "Yi Yuan",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11bc",
          "name": "Yuting Gao",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11bd",
          "name": "Yunxiao Sun",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11be",
          "name": "Yipeng Chen",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11bf",
          "name": "Yifei Wu",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11c0",
          "name": "Yongjie Lyu",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11c1",
          "name": "Ziping Ma",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11c2",
          "name": "Zipeng Feng",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11c3",
          "name": "Zhijiang Fang",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11c4",
          "name": "Zhihao Qiu",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11c5",
          "name": "Ziyuan Huang",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11c6",
          "name": "Zhengyu He",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-11T02:50:49.000Z",
      "submittedOnDailyAt": "2025-06-13T01:53:15.172Z",
      "title": "Nom-Omni : Modèle d'intégration de la sensation et de la génération",
      "submittedOnDailyBy": {
        "_id": "644fcbea4f7316588267dc80",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644fcbea4f7316588267dc80/w8-2Gkaw9BN9VzppNXrTP.jpeg",
        "isPro": false,
        "fullname": "Biao Gong",
        "user": "BiaoGong",
        "type": "user"
      },
      "summary": "Men-Omni est un modèle de modalité unifiée capable de traiter des images, du texte, la voix et le vidéo, et montre une puissance forte dans la génération de discours et d'images. Men-Omni utilise un encodeur spécialisé pour extraire des tokens de d'autres modalités, qui est connecté à un nouveau routage spécifique pour traiter dans l'architecture MoE. Ce design fonctionne dans un cadre unifié pour traiter et fusionner plusieurs entrées de modalité de manière efficace, sans nécessiter de modèles ou d'entraînements finaux spécifiques pour des tâches différentes, ni rédesigns structurels. Il est crucial que Men-Omni augmente son valeur en soutenant la génération de la voix et des images. Cela est réalisé en combinant un décodage avancé de la voix avec la génération d'images de haute qualité de Men-Lite-Uni, permettant la création de voix naturelles, des conversations contextualisées, la transformation du texte en discours et diverses éditions d'images. Les résultats des expériences montrent que Men-Omni fournit des solutions solides pour la génération et l'intégration de toutes les modalités. En particulier, Men-Omni est le premier modèle open-source qui implémente le support pour plusieurs modalités, et propose de publier tous les codes et poids du modèle pour encourager le progrès de la communauté.",
      "upvotes": 4,
      "discussionId": "684ae277dbd21a9cc27b11c7",
      "ai_summary": "Ming-Omni is a unified multimodal model with dedicated encoders and modality-specific routers that can process images, text, audio, and video, and performs tasks like speech and image generation, context-aware chatting, and versatile image editing.",
      "ai_keywords": [
        "multimodal model",
        "encoders",
        "tokens",
        "MoE architecture",
        "modality-specific routers",
        "audio decoder",
        "Ming-Lite-Uni",
        "context-aware chatting",
        "text-to-speech conversion",
        "image editing",
        "unified perception",
        "generation",
        "open-source"
      ]
    },
    "publishedAt": "2025-06-10T22:50:49.000Z",
    "title": "Ming-Omni: A Unified Multimodal Model for Perception and Generation",
    "summary": "We propose Ming-Omni, a unified multimodal model capable of processing\nimages, text, audio, and video, while demonstrating strong proficiency in both\nspeech and image generation. Ming-Omni employs dedicated encoders to extract\ntokens from different modalities, which are then processed by Ling, an MoE\narchitecture equipped with newly proposed modality-specific routers. This\ndesign enables a single model to efficiently process and fuse multimodal inputs\nwithin a unified framework, thereby facilitating diverse tasks without\nrequiring separate models, task-specific fine-tuning, or structural redesign.\nImportantly, Ming-Omni extends beyond conventional multimodal models by\nsupporting audio and image generation. This is achieved through the integration\nof an advanced audio decoder for natural-sounding speech and Ming-Lite-Uni for\nhigh-quality image generation, which also allow the model to engage in\ncontext-aware chatting, perform text-to-speech conversion, and conduct\nversatile image editing. Our experimental results showcase Ming-Omni offers a\npowerful solution for unified perception and generation across all modalities.\nNotably, our proposed Ming-Omni is the first open-source model we are aware of\nto match GPT-4o in modality support, and we release all code and model weights\nto encourage further research and development in the community.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09344.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "644fcbea4f7316588267dc80",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644fcbea4f7316588267dc80/w8-2Gkaw9BN9VzppNXrTP.jpeg",
      "fullname": "Biao Gong",
      "name": "BiaoGong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.08060",
      "authors": [
        {
          "_id": "6848e0b042e4f9106973f280",
          "user": {
            "_id": "62f32eab52ad88c930bb3f3b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1677134945205-62f32eab52ad88c930bb3f3b.png",
            "isPro": true,
            "fullname": "Asankhaya Sharma",
            "user": "codelion",
            "type": "user"
          },
          "name": "Asankhaya Sharma",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-11T08:35:08.045Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-09T08:37:19.000Z",
      "submittedOnDailyAt": "2025-06-13T00:31:17.814Z",
      "title": "Capacités d'un Transformer Fin-Tuné par Inférence",
      "submittedOnDailyBy": {
        "_id": "62f32eab52ad88c930bb3f3b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1677134945205-62f32eab52ad88c930bb3f3b.png",
        "isPro": true,
        "fullname": "Asankhaya Sharma",
        "user": "codelion",
        "type": "user"
      },
      "summary": "Les modèles de langage naturel ont modifié le traitement de la nature du langage, mais l'entraînement de ajustement de détails (SFT) est très coûteux en termes de calcul. Dans cet article, nous montrons formellement que, dans des cas idéaux, un méthode d'inférence qui inclut le soutien de calculs infinis et l'accès à des ensembles de données d'entraînement de détails (SFT) peut approcher les compétences obtenues dans un modèle de base, en utilisant en particulier l'apprentissage en contexte (ICL). Ces résultats s'étendent aux scénarios réels. Pour la tâche de génération de texte de longueur de sortie fixe l, un ensemble de données suffisant pour approcher les actions d'entraînement de détails en m contextes avec un erreur ε est O(mVε² log m/δ) ou, dans le cas de contextes finis, O(l log Vε² log 1/δ). En classification linéaire, un ensemble de données suffisant est O(dε) ou, dans le cas de contextes fixes, O(1/ε² log 1/δ). Ces résultats fournissent une base théorique pour les modèles basés sur la complétude de Turing et relient l'efficacité des ressources des modèles de langage naturel.",
      "upvotes": 4,
      "discussionId": "6848e0b042e4f9106973f281",
      "githubRepo": "https://github.com/codelion/optillm",
      "ai_summary": "Transformers can approximate supervised fine-tuning capabilities through in-context learning without altering model parameters, supported by theoretical bounds and practical techniques.",
      "ai_keywords": [
        "supervised fine-tuning",
        "in-context learning",
        "base transformer model",
        "Turing completeness",
        "retrieval-augmented generation",
        "text generation",
        "linear classification"
      ]
    },
    "publishedAt": "2025-06-09T04:37:19.000Z",
    "title": "Eliciting Fine-Tuned Transformer Capabilities via Inference-Time\n  Techniques",
    "summary": "Large language models have transformed natural language processing, yet\nsupervised fine-tuning (SFT) remains computationally intensive. This paper\nformally proves that capabilities acquired through SFT can be approximated by a\nbase transformer model using inference-time techniques, specifically in-context\nlearning (ICL), without altering model parameters, under idealized assumptions\nincluding unbounded computational resources and access to the fine-tuning\ndataset. We extend these results to practical scenarios with finite context\nlengths and partial dataset access. For text generation tasks with fixed output\nlength l, datasets of size Oleft( m V{varepsilon^2} log\nm{delta} right) or, with bounded context, Oleft( l\nlog V{varepsilon^2} log 1{delta} right) suffice to approximate\nfine-tuned behavior across m contexts within error varepsilon, where V\nis the vocabulary size and delta is the failure probability. For linear\nclassification, datasets of size Oleft( d{varepsilon}\nright) or, with fixed context, Oleft( 1{varepsilon^2} log\n1{delta} right) are sufficient, where d is the input dimension.\nGrounded in the Turing completeness of transformers, these results provide a\ntheoretical foundation for resource-efficient deployment of large language\nmodels, with practical techniques like retrieval-augmented generation bridging\ntheory to real-world applications.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08060.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62f32eab52ad88c930bb3f3b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1677134945205-62f32eab52ad88c930bb3f3b.png",
      "fullname": "Asankhaya Sharma",
      "name": "codelion",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 91
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.09952",
      "authors": [
        {
          "_id": "684ae226dbd21a9cc27b107a",
          "user": {
            "_id": "63579b21a8e247a69d4e13de",
            "avatarUrl": "/avatars/7892fbc842eaf616228dfade9f13c712.svg",
            "isPro": false,
            "fullname": "Ziyi Wang",
            "user": "LavenderLA",
            "type": "user"
          },
          "name": "Ziyi Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:40:18.063Z",
          "hidden": false
        },
        {
          "_id": "684ae226dbd21a9cc27b107b",
          "user": {
            "_id": "661cfae9a853782abad2a495",
            "avatarUrl": "/avatars/39723a07bf9efed8278e009fe966d044.svg",
            "isPro": false,
            "fullname": "Yanran Zhang",
            "user": "Yanran21",
            "type": "user"
          },
          "name": "Yanran Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:41:09.847Z",
          "hidden": false
        },
        {
          "_id": "684ae226dbd21a9cc27b107c",
          "name": "Jie Zhou",
          "hidden": false
        },
        {
          "_id": "684ae226dbd21a9cc27b107d",
          "name": "Jiwen Lu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-11T17:23:21.000Z",
      "submittedOnDailyAt": "2025-06-13T06:56:09.722Z",
      "title": "UniPre3D : Apprentissage intégré de prédiction pour nuages de points 3D et smoothé gaussien croisé de manière",
      "submittedOnDailyBy": {
        "_id": "63579b21a8e247a69d4e13de",
        "avatarUrl": "/avatars/7892fbc842eaf616228dfade9f13c712.svg",
        "isPro": false,
        "fullname": "Ziyi Wang",
        "user": "LavenderLA",
        "type": "user"
      },
      "summary": "La variété d'échelle dans les données de point cloud représente un grand défi pour le développement de méthodes d'apprentissage de représentation unifiée en vision 3D. Actuellement, il existe peu de modèles 3D unifiés et aucuns méthodes de pré-entraînement efficaces pour les deux types de modèles : objets et espaces. Dans cet article, nous présentons UniPre3D, le premier méthode de pré-entraînement unifié qui peut être appliquée sans restrictions à n'importe quelle échelle de données de point cloud ou à n'importe quelle architecture de modèle 3D. Notre approche consiste à prédire des primitives gaussiennes comme tâche de pré-entraînement, à utiliser un splitting gaussien différenciable pour rendre des images et à optimiser au niveau des pixels pour les sous-objets et les objets finaux. De plus, nous contrôlons la complexité de la tâche de pré-entraînement et ajustons l'approche du modèle vers la structure géométrique, en intégrant des caractéristiques 2D de modèles d'images pré-entraînés et des techniques existantes. Nous démontrons la généralité de notre méthode par une large gamme d'expériences sur différents modèles de point cloud et diverses tâches pour les modèles d'objets et d'espaces. Le code est disponible sur https://github.com/wangzy22/UniPre3D.",
      "upvotes": 3,
      "discussionId": "684ae226dbd21a9cc27b107e",
      "ai_summary": "UniPre3D is a unified pre-training method for 3D point clouds and models of any scale, using Gaussian primitives and 2D feature integration for effective performance across object and scene tasks.",
      "ai_keywords": [
        "point cloud",
        "3D vision",
        "representation learning",
        "UniPre3D",
        "Gaussian primitives",
        "differentiable Gaussian splatting",
        "pixel-level supervision",
        "end-to-end optimization",
        "2D features",
        "pre-trained image models",
        "geometric structures"
      ]
    },
    "publishedAt": "2025-06-11T13:23:21.000Z",
    "title": "UniPre3D: Unified Pre-training of 3D Point Cloud Models with Cross-Modal\n  Gaussian Splatting",
    "summary": "The scale diversity of point cloud data presents significant challenges in\ndeveloping unified representation learning techniques for 3D vision. Currently,\nthere are few unified 3D models, and no existing pre-training method is equally\neffective for both object- and scene-level point clouds. In this paper, we\nintroduce UniPre3D, the first unified pre-training method that can be\nseamlessly applied to point clouds of any scale and 3D models of any\narchitecture. Our approach predicts Gaussian primitives as the pre-training\ntask and employs differentiable Gaussian splatting to render images, enabling\nprecise pixel-level supervision and end-to-end optimization. To further\nregulate the complexity of the pre-training task and direct the model's focus\ntoward geometric structures, we integrate 2D features from pre-trained image\nmodels to incorporate well-established texture knowledge. We validate the\nuniversal effectiveness of our proposed method through extensive experiments\nacross a variety of object- and scene-level tasks, using diverse point cloud\nmodels as backbones. Code is available at https://github.com/wangzy22/UniPre3D.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09952.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63579b21a8e247a69d4e13de",
      "avatarUrl": "/avatars/7892fbc842eaf616228dfade9f13c712.svg",
      "fullname": "Ziyi Wang",
      "name": "LavenderLA",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.09942",
      "authors": [
        {
          "_id": "684ae26adbd21a9cc27b1177",
          "user": {
            "_id": "625a5446f1063e7085d5178a",
            "avatarUrl": "/avatars/5e78186f13f74b14e01583e06ff6c4dc.svg",
            "isPro": false,
            "fullname": "Hao Peng",
            "user": "Wesleythu",
            "type": "user"
          },
          "name": "Hao Peng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:40:06.456Z",
          "hidden": false
        },
        {
          "_id": "684ae26adbd21a9cc27b1178",
          "name": "Yunjia Qi",
          "hidden": false
        },
        {
          "_id": "684ae26adbd21a9cc27b1179",
          "name": "Xiaozhi Wang",
          "hidden": false
        },
        {
          "_id": "684ae26adbd21a9cc27b117a",
          "name": "Bin Xu",
          "hidden": false
        },
        {
          "_id": "684ae26adbd21a9cc27b117b",
          "name": "Lei Hou",
          "hidden": false
        },
        {
          "_id": "684ae26adbd21a9cc27b117c",
          "name": "Juanzi Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-11T17:10:36.000Z",
      "submittedOnDailyAt": "2025-06-13T00:15:19.828Z",
      "title": "VerIF : Amélioration de l'Apprentissage par Référence dans l'Ingénierie de Vérification pour l'Instrumentation Pure",
      "submittedOnDailyBy": {
        "_id": "625a5446f1063e7085d5178a",
        "avatarUrl": "/avatars/5e78186f13f74b14e01583e06ff6c4dc.svg",
        "isPro": false,
        "fullname": "Hao Peng",
        "user": "Wesleythu",
        "type": "user"
      },
      "summary": "Dans l'apprentissage par récompense, la récompense fiable (RLVR) a été établie comme une technologie importante pour améliorer les fonctions des modèles de langage grands (LLMs), jouant un rôle central dans la validation de théorie. Cependant, l'optimisation de l'apprentissage par récompense basée sur des instructions reste en un état d'investigation peu profond. Dans cette étude, on examine le problème de la validation de l'apprentissage par récompense basée sur des instructions et propose un méthode de validation qui combine la validation basée sur des règles de code et la validation avec des modèles de langage grands (LLMs) (par exemple, QwQ-32B), appelée VerIF. Pour cela, on a construit un jeu de données de haute qualité nommé VerInstruct, qui comprend environ 22 000 instances. L'application de l'apprentissage par récompense avec VerIF a été réalisée sur deux modèles, et les modèles entraînés sur un ensemble de tests d'instructions représentatifs ont amélioré significativement leur performance dans plusieurs domaines, atteignant un niveau avancé avec des modèles relativement petits et s'adaptant largement aux nouvelles restrictions. De plus, sa capacité générale n'est pas limitée par la RLVR et démontre qu'elle peut améliorer le rendement des modèles actuels. Dans cette étude, le jeu de données, le code et les modèles sont publiés, et on invite à accéder à https://github.com/THU-KEG/VerIF pour promouvoir futures recherches.",
      "upvotes": 3,
      "discussionId": "684ae26adbd21a9cc27b117d",
      "githubRepo": "https://github.com/THU-KEG/VerIF",
      "ai_summary": "VerIF, a hybrid verification method combining rule-based and LLM-based approaches, enhances instruction-following RL with significant performance improvements and generalization.",
      "ai_keywords": [
        "reinforcement learning",
        "verifiable rewards",
        "RLVR",
        "large language models",
        "LLMs",
        "rule-based code verification",
        "QwQ-32B",
        "instruction-following",
        "VerInstruct",
        "RL training",
        "instruction-following benchmarks",
        "state-of-the-art performance",
        "existing RL recipes"
      ]
    },
    "publishedAt": "2025-06-11T13:10:36.000Z",
    "title": "VerIF: Verification Engineering for Reinforcement Learning in\n  Instruction Following",
    "summary": "Reinforcement learning with verifiable rewards (RLVR) has become a key\ntechnique for enhancing large language models (LLMs), with verification\nengineering playing a central role. However, best practices for RL in\ninstruction following remain underexplored. In this work, we explore the\nverification challenge in RL for instruction following and propose VerIF, a\nverification method that combines rule-based code verification with LLM-based\nverification from a large reasoning model (e.g., QwQ-32B). To support this\napproach, we construct a high-quality instruction-following dataset,\nVerInstruct, containing approximately 22,000 instances with associated\nverification signals. We apply RL training with VerIF to two models, achieving\nsignificant improvements across several representative instruction-following\nbenchmarks. The trained models reach state-of-the-art performance among models\nof comparable size and generalize well to unseen constraints. We further\nobserve that their general capabilities remain unaffected, suggesting that RL\nwith VerIF can be integrated into existing RL recipes to enhance overall model\nperformance. We have released our datasets, codes, and models to facilitate\nfuture research at https://github.com/THU-KEG/VerIF.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09942.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "625a5446f1063e7085d5178a",
      "avatarUrl": "/avatars/5e78186f13f74b14e01583e06ff6c4dc.svg",
      "fullname": "Hao Peng",
      "name": "Wesleythu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.10953",
      "authors": [
        {
          "_id": "684b9fa13b733ba333687066",
          "user": {
            "_id": "5fa9ff3ea13e063b8b2b60cb",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1633380224986-5fa9ff3ea13e063b8b2b60cb.jpeg",
            "isPro": false,
            "fullname": "Xing Han Lù",
            "user": "xhluca",
            "type": "user"
          },
          "name": "Xing Han Lù",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:39:26.665Z",
          "hidden": false
        },
        {
          "_id": "684b9fa13b733ba333687067",
          "name": "Gaurav Kamath",
          "hidden": false
        },
        {
          "_id": "684b9fa13b733ba333687068",
          "name": "Marius Mosbach",
          "hidden": false
        },
        {
          "_id": "684b9fa13b733ba333687069",
          "name": "Siva Reddy",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-12T17:53:58.000Z",
      "submittedOnDailyAt": "2025-06-13T02:22:59.640Z",
      "title": "Préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la web, préparer la web pour l'outbound et l'outbound pour la",
      "submittedOnDailyBy": {
        "_id": "5fa9ff3ea13e063b8b2b60cb",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1633380224986-5fa9ff3ea13e063b8b2b60cb.jpeg",
        "isPro": false,
        "fullname": "Xing Han Lù",
        "user": "xhluca",
        "type": "user"
      },
      "summary": "Récemment, le développement de modèles de langage grands (LLMs) et de diversification a généré un grand intérêt pour le développement d'agents web (systèmes AI). Ce type de systèmes peuvent effectuer des tâches automatiques dans des environnements web. En raison des différences fondamentales entre les interfaces web et les capacités des LLMs, les méthodes actuelles d'accès présentent des problèmes significatifs dans l'automatisation des interactions web complexes. Actuellement, ces méthodes consistent à traiter des arbres DOM vastes, à ajouter de l'information supplémentaire à des écrans de scène ou à interagir avec des API qui évitent complètement l'interface utilisateur. Cet article propose une transition du paradigme de la recherche sur les agents web : il est possible que les agents web se adaptent aux interfaces humaines, mais pas nécessairement. Au lieu de cela, il est nécessaire de développer un nouveau paradigme d'interface optimisée pour les capacités des agents. Dans ce travail, on présente le concept d'une interface web centrée sur l'agent (AWI) et on met en avant des principes de conception axés sur la sécurité, l'efficacité et la standardisation, ainsi que six principes directeurs qui considèrent les intérêts des principales parties prenantes. Cette référence transcende les limites fondamentales des interfaces actuelles et permet de concevoir des agents web plus efficaces et plus fiables avec une plus grande transparence. Cela nécessite une collaboration accrue de la communauté plus large de ML.",
      "upvotes": 2,
      "discussionId": "684b9fa13b733ba33368706a",
      "ai_summary": "A new Agentic Web Interface (AWI) design paradigm is proposed to optimize web agents for navigating websites, focusing on safety, efficiency, and standardization to address fundamental interface mismatches.",
      "ai_keywords": [
        "Large Language Models",
        "multimodal",
        "web agents",
        "Agentic Web Interface",
        "AWI",
        "DOM trees",
        "screenshots",
        "API interactions"
      ]
    },
    "publishedAt": "2025-06-12T13:53:58.000Z",
    "title": "Build the web for agents, not agents for the web",
    "summary": "Recent advancements in Large Language Models (LLMs) and multimodal\ncounterparts have spurred significant interest in developing web agents -- AI\nsystems capable of autonomously navigating and completing tasks within web\nenvironments. While holding tremendous promise for automating complex web\ninteractions, current approaches face substantial challenges due to the\nfundamental mismatch between human-designed interfaces and LLM capabilities.\nCurrent methods struggle with the inherent complexity of web inputs, whether\nprocessing massive DOM trees, relying on screenshots augmented with additional\ninformation, or bypassing the user interface entirely through API interactions.\nThis position paper advocates for a paradigm shift in web agent research:\nrather than forcing web agents to adapt to interfaces designed for humans, we\nshould develop a new interaction paradigm specifically optimized for agentic\ncapabilities. To this end, we introduce the concept of an Agentic Web Interface\n(AWI), an interface specifically designed for agents to navigate a website. We\nestablish six guiding principles for AWI design, emphasizing safety,\nefficiency, and standardization, to account for the interests of all primary\nstakeholders. This reframing aims to overcome fundamental limitations of\nexisting interfaces, paving the way for more efficient, reliable, and\ntransparent web agent design, which will be a collaborative effort involving\nthe broader ML community.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10953.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5fa9ff3ea13e063b8b2b60cb",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1633380224986-5fa9ff3ea13e063b8b2b60cb.jpeg",
      "fullname": "Xing Han Lù",
      "name": "xhluca",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.07795",
      "authors": [
        {
          "_id": "6848dca942e4f9106973f25c",
          "user": {
            "_id": "6659b410a69183808d04b22f",
            "avatarUrl": "/avatars/a16d1fed9ef87163fe458b10c477140b.svg",
            "isPro": false,
            "fullname": "Xiaotian Ye",
            "user": "Acruxos",
            "type": "user"
          },
          "name": "Xiaotian Ye",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-11T08:35:10.086Z",
          "hidden": false
        },
        {
          "_id": "6848dca942e4f9106973f25d",
          "name": "Mengqi Zhang",
          "hidden": false
        },
        {
          "_id": "6848dca942e4f9106973f25e",
          "name": "Shu Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-09T14:21:25.000Z",
      "submittedOnDailyAt": "2025-06-13T05:51:30.395Z",
      "title": "LLM Unlearning ne doit pas dépendre de la forme.",
      "submittedOnDailyBy": {
        "_id": "6659b410a69183808d04b22f",
        "avatarUrl": "/avatars/a16d1fed9ef87163fe458b10c477140b.svg",
        "isPro": false,
        "fullname": "Xiaotian Ye",
        "user": "Acruxos",
        "type": "user"
      },
      "summary": "Les grands modèles de langue (LLM) ont la capacité de oublier, ce qui permet d'éliminer ou de supprimer des connaissances inadéquates, de contrôler des informations dangereuses ou de les cacher, et de prévenir des utilisations erronées. Cependant, des études récentes indiquent que cet effet est limité à l'échelle réelle et peut empêcher l'introduction pratique. Dans ce travail, on a identifié généralement les problèmes de défaillance dans plusieurs sous-états : l'effet des méthodes d'oubli est basé sur une forte dépendance de la forme des exemples d'entraînement, et la généralisation vers d'autres représentations du même savoir est déficitaire. Ces problèmes ont été caractérisés formellement comme \"Biais de Dépendance de la Forme\" et ont été étudiés des motifs de représentation spécifiques dans différentes tâches. Pour atténuer la propagation de ces problèmes, on a introduit un nouveau benchmark appelé \"ORT\" pour évaluer la robustesse des méthodes d'oubli face aux changements dans la représentation du savoir. Les résultats montrent que dans la technologie actuelle, le \"Biais de Dépendance de la Forme\" est largement et gravement présent.\n\nPour éviter que la fonction d'oubli dépende de la forme des sous-tâches qui apparaissent dans le monde réel, ces méthodes ne doivent pas dépendre de la forme. Pour y parvenir, on présente un nouveau méthode basée sur l'apprentissage non supervisé appelé \"Redirection des Concepts de Rang Un (ROCR)\", qui introduit les racines de solutions adéquates. Le ROCR se concentre sur l'invariance des sous-tâches et oublie les concepts dangereux activés, en modifiant les paramètres du modèle en temps réel pour rediriger la perception du modèle vers d'autres concepts non dangereux. Les expériences étendues montrent que comparés aux méthodes traditionnelles, le ROCR améliore significativement l'effet d'oubli et génère des sorties naturelles de haute qualité.",
      "upvotes": 2,
      "discussionId": "6848dca942e4f9106973f25f",
      "ai_summary": "Form-Dependent Bias limits the effectiveness of LLM unlearning across different knowledge expressions, and Rank-one Concept Redirection (ROCR) is proposed as a form-independent solution that enhances unlearning efficacy.",
      "ai_keywords": [
        "Large Language Model (LLM)",
        "unlearning",
        "Form-Dependent Bias",
        "ORT",
        "Rank-one Concept Redirection (ROCR)",
        "downstream tasks",
        "unlearning methods",
        "concept redirection",
        "model parameters",
        "activated dangerous concepts"
      ]
    },
    "publishedAt": "2025-06-09T10:21:25.000Z",
    "title": "LLM Unlearning Should Be Form-Independent",
    "summary": "Large Language Model (LLM) unlearning aims to erase or suppress undesirable\nknowledge within the model, offering promise for controlling harmful or private\ninformation to prevent misuse. However, recent studies highlight its limited\nefficacy in real-world scenarios, hindering practical adoption. In this study,\nwe identify a pervasive issue underlying many downstream failures: the\neffectiveness of existing unlearning methods heavily depends on the form of\ntraining samples and frequently fails to generalize to alternate expressions of\nthe same knowledge. We formally characterize this problem as Form-Dependent\nBias and systematically investigate its specific manifestation patterns across\nvarious downstream tasks. To quantify its prevalence and support future\nresearch, we introduce ORT, a novel benchmark designed to evaluate the\nrobustness of unlearning methods against variations in knowledge expression.\nResults reveal that Form-Dependent Bias is both widespread and severe among\ncurrent techniques.\n  We argue that LLM unlearning should be form-independent to address the\nendless forms of downstream tasks encountered in real-world security-critical\nscenarios. Towards this goal, we introduce Rank-one Concept Redirection (ROCR),\na novel training-free method, as a promising solution path. ROCR performs\nunlearning by targeting the invariants in downstream tasks, specifically the\nactivated dangerous concepts. It is capable of modifying model parameters\nwithin seconds to redirect the model's perception of a specific unlearning\ntarget concept to another harmless concept. Extensive experiments demonstrate\nthat ROCR significantly improves unlearning effectiveness compared to\ntraditional methods while generating highly natural outputs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07795.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6659b410a69183808d04b22f",
      "avatarUrl": "/avatars/a16d1fed9ef87163fe458b10c477140b.svg",
      "fullname": "Xiaotian Ye",
      "name": "Acruxos",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.06694",
      "authors": [
        {
          "_id": "684ba6bc3b733ba333687093",
          "name": "Yuan Yuan",
          "hidden": false
        },
        {
          "_id": "684ba6bc3b733ba333687094",
          "name": "Yukun Liu",
          "hidden": false
        },
        {
          "_id": "684ba6bc3b733ba333687095",
          "name": "Chonghua Han",
          "hidden": false
        },
        {
          "_id": "684ba6bc3b733ba333687096",
          "user": {
            "_id": "6465d3bd63e7e09dd02e95c3",
            "avatarUrl": "/avatars/b2798bd5f8368f956bf7fab79d9432f0.svg",
            "isPro": false,
            "fullname": "Jie Feng",
            "user": "JJ-TMT",
            "type": "user"
          },
          "name": "Jie Feng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:39:13.023Z",
          "hidden": false
        },
        {
          "_id": "684ba6bc3b733ba333687097",
          "name": "Yong Li",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6465d3bd63e7e09dd02e95c3/LfNEmKBa7cYZhNNRnZRIx.png"
      ],
      "publishedAt": "2025-06-07T07:19:11.000Z",
      "submittedOnDailyAt": "2025-06-13T02:53:49.430Z",
      "title": "Destruction du Data Scroll : Construire un modèle à travers l'apprentissage génératif avec un approche ouverte et scalable basée sur le mouvement.",
      "submittedOnDailyBy": {
        "_id": "6465d3bd63e7e09dd02e95c3",
        "avatarUrl": "/avatars/b2798bd5f8368f956bf7fab79d9432f0.svg",
        "isPro": false,
        "fullname": "Jie Feng",
        "user": "JJ-TMT",
        "type": "user"
      },
      "summary": "Le modèle de base a changé de manière innovante les domaines du traitement du langage naturel et de la vision par ordinateur. Ces modèles ont permis un apprentissage généralisé pour plusieurs tâches et ensembles de données. Cependant, la construction de modèles de base pour le mouvement a été difficile en raison de la nature cachée des données de mouvement et de l'existence de différentes échelles de données entre institutions. Pour résoudre ce problème, nous proposons le \"MoveGCL\", un cadre d'œuvre échellable et préservateur de la vie privée qui permet l'entraînement de modèles de mouvement à travers un apprentissage continu génératif. MoveGCL répète le trafic synthétique généré par des modèles d'apprentissage libre, permettant l'évolution distribuée et progressive du modèle, et évite la comparaison de données en utilisant une stratégie de désacélération des catastrophes pour renforcer la mémoire. Pour aborder la diversité des patrons de mouvement, MoveGCL utilise un Transformer Mixture-of-Experts avec des experts en connaissance du mouvement, adaptant-se de manière hiérarchique et stable à travers des mises à jour continuelles. Les expériences avec 6 ensembles de données réels de villes montrent que MoveGCL atteint des résultats comparables à ceux d'entraînement conjoint, dépasse significativement l'apprentissage fédérateur et offre une forte protection de la vie privée. MoveGCL marque un important pas dans le développement de modèles de base pour le mouvement et fournit une planification pratique pour le développement de modèles ouverts et échellables qui préservent la vie privée.",
      "upvotes": 2,
      "discussionId": "684ba6bc3b733ba333687098",
      "githubRepo": "https://github.com/ScottLiu2003/MoveGCL",
      "ai_summary": "MoveGCL is a privacy-preserving framework using generative continual learning and a Mixture-of-Experts Transformer for training mobility foundation models without sharing raw data.",
      "ai_keywords": [
        "generative continual learning",
        "privacy-preserving",
        "Mixture-of-Experts Transformer",
        "mobility-aware expert routing mechanism",
        "layer-wise progressive adaptation",
        "catastrophic forgetting",
        "federated learning"
      ]
    },
    "publishedAt": "2025-06-07T03:19:11.000Z",
    "title": "Breaking Data Silos: Towards Open and Scalable Mobility Foundation\n  Models via Generative Continual Learning",
    "summary": "Foundation models have revolutionized fields such as natural language\nprocessing and computer vision by enabling general-purpose learning across\ndiverse tasks and datasets. However, building analogous models for human\nmobility remains challenging due to the privacy-sensitive nature of mobility\ndata and the resulting data silos across institutions. To bridge this gap, we\npropose MoveGCL, a scalable and privacy-preserving framework for training\nmobility foundation models via generative continual learning. Without sharing\nraw data, MoveGCL enables decentralized and progressive model evolution by\nreplaying synthetic trajectories generated from a frozen teacher model, and\nreinforces knowledge retention through a tailored distillation strategy that\nmitigates catastrophic forgetting. To address the heterogeneity of mobility\npatterns, MoveGCL incorporates a Mixture-of-Experts Transformer with a\nmobility-aware expert routing mechanism, and employs a layer-wise progressive\nadaptation strategy to stabilize continual updates. Experiments on six\nreal-world urban datasets demonstrate that MoveGCL achieves performance\ncomparable to joint training and significantly outperforms federated learning\nbaselines, while offering strong privacy protection. MoveGCL marks a crucial\nstep toward unlocking foundation models for mobility, offering a practical\nblueprint for open, scalable, and privacy-preserving model development in the\nera of foundation models.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6465d3bd63e7e09dd02e95c3/LfNEmKBa7cYZhNNRnZRIx.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.06694.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6465d3bd63e7e09dd02e95c3",
      "avatarUrl": "/avatars/b2798bd5f8368f956bf7fab79d9432f0.svg",
      "fullname": "Jie Feng",
      "name": "JJ-TMT",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.10036",
      "authors": [
        {
          "_id": "684bc8db3b733ba33368718c",
          "name": "Javad Rajabi",
          "hidden": false
        },
        {
          "_id": "684bc8db3b733ba33368718d",
          "name": "Soroush Mehraban",
          "hidden": false
        },
        {
          "_id": "684bc8db3b733ba33368718e",
          "user": {
            "_id": "63b4b02a103617b0a5b0ee2e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b4b02a103617b0a5b0ee2e/LasBC-pCnPJ6XuCt6qqcp.jpeg",
            "isPro": false,
            "fullname": "Seyedmorteza Sadat",
            "user": "msadat97",
            "type": "user"
          },
          "name": "Seyedmorteza Sadat",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:38:32.584Z",
          "hidden": false
        },
        {
          "_id": "684bc8db3b733ba33368718f",
          "name": "Babak Taati",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-10T21:25:46.000Z",
      "submittedOnDailyAt": "2025-06-13T05:21:38.595Z",
      "title": "Token PaBERBER GUIDFLIVE Model",
      "submittedOnDailyBy": {
        "_id": "63b4b02a103617b0a5b0ee2e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b4b02a103617b0a5b0ee2e/LasBC-pCnPJ6XuCt6qqcp.jpeg",
        "isPro": false,
        "fullname": "Seyedmorteza Sadat",
        "user": "msadat97",
        "type": "user"
      },
      "summary": "La Classification des Préférences (CFG) est l'un des éléments importants des modèles de différenciation modernes et est essentiel pour améliorer la qualité de la génération et la conformité aux conditions d'entrée. Cependant, la CFG nécessite un processus d'entraînement spécifique et est limitée dans la génération conditionnelle. Pour résoudre ces limitations, nous proposons un nouveau méthode appelée Guidance par Perturbation de Tokens (TPG). Le TPG applique une matrice de perturbation directement aux représentations de tokens intermédiaires au sein de la réseau de différenciation. En utilisant des opérations de mélange pour maintenir la normalité, le TPG fournit un signal de guidance efficace et stable qui améliore la qualité de la génération malgré les changements structurels. De cette manière, le TPG peut être appliqué à la génération conditionnelle ainsi qu'à la non conditionnelle sans nécessité d'entraînement et sans dépendre des conditions d'entrée. De plus, l'analyse supplémentaire de l'information de guidance fournie par le TPG montre des méthodes de guidance qui ne nécessitent pas d'entraînement, comme le TPG, et compare leur performance à la CFG. Les expériences élargies sur SDXL et Stable Diffusion 2.1 montrent que le TPG réalise une amélioration approximativement de deux fois en qualité de la génération non conditionnelle par rapport à la ligne SDXL, et montre également une conformité similaire à la CFG en termes de qualité des prompts. Ces résultats démontrent que le TPG est un méthode de guidance qui ne dépend pas des conditions générales et établit que elle offre les mêmes avantages que la CFG dans une large gamme de modèles de différenciation. Le code est disponible sur la suivante URL.\nhttps://github.com/TaatiTeam/Token-Perturbation-Guidance",
      "upvotes": 1,
      "discussionId": "684bc8db3b733ba333687190",
      "githubRepo": "https://github.com/TaatiTeam/Token-Perturbation-Guidance",
      "ai_summary": "Token Perturbation Guidance (TPG) enhances diffusion models with condition-agnostic, training-free guidance, similar to classifier-free guidance (CFG), without requiring architectural changes.",
      "ai_keywords": [
        "classifier-free guidance (CFG)",
        "Token Perturbation Guidance (TPG)",
        "perturbation matrices",
        "intermediate token representations",
        "norm-preserving shuffling",
        "FID",
        "prompt alignment",
        "SDXL",
        "Stable Diffusion 2.1"
      ]
    },
    "publishedAt": "2025-06-10T17:25:46.000Z",
    "title": "Token Perturbation Guidance for Diffusion Models",
    "summary": "Classifier-free guidance (CFG) has become an essential component of modern\ndiffusion models to enhance both generation quality and alignment with input\nconditions. However, CFG requires specific training procedures and is limited\nto conditional generation. To address these limitations, we propose Token\nPerturbation Guidance (TPG), a novel method that applies perturbation matrices\ndirectly to intermediate token representations within the diffusion network.\nTPG employs a norm-preserving shuffling operation to provide effective and\nstable guidance signals that improve generation quality without architectural\nchanges. As a result, TPG is training-free and agnostic to input conditions,\nmaking it readily applicable to both conditional and unconditional generation.\nWe further analyze the guidance term provided by TPG and show that its effect\non sampling more closely resembles CFG compared to existing training-free\nguidance techniques. Extensive experiments on SDXL and Stable Diffusion 2.1\nshow that TPG achieves nearly a 2times improvement in FID for unconditional\ngeneration over the SDXL baseline, while closely matching CFG in prompt\nalignment. These results establish TPG as a general, condition-agnostic\nguidance method that brings CFG-like benefits to a broader class of diffusion\nmodels. The code is available at\nhttps://github.com/TaatiTeam/Token-Perturbation-Guidance",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10036.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63b4b02a103617b0a5b0ee2e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b4b02a103617b0a5b0ee2e/LasBC-pCnPJ6XuCt6qqcp.jpeg",
      "fullname": "Seyedmorteza Sadat",
      "name": "msadat97",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.08373",
      "authors": [
        {
          "_id": "684ae1f3dbd21a9cc27b0f32",
          "name": "Kevin Galim",
          "hidden": false
        },
        {
          "_id": "684ae1f3dbd21a9cc27b0f33",
          "name": "Ethan Ewer",
          "hidden": false
        },
        {
          "_id": "684ae1f3dbd21a9cc27b0f34",
          "name": "Wonjun Kang",
          "hidden": false
        },
        {
          "_id": "684ae1f3dbd21a9cc27b0f35",
          "name": "Minjae Lee",
          "hidden": false
        },
        {
          "_id": "684ae1f3dbd21a9cc27b0f36",
          "name": "Hyung Il Koo",
          "hidden": false
        },
        {
          "_id": "684ae1f3dbd21a9cc27b0f37",
          "name": "Kangwook Lee",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-10T02:37:46.000Z",
      "submittedOnDailyAt": "2025-06-13T00:35:36.456Z",
      "title": "Inférence Approximative Basée sur le Dramaturge pour les Modèles de Langue Grands",
      "submittedOnDailyBy": {
        "_id": "630c90123dc31beba6e8f406",
        "avatarUrl": "/avatars/2188b41fff122d4f5683b46c529ed79d.svg",
        "isPro": false,
        "fullname": "Kevin Galim",
        "user": "kev95",
        "type": "user"
      },
      "summary": "Optimiser l'inférence de modèles de langage à grande échelle dans des contextes de longs textes est maintenant crucial, en raison de l'importance de la complexité du calcul en deux dimensions et de la complexité mémoire linéaire dans les Transformers. Les méthodes approximatives actuelles principalement utilisent l'élimination de la cache de KV, la sparseification des paramètres de fonction et la compression des blocs pour prédire approximativement l'importance des tokens ou des pairs de KV. Nous proposons un nouveau cadre qui utilise de petits modèles de \"draft\" pour prédire avec plus de précision l'importance des tokens et des pairs de KV. En particulier, nous proposons deux implémentations : (i) SpecKV, qui utilise les sorties du modèle de \"draft\" pour évaluer précisément l'importance de chaque pair de KV, ce qui permet une élimination plus efficace de la cache de KV. (ii) SpecPC, qui utilise l'activation de l'attention du modèle de \"draft\" pour identifier et éliminer les tokens de bloc les plus importants. Selon notre expérience, ceci est le premier étude qui applique des approximations à la vitesse d'inférence de LLMs en utilisant des modèles de \"draft\". Cela élargit le rôle traditionnel de l'inférence sans perte de qualité. Nous utilisons un analyse théorique et expérimentale pour expliquer notre méthode et montrer la relation entre les motifs d'attention des modèles de \"draft\" et les modèles objectifs. A travers des expériences dans un cadre de référence de long texte, notre méthode a démontré être plus précise et améliorer la mémoire utilisée, le nombre de tours et le nombre de cycles par rapport aux standards existants. Notre code est disponible sur https://github.com/furiosa-ai/draft-based-approx-llm.",
      "upvotes": 1,
      "discussionId": "684ae1f3dbd21a9cc27b0f38",
      "ai_summary": "A new framework using draft models enhances approximate inference for long-context LLMs by better predicting token and key-value pair importance, improving accuracy while maintaining memory and compute efficiency.",
      "ai_keywords": [
        "Large Language Models",
        "Transformers",
        "key-value cache dropping",
        "sparse attention",
        "prompt compression",
        "draft models",
        "SpecKV",
        "SpecPC",
        "attention activations",
        "long-context benchmarks"
      ]
    },
    "publishedAt": "2025-06-09T22:37:46.000Z",
    "title": "Draft-based Approximate Inference for LLMs",
    "summary": "Optimizing inference for long-context Large Language Models (LLMs) is\nincreasingly important due to the quadratic compute and linear memory\ncomplexity of Transformers. Existing approximation methods, such as key-value\n(KV) cache dropping, sparse attention, and prompt compression, typically rely\non rough predictions of token or KV pair importance. We propose a novel\nframework for approximate LLM inference that leverages small draft models to\nmore accurately predict the importance of tokens and KV pairs. Specifically, we\nintroduce two instantiations of our proposed framework: (i) SpecKV, which\nleverages a draft output to accurately assess the importance of each KV pair\nfor more effective KV cache dropping, and (ii) SpecPC, which uses the draft\nmodel's attention activations to identify and discard unimportant prompt\ntokens. To the best of our knowledge, this is the first work to use draft\nmodels for approximate LLM inference acceleration, extending their utility\nbeyond traditional lossless speculative decoding. We motivate our methods with\ntheoretical and empirical analyses, and show a strong correlation between the\nattention patterns of draft and target models. Extensive experiments on\nlong-context benchmarks show that our methods consistently achieve higher\naccuracy than existing baselines, while preserving the same improvements in\nmemory usage, latency, and throughput. Our code is available at\nhttps://github.com/furiosa-ai/draft-based-approx-llm.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08373.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "630c90123dc31beba6e8f406",
      "avatarUrl": "/avatars/2188b41fff122d4f5683b46c529ed79d.svg",
      "fullname": "Kevin Galim",
      "name": "kev95",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.08234",
      "authors": [
        {
          "_id": "684ae1dddbd21a9cc27b0edc",
          "name": "Yu-Ang Lee",
          "hidden": false
        },
        {
          "_id": "684ae1dddbd21a9cc27b0edd",
          "name": "Guan-Ting Yi",
          "hidden": false
        },
        {
          "_id": "684ae1dddbd21a9cc27b0ede",
          "name": "Mei-Yi Liu",
          "hidden": false
        },
        {
          "_id": "684ae1dddbd21a9cc27b0edf",
          "name": "Jui-Chao Lu",
          "hidden": false
        },
        {
          "_id": "684ae1dddbd21a9cc27b0ee0",
          "name": "Guan-Bo Yang",
          "hidden": false
        },
        {
          "_id": "684ae1dddbd21a9cc27b0ee1",
          "name": "Yun-Nung Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-09T21:04:14.000Z",
      "submittedOnDailyAt": "2025-06-13T07:22:29.081Z",
      "title": "Optimisation des systèmes d'IA de synthèse : résumé des méthodes, problèmes et perspectives futures",
      "submittedOnDailyBy": {
        "_id": "6615752da15c52fa7ab3e2f7",
        "avatarUrl": "/avatars/37e72cfb829a42630d229080ad8d60f3.svg",
        "isPro": false,
        "fullname": "Lee",
        "user": "Speeeed",
        "type": "user"
      },
      "summary": "Récemment, le développement de grands modèles de langue (LLMs) et de systèmes d'IA a introduit un nouveau paradigme dans le design et l'optimisation de processus complexes d'IA. Les systèmes d'IA complexes ont intégré diverses composantes pour réaliser des tâches complexes. Cependant, avec la complexité de ces systèmes, des problèmes nouveaux ont émergé non seulement dans l'optimisation de chaque composante individuelle, mais également dans l'optimisation de leur interaction mutuelle. Malgré que les méthodes traditionnelles telles que l'apprentissage supervisé basé sur le texte (SFT) et l'apprentissage par renforcement (RL) restent essentielles, l'augmentation de la rétroaction naturelle a conduit à l'identification de nouvelles approches pour l'optimisation de systèmes non différenciables. Cet article étudie les avancées récentes dans l'optimisation de systèmes d'IA complexes de manière systématique et fournit des techniques numériques et basées sur le langage. Il formalise le concept d'optimisation de systèmes d'IA complexes et classe les méthodes existantes pour éclairer les recherches ouvertes et futures dans ce domaine qui évolue rapidement. La liste des articles étudiés est disponible sur https://github.com/MiuLab/AISysOpt-Survey.",
      "upvotes": 1,
      "discussionId": "684ae1dedbd21a9cc27b0ee2",
      "ai_summary": "Recent advancements in optimizing compound AI systems highlight challenges in integrating various components, with an emphasis on natural language feedback methods for non-differentiable systems.",
      "ai_keywords": [
        "large language models",
        "AI systems",
        "compound AI systems",
        "supervised fine-tuning",
        "reinforcement learning",
        "natural language feedback",
        "non-differentiable systems"
      ]
    },
    "publishedAt": "2025-06-09T17:04:14.000Z",
    "title": "Compound AI Systems Optimization: A Survey of Methods, Challenges, and\n  Future Directions",
    "summary": "Recent advancements in large language models (LLMs) and AI systems have led\nto a paradigm shift in the design and optimization of complex AI workflows. By\nintegrating multiple components, compound AI systems have become increasingly\nadept at performing sophisticated tasks. However, as these systems grow in\ncomplexity, new challenges arise in optimizing not only individual components\nbut also their interactions. While traditional optimization methods such as\nsupervised fine-tuning (SFT) and reinforcement learning (RL) remain\nfoundational, the rise of natural language feedback introduces promising new\napproaches, especially for optimizing non-differentiable systems. This paper\nprovides a systematic review of recent progress in optimizing compound AI\nsystems, encompassing both numerical and language-based techniques. We\nformalize the notion of compound AI system optimization, classify existing\nmethods along several key dimensions, and highlight open research challenges\nand future directions in this rapidly evolving field. A list of surveyed papers\nis publicly available at https://github.com/MiuLab/AISysOpt-Survey.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08234.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6615752da15c52fa7ab3e2f7",
      "avatarUrl": "/avatars/37e72cfb829a42630d229080ad8d60f3.svg",
      "fullname": "Lee",
      "name": "Speeeed",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 0
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.06950",
      "authors": [
        {
          "_id": "684ae1fbdbd21a9cc27b0f51",
          "name": "Do Xuan Long",
          "hidden": false
        },
        {
          "_id": "684ae1fbdbd21a9cc27b0f52",
          "name": "Duy Dinh",
          "hidden": false
        },
        {
          "_id": "684ae1fbdbd21a9cc27b0f53",
          "name": "Ngoc-Hai Nguyen",
          "hidden": false
        },
        {
          "_id": "684ae1fbdbd21a9cc27b0f54",
          "name": "Kenji Kawaguchi",
          "hidden": false
        },
        {
          "_id": "684ae1fbdbd21a9cc27b0f55",
          "name": "Nancy F. Chen",
          "hidden": false
        },
        {
          "_id": "684ae1fbdbd21a9cc27b0f56",
          "name": "Shafiq Joty",
          "hidden": false
        },
        {
          "_id": "684ae1fbdbd21a9cc27b0f57",
          "name": "Min-Yen Kan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-07T23:19:27.000Z",
      "submittedOnDailyAt": "2025-06-13T03:14:57.042Z",
      "title": "Que éléments sont nécessaires pour créer un bon prompt naturel ?",
      "submittedOnDailyBy": {
        "_id": "63a9a0d13453852ef53c0b37",
        "avatarUrl": "/avatars/411c4ffc2a3e89047a23c6e7442f6ed5.svg",
        "isPro": false,
        "fullname": "Do Xuan Long",
        "user": "dxlong2000",
        "type": "user"
      },
      "summary": "LLM se développe de manière presque identique à celle des humains dans le développement de la communication. Le prompt est devenu un élément déterminant. Cependant, la conscience commune conceptuelle est limitée pour quantifier les prompts de langage naturel. Plus de 150 articles et blogs sur les prompts ont été examinés dans des conférences de NLP et d'IA avancées de 2022 à 2025 pour aborder ce problème. Des propriétés et un cadre centré sur l'être humain, classifiées en 6 dimensions à partir de 21 propriétés, sont proposées. La répercussion de la recherche actuelle sur les LLMs est également examinée, révélant que l'équilibre entre modèles et tâches peut être déséquilibré et que les défauts de la recherche peuvent être significatifs. De plus, les corrélations entre les propriétés des prompts de haute qualité de langage naturel sont analysées, et des recommandations pour les prompts sont obtenues. La meilleure de ces propriétés est expérimentalement améliorée, et on constate que l'amélioration d'une seule propriété a un impact le plus grand. Enfin, les modèles logiques sont améliorés lors de l'entraînement avec des prompts qui incluent des propriétés étendues. Nos résultats préparent la base pour l'évaluation et l'optimisation axées sur les propriétés des prompts, augmentent la différence entre l'être humain et l'IA, et ouvrent de nouvelles directions pour la recherche sur les prompts.",
      "upvotes": 1,
      "discussionId": "684ae1fbdbd21a9cc27b0f58",
      "ai_summary": "A framework for evaluating and optimizing natural language prompts in large language models is proposed, revealing correlations between prompt properties and their impact on reasoning tasks.",
      "ai_keywords": [
        "large language models",
        "prompting",
        "meta-analysis",
        "property-centric framework",
        "instruction-tuning",
        "reasoning tasks"
      ]
    },
    "publishedAt": "2025-06-07T19:19:27.000Z",
    "title": "What Makes a Good Natural Language Prompt?",
    "summary": "As large language models (LLMs) have progressed towards more human-like and\nhuman--AI communications have become prevalent, prompting has emerged as a\ndecisive component. However, there is limited conceptual consensus on what\nexactly quantifies natural language prompts. We attempt to address this\nquestion by conducting a meta-analysis surveying more than 150\nprompting-related papers from leading NLP and AI conferences from 2022 to 2025\nand blogs. We propose a property- and human-centric framework for evaluating\nprompt quality, encompassing 21 properties categorized into six dimensions. We\nthen examine how existing studies assess their impact on LLMs, revealing their\nimbalanced support across models and tasks, and substantial research gaps.\nFurther, we analyze correlations among properties in high-quality natural\nlanguage prompts, deriving prompting recommendations. We then empirically\nexplore multi-property prompt enhancements in reasoning tasks, observing that\nsingle-property enhancements often have the greatest impact. Finally, we\ndiscover that instruction-tuning on property-enhanced prompts can result in\nbetter reasoning models. Our findings establish a foundation for\nproperty-centric prompt evaluation and optimization, bridging the gaps between\nhuman--AI communication and opening new prompting research directions.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.06950.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a9a0d13453852ef53c0b37",
      "avatarUrl": "/avatars/411c4ffc2a3e89047a23c6e7442f6ed5.svg",
      "fullname": "Do Xuan Long",
      "name": "dxlong2000",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.06561",
      "authors": [
        {
          "_id": "684b88113b733ba333686fc7",
          "name": "Ho Yin 'Sam' Ng",
          "hidden": false
        },
        {
          "_id": "684b88113b733ba333686fc8",
          "name": "Ting-Yao Hsu",
          "hidden": false
        },
        {
          "_id": "684b88113b733ba333686fc9",
          "name": "Aashish Anantha Ramakrishnan",
          "hidden": false
        },
        {
          "_id": "684b88113b733ba333686fca",
          "name": "Branislav Kveton",
          "hidden": false
        },
        {
          "_id": "684b88113b733ba333686fcb",
          "name": "Nedim Lipka",
          "hidden": false
        },
        {
          "_id": "684b88113b733ba333686fcc",
          "user": {
            "_id": "62c5947524171688a9feb992",
            "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
            "isPro": false,
            "fullname": "Franck Dernoncourt",
            "user": "Franck-Dernoncourt",
            "type": "user"
          },
          "name": "Franck Dernoncourt",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:39:49.692Z",
          "hidden": false
        },
        {
          "_id": "684b88113b733ba333686fcd",
          "name": "Dongwon Lee",
          "hidden": false
        },
        {
          "_id": "684b88113b733ba333686fce",
          "name": "Tong Yu",
          "hidden": false
        },
        {
          "_id": "684b88113b733ba333686fcf",
          "name": "Sungchul Kim",
          "hidden": false
        },
        {
          "_id": "684b88113b733ba333686fd0",
          "name": "Ryan A. Rossi",
          "hidden": false
        },
        {
          "_id": "684b88113b733ba333686fd1",
          "name": "Ting-Hao 'Kenneth' Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-06T22:16:16.000Z",
      "submittedOnDailyAt": "2025-06-13T00:38:33.715Z",
      "title": "LaMP-Cap : Génération de captures de fixation personnalisées via des profils de fixation multimodals",
      "submittedOnDailyBy": {
        "_id": "62c5947524171688a9feb992",
        "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
        "isPro": false,
        "fullname": "Franck Dernoncourt",
        "user": "Franck-Dernoncourt",
        "type": "user"
      },
      "summary": "Le titre d'un graphique est crucial pour que les lecteurs puissent comprendre et se souvenir de la principale messagerie du graphique. De nombreux modèles ont été développés pour générer ces titres. Ces modèles aident les auteurs à créer des titres de meilleure qualité de manière simple. Cependant, les auteurs doivent éditer les titres standards générés par l'IA pour les ajuster aux styles d'écriture et aux domaines spécifiques. Cela souligne clairement la nécessité de personnalisation. Malgré le développement de LaMP (Personnalisation de Modèle de Langue), ces technologies se concentrent principalement sur la configuration du langage et peu d'études sont menées sur les cas d'entrées et de profils divers. Dans cet article, on présente LaMP-Cap. LaMP-Cap fournit un ensemble de données pour la génération de titres personnalisés de graphiques en utilisant des fichiers de profils de différents graphiques. LaMP-Cap fournit l'information nécessaire pour le graphique en question, ainsi qu'un profil qui comprend trois images de graphiques, des titres et des phrases qui mentionnent le graphique, caractérisant le contexte. Les expériences avec quatre modèles de langage (LLM) montrent que les titres sont générés à l'aide de l'information du profil, et que ceux-ci sont similaires à ceux que l'auteur original a écrit. Les tests d'exclusion montrent que les images du profil sont plus utiles que les phrases qui mentionnent le graphique, et que l'utilisation de divers profils est plus bénéfique que l'utilisation seule du langage.",
      "upvotes": 1,
      "discussionId": "684b88123b733ba333686fd2",
      "ai_summary": "LaMP-Cap introduces a dataset for personalized figure caption generation using multimodal profiles to improve the quality of AI-generated captions.",
      "ai_keywords": [
        "LaMP-Cap",
        "personalized figure caption generation",
        "multimodal figures",
        "figure profiles",
        "ablation studies"
      ]
    },
    "publishedAt": "2025-06-06T18:16:16.000Z",
    "title": "LaMP-Cap: Personalized Figure Caption Generation With Multimodal Figure\n  Profiles",
    "summary": "Figure captions are crucial for helping readers understand and remember a\nfigure's key message. Many models have been developed to generate these\ncaptions, helping authors compose better quality captions more easily. Yet,\nauthors almost always need to revise generic AI-generated captions to match\ntheir writing style and the domain's style, highlighting the need for\npersonalization. Despite language models' personalization (LaMP) advances,\nthese technologies often focus on text-only settings and rarely address\nscenarios where both inputs and profiles are multimodal. This paper introduces\nLaMP-Cap, a dataset for personalized figure caption generation with multimodal\nfigure profiles. For each target figure, LaMP-Cap provides not only the needed\ninputs, such as figure images, but also up to three other figures from the same\ndocument--each with its image, caption, and figure-mentioning paragraphs--as a\nprofile to characterize the context. Experiments with four LLMs show that using\nprofile information consistently helps generate captions closer to the original\nauthor-written ones. Ablation studies reveal that images in the profile are\nmore helpful than figure-mentioning paragraphs, highlighting the advantage of\nusing multimodal profiles over text-only ones.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.06561.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62c5947524171688a9feb992",
      "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
      "fullname": "Franck Dernoncourt",
      "name": "Franck-Dernoncourt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.05982",
      "authors": [
        {
          "_id": "684b86913b733ba333686fb8",
          "name": "Zonglin Wu",
          "hidden": false
        },
        {
          "_id": "684b86913b733ba333686fb9",
          "name": "Yule Xue",
          "hidden": false
        },
        {
          "_id": "684b86913b733ba333686fba",
          "name": "Xin Wei",
          "hidden": false
        },
        {
          "_id": "684b86913b733ba333686fbb",
          "name": "Yiren Song",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-06T11:02:01.000Z",
      "submittedOnDailyAt": "2025-06-13T00:33:34.648Z",
      "title": "MCA-Bench : Critère d'Évaluation de la Diversité pour l'Évaluation de la Résistance de CAPTCHA Face aux Attaques basées sur des VLM",
      "submittedOnDailyBy": {
        "_id": "64311a95034ecbefddd141ef",
        "avatarUrl": "/avatars/b6dc5ca373bedbaa368208517954c375.svg",
        "isPro": true,
        "fullname": "Yiren Song",
        "user": "yiren98",
        "type": "user"
      },
      "summary": "Les méthodes d'attaques automatisées se développent rapidement, ce qui rend la CAPTCHA une structure de défense importante pour bloquer les bots malicieux. Cependant, bien que les scanneurs de CAPTCHA actuels aient élargi leur portée pour inclure diverses modalités, comme des lettres en rotation statique, des images polluées par du bruit, des clics croisés, des puzzles de glissement et des questions basées sur la logique, la communauté n'a toujours pas un cadre de référence unifié et à grande échelle qui inclue diverses modalités. Pour résoudre cette limitation, nous présentons MCA-Bench, un système de référence de test intégré qui inclut divers types de CAPTCHA. En utilisant le rétrotrack de modèles de vision partagés, nous ajustons des crânes spécialisés pour chaque catégorie de CAPTCHA et permettons une évaluation cohérente et croisée de modalités. À travers une large gamme d'expériences, MCA-Bench a efficacement converti le spectre de vulnérabilités des designs modernes de CAPTCHA en un banc de mots, et, de manière importante, fournit pour la première fois un analyse quantitative de la complexité des défis, de la profondeur de l'interaction et de la relation entre la résolution du modèle. Sur ces découvertes, nous proposons trois principes opérationnels de conception, identifions les problèmes principaux ouverts, et établissons une base pour le renforcement systématique des CAPTCHA, la création d'un cadre de référence juste et la construction d'une coopération plus large de la communauté. Le jeu de données et le code sont disponibles en ligne.",
      "upvotes": 1,
      "discussionId": "684b86923b733ba333686fbc",
      "ai_summary": "MCA-Bench provides a unified benchmark for evaluating CAPTCHA security using a shared vision-language model and attackers specialized for each type of CAPTCHA.",
      "ai_keywords": [
        "vision-language model",
        "CAPTCHA",
        "benchmark",
        "evaluation protocol",
        "cracking agents",
        "vulnerability spectrum",
        "challenge complexity",
        "interaction depth",
        "model solvability"
      ]
    },
    "publishedAt": "2025-06-06T07:02:01.000Z",
    "title": "MCA-Bench: A Multimodal Benchmark for Evaluating CAPTCHA Robustness\n  Against VLM-based Attacks",
    "summary": "As automated attack techniques rapidly advance, CAPTCHAs remain a critical\ndefense mechanism against malicious bots. However, existing CAPTCHA schemes\nencompass a diverse range of modalities -- from static distorted text and\nobfuscated images to interactive clicks, sliding puzzles, and logic-based\nquestions -- yet the community still lacks a unified, large-scale, multimodal\nbenchmark to rigorously evaluate their security robustness. To address this\ngap, we introduce MCA-Bench, a comprehensive and reproducible benchmarking\nsuite that integrates heterogeneous CAPTCHA types into a single evaluation\nprotocol. Leveraging a shared vision-language model backbone, we fine-tune\nspecialized cracking agents for each CAPTCHA category, enabling consistent,\ncross-modal assessments. Extensive experiments reveal that MCA-Bench\neffectively maps the vulnerability spectrum of modern CAPTCHA designs under\nvaried attack settings, and crucially offers the first quantitative analysis of\nhow challenge complexity, interaction depth, and model solvability interrelate.\nBased on these findings, we propose three actionable design principles and\nidentify key open challenges, laying the groundwork for systematic CAPTCHA\nhardening, fair benchmarking, and broader community collaboration. Datasets and\ncode are available online.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05982.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64311a95034ecbefddd141ef",
      "avatarUrl": "/avatars/b6dc5ca373bedbaa368208517954c375.svg",
      "fullname": "Yiren Song",
      "name": "yiren98",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 22
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.10378",
      "authors": [
        {
          "_id": "684bb08e3b733ba3336870bf",
          "name": "Jikai Jin",
          "hidden": false
        },
        {
          "_id": "684bb08e3b733ba3336870c0",
          "name": "Vasilis Syrgkanis",
          "hidden": false
        },
        {
          "_id": "684bb08e3b733ba3336870c1",
          "name": "Sham Kakade",
          "hidden": false
        },
        {
          "_id": "684bb08e3b733ba3336870c2",
          "name": "Hanlin Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-12T06:07:42.000Z",
      "submittedOnDailyAt": "2025-06-13T03:36:14.455Z",
      "title": "Débarquillement de la capacité de rétention des whirlixiques par un module de format et par apprentissage régulé causalement",
      "submittedOnDailyBy": {
        "_id": "624054bcc2c17da6a63eb539",
        "avatarUrl": "/avatars/bf52dc0683b4100733f8696a97696d0e.svg",
        "isPro": false,
        "fullname": "hlzhang109",
        "user": "hlzhang109",
        "type": "user"
      },
      "summary": "L'évaluation des capacités de modèles de langage fiables est un élément important dans le processus de développement. Cependant, dans ce domaine, il existe de grands problèmes méthodologiques, comme les effets complexes de la confusion et les coûts de calcul élevés liés à l'expansion de l'entraînement de plusieurs modèles. Pour résoudre ces problèmes, nous proposons un cadre d'apprentissage causal qui modélise le rendement des benchmarks observés à travers une transformation linéaire de quelques coefficients potentiels de capacité. Il est crucial que ces coefficients potentiels puissent contrôler adéquatement la confusion commune du modèle et reconnaître les relations causales entre eux. En appliquant cette approche à un ensemble détaillé de 1500 modèles, comprenant 6 benchmarks sélectionnés sur le Open LLM Leaderboard, nous avons démontré que les changements dans le rendement observé peuvent être expliqués par une structure causale linéaire de confiance en trois nœuds. L'interprétation supplémentaire de cette structure causale fournit une compréhension scientifique plus profonde que des simples classements numériques, allant du pouvoir de résoudre des problèmes généraux jusqu'à la démonstration d'une direction causale claire dans la capacité de raisonnement mathématique. Nos résultats soulignent l'importance de contrôler soigneusement les changements dans le modèle de base lors de l'évaluation et expliquent que c'est un pas essentiel pour clarifier les possibles relations causales potentielles entre les capacités du modèle.",
      "upvotes": 0,
      "discussionId": "684bb08e3b733ba3336870c3",
      "ai_summary": "A causal representation learning framework identifies a concise causal structure to explain performance variations in language models across benchmarks by controlling for base model variations.",
      "ai_keywords": [
        "causal representation learning",
        "latent capability factors",
        "causal interrelated",
        "base model confounder",
        "Open LLM Leaderboard",
        "linear causal structure",
        "general problem-solving capabilities",
        "instruction-following proficiency",
        "mathematical reasoning ability"
      ]
    },
    "publishedAt": "2025-06-12T02:07:42.000Z",
    "title": "Discovering Hierarchical Latent Capabilities of Language Models via\n  Causal Representation Learning",
    "summary": "Faithful evaluation of language model capabilities is crucial for deriving\nactionable insights that can inform model development. However, rigorous causal\nevaluations in this domain face significant methodological challenges,\nincluding complex confounding effects and prohibitive computational costs\nassociated with extensive retraining. To tackle these challenges, we propose a\ncausal representation learning framework wherein observed benchmark performance\nis modeled as a linear transformation of a few latent capability factors.\nCrucially, these latent factors are identified as causally interrelated after\nappropriately controlling for the base model as a common confounder. Applying\nthis approach to a comprehensive dataset encompassing over 1500 models\nevaluated across six benchmarks from the Open LLM Leaderboard, we identify a\nconcise three-node linear causal structure that reliably explains the observed\nperformance variations. Further interpretation of this causal structure\nprovides substantial scientific insights beyond simple numerical rankings:\nspecifically, we reveal a clear causal direction starting from general\nproblem-solving capabilities, advancing through instruction-following\nproficiency, and culminating in mathematical reasoning ability. Our results\nunderscore the essential role of carefully controlling base model variations\nduring evaluation, a step critical to accurately uncovering the underlying\ncausal relationships among latent model capabilities.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10378.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "624054bcc2c17da6a63eb539",
      "avatarUrl": "/avatars/bf52dc0683b4100733f8696a97696d0e.svg",
      "fullname": "hlzhang109",
      "name": "hlzhang109",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.08862",
      "authors": [
        {
          "_id": "684ae26ddbd21a9cc27b117f",
          "name": "Zike Wu",
          "hidden": false
        },
        {
          "_id": "684ae26ddbd21a9cc27b1180",
          "name": "Qi Yan",
          "hidden": false
        },
        {
          "_id": "684ae26ddbd21a9cc27b1181",
          "name": "Xuanyu Yi",
          "hidden": false
        },
        {
          "_id": "684ae26ddbd21a9cc27b1182",
          "name": "Lele Wang",
          "hidden": false
        },
        {
          "_id": "684ae26ddbd21a9cc27b1183",
          "name": "Renjie Liao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-10T14:52:36.000Z",
      "submittedOnDailyAt": "2025-06-13T07:33:07.278Z",
      "title": "StreamSplat : Vers la reconstruction dynamique 3D en ligne depuis un flux vidéo non corrigé",
      "submittedOnDailyBy": {
        "_id": "648058ff8c6a3b8f11f77893",
        "avatarUrl": "/avatars/043c832314a8d6713af90d7c255fc2f2.svg",
        "isPro": false,
        "fullname": "Wu Zike",
        "user": "Nickwzk",
        "type": "user"
      },
      "summary": "Dans un espace 3D dynamique, le streaming de vidéos de textures sans défi et leur reconstruction en unités de temps sont essentiels pour de nombreuses applications en réalité virtuelle. Cependant, les méthodes existantes rencontrent des difficultés pour résoudre simultanément trois problèmes principaux : 1) traiter l'entrée de textures sans défi en unités de temps, 2) modéliser précisément l'évolution de l'espace dynamique, et 3) maintenir la stabilité à long terme et l'efficacité en calcul. Dans ce contexte, nous présentons le premier cadre de propagation complet pour transformer le streaming de textures sans défi de longueurs arbitraires en une représentation de Gauss Dynamique 3D (3DGS). Ce cadre permet de reconstruire le mouvement de l'espace à partir d'observations temporelles proches. Nous proposons deux innovations techniques fondamentales : une structure de sampling probabiliste pour la prédiction de positions dans le 3DGS à partir d'un encodeur statique, et un champ de déformation Bilinear dans le décodeur dynamique, qui sont adaptés pour modéliser dynamiquement avec robustesse et efficacité. À travers des expériences étendues dans des cadres statiques et dynamiques, StreamSplat montre une excellente qualité de reconstruction et un excellent modèle spatial dynamique par rapport aux travaux précédents, en particulier en soutenant la reconstruction en ligne de vidéos de longueurs arbitraires. Le code et les modèles sont disponibles sur https://github.com/nickwzk/StreamSplat.",
      "upvotes": 0,
      "discussionId": "684ae26ddbd21a9cc27b1184",
      "ai_summary": "StreamSplat, a fully feed-forward framework, addresses real-time 3D scene reconstruction from uncalibrated video with accurate dynamics and long-term stability.",
      "ai_keywords": [
        "3D Gaussian Splatting",
        "3DGS",
        "probabilistic sampling mechanism",
        "bidirectional deformation field",
        "online reconstruction"
      ]
    },
    "publishedAt": "2025-06-10T10:52:36.000Z",
    "title": "StreamSplat: Towards Online Dynamic 3D Reconstruction from Uncalibrated\n  Video Streams",
    "summary": "Real-time reconstruction of dynamic 3D scenes from uncalibrated video streams\nis crucial for numerous real-world applications. However, existing methods\nstruggle to jointly address three key challenges: 1) processing uncalibrated\ninputs in real time, 2) accurately modeling dynamic scene evolution, and 3)\nmaintaining long-term stability and computational efficiency. To this end, we\nintroduce StreamSplat, the first fully feed-forward framework that transforms\nuncalibrated video streams of arbitrary length into dynamic 3D Gaussian\nSplatting (3DGS) representations in an online manner, capable of recovering\nscene dynamics from temporally local observations. We propose two key technical\ninnovations: a probabilistic sampling mechanism in the static encoder for 3DGS\nposition prediction, and a bidirectional deformation field in the dynamic\ndecoder that enables robust and efficient dynamic modeling. Extensive\nexperiments on static and dynamic benchmarks demonstrate that StreamSplat\nconsistently outperforms prior works in both reconstruction quality and dynamic\nscene modeling, while uniquely supporting online reconstruction of arbitrarily\nlong video streams. Code and models are available at\nhttps://github.com/nickwzk/StreamSplat.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08862.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "648058ff8c6a3b8f11f77893",
      "avatarUrl": "/avatars/043c832314a8d6713af90d7c255fc2f2.svg",
      "fullname": "Wu Zike",
      "name": "Nickwzk",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  }
]