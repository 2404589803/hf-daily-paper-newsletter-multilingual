[
  {
    "paper": {
      "id": "2507.01853",
      "authors": [
        {
          "_id": "686b4e69213f123a1f88bd76",
          "name": "Samridhi Raj Sinha",
          "hidden": false
        },
        {
          "_id": "686b4e69213f123a1f88bd77",
          "name": "Rajvee Sheth",
          "hidden": false
        },
        {
          "_id": "686b4e69213f123a1f88bd78",
          "name": "Abhishek Upperwal",
          "hidden": false
        },
        {
          "_id": "686b4e69213f123a1f88bd79",
          "name": "Mayank Singh",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-02T16:07:54.000Z",
      "submittedOnDailyAt": "2025-07-07T03:06:37.666Z",
      "title": "ECA-EVAL : Évaluation Détaillée des Modèles de Langue Indigène",
      "submittedOnDailyBy": {
        "_id": "66e1425c919f283fbd7dfb5e",
        "avatarUrl": "/avatars/b43ee9b9042486b5faf91472b4c49c5d.svg",
        "isPro": false,
        "fullname": "Rajvee Sheth",
        "user": "RajveeSheth",
        "type": "user"
      },
      "summary": "Le rapide développement des LLM a mis en évidence la nécessité d'un cadre d'évaluation qui dépasse les indicateurs de tests axés sur l'anglais et satisfait les exigences des communautés qui utilisent divers langues. Nous présentons EKA-EVAL, un cadre d'évaluation prêt à l'emploi et unifié. Ce cadre intègre plus de 35 indicateurs de tests et comprend plus de 10 datasets propres à l'Indic, ainsi que des catégories variées telles que le raisonnement, les mathématiques, l'utilisation de outils, la compréhension de textes longs et l'évaluation de la lecture. En comparaison avec les outils d'évaluation actuels en indien, EKA-EVAL offre des fonctionnalités telles que l'inférence distribuée, le rendement et l'utilisation de plusieurs GPU, étendant la couverture des indicateurs de tests. Notre comparaison systématique signifie que EKA-EVAL devient le premier système d'évaluation extensible pour les LLM mondiaux et indien. Il a réduit significativement les barrières des indicateurs de tests multilingues. Le cadre est open-source et est disponible sur https://github.com/lingo-iitgn/eka-eval. Il fait partie du lancement de EKA, qui est en développement sur https://eka.soket.ai, avec l'objectif de s'étendre à plus de 100 indicateurs de tests et de construire un solide écosystème d'évaluation multilingue.",
      "upvotes": 2,
      "discussionId": "686b4e69213f123a1f88bd7a",
      "ai_summary": "EKA-EVAL is a comprehensive multilingual evaluation framework for large language models, supporting diverse benchmarks and features for efficient distributed inference and GPU usage.",
      "ai_keywords": [
        "Large Language Models",
        "LLMs",
        "unified evaluation framework",
        "production-ready",
        "Indic-specific datasets",
        "reasoning",
        "mathematics",
        "tool use",
        "long-context understanding",
        "reading comprehension",
        "distributed inference",
        "quantization",
        "multi-GPU usage",
        "end-to-end",
        "extensible evaluation suite",
        "multilingual benchmarking",
        "open-source"
      ]
    },
    "publishedAt": "2025-07-02T12:07:54.000Z",
    "title": "Eka-Eval : A Comprehensive Evaluation Framework for Large Language\n  Models in Indian Languages",
    "summary": "The rapid advancement of Large Language Models (LLMs) has intensified the\nneed for evaluation frameworks that go beyond English centric benchmarks and\naddress the requirements of linguistically diverse regions such as India. We\npresent EKA-EVAL, a unified and production-ready evaluation framework that\nintegrates over 35 benchmarks, including 10 Indic-specific datasets, spanning\ncategories like reasoning, mathematics, tool use, long-context understanding,\nand reading comprehension. Compared to existing Indian language evaluation\ntools, EKA-EVAL offers broader benchmark coverage, with built-in support for\ndistributed inference, quantization, and multi-GPU usage. Our systematic\ncomparison positions EKA-EVAL as the first end-to-end, extensible evaluation\nsuite tailored for both global and Indic LLMs, significantly lowering the\nbarrier to multilingual benchmarking. The framework is open-source and publicly\navailable at https://github.com/lingo-iitgn/ eka-eval and a part of ongoing EKA\ninitiative (https://eka.soket.ai), which aims to scale up to over 100\nbenchmarks and establish a robust, multilingual evaluation ecosystem for LLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.01853.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66e1425c919f283fbd7dfb5e",
      "avatarUrl": "/avatars/b43ee9b9042486b5faf91472b4c49c5d.svg",
      "fullname": "Rajvee Sheth",
      "name": "RajveeSheth",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.01955",
      "authors": [
        {
          "_id": "686b8347213f123a1f88bdc8",
          "name": "Rahul Ramachandran",
          "hidden": false
        },
        {
          "_id": "686b8347213f123a1f88bdc9",
          "name": "Ali Garjani",
          "hidden": false
        },
        {
          "_id": "686b8347213f123a1f88bdca",
          "name": "Roman Bachmann",
          "hidden": false
        },
        {
          "_id": "686b8347213f123a1f88bdcb",
          "name": "Andrei Atanov",
          "hidden": false
        },
        {
          "_id": "686b8347213f123a1f88bdcc",
          "name": "Oğuzhan Fatih Kar",
          "hidden": false
        },
        {
          "_id": "686b8347213f123a1f88bdcd",
          "name": "Amir Zamir",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-02T17:59:07.000Z",
      "submittedOnDailyAt": "2025-07-07T06:51:04.452Z",
      "title": "GPT-4o évalue dans quelle mesure il comprend la vision. L'évaluation se fait en se basant sur des tâches standards de vision par ordinateur.",
      "submittedOnDailyBy": {
        "_id": "5f1158120c833276f61f1a84",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
        "isPro": true,
        "fullname": "Niels Rogge",
        "user": "nielsr",
        "type": "user"
      },
      "summary": "Les modèles de transformeurs (par exemple, GPT-4o) ont connu un développement impressionnant récemment, mais il n'est pas encore clair leur capacité à comprendre la vision. Dans cet article, nous évaluons les performances des modèles de transformeurs populaires dans des tâches de vision par ordinateur standard (visualisation textuelle, détection d'objets, classification d'images, prédiction de profondeur et normalisation de surfaces) en utilisant des jeux de données existants (comme COCO, ImageNet et leurs variantes).\n\nPour réaliser cette évaluation, nous identifions les principales difficultés suivantes : 1) de nombreux modèles ont été entraînés pour générer du texte, ce qui limite leur capacité à représenter de manière nature des phrases ou des zones 3D. 2) De nombreux modèles récents sont conçus pour des logiciels spécialisés, ce qui limite l'accès à l'API et ne permet pas la manipulation des poids. Pour résoudre ces problèmes, nous traduisons les tâches de vision par ordinateur standard en tâches de texte synthétiques et API compatibles, et nous créons un cadre d'évaluation standardisé en utilisant des techniques de synthèse de texte.\n\nNous confirmons les points suivants sur le rendement de ces modèles : 1) ils ne montrent pas un rendement comparable aux modèles professionnels les plus avancés dans aucune tâche, 2) ils sont des modèles généraux, ce qui est principalement dû à leur entraînement sur des tâches basées sur des images et du texte, 3) leur rendement dans des tâches textuelles est particulièrement bon, 4) la technique de synthèse de texte influence le rendement, mais les meilleurs modèles sont moins sensibles aux changements dans le texte. 5) GPT-4o est le meilleur modèle parmi les non-inférentiels, et a occupé le premier rang sur quatre des six tâches évaluées. 6) Les modèles d'inférence (par exemple, o3) montrent des améliorations dans des tâches générales, 7) l'analyse initiale des modèles comme GPT-4o, qui possèdent des fonctions de génération d'images, montre des caractéristiques telles que des erreurs d'hallucination et d'ajustement spatial.",
      "upvotes": 0,
      "discussionId": "686b8348213f123a1f88bdce",
      "ai_summary": "Multimodal foundation models, despite being primarily trained on image-text tasks, demonstrate respectable performance across various vision tasks when adapted through prompt chaining, though they fall short compared to specialized models.",
      "ai_keywords": [
        "GPT-4o",
        "o4-mini",
        "Gemini 1.5 Pro",
        "Gemini 2.0 Flash",
        "Claude 3.5 Sonnet",
        "Qwen2-VL",
        "Llama 3.2",
        "semantic segmentation",
        "object detection",
        "image classification",
        "depth prediction",
        "surface normal prediction",
        "COCO",
        "ImageNet",
        "prompt chaining",
        "reasoning models",
        "hallucinations",
        "spatial misalignments"
      ]
    },
    "publishedAt": "2025-07-02T13:59:07.000Z",
    "title": "How Well Does GPT-4o Understand Vision? Evaluating Multimodal Foundation\n  Models on Standard Computer Vision Tasks",
    "summary": "Multimodal foundation models, such as GPT-4o, have recently made remarkable\nprogress, but it is not clear where exactly these models stand in terms of\nunderstanding vision. In this paper, we benchmark the performance of popular\nmultimodal foundation models (GPT-4o, o4-mini, Gemini 1.5 Pro and Gemini 2.0\nFlash, Claude 3.5 Sonnet, Qwen2-VL, Llama 3.2) on standard computer vision\ntasks (semantic segmentation, object detection, image classification, depth and\nsurface normal prediction) using established datasets (e.g., COCO, ImageNet and\nits variants, etc).\n  The main challenges to performing this are: 1) most models are trained to\noutput text and cannot natively express versatile domains, such as segments or\n3D geometry, and 2) many leading models are proprietary and accessible only at\nan API level, i.e., there is no weight access to adapt them. We address these\nchallenges by translating standard vision tasks into equivalent text-promptable\nand API-compatible tasks via prompt chaining to create a standardized\nbenchmarking framework.\n  We observe that 1) the models are not close to the state-of-the-art\nspecialist models at any task. However, 2) they are respectable generalists;\nthis is remarkable as they are presumably trained on primarily image-text-based\ntasks. 3) They perform semantic tasks notably better than geometric ones. 4)\nWhile the prompt-chaining techniques affect performance, better models exhibit\nless sensitivity to prompt variations. 5) GPT-4o performs the best among\nnon-reasoning models, securing the top position in 4 out of 6 tasks, 6)\nreasoning models, e.g. o3, show improvements in geometric tasks, and 7) a\npreliminary analysis of models with native image generation, like the latest\nGPT-4o, shows they exhibit quirks like hallucinations and spatial\nmisalignments.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.01955.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5f1158120c833276f61f1a84",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
      "fullname": "Niels Rogge",
      "name": "nielsr",
      "type": "user",
      "isPro": true,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 908
    },
    "isAuthorParticipating": false
  }
]