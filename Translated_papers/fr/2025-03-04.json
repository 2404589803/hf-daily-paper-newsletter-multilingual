[
  {
    "paper": {
      "id": "2503.01785",
      "authors": [
        {
          "_id": "67c6816614a1bf9855188b8b",
          "user": {
            "_id": "66fe1334ff3ee1f7569fab6d",
            "avatarUrl": "/avatars/6868b1a545028a9b8bbded52490dc093.svg",
            "isPro": false,
            "fullname": "ziyuliu",
            "user": "ziyuliu",
            "type": "user"
          },
          "name": "Ziyu Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:12:57.481Z",
          "hidden": false
        },
        {
          "_id": "67c6816614a1bf9855188b8c",
          "user": {
            "_id": "63fda3fced9eead590ff6918",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1677566802735-noauth.jpeg",
            "isPro": false,
            "fullname": "Zeyi Sun",
            "user": "Zery",
            "type": "user"
          },
          "name": "Zeyi Sun",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-04T08:35:03.275Z",
          "hidden": false
        },
        {
          "_id": "67c6816614a1bf9855188b8d",
          "user": {
            "_id": "63859cf3b2906edaf83af9f0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63859cf3b2906edaf83af9f0/iUQm5FAomzqYi6fkqIn9F.jpeg",
            "isPro": false,
            "fullname": "Yuhang Zang",
            "user": "yuhangzang",
            "type": "user"
          },
          "name": "Yuhang Zang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:12:32.723Z",
          "hidden": false
        },
        {
          "_id": "67c6816614a1bf9855188b8e",
          "user": {
            "_id": "67c0849ee08c178ef8d4e05c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/mQ6VdnjZnRhb0H_waPclo.png",
            "isPro": false,
            "fullname": "Xiaoyi Dong",
            "user": "sweetFruit",
            "type": "user"
          },
          "name": "Xiaoyi Dong",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:12:25.627Z",
          "hidden": false
        },
        {
          "_id": "67c6816614a1bf9855188b8f",
          "user": {
            "_id": "65000bef18830fabea469fdd",
            "avatarUrl": "/avatars/b320c77dfad039d9f9c54127f610d44f.svg",
            "isPro": false,
            "fullname": "Cao Yuhang",
            "user": "yhcao",
            "type": "user"
          },
          "name": "Yuhang Cao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:12:19.177Z",
          "hidden": false
        },
        {
          "_id": "67c6816614a1bf9855188b90",
          "user": {
            "_id": "63ee1379190ddd6214efd73a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676546883247-noauth.png",
            "isPro": false,
            "fullname": "HAODONG DUAN",
            "user": "KennyUTC",
            "type": "user"
          },
          "name": "Haodong Duan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:12:05.281Z",
          "hidden": false
        },
        {
          "_id": "67c6816614a1bf9855188b91",
          "user": {
            "_id": "636317ed80c1a705a6eff396",
            "avatarUrl": "/avatars/3db090e101b916d9256d0d3e043db71d.svg",
            "isPro": false,
            "fullname": "Dahua Lin",
            "user": "lindahua",
            "type": "user"
          },
          "name": "Dahua Lin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:11:57.087Z",
          "hidden": false
        },
        {
          "_id": "67c6816614a1bf9855188b92",
          "user": {
            "_id": "64638c4d51fa6e63060521b5",
            "avatarUrl": "/avatars/c863ace5b1dc788a341bcf4ddbdfaec1.svg",
            "isPro": false,
            "fullname": "JIaqi",
            "user": "Jiaqiwang",
            "type": "user"
          },
          "name": "Jiaqi Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:11:48.889Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-03T18:16:32.000Z",
      "title": "Visual-RFT : Régulation Forte de Vision Micro",
      "summary": "La Reinforcement Fine-Tuning (RFT) est une technique appliquée à des grands modèles de logique, tels que OpenAI ou 1, pour qu'ils apprennent à fournir des rétroactions sur les réponses, ce qui est particulièrement utile lorsque les données de fine-tuning sont limitées. Dans des travaux récents, des modèles comme DeepSeek-R1 ont montré que l'apprentissage par renforcement avec des récompenses vérifiables peut être une voie importante pour repliquer des modèles comme OpenAI. Les modèles de type R1 ont réussi dans les modèles de langue, mais leur application dans diverses domaines nécessite encore plus d'investigation. Dans ce travail, nous présentons l'Introduction de la Visual Reinforcement Fine-Tuning (Visual-RFT), ce qui élargit l'application de RFT aux tâches visuelles. Spécifiquement, Visual-RFT utilise des Grands Modèles de Vision et de Langue (LVLMs) pour générer plusieurs réponses, chacune avec un token de raisonnement et une réponse finale, et met à jour le modèle en utilisant des algorithmes d'optimisation de politiques comme Group Relative Policy Optimization (GRPO) et une fonction de récompense vérifiable de reconnaissance visuelle proposée. Des fonctions de récompense vérifiables sont conçues, telles que la récompense de Intersection over Union (IoU), pour différentes tâches de reconnaissance. À travers des expériences sur des benchmarks de classification d'images détaillées, de détection d'objets peu supervisées, de perception de raisonnement et de détection d'objets dans des boîtes ouvertes, nous montrons que Visual-RFT présente un rendement compétitif et une forte capacité de généralisation par rapport au Fine-Tuning Supervisé (SFT). Par exemple, dans la classification d'images détaillées avec un seul exemple, Visual-RFT augmente la précision de 24,3% en utilisant 100 échantillons de référence. Dans la détection d'objets peu supervisées avec deux exemples sur COCO, il dépasse le rendement de référence de 21,9 points, et sur LVIS de 15,4 points. Visual-RFT marque un changement du paradigme de fine-tuning des LVLMs, offrant une approche plus efficace en données et avec des récompenses, améliorant ainsi la raisonnabilité et l'applicabilité dans des tâches spécifiques.",
      "upvotes": 31,
      "discussionId": "67c6816c14a1bf9855188d8c",
      "projectPage": "https://github.com/Liuziyu77/Visual-RFT",
      "githubRepo": "https://github.com/Liuziyu77/Visual-RFT"
    },
    "publishedAt": "2025-03-03T23:29:27.952Z",
    "title": "Visual-RFT: Visual Reinforcement Fine-Tuning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01785.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63fda3fced9eead590ff6918",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1677566802735-noauth.jpeg",
      "fullname": "Zeyi Sun",
      "name": "Zery",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 16
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.01774",
      "authors": [
        {
          "_id": "67c694febdab31ec59fea175",
          "user": {
            "_id": "633aaf695df91da9cea92960",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/633aaf695df91da9cea92960/9T4y1ru5wt5iKUUqf9_Tt.png",
            "isPro": false,
            "fullname": "Jay Wu",
            "user": "jayw",
            "type": "user"
          },
          "name": "Jay Zhangjie Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-04T08:34:53.874Z",
          "hidden": false
        },
        {
          "_id": "67c694febdab31ec59fea176",
          "name": "Yuxuan Zhang",
          "hidden": false
        },
        {
          "_id": "67c694febdab31ec59fea177",
          "user": {
            "_id": "656e000253703dd78fd072a9",
            "avatarUrl": "/avatars/6702ba8fabe3d08884aa757f90cea333.svg",
            "isPro": false,
            "fullname": "Haithem Turki",
            "user": "hturki",
            "type": "user"
          },
          "name": "Haithem Turki",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:13:26.878Z",
          "hidden": false
        },
        {
          "_id": "67c694febdab31ec59fea178",
          "user": {
            "_id": "658529d61c461dfe88afe8e8",
            "avatarUrl": "/avatars/a22c1b07d28c2662833c462c6537d835.svg",
            "isPro": false,
            "fullname": "Xuanchi Ren",
            "user": "xrenaa",
            "type": "user"
          },
          "name": "Xuanchi Ren",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:13:33.467Z",
          "hidden": false
        },
        {
          "_id": "67c694febdab31ec59fea179",
          "name": "Jun Gao",
          "hidden": false
        },
        {
          "_id": "67c694febdab31ec59fea17a",
          "user": {
            "_id": "661ab3da2b14565c7acccf5c",
            "avatarUrl": "/avatars/fa4fc03664803e02aede4d4c3d50b393.svg",
            "isPro": false,
            "fullname": "Mike Zheng Shou",
            "user": "AnalMom",
            "type": "user"
          },
          "name": "Mike Zheng Shou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:27:21.825Z",
          "hidden": false
        },
        {
          "_id": "67c694febdab31ec59fea17b",
          "name": "Sanja Fidler",
          "hidden": false
        },
        {
          "_id": "67c694febdab31ec59fea17c",
          "user": {
            "_id": "6366cda3361a96184dc22139",
            "avatarUrl": "/avatars/d8a88c84cb5f69e69dd038674a29be89.svg",
            "isPro": false,
            "fullname": "Zan Gojcic",
            "user": "zgojcic",
            "type": "user"
          },
          "name": "Zan Gojcic",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:27:34.034Z",
          "hidden": false
        },
        {
          "_id": "67c694febdab31ec59fea17d",
          "name": "Huan Ling",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-03T17:58:33.000Z",
      "title": "Difix3D+: Amélioration de la reconstruction 3D en utilisant un modèle de diffusion de premier niveau",
      "summary": "Le nouveau pipeline Difix3D+ propose un modèle de diffusion unique pour améliorer la reconstruction 3D et la synthèse visuelle. Notre approche centrale est d'éliminer et renforcer les artefacts de rendu de vision nouveaux dans les zones incertaines de la représentation 3D, en utilisant un modèle de diffusion en un seul pas appelé Difix. Difix joue deux fonctions clés dans le pipeline : d'abord, lors de la reconstruction, il nettoie les images de vision fausses rendues et améliore significativement les zones incertaines grâce à un rédesign 3D, augmentant la qualité de la représentation 3D complète. Plus important encore, Difix fonctionne comme un enhanceur neural lors de l'inférence, éliminant efficacement les artefacts restants en raison des limitations et déficiences des modèles de reconstruction actuels et des sous-positions 3D insuffisantes. Difix3D+ est une solution générale, s'adapte aux représentations de NeRF et 3DGS, et améliore, par rapport aux normes, en moyenne deux fois le score de FID tout en maintenant la cohérence 3D.",
      "upvotes": 26,
      "discussionId": "67c69500bdab31ec59fea24d",
      "projectPage": "https://research.nvidia.com/labs/toronto-ai/difix3d"
    },
    "publishedAt": "2025-03-04T00:52:22.204Z",
    "title": "Difix3D+: Improving 3D Reconstructions with Single-Step Diffusion Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01774.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "633aaf695df91da9cea92960",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/633aaf695df91da9cea92960/9T4y1ru5wt5iKUUqf9_Tt.png",
      "fullname": "Jay Wu",
      "name": "jayw",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.01743",
      "authors": [
        {
          "_id": "67c67d0dfe135a5f482599bb",
          "name": "Abdelrahman Abouelenin",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599bc",
          "user": {
            "_id": "669ed17498ba26df962584f5",
            "avatarUrl": "/avatars/996c9cf05a4f8e5447552220085157c7.svg",
            "isPro": false,
            "fullname": "Atabak Ashfaq",
            "user": "atabakashfaqMSFT",
            "type": "user"
          },
          "name": "Atabak Ashfaq",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:45:15.511Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599bd",
          "name": "Adam Atkinson",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599be",
          "name": "Hany Awadalla",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599bf",
          "name": "Nguyen Bach",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599c0",
          "user": {
            "_id": "6481e690f9ed842838a2b106",
            "avatarUrl": "/avatars/e89a3c8366df504a95dc08a1a412bf3d.svg",
            "isPro": false,
            "fullname": "Jianmin Bao",
            "user": "jianmin-ustc",
            "type": "user"
          },
          "name": "Jianmin Bao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:46:34.578Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599c1",
          "user": {
            "_id": "65b9b627e7c838136275a681",
            "avatarUrl": "/avatars/22423f3d9a6c4ee34cad3b0894d27d23.svg",
            "isPro": false,
            "fullname": "Alon Benhaim",
            "user": "alonbenhaim",
            "type": "user"
          },
          "name": "Alon Benhaim",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:46:41.117Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599c2",
          "user": {
            "_id": "66f81b5b3c7ffa7931b4829a",
            "avatarUrl": "/avatars/a7f34e8e3fd92fdb96affc367b522fbe.svg",
            "isPro": false,
            "fullname": "cai",
            "user": "martincai",
            "type": "user"
          },
          "name": "Martin Cai",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:46:47.556Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599c3",
          "user": {
            "_id": "659c7ac977ac6f1bf5e63d7e",
            "avatarUrl": "/avatars/86a6efde0d483564a67ed5f344d479a0.svg",
            "isPro": false,
            "fullname": "Vishrav Chaudhary",
            "user": "vishravmsft",
            "type": "user"
          },
          "name": "Vishrav Chaudhary",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:46:56.428Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599c4",
          "user": {
            "_id": "66c7a93b92e9f5b19f7533ab",
            "avatarUrl": "/avatars/e26ebf5cf083a3ec09fce24026ecc76e.svg",
            "isPro": false,
            "fullname": "Chen",
            "user": "congcongchen",
            "type": "user"
          },
          "name": "Congcong Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:47:04.205Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599c5",
          "user": {
            "_id": "666470a28f5513b0cf11e850",
            "avatarUrl": "/avatars/7beea758882677ad32a12ce56d4d084a.svg",
            "isPro": false,
            "fullname": "Dong Chen",
            "user": "DongChen06",
            "type": "user"
          },
          "name": "Dong Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:47:11.865Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599c6",
          "user": {
            "_id": "6567651c6fcc82e5e8c36d4d",
            "avatarUrl": "/avatars/ba3cc037a7688c4f8d967fc6043e540d.svg",
            "isPro": false,
            "fullname": "Dongdong Chen",
            "user": "dongdongchen",
            "type": "user"
          },
          "name": "Dongdong Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:47:18.197Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599c7",
          "user": {
            "_id": "669db44d61278f96d8c608a4",
            "avatarUrl": "/avatars/92a493da10c086af5f2af680f4e2c6c6.svg",
            "isPro": false,
            "fullname": "Junkun Chen",
            "user": "shtpgshus",
            "type": "user"
          },
          "name": "Junkun Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:47:43.236Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599c8",
          "user": {
            "_id": "64da876370446182be5b608d",
            "avatarUrl": "/avatars/e412fdc71404ecdf638e416846e3ebfb.svg",
            "isPro": false,
            "fullname": "Weizhu Chen",
            "user": "chenweizhu",
            "type": "user"
          },
          "name": "Weizhu Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:47:51.832Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599c9",
          "user": {
            "_id": "662d6b09a47b4da4b23c8b2a",
            "avatarUrl": "/avatars/6770b1d7e25b2cdce04f9904b543d122.svg",
            "isPro": false,
            "fullname": "Yen-Chun Chen",
            "user": "Yen-ChunChen",
            "type": "user"
          },
          "name": "Yen-Chun Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:47:58.051Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599ca",
          "name": "Yi-ling Chen",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599cb",
          "name": "Qi Dai",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599cc",
          "name": "Xiyang Dai",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599cd",
          "user": {
            "_id": "64a8b800b35f48e37dfd20fe",
            "avatarUrl": "/avatars/1e66be9a5238ce86df8b54150520bcc8.svg",
            "isPro": false,
            "fullname": "Ruchao Fan",
            "user": "fanruchao",
            "type": "user"
          },
          "name": "Ruchao Fan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-04T08:40:17.936Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599ce",
          "name": "Mei Gao",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599cf",
          "name": "Min Gao",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599d0",
          "name": "Amit Garg",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599d1",
          "user": {
            "_id": "62cdae333529c21a2283a0a1",
            "avatarUrl": "/avatars/cafc2821e522bbd06d49830e36a073e3.svg",
            "isPro": false,
            "fullname": "Abhishek GOSWAMI",
            "user": "abgoswam",
            "type": "user"
          },
          "name": "Abhishek Goswami",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:49:02.466Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599d2",
          "user": {
            "_id": "5f04c4394ec31d33a72116d6",
            "avatarUrl": "/avatars/75d4b9020070e73604b12e5adc1c8201.svg",
            "isPro": false,
            "fullname": "Junheng Hao",
            "user": "jeffhao",
            "type": "user"
          },
          "name": "Junheng Hao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:53:16.356Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599d3",
          "user": {
            "_id": "660480db07619487a3718a16",
            "avatarUrl": "/avatars/9c08d541913e57fd79988ef93d5095d4.svg",
            "isPro": false,
            "fullname": "Amr Hendy",
            "user": "amrhendy",
            "type": "user"
          },
          "name": "Amr Hendy",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:53:24.716Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599d4",
          "name": "Yuxuan Hu",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599d5",
          "name": "Xin Jin",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599d6",
          "user": {
            "_id": "6440905e27dc46cca590994c",
            "avatarUrl": "/avatars/0346f8ad17038fba87649a0fc59d64ab.svg",
            "isPro": false,
            "fullname": "Mahmoud Khademi",
            "user": "mkhademi",
            "type": "user"
          },
          "name": "Mahmoud Khademi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:53:53.225Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599d7",
          "user": {
            "_id": "662476aec8920ec351b8d3d8",
            "avatarUrl": "/avatars/791e40f53073563680ef18f75b3ea95e.svg",
            "isPro": false,
            "fullname": "Dongwoo Kim",
            "user": "dongwookim-ms",
            "type": "user"
          },
          "name": "Dongwoo Kim",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:54:04.257Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599d8",
          "user": {
            "_id": "63f5173bb51da4d61da6c038",
            "avatarUrl": "/avatars/0ee530cf80476aa3985c4d591cd384a1.svg",
            "isPro": false,
            "fullname": "Young Jin Kim",
            "user": "ykim362",
            "type": "user"
          },
          "name": "Young Jin Kim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-04T08:40:19.902Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599d9",
          "name": "Gina Lee",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599da",
          "user": {
            "_id": "64004b72330a45b03604303b",
            "avatarUrl": "/avatars/a1fa3fc700173238d0336258b000d934.svg",
            "isPro": false,
            "fullname": "Jinyu Li",
            "user": "FallTraveler",
            "type": "user"
          },
          "name": "Jinyu Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:54:17.115Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599db",
          "name": "Yunsheng Li",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599dc",
          "name": "Chen Liang",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599dd",
          "user": {
            "_id": "6464f05e5cdb9ab50f846c98",
            "avatarUrl": "/avatars/3cb2f60a909b59289209ecc7ba75a338.svg",
            "isPro": false,
            "fullname": "Xihui Lin",
            "user": "linxihui",
            "type": "user"
          },
          "name": "Xihui Lin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:56:29.024Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599de",
          "user": {
            "_id": "62c3a0caf5e2eb44f51de87d",
            "avatarUrl": "/avatars/3c535c5488476b75443666176fcb4c9b.svg",
            "isPro": false,
            "fullname": "Zeqi Lin",
            "user": "linzeqi",
            "type": "user"
          },
          "name": "Zeqi Lin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:56:38.534Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599df",
          "name": "Mengchen Liu",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599e0",
          "name": "Yang Liu",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599e1",
          "user": {
            "_id": "60c790f1accf7da31ed8240d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60c790f1accf7da31ed8240d/YDohCmgf9OUeWqZIs3Thh.jpeg",
            "isPro": false,
            "fullname": "Gilsinia Lopez",
            "user": "lgg",
            "type": "user"
          },
          "name": "Gilsinia Lopez",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:59:55.169Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599e2",
          "name": "Chong Luo",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599e3",
          "user": {
            "_id": "66269a329014ef4d10f55d9d",
            "avatarUrl": "/avatars/d4866c32419a7dd07e9aa0660f4bafa9.svg",
            "isPro": false,
            "fullname": "Piyush Madan",
            "user": "PiyushMadan",
            "type": "user"
          },
          "name": "Piyush Madan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T10:02:38.019Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599e4",
          "user": {
            "_id": "65301591944086d1d5fcf656",
            "avatarUrl": "/avatars/250a2e898a4fcbe78feaf6e812851bd6.svg",
            "isPro": false,
            "fullname": "Vadim Mazalovskii",
            "user": "JakeRiley",
            "type": "user"
          },
          "name": "Vadim Mazalov",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T10:02:47.430Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599e5",
          "name": "Ali Mousavi",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599e6",
          "user": {
            "_id": "649bc84833486cdd77c01c66",
            "avatarUrl": "/avatars/36f4e4bb15c337c4391bfbd234051f4c.svg",
            "isPro": false,
            "fullname": "Nguyen Anh",
            "user": "Anhnguyen",
            "type": "user"
          },
          "name": "Anh Nguyen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:57:52.311Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599e7",
          "name": "Jing Pan",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599e8",
          "user": {
            "_id": "673b7f70cdc852f69bebfed1",
            "avatarUrl": "/avatars/1efad61a42b948c750c96472a6192de5.svg",
            "isPro": false,
            "fullname": "Daniel Perez-Becker",
            "user": "perezbecker",
            "type": "user"
          },
          "name": "Daniel Perez-Becker",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:59:09.929Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599e9",
          "name": "Jacob Platin",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599ea",
          "user": {
            "_id": "65c52dad286bf45e79491697",
            "avatarUrl": "/avatars/01ebc7979273df6e53971ae9835b503f.svg",
            "isPro": false,
            "fullname": "Thomas Portet",
            "user": "thopo",
            "type": "user"
          },
          "name": "Thomas Portet",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:59:39.865Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599eb",
          "name": "Kai Qiu",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599ec",
          "user": {
            "_id": "668dcf92835bf7e64bbca904",
            "avatarUrl": "/avatars/416eb3a3c5318a6a45aad87012296470.svg",
            "isPro": false,
            "fullname": "Bo Ren",
            "user": "rosrad",
            "type": "user"
          },
          "name": "Bo Ren",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-04T08:40:15.919Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599ed",
          "user": {
            "_id": "63815eff4761ddfa00903762",
            "avatarUrl": "/avatars/3419b239d42e091586f1c51b526d88e5.svg",
            "isPro": false,
            "fullname": "Liliang Ren",
            "user": "renll",
            "type": "user"
          },
          "name": "Liliang Ren",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:57:37.996Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599ee",
          "name": "Sambuddha Roy",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599ef",
          "name": "Ning Shang",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599f0",
          "user": {
            "_id": "6454c337a13edf669cd5d8ea",
            "avatarUrl": "/avatars/a383a0dda7c2ef6a0d6c3c64651f42ff.svg",
            "isPro": false,
            "fullname": "Yelong Shen",
            "user": "uuu6",
            "type": "user"
          },
          "name": "Yelong Shen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T10:00:05.457Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599f1",
          "user": {
            "_id": "62743aec8cb70eed79073bc0",
            "avatarUrl": "/avatars/3c8b9a91d898f616265f823ab7d432df.svg",
            "isPro": false,
            "fullname": "Saksham Singhal",
            "user": "sakshamsinghal",
            "type": "user"
          },
          "name": "Saksham Singhal",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:59:03.188Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599f2",
          "user": {
            "_id": "678bc6b432ee4968eca9bb6a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/wT-Xa3TYem_EzkZZMyDG0.png",
            "isPro": false,
            "fullname": "Subhojit Som",
            "user": "susom",
            "type": "user"
          },
          "name": "Subhojit Som",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:59:47.241Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599f3",
          "name": "Xia Song",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599f4",
          "user": {
            "_id": "64692ad25d701566394fd8da",
            "avatarUrl": "/avatars/d6811ccceb14788bfa0aa10fe4ee1054.svg",
            "isPro": false,
            "fullname": "Tetyana Sych",
            "user": "tesych",
            "type": "user"
          },
          "name": "Tetyana Sych",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:58:27.814Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599f5",
          "name": "Praneetha Vaddamanu",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599f6",
          "name": "Shuohang Wang",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599f7",
          "name": "Yiming Wang",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599f8",
          "name": "Zhenghao Wang",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599f9",
          "name": "Haibin Wu",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599fa",
          "user": {
            "_id": "61384b860317b0a5c10877d3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1631080954171-61384b860317b0a5c10877d3.jpeg",
            "isPro": false,
            "fullname": "Haoran Xu",
            "user": "haoranxu",
            "type": "user"
          },
          "name": "Haoran Xu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:56:04.939Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599fb",
          "user": {
            "_id": "6398f4b32c20654083f36cde",
            "avatarUrl": "/avatars/4591f514483890997c55e9e6d60bbb0f.svg",
            "isPro": false,
            "fullname": "Weijian Xu",
            "user": "xwjabc",
            "type": "user"
          },
          "name": "Weijian Xu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:58:36.082Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599fc",
          "name": "Yifan Yang",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599fd",
          "name": "Ziyi Yang",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599fe",
          "user": {
            "_id": "65b01b8a29ae836e9ed5af24",
            "avatarUrl": "/avatars/a8b78a4b54d3f10858c5925521357001.svg",
            "isPro": false,
            "fullname": "Donghan Yu",
            "user": "donghanyu",
            "type": "user"
          },
          "name": "Donghan Yu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:55:41.798Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599ff",
          "name": "Ishmam Zabir",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f48259a00",
          "user": {
            "_id": "63601ee38fb9c2420ffbe45d",
            "avatarUrl": "/avatars/56af091aaff1b42dcfbae84a6ee1e7f7.svg",
            "isPro": false,
            "fullname": "Zhang",
            "user": "Jianwen",
            "type": "user"
          },
          "name": "Jianwen Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:55:12.465Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f48259a01",
          "user": {
            "_id": "62b0009c72043b05d29492b2",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b0009c72043b05d29492b2/NqRkX2YLhlfOLvYysa7dD.png",
            "isPro": false,
            "fullname": "Li Lyna Zhang",
            "user": "lynazhang",
            "type": "user"
          },
          "name": "Li Lyna Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:55:01.540Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f48259a02",
          "name": "Yunan Zhang",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f48259a03",
          "user": {
            "_id": "66ce4c9f864befb39cfc74e9",
            "avatarUrl": "/avatars/ef66398466c470fc1d384c6817d9e461.svg",
            "isPro": false,
            "fullname": "Xiren Zhou",
            "user": "XirenZhou",
            "type": "user"
          },
          "name": "Xiren Zhou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:54:26.629Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-03T17:05:52.000Z",
      "title": "Phi-4-Mini Rapport Technique : Modèle de Langage Multimodal Léger et Fort Basé sur des Processus Hybrides",
      "summary": "Introduis Fiesta 4 Mini et la bibliothèque de modèles Fiesta 4. Ils sont des modèles de langage et multimodal avec des performances élevées à petite échelle. Fiesta 4 Mini a été entraîné avec 38 milliards de paramètres en utilisant des données de web de haute qualité et des données synthétiques, surpassant significativement les modèles les plus récents de code ouvert de la même dimension et montrant un rendement similaire aux tâches de mathématiques ou de codage nécessitant des inférences complexes. Cet succès est attribué à l'ajustement des données synthétiques qui favorisent les données de mathématiques et de codage. Comparé aux modèles précédents, Fiesta 4 Mini a élargi son vocabulaire à 200K tokens, amélioré le support pour les applications multilingues et implémenté la technique de groupe de requêtes pour la génération efficace de séquences longues. La bibliothèque de modèles Fiesta 4 est un modèle multimodal qui intègre des modèles de texte, vision et voix/audio. Il utilise un nouvel approche d'expansion de modèles, avec des adaptateurs LoRA et des rotors par modèle, permettant la combinaison de différents modèles pour mettre en œuvre divers modes d'inférence et éviter les interférences entre modèles. Par exemple, actuellement il occupe le premier rang dans le classement OpenASR, avec un composant LoRA de 460 milliards de paramètres. La bibliothèque de modèles Fiesta 4 montre un rendement supérieur aux modèles de langage visuel ou de langage vocal dans des tâches complexes, y compris des scénarios qui traitent des entrées de (vision + langage), (vision + voix) et (voix/audio). De plus, elle a été entraînée supplémentairement pour améliorer la capacité logique de Fiesta 4 Mini, démontrant un rendement logique supérieur aux modèles plus grands comme DeepSeek-R1-Distill-Qwen-7B et DeepSeek-R1-Distill-Llama-8B.",
      "upvotes": 22,
      "discussionId": "67c67d0efe135a5f48259a38",
      "projectPage": "https://huggingface.co/microsoft/Phi-4-multimodal-instruct"
    },
    "publishedAt": "2025-03-03T23:15:05.187Z",
    "title": "Phi-4-Mini Technical Report: Compact yet Powerful Multimodal Language Models via Mixture-of-LoRAs",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01743.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63f5173bb51da4d61da6c038",
      "avatarUrl": "/avatars/0ee530cf80476aa3985c4d591cd384a1.svg",
      "fullname": "Young Jin Kim",
      "name": "ykim362",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.01496",
      "authors": [
        {
          "_id": "67c6b05f35198d0f397adc98",
          "user": {
            "_id": "66ea643899af9ac3463639b1",
            "avatarUrl": "/avatars/252d470e761a57834dee3dbc60dfefed.svg",
            "isPro": false,
            "fullname": "Disen Lan",
            "user": "landisen",
            "type": "user"
          },
          "name": "Disen Lan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-04T08:34:46.117Z",
          "hidden": false
        },
        {
          "_id": "67c6b05f35198d0f397adc99",
          "user": {
            "_id": "6246bb33da617c00b48e4d92",
            "avatarUrl": "/avatars/0304a9f6eb7f5dee4d933d03222f94e9.svg",
            "isPro": false,
            "fullname": "Weigao Sun",
            "user": "weigao266",
            "type": "user"
          },
          "name": "Weigao Sun",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-03-04T08:10:52.130Z",
          "hidden": false
        },
        {
          "_id": "67c6b05f35198d0f397adc9a",
          "user": {
            "_id": "665dc35752ff9daa9ba5a4ed",
            "avatarUrl": "/avatars/df8b01879d97e599b610fa51414d3a18.svg",
            "isPro": false,
            "fullname": "Hu Jiaxi",
            "user": "Jiaxihu2",
            "type": "user"
          },
          "name": "Jiaxi Hu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T10:04:18.982Z",
          "hidden": false
        },
        {
          "_id": "67c6b05f35198d0f397adc9b",
          "user": {
            "_id": "65003e857804f04a163328d9",
            "avatarUrl": "/avatars/fe32150aabfde8d283b38ccebcf6982e.svg",
            "isPro": false,
            "fullname": "Jusen Du",
            "user": "JusenK",
            "type": "user"
          },
          "name": "Jusen Du",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T10:04:26.432Z",
          "hidden": false
        },
        {
          "_id": "67c6b05f35198d0f397adc9c",
          "name": "Yu Cheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-03T13:08:00.000Z",
      "title": "Laser : Linéarise les grands modèles de langue dans la structure des unités linéaires à portes.",
      "summary": "Transformateurs avec un modèle récurrent linéaire offrent un entraînement en temps linéaire et une inférence en mémoire constante. Malgré leur efficacité et leur performance démontrées, l'entraînement à zéro d'architectures non standard est coûteux et dangereux. La linéarisation de grands modèles de langage (LLMs) transforme des modèles prétrainés standards en structures récurrentes linéaires, permettant une déploiement plus efficace. Cependant, les méthodes actuelles de linéarisation introduisent souvent des modules supplémentaires de carte de caractéristiques nécessitant un entraînement étendu et ignorent les mécanismes de gating utilisés dans les modèles de référence de modèles récurrents linéaires. Pour aborder ces problèmes, cet article présente Liger, court pour \"Linearizing LLMs to gated recurrent structures\". Liger est une approche novatrice pour convertir les LLMs prétrainés en modèles récurrents linéaires avec un gating, sans ajouter de paramètres supplémentaires. Il réutilise les poids de la matrice clé prétrainée pour construire divers mécanismes de gating, facilitant la formation de différentes structures récurrentes tout en évitant le besoin d'entraîner des composants supplémentaires à zéro. En utilisant un entraînement léger avec Adaptation de Rang Bas (LoRA), Liger restaure le rendement des modèles récurrents linéaires avec un gating tout comme celui des LLMs originaux. De plus, nous présentons Liger Attention, un mécanisme d'attention hybride intra-couche, qui récupère significativement 93% du LLM basé sur le Transformer en 0,02% de tokens d'entraînement lors du processus de linéarisation, atteignant des résultats compétitifs dans de multiples évaluations, comme validé dans des modèles allant de 1B à 8B paramètres. Le code est disponible sur https://github.com/OpenSparseLLMs/Linearization.",
      "upvotes": 12,
      "discussionId": "67c6b06035198d0f397adcc4"
    },
    "publishedAt": "2025-03-04T02:48:58.261Z",
    "title": "Liger: Linearizing Large Language Models to Gated Recurrent Structures",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01496.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6246bb33da617c00b48e4d92",
      "avatarUrl": "/avatars/0304a9f6eb7f5dee4d933d03222f94e9.svg",
      "fullname": "Weigao Sun",
      "name": "weigao266",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.01307",
      "authors": [
        {
          "_id": "67c68adc0457c9f809c22df8",
          "user": {
            "_id": "63e6a880f2e9a8f22c5a1630",
            "avatarUrl": "/avatars/53b57690fe052ce6882bbfc87b11567c.svg",
            "isPro": false,
            "fullname": "Kanishk Gandhi",
            "user": "obiwan96",
            "type": "user"
          },
          "name": "Kanishk Gandhi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-04T08:35:01.161Z",
          "hidden": false
        },
        {
          "_id": "67c68adc0457c9f809c22df9",
          "user": {
            "_id": "624f9e3d07bd004fb855f5e9",
            "avatarUrl": "/avatars/86a349cd4053bc0317e27e75a51c69fa.svg",
            "isPro": false,
            "fullname": "Ayush Chakravarthy",
            "user": "ayushchakravarthy",
            "type": "user"
          },
          "name": "Ayush Chakravarthy",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T10:04:44.344Z",
          "hidden": false
        },
        {
          "_id": "67c68adc0457c9f809c22dfa",
          "user": {
            "_id": "6511ee845b7e52b0251fdee9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6511ee845b7e52b0251fdee9/hTIwiIYBGOVnIrxtpri83.png",
            "isPro": false,
            "fullname": "Anikait Singh",
            "user": "Asap7772",
            "type": "user"
          },
          "name": "Anikait Singh",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T10:05:05.759Z",
          "hidden": false
        },
        {
          "_id": "67c68adc0457c9f809c22dfb",
          "user": {
            "_id": "61aa15fd8a9625ebfe284286",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61aa15fd8a9625ebfe284286/KaGzIeijcgcN15JErCqft.jpeg",
            "isPro": false,
            "fullname": "nathan lile",
            "user": "nlile",
            "type": "user"
          },
          "name": "Nathan Lile",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-04T08:34:58.582Z",
          "hidden": false
        },
        {
          "_id": "67c68adc0457c9f809c22dfc",
          "user": {
            "_id": "67321274c1f20c742bcf7a8d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/ltcQhre6eDRVzn6Vbbyhu.png",
            "isPro": false,
            "fullname": "Noah D. Goodman",
            "user": "ngoodman",
            "type": "user"
          },
          "name": "Noah D. Goodman",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T10:05:12.186Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-03T08:46:22.000Z",
      "title": "Comment Kogaku Mite et Bahabi Alze présentent des théories sur le développement personnel, et les 4 habitudes efficaces de STaR à haut niveau.",
      "summary": "Durant l'entraînement, l'inférence devient un phénomène puissant qui montre comment un modèle de langue peut penser de manières plus efficaces dans des problèmes complexes. L'apprentissage par renforcement (RL) peut pousser l'amélioration automatique des modèles de langue dans des tâches probables ; cependant, certains modèles obtiennent des améliorations considérables tandis que d'autres s'arrêtent rapidement. Par exemple, lorsque l'on effectue le même entraînement par renforcement dans un jeu de compteur, Qwen-2.5-3B peut surpasser considérablement Llama-3.2-3B. Cette différence soulève une question importante : quelles caractéristiques internes sont utiles pour l'amélioration automatique ? Nous proposons un cadre qui analyse 4 actions cognitivas (vérification, rétrograde, établissement de sous-objectifs, rétrograde en chaîne) pour étudier ces questions. Ces actions sont utilisées par résolveurs de problèmes experts ou modèles de langue réussis. Notre étude montre que Qwen exprime naturellement ces actions cognitivas, tandis que Llama l'a commencé à faire. À travers des expériences systématiques, lorsque ces actions cognitivas sont incluses dans Llama, on observe un grand amélioration de l'entraînement par renforcement et on peut surpasser Qwen. Un point clé est que la précision de la réponse est un élément crucial pour la capacité d'amélioration. Par conséquent, le rendement des modèles entraînés sur des solutions correctes est relativement similaire à ceux entraînés sur des solutions non normales qui incluent des solutions correctes. Enfin, en utilisant des données OpenWebMath pour l'entraînement continu, Llama peut surpasser le patron d'amélioration automatique de Qwen en renforçant des patrons de raisonnement. Ces résultats fondamentaux révèlent la relation entre les actions cognitivas initiales et la capacité d'amélioration, expliquant pourquoi certains modèles de langue peuvent utiliser des calculs supplémentaires efficacement tandis que d'autres s'arrêtent.",
      "upvotes": 9,
      "discussionId": "67c68add0457c9f809c22e31"
    },
    "publishedAt": "2025-03-04T00:09:04.418Z",
    "title": "Cognitive Behaviors that Enable Self-Improving Reasoners, or, Four Habits of Highly Effective STaRs",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01307.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63e6a880f2e9a8f22c5a1630",
      "avatarUrl": "/avatars/53b57690fe052ce6882bbfc87b11567c.svg",
      "fullname": "Kanishk Gandhi",
      "name": "obiwan96",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.00714",
      "authors": [
        {
          "_id": "67c6a803025b72f14ccb0939",
          "user": {
            "_id": "6577437552f02732a463d97d",
            "avatarUrl": "/avatars/8eb271ec249fa9b0d97dfe0eace6da88.svg",
            "isPro": false,
            "fullname": "Haoyu Li",
            "user": "Haoyu0529",
            "type": "user"
          },
          "name": "Haoyu Li",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-04T07:13:08.306Z",
          "hidden": false
        },
        {
          "_id": "67c6a803025b72f14ccb093a",
          "name": "Srikanth Kandula",
          "hidden": false
        },
        {
          "_id": "67c6a803025b72f14ccb093b",
          "name": "Maria Angels de Luis Balaguer",
          "hidden": false
        },
        {
          "_id": "67c6a803025b72f14ccb093c",
          "name": "Aditya Akella",
          "hidden": false
        },
        {
          "_id": "67c6a803025b72f14ccb093d",
          "name": "Venkat Arun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-02T03:44:31.000Z",
      "title": "Spectral Ead Hacking Queries",
      "summary": "Dans l'analyse de grands ensembles de données, il est nécessaire une exécution efficace des requêtes, mais l'exécution de requêtes SQL sur grands ensembles de données peut être lente. Dans cet article, nous explorons si les requêtes sont exécutées avant que l'utilisateur complete l'entrée des clés et nous essayons des méthodes pour approcher les résultats. Nous proposons le système SpeQL. Ce système utilise de grands modèles de langage (LLMs) pour prédire des requêtes prévisibles en fonction du schéma de la base de données, des requêtes passées par l'utilisateur et des requêtes incomplètes. La prédiction de requêtes précises est difficile, par conséquent SpeQL prédit les requêtes de deux manières : 1) prédire la structure de la requête et la modifier et planifier avant. 2) calculer des tables temporaires plus petites que la base de données originale pour prédire l'information nécessaire pour la requête finale de l'utilisateur. De plus, SpeQL affiche de manière continue les résultats des requêtes prédites et partielles par temps, ce qui peut aider dans l'analyse exploratoire. Selon l'évaluation basée sur l'utilité/étude de l'utilisateur, SpeQL améliore le temps de terminaison des tâches et aide les participants à découvrir des patrons rapidement grâce aux résultats prédits. Dans cette évaluation, SpeQL améliore la latence des requêtes d'un maximum de 289 fois, contrôle les coûts raisonnables et peut être exécuté à un coût de 4$ par heure.",
      "upvotes": 8,
      "discussionId": "67c6a804025b72f14ccb0994",
      "projectPage": "https://github.com/lihy0529/SpeQL",
      "githubRepo": "https://github.com/lihy0529/SpeQL"
    },
    "publishedAt": "2025-03-04T02:21:00.460Z",
    "title": "Speculative Ad-hoc Querying",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6577437552f02732a463d97d/fEkQ4BZ8Yx_CzsjvHBWFq.qt"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.00714.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6577437552f02732a463d97d",
      "avatarUrl": "/avatars/8eb271ec249fa9b0d97dfe0eace6da88.svg",
      "fullname": "Haoyu Li",
      "name": "Haoyu0529",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.00784",
      "authors": [
        {
          "_id": "67c673bcf47209364f0cec96",
          "name": "Kai Lv",
          "hidden": false
        },
        {
          "_id": "67c673bcf47209364f0cec97",
          "name": "Honglin Guo",
          "hidden": false
        },
        {
          "_id": "67c673bcf47209364f0cec98",
          "name": "Qipeng Guo",
          "hidden": false
        },
        {
          "_id": "67c673bcf47209364f0cec99",
          "name": "Xipeng Qiu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-02T08:27:48.000Z",
      "title": "Dow Decoding : Début de ré-séquençage par décodage dynamique de spectre pour divers systèmes d'intérêt en matière de logiciel",
      "summary": "Les modèles de langage grands (LLMs) montrent un excellent rendement sur une large gamme de tâches, mais le processus de génération automatique de tokens en temps réel affecte significativement la vitesse d'inférence. La décodification spéculative (Speculative decoding) propose un cadre qui permet de générer et de confirmer des prédictions de la distribution de sortie, avec l'objectif de réduire la perte de génération. Cependant, les modèles proposés ajoutent une surcharge de calcul supplémentaire, ce qui peut conduire à un désaccord dans le rendement et à un temps de premier token (TTFT) plus long. Les solutions antérieures pour réduire cette surcharge ont été principalement basées sur des heuristiques, ce qui a échoué à maintenir la qualité du modèle. Pour faire face à ces défis, nous proposons le DuoDecoding, qui distribue le modèle proposé et l'objectif de manière stratégique sur le CPU et le GPU, permettant une interprétation parallèle et maintenant la qualité du modèle. Notre méthode utilise un bucket optimal pour l'architecture de la machine, minimise le temps de conversation et améliore la qualité du modèle proposé par la génération dynamique de multiples séquences. Des expériences larges qui comprennent 7 tâches montrent que le DuoDecoding atteint un accroissement de vitesse de la génération de texte latin de 2,61 fois et réduit le TTFT de la décodification spéculative traditionnelle de 83%. Le code est disponible sur https://github.com/KaiLv69/DuoDecoding.",
      "upvotes": 7,
      "discussionId": "67c673bdf47209364f0cecb7",
      "githubRepo": "https://github.com/KaiLv69/DuoDecoding"
    },
    "publishedAt": "2025-03-03T22:35:45.299Z",
    "title": "DuoDecoding: Hardware-aware Heterogeneous Speculative Decoding with Dynamic Multi-Sequence Drafting",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.00784.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6485d5b300c9cfe5c2470c81",
      "avatarUrl": "/avatars/c29aa81d2add795e8448b99274a04b83.svg",
      "fullname": "Kai",
      "name": "KaiLv",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.18965",
      "authors": [
        {
          "_id": "67c6bfdf96b9f5fa18c517db",
          "name": "Jiaxin Deng",
          "hidden": false
        },
        {
          "_id": "67c6bfdf96b9f5fa18c517dc",
          "name": "Shiyao Wang",
          "hidden": false
        },
        {
          "_id": "67c6bfdf96b9f5fa18c517dd",
          "name": "Kuo Cai",
          "hidden": false
        },
        {
          "_id": "67c6bfdf96b9f5fa18c517de",
          "name": "Lejian Ren",
          "hidden": false
        },
        {
          "_id": "67c6bfdf96b9f5fa18c517df",
          "name": "Qigen Hu",
          "hidden": false
        },
        {
          "_id": "67c6bfdf96b9f5fa18c517e0",
          "name": "Weifeng Ding",
          "hidden": false
        },
        {
          "_id": "67c6bfdf96b9f5fa18c517e1",
          "name": "Qiang Luo",
          "hidden": false
        },
        {
          "_id": "67c6bfdf96b9f5fa18c517e2",
          "name": "Guorui Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-26T09:25:10.000Z",
      "title": "OneRec : La recherche et la classification se intègrent grâce à un recommandateur génératif et à un ajustement répétitif des préférences.",
      "summary": "Récemment, des systèmes de recommandation basés sur la recherche générative ont émergé comme un paradigme excellent, mais la plupart des systèmes modernes de recommandation se basent sur la recherche et le classement, et les modèles génératifs ne fonctionnent que comme sélectionneurs lors de la phase de recherche. Dans cet article, nous proposons OneRec, un modèle génératif qui remplace le cadre de l'apprentissage continu actuel. Malgré nos limitations, OneRec est le premier modèle génératif qui dépasse significativement les systèmes complexes et bien conçus actuels, démontrant un grand avancé jusqu'à présent. En particulier, OneRec comprend trois aspects : 1) Structure encoder-decoder, qui codifie les séquences d'actions historiques de l'utilisateur et décodifie progressivement les films qui intéressent l'utilisateur. Il utilise une Mixture-of-Experts (MoE) rare pour élargir la capacité du modèle sans accroître proportionnellement les coûts computationnels (FLOPs). 2) Méthode de génération par session, qui propose une méthode de génération plus sophistiquée et contextuelle, sans dépendre de règles de jeu, pour prédire l'item suivant dans le projet. 3) Combinaison du module Alignment de Préférences Itératives et de l'Optimisation de Préférences Directes (DPO) pour améliorer les résultats générés. A différence du DPO dans le NLP, dans les systèmes de recommandation, les résultats sont affichés une seule fois à l'utilisateur et il n'est pas possible d'obtenir des échantillons positifs et négatifs. Pour résoudre ce problème, nous avons conçu un modèle de récompense pour simuler la génération de l'utilisateur et personnalisé la stratégie d'échantillonnage. La validation extensive montre que un nombre limité d'échantillons de DPO peut ajuster les préférences de l'utilisateur et améliorer considérablement les résultats générés. OneRec a été introduit dans les principaux scénarios de Kuaishou, augmentant le temps de vision sur 1,6% et démontrant un grand avancé.",
      "upvotes": 5,
      "discussionId": "67c6bfe396b9f5fa18c518e5"
    },
    "publishedAt": "2025-03-04T03:56:04.503Z",
    "title": "OneRec: Unifying Retrieve and Rank with Generative Recommender and Iterative Preference Alignment",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.18965.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "668f5875b5b3081d776e4094",
      "avatarUrl": "/avatars/8c763393f25afbe5fb8b132f775e746a.svg",
      "fullname": "Xiaohuan Zhou",
      "name": "XiaohuanZhou",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.00501",
      "authors": [
        {
          "_id": "67c6a343ad6b7c2fa29d5e7e",
          "name": "Jia Chen",
          "hidden": false
        },
        {
          "_id": "67c6a343ad6b7c2fa29d5e7f",
          "user": {
            "_id": "60c0ed29d8bc072769d78f48",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60c0ed29d8bc072769d78f48/V6q6Tn4kzB46NIbTYw9pQ.jpeg",
            "isPro": false,
            "fullname": "Qian Dong",
            "user": "qian",
            "type": "user"
          },
          "name": "Qian Dong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-04T08:34:51.762Z",
          "hidden": false
        },
        {
          "_id": "67c6a343ad6b7c2fa29d5e80",
          "name": "Haitao Li",
          "hidden": false
        },
        {
          "_id": "67c6a343ad6b7c2fa29d5e81",
          "name": "Xiaohui He",
          "hidden": false
        },
        {
          "_id": "67c6a343ad6b7c2fa29d5e82",
          "name": "Yan Gao",
          "hidden": false
        },
        {
          "_id": "67c6a343ad6b7c2fa29d5e83",
          "name": "Shaosheng Cao",
          "hidden": false
        },
        {
          "_id": "67c6a343ad6b7c2fa29d5e84",
          "name": "Yi Wu",
          "hidden": false
        },
        {
          "_id": "67c6a343ad6b7c2fa29d5e85",
          "name": "Ping Yang",
          "hidden": false
        },
        {
          "_id": "67c6a343ad6b7c2fa29d5e86",
          "name": "Chen Xu",
          "hidden": false
        },
        {
          "_id": "67c6a343ad6b7c2fa29d5e87",
          "name": "Yao Hu",
          "hidden": false
        },
        {
          "_id": "67c6a343ad6b7c2fa29d5e88",
          "name": "Qingyao Ai",
          "hidden": false
        },
        {
          "_id": "67c6a343ad6b7c2fa29d5e89",
          "name": "Yiqun Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-01T14:15:00.000Z",
      "title": "Niveau d'interaction avec les utilisateurs incluant divers ensembles de données de recherche d'information",
      "summary": "Dans les communautés de contenu créé par les utilisateurs (CCC), et plus particulièrement dans les contenus multimodales, l'intégration d'informations visuelles et textuelles sur les résultats (ou éléments) est effectuée pour améliorer l'expérience utilisateur. Récemment, le problème d'améliorer l'expérience utilisateur dans les services de recherche et de recommandation (S\\&R) dans des systèmes complexes a attiré l'attention de l'académie et de l'industrie. Cependant, l'insuffisance de jeux de données de haute qualité limite le développement des recherches en S\\&R multimodales. Étant donné que la demande pour améliorer l'expérience utilisateur est en augmentation, cet article présente un nouveau jeu de données de recherche multimodal appelé \"Qilin\". Ce jeu de données a été collecté sur la plateforme social \"小红书\" avec plus de 300 millions d'utilisateurs actifs mensuellement et un taux de coïncidence de recherche moyen supérieur à 70%. Comparé aux jeux de données existants, Qilin offre une collection cohérente de sessions utilisateur qui incluent divers résultats, tels que des notes d'image et de texte, des notes de vidéo, des notes de commerce et des réponses directes, favorisant le développement de modèles neuronaux de recherche multimodal qui ont évolué vers différentes configurations de tâches. De plus, des signaux de contexte étendus au niveau d'application et le feedback réel de l'utilisateur sont collectés et analysés pour modéliser la satisfaction utilisateur et analyser d'autres actions de l'utilisateur. En particulier, Qilin inclut des réponses préférées par l'utilisateur et des résultats mentionnés dans la réponse du module de Réponse Profonde (DQA), ce qui permet d'investiguer comment ces réponses affectent les actions de recherche de l'utilisateur. A travers des analyses détaillées et des expériences, des découvertes intéressantes et des pistes qui peuvent aider à améliorer les systèmes de S\\&R sont fournies. Dans le futur, on espère que Qilin contribuera significativement au développement de plateformes de contenu multimodal qui entourent les services de S\\&R.",
      "upvotes": 5,
      "discussionId": "67c6a346ad6b7c2fa29d5f88",
      "projectPage": "https://huggingface.co/datasets/THUIR/Qilin",
      "githubRepo": "https://github.com/RED-Search/Qilin/"
    },
    "publishedAt": "2025-03-04T01:56:03.632Z",
    "title": "Qilin: A Multimodal Information Retrieval Dataset with APP-level User Sessions",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.00501.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60c0ed29d8bc072769d78f48",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60c0ed29d8bc072769d78f48/V6q6Tn4kzB46NIbTYw9pQ.jpeg",
      "fullname": "Qian Dong",
      "name": "qian",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.01370",
      "authors": [
        {
          "_id": "67c691673ff65c55829685a0",
          "name": "Jiantao Lin",
          "hidden": false
        },
        {
          "_id": "67c691673ff65c55829685a1",
          "name": "Xin Yang",
          "hidden": false
        },
        {
          "_id": "67c691673ff65c55829685a2",
          "name": "Meixi Chen",
          "hidden": false
        },
        {
          "_id": "67c691673ff65c55829685a3",
          "name": "Yingjie Xu",
          "hidden": false
        },
        {
          "_id": "67c691673ff65c55829685a4",
          "user": {
            "_id": "64049ae20ab5e22719f35103",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678023295407-noauth.jpeg",
            "isPro": false,
            "fullname": "Dongyu Yan",
            "user": "StarYDY",
            "type": "user"
          },
          "name": "Dongyu Yan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-04T08:34:56.252Z",
          "hidden": false
        },
        {
          "_id": "67c691673ff65c55829685a5",
          "name": "Leyi Wu",
          "hidden": false
        },
        {
          "_id": "67c691673ff65c55829685a6",
          "name": "Xinli Xu",
          "hidden": false
        },
        {
          "_id": "67c691673ff65c55829685a7",
          "name": "Lie XU",
          "hidden": false
        },
        {
          "_id": "67c691673ff65c55829685a8",
          "name": "Shunsi Zhang",
          "hidden": false
        },
        {
          "_id": "67c691673ff65c55829685a9",
          "name": "Ying-Cong Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-03T10:07:19.000Z",
      "title": "Kiss3DGen : Réproduire des modèles de diffusion en génération de scènes 3D",
      "summary": "La diffusion model a réussi à générer des images 2D de grande qualité. Cependant, la qualité de la génération et la capacité de génération de contenu 3D sont limitées. Les méthodes les plus récentes nécessitent l'apprentissage sur de grandes échelles de contenu 3D, ce qui est difficile à collecter. Dans cette étude, nous présentons un cadre efficace pour la réutilisation d'un modèle de diffusion pour la génération 3D, appelé Kiss3DGen (\"3D Génération un peu froide\"). Spécifiquement, nous fine-tuneons le modèle de diffusion pour générer des \"3D BANDER images\", qui sont des représentations en tuiles constituées d'images de plusieurs points et de leurs cartes de normals relatives. Les cartes de normals permettent de réorganiser la mécanique 3D, tandis que les images de plusieurs points fournissent la texture pour générer des modèles 3D complets. Ce méthode simple transforme le problème de la génération 3D en une tâche de génération d'images 2D, maximisant le savoir d'un modèle de diffusion pré-entraîné. De plus, le modèle Kiss3DGen permet des fonctions comme l'édition 3D, l'amélioration de la mécanique et de la texture, entre autres. Les expériences détaillées ont démontré l'efficacité de notre approche et la capacité à générer des modèles 3D de haute qualité de manière efficace.",
      "upvotes": 4,
      "discussionId": "67c6916b3ff65c5582968702",
      "projectPage": "https://ltt-o.github.io/Kiss3dgen.github.io/",
      "githubRepo": "https://github.com/EnVision-Research/Kiss3DGen"
    },
    "publishedAt": "2025-03-04T01:19:45.715Z",
    "title": "Kiss3DGen: Repurposing Image Diffusion Models for 3D Asset Generation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01370.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6332e2689bf698ce68a22e8c",
      "avatarUrl": "/avatars/c1922acfda2e6d2fe7b03194a404eb10.svg",
      "fullname": "JIANTAO LIN",
      "name": "LTT",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.01183",
      "authors": [
        {
          "_id": "67c6a15e21d722b4248bd9c2",
          "name": "Ziqian Ning",
          "hidden": false
        },
        {
          "_id": "67c6a15e21d722b4248bd9c3",
          "name": "Huakang Chen",
          "hidden": false
        },
        {
          "_id": "67c6a15e21d722b4248bd9c4",
          "name": "Yuepeng Jiang",
          "hidden": false
        },
        {
          "_id": "67c6a15e21d722b4248bd9c5",
          "name": "Chunbo Hao",
          "hidden": false
        },
        {
          "_id": "67c6a15e21d722b4248bd9c6",
          "name": "Guobin Ma",
          "hidden": false
        },
        {
          "_id": "67c6a15e21d722b4248bd9c7",
          "name": "Shuai Wang",
          "hidden": false
        },
        {
          "_id": "67c6a15e21d722b4248bd9c8",
          "name": "Jixun Yao",
          "hidden": false
        },
        {
          "_id": "67c6a15e21d722b4248bd9c9",
          "name": "Lei Xie",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-03T05:15:34.000Z",
      "title": "DiffRhythm : Très surprisantment rapide et agréable depuis le début jusqu'à la fin\nLa chanson complète est générée par la somme d'un diffuseur de puissance",
      "summary": "Le progrès récent dans la génération de musique a reçu une grande attention, bien qu'il présente des limitations importantes. Certains modèles de génération actuels peuvent seulement synthétiser le titre de l'artiste ou seulement l'accompagnement. D'autre part, d'autres modèles combinent l'artiste avec sa interprétation, mais généralement utilisent des architectures complexes à plusieurs étapes et des flux de données intriqués, ce qui limite leur scalabilité. De plus, de nombreux systèmes sont restreints à la génération de petites parties musicales, ce qui empêche la création de longues chansons. De plus, les méthodes basées sur des modèles de langage large-scale présentent des vitesses d'inférence lentes. Pour aborder ces problèmes, on propose DiffRhythm. DiffRhythm est un modèle initial basé sur les différentielles pour la génération de chansons, qui peut synthétiser 4 minutes 45 secondes d'artiste et d'accompagnement en 10 secondes, en maintenant une qualité musicale et une compréhension élevées. De plus, DiffRhythm est conçu pour être simple et attrayant, éliminant la nécessité de préparation de données complexes, en utilisant une structure de modèle simple et en demandant seulement l'artiste et un style de projet pendant l'inférence. De plus, la structure d'algorithme de rétroaction non automatique maintient la vitesse lente d'inférence. Cette simplicité garantit la scalabilité de DiffRhythm. De plus, des codes d'entraînement avec des données d'échelle et des modèles pré-entraînés sont publiés pour encourager la recherche en reproductibilité et en développement.",
      "upvotes": 3,
      "discussionId": "67c6a16021d722b4248bda37",
      "projectPage": "https://aslp-lab.github.io/DiffRhythm.github.io/",
      "githubRepo": "https://github.com/ASLP-lab/DiffRhythm"
    },
    "publishedAt": "2025-03-04T04:54:04.054Z",
    "title": "DiffRhythm: Blazingly Fast and Embarrassingly Simple End-to-End Full-Length Song Generation with Latent Diffusion",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01183.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "624bebf604abc7ebb01789af",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1649143001781-624bebf604abc7ebb01789af.jpeg",
      "fullname": "Apolinário from multimodal AI art",
      "name": "multimodalart",
      "type": "user",
      "isPro": true,
      "isHf": true,
      "isMod": false,
      "followerCount": 3862
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.18890",
      "authors": [
        {
          "_id": "67c6cbd6e52534aa6ada2e26",
          "name": "Tong Wu",
          "hidden": false
        },
        {
          "_id": "67c6cbd6e52534aa6ada2e27",
          "name": "Junzhe Shen",
          "hidden": false
        },
        {
          "_id": "67c6cbd6e52534aa6ada2e28",
          "name": "Zixia Jia",
          "hidden": false
        },
        {
          "_id": "67c6cbd6e52534aa6ada2e29",
          "name": "Yuxuan Wang",
          "hidden": false
        },
        {
          "_id": "67c6cbd6e52534aa6ada2e2a",
          "user": {
            "_id": "63a95a6a7930fa8c7dd63d4e",
            "avatarUrl": "/avatars/d9d0420f7ddfe2f3a7e029fb05f1c89f.svg",
            "isPro": false,
            "fullname": "Zilong Zheng",
            "user": "zlzheng",
            "type": "user"
          },
          "name": "Zilong Zheng",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-04T09:45:59.571Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-26T07:10:08.000Z",
      "title": "Desde les heures jusqu'aux minutes : accélération sans perte de qualité pour séquences très longues jusqu'à 100K Tokens",
      "summary": "L'utilisation de modèles de langage grands (LLMs) est cruciale dans la génération de séquences longues, surtout pour des tâches qui nécessitent du temps pour traiter des séquences d'environ 100K tokens. Bien que des méthodes prédictives traditionnelles existent, elles peuvent ne pas améliorer la vitesse en simplement étendues, ou même causer des pertes. À travers un analyse détaillée, trois problèmes principaux ont été identifiés et un nouveau cadre de travail appelé TOKENSWIFT a été introduit pour les résoudre. Cette approche vise à accélérer significativement la génération de séquences longues tout en maintenant la qualité propre du modèle. Les résultats des expériences montrent que TOKENSWIFT peut accélérer la vitesse du modèle plus de trois fois, indépendamment de son taille (1.5B, 7B, 8B, 14B), et réduire considérablement le temps de génération de séquences longues. Le code est disponible sur https://github.com/bigai-nlco/TokenSwift.",
      "upvotes": 2,
      "discussionId": "67c6cbd7e52534aa6ada2e79",
      "githubRepo": "https://github.com/bigai-nlco/TokenSwift"
    },
    "publishedAt": "2025-03-04T04:56:33.061Z",
    "title": "From Hours to Minutes: Lossless Acceleration of Ultra Long Sequence Generation up to 100K Tokens",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63a95a6a7930fa8c7dd63d4e/3WZ10b-Ku3GcY1fc1MWx8.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.18890.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a95a6a7930fa8c7dd63d4e",
      "avatarUrl": "/avatars/d9d0420f7ddfe2f3a7e029fb05f1c89f.svg",
      "fullname": "Zilong Zheng",
      "name": "zlzheng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.16779",
      "authors": [
        {
          "_id": "67c65c06e116e361574405e9",
          "user": {
            "_id": "642bdfc65edcc5760cb1ea12",
            "avatarUrl": "/avatars/599b0bbb379b43cd39097c204c946075.svg",
            "isPro": false,
            "fullname": "huang",
            "user": "yxuan",
            "type": "user"
          },
          "name": "Yaxuan Huang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-04T08:51:27.582Z",
          "hidden": false
        },
        {
          "_id": "67c65c06e116e361574405ea",
          "name": "Xili Dai",
          "hidden": false
        },
        {
          "_id": "67c65c06e116e361574405eb",
          "name": "Jianan Wang",
          "hidden": false
        },
        {
          "_id": "67c65c06e116e361574405ec",
          "name": "Xianbiao Qi",
          "hidden": false
        },
        {
          "_id": "67c65c06e116e361574405ed",
          "name": "Yixing Yuan",
          "hidden": false
        },
        {
          "_id": "67c65c06e116e361574405ee",
          "name": "Xiangyu Yue",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-24T02:14:19.000Z",
      "title": "Modèle de réestructuration de chambres pour le temps de matriculation sans limites en utilisant des vues sparses",
      "summary": "Estimating the layout of rooms in panoramic images is a complex challenge that is difficult to investigate due to multi-point photogrammetry. This requires solutions in multiple steps, such as the estimation of internal and external camera configurations, image alignment, and triangulation. However, in 3D reconstruction, the recent development of 3D-based models (e.g., DUSt3R) has introduced a one-step approach, changing the paradigm of traditional multi-step methods. Consequently, we present a new method called Plane-DUSt3R, which uses the 3D-based model of DUSt3R. Plane-DUSt3R leverages the framework of DUSt3R and adjusts a modified objective to estimate structural planes, applied to a dataset of room layouts (Structure3D). With this result, Plane-DUSt3R can estimate the room layout using 2D detection results and subsequent processing steps. Unlike previous methods that depend on a single visual image or a schematic, Plane-DUSt3R extends the treatment of multiple perspectives from the visual image. Additionally, it provides a one-step and flow-based approach, simplifying the process and reducing error accumulation. Experimental results surpass advanced methods on synthetic datasets and demonstrate robust and effective results on natural images of different styles, such as panoramas and other types of images. The code is available at the following URL:\nhttps://github.com/justacar/Plane-DUSt3R",
      "upvotes": 2,
      "discussionId": "67c65c0be116e36157440751",
      "githubRepo": "https://github.com/justacar/Plane-DUSt3R"
    },
    "publishedAt": "2025-03-04T04:17:23.806Z",
    "title": "Unposed Sparse Views Room Layout Reconstruction in the Age of Pretrain Model",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.16779.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642bdfc65edcc5760cb1ea12",
      "avatarUrl": "/avatars/599b0bbb379b43cd39097c204c946075.svg",
      "fullname": "huang",
      "name": "yxuan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.01295",
      "authors": [
        {
          "_id": "67c6a8b534aeb86063e94010",
          "user": {
            "_id": "61711f02e0b1ddb56eb9b526",
            "avatarUrl": "/avatars/3e2fdf774f5bc1f73b450486d6da42d4.svg",
            "isPro": false,
            "fullname": "Mingzhe Du",
            "user": "Elfsong",
            "type": "user"
          },
          "name": "Mingzhe Du",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-04T08:34:49.954Z",
          "hidden": false
        },
        {
          "_id": "67c6a8b534aeb86063e94011",
          "name": "Anh Tuan Luu",
          "hidden": false
        },
        {
          "_id": "67c6a8b534aeb86063e94012",
          "name": "Bin Ji",
          "hidden": false
        },
        {
          "_id": "67c6a8b534aeb86063e94013",
          "name": "Xiaobao Wu",
          "hidden": false
        },
        {
          "_id": "67c6a8b534aeb86063e94014",
          "name": "Dong Huang",
          "hidden": false
        },
        {
          "_id": "67c6a8b534aeb86063e94015",
          "name": "Terry Yue Zhuo",
          "hidden": false
        },
        {
          "_id": "67c6a8b534aeb86063e94016",
          "name": "Qian Liu",
          "hidden": false
        },
        {
          "_id": "67c6a8b534aeb86063e94017",
          "name": "See-Kiong Ng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-03T08:31:16.000Z",
      "title": "CodeArena : Plateforme de Validation Concentrée pour la Création de Code avec des LLM",
      "summary": "Les modèles de langage grand (LLMs) ont amélioré l'efficacité dans la génération de code et augmenté la productivité des développeurs en combinant le compréhension de la nature du langage et du langage de programmation. Ces avancées ont défendu les efforts d'évaluation divers en ce qui concerne la capacité de génération de code. Cependant, des obstacles tels que la perte de points sur les benchmarks, la perte de données et les limitations d'accessibilité du système continuent d'obstaculer les évaluations précises au fil du temps. Pour résoudre ces limitations, nous présentons CodeArena, un cadre d'évaluation en ligne pour la génération de code de LLMs. L'innovation principale consiste à un système d'évaluation centré sur la base de rendement pour ajuster de manière dynamique les scores des modèles individuels, ce qui atténue le biais des scores causé par la perte de benchmarks. De plus, CodeArena garantit la publication de toutes les solutions présentées et des cas d'utilisation, et fournit des APIs appropriés pour l'automatisation pour streamline le processus d'évaluation de code. Nos principales contributions sont : (1) un système d'évaluation centré pour l'évaluation sans biais, (2) un dépôt public de solutions et cas d'utilisation, et (3) des APIs prêts pour l'intégration automatique de manière continue.",
      "upvotes": 2,
      "discussionId": "67c6a8b634aeb86063e9406a"
    },
    "publishedAt": "2025-03-04T02:16:25.633Z",
    "title": "CodeArena: A Collective Evaluation Platform for LLM Code Generation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01295.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "61711f02e0b1ddb56eb9b526",
      "avatarUrl": "/avatars/3e2fdf774f5bc1f73b450486d6da42d4.svg",
      "fullname": "Mingzhe Du",
      "name": "Elfsong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.01739",
      "authors": [
        {
          "_id": "67c68f7828a037872c5ce5bb",
          "name": "Wenhao Wang",
          "hidden": false
        },
        {
          "_id": "67c68f7828a037872c5ce5bc",
          "name": "Yi Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-03T17:00:36.000Z",
      "title": "VideoUFO : Dataset de 100 000 utilisateurs axé sur l'utilisateur qui génère des vidéos à partir de texte",
      "summary": "Le modèle de génération de vidéos à partir de texte peut transformer des textes en contenu visuel dynamique, ce qui permettra son application dans diverses domaines tels que la production de films, les jeux et l'éducation. Cependant, son rendement réel est souvent inférieur aux attentes des utilisateurs. Une des principales causes est que ces modèles n'ont pas été entraînés avec des contenus vidéos liés aux thèmes que les utilisateurs souhaitent créer. Dans cet article, nous proposons le premier ensemble de données de vidéos (VideoUFO) qui est aligné avec les préoccupations réelles des utilisateurs. VideoUFO présente deux caractéristiques notables : (1) un minimum de répétition avec d'autres ensembles de données de vidéo (0,29%), et (2) il a été recherché et sélectionné sous la licence Creative Commons en utilisant l'API officielle de YouTube. Ces caractéristiques offrent aux chercheurs une grande liberté pour étendre leurs sources d'entraînement. VideoUFO comprend plus de 1,090,000 clips de vidéo, chacun avec une brève et une description détaillée. Spécifiquement, grâce au clustering, 1 291 thèmes d'intérêt des utilisateurs ont été identifiés dans l'ensemble de données de vidéos basée sur le texte (VidProM). Ensuite, ces thèmes ont été utilisés pour rechercher des vidéos sur YouTube, séparer les clips et créer des descriptions brèves et détaillées pour chaque clip. Après avoir vérifié les clips des thèmes spécifiques, il restait approximativement 1,090,000 clips de vidéo. Les expériences ont montré que : (1) un modèle de vidéo généré à partir de 16 textes n'a pas réussi à atteindre un rendement constant pour tous les thèmes d'intérêt des utilisateurs, et (2) un modèle simple entraîné avec VideoUFO a démontré être plus efficace que d'autres modèles. L'ensemble de données est disponible sous la licence CC BY 4.0 (https://huggingface.co/datasets/WenhaoWang/VideoUFO).",
      "upvotes": 2,
      "discussionId": "67c68f7a28a037872c5ce60d"
    },
    "publishedAt": "2025-03-04T00:29:56.570Z",
    "title": "VideoUFO: A Million-Scale User-Focused Dataset for Text-to-Video Generation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01739.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62b32a4429a410b7f6b06710",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b32a4429a410b7f6b06710/VzgvmnlYZWuifZTkIkCxy.jpeg",
      "fullname": "Wenhao Wang",
      "name": "WenhaoWang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 13
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.01807",
      "authors": [
        {
          "_id": "67c67ff6dec55d10cb10fc9e",
          "user": {
            "_id": "62608fc2ffe8827cb1d89f9f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654027835241-62608fc2ffe8827cb1d89f9f.png",
            "isPro": false,
            "fullname": "Hamish Ivison",
            "user": "hamishivi",
            "type": "user"
          },
          "name": "Hamish Ivison",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-04T08:40:13.649Z",
          "hidden": false
        },
        {
          "_id": "67c67ff6dec55d10cb10fc9f",
          "name": "Muru Zhang",
          "hidden": false
        },
        {
          "_id": "67c67ff6dec55d10cb10fca0",
          "name": "Faeze Brahman",
          "hidden": false
        },
        {
          "_id": "67c67ff6dec55d10cb10fca1",
          "name": "Pang Wei Koh",
          "hidden": false
        },
        {
          "_id": "67c67ff6dec55d10cb10fca2",
          "name": "Pradeep Dasigi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-03T18:37:26.000Z",
      "title": "Sélection de données à grande échelle et ajustement d'instructions (ou ajustement d'instructions et entraînement)",
      "summary": "La sélection des données de classement est un pas important dans l'entraînement de modèles de langage pour des commandes. Un ensemble de données bien sélectionné peut permettre qu'un modèle entraîné surpasse un modèle utilisant un ensemble de données plus large et plus bruité. L'approche de sélection automatique de données pour l'entraînement de commandes consiste généralement à sélectionner un petit ensemble de données (environ 10k échantillons) depuis un petit pool (100-200k échantillons) pour tester. Cependant, les modèles populaires d'entraînement de commandes utilisent des millions d'échantillons dans un pool plus grand pendant l'entraînement. Nous avons étudié systématiquement comment échelonner le méthode de sélection de données dans ces configurations. Nous avons sélectionné jusqu'à 2,5 millions d'échantillons du pool et évalué sur 7 tâches différentes. Nous montrons que de nombreux méthodes récentes sont moins performantes que la sélection aléatoire et que leur performance diminue avec le croissance du pool de données. Cependant, nous avons démontré que la représentation de la sélection de données basée sur la représentation des états cachés de modèles de prédiction (RDS+) dépasse les méthodes complexes dans tous les scénarios d'évaluation, tout en prioritisant l'efficacité du calcul. Cette méthode est basée sur la représentation des états cachés de modèles de prédiction. Nos résultats soulignent la nécessité de revoir avec plus de précision les caractéristiques d'échellabilité des méthodes de sélection de données automatiques proposées. Notre code, nos données et nos modèles sont disponibles sur la URL suivante : https://github.com/hamishivi/automated-instruction-selection",
      "upvotes": 2,
      "discussionId": "67c67ff9dec55d10cb10fcef"
    },
    "publishedAt": "2025-03-03T23:44:06.105Z",
    "title": "Large-Scale Data Selection for Instruction Tuning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01807.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62608fc2ffe8827cb1d89f9f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654027835241-62608fc2ffe8827cb1d89f9f.png",
      "fullname": "Hamish Ivison",
      "name": "hamishivi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 11
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.01063",
      "authors": [
        {
          "_id": "67c6b72b7aad9a016ae60797",
          "name": "David Noever",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-02T23:59:52.000Z",
      "title": "Le langage vocal développé par l'IA : méthodes pour éviter que le langage de la machine soit incompréhensible pour les humains",
      "summary": "Cet article explore la possibilité de développer des langages personnels en utilisant des systèmes de communication entre machines (M2M) via des modèles de langage grands (LLMs). Un système a été mis en place pour transformer un ensemble précis de caractères, similaires à ceux de la musique, en fréquences sonores. Chaque caractère est associé à une fréquence unique, commençant par un espace logique (220 Hz) et se terminant à un valeur de 126. Ce système dépasse approximativement 7.9 octaves et mappe les caractères les plus élevés en fréquences d'ultrasons (>20 kHz), qui dépassent le domaine de la perception humaine. Le prototype de logiciel visualise, reproduit auditivement et montre la notation musicale ABC, permettant d'analyser la densité d'information et la vitesse de transmission. Les résultats des tests montrent que l'encodage de caractères peut dépasser la vitesse d'information des langues humaines et que certaines caractéristiques peuvent fonctionner en dehors du domaine de la perception humaine. Cette étude fournit une réponse directe aux préoccupations concernant la possibilité de développer des langages personnels à grande échelle dans les cinq prochaines années, et offre la base technique nécessaire pour la communication, sa détection et son gestion.",
      "upvotes": 0,
      "discussionId": "67c6b72c7aad9a016ae607bb"
    },
    "publishedAt": "2025-03-04T03:20:03.380Z",
    "title": "AI-Invented Tonal Languages: Preventing a Machine Lingua Franca Beyond Human Understanding",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63136a82e29fb2e86d5e5bdd/mgIPjnhtUaGLR2Iv4ViL6.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01063.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63136a82e29fb2e86d5e5bdd",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63136a82e29fb2e86d5e5bdd/pFZDuQtzfUStovbwwZGvn.png",
      "fullname": "David Noever",
      "name": "dnoever",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.00729",
      "authors": [
        {
          "_id": "67c6ab3ec0b62d612c54ddf5",
          "user": {
            "_id": "6628c6107751d297d7025a71",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6628c6107751d297d7025a71/S1rm5VIwV2Uxfv8GetKMU.jpeg",
            "isPro": false,
            "fullname": "Lei Mingcong",
            "user": "SP4595",
            "type": "user"
          },
          "name": "Mingcong Lei",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-04T08:34:48.061Z",
          "hidden": false
        },
        {
          "_id": "67c6ab3ec0b62d612c54ddf6",
          "name": "Ge Wang",
          "hidden": false
        },
        {
          "_id": "67c6ab3ec0b62d612c54ddf7",
          "name": "Yiming Zhao",
          "hidden": false
        },
        {
          "_id": "67c6ab3ec0b62d612c54ddf8",
          "name": "Zhixin Mai",
          "hidden": false
        },
        {
          "_id": "67c6ab3ec0b62d612c54ddf9",
          "name": "Qing Zhao",
          "hidden": false
        },
        {
          "_id": "67c6ab3ec0b62d612c54ddfa",
          "name": "Yao Guo",
          "hidden": false
        },
        {
          "_id": "67c6ab3ec0b62d612c54ddfb",
          "name": "Zhen Li",
          "hidden": false
        },
        {
          "_id": "67c6ab3ec0b62d612c54ddfc",
          "name": "Shuguang Cui",
          "hidden": false
        },
        {
          "_id": "67c6ab3ec0b62d612c54ddfd",
          "name": "Yatong Han",
          "hidden": false
        },
        {
          "_id": "67c6ab3ec0b62d612c54ddfe",
          "name": "Jinke Ren",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-02T04:50:59.000Z",
      "title": "CLEA : Agente léger de cycles fermés pour améliorer l'exécution dans des environnements dynamiques",
      "summary": "Les modèles de langage grand (LLMs) développent des capacités impressionnantes grâce à la décomposition stratégique des tâches et à l'inférence significative. Cependant, l'application de ces systèmes légers à des tâches à long terme est complexe, car la séquence de sous-tâches peut ne pas être fiable pour sa mise en œuvre. Pour résoudre ces limitations dans des environnements dynamiques, nous proposons une nouvelle architecture appelée CLEA (Closed-Loop Robot Agent). Cette architecture est basée sur quatre modèles de langage grands ou spécialisés pour la gestion de tâches fermées. Le cadre comporte deux innovations clés : 1) la planification interactive des tâches génère des sous-tâches exécutables en fonction de la mémoire de l'environnement. 2) l'exécution multimodal curriculum évalue la probabilité de succès des actions et, si les variations de l'environnement dépassent les limites établies, active une structure de réplanification hiérarchique. Pour évaluer les effets de CLEA, nous avons effectué des expériences dans des environnements réels en utilisant des objets pouvant être manipulés. Avec deux robots différents, nous avons réalisé des tâches de recherche et de manipulation, ainsi que des tâches de recherche et de manipulation combinées. À travers 12 essais de tâches, CLEA dépasse les modèles de référence, augmentant la taux de succès de 67,3% et la taux de completion de tâches de 52,8%. Ces résultats montrent que CLEA améliore significativement la robustesse de la planification et de l'exécution de tâches dans des environnements dynamiques.",
      "upvotes": 0,
      "discussionId": "67c6ab42c0b62d612c54df71",
      "projectPage": "https://sp4595.github.io/CLEA/",
      "githubRepo": "https://github.com/SP4595/CLEA-Closed-Loop-Embodied-Agent"
    },
    "publishedAt": "2025-03-04T02:27:17.351Z",
    "title": "CLEA: Closed-Loop Embodied Agent for Enhancing Task Execution in Dynamic Environments",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.00729.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6628c6107751d297d7025a71",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6628c6107751d297d7025a71/S1rm5VIwV2Uxfv8GetKMU.jpeg",
      "fullname": "Lei Mingcong",
      "name": "SP4595",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  }
]