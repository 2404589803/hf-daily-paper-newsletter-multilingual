[
  {
    "paper": {
      "id": "2504.21635",
      "authors": [
        {
          "_id": "68130be55342cbe1ddefb262",
          "user": {
            "_id": "65704741e1cfce1764ce652e",
            "avatarUrl": "/avatars/9189aaf417426af4ebe381ed364a6c0e.svg",
            "isPro": false,
            "fullname": "Zeina Aldallal",
            "user": "ZeinaD",
            "type": "user"
          },
          "name": "Zeina Aldallal",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-01T05:51:34.441Z",
          "hidden": false
        },
        {
          "_id": "68130be55342cbe1ddefb263",
          "name": "Sara Chrouf",
          "hidden": false
        },
        {
          "_id": "68130be55342cbe1ddefb264",
          "user": {
            "_id": "65276c7911a8a521c91bc10f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65276c7911a8a521c91bc10f/39dbuUtqQTJERTJ_WkxSh.jpeg",
            "isPro": false,
            "fullname": "Khalil Hennara",
            "user": "Hennara",
            "type": "user"
          },
          "name": "Khalil Hennara",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-01T06:55:15.787Z",
          "hidden": false
        },
        {
          "_id": "68130be55342cbe1ddefb265",
          "user": {
            "_id": "63aa7667769a10efc404fbbc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63aa7667769a10efc404fbbc/tn8ZxUmTEMS0Gze7_F7JL.jpeg",
            "isPro": false,
            "fullname": "Mohamed Motasim Hamed",
            "user": "Moatasem444",
            "type": "user"
          },
          "name": "Mohamed Motaism Hamed",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-01T07:00:18.613Z",
          "hidden": false
        },
        {
          "_id": "68130be55342cbe1ddefb266",
          "user": {
            "_id": "6496df4b3c64d75523a11973",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6496df4b3c64d75523a11973/I_Qn5-3Czngle-NsGmabO.jpeg",
            "isPro": false,
            "fullname": "Muhammad Hreden",
            "user": "hr99",
            "type": "user"
          },
          "name": "Muhammad Hreden",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-01T07:16:03.684Z",
          "hidden": false
        },
        {
          "_id": "68130be55342cbe1ddefb267",
          "name": "Safwan AlModhayan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-30T13:37:24.000Z",
      "submittedOnDailyAt": "2025-05-01T05:10:38.792Z",
      "title": "Sa'd : Modèle de langage court pour ajouter des commentaires de traduction en arabe",
      "submittedOnDailyBy": {
        "_id": "65276c7911a8a521c91bc10f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65276c7911a8a521c91bc10f/39dbuUtqQTJERTJ_WkxSh.jpeg",
        "isPro": false,
        "fullname": "Khalil Hennara",
        "user": "Hennara",
        "type": "user"
      },
      "summary": "Le discours diaclique de l'arabe reste un problème enduring dans le domaine du traitement du langage naturel en raison de la richesse de son vocabulaire. Dans cet article, nous présentons un nouvel approche basée sur Kuwain 1.5B de Hennara et al. [2025], appelée Sadeed. Ce modèle a été entraîné initialement sur différents corpus arabes et a été ajusté précisément sur un ensemble de données de haute qualité de discours diaclique généré par un processus strict de nettoyage et de normalisation des données. Bien que limité en ressources de calcul, le modèle obtient des résultats relativement excellents comparés aux modèles de langage à grande échelle commercialisés, et dépasse les modèles traditionnels entraînés dans le même domaine. De plus, nous mettons en évidence les limitations importantes du processus actuel d'évaluation du discours diaclique en arabe et présentons un nouveau benchmark appelé SadeedDiac-25 pour résoudre ces problèmes. Ce benchmark permet une évaluation juste et complète sur différents genres de textes et niveaux de complexité. Sadeed et SadeedDiac-25 fournissent une base solide pour le développement d'applications de NLP arabe, comme la traduction automatique, la synthèse de voix et les outils d'apprentissage du langage.",
      "upvotes": 40,
      "discussionId": "68130be65342cbe1ddefb2a6",
      "ai_keywords": [
        "decoder-only language model",
        "Kuwain 1.5B Hennara",
        "fine-tuned",
        "diacritized datasets",
        "data-cleaning",
        "normalization",
        "benchmarking",
        "SadeedDiac-25"
      ]
    },
    "publishedAt": "2025-04-30T09:37:24.000Z",
    "title": "Sadeed: Advancing Arabic Diacritization Through Small Language Model",
    "summary": "Arabic text diacritization remains a persistent challenge in natural language\nprocessing due to the language's morphological richness. In this paper, we\nintroduce Sadeed, a novel approach based on a fine-tuned decoder-only language\nmodel adapted from Kuwain 1.5B Hennara et al. [2025], a compact model\noriginally trained on diverse Arabic corpora. Sadeed is fine-tuned on carefully\ncurated, high-quality diacritized datasets, constructed through a rigorous\ndata-cleaning and normalization pipeline. Despite utilizing modest\ncomputational resources, Sadeed achieves competitive results compared to\nproprietary large language models and outperforms traditional models trained on\nsimilar domains. Additionally, we highlight key limitations in current\nbenchmarking practices for Arabic diacritization. To address these issues, we\nintroduce SadeedDiac-25, a new benchmark designed to enable fairer and more\ncomprehensive evaluation across diverse text genres and complexity levels.\nTogether, Sadeed and SadeedDiac-25 provide a robust foundation for advancing\nArabic NLP applications, including machine translation, text-to-speech, and\nlanguage learning tools.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.21635.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65276c7911a8a521c91bc10f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65276c7911a8a521c91bc10f/39dbuUtqQTJERTJ_WkxSh.jpeg",
      "fullname": "Khalil Hennara",
      "name": "Hennara",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.21776",
      "authors": [
        {
          "_id": "6812d593060494e99e4835e0",
          "name": "Xiaoxi Li",
          "hidden": false
        },
        {
          "_id": "6812d593060494e99e4835e1",
          "name": "Jiajie Jin",
          "hidden": false
        },
        {
          "_id": "6812d593060494e99e4835e2",
          "name": "Guanting Dong",
          "hidden": false
        },
        {
          "_id": "6812d593060494e99e4835e3",
          "name": "Hongjin Qian",
          "hidden": false
        },
        {
          "_id": "6812d593060494e99e4835e4",
          "name": "Yutao Zhu",
          "hidden": false
        },
        {
          "_id": "6812d593060494e99e4835e5",
          "name": "Yongkang Wu",
          "hidden": false
        },
        {
          "_id": "6812d593060494e99e4835e6",
          "name": "Ji-Rong Wen",
          "hidden": false
        },
        {
          "_id": "6812d593060494e99e4835e7",
          "name": "Zhicheng Dou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-30T16:25:25.000Z",
      "submittedOnDailyAt": "2025-05-01T00:33:55.498Z",
      "title": "WebThinker : Attribution de pouvoirs à grands modèles d'inférence basés sur des recherches approfondies",
      "submittedOnDailyBy": {
        "_id": "61cd4b833dd34ba1985e0753",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61cd4b833dd34ba1985e0753/BfHfrwotoMESpXZOHiIe4.png",
        "isPro": false,
        "fullname": "KABI",
        "user": "dongguanting",
        "type": "user"
      },
      "summary": "Les modèles de grandes réseaux logiques (LRMs), notamment OpenAI-o1 et DeepSeek-R1, démontrent leur capacité logique à long terme. Cependant, ces modèles dépendent de connaissances internes statiques et leur performance dans des tâches de densité de connaissance complexe est limitée, ainsi que leur capacité à intégrer plusieurs sources d'information dans des rapports de recherche est limitée. Pour résoudre ces problèmes, on propose un agent de recherche profonde appelé WebThinker. WebThinker automatise la recherche en ligne et le mouvement entre pages pour écrire un brouillon de rapport de recherche. Il inclut un module d'Explorateur de la Recherche Profonde, ce qui permet de rechercher et d'obtenir des informations dynamiques lorsque des défauts de connaissance sont rencontrés. De plus, il utilise une stratégie automatique de pensée, de recherche et de rédaction pour combiner la logique, l'information et la rédaction temporellement en séquence. De plus, une stratégie d'apprentissage par renforcement est utilisée pour promouvoir l'utilisation de outils de recherche en ligne via plusieurs directives de préférence (DPO) en ligne. Dans des benchmarks logiques complexes (GPQA, GAIA, WebWalkerQA, HLE) et dans la génération de rapports scientifiques (Glaive), WebThinker démontre une supériorité significative par rapport aux méthodes actuelles. Notre approche améliore la fiabilité et l'applicabilité des LRMs, ainsi que la préparation de systèmes de recherche profondes et la capacité à travailler dans divers systèmes de recherche. Le code est disponible sur https://github.com/RUC-NLPIR/WebThinker.",
      "upvotes": 18,
      "discussionId": "6812d594060494e99e48361c",
      "ai_keywords": [
        "Large reasoning models (LRMs)",
        "WebThinker",
        "Deep Web Explorer",
        "Autonomous Think-Search-and-Draft strategy",
        "RL-based training strategy",
        "iterative online Direct Preference Optimization (DPO)",
        "GPQA",
        "GAIA",
        "WebWalkerQA",
        "HLE",
        "Glaive"
      ]
    },
    "publishedAt": "2025-04-30T12:25:25.000Z",
    "title": "WebThinker: Empowering Large Reasoning Models with Deep Research\n  Capability",
    "summary": "Large reasoning models (LRMs), such as OpenAI-o1 and DeepSeek-R1, demonstrate\nimpressive long-horizon reasoning capabilities. However, their reliance on\nstatic internal knowledge limits their performance on complex,\nknowledge-intensive tasks and hinders their ability to produce comprehensive\nresearch reports requiring synthesis of diverse web information. To address\nthis, we propose WebThinker, a deep research agent that empowers LRMs\nto autonomously search the web, navigate web pages, and draft research reports\nduring the reasoning process. WebThinker integrates a Deep Web\nExplorer module, enabling LRMs to dynamically search, navigate, and extract\ninformation from the web when encountering knowledge gaps. It also employs an\nAutonomous Think-Search-and-Draft strategy, allowing the model to\nseamlessly interleave reasoning, information gathering, and report writing in\nreal time. To further enhance research tool utilization, we introduce an\nRL-based training strategy via iterative online Direct Preference\nOptimization (DPO). Extensive experiments on complex reasoning benchmarks\n(GPQA, GAIA, WebWalkerQA, HLE) and scientific report generation tasks (Glaive)\ndemonstrate that WebThinker significantly outperforms existing methods and\nstrong proprietary systems. Our approach enhances LRM reliability and\napplicability in complex scenarios, paving the way for more capable and\nversatile deep research systems. The code is available at\nhttps://github.com/RUC-NLPIR/WebThinker.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.21776.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "61cd4b833dd34ba1985e0753",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61cd4b833dd34ba1985e0753/BfHfrwotoMESpXZOHiIe4.png",
      "fullname": "KABI",
      "name": "dongguanting",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 18
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.21233",
      "authors": [
        {
          "_id": "6812d62ae74b39182bd17c9c",
          "name": "Haoran Xu",
          "hidden": false
        },
        {
          "_id": "6812d62ae74b39182bd17c9d",
          "name": "Baolin Peng",
          "hidden": false
        },
        {
          "_id": "6812d62ae74b39182bd17c9e",
          "name": "Hany Awadalla",
          "hidden": false
        },
        {
          "_id": "6812d62ae74b39182bd17c9f",
          "name": "Dongdong Chen",
          "hidden": false
        },
        {
          "_id": "6812d62ae74b39182bd17ca0",
          "name": "Yen-Chun Chen",
          "hidden": false
        },
        {
          "_id": "6812d62ae74b39182bd17ca1",
          "name": "Mei Gao",
          "hidden": false
        },
        {
          "_id": "6812d62ae74b39182bd17ca2",
          "name": "Young Jin Kim",
          "hidden": false
        },
        {
          "_id": "6812d62ae74b39182bd17ca3",
          "name": "Yunsheng Li",
          "hidden": false
        },
        {
          "_id": "6812d62ae74b39182bd17ca4",
          "name": "Liliang Ren",
          "hidden": false
        },
        {
          "_id": "6812d62ae74b39182bd17ca5",
          "name": "Yelong Shen",
          "hidden": false
        },
        {
          "_id": "6812d62ae74b39182bd17ca6",
          "name": "Shuohang Wang",
          "hidden": false
        },
        {
          "_id": "6812d62ae74b39182bd17ca7",
          "name": "Weijian Xu",
          "hidden": false
        },
        {
          "_id": "6812d62ae74b39182bd17ca8",
          "name": "Jianfeng Gao",
          "hidden": false
        },
        {
          "_id": "6812d62ae74b39182bd17ca9",
          "name": "Weizhu Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-30T00:04:35.000Z",
      "submittedOnDailyAt": "2025-05-01T00:32:47.316Z",
      "title": "Phi-4-Mini-Reasoning : En mathématiques, on cherche les limites de petits modèles de langage logique.",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Chain-of-Thought (CoT) est effective pour améliorer significativement la capacité d'inférence de manière formelle dans des modèles de langage grands (LLMs) lors de leur entraînement pour générer des étapes intermédiaires de l'inférence actuelles. Cependant, l'amélioration de la capacité d'inférence dans des modèles de langage petits (SLMs) est confrontée à des limites de taille du modèle. Une recherche récente de Deepseek-R1 montre que l'accumulation d'expériences générées par des LLMs peut significativement améliorer la capacité d'inférence dans des SLMs. Cependant, aucun procédé de modélisation spécifique n'a été publié. Dans cet article, nous proposons un procédé systématique d'entraînement pour des SLMs. Ce procédé est composé de quatre étapes : 1) entraînement à grande échelle avec diverses expériences réduites, 2) ajustement normatif avec une qualité élevée de données réduites de longueur, 3) utilisation de jeux de données de préférence choisies avec plus de soin grâce à Rollout DPO, et 4) apprentissage par renforcement (RL) avec des récompenses vérifiables. Cette méthodologie est appliquée à Phi-4-Mini (un modèle petit de 380M paramètres), ce qui résulte en Phi-4-Mini-Reasoning, un modèle qui dépasse les modèles plus grands dans des tâches d'inférence mathématique. Par exemple, Phi-4-Mini-Reasoning obtient 3,2 points plus que DeepSeek-R1-Distill-Qwen-7B et plus de 7,7 points plus que DeepSeek-R1-Distill-Llama-8B. Ces résultats montrent que un procédé d'entraînement soigneusement conçu avec des données de CoT de haute qualité peut développer une forte capacité d'inférence même dans des modèles petits limités par les ressources.",
      "upvotes": 14,
      "discussionId": "6812d62be74b39182bd17cdb",
      "ai_keywords": [
        "Chain-of-Thought (CoT)",
        "Large Language Models (LLMs)",
        "Small Language Models (SLMs)",
        "distillation",
        "synthetic data",
        "mid-training",
        "diverse distilled long-CoT data",
        "supervised fine-tuning",
        "high-quality long-CoT data",
        "Rollout DPO",
        "preference dataset",
        "Reinforcement Learning (RL)",
        "Verifiable Reward",
        "Phi-4-Mini",
        "resource-constrained small models"
      ]
    },
    "publishedAt": "2025-04-29T20:04:35.000Z",
    "title": "Phi-4-Mini-Reasoning: Exploring the Limits of Small Reasoning Language\n  Models in Math",
    "summary": "Chain-of-Thought (CoT) significantly enhances formal reasoning capabilities\nin Large Language Models (LLMs) by training them to explicitly generate\nintermediate reasoning steps. While LLMs readily benefit from such techniques,\nimproving reasoning in Small Language Models (SLMs) remains challenging due to\ntheir limited model capacity. Recent work by Deepseek-R1 demonstrates that\ndistillation from LLM-generated synthetic data can substantially improve the\nreasoning ability of SLM. However, the detailed modeling recipe is not\ndisclosed. In this work, we present a systematic training recipe for SLMs that\nconsists of four steps: (1) large-scale mid-training on diverse distilled\nlong-CoT data, (2) supervised fine-tuning on high-quality long-CoT data, (3)\nRollout DPO leveraging a carefully curated preference dataset, and (4)\nReinforcement Learning (RL) with Verifiable Reward. We apply our method on\nPhi-4-Mini, a compact 3.8B-parameter model. The resulting Phi-4-Mini-Reasoning\nmodel exceeds, on math reasoning tasks, much larger reasoning models, e.g.,\noutperforming DeepSeek-R1-Distill-Qwen-7B by 3.2 points and\nDeepSeek-R1-Distill-Llama-8B by 7.7 points on Math-500. Our results validate\nthat a carefully designed training recipe, with large-scale high-quality CoT\ndata, is effective to unlock strong reasoning capabilities even in\nresource-constrained small models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.21233.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6753
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.20966",
      "authors": [
        {
          "_id": "6812e473060494e99e4c5116",
          "name": "Zayd M. K. Zuhri",
          "hidden": false
        },
        {
          "_id": "6812e473060494e99e4c5117",
          "name": "Erland Hilman Fuadi",
          "hidden": false
        },
        {
          "_id": "6812e473060494e99e4c5118",
          "name": "Alham Fikri Aji",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/60cf8a354061635e43b28f60/ZQTvDnn0F_MaUg5zlVk62.png"
      ],
      "publishedAt": "2025-04-29T17:36:18.000Z",
      "submittedOnDailyAt": "2025-05-01T01:38:03.643Z",
      "title": "Softpick : Sin atención, il n'y a pas de grands noeuds d'activation pour le Softmax rectifié",
      "submittedOnDailyBy": {
        "_id": "60cf8a354061635e43b28f60",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60cf8a354061635e43b28f60/o8gv9mG5cvnopJZJnNibi.jpeg",
        "isPro": true,
        "fullname": "Zayd Muhammad Kawakibi Zuhri",
        "user": "zaydzuhri",
        "type": "user"
      },
      "summary": "Présentant une version modifiée de la softmax dans la mécanisme d'attention du Transformer. Cette version modifiée de la softmax ne somme pas à 1 mais permet une substitution directe. Softpick réduit significativement les activations grâce à l'Attention Sink et maintient les performances tout en atteignant une somme qui ne soit pas 1. Dans des expériences avec un modèle à 340M paramètres, Softpick atteint des performances identiques à celles de la softmax sur les benchmarks standards et atteint un taux d'Attention Sink de 0%. Le Transformer Softpick génère des états cachés plus faibles (340 vs 33,510) et crée des cartes d'attention plus épars (46,97% d'éparsité). Les modèles utilisant Softpick dépassent la softmax continuellement lors de l'automatisation, surtout lorsqu'ils utilisent une précision en bits basse. L'analyse et la discussion révèlent que Softpick ouvre de nouvelles possibilités pour l'automatisation, l'entraînement à faible précision en bits, l'optimisation d'éparsité et l'interprétabilité. Le code est disponible à https://github.com/zaydzuhri/softpick-attention.",
      "upvotes": 11,
      "discussionId": "6812e475060494e99e4c519f",
      "ai_keywords": [
        "softpick",
        "rectified",
        "drop-in replacement",
        "softmax",
        "transformer attention mechanisms",
        "attention sink",
        "massive activations",
        "performance parity",
        "kurtosis",
        "sparse attention maps",
        "quantized",
        "bit precisions",
        "quantization",
        "low-precision training",
        "sparsity optimization",
        "pruning",
        "interpretability"
      ]
    },
    "publishedAt": "2025-04-29T13:36:18.000Z",
    "title": "Softpick: No Attention Sink, No Massive Activations with Rectified\n  Softmax",
    "summary": "We introduce softpick, a rectified, not sum-to-one, drop-in replacement for\nsoftmax in transformer attention mechanisms that eliminates attention sink and\nmassive activations. Our experiments with 340M parameter models demonstrate\nthat softpick maintains performance parity with softmax on standard benchmarks\nwhile achieving 0% sink rate. The softpick transformer produces hidden states\nwith significantly lower kurtosis (340 vs 33,510) and creates sparse attention\nmaps (46.97% sparsity). Models using softpick consistently outperform softmax\nwhen quantized, with particularly pronounced advantages at lower bit\nprecisions. Our analysis and discussion shows how softpick has the potential to\nopen new possibilities for quantization, low-precision training, sparsity\noptimization, pruning, and interpretability. Our code is available at\nhttps://github.com/zaydzuhri/softpick-attention.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/60cf8a354061635e43b28f60/ZQTvDnn0F_MaUg5zlVk62.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.20966.png",
    "numComments": 4,
    "submittedBy": {
      "_id": "60cf8a354061635e43b28f60",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60cf8a354061635e43b28f60/o8gv9mG5cvnopJZJnNibi.jpeg",
      "fullname": "Zayd Muhammad Kawakibi Zuhri",
      "name": "zaydzuhri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.21318",
      "authors": [
        {
          "_id": "6812d3c3e74b39182bd0dc82",
          "name": "Marah Abdin",
          "hidden": false
        },
        {
          "_id": "6812d3c3e74b39182bd0dc83",
          "name": "Sahaj Agarwal",
          "hidden": false
        },
        {
          "_id": "6812d3c3e74b39182bd0dc84",
          "name": "Ahmed Awadallah",
          "hidden": false
        },
        {
          "_id": "6812d3c3e74b39182bd0dc85",
          "name": "Vidhisha Balachandran",
          "hidden": false
        },
        {
          "_id": "6812d3c3e74b39182bd0dc86",
          "name": "Harkirat Behl",
          "hidden": false
        },
        {
          "_id": "6812d3c3e74b39182bd0dc87",
          "name": "Lingjiao Chen",
          "hidden": false
        },
        {
          "_id": "6812d3c3e74b39182bd0dc88",
          "name": "Gustavo de Rosa",
          "hidden": false
        },
        {
          "_id": "6812d3c3e74b39182bd0dc89",
          "name": "Suriya Gunasekar",
          "hidden": false
        },
        {
          "_id": "6812d3c3e74b39182bd0dc8a",
          "name": "Mojan Javaheripi",
          "hidden": false
        },
        {
          "_id": "6812d3c3e74b39182bd0dc8b",
          "name": "Neel Joshi",
          "hidden": false
        },
        {
          "_id": "6812d3c3e74b39182bd0dc8c",
          "name": "Piero Kauffmann",
          "hidden": false
        },
        {
          "_id": "6812d3c3e74b39182bd0dc8d",
          "name": "Yash Lara",
          "hidden": false
        },
        {
          "_id": "6812d3c3e74b39182bd0dc8e",
          "name": "Caio César Teodoro Mendes",
          "hidden": false
        },
        {
          "_id": "6812d3c3e74b39182bd0dc8f",
          "name": "Arindam Mitra",
          "hidden": false
        },
        {
          "_id": "6812d3c3e74b39182bd0dc90",
          "name": "Besmira Nushi",
          "hidden": false
        },
        {
          "_id": "6812d3c3e74b39182bd0dc91",
          "name": "Dimitris Papailiopoulos",
          "hidden": false
        },
        {
          "_id": "6812d3c3e74b39182bd0dc92",
          "name": "Olli Saarikivi",
          "hidden": false
        },
        {
          "_id": "6812d3c3e74b39182bd0dc93",
          "name": "Shital Shah",
          "hidden": false
        },
        {
          "_id": "6812d3c3e74b39182bd0dc94",
          "name": "Vaishnavi Shrivastava",
          "hidden": false
        },
        {
          "_id": "6812d3c3e74b39182bd0dc95",
          "name": "Vibhav Vineet",
          "hidden": false
        },
        {
          "_id": "6812d3c3e74b39182bd0dc96",
          "name": "Yue Wu",
          "hidden": false
        },
        {
          "_id": "6812d3c3e74b39182bd0dc97",
          "name": "Safoora Yousefi",
          "hidden": false
        },
        {
          "_id": "6812d3c3e74b39182bd0dc98",
          "name": "Guoqing Zheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-30T05:05:09.000Z",
      "submittedOnDailyAt": "2025-05-01T00:22:34.583Z",
      "title": "Phi-4-reasoning Rapport technique",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Introduis un modèle logique qui atteint un excellent rendement dans des tâches complexes avec 140 milliards de paramètres. Grâce à la fin de la normalisation physique 4, on sélectionne des propositions d'apprentissage avec la complexité et la diversité adéquates, et on génère des exemples logiques en utilisant o3-mini, ainsi que développe un raisonnement long et détaillé en exploitant efficacement les calculs pendant l'inférence. De plus, le Physique 4 Plus, développé grâce à l'optimisation de l'apprentissage, génère des chaînes de raisonnement plus longues et améliore son rendement. Dans une large gamme de tâches de raisonnement, les deux modèles dépassent le rendement des modèles comme DeepSeek-R1-Distill-Llama-70B et tous les modèles DeepSeek-R1, ainsi que ceux de plus grands modèles d'apprentissage open. Une évaluation extensive est réalisée, incluant des tests de raisonnement mathématique et scientifique, de résolution de problèmes de code et d'algorithmes, de planification et de compréhension spatiale. Intéressamment, on observe également une amélioration significative dans les benchmarks généraux. Dans ce rapport, nous fournissons des commentaires sur les données d'entraînement, les méthodes d'apprentissage et d'évaluation. Le bénéfice de la normalisation des données pour l'SFT s'étend également aux modèles de raisonnement et peut être évolué grâce à l'RL. Enfin, on souligne l'opportunité d'améliorer les méthodes d'évaluation du rendement et de robustesse des modèles de raisonnement.",
      "upvotes": 8,
      "discussionId": "6812d3c4e74b39182bd0dcd1",
      "ai_keywords": [
        "parameter reasoning model",
        "supervised fine-tuning",
        "reasoning demonstrations",
        "inference-time compute",
        "outcome-based reinforcement learning",
        "reasoning chains",
        "reasoning traces",
        "reasoning language models",
        "general-purpose benchmarks",
        "performance assessment",
        "robustness assessment"
      ]
    },
    "publishedAt": "2025-04-30T01:05:09.000Z",
    "title": "Phi-4-reasoning Technical Report",
    "summary": "We introduce Phi-4-reasoning, a 14-billion parameter reasoning model that\nachieves strong performance on complex reasoning tasks. Trained via supervised\nfine-tuning of Phi-4 on carefully curated set of \"teachable\" prompts-selected\nfor the right level of complexity and diversity-and reasoning demonstrations\ngenerated using o3-mini, Phi-4-reasoning generates detailed reasoning chains\nthat effectively leverage inference-time compute. We further develop\nPhi-4-reasoning-plus, a variant enhanced through a short phase of outcome-based\nreinforcement learning that offers higher performance by generating longer\nreasoning traces. Across a wide range of reasoning tasks, both models\noutperform significantly larger open-weight models such as\nDeepSeek-R1-Distill-Llama-70B model and approach the performance levels of full\nDeepSeek-R1 model. Our comprehensive evaluations span benchmarks in math and\nscientific reasoning, coding, algorithmic problem solving, planning, and\nspatial understanding. Interestingly, we observe a non-trivial transfer of\nimprovements to general-purpose benchmarks as well. In this report, we provide\ninsights into our training data, our training methodologies, and our\nevaluations. We show that the benefit of careful data curation for supervised\nfine-tuning (SFT) extends to reasoning language models, and can be further\namplified by reinforcement learning (RL). Finally, our evaluation points to\nopportunities for improving how we assess the performance and robustness of\nreasoning models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.21318.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6753
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.19720",
      "authors": [
        {
          "_id": "68118b63439785d7ff81a879",
          "user": {
            "_id": "647e96507f9ad5e44bac50bd",
            "avatarUrl": "/avatars/2551f99401540676cfe5cd0ed99e70c1.svg",
            "isPro": false,
            "fullname": "Ranran Zhen",
            "user": "zenRRan",
            "type": "user"
          },
          "name": "Ranran Zhen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-30T07:56:15.143Z",
          "hidden": false
        },
        {
          "_id": "68118b63439785d7ff81a87a",
          "name": "Juntao Li",
          "hidden": false
        },
        {
          "_id": "68118b63439785d7ff81a87b",
          "name": "Yixin Ji",
          "hidden": false
        },
        {
          "_id": "68118b63439785d7ff81a87c",
          "name": "Zhenlin Yang",
          "hidden": false
        },
        {
          "_id": "68118b63439785d7ff81a87d",
          "name": "Tong Liu",
          "hidden": false
        },
        {
          "_id": "68118b63439785d7ff81a87e",
          "name": "Qingrong Xia",
          "hidden": false
        },
        {
          "_id": "68118b63439785d7ff81a87f",
          "name": "Xinyu Duan",
          "hidden": false
        },
        {
          "_id": "68118b63439785d7ff81a880",
          "name": "Zhefeng Wang",
          "hidden": false
        },
        {
          "_id": "68118b63439785d7ff81a881",
          "name": "Baoxing Huai",
          "hidden": false
        },
        {
          "_id": "68118b63439785d7ff81a882",
          "name": "Min Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-28T12:14:02.000Z",
      "submittedOnDailyAt": "2025-05-01T02:08:35.108Z",
      "title": "Tidanze Control: Schéma d'Inférence Efficace pour Services de Modèles de Langue Large",
      "submittedOnDailyBy": {
        "_id": "647e96507f9ad5e44bac50bd",
        "avatarUrl": "/avatars/2551f99401540676cfe5cd0ed99e70c1.svg",
        "isPro": false,
        "fullname": "Ranran Zhen",
        "user": "zenRRan",
        "type": "user"
      },
      "summary": "Les modèles de langage grands (LLMs) ont réalisé un développement impressionnant dans le domaine de l'intelligence artificielle et ont été largement introduits dans diverses zones et applications grâce à leur complexité et à leur large gamme d'outils. Cependant, le grand nombre de paramètres qui génère une charge mémoire importante et le fort besoin de calcul pour la structure d'attention révèlent des problèmes importants de faible efficacité et de haute complexité du service d'inférence des LLMs. Récemment, ce domaine a connu un grand progrès grâce à des recherches avancées. Cet article offre une recherche exhaustive sur ces méthodes, incluant un approche basique au niveau d'instance, des stratégies profondes au niveau de cluster, des scénarios émergents et d'autres zones importantes. Au niveau d'instance, on étudie la configuration du modèle, la programmation de la demande, la prédiction de la longueur de décodage, la gestion de la base de données et les paradigmes de distribution. Au niveau de cluster, on analyse l'utilisation de clusters de GPU, l'équilibre de charge de multiples instances et les solutions de services de nuage. Dans les scénarios émergents, on discute des tâches spécifiques, des modules et des méthodes auxiliaires. Pour garantir une vision générale, on mentionne spécialement diverses zones importantes. Enfin, on présente des directions de recherche potentielles pour le développement de ce domaine.",
      "upvotes": 7,
      "discussionId": "68118b64439785d7ff81a8cd",
      "githubRepo": "https://github.com/zenrran4nlp/Awesome-LLM-Inference-Serving",
      "ai_keywords": [
        "large language models (LLMs)",
        "generative AI",
        "attention mechanism",
        "model placement",
        "request scheduling",
        "decoding length prediction",
        "storage management",
        "disaggregation paradigm",
        "GPU cluster deployment",
        "multi-instance load balancing",
        "cloud service solutions"
      ]
    },
    "publishedAt": "2025-04-28T08:14:02.000Z",
    "title": "Taming the Titans: A Survey of Efficient LLM Inference Serving",
    "summary": "Large Language Models (LLMs) for Generative AI have achieved remarkable\nprogress, evolving into sophisticated and versatile tools widely adopted across\nvarious domains and applications. However, the substantial memory overhead\ncaused by their vast number of parameters, combined with the high computational\ndemands of the attention mechanism, poses significant challenges in achieving\nlow latency and high throughput for LLM inference services. Recent\nadvancements, driven by groundbreaking research, have significantly accelerated\nprogress in this field. This paper provides a comprehensive survey of these\nmethods, covering fundamental instance-level approaches, in-depth cluster-level\nstrategies, emerging scenario directions, and other miscellaneous but important\nareas. At the instance level, we review model placement, request scheduling,\ndecoding length prediction, storage management, and the disaggregation\nparadigm. At the cluster level, we explore GPU cluster deployment,\nmulti-instance load balancing, and cloud service solutions. For emerging\nscenarios, we organize the discussion around specific tasks, modules, and\nauxiliary methods. To ensure a holistic overview, we also highlight several\nniche yet critical areas. Finally, we outline potential research directions to\nfurther advance the field of LLM inference serving.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.19720.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "647e96507f9ad5e44bac50bd",
      "avatarUrl": "/avatars/2551f99401540676cfe5cd0ed99e70c1.svg",
      "fullname": "Ranran Zhen",
      "name": "zenRRan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.18904",
      "authors": [
        {
          "_id": "6812d36790b45f422b0bfdc2",
          "name": "Haoran Geng",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdc3",
          "name": "Feishi Wang",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdc4",
          "name": "Songlin Wei",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdc5",
          "name": "Yuyang Li",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdc6",
          "name": "Bangjun Wang",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdc7",
          "name": "Boshi An",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdc8",
          "name": "Charlie Tianyue Cheng",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdc9",
          "name": "Haozhe Lou",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdca",
          "name": "Peihao Li",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdcb",
          "name": "Yen-Jen Wang",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdcc",
          "name": "Yutong Liang",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdcd",
          "name": "Dylan Goetting",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdce",
          "name": "Chaoyi Xu",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdcf",
          "name": "Haozhe Chen",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdd0",
          "name": "Yuxi Qian",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdd1",
          "name": "Yiran Geng",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdd2",
          "name": "Jiageng Mao",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdd3",
          "name": "Weikang Wan",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdd4",
          "name": "Mingtong Zhang",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdd5",
          "name": "Jiangran Lyu",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdd6",
          "name": "Siheng Zhao",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdd7",
          "name": "Jiazhao Zhang",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdd8",
          "name": "Jialiang Zhang",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdd9",
          "name": "Chengyang Zhao",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdda",
          "name": "Haoran Lu",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfddb",
          "name": "Yufei Ding",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfddc",
          "name": "Ran Gong",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfddd",
          "name": "Yuran Wang",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdde",
          "name": "Yuxuan Kuang",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfddf",
          "name": "Ruihai Wu",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfde0",
          "name": "Baoxiong Jia",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfde1",
          "name": "Carlo Sferrazza",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfde2",
          "name": "Hao Dong",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfde3",
          "name": "Siyuan Huang",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfde4",
          "name": "Yue Wang",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfde5",
          "name": "Jitendra Malik",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfde6",
          "name": "Pieter Abbeel",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-26T12:31:04.000Z",
      "submittedOnDailyAt": "2025-05-01T00:21:25.704Z",
      "title": "Robobaas : Informations sur une plateforme intégrée pour l'apprentissage de robots, ensembles de données et marques de référence de tests, conçue pour l'extension et la généralisation.",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Les échelons de données et les cadres d'évaluation standardisés ont stimulé un grand développement dans le domaine du traitement du langage naturel et de la vision par ordinateur. Cependant, la robotique présente des problèmes spécifiques en ce qui concerne l'échelle des données et la construction de protocoles d'évaluation. La collecte de données dans le monde réel n'est pas efficace ni simple. De plus, les cadres d'évaluation dans des scénarios réels sont extrêmement complexes. Les données synthétiques et les simulations peuvent être des alternatives prometteuses, mais présentent des limitations en termes de qualité, diversité des données et standardisation des cadres d'évaluation. Pour résoudre ces problèmes, nous présentons RoboVerse, un cadre intégré qui combine une plateforme de simulation, un ensemble de données synthétiques et des cadres d'évaluation unitaires. La plateforme de simulation soutient plusieurs simulateurs et la configuration de robots, conçue de manière cyclique pour ignorer les différences entre environnements. L'ensemble de données synthétiques est caractérisé par une précision physique et une réalisation réaliste, construits à travers différents approches. De plus, nous proposons des cadres d'évaluation unitaires pour l'apprentissage par exemple et l'apprentissage par renforcement, permettant des évaluations à différents niveaux de généralisation. Le cœur de la plateforme de simulation est MetaSim, qui abstrait divers environnements de simulation à travers une interface générale, reconstruit des environnements de simulation existants comme un système de configuration sans relation avec le simulateur et fournit des APIs pour standardiser les fonctions des simulateurs. Cette abstraition garantit l'interchangeabilité et l'extensibilité. Des expériences détaillées montrent que RoboVerse améliore le rendement en apprentissage par exemple, apprentissage par renforcement, apprentissage de modèles de monde et la conversion de simulation en réalité. Ces résultats démontrent la fiabilité des ensembles de données et des cadres d'évaluation, établissant que RoboVerse peut être une solution puissante pour le développement de l'apprentissage de robots.",
      "upvotes": 6,
      "discussionId": "6812d36c90b45f422b0bff56",
      "ai_keywords": [
        "simulation platform",
        "synthetic dataset",
        "unified benchmarks",
        "MetaSim",
        "simulator-agnostic configuration system",
        "API",
        "physics engine",
        "imitation learning",
        "reinforcement learning",
        "world model learning",
        "sim-to-real transfer"
      ]
    },
    "publishedAt": "2025-04-26T08:31:04.000Z",
    "title": "RoboVerse: Towards a Unified Platform, Dataset and Benchmark for\n  Scalable and Generalizable Robot Learning",
    "summary": "Data scaling and standardized evaluation benchmarks have driven significant\nadvances in natural language processing and computer vision. However, robotics\nfaces unique challenges in scaling data and establishing evaluation protocols.\nCollecting real-world data is resource-intensive and inefficient, while\nbenchmarking in real-world scenarios remains highly complex. Synthetic data and\nsimulation offer promising alternatives, yet existing efforts often fall short\nin data quality, diversity, and benchmark standardization. To address these\nchallenges, we introduce RoboVerse, a comprehensive framework comprising a\nsimulation platform, a synthetic dataset, and unified benchmarks. Our\nsimulation platform supports multiple simulators and robotic embodiments,\nenabling seamless transitions between different environments. The synthetic\ndataset, featuring high-fidelity physics and photorealistic rendering, is\nconstructed through multiple approaches. Additionally, we propose unified\nbenchmarks for imitation learning and reinforcement learning, enabling\nevaluation across different levels of generalization. At the core of the\nsimulation platform is MetaSim, an infrastructure that abstracts diverse\nsimulation environments into a universal interface. It restructures existing\nsimulation environments into a simulator-agnostic configuration system, as well\nas an API aligning different simulator functionalities, such as launching\nsimulation environments, loading assets with initial states, stepping the\nphysics engine, etc. This abstraction ensures interoperability and\nextensibility. Comprehensive experiments demonstrate that RoboVerse enhances\nthe performance of imitation learning, reinforcement learning, world model\nlearning, and sim-to-real transfer. These results validate the reliability of\nour dataset and benchmarks, establishing RoboVerse as a robust solution for\nadvancing robot learning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.18904.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6753
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.21850",
      "authors": [
        {
          "_id": "6812d4d6ce88881cb51b3b70",
          "name": "Xindi Wu",
          "hidden": false
        },
        {
          "_id": "6812d4d6ce88881cb51b3b71",
          "name": "Hee Seung Hwang",
          "hidden": false
        },
        {
          "_id": "6812d4d6ce88881cb51b3b72",
          "name": "Polina Kirichenko",
          "hidden": false
        },
        {
          "_id": "6812d4d6ce88881cb51b3b73",
          "name": "Olga Russakovsky",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-30T17:57:22.000Z",
      "submittedOnDailyAt": "2025-05-01T00:32:13.807Z",
      "title": "COMPACT : Commence de l'atome pour ajuster les capacités de visualisation complexes.",
      "submittedOnDailyBy": {
        "_id": "613940c0905b1938233881e3",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/613940c0905b1938233881e3/Vb4kFWKEq6AILUP9KmCZA.png",
        "isPro": false,
        "fullname": "Xindi Wu",
        "user": "xindiw",
        "type": "user"
      },
      "summary": "Les modèles de langage multimodal grands (MLLMs) montrent un excellent rendement dans des tâches visuo-linguistiques simples, mais ils se sont avéré incapables dans des tâches plus complexes, où il manque la capacité à reconnaître des objets, à déterminer leur taille et à comprendre leurs relations spatiales. Ce phénomène est attribué à ce que l'Apprentissage par Instructions Visuelles (AIV) est une étape importante dans l'entraînement des MLLMs, mais qui se concentre uniquement sur l'expansion de la quantité de données sans considérer la complexité structurale des exemples d'entraînement. Nous proposons COMPACT (Ajuste de la capacité visuelle complexe à partir de la composition élémentaire), un méthode qui génère des ensembles de données d'entraînement avec une complexité structurale contrôlée explicitement. Les données de COMPACT permettent aux MLLMs d'apprendre la combinaison de compétences élémentaires et de développer efficacement des compétences complexes. Dans tous les benchmarks, COMPACT atteint des rendements comparables à LLaVA-665k VIT, en utilisant moins de 10% des données. En particulier, dans des tâches multifonctionnelles complexes, COMPACT dépasse considérablement ses concurrents. Par exemple, dans MMStar et MM-Vet, surtout dans des problèmes complexes nécessitant la combinaison de 4 ou plus compétences élémentaires, COMPACT atteint un accroissement de 83,3% ou un accroissement de 94,0% en utilisant moins de 10% des données de LLaVA-665k VIT. COMPACT fournit une recette pour l'amélioration des tâches visuo-linguistiques complexes, offrant un approche scalable et efficace en données pour ajuster la complexité structurale.",
      "upvotes": 5,
      "discussionId": "6812d4d7ce88881cb51b3bad",
      "projectPage": "https://princetonvisualai.github.io/compact/",
      "githubRepo": "https://github.com/princetonvisualai/compact",
      "ai_keywords": [
        "Multimodal Large Language Models (MLLMs)",
        "Visual Instruction Tuning (VIT)",
        "compositional complexity",
        "COMPACT (COMPositional Atomic-to-complex visual Capability Tuning)",
        "atomic capabilities",
        "complex capabilities",
        "MMStar",
        "MM-Vet",
        "visual compositional tuning"
      ]
    },
    "publishedAt": "2025-04-30T13:57:22.000Z",
    "title": "COMPACT: COMPositional Atomic-to-Complex Visual Capability Tuning",
    "summary": "Multimodal Large Language Models (MLLMs) excel at simple vision-language\ntasks but struggle when faced with complex tasks that require multiple\ncapabilities, such as simultaneously recognizing objects, counting them, and\nunderstanding their spatial relationships. This might be partially the result\nof the fact that Visual Instruction Tuning (VIT), a critical training step for\nMLLMs, has traditionally focused on scaling data volume, but not the\ncompositional complexity of training examples. We propose COMPACT\n(COMPositional Atomic-to-complex visual Capability Tuning), which generates a\ntraining dataset explicitly controlling for the compositional complexity of the\ntraining examples. The data from COMPACT allows MLLMs to train on combinations\nof atomic capabilities to learn complex capabilities more efficiently. Across\nall benchmarks, COMPACT achieves comparable performance to the LLaVA-665k VIT\nwhile using less than 10% of its data budget, and even outperforms it on\nseveral, especially those involving complex multi-capability tasks. For\nexample, COMPACT achieves substantial 83.3% improvement on MMStar and 94.0%\nimprovement on MM-Vet compared to the full-scale VIT on particularly complex\nquestions that require four or more atomic capabilities. COMPACT offers a\nscalable, data-efficient, visual compositional tuning recipe to improve on\ncomplex visual-language tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.21850.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "613940c0905b1938233881e3",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/613940c0905b1938233881e3/Vb4kFWKEq6AILUP9KmCZA.png",
      "fullname": "Xindi Wu",
      "name": "xindiw",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.21855",
      "authors": [
        {
          "_id": "6812db1567abbb1d11fb4e9d",
          "name": "Qihao Liu",
          "hidden": false
        },
        {
          "_id": "6812db1567abbb1d11fb4e9e",
          "name": "Ju He",
          "hidden": false
        },
        {
          "_id": "6812db1567abbb1d11fb4e9f",
          "name": "Qihang Yu",
          "hidden": false
        },
        {
          "_id": "6812db1567abbb1d11fb4ea0",
          "name": "Liang-Chieh Chen",
          "hidden": false
        },
        {
          "_id": "6812db1567abbb1d11fb4ea1",
          "name": "Alan Yuille",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/639f1e519f1f2baab2f00d22/h3IWiXbIAOy1NxTWKDAkO.mp4"
      ],
      "publishedAt": "2025-04-30T17:59:56.000Z",
      "submittedOnDailyAt": "2025-05-01T00:56:11.135Z",
      "title": "Revue : Génération de vidéos de haute qualité et à faible coût en utilisant un modèle physique 3D clair pour des actions complexes et des interactions.",
      "submittedOnDailyBy": {
        "_id": "639f1e519f1f2baab2f00d22",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/639f1e519f1f2baab2f00d22/pFjd51WZuVZ3A11rItvmk.jpeg",
        "isPro": true,
        "fullname": "Qihao Liu",
        "user": "QHL067",
        "type": "user"
      },
      "summary": "Récemment, on a observé un progrès clair dans le domaine de la génération d'images, bien que des problèmes significatifs persistent dans la génération d'actions complexes et d'interactions. Pour aborder ces défis, on présente ReVision, un cadre de plugin et libre. ReVision intègre explicitement des connaissances physiques 3D paramétrisées dans des modèles de génération d'images conditionnées, améliorant considérablement la capacité à générer des images de haute qualité qui comprennent des actions complexes et des interactions.\n\nSpécifiquement, ReVision est composé de trois étapes. Tout d'abord, des images approximatives sont générées à l'aide d'un modèle de diffusion d'images. Ensuite, des caractéristiques 2D et 3D de ces images sont extraites pour construire une représentation centrée sur les objets 3D. Cette représentation est refinée en utilisant notre modèle physique paramétrique proposé, permettant la génération de séquences de mouvements 3D précises. Enfin, ces séquences de mouvements précises sont renvoyées au même modèle de diffusion d'images, assurant la cohérence des actions à travers des conditions supplémentaires, et garantissant la cohérence des actions même dans des cas impliquant des actions complexes et des interactions. L'effet de notre approche a été démontré par des expériences sur Stable Video Diffusion, améliorant considérablement la fidélité et la cohérence des actions. En particulier, il a montré un effet notablement plus grand que le meilleur modèle de génération d'images avec 1500 millions de paramètres, et a permis de générer des images complexes avec seulement 150 millions de paramètres. Nos résultats montrent que l'intégration de connaissances physiques 3D permet aux modèles de diffusion d'images relativement petits la capacité à générer des actions complexes et des interactions. Cela est considéré comme une solution prometteuse pour le problème de génération d'images physiquement possibles.",
      "upvotes": 3,
      "discussionId": "6812db1767abbb1d11fb4f42",
      "projectPage": "https://revision-video.github.io/",
      "ai_keywords": [
        "video diffusion model",
        "3D object-centric representation",
        "parameterized physical prior model",
        "motion-consistent videos",
        "3D physical knowledge",
        "motion fidelity",
        "coherence",
        "physically plausible video generation"
      ]
    },
    "publishedAt": "2025-04-30T13:59:56.000Z",
    "title": "ReVision: High-Quality, Low-Cost Video Generation with Explicit 3D\n  Physics Modeling for Complex Motion and Interaction",
    "summary": "In recent years, video generation has seen significant advancements. However,\nchallenges still persist in generating complex motions and interactions. To\naddress these challenges, we introduce ReVision, a plug-and-play framework that\nexplicitly integrates parameterized 3D physical knowledge into a pretrained\nconditional video generation model, significantly enhancing its ability to\ngenerate high-quality videos with complex motion and interactions.\nSpecifically, ReVision consists of three stages. First, a video diffusion model\nis used to generate a coarse video. Next, we extract a set of 2D and 3D\nfeatures from the coarse video to construct a 3D object-centric representation,\nwhich is then refined by our proposed parameterized physical prior model to\nproduce an accurate 3D motion sequence. Finally, this refined motion sequence\nis fed back into the same video diffusion model as additional conditioning,\nenabling the generation of motion-consistent videos, even in scenarios\ninvolving complex actions and interactions. We validate the effectiveness of\nour approach on Stable Video Diffusion, where ReVision significantly improves\nmotion fidelity and coherence. Remarkably, with only 1.5B parameters, it even\noutperforms a state-of-the-art video generation model with over 13B parameters\non complex video generation by a substantial margin. Our results suggest that,\nby incorporating 3D physical knowledge, even a relatively small video diffusion\nmodel can generate complex motions and interactions with greater realism and\ncontrollability, offering a promising solution for physically plausible video\ngeneration.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/639f1e519f1f2baab2f00d22/h3IWiXbIAOy1NxTWKDAkO.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.21855.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "639f1e519f1f2baab2f00d22",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/639f1e519f1f2baab2f00d22/pFjd51WZuVZ3A11rItvmk.jpeg",
      "fullname": "Qihao Liu",
      "name": "QHL067",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.21039",
      "authors": [
        {
          "_id": "6812f89338bee548818e68a1",
          "user": {
            "_id": "6573a9fe769f3ee9bdf4d9c7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/xC41F7Vp9SVzVHc3cUiRU.jpeg",
            "isPro": false,
            "fullname": "Paul Kassianik",
            "user": "paulkass",
            "type": "user"
          },
          "name": "Paul Kassianik",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-01T04:29:53.088Z",
          "hidden": false
        },
        {
          "_id": "6812f89338bee548818e68a2",
          "name": "Baturay Saglam",
          "hidden": false
        },
        {
          "_id": "6812f89338bee548818e68a3",
          "name": "Alexander Chen",
          "hidden": false
        },
        {
          "_id": "6812f89338bee548818e68a4",
          "name": "Blaine Nelson",
          "hidden": false
        },
        {
          "_id": "6812f89338bee548818e68a5",
          "name": "Anu Vellore",
          "hidden": false
        },
        {
          "_id": "6812f89338bee548818e68a6",
          "name": "Massimo Aufiero",
          "hidden": false
        },
        {
          "_id": "6812f89338bee548818e68a7",
          "name": "Fraser Burch",
          "hidden": false
        },
        {
          "_id": "6812f89338bee548818e68a8",
          "name": "Dhruv Kedia",
          "hidden": false
        },
        {
          "_id": "6812f89338bee548818e68a9",
          "name": "Avi Zohary",
          "hidden": false
        },
        {
          "_id": "6812f89338bee548818e68aa",
          "name": "Sajana Weerawardhena",
          "hidden": false
        },
        {
          "_id": "6812f89338bee548818e68ab",
          "name": "Aman Priyanshu",
          "hidden": false
        },
        {
          "_id": "6812f89338bee548818e68ac",
          "name": "Adam Swanda",
          "hidden": false
        },
        {
          "_id": "6812f89338bee548818e68ad",
          "name": "Amy Chang",
          "hidden": false
        },
        {
          "_id": "6812f89338bee548818e68ae",
          "name": "Hyrum Anderson",
          "hidden": false
        },
        {
          "_id": "6812f89338bee548818e68af",
          "name": "Kojin Oshiba",
          "hidden": false
        },
        {
          "_id": "6812f89338bee548818e68b0",
          "name": "Omar Santos",
          "hidden": false
        },
        {
          "_id": "6812f89338bee548818e68b1",
          "name": "Yaron Singer",
          "hidden": false
        },
        {
          "_id": "6812f89338bee548818e68b2",
          "name": "Amin Karbasi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-28T08:41:12.000Z",
      "submittedOnDailyAt": "2025-05-01T03:07:21.083Z",
      "title": "**Informe Technique**\n\n**Llama-3.1-FoundationAI-SecurityLLM-Base-8B**\n\n**Résumé**",
      "submittedOnDailyBy": {
        "_id": "620042b28c2eb991da50d34e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/620042b28c2eb991da50d34e/Q5cj1GIq3XnKj-K64Mtyd.jpeg",
        "isPro": true,
        "fullname": "Aman Priyanshu",
        "user": "AmanPriyanshu",
        "type": "user"
      },
      "summary": "Les modèles de langage grands basés sur le Transformer (LLMs) sont en expansion progressive dans la société et ont un impact innovant dans diverses domaines tels que le développement de logiciels, l'activité créative des écrivains et l'art numérique. Cependant, leur introduction dans le domaine de la sécurité informatique est limitée par des problèmes comme la manque de données d'entraînement spécialisées et la complexité d'exprimer le savoir propre de la sécurité informatique. Pour résoudre ces défauts, nous présentons Foundation-Sec-8B, un modèle de langage basé sur l'architecture Llama 3.1 avec un accent sur la sécurité informatique. Ce modèle a été renforcé grâce à des entraînements continus sur un corpus de sécurité informatique sélectionné avec rigueur. Foundation-Sec-8B a été évalué sur de nouveaux et de benchmarks existants de sécurité informatique et a démontré des performances comparables à celles de Llama 3.1-70B et GPT-4o-mini pour des tâches spécifiques de sécurité informatique. La publication du modèle encourage le développement et l'introduction d'outils dirigés par l'intelligence artificielle et accélère l'introduction de ces outils dans des contextes publics et personnels de sécurité informatique.",
      "upvotes": 3,
      "discussionId": "6812f89338bee548818e68e3",
      "ai_keywords": [
        "transformer-based large language models (LLMs)",
        "cybersecurity-focused LLM",
        "Llama 3.1 architecture",
        "continued pretraining",
        "cybersecurity corpus",
        "cybersecurity benchmarks",
        "Llama 3.1-70B",
        "GPT-4o-mini",
        "cybersecurity-specific tasks"
      ]
    },
    "publishedAt": "2025-04-28T04:41:12.000Z",
    "title": "Llama-3.1-FoundationAI-SecurityLLM-Base-8B Technical Report",
    "summary": "As transformer-based large language models (LLMs) increasingly permeate\nsociety, they have revolutionized domains such as software engineering,\ncreative writing, and digital arts. However, their adoption in cybersecurity\nremains limited due to challenges like scarcity of specialized training data\nand complexity of representing cybersecurity-specific knowledge. To address\nthese gaps, we present Foundation-Sec-8B, a cybersecurity-focused LLM built on\nthe Llama 3.1 architecture and enhanced through continued pretraining on a\ncarefully curated cybersecurity corpus. We evaluate Foundation-Sec-8B across\nboth established and new cybersecurity benchmarks, showing that it matches\nLlama 3.1-70B and GPT-4o-mini in certain cybersecurity-specific tasks. By\nreleasing our model to the public, we aim to accelerate progress and adoption\nof AI-driven tools in both public and private cybersecurity contexts.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.21039.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "620042b28c2eb991da50d34e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/620042b28c2eb991da50d34e/Q5cj1GIq3XnKj-K64Mtyd.jpeg",
      "fullname": "Aman Priyanshu",
      "name": "AmanPriyanshu",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.21336",
      "authors": [
        {
          "_id": "68133878475d26db59993ae2",
          "name": "Linshan Wu",
          "hidden": false
        },
        {
          "_id": "68133878475d26db59993ae3",
          "name": "Yuxiang Nie",
          "hidden": false
        },
        {
          "_id": "68133878475d26db59993ae4",
          "name": "Sunan He",
          "hidden": false
        },
        {
          "_id": "68133878475d26db59993ae5",
          "name": "Jiaxin Zhuang",
          "hidden": false
        },
        {
          "_id": "68133878475d26db59993ae6",
          "name": "Hao Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-30T05:51:48.000Z",
      "submittedOnDailyAt": "2025-05-01T07:32:18.778Z",
      "title": "UniBiomed : Modèle de base WIRO-COMBINATORIAL, interprétation d'images biomédicales basée sur une théorie fondamentale.",
      "submittedOnDailyBy": {
        "_id": "65d86ea2685624d5f206d7ec",
        "avatarUrl": "/avatars/b9bc4c398d5def393bc782e9a7c5e302.svg",
        "isPro": false,
        "fullname": "Linshan Wu",
        "user": "Luffy503",
        "type": "user"
      },
      "summary": "L'interprétation d'images biomédicales avec des modèles de langage grand (MLLM) est une nouvelle opportunité dans l'analyse d'images biomédicales. L'approche traditionnelle de l'IA se base généralement sur des entraînements séparés. C'est-à-dire, des modèles de langage grand (LLMs) sont utilisés pour la génération de documents cliniques et des modèles d'extraction d'objets pour l'extraction d'objets. Cette approche échoue dans l'adaptation de l'exécution dans le monde réel et dans l'utilisation de l'information biomédicale en ensemble. Par conséquent, nous présentons UniBiomed, le premier modèle de base général et conçu pour l'interprétation de base d'images biomédicales. UniBiomed intègre efficacement la génération de documents cliniques et la segmentation d'objets relatifs à la médecine avec la combinaison d'un nouveau modèle de langage grand multimodal (MLLM) et du modèle de segmentation de n'importe quoi (SAM). De cette manière, UniBiomed peut résoudre une large gamme de tâches biomédicales avec 10 modèles d'images biomédicales différents. Pendant le développement de UniBiomed, des ensembles de données d'images, d'annotations et d'explications de documents à une échelle significative ont été sélectionnés, comprenant plus de 27 millions de paires d'images pour 10 modèles d'images. Par une validation étendue de 84 pages d'ensembles de données internes et externes, UniBiomed a atteint le meilleur rendement dans des tâches de segmentation, de reconnaissance de maladies, de diagnostic d'aires, de réponses aux problèmes de visualisation et de génération de rapports. De plus, les modèles précédents dépendaient de la prédiction des spécialistes cliniques et de processus manuels pour la génération de documents et de visualisations précises. Cependant, UniBiomed offre une interprétation automatique et complète, effectuant l'analyse d'images biomédicales de manière automatique. Cela représente une transition de paradigme dans le flux de travail clinique et signifie une grande amélioration de l'efficacité diagnostique. En résumé, UniBiomed représente une nouvelle innovation dans l'IA biomédicale et développe une capacité de base pour l'interprétation efficace et précise d'images biomédicales.",
      "upvotes": 2,
      "discussionId": "6813387e475d26db59993c9e",
      "ai_keywords": [
        "Multi-modal Large Language Model (MLLM)",
        "Segment Anything Model (SAM)",
        "grounded biomedical image interpretation",
        "biomedical tasks",
        "biomedical imaging modalities",
        "segmentation",
        "disease recognition",
        "region-aware diagnosis",
        "visual question answering",
        "report generation",
        "grounded interpretation"
      ]
    },
    "publishedAt": "2025-04-30T01:51:48.000Z",
    "title": "UniBiomed: A Universal Foundation Model for Grounded Biomedical Image\n  Interpretation",
    "summary": "Multi-modal interpretation of biomedical images opens up novel opportunities\nin biomedical image analysis. Conventional AI approaches typically rely on\ndisjointed training, i.e., Large Language Models (LLMs) for clinical text\ngeneration and segmentation models for target extraction, which results in\ninflexible real-world deployment and a failure to leverage holistic biomedical\ninformation. To this end, we introduce UniBiomed, the first universal\nfoundation model for grounded biomedical image interpretation. UniBiomed is\nbased on a novel integration of Multi-modal Large Language Model (MLLM) and\nSegment Anything Model (SAM), which effectively unifies the generation of\nclinical texts and the segmentation of corresponding biomedical objects for\ngrounded interpretation. In this way, UniBiomed is capable of tackling a wide\nrange of biomedical tasks across ten diverse biomedical imaging modalities. To\ndevelop UniBiomed, we curate a large-scale dataset comprising over 27 million\ntriplets of images, annotations, and text descriptions across ten imaging\nmodalities. Extensive validation on 84 internal and external datasets\ndemonstrated that UniBiomed achieves state-of-the-art performance in\nsegmentation, disease recognition, region-aware diagnosis, visual question\nanswering, and report generation. Moreover, unlike previous models that rely on\nclinical experts to pre-diagnose images and manually craft precise textual or\nvisual prompts, UniBiomed can provide automated and end-to-end grounded\ninterpretation for biomedical image analysis. This represents a novel paradigm\nshift in clinical workflows, which will significantly improve diagnostic\nefficiency. In summary, UniBiomed represents a novel breakthrough in biomedical\nAI, unlocking powerful grounded interpretation capabilities for more accurate\nand efficient biomedical image analysis.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.21336.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65d86ea2685624d5f206d7ec",
      "avatarUrl": "/avatars/b9bc4c398d5def393bc782e9a7c5e302.svg",
      "fullname": "Linshan Wu",
      "name": "Luffy503",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.20708",
      "authors": [
        {
          "_id": "6813376e2379095ee572f2f8",
          "name": "Hasan Abed Al Kader Hammoud",
          "hidden": false
        },
        {
          "_id": "6813376e2379095ee572f2f9",
          "name": "Hani Itani",
          "hidden": false
        },
        {
          "_id": "6813376e2379095ee572f2fa",
          "name": "Bernard Ghanem",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/642b51385bf2355d02a23d15/lojSGsqwNqzSQxNp4FHXA.png"
      ],
      "publishedAt": "2025-04-29T12:39:07.000Z",
      "submittedOnDailyAt": "2025-05-01T07:28:39.014Z",
      "title": "Enfin, en réponse à la question : tes racines théoriques révèlent plus de ce qu'on imaginait.",
      "submittedOnDailyBy": {
        "_id": "642b51385bf2355d02a23d15",
        "avatarUrl": "/avatars/87985347643b2647555f2453fa4d94fb.svg",
        "isPro": true,
        "fullname": "Hasan Abed Al Kader Hammoud",
        "user": "hammh0a",
        "type": "user"
      },
      "summary": "Les modèles de langage grand (LLMs) utilisent des théories de pas pour résoudre des problèmes complexes. La pratique d'évaluations standard génère une trace complète de théorie et évalue la précision du résultat final fourni. Dans cet article, nous doutons de la dépendance sur le résultat final et posons deux questions : représente-t-il le résultat final de manière fiable la conclusion optimale du modèle ? Peuvent-ils des chemins de théorie différents mener à des résultats différents ? Pour répondre à ces questions, nous proposons l'idée de \"sous-pensées\" pour les étapes intermédiaires de théorie et un méthode basée sur celle-ci. Notre approche se base sur des parenthèses linguistiques pour diviser les traces de théorie séquentiellement et fournir au modèle le suivant dans chaque sous-pensée. Dans chaque sous-pensée, nous extrayons une réponse potentielle du suivant completé. En sélectionnant ces réponses et en choisissant la plus fréquente (mode), on peut obtenir une précision considérablement plus élevée que celle qu'on pourrait obtenir d'une trace complète. En analysant la cohérence des réponses obtenues à partir de différentes sous-pensées, on peut révéler des caractéristiques liées à la confiance et à la précision du modèle, et présenter la possibilité d'identifier des réponses à faible confiance. Dans des expériences larges dans le domaine des LLMs et des ensembles de données de théorie mathématique défisants (AIME2024 et AIME2025), on a observé un amélioration de la précision correspondante, avec un effet de 13% et 10% respectivement. L'implémentation est disponible sur le lien suivant : https://github.com/hammoudhasan/SubthoughtReasoner.",
      "upvotes": 2,
      "discussionId": "6813376f2379095ee572f34c",
      "projectPage": "https://hammoudhasan.github.io/SubthoughtReasoner/",
      "githubRepo": "https://github.com/hammoudhasan/SubthoughtReasoner",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "step-by-step reasoning",
        "reasoning trace",
        "subthoughts",
        "linguistic cues",
        "continuations",
        "aggregating answers",
        "mode",
        "model's confidence",
        "correctness",
        "AIME2024",
        "AIME2025"
      ]
    },
    "publishedAt": "2025-04-29T08:39:07.000Z",
    "title": "Beyond the Last Answer: Your Reasoning Trace Uncovers More than You\n  Think",
    "summary": "Large Language Models (LLMs) leverage step-by-step reasoning to solve complex\nproblems. Standard evaluation practice involves generating a complete reasoning\ntrace and assessing the correctness of the final answer presented at its\nconclusion. In this paper, we challenge the reliance on the final answer by\nposing the following two questions: Does the final answer reliably represent\nthe model's optimal conclusion? Can alternative reasoning paths yield different\nresults? To answer these questions, we analyze intermediate reasoning steps,\ntermed subthoughts, and propose a method based on our findings. Our approach\ninvolves segmenting a reasoning trace into sequential subthoughts based on\nlinguistic cues. We start by prompting the model to generate continuations from\nthe end-point of each intermediate subthought. We extract a potential answer\nfrom every completed continuation originating from different subthoughts. We\nfind that aggregating these answers by selecting the most frequent one (the\nmode) often yields significantly higher accuracy compared to relying solely on\nthe answer derived from the original complete trace. Analyzing the consistency\namong the answers derived from different subthoughts reveals characteristics\nthat correlate with the model's confidence and correctness, suggesting\npotential for identifying less reliable answers. Our experiments across various\nLLMs and challenging mathematical reasoning datasets (AIME2024 and AIME2025)\nshow consistent accuracy improvements, with gains reaching up to 13\\% and 10\\%\nrespectively. Implementation is available at:\nhttps://github.com/hammoudhasan/SubthoughtReasoner.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/642b51385bf2355d02a23d15/lojSGsqwNqzSQxNp4FHXA.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.20708.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642b51385bf2355d02a23d15",
      "avatarUrl": "/avatars/87985347643b2647555f2453fa4d94fb.svg",
      "fullname": "Hasan Abed Al Kader Hammoud",
      "name": "hammh0a",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  }
]