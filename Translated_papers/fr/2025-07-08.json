[
  {
    "paper": {
      "id": "2507.03724",
      "authors": [
        {
          "_id": "686c7266364e2ad167eb5319",
          "name": "Zhiyu Li",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb531a",
          "name": "Shichao Song",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb531b",
          "name": "Chenyang Xi",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb531c",
          "name": "Hanyu Wang",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb531d",
          "name": "Chen Tang",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb531e",
          "name": "Simin Niu",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb531f",
          "name": "Ding Chen",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb5320",
          "name": "Jiawei Yang",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb5321",
          "name": "Chunyu Li",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb5322",
          "name": "Qingchen Yu",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb5323",
          "name": "Jihao Zhao",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb5324",
          "name": "Yezhaohui Wang",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb5325",
          "name": "Peng Liu",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb5326",
          "name": "Zehao Lin",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb5327",
          "name": "Pengyuan Wang",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb5328",
          "name": "Jiahao Huo",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb5329",
          "name": "Tianyi Chen",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb532a",
          "name": "Kai Chen",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb532b",
          "name": "Kehang Li",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb532c",
          "name": "Zhen Tao",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb532d",
          "name": "Junpeng Ren",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb532e",
          "name": "Huayi Lai",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb532f",
          "name": "Hao Wu",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb5330",
          "name": "Bo Tang",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb5331",
          "name": "Zhenren Wang",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb5332",
          "name": "Zhaoxin Fan",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb5333",
          "name": "Ningyu Zhang",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb5334",
          "name": "Linfeng Zhang",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb5335",
          "name": "Junchi Yan",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb5336",
          "name": "Mingchuan Yang",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb5337",
          "name": "Tong Xu",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb5338",
          "name": "Wei Xu",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb5339",
          "name": "Huajun Chen",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb533a",
          "name": "Haofeng Wang",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb533b",
          "name": "Hongkang Yang",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb533c",
          "user": {
            "_id": "686c965f418acea658859af4",
            "avatarUrl": "/avatars/4f453abeb7e82a3042cbec751b5cdb63.svg",
            "isPro": false,
            "fullname": "Wentao Zhang",
            "user": "Wentao-PKU",
            "type": "user"
          },
          "name": "Wentao Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-08T08:06:32.391Z",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb533d",
          "name": "Zhi-Qin John Xu",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb533e",
          "name": "Siheng Chen",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb533f",
          "name": "Feiyu Xiong",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/669e0b93c7cb0568dac6e92e/twRhDA4ThpQMfInSFdYDA.gif",
        "https://cdn-uploads.huggingface.co/production/uploads/669e0b93c7cb0568dac6e92e/9xHrSG0a8X2CYHxvfMFSH.gif",
        "https://cdn-uploads.huggingface.co/production/uploads/669e0b93c7cb0568dac6e92e/aaUeE4u2RhCnFYNKCqvyn.gif"
      ],
      "publishedAt": "2025-07-04T17:21:46.000Z",
      "submittedOnDailyAt": "2025-07-08T01:46:43.501Z",
      "title": "MemOS : Système de Mémoire Optique pour Systèmes d'Intelligence Artificielle",
      "submittedOnDailyBy": {
        "_id": "669e0b93c7cb0568dac6e92e",
        "avatarUrl": "/avatars/a39ea77d7391f164af8a80f94f85f2ca.svg",
        "isPro": false,
        "fullname": "hanyu Wang",
        "user": "UglyToilet",
        "type": "user"
      },
      "summary": "Les modèles de langage à grande échelle (LLMs) ont établi eux-mêmes comme une infrastructure importante pour l'intelligence artificielle (IAG), mais leur manque de gestion de la mémoire a un impact négatif sur la logique de long contexte, la personnalisation continue et le développement de la cohérence de la connaissance. Actuellement, les modèles dépendent de paramètres fixes et d'états de contexte temporels courts, limitant ainsi leur capacité à suivre les préférences de l'utilisateur ou à mettre à jour leurs connaissances à long terme. La RAG (Recherche d'Information avec Texte) ajoute des connaissances externes sous forme de texte, mais c'est une solution légère sans état et manque de contrôle sur la vie cycle de l'information ou l'intégration de représentations durables. Des études récentes ont montré que modéliser le coût d'entraînement et d'inférence des LLMs à partir de la perspective de la couche de mémoire, et d'ajouter une couche claire de mémoire entre les paramètres et les données externes, peut réduire significativement ce coût. En plus de l'efficacité computationnelle, les LLMs peuvent aborder une large gamme de problèmes qui surgissent en fonction du temps et du contexte. Pour résoudre ces problèmes, nous proposons MemOS (Système d'Opération de Mémoire). MemOS traite la mémoire comme un ressource de système, intégrant la représentation de la mémoire en texte, basée sur l'activation et au niveau des paramètres, ainsi que la programmation et l'évolution pour faciliter l'accès et la recherche de manière coûte-efficace. En termes de base, MemCube est un ensemble de données qui regroupe le contenu de la mémoire, les données originales et l'information de version. MemCube peut être assemblé, déplacé et fusionné au fil du temps, permettant une transition flexible des types de mémoire et la combinaison de l'apprentissage basé sur les paramètres avec la recherche. MemOS construit un cadre de système centré sur la mémoire, offrant aux LLMs la possibilité de contrôle, flexibilité et évolution, et se concentre sur la construction d'une base pour l'apprentissage continu et la modélisation de la personnalisation.",
      "upvotes": 65,
      "discussionId": "686c7266364e2ad167eb5340",
      "projectPage": "https://memos.openmem.net/",
      "githubRepo": "https://github.com/MemTensor/MemOS",
      "ai_summary": "MemOS is proposed as a memory operating system for Large Language Models to enhance memory management, enabling efficient storage and retrieval, and facilitating continual learning and personalized modeling.",
      "ai_keywords": [
        "LLMs",
        "Artificial General Intelligence",
        "AGI",
        "Retrieval-Augmented Generation",
        "RAG",
        "MemOS",
        "memory operating system",
        "MemCube",
        "activation-based memory"
      ],
      "githubStars": 527
    },
    "publishedAt": "2025-07-04T13:21:46.000Z",
    "title": "MemOS: A Memory OS for AI System",
    "summary": "Large Language Models (LLMs) have become an essential infrastructure for\nArtificial General Intelligence (AGI), yet their lack of well-defined memory\nmanagement systems hinders the development of long-context reasoning, continual\npersonalization, and knowledge consistency.Existing models mainly rely on\nstatic parameters and short-lived contextual states, limiting their ability to\ntrack user preferences or update knowledge over extended periods.While\nRetrieval-Augmented Generation (RAG) introduces external knowledge in plain\ntext, it remains a stateless workaround without lifecycle control or\nintegration with persistent representations.Recent work has modeled the\ntraining and inference cost of LLMs from a memory hierarchy perspective,\nshowing that introducing an explicit memory layer between parameter memory and\nexternal retrieval can substantially reduce these costs by externalizing\nspecific knowledge. Beyond computational efficiency, LLMs face broader\nchallenges arising from how information is distributed over time and context,\nrequiring systems capable of managing heterogeneous knowledge spanning\ndifferent temporal scales and sources. To address this challenge, we propose\nMemOS, a memory operating system that treats memory as a manageable system\nresource. It unifies the representation, scheduling, and evolution of\nplaintext, activation-based, and parameter-level memories, enabling\ncost-efficient storage and retrieval. As the basic unit, a MemCube encapsulates\nboth memory content and metadata such as provenance and versioning. MemCubes\ncan be composed, migrated, and fused over time, enabling flexible transitions\nbetween memory types and bridging retrieval with parameter-based learning.\nMemOS establishes a memory-centric system framework that brings\ncontrollability, plasticity, and evolvability to LLMs, laying the foundation\nfor continual learning and personalized modeling.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/669e0b93c7cb0568dac6e92e/twRhDA4ThpQMfInSFdYDA.gif",
      "https://cdn-uploads.huggingface.co/production/uploads/669e0b93c7cb0568dac6e92e/9xHrSG0a8X2CYHxvfMFSH.gif",
      "https://cdn-uploads.huggingface.co/production/uploads/669e0b93c7cb0568dac6e92e/aaUeE4u2RhCnFYNKCqvyn.gif"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.03724.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "669e0b93c7cb0568dac6e92e",
      "avatarUrl": "/avatars/a39ea77d7391f164af8a80f94f85f2ca.svg",
      "fullname": "hanyu Wang",
      "name": "UglyToilet",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.05163",
      "authors": [
        {
          "_id": "686c928b364e2ad167eb53f1",
          "name": "Yutian Chen",
          "hidden": false
        },
        {
          "_id": "686c928b364e2ad167eb53f2",
          "name": "Shi Guo",
          "hidden": false
        },
        {
          "_id": "686c928b364e2ad167eb53f3",
          "name": "Tianshuo Yang",
          "hidden": false
        },
        {
          "_id": "686c928b364e2ad167eb53f4",
          "name": "Lihe Ding",
          "hidden": false
        },
        {
          "_id": "686c928b364e2ad167eb53f5",
          "name": "Xiuyuan Yu",
          "hidden": false
        },
        {
          "_id": "686c928b364e2ad167eb53f6",
          "name": "Jinwei Gu",
          "hidden": false
        },
        {
          "_id": "686c928b364e2ad167eb53f7",
          "name": "Tianfan Xue",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-07T16:18:35.000Z",
      "submittedOnDailyAt": "2025-07-08T02:14:44.087Z",
      "title": "4DSloMo : Capture asynchrone de scènes rapides en configuration 4D",
      "submittedOnDailyBy": {
        "_id": "64970d3d9c3b29dca8633f87",
        "avatarUrl": "/avatars/11e3c9c66d28490d6d09925f9aa47cd1.svg",
        "isPro": false,
        "fullname": "JunhaoZhuang",
        "user": "JunhaoZhuang",
        "type": "user"
      },
      "summary": "La reconstruction 4D à haute vitesse est cruciale pour l'analyse de mouvements rapides et la reconstruction 4D en temps réel. Cependant, de nombreux systèmes de capture 4D sont limités à 30 FPS (frames par seconde) ou moins, ce qui peut conduire à des résultats non désirables lorsque l'on essaye de reconstruire des mouvements rapides directement à partir d'entrées à bas FPS. Dans cet article, nous proposons un système de capture 4D à haute vitesse en utilisant uniquement des caméras à bas FPS grâce à de nouveaux modules de capture et de traitement. Au niveau de la capture, nous proposons une stratégie de capture asynchrone pour augmenter la fréquence des frames effectivement, en alignant le début de la caméra avec le temps de l'échantillonnage, ce qui permet d'étendre la base de 25 FPS à une vitesse équivalente de 100-200 FPS. Au niveau du traitement, nous proposons un nouveau modèle génératif pour corriger les artefacts causés par la reconstruction visuelle spatiale 4D, et nous entraînons un modèle de correction d'artefact basé sur la division du vidéo pour réparer les défauts, maintenir la cohérence temporelle et améliorer la qualité de la reconstruction en totalité. Les résultats expérimentaux montrent que cette méthode améliore significativement la reconstruction 4D à haute vitesse par rapport à la capture synchrone.",
      "upvotes": 29,
      "discussionId": "686c928b364e2ad167eb53f8",
      "ai_summary": "A high-speed 4D capturing system using low FPS cameras with asynchronous capture and video-diffusion-based artifact correction enhances reconstruction quality.",
      "ai_keywords": [
        "asynchronous capture",
        "video-diffusion-based artifact-fix model",
        "sparse 4D reconstruction",
        "temporal consistency"
      ]
    },
    "publishedAt": "2025-07-07T12:18:35.000Z",
    "title": "4DSloMo: 4D Reconstruction for High Speed Scene with Asynchronous\n  Capture",
    "summary": "Reconstructing fast-dynamic scenes from multi-view videos is crucial for\nhigh-speed motion analysis and realistic 4D reconstruction. However, the\nmajority of 4D capture systems are limited to frame rates below 30 FPS (frames\nper second), and a direct 4D reconstruction of high-speed motion from low FPS\ninput may lead to undesirable results. In this work, we propose a high-speed 4D\ncapturing system only using low FPS cameras, through novel capturing and\nprocessing modules. On the capturing side, we propose an asynchronous capture\nscheme that increases the effective frame rate by staggering the start times of\ncameras. By grouping cameras and leveraging a base frame rate of 25 FPS, our\nmethod achieves an equivalent frame rate of 100-200 FPS without requiring\nspecialized high-speed cameras. On processing side, we also propose a novel\ngenerative model to fix artifacts caused by 4D sparse-view reconstruction, as\nasynchrony reduces the number of viewpoints at each timestamp. Specifically, we\npropose to train a video-diffusion-based artifact-fix model for sparse 4D\nreconstruction, which refines missing details, maintains temporal consistency,\nand improves overall reconstruction quality. Experimental results demonstrate\nthat our method significantly enhances high-speed 4D reconstruction compared to\nsynchronous capture.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.05163.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64970d3d9c3b29dca8633f87",
      "avatarUrl": "/avatars/11e3c9c66d28490d6d09925f9aa47cd1.svg",
      "fullname": "JunhaoZhuang",
      "name": "JunhaoZhuang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 33
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.04447",
      "authors": [
        {
          "_id": "686cab67364e2ad167eb5464",
          "name": "Wenyao Zhang",
          "hidden": false
        },
        {
          "_id": "686cab67364e2ad167eb5465",
          "name": "Hongsi Liu",
          "hidden": false
        },
        {
          "_id": "686cab67364e2ad167eb5466",
          "user": {
            "_id": "63c3e8abc7d7f4c63a515a02",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63c3e8abc7d7f4c63a515a02/npMHnVP2hHLbvoUGe7C4O.jpeg",
            "isPro": false,
            "fullname": "Zekun Qi",
            "user": "qizekun",
            "type": "user"
          },
          "name": "Zekun Qi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-08T08:05:57.749Z",
          "hidden": false
        },
        {
          "_id": "686cab67364e2ad167eb5467",
          "name": "Yunnan Wang",
          "hidden": false
        },
        {
          "_id": "686cab67364e2ad167eb5468",
          "name": "XinQiang Yu",
          "hidden": false
        },
        {
          "_id": "686cab67364e2ad167eb5469",
          "name": "Jiazhao Zhang",
          "hidden": false
        },
        {
          "_id": "686cab67364e2ad167eb546a",
          "user": {
            "_id": "6201fc5d91d53938a6432fbf",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6201fc5d91d53938a6432fbf/VLs8ZYaZrop4KBpZn53fH.jpeg",
            "isPro": false,
            "fullname": "Runpei Dong",
            "user": "RunpeiDong",
            "type": "user"
          },
          "name": "Runpei Dong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-08T08:06:00.401Z",
          "hidden": false
        },
        {
          "_id": "686cab67364e2ad167eb546b",
          "name": "Jiawei He",
          "hidden": false
        },
        {
          "_id": "686cab67364e2ad167eb546c",
          "name": "He Wang",
          "hidden": false
        },
        {
          "_id": "686cab67364e2ad167eb546d",
          "name": "Zhizheng Zhang",
          "hidden": false
        },
        {
          "_id": "686cab67364e2ad167eb546e",
          "name": "Li Yi",
          "hidden": false
        },
        {
          "_id": "686cab67364e2ad167eb546f",
          "name": "Wenjun Zeng",
          "hidden": false
        },
        {
          "_id": "686cab67364e2ad167eb5470",
          "name": "Xin Jin",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6201fc5d91d53938a6432fbf/Oe8hIH4_I_Pql9N72iHbn.mp4"
      ],
      "publishedAt": "2025-07-06T16:14:29.000Z",
      "submittedOnDailyAt": "2025-07-08T03:55:36.991Z",
      "title": "DREAM VLA : Modèle de Vision, Langue et Action qui comprend largement les connaissances du monde",
      "submittedOnDailyBy": {
        "_id": "6201fc5d91d53938a6432fbf",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6201fc5d91d53938a6432fbf/VLs8ZYaZrop4KBpZn53fH.jpeg",
        "isPro": false,
        "fullname": "Runpei Dong",
        "user": "RunpeiDong",
        "type": "user"
      },
      "summary": "Le développement récent des modèles de langue visuelle et d'action (VLA) a démontré des résultats remarquables dans la génération d'images et la prédiction d'actions, en généralisant et en inférent la manipulation de machines. Cependant, les méthodes actuelles sont limitées par la nécessité de prédire une information complexe et large dans les images, et manquent de connaissances structurelles qui incluent des informations dynamiques, spatiales et significatives. Pour résoudre ces limitations, nous proposons un nouveau cadre de VLA qui intègre la prédiction de connaissances concrètes du monde. En particulier, DreamVLA fournit une représentation compressée et détaillée dans la planification d'actions, en combinant des prédictions de connaissances dynamiques, spatiales et significatives. Ce design est aligné avec la façon dont l'humanité interagit avec le monde, formant une séquence continue de raisons abstraites de multiples types pour effectuer des actions. Pendant l'entraînement, nous utilisons une structure de blocs d'attention pour faciliter l'intégration indirecte d'informations dynamiques, spatiales et significatives, évitant la perte d'information et séparant clairement chaque représentation. De plus, nous modélisons la distribution de conditions pour des actions futures en utilisant un transformateur basé sur la diffusion, séparant la représentation des actions de caractéristiques potentielles communes. Les expériences de diffusion dans des environnements réels et de simulation montrent que DreamVLA atteint un succès de 76,7% dans des tâches de robotisation et un moyenne de longueur de 4,44 sur le benchmark CALVIN ABC-D.",
      "upvotes": 26,
      "discussionId": "686cab67364e2ad167eb5471",
      "projectPage": "https://zhangwenyao1.github.io/DreamVLA/",
      "githubRepo": "https://github.com/Zhangwenyao1/DreamVLA",
      "ai_summary": "DreamVLA improves robot manipulation through a VLA framework that incorporates world knowledge, dynamic-region guidance, and a diffusion-based transformer to ensure clear, disentangled representations for action planning.",
      "ai_keywords": [
        "vision-language-action",
        "dynamic-region-guided",
        "world knowledge prediction",
        "spatial and semantic cues",
        "block-wise structured attention",
        "diffusion-based transformer"
      ],
      "githubStars": 24
    },
    "publishedAt": "2025-07-06T12:14:29.000Z",
    "title": "DreamVLA: A Vision-Language-Action Model Dreamed with Comprehensive\n  World Knowledge",
    "summary": "Recent advances in vision-language-action (VLA) models have shown promise in\nintegrating image generation with action prediction to improve generalization\nand reasoning in robot manipulation. However, existing methods are limited to\nchallenging image-based forecasting, which suffers from redundant information\nand lacks comprehensive and critical world knowledge, including dynamic,\nspatial and semantic information. To address these limitations, we propose\nDreamVLA, a novel VLA framework that integrates comprehensive world knowledge\nforecasting to enable inverse dynamics modeling, thereby establishing a\nperception-prediction-action loop for manipulation tasks. Specifically,\nDreamVLA introduces a dynamic-region-guided world knowledge prediction,\nintegrated with the spatial and semantic cues, which provide compact yet\ncomprehensive representations for action planning. This design aligns with how\nhumans interact with the world by first forming abstract multimodal reasoning\nchains before acting. To mitigate interference among the dynamic, spatial and\nsemantic information during training, we adopt a block-wise structured\nattention mechanism that masks their mutual attention, preventing information\nleakage and keeping each representation clean and disentangled. Moreover, to\nmodel the conditional distribution over future actions, we employ a\ndiffusion-based transformer that disentangles action representations from\nshared latent features. Extensive experiments on both real-world and simulation\nenvironments demonstrate that DreamVLA achieves 76.7% success rate on real\nrobot tasks and 4.44 average length on the CALVIN ABC-D benchmarks.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6201fc5d91d53938a6432fbf/Oe8hIH4_I_Pql9N72iHbn.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.04447.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6201fc5d91d53938a6432fbf",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6201fc5d91d53938a6432fbf/VLs8ZYaZrop4KBpZn53fH.jpeg",
      "fullname": "Runpei Dong",
      "name": "RunpeiDong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.05197",
      "authors": [
        {
          "_id": "686c7f78364e2ad167eb5354",
          "name": "Shihan Dou",
          "hidden": false
        },
        {
          "_id": "686c7f78364e2ad167eb5355",
          "name": "Shichun Liu",
          "hidden": false
        },
        {
          "_id": "686c7f78364e2ad167eb5356",
          "user": {
            "_id": "655c6b1abfb531437a54c0e6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/V8Md2mMX83hrSowKk6qMS.jpeg",
            "isPro": false,
            "fullname": "Yuming Yang",
            "user": "Umean",
            "type": "user"
          },
          "name": "Yuming Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-08T08:06:28.979Z",
          "hidden": false
        },
        {
          "_id": "686c7f78364e2ad167eb5357",
          "name": "Yicheng Zou",
          "hidden": false
        },
        {
          "_id": "686c7f78364e2ad167eb5358",
          "name": "Yunhua Zhou",
          "hidden": false
        },
        {
          "_id": "686c7f78364e2ad167eb5359",
          "name": "Shuhao Xing",
          "hidden": false
        },
        {
          "_id": "686c7f78364e2ad167eb535a",
          "name": "Chenhao Huang",
          "hidden": false
        },
        {
          "_id": "686c7f78364e2ad167eb535b",
          "name": "Qiming Ge",
          "hidden": false
        },
        {
          "_id": "686c7f78364e2ad167eb535c",
          "name": "Demin Song",
          "hidden": false
        },
        {
          "_id": "686c7f78364e2ad167eb535d",
          "name": "Haijun Lv",
          "hidden": false
        },
        {
          "_id": "686c7f78364e2ad167eb535e",
          "name": "Songyang Gao",
          "hidden": false
        },
        {
          "_id": "686c7f78364e2ad167eb535f",
          "name": "Chengqi Lv",
          "hidden": false
        },
        {
          "_id": "686c7f78364e2ad167eb5360",
          "name": "Enyu Zhou",
          "hidden": false
        },
        {
          "_id": "686c7f78364e2ad167eb5361",
          "name": "Honglin Guo",
          "hidden": false
        },
        {
          "_id": "686c7f78364e2ad167eb5362",
          "name": "Zhiheng Xi",
          "hidden": false
        },
        {
          "_id": "686c7f78364e2ad167eb5363",
          "name": "Wenwei Zhang",
          "hidden": false
        },
        {
          "_id": "686c7f78364e2ad167eb5364",
          "name": "Qipeng Guo",
          "hidden": false
        },
        {
          "_id": "686c7f78364e2ad167eb5365",
          "name": "Qi Zhang",
          "hidden": false
        },
        {
          "_id": "686c7f78364e2ad167eb5366",
          "name": "Xipeng Qiu",
          "hidden": false
        },
        {
          "_id": "686c7f78364e2ad167eb5367",
          "name": "Xuanjing Huang",
          "hidden": false
        },
        {
          "_id": "686c7f78364e2ad167eb5368",
          "name": "Tao Gui",
          "hidden": false
        },
        {
          "_id": "686c7f78364e2ad167eb5369",
          "name": "Kai Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-07T16:56:31.000Z",
      "submittedOnDailyAt": "2025-07-08T02:18:45.573Z",
      "title": "L'identification des politiques est un modèle de récompenses générales.",
      "submittedOnDailyBy": {
        "_id": "6234238485575ce6ff1f169a",
        "avatarUrl": "/avatars/e5fff05f21cdea4e5aebc8ba426fac29.svg",
        "isPro": false,
        "fullname": "Yicheng Zou",
        "user": "RowitZou",
        "type": "user"
      },
      "summary": "Nous présentons la modélisation des récompenses comme un formalisateur de politiques pour offrir une nouvelle perspective. Ce méthode mesure les différences entre deux politiques et génère des signaux de récompense pour guider la politique d'apprentissage vers un objectif d'actions souhaitées. En se basant sur ces observations conceptuelles, nous proposons le Méthode d'Entraînement Escalable, l'Apprentissage Discriminatif de Politiques (POLAR). Ce méthode entraîne un modèle de récompense (RM) pour distinguer entre une politique et une autre différente. Au contraire des méthodes traditionnelles de modélisation des récompenses, POLAR identifie les différences relatives entre une politique et n'importe quelle politique cible, permettant un modélisation d'objectifs d'optimisation scalable et de haut niveau. En utilisant le modèle de pré-entraînement de POLAR, nous fournissons des modèles de RM à des échelles de paramètres allant de 1,8B à 7B. Les résultats empiriques montrent une amélioration significative du rendement par rapport aux méthodes traditionnelles non pré-entraînées. Par exemple, dans le défi STEM, la précision de préférence a augmenté de 54,8% à 81,0%, et dans le défi de la créativité de l'écriture, de 57,9% à 85,5%. POLAR montre une capacité de généralisation robuste dans le fine-tuning d'apprentissage par rétroalimentation humaine (RLHF). Cela fournit des signaux de récompenses fiables, ce qui signifie un grand amélioration de l'efficacité de la politique. Par exemple, LLaMa3.1-8B a été amélioré de 47,36% à 56,33%, et Qwen2.5-32B de 64,49% à 70,47%. De plus, les expériences d'extension montrent une relation exponentielle claire entre le calcul et le rendement. Les résultats surprenants, la forte généralisation et les caractéristiques d'extension indiquent que POLAR est une direction prometteuse pour le développement de modèles de récompenses générales.",
      "upvotes": 24,
      "discussionId": "686c7f78364e2ad167eb536a",
      "githubRepo": "https://github.com/InternLM/POLAR",
      "ai_summary": "A scalable reward modeling method, Policy Discriminative Learning (POLAR), enhances reward model performance and generalizes robustly in reinforcement learning through policy comparison.",
      "ai_keywords": [
        "policy discriminator",
        "reward model",
        "Policy Discriminative Learning",
        "POLAR",
        "Reinforcement Fine-tuning"
      ],
      "githubStars": 35
    },
    "publishedAt": "2025-07-07T12:56:31.000Z",
    "title": "Pre-Trained Policy Discriminators are General Reward Models",
    "summary": "We offer a novel perspective on reward modeling by formulating it as a policy\ndiscriminator, which quantifies the difference between two policies to generate\na reward signal, guiding the training policy towards a target policy with\ndesired behaviors. Based on this conceptual insight, we propose a scalable\npre-training method named Policy Discriminative Learning (POLAR), which trains\na reward model (RM) to discern identical policies and discriminate different\nones. Unlike traditional reward modeling methods relying on absolute\npreferences, POLAR captures the relative difference between one policy and an\narbitrary target policy, which is a scalable, high-level optimization objective\nsuitable for modeling generic ranking relationships. Leveraging the POLAR\npre-training paradigm, we present a series of RMs with parameter scales from\n1.8B to 7B. Empirical results show that POLAR substantially outperforms\ntraditional non-pre-trained methods, significantly enhancing RM performance.\nFor instance, POLAR-7B could improve preference accuracy from 54.8% to 81.0% on\nSTEM tasks and from 57.9% to 85.5% on creative writing tasks compared to SOTA\nbaselines. POLAR also shows robust generalization capabilities in RLHF using\nReinforcement Fine-tuning (RFT), providing reliable reward signals and markedly\nenhancing policy performance--improving LLaMa3.1-8B from an average of 47.36%\nto 56.33% and Qwen2.5-32B from 64.49% to 70.47% on 20 benchmarks. Moreover,\nscaling experiments reveal a clear power-law relationship between computation\nand performance, supported by linear correlation coefficients approaching 0.99.\nThe impressive performance, strong generalization, and scaling properties\nsuggest that POLAR is a promising direction for developing general and strong\nreward models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.05197.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6234238485575ce6ff1f169a",
      "avatarUrl": "/avatars/e5fff05f21cdea4e5aebc8ba426fac29.svg",
      "fullname": "Yicheng Zou",
      "name": "RowitZou",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.00994",
      "authors": [
        {
          "_id": "6864e267d59a9eda59024bab",
          "user": {
            "_id": "65fa95405355a52c784633fc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65fa95405355a52c784633fc/rSfBUHPa7eSAsLd8DuOq4.png",
            "isPro": false,
            "fullname": "Hippolyte Gisserot-Boukhlef",
            "user": "hgissbkh",
            "type": "user"
          },
          "name": "Hippolyte Gisserot-Boukhlef",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-05T07:53:51.109Z",
          "hidden": false
        },
        {
          "_id": "6864e267d59a9eda59024bac",
          "user": {
            "_id": "62be186a5f59ff2320e6e32b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62be186a5f59ff2320e6e32b/W_emoC2uItM-MJZyCfIKI.png",
            "isPro": false,
            "fullname": "Nicolas-BZRD",
            "user": "Nicolas-BZRD",
            "type": "user"
          },
          "name": "Nicolas Boizard",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-02T12:22:12.240Z",
          "hidden": false
        },
        {
          "_id": "6864e267d59a9eda59024bad",
          "name": "Manuel Faysse",
          "hidden": false
        },
        {
          "_id": "6864e267d59a9eda59024bae",
          "name": "Duarte M. Alves",
          "hidden": false
        },
        {
          "_id": "6864e267d59a9eda59024baf",
          "name": "Emmanuel Malherbe",
          "hidden": false
        },
        {
          "_id": "6864e267d59a9eda59024bb0",
          "name": "André F. T. Martins",
          "hidden": false
        },
        {
          "_id": "6864e267d59a9eda59024bb1",
          "name": "Céline Hudelot",
          "hidden": false
        },
        {
          "_id": "6864e267d59a9eda59024bb2",
          "name": "Pierre Colombo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-01T17:45:48.000Z",
      "submittedOnDailyAt": "2025-07-08T07:51:12.578Z",
      "title": "Est-ce que l'encodeur du modèle de langue de masque est entraîné directement ?",
      "submittedOnDailyBy": {
        "_id": "62be186a5f59ff2320e6e32b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62be186a5f59ff2320e6e32b/W_emoC2uItM-MJZyCfIKI.png",
        "isPro": false,
        "fullname": "Nicolas-BZRD",
        "user": "Nicolas-BZRD",
        "type": "user"
      },
      "summary": "La représentation de texte de haute qualité a une importance fondamentale dans une variété étendue de tâches de NLP. L'apprentissage préalable d'un encodeur basé sur un modèle de langue cachée (MLM) a été utilisé traditionnellement, mais des preuves récentes ont montré que un modèle décodé préalablement entraîné par un modèle de langue causal (CLM) peut être réutilisé comme un encodeur et peut dépasser les performances des encodeurs traditionnels dans les tests de représentation de texte. Cependant, il n'a pas encore été clairement démontré pourquoi ces résultats sont obtenus, soit parce que les avantages inhérents du CLM, ou parce que d'autres facteurs comme le taille du modèle ou la quantité de données. Dans cet article, ces problèmes ont été abordés par des expériences d'apprentissage préalable à grande échelle, en entraînant 30 modèles dans un intervalle allant de 2,1 million à 10 milliards de paramètres et en effectuant 15 000 ou plus de fine-tuning et d'évaluations. Il a été observé que l'entraînement d'un modèle avec MLM généralement montre un meilleur rendement dans les tâches de représentation de texte en général, tandis que les modèles entraînés avec CLM sont plus efficaces avec les données et améliorent la stabilité du fine-tuning. En se basant sur ces observations, une stratégie d'entraînement en deux étapes a été présentée pour atteindre le meilleur rendement, en utilisant un calcul fixe d'entraînement. De plus, cette stratégie démontre que l'on peut initialiser un modèle décodé préalablement entraîné dans un écosystème de LLMs et réduire la charge d'entraînement des modèles plus récents d'encodeurs, fonctionnant de manière plus efficace. Tous les artefacts du projet ont été publiés pour encourager la recherche.",
      "upvotes": 23,
      "discussionId": "6864e267d59a9eda59024bb3",
      "githubRepo": "https://github.com/Nicolas-BZRD/EuroBERT",
      "githubStars": 59
    },
    "publishedAt": "2025-07-01T13:45:48.000Z",
    "title": "Should We Still Pretrain Encoders with Masked Language Modeling?",
    "summary": "Learning high-quality text representations is fundamental to a wide range of\nNLP tasks. While encoder pretraining has traditionally relied on Masked\nLanguage Modeling (MLM), recent evidence suggests that decoder models\npretrained with Causal Language Modeling (CLM) can be effectively repurposed as\nencoders, often surpassing traditional encoders on text representation\nbenchmarks. However, it remains unclear whether these gains reflect an inherent\nadvantage of the CLM objective or arise from confounding factors such as model\nand data scale. In this paper, we address this question through a series of\nlarge-scale, carefully controlled pretraining ablations, training a total of 30\nmodels ranging from 210 million to 1 billion parameters, and conducting over\n15,000 fine-tuning and evaluation runs. We find that while training with MLM\ngenerally yields better performance across text representation tasks,\nCLM-trained models are more data-efficient and demonstrate improved fine-tuning\nstability. Building on these findings, we experimentally show that a biphasic\ntraining strategy that sequentially applies CLM and then MLM, achieves optimal\nperformance under a fixed computational training budget. Moreover, we\ndemonstrate that this strategy becomes more appealing when initializing from\nreadily available pretrained CLM models (from the existing LLM ecosystem),\nreducing the computational burden needed to train best-in-class encoder models.\nWe release all project artifacts at https://hf.co/MLMvsCLM to foster further\nresearch.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.00994.png",
    "numComments": 5,
    "submittedBy": {
      "_id": "62be186a5f59ff2320e6e32b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62be186a5f59ff2320e6e32b/W_emoC2uItM-MJZyCfIKI.png",
      "fullname": "Nicolas-BZRD",
      "name": "Nicolas-BZRD",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 29
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.03483",
      "authors": [
        {
          "_id": "686c90d4364e2ad167eb53d8",
          "name": "Zhiheng Xi",
          "hidden": false
        },
        {
          "_id": "686c90d4364e2ad167eb53d9",
          "name": "Guanyu Li",
          "hidden": false
        },
        {
          "_id": "686c90d4364e2ad167eb53da",
          "name": "Yutao Fan",
          "hidden": false
        },
        {
          "_id": "686c90d4364e2ad167eb53db",
          "name": "Honglin Guo",
          "hidden": false
        },
        {
          "_id": "686c90d4364e2ad167eb53dc",
          "name": "Yufang Liu",
          "hidden": false
        },
        {
          "_id": "686c90d4364e2ad167eb53dd",
          "name": "Xiaoran Fan",
          "hidden": false
        },
        {
          "_id": "686c90d4364e2ad167eb53de",
          "name": "Jiaqi Liu",
          "hidden": false
        },
        {
          "_id": "686c90d4364e2ad167eb53df",
          "name": "Jingchao Ding",
          "hidden": false
        },
        {
          "_id": "686c90d4364e2ad167eb53e0",
          "name": "Wangmeng Zuo",
          "hidden": false
        },
        {
          "_id": "686c90d4364e2ad167eb53e1",
          "name": "Zhenfei Yin",
          "hidden": false
        },
        {
          "_id": "686c90d4364e2ad167eb53e2",
          "name": "Lei Bai",
          "hidden": false
        },
        {
          "_id": "686c90d4364e2ad167eb53e3",
          "name": "Tao Ji",
          "hidden": false
        },
        {
          "_id": "686c90d4364e2ad167eb53e4",
          "name": "Tao Gui",
          "hidden": false
        },
        {
          "_id": "686c90d4364e2ad167eb53e5",
          "name": "Qi Zhang",
          "hidden": false
        },
        {
          "_id": "686c90d4364e2ad167eb53e6",
          "name": "Xuanjing Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-04T11:20:09.000Z",
      "submittedOnDailyAt": "2025-07-08T02:00:41.875Z",
      "title": "BMMR : Réasons de la grande scène de Broadway de Billy Joel et daski\nDataset",
      "submittedOnDailyBy": {
        "_id": "638ef0b0c67af472d31674a6",
        "avatarUrl": "/avatars/02df97d15a0f46b47f9162221733b121.svg",
        "isPro": false,
        "fullname": "Honglin Guo",
        "user": "KYLN24",
        "type": "user"
      },
      "summary": "Cet article présente BMMR, un ensemble de données d'une grande taille qui comprend des bibliométriques, des modèles multiples et la théorie de la multiculturalité. Cet ensemble de données peut être utilisé par la communauté pour le développement et l'évaluation de modèles multiples (LMMs) à grande échelle. BMMR comprend 110 000 questions d'un niveau universitaire, qui couvrent 300 thèmes définis par l'UNESCO, et est présenté dans différents formats (multiple choix, réponses à blanc, questions ouvertes), tirées de matériaux imprimés et numériques tels que des livres, des examens et des quizzes. Tout l'ensemble de données a été curé à l'aide d'un cadre scalable impliquant la participation humaine, et chaque instance est associée à une bonne raison. L'ensemble de données est divisé en deux sections : BMMR-Eval et BMMR-Train. BMMR-Eval comprend 20 458 instances de haute qualité et permet l'évaluation de la théorie de la multiculturalité et des raisons en chinois et anglais. BMMR-Train se concentre sur la raison actuelle en mathématiques et soutient la recherche et le développement dans différentes disciplines et interdisciplinaires. De plus, un instrument d'évaluation basé sur des processus pour l'expérience multiculturale, le BMMR-Verifier, est proposé, qui effectue une évaluation précise des étapes de raison. Les expériences à grande échelle sur 24 modèles révèlent que : (i) les modèles de pointe (par exemple, o3 et Gemini-2.5-Pro) ont beaucoup de potentiel d'amélioration sur BMMR-Eval ; (ii) les modèles de raison montrent des biais disciplinaires et dépassent les LMMs dans certains thèmes ; (iii) les modèles open-source dépassent les modèles propriétaires ; (iv) le fine-tuning sur BMMR-Train réduit ces erreurs. De plus, grâce à des études détaillées avec BMMR-Verifier, les problèmes actuels des LMMs dans la théorie de la multiculturalité ont été identifiés. L'ensemble de données est publié pour bénéficier à la communauté.",
      "upvotes": 19,
      "discussionId": "686c90d4364e2ad167eb53e7",
      "projectPage": "https://bmmr.pages.dev/",
      "githubRepo": "https://github.com/woooodyy/BMMR",
      "ai_summary": "A large-scale dataset and verification tool are introduced for assessing and improving cross-disciplinary reasoning capabilities in multimodal models.",
      "ai_keywords": [
        "bilingual",
        "multimodal",
        "multi-disciplinary",
        "large multimodal models",
        "LMMs",
        "UNESCO-defined subjects",
        "multiple-choice",
        "fill-in-the-blank",
        "open-ended QA",
        "human-in-the-loop",
        "scalable framework",
        "high-quality reasoning path",
        "multidisciplinary reasoning",
        "BMMR-Verifier",
        "reasoning models",
        "discipline bias",
        "reasoning-chain analysis"
      ],
      "githubStars": 5
    },
    "publishedAt": "2025-07-04T07:20:09.000Z",
    "title": "BMMR: A Large-Scale Bilingual Multimodal Multi-Discipline Reasoning\n  Dataset",
    "summary": "In this paper, we introduce BMMR, a large-scale bilingual, multimodal,\nmulti-disciplinary reasoning dataset for the community to develop and evaluate\nlarge multimodal models (LMMs). BMMR comprises 110k college-level questions\nspanning 300 UNESCO-defined subjects, spanning diverse formats-multiple-choice,\nfill-in-the-blank, and open-ended QA-and sourced from both print and digital\nmedia such as books, exams, and quizzes. All data are curated and filtered via\na human-in-the-loop and scalable framework, and each instance is paired with a\nhigh-quality reasoning path. The dataset is organized into two parts: BMMR-Eval\nthat comprises 20,458 high-quality instances to comprehensively assess LMMs'\nknowledge and reasoning across multiple disciplines in both Chinese and\nEnglish; and BMMR-Train that contains 88,991 instances to support further\nresearch and development, extending the current focus on mathematical reasoning\nto diverse disciplines and domains. In addition, we propose the process-based\nmulti-discipline verifier (i.e., BMMR-Verifier) for accurate and fine-grained\nevaluation of reasoning paths. Extensive experiments on 24 models reveal that\n(i) even SOTA models (e.g., o3 and Gemini-2.5-Pro) leave substantial headroom\non BMMR-Eval; (ii) reasoning models exhibit discipline bias and outperform LMMs\nonly on specific subjects; (iii) open-source models still trail their\nproprietary counterparts; and (iv) fine-tuning on BMMR-Train narrows this gap.\nAdditionally, we conduct reasoning-chain analyses using BMMR-Verifier and other\nin-depth studies, uncovering the challenges LMMs currently face in\nmultidisciplinary reasoning. We will release the data, and we hope our work can\noffer insights and contributions to the community.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.03483.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "638ef0b0c67af472d31674a6",
      "avatarUrl": "/avatars/02df97d15a0f46b47f9162221733b121.svg",
      "fullname": "Honglin Guo",
      "name": "KYLN24",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.02029",
      "authors": [
        {
          "_id": "68679569213f123a1f88b87c",
          "name": "BAAI RoboBrain Team",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b87d",
          "user": {
            "_id": "668fa476cbcaf7ab0e4c58b3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/668fa476cbcaf7ab0e4c58b3/F5Jj-nPCjU6uxZyfkY3qw.jpeg",
            "isPro": false,
            "fullname": "Mingyu Cao",
            "user": "cmyopu",
            "type": "user"
          },
          "name": "Mingyu Cao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-07T11:24:50.715Z",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b87e",
          "name": "Huajie Tan",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b87f",
          "user": {
            "_id": "668f5478b3991ac0c3fc9c2f",
            "avatarUrl": "/avatars/a775853d3b88e7b1c8494ca837b5495c.svg",
            "isPro": false,
            "fullname": "yuhengji",
            "user": "yuheng2000",
            "type": "user"
          },
          "name": "Yuheng Ji",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-05T07:53:03.017Z",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b880",
          "user": {
            "_id": "67e406e64b80e9b39e2a85d6",
            "avatarUrl": "/avatars/a021be64341e5d7a079858916fa34c28.svg",
            "isPro": false,
            "fullname": "MinglanLin",
            "user": "MinglanLin",
            "type": "user"
          },
          "name": "Minglan Lin",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-05T07:52:59.066Z",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b881",
          "name": "Zhiyu Li",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b882",
          "user": {
            "_id": "66aadfc1344279e0243d4569",
            "avatarUrl": "/avatars/d2afbceb2e6279e42ed9ad98dffa7f0a.svg",
            "isPro": false,
            "fullname": "Caozhou",
            "user": "Caozhou1995",
            "type": "user"
          },
          "name": "Zhou Cao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-08T09:05:48.313Z",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b883",
          "name": "Pengwei Wang",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b884",
          "user": {
            "_id": "63f08dc79cf89c9ed1bb89cd",
            "avatarUrl": "/avatars/37290358ad00bbd752f519cfdec02f3e.svg",
            "isPro": false,
            "fullname": "Zhoues",
            "user": "Zhoues",
            "type": "user"
          },
          "name": "Enshen Zhou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-05T07:52:57.069Z",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b885",
          "name": "Yi Han",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b886",
          "name": "Yingbo Tang",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b887",
          "name": "Xiangqi Xu",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b888",
          "name": "Wei Guo",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b889",
          "name": "Yaoxu Lyu",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b88a",
          "name": "Yijie Xu",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b88b",
          "name": "Jiayu Shi",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b88c",
          "user": {
            "_id": "650bf938677f9e45963d672e",
            "avatarUrl": "/avatars/7d4159067b5005a3a635e36b26b7b239.svg",
            "isPro": false,
            "fullname": "Cheng Chi",
            "user": "ChuckChi",
            "type": "user"
          },
          "name": "Cheng Chi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-05T07:53:01.035Z",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b88d",
          "name": "Mengdi Zhao",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b88e",
          "name": "Xiaoshuai Hao",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b88f",
          "name": "Shanyu Rong",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b890",
          "name": "Zhengliang Cai",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b891",
          "name": "Bolun Zhang",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b892",
          "name": "Shuyi Zhang",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b893",
          "name": "Huaihai Lyu",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b894",
          "name": "Mengfei Du",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b895",
          "name": "Lingfeng Zhang",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b896",
          "name": "Xi Feng",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b897",
          "name": "Xiaodan Liu",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b898",
          "name": "Yance Jiao",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b899",
          "name": "Chenrui He",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b89a",
          "user": {
            "_id": "63157214362e3e95ea553db5",
            "avatarUrl": "/avatars/a421c25128c71be6d0c92490cebbbccc.svg",
            "isPro": false,
            "fullname": "lyu",
            "user": "ceci3",
            "type": "user"
          },
          "name": "Mengsi Lyu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-08T09:05:50.528Z",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b89b",
          "name": "Zhuo Chen",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b89c",
          "name": "Yulong Ao",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b89d",
          "name": "Xue Sun",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b89e",
          "name": "Zheqi He",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b89f",
          "name": "Jingshu Zheng",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b8a0",
          "name": "Xi Yang",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b8a1",
          "name": "Donghai Shi",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b8a2",
          "name": "Kunchang Xie",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b8a3",
          "name": "Bochao Zhang",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b8a4",
          "name": "Shaokai Nie",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b8a5",
          "name": "Chunlei Men",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b8a6",
          "name": "Yonghua Lin",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b8a7",
          "name": "Zhongyuan Wang",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b8a8",
          "name": "Tiejun Huang",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b8a9",
          "name": "Shanghang Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-02T17:05:33.000Z",
      "submittedOnDailyAt": "2025-07-08T06:32:16.956Z",
      "title": "RoboBrain 2.0 Rapport Technique",
      "submittedOnDailyBy": {
        "_id": "63a369d98c0c89dcae3b8329",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a369d98c0c89dcae3b8329/AiH2zjy1cnt9OADAAZMLD.jpeg",
        "isPro": true,
        "fullname": "Adina Yakefu",
        "user": "AdinaY",
        "type": "user"
      },
      "summary": "RoboBrain 2.0, nous présentons notre dernière génération de langage de vision basé sur des modèles. Ce modèle intègre le reconnaissance, la logique et la planification de tâches complexes dans des environnements physiques. Nous préparons deux versions : le modèle 7B et le 32B. Il est caractérisé par une architecture structurée limitée, composée d'un encodeur de vision et d'un modèle de langage. Malgré son petit taille, RoboBrain 2.0 montre un excellent rendement sur une large gamme de tâches de vision logique. La version 32B a également atteint des résultats de pointe dans les cadres de référence spatiaux et temporels, dépassant à la fois les modèles ouverts et propriétaires. En particulier, elle soutient la compréhension spatiale (par exemple, prédiction de fonctions, consommation spatiale, prédiction de trajectoires) et la détermination temporelle (par exemple, interaction dans des cycles fermés, planification d'agents multiples et mise à jour de graphes de scènes). Dans ce rapport, nous détaillons l'architecture du modèle, la construction des données, la stratégie d'entraînement multi-étapes et les applications pratiques. Nous espérons que RoboBrain 2.0 encourage la recherche en IA de vision et soit un pas concret pour la construction d'agents visuels généraux. Les codes, points de contrôle et cadres de référence sont disponibles sur https://superrobobrain.github.io.",
      "upvotes": 14,
      "discussionId": "68679569213f123a1f88b8aa",
      "ai_summary": "RoboBrain 2.0, a vision-language foundation model, achieves top performance in embodied tasks through its heterogeneous architecture and multi-stage training strategies.",
      "ai_keywords": [
        "embodied vision-language",
        "embodied reason",
        "vision encoder",
        "spatial understanding",
        "affordance prediction",
        "spatial referring",
        "trajectory forecasting",
        "temporal decision-making",
        "closed-loop interaction",
        "multi-agent long-horizon planning",
        "scene graph updating"
      ]
    },
    "publishedAt": "2025-07-02T13:05:33.000Z",
    "title": "RoboBrain 2.0 Technical Report",
    "summary": "We introduce RoboBrain 2.0, our latest generation of embodied vision-language\nfoundation models, designed to unify perception, reasoning, and planning for\ncomplex embodied tasks in physical environments. It comes in two variants: a\nlightweight 7B model and a full-scale 32B model, featuring a heterogeneous\narchitecture with a vision encoder and a language model. Despite its compact\nsize, RoboBrain 2.0 achieves strong performance across a wide spectrum of\nembodied reasoning tasks. On both spatial and temporal benchmarks, the 32B\nvariant achieves leading results, surpassing prior open-source and proprietary\nmodels. In particular, it supports key real-world embodied AI capabilities,\nincluding spatial understanding (e.g., affordance prediction, spatial\nreferring, trajectory forecasting) and temporal decision-making (e.g.,\nclosed-loop interaction, multi-agent long-horizon planning, and scene graph\nupdating). This report details the model architecture, data construction,\nmulti-stage training strategies, infrastructure and practical applications. We\nhope RoboBrain 2.0 advances embodied AI research and serves as a practical step\ntoward building generalist embodied agents. The code, checkpoint and benchmark\nare available at https://superrobobrain.github.io.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.02029.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a369d98c0c89dcae3b8329",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a369d98c0c89dcae3b8329/AiH2zjy1cnt9OADAAZMLD.jpeg",
      "fullname": "Adina Yakefu",
      "name": "AdinaY",
      "type": "user",
      "isPro": true,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 857
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.04009",
      "authors": [
        {
          "_id": "686cd234cc230c60b4100aec",
          "name": "Ziyang Miao",
          "hidden": false
        },
        {
          "_id": "686cd234cc230c60b4100aed",
          "name": "Qiyu Sun",
          "hidden": false
        },
        {
          "_id": "686cd234cc230c60b4100aee",
          "name": "Jingyuan Wang",
          "hidden": false
        },
        {
          "_id": "686cd234cc230c60b4100aef",
          "user": {
            "_id": "66a48a77f9565635ebc33a87",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66a48a77f9565635ebc33a87/WW8BFd1D9xZbGIPZPfdHk.png",
            "isPro": false,
            "fullname": "GYC",
            "user": "oGYCo",
            "type": "user"
          },
          "name": "Yuchen Gong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-08T09:05:42.785Z",
          "hidden": false
        },
        {
          "_id": "686cd234cc230c60b4100af0",
          "user": {
            "_id": "642fef28a043f0ac7defa8a9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642fef28a043f0ac7defa8a9/RwOEkuj3fOnOA54tGR7Ea.png",
            "isPro": false,
            "fullname": "Yaowei Zheng",
            "user": "hiyouga",
            "type": "user"
          },
          "name": "Yaowei Zheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-08T08:39:35.178Z",
          "hidden": false
        },
        {
          "_id": "686cd234cc230c60b4100af1",
          "name": "Shiqi Li",
          "hidden": false
        },
        {
          "_id": "686cd234cc230c60b4100af2",
          "name": "Richong Zhang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/642fef28a043f0ac7defa8a9/Ztx5877vKyqqZINgge_1B.mp4"
      ],
      "publishedAt": "2025-07-05T11:38:59.000Z",
      "submittedOnDailyAt": "2025-07-08T06:40:18.914Z",
      "title": "Easy Dataset : Utilisant le cadre de travail des unités et un cadre de travail synthétique d'expansion, génération de données de fine-tuning de LLM à partir de documents non structurés",
      "submittedOnDailyBy": {
        "_id": "642fef28a043f0ac7defa8a9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642fef28a043f0ac7defa8a9/RwOEkuj3fOnOA54tGR7Ea.png",
        "isPro": false,
        "fullname": "Yaowei Zheng",
        "user": "hiyouga",
        "type": "user"
      },
      "summary": "Les modèles de langage grands (LLMs) montrent un comportement impressionnant dans des tâches générales, mais leur application dans des domaines spécifiques est limitée par la pénurie de données de haute qualité. Les outils de synthèse de données actuels rencontrent des difficultés pour extraire des données de fine-tuning fiables, ce qui a conduit à la proposition du cadre de travail Easy Dataset. Ce cadre permet la synthèse de données de fine-tuning à partir de documents non structurés à travers une interface utilisateur intuitive graphique (GUI). En particulier, Easy Dataset permet aux utilisateurs de configurer facilement des modèles d'extraction de texte et des étapes de déchiffrement, transformant des documents simples en chats de déchiffrement colonnés. Ensuite, des LLMs publiés sont utilisés et un approche de prompting puissant est appliquée pour générer des paires de questions et réponses variées. Tout au long de ce flux, la personne peut réviser et améliorer les résultats intermédiaires via une interface de visualisation au sein du cycle, garantissant la qualité des données. Les expériences dans des tâches de questions et réponses en finance montrent que l'ajustement des LLMs avec des données synthétiques améliore significativement le rendement spécifique tout en maintenant le connaissance générale. Le code source et les bibliothèques installables sont disponibles sur https://github.com/ConardLi/easy-dataset, et ce projet a atteint plus de 9 000 points de qualité sur GitHub.",
      "upvotes": 12,
      "discussionId": "686cd234cc230c60b4100af3",
      "githubRepo": "https://github.com/ConardLi/easy-dataset",
      "ai_summary": "A unified framework called Easy Dataset synthesizes fine-tuning data from unstructured documents using a GUI and LLMs, improving domain-specific performance of LLMs while maintaining general knowledge.",
      "ai_keywords": [
        "Large language models",
        "fine-tuning",
        "unstructured documents",
        "graphical user interface",
        "text extraction models",
        "chunking strategies",
        "persona-driven prompting",
        "human-in-the-loop",
        "financial question-answering task"
      ],
      "githubStars": 9180
    },
    "publishedAt": "2025-07-05T07:38:59.000Z",
    "title": "Easy Dataset: A Unified and Extensible Framework for Synthesizing LLM\n  Fine-Tuning Data from Unstructured Documents",
    "summary": "Large language models (LLMs) have shown impressive performance on\ngeneral-purpose tasks, yet adapting them to specific domains remains\nchallenging due to the scarcity of high-quality domain data. Existing data\nsynthesis tools often struggle to extract reliable fine-tuning data from\nheterogeneous documents effectively. To address this limitation, we propose\nEasy Dataset, a unified framework for synthesizing fine-tuning data from\nunstructured documents via an intuitive graphical user interface (GUI).\nSpecifically, Easy Dataset allows users to easily configure text extraction\nmodels and chunking strategies to transform raw documents into coherent text\nchunks. It then leverages a persona-driven prompting approach to generate\ndiverse question-answer pairs using public-available LLMs. Throughout the\npipeline, a human-in-the-loop visual interface facilitates the review and\nrefinement of intermediate outputs to ensure data quality. Experiments on a\nfinancial question-answering task show that fine-tuning LLMs on the synthesized\ndataset significantly improves domain-specific performance while preserving\ngeneral knowledge. The source code and installable package are available at\nhttps://github.com/ConardLi/easy-dataset and have garnered over 9,000 GitHub\nstars.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/642fef28a043f0ac7defa8a9/Ztx5877vKyqqZINgge_1B.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.04009.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642fef28a043f0ac7defa8a9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642fef28a043f0ac7defa8a9/RwOEkuj3fOnOA54tGR7Ea.png",
      "fullname": "Yaowei Zheng",
      "name": "hiyouga",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2185
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.03253",
      "authors": [
        {
          "_id": "686c86ff364e2ad167eb53a8",
          "name": "Baolong Bi",
          "hidden": false
        },
        {
          "_id": "686c86ff364e2ad167eb53a9",
          "name": "Shenghua Liu",
          "hidden": false
        },
        {
          "_id": "686c86ff364e2ad167eb53aa",
          "name": "Xingzhang Ren",
          "hidden": false
        },
        {
          "_id": "686c86ff364e2ad167eb53ab",
          "name": "Dayiheng Liu",
          "hidden": false
        },
        {
          "_id": "686c86ff364e2ad167eb53ac",
          "name": "Junyang Lin",
          "hidden": false
        },
        {
          "_id": "686c86ff364e2ad167eb53ad",
          "name": "Yiwei Wang",
          "hidden": false
        },
        {
          "_id": "686c86ff364e2ad167eb53ae",
          "user": {
            "_id": "63120517ae8896941da4c5da",
            "avatarUrl": "/avatars/10e1be026035f3e24225e6782a710083.svg",
            "isPro": false,
            "fullname": "Lingrui Mei",
            "user": "Chevalier",
            "type": "user"
          },
          "name": "Lingrui Mei",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-08T09:05:44.909Z",
          "hidden": false
        },
        {
          "_id": "686c86ff364e2ad167eb53af",
          "name": "Junfeng Fang",
          "hidden": false
        },
        {
          "_id": "686c86ff364e2ad167eb53b0",
          "name": "Jiafeng Guo",
          "hidden": false
        },
        {
          "_id": "686c86ff364e2ad167eb53b1",
          "name": "Xueqi Cheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-04T02:19:58.000Z",
      "submittedOnDailyAt": "2025-07-08T01:53:47.804Z",
      "title": "RefineX : Amélioration de l'entraînement de données préparées dans des programmes d'apprentissage par ordinateur experts selon l'échelle",
      "submittedOnDailyBy": {
        "_id": "642577e06d0f0f5f1dc68904",
        "avatarUrl": "/avatars/3df7cfcf9ca6cd9c9e9b10a73f4efc35.svg",
        "isPro": false,
        "fullname": "Bibaolong",
        "user": "Bibaolong",
        "type": "user"
      },
      "summary": "Les grands modèles de langue (LLMs) ont une capacité fondamentale profondément influencée par la qualité des données préalables. Cependant, améliorer la qualité des données par échelle est un grand défi, principalement en raison de l'équilibre entre la précision et l'efficacité du traitement. Le filtrage basé sur les règles est le paradigme principal, mais fonctionne généralement au niveau des documents et manque de la précision nécessaire pour améliorer spécifiquement les contenus. En se basant sur des recherches émergentes comme ProX, nous proposons un nouveau cadre appelé RefineX. RefineX améliore la précision de grandes quantités de données préalables en dehors des documents à travers des tâches d'édition de langage programmé. RefineX permet une précision efficace des données tout en maintenant la diversité et la nature du texte original avec confiance. L'un des points forts de RefineX est qu'il peut accepter des résultats de précision de haute qualité avec un minimum d'éditions. Ce flux de travail d'intégration de résultats de haute qualité permet de mieux systématiquement toutes les instances du corpus à travers l'échelle, en entraînant des modèles de précision efficaces et fiables. RefineX évalue les données préalables dès le début, montre un meilleur comportement consistent dans de nombreux modèles de taille et de tâches ultérieures, et enregistre un accroissement moyen de 2,6% à 7,2% dans la tâche lighteval sur un modèle de 750M, en réduisant significativement les tokens d'entraînement. Une analyse plus approfondie montre que RefineX améliore de manière fiable la qualité du texte, est plus efficace que les méthodes existantes, et dépasse Prox-C dans la génération final du texte, démontrant que RefineX est une solution échellable, efficace et fiable pour l'optimisation des données préalables lors de l'actualisation des LLMs.",
      "upvotes": 12,
      "discussionId": "686c86ff364e2ad167eb53b2",
      "ai_summary": "RefineX is a scalable framework for improving the quality of large language model pre-training data through programmatic editing, yielding better performance than alternative methods across various downstream tasks.",
      "ai_keywords": [
        "large language models",
        "pre-training corpora",
        "rule-based filtering",
        "document level",
        "granular refinement",
        "programmatic editing",
        "refine model",
        "data refinement",
        "text quality",
        "end-to-end generation",
        "lighteval tasks"
      ]
    },
    "publishedAt": "2025-07-03T22:19:58.000Z",
    "title": "RefineX: Learning to Refine Pre-training Data at Scale from\n  Expert-Guided Programs",
    "summary": "The foundational capabilities of large language models (LLMs) are deeply\ninfluenced by the quality of their pre-training corpora. However, enhancing\ndata quality at scale remains a significant challenge, primarily due to the\ntrade-off between refinement effectiveness and processing efficiency. While\nrule-based filtering remains the dominant paradigm, it typically operates at\nthe document level and lacks the granularity needed to refine specific content\nwithin documents. Inspired by emerging work such as ProX, we propose\nRefineX, a novel framework for large-scale, surgical refinement of\npre-training data through programmatic editing tasks. RefineX enables efficient\nand fine-grained data refinement while reliably preserving the diversity and\nnaturalness of raw text. The core strength of RefineX lies in distilling\nhigh-quality, expert-guided end-to-end refinement results into minimal\nedit-based deletion programs. This high-precision distillation pipeline is used\nto train an efficient and reliable refine model that can systematically improve\nevery instance in the corpus at scale. We evaluate RefineX across from-scratch\npre-training at multiple model scales and find that it consistently outperforms\nmodels trained on raw, filtered, or alternatively refined data across diverse\ndownstream tasks. On the 750M model, RefineX yields 2.6%-7.2% average gains on\nlighteval tasks, and achieves comparable performance using significantly fewer\ntraining tokens. Further analysis shows that RefineX reliably enhances text\nquality with both high efficiency and precision, outperforming prior approaches\nsuch as end-to-end generation and Prox-C. These results position RefineX as a\nscalable, effective, and reliable solution for optimizing pre-training data in\nmodern LLM pipelines.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.03253.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642577e06d0f0f5f1dc68904",
      "avatarUrl": "/avatars/3df7cfcf9ca6cd9c9e9b10a73f4efc35.svg",
      "fullname": "Bibaolong",
      "name": "Bibaolong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.05108",
      "authors": [
        {
          "_id": "686cc8bc364e2ad167eb54e3",
          "name": "Yuyi Zhang",
          "hidden": false
        },
        {
          "_id": "686cc8bc364e2ad167eb54e4",
          "name": "Peirong Zhang",
          "hidden": false
        },
        {
          "_id": "686cc8bc364e2ad167eb54e5",
          "name": "Zhenhua Yang",
          "hidden": false
        },
        {
          "_id": "686cc8bc364e2ad167eb54e6",
          "name": "Pengyu Yan",
          "hidden": false
        },
        {
          "_id": "686cc8bc364e2ad167eb54e7",
          "name": "Yongxin Shi",
          "hidden": false
        },
        {
          "_id": "686cc8bc364e2ad167eb54e8",
          "name": "Pengwei Liu",
          "hidden": false
        },
        {
          "_id": "686cc8bc364e2ad167eb54e9",
          "name": "Fengjun Guo",
          "hidden": false
        },
        {
          "_id": "686cc8bc364e2ad167eb54ea",
          "name": "Lianwen Jin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-07T15:26:17.000Z",
      "submittedOnDailyAt": "2025-07-08T06:01:26.616Z",
      "title": "Cultura Patrimonio : Nouvelles Méthodologies de Représentation Critique de Documents Historiques",
      "submittedOnDailyBy": {
        "_id": "65fba5700b78c48c9e393a3e",
        "avatarUrl": "/avatars/795cc8167460d8e89ff91d27c5da9fb2.svg",
        "isPro": false,
        "fullname": "Yuyi Zhang",
        "user": "ZZXF",
        "type": "user"
      },
      "summary": "Les documents historiques sont des patrimoines culturels précieux qui sont confrontés à des dégradations graves tels que les dommages, la corrosion, l'oxydation et l'érosion en raison du passage du temps. La technique actuelle de reconstruction (HDR) se concentre principalement sur des modèles uniques ou de taille limitée, ce qui ne satisfait pas les besoins pratiques. Pour résoudre ce problème, on propose FPHDR (Ensemble de Données HDR de Page Complète) et AutoHDR (Solution Automatique HDR). FPHDR comprend 1,633 images en image réelle et 6,543 images synthétiques, ainsi que des informations sur la position du niveau de lettre et de ligne, et des descriptions du degré du dommage. AutoHDR modélise le flux de travail des historiens et utilise un approche en trois étapes : la détection des positions endommagées par OCR, la prédiction du texte dans le contexte visuel et la reconstruction automatique de l'apparence des bordures avec des patchs. L'architecture modulaire de AutoHDR permet une collaboration entre les humains et les machines, avec une intervention flexible et une optimisation à chaque étape de reconstruction. Les expériences montrent un performance notable de AutoHDR, avec un augmentation de la précision de OCR de 46,83% à 84,05% pour des documents avec des dommages graves, et un rendement de 94,25% en combinant la collaboration humain-machine. Cette recherche représente un avancement important dans l'automatisation de la reconstruction de documents historiques et contribue de manière significative à la conservation des patrimoines culturels. Le modèle et l'ensemble de données sont disponibles sur https://github.com/SCUT-DLVCLab/AutoHDR.",
      "upvotes": 7,
      "discussionId": "686cc8bd364e2ad167eb54eb",
      "githubRepo": "https://github.com/SCUT-DLVCLab/AutoHDR",
      "githubStars": 22
    },
    "publishedAt": "2025-07-07T11:26:17.000Z",
    "title": "Reviving Cultural Heritage: A Novel Approach for Comprehensive\n  Historical Document Restoration",
    "summary": "Historical documents represent an invaluable cultural heritage, yet have\nundergone significant degradation over time through tears, water erosion, and\noxidation. Existing Historical Document Restoration (HDR) methods primarily\nfocus on single modality or limited-size restoration, failing to meet practical\nneeds. To fill this gap, we present a full-page HDR dataset (FPHDR) and a novel\nautomated HDR solution (AutoHDR). Specifically, FPHDR comprises 1,633 real and\n6,543 synthetic images with character-level and line-level locations, as well\nas character annotations in different damage grades. AutoHDR mimics historians'\nrestoration workflows through a three-stage approach: OCR-assisted damage\nlocalization, vision-language context text prediction, and patch autoregressive\nappearance restoration. The modular architecture of AutoHDR enables seamless\nhuman-machine collaboration, allowing for flexible intervention and\noptimization at each restoration stage. Experiments demonstrate AutoHDR's\nremarkable performance in HDR. When processing severely damaged documents, our\nmethod improves OCR accuracy from 46.83\\% to 84.05\\%, with further enhancement\nto 94.25\\% through human-machine collaboration. We believe this work represents\na significant advancement in automated historical document restoration and\ncontributes substantially to cultural heritage preservation. The model and\ndataset are available at https://github.com/SCUT-DLVCLab/AutoHDR.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.05108.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65fba5700b78c48c9e393a3e",
      "avatarUrl": "/avatars/795cc8167460d8e89ff91d27c5da9fb2.svg",
      "fullname": "Yuyi Zhang",
      "name": "ZZXF",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.03745",
      "authors": [
        {
          "_id": "686ca04e364e2ad167eb543c",
          "user": {
            "_id": "63fedca388b9695964c33ad8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63fedca388b9695964c33ad8/XFE8Tm9yRRbo6XdDw7c2O.png",
            "isPro": false,
            "fullname": "Aki",
            "user": "AkiCumulo",
            "type": "user"
          },
          "name": "Akio Kodaira",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-08T08:06:07.401Z",
          "hidden": false
        },
        {
          "_id": "686ca04e364e2ad167eb543d",
          "name": "Tingbo Hou",
          "hidden": false
        },
        {
          "_id": "686ca04e364e2ad167eb543e",
          "name": "Ji Hou",
          "hidden": false
        },
        {
          "_id": "686ca04e364e2ad167eb543f",
          "name": "Masayoshi Tomizuka",
          "hidden": false
        },
        {
          "_id": "686ca04e364e2ad167eb5440",
          "name": "Yue Zhao",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63fedca388b9695964c33ad8/eFed7d-PiKdUBGIPDkIoo.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/63fedca388b9695964c33ad8/50KoaI5mgYLe_IvIOaVJh.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/63fedca388b9695964c33ad8/zWzNfOJX8C-CNICabQyHV.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/63fedca388b9695964c33ad8/m072g2U7H133AyOGShNZ0.mp4"
      ],
      "publishedAt": "2025-07-04T18:00:01.000Z",
      "submittedOnDailyAt": "2025-07-08T04:21:42.132Z",
      "title": "StreamDiT : Génération de vidéo en temps réel avec texte de mapping",
      "submittedOnDailyBy": {
        "_id": "63fedca388b9695964c33ad8",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63fedca388b9695964c33ad8/XFE8Tm9yRRbo6XdDw7c2O.png",
        "isPro": false,
        "fullname": "Aki",
        "user": "AkiCumulo",
        "type": "user"
      },
      "summary": "Récemment, un grand progrès a été réalisé dans la génération de vidéos de grande qualité grâce à l'expansion des modèles basés sur les transformers à des milliards de paramètres. Cependant, actuellement, ces modèles ne génèrent que des séquences très courtes en ligne, limitant ainsi leur utilisation dans des applications interactives ou en temps réel. Dans cet article, nous proposons StreamDiT, un modèle de génération de vidéos en temps réel, pour résoudre ces problèmes. L'entraînement de StreamDiT est basé sur le matching de flux avec l'ajout d'un buffer de mouvement. Une entraînement mixte est conçu pour différentes sections de la phrase du buffer, améliorant à la fois la cohérence du contenu et la qualité visuelle. Le modèle StreamDiT est basé sur adaLN DiT en utilisant une codification temporelle et une attention par fenêtre. Pour mettre en œuvre cette proposition, un modèle StreamDiT de 4B paramètres est entraîné. De plus, un méthode de distillation multistep adaptée pour StreamDiT est proposée. La distillation est effectuée dans chaque section des séquences de frames sélectionnées, et ensuite, la quantité totale d'évaluations de fonction (NFEs) diminue en fonction du nombre de blocs du buffer. Enfin, notre modèle implémente un fonctionnement en temps réel entre 1 et 16 FPS, permettant la génération de vidéos en temps réel à une résolution de 512p. Notre méthode est évaluée par des métriques statistiques et des évaluations humaines. Notre modèle permet la génération de flux, la génération interactive et l'application en temps réel de vidéos comme des vidéos. Des résultats et des exemples sont disponibles sur le site web du projet : <a href=\"https://cumulo-autumn.github.io/StreamDiT/\">this https URL.</a>",
      "upvotes": 7,
      "discussionId": "686ca04f364e2ad167eb5441",
      "projectPage": "https://cumulo-autumn.github.io/StreamDiT/",
      "ai_summary": "A streaming video generation model named StreamDiT, based on transformer-based diffusion models, enables real-time video generation with high content consistency and visual quality.",
      "ai_keywords": [
        "transformer-based diffusion models",
        "StreamDiT",
        "flow matching",
        "moving buffer",
        "mixed training",
        "adaLN DiT",
        "varying time embedding",
        "window attention",
        "multistep distillation",
        "sampling distillation",
        "function evaluations",
        "real-time performance",
        "streaming generation",
        "interactive generation",
        "video-to-video"
      ]
    },
    "publishedAt": "2025-07-04T14:00:01.000Z",
    "title": "StreamDiT: Real-Time Streaming Text-to-Video Generation",
    "summary": "Recently, great progress has been achieved in text-to-video (T2V) generation\nby scaling transformer-based diffusion models to billions of parameters, which\ncan generate high-quality videos. However, existing models typically produce\nonly short clips offline, restricting their use cases in interactive and\nreal-time applications. This paper addresses these challenges by proposing\nStreamDiT, a streaming video generation model. StreamDiT training is based on\nflow matching by adding a moving buffer. We design mixed training with\ndifferent partitioning schemes of buffered frames to boost both content\nconsistency and visual quality. StreamDiT modeling is based on adaLN DiT with\nvarying time embedding and window attention. To practice the proposed method,\nwe train a StreamDiT model with 4B parameters. In addition, we propose a\nmultistep distillation method tailored for StreamDiT. Sampling distillation is\nperformed in each segment of a chosen partitioning scheme. After distillation,\nthe total number of function evaluations (NFEs) is reduced to the number of\nchunks in a buffer. Finally, our distilled model reaches real-time performance\nat 16 FPS on one GPU, which can generate video streams at 512p resolution. We\nevaluate our method through both quantitative metrics and human evaluation. Our\nmodel enables real-time applications, e.g. streaming generation, interactive\ngeneration, and video-to-video. We provide video results and more examples in\nour project website: <a href=\"https://cumulo-autumn.github.io/StreamDiT/\">this\nhttps URL.</a>",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63fedca388b9695964c33ad8/eFed7d-PiKdUBGIPDkIoo.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/63fedca388b9695964c33ad8/50KoaI5mgYLe_IvIOaVJh.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/63fedca388b9695964c33ad8/zWzNfOJX8C-CNICabQyHV.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/63fedca388b9695964c33ad8/m072g2U7H133AyOGShNZ0.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.03745.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63fedca388b9695964c33ad8",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63fedca388b9695964c33ad8/XFE8Tm9yRRbo6XdDw7c2O.png",
      "fullname": "Aki",
      "name": "AkiCumulo",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.04952",
      "authors": [
        {
          "_id": "686c8487364e2ad167eb5386",
          "name": "Chenchen Zhang",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb5387",
          "name": "Yuhang Li",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb5388",
          "name": "Can Xu",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb5389",
          "name": "Jiaheng Liu",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb538a",
          "name": "Ao Liu",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb538b",
          "name": "Shihui Hu",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb538c",
          "name": "Dengpeng Wu",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb538d",
          "name": "Guanhua Huang",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb538e",
          "name": "Kejiao Li",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb538f",
          "name": "Qi Yi",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb5390",
          "name": "Ruibin Xiong",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb5391",
          "name": "Haotian Zhu",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb5392",
          "name": "Yuanxing Zhang",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb5393",
          "name": "Yuhao Jiang",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb5394",
          "name": "Yue Zhang",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb5395",
          "name": "Zenan Xu",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb5396",
          "name": "Bohui Zhai",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb5397",
          "name": "Guoxiang He",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb5398",
          "name": "Hebin Li",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb5399",
          "name": "Jie Zhao",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb539a",
          "name": "Le Zhang",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb539b",
          "name": "Lingyun Tan",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb539c",
          "name": "Pengyu Guo",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb539d",
          "name": "Xianshu Pang",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb539e",
          "name": "Yang Ruan",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb539f",
          "name": "Zhifeng Zhang",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb53a0",
          "name": "Zhonghu Wang",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb53a1",
          "name": "Ziyan Xu",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb53a2",
          "name": "Zuopu Yin",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb53a3",
          "name": "Wiggin Zhou",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb53a4",
          "name": "Chayse Zhou",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb53a5",
          "name": "Fengzong Lian",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64b74b906ab5d14ca7f289cd/Ib4IFSlNDtkWKjjyVWG-S.png",
        "https://cdn-uploads.huggingface.co/production/uploads/64b74b906ab5d14ca7f289cd/4OtfeyvZ2jMLaebiU0NKA.png",
        "https://cdn-uploads.huggingface.co/production/uploads/64b74b906ab5d14ca7f289cd/k-024E68E9-6iaF0QtZad.png",
        "https://cdn-uploads.huggingface.co/production/uploads/64b74b906ab5d14ca7f289cd/VBS9HcDQ2oHj3bmxgyZVB.png"
      ],
      "publishedAt": "2025-07-07T12:53:00.000Z",
      "submittedOnDailyAt": "2025-07-08T02:53:13.802Z",
      "title": "ArtifactsBench : Évaluation de la génération de code par des IAs basée sur l'interaction visuelle entre elles",
      "submittedOnDailyBy": {
        "_id": "64b74b906ab5d14ca7f289cd",
        "avatarUrl": "/avatars/b131b7c4ce5216708ca4a678f35ead0a.svg",
        "isPro": false,
        "fullname": "xxzcc",
        "user": "xxzcc",
        "type": "user"
      },
      "summary": "La capacité de génération des LLM s'étend rapidement vers le dynamique et interactif visuel en code statique. Ce développement est limité par un important lac de l'évaluation, car les cadres de référence existants se concentrent sur la précision des algorithmes, sans considérer la confiance visuelle et l'intégration interactive qui définissent l'expérience utilisateur moderne. Pour résoudre ce lac, nous présentons ArtifactsBench, un nouveau cadre de référence et paradigme pour l'évaluation automatique multiforme de la génération de code visuel. Notre cadre de référence représente chaque contenu généré des artefacts programmatiquement et le visualise à travers des séquences de captures d'écran pour comprendre sa dynamique. Cette preuve visuelle et le code généré sont évalués par un MLLM (Multimodal LLM) qui agit comme juge, avec une note rigoureuse basée sur des listes de contrôle qui garantit la reproductibilité. Nous avons construit un benchmark de 1 825 tâches différentes et évalué plus de 30 modèles avancés de LLM. Notre évaluation automatique atteint un 94,4% de concordance avec WebDev Arena et dépasse le 90% de la concordance entre experts. Ainsi, ArtifactsBench est le premier cadre de référence qui automatise l'évaluation de la qualité humaine à une échelle fiable. Nos analyses fournissent des cartes de haute résolution, montrent que les modèles généraux peuvent dépasser les modèles de domaine et offrent les résultats du benchmark, l'évaluation et les limites de référence sur https://artifactsbenchmark.github.io/. Cela fournit à la communauté une outil échelonnable et précis, accélérant le développement de modèles de génération axés sur l'utilisateur.",
      "upvotes": 6,
      "discussionId": "686c8487364e2ad167eb53a6",
      "ai_summary": "ArtifactsBench, a novel benchmark and evaluation framework, automates the assessment of visual code generation quality using temporal screenshots and a multimodal language model judge.",
      "ai_keywords": [
        "Large Language Models",
        "LLMS",
        "dynamic",
        "interactive visual artifacts",
        "visual fidelity",
        "interactive integrity",
        "ArtifactsBench",
        "benchmark",
        "Multimodal LLM",
        "MLLM-as-Judge",
        "fine-grained",
        "per-task checklist",
        "ranking consistency",
        "WebDev Arena",
        "pairwise agreement",
        "human-perceived quality",
        "generalist models",
        "domain-specific ones"
      ]
    },
    "publishedAt": "2025-07-07T08:53:00.000Z",
    "title": "ArtifactsBench: Bridging the Visual-Interactive Gap in LLM Code\n  Generation Evaluation",
    "summary": "The generative capabilities of Large Language Models (LLMs) are rapidly\nexpanding from static code to dynamic, interactive visual artifacts. This\nprogress is bottlenecked by a critical evaluation gap: established benchmarks\nfocus on algorithmic correctness and are blind to the visual fidelity and\ninteractive integrity that define modern user experiences. To bridge this gap,\nwe introduce ArtifactsBench, a new benchmark and paradigm for the automated,\nmultimodal evaluation of visual code generation. Our framework programmatically\nrenders each generated artifact and captures its dynamic behavior through\ntemporal screenshots. This visual evidence, alongside the source code, is then\nassessed by a Multimodal LLM (MLLM)-as-Judge, which is rigorously guided by a\nfine-grained, per-task checklist to ensure holistic and reproducible scoring.\nWe construct a new benchmark of 1,825 diverse tasks and evaluate over 30\nleading LLMs. Our automated evaluation achieves a striking 94.4% ranking\nconsistency with WebDev Arena, the gold-standard for human preference in web\ndevelopment, and over 90% pairwise agreement with human experts. This\nestablishes ArtifactsBench as the first framework to reliably automate the\nassessment of human-perceived quality at scale. Our analysis provides a\nhigh-resolution map of the current SOTA, revealing that generalist models often\noutperform domain-specific ones. We open-source ArtifactsBench, including the\nbenchmark, evaluation harness, and baseline results at\nhttps://artifactsbenchmark.github.io/, to provide the community with a scalable\nand accurate tool to accelerate the development of user-centric generative\nmodels.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64b74b906ab5d14ca7f289cd/Ib4IFSlNDtkWKjjyVWG-S.png",
      "https://cdn-uploads.huggingface.co/production/uploads/64b74b906ab5d14ca7f289cd/4OtfeyvZ2jMLaebiU0NKA.png",
      "https://cdn-uploads.huggingface.co/production/uploads/64b74b906ab5d14ca7f289cd/k-024E68E9-6iaF0QtZad.png",
      "https://cdn-uploads.huggingface.co/production/uploads/64b74b906ab5d14ca7f289cd/VBS9HcDQ2oHj3bmxgyZVB.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.04952.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b74b906ab5d14ca7f289cd",
      "avatarUrl": "/avatars/b131b7c4ce5216708ca4a678f35ead0a.svg",
      "fullname": "xxzcc",
      "name": "xxzcc",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.04590",
      "authors": [
        {
          "_id": "686c96a0364e2ad167eb540c",
          "name": "Rui Meng",
          "hidden": false
        },
        {
          "_id": "686c96a0364e2ad167eb540d",
          "user": {
            "_id": "64778fb8168cb428e00f69b0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64778fb8168cb428e00f69b0/D_XYg74zHG9K3HUJj_gD4.jpeg",
            "isPro": true,
            "fullname": "Ziyan Jiang",
            "user": "ziyjiang",
            "type": "user"
          },
          "name": "Ziyan Jiang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-08T08:06:15.117Z",
          "hidden": false
        },
        {
          "_id": "686c96a0364e2ad167eb540e",
          "name": "Ye Liu",
          "hidden": false
        },
        {
          "_id": "686c96a0364e2ad167eb540f",
          "name": "Mingyi Su",
          "hidden": false
        },
        {
          "_id": "686c96a0364e2ad167eb5410",
          "name": "Xinyi Yang",
          "hidden": false
        },
        {
          "_id": "686c96a0364e2ad167eb5411",
          "name": "Yuepeng Fu",
          "hidden": false
        },
        {
          "_id": "686c96a0364e2ad167eb5412",
          "name": "Can Qin",
          "hidden": false
        },
        {
          "_id": "686c96a0364e2ad167eb5413",
          "name": "Zeyuan Chen",
          "hidden": false
        },
        {
          "_id": "686c96a0364e2ad167eb5414",
          "name": "Ran Xu",
          "hidden": false
        },
        {
          "_id": "686c96a0364e2ad167eb5415",
          "name": "Caiming Xiong",
          "hidden": false
        },
        {
          "_id": "686c96a0364e2ad167eb5416",
          "name": "Yingbo Zhou",
          "hidden": false
        },
        {
          "_id": "686c96a0364e2ad167eb5417",
          "name": "Wenhu Chen",
          "hidden": false
        },
        {
          "_id": "686c96a0364e2ad167eb5418",
          "name": "Semih Yavuz",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-07T00:51:57.000Z",
      "submittedOnDailyAt": "2025-07-08T02:26:58.880Z",
      "title": "VLM2Vec-V2 : Développement de l'encodage multimodal des films, images et documents visuels",
      "submittedOnDailyBy": {
        "_id": "64778fb8168cb428e00f69b0",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64778fb8168cb428e00f69b0/D_XYg74zHG9K3HUJj_gD4.jpeg",
        "isPro": true,
        "fullname": "Ziyan Jiang",
        "user": "ziyjiang",
        "type": "user"
      },
      "summary": "Le modèle multimodal joue un rôle crucial dans la facilitation de diverses tâches de bas niveau (similarité sémantique, recherche d'information, regroupement). Cependant, les modèles multimodal actuels (VLM2Vec, E5-V, GME) se concentrent principalement sur les images naturelles. Le support pour d'autres formats visuels (vidéos, documents visuels) est limité, ce qui réduit leur applicabilité dans des situations réelles. Dans des environnements comme des agents intelligents, la recherche et la recommandation multimodal, et la génération de résumés de recherche (RAG), ces limites sont évidentes. Pour résoudre ces problèmes, on propose VLM2Vec-V2 et un cadre unifié pour l'apprentissage multimodal de divers formats visuels. Tout d'abord, on présente un nouveau benchmark détaillé, MMEB-V2, qui inclut cinq nouveaux types de tâches par rapport à MMEB. Ensuite, on entraîne VLM2Vec-V2, un modèle multimodal général qui accepte des entrées de grammaire, des images, des vidéos et des documents visuels. Les résultats d'expériences étendues montrent que VLM2Vec-V2 présente un excellent rendement dans de nouvelles tâches de recherche de vidéos et de documents, et dépasse les normes de référence sur les benchmarks d'images existants. Grâce à ces évaluations étendues, cette étude explique la capacité de généralisation des modèles multimodal et révèle des stratégies efficaces pour l'apprentissage multimodal unifié, établissant une base pour la représentation d'apprentissage plus scalable et applicable dans les domaines académique et réel.",
      "upvotes": 4,
      "discussionId": "686c96a0364e2ad167eb5419",
      "projectPage": "https://tiger-ai-lab.github.io/VLM2Vec/",
      "githubRepo": "https://github.com/TIGER-AI-Lab/VLM2Vec",
      "ai_summary": "A unified framework VLM2Vec-V2 is proposed for learning embeddings across diverse visual forms such as videos and documents, demonstrating strong performance on new tasks and improving upon existing benchmarks for images.",
      "ai_keywords": [
        "multimodal embedding models",
        "VLM2Vec",
        "E5-V",
        "GME",
        "MMEB-V2",
        "visual document retrieval",
        "video retrieval",
        "temporal grounding",
        "video classification",
        "video question answering",
        "general-purpose embedding model",
        "unified embedding learning",
        "representation learning"
      ],
      "githubStars": 290
    },
    "publishedAt": "2025-07-06T20:51:57.000Z",
    "title": "VLM2Vec-V2: Advancing Multimodal Embedding for Videos, Images, and\n  Visual Documents",
    "summary": "Multimodal embedding models have been crucial in enabling various downstream\ntasks such as semantic similarity, information retrieval, and clustering over\ndifferent modalities. However, existing multimodal embeddings like VLM2Vec,\nE5-V, GME are predominantly focused on natural images, with limited support for\nother visual forms such as videos and visual documents. This restricts their\napplicability in real-world scenarios, including AI agents, multi-modal search\nand recommendation, and retrieval-augmented generation (RAG). To close this\ngap, we propose VLM2Vec-V2, a unified framework for learning embeddings across\ndiverse visual forms. First, we introduce MMEB-V2, a comprehensive benchmark\nthat extends MMEB with five new task types: visual document retrieval, video\nretrieval, temporal grounding, video classification and video question\nanswering - spanning text, image, video, and visual document inputs. Next, we\ntrain VLM2Vec-V2, a general-purpose embedding model that supports text, image,\nvideo, and visual document inputs. Extensive experiments show that VLM2Vec-V2\nachieves strong performance not only on the newly introduced video and document\nretrieval tasks, but also improves over prior baselines on the original image\nbenchmarks. Through extensive evaluation, our study offers insights into the\ngeneralizability of various multimodal embedding models and highlights\neffective strategies for unified embedding learning, laying the groundwork for\nmore scalable and adaptable representation learning in both research and\nreal-world settings.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.04590.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64778fb8168cb428e00f69b0",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64778fb8168cb428e00f69b0/D_XYg74zHG9K3HUJj_gD4.jpeg",
      "fullname": "Ziyan Jiang",
      "name": "ziyjiang",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.03607",
      "authors": [
        {
          "_id": "686cb574364e2ad167eb54c3",
          "user": {
            "_id": "65e5bc754230174d547fa1dc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/xQ1NGVasxexuR_Hs0dvNl.jpeg",
            "isPro": false,
            "fullname": "Cédric",
            "user": "cedricbonhomme",
            "type": "user"
          },
          "name": "Cédric Bonhomme",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-08T08:05:41.165Z",
          "hidden": false
        },
        {
          "_id": "686cb574364e2ad167eb54c4",
          "user": {
            "_id": "677d08a57038d6b09078649a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/jEUTW_G_VCgbvqfCxlNTj.png",
            "isPro": false,
            "fullname": "Dulaunoy",
            "user": "adulau",
            "type": "user"
          },
          "name": "Alexandre Dulaunoy",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-08T08:39:37.700Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/65e5bc754230174d547fa1dc/S4_uGwFUDxVHZqJMljzP8.jpeg"
      ],
      "publishedAt": "2025-07-04T14:28:14.000Z",
      "submittedOnDailyAt": "2025-07-08T06:42:20.895Z",
      "title": "VLAI : Classification de la gravité des erreurs automatisées basée sur un modèle ROBERTa",
      "submittedOnDailyBy": {
        "_id": "65e5bc754230174d547fa1dc",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/xQ1NGVasxexuR_Hs0dvNl.jpeg",
        "isPro": false,
        "fullname": "Cédric",
        "user": "cedricbonhomme",
        "type": "user"
      },
      "summary": "Cet article présente un modèle VLAI basé sur le Transformer. VLAI a été développé sur RoBERTa et a été entraîné avec plus de 600 000 données de vulnérabilités réelles, atteignant un niveau de précision de 82% ou plus en prédisant les effets des vulnérabilités. Ce modèle offre une évaluation plus rapide et cohérente que la note automatique CVSS. Le modèle et le jeu de données sont disponibles sous licence ouverte et sont intégrés dans le service Vulnerability-Lookup.",
      "upvotes": 4,
      "discussionId": "686cb574364e2ad167eb54c5",
      "projectPage": "https://www.vulnerability-lookup.org",
      "githubRepo": "https://github.com/vulnerability-lookup/VulnTrain",
      "ai_summary": "A transformer-based model predicts software vulnerability severity levels directly from text, enhancing triage efficiency and consistency.",
      "ai_keywords": [
        "transformer",
        "RoBERTa",
        "parameter-efficient fine-tuning",
        "predicting severity categories",
        "CVSS scoring",
        "open-source",
        "Vulnerability-Lookup service"
      ],
      "githubStars": 11
    },
    "publishedAt": "2025-07-04T10:28:14.000Z",
    "title": "VLAI: A RoBERTa-Based Model for Automated Vulnerability Severity\n  Classification",
    "summary": "This paper presents VLAI, a transformer-based model that predicts software\nvulnerability severity levels directly from text descriptions. Built on\nRoBERTa, VLAI is fine-tuned on over 600,000 real-world vulnerabilities and\nachieves over 82% accuracy in predicting severity categories, enabling faster\nand more consistent triage ahead of manual CVSS scoring. The model and dataset\nare open-source and integrated into the Vulnerability-Lookup service.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/65e5bc754230174d547fa1dc/S4_uGwFUDxVHZqJMljzP8.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.03607.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65e5bc754230174d547fa1dc",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/xQ1NGVasxexuR_Hs0dvNl.jpeg",
      "fullname": "Cédric",
      "name": "cedricbonhomme",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.04036",
      "authors": [
        {
          "_id": "686c92c5364e2ad167eb53fa",
          "name": "Jingwei Shi",
          "hidden": false
        },
        {
          "_id": "686c92c5364e2ad167eb53fb",
          "user": {
            "_id": "64ec877bb93654d4ca5c92e9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
            "isPro": false,
            "fullname": "Zeyu Zhang",
            "user": "SteveZeyuZhang",
            "type": "user"
          },
          "name": "Zeyu Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-08T08:06:20.306Z",
          "hidden": false
        },
        {
          "_id": "686c92c5364e2ad167eb53fc",
          "name": "Biao Wu",
          "hidden": false
        },
        {
          "_id": "686c92c5364e2ad167eb53fd",
          "name": "Yanjie Liang",
          "hidden": false
        },
        {
          "_id": "686c92c5364e2ad167eb53fe",
          "name": "Meng Fang",
          "hidden": false
        },
        {
          "_id": "686c92c5364e2ad167eb53ff",
          "name": "Ling Chen",
          "hidden": false
        },
        {
          "_id": "686c92c5364e2ad167eb5400",
          "name": "Yang Zhao",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64ec877bb93654d4ca5c92e9/Kwa3r-mdnTK5Itx86d1f6.mp4"
      ],
      "publishedAt": "2025-07-05T13:24:15.000Z",
      "submittedOnDailyAt": "2025-07-08T02:14:06.764Z",
      "title": "Agent actuel : Agent multimodal pour la génération de vidéos de présentation",
      "submittedOnDailyBy": {
        "_id": "64ec877bb93654d4ca5c92e9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
        "isPro": false,
        "fullname": "Zeyu Zhang",
        "user": "SteveZeyuZhang",
        "type": "user"
      },
      "summary": "Le présentateur est un agent multimodal qui transforme des documents longs en vidéos de présentation narrative. Les méthodes actuelles sont limitées aux slides statiques ou aux résumés textuels, mais notre méthode dépasse ces limites et génère un contenu visuel et oral completement synchronisé qui ressemble à l'étiquette d'une présentation humaine. Pour atteindre cette intégration, le présentateur divise le document systématiquement, planifie et rend les slides visuels, et utilise un modèle de production en bord avec un modèle de langage grand et de texte contextuel pour générer des narrations orales contextualisées. La vidéo finale est complétée par un reconnaissance de voix et de vision précise. Compte tenu de la complexité de l'évaluation de ce type de multimodal output, nous avons introduit un cadre d'évaluation unifié en trois dimensions : fidélité au contenu, visualisation et compréhension de l'auditoire basée sur les prompts, incluant des modèles de langage de vision. Notre essai expérimental a été réalisé avec un ensemble de données de présentations de 30 pages, montrant que le présentateur approche le niveau humain dans tous les indicateurs de qualité. Ces résultats mettent en évidence le grand potentiel d'un agent multimodal contrôlable pour convertir des matériaux textuels statiques en formats dynamiques, efficaces et accessibles de présentation. Le code est disponible sur https://github.com/AIGeeksGroup/PresentAgent.",
      "upvotes": 3,
      "discussionId": "686c92c6364e2ad167eb5401",
      "githubRepo": "https://github.com/AIGeeksGroup/PresentAgent",
      "ai_summary": "A multimodal agent transforms documents into detailed presentation videos with audio, evaluated using a comprehensive framework involving vision-language models.",
      "ai_keywords": [
        "multimodal agent",
        "narrated presentation videos",
        "static slides",
        "text summaries",
        "large language models",
        "Text-to-Speech models",
        "audio-visual alignment",
        "Vision-Language Models",
        "content fidelity",
        "visual clarity",
        "audience comprehension",
        "prompt-based evaluation"
      ],
      "githubStars": 11
    },
    "publishedAt": "2025-07-05T09:24:15.000Z",
    "title": "PresentAgent: Multimodal Agent for Presentation Video Generation",
    "summary": "We present PresentAgent, a multimodal agent that transforms long-form\ndocuments into narrated presentation videos. While existing approaches are\nlimited to generating static slides or text summaries, our method advances\nbeyond these limitations by producing fully synchronized visual and spoken\ncontent that closely mimics human-style presentations. To achieve this\nintegration, PresentAgent employs a modular pipeline that systematically\nsegments the input document, plans and renders slide-style visual frames,\ngenerates contextual spoken narration with large language models and\nText-to-Speech models, and seamlessly composes the final video with precise\naudio-visual alignment. Given the complexity of evaluating such multimodal\noutputs, we introduce PresentEval, a unified assessment framework powered by\nVision-Language Models that comprehensively scores videos across three critical\ndimensions: content fidelity, visual clarity, and audience comprehension\nthrough prompt-based evaluation. Our experimental validation on a curated\ndataset of 30 document-presentation pairs demonstrates that PresentAgent\napproaches human-level quality across all evaluation metrics. These results\nhighlight the significant potential of controllable multimodal agents in\ntransforming static textual materials into dynamic, effective, and accessible\npresentation formats. Code will be available at\nhttps://github.com/AIGeeksGroup/PresentAgent.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64ec877bb93654d4ca5c92e9/Kwa3r-mdnTK5Itx86d1f6.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.04036.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ec877bb93654d4ca5c92e9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
      "fullname": "Zeyu Zhang",
      "name": "SteveZeyuZhang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.05259",
      "authors": [
        {
          "_id": "686ca7ef364e2ad167eb544d",
          "user": {
            "_id": "6499eca0685215f7247bd5ce",
            "avatarUrl": "/avatars/b6fea0c33c3c930c7314b99b414072a9.svg",
            "isPro": false,
            "fullname": "Chun-Hsiao Yeh",
            "user": "danielchyeh",
            "type": "user"
          },
          "name": "Chun-Hsiao Yeh",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-08T08:06:03.462Z",
          "hidden": false
        },
        {
          "_id": "686ca7ef364e2ad167eb544e",
          "name": "Yilin Wang",
          "hidden": false
        },
        {
          "_id": "686ca7ef364e2ad167eb544f",
          "name": "Nanxuan Zhao",
          "hidden": false
        },
        {
          "_id": "686ca7ef364e2ad167eb5450",
          "name": "Richard Zhang",
          "hidden": false
        },
        {
          "_id": "686ca7ef364e2ad167eb5451",
          "name": "Yuheng Li",
          "hidden": false
        },
        {
          "_id": "686ca7ef364e2ad167eb5452",
          "name": "Yi Ma",
          "hidden": false
        },
        {
          "_id": "686ca7ef364e2ad167eb5453",
          "name": "Krishna Kumar Singh",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-07T17:59:56.000Z",
      "submittedOnDailyAt": "2025-07-08T03:46:34.916Z",
      "title": "X-Planner permet d'éditer des images en utilisant des commandes complexes et est une outil qui permet d'effectuer des éditions d'images non seulement simples, mais qui soutient également des éditions d'images adaptées à des contextes ou conditions complexes.",
      "submittedOnDailyBy": {
        "_id": "6499eca0685215f7247bd5ce",
        "avatarUrl": "/avatars/b6fea0c33c3c930c7314b99b414072a9.svg",
        "isPro": false,
        "fullname": "Chun-Hsiao Yeh",
        "user": "danielchyeh",
        "type": "user"
      },
      "summary": "Récemment, les méthodes d'édition d'images basées sur la diffusion ont démontré des progrès significatifs dans des tâches basées sur le texte, mais leur capacité à comprendre des indications complexes et indirectes est limitée, ce qui rend les modèles actuels confrontés à des difficultés liées à la maintenance de l'output, des éditions non intentionnelles ou à la nécessité de la participation manuelle et de l'utilisation de masques dynamiques. Pour résoudre ces problèmes, nous présentons un système de planification basé sur le Modèle Large de Langage Multimodal (MLLM) appelé X-Planner, qui vise efficacement à relire les intentions du utilisateur aux capacités du modèle d'édition. X-Planner utilise le raisonnement par chaîne de pensée pour décomposer les indications complexes en étapes simples et claires. Pour chaque étape d'instruction, X-Planner génère automatiquement des types d'édition avec une haute précision et des masques de partition, éliminant la participation manuelle et garantissant que les éditions soient localisées et maintiennent l'output. De plus, nous proposons un nouveau processus automatique pour générer de grandes quantités de données pour l'entraînement de X-Planner, obtenant des résultats pionniers sur les benchmarks actuels et sur de nouveaux benchmarks d'édition complexe.",
      "upvotes": 2,
      "discussionId": "686ca7f0364e2ad167eb5454",
      "projectPage": "https://danielchyeh.github.io/x-planner/",
      "ai_summary": "X-Planner, a planning system utilizing a multimodal large language model, decomposes complex text-guided image editing instructions into precise sub-instructions, ensuring localized, identity-preserving edits and achieving top performance on established benchmarks.",
      "ai_keywords": [
        "Multimodal Large Language Model",
        "MLLM",
        "chain-of-thought reasoning",
        "segmentation masks",
        "image editing",
        "identity preservation",
        "automated pipeline"
      ]
    },
    "publishedAt": "2025-07-07T13:59:56.000Z",
    "title": "Beyond Simple Edits: X-Planner for Complex Instruction-Based Image\n  Editing",
    "summary": "Recent diffusion-based image editing methods have significantly advanced\ntext-guided tasks but often struggle to interpret complex, indirect\ninstructions. Moreover, current models frequently suffer from poor identity\npreservation, unintended edits, or rely heavily on manual masks. To address\nthese challenges, we introduce X-Planner, a Multimodal Large Language Model\n(MLLM)-based planning system that effectively bridges user intent with editing\nmodel capabilities. X-Planner employs chain-of-thought reasoning to\nsystematically decompose complex instructions into simpler, clear\nsub-instructions. For each sub-instruction, X-Planner automatically generates\nprecise edit types and segmentation masks, eliminating manual intervention and\nensuring localized, identity-preserving edits. Additionally, we propose a novel\nautomated pipeline for generating large-scale data to train X-Planner which\nachieves state-of-the-art results on both existing benchmarks and our newly\nintroduced complex editing benchmark.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.05259.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6499eca0685215f7247bd5ce",
      "avatarUrl": "/avatars/b6fea0c33c3c930c7314b99b414072a9.svg",
      "fullname": "Chun-Hsiao Yeh",
      "name": "danielchyeh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.04562",
      "authors": [
        {
          "_id": "686ca9e2364e2ad167eb5461",
          "name": "Janna Lu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-06T22:26:59.000Z",
      "submittedOnDailyAt": "2025-07-08T03:51:31.373Z",
      "title": "「Comparaison et Évaluation de LMs et de Predicteurs Humains pour la Prédiction du Monde Réel」",
      "submittedOnDailyBy": {
        "_id": "66b3d98e040c500914ef558f",
        "avatarUrl": "/avatars/a90f8306dbd7747520ce5b941ee3bbcb.svg",
        "isPro": false,
        "fullname": "Janna",
        "user": "jannalu",
        "type": "user"
      },
      "summary": "Les modèles de langage grand (LLMs) montrent des capacités exceptionnelles dans plusieurs tâches, mais leur capacité à prédire des événements futurs est peu étudiée. Un an plus tôt, les modèles de langage grand avaient des difficultés à atteindre la précision d'un groupe humain. Dans Metaculus, 464 problèmes de prédiction ont été évalués et les meilleurs modèles de LLMs ont été comparés aux meilleurs prédicteurs humains. Le modèle Frontier semble dépasser le score de Brier, mais s'efface clairement lorsqu'il est comparé au groupe de prédicteurs humains.",
      "upvotes": 1,
      "discussionId": "686ca9e2364e2ad167eb5462",
      "ai_summary": "State-of-the-art large language models are evaluated on forecasting questions and show lower accuracy compared to human superforecasters.",
      "ai_keywords": [
        "large language models",
        "LLMs",
        "Brier scores",
        "human superforecasters"
      ]
    },
    "publishedAt": "2025-07-06T18:26:59.000Z",
    "title": "Evaluating LLMs on Real-World Forecasting Against Human Superforecasters",
    "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\ndiverse tasks, but their ability to forecast future events remains\nunderstudied. A year ago, large language models struggle to come close to the\naccuracy of a human crowd. I evaluate state-of-the-art LLMs on 464 forecasting\nquestions from Metaculus, comparing their performance against human\nsuperforecasters. Frontier models achieve Brier scores that ostensibly surpass\nthe human crowd but still significantly underperform a group of\nsuperforecasters.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.04562.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66b3d98e040c500914ef558f",
      "avatarUrl": "/avatars/a90f8306dbd7747520ce5b941ee3bbcb.svg",
      "fullname": "Janna",
      "name": "jannalu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.04376",
      "authors": [
        {
          "_id": "686cb0ab364e2ad167eb54a1",
          "name": "Georgios Ioannides",
          "hidden": false
        },
        {
          "_id": "686cb0ab364e2ad167eb54a2",
          "name": "Christos Constantinou",
          "hidden": false
        },
        {
          "_id": "686cb0ab364e2ad167eb54a3",
          "name": "Vinija Jain",
          "hidden": false
        },
        {
          "_id": "686cb0ab364e2ad167eb54a4",
          "user": {
            "_id": "63a4754927f1f64ed7238dac",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
            "isPro": false,
            "fullname": "Aman Chadha",
            "user": "amanchadha",
            "type": "user"
          },
          "name": "Aman Chadha",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-08T08:05:49.355Z",
          "hidden": false
        },
        {
          "_id": "686cb0ab364e2ad167eb54a5",
          "name": "Aaron Elkins",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-06T12:46:57.000Z",
      "submittedOnDailyAt": "2025-07-08T04:20:01.196Z",
      "title": "MOD-X : Proposition d'un cadre d'échange modulaire et distribué ouvert\nInterchangeabilité d'agents artificiels avec plusieurs écritures possibles",
      "submittedOnDailyBy": {
        "_id": "63a4754927f1f64ed7238dac",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
        "isPro": false,
        "fullname": "Aman Chadha",
        "user": "amanchadha",
        "type": "user"
      },
      "summary": "Le système d'IA se déplace de modèle unique vers un écosystème d'agents spécialisés, rendant de plus en plus importante la nécessité de protocoles de communication standardisés. Dans cet article, nous présentons une nouvelle architecture MOD-X (Modular Open Decentralized eXchange), conçue pour résoudre les principales limitations des protocoles actuels. Au contraire des méthodes d'accès actuelles, MOD-X propose une structure de couches qui inclut un Bus de Messages Universel, un gestion détaillée de l'état, des fonctions de traduction et une architecture de sécurité basée sur la blockchain. Nous présentons l'architecture de MOD-X, la comparons aux protocoles actuels, et montrons comment l'intégration efficace d'agents intelligents (systèmes basés sur les règles, réseaux neuronaux, moteurs de logique symbolique, logiciel génétique) est réalisée par un travail de workshop qui permet la représentation d'autres architectures, entreprises, capacités et connaissances. Les principales innovations de MOD-X comprennent le modèle de communication souscriptif de produits, la détection de capacités significatives et l'architecture de flux de travail dynamiques, offrant un cadre qui relie la théorie formelle à la pratique. Cette architecture est essentielle pour l'expansion d'un véritable écosystème d'agents interchangeables et distribués, capable d'être échelonnée efficacement sans nécessité de contrôle centralisé.",
      "upvotes": 1,
      "discussionId": "686cb0ab364e2ad167eb54a6"
    },
    "publishedAt": "2025-07-06T08:46:57.000Z",
    "title": "MOD-X: A Modular Open Decentralized eXchange Framework proposal for\n  Heterogeneous Interoperable Artificial Agents",
    "summary": "As Artificial Intelligence systems evolve from monolithic models to\necosystems of specialized agents, the need for standardized communication\nprotocols becomes increasingly critical. This paper introduces MOD-X (Modular\nOpen Decentralized eXchange), a novel architectural framework proposal for\nagent interoperability that addresses key limitations of existing protocols.\nUnlike current approaches, MOD-X proposes a layered architecture with a\nUniversal Message Bus, thorough state management, translation capabilities, and\nblockchain-based security mechanisms. We present MOD-X's architecture, compare\nit with existing protocols, and demonstrate its application through a worked\nexample how it enables integration between heterogeneous specialist agents\n(agents with different architectures, vendors, capabilities, and knowledge\nrepresentations--including rule-based systems, neural networks, symbolic\nreasoning engines, and legacy software with agent wrappers). MOD-X's key\ninnovations include a publish-subscribe communication model, semantic\ncapability discovery, and dynamic workflow orchestration--providing a framework\nthat bridges theoretical formalism with practical implementation. This\narchitecture addresses the growing need for truly decentralized, interoperable\nagent ecosystems that can scale effectively without the need for central\ncoordination.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.04376.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a4754927f1f64ed7238dac",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
      "fullname": "Aman Chadha",
      "name": "amanchadha",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.03336",
      "authors": [
        {
          "_id": "686c67e2364e2ad167eb5314",
          "user": {
            "_id": "637859f98f288aba3d01f588",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637859f98f288aba3d01f588/eP8YNMOtxTvxH-rYEYn4f.png",
            "isPro": false,
            "fullname": "Ashutosh Hathidara",
            "user": "ashutosh1919",
            "type": "user"
          },
          "name": "Ashutosh Hathidara",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-08T08:06:34.896Z",
          "hidden": false
        },
        {
          "_id": "686c67e2364e2ad167eb5315",
          "name": "Julien Yu",
          "hidden": false
        },
        {
          "_id": "686c67e2364e2ad167eb5316",
          "name": "Sebastian Schreiber",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-04T06:49:02.000Z",
      "submittedOnDailyAt": "2025-07-08T02:47:33.416Z",
      "title": "Une entreprise qui dispose d'un centre de conception de pantalons micro ajustables peut appeler des outils d'affaires pour réduire le risque, car le LLM s'approche de la réalité.",
      "submittedOnDailyBy": {
        "_id": "637859f98f288aba3d01f588",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637859f98f288aba3d01f588/eP8YNMOtxTvxH-rYEYn4f.png",
        "isPro": false,
        "fullname": "Ashutosh Hathidara",
        "user": "ashutosh1919",
        "type": "user"
      },
      "summary": "Les modèles de langage grand (LLMs) augmentent la tâche de faire appel aux API des entreprises, cependant, dans des cas où des outils similaires concurent pour le même objectif d'un utilisateur ou manquent d'arguments nécessaires, par exemple, ils toujours échouent. Dans ce contexte, on présente DiaFORGE (DíaPorge, cadre de dialogue conversationnel pour la génération et l'évaluation de réponses ouvertes-code). DiaFORGE est une chaîne de trois étapes, avec une dynamique de diversité synergique. De plus, il satisfait à (i) la synthèse de dialogues de dialogue de tournée professionnels pour que les assistants puissent différencier des outils similaires, (ii) la configuration de sous-objets qui incluent des registres de raisonnement dans des modèles ouverts-code de 3B à 70B paramètres, et (iii) une série dynamique pour évaluer la préparation dans le monde réel. Les modèles entraînés avec DiaFORGE dans le Benchmark Dynamique DiaBENCH améliorent l'efficacité des prompts, dépassant GPT-4o de 27pp et Claude-3.5-Sonnet de 49pp. De plus, DiaFORGE améliore la taux de succès dans l'appel aux API. De plus, dans DiaFORGE, des corpus ouverts sont publiés en pairs 5000 spécifications de moteurs de produits d'API commerciales et des dialogues axés sur la diversité, avec une stratégie pratique pour la construction d'agents de faire appel à des outils commerciales fiables.",
      "upvotes": 1,
      "discussionId": "686c67e2364e2ad167eb5317",
      "ai_summary": "DiaFORGE is a disambiguation framework that enhances large language models' ability to invoke enterprise APIs accurately through dialogue synthesis, supervised fine-tuning, and real-world evaluation.",
      "ai_keywords": [
        "large language models",
        "LLMs",
        "enterprise APIs",
        "persona-driven",
        "multi-turn dialogues",
        "supervised fine-tuning",
        "reasoning traces",
        "end-to-end goal completion",
        "dynamic benchmark",
        "tool-invocation success",
        "disambiguation-focused dialogues"
      ]
    },
    "publishedAt": "2025-07-04T02:49:02.000Z",
    "title": "Disambiguation-Centric Finetuning Makes Enterprise Tool-Calling LLMs\n  More Realistic and Less Risky",
    "summary": "Large language models (LLMs) are increasingly tasked with invoking enterprise\nAPIs, yet they routinely falter when near-duplicate tools vie for the same user\nintent or when required arguments are left underspecified. We introduce\nDiaFORGE (Dialogue Framework for Organic Response Generation & Evaluation), a\ndisambiguation-centric, three-stage pipeline that (i) synthesizes\npersona-driven, multi-turn dialogues in which the assistant must distinguish\namong highly similar tools, (ii) performs supervised fine-tuning of open-source\nmodels with reasoning traces across 3B - 70B parameters, and (iii) evaluates\nreal-world readiness via a dynamic suite that redeploys each model in a live\nagentic loop and reports end-to-end goal completion alongside conventional\nstatic metrics. On our dynamic benchmark DiaBENCH, models trained with DiaFORGE\nraise tool-invocation success by 27 pp over GPT-4o and by 49 pp over\nClaude-3.5-Sonnet, both under optimized prompting. To spur further research, we\nrelease an open corpus of 5000 production-grade enterprise API specifications\npaired with rigorously validated, disambiguation-focused dialogues, offering a\npractical blueprint for building reliable, enterprise-ready tool-calling\nagents.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.03336.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "637859f98f288aba3d01f588",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637859f98f288aba3d01f588/eP8YNMOtxTvxH-rYEYn4f.png",
      "fullname": "Ashutosh Hathidara",
      "name": "ashutosh1919",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.02659",
      "authors": [
        {
          "_id": "686736ed9db35afc9c304cea",
          "name": "Ramchalam Kinattinkara Ramakrishnan",
          "hidden": false
        },
        {
          "_id": "686736ed9db35afc9c304ceb",
          "user": {
            "_id": "65d989790733541e06823258",
            "avatarUrl": "/avatars/cf658c5f586c02e9b1d610b5e49b2826.svg",
            "isPro": false,
            "fullname": "Zhaocong Yuan",
            "user": "justinyyy",
            "type": "user"
          },
          "name": "Zhaocong Yuan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-07-07T15:46:47.568Z",
          "hidden": false
        },
        {
          "_id": "686736ed9db35afc9c304cec",
          "name": "Shaojie Zhuo",
          "hidden": false
        },
        {
          "_id": "686736ed9db35afc9c304ced",
          "name": "Chen Feng",
          "hidden": false
        },
        {
          "_id": "686736ed9db35afc9c304cee",
          "name": "Yicheng Lin",
          "hidden": false
        },
        {
          "_id": "686736ed9db35afc9c304cef",
          "name": "Chenzheng Su",
          "hidden": false
        },
        {
          "_id": "686736ed9db35afc9c304cf0",
          "name": "Xiaopeng Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-03T14:20:41.000Z",
      "submittedOnDailyAt": "2025-07-08T02:34:16.093Z",
      "title": "Omnidraft : outil de décodage omnidirectionnel spécial pour décodage crossbow en ligne",
      "submittedOnDailyBy": {
        "_id": "65d989790733541e06823258",
        "avatarUrl": "/avatars/cf658c5f586c02e9b1d610b5e49b2826.svg",
        "isPro": false,
        "fullname": "Zhaocong Yuan",
        "user": "justinyyy",
        "type": "user"
      },
      "summary": "L'analyse spécifique généralement recommande l'utilisation de petits modèles efficaces de draught. Ce modèle a été entraîné précédemment pour des séries de modèles méta (par exemple : Llama, Qwen modèles) ou est obsolète. Cependant, dans les configurations d'entraînement en ligne, il existe deux problèmes significatifs majeurs : 1) l'utilisation de modèles méta qui ne correspondent pas au modèle de draught ; 2) l'attente d'améliorations radiactives à mesure que l'on utilise. Dans cet article, on propose le cadre OmniDraft, qui permettra que le modèle de draught se adapte dynamiquement à tous les modèles méta. De plus, pour résoudre le problème de la mauvaise correspondance croisée entre le modèle de draught et le modèle méta, on introduit l'ajustement final combiné de modèles anciens et on utilise la technique d'adaptation pour améliorer la vitesse de décodage. OmniDraft est particulièrement adapté pour des applications de LLM sur des dispositifs, où les principaux points de discussion sont le coût du modèle, son efficacité et la personnalisation pour l'utilisateur. Cela souligne la nécessité de résoudre les problèmes mentionnés et se aligne avec le paradigme de \"un seul modèle de draught\". On montre l'effet du cadre OmniDraft sur des tâches de calcul mathématique, de programmation et de génération de texte par entraînement en ligne. En particulier, OmniDraft peut effectuer une décodage spécialisé avec des modèles méta divers, comme Llama-68M, Vicuna-7B, Qwen2-7B et Llama3-8B, avec un accroissement de vitesse de 1,5 à 2 fois.",
      "upvotes": 0,
      "discussionId": "686736ed9db35afc9c304cf1",
      "ai_summary": "OmniDraft, a unified framework, addresses cross-vocabulary mismatch and improves decoding speed by allowing a single draft model to interact dynamically with diverse target models in online settings.",
      "ai_keywords": [
        "n-gram cache",
        "hybrid distillation fine-tuning",
        "adaptive drafting",
        "on-device LLM applications"
      ]
    },
    "publishedAt": "2025-07-03T10:20:41.000Z",
    "title": "OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device\n  Speculative Decoding",
    "summary": "Speculative decoding generally dictates having a small, efficient draft model\nthat is either pretrained or distilled offline to a particular target model\nseries, for instance, Llama or Qwen models. However, within online deployment\nsettings, there are two major challenges: 1) usage of a target model that is\nincompatible with the draft model; 2) expectation of latency improvements over\nusage and time. In this work, we propose OmniDraft, a unified framework that\nenables a single draft model to operate with any target model and adapt\ndynamically to user data. We introduce an online n-gram cache with hybrid\ndistillation fine-tuning to address the cross-vocabulary mismatch across draft\nand target models; and further improve decoding speed by leveraging adaptive\ndrafting techniques. OmniDraft is particularly suitable for on-device LLM\napplications where model cost, efficiency and user customization are the major\npoints of contention. This further highlights the need to tackle the above\nchallenges and motivates the ``one drafter for all'' paradigm. We\nshowcase the proficiency of the OmniDraft framework by performing online\nlearning on math reasoning, coding and text generation tasks. Notably,\nOmniDraft enables a single Llama-68M model to pair with various target models\nincluding Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding;\nand additionally provides up to 1.5-2x speedup.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.02659.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65d989790733541e06823258",
      "avatarUrl": "/avatars/cf658c5f586c02e9b1d610b5e49b2826.svg",
      "fullname": "Zhaocong Yuan",
      "name": "justinyyy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]