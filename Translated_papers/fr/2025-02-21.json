[
  {
    "paper": {
      "id": "2502.14739",
      "authors": [
        {
          "_id": "67b7efc26348a1df80a8ae53",
          "name": "M-A-P Team",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae54",
          "name": "Xinrun Du",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae55",
          "name": "Yifan Yao",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae56",
          "name": "Kaijing Ma",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae57",
          "name": "Bingli Wang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae58",
          "user": {
            "_id": "64ab99dcb76bfd863eba64c1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ab99dcb76bfd863eba64c1/UBXwDPx17X-gl-SzBPvrc.jpeg",
            "isPro": false,
            "fullname": "TY.Zheng",
            "user": "aaabiao",
            "type": "user"
          },
          "name": "Tianyu Zheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-21T09:58:24.002Z",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae59",
          "name": "Kang Zhu",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae5a",
          "user": {
            "_id": "6417d9ea8f689506e7148417",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6417d9ea8f689506e7148417/bAYcruWNw4WvmuQcGgcwC.jpeg",
            "isPro": false,
            "fullname": "minghao",
            "user": "Liam-Liu",
            "type": "user"
          },
          "name": "Minghao Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-21T09:58:25.894Z",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae5b",
          "name": "Yiming Liang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae5c",
          "name": "Xiaolong Jin",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae5d",
          "name": "Zhenlin Wei",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae5e",
          "user": {
            "_id": "610b70452719facd4ea85e28",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/610b70452719facd4ea85e28/S7nMy7D0Rxq0VIVblhYDG.jpeg",
            "isPro": false,
            "fullname": "Chujie Zheng",
            "user": "chujiezheng",
            "type": "user"
          },
          "name": "Chujie Zheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-21T09:58:34.124Z",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae5f",
          "name": "Kaixing Deng",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae60",
          "name": "Shuyue Guo",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae61",
          "name": "Shian Jia",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae62",
          "name": "Sichao Jiang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae63",
          "name": "Yiyan Liao",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae64",
          "name": "Rui Li",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae65",
          "name": "Qinrui Li",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae66",
          "name": "Sirun Li",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae67",
          "name": "Yizhi Li",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae68",
          "name": "Yunwen Li",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae69",
          "name": "Dehua Ma",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae6a",
          "user": {
            "_id": "64de37ee5e192985054be575",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64de37ee5e192985054be575/fVV7JQMtp_J3uFqszJJHH.jpeg",
            "isPro": false,
            "fullname": "Yuansheng Ni",
            "user": "yuanshengni",
            "type": "user"
          },
          "name": "Yuansheng Ni",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-21T09:58:30.371Z",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae6b",
          "name": "Haoran Que",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae6c",
          "user": {
            "_id": "64560618bfdf9c63ce2d658a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64560618bfdf9c63ce2d658a/GVBWU4yNzRsjdyzKT3z3B.jpeg",
            "isPro": false,
            "fullname": "Mathsion Wong",
            "user": "QiYao-Wang",
            "type": "user"
          },
          "name": "Qiyao Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-21T09:58:28.639Z",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae6d",
          "name": "Zhoufutu Wen",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae6e",
          "name": "Siwei Wu",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae6f",
          "name": "Tianshun Xing",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae70",
          "name": "Ming Xu",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae71",
          "name": "Zhenzhu Yang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae72",
          "name": "Zekun Moore Wang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae73",
          "name": "Junting Zhou",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae74",
          "name": "Yuelin Bai",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae75",
          "name": "Xingyuan Bu",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae76",
          "name": "Chenglin Cai",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae77",
          "name": "Liang Chen",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae78",
          "name": "Yifan Chen",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae79",
          "name": "Chengtuo Cheng",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae7a",
          "name": "Tianhao Cheng",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae7b",
          "name": "Keyi Ding",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae7c",
          "name": "Siming Huang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae7d",
          "name": "Yun Huang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae7e",
          "name": "Yaoru Li",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae7f",
          "name": "Yizhe Li",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae80",
          "name": "Zhaoqun Li",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae81",
          "name": "Tianhao Liang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae82",
          "name": "Chengdong Lin",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae83",
          "name": "Hongquan Lin",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae84",
          "name": "Yinghao Ma",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae85",
          "name": "Zhongyuan Peng",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae86",
          "user": {
            "_id": "65adda5299c3bd19c74d6a8d",
            "avatarUrl": "/avatars/1ce504b64ab60f375b235ebaf81cafd6.svg",
            "isPro": false,
            "fullname": "PENG ZIFAN",
            "user": "Ziffer",
            "type": "user"
          },
          "name": "Zifan Peng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-21T09:58:20.429Z",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae87",
          "name": "Qige Qi",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae88",
          "name": "Shi Qiu",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae89",
          "name": "Xingwei Qu",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae8a",
          "name": "Yizhou Tan",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae8b",
          "name": "Zili Wang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae8c",
          "name": "Chenqing Wang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae8d",
          "name": "Hao Wang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae8e",
          "name": "Yiya Wang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae8f",
          "name": "Yubo Wang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae90",
          "name": "Jiajun Xu",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae91",
          "name": "Kexin Yang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae92",
          "name": "Ruibin Yuan",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae93",
          "name": "Yuanhao Yue",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae94",
          "name": "Tianyang Zhan",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae95",
          "name": "Chun Zhang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae96",
          "name": "Jingyang Zhang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae97",
          "name": "Xiyue Zhang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae98",
          "name": "Xingjian Zhang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae99",
          "name": "Yue Zhang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae9a",
          "name": "Yongchi Zhao",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae9b",
          "name": "Xiangyu Zheng",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae9c",
          "name": "Chenghua Zhong",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae9d",
          "name": "Yang Gao",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae9e",
          "name": "Zhoujun Li",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae9f",
          "name": "Dayiheng Liu",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aea0",
          "user": {
            "_id": "612ee6a7b960e78c6d2319d4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/612ee6a7b960e78c6d2319d4/2Hu9BaAyXbyh1vt0v1Qui.jpeg",
            "isPro": false,
            "fullname": "Qian Liu",
            "user": "SivilTaram",
            "type": "user"
          },
          "name": "Qian Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-21T09:58:32.399Z",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aea1",
          "name": "Tianyu Liu",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aea2",
          "name": "Shiwen Ni",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aea3",
          "name": "Junran Peng",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aea4",
          "name": "Yujia Qin",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aea5",
          "name": "Wenbo Su",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aea6",
          "name": "Guoyin Wang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aea7",
          "name": "Shi Wang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aea8",
          "name": "Jian Yang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aea9",
          "name": "Min Yang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aeaa",
          "name": "Meng Cao",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aeab",
          "name": "Xiang Yue",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aeac",
          "name": "Zhaoxiang Zhang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aead",
          "name": "Wangchunshu Zhou",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aeae",
          "user": {
            "_id": "65377c30e48353201e6fdda0",
            "avatarUrl": "/avatars/a8f803b6f2e598eaee9c52c0d2ddfc16.svg",
            "isPro": false,
            "fullname": "Jiaheng Liu",
            "user": "CheeryLJH",
            "type": "user"
          },
          "name": "Jiaheng Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-21T09:58:22.185Z",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aeaf",
          "name": "Qunshu Lin",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aeb0",
          "name": "Wenhao Huang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aeb1",
          "name": "Ge Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T17:05:58.000Z",
      "title": "SuperGPQA : 285 Départements d'Études Évaluation de LLM Normalisée",
      "summary": "Les modèles de langage grand (LLMs) ont démontré un comportement impressionnant dans les domaines principaux de la science, comme la mathématique, la physique et la science de l'informatique. Cependant, le savoir humain comprend plus de 200 domaines spécialisés et transcende les référentiels actuels. Les LLMs ont été évalués insuffisamment dans de nombreuses spécialités dans des domaines tels que la production, l'agriculture et les services. Pour résoudre ces lacunes, nous présentons SuperGPQA, un benchmark strict qui évalue le savoir doctoral dans 285 domaines et la capacité d'inférence. Notre benchmark utilise un système de filtrage de collaboration entre humains et LLMs pour les réponses et la rétroaction des spécialistes, excluant seulement les questions simples ou incertaines. Les résultats des expériences montrent clairement que les plus avancés LLMs ont un grand potentiel d'amélioration dans divers domaines de connaissance et qu'il y a une différence claire entre les modèles actuels et l'intelligence artificielle. De plus, nous fournissons une méthodologie concrète de gestion des processus de labellisation grand, qui inclut un système de collaboration entre 80 experts spécialisés et LLMs, pour soutenir futurs projets de recherche.",
      "upvotes": 61,
      "discussionId": "67b7efc66348a1df80a8afc8"
    },
    "publishedAt": "2025-02-20T22:15:33.133Z",
    "title": "SuperGPQA: Scaling LLM Evaluation across 285 Graduate Disciplines",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14739.png",
    "numComments": 4,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6161
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.14786",
      "authors": [
        {
          "_id": "67b7ed0d58f6b70b18dda7b4",
          "name": "Michael Tschannen",
          "hidden": false
        },
        {
          "_id": "67b7ed0d58f6b70b18dda7b5",
          "name": "Alexey Gritsenko",
          "hidden": false
        },
        {
          "_id": "67b7ed0d58f6b70b18dda7b6",
          "name": "Xiao Wang",
          "hidden": false
        },
        {
          "_id": "67b7ed0d58f6b70b18dda7b7",
          "name": "Muhammad Ferjad Naeem",
          "hidden": false
        },
        {
          "_id": "67b7ed0d58f6b70b18dda7b8",
          "name": "Ibrahim Alabdulmohsin",
          "hidden": false
        },
        {
          "_id": "67b7ed0d58f6b70b18dda7b9",
          "name": "Nikhil Parthasarathy",
          "hidden": false
        },
        {
          "_id": "67b7ed0d58f6b70b18dda7ba",
          "name": "Talfan Evans",
          "hidden": false
        },
        {
          "_id": "67b7ed0d58f6b70b18dda7bb",
          "name": "Lucas Beyer",
          "hidden": false
        },
        {
          "_id": "67b7ed0d58f6b70b18dda7bc",
          "name": "Ye Xia",
          "hidden": false
        },
        {
          "_id": "67b7ed0d58f6b70b18dda7bd",
          "name": "Basil Mustafa",
          "hidden": false
        },
        {
          "_id": "67b7ed0d58f6b70b18dda7be",
          "name": "Olivier Hénaff",
          "hidden": false
        },
        {
          "_id": "67b7ed0d58f6b70b18dda7bf",
          "name": "Jeremiah Harmsen",
          "hidden": false
        },
        {
          "_id": "67b7ed0d58f6b70b18dda7c0",
          "name": "Andreas Steiner",
          "hidden": false
        },
        {
          "_id": "67b7ed0d58f6b70b18dda7c1",
          "name": "Xiaohua Zhai",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T18:08:29.000Z",
      "title": "SigLIP 2 : Langue de vision multilingue qui améliore la compréhension grammaticale, la localisation et les caractéristiques denses.",
      "summary": "Introducing le Graphite Prime 2, une nouvelle famille d'encodeurs de vision-langage multilingue. Ce modèle est développé sur la base de la réussite du Graphite Prime original. La deuxième génération a été adaptée pour intégrer plusieurs techniques avancées développées en parallèle avec les objectifs d'apprentissage visuel-textuel originels. Cela comprend la prédiction à l'aide d'un capteur, la distillation autonome (prédiction masquée) et la calibration des données en ligne. Ces modifications permettent au modèle Graphite Prime 2 de surpasser le Graphite Prime original dans toutes les caractéristiques clés. Notamment, il précise notamment dans la classification de classes à partie, la recherche visuelle-textuelle et l'extraction de représentations visuelles pour les modèles de vision-langage (VLM). De plus, la nouvelle recette d'apprentissage montre des améliorations significatives tant pour les tâches de localisation que pour les tâches de prédiction proche. De plus, des modèles qui maintiennent les aspects ratios d'entrée et s'adaptent à diverses tailles ont également été entraînés. Entraînés sur des données diverses incluant des technologies de capteur, ces modèles améliorent la compréhension du langage et l'équité. Pour offrir aux utilisateurs la flexibilité de trouver un équilibre entre les coûts d'inférence et la performance, nous publions quatre points de sauvegarde de modèles de tailles différentes : ViT-B (86M), L (303M), So400m (400M) et g (1B).",
      "upvotes": 50,
      "discussionId": "67b7ed0e58f6b70b18dda7f4"
    },
    "publishedAt": "2025-02-20T22:33:22.039Z",
    "title": "SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic Understanding, Localization, and Dense Features",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14786.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6161
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.14382",
      "authors": [
        {
          "_id": "67b7ed3e58f6b70b18ddb4bc",
          "name": "Dacheng Li",
          "hidden": false
        },
        {
          "_id": "67b7ed3e58f6b70b18ddb4bd",
          "user": {
            "_id": "64ebbae6895a36ab28de811a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ebbae6895a36ab28de811a/gBiaQP4paS4L13eu-yRm7.jpeg",
            "isPro": false,
            "fullname": "Shiyi Cao",
            "user": "eva98",
            "type": "user"
          },
          "name": "Shiyi Cao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-21T09:58:43.358Z",
          "hidden": false
        },
        {
          "_id": "67b7ed3e58f6b70b18ddb4be",
          "name": "Chengkun Cao",
          "hidden": false
        },
        {
          "_id": "67b7ed3e58f6b70b18ddb4bf",
          "name": "Xiuyu Li",
          "hidden": false
        },
        {
          "_id": "67b7ed3e58f6b70b18ddb4c0",
          "name": "Shangyin Tan",
          "hidden": false
        },
        {
          "_id": "67b7ed3e58f6b70b18ddb4c1",
          "name": "Kurt Keutzer",
          "hidden": false
        },
        {
          "_id": "67b7ed3e58f6b70b18ddb4c2",
          "name": "Jiarong Xing",
          "hidden": false
        },
        {
          "_id": "67b7ed3e58f6b70b18ddb4c3",
          "name": "Joseph E. Gonzalez",
          "hidden": false
        },
        {
          "_id": "67b7ed3e58f6b70b18ddb4c4",
          "name": "Ion Stoica",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T09:18:53.000Z",
      "title": "S*: Programmation des tests de synchronisation du temps de génération de code",
      "summary": "Augmenter la quantité de calculs pendant le processus de vérification montre des résultats souhaités dans plusieurs domaines des modèles de langage grands (LLM), mais l'investigation mathématique sur la génération de code est insuffisante. Dans cet article, nous proposons le premier cadre d'accélération de temps de vérification hybride S*, qui améliore significativement la couverture et la précision de la sélection de code généré. S* dépasse les limites du paradigme actuel d'accélération en ajoutant l'accélération par ordre. De plus, il utilise une nouvelle structure de sélection pour générer des entrées adaptatives à des entrées hostiles et combine l'information basée sur l'exécution pour renforcer la spécification de solutions précises. Nous vérifions 12 modèles de langage et de logique d'une grande échelle, et obtenons les résultats suivants : 1) S* montre un améliorament constant de rendement selon la famille et le taille du modèle, dépassant le modèle GPT-4o-mini. 2) S* peut dépasser les modèles logiques, et GPT-4o-mini dépasse a-preview dans LiveCodeBench d'un marge de 3,7 %. 3) S* améliore encore plus les modèles logiques plus avancés, et DeepSeek-R1-Distill-Qwen-32B atteint un 85,7 % dans LiveCodeBench, proche du 88,5 % d'a1 (haut). Le code est disponible sur https://github.com/NovaSky-AI/SkyThought.",
      "upvotes": 29,
      "discussionId": "67b7ed3f58f6b70b18ddb510"
    },
    "publishedAt": "2025-02-20T22:04:42.635Z",
    "title": "S*: Test Time Scaling for Code Generation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14382.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6161
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.14258",
      "authors": [
        {
          "_id": "67b7fa96c3f48f8b3fc632fe",
          "name": "Yein Park",
          "hidden": false
        },
        {
          "_id": "67b7fa96c3f48f8b3fc632ff",
          "name": "Chanwoong Yoon",
          "hidden": false
        },
        {
          "_id": "67b7fa96c3f48f8b3fc63300",
          "name": "Jungwoo Park",
          "hidden": false
        },
        {
          "_id": "67b7fa96c3f48f8b3fc63301",
          "name": "Minbyul Jeong",
          "hidden": false
        },
        {
          "_id": "67b7fa96c3f48f8b3fc63302",
          "name": "Jaewoo Kang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T04:52:05.000Z",
      "title": "Où est le temps ? Temps dans la mémoire : lieu où le modèle de langage stocke l'information temporelle",
      "summary": "L'efficacité des modèles de langue pour la gestion de faits a été largement étudiée, mais leur gestion de faits qui évoluent au cours du temps a été peu explorée. Nous avons découvert des « têtes d'attention temporelles » ('Temporal Heads') pour le traitement des connaissances temporelles. Ces têtes existent dans tous les modèles, bien que leur localisation soit différente, mais répondent de manière différente à la nature du savoir ou à l'époque relative. Désactiver ces têtes diminue la capacité spécifique du savoir temporel, mais ne touche pas l'invariance temporelle ni la capacité de résolution de problèmes. De plus, ces têtes sont activées également par des nombres tels que '2004' ou des pronoms textuels tels que '… de l'année', ce qui montre qu'elles représentent plus que des simples nombres. De plus, le savoir temporel peut être édité en ajustant les valeurs de ces têtes temporelles.",
      "upvotes": 17,
      "discussionId": "67b7fa9ac3f48f8b3fc63452"
    },
    "publishedAt": "2025-02-20T23:02:42.672Z",
    "title": "Does Time Have Its Place? Temporal Heads: Where Language Models Recall Time-specific Information",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14258.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64587be872b60ae7a3817858",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64587be872b60ae7a3817858/BbdOOxOCEzWTvEpkWp8MM.png",
      "fullname": "Minbyul Jeong",
      "name": "Minbyul",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.14834",
      "authors": [
        {
          "_id": "67b7f3c4d00e69f10cff219e",
          "name": "Shangqing Tu",
          "hidden": false
        },
        {
          "_id": "67b7f3c4d00e69f10cff219f",
          "name": "Yucheng Wang",
          "hidden": false
        },
        {
          "_id": "67b7f3c4d00e69f10cff21a0",
          "name": "Daniel Zhang-Li",
          "hidden": false
        },
        {
          "_id": "67b7f3c4d00e69f10cff21a1",
          "name": "Yushi Bai",
          "hidden": false
        },
        {
          "_id": "67b7f3c4d00e69f10cff21a2",
          "name": "Jifan Yu",
          "hidden": false
        },
        {
          "_id": "67b7f3c4d00e69f10cff21a3",
          "name": "Yuhao Wu",
          "hidden": false
        },
        {
          "_id": "67b7f3c4d00e69f10cff21a4",
          "name": "Lei Hou",
          "hidden": false
        },
        {
          "_id": "67b7f3c4d00e69f10cff21a5",
          "name": "Huiqin Liu",
          "hidden": false
        },
        {
          "_id": "67b7f3c4d00e69f10cff21a6",
          "name": "Zhiyuan Liu",
          "hidden": false
        },
        {
          "_id": "67b7f3c4d00e69f10cff21a7",
          "name": "Bin Xu",
          "hidden": false
        },
        {
          "_id": "67b7f3c4d00e69f10cff21a8",
          "name": "Juanzi Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T18:47:36.000Z",
      "title": "LongWriter-V : Modèle de langage visuel permettant la génération à long terme et de haute qualité.",
      "summary": "Les LVLMs de aujourd'hui peuvent traiter une longueur de contexte de 128k de tokens d'image et de texte, mais rencontrent des difficultés à générer des sorties de plus de 1,000 mots sur une page. L'un des principaux limitants est la faible disponibilité d'exemples de sortie longue lors de l'entraînement supervisé (SFT). Pour aborder ce problème, nous présentons LongWriter-V-22k, qui comprend 22,158 ensembles d'entraînement SFT, chacun avec plusieurs images d'entrée, instructions et sorties correspondantes pouvant s'étendre de 0 à 10,000 mots. Pour assurer une correspondance précise entre l'image d'entrée et la sortie longue, nous appliquons l'Optimisation de Préférence Directe (DPO) dans le modèle SFT. Étant donné que la collecte de rétroaction humaine pour des sorties longues (par exemple, 3,000 mots) est coûteuse, nous proposons IterDPO, qui divise les sorties longues et utilise la modification des registres pour former des paires de sorties originales et de préférences. De plus, nous avons développé MMLongBench-Write, un benchmark pour évaluer la capacité de génération longue des VLMs, qui offre 6 tâches. Notre modèle de 7B paramètres a été entraîné avec LongWriter-V-22k et IterDPO, montrant un performance notable dans ce benchmark et dépassant d'autres grands modèles publics comme GPT-4o. Les codes et les données sont disponibles sur https://github.com/THU-KEG/LongWriter-V.",
      "upvotes": 15,
      "discussionId": "67b7f3c7d00e69f10cff2258"
    },
    "publishedAt": "2025-02-20T22:39:21.551Z",
    "title": "LongWriter-V: Enabling Ultra-Long and High-Fidelity Generation in Vision-Language Models",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/648c48d8c0ddeee6df5b6d22/8AYx7CcK4CT6flX3nRDlB.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14834.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "648c48d8c0ddeee6df5b6d22",
      "avatarUrl": "/avatars/8706b0b16dfc332b96c91d3ced31bd0b.svg",
      "fullname": "Shangqing Tu",
      "name": "tsq2000",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.14768",
      "authors": [
        {
          "_id": "67b7f08c357c2729ac20a81b",
          "name": "Tian Xie",
          "hidden": false
        },
        {
          "_id": "67b7f08c357c2729ac20a81c",
          "name": "Zitian Gao",
          "hidden": false
        },
        {
          "_id": "67b7f08c357c2729ac20a81d",
          "name": "Qingnan Ren",
          "hidden": false
        },
        {
          "_id": "67b7f08c357c2729ac20a81e",
          "name": "Haoming Luo",
          "hidden": false
        },
        {
          "_id": "67b7f08c357c2729ac20a81f",
          "name": "Yuqian Hong",
          "hidden": false
        },
        {
          "_id": "67b7f08c357c2729ac20a820",
          "name": "Bryan Dai",
          "hidden": false
        },
        {
          "_id": "67b7f08c357c2729ac20a821",
          "name": "Joey Zhou",
          "hidden": false
        },
        {
          "_id": "67b7f08c357c2729ac20a822",
          "name": "Kai Qiu",
          "hidden": false
        },
        {
          "_id": "67b7f08c357c2729ac20a823",
          "name": "Zhirong Wu",
          "hidden": false
        },
        {
          "_id": "67b7f08c357c2729ac20a824",
          "name": "Chong Luo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T17:49:26.000Z",
      "title": "Logic-RL : Libération de la Logique dans l'Apprentissage par Référence des Modèles de Langue de Niveau Élevé",
      "summary": "Basé sur le succès de DeepSeek-R1, on explore la possibilité d'appliquer l'apprentissage par renforcement basé sur les règles (RL) dans des modèles de logique à grande échelle. Pour analyser le processus dynamique des modèles de logique, on utilise comme données d'entraînement les \"Synthetic Logic Puzzles\" qui permettent de vérifier la complexité contrôlable et la breveté de la réponse. On fournit des contributions techniques importantes pour un apprentissage efficace et stable de RL : un système de prompt qui focalise le pensée et la réponse, une fonction de récompense stricte qui valorise des solutions courtes, et un simple recette d'entraînement qui aide à atteindre une convergence stable. Le modèle de 7B développe des habiletés avancées en technologies de logique, ainsi que dans le corpus de logique : réflexion, vérification, résumé, entre autres. En particulier, après avoir été entraîné sur 5K problèmes de logique, il montre une capacité de généralisation dans des cadres de tests mathématiques difficiles tels que AIME et AMC.",
      "upvotes": 14,
      "discussionId": "67b7f08e357c2729ac20a88f"
    },
    "publishedAt": "2025-02-20T22:19:05.902Z",
    "title": "Logic-RL: Unleashing LLM Reasoning with Rule-Based Reinforcement Learning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14768.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6161
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.14282",
      "authors": [
        {
          "_id": "67b7f5587f4d732dc469270e",
          "name": "Haowei Liu",
          "hidden": false
        },
        {
          "_id": "67b7f5587f4d732dc469270f",
          "name": "Xi Zhang",
          "hidden": false
        },
        {
          "_id": "67b7f5587f4d732dc4692710",
          "name": "Haiyang Xu",
          "hidden": false
        },
        {
          "_id": "67b7f5587f4d732dc4692711",
          "name": "Yuyang Wanyan",
          "hidden": false
        },
        {
          "_id": "67b7f5587f4d732dc4692712",
          "name": "Junyang Wang",
          "hidden": false
        },
        {
          "_id": "67b7f5587f4d732dc4692713",
          "name": "Ming Yan",
          "hidden": false
        },
        {
          "_id": "67b7f5587f4d732dc4692714",
          "name": "Ji Zhang",
          "hidden": false
        },
        {
          "_id": "67b7f5587f4d732dc4692715",
          "name": "Chunfeng Yuan",
          "hidden": false
        },
        {
          "_id": "67b7f5587f4d732dc4692716",
          "name": "Changsheng Xu",
          "hidden": false
        },
        {
          "_id": "67b7f5587f4d732dc4692717",
          "name": "Weiming Hu",
          "hidden": false
        },
        {
          "_id": "67b7f5587f4d732dc4692718",
          "name": "Fei Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T05:41:55.000Z",
      "title": "PC-Agent : Agent Multi-Agent Hierarchical pour la Coopération et l'Automatisation des tâches Complexes sur les Ordinateurs",
      "summary": "Dans le domaine des agents d'interface graphique basés sur des modèles multi-modal multi-frame (MLLM), le scénario de PC présente des caractéristiques d'un environnement hautement interchangeable, incluant des flux de travail complexes entre applications, ce qui contraste avec les dispositifs mobiles. Pour aborder ces défis, nous proposons un cadre de travail d'agent hyperagent appelé PC-Agent. En particulier, à partir d'une perspective visuelle, nous avons conçu un module de reconnaissance visuelle active (APM) pour surmonter la capacité actuelle des MLLM de négliger le contenu des captures de panneau. À partir d'une perspective de prise de décision, nous proposons une architecture de collaboration de plusieurs agents hybrides qui divise le processus de prise de décision en niveaux d'ordre, sous-tâches et actions, pour gérer efficacement des instructions complexes et des sous-tâches interdépendants. Dans cette architecture, trois agents sont installés : Manager pour la décomposition des ordres, Progress pour le suivi du progrès et Decision pour la prise de décision à chaque étape. De plus, nous introduisons un agent de réflexion pour éviter les erreurs et ajuster le processus. Nous présentons également un nouveau benchmark appelé PC-Eval, qui comprend 25 commandes complexes de la vie réelle. Les résultats des expériences sur PC-Eval montrent que notre PC-Agent a augmenté la taux de réussite des tâches d'environ 32% par rapport aux méthodes précédentes. Le code est disponible pour l'utilisation publique.",
      "upvotes": 11,
      "discussionId": "67b7f55b7f4d732dc46927c1"
    },
    "publishedAt": "2025-02-20T22:39:48.180Z",
    "title": "PC-Agent: A Hierarchical Multi-Agent Collaboration Framework for Complex Task Automation on PC",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/645b10e80c73ea27d13f7aca/feg9OYb4onJJermpjc6nh.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14282.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645b10e80c73ea27d13f7aca",
      "avatarUrl": "/avatars/95e565306472a15067440b5b43e07a6f.svg",
      "fullname": "xuhaiyang",
      "name": "xhyandwyy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.14499",
      "authors": [
        {
          "_id": "67b7ee1dfedfe971271dcca0",
          "user": {
            "_id": "6114c9fae7a2566ae7d1a1a7",
            "avatarUrl": "/avatars/c71ab1850322fcf5ef239cb8d31cb137.svg",
            "isPro": false,
            "fullname": "Deepak Nathani",
            "user": "dnathani",
            "type": "user"
          },
          "name": "Deepak Nathani",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-02-21T07:20:46.836Z",
          "hidden": false
        },
        {
          "_id": "67b7ee1dfedfe971271dcca1",
          "name": "Lovish Madaan",
          "hidden": false
        },
        {
          "_id": "67b7ee1dfedfe971271dcca2",
          "name": "Nicholas Roberts",
          "hidden": false
        },
        {
          "_id": "67b7ee1dfedfe971271dcca3",
          "name": "Nikolay Bashlykov",
          "hidden": false
        },
        {
          "_id": "67b7ee1dfedfe971271dcca4",
          "name": "Ajay Menon",
          "hidden": false
        },
        {
          "_id": "67b7ee1dfedfe971271dcca5",
          "name": "Vincent Moens",
          "hidden": false
        },
        {
          "_id": "67b7ee1dfedfe971271dcca6",
          "name": "Amar Budhiraja",
          "hidden": false
        },
        {
          "_id": "67b7ee1dfedfe971271dcca7",
          "name": "Despoina Magka",
          "hidden": false
        },
        {
          "_id": "67b7ee1dfedfe971271dcca8",
          "name": "Vladislav Vorotilov",
          "hidden": false
        },
        {
          "_id": "67b7ee1dfedfe971271dcca9",
          "name": "Gaurav Chaurasia",
          "hidden": false
        },
        {
          "_id": "67b7ee1dfedfe971271dccaa",
          "name": "Dieuwke Hupkes",
          "hidden": false
        },
        {
          "_id": "67b7ee1dfedfe971271dccab",
          "name": "Ricardo Silveira Cabral",
          "hidden": false
        },
        {
          "_id": "67b7ee1dfedfe971271dccac",
          "name": "Tatiana Shavrina",
          "hidden": false
        },
        {
          "_id": "67b7ee1dfedfe971271dccad",
          "name": "Jakob Foerster",
          "hidden": false
        },
        {
          "_id": "67b7ee1dfedfe971271dccae",
          "name": "Yoram Bachrach",
          "hidden": false
        },
        {
          "_id": "67b7ee1dfedfe971271dccaf",
          "name": "William Yang Wang",
          "hidden": false
        },
        {
          "_id": "67b7ee1dfedfe971271dccb0",
          "user": {
            "_id": "633e94793a17ab61de8e2b9c",
            "avatarUrl": "/avatars/5f2f58ddeed211393660ada6b135f0d5.svg",
            "isPro": false,
            "fullname": "Roberta Raileanu",
            "user": "rraileanu",
            "type": "user"
          },
          "name": "Roberta Raileanu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-02-21T03:08:15.471Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T12:28:23.000Z",
      "title": "MLGym : Nouveau cadre de travail et de référence pour le développement d'agents AI",
      "summary": "Meta MLGym et MLGym-Bench sont les nouveaux cadres de travail et de évaluation. Ces cadres de travail et d'évaluation sont les premiers dans l'étude des tâches d'intelligence artificielle pour l'évaluation et le développement d'agents de langage de machine (LLMs). Tout d'abord, MLGym est le premier environnement de Gym pour des tâches d'apprentissage automatique (ML). De cette manière, on peut étudier les algorithmes d'apprentissage par renforcement (RL) pour l'apprentissage de ces agents. MLGym-Bench est composé de 13 tâches d'étude d'intelligence artificielle ouvertes dans différentes domaines comme la vision par ordinateur, le traitement du langage naturel, l'apprentissage par renforcement, la théorie des jeux, entre autres. Pour résoudre ces tâches, on nécessite des technologies de recherche en IA réalistes qui incluent la génération de nouvelles idées et hypothèses, la génération et le traitement de données, la mise en œuvre de méthodes ML, l'entraînement de modèles, l'exécution d'expériences, l'analyse des résultats et la répétition de ce processus pour améliorer des tâches spécifiques. Nous évaluons plusieurs modèles avancés comme Claude-3.5-Sonnet, Llama-3.1 405B, GPT-4o, o1-preview et Gemini-1.5 Pro dans notre cadre d'évaluation. Notre cadre de travail MLGym facilite l'ajout de nouvelles tâches, l'intégration et l'évaluation de modèles ou agents, la génération de données synthétiques à grande échelle et le développement de nouveaux algorithmes pour l'apprentissage d'agents dans des tâches de recherche en intelligence artificielle. Notre objectif est d'ouvrir ce cadre de travail et de cadre d'évaluation en tant que code ouvert pour encourager la recherche future qui favorise le développement de capacités de recherche d'agents de langage de machine.",
      "upvotes": 9,
      "discussionId": "67b7ee1ffedfe971271dcd3a"
    },
    "publishedAt": "2025-02-20T22:08:38.225Z",
    "title": "MLGym: A New Framework and Benchmark for Advancing AI Research Agents",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14499.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6161
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.14844",
      "authors": [
        {
          "_id": "67b7f5ee8b3dff28b749be78",
          "name": "Rameen Abdal",
          "hidden": false
        },
        {
          "_id": "67b7f5ee8b3dff28b749be79",
          "name": "Or Patashnik",
          "hidden": false
        },
        {
          "_id": "67b7f5ee8b3dff28b749be7a",
          "name": "Ivan Skorokhodov",
          "hidden": false
        },
        {
          "_id": "67b7f5ee8b3dff28b749be7b",
          "name": "Willi Menapace",
          "hidden": false
        },
        {
          "_id": "67b7f5ee8b3dff28b749be7c",
          "name": "Aliaksandr Siarohin",
          "hidden": false
        },
        {
          "_id": "67b7f5ee8b3dff28b749be7d",
          "name": "Sergey Tulyakov",
          "hidden": false
        },
        {
          "_id": "67b7f5ee8b3dff28b749be7e",
          "name": "Daniel Cohen-Or",
          "hidden": false
        },
        {
          "_id": "67b7f5ee8b3dff28b749be7f",
          "name": "Kfir Aberman",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T18:53:39.000Z",
      "title": "Personalisation des Concepts Dynamiques à Partir de Soles Vidéos",
      "summary": "Le progrès dans les modèles d'images à partir de textes personnalisés est impressionnant, mais l'extension de cette approche aux modèles de vidéo à partir de texte présente des problèmes spéciaux. Contrairement aux concepts statiques, la personnalisation de modèles de vidéo peut comprendre des concepts dynamiques, y compris les mouvements. Dans cet article, nous utilisons un nouveau cadre de travail appelé \"Set-and-Sequence\" pour la personnalisation de modèles de vidéo basés sur les Transformers de Diffusion (DiTs) qui utilisent des concepts dynamiques. Notre approche ne sépare pas explicitement les caractéristiques spatiales et temporelles, mais aborde un espace de poids spatio-temporel. Cette approche se développe en deux étapes principales : d'abord, les couches d'Adaptation de Rang Bas (LoRA) sont ajustées en utilisant les frames du vidéo sans ordre, et la visualisation basée sur LoRA de l'identité est entraînée pour représenter l'extérieur. Ensuite, tout en maintenant la LoRA de l'identité fixe, des résidus de mouvement sont ajoutés pour ajuster le vidéo complet et comprendre la dynamique du mouvement. Le cadre \"Set-and-Sequence\" intègre efficacement des concepts dynamiques dans la zone de sortie du modèle de vidéo, ouvre des possibilités d'édition sans précédent et d'organisation, et établit de nouveaux standards pour la personnalisation de concepts dynamiques.",
      "upvotes": 8,
      "discussionId": "67b7f5f18b3dff28b749bf45"
    },
    "publishedAt": "2025-02-20T22:41:47.210Z",
    "title": "Dynamic Concepts Personalization from Single Videos",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14844.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6161
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.14372",
      "authors": [
        {
          "_id": "67b81870cc6b0136b3d84254",
          "user": {
            "_id": "6530a78069751712276d60ed",
            "avatarUrl": "/avatars/2ef4f16d0be557ed60c11d8dcef85f6f.svg",
            "isPro": false,
            "fullname": "Austin He",
            "user": "basil2115",
            "type": "user"
          },
          "name": "Austin Yubo He",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-02-21T06:30:16.645Z",
          "hidden": false
        },
        {
          "_id": "67b81870cc6b0136b3d84255",
          "name": "Zi-Wen Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T09:05:34.000Z",
      "title": "Recherche de Codes de Correction d'Erreurs Quantiques pour l'Implémentation d'Efficacité Élevée et de Poids Bas avec Apprentissage par Reforcement",
      "summary": "La scalabilité de la computation quantique perdue est supposée dépendre des codes de correction d'erreurs quantiques. En cherchant une efficacité dans la tolérance aux erreurs liées à la correction d'erreurs quantiques, les paramètres importants du code sont les poids de mesure pour identifier les erreurs : un poids élevé de mesure augmente le coût d'implémentation et peut provoquer des erreurs, ce qui est donc crucial d'optimiser le poids de mesure dans le design du code. Cela a augmenté l'intérêt pour des codes quantiques de faible densité de parité (qLDPC). Ces études ont principalement porté sur les propriétés des codes de haute densité de parité (LDPC). Dans ce travail, on présente un approche flexible et efficace basée sur l'apprentissage par renforcement (RL) pour la réduction des poids dans les codes RL. Cela permet de générer de nouveaux codes de faible poids dans des paramètres pratiques, démontrant un grand avancement par rapport aux codes actuels. Par exemple, on peut réduire la surcharge de la cubita de la physique dans les codes de poids 6 d'une ordre de grandeur par rapport aux résultats actuels, ce qui ouvre un espace pratique pour des expériences futures. De plus, on étudie les interactions entre les paramètres du code en utilisant un cadre RL, offrant de nouvelles perspectives sur la possibilité et l'efficacité des stratégies de codage. En général, les résultats de cette étude démontrent que l'RL peut aborder des défis complexes dans la recherche de codes quantiques et accélérer la mise en œuvre pratique de la technologie de computation quantique perdue.",
      "upvotes": 7,
      "discussionId": "67b81873cc6b0136b3d8430a"
    },
    "publishedAt": "2025-02-21T01:11:34.971Z",
    "title": "Discovering highly efficient low-weight quantum error-correcting codes with reinforcement learning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14372.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6530a78069751712276d60ed",
      "avatarUrl": "/avatars/2ef4f16d0be557ed60c11d8dcef85f6f.svg",
      "fullname": "Austin He",
      "name": "basil2115",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.14846",
      "authors": [
        {
          "_id": "67b7f4f1b15c19d57189fc5e",
          "name": "Yue Yang",
          "hidden": false
        },
        {
          "_id": "67b7f4f1b15c19d57189fc5f",
          "name": "Ajay Patel",
          "hidden": false
        },
        {
          "_id": "67b7f4f1b15c19d57189fc60",
          "name": "Matt Deitke",
          "hidden": false
        },
        {
          "_id": "67b7f4f1b15c19d57189fc61",
          "name": "Tanmay Gupta",
          "hidden": false
        },
        {
          "_id": "67b7f4f1b15c19d57189fc62",
          "name": "Luca Weihs",
          "hidden": false
        },
        {
          "_id": "67b7f4f1b15c19d57189fc63",
          "name": "Andrew Head",
          "hidden": false
        },
        {
          "_id": "67b7f4f1b15c19d57189fc64",
          "name": "Mark Yatskar",
          "hidden": false
        },
        {
          "_id": "67b7f4f1b15c19d57189fc65",
          "name": "Chris Callison-Burch",
          "hidden": false
        },
        {
          "_id": "67b7f4f1b15c19d57189fc66",
          "name": "Ranjay Krishna",
          "hidden": false
        },
        {
          "_id": "67b7f4f1b15c19d57189fc67",
          "name": "Aniruddha Kembhavi",
          "hidden": false
        },
        {
          "_id": "67b7f4f1b15c19d57189fc68",
          "name": "Christopher Clark",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T18:55:30.000Z",
      "title": "Guide de code pour la génération de données monomodaux de synthèse et d'échelle de compréhension des images riches en texte",
      "summary": "La prédiction sur des images et des textes riches, comme des graphiques et des documents, est un domaine d'application important pour les modèles de langue viso-linguistique (VLMs). Cependant, dans ces domaines, les VLMs rencontrent des défis en raison de l'insuffisance de données viso-linguistiques basées sur le texte. Pour aborder ces défis, nous présentons CoSyn, un cadre de travail qui exploite le pouvoir de génération de code de modèles de langue grands basés sur le texte (LLMs) pour créer automatiquement des données multimodales de texte synthétiques. En recevant un texte d'entrée qui décrit le domaine cible, CoSyn encourage un LLM pour générer du code (Python, HTML, LaTeX, etc.) pour rendre des images synthétiques. Grâce à la représentation textuelle basique des images synthétiques, CoSyn peut générer des données d'entraînement de haute qualité en utilisant uniquement du texte avec un LLM. Grâce à CoSyn, nous avons construit un ensemble de données de 400K images et de 2,7M lignes de données d'entraînement de langage viso-linguistique. Les expériences complètes sur 7 benchmarks montrent que notre modèle basé sur des données synthétiques atteint le meilleur rendement parmi les modèles open-source compétitifs et dépasse les modèles non publics tels que GPT-4V et Gemini 1.5 Flash. De plus, CoSyn peut générer des données de points synthétiques, ce qui démontre des potentiels pour le développement d'agents multimodals qui peuvent établir des points de référence dans les images d'entrée et fonctionner dans des environnements réels.",
      "upvotes": 5,
      "discussionId": "67b7f4f2b15c19d57189fc95"
    },
    "publishedAt": "2025-02-20T22:38:36.406Z",
    "title": "Scaling Text-Rich Image Understanding via Code-Guided Synthetic Multimodal Data Generation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14846.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6161
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.12853",
      "authors": [
        {
          "_id": "67b69b6717ccb022c6a95b38",
          "name": "Ruotian Ma",
          "hidden": false
        },
        {
          "_id": "67b69b6717ccb022c6a95b39",
          "name": "Peisong Wang",
          "hidden": false
        },
        {
          "_id": "67b69b6717ccb022c6a95b3a",
          "name": "Cheng Liu",
          "hidden": false
        },
        {
          "_id": "67b69b6717ccb022c6a95b3b",
          "name": "Xingyan Liu",
          "hidden": false
        },
        {
          "_id": "67b69b6717ccb022c6a95b3c",
          "name": "Jiaqi Chen",
          "hidden": false
        },
        {
          "_id": "67b69b6717ccb022c6a95b3d",
          "name": "Bang Zhang",
          "hidden": false
        },
        {
          "_id": "67b69b6717ccb022c6a95b3e",
          "name": "Xin Zhou",
          "hidden": false
        },
        {
          "_id": "67b69b6717ccb022c6a95b3f",
          "name": "Nan Du",
          "hidden": false
        },
        {
          "_id": "67b69b6717ccb022c6a95b40",
          "name": "Jia Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-18T13:40:22.000Z",
      "title": "S^2R : Apprentissage par Référence pour l'Authentification Automatique et l'Édition Automatique avec des Modèles de Langue de Grande Taille (LLM)",
      "summary": "Les dernières recherches ont démontré l'effet de l'échelle dans les tests des LLMs. Cependant, les méthodes généralement utilisées pour stimuler la capacité de pensée profonde dans les LLMs nécessitent de grands volumes de données et de efforts d'entraînement. D'autre part, les méthodes pour améliorer la capacité de pensée du modèle de base ne sont pas claires. Dans cette étude, nous proposons un cadre efficace appelé S^2R pour renforcer l'inférence des LLMs et entraîner un modèle qui effectue une auto-revue et un auto-ajustement pendant l'inférence. En particulier, initialement, nous utilisons un entraînement de régulation normative basé sur des données ajustées pour initialiser des actions de revue et d'ajustement itératifs. Ensuite, nous appliquons des apprentissages de renforcement au niveau des résultats et des processus pour renforcer davantage les habiletés de revue et d'ajustement, minimisant la demande en ressources et améliorant de manière adaptative le processus d'inférence. Nos résultats montrent une initialisation de 3.1k actions de revue et d'ajustement, avec un augmentation de la précision de Qwen2.5-math-7B de 51.0% à 81.6%, dépassant les modèles entraînés avec la même quantité de données longues. Les expériences et analyses étendues basées sur trois modèles de base et benchmarks à l'intérieur et à l'extérieur du domaine démontrent l'efficacité de S^2R. Notre code et nos données sont disponibles sur https://github.com/NineAbyss/S2R.",
      "upvotes": 4,
      "discussionId": "67b69b6817ccb022c6a95b6e"
    },
    "publishedAt": "2025-02-21T05:00:18.645Z",
    "title": "S$^2$R: Teaching LLMs to Self-verify and Self-correct via Reinforcement Learning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.12853.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "648294b2eb4befee378951c1",
      "avatarUrl": "/avatars/da5d8bf9d8662cc2ffa2c0de49bd66a3.svg",
      "fullname": "Ruotian Ma",
      "name": "vvibt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.14669",
      "authors": [
        {
          "_id": "67b7eeddaf9f1b1bd95b878b",
          "name": "Alan Dao",
          "hidden": false
        },
        {
          "_id": "67b7eeddaf9f1b1bd95b878c",
          "name": "Dinh Bach Vu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T16:05:18.000Z",
      "title": "AlphaMaze : GRPO comme un projet global pour améliorer la capacité de reconnaissance spatiale dans des modèles de langage grands en basant sur GRPO",
      "summary": "Les modèles de langage grands (LLMs) montrent des habiletés exceptionnelles dans le traitement du langage mais ont des problèmes lorsqu'ils doivent réaliser des tâches nécessitant un raisonnement visuel et spatiel. Dans cet article, un nouveau cadre d'entraînement à deux étapes est utilisé pour ajouter à des modèles généraux la capacité de raisonnement visuel dans l'explorateur de labyrinthes. Tout d'abord, un entraînement supervisé de fine-tuning (SFT) est appliqué sur un ensemble de données de représentations de labyrinthes statistiquement extraites pour enseigner au modèle à prédire les instructions de mouvement par étapes. Ensuite, la Politique d'Optimisation de Groupe Relative (GRPO), utilisée dans DeepSeekR1, est introduite. Cette politique utilise des fonctions de récompenses aléatoires pour améliorer les décisions continues du modèle et promouvoir des actions en considérant les contraintes temporelles. Les résultats des expériences sur des labyrinthes générés de manière synthétique montrent que le modèle de base ne peut pas explorer le labyrinthe, tandis que le modèle entraîné avec SFT atteint une précision de 86%, et l'entraînement supplémentaire avec GRPO augmente la précision à 93%. Une analyse qualitative montre que GRPO pousse une meilleure auto-régulation et révèle la possibilité de combler la breche entre les modèles de langage et les tâches visuelles spatielles avec notre approche. Ces résultats ont un sens pertinent pour les applications nécessitant une vision et un raisonnement continu dans des domaines comme l'ingénierie robotique, la navigation automatique et d'autres.",
      "upvotes": 4,
      "discussionId": "67b7eeddaf9f1b1bd95b87c8"
    },
    "publishedAt": "2025-02-20T22:11:45.130Z",
    "title": "AlphaMaze: Enhancing Large Language Models' Spatial Intelligence via GRPO",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14669.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6161
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.14377",
      "authors": [
        {
          "_id": "67b7f350357c2729ac216494",
          "name": "Ke Cao",
          "hidden": false
        },
        {
          "_id": "67b7f350357c2729ac216495",
          "name": "Jing Wang",
          "hidden": false
        },
        {
          "_id": "67b7f350357c2729ac216496",
          "name": "Ao Ma",
          "hidden": false
        },
        {
          "_id": "67b7f350357c2729ac216497",
          "name": "Jiasong Feng",
          "hidden": false
        },
        {
          "_id": "67b7f350357c2729ac216498",
          "name": "Zhanjie Zhang",
          "hidden": false
        },
        {
          "_id": "67b7f350357c2729ac216499",
          "name": "Xuanhua He",
          "hidden": false
        },
        {
          "_id": "67b7f350357c2729ac21649a",
          "name": "Shanyuan Liu",
          "hidden": false
        },
        {
          "_id": "67b7f350357c2729ac21649b",
          "name": "Bo Cheng",
          "hidden": false
        },
        {
          "_id": "67b7f350357c2729ac21649c",
          "name": "Dawei Leng",
          "hidden": false
        },
        {
          "_id": "67b7f350357c2729ac21649d",
          "name": "Yuhui Yin",
          "hidden": false
        },
        {
          "_id": "67b7f350357c2729ac21649e",
          "name": "Jie Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T09:10:05.000Z",
      "title": "RelaCtrl : Contrôle Efficace Guide par la Relevé avec Transformateurs de Diffusion",
      "summary": "Le Diffusion Transformer joue un rôle important dans la génération d'images et d'animations à partir de texte, et a évolué principalement grâce à sa capacité à être scalable. Cependant, les méthodes actuelles de contrôle du Diffusion Transformer présentent des problèmes de paramètres et de charge de calcul importantes, ainsi qu'une mauvaise relation avec l'information de contrôle au niveau de l'appareil, ce qui rend les ressources inadéquatement allouées et affecte l'efficacité. En réponse à cette situation, nous proposons le cadre de travail de génération contrôlable et efficace guidé par la pertinence, appelé RelaCtrl, pour faciliter la formulation de signaux de contrôle efficaces dans le Diffusion Transformer.\n\nTout d'abord, nous évaluons la pertinence de l'information de contrôle dans chaque couche du Diffusion Transformer et utilisons le \"Score de Relevance ControlNet\" pour évaluer la qualité de la génération et l'effet de contrôle lorsque les couches de contrôle sont supprimées. En fonction de l'intensité de cette pertinence, nous ajustons la position des couches de contrôle, l'échelle des paramètres et la capacité de modélisation pour réduire les paramètres nécessaires et les calculs redondants. De plus, pour atteindre une efficacité accrue, nous remplaçons l'auto-attention et la FFN utilisées dans les blocs de copie généralement par un Shuffle Mixer bidimensionnel (TDSM), ce qui permet une implémentation efficace de ces opérations.\n\nLes résultats expérimentaux qualitatifs et quantitatifs montrent que, comparés à PixArt-delta, en utilisant seulement 15% des paramètres et de la complexité de calcul, il est possible de démontrer un meilleur rendement. De plus, des exemples supplémentaires peuvent être trouvés sur https://relactrl.github.io/RelaCtrl/.",
      "upvotes": 3,
      "discussionId": "67b7f354357c2729ac216582"
    },
    "publishedAt": "2025-02-20T22:30:51.542Z",
    "title": "RelaCtrl: Relevance-Guided Efficient Control for Diffusion Transformers",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14377.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6161
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.13759",
      "authors": [
        {
          "_id": "67b83a1f26e7d5f7cb0b7c9d",
          "user": {
            "_id": "65407ba7a38390065750233f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65407ba7a38390065750233f/1_IPMZbk-S9u2t18PQgMp.jpeg",
            "isPro": false,
            "fullname": "Zirui Song",
            "user": "Ziruibest",
            "type": "user"
          },
          "name": "Zirui Song",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-21T09:58:04.247Z",
          "hidden": false
        },
        {
          "_id": "67b83a1f26e7d5f7cb0b7c9e",
          "name": "Jingpu Yang",
          "hidden": false
        },
        {
          "_id": "67b83a1f26e7d5f7cb0b7c9f",
          "name": "Yuan Huang",
          "hidden": false
        },
        {
          "_id": "67b83a1f26e7d5f7cb0b7ca0",
          "name": "Jonathan Tonglet",
          "hidden": false
        },
        {
          "_id": "67b83a1f26e7d5f7cb0b7ca1",
          "name": "Zeyu Zhang",
          "hidden": false
        },
        {
          "_id": "67b83a1f26e7d5f7cb0b7ca2",
          "name": "Tao Cheng",
          "hidden": false
        },
        {
          "_id": "67b83a1f26e7d5f7cb0b7ca3",
          "name": "Meng Fang",
          "hidden": false
        },
        {
          "_id": "67b83a1f26e7d5f7cb0b7ca4",
          "name": "Iryna Gurevych",
          "hidden": false
        },
        {
          "_id": "67b83a1f26e7d5f7cb0b7ca5",
          "name": "Xiuying Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-19T14:21:25.000Z",
      "title": "Utilisation de données de jeux de joueurs réels avec géolocalisation : grands ensembles de données et un cadre logique similaire au humain",
      "summary": "La localisation, la tâche de déterminer l'emplacement d'une image, nécessite des raisons complexes et est importante pour la cartographie, la surveillance et la conservation culturelle. Cependant, les méthodes actuelles génèrent généralement des informations de localisation avec une précision faible et manquent en interprétabilité. Un des problèmes est la qualité et l'échelle des ensembles de données de localisation actuels. Ces ensembles de données sont généralement construits à une échelle petite automatiquement, ce qui ajoute des données avec du bruit et déséquilibre le niveau de difficulté des tâches, y compris des images exagérées ou qui ne possèdent pas de contenu suffisant pour faire des inférences fiables. Pour résoudre ces problèmes, nous présentons un cadre de travail strict de localisation avec trois composants clés : GeoComp, un grand ensemble de données, GeoCoT, un nouveau méthode de raisonnement, et GeoEval, un indice d'évaluation. Le cadre de travail se concentre sur GeoComp (Ensemble de Données de Compétition de Localisation), qui est un grand ensemble de données de 740K utilisateurs capturés sur deux ans, incluant 25 millions de métadonnées et 3 millions d'étiquettes de localisation associées à des images, chacune desquelles a été signalée plusieurs milliers à des millions de fois par des utilisateurs humains. Ce ensemble de données offre différents niveaux de difficulté pour diverses analyses et met en évidence les limitations importantes des modèles actuels. En se basant sur cet ensemble de données, nous proposons un nouveau cadre de raisonnement multi-niveau pour renforcer les capacités de raisonnement des modèles de vision et langage (LVMs) appelés Chain-of-Thought géographique (GeoCoT). GeoCoT imite le processus de raisonnement humain de localisation, intégrant le contexte et l'espace pour améliorer le rendement. Finalement, en utilisant les indices de GeoEval, nous montrons que GeoCoT améliore significativement la précision de la localisation d'environ 25% et l'interprétabilité.",
      "upvotes": 1,
      "discussionId": "67b83a2226e7d5f7cb0b7d66"
    },
    "publishedAt": "2025-02-21T03:33:28.852Z",
    "title": "Geolocation with Real Human Gameplay Data: A Large-Scale Dataset and Human-Like Reasoning Framework",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13759.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65407ba7a38390065750233f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65407ba7a38390065750233f/1_IPMZbk-S9u2t18PQgMp.jpeg",
      "fullname": "Zirui Song",
      "name": "Ziruibest",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.14409",
      "authors": [
        {
          "_id": "67b83a20a9fa331061e84ecd",
          "user": {
            "_id": "60a643b9213fe60589b8fdf9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60a643b9213fe60589b8fdf9/OOXmW3MkSf88r63tAE6-n.jpeg",
            "isPro": false,
            "fullname": "Dustin Wright",
            "user": "dwright37",
            "type": "user"
          },
          "name": "Dustin Wright",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-21T09:58:02.288Z",
          "hidden": false
        },
        {
          "_id": "67b83a20a9fa331061e84ece",
          "name": "Zain Muhammad Mujahid",
          "hidden": false
        },
        {
          "_id": "67b83a20a9fa331061e84ecf",
          "name": "Lu Wang",
          "hidden": false
        },
        {
          "_id": "67b83a20a9fa331061e84ed0",
          "name": "Isabelle Augenstein",
          "hidden": false
        },
        {
          "_id": "67b83a20a9fa331061e84ed1",
          "name": "David Jurgens",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T09:57:42.000Z",
      "title": "Résumé sans structure d'évidence et axé sur les consultations de long contexte",
      "summary": "Les modèles de langage grands (LLMs) peuvent générer des résumés cohérents à partir de longs contextes de requêtes utilisateurs. La résumé des exemples d'évidence et la citation précise peuvent améliorer la transparence et la confiance. D'autre part, les LLMs présentent un biais en faveur des positions où l'attention se concentre sur des informations spécifiques, ce qui pourrait affecter la citation de l'évidence. Les études précédentes se sont concentrées sur la citation d'évidence dans un taille spécifique (par exemple, phrases, paragraphes, documents). Cependant, nous proposons des résumés basés sur des requêtes de grands contextes qui incluent des citations d'évidence non structurées. Nous montrons que les systèmes actuels ont des difficultés dans la génération d'évidence non structurée et dans la citation précise, et que l'évidence peut \"être dans la confusion\". Pour atténuer cela, nous avons créé un nouveau pipeline indépendant du domaine et un ensemble de données synthétique appelé \"SUnsET\" (Évidence Non Structurée avec Résumés). Cet ensemble de données peut soutenir les LLMs dans cette tâche. Nous avons effectué une recherche croisée sur 5 modèles de LLMs de différents tailles et 4 ensembles de données (différents types et longueurs de documents), et nous montrons que les LLMs appliqués aux données de SUnsET génèrent des évidences cohérentes et pertinentes, des résumés d'évidence à différentes positions du contexte, et des résumés cohérents et pertinents.",
      "upvotes": 0,
      "discussionId": "67b83a21a9fa331061e84f36"
    },
    "publishedAt": "2025-02-21T03:33:40.641Z",
    "title": "Unstructured Evidence Attribution for Long Context Query Focused Summarization",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14409.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60a643b9213fe60589b8fdf9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60a643b9213fe60589b8fdf9/OOXmW3MkSf88r63tAE6-n.jpeg",
      "fullname": "Dustin Wright",
      "name": "dwright37",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  }
]