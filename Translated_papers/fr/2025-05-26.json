[
  {
    "paper": {
      "id": "2505.18125",
      "authors": [
        {
          "_id": "6833f8b419852283c4b3bbd6",
          "name": "Alan Arazi",
          "hidden": false
        },
        {
          "_id": "6833f8b419852283c4b3bbd7",
          "user": {
            "_id": "64802fb6c57f629056c59966",
            "avatarUrl": "/avatars/d5ecabaceeba759969855acf512b6649.svg",
            "isPro": false,
            "fullname": "Eilam Shapira",
            "user": "EilamSha",
            "type": "user"
          },
          "name": "Eilam Shapira",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:09:08.206Z",
          "hidden": false
        },
        {
          "_id": "6833f8b419852283c4b3bbd8",
          "name": "Roi Reichart",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T17:34:28.000Z",
      "submittedOnDailyAt": "2025-05-26T03:50:53.260Z",
      "title": "TabSTAR : Modèle de table centré sur le contexte d'un objectif d'intérêt",
      "submittedOnDailyBy": {
        "_id": "64802fb6c57f629056c59966",
        "avatarUrl": "/avatars/d5ecabaceeba759969855acf512b6649.svg",
        "isPro": false,
        "fullname": "Eilam Shapira",
        "user": "EilamSha",
        "type": "user"
      },
      "summary": "Deep learning a été réussi dans plusieurs domaines, mais dans les tâches d'apprentissage de tables, les Arbres de Décision Gradient Boosting (GBDTs) ont toujours été à la pointe. Cependant, les avancées récentes préparent les modèles d'apprentissage de tables basés sur le texte. Ceux-ci peuvent utiliser des connaissances réelles et s'adapter à différents ensembles de données, surtout lorsque le texte est inclus. Cependant, dans les méthodes actuelles, on essaie d'intégrer la capacité des modèles de langage dans les tâches de tables, mais presque tous utilisent des représentations statiques et indépendantes de l'objectif, ce qui limite leur efficacité. On présente TabSTAR : TabSTAR est un modèle de tables basé sur la sémantique avec des représentations liées à l'objectif. TabSTAR permet d'entraîner des transitions sur des données de tables qui incluent des caractéristiques textuelles. TabSTAR est conçu avec une architecture indépendante de l'ensemble de données. Ce modèle libère l'encodeur pré-entraîné de texte et reçoit le token de l'objectif comme entrée. Cela fournit au modèle le contexte nécessaire pour apprendre les caractéristiques propres à la tâche. TabSTAR a atteint le meilleur rendement dans les cadres de référence de tâches de classification, ainsi que dans des ensembles de données de taille intermédiaire et grande, et montre une scalabilité selon le nombre d'ensembles de données pré-entraînés. TabSTAR offre une voie pour améliorer le rendement final.",
      "upvotes": 66,
      "discussionId": "6833f8b419852283c4b3bc02",
      "ai_summary": "TabSTAR, a tabular foundation model with semantically target-aware representations, achieves state-of-the-art performance in classification tasks with text features through transfer learning without dataset-specific parameters.",
      "ai_keywords": [
        "TabSTAR",
        "foundation tabular model",
        "semantically target-aware representations",
        "transfer learning",
        "pretrained text encoder",
        "target tokens",
        "task-specific embeddings",
        "scaling laws"
      ]
    },
    "publishedAt": "2025-05-23T13:34:28.000Z",
    "title": "TabSTAR: A Foundation Tabular Model With Semantically Target-Aware\n  Representations",
    "summary": "While deep learning has achieved remarkable success across many domains, it\nhas historically underperformed on tabular learning tasks, which remain\ndominated by gradient boosting decision trees (GBDTs). However, recent\nadvancements are paving the way for Tabular Foundation Models, which can\nleverage real-world knowledge and generalize across diverse datasets,\nparticularly when the data contains free-text. Although incorporating language\nmodel capabilities into tabular tasks has been explored, most existing methods\nutilize static, target-agnostic textual representations, limiting their\neffectiveness. We introduce TabSTAR: a Foundation Tabular Model with\nSemantically Target-Aware Representations. TabSTAR is designed to enable\ntransfer learning on tabular data with textual features, with an architecture\nfree of dataset-specific parameters. It unfreezes a pretrained text encoder and\ntakes as input target tokens, which provide the model with the context needed\nto learn task-specific embeddings. TabSTAR achieves state-of-the-art\nperformance for both medium- and large-sized datasets across known benchmarks\nof classification tasks with text features, and its pretraining phase exhibits\nscaling laws in the number of datasets, offering a pathway for further\nperformance improvements.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18125.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64802fb6c57f629056c59966",
      "avatarUrl": "/avatars/d5ecabaceeba759969855acf512b6649.svg",
      "fullname": "Eilam Shapira",
      "name": "EilamSha",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.17667",
      "authors": [
        {
          "_id": "6833d7c5a3262d6b1e4d358e",
          "user": {
            "_id": "62ecbffd99112e99c5f7fded",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62ecbffd99112e99c5f7fded/U6iXAJbpm2vaC5qksEPiH.png",
            "isPro": false,
            "fullname": "Fanqi Wan",
            "user": "Wanfq",
            "type": "user"
          },
          "name": "Fanqi Wan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:09:44.880Z",
          "hidden": false
        },
        {
          "_id": "6833d7c5a3262d6b1e4d358f",
          "name": "Weizhou Shen",
          "hidden": false
        },
        {
          "_id": "6833d7c5a3262d6b1e4d3590",
          "name": "Shengyi Liao",
          "hidden": false
        },
        {
          "_id": "6833d7c5a3262d6b1e4d3591",
          "name": "Yingcheng Shi",
          "hidden": false
        },
        {
          "_id": "6833d7c5a3262d6b1e4d3592",
          "name": "Chenliang Li",
          "hidden": false
        },
        {
          "_id": "6833d7c5a3262d6b1e4d3593",
          "name": "Ziyi Yang",
          "hidden": false
        },
        {
          "_id": "6833d7c5a3262d6b1e4d3594",
          "name": "Ji Zhang",
          "hidden": false
        },
        {
          "_id": "6833d7c5a3262d6b1e4d3595",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "6833d7c5a3262d6b1e4d3596",
          "name": "Jingren Zhou",
          "hidden": false
        },
        {
          "_id": "6833d7c5a3262d6b1e4d3597",
          "name": "Ming Yan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T09:31:55.000Z",
      "submittedOnDailyAt": "2025-05-26T03:36:36.885Z",
      "title": "QwenLong-L1 : Modèle de logique de grand contexte contre l'apprentissage par renforcement",
      "submittedOnDailyBy": {
        "_id": "62ecbffd99112e99c5f7fded",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62ecbffd99112e99c5f7fded/U6iXAJbpm2vaC5qksEPiH.png",
        "isPro": false,
        "fullname": "Fanqi Wan",
        "user": "Wanfq",
        "type": "user"
      },
      "summary": "Récemment, les grands modèles de logique (LRMs) ont démontré une puissante capacité en logique et ont été testés à travers l'apprentissage par renforcement (RL). Ces améliorations se font principalement dans des tâches de logique dans des contextes courts. D'autre part, le traitement efficace des LRMs avec des entrées de contexte long et la réalisation de RL pour la logique constituent des défis importants qui n'ont pas encore été résolus. Pour aborder ces défis, nous formalisons d'abord le paradigme de RL pour la logique dans des contextes longs et identifions les défis clés de l'efficacité de l'entraînement optimal et du processus d'optimisation instable. Pour résoudre ces problèmes, nous proposons le cadre de travail QwenLong-L1. Ce cadre base le développement sur l'escalade contextuel avancée pour appliquer des LRMs courts aux scénarios de contexte long. En particulier, nous utilisons un pas d'ajustement de normalisation accéléré (SFT) pour construire une politique initiale forte et, par la suite, nous appliquons des techniques de RL guidées par un curriculum pour stabiliser le développement de la politique et renforcer la recherche de la politique à travers une révision de la difficulté. Les expérimentations sur les 7 tests de réponses à des questions de documents de contexte long dans le cadre de référence benchmark ont montré que QwenLong-L1-32B dépasse d'autres LRMs comme OpenAI-o3-mini et Qwen3-235B-A22B, atteint un rendement similaire à Claude-3.7-Sonnet-Thinking et montre un rendement spécialisé entre les meilleurs LRMs. Cette recherche a conduit au développement de LRMs de contexte long pratiques qui permettent une forte logique dans des environnements de haute densité d'information.",
      "upvotes": 42,
      "discussionId": "6833d7c6a3262d6b1e4d35c5",
      "githubRepo": "https://github.com/Tongyi-Zhiwen/QwenLong-L1",
      "ai_summary": "A framework called QwenLong-L1 enhances large reasoning models for long-context reasoning through reinforcement learning, achieving leading performance on document question-answering benchmarks.",
      "ai_keywords": [
        "reinforcement learning",
        "long-context reasoning",
        "short-context reasoning",
        "training efficiency",
        "optimization process",
        "QwenLong-L1",
        "progressive context scaling",
        "supervised fine-tuning",
        "curriculum-guided phased RL",
        "difficulty-aware retrospective sampling",
        "document question-answering benchmarks",
        "OpenAI-o3-mini",
        "Qwen3-235B-A22B",
        "Claude-3.7-Sonnet-Thinking"
      ]
    },
    "publishedAt": "2025-05-23T05:31:55.000Z",
    "title": "QwenLong-L1: Towards Long-Context Large Reasoning Models with\n  Reinforcement Learning",
    "summary": "Recent large reasoning models (LRMs) have demonstrated strong reasoning\ncapabilities through reinforcement learning (RL). These improvements have\nprimarily been observed within the short-context reasoning tasks. In contrast,\nextending LRMs to effectively process and reason on long-context inputs via RL\nremains a critical unsolved challenge. To bridge this gap, we first formalize\nthe paradigm of long-context reasoning RL, and identify key challenges in\nsuboptimal training efficiency and unstable optimization process. To address\nthese issues, we propose QwenLong-L1, a framework that adapts short-context\nLRMs to long-context scenarios via progressive context scaling. Specifically,\nwe utilize a warm-up supervised fine-tuning (SFT) stage to establish a robust\ninitial policy, followed by a curriculum-guided phased RL technique to\nstabilize the policy evolution, and enhanced with a difficulty-aware\nretrospective sampling strategy to incentivize the policy exploration.\nExperiments on seven long-context document question-answering benchmarks\ndemonstrate that QwenLong-L1-32B outperforms flagship LRMs like OpenAI-o3-mini\nand Qwen3-235B-A22B, achieving performance on par with\nClaude-3.7-Sonnet-Thinking, demonstrating leading performance among\nstate-of-the-art LRMs. This work advances the development of practical\nlong-context LRMs capable of robust reasoning across information-intensive\nenvironments.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17667.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62ecbffd99112e99c5f7fded",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62ecbffd99112e99c5f7fded/U6iXAJbpm2vaC5qksEPiH.png",
      "fullname": "Fanqi Wan",
      "name": "Wanfq",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 29
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.17612",
      "authors": [
        {
          "_id": "6833c9fd298a7bec9c3da3b0",
          "name": "Minki Kang",
          "hidden": false
        },
        {
          "_id": "6833c9fd298a7bec9c3da3b1",
          "name": "Jongwon Jeong",
          "hidden": false
        },
        {
          "_id": "6833c9fd298a7bec9c3da3b2",
          "name": "Seanie Lee",
          "hidden": false
        },
        {
          "_id": "6833c9fd298a7bec9c3da3b3",
          "name": "Jaewoong Cho",
          "hidden": false
        },
        {
          "_id": "6833c9fd298a7bec9c3da3b4",
          "name": "Sung Ju Hwang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T08:20:15.000Z",
      "submittedOnDailyAt": "2025-05-26T00:25:44.604Z",
      "title": "Méthode pour intégrer un petit modèle en utilisant les sorties des LLM et une réfactorisation et des outils de code",
      "submittedOnDailyBy": {
        "_id": "64b74920fe6a108d03fed767",
        "avatarUrl": "/avatars/a2c05b809c36fa5fab8e1a43b3e67051.svg",
        "isPro": false,
        "fullname": "Minki Kang",
        "user": "Nardien",
        "type": "user"
      },
      "summary": "Les modèles de langue grands (LLMs) montrent des résultats exceptionnels dans des tâches logiques complexes, mais leur coût de calcul est élevé, rendant-ils peu pratiques pour une utilisation large. Pour résoudre ce problème, des recherches récentes ont concentré leur étude sur l'utilisation de la technique \"Chain-of-Thought\" (CoT) pour transmettre aux petits modèles de langue (sLMs) la capacité de réaliser des tâches logiques. Cependant, cette approche souvent échoue lorsque des connaissances spécifiques ou des calculs précis sont nécessaires, car les sLMs ont des limitations dans ces domaines. Dans cette étude, un cadre de travail appelé \"Agent Distillation\" est proposé, dont l'objectif est de transmettre aux sLMs non seulement la capacité de réaliser des tâches logiques, mais aussi toutes les capacités des agents basés sur des LLMs. L'Agent Distillation est amélioré dans deux directions : (1) amélioration de la qualité des travaux élaborés par le modèle enseignant en utilisant des techniques de question ; (2) amélioration de la robustesse des petits agents lors du test en utilisant la génération automatique d'actions. On a évalué 8 tâches logiques, y compris des domaines de connaissances factuelles et mathématiques, à l'intérieur et à l'extérieur de leur domaine. Enfin, des sLMs avec 0.5B, 1.5B et 3B paramètres, en utilisant le méthode de CoT distillé, ont montré une compétitivité avec les modèles plus grands de 1.5B, 3B et 7B, démontrant ainsi la possibilité de construire des petits agents pratiques et utiles. Le code est disponible sur https://github.com/Nardien/agent-distillation.",
      "upvotes": 34,
      "discussionId": "6833ca00298a7bec9c3da444",
      "githubRepo": "https://github.com/Nardien/agent-distillation",
      "ai_summary": "Agent Distillation transfers reasoning and task-solving capabilities from large language models to smaller models using enhanced prompts and self-consistent actions, matching performance of larger models on various reasoning tasks.",
      "ai_keywords": [
        "Large language models",
        "small language models",
        "chain-of-thought",
        "agent distillation",
        "prompting method",
        "first-thought prefix",
        "self-consistent action generation",
        "task-solving behavior",
        "retrieval tools",
        "code tools",
        "in-domain generalization",
        "out-of-domain generalization"
      ]
    },
    "publishedAt": "2025-05-23T04:20:15.000Z",
    "title": "Distilling LLM Agent into Small Models with Retrieval and Code Tools",
    "summary": "Large language models (LLMs) excel at complex reasoning tasks but remain\ncomputationally expensive, limiting their practical deployment. To address\nthis, recent works have focused on distilling reasoning capabilities into\nsmaller language models (sLMs) using chain-of-thought (CoT) traces from teacher\nLLMs. However, this approach struggles in scenarios requiring rare factual\nknowledge or precise computation, where sLMs often hallucinate due to limited\ncapability. In this work, we propose Agent Distillation, a framework for\ntransferring not only reasoning capability but full task-solving behavior from\nLLM-based agents into sLMs with retrieval and code tools. We improve agent\ndistillation along two complementary axes: (1) we introduce a prompting method\ncalled first-thought prefix to enhance the quality of teacher-generated\ntrajectories; and (2) we propose a self-consistent action generation for\nimproving test-time robustness of small agents. We evaluate our method on eight\nreasoning tasks across factual and mathematical domains, covering both\nin-domain and out-of-domain generalization. Our results show that sLMs as small\nas 0.5B, 1.5B, 3B parameters can achieve performance competitive with next-tier\nlarger 1.5B, 3B, 7B models fine-tuned using CoT distillation, demonstrating the\npotential of agent distillation for building practical, tool-using small\nagents. Our code is available at https://github.com/Nardien/agent-distillation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17612.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64b74920fe6a108d03fed767",
      "avatarUrl": "/avatars/a2c05b809c36fa5fab8e1a43b3e67051.svg",
      "fullname": "Minki Kang",
      "name": "Nardien",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.15929",
      "authors": [
        {
          "_id": "6830404effb59afb6569273a",
          "name": "Hui Shen",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb6569273b",
          "user": {
            "_id": "6621cea88850e38ffbb1854f",
            "avatarUrl": "/avatars/6d73d947046faa32260ee325069976d9.svg",
            "isPro": false,
            "fullname": "Taki WU",
            "user": "taki555",
            "type": "user"
          },
          "name": "Taiqiang Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:14:30.851Z",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb6569273c",
          "name": "Qi Han",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb6569273d",
          "name": "Yunta Hsieh",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb6569273e",
          "user": {
            "_id": "67fe265f9698ae4f5f4db718",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/0PLJEbUyJbM9BacFxcScP.png",
            "isPro": false,
            "fullname": "Jizhou Wang",
            "user": "John-ai-bee",
            "type": "user"
          },
          "name": "Jizhou Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:14:33.171Z",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb6569273f",
          "name": "Yuyue Zhang",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb65692740",
          "name": "Yuxin Cheng",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb65692741",
          "name": "Zijian Hao",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb65692742",
          "name": "Yuansheng Ni",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb65692743",
          "name": "Xin Wang",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb65692744",
          "name": "Zhongwei Wan",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb65692745",
          "name": "Kai Zhang",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb65692746",
          "name": "Wendong Xu",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb65692747",
          "name": "Jing Xiong",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb65692748",
          "name": "Ping Luo",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb65692749",
          "name": "Wenhu Chen",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb6569274a",
          "name": "Chaofan Tao",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb6569274b",
          "name": "Zhuoqing Mao",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb6569274c",
          "name": "Ngai Wong",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T18:33:50.000Z",
      "submittedOnDailyAt": "2025-05-26T05:21:02.238Z",
      "title": "As-tu la \"savoir-faire\" de la théorie de la physique ?",
      "submittedOnDailyBy": {
        "_id": "6621cea88850e38ffbb1854f",
        "avatarUrl": "/avatars/6d73d947046faa32260ee325069976d9.svg",
        "isPro": false,
        "fullname": "Taki WU",
        "user": "taki555",
        "type": "user"
      },
      "summary": "Les actuels cadres de référence ne capturent pas les aspects importants de la pensée : la théorie physique, le savoir de domaine et la théorie symbolique, ainsi que la capacité de comprendre les restrictions réelles. Pour résoudre cette lacune, nous présentons PhyX : le premier cadre de référence à grande échelle qui évalue la capacité d'un modèle à appliquer la théorie physique dans des scénarios visuels. PhyX inclut une variété de questions de différentes profondeurs et profondeurs, combinant 6 types de théorie et évaluant 25 sous-domaines et 6 domaines physiques clés (thermodynamique, électromagnétisme, mécanique, physique moderne, optique, ondes et acoustique). Selon notre évaluation détaillée, les modèles les plus avancés encore rencontrent de grands défis en matière de théorie physique. GPT-4o, Claude3.7-Sonnet et GPT-o4-mini atteignent des précisions de 32,5%, 42,2% et 45,8% respectivement, mais l'intervalle de performance d'un expert humain dépasse le 29%. Notre analyse révèle des limitations importantes dans les modèles actuels : dépendance à la mémoire pour les connaissances académiques, dépendance excessive aux formules mathématiques et un approfondissement sur le pattern visuel superficiel plutôt que sur la compréhension physique véritable. Nous évaluons la capacité de théorie physique en détail grâce à des états détaillés, des études de cas spécifiques et plusieurs protocoles d'évaluation. Pour garantir la reproductibilité, nous avons mis en place des protocoles d'évaluation compatibles basés sur des outils de package largement utilisés comme VLMEvalKit.",
      "upvotes": 34,
      "discussionId": "68304052ffb59afb6569282f",
      "projectPage": "https://phyx-bench.github.io/",
      "githubRepo": "https://github.com/NastyMarcus/PhyX",
      "ai_summary": "A new benchmark, PhyX, evaluates models' physics-grounded reasoning in visual scenarios, revealing significant limitations in current models' physical understanding compared to human experts.",
      "ai_keywords": [
        "multimodal questions",
        "reasoning types",
        "sub-domains",
        "core physics domains",
        "thermodynamics",
        "electromagnetism",
        "mechanics",
        "modern physics",
        "optics",
        "wave\\&acoustics",
        "fine-grained statistics",
        "case studies",
        "evaluation paradigms",
        "VLMEvalKit"
      ]
    },
    "publishedAt": "2025-05-21T14:33:50.000Z",
    "title": "PhyX: Does Your Model Have the \"Wits\" for Physical Reasoning?",
    "summary": "Existing benchmarks fail to capture a crucial aspect of intelligence:\nphysical reasoning, the integrated ability to combine domain knowledge,\nsymbolic reasoning, and understanding of real-world constraints. To address\nthis gap, we introduce PhyX: the first large-scale benchmark designed to assess\nmodels capacity for physics-grounded reasoning in visual scenarios. PhyX\nincludes 3K meticulously curated multimodal questions spanning 6 reasoning\ntypes across 25 sub-domains and 6 core physics domains: thermodynamics,\nelectromagnetism, mechanics, modern physics, optics, and wave\\&acoustics. In\nour comprehensive evaluation, even state-of-the-art models struggle\nsignificantly with physical reasoning. GPT-4o, Claude3.7-Sonnet, and\nGPT-o4-mini achieve only 32.5\\%, 42.2\\%, and 45.8\\% accuracy\nrespectively-performance gaps exceeding 29\\% compared to human experts. Our\nanalysis exposes critical limitations in current models: over-reliance on\nmemorized disciplinary knowledge, excessive dependence on mathematical\nformulations, and surface-level visual pattern matching rather than genuine\nphysical understanding. We provide in-depth analysis through fine-grained\nstatistics, detailed case studies, and multiple evaluation paradigms to\nthoroughly examine physical reasoning capabilities. To ensure reproducibility,\nwe implement a compatible evaluation protocol based on widely-used toolkits\nsuch as VLMEvalKit, enabling one-click evaluation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15929.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6621cea88850e38ffbb1854f",
      "avatarUrl": "/avatars/6d73d947046faa32260ee325069976d9.svg",
      "fullname": "Taki WU",
      "name": "taki555",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.18129",
      "authors": [
        {
          "_id": "6833cf89df7cbb5c087a4caa",
          "name": "Yan Ma",
          "hidden": false
        },
        {
          "_id": "6833cf89df7cbb5c087a4cab",
          "name": "Linge Du",
          "hidden": false
        },
        {
          "_id": "6833cf89df7cbb5c087a4cac",
          "user": {
            "_id": "642e4d4d6748dd4f8eeb7732",
            "avatarUrl": "/avatars/fd911e9143d1a7aedd21a7d611543fcc.svg",
            "isPro": false,
            "fullname": "Xuyang Shen",
            "user": "Ryan1122",
            "type": "user"
          },
          "name": "Xuyang Shen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:10:03.920Z",
          "hidden": false
        },
        {
          "_id": "6833cf89df7cbb5c087a4cad",
          "name": "Shaoxiang Chen",
          "hidden": false
        },
        {
          "_id": "6833cf89df7cbb5c087a4cae",
          "name": "Pengfei Li",
          "hidden": false
        },
        {
          "_id": "6833cf89df7cbb5c087a4caf",
          "name": "Qibing Ren",
          "hidden": false
        },
        {
          "_id": "6833cf89df7cbb5c087a4cb0",
          "name": "Lizhuang Ma",
          "hidden": false
        },
        {
          "_id": "6833cf89df7cbb5c087a4cb1",
          "name": "Yuchao Dai",
          "hidden": false
        },
        {
          "_id": "6833cf89df7cbb5c087a4cb2",
          "name": "Pengfei Liu",
          "hidden": false
        },
        {
          "_id": "6833cf89df7cbb5c087a4cb3",
          "name": "Junjie Yan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T17:41:14.000Z",
      "submittedOnDailyAt": "2025-05-26T00:54:44.420Z",
      "title": "Une RL qui peut voir tout : l'apprentissage par récompense de l'intégration tridimensionnelle visuelle",
      "submittedOnDailyBy": {
        "_id": "642e4d4d6748dd4f8eeb7732",
        "avatarUrl": "/avatars/fd911e9143d1a7aedd21a7d611543fcc.svg",
        "isPro": false,
        "fullname": "Xuyang Shen",
        "user": "Ryan1122",
        "type": "user"
      },
      "summary": "L'apprentissage par renforcement (RL) a considérablement amélioré la capacité logique des modèles de langage visuel (VLMs). Cependant, l'utilisation de RL dans des tâches logiques est moins explorée que dans des tâches de détection d'objets ou de segmentation, qui mettent l'accent sur l'observation. Nous proposons le système d'apprentissage par renforcement unifié Visual Triple, appelé V-Triune. Ce système permet aux VLMs d'apprendre la logique visuelle et l'observation dans un seul flux d'apprentissage. V-Triune comprend trois composants complémentaires : la formatation des données à l'échelle de l'échantillon (unification des entrées de différentes tâches), le calcul de la récompense à l'échelle des données de validation (provision de récompenses personnalisées à travers des données spécialisées), et le suivi de métriques à l'échelle de la source (diagnostic des problèmes à l'échelle des données de source). De plus, nous présentons une nouvelle récompense dynamique de IoU qui fournit une rétroaction adaptative, évolutive et décisive pour les tâches d'observation. Notre approche a été mise en œuvre dans le cadre d'apprentissage RL off-policy en utilisant des modèles de 7B et 32B de code ouvert. Les modèles résultants, appelés Logical Vision (LV), montrent des améliorations dans les tâches logiques et d'observation. Ces capacités ont été développées en conjonction avec divers ensembles de données axés sur des tâches visuelles logiques (mathématiques, puzzles, graphiques, sciences) et des tâches d'observation (détection, segmentation, comptage, OCR). Ensuite, LV a démontré des résultats notables sur MEGA-Bench Core, avec des améliorations dans la gamme +2.1 à +14.1 pour les versions de 7B et 32B, et une application élargie dans des tâches plus basiques. Ces résultats confirment l'efficacité et l'échellabilité d'une approche intégrée de RL pour les VLMs. Le système V-Triune et les modèles LV sont disponibles sur https://github.com/MiniMax-AI.",
      "upvotes": 33,
      "discussionId": "6833cf8adf7cbb5c087a4d0c",
      "githubRepo": "https://github.com/MiniMax-AI/One-RL-to-See-Them-All",
      "ai_summary": "A unified reinforcement learning system, V-Triune, combines visual reasoning and perception tasks in vision-language models through a single training pipeline, achieving significant improvements across various tasks.",
      "ai_keywords": [
        "visual triple unified reinforcement learning",
        "sample-level data formatting",
        "verifier-level reward computation",
        "source-level metric monitoring",
        "dynamic IoU reward",
        "reinforcement learning",
        "vision-language models",
        "object detection",
        "grounding",
        "Orsta",
        "MEGA-Bench Core"
      ]
    },
    "publishedAt": "2025-05-23T13:41:14.000Z",
    "title": "One RL to See Them All: Visual Triple Unified Reinforcement Learning",
    "summary": "Reinforcement learning (RL) has significantly advanced the reasoning\ncapabilities of vision-language models (VLMs). However, the use of RL beyond\nreasoning tasks remains largely unexplored, especially for perceptionintensive\ntasks like object detection and grounding. We propose V-Triune, a Visual Triple\nUnified Reinforcement Learning system that enables VLMs to jointly learn visual\nreasoning and perception tasks within a single training pipeline. V-Triune\ncomprises triple complementary components: Sample-Level Data Formatting (to\nunify diverse task inputs), Verifier-Level Reward Computation (to deliver\ncustom rewards via specialized verifiers) , and Source-Level Metric Monitoring\n(to diagnose problems at the data-source level). We further introduce a novel\nDynamic IoU reward, which provides adaptive, progressive, and definite feedback\nfor perception tasks handled by V-Triune. Our approach is instantiated within\noff-the-shelf RL training framework using open-source 7B and 32B backbone\nmodels. The resulting model, dubbed Orsta (One RL to See Them All),\ndemonstrates consistent improvements across both reasoning and perception\ntasks. This broad capability is significantly shaped by its training on a\ndiverse dataset, constructed around four representative visual reasoning tasks\n(Math, Puzzle, Chart, and Science) and four visual perception tasks (Grounding,\nDetection, Counting, and OCR). Subsequently, Orsta achieves substantial gains\non MEGA-Bench Core, with improvements ranging from +2.1 to an impressive +14.1\nacross its various 7B and 32B model variants, with performance benefits\nextending to a wide range of downstream tasks. These results highlight the\neffectiveness and scalability of our unified RL approach for VLMs. The V-Triune\nsystem, along with the Orsta models, is publicly available at\nhttps://github.com/MiniMax-AI.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18129.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642e4d4d6748dd4f8eeb7732",
      "avatarUrl": "/avatars/fd911e9143d1a7aedd21a7d611543fcc.svg",
      "fullname": "Xuyang Shen",
      "name": "Ryan1122",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.18092",
      "authors": [
        {
          "_id": "6833ea049f968fc5c6b64486",
          "name": "Weizhou Shen",
          "hidden": false
        },
        {
          "_id": "6833ea049f968fc5c6b64487",
          "name": "Chenliang Li",
          "hidden": false
        },
        {
          "_id": "6833ea049f968fc5c6b64488",
          "user": {
            "_id": "62ecbffd99112e99c5f7fded",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62ecbffd99112e99c5f7fded/U6iXAJbpm2vaC5qksEPiH.png",
            "isPro": false,
            "fullname": "Fanqi Wan",
            "user": "Wanfq",
            "type": "user"
          },
          "name": "Fanqi Wan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:09:25.991Z",
          "hidden": false
        },
        {
          "_id": "6833ea049f968fc5c6b64489",
          "name": "Shengyi Liao",
          "hidden": false
        },
        {
          "_id": "6833ea049f968fc5c6b6448a",
          "name": "Shaopeng Lai",
          "hidden": false
        },
        {
          "_id": "6833ea049f968fc5c6b6448b",
          "name": "Bo Zhang",
          "hidden": false
        },
        {
          "_id": "6833ea049f968fc5c6b6448c",
          "name": "Yingcheng Shi",
          "hidden": false
        },
        {
          "_id": "6833ea049f968fc5c6b6448d",
          "name": "Yuning Wu",
          "hidden": false
        },
        {
          "_id": "6833ea049f968fc5c6b6448e",
          "name": "Gang Fu",
          "hidden": false
        },
        {
          "_id": "6833ea049f968fc5c6b6448f",
          "name": "Zhansheng Li",
          "hidden": false
        },
        {
          "_id": "6833ea049f968fc5c6b64490",
          "name": "Bin Yang",
          "hidden": false
        },
        {
          "_id": "6833ea049f968fc5c6b64491",
          "name": "Ji Zhang",
          "hidden": false
        },
        {
          "_id": "6833ea049f968fc5c6b64492",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "6833ea049f968fc5c6b64493",
          "name": "Jingren Zhou",
          "hidden": false
        },
        {
          "_id": "6833ea049f968fc5c6b64494",
          "name": "Ming Yan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T16:47:00.000Z",
      "submittedOnDailyAt": "2025-05-26T04:43:04.143Z",
      "title": "QwenLong-CPRS : QwenLong-CPRS : Optimisation du Contexte Dynamique pour les LLMs Infinis",
      "submittedOnDailyBy": {
        "_id": "64777a346e6c7ac608c1e9bf",
        "avatarUrl": "/avatars/b0e65ba781c90c2560606eb5467101eb.svg",
        "isPro": false,
        "fullname": "Weizhou Shen",
        "user": "shenwzh3",
        "type": "user"
      },
      "summary": "Ce rapport technique décrit comment QwenLong-CPRS utilise un cadre de compression du contexte pour optimiser clairement le contexte long, et aborde le surcharge de calcul du pas de remplissage précédent et le phénomène de \"perte intermédiaire\" dans les modèles de langage grands (LLMs) lors du traitement de contextes longs. Une nouvelle structure dynamique d'optimisation du contexte est implémentée, permettant une compression du contexte multigranulaire par instructions de langage naturel, et atteignant des améliorations en efficacité et rendement.\n\nQwenLong-CPRS, développé dans la série d'architecture Qwen, introduit 4 innovations : optimisation dynamique basée sur le langage, couche bidirectionnelle pour améliorer la polarité, structure tokenique avec têtes de modélisation de langage, et inférence parallèle de fenêtres.\n\nLes résultats d'évaluation sur 5 benchmarks (avec des contextes de 4K à 2M mots) montrent que QwenLong-CPRS présente 3 effets : dépasse les autres méthodes de gestion du contexte (RAG, attention sparse) en précision et efficacité. En étant architecturalement indépendant, il intègre tous les LLMs de flash comme GPT-4o, Gemini2.0-pro, Claude3.7-sonnet, DeepSeek-v3 et Qwen2.5-max, atteignant une compression du contexte de 21,59 fois et un accroissement moyen de rendement de 19,15 points. L'introduction de Qwen2.5-32B-Instruct permet à QwenLong-CPRS de dépasser Ruler-128K et InfiniteBench de 4,85 et 10,88 points, respectivement, établissant une nouvelle ligne de rendement standard.",
      "upvotes": 31,
      "discussionId": "6833ea059f968fc5c6b644c1",
      "ai_summary": "QwenLong-CPRS enhances large language models with multi-granularity context compression, dynamic optimization guided by natural language, and efficient bidirectional reasoning and parallel inference, achieving superior performance and context management.",
      "ai_keywords": [
        "context compression",
        "dynamic context optimization",
        "bidirectional reasoning layers",
        "token critic mechanisms",
        "window-parallel inference",
        "Qwen",
        "RAG",
        "sparse attention",
        "large language models",
        "SOTA performance"
      ]
    },
    "publishedAt": "2025-05-23T12:47:00.000Z",
    "title": "QwenLong-CPRS: Towards infty-LLMs with Dynamic Context Optimization",
    "summary": "This technical report presents QwenLong-CPRS, a context compression framework\ndesigned for explicit long-context optimization, addressing prohibitive\ncomputation overhead during the prefill stage and the \"lost in the middle\"\nperformance degradation of large language models (LLMs) during long sequence\nprocessing. Implemented through a novel dynamic context optimization mechanism,\nQwenLong-CPRS enables multi-granularity context compression guided by natural\nlanguage instructions, achieving both efficiency gains and improved\nperformance.\n  Evolved from the Qwen architecture series, QwenLong-CPRS introduces four key\ninnovations: (1) Natural language-guided dynamic optimization, (2)\nBidirectional reasoning layers for enhanced boundary awareness, (3) Token\ncritic mechanisms with language modeling heads, and (4) Window-parallel\ninference.\n  Comprehensive evaluations across five benchmarks (4K-2M word contexts)\ndemonstrate QwenLong-CPRS's threefold effectiveness: (1) Consistent superiority\nover other context management methods like RAG and sparse attention in both\naccuracy and efficiency. (2) Architecture-agnostic integration with all\nflagship LLMs, including GPT-4o, Gemini2.0-pro, Claude3.7-sonnet, DeepSeek-v3,\nand Qwen2.5-max, achieves 21.59times context compression alongside\n19.15-point average performance gains; (3) Deployed with Qwen2.5-32B-Instruct,\nQwenLong-CPRS surpasses leading proprietary LLMs by 4.85 and 10.88 points on\nRuler-128K and InfiniteBench, establishing new SOTA performance.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18092.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64777a346e6c7ac608c1e9bf",
      "avatarUrl": "/avatars/b0e65ba781c90c2560606eb5467101eb.svg",
      "fullname": "Weizhou Shen",
      "name": "shenwzh3",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.17225",
      "authors": [
        {
          "_id": "6833c65d49b9e903d3ddbd11",
          "user": {
            "_id": "62845957b410bd779033759c",
            "avatarUrl": "/avatars/4feef73c06f2f7de6abf7a4789ac13f9.svg",
            "isPro": false,
            "fullname": "Doohyuk Jang",
            "user": "jadohu",
            "type": "user"
          },
          "name": "Doohyuk Jang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:10:25.026Z",
          "hidden": false
        },
        {
          "_id": "6833c65d49b9e903d3ddbd12",
          "user": {
            "_id": "61b15ce1a5dd7dc7024406dc",
            "avatarUrl": "/avatars/682ce5ee7d2fec7180dc8e1144cd12ab.svg",
            "isPro": false,
            "fullname": "Yoonjeon Kim",
            "user": "yjyjyj98",
            "type": "user"
          },
          "name": "Yoonjeon Kim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:10:27.112Z",
          "hidden": false
        },
        {
          "_id": "6833c65d49b9e903d3ddbd13",
          "name": "Chanjae Park",
          "hidden": false
        },
        {
          "_id": "6833c65d49b9e903d3ddbd14",
          "name": "Hyun Ryu",
          "hidden": false
        },
        {
          "_id": "6833c65d49b9e903d3ddbd15",
          "name": "Eunho Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T19:00:01.000Z",
      "submittedOnDailyAt": "2025-05-26T00:11:09.797Z",
      "title": "La ligne de code est :\n\n```\nria_ning_module_is_stubborn : diagnostic des sur-écritures de commandes dans le module ria_ning\n```",
      "submittedOnDailyBy": {
        "_id": "61b15ce1a5dd7dc7024406dc",
        "avatarUrl": "/avatars/682ce5ee7d2fec7180dc8e1144cd12ab.svg",
        "isPro": false,
        "fullname": "Yoonjeon Kim",
        "user": "yjyjyj98",
        "type": "user"
      },
      "summary": "Les modèles de langue générale montrent un excellent rendement dans des tâches de logique complexe et longue. Cependant, ces modèles présentent une inflexibilité logique problématique, qu'on appelle \"rigidité logique\", et souvent privilégient des patrons logiques généraux, même lorsqu'ils reçoivent des instructions claires, ce qui conduit souvent à des conclusions erronées. Cette comportement est particulièrement problématique dans des domaines comme la mathématique et les puzzles logiques, où le respect des contraintes spécifiques est crucial. Pour étudier systématiquement cette rigidité logique, on présente un ensemble de diagnostics ponctuels d'experts, qui ont été peu étudiés dans des études précédentes. Ces données permettent d'identifier des motifs sibériques qui se répètent de manière récurrente dans la logique générale. En particulier, ces motifs sont classifiés en trois catégories : (i) surcharge d'interprétation, (ii) méfiance envers l'entrée, et (iii) attention partielle au projet, qui incitent le modèle à ignorer ou déformer ses instructions. Ces données sont publiées avec l'objectif de favoriser des recherches futures visant à atténuer cette rigidité logique.",
      "upvotes": 30,
      "discussionId": "6833c65e49b9e903d3ddbd6a",
      "projectPage": "https://reasoningtrap.github.io/",
      "githubRepo": "https://github.com/ReasoningTrap/ReasoningTrap",
      "ai_summary": "A diagnostic set examines and categorizes reasoning rigidity in large language models, identifying patterns where models ignore instructions and default to familiar reasoning.",
      "ai_keywords": [
        "reasoning rigidity",
        "large language models",
        "long and complex reasoning tasks",
        "reasoning trajectories",
        "diagnostic set",
        "AIME",
        "MATH500",
        "Interpretation Overload",
        "Input Distrust",
        "Partial Instruction Attention"
      ]
    },
    "publishedAt": "2025-05-22T15:00:01.000Z",
    "title": "Reasoning Model is Stubborn: Diagnosing Instruction Overriding in\n  Reasoning Models",
    "summary": "Large language models have demonstrated remarkable proficiency in long and\ncomplex reasoning tasks. However, they frequently exhibit a problematic\nreliance on familiar reasoning patterns, a phenomenon we term reasoning\nrigidity. Despite explicit instructions from users, these models often\noverride clearly stated conditions and default to habitual reasoning\ntrajectories, leading to incorrect conclusions. This behavior presents\nsignificant challenges, particularly in domains such as mathematics and logic\npuzzle, where precise adherence to specified constraints is critical. To\nsystematically investigate reasoning rigidity, a behavior largely unexplored in\nprior work, we introduce a expert-curated diagnostic set, . Our\ndataset includes specially modified variants of existing mathematical\nbenchmarks, namely AIME and MATH500, as well as well-known puzzles deliberately\nredesigned to require deviation from familiar reasoning strategies. Using this\ndataset, we identify recurring contamination patterns that occur when models\ndefault to ingrained reasoning. Specifically, we categorize this contamination\ninto three distinctive modes: (i) Interpretation Overload, (ii) Input Distrust,\nand (iii) Partial Instruction Attention, each causing models to ignore or\ndistort provided instructions. We publicly release our diagnostic set to\nfacilitate future research on mitigating reasoning rigidity in language models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17225.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "61b15ce1a5dd7dc7024406dc",
      "avatarUrl": "/avatars/682ce5ee7d2fec7180dc8e1144cd12ab.svg",
      "fullname": "Yoonjeon Kim",
      "name": "yjyjyj98",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.17941",
      "authors": [
        {
          "_id": "6833cc35015eb19058ed83d9",
          "user": {
            "_id": "65811eeaa2284a018e51f1ba",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/dH8UZj6Kk5HJkI1DItCNm.jpeg",
            "isPro": true,
            "fullname": "Zigeng Chen",
            "user": "Zigeng",
            "type": "user"
          },
          "name": "Zigeng Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:10:12.285Z",
          "hidden": false
        },
        {
          "_id": "6833cc35015eb19058ed83da",
          "name": "Xinyin Ma",
          "hidden": false
        },
        {
          "_id": "6833cc35015eb19058ed83db",
          "name": "Gongfan Fang",
          "hidden": false
        },
        {
          "_id": "6833cc35015eb19058ed83dc",
          "name": "Ruonan Yu",
          "hidden": false
        },
        {
          "_id": "6833cc35015eb19058ed83dd",
          "name": "Xinchao Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T14:17:56.000Z",
      "submittedOnDailyAt": "2025-05-26T00:36:09.618Z",
      "title": "VeriThinker : Optimisation de modèles d'inférence par apprentissage de vérification",
      "submittedOnDailyBy": {
        "_id": "65811eeaa2284a018e51f1ba",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/dH8UZj6Kk5HJkI1DItCNm.jpeg",
        "isPro": true,
        "fullname": "Zigeng Chen",
        "user": "Zigeng",
        "type": "user"
      },
      "summary": "Les modèles logiques de raisonnement (LRMs) dépassent des tâches complexes en utilisant la logique de la chaîne de pensée (CoT). Cependant, le pensée excessive peut conduire à des chaînes de raisonnement inutilement longues, ce qui augmente significativement les coûts d'inférence. Pour atténuer ce problème, nous présentons un nouvel approche appelée VeriThinker. En contraste avec les méthodes traditionnelles qui ajustent directement les LRMs en utilisant des données CoT synthétiques et simples, VeriThinker ajuste les modèles en utilisant seulement des tâches de test de démonstration auxiliaires. En démontrant avec précision la précision des solutions CoT, les LRMs peuvent mieux déterminer la nécessité d'un état postérieur de réflexion et se montrent plus capables de restreindre le pensée excessive. Les expériences élargies montrent que VeriThinker réduit significativement la longueur des chaînes de raisonnement, tout en maintenant ou légèrement augmentant la précision. Dans le cas de DeepSeek-R1-Distill-Qwen-7B, sur le jeu de données MATH500, le nombre de tokens de raisonnement est réduit de 3790 à 2125, ce qui implique un accroissement de précision de 0,8% (94,0% à 94,8%), et sur AIME25, le nombre de tokens est réduit de 14321 à 10287, ce qui implique un accroissement de précision de 2,1% (38,7% à 40,8%). De plus, les expériences montrent que VeriThinker est capable de généraliser dans le domaine de la logique de l'hypothèse de manière 0-shot. Le code est disponible sur https://github.com/czg1225/VeriThinker.",
      "upvotes": 20,
      "discussionId": "6833cc36015eb19058ed8419",
      "githubRepo": "https://github.com/czg1225/VeriThinker",
      "ai_summary": "VeriThinker reduces the length of complex reasoning chains in Large Reasoning Models (LRMs) by fine-tuning them on a verification task, thereby decreasing inference costs without significantly sacrificing accuracy.",
      "ai_keywords": [
        "Large Reasoning Models (LRMs)",
        "Chain-of-Thought (CoT) reasoning",
        "CoT compression",
        "verification task",
        "reasoning chain lengths",
        "reasoning tokens",
        "accuracy",
        "DeepSeek-R1-Distill-Qwen-7B",
        "MATH500",
        "AIME25",
        "speculative reasoning"
      ]
    },
    "publishedAt": "2025-05-23T10:17:56.000Z",
    "title": "VeriThinker: Learning to Verify Makes Reasoning Model Efficient",
    "summary": "Large Reasoning Models (LRMs) excel at complex tasks using Chain-of-Thought\n(CoT) reasoning. However, their tendency to overthinking leads to unnecessarily\nlengthy reasoning chains, dramatically increasing inference costs. To mitigate\nthis issue, we introduce VeriThinker, a novel approach for CoT compression.\nUnlike conventional methods that fine-tune LRMs directly on the original\nreasoning task using synthetic concise CoT data, we innovatively fine-tune the\nmodel solely through an auxiliary verification task. By training LRMs to\naccurately verify the correctness of CoT solutions, the LRMs inherently become\nmore discerning about the necessity of subsequent self-reflection steps,\nthereby effectively suppressing overthinking. Extensive experiments validate\nthat VeriThinker substantially reduces reasoning chain lengths while\nmaintaining or even slightly improving accuracy. When applied to\nDeepSeek-R1-Distill-Qwen-7B, our approach reduces reasoning tokens on MATH500\nfrom 3790 to 2125 while improving accuracy by 0.8% (94.0% to 94.8%), and on\nAIME25, tokens decrease from 14321 to 10287 with a 2.1% accuracy gain (38.7% to\n40.8%). Additionally, our experiments demonstrate that VeriThinker can also be\nzero-shot generalized to speculative reasoning. Code is available at\nhttps://github.com/czg1225/VeriThinker",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17941.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65811eeaa2284a018e51f1ba",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/dH8UZj6Kk5HJkI1DItCNm.jpeg",
      "fullname": "Zigeng Chen",
      "name": "Zigeng",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.17561",
      "authors": [
        {
          "_id": "6833cb9030cd9df52a117557",
          "name": "Kwanyoung Kim",
          "hidden": false
        },
        {
          "_id": "6833cb9030cd9df52a117558",
          "name": "Sanghyun Kim",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T07:09:10.000Z",
      "submittedOnDailyAt": "2025-05-26T00:33:24.403Z",
      "title": "Le modèle connaît déjà le meilleur bruit : le modèle de dépilation vidéo sélectionne le bruit actif bayésien à travers l'attention.",
      "submittedOnDailyBy": {
        "_id": "63973ee44e7b4959dc98028f",
        "avatarUrl": "/avatars/2e166fee60844729479bfa4291796c8a.svg",
        "isPro": false,
        "fullname": "Kwanyoung",
        "user": "kwanyoung",
        "type": "user"
      },
      "summary": "La choix du bruit initial a un impact significatif sur la qualité du modèle de diffusion vidéo et sur la disposition des motifs. Si un bruit différent est utilisé pour le même motif, la vidéo générée peut varier considérablement. Les méthodes récentes dépendent de conceptions adjacentes comme des filtres de fréquence ou la planification des cadences adjacentes, mais ne ouvrent pas les signaux à l'intérieur du modèle. Pour résoudre ce problème, on propose ANSE (Cadre pour la Sélection Active de Bruit pour la Génération). ANSE dépend d'un modèle qui sélectionne un bruit de haute qualité en quantifiant l'incertitude basée sur l'attention. Son point clé est BANSA (Sélection Active de Bruit Bayésienne via l'Attention). BANSA mesure l'incertitude d'entropie de multiples échantillons d'attention de détorque pour estimer la confiance et la cohérence du modèle. Pour une traitement efficace lors de l'inférence, on introduit une approximation de masque de Bernoulli normalisée pour BANSA, et on utilise un diffuseur à un pas et quelques couches d'attention pour effectuer l'estimation de la note. Dans les expériences avec CogVideoX-2B et 5B, ANSE a amélioré la qualité et la cohérence temporelle de la vidéo avec un augmentation du temps d'inférence de 8% et 13%, offrant une approche rationnelle et généralisable dans la sélection de bruit dans les diffuseurs vidéo. Il peut consulter le site web du projet sur : https://anse-project.github.io/anse-project/",
      "upvotes": 18,
      "discussionId": "6833cb9430cd9df52a11765d",
      "projectPage": "https://anse-project.github.io/anse-project/",
      "ai_summary": "ANSE enhances video diffusion models by selecting noise seeds based on model confidence, improving video quality and temporal coherence with minimal increase in inference time.",
      "ai_keywords": [
        "video diffusion models",
        "noise seeds",
        "prompt alignment",
        "external priors",
        "frequency filters",
        "inter-frame smoothing",
        "ANSE",
        "Active Noise Selection for Generation",
        "BANSA",
        "Bayesian Active Noise Selection via Attention",
        "acquisition function",
        "entropy disagreement",
        "stochastic attention samples",
        "score estimation",
        "diffusion step",
        "temporal coherence"
      ]
    },
    "publishedAt": "2025-05-23T03:09:10.000Z",
    "title": "Model Already Knows the Best Noise: Bayesian Active Noise Selection via\n  Attention in Video Diffusion Model",
    "summary": "The choice of initial noise significantly affects the quality and prompt\nalignment of video diffusion models, where different noise seeds for the same\nprompt can lead to drastically different generations. While recent methods rely\non externally designed priors such as frequency filters or inter-frame\nsmoothing, they often overlook internal model signals that indicate which noise\nseeds are inherently preferable. To address this, we propose ANSE (Active Noise\nSelection for Generation), a model-aware framework that selects high-quality\nnoise seeds by quantifying attention-based uncertainty. At its core is BANSA\n(Bayesian Active Noise Selection via Attention), an acquisition function that\nmeasures entropy disagreement across multiple stochastic attention samples to\nestimate model confidence and consistency. For efficient inference-time\ndeployment, we introduce a Bernoulli-masked approximation of BANSA that enables\nscore estimation using a single diffusion step and a subset of attention\nlayers. Experiments on CogVideoX-2B and 5B demonstrate that ANSE improves video\nquality and temporal coherence with only an 8% and 13% increase in inference\ntime, respectively, providing a principled and generalizable approach to noise\nselection in video diffusion. See our project page:\nhttps://anse-project.github.io/anse-project/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17561.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "63973ee44e7b4959dc98028f",
      "avatarUrl": "/avatars/2e166fee60844729479bfa4291796c8a.svg",
      "fullname": "Kwanyoung",
      "name": "kwanyoung",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.17873",
      "authors": [
        {
          "_id": "68341f661d53989a8ecb685d",
          "user": {
            "_id": "6684b284dc7b0ae2cc67660c",
            "avatarUrl": "/avatars/54b3c0c4d808f293d78085d4d504570a.svg",
            "isPro": false,
            "fullname": "liuwanhao",
            "user": "wanhaoliu",
            "type": "user"
          },
          "name": "Wanhao Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:10:54.880Z",
          "hidden": false
        },
        {
          "_id": "68341f661d53989a8ecb685e",
          "user": {
            "_id": "646a11791556443f24b582e9",
            "avatarUrl": "/avatars/b54d416ddca2f124492ba51bc4b95fd2.svg",
            "isPro": false,
            "fullname": "Zonglin Yang",
            "user": "ZonglinY",
            "type": "user"
          },
          "name": "Zonglin Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:08:35.046Z",
          "hidden": false
        },
        {
          "_id": "68341f661d53989a8ecb685f",
          "name": "Jue Wang",
          "hidden": false
        },
        {
          "_id": "68341f661d53989a8ecb6860",
          "name": "Lidong Bing",
          "hidden": false
        },
        {
          "_id": "68341f661d53989a8ecb6861",
          "user": {
            "_id": "64bce15bafd1e46c5504ad38",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64bce15bafd1e46c5504ad38/vkEjiu-mIagKlrXzDH75o.png",
            "isPro": false,
            "fullname": "Di Zhang",
            "user": "di-zhang-fdu",
            "type": "user"
          },
          "name": "Di Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:40:05.397Z",
          "hidden": false
        },
        {
          "_id": "68341f661d53989a8ecb6862",
          "name": "Dongzhan Zhou",
          "hidden": false
        },
        {
          "_id": "68341f661d53989a8ecb6863",
          "name": "Yuqiang Li",
          "hidden": false
        },
        {
          "_id": "68341f661d53989a8ecb6864",
          "name": "Houqiang Li",
          "hidden": false
        },
        {
          "_id": "68341f661d53989a8ecb6865",
          "name": "Erik Cambria",
          "hidden": false
        },
        {
          "_id": "68341f661d53989a8ecb6866",
          "name": "Wanli Ouyang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T13:24:50.000Z",
      "submittedOnDailyAt": "2025-05-26T06:33:21.775Z",
      "title": "MOOSE-Chem3 : Méthode pour établir la classification des hypothèses à travers la rétroaction sur des expériences",
      "submittedOnDailyBy": {
        "_id": "646a11791556443f24b582e9",
        "avatarUrl": "/avatars/b54d416ddca2f124492ba51bc4b95fd2.svg",
        "isPro": false,
        "fullname": "Zonglin Yang",
        "user": "ZonglinY",
        "type": "user"
      },
      "summary": "La hypothèse d'ordonnancement est un élément important dans l'automatisation du découverte scientifique, surtout dans les domaines de la science naturelle où les expériences de laboratoire sont coûteuses et limitées par le type d'expérience. L'approche actuelle se concentre sur l'ordonnancement préalable des expériences, dépendant uniquement des raisons internes du modèle de langue et sans inclure les résultats réels des expériences. Nous présentons la tâche d'ordonner les hypothèses guidées par l'expérience, avec l'objectif de prioriser les hypothèses basées sur des résultats mesurés précédemment. Cependant, le développement de cette stratégie est difficile dans les domaines de la science naturelle en raison de l'inefficacité de répéter les expériences en pratique. Pour faire face à cette situation, nous proposons un agent virtuel basé sur des informations de trois domaines, où la priorité des hypothèses est décidée en fonction de leur similitude avec des hypothèses connues et où sont modélisées leurs sensibilités au bruit. Nous construisons un ensemble de données basé sur 124 hypothèses chimiques et développons un méthode d'ordonnancement guidée par l'expérience grâce à cet agent virtuel. Nous groupons les hypothèses qui partagent des caractéristiques fonctionnelles et priorisons les hypothèses basées sur les insights obtenus de la rétroaction des expériences simulées. Les expériences montrent que notre méthode dépasse les normes précédentes et les limites fortes avant les expériences.",
      "upvotes": 14,
      "discussionId": "68341f671d53989a8ecb68b8",
      "ai_summary": "A novel simulator and experiment-guided ranking method improve hypothesis prioritization in scientific discovery by incorporating simulated experimental outcomes.",
      "ai_keywords": [
        "hypothesis ranking",
        "automated scientific discovery",
        "natural sciences",
        "wet-lab experiments",
        "large language model",
        "pre-experiment ranking",
        "experiment-guided ranking",
        "hypothesis performance",
        "similarity",
        "noise",
        "dataset",
        "pseudo experiment-guided ranking",
        "clustering",
        "functional characteristics",
        "simulated experimental feedback"
      ]
    },
    "publishedAt": "2025-05-23T09:24:50.000Z",
    "title": "MOOSE-Chem3: Toward Experiment-Guided Hypothesis Ranking via Simulated\n  Experimental Feedback",
    "summary": "Hypothesis ranking is a crucial component of automated scientific discovery,\nparticularly in natural sciences where wet-lab experiments are costly and\nthroughput-limited. Existing approaches focus on pre-experiment ranking,\nrelying solely on large language model's internal reasoning without\nincorporating empirical outcomes from experiments. We introduce the task of\nexperiment-guided ranking, which aims to prioritize candidate hypotheses based\non the results of previously tested ones. However, developing such strategies\nis challenging due to the impracticality of repeatedly conducting real\nexperiments in natural science domains. To address this, we propose a simulator\ngrounded in three domain-informed assumptions, modeling hypothesis performance\nas a function of similarity to a known ground truth hypothesis, perturbed by\nnoise. We curate a dataset of 124 chemistry hypotheses with experimentally\nreported outcomes to validate the simulator. Building on this simulator, we\ndevelop a pseudo experiment-guided ranking method that clusters hypotheses by\nshared functional characteristics and prioritizes candidates based on insights\nderived from simulated experimental feedback. Experiments show that our method\noutperforms pre-experiment baselines and strong ablations.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17873.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "646a11791556443f24b582e9",
      "avatarUrl": "/avatars/b54d416ddca2f124492ba51bc4b95fd2.svg",
      "fullname": "Zonglin Yang",
      "name": "ZonglinY",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.16211",
      "authors": [
        {
          "_id": "6833d9cfdf7cbb5c087cb9cd",
          "name": "Kai Li",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9ce",
          "name": "Can Shen",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9cf",
          "name": "Yile Liu",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9d0",
          "name": "Jirui Han",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9d1",
          "name": "Kelong Zheng",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9d2",
          "name": "Xuechao Zou",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9d3",
          "name": "Zhe Wang",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9d4",
          "name": "Xingjian Du",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9d5",
          "name": "Shun Zhang",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9d6",
          "name": "Hanjun Luo",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9d7",
          "name": "Yingbin Jin",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9d8",
          "name": "Xinxin Xing",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9d9",
          "name": "Ziyang Ma",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9da",
          "name": "Yue Liu",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9db",
          "user": {
            "_id": "64c6627d5671d42e0adfad56",
            "avatarUrl": "/avatars/8b98054b2911b86dcc4856a15306e60f.svg",
            "isPro": false,
            "fullname": "jiaxiaojunQAQ",
            "user": "jiaxiaojunQAQ",
            "type": "user"
          },
          "name": "Xiaojun Jia",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:09:37.847Z",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9dc",
          "name": "Yifan Zhang",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9dd",
          "name": "Junfeng Fang",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9de",
          "name": "Kun Wang",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9df",
          "name": "Yibo Yan",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9e0",
          "name": "Haoyang Li",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9e1",
          "name": "Yiming Li",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9e2",
          "name": "Xiaobin Zhuang",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9e3",
          "name": "Yang Liu",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9e4",
          "name": "Haibo Hu",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9e5",
          "name": "Zhuo Chen",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9e6",
          "name": "Zhizheng Wu",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9e7",
          "name": "Xiaolin Hu",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9e8",
          "name": "Eng-Siong Chng",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9e9",
          "name": "XiaoFeng Wang",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9ea",
          "name": "Wenyuan Xu",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9eb",
          "name": "Wei Dong",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9ec",
          "name": "Xinfeng Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T04:27:46.000Z",
      "submittedOnDailyAt": "2025-05-26T01:33:43.107Z",
      "title": "AudioTrust : Marque de fiabilité multidimensionnelle du modèle de langage vocal",
      "submittedOnDailyBy": {
        "_id": "6387676c23da90491eb9fb16",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669818175965-noauth.jpeg",
        "isPro": false,
        "fullname": "Kai Li",
        "user": "JusperLee",
        "type": "user"
      },
      "summary": "Le développement rapide et l'expansion des applications des modèles de langage audio (ALLMs) a démontré la nécessité de comprendre sa confiance de manière rigoureuse. Cependant, les recherches systématiques sur l'évaluation de ces modèles, surtout sur les risques propres aux modèles audio, ont été peu explorées. Les cadres actuels d'évaluation se concentrent principalement sur les modèles de texte ou sur des dimensions limitées de sécurité, et ne s'adaptent pas adéquatement aux caractéristiques uniques et aux scénarios d'application des modèles audio. Dans ce contexte, AudioTrust est présenté comme le premier cadre d'évaluation de confiance multidimensionnelle et un benchmark spécifiquement conçu pour les ALLMs. AudioTrust encourage l'évaluation sur six dimensions clés : justice, hacking, sécurité, privacité, robustesse et authentification. Pour évaluer ces dimensions, AudioTrust configure 18 configurations expérimentales différentes, avec un ensemble de données exquisement construit avec plus de 4,420 échantillons d'audio/texte. Cet ensemble de données a été conçu pour extraire des scénarios réels tels que des conversations quotidiennes, des appels d'urgence et des interactions avec des assistants vocaux, pour étudier la confiance multidimensionnelle des ALLMs. Pour l'évaluation, le benchmark établit 9 points d'évaluation uniques d'audio et utilise un processus automatique à grande échelle pour fournir des scores objectifs et échelonnables. Les résultats des expériences montrent que les modèles de ALLMs les plus avancés, tant ouverts que fermés, lorsqu'ils sont confrontés à des scénarios de haut risque d'audio, se trouvent à des limites et limites claires de confiance, et offrent une privacité end-to-end précieuse pour l'introduction de confiance sûre dans les modèles audio futurs. La plateforme et le benchmark sont disponibles sur https://github.com/JusperLee/AudioTrust.",
      "upvotes": 14,
      "discussionId": "6833d9d1df7cbb5c087cba85",
      "githubRepo": "https://github.com/JusperLee/AudioTrust",
      "ai_summary": "AudioTrust evaluates the trustworthiness of Audio Large Language Models across multifaceted dimensions, using a comprehensive dataset and specific metrics to assess their performance in real-world audio scenarios.",
      "ai_keywords": [
        "Audio Large Language Models",
        "ALLMs",
        "trustworthiness",
        "fairness",
        "hallucination",
        "safety",
        "privacy",
        "robustness",
        "authentication",
        "AudioTrust",
        "experimental setups",
        "audio-specific evaluation metrics",
        "automated pipeline"
      ]
    },
    "publishedAt": "2025-05-22T00:27:46.000Z",
    "title": "AudioTrust: Benchmarking the Multifaceted Trustworthiness of Audio Large\n  Language Models",
    "summary": "The rapid advancement and expanding applications of Audio Large Language\nModels (ALLMs) demand a rigorous understanding of their trustworthiness.\nHowever, systematic research on evaluating these models, particularly\nconcerning risks unique to the audio modality, remains largely unexplored.\nExisting evaluation frameworks primarily focus on the text modality or address\nonly a restricted set of safety dimensions, failing to adequately account for\nthe unique characteristics and application scenarios inherent to the audio\nmodality. We introduce AudioTrust-the first multifaceted trustworthiness\nevaluation framework and benchmark specifically designed for ALLMs. AudioTrust\nfacilitates assessments across six key dimensions: fairness, hallucination,\nsafety, privacy, robustness, and authentication. To comprehensively evaluate\nthese dimensions, AudioTrust is structured around 18 distinct experimental\nsetups. Its core is a meticulously constructed dataset of over 4,420 audio/text\nsamples, drawn from real-world scenarios (e.g., daily conversations, emergency\ncalls, voice assistant interactions), specifically designed to probe the\nmultifaceted trustworthiness of ALLMs. For assessment, the benchmark carefully\ndesigns 9 audio-specific evaluation metrics, and we employ a large-scale\nautomated pipeline for objective and scalable scoring of model outputs.\nExperimental results reveal the trustworthiness boundaries and limitations of\ncurrent state-of-the-art open-source and closed-source ALLMs when confronted\nwith various high-risk audio scenarios, offering valuable insights for the\nsecure and trustworthy deployment of future audio models. Our platform and\nbenchmark are available at https://github.com/JusperLee/AudioTrust.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16211.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6387676c23da90491eb9fb16",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669818175965-noauth.jpeg",
      "fullname": "Kai Li",
      "name": "JusperLee",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.17618",
      "authors": [
        {
          "_id": "6833eeaf98515618764fc204",
          "user": {
            "_id": "6672937ceac0fb1b9e516595",
            "avatarUrl": "/avatars/5eea5657016572f60b0ecd0fa9a7dae4.svg",
            "isPro": false,
            "fullname": "haoran he",
            "user": "haoranhe",
            "type": "user"
          },
          "name": "Haoran He",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:09:19.593Z",
          "hidden": false
        },
        {
          "_id": "6833eeaf98515618764fc205",
          "name": "Jiajun Liang",
          "hidden": false
        },
        {
          "_id": "6833eeaf98515618764fc206",
          "name": "Xintao Wang",
          "hidden": false
        },
        {
          "_id": "6833eeaf98515618764fc207",
          "name": "Pengfei Wan",
          "hidden": false
        },
        {
          "_id": "6833eeaf98515618764fc208",
          "name": "Di Zhang",
          "hidden": false
        },
        {
          "_id": "6833eeaf98515618764fc209",
          "name": "Kun Gai",
          "hidden": false
        },
        {
          "_id": "6833eeaf98515618764fc20a",
          "name": "Ling Pan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T08:25:46.000Z",
      "submittedOnDailyAt": "2025-05-26T04:32:07.257Z",
      "title": "Génération d'échelles d'images et de vidéos en temps réel au cours de l'essai en utilisant l'évolutionniste",
      "submittedOnDailyBy": {
        "_id": "667187ba9ab144eb3ac43a1b",
        "avatarUrl": "/avatars/db5558aa1c5160b9aee8b58573271959.svg",
        "isPro": false,
        "fullname": "Runze Liu",
        "user": "RyanLiu112",
        "type": "user"
      },
      "summary": "Quand le coût de l'expansion du calcul (données et paramètres) augmente significativement lors de l'entraînement d'un modèle, l'expansion du calcul pendant l'inférence (TTS) apparaît comme une direction potentielle pour améliorer le rendement du modèle de génération en assignant une quantité supplémentaire de calculs. TTS a démontré des succès dans diverses tâches de langage, mais sa compréhension dans le fonctionnement de l'expansion pour des modèles de génération d'images ou de vidéos (basés sur la diffusion ou les flux) a été retardée. Des études récentes ont essayé d'explorer des stratégies pour l'inférence dans des tâches visuelles, mais ces approches ont des limites importantes et sont limitées à des domaines spécifiques, ce qui peut conduire à une perte de diversité ou à un optimum excessif. Dans cet article, nous proposons un nouveau méthode efficace et générale de TTS appelé \"EvoSearch\". Ce méthode améliore efficacement la capacité d'expansion des modèles de diffusion ou de flux pour la génération d'images ou de vidéos, et ne nécessite pas d'entraînements supplémentaires ou d'expansions du modèle. EvoSearch utilise les principes de l'évolution pour rédefinir le problème d'expansion dans des modèles de diffusion ou de flux comme un problème de recherche évolutive, explorant et améliorant des chemins de calcul généraux et efficaces. Il inclut des conceptions appropriées et des structures de mutation qui s'adaptent au processus de diffusion, maintenant la diversité de différentes populations tout en générant de haute qualité de descendants continus. Dans une évaluation large de modèles de diffusion ou de flux pour des tâches de génération d'images ou de vidéos, notre méthode supere expérimentalement les méthodes existantes, offre une grande diversité et montre une excellente généralisation sous de nouvelles évaluations. Ce projet peut être accédé via https://tinnerhrhe.github.io/evosearch.",
      "upvotes": 11,
      "discussionId": "6833eeb198515618764fc277",
      "projectPage": "https://tinnerhrhe.github.io/evosearch/",
      "githubRepo": "https://github.com/tinnerhrhe/EvoSearch-codes",
      "ai_summary": "EvoSearch, an evolutionary search method, enhances test-time scaling for diffusion and flow-based generative models, improving image and video generation quality, diversity, and generalizability.",
      "ai_keywords": [
        "test-time scaling",
        "TTS",
        "image generation",
        "video generation",
        "diffusion models",
        "flow-based models",
        "denoising trajectory",
        "stochastic differential equation",
        "selection",
        "mutation",
        "EvoSearch"
      ]
    },
    "publishedAt": "2025-05-23T04:25:46.000Z",
    "title": "Scaling Image and Video Generation via Test-Time Evolutionary Search",
    "summary": "As the marginal cost of scaling computation (data and parameters) during\nmodel pre-training continues to increase substantially, test-time scaling (TTS)\nhas emerged as a promising direction for improving generative model performance\nby allocating additional computation at inference time. While TTS has\ndemonstrated significant success across multiple language tasks, there remains\na notable gap in understanding the test-time scaling behaviors of image and\nvideo generative models (diffusion-based or flow-based models). Although recent\nworks have initiated exploration into inference-time strategies for vision\ntasks, these approaches face critical limitations: being constrained to\ntask-specific domains, exhibiting poor scalability, or falling into reward\nover-optimization that sacrifices sample diversity. In this paper, we propose\nEvolutionary Search (EvoSearch), a novel, generalist, and\nefficient TTS method that effectively enhances the scalability of both image\nand video generation across diffusion and flow models, without requiring\nadditional training or model expansion. EvoSearch reformulates test-time\nscaling for diffusion and flow models as an evolutionary search problem,\nleveraging principles from biological evolution to efficiently explore and\nrefine the denoising trajectory. By incorporating carefully designed selection\nand mutation mechanisms tailored to the stochastic differential equation\ndenoising process, EvoSearch iteratively generates higher-quality offspring\nwhile preserving population diversity. Through extensive evaluation across both\ndiffusion and flow architectures for image and video generation tasks, we\ndemonstrate that our method consistently outperforms existing approaches,\nachieves higher diversity, and shows strong generalizability to unseen\nevaluation metrics. Our project is available at the website\nhttps://tinnerhrhe.github.io/evosearch.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17618.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "667187ba9ab144eb3ac43a1b",
      "avatarUrl": "/avatars/db5558aa1c5160b9aee8b58573271959.svg",
      "fullname": "Runze Liu",
      "name": "RyanLiu112",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.15692",
      "authors": [
        {
          "_id": "68306ffdff038ca6400a153a",
          "user": {
            "_id": "6747de57f8cab58c22ec94a2",
            "avatarUrl": "/avatars/5bae0341862fac24564781c0fa32aac5.svg",
            "isPro": false,
            "fullname": "Jinyang Wu",
            "user": "Jinyang23",
            "type": "user"
          },
          "name": "Jinyang Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:13:57.397Z",
          "hidden": false
        },
        {
          "_id": "68306ffdff038ca6400a153b",
          "user": {
            "_id": "667fdaee20ee9ac417c7708c",
            "avatarUrl": "/avatars/69dfba6ff392643af1dcfe8af0a42ae9.svg",
            "isPro": false,
            "fullname": "Chonghua Liao",
            "user": "ChonghuaLiao",
            "type": "user"
          },
          "name": "Chonghua Liao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:16:00.056Z",
          "hidden": false
        },
        {
          "_id": "68306ffdff038ca6400a153c",
          "name": "Mingkuan Feng",
          "hidden": false
        },
        {
          "_id": "68306ffdff038ca6400a153d",
          "name": "Shuai Zhang",
          "hidden": false
        },
        {
          "_id": "68306ffdff038ca6400a153e",
          "name": "Zhengqi Wen",
          "hidden": false
        },
        {
          "_id": "68306ffdff038ca6400a153f",
          "name": "Pengpeng Shao",
          "hidden": false
        },
        {
          "_id": "68306ffdff038ca6400a1540",
          "name": "Huazhe Xu",
          "hidden": false
        },
        {
          "_id": "68306ffdff038ca6400a1541",
          "name": "Jianhua Tao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T16:06:10.000Z",
      "submittedOnDailyAt": "2025-05-26T01:19:22.736Z",
      "title": "\"Optimisation de la Politique de Sindre : Une Large Connexion entre Guides Extérieurs et Capacités Intérieures\"",
      "submittedOnDailyBy": {
        "_id": "6747de57f8cab58c22ec94a2",
        "avatarUrl": "/avatars/5bae0341862fac24564781c0fa32aac5.svg",
        "isPro": false,
        "fullname": "Jinyang Wu",
        "user": "Jinyang23",
        "type": "user"
      },
      "summary": "L'apprentissage par renforcement (RL) a apparu comme un méthode efficace pour l'entraînement de modèles de logique, mais les approches actuelles de RL souvent orientent la distribution des sorties vers le chemin de maximisation de la récompense, sans incorporer des connaissances externes. Cela limite la capacité d'exploration et restreint le développement de compétences logiques par rapport aux modèles basiques. Pour résoudre ces limitations, nous proposons un nouveau cadre appelé TAPO (Optimisation de Politique par Agrégation de Modèles de Pensée). TAPO renforce le RL en intégrant des guides de haut niveau externes, connus sous le nom de \"modèles de pensée\". En adaptant des structures de pensée structurées pendant l'entraînement, TAPO équilibre mieux l'exploration interne et l'utilisation des guides externes. À travers des expériences larges, notre approche a amélioré le rendement sur AIME d'au-delà de 99%, sur AMC de 41% et sur Minerva Math de 17%. Bien que ces modèles de pensée soient abstractés à partir de 500 échantillons, ils ont été généralisés largement dans différentes tâches et modèles, démontrant l'applicabilité de TAPO dans divers domaines et tâches. Nos analyses en cours indiquent que l'introduction de guides externes améliore significativement l'explicabilité des actions d'inférence et la compréhension des résultats, permettant la création de modèles logiques robustes.",
      "upvotes": 11,
      "discussionId": "68306ffeff038ca6400a1569",
      "ai_summary": "A novel RL framework, TAPO, integrates external guidance to enhance model performance and exploration compared to existing methods.",
      "ai_keywords": [
        "reinforcement learning",
        "TAPO",
        "Thought-Augmented Policy Optimization",
        "high-level guidance",
        "thought patterns",
        "model exploration",
        "AIME",
        "AMC",
        "Minerva Math",
        "reasoning models",
        "explainability",
        "output readability"
      ]
    },
    "publishedAt": "2025-05-21T12:06:10.000Z",
    "title": "Thought-Augmented Policy Optimization: Bridging External Guidance and\n  Internal Capabilities",
    "summary": "Reinforcement learning (RL) has emerged as an effective method for training\nreasoning models. However, existing RL approaches typically bias the model's\noutput distribution toward reward-maximizing paths without introducing external\nknowledge. This limits their exploration capacity and results in a narrower\nreasoning capability boundary compared to base models. To address this\nlimitation, we propose TAPO (Thought-Augmented Policy Optimization), a novel\nframework that augments RL by incorporating external high-level guidance\n(\"thought patterns\"). By adaptively integrating structured thoughts during\ntraining, TAPO effectively balances model-internal exploration and external\nguidance exploitation. Extensive experiments show that our approach\nsignificantly outperforms GRPO by 99% on AIME, 41% on AMC, and 17% on Minerva\nMath. Notably, these high-level thought patterns, abstracted from only 500\nprior samples, generalize effectively across various tasks and models. This\nhighlights TAPO's potential for broader applications across multiple tasks and\ndomains. Our further analysis reveals that introducing external guidance\nproduces powerful reasoning models with superior explainability of inference\nbehavior and enhanced output readability.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15692.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6747de57f8cab58c22ec94a2",
      "avatarUrl": "/avatars/5bae0341862fac24564781c0fa32aac5.svg",
      "fullname": "Jinyang Wu",
      "name": "Jinyang23",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.17399",
      "authors": [
        {
          "_id": "6833fd69fe87d9433d098068",
          "name": "Haoyu Sun",
          "hidden": false
        },
        {
          "_id": "6833fd69fe87d9433d098069",
          "name": "Huichen Will Wang",
          "hidden": false
        },
        {
          "_id": "6833fd69fe87d9433d09806a",
          "user": {
            "_id": "645b4819f9d4ec91fdd54852",
            "avatarUrl": "/avatars/e12efb8e030688a0afcc72176b453fb3.svg",
            "isPro": false,
            "fullname": "Kuvvi Gu",
            "user": "Kuvvi",
            "type": "user"
          },
          "name": "Jiawei Gu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:09:01.542Z",
          "hidden": false
        },
        {
          "_id": "6833fd69fe87d9433d09806b",
          "name": "Linjie Li",
          "hidden": false
        },
        {
          "_id": "6833fd69fe87d9433d09806c",
          "name": "Yu Cheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T02:16:11.000Z",
      "submittedOnDailyAt": "2025-05-26T08:05:22.618Z",
      "title": "FullFront : Workflow de la Neurociencie Frontale MLLM Benchmark",
      "submittedOnDailyBy": {
        "_id": "645b4819f9d4ec91fdd54852",
        "avatarUrl": "/avatars/e12efb8e030688a0afcc72176b453fb3.svg",
        "isPro": false,
        "fullname": "Kuvvi Gu",
        "user": "Kuvvi",
        "type": "user"
      },
      "summary": "Front-end engineering est un processus complexe qui comprend la conception de designs, leur traduction en code et l'amélioration itérative de l'implémentation. Les derniers benchmarks ont principalement porté sur la conversion de designs visuels en code, mais nous présentons \"FullFront\", un benchmark qui évalue le processus complet du développement d'une écran. FullFront évalue trois tâches fondamentales : le design de page web (étape de conception), la test d'accessibilité et la QA (compréhension de l'organisation visuelle et des éléments), et la génération de code de page (étape d'exécution). Les benchmarks actuels utilisent des codes étendus, en utilisant des scraping de sites web ou des HTML simplifiés générés par des LLMs, alors que FullFront utilise un nouveau processus à deux étapes pour convertir des pages web réelles en HTML clair et standard, en maintenant divers designs visuels et en évitant les problèmes de droits d'auteur. Les expériences d'expansion des MLLMs ont révélé des limitations significatives dans la reconnaissance de pages, la génération de code (en particulier dans le traitement d'images et le design de layout), et l'implémentation d'interactions. Nos résultats montrent des différences quantitatives dans le rendement des modèles et des tâches, et clairement soulignent la grande différence entre les capacités actuelles des MLLMs et le rendement d'un expert en front-end engineering. Le benchmark FullFront et son code sont disponibles sur https://github.com/Mikivishy/FullFront.",
      "upvotes": 10,
      "discussionId": "6833fd6bfe87d9433d0980c2",
      "githubRepo": "https://github.com/Mikivishy/FullFront",
      "ai_summary": "FullFront is a benchmark evaluating Multimodal Large Language Models across conceptualization, comprehension, and implementation phases in front-end engineering.",
      "ai_keywords": [
        "Multimodal Large Language Models",
        "MLLMs",
        "Webpage Design",
        "Webpage Perception QA",
        "Webpage Code Generation",
        "front-end engineering"
      ]
    },
    "publishedAt": "2025-05-22T22:16:11.000Z",
    "title": "FullFront: Benchmarking MLLMs Across the Full Front-End Engineering\n  Workflow",
    "summary": "Front-end engineering involves a complex workflow where engineers\nconceptualize designs, translate them into code, and iteratively refine the\nimplementation. While recent benchmarks primarily focus on converting visual\ndesigns to code, we present FullFront, a benchmark designed to evaluate\nMultimodal Large Language Models (MLLMs) across the full front-end\ndevelopment pipeline. FullFront assesses three fundamental tasks that map\ndirectly to the front-end engineering pipeline: Webpage Design\n(conceptualization phase), Webpage Perception QA (comprehension of visual\norganization and elements), and Webpage Code Generation (implementation phase).\nUnlike existing benchmarks that use either scraped websites with bloated code\nor oversimplified LLM-generated HTML, FullFront employs a novel, two-stage\nprocess to transform real-world webpages into clean, standardized HTML while\nmaintaining diverse visual designs and avoiding copyright issues. Extensive\ntesting of state-of-the-art MLLMs reveals significant limitations in page\nperception, code generation (particularly for image handling and layout), and\ninteraction implementation. Our results quantitatively demonstrate performance\ndisparities across models and tasks, and highlight a substantial gap between\ncurrent MLLM capabilities and human expert performance in front-end\nengineering. The FullFront benchmark and code are available in\nhttps://github.com/Mikivishy/FullFront.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17399.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645b4819f9d4ec91fdd54852",
      "avatarUrl": "/avatars/e12efb8e030688a0afcc72176b453fb3.svg",
      "fullname": "Kuvvi Gu",
      "name": "Kuvvi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.14669",
      "authors": [
        {
          "_id": "682da9d3781210358218a950",
          "name": "Roberto L. Castro",
          "hidden": false
        },
        {
          "_id": "682da9d3781210358218a951",
          "name": "Andrei Panferov",
          "hidden": false
        },
        {
          "_id": "682da9d3781210358218a952",
          "name": "Soroush Tabesh",
          "hidden": false
        },
        {
          "_id": "682da9d3781210358218a953",
          "name": "Oliver Sieberling",
          "hidden": false
        },
        {
          "_id": "682da9d3781210358218a954",
          "name": "Jiale Chen",
          "hidden": false
        },
        {
          "_id": "682da9d3781210358218a955",
          "name": "Mahdi Nikdan",
          "hidden": false
        },
        {
          "_id": "682da9d3781210358218a956",
          "name": "Saleh Ashkboos",
          "hidden": false
        },
        {
          "_id": "682da9d3781210358218a957",
          "name": "Dan Alistarh",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/623753b5eddd7763adc9346a/N9QxR8-CzRd2UcOj9uwRR.png"
      ],
      "publishedAt": "2025-05-20T17:55:50.000Z",
      "submittedOnDailyAt": "2025-05-26T08:34:57.302Z",
      "title": "Capitaine : L'entraînement native FP4 est l'option la plus appropriée pour des modèles de langage à grande échelle.",
      "submittedOnDailyBy": {
        "_id": "623753b5eddd7763adc9346a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623753b5eddd7763adc9346a/rcpQAKZNrkn1-tMtraQBX.jpeg",
        "isPro": false,
        "fullname": "Andrei Panferov",
        "user": "BlackSamorez",
        "type": "user"
      },
      "summary": "Le développement rapide des modèles de langage grands (LLMs) se produit parallèlement à un augmentation sans précédent des demandes de calcul. Le coût d'entraînement des modèles les plus avancés double dans quelques mois. L'utilisation directe de calculs à faible précision pour entraîner les modèles améliore à la fois la vitesse de calcul et l'efficacité énergétique. En particulier, l'architecture Blackwell récente de NVIDIA promeut spécialement l'opération à faible précision FP4 et cherche un grand gain d'efficacité. Cependant, les algorithmes actuellement utilisés pour l'entraînement en précision FP4 dans les LLMs présentent de grands problèmes et dépendent généralement de rétrocours de précision mixte. Dans cet article, on étudie systématiquement l'entraînement avec FP4 soutenu par matériel et présente un nouvel approche appelée Quartet. Cette approche permet d'entraîner de manière précise depuis le début avec FP4, en faisant que toutes les principales opérations (par exemple, les couches linéaires) se réalisent à faible précision. Par des évaluations larges pour des modèles de type Llama, on quantifie l'équilibre entre performance et changements dans la profondeur des bits, et on découvre un nouveau scalateur qui définit un méthode d'entraînement à faible précision appelée approximation optimale. Cela est appelé Quartet, il est implémenté en utilisant des CUDA canards optimisés pour les GPU Blackwell de NVIDIA et il est montré qu'il peut atteindre la précision la plus avancée en FP4 et entraîner avec succès des modèles de grande échelle. Notre méthode est une compétition avec l'entraînement en précision standard et FP8, et montre un entraînement complet basé sur FP4. Notre code est disponible sur https://github.com/IST-DASLab/Quartet.",
      "upvotes": 10,
      "discussionId": "682da9d4781210358218a982",
      "ai_summary": "Quartet, a hardware-supported FP4 training approach for large language models, demonstrates state-of-the-art accuracy while significantly reducing computational costs compared to standard or FP8 precision.",
      "ai_keywords": [
        "large language models",
        "low-precision arithmetic",
        "Blackwell architecture",
        "FP4",
        "mixed-precision",
        "linear layers",
        "low-precision scaling law",
        "CUDA kernels"
      ]
    },
    "publishedAt": "2025-05-20T13:55:50.000Z",
    "title": "Quartet: Native FP4 Training Can Be Optimal for Large Language Models",
    "summary": "The rapid advancement of large language models (LLMs) has been paralleled by\nunprecedented increases in computational demands, with training costs for\nstate-of-the-art models doubling every few months. Training models directly in\nlow-precision arithmetic offers a solution, by improving both computational\nthroughput and energy efficiency. Specifically, NVIDIA's recent Blackwell\narchitecture facilitates extremely low-precision operations, specifically FP4\nvariants, promising substantial efficiency gains. Yet, current algorithms for\ntraining LLMs in FP4 precision face significant accuracy degradation and often\nrely on mixed-precision fallbacks. In this paper, we systematically investigate\nhardware-supported FP4 training and introduce Quartet, a new approach enabling\naccurate, end-to-end FP4 training with all the major computations (in e.g.\nlinear layers) being performed in low precision. Through extensive evaluations\non Llama-type models, we reveal a new low-precision scaling law that quantifies\nperformance trade-offs across varying bit-widths and allows us to identify a\n\"near-optimal\" low-precision training technique in terms of\naccuracy-vs-computation, called Quartet. We implement Quartet using optimized\nCUDA kernels tailored for NVIDIA Blackwell GPUs, and show that it can achieve\nstate-of-the-art accuracy for FP4 precision, successfully training\nbillion-scale models. Our method demonstrates that fully FP4-based training is\na competitive alternative to standard-precision and FP8 training. Our code is\navailable at https://github.com/IST-DASLab/Quartet.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/623753b5eddd7763adc9346a/N9QxR8-CzRd2UcOj9uwRR.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14669.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "623753b5eddd7763adc9346a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623753b5eddd7763adc9346a/rcpQAKZNrkn1-tMtraQBX.jpeg",
      "fullname": "Andrei Panferov",
      "name": "BlackSamorez",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 36
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.17558",
      "authors": [
        {
          "_id": "6833c8af029c4a53a60a5dfa",
          "user": {
            "_id": "648749094dea003c6dae810f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/648749094dea003c6dae810f/gHUHSBt1zrt8wjO1YwTNu.jpeg",
            "isPro": false,
            "fullname": "Shrey Pandit",
            "user": "SP2001",
            "type": "user"
          },
          "name": "Shrey Pandit",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-26T01:49:36.568Z",
          "hidden": false
        },
        {
          "_id": "6833c8af029c4a53a60a5dfb",
          "user": {
            "_id": "62fa7294363251ee40a41dba",
            "avatarUrl": "/avatars/869c6de9a1cb2ded690ae56559916cae.svg",
            "isPro": false,
            "fullname": "Ashwin V",
            "user": "ashwinnv",
            "type": "user"
          },
          "name": "Ashwin Vinod",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:10:22.347Z",
          "hidden": false
        },
        {
          "_id": "6833c8af029c4a53a60a5dfc",
          "name": "Liu Leqi",
          "hidden": false
        },
        {
          "_id": "6833c8af029c4a53a60a5dfd",
          "name": "Ying Ding",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T07:05:09.000Z",
      "submittedOnDailyAt": "2025-05-26T00:20:35.511Z",
      "title": "Librairie d'enseignement : détection d'images basée sur des données de sons synthétiques dans le programme DPO",
      "submittedOnDailyBy": {
        "_id": "648749094dea003c6dae810f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/648749094dea003c6dae810f/gHUHSBt1zrt8wjO1YwTNu.jpeg",
        "isPro": false,
        "fullname": "Shrey Pandit",
        "user": "SP2001",
        "type": "user"
      },
      "summary": "L'avertissement de nouvelles d'accidents en langage général (LLMs) reste un problème grave en raison des caractéristiques complexes des textes d'accidents. Les exemples d'accidents présentent une qualité fausse plus élevée que les exemples traditionnels de négatifs, et ces bien planifiés sont utilisés comme exemples de faux dans les procédures de DPO. Notre méthode introduit un pas d'entraînement intermédiaire dans le CLE, se adaptant de manière graduelle des échantillons plus simples aux échantillons plus difficiles en basant-se sur la réduction maximale des scores de vérification de faits obtenus d'un modèle indépendant. Cette échelle structurée de difficulté garantit un apprentissage stable et progressif. Selon l'évaluation expérimentale, notre modèle HaluCheck, entraîné avec l'approche DPO CLE et des échantillons de faux de haute qualité, a amélioré significativement le rendement du modèle sur différentes métriques, avec un augmentation d'environ 24% sur les benchmarks difficiles de MedHallu et HaluEval. De plus, le modèle HaluCheck montre une robustesse dans la configuration de 0 shot et dépasse considérablement les modèles les plus récents sur divers benchmarks.",
      "upvotes": 9,
      "discussionId": "6833c8b0029c4a53a60a5e3a",
      "ai_summary": "The use of carefully crafted hallucinations in a curriculum learning approach within the DPO alignment procedure significantly enhances LLMs' hallucination detection abilities.",
      "ai_keywords": [
        "LLMs",
        "hallucinations",
        "DPO alignment procedure",
        "curriculum learning",
        "probability scores",
        "fact checking models",
        "HaluCheck models",
        "MedHallu",
        "HaluEval",
        "zero-shot settings"
      ]
    },
    "publishedAt": "2025-05-23T03:05:09.000Z",
    "title": "Teaching with Lies: Curriculum DPO on Synthetic Negatives for\n  Hallucination Detection",
    "summary": "Aligning large language models (LLMs) to accurately detect hallucinations\nremains a significant challenge due to the sophisticated nature of hallucinated\ntext. Recognizing that hallucinated samples typically exhibit higher deceptive\nquality than traditional negative samples, we use these carefully engineered\nhallucinations as negative examples in the DPO alignment procedure. Our method\nincorporates a curriculum learning strategy, gradually transitioning the\ntraining from easier samples, identified based on the greatest reduction in\nprobability scores from independent fact checking models, to progressively\nharder ones. This structured difficulty scaling ensures stable and incremental\nlearning. Experimental evaluation demonstrates that our HaluCheck models,\ntrained with curriculum DPO approach and high quality negative samples,\nsignificantly improves model performance across various metrics, achieving\nimprovements of upto 24% on difficult benchmarks like MedHallu and HaluEval.\nAdditionally, HaluCheck models demonstrate robustness in zero-shot settings,\nsignificantly outperforming larger state-of-the-art models across various\nbenchmarks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17558.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "648749094dea003c6dae810f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/648749094dea003c6dae810f/gHUHSBt1zrt8wjO1YwTNu.jpeg",
      "fullname": "Shrey Pandit",
      "name": "SP2001",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.16479",
      "authors": [
        {
          "_id": "682fdc63bf762029ddcad451",
          "user": {
            "_id": "6640c647acae6bb179eedff5",
            "avatarUrl": "/avatars/bcaafaaa1d4b4c241d72a886401772e3.svg",
            "isPro": false,
            "fullname": "Yuetong Liu",
            "user": "YuetongLiu",
            "type": "user"
          },
          "name": "Yuetong Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:16:39.979Z",
          "hidden": false
        },
        {
          "_id": "682fdc63bf762029ddcad452",
          "user": {
            "_id": "646c77911ee398a4e9404b8b",
            "avatarUrl": "/avatars/05d1ea421dd4f3e2fd47cbe99fc52933.svg",
            "isPro": false,
            "fullname": "Yunqiu Xu",
            "user": "Yunqiu",
            "type": "user"
          },
          "name": "Yunqiu Xu",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-23T04:01:10.107Z",
          "hidden": false
        },
        {
          "_id": "682fdc63bf762029ddcad453",
          "name": "Yang Wei",
          "hidden": false
        },
        {
          "_id": "682fdc63bf762029ddcad454",
          "name": "Xiuli Bi",
          "hidden": false
        },
        {
          "_id": "682fdc63bf762029ddcad455",
          "name": "Bin Xiao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T10:06:35.000Z",
      "submittedOnDailyAt": "2025-05-26T05:13:10.671Z",
      "title": "Claros Noches Ahora : Vers la Restauration des Images Nocturnes dans des Temps de Climat Divers",
      "submittedOnDailyBy": {
        "_id": "646c77911ee398a4e9404b8b",
        "avatarUrl": "/avatars/05d1ea421dd4f3e2fd47cbe99fc52933.svg",
        "isPro": false,
        "fullname": "Yunqiu Xu",
        "user": "Yunqiu",
        "type": "user"
      },
      "summary": "Les recherches sur la perte causée par les effets de la météorologie mauvaise dans les images nocturnes sont pratiques mais manquent d'un étude large. La météorologie mauvaise existe avec une grande clarté durant la nuit. Dans cette étude, nous examinons le problème de la création d'images nocturnes qui combinent l'impact de la météorologie mauvaise et l'effet de la lumière. De plus, nous fournirons tous les ensembles de données Waiver. Ces ensembles de données comprennent des images nocturnes de haute qualité et différents types de dommages structurels. Ces images ont été synthétisées à travers le dommage généré par la lumière que nous proposons. Nous présenterons également notre cadre de travail unifié pour la création d'images nocturnes appelé ClearNight. Ce cadre de travail est capable d'éliminer des dommages complexes de manière simultanée. En particulier, ClearNight extrait des lignes de bord basées sur les rythmes et se concentre sur les régions d'illumination inégale et le contenu de texture propre, ce qui fournit un effet élevé dans les nuits. De plus, nous proposons un méthode de coopération spécifique-commun pour améliorer la représentation des similitudes et des différences entre différents climats. Ce méthode permet d'identifier le dommage causé par la météorologie mauvaise et de sélectionner des unités optimales liées au climat. Notre ClearNight présente les meilleurs résultats sur les images synthétiques et réelles. Les expériences de suppression détaillées démontrent la nécessité de tous les ensembles de données Waiver et l'efficacité de ClearNight. Le site web du projet est disponible sur https://henlyta.github.io/ClearNight/mainpage.html.",
      "upvotes": 9,
      "discussionId": "682fdc67bf762029ddcad58c",
      "projectPage": "https://henlyta.github.io/ClearNight/mainpage.html",
      "githubRepo": "https://github.com/henlyta/ClearNight",
      "ai_summary": "A unified framework for restoring nighttime images under diverse weather conditions using dual priors and adaptive collaboration.",
      "ai_keywords": [
        "Retinex-based dual priors",
        "illumination-aware degradation generation",
        "weather-aware dynamic specific-commonality collaboration",
        "nighttime image restoration"
      ]
    },
    "publishedAt": "2025-05-22T06:06:35.000Z",
    "title": "Clear Nights Ahead: Towards Multi-Weather Nighttime Image Restoration",
    "summary": "Restoring nighttime images affected by multiple adverse weather conditions is\na practical yet under-explored research problem, as multiple weather conditions\noften coexist in the real world alongside various lighting effects at night.\nThis paper first explores the challenging multi-weather nighttime image\nrestoration task, where various types of weather degradations are intertwined\nwith flare effects. To support the research, we contribute the AllWeatherNight\ndataset, featuring large-scale high-quality nighttime images with diverse\ncompositional degradations, synthesized using our introduced illumination-aware\ndegradation generation. Moreover, we present ClearNight, a unified nighttime\nimage restoration framework, which effectively removes complex degradations in\none go. Specifically, ClearNight extracts Retinex-based dual priors and\nexplicitly guides the network to focus on uneven illumination regions and\nintrinsic texture contents respectively, thereby enhancing restoration\neffectiveness in nighttime scenarios. In order to better represent the common\nand unique characters of multiple weather degradations, we introduce a\nweather-aware dynamic specific-commonality collaboration method, which\nidentifies weather degradations and adaptively selects optimal candidate units\nassociated with specific weather types. Our ClearNight achieves\nstate-of-the-art performance on both synthetic and real-world images.\nComprehensive ablation experiments validate the necessity of AllWeatherNight\ndataset as well as the effectiveness of ClearNight. Project page:\nhttps://henlyta.github.io/ClearNight/mainpage.html",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16479.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "646c77911ee398a4e9404b8b",
      "avatarUrl": "/avatars/05d1ea421dd4f3e2fd47cbe99fc52933.svg",
      "fullname": "Yunqiu Xu",
      "name": "Yunqiu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.16483",
      "authors": [
        {
          "_id": "6833cb27e10e89e250a6d9ae",
          "name": "Shuzheng Si",
          "hidden": false
        },
        {
          "_id": "6833cb27e10e89e250a6d9af",
          "user": {
            "_id": "63ff09a02098b9ad105a09f6",
            "avatarUrl": "/avatars/4409ca5d320050cf4c3df05962c7ff58.svg",
            "isPro": false,
            "fullname": "Hans Zhao",
            "user": "BleachNick",
            "type": "user"
          },
          "name": "Haozhe Zhao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:10:14.872Z",
          "hidden": false
        },
        {
          "_id": "6833cb27e10e89e250a6d9b0",
          "name": "Cheng Gao",
          "hidden": false
        },
        {
          "_id": "6833cb27e10e89e250a6d9b1",
          "name": "Yuzhuo Bai",
          "hidden": false
        },
        {
          "_id": "6833cb27e10e89e250a6d9b2",
          "name": "Zhitong Wang",
          "hidden": false
        },
        {
          "_id": "6833cb27e10e89e250a6d9b3",
          "name": "Bofei Gao",
          "hidden": false
        },
        {
          "_id": "6833cb27e10e89e250a6d9b4",
          "name": "Kangyang Luo",
          "hidden": false
        },
        {
          "_id": "6833cb27e10e89e250a6d9b5",
          "name": "Wenhao Li",
          "hidden": false
        },
        {
          "_id": "6833cb27e10e89e250a6d9b6",
          "name": "Yufei Huang",
          "hidden": false
        },
        {
          "_id": "6833cb27e10e89e250a6d9b7",
          "name": "Gang Chen",
          "hidden": false
        },
        {
          "_id": "6833cb27e10e89e250a6d9b8",
          "name": "Fanchao Qi",
          "hidden": false
        },
        {
          "_id": "6833cb27e10e89e250a6d9b9",
          "name": "Minjia Zhang",
          "hidden": false
        },
        {
          "_id": "6833cb27e10e89e250a6d9ba",
          "name": "Baobao Chang",
          "hidden": false
        },
        {
          "_id": "6833cb27e10e89e250a6d9bb",
          "name": "Maosong Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T10:10:07.000Z",
      "submittedOnDailyAt": "2025-05-26T00:36:30.930Z",
      "title": "Utilisant des tâches de synthèse et d'apprentissage par renforcement pour maintenir la dépendance contextuelle des modèles de langage à grande échelle",
      "submittedOnDailyBy": {
        "_id": "637c99bbfe115289cfedfb44",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637c99bbfe115289cfedfb44/344NN9KKF_XXTlVYaGaMW.png",
        "isPro": false,
        "fullname": "ssz",
        "user": "ssz1111",
        "type": "user"
      },
      "summary": "La construction d'un système d'exploration de l'information fiable est essentielle pour fournir des réponses précises aux modèles de langage textuel (LLMs) comme le Teocodoro Jungle Model (LLMs) dans le contexte fourni. Par conséquent, nous proposons un cadre systématique appelé CANOE pour améliorer la précision dans des tâches de génération courte et longue, sans l'utilisation d'annotations humaines. Concrètement, nous avons synthétisé des données de réponses à des questions courtes qui comprennent quatre tâches différentes, et nous avons construit un ensemble de données d'entraînement de haute qualité et facile à vérifier, en excluant les annotations humaines. De plus, nous proposons un méthode d'apprentissage par récompense basée sur des règles de récompense, Dual-GRPO, qui inclut trois règles de récompense basées sur les données de réponses à des questions courtes sans annotations humaines. Cette méthode optimise à la fois la génération de réponses courtes et longues. En particulier, Dual-GRPO ne nécessite pas de données étiquetées directement pour l'entraînement de modèles de récompense, mais dépend exclusivement des données de réponses à des questions courtes sans annotations humaines, évitant ainsi une optimisation excessive de la génération courte. Les résultats des expérimentations montrent que CANOE améliore significativement la précision des LLMs dans 11 tâches de génération de réponses à des questions, démontrant qu'il dépasse les modèles les plus récents de GPT-4o et OpenAI ou1.",
      "upvotes": 8,
      "discussionId": "6833cb28e10e89e250a6da0a",
      "projectPage": "https://github.com/S1s-Z/CANOE",
      "githubRepo": "https://github.com/S1s-Z/CANOE",
      "ai_summary": "CANOE improves LLM faithfulness in generation tasks using synthetic QA data and Dual-GRPO reinforcement learning without human annotations.",
      "ai_keywords": [
        "teaching large language models",
        "faithfulness",
        "context",
        "CANOE",
        "short-form generation",
        "long-form generation",
        "question-answering",
        "Dual-GRPO",
        "rule-based reinforcement learning",
        "preference data",
        "reward models"
      ]
    },
    "publishedAt": "2025-05-22T06:10:07.000Z",
    "title": "Teaching Large Language Models to Maintain Contextual Faithfulness via\n  Synthetic Tasks and Reinforcement Learning",
    "summary": "Teaching large language models (LLMs) to be faithful in the provided context\nis crucial for building reliable information-seeking systems. Therefore, we\npropose a systematic framework, CANOE, to improve the faithfulness of LLMs in\nboth short-form and long-form generation tasks without human annotations.\nSpecifically, we first synthesize short-form question-answering (QA) data with\nfour diverse tasks to construct high-quality and easily verifiable training\ndata without human annotation. Also, we propose Dual-GRPO, a rule-based\nreinforcement learning method that includes three tailored rule-based rewards\nderived from synthesized short-form QA data, while simultaneously optimizing\nboth short-form and long-form response generation. Notably, Dual-GRPO\neliminates the need to manually label preference data to train reward models\nand avoids over-optimizing short-form generation when relying only on the\nsynthesized short-form QA data. Experimental results show that CANOE greatly\nimproves the faithfulness of LLMs across 11 different downstream tasks, even\noutperforming the most advanced LLMs, e.g., GPT-4o and OpenAI o1.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16483.png",
    "numComments": 4,
    "submittedBy": {
      "_id": "637c99bbfe115289cfedfb44",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637c99bbfe115289cfedfb44/344NN9KKF_XXTlVYaGaMW.png",
      "fullname": "ssz",
      "name": "ssz1111",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.13508",
      "authors": [
        {
          "_id": "683148c0018bba5b656c94e3",
          "user": {
            "_id": "65d188a4aa309d842e438ef1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d188a4aa309d842e438ef1/t5zmBIytp-xnMoHUfYhEa.jpeg",
            "isPro": false,
            "fullname": "Zijia Liu",
            "user": "m-serious",
            "type": "user"
          },
          "name": "Zijia Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:12:18.318Z",
          "hidden": false
        },
        {
          "_id": "683148c0018bba5b656c94e4",
          "name": "Peixuan Han",
          "hidden": false
        },
        {
          "_id": "683148c0018bba5b656c94e5",
          "name": "Haofei Yu",
          "hidden": false
        },
        {
          "_id": "683148c0018bba5b656c94e6",
          "name": "Haoru Li",
          "hidden": false
        },
        {
          "_id": "683148c0018bba5b656c94e7",
          "name": "Jiaxuan You",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-16T13:46:28.000Z",
      "submittedOnDailyAt": "2025-05-26T06:47:47.579Z",
      "title": "Temps R1 : Discussion sur les solutions antérieures à la logique des séquences temporelles",
      "submittedOnDailyBy": {
        "_id": "65d188a4aa309d842e438ef1",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d188a4aa309d842e438ef1/t5zmBIytp-xnMoHUfYhEa.jpeg",
        "isPro": false,
        "fullname": "Zijia Liu",
        "user": "m-serious",
        "type": "user"
      },
      "summary": "Les modèles de langage grand (LLMs) ont démontré des capacités impressionnantes, mais leur connaissance temporelle est faible et il est difficile que les modèles pensent aux causes passées ou prédisent les futurs de manière adéquate. De plus, les méthodes actuelles se concentrent généralement sur le développement de compétences temporelles séparées, y compris des réponses à des questions sur des faits passés et des prédictions basiques, mais leur capacité à généraliser est limitée, surtout lorsqu'il s'agit d'événements plus éloignés du savoir ou de la créativité. Pour surmonter ces limites, nous présentons Time-R1. Time-R1 est le premier cadre de travail et fournit à un modèle de LLM de taille moyenne (3B paramètres) des compétences temporelles (reconnaissance, prédiction, génération créative). Notre approche est caractérisée par un nouveau pas de développement en trois étapes, où les deux premières étapes sont basées sur un système de récompenses basé sur des règles dynamiques et sont utilisées pour un programme d'apprentissage par renforcement (RL). Ce cadre permet : (1) une compréhension basique du temps à partir de données historiques et le cartographie des relations temporelles d'événements, (2) une capacité à prédire des événements plus éloignés du savoir, et (3) une excellente généralisation dans la génération de scénarios créatifs du futur. En particulier, les expériences montrent que Time-R1 dépasse les modèles plus de 200 fois son taille, y compris le leader dans le domaine, le 671B DeepSeek-R1, dans des benchmarks très difficiles de prédiction d'événements futurs et de génération de scénarios créatifs. Cette recherche est une preuve forte que l'ingénierie précise et l'ajustement avancé de RL peuvent atteindre des résultats temporels excellents à travers de petits et efficaces modèles. De plus, pour faciliter la recherche, nous avons publié Time-Bench, un ensemble de données de prédiction temporelle multi-tâches à partir de 10 ans de données de nouvelles, et nous avons fourni une série de checkpoints de Time-R1.",
      "upvotes": 8,
      "discussionId": "683148c1018bba5b656c9511",
      "githubRepo": "https://github.com/ulab-uiuc/Time-R1",
      "ai_summary": "A novel framework, Time-R1, enhances moderate-sized LLMs with comprehensive temporal abilities through a reinforcement learning curriculum, outperforming larger models on future event prediction and creative scenario generation benchmarks.",
      "ai_keywords": [
        "Large Language Models",
        "reinforcement learning",
        "RL curriculum",
        "rule-based reward system",
        "temporal understanding",
        "event-time mappings",
        "future event prediction",
        "creative scenario generation",
        "Time-Bench",
        "Time-R1 checkpoints"
      ]
    },
    "publishedAt": "2025-05-16T09:46:28.000Z",
    "title": "Time-R1: Towards Comprehensive Temporal Reasoning in LLMs",
    "summary": "Large Language Models (LLMs) demonstrate impressive capabilities but lack\nrobust temporal intelligence, struggling to integrate reasoning about the past\nwith predictions and plausible generations of the future. Meanwhile, existing\nmethods typically target isolated temporal skills, such as question answering\nabout past events or basic forecasting, and exhibit poor generalization,\nparticularly when dealing with events beyond their knowledge cutoff or\nrequiring creative foresight. To address these limitations, we introduce\nTime-R1, the first framework to endow a moderate-sized (3B-parameter)\nLLM with comprehensive temporal abilities: understanding, prediction, and\ncreative generation. Our approach features a novel three-stage development\npath; the first two constitute a reinforcement learning (RL)\ncurriculum driven by a meticulously designed dynamic rule-based reward system.\nThis framework progressively builds (1) foundational temporal understanding and\nlogical event-time mappings from historical data, (2) future event prediction\nskills for events beyond its knowledge cutoff, and finally (3) enables\nremarkable generalization to creative future scenario generation without any\nfine-tuning. Strikingly, experiments demonstrate that Time-R1 outperforms\nmodels over 200 times larger, including the state-of-the-art 671B DeepSeek-R1,\non highly challenging future event prediction and creative scenario generation\nbenchmarks. This work provides strong evidence that thoughtfully engineered,\nprogressive RL fine-tuning allows smaller, efficient models to achieve superior\ntemporal performance, offering a practical and scalable path towards truly\ntime-aware AI. To foster further research, we also release Time-Bench,\na large-scale multi-task temporal reasoning dataset derived from 10 years of\nnews data, and our series of Time-R1 checkpoints.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.13508.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "65d188a4aa309d842e438ef1",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d188a4aa309d842e438ef1/t5zmBIytp-xnMoHUfYhEa.jpeg",
      "fullname": "Zijia Liu",
      "name": "m-serious",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.17417",
      "authors": [
        {
          "_id": "6833d2df73bebebe5cd6604e",
          "user": {
            "_id": "62d7b2339b629105a5d6888a",
            "avatarUrl": "/avatars/c3f164fde6b8f9a671890e08ce8a3e75.svg",
            "isPro": false,
            "fullname": "Alan Dao",
            "user": "alandao",
            "type": "user"
          },
          "name": "Alan Dao",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-26T02:38:31.379Z",
          "hidden": false
        },
        {
          "_id": "6833d2df73bebebe5cd6604f",
          "name": "Dinh Bach Vu",
          "hidden": false
        },
        {
          "_id": "6833d2df73bebebe5cd66050",
          "name": "Huy Hoang Ha",
          "hidden": false
        },
        {
          "_id": "6833d2df73bebebe5cd66051",
          "name": "Tuan Le Duc Anh",
          "hidden": false
        },
        {
          "_id": "6833d2df73bebebe5cd66052",
          "name": "Shreyas Gopal",
          "hidden": false
        },
        {
          "_id": "6833d2df73bebebe5cd66053",
          "name": "Yue Heng Yeo",
          "hidden": false
        },
        {
          "_id": "6833d2df73bebebe5cd66054",
          "name": "Warren Keng Hoong Low",
          "hidden": false
        },
        {
          "_id": "6833d2df73bebebe5cd66055",
          "name": "Eng Siong Chng",
          "hidden": false
        },
        {
          "_id": "6833d2df73bebebe5cd66056",
          "name": "Jia Qi Yip",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T03:05:47.000Z",
      "submittedOnDailyAt": "2025-05-26T01:03:18.385Z",
      "title": "Langage sans langage de classe : apprentissage de classe de langage original dans l'apprentissage d'instructions sans langage",
      "submittedOnDailyBy": {
        "_id": "62d7b2339b629105a5d6888a",
        "avatarUrl": "/avatars/c3f164fde6b8f9a671890e08ce8a3e75.svg",
        "isPro": false,
        "fullname": "Alan Dao",
        "user": "alandao",
        "type": "user"
      },
      "summary": "Le rapide croissance des systèmes d'assistance vocale avec langage généralisé (LLM) a clairement montré la nécessité de données d'instructions vocales pour leur entraînement. Tant que l'abondance de données de reconnaissance de la langue contraste avec la pénurie de données d'instructions, ces dernières sont particulièrement importantes pour ajuster des modèles capables de comprendre et d'exécuter des langues. Pour générer une qualité de langage synthétique, il est nécessaire d'avoir un bon modèle de conversion de texte en voix (TTS), mais ces modèles ne sont pas visibles dans les langues à peu de ressources. Notre nouvel approche résout ce problème en se concentrant sur la représentation significative du langage synthétique et en évitant la nécessité de TTS. Pour y parvenir, les représentations significatives synthétiques sont combinées avec un codificateur pré-entraîné de Whisper et le modèle LLM est entraîné pour pouvoir s'entraîner avec des instructions de phrases, tout en maintenant la capacité de comprendre des instructions de langue lors de l'inférence. Ce processus d'entraînement simplifié est une prometteuse approche pour construire des systèmes d'assistance vocale dans les langues à peu de ressources.",
      "upvotes": 6,
      "discussionId": "6833d2df73bebebe5cd66074",
      "githubRepo": "https://github.com/menloresearch/ichigo",
      "ai_summary": "A method bypasses the need for TTS models by aligning semantic representations with a Whisper encoder, enabling LLMs to understand both text and spoken instructions for low-resource languages.",
      "ai_keywords": [
        "large language models",
        "LLM",
        "speech instruction data",
        "TTS",
        "semantic representation",
        "Whisper encoder",
        "fine-tuning",
        "low-resource languages"
      ]
    },
    "publishedAt": "2025-05-22T23:05:47.000Z",
    "title": "Speechless: Speech Instruction Training Without Speech for Low Resource\n  Languages",
    "summary": "The rapid growth of voice assistants powered by large language models (LLM)\nhas highlighted a need for speech instruction data to train these systems.\nDespite the abundance of speech recognition data, there is a notable scarcity\nof speech instruction data, which is essential for fine-tuning models to\nunderstand and execute spoken commands. Generating high-quality synthetic\nspeech requires a good text-to-speech (TTS) model, which may not be available\nto low resource languages. Our novel approach addresses this challenge by\nhalting synthesis at the semantic representation level, bypassing the need for\nTTS. We achieve this by aligning synthetic semantic representations with the\npre-trained Whisper encoder, enabling an LLM to be fine-tuned on text\ninstructions while maintaining the ability to understand spoken instructions\nduring inference. This simplified training process is a promising approach to\nbuilding voice assistant for low-resource languages.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17417.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62d7b2339b629105a5d6888a",
      "avatarUrl": "/avatars/c3f164fde6b8f9a671890e08ce8a3e75.svg",
      "fullname": "Alan Dao",
      "name": "alandao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.16770",
      "authors": [
        {
          "_id": "68308e2697d9a81c8521bc6a",
          "user": {
            "_id": "62145614b670cb63a38075ba",
            "avatarUrl": "/avatars/5e33debde75ae6c87640f63c48c560c6.svg",
            "isPro": false,
            "fullname": "MenghaoGuo",
            "user": "MenghaoGuo",
            "type": "user"
          },
          "name": "Meng-Hao Guo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:12:57.462Z",
          "hidden": false
        },
        {
          "_id": "68308e2697d9a81c8521bc6b",
          "user": {
            "_id": "66b711f9512dac2ac08bc5e5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66b711f9512dac2ac08bc5e5/n2kSqNhg-TE56iN_V0xHm.png",
            "isPro": false,
            "fullname": "Xuanyu Chu",
            "user": "CXY07",
            "type": "user"
          },
          "name": "Xuanyu Chu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:12:48.661Z",
          "hidden": false
        },
        {
          "_id": "68308e2697d9a81c8521bc6c",
          "name": "Qianrui Yang",
          "hidden": false
        },
        {
          "_id": "68308e2697d9a81c8521bc6d",
          "user": {
            "_id": "6816bd8e0499f6c7c7b89601",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6816bd8e0499f6c7c7b89601/-dkIPxjOGbdwDZFxhkBMC.jpeg",
            "isPro": false,
            "fullname": "Zhe-Han Mo",
            "user": "Mo-ZheHan",
            "type": "user"
          },
          "name": "Zhe-Han Mo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:12:52.048Z",
          "hidden": false
        },
        {
          "_id": "68308e2697d9a81c8521bc6e",
          "name": "Yiqing Shen",
          "hidden": false
        },
        {
          "_id": "68308e2697d9a81c8521bc6f",
          "name": "Pei-lin Li",
          "hidden": false
        },
        {
          "_id": "68308e2697d9a81c8521bc70",
          "name": "Xinjie Lin",
          "hidden": false
        },
        {
          "_id": "68308e2697d9a81c8521bc71",
          "name": "Jinnian Zhang",
          "hidden": false
        },
        {
          "_id": "68308e2697d9a81c8521bc72",
          "name": "Xin-Sheng Chen",
          "hidden": false
        },
        {
          "_id": "68308e2697d9a81c8521bc73",
          "user": {
            "_id": "63b2efb5922f26a27e76381c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b2efb5922f26a27e76381c/zOQAt_xywiY8eTvvQOrmQ.png",
            "isPro": false,
            "fullname": "Yi Zhang",
            "user": "uyzhang",
            "type": "user"
          },
          "name": "Yi Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:12:54.379Z",
          "hidden": false
        },
        {
          "_id": "68308e2697d9a81c8521bc74",
          "name": "Kiyohiro Nakayama",
          "hidden": false
        },
        {
          "_id": "68308e2697d9a81c8521bc75",
          "name": "Zhengyang Geng",
          "hidden": false
        },
        {
          "_id": "68308e2697d9a81c8521bc76",
          "name": "Houwen Peng",
          "hidden": false
        },
        {
          "_id": "68308e2697d9a81c8521bc77",
          "name": "Han Hu",
          "hidden": false
        },
        {
          "_id": "68308e2697d9a81c8521bc78",
          "name": "Shi-Nin Hu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T15:11:57.000Z",
      "submittedOnDailyAt": "2025-05-26T06:55:20.321Z",
      "title": "RBench-V : Sortie multimodale pour l'évaluation de base de modèles de raisonnement visuel",
      "submittedOnDailyBy": {
        "_id": "62145614b670cb63a38075ba",
        "avatarUrl": "/avatars/5e33debde75ae6c87640f63c48c560c6.svg",
        "isPro": false,
        "fullname": "MenghaoGuo",
        "user": "MenghaoGuo",
        "type": "user"
      },
      "summary": "Nota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice",
      "upvotes": 6,
      "discussionId": "68308e2797d9a81c8521bca5",
      "ai_summary": "A benchmark called RBench-V evaluates multi-modal models' vision-indispensable reasoning through image manipulation and auxiliary line construction, demonstrating that current models struggle with multi-modal outputs.",
      "ai_keywords": [
        "multi-modal models",
        "omni-models",
        "GPT-4o",
        "Gemini",
        "o3",
        "multi-modal chain of thought",
        "M-CoT",
        "RBench-V",
        "image manipulation",
        "auxiliary lines"
      ]
    },
    "publishedAt": "2025-05-22T11:11:57.000Z",
    "title": "RBench-V: A Primary Assessment for Visual Reasoning Models with\n  Multi-modal Outputs",
    "summary": "The rapid advancement of native multi-modal models and omni-models,\nexemplified by GPT-4o, Gemini, and o3, with their capability to process and\ngenerate content across modalities such as text and images, marks a significant\nmilestone in the evolution of intelligence. Systematic evaluation of their\nmulti-modal output capabilities in visual thinking processes (also known as\nmulti-modal chain of thought, M-CoT) becomes critically important. However,\nexisting benchmarks for evaluating multi-modal models primarily focus on\nassessing multi-modal inputs and text-only reasoning while neglecting the\nimportance of reasoning through multi-modal outputs. In this paper, we present\na benchmark, dubbed RBench-V, designed to assess models' vision-indispensable\nreasoning abilities. To construct RBench-V, we carefully hand-pick 803\nquestions covering math, physics, counting, and games. Unlike previous\nbenchmarks that typically specify certain input modalities, RBench-V presents\nproblems centered on multi-modal outputs, which require image manipulation such\nas generating novel images and constructing auxiliary lines to support the\nreasoning process. We evaluate numerous open- and closed-source models on\nRBench-V, including o3, Gemini 2.5 Pro, Qwen2.5-VL, etc. Even the\nbest-performing model, o3, achieves only 25.8% accuracy on RBench-V, far below\nthe human score of 82.3%, highlighting that current models struggle to leverage\nmulti-modal reasoning. Data and code are available at\nhttps://evalmodels.github.io/rbenchv",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16770.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "62145614b670cb63a38075ba",
      "avatarUrl": "/avatars/5e33debde75ae6c87640f63c48c560c6.svg",
      "fullname": "MenghaoGuo",
      "name": "MenghaoGuo",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.15389",
      "authors": [
        {
          "_id": "682f518184a99219c4b3090c",
          "user": {
            "_id": "6540fbf9cb7fffd683942b43",
            "avatarUrl": "/avatars/d4a64fbde511d0949e1c339179586850.svg",
            "isPro": false,
            "fullname": "DongGeon Lee",
            "user": "oneonlee",
            "type": "user"
          },
          "name": "DongGeon Lee",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:17:11.519Z",
          "hidden": false
        },
        {
          "_id": "682f518184a99219c4b3090d",
          "name": "Joonwon Jang",
          "hidden": false
        },
        {
          "_id": "682f518184a99219c4b3090e",
          "name": "Jihae Jeong",
          "hidden": false
        },
        {
          "_id": "682f518184a99219c4b3090f",
          "name": "Hwanjo Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T11:26:40.000Z",
      "submittedOnDailyAt": "2025-05-26T00:35:59.051Z",
      "title": "Le modèle de vision-LUNGRAZ est-il sûr dans son état naturel ? Étude de scénarios de test basés sur la mémoire",
      "submittedOnDailyBy": {
        "_id": "6540fbf9cb7fffd683942b43",
        "avatarUrl": "/avatars/d4a64fbde511d0949e1c339179586850.svg",
        "isPro": false,
        "fullname": "DongGeon Lee",
        "user": "oneonlee",
        "type": "user"
      },
      "summary": "Les modèles de langue visuelle (VLMs) qui se distribuent rapidement augmentent le risque de sécurité, mais la plupart des évaluations dépendent d'images artificielles. Cette étude investigate la sécurité des VLMs face aux images de memes partagés par les utilisateurs. Pour cela, nous introduisons MemeSafetyBench, une base de test standard avec 50,430 exemples, combinant des images réelles de memes et des commandes nuisibles ou innocences. Nous utilisons des méthodes de classification de sécurité générale et de génération de commandes basées sur des modèles de langue pour évaluer plusieurs VLMs avec des interactions d'un et de multiples cycles. Nous étudions comment les memes de la réalité affectent les commandes nuisibles, l'effet du contexte de conversation et la relation entre la taille du modèle et les indicateurs de sécurité. Nos résultats montrent que les VLMs sont plus facilement attaqués par des commandes nuisibles basées sur des memes que par des images synthétiques ou imprimées. Dans les cas d'entrée de texte, les memes augmentent la réponse nuisible et réduisent le rejet. Les interactions de multiples cycles offrent certaines améliorations, mais le haut risque persiste. Ces résultats soulignent la nécessité d'évaluations écologiques efficaces et le renforcement des mécanismes de sécurité.",
      "upvotes": 6,
      "discussionId": "682f518184a99219c4b30956",
      "ai_summary": "VLMs are more vulnerable to harmful meme-based prompts than to synthetic images, and while multi-turn interactions offer some protection, significant vulnerabilities remain.",
      "ai_keywords": [
        "vision-language models",
        "VLMs",
        "MemeSafetyBench",
        "safety taxonomy",
        "LLM-based instruction generation",
        "single and multi-turn interactions"
      ]
    },
    "publishedAt": "2025-05-21T07:26:40.000Z",
    "title": "Are Vision-Language Models Safe in the Wild? A Meme-Based Benchmark\n  Study",
    "summary": "Rapid deployment of vision-language models (VLMs) magnifies safety risks, yet\nmost evaluations rely on artificial images. This study asks: How safe are\ncurrent VLMs when confronted with meme images that ordinary users share? To\ninvestigate this question, we introduce MemeSafetyBench, a 50,430-instance\nbenchmark pairing real meme images with both harmful and benign instructions.\nUsing a comprehensive safety taxonomy and LLM-based instruction generation, we\nassess multiple VLMs across single and multi-turn interactions. We investigate\nhow real-world memes influence harmful outputs, the mitigating effects of\nconversational context, and the relationship between model scale and safety\nmetrics. Our findings demonstrate that VLMs show greater vulnerability to\nmeme-based harmful prompts than to synthetic or typographic images. Memes\nsignificantly increase harmful responses and decrease refusals compared to\ntext-only inputs. Though multi-turn interactions provide partial mitigation,\nelevated vulnerability persists. These results highlight the need for\necologically valid evaluations and stronger safety mechanisms.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15389.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6540fbf9cb7fffd683942b43",
      "avatarUrl": "/avatars/d4a64fbde511d0949e1c339179586850.svg",
      "fullname": "DongGeon Lee",
      "name": "oneonlee",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.17826",
      "authors": [
        {
          "_id": "6833ce1bd5c438959f750d57",
          "name": "Xuchen Pan",
          "hidden": false
        },
        {
          "_id": "6833ce1bd5c438959f750d58",
          "name": "Yanxi Chen",
          "hidden": false
        },
        {
          "_id": "6833ce1bd5c438959f750d59",
          "name": "Yushuo Chen",
          "hidden": false
        },
        {
          "_id": "6833ce1bd5c438959f750d5a",
          "name": "Yuchang Sun",
          "hidden": false
        },
        {
          "_id": "6833ce1bd5c438959f750d5b",
          "name": "Daoyuan Chen",
          "hidden": false
        },
        {
          "_id": "6833ce1bd5c438959f750d5c",
          "name": "Wenhao Zhang",
          "hidden": false
        },
        {
          "_id": "6833ce1bd5c438959f750d5d",
          "name": "Yuexiang Xie",
          "hidden": false
        },
        {
          "_id": "6833ce1bd5c438959f750d5e",
          "name": "Yilun Huang",
          "hidden": false
        },
        {
          "_id": "6833ce1bd5c438959f750d5f",
          "name": "Yilei Zhang",
          "hidden": false
        },
        {
          "_id": "6833ce1bd5c438959f750d60",
          "name": "Dawei Gao",
          "hidden": false
        },
        {
          "_id": "6833ce1bd5c438959f750d61",
          "name": "Yaliang Li",
          "hidden": false
        },
        {
          "_id": "6833ce1bd5c438959f750d62",
          "name": "Bolin Ding",
          "hidden": false
        },
        {
          "_id": "6833ce1bd5c438959f750d63",
          "name": "Jingren Zhou",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6576f9f4654561a1b345610b/kDRshLvxO0EynfeH0BOZK.png"
      ],
      "publishedAt": "2025-05-23T12:41:09.000Z",
      "submittedOnDailyAt": "2025-05-26T00:50:50.296Z",
      "title": "TRINITY-RFT : Un cadre de travail unifié pour l'ajustement de modèles de langage grands comme un cadre général d'apprentissage par renforcement",
      "submittedOnDailyBy": {
        "_id": "6576f9f4654561a1b345610b",
        "avatarUrl": "/avatars/f801f551640caa70368fcc26a0f51d27.svg",
        "isPro": false,
        "fullname": "Yanxi Chen",
        "user": "yanxi-chen",
        "type": "user"
      },
      "summary": "TRINITY-RFT est un cadre général, flexible et scalable approprié pour les ajustements micros d'apprentissage par renforcement (RFT). Ce cadre utilise des conceptions découplées. Les composants comprennent : (1) le noyau RFT-core, qui permet la normalisation et la généralisation de l'RFT pour les synchronisations/désynchronisations, en ligne/en dehors de la ligne, et en ligne/en dehors de la ligne. (2) l'intégration infinie de l'interaction entre agent et environnement, qui met l'accent sur l'efficacité et la robustesse. (3) la configuration d'un système de flux de données optimisés pour l'RFT. TRINITY-RFT peut être appliqué dans divers environnements d'application et constitue une plateforme intégrée pour explorer des paradigmes avancés d'apprentissage par renforcement. Dans ce rapport technique, les perspectives, fonctionnalités, conception et mise en œuvre de TRINITY-RFT sont décrites en détail, et plusieurs exemples sont présentés pour démontrer l'utilité et l'accessibilité de ce cadre proposé.",
      "upvotes": 5,
      "discussionId": "6833ce1cd5c438959f750dab",
      "projectPage": "https://github.com/modelscope/Trinity-RFT",
      "githubRepo": "https://github.com/modelscope/Trinity-RFT",
      "ai_summary": "Trinity-RFT is a flexible and scalable framework for reinforcement fine-tuning of large language models, supporting various interaction modes and data pipelines.",
      "ai_keywords": [
        "reinforcement fine-tuning",
        "RFT-core",
        "synchronous/asynchronous",
        "on-policy/off-policy",
        "online/offline",
        "agent-environment interaction",
        "data pipelines",
        "reinforcement learning paradigms"
      ]
    },
    "publishedAt": "2025-05-23T08:41:09.000Z",
    "title": "Trinity-RFT: A General-Purpose and Unified Framework for Reinforcement\n  Fine-Tuning of Large Language Models",
    "summary": "Trinity-RFT is a general-purpose, flexible and scalable framework designed\nfor reinforcement fine-tuning (RFT) of large language models. It is built with\na decoupled design, consisting of (1) an RFT-core that unifies and generalizes\nsynchronous/asynchronous, on-policy/off-policy, and online/offline modes of\nRFT, (2) seamless integration for agent-environment interaction with high\nefficiency and robustness, and (3) systematic data pipelines optimized for RFT.\nTrinity-RFT can be easily adapted for diverse application scenarios, and serves\nas a unified platform for exploring advanced reinforcement learning paradigms.\nThis technical report outlines the vision, features, design and implementations\nof Trinity-RFT, accompanied by extensive examples demonstrating the utility and\nuser-friendliness of the proposed framework.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6576f9f4654561a1b345610b/kDRshLvxO0EynfeH0BOZK.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17826.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6576f9f4654561a1b345610b",
      "avatarUrl": "/avatars/f801f551640caa70368fcc26a0f51d27.svg",
      "fullname": "Yanxi Chen",
      "name": "yanxi-chen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.17508",
      "authors": [
        {
          "_id": "6833cf5a2d728e2330d572e3",
          "user": {
            "_id": "647bf082aba7062fe5c51ca9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647bf082aba7062fe5c51ca9/p4lY9IjHiWZETKmFq1mtU.jpeg",
            "isPro": false,
            "fullname": "Yifan Zhang",
            "user": "yifAI",
            "type": "user"
          },
          "name": "Yifan Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:10:09.930Z",
          "hidden": false
        },
        {
          "_id": "6833cf5a2d728e2330d572e4",
          "user": {
            "_id": "653d276681f52ceb4d12bd85",
            "avatarUrl": "/avatars/56601a25e5f883a8f6dc15f6fd9dcc57.svg",
            "isPro": false,
            "fullname": "Yifeng Liu",
            "user": "Lewis-Lau",
            "type": "user"
          },
          "name": "Yifeng Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:10:06.913Z",
          "hidden": false
        },
        {
          "_id": "6833cf5a2d728e2330d572e5",
          "name": "Huizhuo Yuan",
          "hidden": false
        },
        {
          "_id": "6833cf5a2d728e2330d572e6",
          "name": "Yang Yuan",
          "hidden": false
        },
        {
          "_id": "6833cf5a2d728e2330d572e7",
          "name": "Quanquan Gu",
          "hidden": false
        },
        {
          "_id": "6833cf5a2d728e2330d572e8",
          "name": "Andrew C Yao",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/647bf082aba7062fe5c51ca9/mGUuNpUhjfafWqcJJZ1V1.png"
      ],
      "publishedAt": "2025-05-23T06:01:21.000Z",
      "submittedOnDailyAt": "2025-05-26T00:50:14.655Z",
      "title": "La politique de normalisation KL pour les algorithmes génétiques de descente de gradient : raisons de conception",
      "submittedOnDailyBy": {
        "_id": "647bf082aba7062fe5c51ca9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647bf082aba7062fe5c51ca9/p4lY9IjHiWZETKmFq1mtU.jpeg",
        "isPro": false,
        "fullname": "Yifan Zhang",
        "user": "yifAI",
        "type": "user"
      },
      "summary": "L'algorithme de gradient de politiques a été appliqué avec succès pour améliorer la capacité logique de modèles de langage grands (LLMs). Cependant, la normalisation KL est largement utilisée dans cet algorithme, ce qui aide à réguler l'apprentissage, mais implique également l'exploration d'un espace de conception complexe et systématique, car il faut évaluer les différentes matrices de données KL et les intégrer comme fonctions de perte en apprentissage par renforcement (RL) en ligne. Dans cet article, nous proposons un cadre systématique pour la généralisation et l'analyse des méthodes de politiques normalisées, appelé Policy Gradient Normalized (RPG). Nous revenons à la fois sur les distributions de politiques normalisées et non normalisées, et nous extrayons des fonctions de politique et des fonctions de perte de l'agent relatif qui s'adaptent aux objectifs normalisés avec des matrices de données KL bidirectionnelles et unidirectionnelles. De plus, nous effectuons des calculs de toutes les fonctions de perte différenciables et des estimations de gradient de type REINFORCE, et nous les ajustons pour satisfaire les besoins de différents algorithmes. Ces méthodologies sont étendues pour mener à bien des expériences en RL de raisonnement logique de LLMs, démontrant des résultats exceptionnels ou compétitifs en termes de stabilité de l'apprentissage et de performance. Le code est disponible sur https://github.com/complex-reasoning/RPG.",
      "upvotes": 4,
      "discussionId": "6833cf5b2d728e2330d57313",
      "projectPage": "https://complex-reasoning.github.io/RPG",
      "githubRepo": "https://github.com/complex-reasoning/RPG",
      "ai_summary": "A regularized policy gradient framework is introduced to explore KL divergence formulations for enhancing the reasoning capabilities of LLMs in online reinforcement learning, demonstrating improved training stability and performance.",
      "ai_keywords": [
        "policy gradient algorithms",
        "KL regularization",
        "KL divergence",
        "surrogate loss functions",
        "online reinforcement learning",
        "full differentiable loss functions",
        "REINFORCE-style gradient estimators",
        "GRPO",
        "REINFORCE++",
        "DAPO",
        "regularized policy gradient (RPG)",
        "forward KL divergence",
        "reverse KL divergence",
        "normalized policy distributions",
        "unnormalized policy distributions"
      ]
    },
    "publishedAt": "2025-05-23T02:01:21.000Z",
    "title": "On the Design of KL-Regularized Policy Gradient Algorithms for LLM\n  Reasoning",
    "summary": "Policy gradient algorithms have been successfully applied to enhance the\nreasoning capabilities of large language models (LLMs). Despite the widespread\nuse of Kullback-Leibler (KL) regularization in policy gradient algorithms to\nstabilize training, the systematic exploration of how different KL divergence\nformulations can be estimated and integrated into surrogate loss functions for\nonline reinforcement learning (RL) presents a nuanced and systematically\nexplorable design space. In this paper, we propose regularized policy gradient\n(RPG), a systematic framework for deriving and analyzing KL-regularized policy\ngradient methods in the online RL setting. We derive policy gradients and\ncorresponding surrogate loss functions for objectives regularized by both\nforward and reverse KL divergences, considering both normalized and\nunnormalized policy distributions. Furthermore, we present derivations for\nfully differentiable loss functions as well as REINFORCE-style gradient\nestimators, accommodating diverse algorithmic needs. We conduct extensive\nexperiments on RL for LLM reasoning using these methods, showing improved or\ncompetitive results in terms of training stability and performance compared to\nstrong baselines such as GRPO, REINFORCE++, and DAPO. The code is available at\nhttps://github.com/complex-reasoning/RPG.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/647bf082aba7062fe5c51ca9/mGUuNpUhjfafWqcJJZ1V1.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17508.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "647bf082aba7062fe5c51ca9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647bf082aba7062fe5c51ca9/p4lY9IjHiWZETKmFq1mtU.jpeg",
      "fullname": "Yifan Zhang",
      "name": "yifAI",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 13
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.17412",
      "authors": [
        {
          "_id": "6833e93697966d18e7c1e4d7",
          "name": "Shuang Wu",
          "hidden": false
        },
        {
          "_id": "6833e93697966d18e7c1e4d8",
          "name": "Youtian Lin",
          "hidden": false
        },
        {
          "_id": "6833e93697966d18e7c1e4d9",
          "name": "Feihu Zhang",
          "hidden": false
        },
        {
          "_id": "6833e93697966d18e7c1e4da",
          "name": "Yifei Zeng",
          "hidden": false
        },
        {
          "_id": "6833e93697966d18e7c1e4db",
          "name": "Yikang Yang",
          "hidden": false
        },
        {
          "_id": "6833e93697966d18e7c1e4dc",
          "name": "Yajie Bao",
          "hidden": false
        },
        {
          "_id": "6833e93697966d18e7c1e4dd",
          "name": "Jiachen Qian",
          "hidden": false
        },
        {
          "_id": "6833e93697966d18e7c1e4de",
          "name": "Siyu Zhu",
          "hidden": false
        },
        {
          "_id": "6833e93697966d18e7c1e4df",
          "name": "Philip Torr",
          "hidden": false
        },
        {
          "_id": "6833e93697966d18e7c1e4e0",
          "name": "Xun Cao",
          "hidden": false
        },
        {
          "_id": "6833e93697966d18e7c1e4e1",
          "name": "Yao Yao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T02:58:01.000Z",
      "submittedOnDailyAt": "2025-05-26T03:12:06.518Z",
      "title": "Direct3D-S2 : Facilite la génération de scènes 3D à grande échelle avec une attention spectrale sparse.",
      "submittedOnDailyBy": {
        "_id": "645a24779f06c5897254d14b",
        "avatarUrl": "/avatars/dd0a635674025dcc9a94ee0f4c952083.svg",
        "isPro": false,
        "fullname": "Youtian Lin",
        "user": "LoYoT",
        "type": "user"
      },
      "summary": "Direct3D S2 est un cadre de génération de 3D qui utilise des représentations volumétriques sous la forme de fonctions de distance signées pour créer des formes 3D, mais rencontre des problèmes significatifs de calcul et de mémoire lors de la génération de formes 3D à haute résolution. Nous avons introduit une structure d'attention spatialement sparsée au sein de volumes efficaces pour traiter un grand nombre de tokens, ce qui a augmenté la vitesse de propagation de 3,9 fois (propagation) et 9,6 fois (retropropagation), et réduit considérablement le coût de calcul. De plus, nous avons intégré un codage distribué qui maintient le format de volumes sparsés cohérent à travers les étapes d'entrée, potentiel et sortie, améliorant significativement l'efficacité et la stabilité de l'entraînement par rapport aux méthodes précédentes qui utilisaient d'autres représentations de VAE 3D. Notre modèle a été entraîné sur des ensembles de données disponibles, et les expériences montrent que Direct3D S2 dépasse les méthodes les plus avancées en termes de qualité de génération et d'efficacité, permettant de générer des formes 3D à 1024 de résolution avec seulement 8 noyaux, et de réaliser des représentations volumétriques à 256 de résolution avec un nombre de noyaux significativement inférieur à ceux nécessaires précédemment, rendant la génération de 3D pratique et accessible. Page du projet : https://nju3dv.github.io/projects/Direct3D-S2/",
      "upvotes": 4,
      "discussionId": "6833e93b97966d18e7c1e676",
      "projectPage": "https://nju-3dv.github.io/projects/Direct3D-S2/",
      "githubRepo": "https://github.com/DreamTechAI/Direct3D-S2",
      "ai_summary": "A scalable 3D shape generation framework using sparse volumes and spatial sparse attention, enabling high-resolution generation with reduced computational requirements.",
      "ai_keywords": [
        "Signed Distance Functions",
        "sparse volumes",
        "Spatial Sparse Attention",
        "Diffusion Transformer",
        "variational autoencoder",
        "gigascale 3D generation"
      ]
    },
    "publishedAt": "2025-05-22T22:58:01.000Z",
    "title": "Direct3D-S2: Gigascale 3D Generation Made Easy with Spatial Sparse\n  Attention",
    "summary": "Generating high resolution 3D shapes using volumetric representations such as\nSigned Distance Functions presents substantial computational and memory\nchallenges. We introduce Direct3D S2, a scalable 3D generation framework based\non sparse volumes that achieves superior output quality with dramatically\nreduced training costs. Our key innovation is the Spatial Sparse Attention\nmechanism, which greatly enhances the efficiency of Diffusion Transformer\ncomputations on sparse volumetric data. SSA allows the model to effectively\nprocess large token sets within sparse volumes, significantly reducing\ncomputational overhead and achieving a 3.9x speedup in the forward pass and a\n9.6x speedup in the backward pass. Our framework also includes a variational\nautoencoder that maintains a consistent sparse volumetric format across input,\nlatent, and output stages. Compared to previous methods with heterogeneous\nrepresentations in 3D VAE, this unified design significantly improves training\nefficiency and stability. Our model is trained on public available datasets,\nand experiments demonstrate that Direct3D S2 not only surpasses\nstate-of-the-art methods in generation quality and efficiency, but also enables\ntraining at 1024 resolution using only 8 GPUs, a task typically requiring at\nleast 32 GPUs for volumetric representations at 256 resolution, thus making\ngigascale 3D generation both practical and accessible. Project page:\nhttps://nju3dv.github.io/projects/Direct3D-S2/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17412.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645a24779f06c5897254d14b",
      "avatarUrl": "/avatars/dd0a635674025dcc9a94ee0f4c952083.svg",
      "fullname": "Youtian Lin",
      "name": "LoYoT",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.17091",
      "authors": [
        {
          "_id": "6833cb25fe87d9433dfd2b1c",
          "name": "Prateek Verma",
          "hidden": false
        },
        {
          "_id": "6833cb25fe87d9433dfd2b1d",
          "name": "Mert Pilanci",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-20T22:20:16.000Z",
      "submittedOnDailyAt": "2025-05-26T00:30:41.320Z",
      "title": "Les modèles de langage grands apprennent automatiquement les habiletés visuelles et auditives grâce à la lecture.",
      "submittedOnDailyBy": {
        "_id": "62d7f1119b629105a5d84aad",
        "avatarUrl": "/avatars/c74045063e7c06cb7be0fa41ebb1d824.svg",
        "isPro": false,
        "fullname": "Prateek Verma",
        "user": "prateekv",
        "type": "user"
      },
      "summary": "Dans cet article, nous présentons un intéressant résultat : un modèle de LLM de récupération automatique de mots est entraîné, ce qui permet au modèle de mots de développer la capacité de comprendre les images et les sons internement et d'acquérir des compétences en lecture et écoute. Les modèles avancés de LLM pour le son et la vision fournissent des sorties de mots basées sur les embeddings d'images et de sons, fine-tunant ainsi les modèles de mots de texte. D'autre part, notre architecture accepte en entrée des images de patches, des sons sous forme d'onde ou des tokens. Elle fournit également un embarras typique et des étiquettes de catégorie comme dans un système de classification général. Nous avons démontré que les poids de mots de notre modèle peuvent être effectivement utilisés pour la classification de sons avec les jeux de données FSD-50K et GTZAN. De plus, nous obtenons les mêmes résultats pour la classification d'images sur CIFAR-10 et Fashion-MNIST. Cela souligne que le modèle de mots d'LLM apprend une forte route interne qui active les connexions nécessaires, et par conséquent, il n'est pas nécessaire de créer un nouveau modèle pour chaque application.",
      "upvotes": 4,
      "discussionId": "6833cb26fe87d9433dfd2b64",
      "ai_summary": "Auto-regressive text LLMs trained on text can develop internal capabilities for understanding images and audio, enabling them to perform classification tasks across different modalities without fine-tuning.",
      "ai_keywords": [
        "auto-regressive",
        "LLM",
        "text tokens",
        "audio",
        "visual",
        "embeddings",
        "category labels",
        "classification",
        "FSD-50K",
        "GTZAN",
        "CIFAR-10",
        "Fashion-MNIST",
        "image patches"
      ]
    },
    "publishedAt": "2025-05-20T18:20:16.000Z",
    "title": "Large Language Models Implicitly Learn to See and Hear Just By Reading",
    "summary": "This paper presents a fascinating find: By training an auto-regressive LLM\nmodel on text tokens, the text model inherently develops internally an ability\nto understand images and audio, thereby developing the ability to see and hear\njust by reading. Popular audio and visual LLM models fine-tune text LLM models\nto give text output conditioned on images and audio embeddings. On the other\nhand, our architecture takes in patches of images, audio waveforms or tokens as\ninput. It gives us the embeddings or category labels typical of a\nclassification pipeline. We show the generality of text weights in aiding audio\nclassification for datasets FSD-50K and GTZAN. Further, we show this working\nfor image classification on CIFAR-10 and Fashion-MNIST, as well on image\npatches. This pushes the notion of text-LLMs learning powerful internal\ncircuits that can be utilized by activating necessary connections for various\napplications rather than training models from scratch every single time.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17091.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "62d7f1119b629105a5d84aad",
      "avatarUrl": "/avatars/c74045063e7c06cb7be0fa41ebb1d824.svg",
      "fullname": "Prateek Verma",
      "name": "prateekv",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.16270",
      "authors": [
        {
          "_id": "6833d08edf7cbb5c087a8bf1",
          "user": {
            "_id": "65c288280aa2d53135734a42",
            "avatarUrl": "/avatars/960422a1482ac8b4a52dd08c02d901f6.svg",
            "isPro": false,
            "fullname": "Jiaru Zou",
            "user": "jiaruz2",
            "type": "user"
          },
          "name": "Jiaru Zou",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-26T02:41:41.249Z",
          "hidden": false
        },
        {
          "_id": "6833d08edf7cbb5c087a8bf2",
          "name": "Yikun Ban",
          "hidden": false
        },
        {
          "_id": "6833d08edf7cbb5c087a8bf3",
          "name": "Zihao Li",
          "hidden": false
        },
        {
          "_id": "6833d08edf7cbb5c087a8bf4",
          "name": "Yunzhe Qi",
          "hidden": false
        },
        {
          "_id": "6833d08edf7cbb5c087a8bf5",
          "name": "Ruizhong Qiu",
          "hidden": false
        },
        {
          "_id": "6833d08edf7cbb5c087a8bf6",
          "name": "Ling Yang",
          "hidden": false
        },
        {
          "_id": "6833d08edf7cbb5c087a8bf7",
          "name": "Jingrui He",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T06:00:45.000Z",
      "submittedOnDailyAt": "2025-05-26T00:53:39.701Z",
      "title": "Transformer Copilot: Journals de l'ajustement de micro-enseignant dans un labyrinthe",
      "submittedOnDailyBy": {
        "_id": "64fde4e252e82dd432b74ce9",
        "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
        "isPro": false,
        "fullname": "Ling Yang",
        "user": "Lingaaaaaaa",
        "type": "user"
      },
      "summary": "Les modèles de langage grands sont introduits généralement par des ajustements micro basés sur des données d'apprentissage profond pour des applications spécifiques de l'apprentissage profond. L'ajustement micro standard se concentre sur la minimisation de la perte pour optimiser les paramètres du modèle, mais nous nous efforçons de conserver le signal d'apprentissage du modèle et d'utiliser-le de manière efficace. Cela peut être considéré comme un concept similaire à la réflexion sur les erreurs passées pour améliorer les résultats futurs, comme dans l'apprentissage humain.\n\nTout d'abord, nous présentons l'idée de \"MISTIE LOG\", un registre systématique pour enregistrer l'action d'apprentissage du modèle et les erreurs de reproduction. Nous appelons les modèles originaux basés sur les transformers \"PREIO\" et nous concevons un modèle appelé \"COPIO\" pour améliorer l'efficacité de l'inférence des PREIO par des ajustements logistiques. Nous ajoutons le nom \"Transformer COPIO\" au ensemble de frameworks pour PREIO et COPIO, introduisant : (i) le design de nouveaux modèles COPIO, (ii) le paradigme d'apprentissage collectif dans le registre MISTIE LOG, et (iii) le paradigme d'inférence fonctionnelle qui améliore la génération par des ajustements logistiques. Nous offrons un analyse théorique et expérimental du nouveau framework d'apprentissage. Les expériences sur 12 benchmarks (connaissance générale, arithmétique, tâches de recommandation) montrent que le Transformer COPIO, comparé aux modèles PREIO, améliore son rendement en moyenne de 34,5% grâce à des ajustements micro, montrant une forte capacité d'échelle et de transfert.",
      "upvotes": 3,
      "discussionId": "6833d08fdf7cbb5c087a8c29",
      "ai_summary": "The Transformer Copilot framework enhances large language model performance through a Copilot model that refines the Pilot's logits based on a Mistake Log, leading to consistent performance improvements across various benchmarks.",
      "ai_keywords": [
        "large language models",
        "supervised fine-tuning",
        "domain-specific data",
        "generation loss",
        "model parameters",
        "learning signals",
        "Mistake Log",
        "transformer-based model",
        "Copilot model",
        "logits rectification",
        "joint training paradigm",
        "fused inference paradigm",
        "performance improvements",
        "computational overhead",
        "scalability",
        "transferability"
      ]
    },
    "publishedAt": "2025-05-22T02:00:45.000Z",
    "title": "Transformer Copilot: Learning from The Mistake Log in LLM Fine-tuning",
    "summary": "Large language models are typically adapted to downstream tasks through\nsupervised fine-tuning on domain-specific data. While standard fine-tuning\nfocuses on minimizing generation loss to optimize model parameters, we take a\ndeeper step by retaining and leveraging the model's own learning signals,\nanalogous to how human learners reflect on past mistakes to improve future\nperformance. We first introduce the concept of Mistake Log to systematically\ntrack the model's learning behavior and recurring errors throughout\nfine-tuning. Treating the original transformer-based model as the Pilot, we\ncorrespondingly design a Copilot model to refine the Pilot's inference\nperformance via logits rectification. We name the overall Pilot-Copilot\nframework the Transformer Copilot, which introduces (i) a novel Copilot model\ndesign, (ii) a joint training paradigm where the Copilot continuously learns\nfrom the evolving Mistake Log alongside the Pilot, and (iii) a fused inference\nparadigm where the Copilot rectifies the Pilot's logits for enhanced\ngeneration. We provide both theoretical and empirical analyses on our new\nlearning framework. Experiments on 12 benchmarks spanning commonsense,\narithmetic, and recommendation tasks demonstrate that Transformer Copilot\nconsistently improves performance by up to 34.5%, while introducing marginal\ncomputational overhead to Pilot models and exhibiting strong scalability and\ntransferability.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16270.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64fde4e252e82dd432b74ce9",
      "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
      "fullname": "Ling Yang",
      "name": "Lingaaaaaaa",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.17063",
      "authors": [
        {
          "_id": "6833e65bf9ae3819ea4c568e",
          "name": "Yiduo Guo",
          "hidden": false
        },
        {
          "_id": "6833e65bf9ae3819ea4c568f",
          "user": {
            "_id": "638e4e66629b4d0a62ce1bf3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638e4e66629b4d0a62ce1bf3/s7uQ2qmee2CaXfjZDOovZ.jpeg",
            "isPro": false,
            "fullname": "Zhen Guo",
            "user": "zguo0525",
            "type": "user"
          },
          "name": "Zhen Guo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:09:28.399Z",
          "hidden": false
        },
        {
          "_id": "6833e65bf9ae3819ea4c5690",
          "name": "Chuanwei Huang",
          "hidden": false
        },
        {
          "_id": "6833e65bf9ae3819ea4c5691",
          "name": "Zi-Ang Wang",
          "hidden": false
        },
        {
          "_id": "6833e65bf9ae3819ea4c5692",
          "name": "Zekai Zhang",
          "hidden": false
        },
        {
          "_id": "6833e65bf9ae3819ea4c5693",
          "name": "Haofei Yu",
          "hidden": false
        },
        {
          "_id": "6833e65bf9ae3819ea4c5694",
          "name": "Huishuai Zhang",
          "hidden": false
        },
        {
          "_id": "6833e65bf9ae3819ea4c5695",
          "name": "Yikang Shen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-18T05:35:13.000Z",
      "submittedOnDailyAt": "2025-05-26T02:26:50.238Z",
      "title": "Synthétiques Data RL : La Définition de tâche est tout ce que vous avez besoin de faire.",
      "submittedOnDailyBy": {
        "_id": "638e4e66629b4d0a62ce1bf3",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638e4e66629b4d0a62ce1bf3/s7uQ2qmee2CaXfjZDOovZ.jpeg",
        "isPro": false,
        "fullname": "Zhen Guo",
        "user": "zguo0525",
        "type": "user"
      },
      "summary": "L'apprentissage par renforcement (RL) est l'un des méthodes puissantes pour appliquer des modèles de base à des tâches spécifiques, mais peut introduire une dépendance extensive dans de grands ensembles de données étiquetés par les humains. Nous présentons un cadre simple et général appelé \"RL avec Données Synthétiques\" pour ajuster des modèles en utilisant uniquement des données synthétiques générées à partir de la définition de la tâche. Notre méthode génère des paires de questions et réponses à partir de la définition de la tâche et d'articles trouvés, ajuste la difficulté des questions en fonction de la capacité de résolution de problèmes du modèle et sélectionne des questions pour l'entraînement par RL en fonction de la taux de réussite moyen du modèle. Avec Qwen-2.5-7B, nous avons réussi à améliorer la base du modèle sur GSM8K d'un 29,2% absolu (en réalité +2,9pp pour le modèle entraîné et +6,6pp pour Self-Instruct), 8,7% sur MATH, 7,0pp sur GPQA (comparé à SynthLLM), 8,9% sur MedQA, 17,7% sur CQA (droit) et 13,7% sur CFA (finances). Dans le même bucket de données, nous avons dépassé l'apprentissage supervisé et atteint une performance proche de l'apprentissage par renforcement avec des données humaines (sur GSM8K, +17,2pp). Ajoutant 100 exemples humains a amélioré la performance sur GSM8K d'un 0,4pp. En réduisant les données humaines, l'apprentissage par renforcement avec des données synthétiques permet une application scalable et efficace des modèles. Le code et les exemples sont disponibles sur https://github.com/gydpku/Data_Synthesis_RL.",
      "upvotes": 3,
      "discussionId": "6833e65cf9ae3819ea4c56c9",
      "projectPage": "https://github.com/gydpku/Data_Synthesis_RL",
      "githubRepo": "https://github.com/gydpku/Data_Synthesis_RL",
      "ai_summary": "Synthetic Data RL enhances foundation models through reinforcement learning using only synthetic data, achieving performance comparable to models trained with full human-labeled data.",
      "ai_keywords": [
        "reinforcement learning",
        "RL",
        "synthetic data",
        "reinforcement fine-tuning",
        "question and answer pairs",
        "model solvability",
        "average pass rate",
        "data budget",
        "supervised fine-tuning"
      ]
    },
    "publishedAt": "2025-05-18T01:35:13.000Z",
    "title": "Synthetic Data RL: Task Definition Is All You Need",
    "summary": "Reinforcement learning (RL) is a powerful way to adapt foundation models to\nspecialized tasks, but its reliance on large-scale human-labeled data limits\nbroad adoption. We introduce Synthetic Data RL, a simple and general framework\nthat reinforcement fine-tunes models using only synthetic data generated from a\ntask definition. Our method first generates question and answer pairs from the\ntask definition and retrieved documents, then adapts the difficulty of the\nquestion based on model solvability, and selects questions using the average\npass rate of the model across samples for RL training. On Qwen-2.5-7B, our\nmethod achieves a 29.2% absolute improvement over the base model on GSM8K (+2.9\npp vs. instruction-tuned, +6.6 pp vs. Self-Instruct), 8.7% on MATH, 13.1% on\nGPQA (+7.0 pp vs. SynthLLM), 8.9% on MedQA, 17.7% on CQA (law) and 13.7% on CFA\n(finance). It surpasses supervised fine-tuning under the same data budget and\nnearly matches RL with full human data across datasets (e.g., +17.2 pp on\nGSM8K). Adding 100 human demonstrations improves the performance of GSM8K only\nby 0.4 pp, showing a limited added value. By reducing human data annotation,\nSynthetic Data RL enables scalable and efficient RL-based model adaptation.\nCode and demos are available at https://github.com/gydpku/Data_Synthesis_RL/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17063.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "638e4e66629b4d0a62ce1bf3",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638e4e66629b4d0a62ce1bf3/s7uQ2qmee2CaXfjZDOovZ.jpeg",
      "fullname": "Zhen Guo",
      "name": "zguo0525",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.17540",
      "authors": [
        {
          "_id": "683409de1869c47bd0c423a4",
          "name": "Mingrui Wu",
          "hidden": false
        },
        {
          "_id": "683409de1869c47bd0c423a5",
          "name": "Lu Wang",
          "hidden": false
        },
        {
          "_id": "683409de1869c47bd0c423a6",
          "name": "Pu Zhao",
          "hidden": false
        },
        {
          "_id": "683409de1869c47bd0c423a7",
          "name": "Fangkai Yang",
          "hidden": false
        },
        {
          "_id": "683409de1869c47bd0c423a8",
          "name": "Jianjin Zhang",
          "hidden": false
        },
        {
          "_id": "683409de1869c47bd0c423a9",
          "name": "Jianfeng Liu",
          "hidden": false
        },
        {
          "_id": "683409de1869c47bd0c423aa",
          "name": "Yuefeng Zhan",
          "hidden": false
        },
        {
          "_id": "683409de1869c47bd0c423ab",
          "name": "Weihao Han",
          "hidden": false
        },
        {
          "_id": "683409de1869c47bd0c423ac",
          "name": "Hao Sun",
          "hidden": false
        },
        {
          "_id": "683409de1869c47bd0c423ad",
          "name": "Jiayi Ji",
          "hidden": false
        },
        {
          "_id": "683409de1869c47bd0c423ae",
          "name": "Xiaoshuai Sun",
          "hidden": false
        },
        {
          "_id": "683409de1869c47bd0c423af",
          "name": "Qingwei Lin",
          "hidden": false
        },
        {
          "_id": "683409de1869c47bd0c423b0",
          "name": "Weiwei Deng",
          "hidden": false
        },
        {
          "_id": "683409de1869c47bd0c423b1",
          "name": "Dongmei Zhang",
          "hidden": false
        },
        {
          "_id": "683409de1869c47bd0c423b2",
          "name": "Feng Sun",
          "hidden": false
        },
        {
          "_id": "683409de1869c47bd0c423b3",
          "name": "Qi Zhang",
          "hidden": false
        },
        {
          "_id": "683409de1869c47bd0c423b4",
          "name": "Rongrong Ji",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T06:44:26.000Z",
      "submittedOnDailyAt": "2025-05-26T04:59:40.648Z",
      "title": "Génération d'images par exécution de ré-programmation basée sur la théorie et l'apprentissage par renforcement",
      "submittedOnDailyBy": {
        "_id": "6416d0b2058f65de43191027",
        "avatarUrl": "/avatars/2d99114e5cff39dccc385adfad7032c5.svg",
        "isPro": false,
        "fullname": "Mingrui Wu",
        "user": "mrwu",
        "type": "user"
      },
      "summary": "Récemment, le développement de la génération d'images (T2I) a conduit les modèles actuels à avoir des difficultés pour comprendre précisément l'intention du utilisateur à partir de petits cadeaux ou de prompts défectueux. Dans des études précédentes, on a essayé d'améliorer la fonction des prompts en utilisant des modèles de langage de grande échelle (LLMs), mais ces méthodes présentent une base insuffisante pour la combinaison du sens et de la réalité des images et génèrent souvent des contenus stylistiques et irréalistes. En se basant sur les avancées récentes dans l'inférence de modèles de langage, nous proposons un nouveau cadre d'expansion de prompts appelé RePrompt. Ce méthode introduit une inférence explicite lors du processus d'expansion des prompts par apprentissage par refonte. Notre méthode ne dépend pas de règles handcrafted ou de changements stylistiques, mais elle entraîne un modèle d'étiquettes qui génère des prompts structurés et subjectifs pour optimiser les résultats à l'échelle de l'image. Le modèle de récompense créé améliore la génération des prompts grâce à des évaluations de préférence humaine, de la cohérence sémantique et de la combinaison de l'image. Notre approche ne utilise pas de données de l'utilisateur et permet d'entraîner directement dans le terminal. Les expériences sur GenEval et T2I-Compbench montrent que RePrompt améliore significativement la précision de la disposition spatiale et la généralisation de la combinaison sur différents back-ends de T2I, obtenant des résultats de pointe actuel.",
      "upvotes": 2,
      "discussionId": "683409e21869c47bd0c4248e",
      "ai_summary": "RePrompt, a reprompting framework using reinforcement learning, enhances text-to-image generation by optimizing for image-level outcomes, significantly improving spatial layout and compositional generalization.",
      "ai_keywords": [
        "text-to-image",
        "T2I",
        "large language models",
        "LLMs",
        "reinforcement learning",
        "structured prompts",
        "self-reflective prompts",
        "reward models",
        "human preference",
        "semantic alignment",
        "visual composition",
        "GenEval",
        "T2I-Compbench",
        "spatial layout fidelity",
        "compositional generalization"
      ]
    },
    "publishedAt": "2025-05-23T02:44:26.000Z",
    "title": "RePrompt: Reasoning-Augmented Reprompting for Text-to-Image Generation\n  via Reinforcement Learning",
    "summary": "Despite recent progress in text-to-image (T2I) generation, existing models\noften struggle to faithfully capture user intentions from short and\nunder-specified prompts. While prior work has attempted to enhance prompts\nusing large language models (LLMs), these methods frequently generate stylistic\nor unrealistic content due to insufficient grounding in visual semantics and\nreal-world composition. Inspired by recent advances in reasoning for language\nmodel, we propose RePrompt, a novel reprompting framework that introduces\nexplicit reasoning into the prompt enhancement process via reinforcement\nlearning. Instead of relying on handcrafted rules or stylistic rewrites, our\nmethod trains a language model to generate structured, self-reflective prompts\nby optimizing for image-level outcomes. The tailored reward models assesse the\ngenerated images in terms of human preference, semantic alignment, and visual\ncomposition, providing indirect supervision to refine prompt generation. Our\napproach enables end-to-end training without human-annotated data. Experiments\non GenEval and T2I-Compbench show that RePrompt significantly boosts spatial\nlayout fidelity and compositional generalization across diverse T2I backbones,\nestablishing new state-of-the-art results.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17540.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6416d0b2058f65de43191027",
      "avatarUrl": "/avatars/2d99114e5cff39dccc385adfad7032c5.svg",
      "fullname": "Mingrui Wu",
      "name": "mrwu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.17016",
      "authors": [
        {
          "_id": "6833f7847e0c637c71de0ec6",
          "user": {
            "_id": "64b64debeb9a833e08d079fd",
            "avatarUrl": "/avatars/62ad6f5a8c1b69252e855ef26cc4e7c2.svg",
            "isPro": false,
            "fullname": "Shuhan Tan",
            "user": "tanshh97",
            "type": "user"
          },
          "name": "Shuhan Tan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:09:10.881Z",
          "hidden": false
        },
        {
          "_id": "6833f7847e0c637c71de0ec7",
          "name": "Kairan Dou",
          "hidden": false
        },
        {
          "_id": "6833f7847e0c637c71de0ec8",
          "name": "Yue Zhao",
          "hidden": false
        },
        {
          "_id": "6833f7847e0c637c71de0ec9",
          "name": "Philipp Krähenbühl",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T17:59:45.000Z",
      "submittedOnDailyAt": "2025-05-26T03:40:36.660Z",
      "title": "Modèle de langage visuel interactif et action ensuite entraîné",
      "submittedOnDailyBy": {
        "_id": "64b64debeb9a833e08d079fd",
        "avatarUrl": "/avatars/62ad6f5a8c1b69252e855ef26cc4e7c2.svg",
        "isPro": false,
        "fullname": "Shuhan Tan",
        "user": "tanshh97",
        "type": "user"
      },
      "summary": "RIPT-VLA est une paradigme interactif d'apprentissage par renforcement simple et extensible. Cette paradigme est utilisée pour fine-tuner des modèles de vision-langue-action (Vision-Language-Action) entraînés précédemment avec des récompenses binaires rares de succès. Le processus actuel d'entraînement de VLA utilise des démonstrations d'EXPERT dans des environnements off-line et un apprentissage par renforcement pour s'adapter à de nouvelles tâches et environnements. RIPT-VLA aborde ces problèmes en utilisant un algorithme d'optimisation de politiques stables basé sur un échantillonnage dynamique de rollout et l'évaluation de priorité leave-one-out.\n\nRIPT-VLA présente les caractéristiques suivantes :\n\n1. **Applicabilité à divers modèles VLA** : Augmente l'efficacité du modèle léger QueST d'un 21,2 %, et atteint un 97,5 % de succès sans précédents dans le modèle OpenVLA-OFT de 7B.\n2. **Efficacité en termes de calcul et de données** : Avec seulement une démonstration de données, RIPT-VLA peut entraîner un modèle de SFT inutile (4 %) en 15 itérations pour atteindre un 97 % de succès.\n3. **Généralisation et robustesse** : La politique entraînée par RIPT-VLA s'adapte largement à différentes tâches et scénarios, et est résistante aux états initiaux.\n\nCes résultats démontrent l'importance du paradigme RIPT-VLA pour entraîner efficacement et de manière pratique des modèles de vision-langue-action ultérieurement.",
      "upvotes": 1,
      "discussionId": "6833f7857e0c637c71de0f07",
      "projectPage": "https://ariostgx.github.io/ript_vla/",
      "githubRepo": "https://github.com/Ariostgx/ript-vla",
      "ai_summary": "RIPT-VLA is a reinforcement learning-based interactive post-training paradigm that enhances pretrained Vision-Language-Action models using sparse binary success rewards, improving adaptability and generalization.",
      "ai_keywords": [
        "reinforcement-learning-based",
        "interactive post-training",
        "Vision-Language-Action (VLA) models",
        "sparse binary success rewards",
        "offline expert demonstration",
        "supervised imitation",
        "dynamic rollout sampling",
        "leave-one-out advantage estimation",
        "policy optimization",
        "lightweight QueST model",
        "OpenVLA-OFT model",
        "success rate",
        "computational efficiency",
        "data-efficient",
        "policy learned",
        "generalization",
        "initial state context"
      ]
    },
    "publishedAt": "2025-05-22T13:59:45.000Z",
    "title": "Interactive Post-Training for Vision-Language-Action Models",
    "summary": "We introduce RIPT-VLA, a simple and scalable reinforcement-learning-based\ninteractive post-training paradigm that fine-tunes pretrained\nVision-Language-Action (VLA) models using only sparse binary success rewards.\nExisting VLA training pipelines rely heavily on offline expert demonstration\ndata and supervised imitation, limiting their ability to adapt to new tasks and\nenvironments under low-data regimes. RIPT-VLA addresses this by enabling\ninteractive post-training with a stable policy optimization algorithm based on\ndynamic rollout sampling and leave-one-out advantage estimation.\n  RIPT-VLA has the following characteristics. First, it applies to various VLA\nmodels, resulting in an improvement on the lightweight QueST model by 21.2%,\nand the 7B OpenVLA-OFT model to an unprecedented 97.5% success rate. Second, it\nis computationally efficient and data-efficient: with only one demonstration,\nRIPT-VLA enables an unworkable SFT model (4%) to succeed with a 97% success\nrate within 15 iterations. Furthermore, we demonstrate that the policy learned\nby RIPT-VLA generalizes across different tasks and scenarios and is robust to\nthe initial state context. These results highlight RIPT-VLA as a practical and\neffective paradigm for post-training VLA models through minimal supervision.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17016.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b64debeb9a833e08d079fd",
      "avatarUrl": "/avatars/62ad6f5a8c1b69252e855ef26cc4e7c2.svg",
      "fullname": "Shuhan Tan",
      "name": "tanshh97",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.16293",
      "authors": [
        {
          "_id": "683400b5231225ee202c20b7",
          "user": {
            "_id": "645c26d423ed9b7788d5e24b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/cZMUluWpYUlSLcn6yoC7c.jpeg",
            "isPro": false,
            "fullname": "Rishabh Maheshwary",
            "user": "rmahesh",
            "type": "user"
          },
          "name": "Rishabh Maheshwary",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:08:47.558Z",
          "hidden": false
        },
        {
          "_id": "683400b5231225ee202c20b8",
          "name": "Masoud Hashemi",
          "hidden": false
        },
        {
          "_id": "683400b5231225ee202c20b9",
          "name": "Khyati Mahajan",
          "hidden": false
        },
        {
          "_id": "683400b5231225ee202c20ba",
          "name": "Shiva Krishna Reddy Malay",
          "hidden": false
        },
        {
          "_id": "683400b5231225ee202c20bb",
          "name": "Sai Rajeswar",
          "hidden": false
        },
        {
          "_id": "683400b5231225ee202c20bc",
          "name": "Sathwik Tejaswi Madhusudhan",
          "hidden": false
        },
        {
          "_id": "683400b5231225ee202c20bd",
          "name": "Spandana Gella",
          "hidden": false
        },
        {
          "_id": "683400b5231225ee202c20be",
          "name": "Vikas Yadav",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T06:45:05.000Z",
      "submittedOnDailyAt": "2025-05-26T04:22:12.337Z",
      "title": "Solution au problème de la complexité de la résolution de questions basée sur la théorie logique des LLM renforcée par la dynamique de notes écrites",
      "submittedOnDailyBy": {
        "_id": "645c26d423ed9b7788d5e24b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/cZMUluWpYUlSLcn6yoC7c.jpeg",
        "isPro": false,
        "fullname": "Rishabh Maheshwary",
        "user": "rmahesh",
        "type": "user"
      },
      "summary": "Utilisant l'Iterative RAG pour répondre à des questions multi-tour, on rencontre des défis tels que l'accumulation de longs contextes et de l'information inadéquate. Cela limite la capacité du modèle à traiter et à inférer sur le contenu récupéré. Les méthodes récentes se concentrent sur la compression de l'information récupérée, mais sont limitées à un seul tour RAG ou nécessitent un fine-tuning, ou présentent une capacité limitée d'extension dans un RAG itératif. Pour résoudre ces problèmes, nous proposons Notes Writing. Notes Writing génère des notes claires et pertinentes à partir de documents récupérés à chaque étape, réduisant le bruit et maintenant seulement l'information essentielle. Cela augmente indirectement la longueur de contexte valide des Grands Modèles de Langue (LLMs), permettant au modèle de pouvoir inférer et planifier plus efficacement avec de grandes quantités de texte d'entrée. Notes Writing est indépendant du cadre et peut être intégré dans divers méthodes RAG itératives. Nous avons testé l'efficacité de cette méthodologie en utilisant trois méthodes RAG itératives, atteignant un accroissement moyen de 15,6 points pour cent, tout en minimisant au même temps l'augmentation de tokens de sortie.",
      "upvotes": 1,
      "discussionId": "683400b6231225ee202c20e3",
      "ai_summary": "Notes Writing enhances iterative RAG by generating concise notes at each step, improving reasoning and performance while minimizing output increase.",
      "ai_keywords": [
        "Iterative RAG",
        "multi-hop question answering",
        "context length",
        "irrelevant information",
        "dimensionality reduction",
        "Notes Writing",
        "Large Language Models",
        "LLMs",
        "framework agnostic",
        "evaluation datasets"
      ]
    },
    "publishedAt": "2025-05-22T02:45:05.000Z",
    "title": "Augmenting LLM Reasoning with Dynamic Notes Writing for Complex QA",
    "summary": "Iterative RAG for multi-hop question answering faces challenges with lengthy\ncontexts and the buildup of irrelevant information. This hinders a model's\ncapacity to process and reason over retrieved content and limits performance.\nWhile recent methods focus on compressing retrieved information, they are\neither restricted to single-round RAG, require finetuning or lack scalability\nin iterative RAG. To address these challenges, we propose Notes Writing, a\nmethod that generates concise and relevant notes from retrieved documents at\neach step, thereby reducing noise and retaining only essential information.\nThis indirectly increases the effective context length of Large Language Models\n(LLMs), enabling them to reason and plan more effectively while processing\nlarger volumes of input text. Notes Writing is framework agnostic and can be\nintegrated with different iterative RAG methods. We demonstrate its\neffectiveness with three iterative RAG methods, across two models and four\nevaluation datasets. Notes writing yields an average improvement of 15.6\npercentage points overall, with minimal increase in output tokens.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16293.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645c26d423ed9b7788d5e24b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/cZMUluWpYUlSLcn6yoC7c.jpeg",
      "fullname": "Rishabh Maheshwary",
      "name": "rmahesh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.16022",
      "authors": [
        {
          "_id": "68342cb2924393051af84722",
          "name": "Wei Liu",
          "hidden": false
        },
        {
          "_id": "68342cb2924393051af84723",
          "name": "Siya Qi",
          "hidden": false
        },
        {
          "_id": "68342cb2924393051af84724",
          "name": "Xinyu Wang",
          "hidden": false
        },
        {
          "_id": "68342cb2924393051af84725",
          "name": "Chen Qian",
          "hidden": false
        },
        {
          "_id": "68342cb2924393051af84726",
          "name": "Yali Du",
          "hidden": false
        },
        {
          "_id": "68342cb2924393051af84727",
          "name": "Yulan He",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T21:12:35.000Z",
      "submittedOnDailyAt": "2025-05-26T07:38:18.450Z",
      "title": "NOVER : Apprentissage visuel de modèles de langage par apprentissage par renforcement sans évaluateur",
      "submittedOnDailyBy": {
        "_id": "66e2932e5c100c12aa2def39",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/FiQ5Fap-qVqnXeULGPYs6.png",
        "isPro": false,
        "fullname": "weiliu",
        "user": "thinkwee",
        "type": "user"
      },
      "summary": "Dans le développement récent, des fonctions comme DeepSeek R1-Zero mettent en avant l'efficacité du paradigme d'apprentissage par récompenses basé sur la partie finale de la réponse du modèle de langue. Ces méthodes sont limitées aux domaines comme la mathématiques ou la programmation, car elles calculent les récompenses en se basant sur des données de vérification externes, qui sont faciles à obtenir. Les modèles de récompenses nécessitent de hauts coûts d'entraînement et de haute qualité de données, ce qui rend leur solution difficile. Dans cet article, nous proposons un cadre d'apprentissage par récompenses général NOVER (Reinforcement Learning sans Vérificateur), qui ne nécessite pas de données de vérification externes. NOVER nécessite seulement des données de fin de parcours standard et permet l'apprentissage de récompenses sans besoin de données de vérification externes. Dans des tâches de résolution de problèmes de texte en texte, NOVER permet l'apprentissage de récompenses et dépasse un rendement de au moins 7,7% par rapport aux modèles de même taille, comme DeepSeek R1 671B. De plus, la flexibilité de NOVER offre de nouvelles possibilités pour l'optimisation de grands modèles de langue et explore de nouvelles opportunités dans l'apprentissage par récompenses inverse.",
      "upvotes": 1,
      "discussionId": "68342cb3924393051af8476b",
      "githubRepo": "https://github.com/thinkwee/NOVER",
      "ai_summary": "NOVER, a reinforcement learning framework that eliminates the need for external verifiers, enhances language model performance across text-to-text tasks.",
      "ai_keywords": [
        "incentive training",
        "reinforcement learning",
        "NOVER",
        "NO-VERifier Reinforcement Learning",
        "DeepSeek R1-Zero",
        "DeepSeek R1 671B",
        "inverse incentive training"
      ]
    },
    "publishedAt": "2025-05-21T17:12:35.000Z",
    "title": "NOVER: Incentive Training for Language Models via Verifier-Free\n  Reinforcement Learning",
    "summary": "Recent advances such as DeepSeek R1-Zero highlight the effectiveness of\nincentive training, a reinforcement learning paradigm that computes rewards\nsolely based on the final answer part of a language model's output, thereby\nencouraging the generation of intermediate reasoning steps. However, these\nmethods fundamentally rely on external verifiers, which limits their\napplicability to domains like mathematics and coding where such verifiers are\nreadily available. Although reward models can serve as verifiers, they require\nhigh-quality annotated data and are costly to train. In this work, we propose\nNOVER, NO-VERifier Reinforcement Learning, a general reinforcement learning\nframework that requires only standard supervised fine-tuning data with no need\nfor an external verifier. NOVER enables incentive training across a wide range\nof text-to-text tasks and outperforms the model of the same size distilled from\nlarge reasoning models such as DeepSeek R1 671B by 7.7 percent. Moreover, the\nflexibility of NOVER enables new possibilities for optimizing large language\nmodels, such as inverse incentive training.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16022.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66e2932e5c100c12aa2def39",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/FiQ5Fap-qVqnXeULGPYs6.png",
      "fullname": "weiliu",
      "name": "thinkwee",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.15805",
      "authors": [
        {
          "_id": "682eeb06720821973d643576",
          "user": {
            "_id": "647c4a2692182942d7c2e698",
            "avatarUrl": "/avatars/bcddf5fe49aa092a2645f70812108348.svg",
            "isPro": false,
            "fullname": "HWANCHANG",
            "user": "HwanChang0106",
            "type": "user"
          },
          "name": "Hwan Chang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:18:42.752Z",
          "hidden": false
        },
        {
          "_id": "682eeb06720821973d643577",
          "name": "Yumin Kim",
          "hidden": false
        },
        {
          "_id": "682eeb06720821973d643578",
          "name": "Yonghyun Jun",
          "hidden": false
        },
        {
          "_id": "682eeb06720821973d643579",
          "name": "Hwanhee Lee",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T17:58:11.000Z",
      "submittedOnDailyAt": "2025-05-26T07:18:19.930Z",
      "title": "Sécurisez-vous ! Benchmark pour l'attaque indirecte de type question dans le contexte de grands modèles de langue : conservation des politiques de sécurité.",
      "submittedOnDailyBy": {
        "_id": "647c4a2692182942d7c2e698",
        "avatarUrl": "/avatars/bcddf5fe49aa092a2645f70812108348.svg",
        "isPro": false,
        "fullname": "HWANCHANG",
        "user": "HwanChang0106",
        "type": "user"
      },
      "summary": "L'évolution des LLMs dans des domaines sensibles comme les entreprises et le gouvernement s'étend de plus en plus, ce qui rend important l'introduction de politiques de sécurité personnalisées qui sont respectées dans le contexte, en particulier pour la protection de l'information. L'étude précédente des LLMs a principalement porté sur la sécurité générale et les données sensibles, mais a eu peu d'attention à la maintenance de la sécurité contextuelle face aux attaques. Dans ce sens, nous présentons un nouveau grand ensemble de données de benchmark appelé CoPriva, qui évalue dans quelle mesure les LLMs respectent des politiques de non-publication contextuelles personnalisées. Ces données sont générées dans des contextes réels, incluent des politiques et des questions spécifiques, et explorent des informations interdites face aux attaques directes et indirectes. Nous évaluons 10 modèles de LLM dans notre benchmark et découvrons des importantes vulnérabilités : de nombreux modèles détruisent les politiques personnalisées et exposent des informations sensibles. Ces échecs sont particulièrement graves face aux attaques indirectes. Notre analyse révèle que les applications sensibles actuelles en sécurité des LLMs ont des faiblesses importantes. Les modèles montrent la capacité de fournir des réponses correctes, mais présentent des difficultés à appliquer les restrictions des politiques lors de la génération. D'autre part, ils ont démontré leur capacité à modifier la sortie lorsque des prompts sont fournis directement. Ces résultats soulignent la nécessité de développer des méthodes plus robustes pour garantir la sécurité contextuelle.",
      "upvotes": 1,
      "discussionId": "682eeb07720821973d6435ec",
      "githubRepo": "https://github.com/hwanchang00/CoPriva",
      "ai_summary": "LLMs frequently violate contextual security policies by leaking sensitive information, particularly under indirect attacks, indicating a critical gap in current safety mechanisms.",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "security policies",
        "information non-disclosure",
        "CoPriva",
        "contextual security preservation",
        "question answering",
        "explicit policies",
        "indirect attacks",
        "virus",
        "policy constraints",
        "output revision"
      ]
    },
    "publishedAt": "2025-05-21T13:58:11.000Z",
    "title": "Keep Security! Benchmarking Security Policy Preservation in Large\n  Language Model Contexts Against Indirect Attacks in Question Answering",
    "summary": "As Large Language Models (LLMs) are increasingly deployed in sensitive\ndomains such as enterprise and government, ensuring that they adhere to\nuser-defined security policies within context is critical-especially with\nrespect to information non-disclosure. While prior LLM studies have focused on\ngeneral safety and socially sensitive data, large-scale benchmarks for\ncontextual security preservation against attacks remain lacking. To address\nthis, we introduce a novel large-scale benchmark dataset, CoPriva, evaluating\nLLM adherence to contextual non-disclosure policies in question answering.\nDerived from realistic contexts, our dataset includes explicit policies and\nqueries designed as direct and challenging indirect attacks seeking prohibited\ninformation. We evaluate 10 LLMs on our benchmark and reveal a significant\nvulnerability: many models violate user-defined policies and leak sensitive\ninformation. This failure is particularly severe against indirect attacks,\nhighlighting a critical gap in current LLM safety alignment for sensitive\napplications. Our analysis reveals that while models can often identify the\ncorrect answer to a query, they struggle to incorporate policy constraints\nduring generation. In contrast, they exhibit a partial ability to revise\noutputs when explicitly prompted. Our findings underscore the urgent need for\nmore robust methods to guarantee contextual security.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15805.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "647c4a2692182942d7c2e698",
      "avatarUrl": "/avatars/bcddf5fe49aa092a2645f70812108348.svg",
      "fullname": "HWANCHANG",
      "name": "HwanChang0106",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.12891",
      "authors": [
        {
          "_id": "683059e8e2f446ed653e8512",
          "name": "Shaohang Wei",
          "hidden": false
        },
        {
          "_id": "683059e8e2f446ed653e8513",
          "name": "Wei Li",
          "hidden": false
        },
        {
          "_id": "683059e8e2f446ed653e8514",
          "user": {
            "_id": "6447ca6ca478b20f1755b294",
            "avatarUrl": "/avatars/5049856b5ed1b74533fff902e14b4c7c.svg",
            "isPro": false,
            "fullname": "Feifan Song",
            "user": "songff",
            "type": "user"
          },
          "name": "Feifan Song",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:14:11.572Z",
          "hidden": false
        },
        {
          "_id": "683059e8e2f446ed653e8515",
          "name": "Wen Luo",
          "hidden": false
        },
        {
          "_id": "683059e8e2f446ed653e8516",
          "name": "Tianyi Zhuang",
          "hidden": false
        },
        {
          "_id": "683059e8e2f446ed653e8517",
          "name": "Haochen Tan",
          "hidden": false
        },
        {
          "_id": "683059e8e2f446ed653e8518",
          "name": "Zhijiang Guo",
          "hidden": false
        },
        {
          "_id": "683059e8e2f446ed653e8519",
          "name": "Houfeng Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-19T09:22:02.000Z",
      "submittedOnDailyAt": "2025-05-26T03:54:28.256Z",
      "title": "TIME : Scénario réel de la logique temporelle des modèles de langage LLMs",
      "submittedOnDailyBy": {
        "_id": "6447ca6ca478b20f1755b294",
        "avatarUrl": "/avatars/5049856b5ed1b74533fff902e14b4c7c.svg",
        "isPro": false,
        "fullname": "Feifan Song",
        "user": "songff",
        "type": "user"
      },
      "summary": "La logique temporelle est essentielle pour que les modèles de langage grands (LLMs) comprennent la réalité. Cependant, la recherche actuelle ne comprend pas correctement les problèmes de la réalité en termes temporels. Cela se doit à trois facteurs : (1) l'information temporelle forte, (2) la dynamique des événements qui changent rapidement, et (3) les relations temporelles complexes qui se développent dans la société. Pour résoudre ce problème, nous proposons un cadre d'évaluation temporel appelé TIME. TIME comprend 38,522 paires de questions et réponses et est divisé en trois niveaux avec 11 tâches détaillées. Ce cadre d'évaluation inclut trois sous-ensembles de données qui reflètent différents problèmes de la réalité : TIME-Wiki, TIME-News et TIME-Dial. Nous avons effectué des expériences larges avec des modèles d'inférence et des modèles non-inférentiels, analysant le rendement en termes de logique temporelle et résumant l'impact de l'échelle temporelle sur la capacité de logique temporelle. De plus, pour encourager la recherche future et l'évaluation standard, nous avons lancé TIME-Lite. Le code est disponible sur https://github.com/sylvain-wei/TIME et les ensembles de données sur https://huggingface.co/datasets/SylvainWei/TIME.",
      "upvotes": 1,
      "discussionId": "683059eae2f446ed653e85d7",
      "ai_summary": "A benchmark called TIME assesses temporal reasoning in LLMs across varied real-world challenges, including intensive temporal information, fast-changing event dynamics, and complex social interactions, and evaluates the impact of test-time scaling.",
      "ai_keywords": [
        "Temporal reasoning",
        "Large Language Models (LLMs)",
        "QA pairs",
        "benchmark",
        "TIME-Wiki",
        "TIME-News",
        "TIME-Dial",
        "reasoning models",
        "non-reasoning models",
        "TIME-Lite"
      ]
    },
    "publishedAt": "2025-05-19T05:22:02.000Z",
    "title": "TIME: A Multi-level Benchmark for Temporal Reasoning of LLMs in\n  Real-World Scenarios",
    "summary": "Temporal reasoning is pivotal for Large Language Models (LLMs) to comprehend\nthe real world. However, existing works neglect the real-world challenges for\ntemporal reasoning: (1) intensive temporal information, (2) fast-changing event\ndynamics, and (3) complex temporal dependencies in social interactions. To\nbridge this gap, we propose a multi-level benchmark TIME, designed for temporal\nreasoning in real-world scenarios. TIME consists of 38,522 QA pairs, covering 3\nlevels with 11 fine-grained sub-tasks. This benchmark encompasses 3\nsub-datasets reflecting different real-world challenges: TIME-Wiki, TIME-News,\nand TIME-Dial. We conduct extensive experiments on reasoning models and\nnon-reasoning models. And we conducted an in-depth analysis of temporal\nreasoning performance across diverse real-world scenarios and tasks, and\nsummarized the impact of test-time scaling on temporal reasoning capabilities.\nAdditionally, we release TIME-Lite, a human-annotated subset to foster future\nresearch and standardized evaluation in temporal reasoning. The code is\navailable at https://github.com/sylvain-wei/TIME , and the dataset is available\nat https://huggingface.co/datasets/SylvainWei/TIME .",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.12891.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6447ca6ca478b20f1755b294",
      "avatarUrl": "/avatars/5049856b5ed1b74533fff902e14b4c7c.svg",
      "fullname": "Feifan Song",
      "name": "songff",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.16056",
      "authors": [
        {
          "_id": "6830894db51948863e05b68c",
          "user": {
            "_id": "64bfa1401d40292dd32f93d7",
            "avatarUrl": "/avatars/39a3d1772e0bc54f9bb2db7ce7047784.svg",
            "isPro": false,
            "fullname": "Leo Liang",
            "user": "ljcleo",
            "type": "user"
          },
          "name": "Jingcong Liang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:13:21.361Z",
          "hidden": false
        },
        {
          "_id": "6830894db51948863e05b68d",
          "name": "Siyuan Wang",
          "hidden": false
        },
        {
          "_id": "6830894db51948863e05b68e",
          "name": "Miren Tian",
          "hidden": false
        },
        {
          "_id": "6830894db51948863e05b68f",
          "name": "Yitong Li",
          "hidden": false
        },
        {
          "_id": "6830894db51948863e05b690",
          "name": "Duyu Tang",
          "hidden": false
        },
        {
          "_id": "6830894db51948863e05b691",
          "name": "Zhongyu Wei",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T22:13:09.000Z",
      "submittedOnDailyAt": "2025-05-26T07:53:13.385Z",
      "title": "Tous les modèles ne sont pas adaptés à l'accumulation d'experts : La cohérence de la route locale dans les modèles Mixture-of-Experts",
      "submittedOnDailyBy": {
        "_id": "64bfa1401d40292dd32f93d7",
        "avatarUrl": "/avatars/39a3d1772e0bc54f9bb2db7ce7047784.svg",
        "isPro": false,
        "fullname": "Leo Liang",
        "user": "ljcleo",
        "type": "user"
      },
      "summary": "Mixture-of-Experts (MoE) permet une échellanisation efficace de grands modèles de langage naturel (LLMs) en utilisant des experts rarement activés pendant l'inférence. Pour appliquer efficacement MoE sur des dispositifs avec des limitations de mémoire, plusieurs systèmes ont introduit un *surcharge d'experts*, désignant pour stocker partie des experts en mémoire rapide et exécuter le reste sur le CPU ou le charger lorsque nécessaire. Malgré cela, il existe également des recherches qui exploitent la localité de l'activation des experts (où des tokens continus activent des experts similaires), bien que la **cohérence de la route locale** varie entre les modèles et n'a pas encore été étudiée. Dans ce travail, on propose deux métriques pour évaluer la cohérence de la route locale dans les modèles MoE : 1. **Segment Routing Best Performance (SRP)** évalue dans quelle mesure un groupe fixe d'experts répond aux besoins de segments de tokens. 2. **Segment Cache Best Hit Rate (SCH)** évalue le meilleur taux de hit dans le cache pour un niveau de segments donné sous une limite de taille de cache. On a analysé 20 modèles MoE de différentes tailles et structures, et on a trouvé que les modèles qui appliquent MoE à chaque couche et ne partagent pas d'experts montrent la plus grande cohérence de la route locale. De plus, les experts spécialisés dans des zones contribuent davantage à la cohérence de la route locale, et presque tous les modèles atteignent un équilibre entre la taille de cache et son efficacité en utilisant une taille de cache approximativement double celle des experts actifs. Ces résultats sont reliés à la conception et à l'application efficaces de MoE en mémoire, sans perdre de la vitesse d'inférence. Dans ce travail, on publie le code pour reproduire les expériences (https://github.com/ljcleo/moe-lrc).",
      "upvotes": 0,
      "discussionId": "6830894eb51948863e05b6e8",
      "ai_summary": "MoE models achieve efficient scaling in LLMs with expert offloading, emphasizing the importance of local routing consistency and cache effectiveness.",
      "ai_keywords": [
        "Mixture-of-Experts (MoE)",
        "large language models (LLMs)",
        "expert offloading",
        "fast memory",
        "slow memory",
        "local routing consistency",
        "Segment Routing Best Performance (SRP)",
        "Segment Cache Best Hit Rate (SCH)",
        "domain-specialized experts",
        "vocabulary-specialized experts"
      ]
    },
    "publishedAt": "2025-05-21T18:13:09.000Z",
    "title": "Not All Models Suit Expert Offloading: On Local Routing Consistency of\n  Mixture-of-Expert Models",
    "summary": "Mixture-of-Experts (MoE) enables efficient scaling of large language models\n(LLMs) with sparsely activated experts during inference. To effectively deploy\nlarge MoE models on memory-constrained devices, many systems introduce *expert\noffloading* that caches a subset of experts in fast memory, leaving others on\nslow memory to run on CPU or load on demand. While some research has exploited\nthe locality of expert activations, where consecutive tokens activate similar\nexperts, the degree of this **local routing consistency** varies across models\nand remains understudied. In this paper, we propose two metrics to measure\nlocal routing consistency of MoE models: (1) **Segment Routing Best Performance\n(SRP)**, which evaluates how well a fixed group of experts can cover the needs\nof a segment of tokens, and (2) **Segment Cache Best Hit Rate (SCH)**, which\nmeasures the optimal segment-level cache hit rate under a given cache size\nlimit. We analyzed 20 MoE LLMs with diverse sizes and architectures and found\nthat models that apply MoE on every layer and do not use shared experts exhibit\nthe highest local routing consistency. We further showed that\ndomain-specialized experts contribute more to routing consistency than\nvocabulary-specialized ones, and that most models can balance between cache\neffectiveness and efficiency with cache sizes approximately 2x the active\nexperts. These findings pave the way for memory-efficient MoE design and\ndeployment without compromising inference speed. We publish the code for\nreplicating experiments at https://github.com/ljcleo/moe-lrc .",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16056.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64bfa1401d40292dd32f93d7",
      "avatarUrl": "/avatars/39a3d1772e0bc54f9bb2db7ce7047784.svg",
      "fullname": "Leo Liang",
      "name": "ljcleo",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.11881",
      "authors": [
        {
          "_id": "6833ebdb142b0e50399413d3",
          "name": "Giyeong Oh",
          "hidden": false
        },
        {
          "_id": "6833ebdb142b0e50399413d4",
          "name": "Woohyun Cho",
          "hidden": false
        },
        {
          "_id": "6833ebdb142b0e50399413d5",
          "name": "Siyeol Kim",
          "hidden": false
        },
        {
          "_id": "6833ebdb142b0e50399413d6",
          "name": "Suhwan Choi",
          "hidden": false
        },
        {
          "_id": "6833ebdb142b0e50399413d7",
          "name": "Younjae Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-17T07:16:11.000Z",
      "submittedOnDailyAt": "2025-05-26T02:52:51.028Z",
      "title": "Retour de l'échec de la connexion : mise à jour de la normalisation efficace dans une grande réseau profond établi",
      "submittedOnDailyBy": {
        "_id": "63d93667255ef6add20f9272",
        "avatarUrl": "/avatars/99a3aeadcc81ef85164cdfb6ab186b17.svg",
        "isPro": false,
        "fullname": "Giyeong Oh",
        "user": "BootsofLagrangian",
        "type": "user"
      },
      "summary": "Les résidus de connexion sont un élément important dans les réseaux neuronaux profonds et peuvent atténuer le phénomène de perte de gradient lors du processus d'entraînement, permettant ainsi d'augmenter la profondeur. Cependant, le méthode générale d'actualisation des résidus ajoute directement le sortie du module au flux d'entrée. Dans ce cas, l'actualisation principale se concentre sur renforcer ou régulariser la direction du flux existant, ce qui peut réduire la capacité d'apprentissage du module. Dans cet article, nous introduisons une actualisation des résidus orthogonaux : le sortie du module est décomposée par rapport au flux d'entrée et seule la composante orthogonale à ce flux est ajoutée. Cette conception encourage le module à fournir principalement de nouvelles directions de représentation, favorise un apprentissage de caractéristiques riche et encourage un entraînement efficace. L'actualisation orthogonale a démontré améliorer la précision de généralisation et la stabilité de l'entraînement dans différentes architectures (ResNetV2, Vision Transformers) et ensembles de données (CIFARs, TinyImageNet, ImageNet-1k). Par exemple, la précision top-1 de ViT-B sur ImageNet-1k a été améliorée de +4.3%p.",
      "upvotes": 0,
      "discussionId": "6833ebdc142b0e5039941420",
      "ai_summary": "Orthogonal Residual Updates enhance feature learning and training stability by decomposing module outputs to contribute primarily novel features.",
      "ai_keywords": [
        "residual connections",
        "vanishing gradients",
        "orthogonal update",
        "ResNetV2",
        "Vision Transformers",
        "CIFARs",
        "TinyImageNet",
        "ImageNet-1k",
        "top-1 accuracy"
      ]
    },
    "publishedAt": "2025-05-17T03:16:11.000Z",
    "title": "Revisiting Residual Connections: Orthogonal Updates for Stable and\n  Efficient Deep Networks",
    "summary": "Residual connections are pivotal for deep neural networks, enabling greater\ndepth by mitigating vanishing gradients. However, in standard residual updates,\nthe module's output is directly added to the input stream. This can lead to\nupdates that predominantly reinforce or modulate the existing stream direction,\npotentially underutilizing the module's capacity for learning entirely novel\nfeatures. In this work, we introduce Orthogonal Residual Update: we decompose\nthe module's output relative to the input stream and add only the component\northogonal to this stream. This design aims to guide modules to contribute\nprimarily new representational directions, fostering richer feature learning\nwhile promoting more efficient training. We demonstrate that our orthogonal\nupdate strategy improves generalization accuracy and training stability across\ndiverse architectures (ResNetV2, Vision Transformers) and datasets (CIFARs,\nTinyImageNet, ImageNet-1k), achieving, for instance, a +4.3\\%p top-1 accuracy\ngain for ViT-B on ImageNet-1k.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.11881.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63d93667255ef6add20f9272",
      "avatarUrl": "/avatars/99a3aeadcc81ef85164cdfb6ab186b17.svg",
      "fullname": "Giyeong Oh",
      "name": "BootsofLagrangian",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  }
]