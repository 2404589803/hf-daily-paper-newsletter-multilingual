[
  {
    "paper": {
      "id": "2504.15120",
      "authors": [
        {
          "_id": "680733cf7722bb6407ca0787",
          "user": {
            "_id": "65276c7911a8a521c91bc10f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65276c7911a8a521c91bc10f/39dbuUtqQTJERTJ_WkxSh.jpeg",
            "isPro": false,
            "fullname": "Khalil Hennara",
            "user": "Hennara",
            "type": "user"
          },
          "name": "Khalil Hennara",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-04-22T09:37:47.479Z",
          "hidden": false
        },
        {
          "_id": "680733cf7722bb6407ca0788",
          "name": "Sara Chrouf",
          "hidden": false
        },
        {
          "_id": "680733cf7722bb6407ca0789",
          "user": {
            "_id": "63aa7667769a10efc404fbbc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63aa7667769a10efc404fbbc/tn8ZxUmTEMS0Gze7_F7JL.jpeg",
            "isPro": false,
            "fullname": "Mohamed Motasim Hamed",
            "user": "Moatasem444",
            "type": "user"
          },
          "name": "Mohamed Motaism Hamed",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-23T08:37:25.702Z",
          "hidden": false
        },
        {
          "_id": "680733cf7722bb6407ca078a",
          "user": {
            "_id": "65704741e1cfce1764ce652e",
            "avatarUrl": "/avatars/9189aaf417426af4ebe381ed364a6c0e.svg",
            "isPro": false,
            "fullname": "Zeina Aldallal",
            "user": "ZeinaD",
            "type": "user"
          },
          "name": "Zeina Aldallal",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-23T05:30:42.569Z",
          "hidden": false
        },
        {
          "_id": "680733cf7722bb6407ca078b",
          "name": "Omar Hadid",
          "hidden": false
        },
        {
          "_id": "680733cf7722bb6407ca078c",
          "name": "Safwan AlModhayan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-21T14:17:25.000Z",
      "submittedOnDailyAt": "2025-04-23T03:28:02.778Z",
      "title": "クワイン 1.5B : SLM en arabe grâce à l'injection de langage",
      "submittedOnDailyBy": {
        "_id": "65276c7911a8a521c91bc10f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65276c7911a8a521c91bc10f/39dbuUtqQTJERTJ_WkxSh.jpeg",
        "isPro": false,
        "fullname": "Khalil Hennara",
        "user": "Hennara",
        "type": "user"
      },
      "summary": "Une étape importante dans le développement de l'intelligence artificielle est l'ajout de nouveaux savoirs à des modèles existants. Dans cet article, nous présentons une nouvelle méthodologie pour l'intégration de nouvelles langues dans des modèles de langage grand (LLM). Notre approche consiste à enregistrer dans les modèles existants des langues cibles qui n'ont pas été vues avant, réussiant ainsi l'incorporation de nouveau savoir sans le détruire. Nous avons utilisé un petit modèle de code open entraîné en anglais pour injecter l'arabe, et nous avons entraîné un petit modèle de 1500 millions de paramètres appelé \"Kuwain\". Notre méthode a amélioré en moyenne le rendement en arabe dans différents référentiels de 8%, en maintenant le savoir précédent et en entraînant le modèle avec un minimum de données du modèle original. C'est une choix efficace en termes de coût, au lieu d'entraîner des modèles détaillés pour chaque langue individuellement. Les résultats montrent la possibilité d'une expansion efficace des modèles de langage et la capacité d'étendre et d'éviter le processus de force de ressources, démontrant ainsi l'efficacité et la viabilité de cette approche.",
      "upvotes": 58,
      "discussionId": "680733d07722bb6407ca07da",
      "githubRepo": "https://github.com/misraj-ai/Kuwain-Arabic-cleaner",
      "ai_keywords": [
        "large language model (LLM)",
        "tiny model",
        "Kuwain",
        "language integration",
        "Arabic language",
        "benchmarks",
        "language model expansion"
      ]
    },
    "publishedAt": "2025-04-21T10:17:25.000Z",
    "title": "Kuwain 1.5B: An Arabic SLM via Language Injection",
    "summary": "Enhancing existing models with new knowledge is a crucial aspect of AI\ndevelopment. This paper introduces a novel method for integrating a new\nlanguage into a large language model (LLM). Our approach successfully\nincorporates a previously unseen target language into an existing LLM without\ncompromising its prior knowledge. We trained a tiny model with 1.5 billion\nparameters named Kuwain by injecting the Arabic language into a small\nopen-source model mainly trained in English. Our method demonstrates\nsignificant improvements in Arabic language performance, with an average 8%\nimprovement across various benchmarks, while retaining the model's existing\nknowledge with a minimum amount of the original model's data. This offers a\ncost-effective alternative to training a comprehensive model in both English\nand Arabic. The results highlight the potential for efficient, targeted\nlanguage model expansion without extensive retraining or resource-intensive\nprocesses.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15120.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65276c7911a8a521c91bc10f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65276c7911a8a521c91bc10f/39dbuUtqQTJERTJ_WkxSh.jpeg",
      "fullname": "Khalil Hennara",
      "name": "Hennara",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.16084",
      "authors": [
        {
          "_id": "6808558a07e80b69b2e351b5",
          "name": "Yuxin Zuo",
          "hidden": false
        },
        {
          "_id": "6808558a07e80b69b2e351b6",
          "user": {
            "_id": "60bc94cd85a3ab33829b6211",
            "avatarUrl": "/avatars/b57d36c7577fbbb42ea5b963eef4144a.svg",
            "isPro": false,
            "fullname": "Kaiyan Zhang",
            "user": "iseesaw",
            "type": "user"
          },
          "name": "Kaiyan Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-23T08:27:51.438Z",
          "hidden": false
        },
        {
          "_id": "6808558a07e80b69b2e351b7",
          "name": "Shang Qu",
          "hidden": false
        },
        {
          "_id": "6808558a07e80b69b2e351b8",
          "name": "Li Sheng",
          "hidden": false
        },
        {
          "_id": "6808558a07e80b69b2e351b9",
          "name": "Xuekai Zhu",
          "hidden": false
        },
        {
          "_id": "6808558a07e80b69b2e351ba",
          "name": "Biqing Qi",
          "hidden": false
        },
        {
          "_id": "6808558a07e80b69b2e351bb",
          "name": "Youbang Sun",
          "hidden": false
        },
        {
          "_id": "6808558a07e80b69b2e351bc",
          "name": "Ganqu Cui",
          "hidden": false
        },
        {
          "_id": "6808558a07e80b69b2e351bd",
          "name": "Ning Ding",
          "hidden": false
        },
        {
          "_id": "6808558a07e80b69b2e351be",
          "name": "Bowen Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-22T17:59:56.000Z",
      "submittedOnDailyAt": "2025-04-23T01:22:31.055Z",
      "title": "TTRL: Apprentissage par renforcement pendant l'entraînement",
      "submittedOnDailyBy": {
        "_id": "60bc94cd85a3ab33829b6211",
        "avatarUrl": "/avatars/b57d36c7577fbbb42ea5b963eef4144a.svg",
        "isPro": false,
        "fullname": "Kaiyan Zhang",
        "user": "iseesaw",
        "type": "user"
      },
      "summary": "Cet article explore l'apprentissage par renforcement (RL) dans les modèles de langage grands (LLMs) pour des tâches d'inférence sur des données non étiquetées. Le problème central est que dans cette configuration, on ne peut accéder à l'information des valeurs qui évaluent la récompense dans les états où la récompense est possible. Cette situation semble simple, mais elle est courante dans la pratique de l'échelle temporelle de test (TTS), ce qui génère des résultats surprenants. Dans cette étude, on présente un nouveau méthode d'apprentissage de LLMs en utilisant RL sans étiquetage, appelé apprentissage par renforcement en temps de test (TTRL). TTRL permet aux modèles de LLMs d'auto-évoluer en utilisant des résultats précédents de modèles pré-entraînés. Les résultats des expériences montrent des améliorations expérimentales dans différentes tâches et modèles. En particulier, lors de l'AIME 2024, le rendement pass@1 de Qwen-2.5-Math-7B avec des données de test non étiquetées a été amélioré d'environ 159%. De plus, TTRL nécessite seulement un support pour la métrique de @N, dépasse les limites de rendement du modèle initial et approche les rendements des modèles entraînés directement avec des données de test et des étiquettes réelles. Les résultats des expériences démontrent l'efficacité générale de TTRL dans différentes tâches et ses possibilités dans une large gamme de tâches et de domaines. GitHub : https://github.com/PRIME-RL/TTRL",
      "upvotes": 44,
      "discussionId": "6808558b07e80b69b2e351f3",
      "ai_keywords": [
        "Reinforcement Learning (RL)",
        "Large Language Models (LLMs)",
        "reward estimation",
        "Test-Time Scaling (TTS)",
        "majority voting",
        "Test-Time Reinforcement Learning (TTRL)",
        "self-evolution",
        "pre-trained models",
        "pass@1",
        "Qwen-2.5-Math-7B",
        "AIME 2024",
        "Maj@N metric"
      ]
    },
    "publishedAt": "2025-04-22T13:59:56.000Z",
    "title": "TTRL: Test-Time Reinforcement Learning",
    "summary": "This paper investigates Reinforcement Learning (RL) on data without explicit\nlabels for reasoning tasks in Large Language Models (LLMs). The core challenge\nof the problem is reward estimation during inference while not having access to\nground-truth information. While this setting appears elusive, we find that\ncommon practices in Test-Time Scaling (TTS), such as majority voting, yield\nsurprisingly effective rewards suitable for driving RL training. In this work,\nwe introduce Test-Time Reinforcement Learning (TTRL), a novel method for\ntraining LLMs using RL on unlabeled data. TTRL enables self-evolution of LLMs\nby utilizing the priors in the pre-trained models. Our experiments demonstrate\nthat TTRL consistently improves performance across a variety of tasks and\nmodels. Notably, TTRL boosts the pass@1 performance of Qwen-2.5-Math-7B by\napproximately 159% on the AIME 2024 with only unlabeled test data. Furthermore,\nalthough TTRL is only supervised by the Maj@N metric, TTRL has demonstrated\nperformance to consistently surpass the upper limit of the initial model, and\napproach the performance of models trained directly on test data with\nground-truth labels. Our experimental findings validate the general\neffectiveness of TTRL across various tasks, and highlight TTRL's potential for\nbroader tasks and domains. GitHub: https://github.com/PRIME-RL/TTRL",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16084.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60bc94cd85a3ab33829b6211",
      "avatarUrl": "/avatars/b57d36c7577fbbb42ea5b963eef4144a.svg",
      "fullname": "Kaiyan Zhang",
      "name": "iseesaw",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.15521",
      "authors": [
        {
          "_id": "6808458f07e80b69b2df2440",
          "name": "Minghao Wu",
          "hidden": false
        },
        {
          "_id": "6808458f07e80b69b2df2441",
          "name": "Weixuan Wang",
          "hidden": false
        },
        {
          "_id": "6808458f07e80b69b2df2442",
          "name": "Sinuo Liu",
          "hidden": false
        },
        {
          "_id": "6808458f07e80b69b2df2443",
          "name": "Huifeng Yin",
          "hidden": false
        },
        {
          "_id": "6808458f07e80b69b2df2444",
          "name": "Xintong Wang",
          "hidden": false
        },
        {
          "_id": "6808458f07e80b69b2df2445",
          "name": "Yu Zhao",
          "hidden": false
        },
        {
          "_id": "6808458f07e80b69b2df2446",
          "user": {
            "_id": "6527d8b077bceabaab382a75",
            "avatarUrl": "/avatars/69caacf9153dbf6a3796693a968b363f.svg",
            "isPro": false,
            "fullname": "Chenyang Lyu",
            "user": "ChenyangLyu",
            "type": "user"
          },
          "name": "Chenyang Lyu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-23T08:28:16.770Z",
          "hidden": false
        },
        {
          "_id": "6808458f07e80b69b2df2447",
          "name": "Longyue Wang",
          "hidden": false
        },
        {
          "_id": "6808458f07e80b69b2df2448",
          "name": "Weihua Luo",
          "hidden": false
        },
        {
          "_id": "6808458f07e80b69b2df2449",
          "name": "Kaifu Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-22T01:47:37.000Z",
      "submittedOnDailyAt": "2025-04-23T00:13:52.385Z",
      "title": "La Leçon Adorée Apprise à partir de 2,000+ Tests Multilingues",
      "submittedOnDailyBy": {
        "_id": "62d4bf8c97ab9eb08762a975",
        "avatarUrl": "/avatars/73c6228e317cf37b4e3c3e7a4b3d8ae8.svg",
        "isPro": false,
        "fullname": "Minghao Wu",
        "user": "minghaowu",
        "type": "user"
      },
      "summary": "L'évolution constante des capacités linguistiques des modèles de langage grands (LLMs) a conduit à ce que l'évaluation multilingue devienne un facteur crucial pour promouvoir l'équité de la technologie. Dans cet article, plus de 2 000 indicateurs de performance multilingue (dans des langues différentes de l'anglais) publiés de 2021 à 2024 sont évalués, et l'utilité de ces indicateurs passés, actuels et futurs est analysée. Notre recherche révèle que, malgré des investissements de centaines de mille dollars, l'anglais a montré une représentation excessive dans ces indicateurs. De plus, la majorité des indicateurs ne sont pas des traductions, mais des contenus originaux dans leurs langues respectives, et la plupart des données proviennent de pays riches, comme la Chine, l'Inde, l'Allemagne, le Royaume-Uni et les États-Unis. De plus, il existe des différences claires entre le rendement des indicateurs et l'évaluation humaine. Les tâches liées aux STEM montrent une forte corrélation avec l'évaluation humaine (0,70 à 0,85), tandis que les tâches traditionnelles de l'NLP, comme les systèmes de questions et réponses (par exemple, XQuAD), montrent une corrélation plus faible (0,11 à 0,30). De plus, traduire les indicateurs anglais vers d'autres langues n'est pas suffisant. Les indicateurs régionaux montrent un haut degré de concordance avec la perception humaine (0,68), ce qui est supérieur aux indicateurs traduits (0,47). Cela souligne l'importance des indicateurs qui s'alignent avec la culture et la langue, et qui ne se basent pas uniquement sur la traduction. Grâce à ces analyses détaillées, six limitations importantes dans la pratique actuelle de l'évaluation multilingue sont identifiées, et des directives efficaces pour les indicateurs multilingues sont proposées, ainsi que cinq directions de recherche importantes. Enfin, on invite à la coopération internationale pour le développement d'indicateurs de performance réalistes et en accord avec la perception humaine.",
      "upvotes": 41,
      "discussionId": "6808459007e80b69b2df249e",
      "ai_keywords": [
        "multilingual large language models (LLMs)",
        "multilingual benchmarks",
        "benchmark performance",
        "human judgments",
        "STEM-related tasks",
        "question answering (e.g., XQuAD)",
        "culturally and linguistically tailored benchmarks",
        "human-aligned benchmarks",
        "real-world applications"
      ]
    },
    "publishedAt": "2025-04-21T21:47:37.000Z",
    "title": "The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks",
    "summary": "As large language models (LLMs) continue to advance in linguistic\ncapabilities, robust multilingual evaluation has become essential for promoting\nequitable technological progress. This position paper examines over 2,000\nmultilingual (non-English) benchmarks from 148 countries, published between\n2021 and 2024, to evaluate past, present, and future practices in multilingual\nbenchmarking. Our findings reveal that, despite significant investments\namounting to tens of millions of dollars, English remains significantly\noverrepresented in these benchmarks. Additionally, most benchmarks rely on\noriginal language content rather than translations, with the majority sourced\nfrom high-resource countries such as China, India, Germany, the UK, and the\nUSA. Furthermore, a comparison of benchmark performance with human judgments\nhighlights notable disparities. STEM-related tasks exhibit strong correlations\nwith human evaluations (0.70 to 0.85), while traditional NLP tasks like\nquestion answering (e.g., XQuAD) show much weaker correlations (0.11 to 0.30).\nMoreover, translating English benchmarks into other languages proves\ninsufficient, as localized benchmarks demonstrate significantly higher\nalignment with local human judgments (0.68) than their translated counterparts\n(0.47). This underscores the importance of creating culturally and\nlinguistically tailored benchmarks rather than relying solely on translations.\nThrough this comprehensive analysis, we highlight six key limitations in\ncurrent multilingual evaluation practices, propose the guiding principles\naccordingly for effective multilingual benchmarking, and outline five critical\nresearch directions to drive progress in the field. Finally, we call for a\nglobal collaborative effort to develop human-aligned benchmarks that prioritize\nreal-world applications.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15521.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62d4bf8c97ab9eb08762a975",
      "avatarUrl": "/avatars/73c6228e317cf37b4e3c3e7a4b3d8ae8.svg",
      "fullname": "Minghao Wu",
      "name": "minghaowu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 15
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.16072",
      "authors": [
        {
          "_id": "6808467a867c3ef14f8326ce",
          "user": {
            "_id": "63797c273f575acc2f6893c0",
            "avatarUrl": "/avatars/32d7a6a8881c8c4d80a097b732ed24b6.svg",
            "isPro": true,
            "fullname": "Long(Tony) Lian",
            "user": "longlian",
            "type": "user"
          },
          "name": "Long Lian",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-23T08:28:14.686Z",
          "hidden": false
        },
        {
          "_id": "6808467a867c3ef14f8326cf",
          "name": "Yifan Ding",
          "hidden": false
        },
        {
          "_id": "6808467a867c3ef14f8326d0",
          "name": "Yunhao Ge",
          "hidden": false
        },
        {
          "_id": "6808467a867c3ef14f8326d1",
          "name": "Sifei Liu",
          "hidden": false
        },
        {
          "_id": "6808467a867c3ef14f8326d2",
          "name": "Hanzi Mao",
          "hidden": false
        },
        {
          "_id": "6808467a867c3ef14f8326d3",
          "user": {
            "_id": "620dd3888528f797e88cb9b5",
            "avatarUrl": "/avatars/af04728788d78fe7d6375e19e32a535e.svg",
            "isPro": false,
            "fullname": "Boyi Li",
            "user": "Boyiliee",
            "type": "user"
          },
          "name": "Boyi Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-23T08:28:09.738Z",
          "hidden": false
        },
        {
          "_id": "6808467a867c3ef14f8326d4",
          "name": "Marco Pavone",
          "hidden": false
        },
        {
          "_id": "6808467a867c3ef14f8326d5",
          "name": "Ming-Yu Liu",
          "hidden": false
        },
        {
          "_id": "6808467a867c3ef14f8326d6",
          "name": "Trevor Darrell",
          "hidden": false
        },
        {
          "_id": "6808467a867c3ef14f8326d7",
          "user": {
            "_id": "6333a9195a032dcd095dda13",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1664329996201-noauth.jpeg",
            "isPro": false,
            "fullname": "Adam Yala",
            "user": "yala",
            "type": "user"
          },
          "name": "Adam Yala",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-23T08:28:12.415Z",
          "hidden": false
        },
        {
          "_id": "6808467a867c3ef14f8326d8",
          "user": {
            "_id": "649f05367b57fab3a5b27c8b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649f05367b57fab3a5b27c8b/UDJB4yqF2NmaRwCyTOfcl.jpeg",
            "isPro": true,
            "fullname": "Yin Cui",
            "user": "richardaecn",
            "type": "user"
          },
          "name": "Yin Cui",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-23T08:28:06.739Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63797c273f575acc2f6893c0/37vXp0iDKwhBbVyNnCGHy.qt"
      ],
      "publishedAt": "2025-04-22T17:51:41.000Z",
      "submittedOnDailyAt": "2025-04-23T00:22:38.011Z",
      "title": "Détails des captures d'images et vidéos par zones géographiques.",
      "submittedOnDailyBy": {
        "_id": "63797c273f575acc2f6893c0",
        "avatarUrl": "/avatars/32d7a6a8881c8c4d80a097b732ed24b6.svg",
        "isPro": true,
        "fullname": "Long(Tony) Lian",
        "user": "longlian",
        "type": "user"
      },
      "summary": "Générer une description détaillée et précise sur une zone spécifique d'une image ou vidéo est un problème de base des modèles de langue visuolinguistique. Dans ce travail, nous présentons un modèle dont l'objectif est de créer une description concrète. Le Modèle Describe Anything (DAM) a introduit deux éléments innovants pour maintenir à la fois l'information locale et le contexte global. Ceci est réalisé grâce à un prompt de focalisation qui assure une codification à haute résolution dans la zone cible et un rétro-tracking visuel local qui intègre la description locale avec le contexte global. Pour aborder la rareté des données de haute qualité de DLC, nous proposons une pipeline d'apprentissage semi-supervisé (SSL) basée sur les données, appelée DLC-SDP. DLC-SDP commence avec des ensembles de données de segmentation existantes et étend les données aux images web non étiquetées en utilisant SSL. Nous présentons le DLC-Bench, un cadre d'évaluation conçu pour évaluer le DLC sans dépendre des captures de référence. Le DAM a atteint un nouveau record sur 7 cadres d'évaluation, qui comprennent des niveaux de mots clés, des niveaux de phrases et des niveaux de descriptions spécifiques pour des images et des vidéos locales.",
      "upvotes": 28,
      "discussionId": "6808467e867c3ef14f832831",
      "projectPage": "https://describe-anything.github.io",
      "githubRepo": "https://github.com/NVlabs/describe-anything",
      "ai_keywords": [
        "focal prompt",
        "localized vision backbone",
        "Semi-supervised learning (SSL)-based Data Pipeline (DLC-SDP)",
        "segmentation datasets",
        "DLC-Bench",
        "keyword-level",
        "phrase-level",
        "detailed multi-sentence localized image and video captioning"
      ]
    },
    "publishedAt": "2025-04-22T13:51:41.000Z",
    "title": "Describe Anything: Detailed Localized Image and Video Captioning",
    "summary": "Generating detailed and accurate descriptions for specific regions in images\nand videos remains a fundamental challenge for vision-language models. We\nintroduce the Describe Anything Model (DAM), a model designed for detailed\nlocalized captioning (DLC). DAM preserves both local details and global context\nthrough two key innovations: a focal prompt, which ensures high-resolution\nencoding of targeted regions, and a localized vision backbone, which integrates\nprecise localization with its broader context. To tackle the scarcity of\nhigh-quality DLC data, we propose a Semi-supervised learning (SSL)-based Data\nPipeline (DLC-SDP). DLC-SDP starts with existing segmentation datasets and\nexpands to unlabeled web images using SSL. We introduce DLC-Bench, a benchmark\ndesigned to evaluate DLC without relying on reference captions. DAM sets new\nstate-of-the-art on 7 benchmarks spanning keyword-level, phrase-level, and\ndetailed multi-sentence localized image and video captioning.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63797c273f575acc2f6893c0/37vXp0iDKwhBbVyNnCGHy.qt"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16072.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "63797c273f575acc2f6893c0",
      "avatarUrl": "/avatars/32d7a6a8881c8c4d80a097b732ed24b6.svg",
      "fullname": "Long(Tony) Lian",
      "name": "longlian",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.15466",
      "authors": [
        {
          "_id": "6808480c49c8f78b6a4e492f",
          "name": "Jiayi Pan",
          "hidden": false
        },
        {
          "_id": "6808480c49c8f78b6a4e4930",
          "user": {
            "_id": "644570ba2d91b15b4c7f6311",
            "avatarUrl": "/avatars/d5e66012066d0c330b8f23718b1499d8.svg",
            "isPro": false,
            "fullname": "Xiuyu Li",
            "user": "xiuyul",
            "type": "user"
          },
          "name": "Xiuyu Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-23T08:27:59.248Z",
          "hidden": false
        },
        {
          "_id": "6808480c49c8f78b6a4e4931",
          "name": "Long Lian",
          "hidden": false
        },
        {
          "_id": "6808480c49c8f78b6a4e4932",
          "name": "Charlie Snell",
          "hidden": false
        },
        {
          "_id": "6808480c49c8f78b6a4e4933",
          "name": "Yifei Zhou",
          "hidden": false
        },
        {
          "_id": "6808480c49c8f78b6a4e4934",
          "user": {
            "_id": "6333a9195a032dcd095dda13",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1664329996201-noauth.jpeg",
            "isPro": false,
            "fullname": "Adam Yala",
            "user": "yala",
            "type": "user"
          },
          "name": "Adam Yala",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-23T08:28:02.029Z",
          "hidden": false
        },
        {
          "_id": "6808480c49c8f78b6a4e4935",
          "name": "Trevor Darrell",
          "hidden": false
        },
        {
          "_id": "6808480c49c8f78b6a4e4936",
          "name": "Kurt Keutzer",
          "hidden": false
        },
        {
          "_id": "6808480c49c8f78b6a4e4937",
          "name": "Alane Suhr",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-21T22:29:02.000Z",
      "submittedOnDailyAt": "2025-04-23T00:30:52.876Z",
      "title": "Apprentissage de la logique parallèle",
      "submittedOnDailyBy": {
        "_id": "63797c273f575acc2f6893c0",
        "avatarUrl": "/avatars/32d7a6a8881c8c4d80a097b732ed24b6.svg",
        "isPro": true,
        "fullname": "Long(Tony) Lian",
        "user": "longlian",
        "type": "user"
      },
      "summary": "Le calcul du temps d'inférence en échelle a entraîné un grand accroissement de la capacité logique des modèles de langage. Cependant, les méthodes actuelles ont des limitations importantes : l'approche de Consistence de Chaîne génère des sorties trop longues et les problèmes qui surgissent de la Latence et du Context Window. D'autre part, les méthodes dépendant de l'auto-cohérence comme les types parallèles souvent négligent des problèmes tels que des calculs prolongés et des limites de performance dues à une collaboration insuffisante. Pour résoudre ces points, nous proposons un nouveau cadre logique qui intègre les calculs parallèles et de chaîne dans un calcul end-to-end. APR généralise les modèles logiques actuels et permet des calculs adaptatifs en utilisant des opérations comme SPaN() et JOIN(). L'innovation clé est l'optimisation du temps d'inférence des parents et des enfants dès le début par une stratégie d'apprentissage par renforcement, ce qui améliore la taux de succès sans nécessiter de définir des structures d'inférence préalables. Dans les expériences avec la tâche de Compte-à-Cout, la grande avantage de APR a été démontré : (1) un haut rendement dans le même Context Window (83.4% vs. 60.0% à 4k context) ; (2) une flexibilité en échelle pour de plus grandes quantités de tokens (80.1% vs. 66.6% à 20k tokens totaux) ; (3) une amélioration de la précision dans la même Latence (75.2% vs. 57.3% à environ 5 000ms). APR permet aux modèles de langage d'optimiser les processus logiques grâce à une division automatique adaptative des calculs.",
      "upvotes": 27,
      "discussionId": "6808480c49c8f78b6a4e4968",
      "githubRepo": "https://github.com/Parallel-Reasoning/APR",
      "ai_keywords": [
        "Adaptive Parallel Reasoning (APR)",
        "serialized chain-of-thought approaches",
        "parallel methods",
        "self-consistency",
        "adaptive multi-threaded inference",
        "spawn()",
        "join()",
        "reinforcement learning strategy",
        "parent inference threads",
        "child inference threads",
        "Countdown reasoning task",
        "context window",
        "scalability",
        "total tokens",
        "reasoning processes",
        "adaptive allocation of computation"
      ]
    },
    "publishedAt": "2025-04-21T18:29:02.000Z",
    "title": "Learning Adaptive Parallel Reasoning with Language Models",
    "summary": "Scaling inference-time computation has substantially improved the reasoning\ncapabilities of language models. However, existing methods have significant\nlimitations: serialized chain-of-thought approaches generate overly long\noutputs, leading to increased latency and exhausted context windows, while\nparallel methods such as self-consistency suffer from insufficient\ncoordination, resulting in redundant computations and limited performance\ngains. To address these shortcomings, we propose Adaptive Parallel Reasoning\n(APR), a novel reasoning framework that enables language models to orchestrate\nboth serialized and parallel computations end-to-end. APR generalizes existing\nreasoning methods by enabling adaptive multi-threaded inference using spawn()\nand join() operations. A key innovation is our end-to-end reinforcement\nlearning strategy, optimizing both parent and child inference threads to\nenhance task success rate without requiring predefined reasoning structures.\nExperiments on the Countdown reasoning task demonstrate significant benefits of\nAPR: (1) higher performance within the same context window (83.4% vs. 60.0% at\n4k context); (2) superior scalability with increased computation (80.1% vs.\n66.6% at 20k total tokens); (3) improved accuracy at equivalent latency (75.2%\nvs. 57.3% at approximately 5,000ms). APR represents a step towards enabling\nlanguage models to autonomously optimize their reasoning processes through\nadaptive allocation of computation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15466.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63797c273f575acc2f6893c0",
      "avatarUrl": "/avatars/32d7a6a8881c8c4d80a097b732ed24b6.svg",
      "fullname": "Long(Tony) Lian",
      "name": "longlian",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.15415",
      "authors": [
        {
          "_id": "68084b04ba1dd0e6a077e09f",
          "user": {
            "_id": "64c910233d5a0dfed5ce5abb",
            "avatarUrl": "/avatars/8c73f380219c05ae7e7c2fad75a570d8.svg",
            "isPro": false,
            "fullname": "dma",
            "user": "mdh98",
            "type": "user"
          },
          "name": "David Ma",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-23T08:27:56.437Z",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0a0",
          "name": "Yuanxing Zhang",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0a1",
          "user": {
            "_id": "6704ee27386892c420db1938",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6704ee27386892c420db1938/lb5mtEwYhn47RawkynYPs.jpeg",
            "isPro": false,
            "fullname": "JinCheng Ren",
            "user": "JinChengRen",
            "type": "user"
          },
          "name": "Jincheng Ren",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-23T08:36:46.274Z",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0a2",
          "name": "Jarvis Guo",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0a3",
          "name": "Yifan Yao",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0a4",
          "name": "Zhenlin Wei",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0a5",
          "name": "Zhenzhu Yang",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0a6",
          "name": "Zhongyuan Peng",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0a7",
          "name": "Boyu Feng",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0a8",
          "name": "Jun Ma",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0a9",
          "name": "Xiao Gu",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0aa",
          "name": "Zhoufutu Wen",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0ab",
          "name": "King Zhu",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0ac",
          "name": "Yancheng He",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0ad",
          "name": "Meng Cao",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0ae",
          "name": "Shiwen Ni",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0af",
          "name": "Jiaheng Liu",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0b0",
          "name": "Wenhao Huang",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0b1",
          "name": "Ge Zhang",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0b2",
          "name": "Xiaojie Jin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-21T19:53:44.000Z",
      "submittedOnDailyAt": "2025-04-23T00:59:32.168Z",
      "title": "IV-Bench : Reconnaissance vidéo basée sur les images et benchmark basé sur la logique",
      "submittedOnDailyBy": {
        "_id": "638efcf4c67af472d316d424",
        "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
        "isPro": false,
        "fullname": "Ge Zhang",
        "user": "zhangysk",
        "type": "user"
      },
      "summary": "Les actuales cadres d'évaluation des Grands Modèles de Langue Multimodal (MLLMs) se concentrent principalement sur l'interprétation d'images et sur des tâches générales de compréhension des vidéos, où le rôle du contexte des images est crucial pour la compréhension des vidéos. Pour aborder cette limitation, on propose IV-Bench, le premier cadre d'évaluation intégral. IV-Bench comprend 967 vidéos et 2,585 requêtes d'images-texte annotées avec précision, et se compose de 13 tâches (7 de pose et 6 d'interprétation) et de 5 catégories représentatives. Dans des évaluations détaillées des MLLMs les plus récents (par exemple, InternVL2.5, Qwen2.5-VL, GPT-4o, Gemini2-Flash, Gemini2-Pro), les modèles actuels montrent une perte de performance élevée dans la compréhension des vidéos basée sur le contexte des images, atteignant un maximum de 28,9% de précision. Une analyse supplémentaire sur IV-Bench révèle des facteurs qui affectent le rendement des modèles, incluant des modèles d'inférence, le nombre de frames et la résolution, et montre comment ces facteurs influencent. De plus, un approche simple de synthèse de données montre que les problèmes de IV-Bench ne se résument pas uniquement à la correspondance des formats de données lors du processus d'entraînement, mais aussi à un éventail plus large de problèmes. Ces résultats fournissent des conseils précieux pour futures recherches. Le code et les données sont disponibles sur https://github.com/multimodal-art-projection/IV-Bench.",
      "upvotes": 15,
      "discussionId": "68084b0bba1dd0e6a077e279",
      "githubRepo": "https://github.com/multimodal-art-projection/IV-Bench",
      "ai_keywords": [
        "Image-Grounded Video Perception and Reasoning",
        "IV-Bench",
        "image-text queries",
        "frame number",
        "resolution"
      ]
    },
    "publishedAt": "2025-04-21T15:53:44.000Z",
    "title": "IV-Bench: A Benchmark for Image-Grounded Video Perception and Reasoning\n  in Multimodal LLMs",
    "summary": "Existing evaluation frameworks for Multimodal Large Language Models (MLLMs)\nprimarily focus on image reasoning or general video understanding tasks,\nlargely overlooking the significant role of image context in video\ncomprehension. To bridge this gap, we propose IV-Bench, the first comprehensive\nbenchmark for evaluating Image-Grounded Video Perception and Reasoning.\nIV-Bench consists of 967 videos paired with 2,585 meticulously annotated\nimage-text queries across 13 tasks (7 perception and 6 reasoning tasks) and 5\nrepresentative categories. Extensive evaluations of state-of-the-art\nopen-source (e.g., InternVL2.5, Qwen2.5-VL) and closed-source (e.g., GPT-4o,\nGemini2-Flash and Gemini2-Pro) MLLMs demonstrate that current models\nsubstantially underperform in image-grounded video Perception and Reasoning,\nmerely achieving at most 28.9% accuracy. Further analysis reveals key factors\ninfluencing model performance on IV-Bench, including inference pattern, frame\nnumber, and resolution. Additionally, through a simple data synthesis approach,\nwe demonstratethe challenges of IV- Bench extend beyond merely aligning the\ndata format in the training proecss. These findings collectively provide\nvaluable insights for future research. Our codes and data are released in\nhttps://github.com/multimodal-art-projection/IV-Bench.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15415.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "638efcf4c67af472d316d424",
      "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
      "fullname": "Ge Zhang",
      "name": "zhangysk",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 46
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.14538",
      "authors": [
        {
          "_id": "680863ed3767f6ed7c969fbf",
          "name": "Yiting Ran",
          "hidden": false
        },
        {
          "_id": "680863ed3767f6ed7c969fc0",
          "name": "Xintao Wang",
          "hidden": false
        },
        {
          "_id": "680863ed3767f6ed7c969fc1",
          "name": "Tian Qiu",
          "hidden": false
        },
        {
          "_id": "680863ed3767f6ed7c969fc2",
          "name": "Jiaqing Liang",
          "hidden": false
        },
        {
          "_id": "680863ed3767f6ed7c969fc3",
          "name": "Yanghua Xiao",
          "hidden": false
        },
        {
          "_id": "680863ed3767f6ed7c969fc4",
          "name": "Deqing Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-20T08:56:27.000Z",
      "submittedOnDailyAt": "2025-04-23T02:23:09.187Z",
      "title": "Nous invitons le Groupe des Effets Interactifs qui se concentre sur la création d'histoires créatives dans un livre.",
      "submittedOnDailyBy": {
        "_id": "64c7bf2c4524c2aea7eac0b3",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c7bf2c4524c2aea7eac0b3/5ocZ69MvN4RFv86Aa7ks3.png",
        "isPro": false,
        "fullname": "Xintao Wang",
        "user": "Neph0s",
        "type": "user"
      },
      "summary": "Récemment, le développement de grands modèles de langue (LLMs) a permis la simulation avec des systèmes de multiples agents. Les tentatives précédentes ont créé une sociabilité d'agents assignant de nouvelles fonctions, mais la simulation de mondes virtuels et de personnages déjà établis, bien qu'elle ait un valeur pratique, n'a pas été largement étudiée. Dans cet article, nous présentons le système \"BookWorld\", qui intègre la sociabilité basique de multiples agents et des fonctions pour la simulation. Le design de \"BookWorld\" inclut la complexité de la réalité, comme des personnages dynamiques, un monde virtuel, des restrictions géographiques et des changements. \"BookWorld\" offre des applications variées, comme la génération d'histoires, des jeux interactifs et des simulations sociales, en élargissant l'amour pour des œuvres virtuelles et en offrant de nouvelles façons d'explorer. A travers des expériences étendues, \"BookWorld\" a généré des simulations de haute qualité qui sont alignées avec les livres originaux et dépassent les méthodes précédentes (Taux de succès : 75,36%). Le code de l'article est disponible sur la page du projet : https://bookworld2025.github.io/",
      "upvotes": 13,
      "discussionId": "680863ef3767f6ed7c96a026",
      "ai_keywords": [
        "large language models (LLMs)",
        "social simulation",
        "multi-agent systems",
        "agent societies",
        "personas",
        "book-based",
        "comprehensive real-world intricacies",
        "diverse and dynamic characters",
        "fictional worldviews",
        "geographical constraints",
        "story generation",
        "interactive games",
        "creative, high-quality stories",
        "fidelity to the source books"
      ]
    },
    "publishedAt": "2025-04-20T04:56:27.000Z",
    "title": "BookWorld: From Novels to Interactive Agent Societies for Creative Story\n  Generation",
    "summary": "Recent advances in large language models (LLMs) have enabled social\nsimulation through multi-agent systems. Prior efforts focus on agent societies\ncreated from scratch, assigning agents with newly defined personas. However,\nsimulating established fictional worlds and characters remain largely\nunderexplored, despite its significant practical value. In this paper, we\nintroduce BookWorld, a comprehensive system for constructing and simulating\nbook-based multi-agent societies. BookWorld's design covers comprehensive\nreal-world intricacies, including diverse and dynamic characters, fictional\nworldviews, geographical constraints and changes, e.t.c. BookWorld enables\ndiverse applications including story generation, interactive games and social\nsimulation, offering novel ways to extend and explore beloved fictional works.\nThrough extensive experiments, we demonstrate that BookWorld generates\ncreative, high-quality stories while maintaining fidelity to the source books,\nsurpassing previous methods with a win rate of 75.36%. The code of this paper\ncan be found at the project page: https://bookworld2025.github.io/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.14538.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64c7bf2c4524c2aea7eac0b3",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c7bf2c4524c2aea7eac0b3/5ocZ69MvN4RFv86Aa7ks3.png",
      "fullname": "Xintao Wang",
      "name": "Neph0s",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.14992",
      "authors": [
        {
          "_id": "68074ed102571b837f03463c",
          "user": {
            "_id": "64722a616facfb01d8ae8349",
            "avatarUrl": "/avatars/1dce23ae5ebd9996770cf5efe910b857.svg",
            "isPro": false,
            "fullname": "Wu Bohong",
            "user": "bongbohong",
            "type": "user"
          },
          "name": "Bohong Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-22T09:50:15.811Z",
          "hidden": false
        },
        {
          "_id": "68074ed102571b837f03463d",
          "name": "Shen Yan",
          "hidden": false
        },
        {
          "_id": "68074ed102571b837f03463e",
          "name": "Sijun Zhang",
          "hidden": false
        },
        {
          "_id": "68074ed102571b837f03463f",
          "name": "Jianqiao Lu",
          "hidden": false
        },
        {
          "_id": "68074ed102571b837f034640",
          "user": {
            "_id": "6371128eafbe42caa5a5222b",
            "avatarUrl": "/avatars/c3b2ab35949c38aa3dfb2657a1300aac.svg",
            "isPro": false,
            "fullname": "Yutao Zeng",
            "user": "Taoer",
            "type": "user"
          },
          "name": "Yutao Zeng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-22T09:50:18.659Z",
          "hidden": false
        },
        {
          "_id": "68074ed102571b837f034641",
          "name": "Ya Wang",
          "hidden": false
        },
        {
          "_id": "68074ed102571b837f034642",
          "name": "Xun Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-21T09:41:26.000Z",
      "submittedOnDailyAt": "2025-04-23T00:38:26.026Z",
      "title": "Efficient Pretraining Length Scaling",
      "submittedOnDailyBy": {
        "_id": "64722a616facfb01d8ae8349",
        "avatarUrl": "/avatars/1dce23ae5ebd9996770cf5efe910b857.svg",
        "isPro": false,
        "fullname": "Wu Bohong",
        "user": "bongbohong",
        "type": "user"
      },
      "summary": "Le développement récent des modèles de langue de grande échelle a montré l'effet de l'échelle de longueur après l'entraînement, mais la possibilité d'une échelle de longueur avant l'entraînement a encore été peu explorée. Nous proposons un nouveau cadre de travail qui permet d'écaler efficacement la longueur avant l'entraînement. Il est connu sous le nom de PHD-Transformer, et maintient l'efficacité de l'inférence tout en permettant l'échelle de longueur avant l'entraînement. PHD-Transformer introduit une stratégie innovante de gestion de la cache KV et distingue les tokens d'entrée originales des tokens de décodification cachées. Pour maintenir la dépendance à longue distance, il maintient le seul cache KV des tokens originaux et supprime immédiatement les tokens de décodification cachées après leur utilisation, ainsi que la taille du PHD-Transformer est la même que celle du Transformer BERNEE FORM. De plus, il introduit deux versions optimisées pour améliorer le rendement : PHD-SWA utilise l'attention en fenêtre pour maintenir la dépendance locale, et PHD-CSWA met en œuvre l'attention en fenêtre de bloc pour éliminer le croissance linéaire dans le temps de préfiltrage. Les expériences détaillées montrent des améliorations constantes sur plusieurs benchmarks.",
      "upvotes": 12,
      "discussionId": "68074ed202571b837f03468b",
      "ai_keywords": [
        "KV cache",
        "original tokens",
        "hidden decoding tokens",
        "long-range dependencies",
        "local dependencies",
        "sliding window attention",
        "chunk-wise sliding window attention"
      ]
    },
    "publishedAt": "2025-04-21T05:41:26.000Z",
    "title": "Efficient Pretraining Length Scaling",
    "summary": "Recent advances in large language models have demonstrated the effectiveness\nof length scaling during post-training, yet its potential in pre-training\nremains underexplored. We present the Parallel Hidden Decoding Transformer\n(PHD-Transformer), a novel framework that enables efficient length\nscaling during pre-training while maintaining inference efficiency.\nPHD-Transformer achieves this through an innovative KV cache\nmanagement strategy that distinguishes between original tokens and hidden\ndecoding tokens. By retaining only the KV cache of original tokens for\nlong-range dependencies while immediately discarding hidden decoding tokens\nafter use, our approach maintains the same KV cache size as the vanilla\ntransformer while enabling effective length scaling. To further enhance\nperformance, we introduce two optimized variants: PHD-SWA employs\nsliding window attention to preserve local dependencies, while\nPHD-CSWA implements chunk-wise sliding window attention to eliminate\nlinear growth in pre-filling time. Extensive experiments demonstrate consistent\nimprovements across multiple benchmarks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.14992.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64722a616facfb01d8ae8349",
      "avatarUrl": "/avatars/1dce23ae5ebd9996770cf5efe910b857.svg",
      "fullname": "Wu Bohong",
      "name": "bongbohong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.13820",
      "authors": [
        {
          "_id": "6805ab2c40034a5a792a26b2",
          "user": {
            "_id": "63f1d16fbe95ed4c9a9418fe",
            "avatarUrl": "/avatars/a1bdfa97323693808f2f16ec74698ed3.svg",
            "isPro": false,
            "fullname": "Yang Yue",
            "user": "yueyang2000",
            "type": "user"
          },
          "name": "Yang Yue",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-22T09:59:59.547Z",
          "hidden": false
        },
        {
          "_id": "6805ab2c40034a5a792a26b3",
          "name": "Yulin Wang",
          "hidden": false
        },
        {
          "_id": "6805ab2c40034a5a792a26b4",
          "name": "Chenxin Tao",
          "hidden": false
        },
        {
          "_id": "6805ab2c40034a5a792a26b5",
          "name": "Pan Liu",
          "hidden": false
        },
        {
          "_id": "6805ab2c40034a5a792a26b6",
          "name": "Shiji Song",
          "hidden": false
        },
        {
          "_id": "6805ab2c40034a5a792a26b7",
          "name": "Gao Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-18T17:50:43.000Z",
      "submittedOnDailyAt": "2025-04-23T01:09:25.550Z",
      "title": "CheXWorld : Modélisation d'un monde d'images pour la représentation d'images de radiographies - Étude d'apprentissage",
      "submittedOnDailyBy": {
        "_id": "63f1d16fbe95ed4c9a9418fe",
        "avatarUrl": "/avatars/a1bdfa97323693808f2f16ec74698ed3.svg",
        "isPro": false,
        "fullname": "Yang Yue",
        "user": "yueyang2000",
        "type": "user"
      },
      "summary": "L'humain peut enregistrer des connaissances communes, comprendre comment fonctionne le monde et développer un modèle interne du monde pour prédire les résultats de ses actions. Ce concept a apparu dans des recherches précédentes comme un chemin prometteur vers la construction de modules généraux. Par exemple, il a également été observé dans l'apprentissage de représentations visuelles. Dans cet article, on présente le premier essai d'un modèle de monde ajusté automatiquement appelé CheXWorld. En particulier, notre étude a développé un cadre intégré qui modélise simultanément les trois dimensions de connaissances médicales nécessaires pour un radiologue : 1) la structure anatomique locale explique les caractéristiques microscopiques des tissus locaux (par exemple, structure, forme et texture). 2) la structure anatomique locale explique la structure du corps humain (par exemple, la disposition d'organes et de os). 3) le changement d'aire est l'objectif de que CheXWorld modélise le mouvement de différentes zones externes dans les images de radiographies (par exemple, changements de brillance, de contraste et d'exposition). Expérimentalement, un analyse qualitative et quantitative appropriée a été conçue, et il est démontré que CheXWorld capture avec succès ces trois dimensions de connaissance médicale. De plus, les expériences d'apprentissage transferé sur 8 cadres de référence de classification et de segmentation d'images médicales montrent que CheXWorld dépasse significativement les méthodes actuelles d'apprentissage non supervisé et les modèles à grande échelle basés sur des données médicales. Le code et les modèles pré-entraînés sont disponibles sur https://github.com/LeapLabTHU/CheXWorld.",
      "upvotes": 11,
      "discussionId": "6805ab2f40034a5a792a27c8",
      "githubRepo": "https://github.com/LeapLabTHU/CheXWorld",
      "ai_keywords": [
        "CheXWorld",
        "self-supervised world model",
        "radiographic images",
        "local anatomical structures",
        "global anatomical layouts",
        "organs",
        "skeletons",
        "domain variations",
        "medical image classification",
        "medical image segmentation",
        "SSL methods",
        "medical foundation models"
      ]
    },
    "publishedAt": "2025-04-18T13:50:43.000Z",
    "title": "CheXWorld: Exploring Image World Modeling for Radiograph Representation\n  Learning",
    "summary": "Humans can develop internal world models that encode common sense knowledge,\ntelling them how the world works and predicting the consequences of their\nactions. This concept has emerged as a promising direction for establishing\ngeneral-purpose machine-learning models in recent preliminary works, e.g., for\nvisual representation learning. In this paper, we present CheXWorld, the first\neffort towards a self-supervised world model for radiographic images.\nSpecifically, our work develops a unified framework that simultaneously models\nthree aspects of medical knowledge essential for qualified radiologists,\nincluding 1) local anatomical structures describing the fine-grained\ncharacteristics of local tissues (e.g., architectures, shapes, and textures);\n2) global anatomical layouts describing the global organization of the human\nbody (e.g., layouts of organs and skeletons); and 3) domain variations that\nencourage CheXWorld to model the transitions across different appearance\ndomains of radiographs (e.g., varying clarity, contrast, and exposure caused by\ncollecting radiographs from different hospitals, devices, or patients).\nEmpirically, we design tailored qualitative and quantitative analyses,\nrevealing that CheXWorld successfully captures these three dimensions of\nmedical knowledge. Furthermore, transfer learning experiments across eight\nmedical image classification and segmentation benchmarks showcase that\nCheXWorld significantly outperforms existing SSL methods and large-scale\nmedical foundation models. Code & pre-trained models are available at\nhttps://github.com/LeapLabTHU/CheXWorld.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13820.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63f1d16fbe95ed4c9a9418fe",
      "avatarUrl": "/avatars/a1bdfa97323693808f2f16ec74698ed3.svg",
      "fullname": "Yang Yue",
      "name": "yueyang2000",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.16030",
      "authors": [
        {
          "_id": "68084e2c59762f55a5a8b5f3",
          "name": "Joya Chen",
          "hidden": false
        },
        {
          "_id": "68084e2c59762f55a5a8b5f4",
          "name": "Ziyun Zeng",
          "hidden": false
        },
        {
          "_id": "68084e2c59762f55a5a8b5f5",
          "name": "Yiqi Lin",
          "hidden": false
        },
        {
          "_id": "68084e2c59762f55a5a8b5f6",
          "name": "Wei Li",
          "hidden": false
        },
        {
          "_id": "68084e2c59762f55a5a8b5f7",
          "name": "Zejun Ma",
          "hidden": false
        },
        {
          "_id": "68084e2c59762f55a5a8b5f8",
          "name": "Mike Zheng Shou",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/642435a1a3adbc7142c3b0a6/8JExSHYg9ME-w-L_VUt4W.mp4"
      ],
      "publishedAt": "2025-04-22T16:52:09.000Z",
      "submittedOnDailyAt": "2025-04-23T00:56:24.970Z",
      "title": "LiveCC : Apprentissage vidéo avec les modèles de langage utilisant les transcriptions de grandes séances de discussion",
      "submittedOnDailyBy": {
        "_id": "642435a1a3adbc7142c3b0a6",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642435a1a3adbc7142c3b0a6/EBmQ7LnfdTdyuhSUti0-d.png",
        "isPro": true,
        "fullname": "Joya Chen",
        "user": "chenjoya",
        "type": "user"
      },
      "summary": "Récemment, les modèles de langage et vidéo (Video LLMs) sont limités par le coût élevé des annotations humaines ou par l'utilisation d'APIs de modèles propriétaires (comme GPT-4o) pour générer des données d'entraînement. Dans cet article, nous explorons l'entraînement à grande échelle de Video LLMs en utilisant la traduction automatique de la voix en texte (ASR) à faible coût. En particulier, nous proposons un nouvel approche d'entraînement en flux qui croise densement les mots de l'ASR et les cadences vidéo en fonction du temps de l'ASR. Comparée à la recherche précédente sur les représentations visuelles du langage, notre méthode adapte naturellement les caractéristiques du flux de l'ASR et permet au modèle d'apprendre à modéliser le langage visuel avec précision dans le temps. Pour soutenir l'algorithme d'entraînement, nous introduisons une chaîne de traitement de données qui processe des vidéos de YouTube et des sous-titres de commentaires (CC, équivalent à l'ASR), et nous créons le jeu de données Live-CC-5M préalablement enregistré et le jeu de données Live-WhisperX-526K de haute qualité pour l'entraînement de rétroalimentation contrôlée (SFT). En particulier, le modèle préalablement enregistré sans SFT, LiveCC-7B-Base, qui ne dispose que de l'ASR, montre de nouvelles capacités dans les commentaires vidéo en temps réel et présente un rendement compétitif dans l'évaluation globale de la QA vidéo. Pour évaluer ceci, nous avons conçu un nouveau benchmark LiveSports-3K en utilisant LLM-as-a-judge. Les expériences montrent que notre modèle final LiveCC-7B-Instruct dépasse la qualité des commentaires des modèles avancés de 72B (Qwen2.5-VL-72B-Instruct, LLaVA-Video-72B) en mode réel et obtient les meilleurs résultats dans les benchmarks de QA de vidéo comme Video MME et OVOBench, démontrant la capacité généralisée étendue de notre approche. Tous les ressources de l'article sont disponibles sur https://showlab.github.io/livecc.",
      "upvotes": 8,
      "discussionId": "68084e2f59762f55a5a8b721",
      "projectPage": "https://showlab.github.io/livecc/",
      "githubRepo": "https://github.com/showlab/livecc",
      "ai_keywords": [
        "Video LLMs",
        "automatic speech recognition (ASR)",
        "streaming training",
        "timestamps",
        "vision-language representation",
        "temporally-aligned",
        "fine-grained vision-language modeling",
        "data production pipeline",
        "YouTube videos",
        "closed captions (CC)",
        "Live-CC-5M",
        "Live-WhisperX-526K",
        "supervised fine-tuning (SFT)",
        "general video QA",
        "real-time video commentary",
        "LiveSports-3K benchmark",
        "LLM-as-a-judge",
        "LiveCC-7B-Base",
        "LiveCC-7B-Instruct",
        "Qwen2.5-VL-72B-Instruct",
        "LLaVA-Video-72B",
        "VideoMME",
        "OVOBench"
      ]
    },
    "publishedAt": "2025-04-22T12:52:09.000Z",
    "title": "LiveCC: Learning Video LLM with Streaming Speech Transcription at Scale",
    "summary": "Recent video large language models (Video LLMs) often depend on costly human\nannotations or proprietary model APIs (e.g., GPT-4o) to produce training data,\nwhich limits their training at scale. In this paper, we explore large-scale\ntraining for Video LLM with cheap automatic speech recognition (ASR)\ntranscripts. Specifically, we propose a novel streaming training approach that\ndensely interleaves the ASR words and video frames according to their\ntimestamps. Compared to previous studies in vision-language representation with\nASR, our method naturally fits the streaming characteristics of ASR, thus\nenabling the model to learn temporally-aligned, fine-grained vision-language\nmodeling. To support the training algorithm, we introduce a data production\npipeline to process YouTube videos and their closed captions (CC, same as ASR),\nresulting in Live-CC-5M dataset for pre-training and Live-WhisperX-526K dataset\nfor high-quality supervised fine-tuning (SFT). Remarkably, even without SFT,\nthe ASR-only pre-trained LiveCC-7B-Base model demonstrates competitive general\nvideo QA performance and exhibits a new capability in real-time video\ncommentary. To evaluate this, we carefully design a new LiveSports-3K\nbenchmark, using LLM-as-a-judge to measure the free-form commentary.\nExperiments show our final LiveCC-7B-Instruct model can surpass advanced 72B\nmodels (Qwen2.5-VL-72B-Instruct, LLaVA-Video-72B) in commentary quality even\nworking in a real-time mode. Meanwhile, it achieves state-of-the-art results at\nthe 7B/8B scale on popular video QA benchmarks such as VideoMME and OVOBench,\ndemonstrating the broad generalizability of our approach. All resources of this\npaper have been released at https://showlab.github.io/livecc.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/642435a1a3adbc7142c3b0a6/8JExSHYg9ME-w-L_VUt4W.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16030.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642435a1a3adbc7142c3b0a6",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642435a1a3adbc7142c3b0a6/EBmQ7LnfdTdyuhSUti0-d.png",
      "fullname": "Joya Chen",
      "name": "chenjoya",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.15681",
      "authors": [
        {
          "_id": "680846defa5a6cc6bd9d2cf3",
          "name": "Vidi Team",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2cf4",
          "name": "Celong Liu",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2cf5",
          "name": "Chia-Wen Kuo",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2cf6",
          "user": {
            "_id": "6476af4402fc644c810b29a2",
            "avatarUrl": "/avatars/68aefabe6b000443f4601137e6672187.svg",
            "isPro": false,
            "fullname": "Dawei Du",
            "user": "daviddousa",
            "type": "user"
          },
          "name": "Dawei Du",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-23T08:28:04.152Z",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2cf7",
          "name": "Fan Chen",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2cf8",
          "name": "Guang Chen",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2cf9",
          "name": "Jiamin Yuan",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2cfa",
          "name": "Lingxi Zhang",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2cfb",
          "name": "Lu Guo",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2cfc",
          "name": "Lusha Li",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2cfd",
          "name": "Longyin Wen",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2cfe",
          "name": "Qingyu Chen",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2cff",
          "name": "Rachel Deng",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2d00",
          "name": "Sijie Zhu",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2d01",
          "name": "Stuart Siew",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2d02",
          "name": "Tong Jin",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2d03",
          "name": "Wei Lu",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2d04",
          "name": "Wen Zhong",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2d05",
          "name": "Xiaohui Shen",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2d06",
          "name": "Xin Gu",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2d07",
          "name": "Xing Mei",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2d08",
          "name": "Xueqiong Qu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-22T08:04:45.000Z",
      "submittedOnDailyAt": "2025-04-23T00:19:34.185Z",
      "title": "Vidéo : Compréhension et édition de films dans des modèles multi-modèles à grande échelle",
      "submittedOnDailyBy": {
        "_id": "65cbdea6d6c974694f09249a",
        "avatarUrl": "/avatars/a317a1f545117e0699e1c56258980fd8.svg",
        "isPro": false,
        "fullname": "Jay",
        "user": "Zilence006",
        "type": "user"
      },
      "summary": "La humanité établit des relations proches avec la nature avec ceux qui la connaissent et partagent des informations et des images, qui sont les principaux moyens de communication et d'expression sur Internet. Afin de soutenir la production de contenu de haute qualité en images à grande échelle, les systèmes de pipeline actuels nécessitent une compréhension détaillée des données vivantes (par exemple, des images capturées par des caméras sans édition) et des composants d'édition (par exemple, des effets visuels). Dans le cas d'édition d'images, le modèle doit avoir un fort connaissance, processer des longueurs d'entrée flexibles (par exemple, des images d'une heure de vie) et cela représente un grand défi par rapport aux modèles traditionnels. Dans ce rapport, nous présentons la famille de modèles de grande diversité (LMMs) appelés Vidi, conçus pour soutenir une compréhension et une édition large de l'image. La première version de Vidi se concentre sur la recherche temporelle (temporal retrieval), c'est-à-dire, sur la détermination du temps d'une image d'entrée qui correspond à une requête textuelle, ce qui est essentiel pour réaliser des éditions intelligentes. Ce modèle a une forte capacité pour comprendre le temps et peut rechercher des temps spécifiques pour une requête. Pour soutenir une évaluation détaillée dans les scans réels, nous présentons le benchmark VUE-TR, qui comporte cinq améliorations : 1) longueur de l'image : un ensemble de données de recherche temporelle plus long, 2) support vocal : requêtes basées sur le vocal, 3) forme de la requête : diverses longueurs et formats de requête, 4) qualité des annotations : les zones de temps sont annotées manuellement, 5) métrique d'évaluation : amélioration du métrique IoU pour évaluer des multiples temps. Surprenant, Vidi dépasse significativement les modèles leaders dans les tâches de recherche temporelle (par exemple, GPT-4o et Gemini) et montre des résultats excellents dans le scan de l'édition d'images.",
      "upvotes": 7,
      "discussionId": "680846dffa5a6cc6bd9d2d59",
      "ai_keywords": [
        "Vidi",
        "Large Multimodal Models (LMMs)",
        "temporal retrieval",
        "video editing scenarios",
        "temporal understanding",
        "VUE-TR benchmark",
        "IoU metric"
      ]
    },
    "publishedAt": "2025-04-22T04:04:45.000Z",
    "title": "Vidi: Large Multimodal Models for Video Understanding and Editing",
    "summary": "Humans naturally share information with those they are connected to, and\nvideo has become one of the dominant mediums for communication and expression\non the Internet. To support the creation of high-quality large-scale video\ncontent, a modern pipeline requires a comprehensive understanding of both the\nraw input materials (e.g., the unedited footage captured by cameras) and the\nediting components (e.g., visual effects). In video editing scenarios, models\nmust process multiple modalities (e.g., vision, audio, text) with strong\nbackground knowledge and handle flexible input lengths (e.g., hour-long raw\nvideos), which poses significant challenges for traditional models. In this\nreport, we introduce Vidi, a family of Large Multimodal Models (LMMs) for a\nwide range of video understand editing scenarios. The first release focuses on\ntemporal retrieval, i.e., identifying the time ranges within the input videos\ncorresponding to a given text query, which plays a critical role in intelligent\nediting. The model is capable of processing hour-long videos with strong\ntemporal understanding capability, e.g., retrieve time ranges for certain\nqueries. To support a comprehensive evaluation in real-world scenarios, we also\npresent the VUE-TR benchmark, which introduces five key advancements. 1) Video\nduration: significantly longer than existing temporal retrival datasets, 2)\nAudio support: includes audio-based queries, 3) Query format: diverse query\nlengths/formats, 4) Annotation quality: ground-truth time ranges are manually\nannotated. 5) Evaluation metric: a refined IoU metric to support evaluation\nover multiple time ranges. Remarkably, Vidi significantly outperforms leading\nproprietary models, e.g., GPT-4o and Gemini, on the temporal retrieval task,\nindicating its superiority in video editing scenarios.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15681.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65cbdea6d6c974694f09249a",
      "avatarUrl": "/avatars/a317a1f545117e0699e1c56258980fd8.svg",
      "fullname": "Jay",
      "name": "Zilence006",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.16080",
      "authors": [
        {
          "_id": "680845d997f32b8ffc13569c",
          "name": "Le Zhuo",
          "hidden": false
        },
        {
          "_id": "680845d997f32b8ffc13569d",
          "name": "Liangbing Zhao",
          "hidden": false
        },
        {
          "_id": "680845d997f32b8ffc13569e",
          "name": "Sayak Paul",
          "hidden": false
        },
        {
          "_id": "680845d997f32b8ffc13569f",
          "name": "Yue Liao",
          "hidden": false
        },
        {
          "_id": "680845d997f32b8ffc1356a0",
          "name": "Renrui Zhang",
          "hidden": false
        },
        {
          "_id": "680845d997f32b8ffc1356a1",
          "name": "Yi Xin",
          "hidden": false
        },
        {
          "_id": "680845d997f32b8ffc1356a2",
          "name": "Peng Gao",
          "hidden": false
        },
        {
          "_id": "680845d997f32b8ffc1356a3",
          "name": "Mohamed Elhoseiny",
          "hidden": false
        },
        {
          "_id": "680845d997f32b8ffc1356a4",
          "name": "Hongsheng Li",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/5f7fbd813e94f16a85448745/L_S4ww0dm1-ZUiNRLQm_h.png"
      ],
      "publishedAt": "2025-04-22T17:58:07.000Z",
      "submittedOnDailyAt": "2025-04-23T00:16:04.186Z",
      "title": "「Optimisation de l'expansion bidirectionnelle : Optimisation de la propagation des modèles d'expansion de texte à images pendant l'inférence」",
      "submittedOnDailyBy": {
        "_id": "5f7fbd813e94f16a85448745",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1649681653581-5f7fbd813e94f16a85448745.jpeg",
        "isPro": false,
        "fullname": "Sayak Paul",
        "user": "sayakpaul",
        "type": "user"
      },
      "summary": "Récemment, les modèles de diffusion d'images ont atteint une qualité visuelle impressionnante grâce à l'expansion des données d'entraînement et des paramètres du modèle, mais rencontrent des difficultés dans les scénarios complexes et les détails minimaux. En se basant sur la capacité de auto-reflexion observée dans les grands modèles de langage, nous proposons la ReflectionFlow. C'est un flux de travail d'inférence qui permet aux modèles de diffusion de réfléchir et d'améliorer leur sortie. La ReflectionFlow introduit trois approches d'expansion lors de l'inférence : (1) l'expansion du niveau de bruit optimise les initialisations possibles ; (2) l'expansion du niveau de prompt fournit une orientation précise ; (3) particulièrement, l'expansion du niveau de réflexion définit clairement les actions permettant de réfléchir et de modifier la génération précédente. Pour encourager l'expansion du niveau de réflexion, nous avons construit le GenRef, qui consiste en un ensemble de données de 1 million de tuples, incluant des images de réflexion, des images avec des défauts et des images d'ouverture. En utilisant cet ensemble de données, nous avons réalisé la réflexion d'inférence appropriée dans le modèle de diffusion transformateur FLUX.1-dev, un des leaders dans le domaine. Les résultats des expériences montrent que la ReflectionFlow dépasse significativement un méthode simple d'expansion du niveau de bruit, offrant une solution efficace et calculable pour la synthèse d'images de haute qualité dans des tâches difficiles.",
      "upvotes": 5,
      "discussionId": "680845de97f32b8ffc1357c7",
      "ai_keywords": [
        "text-to-image diffusion models",
        "visual quality",
        "training data",
        "model parameters",
        "self-reflection capabilities",
        "diffusion models",
        "inference-time framework",
        "noise-level scaling",
        "latent initialization",
        "prompt-level scaling",
        "semantic guidance",
        "reflection-level scaling",
        "GenRef",
        "multimodal inputs",
        "unified framework",
        "image synthesis"
      ]
    },
    "publishedAt": "2025-04-22T13:58:07.000Z",
    "title": "From Reflection to Perfection: Scaling Inference-Time Optimization for\n  Text-to-Image Diffusion Models via Reflection Tuning",
    "summary": "Recent text-to-image diffusion models achieve impressive visual quality\nthrough extensive scaling of training data and model parameters, yet they often\nstruggle with complex scenes and fine-grained details. Inspired by the\nself-reflection capabilities emergent in large language models, we propose\nReflectionFlow, an inference-time framework enabling diffusion models to\niteratively reflect upon and refine their outputs. ReflectionFlow introduces\nthree complementary inference-time scaling axes: (1) noise-level scaling to\noptimize latent initialization; (2) prompt-level scaling for precise semantic\nguidance; and most notably, (3) reflection-level scaling, which explicitly\nprovides actionable reflections to iteratively assess and correct previous\ngenerations. To facilitate reflection-level scaling, we construct GenRef, a\nlarge-scale dataset comprising 1 million triplets, each containing a\nreflection, a flawed image, and an enhanced image. Leveraging this dataset, we\nefficiently perform reflection tuning on state-of-the-art diffusion\ntransformer, FLUX.1-dev, by jointly modeling multimodal inputs within a unified\nframework. Experimental results show that ReflectionFlow significantly\noutperforms naive noise-level scaling methods, offering a scalable and\ncompute-efficient solution toward higher-quality image synthesis on challenging\ntasks.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/5f7fbd813e94f16a85448745/L_S4ww0dm1-ZUiNRLQm_h.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16080.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5f7fbd813e94f16a85448745",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1649681653581-5f7fbd813e94f16a85448745.jpeg",
      "fullname": "Sayak Paul",
      "name": "sayakpaul",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 605
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.16078",
      "authors": [
        {
          "_id": "68087a231e425a6eee93570d",
          "name": "Thomas Schmied",
          "hidden": false
        },
        {
          "_id": "68087a231e425a6eee93570e",
          "name": "Jörg Bornschein",
          "hidden": false
        },
        {
          "_id": "68087a231e425a6eee93570f",
          "name": "Jordi Grau-Moya",
          "hidden": false
        },
        {
          "_id": "68087a231e425a6eee935710",
          "name": "Markus Wulfmeier",
          "hidden": false
        },
        {
          "_id": "68087a231e425a6eee935711",
          "name": "Razvan Pascanu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-22T17:57:14.000Z",
      "submittedOnDailyAt": "2025-04-23T03:57:30.931Z",
      "title": "Les LLMs ont l'effet de l'extrême de l'aide : l'impact sur la décision par l'ajustement micro de la RL.",
      "submittedOnDailyBy": {
        "_id": "64c3849269b1a6796052eac7",
        "avatarUrl": "/avatars/9f0c832d5b51b659c7bb83074f02a648.svg",
        "isPro": false,
        "fullname": "Thomas Schmied",
        "user": "thomasschmied",
        "type": "user"
      },
      "summary": "L'échec des LLM a augmenté rapidement l'intérêt pour des applications basées sur des agents divers. La principale hypothèse est que les LLM peuvent explorer et résoudre des domaines complexes efficacement en utilisant des sentiments communs et l'inférence de la chaîne de pensée (CoT). Cependant, les agents basés sur des LLM rencontrent des défis en raison de la différence entre l'exploration optimale et le savoir nécessaire pour des actions réelles (écart savoir-faire). Dans cette étude, on examine systématiquement les raisons pour lesquelles les LLM ne montrent pas un rendement optimal dans des scénarios de décision. On se concentre particulièrement sur trois façons de faille communes : le manque de souci, le biais de fréquence et l'écart savoir-faire. On propose un méthode d'amélioration en utilisant l'apprentissage par renforcement (RL) basé sur des raisons de CoT auto-générées. Les expériences dans différents domaines (bandits à plusieurs bras, bandits contextuels) et Tic-tac-toe montrent que l'amélioration par RL augmente l'exploration et réduit la distance entre le savoir et les actions réelles, améliorant ainsi la capacité de décision des LLM. Enfin, on combine un épisode vide et le mode d'accès propre du LLM (auto-amélioration, auto-cohérence) pour améliorer encore plus l'amélioration des LLM.",
      "upvotes": 5,
      "discussionId": "68087a241e425a6eee93576b",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "Chain-of-Thought (CoT) reasoning",
        "sub-optimal exploration",
        "knowing-doing gap",
        "decision-making scenarios",
        "greediness",
        "frequency bias",
        "fine-tuning",
        "Reinforcement Learning (RL)",
        "self-generated CoT rationales",
        "multi-armed bandits",
        "contextual bandits",
        "Tic-tac-toe",
        "$\\epsilon$-greedy",
        "self-correction",
        "self-consistency"
      ]
    },
    "publishedAt": "2025-04-22T13:57:14.000Z",
    "title": "LLMs are Greedy Agents: Effects of RL Fine-tuning on Decision-Making\n  Abilities",
    "summary": "The success of Large Language Models (LLMs) has sparked interest in various\nagentic applications. A key hypothesis is that LLMs, leveraging common sense\nand Chain-of-Thought (CoT) reasoning, can effectively explore and efficiently\nsolve complex domains. However, LLM agents have been found to suffer from\nsub-optimal exploration and the knowing-doing gap, the inability to effectively\nact on knowledge present in the model. In this work, we systematically study\nwhy LLMs perform sub-optimally in decision-making scenarios. In particular, we\nclosely examine three prevalent failure modes: greediness, frequency bias, and\nthe knowing-doing gap. We propose mitigation of these shortcomings by\nfine-tuning via Reinforcement Learning (RL) on self-generated CoT rationales.\nOur experiments across multi-armed bandits, contextual bandits, and\nTic-tac-toe, demonstrate that RL fine-tuning enhances the decision-making\nabilities of LLMs by increasing exploration and narrowing the knowing-doing\ngap. Finally, we study both classic exploration mechanisms, such as\nepsilon-greedy, and LLM-specific approaches, such as self-correction and\nself-consistency, to enable more effective fine-tuning of LLMs for\ndecision-making.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16078.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64c3849269b1a6796052eac7",
      "avatarUrl": "/avatars/9f0c832d5b51b659c7bb83074f02a648.svg",
      "fullname": "Thomas Schmied",
      "name": "thomasschmied",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.15785",
      "authors": [
        {
          "_id": "6808579f91ba7dbcc19dbd3e",
          "name": "Siyu Zhou",
          "hidden": false
        },
        {
          "_id": "6808579f91ba7dbcc19dbd3f",
          "user": {
            "_id": "647f5af5b0e96764589f3b2a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
            "isPro": false,
            "fullname": "Tianyi Zhou",
            "user": "zhoutianyi",
            "type": "user"
          },
          "name": "Tianyi Zhou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-23T08:27:49.077Z",
          "hidden": false
        },
        {
          "_id": "6808579f91ba7dbcc19dbd40",
          "name": "Yijun Yang",
          "hidden": false
        },
        {
          "_id": "6808579f91ba7dbcc19dbd41",
          "name": "Guodong Long",
          "hidden": false
        },
        {
          "_id": "6808579f91ba7dbcc19dbd42",
          "name": "Deheng Ye",
          "hidden": false
        },
        {
          "_id": "6808579f91ba7dbcc19dbd43",
          "name": "Jing Jiang",
          "hidden": false
        },
        {
          "_id": "6808579f91ba7dbcc19dbd44",
          "name": "Chengqi Zhang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/647f5af5b0e96764589f3b2a/ESSjdqSUTzyiFhpuhfUhO.png",
        "https://cdn-uploads.huggingface.co/production/uploads/647f5af5b0e96764589f3b2a/-ScBiECcYZw2_83Qu3-29.png"
      ],
      "publishedAt": "2025-04-22T10:58:27.000Z",
      "submittedOnDailyAt": "2025-04-23T01:40:28.955Z",
      "title": "WALL-E 2.0 : L'apprentissage par renforcement neurodynamique gère le monde, améliorant les intelligences artificielles basées sur des modèles de monde.",
      "submittedOnDailyBy": {
        "_id": "647f5af5b0e96764589f3b2a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
        "isPro": false,
        "fullname": "Tianyi Zhou",
        "user": "zhoutianyi",
        "type": "user"
      },
      "summary": "Est-il possible de construire un modèle mondial précis dans les LLMs ? Et quels bénéfices offrirait-il un modèle mondial à un agent de LLM ? La différence entre le savoir préalable des LLMs et la dynamique d'un environnement spécifique souvent est la cause d'un impact négatif sur le rendement. Pour atténuer cet effet, on propose une méthodologie d'entraînement sans limites appelée \"ajustement du monde\", qui permet aux LLMs d'apprendre indirectement sur l'environnement. Le savoir objectif inclut des règles d'action, des graphes et des graphes d'échelle, que les LLMs extraient des flux d'exploration et qui sont codifiés en code executable pour ajuster la politique des agents de LLMs. De plus, on propose un agent RL sans limites basé sur des modèles, appelé WALL-E 2.0, à travers le cadre de MPC pour le contrôle par prédiction du modèle, en contraste avec l'MPC traditionnel qui nécessite des calculs en temps réel coûteux, les agents de LLMs interagissent avec le monde cognitif et optimisent efficacement les actions futures. La forte heuristique des agents de LLMs agit comme un planificateur efficace en MPC, garantissant la qualité des actions planifiées selon la prédiction précise du modèle mondial. Cela peut significativement améliorer l'efficacité de l'apprentissage dans de nouveaux environnements. Dans des défis d'un monde plus large que Minecraft, comme celui de Mars (Minecraft plus large) et ALFWorld (environnements intérieurs structurés), WALL-E 2.0 dépasse les méthodes actuelles de manière notable. Sur Mars, il atteint un pourcentage de succès de 16,1% à 51,6% plus que le standard et améliore la note de plus de 61,7%. Sur ALFWorld, il réussit à 98% de nouveaux liens en quatre itérations.",
      "upvotes": 5,
      "discussionId": "680857a191ba7dbcc19dbda6",
      "githubRepo": "https://github.com/elated-sawyer/WALL-E",
      "ai_keywords": [
        "world models",
        "large language models (LLMs)",
        "symbolic knowledge",
        "knowledge graphs",
        "scene graphs",
        "exploration trajectories",
        "executable codes",
        "policies",
        "RL-free",
        "model-based agent",
        "WALL-E 2.0",
        "model-predictive control (MPC)",
        "neurosymbolic world model",
        "look-ahead optimizer",
        "heuristics",
        "planner",
        "predictions",
        "learning efficiency",
        "open-world challenges",
        "Mars (Minecraft like)",
        "ALFWorld (embodied indoor environments)",
        "success rate",
        "score"
      ]
    },
    "publishedAt": "2025-04-22T06:58:27.000Z",
    "title": "WALL-E 2.0: World Alignment by NeuroSymbolic Learning improves World\n  Model-based LLM Agents",
    "summary": "Can we build accurate world models out of large language models (LLMs)? How\ncan world models benefit LLM agents? The gap between the prior knowledge of\nLLMs and the specified environment's dynamics usually bottlenecks LLMs'\nperformance as world models. To bridge the gap, we propose a training-free\n\"world alignment\" that learns an environment's symbolic knowledge complementary\nto LLMs. The symbolic knowledge covers action rules, knowledge graphs, and\nscene graphs, which are extracted by LLMs from exploration trajectories and\nencoded into executable codes to regulate LLM agents' policies. We further\npropose an RL-free, model-based agent \"WALL-E 2.0\" through the model-predictive\ncontrol (MPC) framework. Unlike classical MPC requiring costly optimization on\nthe fly, we adopt an LLM agent as an efficient look-ahead optimizer of future\nsteps' actions by interacting with the neurosymbolic world model. While the LLM\nagent's strong heuristics make it an efficient planner in MPC, the quality of\nits planned actions is also secured by the accurate predictions of the aligned\nworld model. They together considerably improve learning efficiency in a new\nenvironment. On open-world challenges in Mars (Minecraft like) and ALFWorld\n(embodied indoor environments), WALL-E 2.0 significantly outperforms existing\nmethods, e.g., surpassing baselines in Mars by 16.1%-51.6% of success rate and\nby at least 61.7% in score. In ALFWorld, it achieves a new record 98% success\nrate after only 4 iterations.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/647f5af5b0e96764589f3b2a/ESSjdqSUTzyiFhpuhfUhO.png",
      "https://cdn-uploads.huggingface.co/production/uploads/647f5af5b0e96764589f3b2a/-ScBiECcYZw2_83Qu3-29.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15785.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "647f5af5b0e96764589f3b2a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
      "fullname": "Tianyi Zhou",
      "name": "zhoutianyi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.16082",
      "authors": [
        {
          "_id": "6808486f043aa415b647ca77",
          "name": "Ziqi Pang",
          "hidden": false
        },
        {
          "_id": "6808486f043aa415b647ca78",
          "name": "Yu-Xiong Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-22T17:59:41.000Z",
      "submittedOnDailyAt": "2025-04-23T00:25:18.841Z",
      "title": "Master Video: \"MapReduce\" est une base de principes de base pour comprendre un long video.",
      "submittedOnDailyBy": {
        "_id": "642a33ea5673845d9854f458",
        "avatarUrl": "/avatars/ea8ed29a0b4ad629a2ba6e7d26cbd923.svg",
        "isPro": false,
        "fullname": "Ziqi Pang",
        "user": "ziqipang",
        "type": "user"
      },
      "summary": "Le Manège Vidéo est une architecture efficace pour comprendre des vidéos longues. Cette architecture présente une version simple et efficace du principe MapReduce dans le traitement de vidéos longues. (1) Map : Reconnaît de manière dense des courts vidéos en séquences indépendantes. (2) Reduce : Gère l'information de tous les courts vidéos. Comparé à des modèles de langage visuel (VLMs) basés sur des séquences, le Manège Vidéo effectue un reconnaissance détaillée des courts vidéos indépendamment de la longueur du contexte. Contrairement aux agents vidéo existants, le Manège Vidéo ne dépend pas de la sélection séquentielle de segments clés, mais permet un reconnaissance de segments courts de vidéos de manière ordonnée et scalable par l'opération Map. Dans le pas Reduce, une agrégation plus détaillée du contexte est effectuée et les raisons sont justifiées, dépassant la recherche explicite de segments clés. Ce principe de MapReduce est également applicable aux VLMs et agents vidéo. Son efficacité est testée en utilisant des agents de langage long (LLM).\n\nEn fait, le Manège Vidéo utilise deux étapes de MapReduce. (A) Capture : Génère des captures de courts vidéos et (map) normalise les mots et objets répétés avec des noms communs (reduce); (B) Analyse : Analyse l'information liée des courts vidéos selon les questions du utilisateur et les intègre dans la réponse finale (map), (reduce). Comparé à des VLMs et agents vidéo, le Manège Vidéo atteint un améliorament de précision de 10% ou plus sur LVBench, un benchmark difficile.\n\nLe code est disponible sur la URL suivante : https://github.com/ziqipang/MR-Video",
      "upvotes": 3,
      "discussionId": "68084870043aa415b647caaf",
      "ai_keywords": [
        "MapReduce",
        "short video clips",
        "sequence-to-sequence vision-language models (VLMs)",
        "sequence parallel perception",
        "context aggregation",
        "context reasoning",
        "key segment selection",
        "key segment retrieval",
        "Captioning",
        "standardizing",
        "repeated characters",
        "shared names",
        "Analysis",
        "relevant information",
        "final answer",
        "LVBench"
      ]
    },
    "publishedAt": "2025-04-22T13:59:41.000Z",
    "title": "MR. Video: \"MapReduce\" is the Principle for Long Video Understanding",
    "summary": "We propose MR. Video, an agentic long video understanding framework that\ndemonstrates the simple yet effective MapReduce principle for processing long\nvideos: (1) Map: independently and densely perceiving short video clips, and\n(2) Reduce: jointly aggregating information from all clips. Compared with\nsequence-to-sequence vision-language models (VLMs), MR. Video performs detailed\nshort video perception without being limited by context length. Compared with\nexisting video agents that typically rely on sequential key segment selection,\nthe Map operation enables simpler and more scalable sequence parallel\nperception of short video segments. Its Reduce step allows for more\ncomprehensive context aggregation and reasoning, surpassing explicit key\nsegment retrieval. This MapReduce principle is applicable to both VLMs and\nvideo agents, and we use LLM agents to validate its effectiveness.\n  In practice, MR. Video employs two MapReduce stages: (A) Captioning:\ngenerating captions for short video clips (map), then standardizing repeated\ncharacters and objects into shared names (reduce); (B) Analysis: for each user\nquestion, analyzing relevant information from individual short videos (map),\nand integrating them into a final answer (reduce). MR. Video achieves over 10%\naccuracy improvement on the challenging LVBench compared to state-of-the-art\nVLMs and video agents.\n  Code is available at: https://github.com/ziqipang/MR-Video",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16082.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642a33ea5673845d9854f458",
      "avatarUrl": "/avatars/ea8ed29a0b4ad629a2ba6e7d26cbd923.svg",
      "fullname": "Ziqi Pang",
      "name": "ziqipang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.11703",
      "authors": [
        {
          "_id": "6800a4f9f16f9f820ed748af",
          "user": {
            "_id": "64f27f74f1b6c235aed4b904",
            "avatarUrl": "/avatars/1129af4fcf566b87a5ff81375376bf3a.svg",
            "isPro": false,
            "fullname": "stneng",
            "user": "stneng",
            "type": "user"
          },
          "name": "Tianneng Shi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-22T10:13:56.123Z",
          "hidden": true
        },
        {
          "_id": "6800a4f9f16f9f820ed748b0",
          "name": "Jingxuan He",
          "hidden": false
        },
        {
          "_id": "6800a4f9f16f9f820ed748b1",
          "name": "Zhun Wang",
          "hidden": false
        },
        {
          "_id": "6800a4f9f16f9f820ed748b2",
          "name": "Linyu Wu",
          "hidden": false
        },
        {
          "_id": "6800a4f9f16f9f820ed748b3",
          "name": "Hongwei Li",
          "hidden": false
        },
        {
          "_id": "6800a4f9f16f9f820ed748b4",
          "name": "Wenbo Guo",
          "hidden": false
        },
        {
          "_id": "6800a4f9f16f9f820ed748b5",
          "name": "Dawn Song",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-16T01:58:40.000Z",
      "submittedOnDailyAt": "2025-04-23T00:27:37.099Z",
      "title": "Progent: Limitation de défense contre les restrictions de droits de programmation dans l'Agent Large Modèle",
      "submittedOnDailyBy": {
        "_id": "64f27f74f1b6c235aed4b904",
        "avatarUrl": "/avatars/1129af4fcf566b87a5ff81375376bf3a.svg",
        "isPro": false,
        "fullname": "stneng",
        "user": "stneng",
        "type": "user"
      },
      "summary": "Le LLM Agent est un nouveau processus final qui complète des tâches assignées à un utilisateur en utilisant différentes outils, avec le LLM en tant que composant essentiel. Cependant, le LLM Agent présente un risque élevé de sécurité. En interagissant avec le monde extérieur, il peut recevoir des instructions malicieuses qui le poussent à effectuer des actions dangereuses. Pour résoudre ce problème, limiter les autorisations est une bonne stratégie. Permettre seulement ce qui est nécessaire pour accomplir la tâche et bloquer ce qui n'est pas nécessaire. Cependant, lorsque cela est fait, il est nécessaire de maintenir à la fois la sécurité et l'utilité. Cela est difficile.\n\nNous présentons pour la première fois Progent, le premier contrôleur d'autorisations. Le point clé est l'utilisation d'un langage de domaine spécifique pour exprimer des politiques de restriction d'autorisations qui sont appliquées lors de l'exécution de l'agent. Ces politiques imposent des restrictions spécifiques sur les appels à des outils et décident si un appel est autorisé ou non, en désignant un backup par défaut en cas d'interdiction. De cette manière, les développeurs d'agents et les utilisateurs peuvent écrire et exécuter des politiques adaptées aux cas de usage spécifiques, assurant ainsi la sécurité. Le design modulaire de Progent permet son intégration sans modifier l'intérieur de l'agent, nécessitant seulement des changements dans la mise en œuvre de l'agent et améliorant la praticité et la large capacité. Il est possible d'automatiser la création de politiques en utilisant le LLM pour générer des politiques basées sur des questions du utilisateur et les mettre à jour dynamiquement pour améliorer la sécurité et l'utilité. Selon notre évaluation large, AgentDojo, ASB et AgentPoison, dans trois scénarios et benchmarks différents, ils montrent leur capacité à maintenir une forte sécurité et une haute utilité. De plus, on analyse en détail l'effet des composants clés et la correspondance avec la génération automatique de politiques, démontrant leur résistance face aux attaques.",
      "upvotes": 3,
      "discussionId": "6800a4faf16f9f820ed748ee",
      "ai_keywords": [
        "LLM agents",
        "large language models (LLMs)",
        "principle of least privilege",
        "privilege control mechanism",
        "domain-specific language",
        "privilege control policies",
        "tool calls",
        "agent execution",
        "fine-grained constraints",
        "fallbacks",
        "security",
        "utility",
        "policy writing",
        "automated policy generation",
        "AgentDojo",
        "ASB",
        "AgentPoison"
      ]
    },
    "publishedAt": "2025-04-15T21:58:40.000Z",
    "title": "Progent: Programmable Privilege Control for LLM Agents",
    "summary": "LLM agents are an emerging form of AI systems where large language models\n(LLMs) serve as the central component, utilizing a diverse set of tools to\ncomplete user-assigned tasks. Despite their great potential, LLM agents pose\nsignificant security risks. When interacting with the external world, they may\nencounter malicious commands from attackers, leading to the execution of\ndangerous actions. A promising way to address this is by enforcing the\nprinciple of least privilege: allowing only essential actions for task\ncompletion while blocking unnecessary ones. However, achieving this is\nchallenging, as it requires covering diverse agent scenarios while preserving\nboth security and utility.\n  We introduce Progent, the first privilege control mechanism for LLM agents.\nAt its core is a domain-specific language for flexibly expressing privilege\ncontrol policies applied during agent execution. These policies provide\nfine-grained constraints over tool calls, deciding when tool calls are\npermissible and specifying fallbacks if they are not. This enables agent\ndevelopers and users to craft suitable policies for their specific use cases\nand enforce them deterministically to guarantee security. Thanks to its modular\ndesign, integrating Progent does not alter agent internals and requires only\nminimal changes to agent implementation, enhancing its practicality and\npotential for widespread adoption. To automate policy writing, we leverage LLMs\nto generate policies based on user queries, which are then updated dynamically\nfor improved security and utility. Our extensive evaluation shows that it\nenables strong security while preserving high utility across three distinct\nscenarios or benchmarks: AgentDojo, ASB, and AgentPoison. Furthermore, we\nperform an in-depth analysis, showcasing the effectiveness of its core\ncomponents and the resilience of its automated policy generation against\nadaptive attacks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11703.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64f27f74f1b6c235aed4b904",
      "avatarUrl": "/avatars/1129af4fcf566b87a5ff81375376bf3a.svg",
      "fullname": "stneng",
      "name": "stneng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.14977",
      "authors": [
        {
          "_id": "68084dbc2eff5d45775d8f14",
          "name": "Jingkai Zhou",
          "hidden": false
        },
        {
          "_id": "68084dbc2eff5d45775d8f15",
          "name": "Yifan Wu",
          "hidden": false
        },
        {
          "_id": "68084dbc2eff5d45775d8f16",
          "name": "Shikai Li",
          "hidden": false
        },
        {
          "_id": "68084dbc2eff5d45775d8f17",
          "name": "Min Wei",
          "hidden": false
        },
        {
          "_id": "68084dbc2eff5d45775d8f18",
          "name": "Chao Fan",
          "hidden": false
        },
        {
          "_id": "68084dbc2eff5d45775d8f19",
          "name": "Weihua Chen",
          "hidden": false
        },
        {
          "_id": "68084dbc2eff5d45775d8f1a",
          "name": "Wei Jiang",
          "hidden": false
        },
        {
          "_id": "68084dbc2eff5d45775d8f1b",
          "name": "Fan Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-21T09:09:21.000Z",
      "submittedOnDailyAt": "2025-04-23T00:49:31.236Z",
      "title": "RealisDance-DiT : Créez un simple et puissant standard pour les animations de personnages manipulables dans la nature",
      "submittedOnDailyBy": {
        "_id": "6434caa64b34368fdb07da48",
        "avatarUrl": "/avatars/8da3d3d1e274ff4ad409234678b1b952.svg",
        "isPro": false,
        "fullname": "Jingkai Zhou",
        "user": "theFoxofSky",
        "type": "user"
      },
      "summary": "L'animation de personnages contrôlables est particulièrement rare, spécialisée dans les personnages et l'interaction avec les objets, l'illumination complexe et les scènes dynamiques, ce qui génère des problèmes difficiles. Pour aborder ces problèmes, les études précédentes ont principalement utilisé des réseaux de neurones à deux passages et ont introduit des guides pour la posture et l'apparence. Cependant, elles généralement ont des difficultés à s'adapter aux scènes ouvertes. Dans cet article, nous proposons une nouvelle perspective pour résoudre ces problèmes, en utilisant un modèle de base suffisamment puissant et une stratégie flexible d'ajustement. Spécifiquement, nous présentons RealisDance-DiT, un modèle construit sur le modèle Wan-2.1 de vidéo. Notre analyse exhaustive montre que le design de la Réseau de Référence, largement utilisée, n'est pas optimal pour de grands modèles DiT. Au lieu de cela, nous effectuons un ajustement minimal dans l'architecture du modèle de base et nous montrons une forte base de référence. De plus, nous proposons un entraînement avec faible bruit et la stratégie « grandes batches et petites itérations » pour accélérer la convergence du modèle lors de l'ajustement et préserver les meilleures caractéristiques du modèle de base. De plus, nous présentons un nouveau jeu de données de test pour aborder divers défis réels, complémentant les benchmarks existants tels que celui de TikTok ou celui des vidéos de mode de UBC. À travers une large gamme d'expériences, nous démontrons que RealisDance-DiT dépasse significativement les méthodes actuelles.",
      "upvotes": 2,
      "discussionId": "68084dc02eff5d45775d902c",
      "projectPage": "https://thefoxofsky.github.io/project_pages/RealisDance-DiT/index",
      "githubRepo": "https://github.com/damo-cv/RealisDance",
      "ai_keywords": [
        "RealisDance-DiT",
        "Wan-2.1 video foundation model",
        "Reference Net design",
        "DiT models",
        "low-noise warmup",
        "large batches and small iterations strategies",
        "foundation model architecture",
        "model convergence",
        "priors of the foundation model",
        "TikTok dataset",
        "UBC fashion video dataset"
      ]
    },
    "publishedAt": "2025-04-21T05:09:21.000Z",
    "title": "RealisDance-DiT: Simple yet Strong Baseline towards Controllable\n  Character Animation in the Wild",
    "summary": "Controllable character animation remains a challenging problem, particularly\nin handling rare poses, stylized characters, character-object interactions,\ncomplex illumination, and dynamic scenes. To tackle these issues, prior work\nhas largely focused on injecting pose and appearance guidance via elaborate\nbypass networks, but often struggles to generalize to open-world scenarios. In\nthis paper, we propose a new perspective that, as long as the foundation model\nis powerful enough, straightforward model modifications with flexible\nfine-tuning strategies can largely address the above challenges, taking a step\ntowards controllable character animation in the wild. Specifically, we\nintroduce RealisDance-DiT, built upon the Wan-2.1 video foundation model. Our\nsufficient analysis reveals that the widely adopted Reference Net design is\nsuboptimal for large-scale DiT models. Instead, we demonstrate that minimal\nmodifications to the foundation model architecture yield a surprisingly strong\nbaseline. We further propose the low-noise warmup and \"large batches and small\niterations\" strategies to accelerate model convergence during fine-tuning while\nmaximally preserving the priors of the foundation model. In addition, we\nintroduce a new test dataset that captures diverse real-world challenges,\ncomplementing existing benchmarks such as TikTok dataset and UBC fashion video\ndataset, to comprehensively evaluate the proposed method. Extensive experiments\nshow that RealisDance-DiT outperforms existing methods by a large margin.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.14977.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6434caa64b34368fdb07da48",
      "avatarUrl": "/avatars/8da3d3d1e274ff4ad409234678b1b952.svg",
      "fullname": "Jingkai Zhou",
      "name": "theFoxofSky",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.15524",
      "authors": [
        {
          "_id": "68084b46fa5a6cc6bd9e6a83",
          "user": {
            "_id": "64560618bfdf9c63ce2d658a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64560618bfdf9c63ce2d658a/GVBWU4yNzRsjdyzKT3z3B.jpeg",
            "isPro": false,
            "fullname": "Mathsion Wong",
            "user": "QiYao-Wang",
            "type": "user"
          },
          "name": "Qiyao Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-23T08:27:53.885Z",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a84",
          "name": "Guhong Chen",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a85",
          "name": "Hongbo Wang",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a86",
          "name": "Huaren Liu",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a87",
          "name": "Minghui Zhu",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a88",
          "name": "Zhifei Qin",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a89",
          "name": "Linwei Li",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a8a",
          "name": "Yilin Yue",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a8b",
          "name": "Shiqiang Wang",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a8c",
          "name": "Jiayan Li",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a8d",
          "name": "Yihang Wu",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a8e",
          "name": "Ziqiang Liu",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a8f",
          "name": "Longze Chen",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a90",
          "name": "Run Luo",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a91",
          "name": "Liyang Fan",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a92",
          "name": "Jiaming Li",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a93",
          "name": "Lei Zhang",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a94",
          "name": "Kan Xu",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a95",
          "name": "Hongfei Lin",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a96",
          "name": "Hamid Alinejad-Rokny",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a97",
          "name": "Shiwen Ni",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a98",
          "name": "Yuan Lin",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a99",
          "name": "Min Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-22T02:00:41.000Z",
      "submittedOnDailyAt": "2025-04-23T05:32:33.756Z",
      "title": "IPBench : Marqueur de test du savoir des modèles de langue à grande échelle sur la propriété intellectuelle",
      "submittedOnDailyBy": {
        "_id": "64560618bfdf9c63ce2d658a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64560618bfdf9c63ce2d658a/GVBWU4yNzRsjdyzKT3z3B.jpeg",
        "isPro": false,
        "fullname": "Mathsion Wong",
        "user": "QiYao-Wang",
        "type": "user"
      },
      "summary": "Les droits de propriété intellectuelle (DPI) constituent un domaine spécial, riche en complexité unique et en concentration de connaissances, intégrant des connaissances techniques et juridiques. À mesure que les modèles de langage grands (LLMs) se développent, ils présentent un grand potentiel dans le domaine des DPI, permettant une meilleure analyse, compréhension et génération de contenu lié aux DPI de manière plus efficace. Cependant, les ensembles de données et les cadres de référence actuels se concentrent principalement sur les brevets ou sur des aspects spécifiques du domaine des DPI, ne se démontrant pas adaptés aux scénarios réels. Pour combler cette lacune, nous présentons IPBench, le premier cadre de référence multilingue pratique qui aborde 8 institutions de DPI et 20 travaux. Ce cadre de référence est conçu pour évaluer les modèles d'infrastructure de connaissances réelles, y compris la compréhension et la génération. Nous avons évalué 16 modèles de LLMs, couvrant un éventail allant des modèles généraux jusqu'à ceux spécialisés dans le domaine des DPI. Le meilleur modèle a atteint une précision de 75,8%, démontrant une grande amélioration. En particulier, les modèles open-source et spécialisés dans le domaine des DPI ont obtenu des résultats meilleurs que les modèles généraux fermés-source. Tous les données et codes d'IPBench sont publiés, et nous continuons à étendre les travaux liés aux DPI pour refléter plus précisément les défis d'une infrastructure de connaissances réelles.",
      "upvotes": 1,
      "discussionId": "68084b4cfa5a6cc6bd9e6c75",
      "projectPage": "https://ipbench.github.io/",
      "githubRepo": "https://github.com/IPBench/IPBench"
    },
    "publishedAt": "2025-04-21T22:00:41.000Z",
    "title": "IPBench: Benchmarking the Knowledge of Large Language Models in\n  Intellectual Property",
    "summary": "Intellectual Property (IP) is a unique domain that integrates technical and\nlegal knowledge, making it inherently complex and knowledge-intensive. As large\nlanguage models (LLMs) continue to advance, they show great potential for\nprocessing IP tasks, enabling more efficient analysis, understanding, and\ngeneration of IP-related content. However, existing datasets and benchmarks\neither focus narrowly on patents or cover limited aspects of the IP field,\nlacking alignment with real-world scenarios. To bridge this gap, we introduce\nthe first comprehensive IP task taxonomy and a large, diverse bilingual\nbenchmark, IPBench, covering 8 IP mechanisms and 20 tasks. This benchmark is\ndesigned to evaluate LLMs in real-world intellectual property applications,\nencompassing both understanding and generation. We benchmark 16 LLMs, ranging\nfrom general-purpose to domain-specific models, and find that even the\nbest-performing model achieves only 75.8% accuracy, revealing substantial room\nfor improvement. Notably, open-source IP and law-oriented models lag behind\nclosed-source general-purpose models. We publicly release all data and code of\nIPBench and will continue to update it with additional IP-related tasks to\nbetter reflect real-world challenges in the intellectual property domain.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15524.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64560618bfdf9c63ce2d658a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64560618bfdf9c63ce2d658a/GVBWU4yNzRsjdyzKT3z3B.jpeg",
      "fullname": "Mathsion Wong",
      "name": "QiYao-Wang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.14735",
      "authors": [
        {
          "_id": "6807f85efcd784a902c87126",
          "user": {
            "_id": "621d85a10e35b2fbbf3e6196",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/621d85a10e35b2fbbf3e6196/D6DMntYh-eV5fVef_Mx4Z.png",
            "isPro": false,
            "fullname": "Chin-Yun Yu",
            "user": "yoyolicoris",
            "type": "user"
          },
          "name": "Chin-Yun Yu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-23T08:28:18.913Z",
          "hidden": false
        },
        {
          "_id": "6807f85efcd784a902c87127",
          "name": "Marco A. Martínez-Ramírez",
          "hidden": false
        },
        {
          "_id": "6807f85efcd784a902c87128",
          "name": "Junghyun Koo",
          "hidden": false
        },
        {
          "_id": "6807f85efcd784a902c87129",
          "name": "Ben Hayes",
          "hidden": false
        },
        {
          "_id": "6807f85efcd784a902c8712a",
          "name": "Wei-Hsiang Liao",
          "hidden": false
        },
        {
          "_id": "6807f85efcd784a902c8712b",
          "name": "György Fazekas",
          "hidden": false
        },
        {
          "_id": "6807f85efcd784a902c8712c",
          "name": "Yuki Mitsufuji",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/621d85a10e35b2fbbf3e6196/alz9iP58RZiY22SYNBEzr.png"
      ],
      "publishedAt": "2025-04-20T20:52:58.000Z",
      "submittedOnDailyAt": "2025-04-23T08:05:27.097Z",
      "title": "DiffVox : Modèle différentiel pour la distribution efficace des experts",
      "submittedOnDailyBy": {
        "_id": "621d85a10e35b2fbbf3e6196",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/621d85a10e35b2fbbf3e6196/D6DMntYh-eV5fVef_Mx4Z.png",
        "isPro": false,
        "fullname": "Chin-Yun Yu",
        "user": "yoyolicoris",
        "type": "user"
      },
      "summary": "Dans cette étude, nous présentons un nouveau modèle interprétable \"DiffVox\" pour l'affectation efficace de sons dans la création de musique. DiffVox est une abréviation de \"Differentiable Vocal Fx\", qui intègre les effets de rétroalimentation de paramètres, le contrôle dynamique de gamme, le desfade et la réverbération à travers des implémentations valides de différenciation, permettant ainsi l'optimisation gradient-basée dans l'évaluation de paramètres. Les pré-emplaces de sons sont recherchés dans MedleyDB avec 70 œuvres et dans des collections personnelles avec 365 œuvres. L'analyse de corrélation de paramètres montre clairement que le filtre de douleur et le filtre de bas sont nécessaires pour former le son, et que le temps de desfade et l'intensité de desfade sont corrélés. Dans l'analyse en composantes principales, nous démontrons la dimension de couleur de McAdams, où le composante la plus importante contrôle l'espace observé et le deuxième composante affecte la brillance du spectre. Dans la statistique de test, nous confirmons la non-normalité de la distribution de paramètres et soulignons la complexité de l'espace d'effets sonores. Ces résultats initiaux constituent la base pour des futures recherches sur la modélisation d'effets sonores et l'automatisation de la mix. Le code source et les ensembles de données sont disponibles sur https://github.com/SonyResearch/diffvox.",
      "upvotes": 0,
      "discussionId": "6807f860fcd784a902c87194",
      "githubRepo": "https://github.com/SonyResearch/diffvox",
      "ai_keywords": [
        "parametric equalisation",
        "dynamic range control",
        "delay",
        "reverb",
        "differentiable implementations",
        "gradient-based optimisation",
        "parameter estimation",
        "principal component analysis",
        "McAdams' timbre dimensions",
        "perceived spaciousness",
        "spectral brightness",
        "non-Gaussian nature"
      ]
    },
    "publishedAt": "2025-04-20T16:52:58.000Z",
    "title": "DiffVox: A Differentiable Model for Capturing and Analysing Professional\n  Effects Distributions",
    "summary": "This study introduces a novel and interpretable model, DiffVox, for matching\nvocal effects in music production. DiffVox, short for ``Differentiable Vocal\nFx\", integrates parametric equalisation, dynamic range control, delay, and\nreverb with efficient differentiable implementations to enable gradient-based\noptimisation for parameter estimation. Vocal presets are retrieved from two\ndatasets, comprising 70 tracks from MedleyDB and 365 tracks from a private\ncollection. Analysis of parameter correlations highlights strong relationships\nbetween effects and parameters, such as the high-pass and low-shelf filters\noften behaving together to shape the low end, and the delay time correlates\nwith the intensity of the delayed signals. Principal component analysis reveals\nconnections to McAdams' timbre dimensions, where the most crucial component\nmodulates the perceived spaciousness while the secondary components influence\nspectral brightness. Statistical testing confirms the non-Gaussian nature of\nthe parameter distribution, highlighting the complexity of the vocal effects\nspace. These initial findings on the parameter distributions set the foundation\nfor future research in vocal effects modelling and automatic mixing. Our source\ncode and datasets are accessible at https://github.com/SonyResearch/diffvox.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/621d85a10e35b2fbbf3e6196/alz9iP58RZiY22SYNBEzr.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.14735.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "621d85a10e35b2fbbf3e6196",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/621d85a10e35b2fbbf3e6196/D6DMntYh-eV5fVef_Mx4Z.png",
      "fullname": "Chin-Yun Yu",
      "name": "yoyolicoris",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]