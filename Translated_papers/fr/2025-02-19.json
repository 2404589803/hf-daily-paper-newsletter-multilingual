[
  {
    "paper": {
      "id": "2502.12900",
      "authors": [
        {
          "_id": "67b54851b986e35c41e063da",
          "user": {
            "_id": "66975b9f8031bf92b428e138",
            "avatarUrl": "/avatars/3254281a7bac1c8ddde1d6bc7e518b2f.svg",
            "isPro": false,
            "fullname": "Yuhao Zhang",
            "user": "Yoohao",
            "type": "user"
          },
          "name": "Yuhao Zhang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-02-19T02:56:18.848Z",
          "hidden": false
        },
        {
          "_id": "67b54851b986e35c41e063db",
          "user": {
            "_id": "66597f2cf769c3c443b7cf41",
            "avatarUrl": "/avatars/735cc8aa430748d20ca7312c72b1eaf1.svg",
            "isPro": false,
            "fullname": "Chihang Lau",
            "user": "puccho",
            "type": "user"
          },
          "name": "Zhiheng Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-19T09:01:05.678Z",
          "hidden": false
        },
        {
          "_id": "67b54851b986e35c41e063dc",
          "user": {
            "_id": "668e7f46c243a12604035758",
            "avatarUrl": "/avatars/35bd20032fafb7d7603266cf9a72d1e0.svg",
            "isPro": false,
            "fullname": "Fan Bu",
            "user": "FanBuCUHK",
            "type": "user"
          },
          "name": "Fan Bu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:42:08.544Z",
          "hidden": false
        },
        {
          "_id": "67b54851b986e35c41e063dd",
          "user": {
            "_id": "67b587c8882e49771f610b51",
            "avatarUrl": "/avatars/aecfb38b44141b8284416fc261692909.svg",
            "isPro": false,
            "fullname": "Ruiyu Zhang",
            "user": "PhoenixAxis",
            "type": "user"
          },
          "name": "Ruiyu Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:42:14.866Z",
          "hidden": false
        },
        {
          "_id": "67b54851b986e35c41e063de",
          "user": {
            "_id": "637c6703ca8542a0ba900ccb",
            "avatarUrl": "/avatars/288ed63a1efa566c3f01e850c6ba5dd5.svg",
            "isPro": false,
            "fullname": "Wang",
            "user": "Benyou",
            "type": "user"
          },
          "name": "Benyou Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:42:23.845Z",
          "hidden": false
        },
        {
          "_id": "67b54851b986e35c41e063df",
          "name": "Haizhou Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-18T14:36:39.000Z",
      "title": "Sonore : Le plus petit est le plus grand.",
      "summary": "Aujourd'hui, les modèles de langage à langage (LLMs) jusqu'à l'endpoint sont généralement entraînés avec de grandes quantités de données étiquetées, et l'efficacité de l'entraînement des données n'est pas discutée de manière profonde. Nous abordons deux problèmes fondamentaux entre son et texte : l'espace de représentation et l'incohérence de la longueur de séquence. Nous proposons une nouvelle architecture appelée Soundwave et des stratégies d'entraînement efficaces pour résoudre ces problèmes. En conséquence, Soundwave montre un comportement supérieur à Qwen2-Audio, le leader en traduction audio et tâches audio sur AIR-Bench. De plus, elle utilise seulement un 1/50 des données d'entraînement. L'analyse réalisée montre que Soundwave maintient son intelligence pendant le processus de transformation. Ce projet peut être consulté sur https://github.com/FreedomIntelligence/Soundwave.",
      "upvotes": 47,
      "discussionId": "67b54852b986e35c41e06426"
    },
    "publishedAt": "2025-02-19T00:22:36.628Z",
    "title": "Soundwave: Less is More for Speech-Text Alignment in LLMs",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.12900.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66975b9f8031bf92b428e138",
      "avatarUrl": "/avatars/3254281a7bac1c8ddde1d6bc7e518b2f.svg",
      "fullname": "Yuhao Zhang",
      "name": "Yoohao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.11564",
      "authors": [
        {
          "_id": "67b40f93aba9e111862052ab",
          "user": {
            "_id": "65e5bd4568234ef5d6decadc",
            "avatarUrl": "/avatars/c41095a946c0176b949c0b3566136c05.svg",
            "isPro": false,
            "fullname": "Jaehyeong Jo",
            "user": "harryjo97",
            "type": "user"
          },
          "name": "Jaehyeong Jo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-18T09:31:27.544Z",
          "hidden": false
        },
        {
          "_id": "67b40f93aba9e111862052ac",
          "name": "Sung Ju Hwang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-17T08:54:29.000Z",
      "title": "Modélisation du langage dans les modèles d'expansion continue",
      "summary": "Les modèles de diffusion ont apparu comme des alternatives prometteuses à la régression automatique dans la modélisation de données discrètes. Cependant, les modèles de diffusion qui fonctionnent directement dans l'espace des données discrètes perdent des signaux entre les états discrètes et ne peuvent pas exploiter complètement la force des améliorations continues. Les modèles de diffusion continus actuels pour des données discrètes sont limités en termes de performance par rapport aux méthodes discrètes, et les connexions incertaines entre eux empêchent le développement de modèles de diffusion pour des données discrètes. Dans cette étude, nous proposons un modèle de diffusion continu qui intègre la structure de la distribution discrète de classification. Nous établissons la connexion entre la diffusion discrète et les flux continus dans la variété statistique, et nous proposons un design simple de processus de diffusion pour généraliser les modèles de diffusion discrètes précédents basés sur cette similitude. De plus, nous proposons un cadre d'apprentissage sans limites de simulation basé sur la symétrie radiale et un méthode simple pour résoudre les problèmes de variétés de haute dimension. Les expériences détaillées sur les benchmarks de modèles de langage montrent que notre méthode dépasse les modèles de diffusion discrètes actuels et approche le rendement des modèles de régression automatique. Le code est disponible sur https://github.com/harryjo97/RDLM.",
      "upvotes": 30,
      "discussionId": "67b40f94aba9e111862052d5"
    },
    "publishedAt": "2025-02-18T22:43:02.567Z",
    "title": "Continuous Diffusion Model for Language Modeling",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.11564.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "65e5bd4568234ef5d6decadc",
      "avatarUrl": "/avatars/c41095a946c0176b949c0b3566136c05.svg",
      "fullname": "Jaehyeong Jo",
      "name": "harryjo97",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.11079",
      "authors": [
        {
          "_id": "67b40141ad717fe02e188c1a",
          "user": {
            "_id": "63a950ac3453852ef5394178",
            "avatarUrl": "/avatars/48a5e537b10e2247a17e63502e3201a6.svg",
            "isPro": false,
            "fullname": "Lijie Liu",
            "user": "liulj13",
            "type": "user"
          },
          "name": "Lijie Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-18T09:31:42.570Z",
          "hidden": false
        },
        {
          "_id": "67b40141ad717fe02e188c1b",
          "user": {
            "_id": "657ab4705e1c941f4c2f7877",
            "avatarUrl": "/avatars/c450f81f83dd0436ae120ab15616c4f7.svg",
            "isPro": false,
            "fullname": "Tianxiang Ma",
            "user": "Grayson111",
            "type": "user"
          },
          "name": "Tianxiang Ma",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:45:00.117Z",
          "hidden": false
        },
        {
          "_id": "67b40141ad717fe02e188c1c",
          "user": {
            "_id": "63b415037af2e415f2599c18",
            "avatarUrl": "/avatars/4afbe7d6d05a702f1beeed9c53e78153.svg",
            "isPro": false,
            "fullname": "Bingchuan Li",
            "user": "lbc402",
            "type": "user"
          },
          "name": "Bingchuan Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:47:57.441Z",
          "hidden": false
        },
        {
          "_id": "67b40141ad717fe02e188c1d",
          "user": {
            "_id": "6304e2dabad6ce7fc0287d57",
            "avatarUrl": "/avatars/3fd4a9a62b0ef98db2573411463a9247.svg",
            "isPro": false,
            "fullname": "Zhuowei_Chen",
            "user": "ZhuoweiChen",
            "type": "user"
          },
          "name": "Zhuowei Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:47:50.995Z",
          "hidden": false
        },
        {
          "_id": "67b40141ad717fe02e188c1e",
          "name": "Jiawei Liu",
          "hidden": false
        },
        {
          "_id": "67b40141ad717fe02e188c1f",
          "name": "Qian He",
          "hidden": false
        },
        {
          "_id": "67b40141ad717fe02e188c20",
          "name": "Xinglong Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-16T11:02:50.000Z",
      "title": "Fantom : Création de vidéos en accord avec des thèmes d'arrayénement de modalités croisées",
      "summary": "Le modèle de base de génération de vidéos continue a évolué pour de nombreuses applications, mais la génération de vidéos sur un thème cohérent reste dans un état d'exploration. Cela est appelé \"Subject-to-Video\", ce qui implique d'extraire des éléments thématiques d'une image de référence et de générer des vidéos qui correspondent au thème selon des indications de contexte. Nous croyons que l'essence de Subject-to-Video consiste à équilibrer le balance entre le contexte et la programmation des deux modalités, ainsi que à répondre à un niveau profond à la fois au contexte et au contenu visuel. Par conséquent, nous proposons le cadre intégré de génération de vidéos appelé Phantom. Basé sur l'architecture de vidéos à partir du contexte existant, nous redéfinissons le modèle d'injection d'image de contexte et nous effectuons une formation plus approfondie sur les triplets de vidéos d'image de contexte, pour atteindre une correspondance entre le contexte et l'image dans différentes modalités. En particulier, nous mettons l'accent sur la cohérence thématique dans la génération humaine, incluant l'amélioration de la génération de vidéos qui maintiennent l'identité existante. Le site web du projet est disponible sur https://phantom-video.github.io/Phantom/.",
      "upvotes": 27,
      "discussionId": "67b40144ad717fe02e188cb2"
    },
    "publishedAt": "2025-02-18T21:56:39.407Z",
    "title": "Phantom: Subject-consistent video generation via cross-modal alignment",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63a950ac3453852ef5394178/HuVZ5d9xTlI4R1onRv_F5.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.11079.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a950ac3453852ef5394178",
      "avatarUrl": "/avatars/48a5e537b10e2247a17e63502e3201a6.svg",
      "fullname": "Lijie Liu",
      "name": "liulj13",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.12464",
      "authors": [
        {
          "_id": "67b55b2cc92c4aa82c13562d",
          "user": {
            "_id": "64ad5f59b7e4b2c1ce47eb43",
            "avatarUrl": "/avatars/1f13ebe21a90d8c99920aa2c8cd9ac45.svg",
            "isPro": false,
            "fullname": "Seanie Lee",
            "user": "Seanie-lee",
            "type": "user"
          },
          "name": "Seanie Lee",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-19T09:00:53.341Z",
          "hidden": false
        },
        {
          "_id": "67b55b2cc92c4aa82c13562e",
          "name": "Dong Bok Lee",
          "hidden": false
        },
        {
          "_id": "67b55b2cc92c4aa82c13562f",
          "name": "Dominik Wagner",
          "hidden": false
        },
        {
          "_id": "67b55b2cc92c4aa82c135630",
          "name": "Minki Kang",
          "hidden": false
        },
        {
          "_id": "67b55b2cc92c4aa82c135631",
          "user": {
            "_id": "63a9379e2e05ca32e352d93b",
            "avatarUrl": "/avatars/6cda37befc873a92ed6d5dcba507954a.svg",
            "isPro": false,
            "fullname": "Haebin Seong",
            "user": "hbseong",
            "type": "user"
          },
          "name": "Haebin Seong",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:51:37.783Z",
          "hidden": false
        },
        {
          "_id": "67b55b2cc92c4aa82c135632",
          "name": "Tobias Bocklet",
          "hidden": false
        },
        {
          "_id": "67b55b2cc92c4aa82c135633",
          "name": "Juho Lee",
          "hidden": false
        },
        {
          "_id": "67b55b2cc92c4aa82c135634",
          "name": "Sung Ju Hwang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-18T02:51:17.000Z",
      "title": "SafeRoute : Sélection de modèles adaptatifs pour l'efficacité et la précision de la sécurité\nGuide des normes de sécurité pour les modèles de langage à grande échelle",
      "summary": "En la réalité des dispositifs de visualisation, l'introduction de modèles de langage grands (LLMs) nécessite l'implémentation de modèles de contrôle de sécurité robustes. Ces modèles détectent et bloquent les requêtes dangereuses des utilisateurs. Cependant, les grands modèles de contrôle de sécurité, bien qu'efficaces, ont un coût informatique élevé. Pour atténuer ce problème, on utilise des petits modèles de contrôle de sécurité. Cependant, dans des cas \"difficiles\" d'exemple, le rendement de ces petits modèles est inférieur à celui des grands modèles utilisés pour des prédictions précises. Nous avons constaté que de nombreux données peuvent être traitées de manière fiable avec des petits modèles, et qu'il suffit de calculer les grands modèles pour un petit pourcentage des données. En conséquence, nous proposons SafeRoute, un rotor binaire. Ce méthode applique de manière sélective les grands modèles de contrôle de sécurité uniquement dans des cas de données difficiles, optimisant ainsi le rendement et maintenant la précision. Les résultats des expériences sur plusieurs ensembles de données de test montrent que notre sélection adaptative de modèles améliore significativement le compromis entre coût informatique et rendement de sécurité, dépassant les références de base liées.",
      "upvotes": 24,
      "discussionId": "67b55b2dc92c4aa82c13568b"
    },
    "publishedAt": "2025-02-18T23:23:34.214Z",
    "title": "SafeRoute: Adaptive Model Selection for Efficient and Accurate Safety Guardrails in Large Language Models",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64ad5f59b7e4b2c1ce47eb43/ZEq_vSLjsXuPX3O-TWIpE.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.12464.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ad5f59b7e4b2c1ce47eb43",
      "avatarUrl": "/avatars/1f13ebe21a90d8c99920aa2c8cd9ac45.svg",
      "fullname": "Seanie Lee",
      "name": "Seanie-lee",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.13131",
      "authors": [
        {
          "_id": "67b5461d29cc269e5a4eb823",
          "name": "Feng Luo",
          "hidden": false
        },
        {
          "_id": "67b5461d29cc269e5a4eb824",
          "user": {
            "_id": "64d45451c34a346181b130dd",
            "avatarUrl": "/avatars/9bb8205b889337df5d321539c9b5d69d.svg",
            "isPro": false,
            "fullname": "Rui Yang",
            "user": "Ray2333",
            "type": "user"
          },
          "name": "Rui Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-19T09:01:23.095Z",
          "hidden": false
        },
        {
          "_id": "67b5461d29cc269e5a4eb825",
          "name": "Hao Sun",
          "hidden": false
        },
        {
          "_id": "67b5461d29cc269e5a4eb826",
          "user": {
            "_id": "634b9914dcf125e4da02498b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/634b9914dcf125e4da02498b/crRgFroWq5U6XWtvlTXSZ.jpeg",
            "isPro": false,
            "fullname": "Chunyuan Deng",
            "user": "CharlesDDDD",
            "type": "user"
          },
          "name": "Chunyuan Deng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:56:33.053Z",
          "hidden": false
        },
        {
          "_id": "67b5461d29cc269e5a4eb827",
          "name": "Jiarui Yao",
          "hidden": false
        },
        {
          "_id": "67b5461d29cc269e5a4eb828",
          "name": "Jingyan Shen",
          "hidden": false
        },
        {
          "_id": "67b5461d29cc269e5a4eb829",
          "user": {
            "_id": "6719d581a6cad13741b8bc7f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6719d581a6cad13741b8bc7f/w4EttqfXRgWZJc6HpYOS9.jpeg",
            "isPro": false,
            "fullname": "Huan Zhang",
            "user": "huanzhang12",
            "type": "user"
          },
          "name": "Huan Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:52:47.329Z",
          "hidden": false
        },
        {
          "_id": "67b5461d29cc269e5a4eb82a",
          "name": "Hanjie Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-18T18:55:26.000Z",
      "title": "Réévaluation de l'apprentissage de différentes préférences humaines par l'analyse des composantes principales",
      "summary": "Comprendre les inoccupations humaines est crucial pour le développement de modèles de base et la construction de systèmes d'IA dotés de caractéristiques. Cependant, les inoccupations sont essentiellement diverses et complexes, et les modèles de récompense traditionnels ont des difficultés à comprendre leur gamme totale. De plus, les données spécifiques des inoccupations sont utiles mais leur collecte est coûteuse et leur expansion est difficile. Dans cet article, nous utilisons une nouvelle approche pour extraire différentes inoccupations humaines : les modèles de récompense décomposés (DRMs). Notre approche principale est de représenter les inoccupations humaines comme des vecteurs et d'analyser elles par Analyse en Composantes Principales (PCA). En construisant un ensemble de données à partir de la différence de vecteurs cachés des réponses acceptées et refusées, les DRMs identifient des vecteurs orthogonaux qui détectent différents aspects des inoccupations. Ces modèles de récompense décomposés peuvent être combinés de manière flexible selon les besoins des utilisateurs et constituent une alternative aux modèles de récompense traditionnels, qui sont explicables et extensibles. Les DRMs extraient efficacement des dimensions significatives des inoccupations (par exemple, efficacité, sécurité, excellence) et peuvent s'adapter à de nouveaux utilisateurs sans nécessité d'entraînement supplémentaire. Ces résultats révèlent que les DRMs jouent un rôle crucial en tant que cadre robuste pour l'alignement explicable des LLMs avec des caractéristiques.",
      "upvotes": 23,
      "discussionId": "67b5461f29cc269e5a4eb8bc"
    },
    "publishedAt": "2025-02-18T21:59:45.466Z",
    "title": "Rethinking Diverse Human Preference Learning through Principal Component Analysis",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13131.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64d45451c34a346181b130dd",
      "avatarUrl": "/avatars/9bb8205b889337df5d321539c9b5d69d.svg",
      "fullname": "Rui Yang",
      "name": "Ray2333",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.13143",
      "authors": [
        {
          "_id": "67b546c0d8a1eac02c605f6a",
          "user": {
            "_id": "63c3e8abc7d7f4c63a515a02",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63c3e8abc7d7f4c63a515a02/npMHnVP2hHLbvoUGe7C4O.jpeg",
            "isPro": false,
            "fullname": "Zekun Qi",
            "user": "qizekun",
            "type": "user"
          },
          "name": "Zekun Qi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-19T09:01:21.001Z",
          "hidden": false
        },
        {
          "_id": "67b546c0d8a1eac02c605f6b",
          "user": {
            "_id": "65f9533b136fb8ddbd14e1fa",
            "avatarUrl": "/avatars/d88f75da0448093ccd1babba2a37d73f.svg",
            "isPro": false,
            "fullname": "Zhang",
            "user": "WenyaoZhang",
            "type": "user"
          },
          "name": "Wenyao Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T10:08:31.789Z",
          "hidden": false
        },
        {
          "_id": "67b546c0d8a1eac02c605f6c",
          "user": {
            "_id": "66bde456198f9d79f2be2d17",
            "avatarUrl": "/avatars/8c349aecb8a3a7cd7ef9d69e94eca8bd.svg",
            "isPro": false,
            "fullname": "Yufei Ding",
            "user": "YufeiD",
            "type": "user"
          },
          "name": "Yufei Ding",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T10:08:57.294Z",
          "hidden": false
        },
        {
          "_id": "67b546c0d8a1eac02c605f6d",
          "user": {
            "_id": "6201fc5d91d53938a6432fbf",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6201fc5d91d53938a6432fbf/VLs8ZYaZrop4KBpZn53fH.jpeg",
            "isPro": false,
            "fullname": "Runpei Dong",
            "user": "RunpeiDong",
            "type": "user"
          },
          "name": "Runpei Dong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-19T09:01:18.622Z",
          "hidden": false
        },
        {
          "_id": "67b546c0d8a1eac02c605f6e",
          "name": "Xinqiang Yu",
          "hidden": false
        },
        {
          "_id": "67b546c0d8a1eac02c605f6f",
          "name": "Jingwen Li",
          "hidden": false
        },
        {
          "_id": "67b546c0d8a1eac02c605f70",
          "name": "Lingyun Xu",
          "hidden": false
        },
        {
          "_id": "67b546c0d8a1eac02c605f71",
          "name": "Baoyu Li",
          "hidden": false
        },
        {
          "_id": "67b546c0d8a1eac02c605f72",
          "name": "Xialin He",
          "hidden": false
        },
        {
          "_id": "67b546c0d8a1eac02c605f73",
          "name": "Guofan Fan",
          "hidden": false
        },
        {
          "_id": "67b546c0d8a1eac02c605f74",
          "name": "Jiazhao Zhang",
          "hidden": false
        },
        {
          "_id": "67b546c0d8a1eac02c605f75",
          "name": "Jiawei He",
          "hidden": false
        },
        {
          "_id": "67b546c0d8a1eac02c605f76",
          "name": "Jiayuan Gu",
          "hidden": false
        },
        {
          "_id": "67b546c0d8a1eac02c605f77",
          "name": "Xin Jin",
          "hidden": false
        },
        {
          "_id": "67b546c0d8a1eac02c605f78",
          "name": "Kaisheng Ma",
          "hidden": false
        },
        {
          "_id": "67b546c0d8a1eac02c605f79",
          "name": "Zhizheng Zhang",
          "hidden": false
        },
        {
          "_id": "67b546c0d8a1eac02c605f7a",
          "name": "He Wang",
          "hidden": false
        },
        {
          "_id": "67b546c0d8a1eac02c605f7b",
          "name": "Li Yi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-18T18:59:02.000Z",
      "title": "Jusqu'à présent : les ponts de direction basés sur le langage connectent la théorie spatiale et la manipulation de l'objet.",
      "summary": "La capacité d'interpréter l'espace est l'un des éléments importants de l'intelligence artificielle, aidant les robots à comprendre et à interagir avec leur environnement. Les avancées récentes ont amélioré la capacité des VLM à reconnaître les positions des objets et à comprendre les relations de position, mais elles n'ont pas encore la capacité de déterminer la direction des objets. Pour résoudre ce problème, il est nécessaire une inférence géométrique et une représentation de la direction avec intuition et expressivité. Dans ce contexte, la nature humaine offre un espace de représentation plus flexible que les marques standard, et il est proposé qu'elle soit particulièrement adaptée pour les systèmes de robots qui écoutent des commandes. Dans cet article, une méthodologie est présentée pour définir la direction des objets dans la nature humaine (par exemple, la direction 'PLUG IN' d'un USB ou celle de 'HANDLE' d'une cuillère). Pour soutenir cela, un ensemble de données grand de modèles 3D appelé OrientText300K est construit, dont l'objectif est de combiner le sémantique fonctionnel et la compréhension géométrique. En incluant la direction contextuelle dans les systèmes VLM, on peut que les robots puissent générer des actions avec des contraintes de position et de direction. Les expériences d'expansion en simulation et dans le monde réel, comme dans Open6DOR (montrant une précision de 48,7%) et SIMPLER (montrant une précision de 74,9%), démontrent une amélioration significative de la capacité d'action des robots.",
      "upvotes": 22,
      "discussionId": "67b546c5d8a1eac02c606090"
    },
    "publishedAt": "2025-02-18T21:51:33.957Z",
    "title": "SoFar: Language-Grounded Orientation Bridges Spatial Reasoning and Object Manipulation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13143.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63c3e8abc7d7f4c63a515a02",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63c3e8abc7d7f4c63a515a02/npMHnVP2hHLbvoUGe7C4O.jpeg",
      "fullname": "Zekun Qi",
      "name": "qizekun",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.13145",
      "authors": [
        {
          "_id": "67b54b04bd51b4e46e39d287",
          "user": {
            "_id": "6577073fc2bf55b1f6bafb49",
            "avatarUrl": "/avatars/58803398b1a918b7570db17893e65122.svg",
            "isPro": false,
            "fullname": "liao",
            "user": "LegendBC",
            "type": "user"
          },
          "name": "Bencheng Liao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-19T09:01:00.934Z",
          "hidden": false
        },
        {
          "_id": "67b54b04bd51b4e46e39d288",
          "name": "Hongyuan Tao",
          "hidden": false
        },
        {
          "_id": "67b54b04bd51b4e46e39d289",
          "name": "Qian Zhang",
          "hidden": false
        },
        {
          "_id": "67b54b04bd51b4e46e39d28a",
          "user": {
            "_id": "646b3db131968a60a01e4cf5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646b3db131968a60a01e4cf5/DhfdqUYQaD1Qa8Svw996J.jpeg",
            "isPro": false,
            "fullname": "Tianheng Cheng",
            "user": "wondervictor",
            "type": "user"
          },
          "name": "Tianheng Cheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-19T09:00:58.351Z",
          "hidden": false
        },
        {
          "_id": "67b54b04bd51b4e46e39d28b",
          "name": "Yingyue Li",
          "hidden": false
        },
        {
          "_id": "67b54b04bd51b4e46e39d28c",
          "name": "Haoran Yin",
          "hidden": false
        },
        {
          "_id": "67b54b04bd51b4e46e39d28d",
          "name": "Wenyu Liu",
          "hidden": false
        },
        {
          "_id": "67b54b04bd51b4e46e39d28e",
          "name": "Xinggang Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-18T18:59:57.000Z",
      "title": "Marcimoderal Maba : Spectre des États Marcimoderal pour Modèles Décodificateurs dans les Processus de Chaleur en 2D Linéaires",
      "summary": "Récemment, les modèles de langage multimodal de haut niveau (MLLM) ont atteint un rendement impressionnant, bien que des augmentations de complexité informatique et de demande de cache de clé-valeur aient été observées, ainsi que des problèmes d'implémentation liés à la dépendance des encodeurs visuels. Nous proposons un cadre de développement appelé mmMamba, permettant la création de modèles de espaces de états multimodals avec une complexité de calcul intermédiaire, pour améliorer les MLLM actuels. Ce cadre permet de transformer directement les MLLM qui n'ont qu'un décodificateur entraîné en une architecture basée sur des RNN ou qui ne nécessite pas un encodeur visuel, avec une complexité de calcul linéaire. De plus, nous proposons une stratégie de point final pour créer Mamba à partir de Transformer et un récipient de trois étapes pour transmettre efficacement le savoir de Transformer à Mamba. Cette méthodologie permet de soutenir des architectures hybrides flexibles par la combinaison de couches de Transformer et de Mamba, ainsi que d'ajuster la continuité et le rendement. Dans HoVLE, le mmMamba-linear, après optimisation, présente un rendement compétitif avec les VLMs de complexité linéaire et ordonnée, tandis que le mmMamba-hybrid atteint un rendement plus élevé et s'approche des capacités de HoVLE. Sur un ensemble de 103K tokens, le mmMamba-linear offre un accélération de 20,6 fois et une réduction de 75,8% de la mémoire GPU par rapport à HoVLE, tandis que le mmMamba-hybrid atteint un accélération de 13,5 fois et une réduction de 60,2% de l'utilisation de mémoire. Les codes et modèles sont disponibles sur https://github.com/hustvl/mmMamba.",
      "upvotes": 18,
      "discussionId": "67b54b05bd51b4e46e39d2bb"
    },
    "publishedAt": "2025-02-18T22:08:27.750Z",
    "title": "Multimodal Mamba: Decoder-only Multimodal State Space Model via Quadratic to Linear Distillation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13145.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6577073fc2bf55b1f6bafb49",
      "avatarUrl": "/avatars/58803398b1a918b7570db17893e65122.svg",
      "fullname": "liao",
      "name": "LegendBC",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.11433",
      "authors": [
        {
          "_id": "67b54a644508bd0617598c21",
          "name": "Guojun Xiong",
          "hidden": false
        },
        {
          "_id": "67b54a644508bd0617598c22",
          "name": "Zhiyang Deng",
          "hidden": false
        },
        {
          "_id": "67b54a644508bd0617598c23",
          "name": "Keyi Wang",
          "hidden": false
        },
        {
          "_id": "67b54a644508bd0617598c24",
          "name": "Yupeng Cao",
          "hidden": false
        },
        {
          "_id": "67b54a644508bd0617598c25",
          "name": "Haohang Li",
          "hidden": false
        },
        {
          "_id": "67b54a644508bd0617598c26",
          "name": "Yangyang Yu",
          "hidden": false
        },
        {
          "_id": "67b54a644508bd0617598c27",
          "name": "Xueqing Peng",
          "hidden": false
        },
        {
          "_id": "67b54a644508bd0617598c28",
          "name": "Mingquan Lin",
          "hidden": false
        },
        {
          "_id": "67b54a644508bd0617598c29",
          "name": "Kaleb E Smith",
          "hidden": false
        },
        {
          "_id": "67b54a644508bd0617598c2a",
          "name": "Xiao-Yang Liu",
          "hidden": false
        },
        {
          "_id": "67b54a644508bd0617598c2b",
          "user": {
            "_id": "63b58ed5889aa6707f0bb0f4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b58ed5889aa6707f0bb0f4/9-6SJBOLdqUoc2LrKsI6y.jpeg",
            "isPro": true,
            "fullname": "Jimin Huang",
            "user": "jiminHuang",
            "type": "user"
          },
          "name": "Jimin Huang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-19T09:01:03.181Z",
          "hidden": false
        },
        {
          "_id": "67b54a644508bd0617598c2c",
          "name": "Sophia Ananiadou",
          "hidden": false
        },
        {
          "_id": "67b54a644508bd0617598c2d",
          "name": "Qianqian Xie",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-17T04:45:53.000Z",
      "title": "FLAG-Trader : Apprentissage par Référence avec Base de Gradient pour le Financement des Investissements avec la Fusion LLM-Agent",
      "summary": "L'ajuste à plusieurs types de données financières d'un modèle de langage grand (LLMs) montre un rendement impressionnant dans diverses opérations financières. Cependant, pour des scénarios progressifs et des objectifs dans un marché financier interactive, un approche complexe est nécessaire pour les sorties, ce qui peut être difficile pour améliorer les décisions. Pour y remédier, on propose FLAG-Trader. FLAG-Trader est une série d'architectures qui intègrent le traitement du langage (via les LLMs) et l'optimisation de politiques basée sur des gradients d'apprentissage par renforcement (RL). Les LLMs ajustés utilisent des connaissances existantes et fonctionnent comme des réseaux de politiques de manière efficace avec des paramètres ajustés sur des données financières. Par l'optimisation de politiques basée sur la compensation du trading, notre cadre améliore le rendement des LLMs dans le trading et peut améliorer les résultats d'autres tâches basées sur des données financières. Ces améliorations sont démontrées par des données expérimentales larges.",
      "upvotes": 18,
      "discussionId": "67b54a654508bd0617598c7e"
    },
    "publishedAt": "2025-02-18T22:06:19.200Z",
    "title": "FLAG-Trader: Fusion LLM-Agent with Gradient-based Reinforcement Learning for Financial Trading",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63b58ed5889aa6707f0bb0f4/2C9mhT-1Qz14hik7sxjf2.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.11433.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63b58ed5889aa6707f0bb0f4",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b58ed5889aa6707f0bb0f4/9-6SJBOLdqUoc2LrKsI6y.jpeg",
      "fullname": "Jimin Huang",
      "name": "jiminHuang",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.09245",
      "authors": [
        {
          "_id": "67b57a993d4f319f1fa9424b",
          "name": "Gleb Gerasimov",
          "hidden": false
        },
        {
          "_id": "67b57a993d4f319f1fa9424c",
          "user": {
            "_id": "63ed5676684767daecac6f8a",
            "avatarUrl": "/avatars/d0e4a715f9c3fb6d74c183bab751ec35.svg",
            "isPro": false,
            "fullname": "Yaroslav Aksenov",
            "user": "yaraksen",
            "type": "user"
          },
          "name": "Yaroslav Aksenov",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-19T09:00:41.123Z",
          "hidden": false
        },
        {
          "_id": "67b57a993d4f319f1fa9424d",
          "user": {
            "_id": "60b364e7f88532cd79eaff7b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654185363389-60b364e7f88532cd79eaff7b.jpeg",
            "isPro": false,
            "fullname": "Nikita Balagansky",
            "user": "elephantmipt",
            "type": "user"
          },
          "name": "Nikita Balagansky",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-19T09:33:26.858Z",
          "hidden": false
        },
        {
          "_id": "67b57a993d4f319f1fa9424e",
          "name": "Viacheslav Sinii",
          "hidden": false
        },
        {
          "_id": "67b57a993d4f319f1fa9424f",
          "user": {
            "_id": "62a9c8edc19f92ae443ab37f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669110208492-62a9c8edc19f92ae443ab37f.png",
            "isPro": false,
            "fullname": "Daniil Gavrilov",
            "user": "kefirski",
            "type": "user"
          },
          "name": "Daniil Gavrilov",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-19T09:00:43.143Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-13T12:00:50.000Z",
      "title": "Vous n'utilisez pas complètement l'exprimé de Transformer.",
      "summary": "A différence des RNN, les Transformers peuvent traiter directement tous les tokens antérieurs. Cependant, les Transformers standards utilisent uniquement la représentation de la couche précédente. Cet article montre que ces décisions de conception peuvent entraîner la déstruction de la représentation et limiter le rendement optimal. Pour résoudre ce problème, on présente la Layer-Integrated Memory (LIMe). LIMe élargit la représentation en permettant l'accès aux états cachés des couches précédentes tout en maintenant l'état de mémoire global du modèle. Grâce à des expériences larges qui combinent différentes architectures et structures de recherche, LIMe montre un améliorament uniforme dans une variété de tâches. De plus, l'analyse de la dynamique des représentations apprises et l'exploration de circuits en profondeur démontrent comment LIMe intègre l'information entre couches, offrant une orientation pour futures recherches.",
      "upvotes": 12,
      "discussionId": "67b57a9a3d4f319f1fa94274"
    },
    "publishedAt": "2025-02-19T03:03:51.930Z",
    "title": "You Do Not Fully Utilize Transformer's Representation Capacity",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63ed5676684767daecac6f8a/tZDsnW0gjHoYCpbZ-wwJi.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.09245.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63ed5676684767daecac6f8a",
      "avatarUrl": "/avatars/d0e4a715f9c3fb6d74c183bab751ec35.svg",
      "fullname": "Yaroslav Aksenov",
      "name": "yaraksen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.13130",
      "authors": [
        {
          "_id": "67b5625fb27eb6046b2ceec5",
          "name": "Jianwei Yang",
          "hidden": false
        },
        {
          "_id": "67b5625fb27eb6046b2ceec6",
          "name": "Reuben Tan",
          "hidden": false
        },
        {
          "_id": "67b5625fb27eb6046b2ceec7",
          "name": "Qianhui Wu",
          "hidden": false
        },
        {
          "_id": "67b5625fb27eb6046b2ceec8",
          "name": "Ruijie Zheng",
          "hidden": false
        },
        {
          "_id": "67b5625fb27eb6046b2ceec9",
          "name": "Baolin Peng",
          "hidden": false
        },
        {
          "_id": "67b5625fb27eb6046b2ceeca",
          "name": "Yongyuan Liang",
          "hidden": false
        },
        {
          "_id": "67b5625fb27eb6046b2ceecb",
          "name": "Yu Gu",
          "hidden": false
        },
        {
          "_id": "67b5625fb27eb6046b2ceecc",
          "name": "Mu Cai",
          "hidden": false
        },
        {
          "_id": "67b5625fb27eb6046b2ceecd",
          "name": "Seonghyeon Ye",
          "hidden": false
        },
        {
          "_id": "67b5625fb27eb6046b2ceece",
          "name": "Joel Jang",
          "hidden": false
        },
        {
          "_id": "67b5625fb27eb6046b2ceecf",
          "name": "Yuquan Deng",
          "hidden": false
        },
        {
          "_id": "67b5625fb27eb6046b2ceed0",
          "name": "Lars Liden",
          "hidden": false
        },
        {
          "_id": "67b5625fb27eb6046b2ceed1",
          "name": "Jianfeng Gao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-18T18:55:21.000Z",
      "title": "Magma : Modèle de base d'agent AI de DamoPai",
      "summary": "Le magma est une extension importante du modèle de langage visuel (VL), capable de maintenir sa capacité de compréhension visuelle (capacité cognitive de la vision) tout en acquirant des habiletés de planification et d'action dans l'espace visuel (capacité cognitive du temps spatial). De plus, il peut traiter des tâches de navigation des interfaces utilisateur (UI) jusqu'à la manipulation de robots. Pour acquérir ces habiletés à la sortie du boîtier, le magma est entraîné à grande échelle avec des ensembles de données variés, allant des images et des vidéos aux données de robots. Dans les images, il étiquette les objets visuels pouvant être actionnés (par exemple, les boutons cliquables dans une interface graphique) avec un Set-of-Mark (SoM), et dans les vidéos, il étiquette le mouvement des objets (par exemple, les traces des mains d'une personne ou celles des bras d'un robot) avec un Trace-of-Mark (ToM). Les expériences prolongées montrent que les SoM et les ToM s'intègrent bien, aidant à l'acquisition de la capacité cognitive du temps spatial du modèle de magma. Cela constitue la base pour une large gamme de tâches, comme le montre la figure 1. En particulier, le magma dépasse les modèles précédents spécifiques à ces tâches et obtient de nouveaux résultats de pointe en matière de navigation des UI et de manipulation de robots. Dans différentes tâches liées aux images et aux vidéos, le magma est également populaire par rapport aux modèles à grande échelle et est adapté à l'entraînement avec des ensembles de données similaires. Notre modèle et notre code sont disponibles et peuvent être reproduits de manière reproductible en suivant le URL suivant : https://microsoft.github.io/Magma",
      "upvotes": 11,
      "discussionId": "67b56265b27eb6046b2cf08f"
    },
    "publishedAt": "2025-02-18T23:51:36.910Z",
    "title": "Magma: A Foundation Model for Multimodal AI Agents",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13130.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6142
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.12513",
      "authors": [
        {
          "_id": "67b545fd88527668fa8bcc14",
          "name": "Tiancheng Gu",
          "hidden": false
        },
        {
          "_id": "67b545fd88527668fa8bcc15",
          "name": "Kaicheng Yang",
          "hidden": false
        },
        {
          "_id": "67b545fd88527668fa8bcc16",
          "name": "Chaoyi Zhang",
          "hidden": false
        },
        {
          "_id": "67b545fd88527668fa8bcc17",
          "name": "Yin Xie",
          "hidden": false
        },
        {
          "_id": "67b545fd88527668fa8bcc18",
          "name": "Xiang An",
          "hidden": false
        },
        {
          "_id": "67b545fd88527668fa8bcc19",
          "name": "Ziyong Feng",
          "hidden": false
        },
        {
          "_id": "67b545fd88527668fa8bcc1a",
          "name": "Dongnan Liu",
          "hidden": false
        },
        {
          "_id": "67b545fd88527668fa8bcc1b",
          "name": "Weidong Cai",
          "hidden": false
        },
        {
          "_id": "67b545fd88527668fa8bcc1c",
          "name": "Jiankang Deng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-18T03:58:38.000Z",
      "title": "RealSyn : Paradigme efficace et extensible de document transformateur multimodal",
      "summary": "Après un entraînement préalable sur de grandes paires d'images et de texte, CLIP (Contrastive Language-Image Pre-training) montre un rendement attendu dans différents cadres de référence, mais ne profite pas pleinement de la grande quantité de données non paires pour l'apprentissage de représentations visuelles. Pour maximiser l'utilisation de ces données non paires, un premier pas consiste à construire un pipeline d'extraction de données de la réalité et à extraire des images de haute qualité et des textes. Ensuite, un méthode de recherche hiérarchique est conçue pour associer efficacement chaque image à des textes liés textuellement. De plus, un module d'expansion syntaxique est proposé pour générer des textes synthétiques qui favorisent l'amélioration de l'information visuelle. Une stratégie d'échantillonnage syntaxique équilibré est également introduite pour augmenter la diversité du jeu de données et améliorer l'apprentissage de concepts de longue portée. Sur la base de ces innovations, le jeu de données RealSyn est construit, qui combine des textes réels et synthétiques, et est proposé en trois échelles : 15M, 30M et 100M. Les tests d'extension démontrent clairement que l'entraînement préalable sur RealSyn favorise l'apprentissage de représentations visuelles et montre une forte scalabilité. Les modèles pré-entraînés sur RealSyn atteignent les meilleurs rendements dans plusieurs tâches ultérieures. Les poids du jeu de données RealSyn et les modèles pré-entraînés sont disponibles sur https://github.com/deepglint/RealSyn pour encourager futures recherches.",
      "upvotes": 10,
      "discussionId": "67b545fe88527668fa8bcc65"
    },
    "publishedAt": "2025-02-18T21:52:22.326Z",
    "title": "RealSyn: An Effective and Scalable Multimodal Interleaved Document Transformation Paradigm",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.12513.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63e202f352b7578dba448ab5",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e202f352b7578dba448ab5/8itVBLcv14m7OVsoF8h1o.jpeg",
      "fullname": "Yang",
      "name": "Kaichengalex",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.12859",
      "authors": [
        {
          "_id": "67b576aa489d68b981e086ad",
          "name": "Chenxing Wei",
          "hidden": false
        },
        {
          "_id": "67b576aa489d68b981e086ae",
          "name": "Yao Shu",
          "hidden": false
        },
        {
          "_id": "67b576aa489d68b981e086af",
          "name": "Mingwen Ou",
          "hidden": false
        },
        {
          "_id": "67b576aa489d68b981e086b0",
          "name": "Ying Tiffany He",
          "hidden": false
        },
        {
          "_id": "67b576aa489d68b981e086b1",
          "name": "Fei Richard Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-18T13:46:47.000Z",
      "title": "PAFT : Ajuste micro de prompts non pertinents",
      "summary": "Les modèles de langue de haut niveau (LLMs) s'adaptent à des tâches ultérieures après un ajuste fin, mais cet ajustement peut être significativement affecté par des petites variations dans les prompts, ce qui peut détruire la robustesse des prompts. Pour aborder ce problème, on propose l'Ajuste Fin à Sans Connaissance du Prompt (PAFT). Le PAFT est un approche simple et efficace qui ajuste dynamiquement le prompt pendant l'ajuste fin. Ainsi, le modèle apprend les principes qui fondent le problème et évite l'ajuste trop forte à la représentation spécifique d'un prompt. Le PAFT fonctionne en deux étapes : d'abord, on construit différents ensembles de candidats de prompts synthétiques et significatifs. Ensuite, on sélectionne de manière aléatoire les prompts pour générer des entrées d'apprentissage dynamiques. Dans des expériences larges avec différents ensembles de données et de modèles de LLMs, il a été démontré que les modèles entraînés avec PAFT montrent une forte robustesse et généralisation vers des prompts non vus. Cette augmentation de robustesse améliore tant le rendement du modèle que la vitesse d'inférence, tout en maintenant l'efficacité de l'apprentissage. Les expériences de disparition du prompt confirment l'efficacité du PAFT.",
      "upvotes": 8,
      "discussionId": "67b576aa489d68b981e08708"
    },
    "publishedAt": "2025-02-19T01:21:54.836Z",
    "title": "PAFT: Prompt-Agnostic Fine-Tuning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.12859.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65ed3051492a7f35db21fea2",
      "avatarUrl": "/avatars/4fc0ccc21aa88e4e8ff74a6f850570b8.svg",
      "fullname": "Chenxing Wei",
      "name": "kittttttt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.12170",
      "authors": [
        {
          "_id": "67b5434f2b2ec6908fffe75e",
          "name": "Da Xiao",
          "hidden": false
        },
        {
          "_id": "67b5434f2b2ec6908fffe75f",
          "name": "Qingye Meng",
          "hidden": false
        },
        {
          "_id": "67b5434f2b2ec6908fffe760",
          "name": "Shengping Li",
          "hidden": false
        },
        {
          "_id": "67b5434f2b2ec6908fffe761",
          "name": "Xingyuan Yuan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-13T10:26:27.000Z",
      "title": "MUDDFormer : Détruire le résiduel du bouton de soudure de Transformer par couplage dynamique étroit multidirectionnel",
      "summary": "Nous proposons la connexion MUltiway Dynamic Dense (MUDD) pour résoudre les limites des connexions résiduelles et améliorer le flux d'information entre les couches dans les Transformers de manière simple et efficace. À différence des méthodes de connexion dense existantes, MUDD génère des poids de connexion dynamiquement pour chaque flux d'entrée séparé (consultation, clé, valeur ou résiduel) à chaque position de la séquence de chaque bloc de Transformer. La connexion MUDD peut être intégrée facilement dans n'importe quelle partie de l'architecture de Transformer et permet la création de MUDDFormer. Des expériences extensives montrent que MUDDFormer dépasse significativement les Transformers dans des modèles de différentes architectures et échelles de modélisation de langage, atteignant des résultats que les Transformers entraînés avec 1.8X-2.4X plus de calculs ne pourraient pas atteindre. En particulier, MUDDPythia-2.8B coïncide dans le pré-entraînement et les tâches d'inférence avec Pythia-6.9B, concurrence dans la configuration 5-shot avec Pythia-12B, et ajoute seulement 0.23% de paramètres et 0.4% de calculs. Les codes et modèles pré-entraînés en JAX et PyTorch sont disponibles sur https://github.com/Caiyun-AI/MUDDFormer.",
      "upvotes": 6,
      "discussionId": "67b543502b2ec6908fffe788"
    },
    "publishedAt": "2025-02-18T22:59:16.530Z",
    "title": "MUDDFormer: Breaking Residual Bottlenecks in Transformers via Multiway Dynamic Dense Connections",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.12170.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62d77440bad37ef354028365",
      "avatarUrl": "/avatars/df0dea879e06fa814867e9aad03d1e68.svg",
      "fullname": "Da Xiao",
      "name": "xiaoda99",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.12215",
      "authors": [
        {
          "_id": "67b56007fa141a55e51d9d78",
          "name": "Zhiyuan Zeng",
          "hidden": false
        },
        {
          "_id": "67b56007fa141a55e51d9d79",
          "name": "Qinyuan Cheng",
          "hidden": false
        },
        {
          "_id": "67b56007fa141a55e51d9d7a",
          "name": "Zhangyue Yin",
          "hidden": false
        },
        {
          "_id": "67b56007fa141a55e51d9d7b",
          "name": "Yunhua Zhou",
          "hidden": false
        },
        {
          "_id": "67b56007fa141a55e51d9d7c",
          "name": "Xipeng Qiu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-17T07:21:11.000Z",
      "title": "Nuevamente, on examine l'escalage des temps de test du modèle O1 de Rashiin : ont-ils vraiment la capacité d'escalager les temps de test ?",
      "summary": "Dans les modèles de langue grand (LLMs), comme le montre la série o1 de OpenAI, il est possible d'améliorer la capacité de calcul des ressources de calcul dans l'inférence pour développer des habiletés logiques. En examinant des modèles plus récents comme Deepseek-R1 (R1) et LIMIT, on peut constater que ces modèles répètent ces améliorations, mais leur capacité à s'écheller n'a pas encore été étudiée dans les tests. Dans cette étude, on montre que la longueur des CoTs (compteurs logiques) dans des modèles comme o1 ne améliore pas la précision de manière constante. En fait, parfois, la réponse correcte apparaît plus rapidement que la mauvaise. Ce phénomène est étroitement lié à la capacité d'autocorrige du modèle. Les CoTs plus longs contiennent plus d'autocorriges, ce qui généralement réduit leur performance. En comparant des permutations et des échelles parallèles dans R1 et LIMIT, on conclut que l'échelle parallèle réalise un meilleur couverture et moins d'erreurs dans l'échelle. En se basant sur cette observation, on propose un méthode qui combine l'échelle parallèle et les caractéristiques de la longueur des CoTs, appelée \"votation par la majorité minimale\", et on montre que cette méthode améliore significativement la capacité d'échelle dans les tests par rapport au méthode traditionnelle de votation par la majorité.",
      "upvotes": 5,
      "discussionId": "67b56007fa141a55e51d9da7"
    },
    "publishedAt": "2025-02-18T23:37:46.756Z",
    "title": "Revisiting the Test-Time Scaling of o1-like Models: Do they Truly Possess Test-Time Scaling Capabilities?",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.12215.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6142
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.12501",
      "authors": [
        {
          "_id": "67b547ffc9071a3e97139532",
          "user": {
            "_id": "62a42f22c683d02f5b63320c",
            "avatarUrl": "/avatars/bc611abe9c4ef8d378123cb8ac9fdbf2.svg",
            "isPro": false,
            "fullname": "Qiyuan Zhang",
            "user": "DonJoey",
            "type": "user"
          },
          "name": "Qiyuan Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-19T09:01:10.215Z",
          "hidden": false
        },
        {
          "_id": "67b547ffc9071a3e97139533",
          "name": "Yufei Wang",
          "hidden": false
        },
        {
          "_id": "67b547ffc9071a3e97139534",
          "user": {
            "_id": "63c20105726f62e411fbe882",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63c20105726f62e411fbe882/2UsU9O2psbDjJzz-sAmGH.jpeg",
            "isPro": false,
            "fullname": "Yuxin Jiang",
            "user": "YuxinJiang",
            "type": "user"
          },
          "name": "Yuxin Jiang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-19T09:01:08.101Z",
          "hidden": false
        },
        {
          "_id": "67b547ffc9071a3e97139535",
          "name": "Liangyou Li",
          "hidden": false
        },
        {
          "_id": "67b547ffc9071a3e97139536",
          "name": "Chuhan Wu",
          "hidden": false
        },
        {
          "_id": "67b547ffc9071a3e97139537",
          "name": "Yasheng Wang",
          "hidden": false
        },
        {
          "_id": "67b547ffc9071a3e97139538",
          "name": "Xin Jiang",
          "hidden": false
        },
        {
          "_id": "67b547ffc9071a3e97139539",
          "name": "Lifeng Shang",
          "hidden": false
        },
        {
          "_id": "67b547ffc9071a3e9713953a",
          "name": "Ruiming Tang",
          "hidden": false
        },
        {
          "_id": "67b547ffc9071a3e9713953b",
          "name": "Fuyuan Lyu",
          "hidden": false
        },
        {
          "_id": "67b547ffc9071a3e9713953c",
          "name": "Chen Ma",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-18T03:31:06.000Z",
      "title": "Raisons de la comparaison de clusters : Comprendre l'évaluation intégrale de LLM-as-Judge",
      "summary": "LLM-as-a-Judge est un méthode d'évaluation automatique largement appliquée qui génère une série de raisonnements de type \"chain-of-thought\" (CoT). Cependant, la confiance dans cette méthode est compromise parce que les CoT ne peuvent pas comprendre complètement les détails, ce qui peut conduire à la génération de résultats incomplets. Actuellement, les méthodes principales se basent sur le vote majoritaire ou sur l'extension des critères d'évaluation, mais ne résolvent pas les limites des CoT. Nous proposons l'Évaluation Comparative Basée sur la Fouille. Ce méthode révèle les détails des réponses candidates en les comparant à des réponses supplémentaires générées par code. Ce processus encourage LLM-as-a-Judge à fournir une évaluation détaillée de CoT. Au travers d'une large gamme d'expérimentations, notre approche a amélioré la confiance dans l'évaluation et augmenté la précision d'un moyen de 6,7% sur 5 cadres de référence. De plus, notre méthode utilise l'expérience de l'évaluation pour générer des CoT de haute qualité et améliorer la convergence d'entraînement (SFT), connue sous le nom de \"convergence des échantillons de code\", facilitant l'exécution efficace de la SFT. L'analyse montre que les CoT générés par nous sont détaillés et de haute qualité, et que la précision de l'évaluation s'améliore avec l'escalade de la complexité de l'inférence.",
      "upvotes": 5,
      "discussionId": "67b54800c9071a3e9713956c"
    },
    "publishedAt": "2025-02-18T21:55:26.822Z",
    "title": "Crowd Comparative Reasoning: Unlocking Comprehensive Evaluations for LLM-as-a-Judge",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.12501.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62a42f22c683d02f5b63320c",
      "avatarUrl": "/avatars/bc611abe9c4ef8d378123cb8ac9fdbf2.svg",
      "fullname": "Qiyuan Zhang",
      "name": "DonJoey",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.11271",
      "authors": [
        {
          "_id": "67b4322c217ec18a40587bec",
          "user": {
            "_id": "60f5f68fa7fd83d025749234",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60f5f68fa7fd83d025749234/gCeJAZfzaANAcEvI6v5-P.jpeg",
            "isPro": false,
            "fullname": "Pan Lu",
            "user": "lupantech",
            "type": "user"
          },
          "name": "Pan Lu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-19T09:04:43.677Z",
          "hidden": false
        },
        {
          "_id": "67b4322c217ec18a40587bed",
          "name": "Bowen Chen",
          "hidden": false
        },
        {
          "_id": "67b4322c217ec18a40587bee",
          "name": "Sheng Liu",
          "hidden": false
        },
        {
          "_id": "67b4322c217ec18a40587bef",
          "name": "Rahul Thapa",
          "hidden": false
        },
        {
          "_id": "67b4322c217ec18a40587bf0",
          "name": "Joseph Boen",
          "hidden": false
        },
        {
          "_id": "67b4322c217ec18a40587bf1",
          "name": "James Zou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-16T21:18:47.000Z",
      "title": "OctoTools : Marque agile avec des outils extensibles pour aborder des problèmes complexes de logique et de raisonnement.",
      "summary": "Pour résoudre des tâches logiques complexes, on inclut des éléments comme la compréhension visuelle, la recherche de connaissances dans des domaines spécifiques, les calculs numériques et la logique multiniveau. Les méthodes actuelles renforcent les modèles de langage à grande échelle (LLMs) en utilisant des outils externes, mais présentent des limitations dans des domaines spécifiques et nécessitent des données de formation supplémentaires. Dans cet article, nous présentons OctoTools, un cadre d'agent ouvert, nécessitant peu de formation, utilisateur-amiable et facilement extensible. Ce cadre est conçu pour résoudre des tâches logiques complexes dans différentes domaines. OctoTools inclut des fonctions de tools via des cartes standards de tools, un planificateur pour la planification à haut et bas niveau, et une fonction d'exécution pour utiliser les tools. OctoTools a démontré sa généralité sur 16 tâches différentes, comme MathVista, MMLU-Pro, MedQA et GAIA-Text, avec une augmentation significative de la précision moyenne (9,3%) par rapport à GPT-4o. De plus, en utilisant la même collection de tools, OctoTools dépasse AutoGen, GPT-Functions et LangChain avec un accroissement maximum de 10,6%. A travers des analyses détaillées et des tests d'élimination, OctoTools montre des résultats exceptionnels dans la planification de tâches, l'utilisation de tools valides et la résolution de problèmes multiniveaux.",
      "upvotes": 4,
      "discussionId": "67b4322d217ec18a40587c27"
    },
    "publishedAt": "2025-02-19T02:27:36.940Z",
    "title": "OctoTools: An Agentic Framework with Extensible Tools for Complex Reasoning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.11271.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "60f5f68fa7fd83d025749234",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60f5f68fa7fd83d025749234/gCeJAZfzaANAcEvI6v5-P.jpeg",
      "fullname": "Pan Lu",
      "name": "lupantech",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.09838",
      "authors": [
        {
          "_id": "67b55078a64445f58c771d84",
          "name": "Tianwei Lin",
          "hidden": false
        },
        {
          "_id": "67b55078a64445f58c771d85",
          "name": "Wenqiao Zhang",
          "hidden": false
        },
        {
          "_id": "67b55078a64445f58c771d86",
          "name": "Sijing Li",
          "hidden": false
        },
        {
          "_id": "67b55078a64445f58c771d87",
          "name": "Yuqian Yuan",
          "hidden": false
        },
        {
          "_id": "67b55078a64445f58c771d88",
          "name": "Binhe Yu",
          "hidden": false
        },
        {
          "_id": "67b55078a64445f58c771d89",
          "name": "Haoyuan Li",
          "hidden": false
        },
        {
          "_id": "67b55078a64445f58c771d8a",
          "name": "Wanggui He",
          "hidden": false
        },
        {
          "_id": "67b55078a64445f58c771d8b",
          "name": "Hao Jiang",
          "hidden": false
        },
        {
          "_id": "67b55078a64445f58c771d8c",
          "name": "Mengze Li",
          "hidden": false
        },
        {
          "_id": "67b55078a64445f58c771d8d",
          "name": "Xiaohui Song",
          "hidden": false
        },
        {
          "_id": "67b55078a64445f58c771d8e",
          "name": "Siliang Tang",
          "hidden": false
        },
        {
          "_id": "67b55078a64445f58c771d8f",
          "name": "Jun Xiao",
          "hidden": false
        },
        {
          "_id": "67b55078a64445f58c771d90",
          "name": "Hui Lin",
          "hidden": false
        },
        {
          "_id": "67b55078a64445f58c771d91",
          "name": "Yueting Zhuang",
          "hidden": false
        },
        {
          "_id": "67b55078a64445f58c771d92",
          "name": "Beng Chin Ooi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-14T00:42:36.000Z",
      "title": "Salut GPT : Modèle de langage visuolinguistique à grande échelle pour les usages médicaux\nIntégration par compréhension et génération, adaptant différents savoirs entre eux",
      "summary": "BonjourGPT, une puissante version d'un grand modèle de langage visuolinguistique médical (Med-LVLM) est présentée. Cette capacité est intégrée dans un paradigme de récupération automatique unifié, permettant la compréhension et la génération de visions médicales. Notre philosophie initiale est l'application progressive des compréhensions et des connaissances dans le domaine de la computation et de la génération, appliquées à des modèles de langage grands précédemment entraînés (LLMs). Cela a été réalisé grâce à la nouvelle technique H-LoRA, complétée par un approche de reconnaissance visuelle progressive et une stratégie d'entraînement en trois étapes. Pour l'apprentissage efficace de HolaGPT, un VL-Health a été conçu, un ensemble de données spécifique et détaillé dans le domaine médical, qui combine compréhension et génération. Les résultats des expérimentations montrent que HolaGPT démontre une excellente performance et une échellabilité dans des tâches d'intégration visuelle médicale. Ce projet est disponible sur https://github.com/DCDmllm/HealthGPT.",
      "upvotes": 4,
      "discussionId": "67b5507aa64445f58c771df9"
    },
    "publishedAt": "2025-02-18T22:35:23.066Z",
    "title": "HealthGPT: A Medical Large Vision-Language Model for Unifying Comprehension and Generation via Heterogeneous Knowledge Adaptation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.09838.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65fc18edfb66882aba4d548e",
      "avatarUrl": "/avatars/f70d47fe4aba98b5a5cd64f7e002dfd2.svg",
      "fullname": "wenqiao",
      "name": "wannature",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.12574",
      "authors": [
        {
          "_id": "67b547f555d0424a31b9c384",
          "user": {
            "_id": "64cb48f7667f4f808535107e",
            "avatarUrl": "/avatars/8f77f378ad665b246e1ea3aaba2153ae.svg",
            "isPro": false,
            "fullname": "chengluo",
            "user": "wdlctc",
            "type": "user"
          },
          "name": "Cheng Luo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:40:25.130Z",
          "hidden": false
        },
        {
          "_id": "67b547f555d0424a31b9c385",
          "user": {
            "_id": "64b15284372d4340772a3dca",
            "avatarUrl": "/avatars/417d5f1bc1bcb5e4d5de6169673c2cf7.svg",
            "isPro": false,
            "fullname": "Zefan Cai",
            "user": "ZefanCai",
            "type": "user"
          },
          "name": "Zefan Cai",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:40:47.077Z",
          "hidden": false
        },
        {
          "_id": "67b547f555d0424a31b9c386",
          "name": "Hanshi Sun",
          "hidden": false
        },
        {
          "_id": "67b547f555d0424a31b9c387",
          "user": {
            "_id": "64c15c5bea792b1950e302e4",
            "avatarUrl": "/avatars/51f84365cc08a1dcd5da70968389aed2.svg",
            "isPro": false,
            "fullname": "Jinqi Xiao",
            "user": "jinqixiao",
            "type": "user"
          },
          "name": "Jinqi Xiao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:41:01.931Z",
          "hidden": false
        },
        {
          "_id": "67b547f555d0424a31b9c388",
          "name": "Bo Yuan",
          "hidden": false
        },
        {
          "_id": "67b547f555d0424a31b9c389",
          "name": "Wen Xiao",
          "hidden": false
        },
        {
          "_id": "67b547f555d0424a31b9c38a",
          "user": {
            "_id": "675f8271a63fff7b5bcbc478",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/9tJn7NyzLMreCJVH4wRho.png",
            "isPro": false,
            "fullname": "Junjie Hu",
            "user": "junjiehu",
            "type": "user"
          },
          "name": "Junjie Hu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:41:18.304Z",
          "hidden": false
        },
        {
          "_id": "67b547f555d0424a31b9c38b",
          "name": "Jiawei Zhao",
          "hidden": false
        },
        {
          "_id": "67b547f555d0424a31b9c38c",
          "user": {
            "_id": "64b732f832403871593e082c",
            "avatarUrl": "/avatars/dd21932b0c167131ee7545a622c46c3c.svg",
            "isPro": false,
            "fullname": "Beidi Chen",
            "user": "beidic",
            "type": "user"
          },
          "name": "Beidi Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:39:20.563Z",
          "hidden": false
        },
        {
          "_id": "67b547f555d0424a31b9c38d",
          "user": {
            "_id": "6532920b3e385cfc6002938d",
            "avatarUrl": "/avatars/cb9cc6d2733031582c83f56dc6cd1dd5.svg",
            "isPro": false,
            "fullname": "Anima Anandkumar",
            "user": "animakumar",
            "type": "user"
          },
          "name": "Anima Anandkumar",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:39:15.091Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-18T06:26:05.000Z",
      "title": "HeadInfer : Méthode d'inférence efficace en mémoire pour les modèles LLM qui divise et exécute chaque couche du modèle de réseau neuronal de manière séparée",
      "summary": "Les modèles de langage grands basés sur le Transformer (LLMs) montrent un excellent rendement dans la génération de contextes longs. L'expansion de la longueur du contexte a déséquilibré la qualité de la mémoire des LLMs dans la cache des mots clés (KV cache). Dans cet article, nous proposons HEADINFER. HEADINFER évite de stocker complètement en la RAM du CPU la cache des mots clés de toutes les couches de Transformer. HEADINFER laisse dans la GPU la cache des mots clés des têtes d'attention sélectionnées, en calculant les sorties d'attention dynamiquement. A travers un analyse de la mémoire, nous démontrons que HEADINFER peut réduire significativement la qualité de la mémoire tout en maintenant l'efficience computationnelle. Nous avons évalué HEADINFER avec une séquence de 1,000,000 tokens dans le modèle Llama-3-8B. La qualité de la mémoire de la cache des mots clés dans la GPU a été réduite de 128GB à 1GB, et l'utilisation totale de la mémoire de la GPU a été réduite de 207GB à 17GB, atteignant un déclin de 92% dans la base d'inférence linéaire BF16. En particulier, HEADINFER permet de réaliser une inférence de 4,000,000 tokens sur un GPU de consommation de 24GB (par exemple, NVIDIA RTX 4090) pour un modèle de 8B. De plus, il évite l'utilisation de méthodes approximatives.",
      "upvotes": 3,
      "discussionId": "67b547f755d0424a31b9c3e5"
    },
    "publishedAt": "2025-02-18T21:57:00.289Z",
    "title": "HeadInfer: Memory-Efficient LLM Inference by Head-wise Offloading",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.12574.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64cb48f7667f4f808535107e",
      "avatarUrl": "/avatars/8f77f378ad665b246e1ea3aaba2153ae.svg",
      "fullname": "chengluo",
      "name": "wdlctc",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.13063",
      "authors": [
        {
          "_id": "67b5a7896f72266cb765e744",
          "user": {
            "_id": "618b9540682ec1c38327e586",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/618b9540682ec1c38327e586/v_ZBkfh8O9Zh6C2YQpuBX.jpeg",
            "isPro": false,
            "fullname": "Yury Kuratov",
            "user": "yurakuratov",
            "type": "user"
          },
          "name": "Yuri Kuratov",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-02-19T09:42:34.422Z",
          "hidden": false
        },
        {
          "_id": "67b5a7896f72266cb765e745",
          "name": "Mikhail Arkhipov",
          "hidden": false
        },
        {
          "_id": "67b5a7896f72266cb765e746",
          "name": "Aydar Bulatov",
          "hidden": false
        },
        {
          "_id": "67b5a7896f72266cb765e747",
          "user": {
            "_id": "639c6e978a34ed9a404c6a7b",
            "avatarUrl": "/avatars/c98ca8c9f9ed8509c2f1bb6aa994fd57.svg",
            "isPro": false,
            "fullname": "MIKHAIL BURTSEV",
            "user": "mbur",
            "type": "user"
          },
          "name": "Mikhail Burtsev",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-19T09:56:59.080Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-18T17:08:45.000Z",
      "title": "Nous avons ajouté le token CLAMING 1568 dans un seul vecteur et nous l'avons revu : cela explore les limites de l'espace embarras.",
      "summary": "Recentes études diverses cherchent à rendre la permutation de tokens comprise dans une permutation d'un vecteur de valeurs réelles courts, remplaçant l'embedding ou la clé-valeur du token. Cette approximation peut réduire la quantité de calculs des modèles de langage actuels. Cependant, lorsqu'un modèle fort comme un codageur est utilisé, le ratio de compression ne dépasse généralement pas l'x10. Ce fait est très intéressant. Théoriquement, la capacité d'information maximale d'un grand vecteur de valeurs réelles peut être beaucoup plus élevée, même avec une précision de 16 bits et un taille de vecteur légère, que la vitesse actuelle. Dans cet article, les codageurs sont remplacés par un processus d'optimisation par blocs pour explorer les limites de la compression et montrer des vecteurs qui peuvent être compressés jusqu'à x1500. De plus, la différence entre les solutions existantes et les pratiques utiles est clairement définie. Expérimentalement, il est démontré que la limitation de la compression dépend de la quantité d'incertitude réduite, plutôt que de la longueur de l'entrée. C'est-à-dire, la perte d'entropie croisée de ces permutations. Cette limite obtenue montre une grande différence entre la capacité théorique du codage de l'entrée et son utilisation pratique, montrant qu'il y a un grand potentiel d'optimisation dans le design de modèles.",
      "upvotes": 1,
      "discussionId": "67b5a78a6f72266cb765e779"
    },
    "publishedAt": "2025-02-19T04:43:42.973Z",
    "title": "Cramming 1568 Tokens into a Single Vector and Back Again: Exploring the Limits of Embedding Space Capacity",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13063.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "639c6e978a34ed9a404c6a7b",
      "avatarUrl": "/avatars/c98ca8c9f9ed8509c2f1bb6aa994fd57.svg",
      "fullname": "MIKHAIL BURTSEV",
      "name": "mbur",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.10708",
      "authors": [
        {
          "_id": "67b58e32e972a2806a9a0451",
          "user": {
            "_id": "65407ba7a38390065750233f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65407ba7a38390065750233f/1_IPMZbk-S9u2t18PQgMp.jpeg",
            "isPro": false,
            "fullname": "Zirui Song",
            "user": "Ziruibest",
            "type": "user"
          },
          "name": "Zirui Song",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-19T09:00:38.943Z",
          "hidden": false
        },
        {
          "_id": "67b58e32e972a2806a9a0452",
          "name": "Bin Yan",
          "hidden": false
        },
        {
          "_id": "67b58e32e972a2806a9a0453",
          "name": "Yuhan Liu",
          "hidden": false
        },
        {
          "_id": "67b58e32e972a2806a9a0454",
          "name": "Miao Fang",
          "hidden": false
        },
        {
          "_id": "67b58e32e972a2806a9a0455",
          "name": "Mingzhe Li",
          "hidden": false
        },
        {
          "_id": "67b58e32e972a2806a9a0456",
          "name": "Rui Yan",
          "hidden": false
        },
        {
          "_id": "67b58e32e972a2806a9a0457",
          "name": "Xiuying Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-15T07:43:43.000Z",
      "title": "Méthodologie d'Injection de Connaissances Spécifiques dans des Modèles de Langue de Grande Échelle : Recherche Cohérente",
      "summary": "Les modèles de langage grands (LLMs) ont démontré un extraordinaire succès dans des tâches telles que la compréhension du langage naturel, la résumé de texte et la traduction automatique. Cependant, leurs caractéristiques générales sont limitées dans leur efficacité dans des domaines spécifiques qui nécessitent des connaissances spécialisées, comme l'attention médicale, la chimie et l'analyse des lois. Par conséquent, les chercheurs cherchent des méthodes pour intégrer des connaissances spécifiques dans les LLMs et les renforcer. Dans cette revue, ces méthodes sont classées dans quatre approches principales : injection dynamique de connaissances, injection statique de connaissances, modularisation des décodificateurs et optimisation de la profondeur. On explique comment chaque approche fournit des connaissances spécifiques aux LLMs, équilibrant le compromis entre flexibilité, scalabilité et efficacité. Ces méthodes permettent aux LLMs d'adapter à des tâches spécifiques, en comparant leurs avantages et inconvénients, et en comparant les LLMs spécifiques aux généraux, clarifiant les problèmes et opportunités dans ce nouveau domaine. On recommande à ceux qui sont intéressés, mais pour ceux qui souhaitent un intérêt plus profond, on fournit une organisation des jeux de données et des cadres d'évaluation généralement utilisés : https://github.com/abilliyb/Knowledge_Injection_Survey_Papers, où sont enregistrés des études de recherche spécialisées sur les LLMs.",
      "upvotes": 1,
      "discussionId": "67b58e33e972a2806a9a04b8"
    },
    "publishedAt": "2025-02-19T02:56:09.510Z",
    "title": "Injecting Domain-Specific Knowledge into Large Language Models: A Comprehensive Survey",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.10708.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65407ba7a38390065750233f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65407ba7a38390065750233f/1_IPMZbk-S9u2t18PQgMp.jpeg",
      "fullname": "Zirui Song",
      "name": "Ziruibest",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.12669",
      "authors": [
        {
          "_id": "67b58c806e53744c2a373351",
          "user": {
            "_id": "63024676056ec3a2a8714b24",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1661093436322-noauth.jpeg",
            "isPro": false,
            "fullname": "Xiang Liu",
            "user": "Dominic789654",
            "type": "user"
          },
          "name": "Xiang Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:34:03.429Z",
          "hidden": false
        },
        {
          "_id": "67b58c806e53744c2a373352",
          "user": {
            "_id": "64eded5fdfe0a679d840bc98",
            "avatarUrl": "/avatars/4d4c67c13e547a4d296a301e8694e79e.svg",
            "isPro": false,
            "fullname": "sunpenglei",
            "user": "sunpenglei",
            "type": "user"
          },
          "name": "Penglei Sun",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:34:15.889Z",
          "hidden": false
        },
        {
          "_id": "67b58c806e53744c2a373353",
          "name": "Shuyan Chen",
          "hidden": false
        },
        {
          "_id": "67b58c806e53744c2a373354",
          "name": "Longhan Zhang",
          "hidden": false
        },
        {
          "_id": "67b58c806e53744c2a373355",
          "name": "Peijie Dong",
          "hidden": false
        },
        {
          "_id": "67b58c806e53744c2a373356",
          "name": "Huajie You",
          "hidden": false
        },
        {
          "_id": "67b58c806e53744c2a373357",
          "user": {
            "_id": "64473221dcbe1333b64b2db2",
            "avatarUrl": "/avatars/5e4495d3581ad3e6ea3c47650f20b993.svg",
            "isPro": false,
            "fullname": "yongqi zhang",
            "user": "yongqi2023",
            "type": "user"
          },
          "name": "Yongqi Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:35:12.059Z",
          "hidden": false
        },
        {
          "_id": "67b58c806e53744c2a373358",
          "name": "Chang Yan",
          "hidden": false
        },
        {
          "_id": "67b58c806e53744c2a373359",
          "user": {
            "_id": "6676935fcd0b89a0115174b0",
            "avatarUrl": "/avatars/4caca1b672d29e787814f9a30bf20bcc.svg",
            "isPro": false,
            "fullname": "Xiaowen Chu",
            "user": "wenxinsiju",
            "type": "user"
          },
          "name": "Xiaowen Chu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:35:20.611Z",
          "hidden": false
        },
        {
          "_id": "67b58c806e53744c2a37335a",
          "name": "Tong-yi Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-18T09:19:24.000Z",
      "title": "Perovskite-LLM : Modèle de langage d'intelligence artificielle de type LLM spécialisé dans les perovskites",
      "summary": "Les PSCs de Polovita Solar Cells (PSCs) ont connu un rapide développement, ce qui a entraîné une croissance exponentielle dans la quantité d'articles scientifiques publiés. Cela a soulevé une urgence pour des systèmes efficaces de gestion du savoir et de l'analyse logique dans ce domaine. Nous présentons un système d'expansion du savoir conçu pour des graphes de savoir spécifiques dans les PSCs. Ce système intègre trois composants principaux :\n\n1. **Développement d'un graphe de savoir spécialisé** : Un graphe de savoir a été construit sur la base de 1,517 articles de recherche, comprenant 23,789 entités et 22,272 relations.\n\n2. **Génération de deux ensembles de données d'interpolation** :\n   - **PSC샹** : Comprend 55,101 paires de questions et réponses de haute qualité générées par un nouveau cadre d'agents, offrant une large gamme de réponses.\n   - **PSC랩핑** : Contient 2,217 problèmes de science des matériaux soigneusement sélectionnés, offrant une base solide pour la recherche.\n\n3. **Utilisation de deux modèles de langage à grande échelle de savoir spécialisé** :\n   - **PSC샹 LLM** : Fournit un soutien au savoir spécialisé.\n   - **PSC랩핑 LLM** : Gère les tâches d'analyse logique en science.\n\nLes résultats des expériences montrent que ce système présente des performances significativement meilleures en matière de recherche spécialisée et d'analyse logique scientifique, comparés aux modèles actuels. De plus, il fournit aux chercheurs des outils efficaces pour la recherche d'articles, le conception d'expériences et la résolution de problèmes complexes.",
      "upvotes": 1,
      "discussionId": "67b58c826e53744c2a3733c2"
    },
    "publishedAt": "2025-02-19T02:47:33.654Z",
    "title": "Perovskite-LLM: Knowledge-Enhanced Large Language Models for Perovskite Solar Cell Research",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.12669.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63024676056ec3a2a8714b24",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1661093436322-noauth.jpeg",
      "fullname": "Xiang Liu",
      "name": "Dominic789654",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.10990",
      "authors": [
        {
          "_id": "67b3ee6c1e80a69e79c3155a",
          "user": {
            "_id": "647d834618274bce03013cc2",
            "avatarUrl": "/avatars/a95c7df96dc4fb6a96193f6dd5068227.svg",
            "isPro": true,
            "fullname": "yixuan",
            "user": "yixuantt",
            "type": "user"
          },
          "name": "Yixuan Tang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-19T09:04:50.969Z",
          "hidden": false
        },
        {
          "_id": "67b3ee6c1e80a69e79c3155b",
          "name": "Yi Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-16T04:23:52.000Z",
      "title": "FinMTEB : Maître de Benchmark de Financeur de Texte de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Financeur de Finance",
      "summary": "Les modèles d'embedding jouent un rôle crucial dans la représentation de l'information et dans la recherche dans diverses applications de l'IANLP. Le développement récent de grands modèles de langage (LLMs) a amélioré le rendement de ces modèles d'embedding. En général, ces modèles sont évalués sur des ensembles de données communs, mais les applications réelles dans le monde actuel nécessitent une évaluation dans des contextes de domaine. Dans cet article, nous présentons FinMTEB (Benchmark d'Embedding de Texte de Finance Massif), un conteneur spécialisé basé sur MTEB, qui s'adapte à l'industrie financière. FinMTEB comprend 7 tâches et couvre différents types d'énoncés en chinois et en anglais, tels que les articles de presse financière, les rapports annuels des entreprises, les rapports de CSR, les demandes de réglementation et les discours de réunions comptables. De plus, nous avons développé FinPersona-E5, un modèle adaptatif financier qui utilise une méthodologie de synthèse de données basée sur les personnes, et couvre une variété de tâches d'embedding financière. À travers des évaluations étendues de 15 modèles d'embedding, nous présentons trois principales conclusions : (1) le rendement sur les benchmarks généraux a une relation limitée avec les tâches de domaine financier, (2) les modèles adaptatifs pour le domaine financier dépassent clairement les modèles généraux, et (3) une simple approximation de \"bag of words\" (BoW) dépasse les techniques denses d'embedding dans les tâches de similarité textuelle (STS) financière, révélant les limites actuelles des technologies d'embedding denses. Cet article contribue à la construction d'un fort cadre d'évaluation pour les applications de l'IANLP dans le secteur financier et met en avant l'importance de développer des modèles d'embedding adaptés au domaine financier.",
      "upvotes": 0,
      "discussionId": "67b3ee6d1e80a69e79c3158f"
    },
    "publishedAt": "2025-02-19T04:54:27.788Z",
    "title": "FinMTEB: Finance Massive Text Embedding Benchmark",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.10990.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "647d834618274bce03013cc2",
      "avatarUrl": "/avatars/a95c7df96dc4fb6a96193f6dd5068227.svg",
      "fullname": "yixuan",
      "name": "yixuantt",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.13142",
      "authors": [
        {
          "_id": "67b5790132be608036ee94e5",
          "user": {
            "_id": "65c3fdf79d062be813813e45",
            "avatarUrl": "/avatars/52528a61abe5bbbef4a4a431944973cd.svg",
            "isPro": false,
            "fullname": "Dantong Niu",
            "user": "NdtSoCool",
            "type": "user"
          },
          "name": "Dantong Niu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:12:28.457Z",
          "hidden": false
        },
        {
          "_id": "67b5790132be608036ee94e6",
          "user": {
            "_id": "65406e82deee4716f1c29271",
            "avatarUrl": "/avatars/25331a773f8125f9ad1c3d6ac3375586.svg",
            "isPro": false,
            "fullname": "Yuvan Sharma",
            "user": "yuvansharma",
            "type": "user"
          },
          "name": "Yuvan Sharma",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:12:35.531Z",
          "hidden": false
        },
        {
          "_id": "67b5790132be608036ee94e7",
          "name": "Haoru Xue",
          "hidden": false
        },
        {
          "_id": "67b5790132be608036ee94e8",
          "user": {
            "_id": "650bd36a7c99ca283e58e973",
            "avatarUrl": "/avatars/606d24b2dac190ebcbb4b2a2e4671380.svg",
            "isPro": false,
            "fullname": "Giscard Biamby",
            "user": "gbiamby",
            "type": "user"
          },
          "name": "Giscard Biamby",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:12:49.219Z",
          "hidden": false
        },
        {
          "_id": "67b5790132be608036ee94e9",
          "name": "Junyi Zhang",
          "hidden": false
        },
        {
          "_id": "67b5790132be608036ee94ea",
          "user": {
            "_id": "66a09aec369dd38cf2113070",
            "avatarUrl": "/avatars/cc13bdd3dc1271d33b083b61e12f1a05.svg",
            "isPro": false,
            "fullname": "Ziteng Ji",
            "user": "zitengj0618",
            "type": "user"
          },
          "name": "Ziteng Ji",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:13:13.907Z",
          "hidden": false
        },
        {
          "_id": "67b5790132be608036ee94eb",
          "user": {
            "_id": "64cbdf02f103036e23d1c7f3",
            "avatarUrl": "/avatars/496069463900dea20929b57381182d39.svg",
            "isPro": false,
            "fullname": "Trevor Darrell",
            "user": "trevordarrell",
            "type": "user"
          },
          "name": "Trevor Darrell",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:13:20.379Z",
          "hidden": false
        },
        {
          "_id": "67b5790132be608036ee94ec",
          "user": {
            "_id": "667c5764186b27ef806636d3",
            "avatarUrl": "/avatars/5c08f0109bc0e350624112c0aff544f6.svg",
            "isPro": false,
            "fullname": "Roei Herzig",
            "user": "roeiherz",
            "type": "user"
          },
          "name": "Roei Herzig",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:13:26.134Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-18T18:59:01.000Z",
      "title": "Entraînement d'un modèle de robot automatique de régression de rétrograde à l'aide de l'apprentissage préalable avec des représentations 4D",
      "summary": "Fundamentāl mōdelu, ŏi jūi naru label dā'itā sutītōn de sakain sūrīn sareta mono de, kon'esutā suto ni tōkon'i hōyō ni kaikaku-teki eiyō o age, kyōi-teki ichigai kōka o shijishin suru, sakain sūrīn no jūyō-sei o shōmei shite imāsu. shika, rōbutikuku ni oite, kono yōna seikō o tachiru koto wa kōnan de, tōgō no tōgō no dā'itā no rekō to wa, fysik-u kai-sen o yōgo ni arau hyōgen no kōraku ni yotte seigen sarete imasu. kono ronbun de, jinjin no bi-deo-dā'itā kara sūrīn sareta rō-ru-bīn rē-bī-rō-mō-dēl o kōyō shite, rōbuto no mō-dēl o kaihatsu suru tame no jidō-hō-eki-teki rōbuto mō-dēl o tōji shite, ARM4R to iu mono o shōryū shimasu. tokubetsu, jikan ni wa tachite mono-karamu depusutō-seido tōkei o yōgo suru 2D hyōgen o 3D kō-sen ni hiku-ageta 3D pīn tō-chō hyōgen o riyū shimasu. konra no 4D hyōgen o, pīn to rōbuto no jōtai hyōgen no kanjō o kōgyō suru, senritsu henkan ni yotte kōnan naru yōni kaizen sarete iru tame, jinnin no bi-deo-dā'itā kara rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrī",
      "upvotes": 0,
      "discussionId": "67b5790832be608036ee9638"
    },
    "publishedAt": "2025-02-19T01:24:26.365Z",
    "title": "Pre-training Auto-regressive Robotic Models with 4D Representations",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13142.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "667c5764186b27ef806636d3",
      "avatarUrl": "/avatars/5c08f0109bc0e350624112c0aff544f6.svg",
      "fullname": "Roei Herzig",
      "name": "roeiherz",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.10852",
      "authors": [
        {
          "_id": "67b55321f703732d151de666",
          "name": "Zeli Su",
          "hidden": false
        },
        {
          "_id": "67b55321f703732d151de667",
          "name": "Ziyin Zhang",
          "hidden": false
        },
        {
          "_id": "67b55321f703732d151de668",
          "name": "Guixian Xu",
          "hidden": false
        },
        {
          "_id": "67b55321f703732d151de669",
          "name": "Jianing Liu",
          "hidden": false
        },
        {
          "_id": "67b55321f703732d151de66a",
          "name": "XU Han",
          "hidden": false
        },
        {
          "_id": "67b55321f703732d151de66b",
          "name": "Ting Zhang",
          "hidden": false
        },
        {
          "_id": "67b55321f703732d151de66c",
          "name": "Yushuang Dong",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-15T16:53:10.000Z",
      "title": "Le multilingue encodeur pense qu'il sait ce qu'il n'a pas vraiment pensé : la partage des poids. L'apprentissage préalable avec très peu de ressources dans les langues peu riches.",
      "summary": "Le modèle de multilinguisme XLM-R promeut la multilinguisation dans le NLP, tandis que son rendement en langues avec peu de ressources linguistiques est insatisfaisant. Cette situation est aggravée parce que les moderns LLMs (comme LLaMA et Qwen) offrent moins de langues que XLM-R. Pour relever ces défis, nous proposons un nouveau cadre de travail pour la génération de phrases dans les langues avec très peu de ressources linguistiques. Ce cadre permet de réutiliser les poids de l'encodeur pour exploiter l'espace de signification entraîné, ce qui aide à l'apprentissage efficace et à la généralisation efficace dans les langues avec peu de ressources linguistiques. Nous appliquons ce cadre à quatre langues minoritaires de Chine, présentant XLM-SWCM et montrant son excellent rendement.",
      "upvotes": 0,
      "discussionId": "67b55322f703732d151de69d"
    },
    "publishedAt": "2025-02-18T22:46:16.586Z",
    "title": "Multilingual Encoder Knows more than You Realize: Shared Weights Pretraining for Extremely Low-Resource Languages",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.10852.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6430bdd8cd31d174a9f900fb",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/Y9SPnRfpKSbYc7MhNdP-H.jpeg",
      "fullname": "Ziyin Zhang",
      "name": "Geralt-Targaryen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  }
]