[
  {
    "paper": {
      "id": "2506.17450",
      "authors": [
        {
          "_id": "68620adf9e7509383d29ab98",
          "user": {
            "_id": "655bca95360e4f90cb61ba83",
            "avatarUrl": "/avatars/1a187beb91a5e2fdc2303620b742aab1.svg",
            "isPro": true,
            "fullname": "Jiacheng Chen",
            "user": "cccjc",
            "type": "user"
          },
          "name": "Jiacheng Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-30T06:22:01.226Z",
          "hidden": false
        },
        {
          "_id": "68620adf9e7509383d29ab99",
          "name": "Ramin Mehran",
          "hidden": false
        },
        {
          "_id": "68620adf9e7509383d29ab9a",
          "name": "Xuhui Jia",
          "hidden": false
        },
        {
          "_id": "68620adf9e7509383d29ab9b",
          "name": "Saining Xie",
          "hidden": false
        },
        {
          "_id": "68620adf9e7509383d29ab9c",
          "name": "Sanghyun Woo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-20T19:38:34.000Z",
      "submittedOnDailyAt": "2025-06-30T02:33:26.106Z",
      "title": "BlenderFusion : Visualisation et Édition 3D Basée sur la Génération et la Synthèse Optique",
      "submittedOnDailyBy": {
        "_id": "655bca95360e4f90cb61ba83",
        "avatarUrl": "/avatars/1a187beb91a5e2fdc2303620b742aab1.svg",
        "isPro": true,
        "fullname": "Jiacheng Chen",
        "user": "cccjc",
        "type": "user"
      },
      "summary": "BlenderFusion est un cadre de fusion visuelle générative qui recombine des objets, des caméras et des fonds pour synthétiser de nouvelles images. Ce processus comprend : (i) la conversion d'entrées visuelles en entités 3D visuelles (traitement de couches), (ii) l'édition basée sur le contrôle 3D dans Blender (édition), et (iii) la génération d'images cohérentes en utilisant un générateur de fusion (fusion). Notre générateur de fusion étend des modèles pré-entraînés de diffusion pour traiter en parallèle des images originales (source) et des images éditées (objectif). Cela est ajusté grâce à deux stratégies de formation principales : (i) des masques de source et (ii) des simulations d'objets. BlenderFusion dépasse significativement les méthodes existantes dans les tâches d'édition d'images complexes.",
      "upvotes": 28,
      "discussionId": "68620adf9e7509383d29ab9d",
      "projectPage": "https://blenderfusion.github.io/",
      "ai_summary": "A generative visual compositing framework using a diffusion model for scene editing and composition with source masking and simulated object jittering.",
      "ai_keywords": [
        "diffusion model",
        "source masking",
        "simulated object jittering"
      ]
    },
    "publishedAt": "2025-06-20T15:38:34.000Z",
    "title": "BlenderFusion: 3D-Grounded Visual Editing and Generative Compositing",
    "summary": "We present BlenderFusion, a generative visual compositing framework that\nsynthesizes new scenes by recomposing objects, camera, and background. It\nfollows a layering-editing-compositing pipeline: (i) segmenting and converting\nvisual inputs into editable 3D entities (layering), (ii) editing them in\nBlender with 3D-grounded control (editing), and (iii) fusing them into a\ncoherent scene using a generative compositor (compositing). Our generative\ncompositor extends a pre-trained diffusion model to process both the original\n(source) and edited (target) scenes in parallel. It is fine-tuned on video\nframes with two key training strategies: (i) source masking, enabling flexible\nmodifications like background replacement; (ii) simulated object jittering,\nfacilitating disentangled control over objects and camera. BlenderFusion\nsignificantly outperforms prior methods in complex compositional scene editing\ntasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.17450.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "655bca95360e4f90cb61ba83",
      "avatarUrl": "/avatars/1a187beb91a5e2fdc2303620b742aab1.svg",
      "fullname": "Jiacheng Chen",
      "name": "cccjc",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.21862",
      "authors": [
        {
          "_id": "6861eea79e7509383d29ab2f",
          "name": "Boyuan Sun",
          "hidden": false
        },
        {
          "_id": "6861eea79e7509383d29ab30",
          "name": "Jiaxing Zhao",
          "hidden": false
        },
        {
          "_id": "6861eea79e7509383d29ab31",
          "name": "Xihan Wei",
          "hidden": false
        },
        {
          "_id": "6861eea79e7509383d29ab32",
          "name": "Qibin Hou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-27T02:29:58.000Z",
      "submittedOnDailyAt": "2025-06-30T00:31:12.107Z",
      "title": "LLaVA-Scissor : Compression de Token en Utilisant des Composantes Connex Significatives pour le Modèle de Langage Vidéo",
      "submittedOnDailyBy": {
        "_id": "6686044047f2a33570e59e31",
        "avatarUrl": "/avatars/2656bf2cecd6d7cbffd0a912a54d25de.svg",
        "isPro": false,
        "fullname": "Jiaxing Zhao",
        "user": "StarJiaxing",
        "type": "user"
      },
      "summary": "Dans cet article, nous proposons une stratégie de compression adéquate de tokens pour des modèles de langage multimodal vidéo, appelée LLaVA-Scissor. Les méthodes existantes principalement basent la compression de tokens sur les scores d'attention, mais ne peuvent pas capturer efficacement tous les domaines sémantiques, ce qui entraîne l'apparition de tokens redondants. De plus, nous utilisons un approche d'accès à des composants sémantiques (SCC) pour attribuer des tokens à différents domaines sémantiques au sein d'un ensemble de tokens, garantissant ainsi une couverture sémantique générale. Par conséquent, nous avons mis en place une stratégie de compression de tokens en deux étapes, en utilisant la SCC sur le spectre et dans le domaine temporel. Cette stratégie permet efficacement de compresser les tokens de manière non redondante, facilitant ainsi la représentation de tout le vidéo. La capacité de compression de tokens de LLaVA-Scissor a été évaluée par diverses évaluations de compréhension vidéo, y compris des évaluations des réponses à des questions vidéo, de la compréhension de vidéos longues et de benchmarks de multiple choix détaillés. Les résultats des expériences montrent que la proposition de LLaVA-Scissor présente un comportement exceptionnel comparé aux autres méthodes de compression de tokens, et montre également un comportement supérieur dans plusieurs benchmarks de compréhension vidéo, même à des ratios de maintenance de tokens faibles. Page du projet : https://github.com/HumanMLLM/LLaVA-Scissor.",
      "upvotes": 26,
      "discussionId": "6861eea89e7509383d29ab33",
      "ai_summary": "LLaVA-Scissor, a token compression strategy for video multimodal large language models, uses Semantic Connected Components to compress tokens effectively while maintaining semantic coverage and outperforming other methods.",
      "ai_keywords": [
        "token compression strategy",
        "Semantic Connected Components (SCC)",
        "spatio-temporal token compression strategy",
        "video question answering",
        "long video understanding",
        "comprehensive multi-choice benchmarks"
      ]
    },
    "publishedAt": "2025-06-26T22:29:58.000Z",
    "title": "LLaVA-Scissor: Token Compression with Semantic Connected Components for\n  Video LLMs",
    "summary": "In this paper, we present LLaVA-Scissor, a training-free token compression\nstrategy designed for video multimodal large language models. Previous methods\nmostly attempt to compress tokens based on attention scores, but fail to\neffectively capture all semantic regions and often lead to token redundancy.\nDifferently, we propose to leverage the Semantic Connected Components (SCC)\napproach that assigns tokens to distinct semantic regions within the token set,\nensuring comprehensive semantic coverage. The outcome is a two-step\nspatio-temporal token compression strategy that utilizes SCC in both spatial\nand temporal domains. This strategy can effectively compress tokens by\nrepresenting the entire video with a set of non-overlapping semantic tokens. We\nconduct extensive evaluations of the token compression capabilities of\nLLaVA-Scissor across diverse video understanding benchmarks, including video\nquestion answering, long video understanding, and comprehensive multi-choices\nbenchmarks. Experimental results show that the proposed LLaVA-Scissor\noutperforms other token compression methods, achieving superior performance in\nvarious video understanding benchmarks, particularly at low token retention\nratios. Project page: https://github.com/HumanMLLM/LLaVA-Scissor.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.21862.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "6686044047f2a33570e59e31",
      "avatarUrl": "/avatars/2656bf2cecd6d7cbffd0a912a54d25de.svg",
      "fullname": "Jiaxing Zhao",
      "name": "StarJiaxing",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 17
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.21416",
      "authors": [
        {
          "_id": "685e084071131fa43be08acc",
          "user": {
            "_id": "6361dd166945df7441b893fa",
            "avatarUrl": "/avatars/b3ae6888a41aab8c2a7ef9f7320565c4.svg",
            "isPro": false,
            "fullname": "Bowen Chen ",
            "user": "chenbowen",
            "type": "user"
          },
          "name": "Bowen Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-30T06:22:45.351Z",
          "hidden": false
        },
        {
          "_id": "685e084071131fa43be08acd",
          "name": "Mengyi Zhao",
          "hidden": false
        },
        {
          "_id": "685e084071131fa43be08ace",
          "name": "Haomiao Sun",
          "hidden": false
        },
        {
          "_id": "685e084071131fa43be08acf",
          "name": "Li Chen",
          "hidden": false
        },
        {
          "_id": "685e084071131fa43be08ad0",
          "name": "Xu Wang",
          "hidden": false
        },
        {
          "_id": "685e084071131fa43be08ad1",
          "name": "Kang Du",
          "hidden": false
        },
        {
          "_id": "685e084071131fa43be08ad2",
          "name": "Xinglong Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-26T16:04:16.000Z",
      "submittedOnDailyAt": "2025-06-30T04:43:14.606Z",
      "title": "XVerse : Commande de variables multiples en ligne avec DiT - Confluence d'attributs identificatoires et sémantiques",
      "submittedOnDailyBy": {
        "_id": "6498038ece9190ebb8693034",
        "avatarUrl": "/avatars/06ec2457932e05572d917ba286cdef25.svg",
        "isPro": false,
        "fullname": "Zhao",
        "user": "Mengyi",
        "type": "user"
      },
      "summary": "Atteindre un contrôle fine sur l'identité des sujets et les attributs sémantiques (posture, style, illumination) dans la génération d'images à partir de texte, surtout pour plusieurs sujets, souvent entraîne une édibilité et une cohérence limitées des Transformateurs de Diffusion (DiTs). De nombreuses approches introduisent des artefacts ou souffrent d'une compréhension des attributs. Pour surmonter ces défis, nous proposons un nouveau modèle de génération contrôlée de plusieurs sujets, le XVerse. En transformant des images de référence en déviations pour la modulation du flux de texte spécifique des tokens, le XVerse permet un contrôle précis et indépendant pour des sujets spécifiques sans perturber les latents ou les caractéristiques des images. Par conséquent, le XVerse offre une synthèse d'images multi-sujets de haute fidélité avec un contrôle robuste sur les caractéristiques individuelles des sujets et les attributs sémantiques. Ce progrès améliore significativement les capacités de génération de scénarios personnalisés et complexes.",
      "upvotes": 18,
      "discussionId": "685e084071131fa43be08ad3",
      "projectPage": "https://bytedance.github.io/XVerse/",
      "githubRepo": "https://github.com/bytedance/XVerse",
      "ai_summary": "XVerse enhances text-to-image generation by enabling precise and independent control over multiple subjects using token-specific text-stream modulation, improving image coherence and fidelity.",
      "ai_keywords": [
        "Diffusion Transformers",
        "DiTs",
        "text-to-image generation",
        "multi-subject controlled generation",
        "reference images",
        "token-specific text-stream modulation",
        "image latents",
        "multi-subject image synthesis",
        "semantic attributes"
      ],
      "githubStars": 68
    },
    "publishedAt": "2025-06-26T12:04:16.000Z",
    "title": "XVerse: Consistent Multi-Subject Control of Identity and Semantic\n  Attributes via DiT Modulation",
    "summary": "Achieving fine-grained control over subject identity and semantic attributes\n(pose, style, lighting) in text-to-image generation, particularly for multiple\nsubjects, often undermines the editability and coherence of Diffusion\nTransformers (DiTs). Many approaches introduce artifacts or suffer from\nattribute entanglement. To overcome these challenges, we propose a novel\nmulti-subject controlled generation model XVerse. By transforming reference\nimages into offsets for token-specific text-stream modulation, XVerse allows\nfor precise and independent control for specific subject without disrupting\nimage latents or features. Consequently, XVerse offers high-fidelity, editable\nmulti-subject image synthesis with robust control over individual subject\ncharacteristics and semantic attributes. This advancement significantly\nimproves personalized and complex scene generation capabilities.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.21416.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6498038ece9190ebb8693034",
      "avatarUrl": "/avatars/06ec2457932e05572d917ba286cdef25.svg",
      "fullname": "Zhao",
      "name": "Mengyi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.21356",
      "authors": [
        {
          "_id": "6861fb7a9e7509383d29ab4b",
          "name": "Hongbo Liu",
          "hidden": false
        },
        {
          "_id": "6861fb7a9e7509383d29ab4c",
          "name": "Jingwen He",
          "hidden": false
        },
        {
          "_id": "6861fb7a9e7509383d29ab4d",
          "name": "Yi Jin",
          "hidden": false
        },
        {
          "_id": "6861fb7a9e7509383d29ab4e",
          "name": "Dian Zheng",
          "hidden": false
        },
        {
          "_id": "6861fb7a9e7509383d29ab4f",
          "name": "Yuhao Dong",
          "hidden": false
        },
        {
          "_id": "6861fb7a9e7509383d29ab50",
          "name": "Fan Zhang",
          "hidden": false
        },
        {
          "_id": "6861fb7a9e7509383d29ab51",
          "name": "Ziqi Huang",
          "hidden": false
        },
        {
          "_id": "6861fb7a9e7509383d29ab52",
          "name": "Yinan He",
          "hidden": false
        },
        {
          "_id": "6861fb7a9e7509383d29ab53",
          "name": "Yangguang Li",
          "hidden": false
        },
        {
          "_id": "6861fb7a9e7509383d29ab54",
          "name": "Weichao Chen",
          "hidden": false
        },
        {
          "_id": "6861fb7a9e7509383d29ab55",
          "name": "Yu Qiao",
          "hidden": false
        },
        {
          "_id": "6861fb7a9e7509383d29ab56",
          "name": "Wanli Ouyang",
          "hidden": false
        },
        {
          "_id": "6861fb7a9e7509383d29ab57",
          "name": "Shengjie Zhao",
          "hidden": false
        },
        {
          "_id": "6861fb7a9e7509383d29ab58",
          "name": "Ziwei Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-26T15:09:21.000Z",
      "submittedOnDailyAt": "2025-06-30T04:32:34.261Z",
      "title": "ShotBench : Modèle de langage visuel cinématographique, modèle qui possède une compréhension visuelle au niveau expert.",
      "submittedOnDailyBy": {
        "_id": "652965773a416e1f2173443b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652965773a416e1f2173443b/y9MB8YgHzbwCXAc4EI9T3.jpeg",
        "isPro": false,
        "fullname": "Yuhao Dong",
        "user": "THUdyh",
        "type": "user"
      },
      "summary": "La technique de registrage cinématographique, un langage visuel de base de la cinématographie, est essentielle pour la transmission d'histoires, d'émotions et de questions artistiques. Les modèles de langage visuel (VLMs) récents ont démontré une compréhension visuelle large, mais manquent de recherches et d'évaluations solides pour comprendre les microtextes de registrage cinématographique dans chaque scène. Cette lacune est critique pour la compréhension précise des détails visuels et la précision de la génération de vidéos par IA. En réponse à cette problématique, nous présentons ShotBench, un cadre de référence détaillé pour la compréhension du langage cinématographique. Il a été extrait de 200 films évalués (principalement des candidats aux Oscar), et plus de 3500 paires de questions et réponses évaluées par experts dans 8 dimensions importantes de registrage cinématographique ont été préparées. L'évaluation de 24 VLMs avancés sur ShotBench a révélé leurs limites : même les modèles avec les meilleurs résultats ont un pourcentage de réponses correctes inférieur à 60%, et particulièrement, ils ont des difficultés avec le code visuel détaillé et la logique spatiale complexe. Pour encourager le développement de ce domaine, nous avons construit ShotQA, un grand ensemble de données diversifiées qui comprend environ 70 000 paires de questions et réponses. En utilisant ShotQA, nous avons développé ShotVL grâce à des ajustements directeurs et l'optimisation de politiques de groupe. ShotVL dépasse tous les modèles open-source et propriétaire actuels, et établit de nouveaux rendements optimaux sur ShotBench. Nous publions les modèles, les données et le code pour encourager rapidement le développement de la compréhension et de la génération de registrage cinématographique par IA.",
      "upvotes": 15,
      "discussionId": "6861fb7a9e7509383d29ab59",
      "projectPage": "https://vchitect.github.io/ShotBench-project/",
      "githubRepo": "https://github.com/Vchitect/ShotBench/tree/main",
      "ai_summary": "ShotBench and ShotQA datasets, along with ShotVL model, enhance AI's understanding and generation capabilities by specifically targeting nuanced cinematic language comprehension.",
      "ai_keywords": [
        "Vision-Language Models",
        "VLMs",
        "ShotBench",
        "QA pairs",
        "cinematic grammar",
        "fine-grained visual comprehension",
        "AI-assisted video generation",
        "ShotQA",
        "multimodal dataset",
        "supervised fine-tuning",
        "Group Relative Policy Optimization",
        "ShotVL",
        "AI-driven cinematic understanding",
        "state-of-the-art performance"
      ],
      "githubStars": 5
    },
    "publishedAt": "2025-06-26T11:09:21.000Z",
    "title": "ShotBench: Expert-Level Cinematic Understanding in Vision-Language\n  Models",
    "summary": "Cinematography, the fundamental visual language of film, is essential for\nconveying narrative, emotion, and aesthetic quality. While recent\nVision-Language Models (VLMs) demonstrate strong general visual understanding,\ntheir proficiency in comprehending the nuanced cinematic grammar embedded\nwithin individual shots remains largely unexplored and lacks robust evaluation.\nThis critical gap limits both fine-grained visual comprehension and the\nprecision of AI-assisted video generation. To address this, we introduce\nShotBench, a comprehensive benchmark specifically designed for cinematic\nlanguage understanding. It features over 3.5k expert-annotated QA pairs from\nimages and video clips, meticulously curated from over 200 acclaimed\n(predominantly Oscar-nominated) films and spanning eight key cinematography\ndimensions. Our evaluation of 24 leading VLMs on ShotBench reveals their\nsubstantial limitations: even the top-performing model achieves less than 60%\naverage accuracy, particularly struggling with fine-grained visual cues and\ncomplex spatial reasoning. To catalyze advancement in this domain, we construct\nShotQA, a large-scale multimodal dataset comprising approximately 70k cinematic\nQA pairs. Leveraging ShotQA, we develop ShotVL through supervised fine-tuning\nand Group Relative Policy Optimization. ShotVL significantly outperforms all\nexisting open-source and proprietary models on ShotBench, establishing new\nstate-of-the-art performance. We open-source our models, data, and code to\nfoster rapid progress in this crucial area of AI-driven cinematic understanding\nand generation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.21356.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "652965773a416e1f2173443b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652965773a416e1f2173443b/y9MB8YgHzbwCXAc4EI9T3.jpeg",
      "fullname": "Yuhao Dong",
      "name": "THUdyh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 43
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.20279",
      "authors": [
        {
          "_id": "686218679e7509383d29abb3",
          "name": "Changliang Xia",
          "hidden": false
        },
        {
          "_id": "686218679e7509383d29abb4",
          "name": "Chengyou Jia",
          "hidden": false
        },
        {
          "_id": "686218679e7509383d29abb5",
          "name": "Zhuohang Dang",
          "hidden": false
        },
        {
          "_id": "686218679e7509383d29abb6",
          "name": "Minnan Luo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-25T09:40:50.000Z",
      "submittedOnDailyAt": "2025-06-30T03:24:47.090Z",
      "title": "Idéaux vers la réalité : prédiction concentrée d'unification efficace de données dans le scan de la réalité.",
      "submittedOnDailyBy": {
        "_id": "6602548a68d519ed324b47c5",
        "avatarUrl": "/avatars/5ab411f87440cc2a98c7a1c6a3ed5548.svg",
        "isPro": false,
        "fullname": "ChengyouJia",
        "user": "ChengyouJia",
        "type": "user"
      },
      "summary": "La prédiction dense joue un rôle important dans le domaine de la vision par ordinateur, avec l'objectif d'entraîner l'étiquetage de chaque pixel de l'image d'entrée. Bien que elle ait évolué, les méthodes actuelles se concentrent sur des conditions idéales et ont une capacité de généralisation limitée dans des scénarios réels ainsi que dans le manque de données. Pour étudier ces problèmes de manière systématique, nous présentons le benchmark DenseWorld. DenseWorld constitue une large gamme de tâches de prédiction dense qui correspondent à 25 applications réelles efficaces et a la caractéristique d'une évaluation unifiée. Nous proposons ensuite le modèle DenseDiT. DenseDiT se concentre sur maximiser l'utilisation des limites visuelles des modèles génératifs, avec l'objectif de réaliser diverses tâches de prédiction dense de manière uniforme. DenseDiT combine une structure de réutilisation de paramètres et deux branches légères, fonctionnant avec moins de 0.1% de paramètres additionnels. Dans l'évaluation sur DenseWorld, DenseDiT réduit significativement le rendement des baselines généraux et montre ses limites en termes de capacité de généralisation dans des scénarios réels. D'autre part, DenseDiT obtient des résultats exceptionnels en utilisant moins de 0.01% des données d'entraînement, soulignant la valeur pratique de ses caractéristiques réelles. Les données, checkpoints et code sont disponibles sur https://xcltql666.github.io/DenseDiTProj.",
      "upvotes": 13,
      "discussionId": "686218689e7509383d29abb7",
      "projectPage": "https://xcltql666.github.io/DenseDiTProj/",
      "githubRepo": "https://github.com/xcltql666/DenseDiT",
      "ai_summary": "DenseDiT, a generative model-based approach, achieves superior performance in real-world dense prediction tasks using minimal training data compared to existing methods.",
      "ai_keywords": [
        "dense prediction",
        "generative models",
        "visual priors",
        "parameter-reuse mechanism",
        "lightweight branches",
        "multi-scale context",
        "DenseWorld",
        "DenseDiT"
      ],
      "githubStars": 15
    },
    "publishedAt": "2025-06-25T05:40:50.000Z",
    "title": "From Ideal to Real: Unified and Data-Efficient Dense Prediction for\n  Real-World Scenarios",
    "summary": "Dense prediction tasks hold significant importance of computer vision, aiming\nto learn pixel-wise annotated label for an input image. Despite advances in\nthis field, existing methods primarily focus on idealized conditions, with\nlimited generalization to real-world scenarios and facing the challenging\nscarcity of real-world data. To systematically study this problem, we first\nintroduce DenseWorld, a benchmark spanning a broad set of 25 dense prediction\ntasks that correspond to urgent real-world applications, featuring unified\nevaluation across tasks. Then, we propose DenseDiT, which maximally exploits\ngenerative models' visual priors to perform diverse real-world dense prediction\ntasks through a unified strategy. DenseDiT combines a parameter-reuse mechanism\nand two lightweight branches that adaptively integrate multi-scale context,\nworking with less than 0.1% additional parameters. Evaluations on DenseWorld\nreveal significant performance drops in existing general and specialized\nbaselines, highlighting their limited real-world generalization. In contrast,\nDenseDiT achieves superior results using less than 0.01% training data of\nbaselines, underscoring its practical value for real-world deployment. Our\ndata, and checkpoints and codes are available at\nhttps://xcltql666.github.io/DenseDiTProj",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.20279.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6602548a68d519ed324b47c5",
      "avatarUrl": "/avatars/5ab411f87440cc2a98c7a1c6a3ed5548.svg",
      "fullname": "ChengyouJia",
      "name": "ChengyouJia",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.22434",
      "authors": [
        {
          "_id": "686205ad9e7509383d29ab80",
          "name": "Xi Chen",
          "hidden": false
        },
        {
          "_id": "686205ad9e7509383d29ab81",
          "name": "Mingkang Zhu",
          "hidden": false
        },
        {
          "_id": "686205ad9e7509383d29ab82",
          "name": "Shaoteng Liu",
          "hidden": false
        },
        {
          "_id": "686205ad9e7509383d29ab83",
          "name": "Xiaoyang Wu",
          "hidden": false
        },
        {
          "_id": "686205ad9e7509383d29ab84",
          "name": "Xiaogang Xu",
          "hidden": false
        },
        {
          "_id": "686205ad9e7509383d29ab85",
          "name": "Yu Liu",
          "hidden": false
        },
        {
          "_id": "686205ad9e7509383d29ab86",
          "name": "Xiang Bai",
          "hidden": false
        },
        {
          "_id": "686205ad9e7509383d29ab87",
          "name": "Hengshuang Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-27T17:59:27.000Z",
      "submittedOnDailyAt": "2025-06-30T02:04:48.511Z",
      "title": "Comparaison d'images multiples pour la description visuelle renforcée",
      "submittedOnDailyBy": {
        "_id": "644a1b6401e18bf93a6f45c1",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644a1b6401e18bf93a6f45c1/P0i_CgCrIzOS2tYRlxoE9.png",
        "isPro": false,
        "fullname": "xichen",
        "user": "xichenhku",
        "type": "user"
      },
      "summary": "Cette étude explore des méthodes pour faciliter l'inférence de la Chaîne de Pensée (CoT) entre plusieurs images. Une solution intuitive pourrait être l'apprentissage de règles basée sur l'apprentissage par renforcement appliqué à des modèles de vision-langage (VLMs). Cependant, ces méthodes souvent se basent sur des paires de questions et réponses automatiquement préparées, ce qui rend particulièrement difficile le traitement de détails visuels complexes des images ou le processus logique entre images. En se référant à un modèle d'apprentissage de représentations visuelles ajustées automatiquement, on a découvert que les images ont des contraintes implicites. Selon cette approche, on a construit des tuples d'images composés de deux expansions visuelles de la même image et d'une troisième image similaire. Pendant le processus d'entraînement, on encourage le modèle à générer des raisons pour comparer ces images (c'est-à-dire, déterminer si elles sont les mêmes ou différentes). Ensuite, on optimise le modèle en utilisant l'apprentissage par renforcement basé sur des règles. En raison de la forte similitude visuelle et de l'existence de l'expansion, le modèle doit s'intéresser aux petits changements visuels et exécuter des logiques. Les expérimentations montrent que le modèle apprend pourquoi les images sont comparées dans des tâches de comparaison visuelle, ce qui s'étend à une large gamme de questions. Sans dépendre de paires de questions et réponses enregistrées par des humains, cette méthode réalise des améliorations claires sur les benchmarks de traitement de raisonnement entre plusieurs images et montre un rendement fort dans des tâches visuelles générales.",
      "upvotes": 8,
      "discussionId": "686205ad9e7509383d29ab88",
      "ai_summary": "Self-supervised learning using image triplets enhances the reasoning ability of Vision-Language Models (VLMs) on multi-image tasks without the need for human-annotated question-answer pairs.",
      "ai_keywords": [
        "Vision-Language Models",
        "self-supervised learning",
        "image triplets",
        "reasoning ability",
        "multi-image reasoning benchmarks",
        "general vision tasks"
      ]
    },
    "publishedAt": "2025-06-27T13:59:27.000Z",
    "title": "MiCo: Multi-image Contrast for Reinforcement Visual Reasoning",
    "summary": "This work explores enabling Chain-of-Thought (CoT) reasoning to link visual\ncues across multiple images. A straightforward solution is to adapt rule-based\nreinforcement learning for Vision-Language Models (VLMs). However, such methods\ntypically rely on manually curated question-answer pairs, which can be\nparticularly challenging when dealing with fine grained visual details and\ncomplex logic across images. Inspired by self-supervised visual representation\nlearning, we observe that images contain inherent constraints that can serve as\nsupervision. Based on this insight, we construct image triplets comprising two\naugmented views of the same image and a third, similar but distinct image.\nDuring training, the model is prompted to generate a reasoning process to\ncompare these images (i.e., determine same or different). Then we optimize the\nmodel with rule-based reinforcement learning. Due to the high visual similarity\nand the presence of augmentations, the model must attend to subtle visual\nchanges and perform logical reasoning to succeed. Experiments show that,\nalthough trained solely on visual comparison tasks, the learned reasoning\nability generalizes effectively to a wide range of questions. Without relying\non any human-annotated question-answer pairs, our method achieves significant\nimprovements on multi-image reasoning benchmarks and shows strong performance\non general vision tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.22434.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "644a1b6401e18bf93a6f45c1",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644a1b6401e18bf93a6f45c1/P0i_CgCrIzOS2tYRlxoE9.png",
      "fullname": "xichen",
      "name": "xichenhku",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 43
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.21656",
      "authors": [
        {
          "_id": "6861f2b89e7509383d29ab35",
          "name": "Yifan Shen",
          "hidden": false
        },
        {
          "_id": "6861f2b89e7509383d29ab36",
          "name": "Yuanzhe Liu",
          "hidden": false
        },
        {
          "_id": "6861f2b89e7509383d29ab37",
          "name": "Jingyuan Zhu",
          "hidden": false
        },
        {
          "_id": "6861f2b89e7509383d29ab38",
          "name": "Xu Cao",
          "hidden": false
        },
        {
          "_id": "6861f2b89e7509383d29ab39",
          "name": "Xiaofeng Zhang",
          "hidden": false
        },
        {
          "_id": "6861f2b89e7509383d29ab3a",
          "name": "Yixiao He",
          "hidden": false
        },
        {
          "_id": "6861f2b89e7509383d29ab3b",
          "name": "Wenming Ye",
          "hidden": false
        },
        {
          "_id": "6861f2b89e7509383d29ab3c",
          "name": "James Matthew Rehg",
          "hidden": false
        },
        {
          "_id": "6861f2b89e7509383d29ab3d",
          "name": "Ismini Lourentzou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-26T18:00:00.000Z",
      "submittedOnDailyAt": "2025-06-30T00:44:37.025Z",
      "title": "Fine-Grained Preference Optimization est utilisé pour améliorer la capacité de reconnaissance spatiale des VLMs.",
      "submittedOnDailyBy": {
        "_id": "65e387095132c2edd193ae49",
        "avatarUrl": "/avatars/39278e5b026bcbdde88c560fc54018c5.svg",
        "isPro": false,
        "fullname": "Yifan Shen",
        "user": "SivanSX",
        "type": "user"
      },
      "summary": "Les modèles actuels de langage visuel (VLMs) sont particulièrement affectés lorsqu'il est nécessaire de faire des logiques à plusieurs étapes et une alignement spatial précis. Dans cette étude, nous présentons SpatialReasoner-R1, un modèle de logique visuelle qui vise à résoudre cette limitation. Pour établir des normes de qualité élevée en logique spatiale, nous avons conçu le méthode de recherche d'arbres M3CTS avec plusieurs modèles, et nous avons généré des trajectoires logiques de concepts longs et continus. De plus, nous proposons l'optimisation directe des fins physiques (fDPO), qui introduit la granularité directe et la nature des fins de la logique observationnelle et spatiale, et nous évaluons les réponses candidates en basant nous sur la compensation spatiale. Les résultats des expérimentations montrent que fDPO améliore en moyenne de 4,1% dans les tâches de qualité spatiale et de 9,0% dans les tâches de quantité spatiale. Avec fDPO, SpatialReasoner-R1 établit un nouveau standard de référence dans SPATIALRGPT-Bench, améliorant la précision moyenne de 9,8% par rapport aux meilleurs standards, et maintient une compétitivité dans les tâches générales de langage visuel.",
      "upvotes": 6,
      "discussionId": "6861f2b99e7509383d29ab3e",
      "ai_summary": "SpatialReasoner-R1, a vision-language reasoning model, uses Multi-Model Monte Carlo Tree Search and fine-grained Direct Preference Optimization to improve spatial reasoning, setting a new state-of-the-art on SPATIALRGPT-Bench.",
      "ai_keywords": [
        "vision-language models",
        "SpatialReasoner-R1",
        "Multi-Model Monte Carlo Tree Search",
        "M3CTS",
        "Long Chain-of-Thought",
        "LongCoT",
        "fine-grained Direct Preference Optimization",
        "fDPO",
        "segment-specific preference granularity",
        "descriptive grounding",
        "logical reasoning",
        "spatial reward mechanism",
        "visual consistency",
        "spatial grounding",
        "logical coherence",
        "SPATIALRGPT-Bench"
      ]
    },
    "publishedAt": "2025-06-26T14:00:00.000Z",
    "title": "Fine-Grained Preference Optimization Improves Spatial Reasoning in VLMs",
    "summary": "Current Vision-Language Models (VLMs) struggle with fine-grained spatial\nreasoning, particularly when multi-step logic and precise spatial alignment are\nrequired. In this work, we introduce SpatialReasoner-R1, a vision-language\nreasoning model designed to address these limitations. To construct\nhigh-quality supervision for spatial reasoning, we design a Multi-Model Monte\nCarlo Tree Search (M3CTS) method that generates diverse, logically consistent\nLong Chain-of-Thought (LongCoT) reasoning trajectories. In addition, we propose\nfine-grained Direct Preference Optimization (fDPO), which introduces\nsegment-specific preference granularity for descriptive grounding and logical\nreasoning, guided by a spatial reward mechanism that evaluates candidate\nresponses based on visual consistency, spatial grounding, and logical\ncoherence. Experimental results demonstrate that fDPO achieves an average\nimprovement of 4.1% over standard DPO across spatial quality tasks, and a 9.0%\ngain in spatial quantity tasks. SpatialReasoner-R1, trained with fDPO, sets a\nnew SoTA on SPATIALRGPT-Bench, outperforming the strongest baseline by 9.8% in\naverage accuracy, while maintaining competitive performance on general\nvision-language tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.21656.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65e387095132c2edd193ae49",
      "avatarUrl": "/avatars/39278e5b026bcbdde88c560fc54018c5.svg",
      "fullname": "Yifan Shen",
      "name": "SivanSX",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.21628",
      "authors": [
        {
          "_id": "686261739e7509383d29ac6e",
          "name": "Magnus Dierking",
          "hidden": false
        },
        {
          "_id": "686261739e7509383d29ac6f",
          "name": "Christopher E. Mower",
          "hidden": false
        },
        {
          "_id": "686261739e7509383d29ac70",
          "name": "Sarthak Das",
          "hidden": false
        },
        {
          "_id": "686261739e7509383d29ac71",
          "name": "Huang Helong",
          "hidden": false
        },
        {
          "_id": "686261739e7509383d29ac72",
          "name": "Jiacheng Qiu",
          "hidden": false
        },
        {
          "_id": "686261739e7509383d29ac73",
          "name": "Cody Reading",
          "hidden": false
        },
        {
          "_id": "686261739e7509383d29ac74",
          "name": "Wei Chen",
          "hidden": false
        },
        {
          "_id": "686261739e7509383d29ac75",
          "name": "Huidong Liang",
          "hidden": false
        },
        {
          "_id": "686261739e7509383d29ac76",
          "name": "Huang Guowei",
          "hidden": false
        },
        {
          "_id": "686261739e7509383d29ac77",
          "name": "Jan Peters",
          "hidden": false
        },
        {
          "_id": "686261739e7509383d29ac78",
          "name": "Quan Xingyue",
          "hidden": false
        },
        {
          "_id": "686261739e7509383d29ac79",
          "name": "Jun Wang",
          "hidden": false
        },
        {
          "_id": "686261739e7509383d29ac7a",
          "name": "Haitham Bou-Ammar",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-24T20:23:39.000Z",
      "submittedOnDailyAt": "2025-06-30T08:38:41.700Z",
      "title": "Ark : Cadre Python ouvert pour l'apprentissage de robots",
      "submittedOnDailyBy": {
        "_id": "631c375768f7da9ad2496bf6",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/631c375768f7da9ad2496bf6/1sDOoecA6e1v_hn_VAgUq.jpeg",
        "isPro": false,
        "fullname": "Haitham Bou Ammar",
        "user": "hba123",
        "type": "user"
      },
      "summary": "La technologie de la robotique a connu un développement impressionnant dans le matériel. Il comprend des événements comme le défi de villes et les robots de la DARPA, ainsi que le premier concours de robots humainoïdes de CAPBOCKSING. Cependant, l'irrégularité commerciale ne peut pas être capturée par le développement de l'apprentissage automatique. Le logiciel comme bloc logique forme de grands blocs. Le stack actuel de robots exige des courbes d'apprentissage élevées, un savoir avancé en C/C++, la distribution de l'entraînement et une architecture complexe de matériel. Cela contraste avec l'écosystème détaillé centré sur Python qui pousse l'IA moderne. On présente ARK, un cadre de référence de technologie robotique ouvert, qui privilégie Python. ARK utilise des algorithmes d'apprentissage avancés (comme ACT et Diffusion Policy) pour la collecte et le pré-traitement des données, offrant une interface d'environnement de type Gym pour une transition sans interruption entre la simulation précise et les robots physiques. Sa architecture légère client-serveur fournit des fournisseurs de réseau et de communication sous-sous, avec des options de binding en C/C++ pour garantir un rendement en temps réel. ARK inclut des modules réutilisables pour le contrôle, SLAM, la planification du mouvement, l'identification des systèmes et la visualisation, ainsi que la nature interchangeable de ROS. On présente des détails et des études de cas, y compris des prototypes de recherche et de mouvement, l'échange de matériel simple, des pipelines construits à des points d'entrée et une facilité comparable au flux de travail dominant de l'apprentissage automatique. ARK intègre la pratique de la robotique et l'IA en Python partagé, accélère l'étude des robots autonomes et l'emploi commercial, et réduit le coût d'entrée.",
      "upvotes": 4,
      "discussionId": "686261739e7509383d29ac7b",
      "ai_summary": "ARK is an open-source Python-first framework that integrates modern imitation-learning algorithms and seamless simulation-physical robot interactions to simplify robotics development and deployment.",
      "ai_keywords": [
        "Gym-style environment interface",
        "imitation-learning algorithms",
        "ACT",
        "Diffusion Policy",
        "lightweight client-server architecture",
        "publisher-subscriber communication",
        "reusable modules",
        "control",
        "SLAM",
        "motion planning",
        "system identification",
        "visualization",
        "native ROS interoperability",
        "end-to-end pipelines"
      ]
    },
    "publishedAt": "2025-06-24T16:23:39.000Z",
    "title": "Ark: An Open-source Python-based Framework for Robot Learning",
    "summary": "Robotics has made remarkable hardware strides-from DARPA's Urban and Robotics\nChallenges to the first humanoid-robot kickboxing tournament-yet commercial\nautonomy still lags behind progress in machine learning. A major bottleneck is\nsoftware: current robot stacks demand steep learning curves, low-level C/C++\nexpertise, fragmented tooling, and intricate hardware integration, in stark\ncontrast to the Python-centric, well-documented ecosystems that propelled\nmodern AI. We introduce ARK, an open-source, Python-first robotics framework\ndesigned to close that gap. ARK presents a Gym-style environment interface that\nallows users to collect data, preprocess it, and train policies using\nstate-of-the-art imitation-learning algorithms (e.g., ACT, Diffusion Policy)\nwhile seamlessly toggling between high-fidelity simulation and physical robots.\nA lightweight client-server architecture provides networked\npublisher-subscriber communication, and optional C/C++ bindings ensure\nreal-time performance when needed. ARK ships with reusable modules for control,\nSLAM, motion planning, system identification, and visualization, along with\nnative ROS interoperability. Comprehensive documentation and case studies-from\nmanipulation to mobile navigation-demonstrate rapid prototyping, effortless\nhardware swapping, and end-to-end pipelines that rival the convenience of\nmainstream machine-learning workflows. By unifying robotics and AI practices\nunder a common Python umbrella, ARK lowers entry barriers and accelerates\nresearch and commercial deployment of autonomous robots.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.21628.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "631c375768f7da9ad2496bf6",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/631c375768f7da9ad2496bf6/1sDOoecA6e1v_hn_VAgUq.jpeg",
      "fullname": "Haitham Bou Ammar",
      "name": "hba123",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 16
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.19741",
      "authors": [
        {
          "_id": "686209869e7509383d29ab92",
          "name": "Yihong Luo",
          "hidden": false
        },
        {
          "_id": "686209869e7509383d29ab93",
          "name": "Shuchen Xue",
          "hidden": false
        },
        {
          "_id": "686209869e7509383d29ab94",
          "name": "Tianyang Hu",
          "hidden": false
        },
        {
          "_id": "686209869e7509383d29ab95",
          "name": "Jing Tang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-24T15:58:55.000Z",
      "submittedOnDailyAt": "2025-06-30T02:20:48.977Z",
      "title": "\"Apprentissage en Consistance : Une Approche de Contrôle de l'Apprentissage Additionnelle dans la Méthode d'Accès Native d'un Générateur Mono-Etape\"",
      "submittedOnDailyBy": {
        "_id": "65f7e6856bd4bac5b6a4ecc3",
        "avatarUrl": "/avatars/9c0c48310fb5e99c19700316a0ab531e.svg",
        "isPro": false,
        "fullname": "Yihong Luo",
        "user": "Luo-Yihong",
        "type": "user"
      },
      "summary": "La optimisation et la possibilité de contrôle dans la génération de contenu de haute qualité sont un problème essentiel de la création de contenu généré par l'intelligence artificielle (AIGC). Un générateur à un pas fournit une haute qualité de génération et une efficacité informatique par la technique d'évaporation de la chaleur, mais présente de grands problèmes pour s'adapter à de nouvelles conditions de contrôle (contraintes structurelles, guides grammaticaux, entrées externes, etc.). Les méthodes traditionnelles nécessitent une grande quantité de calculs et la nécessité d'un évaporateur de la chaleur ultérieur. Dans cet article, nous présentons un nouvel approche légère appelée Entraînement de la Cohérence du Bruit (NCT), qui permet l'application directe de nouveaux contrôles. NCT ne nécessite pas d'accès à des images d'entraînement ou de réentraînement du modèle d'évaporation de la chaleur. Un module adaptatif est introduit et une perte de cohérence du bruit est mise en œuvre dans l'espace du bruit du générateur. Cette perte ajuste l'action de génération du modèle adaptatif aux différents niveaux de bruit conditionné et guide clairement les nouvelles conditions de contrôle. Théoriquement, cet objectif d'entraînement peut être compris comme minimisant la différence de distribution entre le modèle adaptatif et la distribution conditionnée par de nouvelles conditions. NCT est modulaire, efficace en données et facile à implémenter, en fonction d'un générateur à un pas pré-traité et d'un modèle de contrôle. Les expériences détaillées montrent que NCT effectue la génération contrôlée la plus avancée en un pas et dépasse les méthodes basées sur plusieurs pas ou sur l'évaporation de la chaleur en termes de qualité de génération et d'efficience informatique. Le code est disponible sur https://github.com/Luo-Yihong/NCT.",
      "upvotes": 4,
      "discussionId": "686209869e7509383d29ab96",
      "ai_summary": "A novel Noise Consistency Training approach integrates new control signals into pre-trained one-step generators efficiently without retraining, outperforming existing methods in quality and computational efficiency.",
      "ai_keywords": [
        "diffusion distillation",
        "Noise Consistency Training",
        "NCT",
        "one-step generators",
        "adapter module",
        "noise consistency loss",
        "noise space",
        "conditional distribution",
        "generative modeling",
        "data-efficient",
        "computational efficiency"
      ]
    },
    "publishedAt": "2025-06-24T11:58:55.000Z",
    "title": "Noise Consistency Training: A Native Approach for One-Step Generator in\n  Learning Additional Controls",
    "summary": "The pursuit of efficient and controllable high-quality content generation\nremains a central challenge in artificial intelligence-generated content\n(AIGC). While one-step generators, enabled by diffusion distillation\ntechniques, offer excellent generation quality and computational efficiency,\nadapting them to new control conditions--such as structural constraints,\nsemantic guidelines, or external inputs--poses a significant challenge.\nConventional approaches often necessitate computationally expensive\nmodifications to the base model and subsequent diffusion distillation. This\npaper introduces Noise Consistency Training (NCT), a novel and lightweight\napproach to directly integrate new control signals into pre-trained one-step\ngenerators without requiring access to original training images or retraining\nthe base diffusion model. NCT operates by introducing an adapter module and\nemploys a noise consistency loss in the noise space of the generator. This loss\naligns the adapted model's generation behavior across noises that are\nconditionally dependent to varying degrees, implicitly guiding it to adhere to\nthe new control. Theoretically, this training objective can be understood as\nminimizing the distributional distance between the adapted generator and the\nconditional distribution induced by the new conditions. NCT is modular,\ndata-efficient, and easily deployable, relying only on the pre-trained one-step\ngenerator and a control signal model. Extensive experiments demonstrate that\nNCT achieves state-of-the-art controllable generation in a single forward pass,\nsurpassing existing multi-step and distillation-based methods in both\ngeneration quality and computational efficiency. Code is available at\nhttps://github.com/Luo-Yihong/NCT",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.19741.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65f7e6856bd4bac5b6a4ecc3",
      "avatarUrl": "/avatars/9c0c48310fb5e99c19700316a0ab531e.svg",
      "fullname": "Yihong Luo",
      "name": "Luo-Yihong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.21411",
      "authors": [
        {
          "_id": "686237f69e7509383d29abe9",
          "user": {
            "_id": "64d5deb154bb9eb704f83122",
            "avatarUrl": "/avatars/86ce09bcca903319051e2307581a43f4.svg",
            "isPro": false,
            "fullname": "Yehui Tang",
            "user": "tangyehui",
            "type": "user"
          },
          "name": "Yehui Tang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-30T07:11:35.262Z",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abea",
          "name": "Xiaosong Li",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abeb",
          "user": {
            "_id": "64b78295479b934973e2c40e",
            "avatarUrl": "/avatars/9213e385964132fa50859264a838d891.svg",
            "isPro": false,
            "fullname": "liu",
            "user": "Fangcheng2",
            "type": "user"
          },
          "name": "Fangcheng Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-30T07:11:52.170Z",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abec",
          "name": "Wei Guo",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abed",
          "name": "Hang Zhou",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abee",
          "name": "Yaoyuan Wang",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abef",
          "name": "Kai Han",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abf0",
          "name": "Xianzhi Yu",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abf1",
          "name": "Jinpeng Li",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abf2",
          "name": "Hui Zang",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abf3",
          "name": "Fei Mi",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abf4",
          "name": "Xiaojun Meng",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abf5",
          "name": "Zhicheng Liu",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abf6",
          "name": "Hanting Chen",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abf7",
          "name": "Binfan Zheng",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abf8",
          "name": "Can Chen",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abf9",
          "name": "Youliang Yan",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abfa",
          "name": "Ruiming Tang",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abfb",
          "name": "Peifeng Qin",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abfc",
          "name": "Xinghao Chen",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abfd",
          "name": "Dacheng Tao",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abfe",
          "user": {
            "_id": "658bdf7b925aadd43304f05c",
            "avatarUrl": "/avatars/64d9e9dea27c376c3bc7b2a54efc2a46.svg",
            "isPro": false,
            "fullname": "Yunhe Wang",
            "user": "MightyCrane",
            "type": "user"
          },
          "name": "Yunhe Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-30T07:11:59.146Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-27T16:40:21.000Z",
      "submittedOnDailyAt": "2025-06-30T05:41:23.309Z",
      "title": "Pangu Pro MoE : Efficacité par la confusion des experts par groupes de l'hypérsparsité",
      "submittedOnDailyBy": {
        "_id": "63a369d98c0c89dcae3b8329",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a369d98c0c89dcae3b8329/AiH2zjy1cnt9OADAAZMLD.jpeg",
        "isPro": true,
        "fullname": "Adina Yakefu",
        "user": "AdinaY",
        "type": "user"
      },
      "summary": "Le modèle d'Experts Mixtes (MoE) a émergé dans les modèles de langage grands, avec un augmentation significative du nombre de paramètres et de la capacité d'entraînement, ce qui suggère une réduction légère des coûts d'exécution. Cependant, on observe une fréquence d'activation de ces Experts biaisée, et lorsqu'ils sont exécutés en parallèle sur des dispositifs différents, l'efficacité du système souvent diminue. En réponse à ce problème, on introduit le modèle d'Experts Mixtes de Groupes (MoGE), qui montre comment l'équilibrer la charge des Experts en les sélectionnant en groupes, ce qui fournit une distribution plus équitable de la charge par rapport à MoE. MoGE active un nombre égal d'Experts à l'intérieur de chaque groupe d'Experts réservés, et si l'exécution du modèle est distribuée entre plusieurs dispositifs, cette architecture garantit un équilibre dans la charge de calcul entre ces dispositifs, améliorant significativement l'exécution pendant la phase d'inférence. De plus, le MoE de Pangu Pro a été construit sur des NPUs Ascend, et un modèle rare basé sur MoGE a été implémenté, satisfaisant un total de 720 milliards de paramètres et une quantité de paramètres actifs par token de 160 milliards. Le design de MoE de Pangu Pro a été optimisé grâce à des simulations de systèmes larges avec Ascend 300I Duo et 800I A2. Les résultats des expérimentations montrent que MoGE améliore encore plus la distribution de la charge des Experts en entraînement et en inférence, atteignant une exécution efficace. L'efficacité de MoE de Pangu Pro en inférence atteint 1148 tokens/seconde par carte, et avec l'accélération spectrale, on atteint jusqu'à 1528 tokens/seconde, dépassant les modèles denses. De plus, le rendement coût-efficace de l'inférence sur Ascend 300I Duo est très bon, et MoE de Pangu Pro est capable d'être entraînée à l'aide de la Magic Space Parallelization, occupant une position de leadership dans la catégorie des modèles de 100 milliards de paramètres, dépassant les modèles open comme GLM-Z1-32B et Qwen3-32B.",
      "upvotes": 4,
      "discussionId": "686237f79e7509383d29abff",
      "ai_summary": "Mixture of Grouped Experts (MoGE) improves expert load balancing and execution efficiency for large language models, enhancing throughput and cost-to-performance on Ascend NPUs.",
      "ai_keywords": [
        "Mixture of Experts (MoE)",
        "Mixture of Grouped Experts (MoGE)",
        "large language models",
        "expert load balancing",
        "computational load",
        "inference phase",
        "sparse model",
        "Ascend NPUs",
        "system simulation",
        "speculative acceleration",
        "Dense models",
        "GLM-Z1-32B",
        "Qwen3-32B"
      ]
    },
    "publishedAt": "2025-05-27T12:40:21.000Z",
    "title": "Pangu Pro MoE: Mixture of Grouped Experts for Efficient Sparsity",
    "summary": "The surgence of Mixture of Experts (MoE) in Large Language Models promises a\nsmall price of execution cost for a much larger model parameter count and\nlearning capacity, because only a small fraction of parameters are activated\nfor each input token. However, it is commonly observed that some experts are\nactivated far more often than others, leading to system inefficiency when\nrunning the experts on different devices in parallel. Therefore, we introduce\nMixture of Grouped Experts (MoGE), which groups the experts during selection\nand balances the expert workload better than MoE in nature. It constrains\ntokens to activate an equal number of experts within each predefined expert\ngroup. When a model execution is distributed on multiple devices, this\narchitectural design ensures a balanced computational load across devices,\nsignificantly enhancing throughput, particularly for the inference phase.\nFurther, we build Pangu Pro MoE on Ascend NPUs, a sparse model based on MoGE\nwith 72 billion total parameters, 16 billion of which are activated for each\ntoken. The configuration of Pangu Pro MoE is optimized for Ascend 300I Duo and\n800I A2 through extensive system simulation studies. Our experiments indicate\nthat MoGE indeed leads to better expert load balancing and more efficient\nexecution for both model training and inference on Ascend NPUs. The inference\nperformance of Pangu Pro MoE achieves 1148 tokens/s per card and can be further\nimproved to 1528 tokens/s per card by speculative acceleration, outperforming\ncomparable 32B and 72B Dense models. Furthermore, we achieve an excellent\ncost-to-performance ratio for model inference on Ascend 300I Duo. Our studies\nshow that Ascend NPUs are capable of training Pangu Pro MoE with massive\nparallelization to make it a leading model within the sub-100B total parameter\nclass, outperforming prominent open-source models like GLM-Z1-32B and\nQwen3-32B.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21411.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a369d98c0c89dcae3b8329",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a369d98c0c89dcae3b8329/AiH2zjy1cnt9OADAAZMLD.jpeg",
      "fullname": "Adina Yakefu",
      "name": "AdinaY",
      "type": "user",
      "isPro": true,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 774
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.22419",
      "authors": [
        {
          "_id": "686229249e7509383d29abd0",
          "name": "Bingchen Zhao",
          "hidden": false
        },
        {
          "_id": "686229249e7509383d29abd1",
          "name": "Despoina Magka",
          "hidden": false
        },
        {
          "_id": "686229249e7509383d29abd2",
          "name": "Minqi Jiang",
          "hidden": false
        },
        {
          "_id": "686229249e7509383d29abd3",
          "name": "Xian Li",
          "hidden": false
        },
        {
          "_id": "686229249e7509383d29abd4",
          "name": "Roberta Raileanu",
          "hidden": false
        },
        {
          "_id": "686229249e7509383d29abd5",
          "name": "Tatiana Shavrina",
          "hidden": false
        },
        {
          "_id": "686229249e7509383d29abd6",
          "name": "Jean-Christophe Gagnon-Audet",
          "hidden": false
        },
        {
          "_id": "686229249e7509383d29abd7",
          "name": "Kelvin Niu",
          "hidden": false
        },
        {
          "_id": "686229249e7509383d29abd8",
          "name": "Shagun Sodhani",
          "hidden": false
        },
        {
          "_id": "686229249e7509383d29abd9",
          "name": "Michael Shvartsman",
          "hidden": false
        },
        {
          "_id": "686229249e7509383d29abda",
          "name": "Andrei Lupu",
          "hidden": false
        },
        {
          "_id": "686229249e7509383d29abdb",
          "name": "Alisia Lupidi",
          "hidden": false
        },
        {
          "_id": "686229249e7509383d29abdc",
          "name": "Edan Toledo",
          "hidden": false
        },
        {
          "_id": "686229249e7509383d29abdd",
          "name": "Karen Hambardzumyan",
          "hidden": false
        },
        {
          "_id": "686229249e7509383d29abde",
          "name": "Martin Josifoski",
          "hidden": false
        },
        {
          "_id": "686229249e7509383d29abdf",
          "name": "Thomas Foster",
          "hidden": false
        },
        {
          "_id": "686229249e7509383d29abe0",
          "name": "Lucia Cipolina-Kun",
          "hidden": false
        },
        {
          "_id": "686229249e7509383d29abe1",
          "name": "Abhishek Charnalia",
          "hidden": false
        },
        {
          "_id": "686229249e7509383d29abe2",
          "name": "Derek Dunfield",
          "hidden": false
        },
        {
          "_id": "686229249e7509383d29abe3",
          "name": "Alexander H. Miller",
          "hidden": false
        },
        {
          "_id": "686229249e7509383d29abe4",
          "name": "Oisin Mac Aodha",
          "hidden": false
        },
        {
          "_id": "686229249e7509383d29abe5",
          "name": "Jakob Foerster",
          "hidden": false
        },
        {
          "_id": "686229249e7509383d29abe6",
          "name": "Yoram Bachrach",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-27T17:44:32.000Z",
      "submittedOnDailyAt": "2025-06-30T06:29:15.385Z",
      "title": "Automatisation de la vitesse d'entraînement de LLM : Réplication et amélioration de NanoGPT",
      "submittedOnDailyBy": {
        "_id": "62dcd71075e9787ec5aa41ba",
        "avatarUrl": "/avatars/f37ce036b76180ed0fa004f9c8c09363.svg",
        "isPro": true,
        "fullname": "Bingchen Zhao",
        "user": "tennant",
        "type": "user"
      },
      "summary": "Le développement rapide des modèles de langage grands (LLMs) peut être d'une grande aide au développement scientifique. L'une des capacités importantes de ces modèles est leur pouvoir de répliquer des activités existantes. Pour évaluer cette capacité de reproduction de résultats dans divers domaines de recherche, nous avons utilisé la contribution de la communauté NanoGPT speedrun pour introduire un cadre de référence automatisé pour le speedrunning de LLMs. Dans ce concours, l'objectif est d'entraîner le modèle GPT-2 au plus court délai possible. Pour chacune des 19 tâches de speedrun, l'entité reçoit les registres et scripts d'entraînement précédents et peut choisir d'obtenir une des trois formats de pistes disponibles. Ces pistes peuvent être décrites comme du code de programmation ou comme un article. Les registres sont conçus pour être rapides, et les améliorations des speedruns incluent des développements algorithmiques avancés jusqu'à des optimisations liées au matériel, ainsi que des modifications à différents niveaux de code. Ces capacités montrent la valeur et l'utilité d'améliorer l'entraînement des LLMs, mais aussi que la technologie nécessaire, bien qu'elle soit nécessaire, ne suffit pas. La combinaison de modèles récents et des meilleures techniques (SoTA) est difficile à reproduire, même avec des pistes disponibles, dans notre cadre de référence. Ce cadre de référence montre que la capacité de reproduire automatiquement la science scientifique dans les LLMs est simple, mais pas suffisante.",
      "upvotes": 2,
      "discussionId": "686229249e7509383d29abe7",
      "ai_summary": "An Automated LLM Speedrunning Benchmark evaluates AI agents' ability to reproduce scientific results by leveraging NanoGPT speedrun tasks, indicating that even recent reasoning LLMs struggle with re-implementing known improvements.",
      "ai_keywords": [
        "large language models",
        "LLMs",
        "AI agents",
        "Automated LLM Speedrunning Benchmark",
        "NanoGPT speedrun",
        "GPT-2",
        "high-level algorithmic advancements",
        "hardware-aware optimizations"
      ]
    },
    "publishedAt": "2025-06-27T13:44:32.000Z",
    "title": "The Automated LLM Speedrunning Benchmark: Reproducing NanoGPT\n  Improvements",
    "summary": "Rapid advancements in large language models (LLMs) have the potential to\nassist in scientific progress. A critical capability toward this endeavor is\nthe ability to reproduce existing work. To evaluate the ability of AI agents to\nreproduce results in an active research area, we introduce the Automated LLM\nSpeedrunning Benchmark, leveraging the research community contributions on the\nNanoGPT speedrun, a competition to train a GPT-2 model in the shortest time.\nEach of the 19 speedrun tasks provides the agent with the previous records\ntraining script, optionally paired with one of three hint formats, ranging from\npseudocode to paper-like descriptions of the new records improvements. Records\nexecute quickly by design and speedrun improvements encompass diverse\ncode-level changes, ranging from high-level algorithmic advancements to\nhardware-aware optimizations. These features make the benchmark both accessible\nand realistic for the frontier problem of improving LLM training. We find that\nrecent reasoning LLMs combined with SoTA scaffolds struggle to reimplement\nalready-known innovations in our benchmark, even when given detailed hints. Our\nbenchmark thus provides a simple, non-saturated measure of an LLMs ability to\nautomate scientific reproduction, a necessary (but not sufficient) skill for an\nautonomous research agent.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.22419.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62dcd71075e9787ec5aa41ba",
      "avatarUrl": "/avatars/f37ce036b76180ed0fa004f9c8c09363.svg",
      "fullname": "Bingchen Zhao",
      "name": "tennant",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.21594",
      "authors": [
        {
          "_id": "68625a4c9e7509383d29ac4c",
          "name": "Ahmed M. Adly",
          "hidden": false
        },
        {
          "_id": "68625a4c9e7509383d29ac4d",
          "name": "Mostafa Samy",
          "hidden": false
        },
        {
          "_id": "68625a4c9e7509383d29ac4e",
          "name": "Amr Fawzy",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-18T09:44:21.000Z",
      "submittedOnDailyAt": "2025-06-30T08:07:09.494Z",
      "title": "Gazelle-R1 : Le plus avancé en logique médicale atteint avec des paramètres d'entraînement efficaces en deux étapes",
      "submittedOnDailyBy": {
        "_id": "63aca106e3b217fb36cf1950",
        "avatarUrl": "/avatars/b37cc9102f875b6ce0c55a294c052078.svg",
        "isPro": false,
        "fullname": "Ahmed Mostafa",
        "user": "AhmedMostafa",
        "type": "user"
      },
      "summary": "GAZAL-R1 se présente comme un modèle de langage à 320 milliards de paramètres. Ce modèle atteint le meilleur rendement en logique médicale et fournit une explication étape par étape pour la prise de décisions cliniques. Basé sur Qwen3 32B, GAZAL-R1 a démontré que un modèle de taille intermédiaire peut surpasser des modèles beaucoup plus grands dans des spécialités spécifiques grâce à un entraînement stratégique. Un nouveau processus d'entraînement en deux étapes a été développé. Tout d'abord, un ajustement de sous-promotion a été réalisé à l'aide d'une collection de 107 033 exemples de logique médicale synthétique, enseignant un pensée clinique structurée et ajoutant des techniques efficaces en paramètres comme Weight-Decomposed Low-Rank Adaptation (DoRA) et Rank-Stabilized LoRA (rsLoRA). Ensuite, le Group Relative Policy Optimization (GRPO) a été utilisé pour réaliser l'apprentissage par renforcement et un complexe système d'entraînement multi-composant a été mis en place pour améliorer la précision, la conformité formelle et la qualité logique. GAZAL-R1 a enregistré un excellent rendement sur des benchmarks médicaux, atteignant 87,1% sur MedQA, 81,6% sur MMLU Pro (Médecine) et 79,6% sur PubMedQA, surpassant significativement des modèles beaucoup plus grands. Ce travail offre une compréhension profonde des défis dans l'entraînement de modèles de logique dans des spécialités, expliquant des problèmes comme l'instabilité de l'entraînement, la mémoire factuelle et la tension fondamentale entre détails logiques. Notre approche fournit un cadre reproductible pour le développement de modèles de langage de haut rendement dans des spécialités, équilibrant le rendement, l'efficacité et l'explicabilité.",
      "upvotes": 2,
      "discussionId": "68625a4c9e7509383d29ac4f",
      "ai_summary": "Gazal-R1, a 32-billion-parameter language model, achieves top performance in medical reasoning through strategic training, including advanced parameter-efficient techniques and reinforcement learning, providing detailed explanations for clinical decisions.",
      "ai_keywords": [
        "Weight-Decomposed Low-Rank Adaptation (DoRA)",
        "Rank-Stabilized LoRA (rsLoRA)",
        "Group Relative Policy Optimization (GRPO)",
        "MedQA",
        "MMLU Pro (Medical)",
        "PubMedQA",
        "reasoning-capable models",
        "reward hacking",
        "training instability"
      ]
    },
    "publishedAt": "2025-06-18T05:44:21.000Z",
    "title": "Gazal-R1: Achieving State-of-the-Art Medical Reasoning with\n  Parameter-Efficient Two-Stage Training",
    "summary": "We present Gazal-R1, a 32-billion-parameter language model that achieves\nstate-of-the-art performance in medical reasoning while providing transparent,\nstep-by-step explanations for clinical decision-making. Built upon Qwen3 32B,\nour model demonstrates that strategic training can enable mid-sized models to\noutperform significantly larger counterparts in specialized domains. We\ndeveloped a novel two-stage training pipeline: first, supervised fine-tuning on\na carefully curated dataset of 107,033 synthetic medical reasoning examples\nthat teaches structured clinical thinking, enhanced by advanced\nparameter-efficient techniques including Weight-Decomposed Low-Rank Adaptation\n(DoRA) and Rank-Stabilized LoRA (rsLoRA); second, reinforcement learning using\nGroup Relative Policy Optimization (GRPO) with a sophisticated multi-component\nreward system that refines accuracy, format adherence, and reasoning quality.\nGazal-R1 achieves exceptional performance across medical benchmarks, scoring\n87.1% on MedQA, 81.6% on MMLU Pro (Medical), and 79.6% on PubMedQA, surpassing\nmodels up to 12x larger. Beyond its strong empirical results, this work\nprovides detailed insights into the challenges of training reasoning-capable\nmodels in specialized domains, including issues with reward hacking, training\ninstability, and the fundamental tension between factual recall and detailed\nreasoning. Our methodology offers a reproducible framework for developing\nhigh-capability, domain-specific language models that balance performance,\nefficiency, and explainability.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.21594.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63aca106e3b217fb36cf1950",
      "avatarUrl": "/avatars/b37cc9102f875b6ce0c55a294c052078.svg",
      "fullname": "Ahmed Mostafa",
      "name": "AhmedMostafa",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.22149",
      "authors": [
        {
          "_id": "68625f5d9e7509383d29ac62",
          "name": "Ronald Fecso",
          "hidden": false
        },
        {
          "_id": "68625f5d9e7509383d29ac63",
          "name": "José Morano",
          "hidden": false
        },
        {
          "_id": "68625f5d9e7509383d29ac64",
          "name": "Ursula Schmidt-Erfurth",
          "hidden": false
        },
        {
          "_id": "68625f5d9e7509383d29ac65",
          "name": "Hrvoje Bogunović",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-27T11:53:54.000Z",
      "submittedOnDailyAt": "2025-06-30T08:31:01.610Z",
      "title": "RetFiner : Stratégie d'amélioration du langage visuel basée sur des modèles de couche de lait",
      "submittedOnDailyBy": {
        "_id": "655b3383ed8df831286969f0",
        "avatarUrl": "/avatars/38f9a73c6ec40ba0e00de5bffec03bc0.svg",
        "isPro": false,
        "fullname": "José Morano",
        "user": "j-morano",
        "type": "user"
      },
      "summary": "Le développement de la technologie des incitations vidéo et de l'apprentissage profond (DL) a permis aux médecins et chercheurs de classifier les maladies de la membrane limitante en temps réel. La popularité de l'apprentissage basé sur la privacité (SSL) pour l'entraînement de la réseau neuronale est remarquable, car elle permet de réduire les coûts en utilisant des données non étiquetées. Bien que SSL ait facilité le développement de modèles de base (FM), les FM actuels pour OCT sont entraînés uniquement sur des données d'image, ce qui limite leur compréhension des détails des images. Cela se manifeste clairement dans les performances des données d'entraînement, surtout pour des tâches complexes. Ces FM doivent être ajustés pour être efficaces dans des applications spécifiques ou des populations, ce qui nécessite une rétroaction (même si, dans certains cas, elle peut être impraticable). Pour résoudre ces problèmes, nous proposons RetFiner. RetFiner est un ajustement de langage visuel basé sur la privacité, qui améliore la représentation des FM actuels et les applique de manière efficace à des populations spécifiques. Notre méthode utilise des signaux riches de sous-objets de données textuelles riches pour entraîner différents objectifs. RetFiner a été validé pour RETFound, UrFound et VisionFM de OCT, et a montré un amélioration significative de la performance d'apprentissage linéaire dans 7 tâches très diverses de classification d'OCT. En moyenne, on observe un amélioration de 5,8, 3,9 et 2,1 points pourcentaires par rapport aux données de référence. Notre code et les poids du modèle sont disponibles sur https://github.com/ronnief1/RetFiner.",
      "upvotes": 1,
      "discussionId": "68625f5d9e7509383d29ac66",
      "githubRepo": "https://github.com/ronnief1/RetFiner",
      "ai_summary": "RetFiner, a vision-language refinement scheme, enhances self-supervised foundation models for OCT by leveraging textual data, improving their downstream performance in retinal disease classification tasks.",
      "ai_keywords": [
        "optical coherence tomography (OCT)",
        "deep learning (DL)",
        "self-supervised learning (SSL)",
        "foundation models (FMs)",
        "supervised fine-tuning",
        "RetFiner",
        "vision-language refinement",
        "linear probing performance"
      ],
      "githubStars": 2
    },
    "publishedAt": "2025-06-27T07:53:54.000Z",
    "title": "RetFiner: A Vision-Language Refinement Scheme for Retinal Foundation\n  Models",
    "summary": "The rise of imaging techniques such as optical coherence tomography (OCT) and\nadvances in deep learning (DL) have enabled clinicians and researchers to\nstreamline retinal disease staging. A popular DL approach is self-supervised\nlearning (SSL), where models learn from vast amounts of unlabeled data,\navoiding costly annotation. SSL has allowed the development of foundation\nmodels (FMs), large models that can be used for a variety of downstream tasks.\nHowever, existing FMs for OCT, trained solely on image data, lack a\ncomprehensive and robust semantic understanding of images, as evidenced by\ntheir downstream performance (especially for complex tasks), and thus require\nsupervised fine-tuning (which may be unfeasible) to better adapt to specific\napplications and populations. To address this, we propose RetFiner, an SSL\nvision-language refinement scheme that improves the representations of existing\nFMs and enables their efficient and direct adaptation to specific populations\nfor improved downstream performance. Our method uses a diverse set of training\nobjectives which take advantage of the rich supervisory signal found in textual\ndata. We tested RetFiner on the retinal FMs RETFound, UrFound, and VisionFM,\nshowing significant improvements in linear probing performance on seven highly\ndiverse OCT classification tasks, with an average increase of 5.8, 3.9, and 2.1\npercentage points over their baselines, respectively. Our code and model\nweights are publicly available at https://github.com/ronnief1/RetFiner.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.22149.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "655b3383ed8df831286969f0",
      "avatarUrl": "/avatars/38f9a73c6ec40ba0e00de5bffec03bc0.svg",
      "fullname": "José Morano",
      "name": "j-morano",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  }
]