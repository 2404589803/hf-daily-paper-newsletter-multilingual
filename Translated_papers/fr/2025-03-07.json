[
  {
    "paper": {
      "id": "2503.04625",
      "authors": [
        {
          "_id": "67ca670d3e81e3344dc4c2d9",
          "user": {
            "_id": "65294b334d7cf551ac50d6a6",
            "avatarUrl": "/avatars/75d21e20b711b871616ef3850bb900b7.svg",
            "isPro": false,
            "fullname": "ChengpengLi",
            "user": "ChengpengLi",
            "type": "user"
          },
          "name": "Chengpeng Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:12:37.350Z",
          "hidden": false
        },
        {
          "_id": "67ca670d3e81e3344dc4c2da",
          "user": {
            "_id": "5f8946925d083370c711f296",
            "avatarUrl": "/avatars/14246aae3b1f8b7ad050f8ff2c8b260e.svg",
            "isPro": false,
            "fullname": "Mingfeng Xue",
            "user": "mingfengxue",
            "type": "user"
          },
          "name": "Mingfeng Xue",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:12:28.354Z",
          "hidden": false
        },
        {
          "_id": "67ca670d3e81e3344dc4c2db",
          "user": {
            "_id": "64704e973601bb7b06643e98",
            "avatarUrl": "/avatars/52e51f4d1be6769e4397b8be2799cf32.svg",
            "isPro": false,
            "fullname": "Zhang",
            "user": "Zhenru",
            "type": "user"
          },
          "name": "Zhenru Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:12:48.194Z",
          "hidden": false
        },
        {
          "_id": "67ca670d3e81e3344dc4c2dc",
          "user": {
            "_id": "646df403ad20c6fa4f30b7ec",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646df403ad20c6fa4f30b7ec/Q64-XMghOcBoo3itZDGYA.jpeg",
            "isPro": false,
            "fullname": "Jiaxi Yang",
            "user": "jx-yang",
            "type": "user"
          },
          "name": "Jiaxi Yang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:12:57.082Z",
          "hidden": false
        },
        {
          "_id": "67ca670d3e81e3344dc4c2dd",
          "user": {
            "_id": "64b93578ee257c3a4cfceed1",
            "avatarUrl": "/avatars/e6188562254f75a09b4048b800860016.svg",
            "isPro": false,
            "fullname": "Beichen Zhang",
            "user": "BeichenZhang",
            "type": "user"
          },
          "name": "Beichen Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:13:11.641Z",
          "hidden": false
        },
        {
          "_id": "67ca670d3e81e3344dc4c2de",
          "name": "Xiang Wang",
          "hidden": false
        },
        {
          "_id": "67ca670d3e81e3344dc4c2df",
          "user": {
            "_id": "6583ab7983a9e1460c67d876",
            "avatarUrl": "/avatars/74400bc448c3f07e23a4cd53d68a6af7.svg",
            "isPro": false,
            "fullname": "bowen",
            "user": "bowenYu",
            "type": "user"
          },
          "name": "Bowen Yu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:13:30.530Z",
          "hidden": false
        },
        {
          "_id": "67ca670d3e81e3344dc4c2e0",
          "user": {
            "_id": "61e4c4ca1ab24785ac11ba69",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61e4c4ca1ab24785ac11ba69/1Q1zhhyGSJ9RJG9MzwxVv.jpeg",
            "isPro": false,
            "fullname": "Binyuan Hui",
            "user": "huybery",
            "type": "user"
          },
          "name": "Binyuan Hui",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:13:37.341Z",
          "hidden": false
        },
        {
          "_id": "67ca670d3e81e3344dc4c2e1",
          "user": {
            "_id": "620760a26e3b7210c2ff1943",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/620760a26e3b7210c2ff1943/VC-rKqimF6yxGESNVlPoR.jpeg",
            "isPro": false,
            "fullname": "Junyang Lin",
            "user": "JustinLin610",
            "type": "user"
          },
          "name": "Junyang Lin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:13:44.084Z",
          "hidden": false
        },
        {
          "_id": "67ca670d3e81e3344dc4c2e2",
          "user": {
            "_id": "6434d4989bd5a84b5dd0b0f5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6434d4989bd5a84b5dd0b0f5/0Elf9qbfG9Hkgypm9pTGm.jpeg",
            "isPro": false,
            "fullname": "Dayiheng Liu",
            "user": "Losin94",
            "type": "user"
          },
          "name": "Dayiheng Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:13:52.711Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-06T17:11:51.000Z",
      "title": "Autodidacte Réfléchisseur, utilisant des outils.",
      "summary": "Les modèles logiques de haut niveau (LRMs), comme OpenAI-o1 et DeepSeek-R1, montrent une excellente capacité pour des tâches logiques complexes en utilisant des chaînes de raisonnement longues (CoT). Cependant, ces modèles peuvent dépendre simplement de leurs processus logiques internes, ce qui peut conduire à des conversations excellentes et à des expressions inadéquates. Dans cet article, nous présentons START (Self-Taught Reasoner with Tools), un modèle de logique longue de chaîne (LLM) qui intègre des outils externes et améliore considérablement sa capacité logique. START peut exécuter des calculs complexes, effectuer des vérifications autonomes, explorer diverses méthodologies et réaliser un auto-debugging en exécutant du code, ce qui résout les limitations des LRMs. L'innovation clé de START est un cadre d'apprentissage automatique, composé de deux technologies principales : 1) Hint-infer : insère des hints artificiellement conçus (par exemple, « C'est une bonne idée d'utiliser Python là ») dans les erreurs logiques pour inciter les LRMs à utiliser des outils externes. Hint-infer est un méthode efficace et séquentielle d'accroissement du temps ; 2) Hint Rejection Sampling Fine-Tuning (Hint-RFT) : combine Hint-infer et RFT pour évaluer, filtrer, changer et finalement ajuster le processus logique des LRMs générés par Hint-infer. Ce cadre de travail a été utilisé pour fine-tuner le modèle QwQ-32B dans START. Dans l'évaluation de la pré-préparation de la science (GPQA), le rendement dans les tests de mathématiques de niveau de compétence (AMC23, AIME24, AIME25) et dans les tests de code de niveau de compétence (LiveCodeBench) était de 63.6%, 95.0%, 66.7%, 47.1% et 47.3% respectivement. Ces résultats dépassent considérablement les modèles de base et atteignent un rendement égal à celui des modèles les plus récents de Open Weight (R1-Distill-Qwen-32B) et des modèles de propriété (o1-Preview).",
      "upvotes": 36,
      "discussionId": "67ca67103e81e3344dc4c366"
    },
    "publishedAt": "2025-03-06T22:35:47.725Z",
    "title": "START: Self-taught Reasoner with Tools",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04625.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6299
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.03803",
      "authors": [
        {
          "_id": "67ca874c3ac187dbbed924d6",
          "user": {
            "_id": "62b5777f593a2c49da69dc02",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1658152070753-62b5777f593a2c49da69dc02.jpeg",
            "isPro": false,
            "fullname": "Jingkang Yang",
            "user": "Jingkang",
            "type": "user"
          },
          "name": "Jingkang Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-07T09:09:32.949Z",
          "hidden": false
        },
        {
          "_id": "67ca874c3ac187dbbed924d7",
          "name": "Shuai Liu",
          "hidden": false
        },
        {
          "_id": "67ca874c3ac187dbbed924d8",
          "name": "Hongming Guo",
          "hidden": false
        },
        {
          "_id": "67ca874c3ac187dbbed924d9",
          "user": {
            "_id": "652965773a416e1f2173443b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652965773a416e1f2173443b/y9MB8YgHzbwCXAc4EI9T3.jpeg",
            "isPro": false,
            "fullname": "Yuhao Dong",
            "user": "THUdyh",
            "type": "user"
          },
          "name": "Yuhao Dong",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:15:21.671Z",
          "hidden": false
        },
        {
          "_id": "67ca874c3ac187dbbed924da",
          "name": "Xiamengwei Zhang",
          "hidden": false
        },
        {
          "_id": "67ca874c3ac187dbbed924db",
          "user": {
            "_id": "63f87c42b0ae1748524a9cfb",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f87c42b0ae1748524a9cfb/I5ukv6iWoJVToWmcERmvx.jpeg",
            "isPro": false,
            "fullname": "Sicheng Zhang",
            "user": "fesvhtr",
            "type": "user"
          },
          "name": "Sicheng Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:15:37.377Z",
          "hidden": false
        },
        {
          "_id": "67ca874c3ac187dbbed924dc",
          "user": {
            "_id": "62f113d3b58090c873d66481",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1659966415211-noauth.jpeg",
            "isPro": false,
            "fullname": "Pengyun Wang",
            "user": "Alarak",
            "type": "user"
          },
          "name": "Pengyun Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:15:44.290Z",
          "hidden": false
        },
        {
          "_id": "67ca874c3ac187dbbed924dd",
          "user": {
            "_id": "668eb3a1a2f3f9d5edf029eb",
            "avatarUrl": "/avatars/383636e449f5e48c790f428818dd6863.svg",
            "isPro": false,
            "fullname": "zhou zitang",
            "user": "Zzitang",
            "type": "user"
          },
          "name": "Zitang Zhou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:15:56.178Z",
          "hidden": false
        },
        {
          "_id": "67ca874c3ac187dbbed924de",
          "user": {
            "_id": "63f886a99f87cc3e645c99a8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f886a99f87cc3e645c99a8/qwj16BrFaDjN0DPFsJ-6v.jpeg",
            "isPro": false,
            "fullname": "Binzhu Xie",
            "user": "Nicous",
            "type": "user"
          },
          "name": "Binzhu Xie",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:16:02.570Z",
          "hidden": false
        },
        {
          "_id": "67ca874c3ac187dbbed924df",
          "name": "Ziyue Wang",
          "hidden": false
        },
        {
          "_id": "67ca874c3ac187dbbed924e0",
          "name": "Bei Ouyang",
          "hidden": false
        },
        {
          "_id": "67ca874c3ac187dbbed924e1",
          "name": "Zhengyu Lin",
          "hidden": false
        },
        {
          "_id": "67ca874c3ac187dbbed924e2",
          "name": "Marco Cominelli",
          "hidden": false
        },
        {
          "_id": "67ca874c3ac187dbbed924e3",
          "user": {
            "_id": "652d06833b5997ed71ce5c46",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/xZTXEcnEogEmBm_ledJQr.jpeg",
            "isPro": false,
            "fullname": "Zhongang Cai",
            "user": "caizhongang",
            "type": "user"
          },
          "name": "Zhongang Cai",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:16:33.666Z",
          "hidden": false
        },
        {
          "_id": "67ca874c3ac187dbbed924e4",
          "user": {
            "_id": "62a993d80472c0b7f94027df",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62a993d80472c0b7f94027df/j5vp-IwLA2YBexylUHiQU.png",
            "isPro": false,
            "fullname": "Zhang Yuanhan",
            "user": "ZhangYuanhan",
            "type": "user"
          },
          "name": "Yuanhan Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:16:45.989Z",
          "hidden": false
        },
        {
          "_id": "67ca874c3ac187dbbed924e5",
          "user": {
            "_id": "63565cc56d7fcf1bedb7d347",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63565cc56d7fcf1bedb7d347/XGcHP4VkO_oieA1gZ4IAX.jpeg",
            "isPro": false,
            "fullname": "Zhang Peiyuan",
            "user": "PY007",
            "type": "user"
          },
          "name": "Peiyuan Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:16:57.532Z",
          "hidden": false
        },
        {
          "_id": "67ca874c3ac187dbbed924e6",
          "user": {
            "_id": "67443675924e80c3c8807b40",
            "avatarUrl": "/avatars/fb45422391e51d2ad641f09c8535653c.svg",
            "isPro": false,
            "fullname": "fangzhou HONG",
            "user": "h12345678",
            "type": "user"
          },
          "name": "Fangzhou Hong",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:17:05.845Z",
          "hidden": false
        },
        {
          "_id": "67ca874c3ac187dbbed924e7",
          "name": "Joerg Widmer",
          "hidden": false
        },
        {
          "_id": "67ca874c3ac187dbbed924e8",
          "name": "Francesco Gringoli",
          "hidden": false
        },
        {
          "_id": "67ca874c3ac187dbbed924e9",
          "name": "Lei Yang",
          "hidden": false
        },
        {
          "_id": "67ca874c3ac187dbbed924ea",
          "name": "Bo Li",
          "hidden": false
        },
        {
          "_id": "67ca874c3ac187dbbed924eb",
          "user": {
            "_id": "62ab1ac1d48b4d8b048a3473",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656826685333-62ab1ac1d48b4d8b048a3473.png",
            "isPro": false,
            "fullname": "Ziwei Liu",
            "user": "liuziwei7",
            "type": "user"
          },
          "name": "Ziwei Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:15:05.677Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-05T18:54:16.000Z",
      "title": "Ceci est la vie à la maison pour aider\n\nEgoLife: \"L'assistant de vie pour le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Le but d'EgoLife pour aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"\n\nCeci est la vie à la maison pour aider\n\nEgoLife: \"Aider à la vie dans le mode ego\"",
      "summary": "Ce projet présente un assistant de vie quotidienne centré sur soi développé en utilisant le egolife et le cadre de travail AI proberto. Cet assistant enregistre de manière continue ses activités quotidiennes à l'aide de caméras, capture différents vidéos centrées sur soi avec des lentilles d'IA et obtient des vidéos qui reflètent la perspective des tiers. Dans cette étude, des ensembles de données quotidiennes de 300 heures ont été générés, comprenant des données centrées sur soi, interprétatives, multivisuelles et multimodaux, ainsi que des ensembles de données egolife. Ces ensembles de données sont utilisés pour présenter des tâches de réponse à des questions avec un contexte long et axées sur la vie quotidienne, ainsi que des réponses à des questions egolife. En résolvant des questions spécifiques, l'assistant peut rappeler des événements passés, surveiller les habitudes de santé et fournir des recommandations personnalisées. Un système intégré de egobater, egoGPT et egoRAG est présenté pour développer, identifier et répondre à des questions de contexte long. EgoGPT est entraîné avec des ensembles de données centrées sur soi et atteint le meilleur rendement dans la compréhension de vidéos centrées sur soi. EgoRAG est un composant basé sur la recherche pour répondre à des questions de contexte long. Les structures fonctionnelles de ce système sont testées et les problèmes critiques et les points de fusion sont identifiés. Les ensembles de données, les modèles et les cadres de référence sont publiés pour encourager le développement d'assistants d'IA centrés sur soi.",
      "upvotes": 9,
      "discussionId": "67ca874f3ac187dbbed925cc",
      "projectPage": "https://egolife-ai.github.io/",
      "githubRepo": "https://github.com/EvolvingLMMs-Lab/EgoLife"
    },
    "publishedAt": "2025-03-07T00:44:13.546Z",
    "title": "EgoLife: Towards Egocentric Life Assistant",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.03803.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62b5777f593a2c49da69dc02",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1658152070753-62b5777f593a2c49da69dc02.jpeg",
      "fullname": "Jingkang Yang",
      "name": "Jingkang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.04598",
      "authors": [
        {
          "_id": "67ca69063a6e3e8656bcc1d2",
          "user": {
            "_id": "66335b9c95c5b79ebf306f30",
            "avatarUrl": "/avatars/d57784ee65cbef014360c9bac1ad4119.svg",
            "isPro": false,
            "fullname": "Zhijian Zhuo",
            "user": "BryceZhuo",
            "type": "user"
          },
          "name": "Zhijian Zhuo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:27:08.401Z",
          "hidden": false
        },
        {
          "_id": "67ca69063a6e3e8656bcc1d3",
          "user": {
            "_id": "6371128eafbe42caa5a5222b",
            "avatarUrl": "/avatars/c3b2ab35949c38aa3dfb2657a1300aac.svg",
            "isPro": false,
            "fullname": "Yutao Zeng",
            "user": "Taoer",
            "type": "user"
          },
          "name": "Yutao Zeng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-07T09:09:43.334Z",
          "hidden": false
        },
        {
          "_id": "67ca69063a6e3e8656bcc1d4",
          "name": "Ya Wang",
          "hidden": false
        },
        {
          "_id": "67ca69063a6e3e8656bcc1d5",
          "name": "Sijun Zhang",
          "hidden": false
        },
        {
          "_id": "67ca69063a6e3e8656bcc1d6",
          "name": "Jian Yang",
          "hidden": false
        },
        {
          "_id": "67ca69063a6e3e8656bcc1d7",
          "user": {
            "_id": "64648638351adef1a847a7ad",
            "avatarUrl": "/avatars/7518e058fcf81ee81a06c96e996531e9.svg",
            "isPro": false,
            "fullname": "Xiaoqing Li",
            "user": "LLIXQ",
            "type": "user"
          },
          "name": "Xiaoqing Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:26:45.058Z",
          "hidden": false
        },
        {
          "_id": "67ca69063a6e3e8656bcc1d8",
          "name": "Xun Zhou",
          "hidden": false
        },
        {
          "_id": "67ca69063a6e3e8656bcc1d9",
          "user": {
            "_id": "663a684d08778abaf0556df8",
            "avatarUrl": "/avatars/d95b517df5b80a8b42bac2b171604742.svg",
            "isPro": false,
            "fullname": "Majinwen",
            "user": "Breeze0417",
            "type": "user"
          },
          "name": "Jinwen Ma",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:27:23.639Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-06T16:40:48.000Z",
      "title": "HybridNorm : Stabilisation, entraînement de Transformers anciens et bruités, par le biais du hybride de normalisation.",
      "summary": "Transformers occupent une position fondamentale en tant qu'architecture dans une large gamme de tâches d'apprentissage automatique, en particulier dans les modèles de langage à grande échelle (LLMs). Bien que dépassent les performances, des problèmes liés à la localisation de la normalisation dans les réseaux Transformer profonds existent. La structure Pre-Norm offre une identité plus claire et facilite l'entraînement. Cependant, son rendement est généralement inférieur à celle de la structure Post-Norm. Dans cet article, nous proposons une stratégie de normalisation hybride, appelée HybridNorm, qui intègre efficacement les avantages de Pre-Norm et Post-Norm. En particulier, HybridNorm utilise la normalisation de QKV dans la structure d'Attention et applique Post-Norm dans le Feed-Forward Network (FFN) de chaque bloc de Transformer. Cette découpe atteint à la fois la stabilité de l'entraînement et l'amélioration du rendement, notamment dans les domaines de la génération et de la compréhension de texte dans les LLMs. Des expériences détaillées sont réalisées sur les deux architectures dense et éparse, démontrant que HybridNorm obtient des résultats constamment excellents lorsqu'il est comparé à Pre-Norm et Post-Norm, et atteint les meilleurs résultats sur de nombreux benchmarks. Ces résultats montrent la possibilité de HybridNorm comme une forme stable et efficace pour améliorer l'entraînement et le rendement de modèles profonds de Transformer. Le code est disponible publiquement et peut être utilisé dans le dépôt GitHub https://github.com/BryceZhuo/HybridNorm.",
      "upvotes": 7,
      "discussionId": "67ca69073a6e3e8656bcc244",
      "projectPage": "https://github.com/BryceZhuo/HybridNorm",
      "githubRepo": "https://github.com/BryceZhuo/HybridNorm"
    },
    "publishedAt": "2025-03-06T23:04:06.421Z",
    "title": "HybridNorm: Towards Stable and Efficient Transformer Training via Hybrid Normalization",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6371128eafbe42caa5a5222b/DB_sfuRG7M-k8w6UVTgXy.png",
      "https://cdn-uploads.huggingface.co/production/uploads/6371128eafbe42caa5a5222b/F0lAhIiju8M-0fKBaPATA.png",
      "https://cdn-uploads.huggingface.co/production/uploads/6371128eafbe42caa5a5222b/g_741Ez-YVcMK69EqCsPa.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04598.png",
    "numComments": 4,
    "submittedBy": {
      "_id": "6371128eafbe42caa5a5222b",
      "avatarUrl": "/avatars/c3b2ab35949c38aa3dfb2657a1300aac.svg",
      "fullname": "Yutao Zeng",
      "name": "Taoer",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.20258",
      "authors": [
        {
          "_id": "67ca7b557436e6327ca877ff",
          "user": {
            "_id": "655efd24afee0e00788bb589",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/655efd24afee0e00788bb589/22guLxIWNybbJR3jI-c4w.jpeg",
            "isPro": false,
            "fullname": "Amr Mohamed",
            "user": "amr-mohamed",
            "type": "user"
          },
          "name": "Amr Mohamed",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-07T09:09:41.298Z",
          "hidden": false
        },
        {
          "_id": "67ca7b557436e6327ca87800",
          "user": {
            "_id": "67890323f8796231c857231e",
            "avatarUrl": "/avatars/f5ccd5186968d880fee9c36324a5f713.svg",
            "isPro": false,
            "fullname": "Mingmeng Geng",
            "user": "mgeng",
            "type": "user"
          },
          "name": "Mingmeng Geng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:34:23.443Z",
          "hidden": false
        },
        {
          "_id": "67ca7b557436e6327ca87801",
          "name": "Michalis Vazirgiannis",
          "hidden": false
        },
        {
          "_id": "67ca7b557436e6327ca87802",
          "user": {
            "_id": "6087e598e2b7cc3a117b0dc5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6087e598e2b7cc3a117b0dc5/Ctz_W-uo1gOQRBHXalD1P.png",
            "isPro": false,
            "fullname": "Guokan Shang",
            "user": "guokan-shang",
            "type": "user"
          },
          "name": "Guokan Shang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-07T09:09:38.933Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-27T16:46:23.000Z",
      "title": "LLM ressemble à un bras téléphonique, où l'information est déformée en raison de la génération répétitive.",
      "summary": "Comme les grands modèles de langue, les responsabilités des modèles de contenu internet changent de fois en autres. On s'inquiète par le impact de traiter répétitivement leur sortie. En se basant sur la communication humaine continue, cette étude cherche à expliquer pourquoi les grands modèles de langue (LLM) peuvent déformer l'information par la génération répétitive. À travers des expériences basées sur la traduction, on observe que la déformation accumule au fil du temps et est affectée par la sélection du langage et la complexité des séquences. Bien que la détérioration ne puisse pas être évitée, elle peut être atténuée par des techniques stratégiques de prompting. Ces résultats contribuent à la discussion sur les effets à long terme de la propagation de l'information à travers l'IA, et soulèvent des questions importantes sur la confiance des contenus générés dans des flux de travail répétitifs avec des modèles de langue grands.",
      "upvotes": 6,
      "discussionId": "67ca7b577436e6327ca878ec",
      "githubRepo": "https://github.com/amr-mohamedd/LLM-as-a-Broken-Telephone"
    },
    "publishedAt": "2025-03-06T23:56:18.841Z",
    "title": "LLM as a Broken Telephone: Iterative Generation Distorts Information",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.20258.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "655efd24afee0e00788bb589",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/655efd24afee0e00788bb589/22guLxIWNybbJR3jI-c4w.jpeg",
      "fullname": "Amr Mohamed",
      "name": "amr-mohamed",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.04094",
      "authors": [
        {
          "_id": "67ca7bcc06501013d727a5d7",
          "user": {
            "_id": "6658e1c8ce1b2838885b2d7f",
            "avatarUrl": "/avatars/8623555f14b62f40fd372da20cb59ccc.svg",
            "isPro": false,
            "fullname": "Seth Karten",
            "user": "milkkarten",
            "type": "user"
          },
          "name": "Seth Karten",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-03-07T06:07:26.564Z",
          "hidden": false
        },
        {
          "_id": "67ca7bcc06501013d727a5d8",
          "name": "Andy Luu Nguyen",
          "hidden": false
        },
        {
          "_id": "67ca7bcc06501013d727a5d9",
          "user": {
            "_id": "66749c510974bbc971139f6a",
            "avatarUrl": "/avatars/bfab9d8d8bc589bb9bd49925b76e04a4.svg",
            "isPro": false,
            "fullname": "Chi Jin",
            "user": "chijin",
            "type": "user"
          },
          "name": "Chi Jin",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-07T04:53:33.942Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-06T05:06:27.000Z",
      "title": "PokéChamp : Agent de langage Minimax au niveau d'Expert",
      "summary": "PokeChamp est le modèle le plus petit d'agents minimum-maximum qui inclut un modèle de langage large (LLM). Il a été créé en s'appuyant sur un cadre général pour l'application dans l'entraînement de deux joueurs et a été renforcé en utilisant les capacités générales du LLM. Concrètement, il remplace trois modules clés pour réduire l'intervalle d'exploration en fonction de l'historique du jeu et de la connaissance humaine, et pour aborder l'observation partielle. En particulier, le cadre ne nécessite pas d'entraînements supplémentaires de LLM. PokeChamp est évalué dans le format de jeu de Pokémon Showdown. En configuré GPT-4o comme bot, il atteint 76% de victoires par rapport aux meilleurs bots basés sur un LLM et 84% par rapport aux plus puissants bots basés sur des règles, démontrant un excellent rendement. De plus, en utilisant le modèle open de Llama 3.1 avec 8 bits de paramètres, il atteint 64% de victoires plus que Poke'ellmon configuré avec GPT-4o. PokeChamp atteint une prédiction de 1300-1500 sur le classement de Pokémon Showdown en ligne, ce qui le place au sommet des 30%-10% des joueurs humains. De plus, dans cette recherche, une base de données la plus grande de Pokémon des joueurs réels a été construite, qui comprend plus de 3 millions de parties et 500 000 de rencontres de 1300-1500. Cette base de données est utilisée pour établir un benchmark de combat et de puzzles, et pour créer des évaluations et des puzzles spécifiques pour les techniques de combat. De plus, des mises à jour importantes pour le moteur de jeu local sont fournies. Cette étude vise à encourager la recherche qui applique des technologies de LLM et des algorithmes de théorie du jeu aux problèmes généraux multi-agents. Les vidéos, le code et la base de données sont disponibles sur l'URL fournie : https://sites.google.com/view/pokechamp-llm",
      "upvotes": 5,
      "discussionId": "67ca7bcd06501013d727a668",
      "projectPage": "https://sites.google.com/view/pokechamp-llm",
      "githubRepo": "https://github.com/sethkarten/pokechamp"
    },
    "publishedAt": "2025-03-06T23:53:38.838Z",
    "title": "PokéChamp: an Expert-level Minimax Language Agent",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04094.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6299
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.04222",
      "authors": [
        {
          "_id": "67ca64cdd153739fa9b9dbe6",
          "user": {
            "_id": "64c9b0f28d2d187c24d1e6c1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/1CPnAaB3gsupdpiNWaoDc.png",
            "isPro": false,
            "fullname": "ZiYi Yang",
            "user": "AALF",
            "type": "user"
          },
          "name": "Ziyi Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-07T09:09:46.287Z",
          "hidden": false
        },
        {
          "_id": "67ca64cdd153739fa9b9dbe7",
          "user": {
            "_id": "62ecbffd99112e99c5f7fded",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62ecbffd99112e99c5f7fded/U6iXAJbpm2vaC5qksEPiH.png",
            "isPro": false,
            "fullname": "Fanqi Wan",
            "user": "Wanfq",
            "type": "user"
          },
          "name": "Fanqi Wan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:33:34.691Z",
          "hidden": false
        },
        {
          "_id": "67ca64cdd153739fa9b9dbe8",
          "user": {
            "_id": "62b6d20416ff90e6198301b6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656148456743-noauth.png",
            "isPro": false,
            "fullname": "Longguang Zhong",
            "user": "GGLS",
            "type": "user"
          },
          "name": "Longguang Zhong",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:33:41.092Z",
          "hidden": false
        },
        {
          "_id": "67ca64cdd153739fa9b9dbe9",
          "user": {
            "_id": "63b93e6921add32ac6190b5c",
            "avatarUrl": "/avatars/7aa6a94d48e7f7c2bc56f8734d6c4e3d.svg",
            "isPro": false,
            "fullname": "Canbin Huang",
            "user": "OnewayLab",
            "type": "user"
          },
          "name": "Canbin Huang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:33:47.458Z",
          "hidden": false
        },
        {
          "_id": "67ca64cdd153739fa9b9dbea",
          "name": "Guosheng Liang",
          "hidden": false
        },
        {
          "_id": "67ca64cdd153739fa9b9dbeb",
          "user": {
            "_id": "63b57d75bda8d44adf2ff3ff",
            "avatarUrl": "/avatars/8a387036758b2f7fc7d7529dea206669.svg",
            "isPro": false,
            "fullname": "Xiaojun Quan",
            "user": "passerqxj",
            "type": "user"
          },
          "name": "Xiaojun Quan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:34:05.521Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-06T09:03:36.000Z",
      "title": "FuseChat-3.0 : Optimisation des Préférences et Combinaison de Modèles Différents",
      "summary": "FuseChat-3.0 est un système de grands modèles de langage (LLMs) conçu pour intégrer les forces de différentes sources de modèles LLMs. Les sources de modèles incluent Gemma-2-27B-it, Mistral-Large-Instruct-2407, Qwen-2.5-72B-Instruct et Llama-3.1-70B-Instruct. Dans les modèles cibles, l'accent est principalement mis sur des versions plus petites utilisées largement, comme Llama-3.1-8B-Instruct, Gemma-2-9B-it et Qwen-2.5-7B-Instruct. De plus, des options de réduction sont incluses, comme Llama-3.2-3B-Instruct et Llama-3.2-1B-Instruct. Pour exploiter les différentes capacités de ces modèles sources, un protocole de construction de données spécialisé a été développé pour chaque tâche et domaine. Le pipeline d'entraînement de FuseChat-3.0 comprend deux étapes principales : 1) l'ajustement de la distribution entre le modèle cible et les modèles sources par rétro-alimentation douce (SFT), et 2) l'optimisation directe des préférences (DPO) pour appliquer les préférences de plusieurs modèles LLMs sources et améliorer le modèle cible. Ce modèle de FuseChat-3.0 montre un amélioration significative du rendement dans des tâches générales, en mathématiques, en programmation et plus encore, selon les instructions. Selon la figure 1, lorsque Llama-3.1-8B-Instruct est utilisé comme modèle cible, l'augmentation moyenne sur 14 benchmarks est de 6,8 points. De plus, des améliorations surprenantes de 37,1 points sur le benchmark AlpacaEval-2 et de 30,1 points sur Arena-Hard, qui évaluent le comportement selon les instructions, sont observées. Les codes, modèles et ensembles de données sont disponibles sur https://github.com/SLIT-AI/FuseChat-3.0.",
      "upvotes": 5,
      "discussionId": "67ca64ced153739fa9b9dc1b",
      "projectPage": "https://slit-ai.github.io/FuseChat-3.0/",
      "githubRepo": "https://github.com/SLIT-AI/FuseChat-3.0"
    },
    "publishedAt": "2025-03-06T22:20:34.462Z",
    "title": "FuseChat-3.0: Preference Optimization Meets Heterogeneous Model Fusion",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62ecbffd99112e99c5f7fded/nmr7w6NOioBYwMmNfezcf.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04222.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62ecbffd99112e99c5f7fded",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62ecbffd99112e99c5f7fded/U6iXAJbpm2vaC5qksEPiH.png",
      "fullname": "Fanqi Wan",
      "name": "Wanfq",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 29
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.04130",
      "authors": [
        {
          "_id": "67ca7baf6d5c2eafede56d35",
          "user": {
            "_id": "6449e5a3df4e6cb7eaefd2b8",
            "avatarUrl": "/avatars/a671cb507d5e02b238d8cd631e71649d.svg",
            "isPro": false,
            "fullname": "Jindong Jiang",
            "user": "jdps",
            "type": "user"
          },
          "name": "Jindong Jiang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:34:59.750Z",
          "hidden": false
        },
        {
          "_id": "67ca7baf6d5c2eafede56d36",
          "user": {
            "_id": "644570ba2d91b15b4c7f6311",
            "avatarUrl": "/avatars/d5e66012066d0c330b8f23718b1499d8.svg",
            "isPro": false,
            "fullname": "Xiuyu Li",
            "user": "xiuyul",
            "type": "user"
          },
          "name": "Xiuyu Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:35:06.739Z",
          "hidden": false
        },
        {
          "_id": "67ca7baf6d5c2eafede56d37",
          "user": {
            "_id": "650dac79b959b0e1d41d7378",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/650dac79b959b0e1d41d7378/mzbN0MFk3k8b94FQ40I7L.jpeg",
            "isPro": false,
            "fullname": "Zhijian Liu",
            "user": "zhijianliu",
            "type": "user"
          },
          "name": "Zhijian Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:35:13.834Z",
          "hidden": false
        },
        {
          "_id": "67ca7baf6d5c2eafede56d38",
          "user": {
            "_id": "66d8b322cf789857d384e5c4",
            "avatarUrl": "/avatars/1276726b27d312f48e69f5ae982daa24.svg",
            "isPro": false,
            "fullname": "Muyang Li",
            "user": "MuyangLI",
            "type": "user"
          },
          "name": "Muyang Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:37:06.962Z",
          "hidden": false
        },
        {
          "_id": "67ca7baf6d5c2eafede56d39",
          "name": "Guo Chen",
          "hidden": false
        },
        {
          "_id": "67ca7baf6d5c2eafede56d3a",
          "user": {
            "_id": "672aae13b2f2dc21e18570e0",
            "avatarUrl": "/avatars/0253107a2116d197dc0fe18660c2af90.svg",
            "isPro": false,
            "fullname": "Zhiqi Li",
            "user": "zhiqilinv",
            "type": "user"
          },
          "name": "Zhiqi Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:36:57.283Z",
          "hidden": false
        },
        {
          "_id": "67ca7baf6d5c2eafede56d3b",
          "user": {
            "_id": "641d1c5ec3983aa94915c162",
            "avatarUrl": "/avatars/127985b837ecf61e43c835deee578b5e.svg",
            "isPro": false,
            "fullname": "De-An Huang",
            "user": "deahuang",
            "type": "user"
          },
          "name": "De-An Huang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:36:27.913Z",
          "hidden": false
        },
        {
          "_id": "67ca7baf6d5c2eafede56d3c",
          "user": {
            "_id": "6656eb16e50d7c40881a14f0",
            "avatarUrl": "/avatars/c6822a51c8d5918debf6ee1d25fe1825.svg",
            "isPro": false,
            "fullname": "GuilinLiu",
            "user": "GuilinLiu",
            "type": "user"
          },
          "name": "Guilin Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:36:17.904Z",
          "hidden": false
        },
        {
          "_id": "67ca7baf6d5c2eafede56d3d",
          "user": {
            "_id": "66c8037c737ba92ae3fe0322",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c8037c737ba92ae3fe0322/WR_Yh5DWOVVh7IFlF24NM.jpeg",
            "isPro": false,
            "fullname": "Zhiding Yu",
            "user": "Zhiding",
            "type": "user"
          },
          "name": "Zhiding Yu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:36:09.646Z",
          "hidden": false
        },
        {
          "_id": "67ca7baf6d5c2eafede56d3e",
          "user": {
            "_id": "6251bf4b183aa4266924ad91",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678041834400-6251bf4b183aa4266924ad91.jpeg",
            "isPro": true,
            "fullname": "Kurt Keutzer",
            "user": "kurtkeutzer",
            "type": "user"
          },
          "name": "Kurt Keutzer",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:36:02.351Z",
          "hidden": false
        },
        {
          "_id": "67ca7baf6d5c2eafede56d3f",
          "user": {
            "_id": "60e1d6d3de3cd4c1bfb0c208",
            "avatarUrl": "/avatars/0bc59ede9074557f15447d2457aaf07b.svg",
            "isPro": false,
            "fullname": "Sungjin Ahn",
            "user": "sdstony",
            "type": "user"
          },
          "name": "Sungjin Ahn",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:35:54.160Z",
          "hidden": false
        },
        {
          "_id": "67ca7baf6d5c2eafede56d40",
          "name": "Jan Kautz",
          "hidden": false
        },
        {
          "_id": "67ca7baf6d5c2eafede56d41",
          "user": {
            "_id": "65a8b7f69aec1645994e7a15",
            "avatarUrl": "/avatars/debc086f3fea029db22847bde80799a0.svg",
            "isPro": false,
            "fullname": "Hongxu Yin",
            "user": "yinhongxu",
            "type": "user"
          },
          "name": "Hongxu Yin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:35:40.242Z",
          "hidden": false
        },
        {
          "_id": "67ca7baf6d5c2eafede56d42",
          "name": "Yao Lu",
          "hidden": false
        },
        {
          "_id": "67ca7baf6d5c2eafede56d43",
          "name": "Song Han",
          "hidden": false
        },
        {
          "_id": "67ca7baf6d5c2eafede56d44",
          "user": {
            "_id": "66bf958296583c59b049085b",
            "avatarUrl": "/avatars/04df8dd45835b7ea0991e242784e7810.svg",
            "isPro": false,
            "fullname": "Wonmin Byeon",
            "user": "wbyeon",
            "type": "user"
          },
          "name": "Wonmin Byeon",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:35:32.527Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-06T06:17:38.000Z",
      "title": "Modèles efficaces de tokenisation pour la compréhension des vidéos longues et divers modèles de LLM",
      "summary": "Récemment, le développement de modèles basés sur des images pour des modèles de langage (Video-LLMs) a considérablement amélioré la compréhension des images en les traitant comme colonnes de frames d'image. Cependant, les méthodes actuelles traitent les frames de manière indépendante comme des candidats visuels et ont une faible modélisation temporelle explicite, ce qui limite leur capacité à détecter des motifs dynamiques et à traiter de manière efficace de longues séquences de vidéos. Pour résoudre ces limitations, nous présentons STORM (Réduction de Tokens Espacetemporels pour Multimodal LLMs). Ceci est une nouvelle architecture qui introduit un encodeur temporel spécialisé entre l'encodeur d'images et le LLM. Notre encodeur temporel utilise le Modèle d'État de l'Espace de Mamba pour intégrer l'information temporelle dans les tokens d'images et génère une représentation riche qui stocke des motifs dynamiques entre les images tout au long de la séquence vidéo. Cette riche codification améliore la capacité d'interprétation logique des vidéos, permet efficacement des stratégies de réduction de tokens comme le sampling temporel en temps de test et d'entraînement, et réduit considérablement la dynamique de calcul du LLM. Avec l'intégration de cette technologie, notre approche réduit à la fois le temps d'entraînement et d'inférence, améliore le rendement et permet une compréhension efficace et puissante des vidéos dans des contextes temporels étendus. Selon des évaluations étendues, STORM atteint les meilleurs résultats dans presque tous les cadres de référence de compréhension de vidéos longues comme MLVU et LongVideoBench, atteignant une réduction du coût de calcul de jusqu'à 8 fois et une réduction du délai de décodage de 2,4 à 2,9 fois, sous un nombre fixe de frames d'entrée. La page du projet peut être accédée à https://research.nvidia.com/labs/lpr/storm.",
      "upvotes": 3,
      "discussionId": "67ca7bb16d5c2eafede56df1"
    },
    "publishedAt": "2025-03-06T23:53:09.588Z",
    "title": "Token-Efficient Long Video Understanding for Multimodal LLMs",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04130.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6299
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.04725",
      "authors": [
        {
          "_id": "67ca9092ba1ee2b914e3fa4a",
          "user": {
            "_id": "65e0027c960938e63e4a0157",
            "avatarUrl": "/avatars/c8ca0b082ee8e8004f47a23d9393df67.svg",
            "isPro": false,
            "fullname": "Zhuo Chen",
            "user": "zhuoc3",
            "type": "user"
          },
          "name": "Zhuo Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-07T09:09:31.232Z",
          "hidden": false
        },
        {
          "_id": "67ca9092ba1ee2b914e3fa4b",
          "user": {
            "_id": "66e0619714d7a7711c6fc139",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66e0619714d7a7711c6fc139/IZxEQ6Iv62DDHpCxFM-QG.jpeg",
            "isPro": false,
            "fullname": "Oriol Mayné i Comas",
            "user": "oriolmayne",
            "type": "user"
          },
          "name": "Oriol Mayné i Comas",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:37:41.309Z",
          "hidden": false
        },
        {
          "_id": "67ca9092ba1ee2b914e3fa4c",
          "name": "Zhuotao Jin",
          "hidden": false
        },
        {
          "_id": "67ca9092ba1ee2b914e3fa4d",
          "name": "Di Luo",
          "hidden": false
        },
        {
          "_id": "67ca9092ba1ee2b914e3fa4e",
          "name": "Marin Soljačić",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-06T18:59:48.000Z",
      "title": "L^2M : Règle d'échellabilité de l'information mutuelle dans les modèles de langage avec contexte étendu",
      "summary": "Dans le traitement du langage naturel, une méthodologie de mise à l'échelle de multiples Fibonacci binaires est définie avec précision pour dominer la dépendance à longue distance. Cette méthodologie montre une mise à l'échelle indépendante, ce qui est essentiel pour le modélisation de langages de longs textes. En utilisant cette méthode d'échelle, on définit la condition de modélisation de langages de longs textes (L^2M), qui relie le taille de la puissance d'état qui stocke l'information passée à la capacité de modéliser des textes longs de manière valide. Nos résultats ont été vérifiés expérimentalement dans des modèles Transformer et des modèles d'espace d'état. Cette recherche fournit une base théorique pour le développement de longueur de texte dans des modèles de langage à grande échelle.",
      "upvotes": 2,
      "discussionId": "67ca90c1ba1ee2b914e405b9",
      "projectPage": "https://github.com/LSquaredM/mutual_info_scaling_law",
      "githubRepo": "https://github.com/LSquaredM/mutual_info_scaling_law"
    },
    "publishedAt": "2025-03-07T01:42:13.847Z",
    "title": "L$^2$M: Mutual Information Scaling Law for Long-Context Language Modeling",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/65e0027c960938e63e4a0157/lMxohK6cMFgsw39hn0jga.jpeg",
      "https://cdn-uploads.huggingface.co/production/uploads/65e0027c960938e63e4a0157/EqSl1OwTeggMI59pVQzeR.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04725.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65e0027c960938e63e4a0157",
      "avatarUrl": "/avatars/c8ca0b082ee8e8004f47a23d9393df67.svg",
      "fullname": "Zhuo Chen",
      "name": "zhuoc3",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.03983",
      "authors": [
        {
          "_id": "67ca66c1cb7e422997cbd148",
          "user": {
            "_id": "627a354cc488a8ce15a2dec5",
            "avatarUrl": "/avatars/0d99a2fea8b193993fe5b9b7e5b74f40.svg",
            "isPro": true,
            "fullname": "Sreyan Ghosh",
            "user": "SreyanG-NVIDIA",
            "type": "user"
          },
          "name": "Sreyan Ghosh",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:40:39.324Z",
          "hidden": false
        },
        {
          "_id": "67ca66c1cb7e422997cbd149",
          "user": {
            "_id": "652a4dfc36f031c5e6f8b8a6",
            "avatarUrl": "/avatars/9fb56b025dc25f91ca6c31136eaf74b2.svg",
            "isPro": false,
            "fullname": "Zhifeng Kong",
            "user": "ZhifengKong",
            "type": "user"
          },
          "name": "Zhifeng Kong",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-07T03:23:47.395Z",
          "hidden": false
        },
        {
          "_id": "67ca66c1cb7e422997cbd14a",
          "name": "Sonal Kumar",
          "hidden": false
        },
        {
          "_id": "67ca66c1cb7e422997cbd14b",
          "name": "S Sakshi",
          "hidden": false
        },
        {
          "_id": "67ca66c1cb7e422997cbd14c",
          "user": {
            "_id": "63fc1124a3c067e62897a73f",
            "avatarUrl": "/avatars/aa63337a7cd73181b7c1e92decf635f4.svg",
            "isPro": false,
            "fullname": "Jaehyeon Kim",
            "user": "firecomputer",
            "type": "user"
          },
          "name": "Jaehyeon Kim",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:41:17.053Z",
          "hidden": false
        },
        {
          "_id": "67ca66c1cb7e422997cbd14d",
          "name": "Wei Ping",
          "hidden": false
        },
        {
          "_id": "67ca66c1cb7e422997cbd14e",
          "user": {
            "_id": "6440ddd65d600fb09518daa8",
            "avatarUrl": "/avatars/ac5898afd2082d230e2ebf6fb867ad4f.svg",
            "isPro": false,
            "fullname": "Rafael Valle",
            "user": "rafaelvalle",
            "type": "user"
          },
          "name": "Rafael Valle",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:41:08.488Z",
          "hidden": false
        },
        {
          "_id": "67ca66c1cb7e422997cbd14f",
          "user": {
            "_id": "6537a569568d8be8fa096b8c",
            "avatarUrl": "/avatars/bfda5cb252d8b5bc3ad737d99c0d7f49.svg",
            "isPro": false,
            "fullname": "Dinesh Manocha",
            "user": "manocha",
            "type": "user"
          },
          "name": "Dinesh Manocha",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:41:01.710Z",
          "hidden": false
        },
        {
          "_id": "67ca66c1cb7e422997cbd150",
          "user": {
            "_id": "6311021788942700629e6247",
            "avatarUrl": "/avatars/e7adc1632b76e80e7e4a590033d1c20a.svg",
            "isPro": false,
            "fullname": "Bryan Catanzaro",
            "user": "ctnzr",
            "type": "user"
          },
          "name": "Bryan Catanzaro",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:40:55.626Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-06T00:10:26.000Z",
      "title": "Audio Flamingo 2 : Compréhension de la voix longue et compétences d'expert en langage vocal d'un modèle de langage vocal",
      "summary": "Comprendre le langage non verbal et la musique, et chercher la raison pour laquelle cela se produit, est un élément crucial pour que les êtres humains et les IAs agissent efficacement dans leur environnement. Dans cet article, nous présentons Audio Flamingo 2 (AF2), un modèle de langage audio récent qui intègre la compréhension de la voix et la capacité à comprendre les raisons. AF2 utilise (i) des modèles CLAP personnalisés, (ii) des données de questions et réponses de son synthétique conçues pour des fins de raisonnement par voix, et (iii) une stratégie d'apprentissage multiniveau. AF2 a atteint un niveau de performance avancé, même avec un petit modèle de langage de 3B paramètres, dépassant les modèles open et propriétaires dans plus de 20 référentiels. Ensuite, nous étendons la compréhension de la voix aux sections de son long (de 30 secondes à 5 minutes), et proposons LongAudio, un nouveau ensemble de données grand pour la capture de son long et l'apprentissage de tâches de questions et réponses dans AF2. L'ajustement de AF2 sur LongAudio a démontré des résultats exceptionnels dans un référentiel expert annoté pour évaluer la compréhension de son long, LongAudioBench. De plus, une étude de dispersion a été réalisée et l'efficacité de l'approche a été confirmée. Site web du projet : https://research.nvidia.com/labs/adlr/AF2/.",
      "upvotes": 2,
      "discussionId": "67ca66c3cb7e422997cbd178"
    },
    "publishedAt": "2025-03-07T00:12:47.515Z",
    "title": "Audio Flamingo 2: An Audio-Language Model with Long-Audio Understanding and Expert Reasoning Abilities",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.03983.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62c9664eb34e600d7eaa4beb",
      "avatarUrl": "/avatars/ca23ecdec2d31c99ecce97d9b180ae0c.svg",
      "fullname": "Ghosh",
      "name": "Sreyan88",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.04606",
      "authors": [
        {
          "_id": "67ca7b8a2a2c299d98944909",
          "name": "Aoxiong Yin",
          "hidden": false
        },
        {
          "_id": "67ca7b8a2a2c299d9894490a",
          "name": "Kai Shen",
          "hidden": false
        },
        {
          "_id": "67ca7b8a2a2c299d9894490b",
          "user": {
            "_id": "64a0347b528a9bbe59d6e08c",
            "avatarUrl": "/avatars/6dd0bad84d711d1048a0a4169e621773.svg",
            "isPro": false,
            "fullname": "Yichong Leng",
            "user": "ustcscallion",
            "type": "user"
          },
          "name": "Yichong Leng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:41:57.733Z",
          "hidden": false
        },
        {
          "_id": "67ca7b8a2a2c299d9894490c",
          "name": "Xu Tan",
          "hidden": false
        },
        {
          "_id": "67ca7b8a2a2c299d9894490d",
          "name": "Xinyu Zhou",
          "hidden": false
        },
        {
          "_id": "67ca7b8a2a2c299d9894490e",
          "user": {
            "_id": "67bc247b593452cc18965cb1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/EA3kTYaaff0Hr7-dGiOOj.png",
            "isPro": false,
            "fullname": "JUNCHENG LI",
            "user": "JunchengLi",
            "type": "user"
          },
          "name": "Juncheng Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:42:06.818Z",
          "hidden": false
        },
        {
          "_id": "67ca7b8a2a2c299d9894490f",
          "name": "Siliang Tang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-06T16:53:14.000Z",
      "title": "Les deux meilleurs : l'intégration de modèles de langue et de modèles de diffusion dans la génération de vidéos",
      "summary": "Le développement récent de la génération de vidéo (T2V) est motivé par deux paradigmes compétitifs : les modèles de récupération automatique du langage et les modèles de branchement. Cependant, chaque paradigme a des limites intrinsèques : les modèles de langage font face à des difficultés de qualité visuelle et à l'accumulation d'erreurs, tandis que les modèles de branchement sont déficients en compréhension du sens et en modélisation causale. Dans cet article, nous proposons LanDiff, un cadre hybride, pour fusionner les forces de chacun des paradigmes à travers une génération de grand à petit. La structure introduit trois innovations : (1) un tokenisateur de sens qui compresse efficacement à travers une représentation discrète 1D à partir de caractéristiques visuelles 3D, atteignant une compression de 14 000 fois ; (2) un modèle de langage qui génère des tokens de sens à des niveaux de relation élevés ; (3) un modèle de branchement qui raffine des sens grossiers dans des vidéos de haute qualité. Les expérimentations avec le modèle LanDiff 5B atteignent un score de 85,43 sur le benchmark VBench T2V, dépassant à la fois le modèle leader du code ouvert Fanyuan Video (13B) et des modèles commerciaux comme Shiro, Karin et Highway. En outre, ils atteignent les meilleurs résultats en génération de vidéos longues, dépassant les autres modèles du code ouvert dans ce domaine. Vous pouvez voir le démo sur https://landiff.github.io/.",
      "upvotes": 2,
      "discussionId": "67ca7b8d2a2c299d989449a8"
    },
    "publishedAt": "2025-03-06T23:52:33.338Z",
    "title": "The Best of Both Worlds: Integrating Language Models and Diffusion Models for Video Generation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04606.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6299
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.02972",
      "authors": [
        {
          "_id": "67c96a61df4d64bfebd396d4",
          "name": "Jude Khouja",
          "hidden": false
        },
        {
          "_id": "67c96a61df4d64bfebd396d5",
          "name": "Karolina Korgul",
          "hidden": false
        },
        {
          "_id": "67c96a61df4d64bfebd396d6",
          "name": "Simi Hellsten",
          "hidden": false
        },
        {
          "_id": "67c96a61df4d64bfebd396d7",
          "name": "Lingyi Yang",
          "hidden": false
        },
        {
          "_id": "67c96a61df4d64bfebd396d8",
          "name": "Vlad Neacs",
          "hidden": false
        },
        {
          "_id": "67c96a61df4d64bfebd396d9",
          "name": "Harry Mayne",
          "hidden": false
        },
        {
          "_id": "67c96a61df4d64bfebd396da",
          "name": "Ryan Kearns",
          "hidden": false
        },
        {
          "_id": "67c96a61df4d64bfebd396db",
          "name": "Andrew Bean",
          "hidden": false
        },
        {
          "_id": "67c96a61df4d64bfebd396dc",
          "name": "Adam Mahdi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-04T19:57:47.000Z",
      "title": "LINGOLY-TOO : Répertorisation du langage et tri du texte pour séparer la mémoire et la logique",
      "summary": "La évaluation de la capacité d'inférence des modèles de langage grands (LLMs) est en danger de être exagérée en raison de l'exposition des données des cadres d'évaluation. Nous avons développé un cadre d'évaluation de logique linguistique difficile appelé 'LINGOLY-TOO', en utilisant un cadre de création de problèmes de logique linguistique, pour réduire l'impact de la mémoire sur l'évaluation du rendement du modèle. Nous avons développé un templé pour masquer dynamiquement les notations de l'écriture réelle de la langue et générer des variations de problèmes. Ces variations réduisent la probabilité qu'un exemple spécifique de problème soit inclus dans le jeu de données d'entraînement du modèle, tout en maintenant les étapes d'inférence nécessaires pour résoudre chaque problème. Les expériences ont affecté des modèles avancés comme OpenAI ou 1-preview et DeepSeem R1, poussant les modèles à ne pas s'adapter à l'inférence avancée. De plus, la variation des paramètres du même problème a clairement montré des différences en termes de précision, tandis que les problèmes intrinsèques au modèle montrent un rendement moyen plus bon. Ces résultats révèlent l'incertitude dans la génération de réponses dans les LLMs et démontrent l'impact de l'exposition aux données antérieures sur l'évaluation exagérée de la capacité d'inférence des modèles avancés.",
      "upvotes": 1,
      "discussionId": "67c96a62df4d64bfebd3976e"
    },
    "publishedAt": "2025-03-07T04:50:00.681Z",
    "title": "LINGOLY-TOO: Disentangling Memorisation from Reasoning with Linguistic Templatisation and Orthographic Obfuscation",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/671d7763572a9cfd9a6ea053/apKiu-1ILDtcrTYqiP53g.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.02972.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "671d7763572a9cfd9a6ea053",
      "avatarUrl": "/avatars/0a0225b50d949bb7ab0971bec531fc92.svg",
      "fullname": "Jude Khouja",
      "name": "jkhouja",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.01901",
      "authors": [
        {
          "_id": "67c90398ae4b9276f2d03643",
          "user": {
            "_id": "67bf67ade43da88cdfc1348e",
            "avatarUrl": "/avatars/7bd900ade802d99db7c562ad6c2f6661.svg",
            "isPro": false,
            "fullname": "Yuezhou Hu",
            "user": "yuezhouhu",
            "type": "user"
          },
          "name": "Yuezhou Hu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:45:37.578Z",
          "hidden": false
        },
        {
          "_id": "67c90398ae4b9276f2d03644",
          "name": "Weiyu Huang",
          "hidden": false
        },
        {
          "_id": "67c90398ae4b9276f2d03645",
          "user": {
            "_id": "67286718746a95c09d04cb1d",
            "avatarUrl": "/avatars/317efa8459cca08c2ff56c3ab116e15c.svg",
            "isPro": false,
            "fullname": "Zichen Liang",
            "user": "zcliang22",
            "type": "user"
          },
          "name": "Zichen Liang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:45:50.605Z",
          "hidden": false
        },
        {
          "_id": "67c90398ae4b9276f2d03646",
          "name": "Chang Chen",
          "hidden": false
        },
        {
          "_id": "67c90398ae4b9276f2d03647",
          "user": {
            "_id": "66c0a08bac74db25de8427ec",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c0a08bac74db25de8427ec/9D-piDBZqSt6KNkHImmkv.jpeg",
            "isPro": false,
            "fullname": "Jintao Zhang",
            "user": "jt-zhang",
            "type": "user"
          },
          "name": "Jintao Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-07T09:26:51.606Z",
          "hidden": false
        },
        {
          "_id": "67c90398ae4b9276f2d03648",
          "name": "Jun Zhu",
          "hidden": false
        },
        {
          "_id": "67c90398ae4b9276f2d03649",
          "user": {
            "_id": "65fcad0ba0d7adc40b54fac2",
            "avatarUrl": "/avatars/7564b5642378fddb46ec3b5ae57c0402.svg",
            "isPro": false,
            "fullname": "Jianfei Chen",
            "user": "surfingtomchen",
            "type": "user"
          },
          "name": "Jianfei Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:46:07.369Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-28T07:04:19.000Z",
      "title": "Méthode pour catégoriser la masse mesurée par la technique d'intégration quantitative postérieure",
      "summary": "La quantification des poids après l'entraînement d'un modèle de grande taille réduit le volume mémoire et résout les problèmes de coût en économie de bits pour accélérer. Non tous les dimensions des poids ont la même importance. Par conséquent, un métrique de sensibilité mesure l'influence de chaque élément de la fonction de perte pour traiter les poids originaux de manière plus appropriée pour la quantification. Dans cette étude, une recherche expérimentale a été menée sur la précision de ce métrique de sensibilité et il a été découvert que les métriques basées sur les gradients et les matrices de Hessian sont très peu précises : cela est dû aux petits domaines de convergence de l'approximation secondaire, ce qui diminue l'influence des mesures de quantification sur la fonction de perte. Pour résoudre ce problème, on propose le Post-quantization Integral (PQI), un métrique précise pour évaluer la sensibilité après la quantification. Pour utiliser ce métrique précise, on propose ReQuant, un cadre simple et puissant. ReQuant comprend deux composantes denses et sparses, principalement constituées de la sélection de décalages auto-ajustés et de la séparation des poids importants en étapes. Enfin, ReQuant améliore le rendement de la quantification postérieure, et avec l'utilisation du QTIP, on a réussi à améliorer significativement la perplexité dans le modèle Llama 3.2 1B, en l'augmentant à 2.66.",
      "upvotes": 1,
      "discussionId": "67c90399ae4b9276f2d03671"
    },
    "publishedAt": "2025-03-07T04:23:41.486Z",
    "title": "Identifying Sensitive Weights via Post-quantization Integral",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01901.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66c0a08bac74db25de8427ec",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c0a08bac74db25de8427ec/9D-piDBZqSt6KNkHImmkv.jpeg",
      "fullname": "Jintao Zhang",
      "name": "jt-zhang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.01375",
      "authors": [
        {
          "_id": "67c6bdf644c2f41804ca95c6",
          "user": {
            "_id": "64e57772b15cf1b5d017b8ee",
            "avatarUrl": "/avatars/24653ae6259a706d9d4ed63692eac5b7.svg",
            "isPro": false,
            "fullname": "Daniil Sherki",
            "user": "dsherki",
            "type": "user"
          },
          "name": "Daniil Sherki",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-04T09:44:52.446Z",
          "hidden": false
        },
        {
          "_id": "67c6bdf644c2f41804ca95c7",
          "user": {
            "_id": "6169a581d05945bfd8718dfa",
            "avatarUrl": "/avatars/1892ab06a7ddb557232777de3cbec470.svg",
            "isPro": false,
            "fullname": "Ivan Oseledets",
            "user": "oseledets",
            "type": "user"
          },
          "name": "Ivan Oseledets",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:46:17.295Z",
          "hidden": false
        },
        {
          "_id": "67c6bdf644c2f41804ca95c8",
          "name": "Ekaterina Muravleva",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-03T10:17:56.000Z",
      "title": "Combinant Flow Matching et Transformers pour une solution efficace du problème inverse de Bayes",
      "summary": "Résoudre efficacement des problèmes bayésiens est un défi significatif en raison de la complexité de la distribution postérieure et du coût élevé de calcul des méthodes de répartition traditionnelles. Étant donné une séquence de données observées et un modèle de processus, l'objectif est de reconstruire la distribution des paramètres en fonction des données expérimentales observées. En combinant le Méthode de Flux de Conditionnement (CFM) et une architecture basée sur les transformeurs, nous avons démontré que c'est possible de réaliser une échantillonnage efficace de ces distributions selon la quantité de variables des données observées.",
      "upvotes": 1,
      "discussionId": "67c6bdf744c2f41804ca960a"
    },
    "publishedAt": "2025-03-07T02:51:01.486Z",
    "title": "Combining Flow Matching and Transformers for Efficient Solution of Bayesian Inverse Problems",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01375.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64e57772b15cf1b5d017b8ee",
      "avatarUrl": "/avatars/24653ae6259a706d9d4ed63692eac5b7.svg",
      "fullname": "Daniil Sherki",
      "name": "dsherki",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.02191",
      "authors": [
        {
          "_id": "67ca7fe72a83a60adcb6611a",
          "user": {
            "_id": "6331c3f618711776b468e9ec",
            "avatarUrl": "/avatars/af2c4bba031e474bf4fd2ea19e415aaf.svg",
            "isPro": false,
            "fullname": "Mia Mohammad Imran",
            "user": "imranraad",
            "type": "user"
          },
          "name": "Mia Mohammad Imran",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-07T09:09:36.214Z",
          "hidden": false
        },
        {
          "_id": "67ca7fe72a83a60adcb6611b",
          "name": "Robert Zita",
          "hidden": false
        },
        {
          "_id": "67ca7fe72a83a60adcb6611c",
          "name": "Rebekah Copeland",
          "hidden": false
        },
        {
          "_id": "67ca7fe72a83a60adcb6611d",
          "name": "Preetha Chatterjee",
          "hidden": false
        },
        {
          "_id": "67ca7fe72a83a60adcb6611e",
          "user": {
            "_id": "64085e1992033c150739aa74",
            "avatarUrl": "/avatars/621a5ef8aaf27d9c322c4a22c7bbcf5b.svg",
            "isPro": false,
            "fullname": "Rahat Rizvi Rahman",
            "user": "rahat-rizvi",
            "type": "user"
          },
          "name": "Rahat Rizvi Rahman",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:48:32.885Z",
          "hidden": false
        },
        {
          "_id": "67ca7fe72a83a60adcb6611f",
          "user": {
            "_id": "64ca97e1d469fc2cf822d9f6",
            "avatarUrl": "/avatars/efec75e454ada7026e8497137de5bceb.svg",
            "isPro": false,
            "fullname": "Kostadin Damevski",
            "user": "kdamevski",
            "type": "user"
          },
          "name": "Kostadin Damevski",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:48:25.771Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-04T02:01:37.000Z",
      "title": "L'entendement et la prédiction de la perte d'information dans la convolution des codes de taxi",
      "summary": "Les projets de logiciel prospèrent grâce à la participation et à la contribution de personnes de différentes origines. Cependant, des expressions uniques et des interactions négatives peuvent entraver la participation et le maintien des contributeurs, ainsi que leur exclusion de nouveaux participants. Une stratégie de modération active vise à prévenir les expressions uniques qui s'éloignent du but du dialogue. L'objectif de cette étude est d'entendre et de prédire comment les expressions uniques peuvent s'éloigner du but du dialogue sur GitHub.\n\nPour promouvoir cette étude, nous avons collecté 202 dialogues uniques sur GitHub et créé un ensemble de données marquées avec des points d'éloignement. Cet ensemble de données comprend 696 dialogues non uniques. En fonction de cet ensemble de données, nous avons identifié des caractéristiques des dialogues uniques et des points d'éloignement, y compris des marqueurs linguistiques, des pronoms du second personne, des mots négatifs, l'environnement de frustration et d'impatience, et des motifs dynamiques de dialogue entre contributeurs et participants extérieurs.\n\nEn fonction de ces observations expérimentales, nous proposons une approche active pour détecter et répondre aux dialogues potentiellement nuisibles. Nous avons développé une technologie de suivi de dialogues pour identifier l'évolution du sujet et les premiers signes d'éloignement dans les dialogues de GitHub, en utilisant des techniques de résumé de dialogues. Nos expériences ont montré que le prompt pour fournir des résumés de dialogues de GitHub à travers un modèle de langage de machine (LLM) atteint un F1-score de 69% pour prédire l'éloignement, montrant un améliorament significatif par rapport à une approche standard.",
      "upvotes": 1,
      "discussionId": "67ca7fe82a83a60adcb6615b",
      "githubRepo": "https://github.com/imranraad07/derailment-oss-replication"
    },
    "publishedAt": "2025-03-07T00:11:25.116Z",
    "title": "Understanding and Predicting Derailment in Toxic Conversations on GitHub",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.02191.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6331c3f618711776b468e9ec",
      "avatarUrl": "/avatars/af2c4bba031e474bf4fd2ea19e415aaf.svg",
      "fullname": "Mia Mohammad Imran",
      "name": "imranraad",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.04378",
      "authors": [
        {
          "_id": "67ca637e4cb4283da8ae2979",
          "name": "Zhilin Wang",
          "hidden": false
        },
        {
          "_id": "67ca637e4cb4283da8ae297a",
          "name": "Jiaqi Zeng",
          "hidden": false
        },
        {
          "_id": "67ca637e4cb4283da8ae297b",
          "user": {
            "_id": "6556379e10428134ff235afd",
            "avatarUrl": "/avatars/ec569729870d7392e806e59a02f37d0c.svg",
            "isPro": false,
            "fullname": "Olivier Delalleau",
            "user": "odelalleau",
            "type": "user"
          },
          "name": "Olivier Delalleau",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:51:28.127Z",
          "hidden": false
        },
        {
          "_id": "67ca637e4cb4283da8ae297c",
          "name": "Daniel Egert",
          "hidden": false
        },
        {
          "_id": "67ca637e4cb4283da8ae297d",
          "name": "Ellie Evans",
          "hidden": false
        },
        {
          "_id": "67ca637e4cb4283da8ae297e",
          "name": "Hoo-Chang Shin",
          "hidden": false
        },
        {
          "_id": "67ca637e4cb4283da8ae297f",
          "name": "Felipe Soares",
          "hidden": false
        },
        {
          "_id": "67ca637e4cb4283da8ae2980",
          "name": "Yi Dong",
          "hidden": false
        },
        {
          "_id": "67ca637e4cb4283da8ae2981",
          "name": "Oleksii Kuchaiev",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-06T12:30:24.000Z",
      "title": "Feedback professionnels et modèles d'édition permettent d'escaler l'inférence et s'adaptent aux tâches générales dans le domaine ouvert.",
      "summary": "La progression en inférence a joué un rôle crucial dans le succès récent des modèles tels que OpenAI ou 1 et DeepSeek R1. Cependant, les technologies utilisées pour la progression en inférence sont limitées aux tâches où la réponse peut être confirmée, et se concentrent sur des domaines comme la mathématique, le code et la logistique. Les humains apprennent à demander une rétroaction détaillée grâce aux premiers essais et à améliorer en se basant sur ces rétroactions, et ils s'efforcent de garantir de recevoir le même type de rétroaction dans des expériences ouvertes et larges. Par conséquent, nous collectons et entraînons des données pour des modèles de rétroaction et d'édition spécifiques pour la progression en inférence. Notre système est conçu de manière que un modèle initial génère une réponse, suivi d'un second modèle qui fournit une rétroaction, et d'un troisième modèle qui édite la réponse en se basant sur ces rétroactions. Nous montrons que, grâce à la progression, nous pouvons améliorer le rendement sur le benchmark Arena Hard, montrant une amélioration du nombre de réponses initiales, de la quantité de rétroaction valide et de la quantité de réponses éditées. Avec la progression optimale, notre système basé sur le modèle Llama 3 famille de 70B pourrait atteindre le meilleur rendement sur Arena Hard le 5 mars 2025, dépassant OpenAI ou1-preview-2024-09-12 (90,4) et DeepSeek R1 (92,3).",
      "upvotes": 1,
      "discussionId": "67ca63804cb4283da8ae29da"
    },
    "publishedAt": "2025-03-06T22:10:18.014Z",
    "title": "Dedicated Feedback and Edit Models Empower Inference-Time Scaling for Open-Ended General-Domain Tasks",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04378.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6299
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.04644",
      "authors": [
        {
          "_id": "67ca5d2783ac16a063a56241",
          "user": {
            "_id": "64dc29d9b5d625e0e9a6ecb9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/QxGBsnk1cNsBEPqSx4ae-.jpeg",
            "isPro": false,
            "fullname": "Tingyu Song",
            "user": "songtingyu",
            "type": "user"
          },
          "name": "Tingyu Song",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:53:24.813Z",
          "hidden": false
        },
        {
          "_id": "67ca5d2783ac16a063a56242",
          "user": {
            "_id": "65dfeee3d16fb170031df293",
            "avatarUrl": "/avatars/05e6fe0e61d4bb87536554c782385dac.svg",
            "isPro": false,
            "fullname": "gan",
            "user": "guo9",
            "type": "user"
          },
          "name": "Guo Gan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-07T09:09:52.646Z",
          "hidden": false
        },
        {
          "_id": "67ca5d2783ac16a063a56243",
          "name": "Mingsheng Shang",
          "hidden": false
        },
        {
          "_id": "67ca5d2783ac16a063a56244",
          "user": {
            "_id": "62f662bcc58915315c4eccea",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
            "isPro": true,
            "fullname": "Yilun",
            "user": "yilunzhao",
            "type": "user"
          },
          "name": "Yilun Zhao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:52:59.499Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-06T17:32:22.000Z",
      "title": "IFIR : Référentiel constant pour l'évaluation du respect des directives dans la recherche d'information sur les domaines professionnels",
      "summary": "IFIR est le premier benchmark spécifique pour évaluer la recherche d'information (IR) qui suit des instructions dans des domaines spécialisés. IFIR comprend 2 426 exemples de grande qualité et couvre 8 ensembles de 4 domaines : financier, juridique, médical et scientifique. Chaque ensemble crée des scénarios réalistes où les instructions personnalisées sont cruciales et abordent des tâches de recherche propres à chaque domaine. IFIR peut analyser en détail la capacité à traiter des instructions à différents niveaux de complexité structurelle. De plus, il propose un nouveau méthode d'évaluation basée sur des modèles de langage de machine (LLM) pour offrir une évaluation plus précise et fiable du rendement des modèles. Des expériences larges ont été menées avec 15 modèles de recherche de pointe (y compris des modèles basés sur des LLM) et ont révélé les graves problèmes auxquels les modèles actuels sont confrontés lors de la prise en charge d'instructions complexes et propres aux domaines spécialisés. De plus, des analyses détaillées ont été réalisées pour clarifier ces limitations et fournissent des conseils précieux pour les avancées futures.",
      "upvotes": 0,
      "discussionId": "67ca5d2983ac16a063a562a1"
    },
    "publishedAt": "2025-03-07T04:37:52.576Z",
    "title": "IFIR: A Comprehensive Benchmark for Evaluating Instruction-Following in Expert-Domain Information Retrieval",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04644.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65dfeee3d16fb170031df293",
      "avatarUrl": "/avatars/05e6fe0e61d4bb87536554c782385dac.svg",
      "fullname": "gan",
      "name": "guo9",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]