[
  {
    "paper": {
      "id": "2505.02707",
      "authors": [
        {
          "_id": "6819982f17007d963b9d4166",
          "name": "Yemin Shi",
          "hidden": false
        },
        {
          "_id": "6819982f17007d963b9d4167",
          "name": "Yu Shu",
          "hidden": false
        },
        {
          "_id": "6819982f17007d963b9d4168",
          "name": "Siwei Dong",
          "hidden": false
        },
        {
          "_id": "6819982f17007d963b9d4169",
          "user": {
            "_id": "6108ae87823007eaf0c7bd1e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6108ae87823007eaf0c7bd1e/dKjdx9I5waJs6oUQ0_mmT.png",
            "isPro": false,
            "fullname": "Guangyi Liu",
            "user": "guangyil",
            "type": "user"
          },
          "name": "Guangyi Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:46:52.667Z",
          "hidden": false
        },
        {
          "_id": "6819982f17007d963b9d416a",
          "user": {
            "_id": "6438a9027de34e8ea7e4b257",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6438a9027de34e8ea7e4b257/vib8QSd1AWMr_bR9ig_xJ.jpeg",
            "isPro": false,
            "fullname": "Jaward Sesay",
            "user": "Jaward",
            "type": "user"
          },
          "name": "Jaward Sesay",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-06T08:32:48.746Z",
          "hidden": false
        },
        {
          "_id": "6819982f17007d963b9d416b",
          "name": "Jingwen Li",
          "hidden": false
        },
        {
          "_id": "6819982f17007d963b9d416c",
          "user": {
            "_id": "665bfa1b0d71762b8613282d",
            "avatarUrl": "/avatars/edbde7b1b47032339a1ecc59f8ea8f1a.svg",
            "isPro": false,
            "fullname": "Zhiting Hu",
            "user": "zhitinghu",
            "type": "user"
          },
          "name": "Zhiting Hu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:46:15.191Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/665bfa1b0d71762b8613282d/zbWarqt8nFt0AwhF0gElE.mp4"
      ],
      "publishedAt": "2025-05-05T15:05:01.000Z",
      "submittedOnDailyAt": "2025-05-06T03:36:16.945Z",
      "title": "Voilà : Interaction de conduite autonome dans des unités de temps à travers des modèles de langage vocal et du jeu de rôle professionnel de langage vocal",
      "submittedOnDailyBy": {
        "_id": "665bfa1b0d71762b8613282d",
        "avatarUrl": "/avatars/edbde7b1b47032339a1ecc59f8ea8f1a.svg",
        "isPro": false,
        "fullname": "Zhiting Hu",
        "user": "zhitinghu",
        "type": "user"
      },
      "summary": "Voilà un assistant d'intelligence artificielle vocale qui intègre et transforme la vie quotidienne et celle des séminaires, interagissant en temps réel avec une expression émotionnelle et humaine. Il dépasse les réponses simples aux commandes, continue d'écouter, d'analyser des raisons et de réagir de manière proactive, promouvant des conversations naturelles et dynamiques qui résonnent émotionnellement. Voilà a avancé vers cette vision. Voilà présente un modèle de langage vocal à grande échelle pour les familles. Il a introduit une architecture qui dépasse les systèmes de pipeline existants, permettant des dialogues riches en ton, rythme et émotion depuis le point d'entrée jusqu'au point de sortie, maintenant l'harmonie du son. Il atteint une réponse de 195 millisecondes, répondant plus rapidement que la moyenne de réponse humaine. Il utilise un Transformer multi-échelle pour intégrer la capacité de raisonnement des modèles de langage grands (LLMs) et un modélisation de son pour faciliter la génération de sons naturels et professionnels. Les utilisateurs peuvent définir le reconnaissance du son, le ton et d'autres caractéristiques avec des instructions simples de texte. De plus, Voilà recommande d'adapter de nouveaux sons appropriément à partir de échantillons de sons d'environ 10 secondes, et supporte un dictionnaire pré-construit de plus d'un million de sons. En plus de la conversation, il gère le reconnaissance automatique du son (ASR), la conversion du texte en voix (TTS), et adapte minimalement pour les traductions grammaticales en plusieurs langues. Voilà est complètement open-source et soutient la recherche publique, avec l'objectif de favoriser le développement de la conversation humain-machine des prochaines générations.",
      "upvotes": 49,
      "discussionId": "6819983117007d963b9d4247",
      "projectPage": "https://voila.maitrix.org",
      "githubRepo": "https://github.com/maitrix-org/Voila",
      "ai_keywords": [
        "full-duplex",
        "low-latency conversations",
        "hierarchical multi-scale Transformer",
        "reasoning capabilities",
        "large language models (LLMs)",
        "acoustic modeling",
        "persona-aware voice generation",
        "automatic speech recognition (ASR)",
        "Text-to-Speech (TTS)",
        "multilingual speech translation",
        "pre-built voices",
        "efficient customization"
      ]
    },
    "publishedAt": "2025-05-05T11:05:01.000Z",
    "title": "Voila: Voice-Language Foundation Models for Real-Time Autonomous\n  Interaction and Voice Role-Play",
    "summary": "A voice AI agent that blends seamlessly into daily life would interact with\nhumans in an autonomous, real-time, and emotionally expressive manner. Rather\nthan merely reacting to commands, it would continuously listen, reason, and\nrespond proactively, fostering fluid, dynamic, and emotionally resonant\ninteractions. We introduce Voila, a family of large voice-language foundation\nmodels that make a step towards this vision. Voila moves beyond traditional\npipeline systems by adopting a new end-to-end architecture that enables\nfull-duplex, low-latency conversations while preserving rich vocal nuances such\nas tone, rhythm, and emotion. It achieves a response latency of just 195\nmilliseconds, surpassing the average human response time. Its hierarchical\nmulti-scale Transformer integrates the reasoning capabilities of large language\nmodels (LLMs) with powerful acoustic modeling, enabling natural, persona-aware\nvoice generation -- where users can simply write text instructions to define\nthe speaker's identity, tone, and other characteristics. Moreover, Voila\nsupports over one million pre-built voices and efficient customization of new\nones from brief audio samples as short as 10 seconds. Beyond spoken dialogue,\nVoila is designed as a unified model for a wide range of voice-based\napplications, including automatic speech recognition (ASR), Text-to-Speech\n(TTS), and, with minimal adaptation, multilingual speech translation. Voila is\nfully open-sourced to support open research and accelerate progress toward\nnext-generation human-machine interactions.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/665bfa1b0d71762b8613282d/zbWarqt8nFt0AwhF0gElE.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02707.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "665bfa1b0d71762b8613282d",
      "avatarUrl": "/avatars/edbde7b1b47032339a1ecc59f8ea8f1a.svg",
      "fullname": "Zhiting Hu",
      "name": "zhitinghu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.02387",
      "authors": [
        {
          "_id": "681988d6d6a5fee26b52ac28",
          "user": {
            "_id": "6270ff726417aed8a7340c8b",
            "avatarUrl": "/avatars/3f14913c55cc4fc78678ac43fb603e80.svg",
            "isPro": false,
            "fullname": "Xiusi Chen",
            "user": "XtremSup",
            "type": "user"
          },
          "name": "Xiusi Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:47:11.654Z",
          "hidden": false
        },
        {
          "_id": "681988d6d6a5fee26b52ac29",
          "user": {
            "_id": "654d784d71a30c4bca09a319",
            "avatarUrl": "/avatars/ab9f93122903ccd662267232bab30ad8.svg",
            "isPro": false,
            "fullname": "Gaotang Li",
            "user": "gaotang",
            "type": "user"
          },
          "name": "Gaotang Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-06T08:33:13.258Z",
          "hidden": false
        },
        {
          "_id": "681988d6d6a5fee26b52ac2a",
          "name": "Ziqi Wang",
          "hidden": false
        },
        {
          "_id": "681988d6d6a5fee26b52ac2b",
          "name": "Bowen Jin",
          "hidden": false
        },
        {
          "_id": "681988d6d6a5fee26b52ac2c",
          "name": "Cheng Qian",
          "hidden": false
        },
        {
          "_id": "681988d6d6a5fee26b52ac2d",
          "name": "Yu Wang",
          "hidden": false
        },
        {
          "_id": "681988d6d6a5fee26b52ac2e",
          "user": {
            "_id": "65f906e5c3dbdcae83ff7aac",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65f906e5c3dbdcae83ff7aac/mdjiVkLDJgJcGLwv0rMe4.jpeg",
            "isPro": false,
            "fullname": "Hongru Wang",
            "user": "Merlin-Hongru",
            "type": "user"
          },
          "name": "Hongru Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-06T08:33:11.136Z",
          "hidden": false
        },
        {
          "_id": "681988d6d6a5fee26b52ac2f",
          "name": "Yu Zhang",
          "hidden": false
        },
        {
          "_id": "681988d6d6a5fee26b52ac30",
          "user": {
            "_id": "66285acb73af5913c6bbf1ec",
            "avatarUrl": "/avatars/8969e3a6ae2dcc0b1c49768fd044b9e0.svg",
            "isPro": false,
            "fullname": "Denghui Zhang",
            "user": "zhangdenghui123",
            "type": "user"
          },
          "name": "Denghui Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:48:00.793Z",
          "hidden": false
        },
        {
          "_id": "681988d6d6a5fee26b52ac31",
          "name": "Tong Zhang",
          "hidden": false
        },
        {
          "_id": "681988d6d6a5fee26b52ac32",
          "name": "Hanghang Tong",
          "hidden": false
        },
        {
          "_id": "681988d6d6a5fee26b52ac33",
          "name": "Heng Ji",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-05T06:11:12.000Z",
      "submittedOnDailyAt": "2025-05-06T02:32:05.558Z",
      "title": "RM-R1 : Modéliser la compensation à partir de la logique de la théorie",
      "submittedOnDailyBy": {
        "_id": "654d784d71a30c4bca09a319",
        "avatarUrl": "/avatars/ab9f93122903ccd662267232bab30ad8.svg",
        "isPro": false,
        "fullname": "Gaotang Li",
        "user": "gaotang",
        "type": "user"
      },
      "summary": "Le modélisation des récompenses est essentielle pour aligner les grands modèles de langage (LLMs) avec les préférences humaines, surtout grâce au apprentissage par récompense avec des objectifs humains (RLHF). Pour fournir une signale de récompense précise, le modèle de récompense (RM) doit déclencher une profonde réflexion et présenter des raisons interprétables. Cependant, les RM actuels génèrent des échelles obscures ou prédisent des réponses positives directement, ce qui rend l'intégration des évaluations en nature de langage difficile et réduit leur interprétabilité.\n\nAvec l'avancée récente de l'inférence continue longue (CoT), nous avons assumé et démontré que nous pouvons améliorer significativement l'interprétabilité et le rendement du RM en intégrant des habiletés de raisonnement. Dans cet article, nous présentons une nouvelle classe de modèles de récompense, les modèles de récompense rationnelles (ReasRMs), et proposons une stratégie pour configurer le modélisation de récompense comme une tâche de raisonnement. Nous proposons un processus d'entraînement pour les raisons et entraînons un RM-R1, un type de ReasRM. L'entraînement est divisé en deux étapes principales : (1) la refinement continu de raisons de haute qualité, et (2) l'apprentissage par récompense avec des récompenses vérifiables. Le RM-R1 génère automatiquement des traces de raisons ou des guides de révision uniques de dialogues, compare des réponses candidates et améliore le rendement de la réseau de la LLM. Expérimentalement, notre modèle atteint les meilleurs ou les meilleurs voisins sur plusieurs benchmarks détaillés de modèles de récompense génératives, et dépasse les modèles grands ouverts de poids (comme Llama3.1-405B) ou les modèles propriétaires (comme GPT-4o) d'un marge d'au moins 13,8%. En plus de dépasser le rendement final, nous effectuons un analyse détaillée expérimentale pour comprendre les composants clés de l'entraînement d'un ReasRM. Pour des futures recherches, nous publions six modèles de ReasRM, du code et des données (https://github.com/RM-R1-UIUC/RM-R1).",
      "upvotes": 28,
      "discussionId": "681988d7d6a5fee26b52ac7e",
      "githubRepo": "https://github.com/RM-R1-UIUC/RM-R1",
      "ai_keywords": [
        "reward modeling",
        "reinforcement learning from human feedback (RLHF)",
        "reward model (RM)",
        "scalar scores",
        "preferred answer",
        "natural language critiques",
        "long chain-of-thought (CoT)",
        "reasoning capabilities",
        "Reasoning Reward Models (ReasRMs)",
        "reasoning-oriented training pipeline",
        "distillation",
        "high-quality reasoning chains",
        "reinforcement learning",
        "verifiable rewards",
        "LLM rollouts",
        "self-generating reasoning traces",
        "chat-specific rubrics",
        "candidate responses",
        "generative reward models",
        "state-of-the-art",
        "near state-of-the-art",
        "reward model benchmarks",
        "open-weight models",
        "proprietary models",
        "empirical analysis",
        "ReasRM models"
      ]
    },
    "publishedAt": "2025-05-05T02:11:12.000Z",
    "title": "RM-R1: Reward Modeling as Reasoning",
    "summary": "Reward modeling is essential for aligning large language models (LLMs) with\nhuman preferences, especially through reinforcement learning from human\nfeedback (RLHF). To provide accurate reward signals, a reward model (RM) should\nstimulate deep thinking and conduct interpretable reasoning before assigning a\nscore or a judgment. However, existing RMs either produce opaque scalar scores\nor directly generate the prediction of a preferred answer, making them struggle\nto integrate natural language critiques, thus lacking interpretability.\nInspired by recent advances of long chain-of-thought (CoT) on\nreasoning-intensive tasks, we hypothesize and validate that integrating\nreasoning capabilities into reward modeling significantly enhances RM's\ninterpretability and performance. In this work, we introduce a new class of\ngenerative reward models -- Reasoning Reward Models (ReasRMs) -- which\nformulate reward modeling as a reasoning task. We propose a reasoning-oriented\ntraining pipeline and train a family of ReasRMs, RM-R1. The training consists\nof two key stages: (1) distillation of high-quality reasoning chains and (2)\nreinforcement learning with verifiable rewards. RM-R1 improves LLM rollouts by\nself-generating reasoning traces or chat-specific rubrics and evaluating\ncandidate responses against them. Empirically, our models achieve\nstate-of-the-art or near state-of-the-art performance of generative RMs across\nmultiple comprehensive reward model benchmarks, outperforming much larger\nopen-weight models (e.g., Llama3.1-405B) and proprietary ones (e.g., GPT-4o) by\nup to 13.8%. Beyond final performance, we perform thorough empirical analysis\nto understand the key ingredients of successful ReasRM training. To facilitate\nfuture research, we release six ReasRM models along with code and data at\nhttps://github.com/RM-R1-UIUC/RM-R1.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02387.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "654d784d71a30c4bca09a319",
      "avatarUrl": "/avatars/ab9f93122903ccd662267232bab30ad8.svg",
      "fullname": "Gaotang Li",
      "name": "gaotang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.20752",
      "authors": [
        {
          "_id": "6818c145daa8955b2085667d",
          "name": "Roman Abramov",
          "hidden": false
        },
        {
          "_id": "6818c145daa8955b2085667e",
          "user": {
            "_id": "6679882913c63ebaa8ff62fe",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6679882913c63ebaa8ff62fe/zufYEHw7QNp50pfZx9SmF.jpeg",
            "isPro": false,
            "fullname": "Felix Steinbauer",
            "user": "fsteinbauer",
            "type": "user"
          },
          "name": "Felix Steinbauer",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-05T13:46:46.742Z",
          "hidden": false
        },
        {
          "_id": "6818c145daa8955b2085667f",
          "name": "Gjergji Kasneci",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-29T13:33:29.000Z",
      "submittedOnDailyAt": "2025-05-06T03:38:21.809Z",
      "title": "Le domaine de la joueterie dans le domaine des sauts : extension de données pour les Transformers qui exécutent une inférence multiniveau dans le monde réel.",
      "submittedOnDailyBy": {
        "_id": "6679882913c63ebaa8ff62fe",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6679882913c63ebaa8ff62fe/zufYEHw7QNp50pfZx9SmF.jpeg",
        "isPro": false,
        "fullname": "Felix Steinbauer",
        "user": "fsteinbauer",
        "type": "user"
      },
      "summary": "Les transformeurs ont réussi à de nombreuses tâches de traitement du langage naturel, mais présentent des limites claires à plusieurs étapes de l'inférence réelle. En particulier, ces limites sont évidentes lorsque la connaissance réelle est insuffisante. Récemment, les avancées dans le grokking ont montré que les réseaux neuronaux peuvent détecter des motifs logiques potentiels et les généraliser complètement de la mémoire. Cependant, ces recherches ont été principalement basées sur de petites tâches synthétiques. Dans cet article, nous étendons le grokking aux données de faits réels, et nous résolvons la rareté des données en ajoutant des données synthétiques soigneusement conçues à un graphe de connaissance existant, atteignant ainsi que le pourcentage de faits inférés, phi_r, dépasse un seuil critique par rapport aux faits réels. Surprenant, nous avons trouvé que les données synthétiques réellement incorrectes renforcent le cycle d'inférence et obligent à croire dans des structures relationnelles plutôt que dans des structures basées sur la mémoire. Dans des évaluations dans des cadres de référence de plusieurs étapes d'inférence, notre approche a atteint une précision de 95-100% dans 2WikiMultiHopQA, améliorant considérablement un fort baseline et dépassant les résultats actuels. De plus, nous analysons en détail la formation du cycle de généralisation interne des transformeurs en fonction de l'augmentation de phi_r. Nos résultats indiquent que l'augmentation de données basée sur le grokking libère des capacités d'inférence multi-étapes cachées et permet aux grands modèles de langage réaliser des inférences factuelles plus robustes et interprétables.",
      "upvotes": 19,
      "discussionId": "6818c146daa8955b208566f1",
      "ai_keywords": [
        "Transformers",
        "multi-step factual reasoning",
        "grokking",
        "neural networks",
        "perfect generalization",
        "logical patterns",
        "real-world factual data",
        "dataset sparsity",
        "knowledge graphs",
        "synthetic data",
        "inferred facts",
        "atomic facts",
        "factually incorrect synthetic data",
        "relational structure",
        "memorization",
        "multi-hop reasoning",
        "benchmarks",
        "2WikiMultiHopQA",
        "baselines",
        "state-of-the-art results",
        "generalizing circuits",
        "grokking-based data augmentation",
        "implicit multi-hop reasoning capabilities",
        "robust",
        "interpretable factual reasoning"
      ]
    },
    "publishedAt": "2025-04-29T09:33:29.000Z",
    "title": "Grokking in the Wild: Data Augmentation for Real-World Multi-Hop\n  Reasoning with Transformers",
    "summary": "Transformers have achieved great success in numerous NLP tasks but continue\nto exhibit notable gaps in multi-step factual reasoning, especially when\nreal-world knowledge is sparse. Recent advances in grokking have demonstrated\nthat neural networks can transition from memorizing to perfectly generalizing\nonce they detect underlying logical patterns - yet these studies have primarily\nused small, synthetic tasks. In this paper, for the first time, we extend\ngrokking to real-world factual data and address the challenge of dataset\nsparsity by augmenting existing knowledge graphs with carefully designed\nsynthetic data to raise the ratio phi_r of inferred facts to atomic facts\nabove the threshold required for grokking. Surprisingly, we find that even\nfactually incorrect synthetic data can strengthen emergent reasoning circuits\nrather than degrade accuracy, as it forces the model to rely on relational\nstructure rather than memorization. When evaluated on multi-hop reasoning\nbenchmarks, our approach achieves up to 95-100% accuracy on 2WikiMultiHopQA -\nsubstantially improving over strong baselines and matching or exceeding current\nstate-of-the-art results. We further provide an in-depth analysis of how\nincreasing phi_r drives the formation of generalizing circuits inside\nTransformers. Our findings suggest that grokking-based data augmentation can\nunlock implicit multi-hop reasoning capabilities, opening the door to more\nrobust and interpretable factual reasoning in large-scale language models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.20752.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6679882913c63ebaa8ff62fe",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6679882913c63ebaa8ff62fe/zufYEHw7QNp50pfZx9SmF.jpeg",
      "fullname": "Felix Steinbauer",
      "name": "fsteinbauer",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.02819",
      "authors": [
        {
          "_id": "6819b5da3d9c61444380f4c5",
          "user": {
            "_id": "66465dfa508db0bde50d95f2",
            "avatarUrl": "/avatars/8b4a583dc0f3cab0f1cd9a1be3daa01b.svg",
            "isPro": false,
            "fullname": "Dmitry Shophoev",
            "user": "dimitriish",
            "type": "user"
          },
          "name": "Dmitriy Shopkhoev",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-06T07:10:19.519Z",
          "hidden": false
        },
        {
          "_id": "6819b5da3d9c61444380f4c6",
          "user": {
            "_id": "6166db59f78a267701a78c2a",
            "avatarUrl": "/avatars/8784efc36f67719e9455b1f081340ed9.svg",
            "isPro": false,
            "fullname": "Ammar Ali",
            "user": "ammarali32",
            "type": "user"
          },
          "name": "Ammar Ali",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-06T08:32:17.870Z",
          "hidden": false
        },
        {
          "_id": "6819b5da3d9c61444380f4c7",
          "name": "Magauiya Zhussip",
          "hidden": false
        },
        {
          "_id": "6819b5da3d9c61444380f4c8",
          "user": {
            "_id": "66b1ce4ca14db5aac3e5e755",
            "avatarUrl": "/avatars/ab55ef112fba091813e1cc1f43857cf9.svg",
            "isPro": false,
            "fullname": "Valentin Malykh",
            "user": "madrugado",
            "type": "user"
          },
          "name": "Valentin Malykh",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:04:42.358Z",
          "hidden": false
        },
        {
          "_id": "6819b5da3d9c61444380f4c9",
          "user": {
            "_id": "6683cc62b466c0d8e60e1bbc",
            "avatarUrl": "/avatars/d781cfb113263f88eaa3250bef521c53.svg",
            "isPro": false,
            "fullname": "Stamatis Lefkimmiatis",
            "user": "stamatisl",
            "type": "user"
          },
          "name": "Stamatios Lefkimmiatis",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-06T08:32:13.923Z",
          "hidden": false
        },
        {
          "_id": "6819b5da3d9c61444380f4ca",
          "name": "Nikos Komodakis",
          "hidden": false
        },
        {
          "_id": "6819b5da3d9c61444380f4cb",
          "user": {
            "_id": "667e7f968c6d7aede7ecb94b",
            "avatarUrl": "/avatars/d6dabd9b909b1f20f661dc4bc07af23f.svg",
            "isPro": false,
            "fullname": "Sergey Zagoruyko",
            "user": "szagoruyko121",
            "type": "user"
          },
          "name": "Sergey Zagoruyko",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:04:52.244Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-05T17:47:42.000Z",
      "submittedOnDailyAt": "2025-05-06T07:03:26.032Z",
      "title": "Réduction de la réseau par remplacement de couches et de transformations linéaires",
      "submittedOnDailyBy": {
        "_id": "610e8c12119bebecb4d807b6",
        "avatarUrl": "/avatars/7230b1584ec45585c12eb5703fd80ff3.svg",
        "isPro": false,
        "fullname": "Ivan Sedykh",
        "user": "idsedykh",
        "type": "user"
      },
      "summary": "ReplaceMe est un méthode généralisée de réduction de profondeur sans limites d'entraînement. Ce méthode permet de maintenir de hauts rendements à des ratios de compression faibles, en remplaçant efficacement les blocs transformer par des opérations linéaires. À différence de d'autres méthodes de réduction, elle ne nécessite pas d'entraînements supplémentaires ou d'ajustements. Notre méthode seulement nécessite un petit ensemble de données de correction pour estimer des transformations linéaires qui approchent les blocs réduits. Ces transformations linéaires peuvent être intégrées sans restrictions avec les autres blocs transformer, sans avoir à ajouter de paramètres supplémentaires. Selon les expériences, ReplaceMe montre un rendement comparable à d'autres méthodes sans limites d'entraînement et concurrence avec les meilleurs méthodes de réduction dans l'architecture plus complexe, y compris le retraite, l'ajustement final et les changements d'architecture. ReplaceMe peut maintenir approximativement 90% du rendement du modèle original sur les benchmarks ouverts, en réduisant le modèle d'un 25%, avec un minimum de surcharge computationnelle (référence à Fig.1). Nous proposons l'implémentation de ReplaceMe ainsi qu'une bibliothèque ouverte qui inclut certains des meilleurs méthodes de réduction de profondeur.",
      "upvotes": 16,
      "discussionId": "6819b5db3d9c61444380f518",
      "githubRepo": "https://github.com/mts-ai/ReplaceMe",
      "ai_keywords": [
        "training-free depth pruning",
        "transformer blocks",
        "linear operation",
        "calibration dataset",
        "linear transformation",
        "computational overhead",
        "large language models (LLMs)",
        "open benchmarks",
        "open-source library"
      ]
    },
    "publishedAt": "2025-05-05T13:47:42.000Z",
    "title": "ReplaceMe: Network Simplification via Layer Pruning and Linear\n  Transformations",
    "summary": "We introduce ReplaceMe, a generalized training-free depth pruning method that\neffectively replaces transformer blocks with a linear operation, while\nmaintaining high performance for low compression ratios. In contrast to\nconventional pruning approaches that require additional training or\nfine-tuning, our approach requires only a small calibration dataset that is\nused to estimate a linear transformation to approximate the pruned blocks. This\nestimated linear mapping can be seamlessly merged with the remaining\ntransformer blocks, eliminating the need for any additional network parameters.\nOur experiments show that ReplaceMe consistently outperforms other\ntraining-free approaches and remains highly competitive with state-of-the-art\npruning methods that involve extensive retraining/fine-tuning and architectural\nmodifications. Applied to several large language models (LLMs), ReplaceMe\nachieves up to 25% pruning while retaining approximately 90% of the original\nmodel's performance on open benchmarks - without any training or healing steps,\nresulting in minimal computational overhead (see Fig.1). We provide an\nopen-source library implementing ReplaceMe alongside several state-of-the-art\ndepth pruning techniques, available at this repository.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02819.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "610e8c12119bebecb4d807b6",
      "avatarUrl": "/avatars/7230b1584ec45585c12eb5703fd80ff3.svg",
      "fullname": "Ivan Sedykh",
      "name": "idsedykh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.02735",
      "authors": [
        {
          "_id": "6819742e0d1c56fe9124fe3a",
          "user": {
            "_id": "62a80fe3ac97233f1625235a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62a80fe3ac97233f1625235a/_rGtpqdY7OEBz3pyqb6fE.jpeg",
            "isPro": false,
            "fullname": "Zhouliang Yu",
            "user": "zhouliang",
            "type": "user"
          },
          "name": "Zhouliang Yu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-06T08:34:10.190Z",
          "hidden": false
        },
        {
          "_id": "6819742e0d1c56fe9124fe3b",
          "user": {
            "_id": "662f2c8435ab6df959b005de",
            "avatarUrl": "/avatars/3e30053ecbe9cc14b5e1eb2b014755de.svg",
            "isPro": false,
            "fullname": "ruotian peng",
            "user": "prt66",
            "type": "user"
          },
          "name": "Ruotian Peng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:48:20.491Z",
          "hidden": false
        },
        {
          "_id": "6819742e0d1c56fe9124fe3c",
          "name": "Keyi Ding",
          "hidden": false
        },
        {
          "_id": "6819742e0d1c56fe9124fe3d",
          "name": "Yizhe Li",
          "hidden": false
        },
        {
          "_id": "6819742e0d1c56fe9124fe3e",
          "name": "Zhongyuan Peng",
          "hidden": false
        },
        {
          "_id": "6819742e0d1c56fe9124fe3f",
          "user": {
            "_id": "6417d9ea8f689506e7148417",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6417d9ea8f689506e7148417/bAYcruWNw4WvmuQcGgcwC.jpeg",
            "isPro": false,
            "fullname": "minghao",
            "user": "Liam-Liu",
            "type": "user"
          },
          "name": "Minghao Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-06T08:33:31.975Z",
          "hidden": false
        },
        {
          "_id": "6819742e0d1c56fe9124fe40",
          "user": {
            "_id": "623d8ca4c29adf5ef6175615",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623d8ca4c29adf5ef6175615/q7lHao7UPwU1u7YLSP56m.jpeg",
            "isPro": false,
            "fullname": "Yi-Fan Zhang",
            "user": "yifanzhang114",
            "type": "user"
          },
          "name": "Yifan Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:49:14.785Z",
          "hidden": false
        },
        {
          "_id": "6819742e0d1c56fe9124fe41",
          "user": {
            "_id": "649da6b4599302cdb9bc232b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/DxQT6LCDTZvyGUUe2t19c.jpeg",
            "isPro": false,
            "fullname": "Zheng Yuan",
            "user": "ZhengYuan",
            "type": "user"
          },
          "name": "Zheng Yuan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:49:20.735Z",
          "hidden": false
        },
        {
          "_id": "6819742e0d1c56fe9124fe42",
          "user": {
            "_id": "6532a060a78e70d19c669103",
            "avatarUrl": "/avatars/3cc9309b0e31da0fb83f1c3ef87dbe9f.svg",
            "isPro": false,
            "fullname": "HuajianXin",
            "user": "HuajianXin",
            "type": "user"
          },
          "name": "Huajian Xin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:49:28.104Z",
          "hidden": false
        },
        {
          "_id": "6819742e0d1c56fe9124fe43",
          "user": {
            "_id": "641e5bf65f274a0a92c2f6a2",
            "avatarUrl": "/avatars/c15a54c51998c0e6367685e8e1737ec9.svg",
            "isPro": false,
            "fullname": "Wenhao Huang",
            "user": "EZ-hwh",
            "type": "user"
          },
          "name": "Wenhao Huang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:49:44.482Z",
          "hidden": false
        },
        {
          "_id": "6819742e0d1c56fe9124fe44",
          "user": {
            "_id": "643c21735fcffe09fb68a46f",
            "avatarUrl": "/avatars/76aabacd318aa954d4c53094ad456056.svg",
            "isPro": false,
            "fullname": "Yandong Wen",
            "user": "ydwen",
            "type": "user"
          },
          "name": "Yandong Wen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:49:51.642Z",
          "hidden": false
        },
        {
          "_id": "6819742e0d1c56fe9124fe45",
          "user": {
            "_id": "638efcf4c67af472d316d424",
            "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
            "isPro": false,
            "fullname": "Ge Zhang",
            "user": "zhangysk",
            "type": "user"
          },
          "name": "Ge Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:49:59.764Z",
          "hidden": false
        },
        {
          "_id": "6819742e0d1c56fe9124fe46",
          "user": {
            "_id": "648905d1a15c43c791d4381f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/648905d1a15c43c791d4381f/GpqGBzsLiMHX0gWZEz3qn.jpeg",
            "isPro": false,
            "fullname": "Weiyang Liu",
            "user": "wy1iu",
            "type": "user"
          },
          "name": "Weiyang Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:50:07.063Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-05T15:37:00.000Z",
      "submittedOnDailyAt": "2025-05-06T01:00:48.636Z",
      "title": "Test des critères mathématiques d'inférence formelle pour des modèles de langage à grande échelle",
      "submittedOnDailyBy": {
        "_id": "62a80fe3ac97233f1625235a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62a80fe3ac97233f1625235a/_rGtpqdY7OEBz3pyqb6fE.jpeg",
        "isPro": false,
        "fullname": "Zhouliang Yu",
        "user": "zhouliang",
        "type": "user"
      },
      "summary": "La formation des mathématiques est un sujet important dans l'intelligence artificielle, et actuellement est limité par le domaine et l'échelle des cadres de référence. En réponse à cela, on présente FormalMATH. FormalMATH est un cadre de référence à grande échelle dans Lean4 qui comprend 5 560 problèmes vérifiés formellement. Ces problèmes couvrent des défis d'olympiades secondaires jusqu'aux théorèmes de niveau universitaire, et s'étendent à diverses disciplines comme l'algèbre, les mathématiques appliquées, le calcul différentiel, la théorie des nombres, les mathématiques discrètes, etc. Pour réduire l'inadéquation de la validation manuelle, nous avons combiné trois éléments pour introduire une nouvelle ligne de profilage automatique de la logique : 1) un modèle de langage grand (LLMs) spécialisé dans l'automatisation de la logique, 2) la vérification linguistique des modèles des deux extrémités, et 3) une stratégie de filtrage de démonstrations basée sur le test de lignes de preuve de LLMs. Cette approche laisse seulement 72,09% des séquences logiques à la validation manuelle, assurant la fidélité aux problèmes naturels. Selon l'évaluation des meilleurs modèles basés sur les LLMs, des limitations claires ont été identifiées. Même les modèles les plus robustes sous le seuil pratique ont atteint un 16,46% de succès, observant également un biais par domaine (par exemple, une excellence en algèbre tandis qu'il échoue dans le calcul différentiel) et une dépendance excessive à l'automatisation simplifiée. Spécifiquement, une relation inverse entre le guichet de solutions en langage naturel et le succès de la démonstration a été découverte, montrant que les logiques non formelles écrites par des humains peuvent agir comme bruit dans la configuration de logiques formelles, ce qui n'est pas facile à comprendre. Nous pensons que FormalMATH peut être un cadre de référence fort pour l'évaluation des logiques mathématiques formelles.",
      "upvotes": 16,
      "discussionId": "6819742f0d1c56fe9124fe8a",
      "projectPage": "https://spherelab.ai/FormalMATH/",
      "githubRepo": "https://github.com/Sphere-AI-Lab/FormalMATH-Bench"
    },
    "publishedAt": "2025-05-05T11:37:00.000Z",
    "title": "FormalMATH: Benchmarking Formal Mathematical Reasoning of Large Language\n  Models",
    "summary": "Formal mathematical reasoning remains a critical challenge for artificial\nintelligence, hindered by limitations of existing benchmarks in scope and\nscale. To address this, we present FormalMATH, a large-scale Lean4 benchmark\ncomprising 5,560 formally verified problems spanning from high-school Olympiad\nchallenges to undergraduate-level theorems across diverse domains (e.g.,\nalgebra, applied mathematics, calculus, number theory, and discrete\nmathematics). To mitigate the inefficiency of manual formalization, we\nintroduce a novel human-in-the-loop autoformalization pipeline that integrates:\n(1) specialized large language models (LLMs) for statement autoformalization,\n(2) multi-LLM semantic verification, and (3) negation-based disproof filtering\nstrategies using off-the-shelf LLM-based provers. This approach reduces expert\nannotation costs by retaining 72.09% of statements before manual verification\nwhile ensuring fidelity to the original natural-language problems. Our\nevaluation of state-of-the-art LLM-based theorem provers reveals significant\nlimitations: even the strongest models achieve only 16.46% success rate under\npractical sampling budgets, exhibiting pronounced domain bias (e.g., excelling\nin algebra but failing in calculus) and over-reliance on simplified automation\ntactics. Notably, we identify a counterintuitive inverse relationship between\nnatural-language solution guidance and proof success in chain-of-thought\nreasoning scenarios, suggesting that human-written informal reasoning\nintroduces noise rather than clarity in the formal reasoning settings. We\nbelieve that FormalMATH provides a robust benchmark for benchmarking formal\nmathematical reasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02735.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62a80fe3ac97233f1625235a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62a80fe3ac97233f1625235a/_rGtpqdY7OEBz3pyqb6fE.jpeg",
      "fullname": "Zhouliang Yu",
      "name": "zhouliang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.02835",
      "authors": [
        {
          "_id": "6819762e64ae18f1b6fde347",
          "user": {
            "_id": "623d8ca4c29adf5ef6175615",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623d8ca4c29adf5ef6175615/q7lHao7UPwU1u7YLSP56m.jpeg",
            "isPro": false,
            "fullname": "Yi-Fan Zhang",
            "user": "yifanzhang114",
            "type": "user"
          },
          "name": "Yi-Fan Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:57:15.220Z",
          "hidden": false
        },
        {
          "_id": "6819762e64ae18f1b6fde348",
          "user": {
            "_id": "664ba004bfd9b93ba4bfb353",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/UHaEcXmMSKvvFDsY3hCnb.jpeg",
            "isPro": false,
            "fullname": "LuXingyu",
            "user": "XingyuLu",
            "type": "user"
          },
          "name": "Xingyu Lu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:57:24.963Z",
          "hidden": false
        },
        {
          "_id": "6819762e64ae18f1b6fde349",
          "name": "Xiao Hu",
          "hidden": false
        },
        {
          "_id": "6819762e64ae18f1b6fde34a",
          "name": "Chaoyou Fu",
          "hidden": false
        },
        {
          "_id": "6819762e64ae18f1b6fde34b",
          "name": "Bin Wen",
          "hidden": false
        },
        {
          "_id": "6819762e64ae18f1b6fde34c",
          "name": "Tianke Zhang",
          "hidden": false
        },
        {
          "_id": "6819762e64ae18f1b6fde34d",
          "user": {
            "_id": "673421bf18caf8e877861cc6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/a8UfIZTUTFaCnWmJ_Bztr.png",
            "isPro": false,
            "fullname": "Changyi Liu",
            "user": "bhsc24",
            "type": "user"
          },
          "name": "Changyi Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:58:28.151Z",
          "hidden": false
        },
        {
          "_id": "6819762e64ae18f1b6fde34e",
          "user": {
            "_id": "63774c47455f6ad89ac41be1",
            "avatarUrl": "/avatars/e7d6048155cdf4497d58aa18523e745e.svg",
            "isPro": false,
            "fullname": "Kaiyu Jiang",
            "user": "KaiyuValley",
            "type": "user"
          },
          "name": "Kaiyu Jiang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:58:21.743Z",
          "hidden": false
        },
        {
          "_id": "6819762e64ae18f1b6fde34f",
          "name": "Kaibing Chen",
          "hidden": false
        },
        {
          "_id": "6819762e64ae18f1b6fde350",
          "user": {
            "_id": "66c605e808fee728d0dd94f5",
            "avatarUrl": "/avatars/d2ff37fedc5ac1b5b817543b80bf5256.svg",
            "isPro": false,
            "fullname": "Kaiyu Tang",
            "user": "KevinTowne",
            "type": "user"
          },
          "name": "Kaiyu Tang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:58:06.587Z",
          "hidden": false
        },
        {
          "_id": "6819762e64ae18f1b6fde351",
          "user": {
            "_id": "6610f64ee94d9046b71e19c8",
            "avatarUrl": "/avatars/11cc11199669129a740956d12c7214e8.svg",
            "isPro": false,
            "fullname": "Haojie Ding",
            "user": "haojieding",
            "type": "user"
          },
          "name": "Haojie Ding",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:57:59.222Z",
          "hidden": false
        },
        {
          "_id": "6819762e64ae18f1b6fde352",
          "user": {
            "_id": "6433abff546e16f17a0f1cd8",
            "avatarUrl": "/avatars/7c9bbcba69b823834eb0232da12cc7a9.svg",
            "isPro": false,
            "fullname": "chen",
            "user": "jiankang",
            "type": "user"
          },
          "name": "Jiankang Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:57:51.408Z",
          "hidden": false
        },
        {
          "_id": "6819762e64ae18f1b6fde353",
          "name": "Fan Yang",
          "hidden": false
        },
        {
          "_id": "6819762e64ae18f1b6fde354",
          "name": "Zhang Zhang",
          "hidden": false
        },
        {
          "_id": "6819762e64ae18f1b6fde355",
          "user": {
            "_id": "656453832bdaccfcd5379431",
            "avatarUrl": "/avatars/a0d764ce6b3fd05532c7a9cb2f263e33.svg",
            "isPro": false,
            "fullname": "Gao Ting",
            "user": "TingTingGao",
            "type": "user"
          },
          "name": "Tingting Gao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:57:35.059Z",
          "hidden": false
        },
        {
          "_id": "6819762e64ae18f1b6fde356",
          "name": "Liang Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-05T17:59:50.000Z",
      "submittedOnDailyAt": "2025-05-06T01:09:45.446Z",
      "title": "R1-Récompense : Entraînement d'un modèle de récompense Damo par apprentissage par renforcement stable",
      "submittedOnDailyBy": {
        "_id": "623d8ca4c29adf5ef6175615",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623d8ca4c29adf5ef6175615/q7lHao7UPwU1u7YLSP56m.jpeg",
        "isPro": false,
        "fullname": "Yi-Fan Zhang",
        "user": "yifanzhang114",
        "type": "user"
      },
      "summary": "Les modèles de récompense du modèle Damo (MRMs) jouent un rôle crucial dans l'amélioration du rendement des modèles de Damo de langage (MLLMs). Le développement récent a principalement été axé sur l'amélioration de la structure du modèle et des données d'entraînement, mais la recherche sur l'efficacité à long terme et les méthodes pour activer ces capacités par le biais des MRMs a été limitée. Dans cet article, nous revisons l'utilisation de la modélisation de récompense avec apprentissage par renforcement (RL). En particulier, nous réécrivons le problème de modélisation de récompense comme une tâche de RL basée sur des règles. Cependant, l'application directe d'algorithmes actuels de RL (par exemple, Reinforce++) à ce problème peut entraîner des instabilités ou des pannes d'entraînement en raison de leurs limitations intrinsèques. Pour résoudre ces problèmes, nous proposons l'algorithme StableReinforce. Cet algorithme améliore la perte d'entraînement, la stratégie d'estimation de l'avantage et le design de récompenses dans les méthodes actuelles de RL, ce qui permet une amélioration de la stabilité de l'entraînement et un rendement élevé. Pour soutenir l'entraînement des MRMs, nous avons collecté 200K données de préférence dans différents ensembles de données. En utilisant cet ensemble de données avec l'algorithme StableReinforce, nous entraînons un modèle de récompense, R1-Reward, qui peut obtenir des améliorations significatives sur le benchmark des MRMs de Damo. Comparé aux modèles de l'état de l'art précédents, R1-Reward peut obtenir un augmentation de 8,4% sur le benchmark VL Reward-Bench et une augmentation de 14,3% sur le Multimodal Reward Bench. De plus, en établissant un plus grand nombre de calculs d'inférence, le rendement de R1-Reward peut améliorer et il est possible de découvrir la possibilité que l'algorithme de RL optimise les MRMs.",
      "upvotes": 15,
      "discussionId": "6819762f64ae18f1b6fde387",
      "projectPage": "https://github.com/yfzhang114/r1_reward",
      "githubRepo": "https://github.com/yfzhang114/r1_reward",
      "ai_keywords": [
        "Multimodal Reward Models (MRMs)",
        "Multimodal Large Language Models (MLLMs)",
        "Reinforcement Learning (RL)",
        "rule-based RL task",
        "Reinforce++",
        "StableReinforce",
        "training loss",
        "advantage estimation strategy",
        "reward design",
        "preference data",
        "VL Reward-Bench",
        "Multimodal Reward Bench"
      ]
    },
    "publishedAt": "2025-05-05T13:59:50.000Z",
    "title": "R1-Reward: Training Multimodal Reward Model Through Stable Reinforcement\n  Learning",
    "summary": "Multimodal Reward Models (MRMs) play a crucial role in enhancing the\nperformance of Multimodal Large Language Models (MLLMs). While recent\nadvancements have primarily focused on improving the model structure and\ntraining data of MRMs, there has been limited exploration into the\neffectiveness of long-term reasoning capabilities for reward modeling and how\nto activate these capabilities in MRMs. In this paper, we explore how\nReinforcement Learning (RL) can be used to improve reward modeling.\nSpecifically, we reformulate the reward modeling problem as a rule-based RL\ntask. However, we observe that directly applying existing RL algorithms, such\nas Reinforce++, to reward modeling often leads to training instability or even\ncollapse due to the inherent limitations of these algorithms. To address this\nissue, we propose the StableReinforce algorithm, which refines the training\nloss, advantage estimation strategy, and reward design of existing RL methods.\nThese refinements result in more stable training dynamics and superior\nperformance. To facilitate MRM training, we collect 200K preference data from\ndiverse datasets. Our reward model, R1-Reward, trained using the\nStableReinforce algorithm on this dataset, significantly improves performance\non multimodal reward modeling benchmarks. Compared to previous SOTA models,\nR1-Reward achieves a 8.4% improvement on the VL Reward-Bench and a 14.3%\nimprovement on the Multimodal Reward Bench. Moreover, with more inference\ncompute, R1-Reward's performance is further enhanced, highlighting the\npotential of RL algorithms in optimizing MRMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02835.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "623d8ca4c29adf5ef6175615",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623d8ca4c29adf5ef6175615/q7lHao7UPwU1u7YLSP56m.jpeg",
      "fullname": "Yi-Fan Zhang",
      "name": "yifanzhang114",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.02391",
      "authors": [
        {
          "_id": "6819a63c64ae18f1b60a5c43",
          "user": {
            "_id": "66f8689725464a7989b75845",
            "avatarUrl": "/avatars/43a61a528c5779103eaf5687ba44ee14.svg",
            "isPro": false,
            "fullname": "Jiarui Yao",
            "user": "FlippyDora",
            "type": "user"
          },
          "name": "Jiarui Yao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-06T08:32:24.344Z",
          "hidden": false
        },
        {
          "_id": "6819a63c64ae18f1b60a5c44",
          "name": "Yifan Hao",
          "hidden": false
        },
        {
          "_id": "6819a63c64ae18f1b60a5c45",
          "user": {
            "_id": "6470e0f1cfd57849519033a5",
            "avatarUrl": "/avatars/7ffefee3e36a4e37b9f4510bc6b689d1.svg",
            "isPro": false,
            "fullname": "Hanning Zhang",
            "user": "HanningZhang",
            "type": "user"
          },
          "name": "Hanning Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:59:20.459Z",
          "hidden": false
        },
        {
          "_id": "6819a63c64ae18f1b60a5c46",
          "user": {
            "_id": "63a3ff69f91ad3ea5703841d",
            "avatarUrl": "/avatars/69227c4bce01d33747c1377b6f9672db.svg",
            "isPro": false,
            "fullname": "Hanze Dong",
            "user": "hendrydong",
            "type": "user"
          },
          "name": "Hanze Dong",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:59:27.200Z",
          "hidden": false
        },
        {
          "_id": "6819a63c64ae18f1b60a5c47",
          "user": {
            "_id": "6319b29809baf858241f05de",
            "avatarUrl": "/avatars/29eef2c52814abea82e2aa9bf37a7f9c.svg",
            "isPro": false,
            "fullname": "Xiong",
            "user": "WeiXiong",
            "type": "user"
          },
          "name": "Wei Xiong",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:59:34.381Z",
          "hidden": false
        },
        {
          "_id": "6819a63c64ae18f1b60a5c48",
          "user": {
            "_id": "64b8922ca1827cc8d04ae919",
            "avatarUrl": "/avatars/0aaa83e3d09a82434e1d6af724aaa485.svg",
            "isPro": false,
            "fullname": "Nan Jiang",
            "user": "nanjiang",
            "type": "user"
          },
          "name": "Nan Jiang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:59:47.400Z",
          "hidden": false
        },
        {
          "_id": "6819a63c64ae18f1b60a5c49",
          "name": "Tong Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-05T06:26:00.000Z",
      "submittedOnDailyAt": "2025-05-06T04:34:14.120Z",
      "title": "Optimisation de la Stratégie d'Inférence de Chaîne de Descartes pour Minimiser la Variation du Gradient dans le Sampling par Rejetion et l'Apprentissage par Renforcement",
      "submittedOnDailyBy": {
        "_id": "64d45451c34a346181b130dd",
        "avatarUrl": "/avatars/9bb8205b889337df5d321539c9b5d69d.svg",
        "isPro": false,
        "fullname": "Rui Yang",
        "user": "Ray2333",
        "type": "user"
      },
      "summary": "La déduction de la chaîne de pensée (CoT) peut être formalisée comme un problème de variables latentes dans les modèles de langage grands (LLMs), et le modèle doit générer des étapes intermédiaires de raisonnement. En contraste avec les méthodes précédentes, qui dépendaient de réglages micro d'évaluation de récompenses itératives (RAFT) et d'autres similaires, cet article identifie une limitation principale de l'entraînement de CoT : l'efficacité de l'estimation des gradients aléatoires par des stratégies de sampling statique. Il est proposé le GVM-RAFT, qui concevoit une stratégie de sampling dynamique qui s'adapte aux Prompts et minimise la variance standard sous des contraintes de vecteurs de calcul. En mesurant la taux d'acceptation des Prompts et la norme des descentes aléatoires, les ressources de calcul sont efficacement réparties, réduisant ainsi la variance standard des descentes. Une analyse théorique montre que la stratégie de sampling dynamique fournit une garantie de convergence accélérée sous certaines conditions. Dans des expériences mathématiques, le GVM-RAFT a démontré une augmentation de vitesse de 2 à 4 fois et une amélioration en précision similaire à celle de la RAFT originale. La stratégie de sampling dynamique est générale et peut être intégrée dans d'autres algorithmes d'apprentissage par renforcement, comme GRPO, obtenant ainsi la même amélioration en convergence et précision de test. Le code est disponible sur https://github.com/RLHFlow/GVM.",
      "upvotes": 15,
      "discussionId": "6819a63d64ae18f1b60a5c75",
      "ai_keywords": [
        "Chain-of-thought (CoT)",
        "latent variable problem",
        "iterative reward-ranked fine-tuning (RAFT)",
        "inference budget",
        "static sampling strategies",
        "GVM-RAFT",
        "Dynamic Sample Allocation Strategy",
        "prompt-specific",
        "computational budget constraint",
        "prompt acceptance rates",
        "stochastic gradient norms",
        "stochastic gradient variance",
        "accelerated convergence guarantees",
        "GRPO",
        "convergence",
        "test accuracy"
      ]
    },
    "publishedAt": "2025-05-05T02:26:00.000Z",
    "title": "Optimizing Chain-of-Thought Reasoners via Gradient Variance Minimization\n  in Rejection Sampling and RL",
    "summary": "Chain-of-thought (CoT) reasoning in large language models (LLMs) can be\nformalized as a latent variable problem, where the model needs to generate\nintermediate reasoning steps. While prior approaches such as iterative\nreward-ranked fine-tuning (RAFT) have relied on such formulations, they\ntypically apply uniform inference budgets across prompts, which fails to\naccount for variability in difficulty and convergence behavior. This work\nidentifies the main bottleneck in CoT training as inefficient stochastic\ngradient estimation due to static sampling strategies. We propose GVM-RAFT, a\nprompt-specific Dynamic Sample Allocation Strategy designed to minimize\nstochastic gradient variance under a computational budget constraint. The\nmethod dynamically allocates computational resources by monitoring prompt\nacceptance rates and stochastic gradient norms, ensuring that the resulting\ngradient variance is minimized. Our theoretical analysis shows that the\nproposed dynamic sampling strategy leads to accelerated convergence guarantees\nunder suitable conditions. Experiments on mathematical reasoning show that\nGVM-RAFT achieves a 2-4x speedup and considerable accuracy improvements over\nvanilla RAFT. The proposed dynamic sampling strategy is general and can be\nincorporated into other reinforcement learning algorithms, such as GRPO,\nleading to similar improvements in convergence and test accuracy. Our code is\navailable at https://github.com/RLHFlow/GVM.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02391.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64d45451c34a346181b130dd",
      "avatarUrl": "/avatars/9bb8205b889337df5d321539c9b5d69d.svg",
      "fullname": "Rui Yang",
      "name": "Ray2333",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.02222",
      "authors": [
        {
          "_id": "6819780dc3d212ad5b48cc07",
          "name": "Essential AI",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc09",
          "user": {
            "_id": "65ef97d0e5fc4abe66c05ed0",
            "avatarUrl": "/avatars/1d601a22639b3136bfb3519826451ddb.svg",
            "isPro": false,
            "fullname": "Ishaan Shah",
            "user": "ishaan-essential",
            "type": "user"
          },
          "name": "Ishaan Shah",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-06T08:33:24.884Z",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc0a",
          "user": {
            "_id": "6675e3ed66c4fa6d0c10e229",
            "avatarUrl": "/avatars/f73c347d824a56079729c82d60d3edc3.svg",
            "isPro": false,
            "fullname": "Anthony Polloreno",
            "user": "ampolloreno",
            "type": "user"
          },
          "name": "Anthony M. Polloreno",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:01:47.445Z",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc0b",
          "user": {
            "_id": "64d9ac38badf1110f7fcf030",
            "avatarUrl": "/avatars/c55c61af8dd52e6b4856684638b850a6.svg",
            "isPro": false,
            "fullname": "Karl Stratos",
            "user": "karlstratos",
            "type": "user"
          },
          "name": "Karl Stratos",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:01:53.826Z",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc0c",
          "user": {
            "_id": "66622dacec18341b268f97a6",
            "avatarUrl": "/avatars/8bc7d6a7c28c83aacdbeeb770716b1c0.svg",
            "isPro": false,
            "fullname": "Philip Monk",
            "user": "monk-essential",
            "type": "user"
          },
          "name": "Philip Monk",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:01:59.992Z",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc0d",
          "user": {
            "_id": "67bfd6daca6e3c22b6de31ee",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/TTt6_o9tYEgeWK26DgI_7.png",
            "isPro": false,
            "fullname": "Adarsh Chaluvaraju",
            "user": "cadarsh-essential",
            "type": "user"
          },
          "name": "Adarsh Chaluvaraju",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:02:16.867Z",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc0e",
          "user": {
            "_id": "6408e4f93461c51cf7345060",
            "avatarUrl": "/avatars/328b508e2de9e50dca2412adeb3542f5.svg",
            "isPro": false,
            "fullname": "Andrew Hojel",
            "user": "andrewhojel",
            "type": "user"
          },
          "name": "Andrew Hojel",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:02:24.655Z",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc0f",
          "user": {
            "_id": "62e24efc3a616d16e2f426ea",
            "avatarUrl": "/avatars/a2433c971f80e6cf738c03e843666cff.svg",
            "isPro": false,
            "fullname": "Andrew Ma",
            "user": "AndrewMa",
            "type": "user"
          },
          "name": "Andrew Ma",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:02:30.884Z",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc10",
          "name": "Anil Thomas",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc11",
          "name": "Ashish Tanwer",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc12",
          "name": "Darsh J Shah",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc13",
          "user": {
            "_id": "67ed7aa9290a7f9d33113fb5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/jMMTObGZktuXWJ3wVVSxj.png",
            "isPro": false,
            "fullname": "Khoi Nguyen",
            "user": "KTLK",
            "type": "user"
          },
          "name": "Khoi Nguyen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-06T08:33:20.971Z",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc14",
          "name": "Kurt Smith",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc15",
          "name": "Michael Callahan",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc16",
          "user": {
            "_id": "66cd078ea796074d428fde0f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/qThw3H6ukqIAuRy7aJTN1.jpeg",
            "isPro": false,
            "fullname": "Michael Pust",
            "user": "essentialpust",
            "type": "user"
          },
          "name": "Michael Pust",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:03:14.427Z",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc17",
          "user": {
            "_id": "674c2737d369a6de1f8f58e1",
            "avatarUrl": "/avatars/40af3aa9b9d574cc63dc328c3a465fff.svg",
            "isPro": false,
            "fullname": "Parmar Mohit",
            "user": "mohitparmar",
            "type": "user"
          },
          "name": "Mohit Parmar",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:03:21.257Z",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc18",
          "name": "Peter Rushton",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc19",
          "user": {
            "_id": "67f4ced58c4cbc2f5d95cd17",
            "avatarUrl": "/avatars/3f440a59c38f5c0c7a77746ef54ed0a5.svg",
            "isPro": false,
            "fullname": "Platon Mazarakis",
            "user": "Platona",
            "type": "user"
          },
          "name": "Platon Mazarakis",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:03:32.983Z",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc1a",
          "user": {
            "_id": "654bdcf2e06d25def57cc54b",
            "avatarUrl": "/avatars/2d2612bd7072edd60876b504345fbf25.svg",
            "isPro": false,
            "fullname": "Ritvik Kapila",
            "user": "rkapila",
            "type": "user"
          },
          "name": "Ritvik Kapila",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:03:51.154Z",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc1b",
          "name": "Saurabh Srivastava",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc1c",
          "user": {
            "_id": "679bc0b23e12a166672e5275",
            "avatarUrl": "/avatars/fe9d0e79c21c9d3594420e69e3809f0f.svg",
            "isPro": false,
            "fullname": "Somanshu Singla",
            "user": "somanshu-essential",
            "type": "user"
          },
          "name": "Somanshu Singla",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-06T08:33:27.349Z",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc1d",
          "user": {
            "_id": "67101a7165442ddc48cb4b07",
            "avatarUrl": "/avatars/551777fcd1638998ad9fd16804b313ec.svg",
            "isPro": false,
            "fullname": "Tim Romanski",
            "user": "tim-essential",
            "type": "user"
          },
          "name": "Tim Romanski",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:04:09.387Z",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc1e",
          "user": {
            "_id": "66f5f3f99ef08fe3c1f4c35a",
            "avatarUrl": "/avatars/41b8b6f90eb87e685b74587317296a1b.svg",
            "isPro": false,
            "fullname": "Yash Vanjani",
            "user": "yash-essential",
            "type": "user"
          },
          "name": "Yash Vanjani",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:04:16.859Z",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc1f",
          "name": "Ashish Vaswani",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-04T19:14:43.000Z",
      "submittedOnDailyAt": "2025-05-06T07:30:25.714Z",
      "title": "L'efficacité pratique de l'entraînement préalable des mouvements",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "MOON (Muon) est le module final le plus simple du deuxième niveau et le meilleur optimiseur d'efficacité. Il clairement étend la charge de calcul dans la frontière de la roue de la rentabilité par rapport à AdamW, et est plus efficace et efficace, permettant un entraînement plus efficace avec de grands tailles de jeux de données, tout en maintenant l'efficacité des données. De cette manière, MOON permet des entraînements de ressources efficaces. De plus, une recherche a été menée sur la transfert de hyperparamètres efficace de la combinaison de MOON et de la mise à jour maximale des paramètres (muP), et un algorithme efficace a été proposé pour minimiser le surcharge manuelle des ressources en considérant tous les erreurs de muP. Ces résultats ont été vérifiés par des expériences avec des modèles de taille de paramètres supérieure à 40 milliards et des tests de distribution de données et d'architecture.",
      "upvotes": 15,
      "discussionId": "6819780fc3d212ad5b48cc89",
      "ai_keywords": [
        "second-order optimizer",
        "Pareto frontier",
        "AdamW",
        "data efficiency",
        "critical batch size",
        "computationally efficient",
        "maximal update parameterization",
        "telescoping algorithm",
        "hyperparameter transfer",
        "error sources",
        "model sizes",
        "data distribution",
        "architecture"
      ]
    },
    "publishedAt": "2025-05-04T15:14:43.000Z",
    "title": "Practical Efficiency of Muon for Pretraining",
    "summary": "We demonstrate that Muon, the simplest instantiation of a second-order\noptimizer, explicitly expands the Pareto frontier over AdamW on the\ncompute-time tradeoff. We find that Muon is more effective than AdamW in\nretaining data efficiency at large batch sizes, far beyond the so-called\ncritical batch size, while remaining computationally efficient, thus enabling\nmore economical training. We study the combination of Muon and the maximal\nupdate parameterization (muP) for efficient hyperparameter transfer and present\na simple telescoping algorithm that accounts for all sources of error in muP\nwhile introducing only a modest overhead in resources. We validate our findings\nthrough extensive experiments with model sizes up to four billion parameters\nand ablations on the data distribution and architecture.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02222.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6784
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.02094",
      "authors": [
        {
          "_id": "681992911e0fae3880173d43",
          "user": {
            "_id": "66d59dc9b005ad82ca6fc61d",
            "avatarUrl": "/avatars/0ba424690afd1144a89665c5bacdfde7.svg",
            "isPro": false,
            "fullname": "Runyi YU",
            "user": "IngridYU",
            "type": "user"
          },
          "name": "Runyi Yu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:00:09.333Z",
          "hidden": false
        },
        {
          "_id": "681992911e0fae3880173d44",
          "name": "Yinhuai Wang",
          "hidden": false
        },
        {
          "_id": "681992911e0fae3880173d45",
          "user": {
            "_id": "64341911546e16f17a129733",
            "avatarUrl": "/avatars/ae12aafc8932a7537838e6d3964858cb.svg",
            "isPro": false,
            "fullname": "QiHan Zhao",
            "user": "Crimnos",
            "type": "user"
          },
          "name": "Qihan Zhao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:00:32.182Z",
          "hidden": false
        },
        {
          "_id": "681992911e0fae3880173d46",
          "name": "Hok Wai Tsui",
          "hidden": false
        },
        {
          "_id": "681992911e0fae3880173d47",
          "name": "Jingbo Wang",
          "hidden": false
        },
        {
          "_id": "681992911e0fae3880173d48",
          "name": "Ping Tan",
          "hidden": false
        },
        {
          "_id": "681992911e0fae3880173d49",
          "user": {
            "_id": "6467b121e7a6a374fd19b44b",
            "avatarUrl": "/avatars/3f2874d58986d651aef55e3408b05700.svg",
            "isPro": false,
            "fullname": "Qifeng Chen",
            "user": "cqf",
            "type": "user"
          },
          "name": "Qifeng Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:01:21.751Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-04T13:00:29.000Z",
      "submittedOnDailyAt": "2025-05-06T03:11:28.738Z",
      "title": "スキルミモク-V2 : Apprendre des habiletés d'interaction robustes et généralisables à partir de démonstrations rares et avec beaucoup de bruit.",
      "submittedOnDailyBy": {
        "_id": "66d59dc9b005ad82ca6fc61d",
        "avatarUrl": "/avatars/0ba424690afd1144a89665c5bacdfde7.svg",
        "isPro": false,
        "fullname": "Runyi YU",
        "user": "IngridYU",
        "type": "user"
      },
      "summary": "Nous abordons les défis fondamentaux de l'apprentissage par renforcement (RLID) à travers des simulations d'interaction : le bruit de la simulation et les limites de couverture. Les méthodes existantes pour la collecte de données fournissent des simulations d'interaction utiles, mais généralement génèrent des trajectoires rares, déconnectées et avec beaucoup de bruit, ce qui ne capture pas complètement le spectre de changements technologiques et de transitions possibles. Notre conclusion centrale est que, malgré le bruit et la rarité des simulations, il existe une infinité de trajectoires physiquement possibles qui se connectent naturellement entre les techniques de simulation et forment une continuité spatiale de changements et de transitions technologiques possibles. Sur la base de cette conclusion, nous présentons deux technologies d'augmentation de données : le Graphe de Trajectoires Proches (STG) trouve les transitions potentielles entre techniques de simulation et le Camp de Transitions d'État (STF) établit des connexions uniques pour n'importe quel état dans l'environnement de la simulation. Pour faciliter un apprentissage par renforcement efficace, nous avons développé une stratégie d'échantillonnage de trajectoires adaptatif pour la génération de cours dynamiques et un mécanisme d'encodage historique pour l'apprentissage de techniques en mémoire. Notre approche permet l'obtention de technologies robustes et améliore significativement la généralisation au-delà des simulations. À travers une large gamme d'expériences sur des tâches d'interaction, nous avons démontré des améliorations significatives en stabilité de convergence, capacité de généralisation et robustesse de récupération, comparées aux méthodes les plus récentes.",
      "upvotes": 12,
      "discussionId": "681992931e0fae3880173dcf",
      "ai_keywords": [
        "Reinforcement Learning from Interaction Demonstration (RLID)",
        "demonstration noise",
        "coverage limitations",
        "interaction demonstrations",
        "sparse trajectories",
        "disconnected trajectories",
        "noise",
        "skill variations",
        "transitions",
        "physically feasible trajectories",
        "Stitched Trajectory Graph (STG)",
        "State Transition Field (STF)",
        "Adaptive Trajectory Sampling (ATS)",
        "dynamic curriculum generation",
        "historical encoding mechanism",
        "skill acquisition",
        "convergence stability",
        "generalization capability",
        "recovery robustness"
      ]
    },
    "publishedAt": "2025-05-04T09:00:29.000Z",
    "title": "SkillMimic-V2: Learning Robust and Generalizable Interaction Skills from\n  Sparse and Noisy Demonstrations",
    "summary": "We address a fundamental challenge in Reinforcement Learning from Interaction\nDemonstration (RLID): demonstration noise and coverage limitations. While\nexisting data collection approaches provide valuable interaction\ndemonstrations, they often yield sparse, disconnected, and noisy trajectories\nthat fail to capture the full spectrum of possible skill variations and\ntransitions. Our key insight is that despite noisy and sparse demonstrations,\nthere exist infinite physically feasible trajectories that naturally bridge\nbetween demonstrated skills or emerge from their neighboring states, forming a\ncontinuous space of possible skill variations and transitions. Building upon\nthis insight, we present two data augmentation techniques: a Stitched\nTrajectory Graph (STG) that discovers potential transitions between\ndemonstration skills, and a State Transition Field (STF) that establishes\nunique connections for arbitrary states within the demonstration neighborhood.\nTo enable effective RLID with augmented data, we develop an Adaptive Trajectory\nSampling (ATS) strategy for dynamic curriculum generation and a historical\nencoding mechanism for memory-dependent skill learning. Our approach enables\nrobust skill acquisition that significantly generalizes beyond the reference\ndemonstrations. Extensive experiments across diverse interaction tasks\ndemonstrate substantial improvements over state-of-the-art methods in terms of\nconvergence stability, generalization capability, and recovery robustness.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02094.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66d59dc9b005ad82ca6fc61d",
      "avatarUrl": "/avatars/0ba424690afd1144a89665c5bacdfde7.svg",
      "fullname": "Runyi YU",
      "name": "IngridYU",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.02156",
      "authors": [
        {
          "_id": "681975a9fdcf582e6d0effdb",
          "user": {
            "_id": "64bcc373ef8c0e42bf16acc5",
            "avatarUrl": "/avatars/873308203d28115ae1a9e4d0e26508f4.svg",
            "isPro": false,
            "fullname": "mz.w",
            "user": "iiiiwis",
            "type": "user"
          },
          "name": "Minzheng Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-06T08:33:29.678Z",
          "hidden": false
        },
        {
          "_id": "681975a9fdcf582e6d0effdc",
          "user": {
            "_id": "66641b2fd8e1e34bc621e688",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66641b2fd8e1e34bc621e688/csPETwnx2zCIHSWi9uAi-.png",
            "isPro": false,
            "fullname": "Yongbin Li",
            "user": "Yongbin-Li",
            "type": "user"
          },
          "name": "Yongbin Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:05:06.051Z",
          "hidden": false
        },
        {
          "_id": "681975a9fdcf582e6d0effdd",
          "name": "Haobo Wang",
          "hidden": false
        },
        {
          "_id": "681975a9fdcf582e6d0effde",
          "name": "Xinghua Zhang",
          "hidden": false
        },
        {
          "_id": "681975a9fdcf582e6d0effdf",
          "name": "Nan Xu",
          "hidden": false
        },
        {
          "_id": "681975a9fdcf582e6d0effe0",
          "user": {
            "_id": "668bd45044ab5de7e4c5b1e7",
            "avatarUrl": "/avatars/9b087cfcac65a649a12568b601d5ca53.svg",
            "isPro": false,
            "fullname": "bingli wu",
            "user": "bingliwu",
            "type": "user"
          },
          "name": "Bingli Wu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:05:37.861Z",
          "hidden": false
        },
        {
          "_id": "681975a9fdcf582e6d0effe1",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "681975a9fdcf582e6d0effe2",
          "name": "Haiyang Yu",
          "hidden": false
        },
        {
          "_id": "681975a9fdcf582e6d0effe3",
          "name": "Wenji Mao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-04T15:39:58.000Z",
      "submittedOnDailyAt": "2025-05-06T01:07:27.275Z",
      "title": "Penser en Tes Pieds : Pensée Adaptative par Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge de Charge",
      "submittedOnDailyBy": {
        "_id": "64bcc373ef8c0e42bf16acc5",
        "avatarUrl": "/avatars/873308203d28115ae1a9e4d0e26508f4.svg",
        "isPro": false,
        "fullname": "mz.w",
        "user": "iiiiwis",
        "type": "user"
      },
      "summary": "En la simulation efficace de l'intelligence social, les entités de langage doivent avoir la capacité de régler dynamiquement la profondeur de la raison. Dans les méthodes actuelles, cette capacité de raisonnement est particulièrement insuffisante. Les méthodes existantes obligent à relier de longs et continus sentiments sur tout l'écran, ce qui conduit à un usage excessif de tokens et à une simulation social inadéquate. Dans cet article, nous proposons un approche basée sur le contexte temporel pour sélectionner stratégiquement entre quatre modes de pensée (réaction intuitive → réflexion profonde) par l'Apprentissage du Modèle Adaptatif (AML). L'innovation centrale de ce cadre est l'algorithme d'Optimisation des Politiques du Modèle Adaptatif (AMPO), qui introduit trois améliorations par rapport aux méthodes existantes : (1) conception de modes de pensée multigranulaires, (2) changement de mode en fonction du contexte d'interaction social, et (3) traitement adaptatif de la profondeur basé sur l'efficacité de tokens. Des expériences larges ont été réalisées sur des tâches d'intelligence social et l'AML a atteint un rendement de tâche 15,6% plus élevé que les méthodes les plus avancées. En particulier, notre méthode dépasse GRPO de 7,0% et réduit la longueur de la phrase de raisonnement de 32,8%. Ces résultats montrent que la sélection de modes de pensée contextuelle mise en œuvre par AMPO permet un approche de raisonnement plus humain que l'approche de profondeur fixe de GRPO.",
      "upvotes": 11,
      "discussionId": "681975a9fdcf582e6d0f0014",
      "githubRepo": "https://github.com/MozerWang/AMPO",
      "ai_keywords": [
        "Adaptive Mode Learning (AML)",
        "Adaptive Mode Policy Optimization (AMPO)",
        "multi-granular thinking mode design",
        "context-aware mode switching",
        "token-efficient reasoning",
        "depth-adaptive processing",
        "intuitive reaction",
        "deep contemplation",
        "social interaction",
        "reasoning chains",
        "fixed-depth approach"
      ]
    },
    "publishedAt": "2025-05-04T11:39:58.000Z",
    "title": "Think on your Feet: Adaptive Thinking via Reinforcement Learning for\n  Social Agents",
    "summary": "Effective social intelligence simulation requires language agents to\ndynamically adjust reasoning depth, a capability notably absent in current\napproaches. While existing methods either lack this kind of reasoning\ncapability or enforce uniform long chain-of-thought reasoning across all\nscenarios, resulting in excessive token usage and inappropriate social\nsimulation. In this paper, we propose Adaptive Mode\nLearning (AML) that strategically selects from four\nthinking modes (intuitive reaction rightarrow deep contemplation) based on\nreal-time context. Our framework's core innovation, the Adaptive\nMode Policy Optimization (AMPO)\nalgorithm, introduces three key advancements over existing methods: (1)\nMulti-granular thinking mode design, (2) Context-aware mode switching across\nsocial interaction, and (3) Token-efficient reasoning via depth-adaptive\nprocessing. Extensive experiments on social intelligence tasks confirm that AML\nachieves 15.6% higher task performance than state-of-the-art methods. Notably,\nour method outperforms GRPO by 7.0% with 32.8% shorter reasoning chains. These\nresults demonstrate that context-sensitive thinking mode selection, as\nimplemented in AMPO, enables more human-like adaptive reasoning than GRPO's\nfixed-depth approach",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02156.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64bcc373ef8c0e42bf16acc5",
      "avatarUrl": "/avatars/873308203d28115ae1a9e4d0e26508f4.svg",
      "fullname": "mz.w",
      "name": "iiiiwis",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.01658",
      "authors": [
        {
          "_id": "6819950bd55db085708dd2e5",
          "user": {
            "_id": "670cb786e73576f33a339144",
            "avatarUrl": "/avatars/c172887c32878aebafd786061680ea1e.svg",
            "isPro": false,
            "fullname": "Sihyeong Park",
            "user": "inputsh",
            "type": "user"
          },
          "name": "Sihyeong Park",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:09:00.902Z",
          "hidden": false
        },
        {
          "_id": "6819950bd55db085708dd2e6",
          "name": "Sungryeol Jeon",
          "hidden": false
        },
        {
          "_id": "6819950bd55db085708dd2e7",
          "user": {
            "_id": "64aaa12a04e7b379fed24327",
            "avatarUrl": "/avatars/327482e569c24ee4c97064f07ddd6de7.svg",
            "isPro": false,
            "fullname": "Chaelyn Lee",
            "user": "oos2",
            "type": "user"
          },
          "name": "Chaelyn Lee",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:09:13.239Z",
          "hidden": false
        },
        {
          "_id": "6819950bd55db085708dd2e8",
          "user": {
            "_id": "6719f17ac5837d514cfff13b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/GnL4RCj7xncVhCIFN5y35.png",
            "isPro": false,
            "fullname": "Seokhun Jeon",
            "user": "Devcow",
            "type": "user"
          },
          "name": "Seokhun Jeon",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:09:18.870Z",
          "hidden": false
        },
        {
          "_id": "6819950bd55db085708dd2e9",
          "name": "Byung-Soo Kim",
          "hidden": false
        },
        {
          "_id": "6819950bd55db085708dd2ea",
          "user": {
            "_id": "65b9dee19c4955ae7aee4954",
            "avatarUrl": "/avatars/263f129605c7763185c49076174b891b.svg",
            "isPro": false,
            "fullname": "Jemin Lee",
            "user": "leejaymin",
            "type": "user"
          },
          "name": "Jemin Lee",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-06T08:32:52.950Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-03T02:47:43.000Z",
      "submittedOnDailyAt": "2025-05-06T03:21:53.083Z",
      "title": "Étude sur le moteur d'inférence des modèles de langage grands : depuis la perspective d'optimisation et d'efficacité",
      "submittedOnDailyBy": {
        "_id": "65b9dee19c4955ae7aee4954",
        "avatarUrl": "/avatars/263f129605c7763185c49076174b891b.svg",
        "isPro": false,
        "fullname": "Jemin Lee",
        "user": "leejaymin",
        "type": "user"
      },
      "summary": "Les modèles de langage grand (LLMs) sont largement appliqués dans diverses domaines tels que les bots de chat, les services de génération de code et les moteurs de recherche. Des tâches complexes, comme Chain Short, la logique complexe et les services d'agent, augmentent significativement les coûts d'inférence en nécessitant le redéploiement des modèles. Des méthodes d'optimisation comme la parallélisation, la compression et le caching ont été introduites, mais la sélection de ces méthodes peut être complexe en fonction des besoins des services. Récemment, des moteurs d'inférence spécialisés ont apparu comme des composants cruciaux pour intégrer des méthodes d'optimisation spécifiques aux services. Cependant, la recherche systématique sur ces moteurs d'inférence est encore insuffisante. Dans cet article, nous présentons une évaluation détaillée de 25 moteurs d'inférence ouverts et commerciaux. Nous examinons les méthodes d'utilisation, la distribution, le support aux langages communs, l'échelle, la portabilité et les caractéristiques appropriées pour les calculs en latin dans chaque moteur d'inférence. De plus, nous examinons la technologie d'optimisation que chaque moteur d'inférence supporte pour définir clairement ses objectifs de conception. Nous évaluons également la maturité de l'écosystème des moteurs d'inférence ouverts et comparons le rendement et les politiques de coût des solutions commerciales. Nous définissons des directions futures de recherche et fournissons des guides pratiques pour les chercheurs et développeurs, incluant le soutien aux services complexes basés sur les LLMs, des matériels variés et des améliorations en sécurité. De plus, nous proposons des dépôts publics pour suivre de manière continue le développement de ce domaine en pleine évolution : https://github.com/sihyeong/Awesome-LLM-Inference-Engine",
      "upvotes": 9,
      "discussionId": "6819950cd55db085708dd32a",
      "ai_keywords": [
        "chain-of-thought",
        "complex reasoning",
        "agent services",
        "inference cost",
        "parallelism",
        "compression",
        "caching",
        "LLM inference engines",
        "ease-of-use",
        "ease-of-deployment",
        "general-purpose support",
        "scalability",
        "throughput-aware computation",
        "latency-aware computation",
        "optimization techniques",
        "ecosystem maturity",
        "performance",
        "cost policy",
        "LLM-based services",
        "enhanced security"
      ]
    },
    "publishedAt": "2025-05-02T22:47:43.000Z",
    "title": "A Survey on Inference Engines for Large Language Models: Perspectives on\n  Optimization and Efficiency",
    "summary": "Large language models (LLMs) are widely applied in chatbots, code generators,\nand search engines. Workloads such as chain-of-thought, complex reasoning, and\nagent services significantly increase the inference cost by invoking the model\nrepeatedly. Optimization methods such as parallelism, compression, and caching\nhave been adopted to reduce costs, but the diverse service requirements make it\nhard to select the right method. Recently, specialized LLM inference engines\nhave emerged as a key component for integrating the optimization methods into\nservice-oriented infrastructures. However, a systematic study on inference\nengines is still lacking. This paper provides a comprehensive evaluation of 25\nopen-source and commercial inference engines. We examine each inference engine\nin terms of ease-of-use, ease-of-deployment, general-purpose support,\nscalability, and suitability for throughput- and latency-aware computation.\nFurthermore, we explore the design goals of each inference engine by\ninvestigating the optimization techniques it supports. In addition, we assess\nthe ecosystem maturity of open source inference engines and handle the\nperformance and cost policy of commercial solutions. We outline future research\ndirections that include support for complex LLM-based services, support of\nvarious hardware, and enhanced security, offering practical guidance to\nresearchers and developers in selecting and designing optimized LLM inference\nengines. We also provide a public repository to continually track developments\nin this fast-evolving field:\nhttps://github.com/sihyeong/Awesome-LLM-Inference-Engine",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.01658.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65b9dee19c4955ae7aee4954",
      "avatarUrl": "/avatars/263f129605c7763185c49076174b891b.svg",
      "fullname": "Jemin Lee",
      "name": "leejaymin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.01441",
      "authors": [
        {
          "_id": "68198aea57d4de18fb3e69d6",
          "user": {
            "_id": "61ffaa2943eb0913fa2df74a",
            "avatarUrl": "/avatars/a19971f830abb8a8ae95e5800beb9fcd.svg",
            "isPro": false,
            "fullname": "Singh",
            "user": "joykirat",
            "type": "user"
          },
          "name": "Joykirat Singh",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-06T08:33:08.255Z",
          "hidden": false
        },
        {
          "_id": "68198aea57d4de18fb3e69d7",
          "user": {
            "_id": "622ca32345261ac5cc0bdade",
            "avatarUrl": "/avatars/7e1d633be69cf86a3affb9168b1cc27b.svg",
            "isPro": false,
            "fullname": "Raghav Magazine",
            "user": "Raghav2002",
            "type": "user"
          },
          "name": "Raghav Magazine",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:06:32.498Z",
          "hidden": false
        },
        {
          "_id": "68198aea57d4de18fb3e69d8",
          "user": {
            "_id": "64aba383fddf117e6e5ba818",
            "avatarUrl": "/avatars/ee7d25d865b34be5902872d060ad9153.svg",
            "isPro": false,
            "fullname": "Akshay  Nambi",
            "user": "akshaynambi",
            "type": "user"
          },
          "name": "Yash Pandya",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-06T04:08:10.843Z",
          "hidden": false
        },
        {
          "_id": "68198aea57d4de18fb3e69d9",
          "user": {
            "_id": "64aba383fddf117e6e5ba818",
            "avatarUrl": "/avatars/ee7d25d865b34be5902872d060ad9153.svg",
            "isPro": false,
            "fullname": "Akshay  Nambi",
            "user": "akshaynambi",
            "type": "user"
          },
          "name": "Akshay Nambi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:06:46.024Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-28T10:42:49.000Z",
      "submittedOnDailyAt": "2025-05-06T02:43:42.049Z",
      "title": "Le méthode d'application de la Réasonnement Agentique et de l'Intégration de Outils dans les Modèles de Langue d'Apprentissage par Apprentissage par Renforcement (RL) pour sa mise en œuvre.",
      "submittedOnDailyBy": {
        "_id": "64aba383fddf117e6e5ba818",
        "avatarUrl": "/avatars/ee7d25d865b34be5902872d060ad9153.svg",
        "isPro": false,
        "fullname": "Akshay  Nambi",
        "user": "akshaynambi",
        "type": "user"
      },
      "summary": "Les modèles de langage grands (LLMs) causant des étonnements sur des problèmes logiques complexes, mais ayant des limites fondamentales en raison de leur fonctionnement basé sur des connaissances internes fixes et des contextes. La résolution de problèmes mondiaux réels nécessite la capacité de prendre des décisions adaptatives, d'interagir avec des outils et des environnements externes, et d'appliquer une logique multiniveau. Dans cet article, nous présentons un cadre intégré appelé Agentic Reasoning and Tool Integration in Self-improving Transformers (ARTIST). Ce cadre combine strictement la logique des agents, l'apprentissage par renforcement et l'intégration d'outils dans les modèles de LLMs. ARTIST peut décider automatiquement à quel moment et comment appliquer une outil dans une chaîne de raisonnement multiniveau, et apprend des stratégies puissantes d'interaction avec les outils et l'environnement en apprenant par renforcement. Les expériences en logique mathématique et dans des cadres de tests de fonctions multiniveau montrent que ARTIST concorde avec les normes les plus avancées et améliore significativement (jusqu'à 22% absolu) par rapport aux modèles de base. Les analyses détaillées et les métriques montrent que l'apprentissage par renforcement basé sur les agents permet aux modèles d'apprendre des logiques profondes, d'utiliser les outils de manière plus efficace et de générer des solutions de haute qualité. Nos résultats ouvrent une nouvelle direction vers la résolution de problèmes solides, interprétables et généralisables dans les modèles de LLMs, et ont ouvert une nouvelle perspective sur la combinaison de l'intégration d'outils et l'apprentissage par renforcement.",
      "upvotes": 9,
      "discussionId": "68198aec57d4de18fb3e6a30",
      "projectPage": "https://www.microsoft.com/en-us/research/people/akshayn/unlocking-agentic-reasoning-in-llms/",
      "ai_keywords": [
        "agentic reasoning",
        "reinforcement learning",
        "tool integration",
        "ARTIST",
        "multi-turn reasoning chains",
        "outcome-based RL",
        "mathematical reasoning",
        "function calling",
        "agentic RL",
        "tool use",
        "environment interaction"
      ]
    },
    "publishedAt": "2025-04-28T06:42:49.000Z",
    "title": "Agentic Reasoning and Tool Integration for LLMs via Reinforcement\n  Learning",
    "summary": "Large language models (LLMs) have achieved remarkable progress in complex\nreasoning tasks, yet they remain fundamentally limited by their reliance on\nstatic internal knowledge and text-only reasoning. Real-world problem solving\noften demands dynamic, multi-step reasoning, adaptive decision making, and the\nability to interact with external tools and environments. In this work, we\nintroduce ARTIST (Agentic Reasoning and Tool Integration in Self-improving\nTransformers), a unified framework that tightly couples agentic reasoning,\nreinforcement learning, and tool integration for LLMs. ARTIST enables models to\nautonomously decide when, how, and which tools to invoke within multi-turn\nreasoning chains, leveraging outcome-based RL to learn robust strategies for\ntool use and environment interaction without requiring step-level supervision.\nExtensive experiments on mathematical reasoning and multi-turn function calling\nbenchmarks show that ARTIST consistently outperforms state-of-the-art\nbaselines, with up to 22% absolute improvement over base models and strong\ngains on the most challenging tasks. Detailed studies and metric analyses\nreveal that agentic RL training leads to deeper reasoning, more effective tool\nuse, and higher-quality solutions. Our results establish agentic RL with tool\nintegration as a powerful new frontier for robust, interpretable, and\ngeneralizable problem-solving in LLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.01441.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64aba383fddf117e6e5ba818",
      "avatarUrl": "/avatars/ee7d25d865b34be5902872d060ad9153.svg",
      "fullname": "Akshay  Nambi",
      "name": "akshaynambi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.02370",
      "authors": [
        {
          "_id": "68197c200e4203d6bc84cdfb",
          "user": {
            "_id": "637f0eb22438d7485b8ef5d7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637f0eb22438d7485b8ef5d7/70h7dekqj7LuBobOXckmJ.jpeg",
            "isPro": false,
            "fullname": "Ming Li",
            "user": "limingcv",
            "type": "user"
          },
          "name": "Ming Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-06T08:33:18.243Z",
          "hidden": false
        },
        {
          "_id": "68197c200e4203d6bc84cdfc",
          "name": "Xin Gu",
          "hidden": false
        },
        {
          "_id": "68197c200e4203d6bc84cdfd",
          "name": "Fan Chen",
          "hidden": false
        },
        {
          "_id": "68197c200e4203d6bc84cdfe",
          "user": {
            "_id": "64ca92f738837b12d5f63729",
            "avatarUrl": "/avatars/a361be3a5ccf9368717980d1faf69df0.svg",
            "isPro": false,
            "fullname": "Xiaoying Xing",
            "user": "xiaoying0505",
            "type": "user"
          },
          "name": "Xiaoying Xing",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:06:58.770Z",
          "hidden": false
        },
        {
          "_id": "68197c200e4203d6bc84cdff",
          "user": {
            "_id": "644df7eacfb40c94eae71186",
            "avatarUrl": "/avatars/1daa4967efd34d54c59aa95970093dbd.svg",
            "isPro": false,
            "fullname": "Longyin Wen",
            "user": "lionwen",
            "type": "user"
          },
          "name": "Longyin Wen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:07:05.196Z",
          "hidden": false
        },
        {
          "_id": "68197c200e4203d6bc84ce00",
          "name": "Chen Chen",
          "hidden": false
        },
        {
          "_id": "68197c200e4203d6bc84ce01",
          "user": {
            "_id": "65cbdea6d6c974694f09249a",
            "avatarUrl": "/avatars/a317a1f545117e0699e1c56258980fd8.svg",
            "isPro": false,
            "fullname": "Sijie Zhu",
            "user": "Zilence006",
            "type": "user"
          },
          "name": "Sijie Zhu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-06T03:04:04.536Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-05T05:19:40.000Z",
      "submittedOnDailyAt": "2025-05-06T01:34:41.608Z",
      "title": "Super Edit: Ajuste et optimisation de la régulation de l'édition d'images en fonction de directives",
      "submittedOnDailyBy": {
        "_id": "637f0eb22438d7485b8ef5d7",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637f0eb22438d7485b8ef5d7/70h7dekqj7LuBobOXckmJ.jpeg",
        "isPro": false,
        "fullname": "Ming Li",
        "user": "limingcv",
        "type": "user"
      },
      "summary": "Durant le processus de construction du jeu de données, la collecte directe de données d'édition précise est difficile, ce qui conduit généralement à la construction du jeu de données en utilisant divers méthodes d'automatisation. Cela entraîne l'apparition de signaux d'impuretés de sous-section dues aux bruits causés par la correspondance entre les instructions d'édition, l'image originale et l'image éditée. Récemment, des efforts ont été faits pour améliorer les modèles d'édition en générant des images d'édition de haute qualité, en effectuant des apprentissages préalables pour des tâches de reconnaissance visuelle et en introduisant des modèles de langage visuel et de vision (VLMs), bien qu'il n'aient pas réussi à résoudre les problèmes fondamentaux. Dans cet article, une nouvelle solution est proposée pour construire des instructions d'édition plus efficaces en fonction des images données. Cette solution inclut la précision des instructions d'édition, la garantie d'une meilleure correspondance entre l'image originale et l'image éditée, et l'utilisation d'instructions d'édition relatives pour améliorer leur efficacité. Spécifiquement, le modèle d'édition ne dépend pas du contexte et montre des propriétés génératives spécifiques à chaque étape de l'inférence. En se basant sur ces propriétés préalables, un guide unifié est défini pour raffiner les instructions d'édition pour les VLMs. Cependant, certains scénarios d'édition difficiles ne peuvent pas être résolus uniquement avec ces instructions précises. Par conséquent, des instructions positives et négatives sont utilisées pour construire des signaux d'impuretés relatives et sont introduites dans l'entraînement d'un modèle de réseau de neurones en utilisant une perte de tuple pour améliorer l'efficacité de l'impureté. Notre méthode ne nécessite pas nécessairement des modules de VLM ou des tâches d'apprentissage préalable utilisées dans les recherches précédentes. Elle est plus directe et efficace, fournit une meilleure signalisation d'impuretés et offre une nouvelle solution simple et efficace pour l'édition d'images basée sur des commandes. Les résultats sur de multiples benchmarks montrent que notre méthode dépasse significativement l'approche actuelle. Comparée à SmartEdit SOTA, notre méthode a atteint un augmentation de 9,19% sur le benchmark Real-Edit, en utilisant seulement 30% des données d'entraînement et un modèle 13 fois plus petit.",
      "upvotes": 8,
      "discussionId": "68197c240e4203d6bc84cee9",
      "projectPage": "https://liming-ai.github.io/SuperEdit/",
      "githubRepo": "https://github.com/bytedance/SuperEdit",
      "ai_keywords": [
        "contrastive editing instructions",
        "triplet loss",
        "instruction-based image editing",
        "contrastive supervision signals",
        "generation attributes",
        "unified guide",
        "vision-language models (VLMs)",
        "real-edit benchmark",
        "smartedit"
      ]
    },
    "publishedAt": "2025-05-05T01:19:40.000Z",
    "title": "SuperEdit: Rectifying and Facilitating Supervision for Instruction-Based\n  Image Editing",
    "summary": "Due to the challenges of manually collecting accurate editing data, existing\ndatasets are typically constructed using various automated methods, leading to\nnoisy supervision signals caused by the mismatch between editing instructions\nand original-edited image pairs. Recent efforts attempt to improve editing\nmodels through generating higher-quality edited images, pre-training on\nrecognition tasks, or introducing vision-language models (VLMs) but fail to\nresolve this fundamental issue. In this paper, we offer a novel solution by\nconstructing more effective editing instructions for given image pairs. This\nincludes rectifying the editing instructions to better align with the\noriginal-edited image pairs and using contrastive editing instructions to\nfurther enhance their effectiveness. Specifically, we find that editing models\nexhibit specific generation attributes at different inference steps,\nindependent of the text. Based on these prior attributes, we define a unified\nguide for VLMs to rectify editing instructions. However, there are some\nchallenging editing scenarios that cannot be resolved solely with rectified\ninstructions. To this end, we further construct contrastive supervision signals\nwith positive and negative instructions and introduce them into the model\ntraining using triplet loss, thereby further facilitating supervision\neffectiveness. Our method does not require the VLM modules or pre-training\ntasks used in previous work, offering a more direct and efficient way to\nprovide better supervision signals, and providing a novel, simple, and\neffective solution for instruction-based image editing. Results on multiple\nbenchmarks demonstrate that our method significantly outperforms existing\napproaches. Compared with previous SOTA SmartEdit, we achieve 9.19%\nimprovements on the Real-Edit benchmark with 30x less training data and 13x\nsmaller model size.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02370.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "637f0eb22438d7485b8ef5d7",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637f0eb22438d7485b8ef5d7/70h7dekqj7LuBobOXckmJ.jpeg",
      "fullname": "Ming Li",
      "name": "limingcv",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 22
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.01043",
      "authors": [
        {
          "_id": "68196e23d9cad0bb5c90dd9b",
          "user": {
            "_id": "64a62e3302e46deb19a7937e",
            "avatarUrl": "/avatars/43553a80f2c5f6c91742c4ce2d23fe21.svg",
            "isPro": false,
            "fullname": "Zhiwei Hao",
            "user": "Zhiwei840",
            "type": "user"
          },
          "name": "Zhiwei Hao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-06T08:34:14.150Z",
          "hidden": false
        },
        {
          "_id": "68196e23d9cad0bb5c90dd9c",
          "user": {
            "_id": "65c4a574d2db41f74ab2a808",
            "avatarUrl": "/avatars/997a8a51996e909eeb318dc592b6c67a.svg",
            "isPro": false,
            "fullname": "Jianyuan Guo",
            "user": "GGJY",
            "type": "user"
          },
          "name": "Jianyuan Guo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:08:12.386Z",
          "hidden": false
        },
        {
          "_id": "68196e23d9cad0bb5c90dd9d",
          "name": "Li Shen",
          "hidden": false
        },
        {
          "_id": "68196e23d9cad0bb5c90dd9e",
          "user": {
            "_id": "6306dc1fd37ce67e0e53c202",
            "avatarUrl": "/avatars/d53a29925511a516495b1597fd5dc764.svg",
            "isPro": false,
            "fullname": "Yong Luo",
            "user": "csdvT",
            "type": "user"
          },
          "name": "Yong Luo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:08:19.043Z",
          "hidden": false
        },
        {
          "_id": "68196e23d9cad0bb5c90dd9f",
          "name": "Han Hu",
          "hidden": false
        },
        {
          "_id": "68196e23d9cad0bb5c90dda0",
          "user": {
            "_id": "662520a75480987954af60b5",
            "avatarUrl": "/avatars/75d2509d21901c4bb187e93b23540e19.svg",
            "isPro": false,
            "fullname": "Guoxia Wang",
            "user": "Guoxia",
            "type": "user"
          },
          "name": "Guoxia Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:08:27.139Z",
          "hidden": false
        },
        {
          "_id": "68196e23d9cad0bb5c90dda1",
          "name": "Dianhai Yu",
          "hidden": false
        },
        {
          "_id": "68196e23d9cad0bb5c90dda2",
          "name": "Yonggang Wen",
          "hidden": false
        },
        {
          "_id": "68196e23d9cad0bb5c90dda3",
          "name": "Dacheng Tao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-02T06:33:25.000Z",
      "submittedOnDailyAt": "2025-05-06T00:36:54.063Z",
      "title": "Entraînement de modèles de langue de grande échelle avec faible précision : méthodes, problèmes et opportunités",
      "submittedOnDailyBy": {
        "_id": "64a62e3302e46deb19a7937e",
        "avatarUrl": "/avatars/43553a80f2c5f6c91742c4ce2d23fe21.svg",
        "isPro": false,
        "fullname": "Zhiwei Hao",
        "user": "Zhiwei840",
        "type": "user"
      },
      "summary": "Les modèles de langage grands (LLMs) ont réalisé des performances impressionnantes dans diverses domaines. Cependant, les ressources de matériel de grande échelle nécessaires pour leur entraînement représentent un grand obstacle à l'efficacité et à l'échelle. Pour atténuer ce défi, la technologie d'entraînement à faible précision a été largement introduite, ce qui a apporté un progrès clair en matière d'efficacité d'entraînement. De plus, l'entraînement à faible précision comprend plusieurs composants, tels que les poids, les activations et les gradients, qui peuvent être représentés sous différentes formes numériques. Cette diversité a créé une variété de perspectives dans la recherche sur l'entraînement à faible précision, permettant aux chercheurs de obtenir une vision cohérente de la matière. Dans cette recherche, une revue détaillée des méthodes actuelles d'entraînement à faible précision est proposée. Pour organiser ces méthodologies de manière systématique, elles sont classées en trois groupes principaux basés sur les formes numériques compatibles avec le matériel, l'efficacité du calcul et la facilité de lecture, en considérant des formes numériques de base : (1) les méthodes basées sur le point fixe et les entiers, (2) les méthodes basées sur le point flottant, et (3) les méthodes basées sur des formes numériques personnalisées. De plus, des approches d'entraînement de normalisation sont discutées, qui montrent la relation entre l'approche d'entraînement et l'entraînement à faible précision. Enfin, de nombreuses directions de recherche pour le développement de cette domaine sont présentées. La collection d'articles discutés dans cette recherche est disponible sur https://github.com/Hao840/Awesome-Low-Precision-Training.",
      "upvotes": 8,
      "discussionId": "68196e24d9cad0bb5c90de08",
      "githubRepo": "https://github.com/Hao840/Awesome-Low-Precision-Training",
      "ai_keywords": [
        "low-precision training",
        "weights",
        "activations",
        "gradients",
        "fixed-point",
        "integer-based methods",
        "floating-point-based methods",
        "customized format-based methods",
        "quantization-aware training"
      ]
    },
    "publishedAt": "2025-05-02T02:33:25.000Z",
    "title": "Low-Precision Training of Large Language Models: Methods, Challenges,\n  and Opportunities",
    "summary": "Large language models (LLMs) have achieved impressive performance across\nvarious domains. However, the substantial hardware resources required for their\ntraining present a significant barrier to efficiency and scalability. To\nmitigate this challenge, low-precision training techniques have been widely\nadopted, leading to notable advancements in training efficiency. Despite these\ngains, low-precision training involves several componentsx2013such\nas weights, activations, and gradientsx2013each of which can be\nrepresented in different numerical formats. The resulting diversity has created\na fragmented landscape in low-precision training research, making it difficult\nfor researchers to gain a unified overview of the field. This survey provides a\ncomprehensive review of existing low-precision training methods. To\nsystematically organize these approaches, we categorize them into three primary\ngroups based on their underlying numerical formats, which is a key factor\ninfluencing hardware compatibility, computational efficiency, and ease of\nreference for readers. The categories are: (1) fixed-point and integer-based\nmethods, (2) floating-point-based methods, and (3) customized format-based\nmethods. Additionally, we discuss quantization-aware training approaches, which\nshare key similarities with low-precision training during forward propagation.\nFinally, we highlight several promising research directions to advance this\nfield. A collection of papers discussed in this survey is provided in\nhttps://github.com/Hao840/Awesome-Low-Precision-Training.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.01043.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64a62e3302e46deb19a7937e",
      "avatarUrl": "/avatars/43553a80f2c5f6c91742c4ce2d23fe21.svg",
      "fullname": "Zhiwei Hao",
      "name": "Zhiwei840",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.02471",
      "authors": [
        {
          "_id": "681973cfa70a4728958323aa",
          "user": {
            "_id": "644fcbea4f7316588267dc80",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644fcbea4f7316588267dc80/w8-2Gkaw9BN9VzppNXrTP.jpeg",
            "isPro": false,
            "fullname": "Biao Gong",
            "user": "BiaoGong",
            "type": "user"
          },
          "name": "Biao Gong",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:10:35.774Z",
          "hidden": false
        },
        {
          "_id": "681973cfa70a4728958323ab",
          "name": "Cheng Zou",
          "hidden": false
        },
        {
          "_id": "681973cfa70a4728958323ac",
          "user": {
            "_id": "65dd699a89a2a760d15f7d35",
            "avatarUrl": "/avatars/e098b56c413d147d1f38cf33a4b0ecde.svg",
            "isPro": false,
            "fullname": "Dandan Zheng",
            "user": "zhengdd0422",
            "type": "user"
          },
          "name": "Dandan Zheng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:10:11.829Z",
          "hidden": false
        },
        {
          "_id": "681973cfa70a4728958323ad",
          "name": "Hu Yu",
          "hidden": false
        },
        {
          "_id": "681973cfa70a4728958323ae",
          "user": {
            "_id": "64575ac8cd935d48a47774ec",
            "avatarUrl": "/avatars/5d211e2c13d6c4e011e5e58b738413f7.svg",
            "isPro": false,
            "fullname": "chenjingdong ",
            "user": "chenjingdong",
            "type": "user"
          },
          "name": "Jingdong Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:10:51.440Z",
          "hidden": false
        },
        {
          "_id": "681973cfa70a4728958323af",
          "user": {
            "_id": "6417cd278f689506e71439ac",
            "avatarUrl": "/avatars/0993d834c6c3bbc53081aa139ee14a12.svg",
            "isPro": false,
            "fullname": "jianxinsun",
            "user": "jianxinsun",
            "type": "user"
          },
          "name": "Jianxin Sun",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:10:02.728Z",
          "hidden": false
        },
        {
          "_id": "681973cfa70a4728958323b0",
          "name": "Junbo Zhao",
          "hidden": false
        },
        {
          "_id": "681973cfa70a4728958323b1",
          "name": "Jun Zhou",
          "hidden": false
        },
        {
          "_id": "681973cfa70a4728958323b2",
          "name": "Kaixiang Ji",
          "hidden": false
        },
        {
          "_id": "681973cfa70a4728958323b3",
          "name": "Lixiang Ru",
          "hidden": false
        },
        {
          "_id": "681973cfa70a4728958323b4",
          "name": "Libin Wang",
          "hidden": false
        },
        {
          "_id": "681973cfa70a4728958323b5",
          "name": "Qingpei Guo",
          "hidden": false
        },
        {
          "_id": "681973cfa70a4728958323b6",
          "name": "Rui Liu",
          "hidden": false
        },
        {
          "_id": "681973cfa70a4728958323b7",
          "name": "Weilong Chai",
          "hidden": false
        },
        {
          "_id": "681973cfa70a4728958323b8",
          "user": {
            "_id": "67cc852d2cfa481bce2dd07e",
            "avatarUrl": "/avatars/0c1c32ec066a8de9148b083b39d1fab8.svg",
            "isPro": false,
            "fullname": "xinyu xiao",
            "user": "bear-xxy",
            "type": "user"
          },
          "name": "Xinyu Xiao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:11:49.316Z",
          "hidden": false
        },
        {
          "_id": "681973cfa70a4728958323b9",
          "name": "Ziyuan Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-05T08:56:12.000Z",
      "submittedOnDailyAt": "2025-05-06T01:00:49.692Z",
      "title": "Nombre de l'Unité : Architecture Intégrée des Interactions de la Diversité Naturelle",
      "submittedOnDailyBy": {
        "_id": "644fcbea4f7316588267dc80",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644fcbea4f7316588267dc80/w8-2Gkaw9BN9VzppNXrTP.jpeg",
        "isPro": false,
        "fullname": "Biao Gong",
        "user": "BiaoGong",
        "type": "user"
      },
      "summary": "Men-Lite-Uni est un nouveau cadre de travail multi-modèle ouvert qui propose un design fondamentalement multi-modèle conçu pour intégrer la vision et le langage. Ce projet intègre les MetaQueries et le cadre de travail M2-omni, introduisant de nouvelles structures de tokens échelonnables et des arrangements de représentation d'images avec des modèles de diffusion. En utilisant des modèles fixes et des modèles de diffusion apprenables, Men-Lite-Uni peut générer des images à partir de texte et éditer des images en fonction de commandes, permettant également l'expansion de fonctions au-delà de l'compréhension visuelle. Les résultats expérimentaux démontrent la puissante capacité de Men-Lite-Uni, et des processus d'interaction sont fournis pour évaluer son flux. Tout le code et les poids des modèles sont disponibles sous une licence ouverte, favorisant l'amélioration dans la communauté. Ce projet est notable comme un élément monumental de l'intégration de modèles dans l'actualisation du ChatGPT-4o en mars 2025, qui a mis à jour la génération d'images. Men-Lite-Uni est actuellement en phase alpha et est attendu que cela continue de développer dans le futur.",
      "upvotes": 6,
      "discussionId": "681973d2a70a47289583249d",
      "projectPage": "https://github.com/inclusionAI/Ming/tree/main/Ming-unify",
      "githubRepo": "https://github.com/inclusionAI/Ming/tree/main/Ming-unify",
      "ai_keywords": [
        "unified visual generator",
        "multimodal autoregressive model",
        "MetaQueries",
        "M2-omni framework",
        "multi-scale learnable tokens",
        "multi-scale representation alignment strategy",
        "MLLM",
        "learnable diffusion model",
        "text-to-image generation",
        "instruction based image editing"
      ]
    },
    "publishedAt": "2025-05-05T04:56:12.000Z",
    "title": "Ming-Lite-Uni: Advancements in Unified Architecture for Natural\n  Multimodal Interaction",
    "summary": "We introduce Ming-Lite-Uni, an open-source multimodal framework featuring a\nnewly designed unified visual generator and a native multimodal autoregressive\nmodel tailored for unifying vision and language. Specifically, this project\nprovides an open-source implementation of the integrated MetaQueries and\nM2-omni framework, while introducing the novel multi-scale learnable tokens and\nmulti-scale representation alignment strategy. By leveraging a fixed MLLM and a\nlearnable diffusion model, Ming-Lite-Uni enables native multimodal AR models to\nperform both text-to-image generation and instruction based image editing\ntasks, expanding their capabilities beyond pure visual understanding. Our\nexperimental results demonstrate the strong performance of Ming-Lite-Uni and\nillustrate the impressive fluid nature of its interactive process. All code and\nmodel weights are open-sourced to foster further exploration within the\ncommunity. Notably, this work aligns with concurrent multimodal AI milestones -\nsuch as ChatGPT-4o with native image generation updated in March 25, 2025 -\nunderscoring the broader significance of unified models like Ming-Lite-Uni on\nthe path toward AGI. Ming-Lite-Uni is in alpha stage and will soon be further\nrefined.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02471.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "644fcbea4f7316588267dc80",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644fcbea4f7316588267dc80/w8-2Gkaw9BN9VzppNXrTP.jpeg",
      "fullname": "Biao Gong",
      "name": "BiaoGong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.01583",
      "authors": [
        {
          "_id": "6819814653612b577df718e7",
          "user": {
            "_id": "65cd4d6256671dee8ee46392",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65cd4d6256671dee8ee46392/SH30XVQnGiYqYQIeDk3na.jpeg",
            "isPro": false,
            "fullname": "Jen-Hao (Andy) Cheng",
            "user": "andaba",
            "type": "user"
          },
          "name": "Jen-Hao Cheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-06T08:33:15.728Z",
          "hidden": false
        },
        {
          "_id": "6819814653612b577df718e8",
          "name": "Vivian Wang",
          "hidden": false
        },
        {
          "_id": "6819814653612b577df718e9",
          "name": "Huayu Wang",
          "hidden": false
        },
        {
          "_id": "6819814653612b577df718ea",
          "name": "Huapeng Zhou",
          "hidden": false
        },
        {
          "_id": "6819814653612b577df718eb",
          "name": "Yi-Hao Peng",
          "hidden": false
        },
        {
          "_id": "6819814653612b577df718ec",
          "name": "Hou-I Liu",
          "hidden": false
        },
        {
          "_id": "6819814653612b577df718ed",
          "user": {
            "_id": "647e4e8da49bffab5d72fbe0",
            "avatarUrl": "/avatars/c5fb00019c7cea23fe3351ecb1e43195.svg",
            "isPro": false,
            "fullname": "Hsiang-Wei Huang",
            "user": "hsiangwei0903",
            "type": "user"
          },
          "name": "Hsiang-Wei Huang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:13:19.727Z",
          "hidden": false
        },
        {
          "_id": "6819814653612b577df718ee",
          "name": "Kuang-Ming Chen",
          "hidden": false
        },
        {
          "_id": "6819814653612b577df718ef",
          "name": "Cheng-Yen Yang",
          "hidden": false
        },
        {
          "_id": "6819814653612b577df718f0",
          "user": {
            "_id": "637c7503fe115289cfecbe6b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676361945047-637c7503fe115289cfecbe6b.jpeg",
            "isPro": false,
            "fullname": "Wenhao Chai",
            "user": "wchai",
            "type": "user"
          },
          "name": "Wenhao Chai",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:13:37.240Z",
          "hidden": false
        },
        {
          "_id": "6819814653612b577df718f1",
          "user": {
            "_id": "65f8cb651e0c65c13a2b906a",
            "avatarUrl": "/avatars/ffc8ac8f29ab1a3142fe5fab1b2302ca.svg",
            "isPro": false,
            "fullname": "Yi-Ling Chen",
            "user": "yilche",
            "type": "user"
          },
          "name": "Yi-Ling Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:13:43.313Z",
          "hidden": false
        },
        {
          "_id": "6819814653612b577df718f2",
          "user": {
            "_id": "63c8527becdb7c9fdd9cacc6",
            "avatarUrl": "/avatars/c8a3f5e1e5159ae5ead41bd9fc2b9b34.svg",
            "isPro": false,
            "fullname": "Vibhav Vineet",
            "user": "vibhav-vineet",
            "type": "user"
          },
          "name": "Vibhav Vineet",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:13:49.482Z",
          "hidden": false
        },
        {
          "_id": "6819814653612b577df718f3",
          "name": "Qin Cai",
          "hidden": false
        },
        {
          "_id": "6819814653612b577df718f4",
          "name": "Jenq-Neng Hwang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-02T21:00:17.000Z",
      "submittedOnDailyAt": "2025-05-06T01:56:09.960Z",
      "title": "TEMPURA : Prédiction d'événements temporels avec masques d'événements et théorie des raisons pour les actions basées sur la compréhension",
      "submittedOnDailyBy": {
        "_id": "637c7503fe115289cfecbe6b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676361945047-637c7503fe115289cfecbe6b.jpeg",
        "isPro": false,
        "fullname": "Wenhao Chai",
        "user": "wchai",
        "type": "user"
      },
      "summary": "Comprendre les relations d'événements causaux et localiser le temps à grande détail dans des vidéos reste un défi pour les modèles de langage visuel. Les méthodes existantes, soit elles compressent les tokens de vidéo pour réduire la résolution temporelle, soit elles traitent les vidéos comme des flux non segmentés, ce qui complique les frontières des événements à grande détail et limite la modélisation des dépendances causielles. Nous proposons TEMPURA (Prédiction et Compréhension d'Événements Causales pour le Raisonnement d'Actions), un cadre d'apprentissage en deux étapes pour améliorer la compréhension temporelle des vidéos. TEMPURA applique d'abord le raisonnement de prédiction d'événements cachés pour reconstruire les événements manquants et génère des explications causales pas à pas à partir d'annotations d'événements denses, soutenues par des techniques de remplissage efficaces. TEMPURA ensuite apprend à segmenter les vidéos et à captionner densement, décomposant les vidéos en événements non superposés avec des descriptions détaillées et des alignements de dates. Nous entraînons TEMPURA sur VER, un ensemble de données à grande échelle soigneusement curé contenant 1M instances d'entraînement et 500K vidéos avec des descriptions d'événements alignées en temps et des pas de raisonnement structurés. Les expériences sur les benchmarks de localisation temporelle et de détection de highlights montrent que TEMPURA dépasse les modèles de référence forts, confirmant l'avantage de combiner le raisonnement causal avec la segmentation temporelle à grande détail pour améliorer la compréhension des vidéos.",
      "upvotes": 5,
      "discussionId": "6819814853612b577df71943",
      "ai_keywords": [
        "TEMPURA",
        "masked event prediction",
        "causal explanations",
        "dense event annotations",
        "infilling techniques",
        "video segmentation",
        "dense captioning",
        "non-overlapping events",
        "timestamp-aligned descriptions",
        "VER",
        "temporal grounding",
        "highlight detection",
        "baseline models",
        "causal reasoning",
        "fine-grained temporal segmentation"
      ]
    },
    "publishedAt": "2025-05-02T17:00:17.000Z",
    "title": "TEMPURA: Temporal Event Masked Prediction and Understanding for\n  Reasoning in Action",
    "summary": "Understanding causal event relationships and achieving fine-grained temporal\ngrounding in videos remain challenging for vision-language models. Existing\nmethods either compress video tokens to reduce temporal resolution, or treat\nvideos as unsegmented streams, which obscures fine-grained event boundaries and\nlimits the modeling of causal dependencies. We propose TEMPURA (Temporal Event\nMasked Prediction and Understanding for Reasoning in Action), a two-stage\ntraining framework that enhances video temporal understanding. TEMPURA first\napplies masked event prediction reasoning to reconstruct missing events and\ngenerate step-by-step causal explanations from dense event annotations, drawing\ninspiration from effective infilling techniques. TEMPURA then learns to perform\nvideo segmentation and dense captioning to decompose videos into\nnon-overlapping events with detailed, timestamp-aligned descriptions. We train\nTEMPURA on VER, a large-scale dataset curated by us that comprises 1M training\ninstances and 500K videos with temporally aligned event descriptions and\nstructured reasoning steps. Experiments on temporal grounding and highlight\ndetection benchmarks demonstrate that TEMPURA outperforms strong baseline\nmodels, confirming that integrating causal reasoning with fine-grained temporal\nsegmentation leads to improved video understanding.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.01583.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "637c7503fe115289cfecbe6b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676361945047-637c7503fe115289cfecbe6b.jpeg",
      "fullname": "Wenhao Chai",
      "name": "wchai",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 30
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.02823",
      "authors": [
        {
          "_id": "6819893117007d963b997a0b",
          "user": {
            "_id": "66b2e5f5523bf90aa7057467",
            "avatarUrl": "/avatars/ccdb58c2e56cf861e9dcec50c85d7778.svg",
            "isPro": false,
            "fullname": "Guo",
            "user": "Zinan123212",
            "type": "user"
          },
          "name": "Zinan Guo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:14:34.748Z",
          "hidden": false
        },
        {
          "_id": "6819893117007d963b997a0c",
          "name": "Pengze Zhang",
          "hidden": false
        },
        {
          "_id": "6819893117007d963b997a0d",
          "user": {
            "_id": "639709c2be8a14bb9eeea8f6",
            "avatarUrl": "/avatars/c142d71b541dccff91fcfd08a2cc0ce0.svg",
            "isPro": false,
            "fullname": "Yanze Wu",
            "user": "yanze",
            "type": "user"
          },
          "name": "Yanze Wu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:15:02.034Z",
          "hidden": false
        },
        {
          "_id": "6819893117007d963b997a0e",
          "name": "Chong Mou",
          "hidden": false
        },
        {
          "_id": "6819893117007d963b997a0f",
          "name": "Songtao Zhao",
          "hidden": false
        },
        {
          "_id": "6819893117007d963b997a10",
          "user": {
            "_id": "645dcad7a19f3e64bbf35e6c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/rV1uHDSnZv7jAvFq4ftj4.jpeg",
            "isPro": false,
            "fullname": "Qian He",
            "user": "heqian",
            "type": "user"
          },
          "name": "Qian He",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:15:28.996Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-05T17:50:24.000Z",
      "submittedOnDailyAt": "2025-05-06T02:30:32.888Z",
      "title": "MUSAR : Nous utilisons un approche de révision adaptative à partir d'un ensemble de données sur un seul thème, en utilisant des routines d'attention.",
      "submittedOnDailyBy": {
        "_id": "639709c2be8a14bb9eeea8f6",
        "avatarUrl": "/avatars/c142d71b541dccff91fcfd08a2cc0ce0.svg",
        "isPro": false,
        "fullname": "Yanze Wu",
        "user": "yanze",
        "type": "user"
      },
      "summary": "Actuellement, dans l'approche CUSTOMIZE de Danari, deux problèmes importants sont identifiés : la difficulté d'obtenir des données d'entraînement de différentes versions de Danari et la combinaison de caractéristiques entre différentes logiques. Pour résoudre ces problèmes, nous proposons MUSAR (approche CUSTOMIZE de Danari). C'est un cadre simple et efficace pour atteindre un CUSTOMIZE multilogique robuste, même avec des données d'entraînement d'une seule logique. Tout d'abord, pour aborder la limitation des données, nous introduisons l'apprentissage de dessins de pixels avec des ajustements de biais de dispositifs. Ce méthode construit des paires d'entraînement de dessins de pixels à partir d'images d'une seule logique et ajuste les biais de distribution par l'attention dynamique et LoRA de champs doubles, ce qui est effectué de manière dynamique. De plus, pour éliminer la combinaison de caractéristiques entre différentes logiques, nous introduisons la structure de route de l'attention dynamique. Cette structure établit une correspondance 1 à 1 entre les images générées et les logiques conditionnelles de manière adaptative. Ce design permet la décodification des représentations multilogiques et maintient la généralisation des caractéristiques échangables tout en augmentant les références logiques. Les expériences détaillées montrent que MUSAR dépasse les méthodes existantes en termes de qualité, consistance logique et naturelle de la communication, même avec des données d'une seule logique.",
      "upvotes": 2,
      "discussionId": "6819893317007d963b997ab1",
      "githubRepo": "https://github.com/guozinan126/MUSAR",
      "ai_keywords": [
        "debiased diptych learning",
        "diptych training pairs",
        "static attention routing",
        "dual-branch LoRA",
        "dynamic attention routing mechanism",
        "bijective mappings",
        "multi-subject representations",
        "scalable generalization performance"
      ]
    },
    "publishedAt": "2025-05-05T13:50:24.000Z",
    "title": "MUSAR: Exploring Multi-Subject Customization from Single-Subject Dataset\n  via Attention Routing",
    "summary": "Current multi-subject customization approaches encounter two critical\nchallenges: the difficulty in acquiring diverse multi-subject training data,\nand attribute entanglement across different subjects. To bridge these gaps, we\npropose MUSAR - a simple yet effective framework to achieve robust\nmulti-subject customization while requiring only single-subject training data.\nFirstly, to break the data limitation, we introduce debiased diptych learning.\nIt constructs diptych training pairs from single-subject images to facilitate\nmulti-subject learning, while actively correcting the distribution bias\nintroduced by diptych construction via static attention routing and dual-branch\nLoRA. Secondly, to eliminate cross-subject entanglement, we introduce dynamic\nattention routing mechanism, which adaptively establishes bijective mappings\nbetween generated images and conditional subjects. This design not only\nachieves decoupling of multi-subject representations but also maintains\nscalable generalization performance with increasing reference subjects.\nComprehensive experiments demonstrate that our MUSAR outperforms existing\nmethods - even those trained on multi-subject dataset - in image quality,\nsubject consistency, and interaction naturalness, despite requiring only\nsingle-subject dataset.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02823.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "639709c2be8a14bb9eeea8f6",
      "avatarUrl": "/avatars/c142d71b541dccff91fcfd08a2cc0ce0.svg",
      "fullname": "Yanze Wu",
      "name": "yanze",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 140
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.02625",
      "authors": [
        {
          "_id": "681975abba26bf20601bb7ca",
          "user": {
            "_id": "65b7573482d384513443875e",
            "avatarUrl": "/avatars/0f2175e4adf507f5ccb0636c1cb647de.svg",
            "isPro": false,
            "fullname": "Qingkai Fang",
            "user": "poeroz",
            "type": "user"
          },
          "name": "Qingkai Fang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:15:43.683Z",
          "hidden": false
        },
        {
          "_id": "681975abba26bf20601bb7cb",
          "name": "Yan Zhou",
          "hidden": false
        },
        {
          "_id": "681975abba26bf20601bb7cc",
          "user": {
            "_id": "66680c0505c407bfea87667c",
            "avatarUrl": "/avatars/e3c26d2eb13fe8ad2b3fd16897e61e6d.svg",
            "isPro": false,
            "fullname": "Shoutao Guo",
            "user": "guoshoutao",
            "type": "user"
          },
          "name": "Shoutao Guo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:15:53.154Z",
          "hidden": false
        },
        {
          "_id": "681975abba26bf20601bb7cd",
          "user": {
            "_id": "64803e5dc57f629056c601f1",
            "avatarUrl": "/avatars/a9e9c97c70714e3a29bef2cf929ee6b3.svg",
            "isPro": false,
            "fullname": "Shaolei Zhang",
            "user": "zhangshaolei",
            "type": "user"
          },
          "name": "Shaolei Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:16:00.139Z",
          "hidden": false
        },
        {
          "_id": "681975abba26bf20601bb7ce",
          "name": "Yang Feng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-05T12:53:09.000Z",
      "submittedOnDailyAt": "2025-05-06T01:07:20.259Z",
      "title": "LLaMA-Omni2 : Modèle de langage de séquences temporelle basé sur la conversation en temps réel et la synthèse automatique du retour en arrière",
      "submittedOnDailyBy": {
        "_id": "65b7573482d384513443875e",
        "avatarUrl": "/avatars/0f2175e4adf507f5ccb0636c1cb647de.svg",
        "isPro": false,
        "fullname": "Qingkai Fang",
        "user": "poeroz",
        "type": "user"
      },
      "summary": "L'interaction en temps réel, intelligente et naturelle de voix est une partie cruciale de l'interaction humain-ordinateur des prochaines générations. Les derniers avancés ont démontré la possibilité de construire des conversationnels de langage intelligent basés sur des modèles de langage de grande échelle (LLMs). Dans cet article, nous présentons une série de modèles de langage de voix (SpeechLMs) appelés LLaMA-Omni 2, qui ont entre 0.5B et 14B paramètres. Ces modèles possèdent la capacité de réaliser des interactions en temps réel de haute qualité de voix. LLaMA-Omni 2 s'appuie sur la série de modèles Qwen2.5 et a été construit en intégrant un encodeur de voix et un décodeur de streaming de voix de correction automatique. Bien que ces modèles aient été entraînés sur de nombreux échantillons de dialogues de voix de 200K, LLaMA-Omni 2 montre un excellent rendement dans les tests de questions et réponses de langage et de instructions de voix, dépassant les avancés SpeechLMs comme GLM-4-Voice, entraîné sur des milliers d'heures de données de voix.",
      "upvotes": 2,
      "discussionId": "681975abba26bf20601bb7f2",
      "ai_keywords": [
        "speech language models (SpeechLMs)",
        "Qwen2.5",
        "speech encoder",
        "autoregressive streaming speech decoder",
        "spoken question answering",
        "speech instruction following",
        "GLM-4-Voice"
      ]
    },
    "publishedAt": "2025-05-05T08:53:09.000Z",
    "title": "LLaMA-Omni2: LLM-based Real-time Spoken Chatbot with Autoregressive\n  Streaming Speech Synthesis",
    "summary": "Real-time, intelligent, and natural speech interaction is an essential part\nof the next-generation human-computer interaction. Recent advancements have\nshowcased the potential of building intelligent spoken chatbots based on large\nlanguage models (LLMs). In this paper, we introduce LLaMA-Omni 2, a series of\nspeech language models (SpeechLMs) ranging from 0.5B to 14B parameters, capable\nof achieving high-quality real-time speech interaction. LLaMA-Omni 2 is built\nupon the Qwen2.5 series models, integrating a speech encoder and an\nautoregressive streaming speech decoder. Despite being trained on only 200K\nmulti-turn speech dialogue samples, LLaMA-Omni 2 demonstrates strong\nperformance on several spoken question answering and speech instruction\nfollowing benchmarks, surpassing previous state-of-the-art SpeechLMs like\nGLM-4-Voice, which was trained on millions of hours of speech data.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02625.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65b7573482d384513443875e",
      "avatarUrl": "/avatars/0f2175e4adf507f5ccb0636c1cb647de.svg",
      "fullname": "Qingkai Fang",
      "name": "poeroz",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.01456",
      "authors": [
        {
          "_id": "6819c7577c36c576e9cb6bfa",
          "user": {
            "_id": "64f64da90efa33bfe0a3d9ba",
            "avatarUrl": "/avatars/c45fb015433e46a2eeb9518910f75d35.svg",
            "isPro": false,
            "fullname": "Vaidehi Patil",
            "user": "vaidehi99",
            "type": "user"
          },
          "name": "Vaidehi Patil",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:16:35.372Z",
          "hidden": false
        },
        {
          "_id": "6819c7577c36c576e9cb6bfb",
          "user": {
            "_id": "654ffe334d9e71e17becc660",
            "avatarUrl": "/avatars/022b7a77051d26c4e5cbf254b7352eb9.svg",
            "isPro": false,
            "fullname": "Yi-Lin Sung",
            "user": "a2889184",
            "type": "user"
          },
          "name": "Yi-Lin Sung",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:16:41.799Z",
          "hidden": false
        },
        {
          "_id": "6819c7577c36c576e9cb6bfc",
          "name": "Peter Hase",
          "hidden": false
        },
        {
          "_id": "6819c7577c36c576e9cb6bfd",
          "name": "Jie Peng",
          "hidden": false
        },
        {
          "_id": "6819c7577c36c576e9cb6bfe",
          "name": "Tianlong Chen",
          "hidden": false
        },
        {
          "_id": "6819c7577c36c576e9cb6bff",
          "user": {
            "_id": "665d9d3a057f7c508f98c625",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/665d9d3a057f7c508f98c625/u1R9P9sJoAl4zEIcetbPy.jpeg",
            "isPro": false,
            "fullname": "Mohit Bansal",
            "user": "mohitbansal",
            "type": "user"
          },
          "name": "Mohit Bansal",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:17:14.823Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-01T01:54:00.000Z",
      "submittedOnDailyAt": "2025-05-06T06:55:27.299Z",
      "title": "Élimination de données d'entraînement contenant des informations sensibles dans plusieurs modèles de langage génératif : évaluation de benchmark et défense contre les attaques",
      "submittedOnDailyBy": {
        "_id": "64f64da90efa33bfe0a3d9ba",
        "avatarUrl": "/avatars/c45fb015433e46a2eeb9518910f75d35.svg",
        "isPro": false,
        "fullname": "Vaidehi Patil",
        "user": "vaidehi99",
        "type": "user"
      },
      "summary": "Les Modèles de Langage de Grande Taille (LLMs) sont entraînés sur de grands ensembles de données, ce qui implique un risque potentiel de récupérer des informations sensibles (données personnelles ou contenu potentiellement dangereux). Ce risque augmente encore plus en raison de l'intégration d'informations d'images et de texte dans des modèles diversifiés (qui combinent plusieurs LLMs). Les annonces peuvent extraire des détails sensibles en utilisant ces connaissances par les moyens des modèles diversifiés. Pour évaluer l'effet de l'élimination de certaines informations dans les Modèles de Langage et d'Images (MLLMs), il est nécessaire de créer des paires d'images-texte décrites de haute qualité. Les recherches précédentes ont principalement porté sur le texte, mais la façon de diversifier a reçu peu d'attention. Pour combler ce vide, nous présentons le cadre de référence et le cadre de défense pour le benchmark de l'oubli de la forme de diversification (UnLOK-VQA), et nous avons développé des méthodes pour évaluer l'élimination de connaissances spécifiques de la forme de diversification dans les MLLMs. Nous avons étendu les données de réponse aux questions visuelles par un processus automatique, généré de nouveaux exemples pour vérifier la généralisation et la particularité, et filtré manuellement pour maintenir la qualité. Ensuite, nous évaluons 6 objectifs de défense face à 7 attaques (4 blancs et 3 noirs) en utilisant des méthodes qui incluent l'interprétabilité avec un nouveau méthode blanc. Les résultats montrent que les attaques de la forme de diversification sont plus efficaces que les attaques sur le texte ou les images, et que la meilleure défense est l'élimination de l'information de réponse par l'état interne du modèle. De plus, les grands modèles ont une efficacité plus élevée après l'entraînement et montrent que l'échelle peut améliorer la sécurité. UnLOK-VQA sera transformé en un benchmark strict pour promouvoir l'oubli dans les MLLMs.",
      "upvotes": 0,
      "discussionId": "6819c7597c36c576e9cb6c6b",
      "githubRepo": "https://github.com/Vaidehi99/UnLOK-VQA",
      "ai_keywords": [
        "multimodal LLMs",
        "multimodal prompts",
        "targeted unlearning",
        "high-quality, well-annotated image-text pairs",
        "multimodal unlearning",
        "UnLOK-VQA (Unlearning Outside Knowledge VQA)",
        "visual question-answering dataset",
        "varying-proximity samples",
        "whitebox attacks",
        "blackbox attacks",
        "interpretability of hidden states",
        "multimodal attacks",
        "post-editing robustness"
      ]
    },
    "publishedAt": "2025-04-30T21:54:00.000Z",
    "title": "Unlearning Sensitive Information in Multimodal LLMs: Benchmark and\n  Attack-Defense Evaluation",
    "summary": "LLMs trained on massive datasets may inadvertently acquire sensitive\ninformation such as personal details and potentially harmful content. This risk\nis further heightened in multimodal LLMs as they integrate information from\nmultiple modalities (image and text). Adversaries can exploit this knowledge\nthrough multimodal prompts to extract sensitive details. Evaluating how\neffectively MLLMs can forget such information (targeted unlearning)\nnecessitates the creation of high-quality, well-annotated image-text pairs.\nWhile prior work on unlearning has focused on text, multimodal unlearning\nremains underexplored. To address this gap, we first introduce a multimodal\nunlearning benchmark, UnLOK-VQA (Unlearning Outside Knowledge VQA), as well as\nan attack-and-defense framework to evaluate methods for deleting specific\nmultimodal knowledge from MLLMs. We extend a visual question-answering dataset\nusing an automated pipeline that generates varying-proximity samples for\ntesting generalization and specificity, followed by manual filtering for\nmaintaining high quality. We then evaluate six defense objectives against seven\nattacks (four whitebox, three blackbox), including a novel whitebox method\nleveraging interpretability of hidden states. Our results show multimodal\nattacks outperform text- or image-only ones, and that the most effective\ndefense removes answer information from internal model states. Additionally,\nlarger models exhibit greater post-editing robustness, suggesting that scale\nenhances safety. UnLOK-VQA provides a rigorous benchmark for advancing\nunlearning in MLLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.01456.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64f64da90efa33bfe0a3d9ba",
      "avatarUrl": "/avatars/c45fb015433e46a2eeb9518910f75d35.svg",
      "fullname": "Vaidehi Patil",
      "name": "vaidehi99",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]