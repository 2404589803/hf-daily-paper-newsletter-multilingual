[
  {
    "paper": {
      "id": "2505.24864",
      "authors": [
        {
          "_id": "683d2d05ae87a04bca311b22",
          "name": "Mingjie Liu",
          "hidden": false
        },
        {
          "_id": "683d2d05ae87a04bca311b23",
          "name": "Shizhe Diao",
          "hidden": false
        },
        {
          "_id": "683d2d05ae87a04bca311b24",
          "name": "Ximing Lu",
          "hidden": false
        },
        {
          "_id": "683d2d05ae87a04bca311b25",
          "name": "Jian Hu",
          "hidden": false
        },
        {
          "_id": "683d2d05ae87a04bca311b26",
          "name": "Xin Dong",
          "hidden": false
        },
        {
          "_id": "683d2d05ae87a04bca311b27",
          "name": "Yejin Choi",
          "hidden": false
        },
        {
          "_id": "683d2d05ae87a04bca311b28",
          "name": "Jan Kautz",
          "hidden": false
        },
        {
          "_id": "683d2d05ae87a04bca311b29",
          "name": "Yi Dong",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-30T17:59:01.000Z",
      "submittedOnDailyAt": "2025-06-02T03:18:21.654Z",
      "title": "ProRL : Apprentissage de Réaction Prolongé élargit les Frontières de la Réaction dans les Modèles de Langue Larges",
      "submittedOnDailyBy": {
        "_id": "633bd54b00732349209a18fe",
        "avatarUrl": "/avatars/150aa5b8d9bcca24366402932bf2d049.svg",
        "isPro": false,
        "fullname": "Shizhe Diao",
        "user": "shizhediao",
        "type": "user"
      },
      "summary": "Le développement récent de modèles de langue axés sur la logique a mis en avant l'apprentissage par récompense (RL) comme un méthode potentielle pour que les modèles répondent à une récompense. Cependant, il est discuté si le RL réellement étend la capacité logique du modèle ou si simplement il met en avant la production de hautes récompenses dans la distribution de base, et si l'échelle des calculs RL à long terme conduit à un meilleur rendement logique. Ce travail met en question ces hypothèses et montre que l'apprentissage à long terme (ProRL) peut découvrir de nouvelles stratégies logiques que les modèles de base ne peuvent pas atteindre. Un nouveau méthode d'apprentissage appelé ProRL est proposée, qui inclut le contrôle de la variance KL, le reset des politiques de référence et une plus grande variété de tâches. Les analyses expérimentales montrent que les modèles entraînés avec RL dépassent les modèles de base dans une large gamme d'évaluations PAS@k, même dans les cas où les modèles de base échouent complètement. De plus, l'amélioration de la frontière logique est fortement reliée à la capacité des tâches du modèle de base et au temps d'apprentissage, et montre que le RL explore de nouveaux espaces de solutions au fil du temps, qui peuvent être utiles pour les personnes. Ces résultats offrent de nouveaux compromis sur l'étendue logique des modèles de langue par l'intermédiaire du RL et établissent une base pour des futures recherches sur la logique du RL. Les poids du modèle sont publiés et soutiennent la recherche en cours : https://huggingface.co/nvidia/Nemotron-Research-Reasoning-Qwen-1.5B",
      "upvotes": 44,
      "discussionId": "683d2d08ae87a04bca311bd4",
      "ai_summary": "Prolonged reinforcement learning training (ProRL) uncovers novel reasoning strategies in language models, outperforming base models and suggesting meaningful expansion of reasoning capabilities.",
      "ai_keywords": [
        "reinforcement learning",
        "RL",
        "ProRL",
        "KL divergence control",
        "reference policy resetting",
        "pass@k evaluations",
        "reasoning boundary improvements",
        "task competence",
        "long-horizon RL"
      ]
    },
    "publishedAt": "2025-05-30T13:59:01.000Z",
    "title": "ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in\n  Large Language Models",
    "summary": "Recent advances in reasoning-centric language models have highlighted\nreinforcement learning (RL) as a promising method for aligning models with\nverifiable rewards. However, it remains contentious whether RL truly expands a\nmodel's reasoning capabilities or merely amplifies high-reward outputs already\nlatent in the base model's distribution, and whether continually scaling up RL\ncompute reliably leads to improved reasoning performance. In this work, we\nchallenge prevailing assumptions by demonstrating that prolonged RL (ProRL)\ntraining can uncover novel reasoning strategies that are inaccessible to base\nmodels, even under extensive sampling. We introduce ProRL, a novel training\nmethodology that incorporates KL divergence control, reference policy\nresetting, and a diverse suite of tasks. Our empirical analysis reveals that\nRL-trained models consistently outperform base models across a wide range of\npass@k evaluations, including scenarios where base models fail entirely\nregardless of the number of attempts. We further show that reasoning boundary\nimprovements correlates strongly with task competence of base model and\ntraining duration, suggesting that RL can explore and populate new regions of\nsolution space over time. These findings offer new insights into the conditions\nunder which RL meaningfully expands reasoning boundaries in language models and\nestablish a foundation for future work on long-horizon RL for reasoning. We\nrelease model weights to support further research:\nhttps://huggingface.co/nvidia/Nemotron-Research-Reasoning-Qwen-1.5B",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24864.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "633bd54b00732349209a18fe",
      "avatarUrl": "/avatars/150aa5b8d9bcca24366402932bf2d049.svg",
      "fullname": "Shizhe Diao",
      "name": "shizhediao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.24863",
      "authors": [
        {
          "_id": "683d0b3de2a7d8d9778bd141",
          "user": {
            "_id": "6719bfd07c6e6c83a388aeae",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6719bfd07c6e6c83a388aeae/jHxryk04dzHo23TX5F5sz.png",
            "isPro": false,
            "fullname": "Junyu Zhang",
            "user": "jyzhang1208",
            "type": "user"
          },
          "name": "Junyu Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-02T07:40:59.716Z",
          "hidden": false
        },
        {
          "_id": "683d0b3de2a7d8d9778bd142",
          "user": {
            "_id": "6201fc5d91d53938a6432fbf",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6201fc5d91d53938a6432fbf/VLs8ZYaZrop4KBpZn53fH.jpeg",
            "isPro": false,
            "fullname": "Runpei Dong",
            "user": "RunpeiDong",
            "type": "user"
          },
          "name": "Runpei Dong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-02T07:41:03.079Z",
          "hidden": false
        },
        {
          "_id": "683d0b3de2a7d8d9778bd143",
          "name": "Han Wang",
          "hidden": false
        },
        {
          "_id": "683d0b3de2a7d8d9778bd144",
          "name": "Xuying Ning",
          "hidden": false
        },
        {
          "_id": "683d0b3de2a7d8d9778bd145",
          "name": "Haoran Geng",
          "hidden": false
        },
        {
          "_id": "683d0b3de2a7d8d9778bd146",
          "name": "Peihao Li",
          "hidden": false
        },
        {
          "_id": "683d0b3de2a7d8d9778bd147",
          "name": "Xialin He",
          "hidden": false
        },
        {
          "_id": "683d0b3de2a7d8d9778bd148",
          "name": "Yutong Bai",
          "hidden": false
        },
        {
          "_id": "683d0b3de2a7d8d9778bd149",
          "name": "Jitendra Malik",
          "hidden": false
        },
        {
          "_id": "683d0b3de2a7d8d9778bd14a",
          "name": "Saurabh Gupta",
          "hidden": false
        },
        {
          "_id": "683d0b3de2a7d8d9778bd14b",
          "name": "Huan Zhang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6201fc5d91d53938a6432fbf/dBNLCtnWBtBclw0ZZsYBU.png"
      ],
      "publishedAt": "2025-05-30T17:58:36.000Z",
      "submittedOnDailyAt": "2025-06-02T00:55:04.615Z",
      "title": "AlphaOne : Modèle de pensée rapide vs. lente au cours du temps établi",
      "submittedOnDailyBy": {
        "_id": "6201fc5d91d53938a6432fbf",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6201fc5d91d53938a6432fbf/VLs8ZYaZrop4KBpZn53fH.jpeg",
        "isPro": false,
        "fullname": "Runpei Dong",
        "user": "RunpeiDong",
        "type": "user"
      },
      "summary": "Dans cet article, un cadre général de travail est présenté pour ajuster le processus de raisonnement de grands modèles logiques (LRMs), appelé AlphaOne (alpha1). AlphaOne introduit pour la première fois le concept de « moment alpha », qui représente les étapes de pensée échelonnées avec un paramètre général alpha. Dans le moment alpha, le token de transition du processus de raisonnement est modélisé comme un processus de probabilité de Bernoulli et est utilisé pour programmer dynamiquement la vitesse du processus de raisonnement. Après le moment alpha, AlphaOne utilise le token de fin du processus de raisonnement pour arrêter efficacement les pensées lentes et promouvoir la création de logique rapide et efficace. Cette approche uniformise les méthodes d'échelle constantes et permet d'ajuster de manière flexible et dense le processus de raisonnement lent ou rapide. Les études extensives de tests dans différents domaines comme les mathématiques, la programmation et les sciences montrent la excellente capacité logique et efficacité de AlphaOne. Page du projet : https://alphaone-project.github.io/",
      "upvotes": 29,
      "discussionId": "683d0b3ee2a7d8d9778bd1ce",
      "projectPage": "https://alphaone-project.github.io/",
      "githubRepo": "https://github.com/ASTRAL-Group/AlphaOne",
      "ai_summary": "AlphaOne dynamically modulates reasoning in large models by introducing $\\alpha$ moment and Bernoulli process for slow thinking, improving efficiency and capability across diverse domains.",
      "ai_keywords": [
        "AlphaOne",
        "$\\alpha$ moment",
        "Bernoulli stochastic process",
        "large reasoning models",
        "reasoning transition tokens",
        "end-of-thinking token",
        "monotonic scaling methods",
        "fast reasoning",
        "efficient answer generation"
      ]
    },
    "publishedAt": "2025-05-30T13:58:36.000Z",
    "title": "AlphaOne: Reasoning Models Thinking Slow and Fast at Test Time",
    "summary": "This paper presents AlphaOne (alpha1), a universal framework for\nmodulating reasoning progress in large reasoning models (LRMs) at test time.\nalpha1 first introduces alpha moment, which represents the scaled\nthinking phase with a universal parameter alpha. Within this scaled\npre-alpha moment phase, it dynamically schedules slow thinking transitions\nby modeling the insertion of reasoning transition tokens as a Bernoulli\nstochastic process. After the alpha moment, alpha1 deterministically\nterminates slow thinking with the end-of-thinking token, thereby fostering fast\nreasoning and efficient answer generation. This approach unifies and\ngeneralizes existing monotonic scaling methods by enabling flexible and dense\nslow-to-fast reasoning modulation. Extensive empirical studies on various\nchallenging benchmarks across mathematical, coding, and scientific domains\ndemonstrate alpha1's superior reasoning capability and efficiency. Project\npage: https://alphaone-project.github.io/",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6201fc5d91d53938a6432fbf/dBNLCtnWBtBclw0ZZsYBU.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24863.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6201fc5d91d53938a6432fbf",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6201fc5d91d53938a6432fbf/VLs8ZYaZrop4KBpZn53fH.jpeg",
      "fullname": "Runpei Dong",
      "name": "RunpeiDong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.24867",
      "authors": [
        {
          "_id": "683d3d6f3f97feb881155aef",
          "user": {
            "_id": "5df7ca7cda6d0311fd3d53f2",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5df7ca7cda6d0311fd3d53f2/dtAoDSqgNxeO9AYg9V3na.jpeg",
            "isPro": false,
            "fullname": "Ujjwal Upadhyay",
            "user": "ujjwal9",
            "type": "user"
          },
          "name": "Ujjwal Upadhyay",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-02T05:58:12.617Z",
          "hidden": false
        },
        {
          "_id": "683d3d6f3f97feb881155af0",
          "user": {
            "_id": "65262a396b41932089fd7bae",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65262a396b41932089fd7bae/6YIEoAfJojuTW1UOKlwZT.png",
            "isPro": true,
            "fullname": "Mukul Ranjan",
            "user": "mukul54",
            "type": "user"
          },
          "name": "Mukul Ranjan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-02T07:40:23.895Z",
          "hidden": false
        },
        {
          "_id": "683d3d6f3f97feb881155af1",
          "name": "Zhiqiang Shen",
          "hidden": false
        },
        {
          "_id": "683d3d6f3f97feb881155af2",
          "name": "Mohamed Elhoseiny",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-30T17:59:12.000Z",
      "submittedOnDailyAt": "2025-06-02T04:31:40.253Z",
      "title": "TIMEBRILLING : Le motif pour lequel le module de langage vidéo voit comme une personne",
      "submittedOnDailyBy": {
        "_id": "65262a396b41932089fd7bae",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65262a396b41932089fd7bae/6YIEoAfJojuTW1UOKlwZT.png",
        "isPro": true,
        "fullname": "Mukul Ranjan",
        "user": "mukul54",
        "type": "user"
      },
      "summary": "Le développement récent des modèles de langue visuelle et spatiale (VLMs) a montré un progrès notable dans la compréhension des relations spatiales et temporelles dans les films. Cependant, lorsque l'information spatiale est cachée, ces modèles ne peuvent pas identifier des motifs simplement temporels. Nous présentons un benchmark qui simule la nature de la communication non réelle dans les systèmes biologiques de signalisation, en utilisant des tableaux de temps avec bruit pour représenter des informations. Intéressamment, les humains peuvent reconnaître des formes, des textes et des motifs avec une précision supérieure à 98% à travers ces tableaux, tandis que les VLMs les plus récents ne parviennent à aucune précision. Cette différence de performance montre la dépendance excessive à des caractéristiques spatiales et l'impossibilité d'extraire du sens du code temporel. De plus, lorsqu'ils sont entraînés sur des ensembles de données avec un SNR faible spatial, la compréhension temporelle du modèle diminue rapidement, surtout dans les tâches nécessitant un compréhension de petits intervalles de temps. Pour surmonter cette limite, il est nécessaire de concevoir de nouvelles architectures ou paradigmes d'entraînement qui séparent la dépendance spatiale du traitement temporel. Notre analyse systématique montre que ce problème dépend également de l'échelle et de l'architecture du modèle. Nous présentons SpookyBench pour encourager la recherche sur le reconnaissance de motifs temporels et pour réduire la distance entre l'intelligence humaine et les dispositifs. Les données de base et le code sont disponibles sur le site web du projet (https://timeblindness.github.io/).",
      "upvotes": 23,
      "discussionId": "683d3d743f97feb881155c56",
      "projectPage": "https://timeblindness.github.io",
      "githubRepo": "https://github.com/TimeBlindness/time-blindness",
      "ai_summary": "SpookyBench is a benchmark for temporal pattern recognition in videos that highlights the limitations of vision-language models in processing noise-like frames without spatial information.",
      "ai_keywords": [
        "vision-language models",
        "VLMs",
        "spatio-temporal relationships",
        "temporal sequences",
        "noise-like frames",
        "biological signaling",
        "covert communication",
        "frame-level spatial features",
        "temporal understanding",
        "data sets",
        "low spatial signal-to-noise ratios",
        "SNR",
        "temporal reasoning",
        "novel architectures",
        "training paradigms",
        "systematic analysis"
      ]
    },
    "publishedAt": "2025-05-30T13:59:12.000Z",
    "title": "Time Blindness: Why Video-Language Models Can't See What Humans Can?",
    "summary": "Recent advances in vision-language models (VLMs) have made impressive strides\nin understanding spatio-temporal relationships in videos. However, when spatial\ninformation is obscured, these models struggle to capture purely temporal\npatterns. We introduce SpookyBench, a benchmark where information is\nencoded solely in temporal sequences of noise-like frames, mirroring natural\nphenomena from biological signaling to covert communication. Interestingly,\nwhile humans can recognize shapes, text, and patterns in these sequences with\nover 98% accuracy, state-of-the-art VLMs achieve 0% accuracy. This performance\ngap highlights a critical limitation: an over-reliance on frame-level spatial\nfeatures and an inability to extract meaning from temporal cues. Furthermore,\nwhen trained in data sets with low spatial signal-to-noise ratios (SNR),\ntemporal understanding of models degrades more rapidly than human perception,\nespecially in tasks requiring fine-grained temporal reasoning. Overcoming this\nlimitation will require novel architectures or training paradigms that decouple\nspatial dependencies from temporal processing. Our systematic analysis shows\nthat this issue persists across model scales and architectures. We release\nSpookyBench to catalyze research in temporal pattern recognition and bridge the\ngap between human and machine video understanding. Dataset and code has been\nmade available on our project website: https://timeblindness.github.io/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24867.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "65262a396b41932089fd7bae",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65262a396b41932089fd7bae/6YIEoAfJojuTW1UOKlwZT.png",
      "fullname": "Mukul Ranjan",
      "name": "mukul54",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.18842",
      "authors": [
        {
          "_id": "6839543d6451d371f9e834ec",
          "name": "Jiwan Chung",
          "hidden": false
        },
        {
          "_id": "6839543d6451d371f9e834ed",
          "user": {
            "_id": "646aecb04c1cd18b497a50ee",
            "avatarUrl": "/avatars/de15c724056f36a41cb4f375d05ed836.svg",
            "isPro": false,
            "fullname": "Junhyeok Kim",
            "user": "kjunh",
            "type": "user"
          },
          "name": "Junhyeok Kim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-02T07:46:37.442Z",
          "hidden": false
        },
        {
          "_id": "6839543d6451d371f9e834ee",
          "user": {
            "_id": "67021743e4d49b157afd8260",
            "avatarUrl": "/avatars/2a22a18cd45f6d115e8a3a5d1e477dcb.svg",
            "isPro": false,
            "fullname": "Siyeol Kim",
            "user": "siyeolkim",
            "type": "user"
          },
          "name": "Siyeol Kim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-02T07:46:34.334Z",
          "hidden": false
        },
        {
          "_id": "6839543d6451d371f9e834ef",
          "name": "Jaeyoung Lee",
          "hidden": false
        },
        {
          "_id": "6839543d6451d371f9e834f0",
          "name": "Min Soo Kim",
          "hidden": false
        },
        {
          "_id": "6839543d6451d371f9e834f1",
          "name": "Youngjae Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-24T19:30:47.000Z",
      "submittedOnDailyAt": "2025-06-02T02:58:04.513Z",
      "title": "Il n'est pas nécessairement ce qui ne doit pas être vu : pour diverses types de théories logiques interactives destinées à la révision visuelle sélective.",
      "submittedOnDailyBy": {
        "_id": "646aecb04c1cd18b497a50ee",
        "avatarUrl": "/avatars/de15c724056f36a41cb4f375d05ed836.svg",
        "isPro": false,
        "fullname": "Junhyeok Kim",
        "user": "kjunh",
        "type": "user"
      },
      "summary": "Nous présentons v1. C'est une extension légère des Modèles de Langage de Grands Tailles Multimodal (MLLMs). v1 permet une révision visuelle sélective pendant l'inférence. Les MLLMs actuels consument uniquement une fois les données visuelles et basent leur inférence sur la mémoire interne, mais v1 introduit un mécanisme simple et copie pour que le modèle puisse charger de manière dynamique des zones d'images pertinentes pendant le processus d'inférence. Ce mécanisme étend l'architecture existante avec une petite modification, permettant au modèle d'accéder au contexte des tokens visuels en fonction de leur évolution. Pour entraîner cette capacité, nous avons construit le jeu de données v1g, qui comprend 300K tâches d'inférence multimodal et de notations visuelles croisées. Les expériences sur les benchmarks MathVista, MathVision et MathVerse montrent que v1 améliore significativement les résultats comparatifs, en particulier dans les tâches qui nécessitent des références visuelles détaillées et une inférence multiétapes. Nos résultats indiquent que l'approche visuelle dynamique est une direction prometteuse pour améliorer l'inférence multimodale. Les codes, modèles et données seront publiés pour soutenir les futures recherches.",
      "upvotes": 20,
      "discussionId": "6839543f6451d371f9e83544",
      "githubRepo": "https://github.com/jun297/v1",
      "ai_summary": "v1 enhances Multimodal Large Language Models by enabling selective and dynamic visual region retrieval during inference, improving performance on multimodal reasoning tasks.",
      "ai_keywords": [
        "Multimodal Large Language Models (MLLMs)",
        "point-and-copy mechanism",
        "visual tokens",
        "multimodal reasoning traces",
        "visual grounding annotations",
        "MathVista",
        "MathVision",
        "MathVerse",
        "grounded multimodal reasoning"
      ]
    },
    "publishedAt": "2025-05-24T15:30:47.000Z",
    "title": "Don't Look Only Once: Towards Multimodal Interactive Reasoning with\n  Selective Visual Revisitation",
    "summary": "We present v1, a lightweight extension to Multimodal Large Language Models\n(MLLMs) that enables selective visual revisitation during inference. While\ncurrent MLLMs typically consume visual input only once and reason purely over\ninternal memory, v1 introduces a simple point-and-copy mechanism that allows\nthe model to dynamically retrieve relevant image regions throughout the\nreasoning process. This mechanism augments existing architectures with minimal\nmodifications, enabling contextual access to visual tokens based on the model's\nevolving hypotheses. To train this capability, we construct v1g, a dataset of\n300K multimodal reasoning traces with interleaved visual grounding annotations.\nExperiments on three multimodal mathematical reasoning benchmarks -- MathVista,\nMathVision, and MathVerse -- demonstrate that v1 consistently improves\nperformance over comparable baselines, particularly on tasks requiring\nfine-grained visual reference and multi-step reasoning. Our results suggest\nthat dynamic visual access is a promising direction for enhancing grounded\nmultimodal reasoning. Code, models, and data will be released to support future\nresearch.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18842.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "646aecb04c1cd18b497a50ee",
      "avatarUrl": "/avatars/de15c724056f36a41cb4f375d05ed836.svg",
      "fullname": "Junhyeok Kim",
      "name": "kjunh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.14752",
      "authors": [
        {
          "_id": "6832c2c8ba29b909f4013a6d",
          "user": {
            "_id": "67569b1860146dd8c9c8008f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67569b1860146dd8c9c8008f/f5Tz2yVTry4LGyQE2VC6-.jpeg",
            "isPro": false,
            "fullname": "Yihong Tang",
            "user": "HYTYH",
            "type": "user"
          },
          "name": "Yihong Tang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:12:00.941Z",
          "hidden": false
        },
        {
          "_id": "6832c2c8ba29b909f4013a6e",
          "name": "Menglin Kong",
          "hidden": false
        },
        {
          "_id": "6832c2c8ba29b909f4013a6f",
          "name": "Lijun Sun",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/67569b1860146dd8c9c8008f/VaA9NxCa0ncxzh03aZE8c.png",
        "https://cdn-uploads.huggingface.co/production/uploads/67569b1860146dd8c9c8008f/oEgXPOzTQVTDfuTp5Z575.png"
      ],
      "publishedAt": "2025-05-20T13:35:38.000Z",
      "submittedOnDailyAt": "2025-06-02T02:10:16.659Z",
      "title": "Modèle de langage de base de données de grande taille pour la synthèse",
      "submittedOnDailyBy": {
        "_id": "67569b1860146dd8c9c8008f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67569b1860146dd8c9c8008f/f5Tz2yVTry4LGyQE2VC6-.jpeg",
        "isPro": false,
        "fullname": "Yihong Tang",
        "user": "HYTYH",
        "type": "user"
      },
      "summary": "La génération de données synthétiques, de capturer exactement la structure statistique de la distribution du monde réel, est un défi fondamental dans le modélisation de données. Les méthodes traditionnelles généralement basent leur travail sur des hypothèses fortes de paramètres ou sur le conception directe de la structure, ce qui résulte souvent en une insuffisance dans des domaines hautement multidimensionnels. L'avancée récente dans les grands modèles de langage (LLMs) a démontré le potentiel d'une distribution flexible et hautement multidimensionnelle dans leur pré-entraînement. Cependant, les méthodes de sampling basées sur les LLMs pour la génération de données synthétiques sont inefficaces et sont limitées par des contextes fixes, ce qui empêche de garantir la concordance statistique. Tenant compte de ces points, nous proposons le cadre général de génération de données synthétiques LLMSynthor. Ce cadre transforme les LLMs en simulateurs de reconnaissance de structures guidées par rétroalimentation de la distribution. LLMSynthor considère les LLMs comme simulateurs de Copula non paramétriques pour modéliser des relations hautement multidimensionnelles, et introduit le sampling de proposition de LLM pour générer des distributions de proposition basées sur des situations réelles, améliorant l'efficacité du sampling sans nécessité de rejet. Le cycle de synthèse itératif qui minimise les différences dans l'espace statistique adapte les données réelles et synthétiques tout en révélant et en raffinant la structure potentielle de génération. Nous avons évalué LLMSynthor dans des environnements contrôlés et du monde réel avec des types de données différents (structurées et non structurées, tels que : commerce électronique, population, mobilité). Les données synthétiques générées par LLMSynthor montrent une forte similitude statistique, utilité dans des situations réelles et adaptabilité multidimensionnelle, ce qui les transforme en une outil précieux pour des applications en économie, sciences sociales et recherche urbaine.",
      "upvotes": 19,
      "discussionId": "6832c2c9ba29b909f4013aea",
      "projectPage": "https://yihongt.github.io/llmsynthor_web/",
      "githubRepo": "https://github.com/YihongT/LLMSynthor",
      "ai_summary": "LLMSynthor enhances LLMs for efficient and statistically accurate data synthesis through distributional feedback and proposal sampling.",
      "ai_keywords": [
        "Large Language Models",
        "LLMSynthor",
        "nonparametric copula simulator",
        "LLM Proposal Sampling",
        "summary statistics space",
        "synthetic data",
        "statistical fidelity",
        "practical utility",
        "cross-data adaptability"
      ]
    },
    "publishedAt": "2025-05-20T09:35:38.000Z",
    "title": "Large Language Models for Data Synthesis",
    "summary": "Generating synthetic data that faithfully captures the statistical structure\nof real-world distributions is a fundamental challenge in data modeling.\nClassical approaches often depend on strong parametric assumptions or manual\nstructural design and struggle in high-dimensional or heterogeneous domains.\nRecent progress in Large Language Models (LLMs) reveals their potential as\nflexible, high-dimensional priors over real-world distributions. However, when\napplied to data synthesis, standard LLM-based sampling is inefficient,\nconstrained by fixed context limits, and fails to ensure statistical alignment.\nGiven this, we introduce LLMSynthor, a general framework for data synthesis\nthat transforms LLMs into structure-aware simulators guided by distributional\nfeedback. LLMSynthor treats the LLM as a nonparametric copula simulator for\nmodeling high-order dependencies and introduces LLM Proposal Sampling to\ngenerate grounded proposal distributions that improve sampling efficiency\nwithout requiring rejection. By minimizing discrepancies in the summary\nstatistics space, the iterative synthesis loop aligns real and synthetic data\nwhile gradually uncovering and refining the latent generative structure. We\nevaluate LLMSynthor in both controlled and real-world settings using\nheterogeneous datasets in privacy-sensitive domains (e.g., e-commerce,\npopulation, and mobility) that encompass both structured and unstructured\nformats. The synthetic data produced by LLMSynthor shows high statistical\nfidelity, practical utility, and cross-data adaptability, positioning it as a\nvaluable tool across economics, social science, urban studies, and beyond.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/67569b1860146dd8c9c8008f/VaA9NxCa0ncxzh03aZE8c.png",
      "https://cdn-uploads.huggingface.co/production/uploads/67569b1860146dd8c9c8008f/oEgXPOzTQVTDfuTp5Z575.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14752.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67569b1860146dd8c9c8008f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67569b1860146dd8c9c8008f/f5Tz2yVTry4LGyQE2VC6-.jpeg",
      "fullname": "Yihong Tang",
      "name": "HYTYH",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.24098",
      "authors": [
        {
          "_id": "683d2cee5bdbb3803e42bc8a",
          "name": "Zhongmou He",
          "hidden": false
        },
        {
          "_id": "683d2cee5bdbb3803e42bc8b",
          "name": "Yee Man Choi",
          "hidden": false
        },
        {
          "_id": "683d2cee5bdbb3803e42bc8c",
          "name": "Kexun Zhang",
          "hidden": false
        },
        {
          "_id": "683d2cee5bdbb3803e42bc8d",
          "name": "Jiabao Ji",
          "hidden": false
        },
        {
          "_id": "683d2cee5bdbb3803e42bc8e",
          "user": {
            "_id": "65a374a59acab1998092a9bc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65a374a59acab1998092a9bc/M3s_7bSf9G-6b9nLg7N3Z.jpeg",
            "isPro": false,
            "fullname": "Antonio",
            "user": "JuntingZhou",
            "type": "user"
          },
          "name": "Junting Zhou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-02T07:40:34.926Z",
          "hidden": false
        },
        {
          "_id": "683d2cee5bdbb3803e42bc8f",
          "name": "Dejia Xu",
          "hidden": false
        },
        {
          "_id": "683d2cee5bdbb3803e42bc90",
          "name": "Ivan Bercovich",
          "hidden": false
        },
        {
          "_id": "683d2cee5bdbb3803e42bc91",
          "name": "Aidan Zhang",
          "hidden": false
        },
        {
          "_id": "683d2cee5bdbb3803e42bc92",
          "name": "Lei Li",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/62ee423b4bebb4ab55c674b1/yE3pB5JGaOf-sdyDjpCk6.png"
      ],
      "publishedAt": "2025-05-30T01:00:34.000Z",
      "submittedOnDailyAt": "2025-06-02T03:20:27.903Z",
      "title": "HardTests : Génération de code d'IA à travers la synthèse de cas de test de haute qualité.",
      "submittedOnDailyBy": {
        "_id": "62ee423b4bebb4ab55c674b1",
        "avatarUrl": "/avatars/ce2797937e8225937fc84d6847d50077.svg",
        "isPro": false,
        "fullname": "Kexun Zhang",
        "user": "k1z",
        "type": "user"
      },
      "summary": "Le Validateur de Données est un élément crucial dans l'inférence de modèles de langage grands (LLM) et joue un rôle essentiel dans les technologies d'apprentissage par renforcement et autres processus de post-entraînement. Cependant, obtenir un Validateur de Données fiable est un défi, car les erreurs cachées peuvent être détectées par des cas exceptionnels qui sont lisibles pour l'humain mais ne reflètent pas la réalité. Pour aborder ce problème, nous proposons HARDTESTGEN et présentons un pipeline pour la synthèse de tests de haute qualité en utilisant des LLM. Avec ce pipeline, il est possible de créer un ensemble de données de programmation de concours comprenant 47k de problèmes et de tests de haute qualité synthétiques dans HARDTESTS. En comparaison avec les tests existants, les tests générés par HARDTESTGEN améliorent la précision d'un 11,3% de points et la reproductibilité d'un 17,5% de points lors de l'évaluation de code généré par des LLM. Dans des cas plus complexes, l'amélioration en précision peut atteindre un maximum de 40 points. HARDTESTS est efficace dans l'entraînement de modèles et son performance est mesurée par la génération de code dans les futures générations. Nous publions notre ensemble de données et le pipeline de synthèse.",
      "upvotes": 18,
      "discussionId": "683d2cef5bdbb3803e42bccc",
      "projectPage": "https://leililab.github.io/HardTests/",
      "ai_summary": "HARDTESTGEN creates a large, high-quality competitive programming dataset to enhance the precision and recall of verifiers in evaluating LLM-generated code.",
      "ai_keywords": [
        "LLM reasoning",
        "reinforcement learning",
        "verifiers",
        "test synthesis",
        "LLMs",
        "competitive programming",
        "synthetic tests",
        "precision",
        "recall",
        "code generation performance"
      ]
    },
    "publishedAt": "2025-05-29T21:00:34.000Z",
    "title": "HardTests: Synthesizing High-Quality Test Cases for LLM Coding",
    "summary": "Verifiers play a crucial role in large language model (LLM) reasoning, needed\nby post-training techniques such as reinforcement learning. However, reliable\nverifiers are hard to get for difficult coding problems, because a\nwell-disguised wrong solution may only be detected by carefully human-written\nedge cases that are difficult to synthesize. To address this issue, we propose\nHARDTESTGEN, a pipeline for high-quality test synthesis using LLMs. With this\npipeline, we curate a comprehensive competitive programming dataset HARDTESTS\nwith 47k problems and synthetic high-quality tests. Compared with existing\ntests, HARDTESTGEN tests demonstrate precision that is 11.3 percentage points\nhigher and recall that is 17.5 percentage points higher when evaluating\nLLM-generated code. For harder problems, the improvement in precision can be as\nlarge as 40 points. HARDTESTS also proves to be more effective for model\ntraining, measured by downstream code generation performance. We will\nopen-source our dataset and synthesis pipeline at\nhttps://leililab.github.io/HardTests/.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62ee423b4bebb4ab55c674b1/yE3pB5JGaOf-sdyDjpCk6.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24098.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62ee423b4bebb4ab55c674b1",
      "avatarUrl": "/avatars/ce2797937e8225937fc84d6847d50077.svg",
      "fullname": "Kexun Zhang",
      "name": "k1z",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.24878",
      "authors": [
        {
          "_id": "683d160e51706d12b2c6f79f",
          "name": "Yaxin Luo",
          "hidden": false
        },
        {
          "_id": "683d160e51706d12b2c6f7a0",
          "name": "Zhaoyi Li",
          "hidden": false
        },
        {
          "_id": "683d160e51706d12b2c6f7a1",
          "name": "Jiacheng Liu",
          "hidden": false
        },
        {
          "_id": "683d160e51706d12b2c6f7a2",
          "user": {
            "_id": "683d2ac900c71614bab8ea02",
            "avatarUrl": "/avatars/7cb1a5c2c778774262a7d7cb6d309abe.svg",
            "isPro": false,
            "fullname": "Jiacheng Cui",
            "user": "jiachengcui888",
            "type": "user"
          },
          "name": "Jiacheng Cui",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-02T07:40:52.498Z",
          "hidden": false
        },
        {
          "_id": "683d160e51706d12b2c6f7a3",
          "name": "Xiaohan Zhao",
          "hidden": false
        },
        {
          "_id": "683d160e51706d12b2c6f7a4",
          "name": "Zhiqiang Shen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-30T17:59:55.000Z",
      "submittedOnDailyAt": "2025-06-02T01:40:24.093Z",
      "title": "Open CaptchaWorld : Plateforme basée sur les ordinateurs pour tester et évaluer des agents de LLM monomodaux.",
      "submittedOnDailyBy": {
        "_id": "653cb809b424289c5f384a02",
        "avatarUrl": "/avatars/a1565ab5ae51075c75d6857d64c426a8.svg",
        "isPro": true,
        "fullname": "YaxinLuo",
        "user": "YaxinLuo",
        "type": "user"
      },
      "summary": "CAPTCHA joue un rôle crucial en les applications mondiales en tant qu'obstacle pour empêcher l'introduction d'agents web automatisés. Principalement, ils sont utilisés pour éviter la réalisation de tâches automatisées de l'un extrémité à l'autre. Cependant, les agents modernes de LLM multimodaux montrent un comportement surprenant dans les tâches de reconnaissance statique, mais leur capacité à faire face aux défis d'interaction et d'inférence en pas n'a pas été largement testée. Pour résoudre ces différences, nous présentons Open CaptchaWorld. C'est le premier cadre de référence et plateforme web conçu pour évaluer la capacité d'inférence visuelle et interactive d'agents de MLLM via diverses tâches dynamiques de CAPTCHA. Le cadre de référence comprend 225 problèmes de CAPTCHA modernes, incluant 20 différents types, et est évalué en utilisant un nouveau métrique appelé \"Depth of CAPTCHA Reasoning\", qui quantifie la quantité d'étapes cognitives et fonctionnelles nécessaires pour résoudre chaque problème. Les résultats des expériences montrent que les humains atteignent des scores presque parfaits, tandis que les agents MLLM les plus avancés éprouvent des difficultés significatives, avec un taux de succès du agent Browser-Use Openai-o3 de maximum 40%, qui représente seulement le 93,3% du rendement humain. Cela met en évidence le rôle crucial de Open CaptchaWorld comme un cadre de référence important pour diagnostiquer les limites des agents multimodaux actuels et guider le développement de systèmes d'inférence multimodaux plus robustes. Les codes et données sont disponibles sur la suivante URL.",
      "upvotes": 12,
      "discussionId": "683d160f51706d12b2c6f7f4",
      "githubRepo": "https://github.com/MetaAgentX/OpenCaptchaWorld",
      "ai_summary": "Open CaptchaWorld benchmark evaluates MLLM-powered agents on diverse CAPTCHA puzzles, revealing significant performance gaps compared to humans.",
      "ai_keywords": [
        "multimodal LLM",
        "CAPTCHA",
        "visual reasoning",
        "interaction capabilities",
        "CAPTCHA Reasoning Depth",
        "Browser-Use Openai-o3"
      ]
    },
    "publishedAt": "2025-05-30T13:59:55.000Z",
    "title": "Open CaptchaWorld: A Comprehensive Web-based Platform for Testing and\n  Benchmarking Multimodal LLM Agents",
    "summary": "CAPTCHAs have been a critical bottleneck for deploying web agents in\nreal-world applications, often blocking them from completing end-to-end\nautomation tasks. While modern multimodal LLM agents have demonstrated\nimpressive performance in static perception tasks, their ability to handle\ninteractive, multi-step reasoning challenges like CAPTCHAs is largely untested.\nTo address this gap, we introduce Open CaptchaWorld, the first web-based\nbenchmark and platform specifically designed to evaluate the visual reasoning\nand interaction capabilities of MLLM-powered agents through diverse and dynamic\nCAPTCHA puzzles. Our benchmark spans 20 modern CAPTCHA types, totaling 225\nCAPTCHAs, annotated with a new metric we propose: CAPTCHA Reasoning Depth,\nwhich quantifies the number of cognitive and motor steps required to solve each\npuzzle. Experimental results show that humans consistently achieve near-perfect\nscores, state-of-the-art MLLM agents struggle significantly, with success rates\nat most 40.0% by Browser-Use Openai-o3, far below human-level performance,\n93.3%. This highlights Open CaptchaWorld as a vital benchmark for diagnosing\nthe limits of current multimodal agents and guiding the development of more\nrobust multimodal reasoning systems. Code and Data are available at this https\nURL.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24878.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "653cb809b424289c5f384a02",
      "avatarUrl": "/avatars/a1565ab5ae51075c75d6857d64c426a8.svg",
      "fullname": "YaxinLuo",
      "name": "YaxinLuo",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.24862",
      "authors": [
        {
          "_id": "683d54f364b44c0ccabb9e65",
          "name": "Cailin Zhuang",
          "hidden": false
        },
        {
          "_id": "683d54f364b44c0ccabb9e66",
          "name": "Ailin Huang",
          "hidden": false
        },
        {
          "_id": "683d54f364b44c0ccabb9e67",
          "user": {
            "_id": "64b914c8ace99c0723ad83a9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b914c8ace99c0723ad83a9/B4gxNByeVY_xaOcjwiN1j.jpeg",
            "isPro": false,
            "fullname": "Wei Cheng",
            "user": "wchengad",
            "type": "user"
          },
          "name": "Wei Cheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-02T07:40:49.393Z",
          "hidden": false
        },
        {
          "_id": "683d54f364b44c0ccabb9e68",
          "name": "Jingwei Wu",
          "hidden": false
        },
        {
          "_id": "683d54f364b44c0ccabb9e69",
          "name": "Yaoqi Hu",
          "hidden": false
        },
        {
          "_id": "683d54f364b44c0ccabb9e6a",
          "name": "Jiaqi Liao",
          "hidden": false
        },
        {
          "_id": "683d54f364b44c0ccabb9e6b",
          "name": "Zhewei Huang",
          "hidden": false
        },
        {
          "_id": "683d54f364b44c0ccabb9e6c",
          "name": "Hongyuan Wang",
          "hidden": false
        },
        {
          "_id": "683d54f364b44c0ccabb9e6d",
          "name": "Xinyao Liao",
          "hidden": false
        },
        {
          "_id": "683d54f364b44c0ccabb9e6e",
          "name": "Weiwei Cai",
          "hidden": false
        },
        {
          "_id": "683d54f364b44c0ccabb9e6f",
          "name": "Hengyuan Xu",
          "hidden": false
        },
        {
          "_id": "683d54f364b44c0ccabb9e70",
          "name": "Xuanyang Zhang",
          "hidden": false
        },
        {
          "_id": "683d54f364b44c0ccabb9e71",
          "name": "Xianfang Zeng",
          "hidden": false
        },
        {
          "_id": "683d54f364b44c0ccabb9e72",
          "name": "Gang Yu",
          "hidden": false
        },
        {
          "_id": "683d54f364b44c0ccabb9e73",
          "name": "Chi Zhang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64b914c8ace99c0723ad83a9/ZHmu_F8c4mbhiPtomY6H7.mp4"
      ],
      "publishedAt": "2025-05-30T17:58:21.000Z",
      "submittedOnDailyAt": "2025-06-02T06:09:52.296Z",
      "title": "ViStoryBench : Feuille d'évaluation complète pour la visualisation de vidéos courtes",
      "submittedOnDailyBy": {
        "_id": "64b914c8ace99c0723ad83a9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b914c8ace99c0723ad83a9/B4gxNByeVY_xaOcjwiN1j.jpeg",
        "isPro": false,
        "fullname": "Wei Cheng",
        "user": "wchengad",
        "type": "user"
      },
      "summary": "La simulation de l'histoire et de la réalité virtuelle a pour objectif de créer une séquence d'images visuellement cohérentes par rapport aux notes et images de référence fournies. Avec le développement récent des modèles génératifs, elle a évolué. Pour améliorer le rendement des environnements de simulation de l'histoire et de la réalité virtuelle dans des scénarios réels, on introduit le complexe évaluation benchmark ViStoryBench. On collecte divers ensembles de données comprenant différents types d'histoire et de styles d'artistes, et l'évaluation du modèle est réalisée sur plusieurs dimensions, par exemple, la comédie, l'histoire, le plot (narration) et l'art visuel (comme l'animation, la rendition 3D). ViStoryBench vise à équilibrer la structure de l'histoire et les éléments visuels, présentant des histoires dans un seul environnement et dans plusieurs, pour vérifier la cohérence des personnages du modèle. De plus, on évalue les modèles qui tentent de créer des images précises, y compris des projets complexes et des environnements de monde. En étendant la gamme d'indicateurs d'évaluation, on garantit la réalisation d'évaluations détaillées, permettant aux chercheurs d'identifier avec précision les fortes et les faibles points du modèle et de promouvoir des améliorations ciblées.",
      "upvotes": 12,
      "discussionId": "683d54f764b44c0ccabb9f60",
      "projectPage": "https://vistorybench.github.io/",
      "githubRepo": "https://github.com/vistorybench/vistorybench"
    },
    "publishedAt": "2025-05-30T13:58:21.000Z",
    "title": "ViStoryBench: Comprehensive Benchmark Suite for Story Visualization",
    "summary": "Story visualization, which aims to generate a sequence of visually coherent\nimages aligning with a given narrative and reference images, has seen\nsignificant progress with recent advancements in generative models. To further\nenhance the performance of story visualization frameworks in real-world\nscenarios, we introduce a comprehensive evaluation benchmark, ViStoryBench. We\ncollect a diverse dataset encompassing various story types and artistic styles,\nensuring models are evaluated across multiple dimensions such as different\nplots (e.g., comedy, horror) and visual aesthetics (e.g., anime, 3D\nrenderings). ViStoryBench is carefully curated to balance narrative structures\nand visual elements, featuring stories with single and multiple protagonists to\ntest models' ability to maintain character consistency. Additionally, it\nincludes complex plots and intricate world-building to challenge models in\ngenerating accurate visuals. To ensure comprehensive comparisons, our benchmark\nincorporates a wide range of evaluation metrics assessing critical aspects.\nThis structured and multifaceted framework enables researchers to thoroughly\nidentify both the strengths and weaknesses of different models, fostering\ntargeted improvements.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64b914c8ace99c0723ad83a9/ZHmu_F8c4mbhiPtomY6H7.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24862.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b914c8ace99c0723ad83a9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b914c8ace99c0723ad83a9/B4gxNByeVY_xaOcjwiN1j.jpeg",
      "fullname": "Wei Cheng",
      "name": "wchengad",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.24196",
      "authors": [
        {
          "_id": "683d29da83edd521f116444c",
          "name": "Longze Chen",
          "hidden": false
        },
        {
          "_id": "683d29da83edd521f116444d",
          "name": "Renke Shan",
          "hidden": false
        },
        {
          "_id": "683d29da83edd521f116444e",
          "name": "Huiming Wang",
          "hidden": false
        },
        {
          "_id": "683d29da83edd521f116444f",
          "name": "Lu Wang",
          "hidden": false
        },
        {
          "_id": "683d29da83edd521f1164450",
          "name": "Ziqiang Liu",
          "hidden": false
        },
        {
          "_id": "683d29da83edd521f1164451",
          "name": "Run Luo",
          "hidden": false
        },
        {
          "_id": "683d29da83edd521f1164452",
          "name": "Jiawei Wang",
          "hidden": false
        },
        {
          "_id": "683d29da83edd521f1164453",
          "name": "Hamid Alinejad-Rokny",
          "hidden": false
        },
        {
          "_id": "683d29da83edd521f1164454",
          "name": "Min Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-30T04:15:06.000Z",
      "submittedOnDailyAt": "2025-06-02T03:23:19.536Z",
      "title": "CLaSp : Traitement automatique de prédictions en utilisant l'évasion de couches projectives",
      "submittedOnDailyBy": {
        "_id": "64c7b4d1c547ed5243c07b6c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c7b4d1c547ed5243c07b6c/h96CLBj6dcm01soK2UJzr.jpeg",
        "isPro": false,
        "fullname": "Longze Chen",
        "user": "lzchen2001",
        "type": "user"
      },
      "summary": "L'analyse spécifique (SD) est un méthode potentielle pour accélérer le processus de décodage de modèles de langage grands (LLMs). L'efficacité de la SD dépend significativement de la concordance entre le modèle de suppression et le modèle de vérification. Cependant, les techniques actuelles de suppression généralement nécessitent l'entraînement de modules supplémentaires, ce qui pose des défis dans l'implémentation et la compatibilité entre différents LLMs. Dans cet article, nous proposons CLaSp pour fournir une stratégie de saut de couches spécifique pour le décodage d'un modèle propre dans le contexte. A différence des méthodes existantes, CLaSp ne nécessite pas de modules de suppression supplémentaires ou d'entraînement supplémentaire. Au lieu de cela, elle utilise une structure de plugin et un jeu de jeux pour construire un modèle de suppression compressé en utilisant les couches intermédiaires du modèle de vérification. En particulier, nous avons développé un algorithme de programmation dynamique pour optimiser le processus de saut de couches, avec l'objectif d'atteindre un état complet d'état caché dans le dernier pas de vérification. Ainsi, CLaSp peut ajuster les stratégies de saut de couches dynamiquement après chaque pas de vérification, sans dépendre de couches de saut optimisées précédemment. Les résultats des expérimentations sur des tâches de flux postérieures ont montré que CLaSp atteint une augmentation de vitesse de 1,3 à 1,7 fois pour les séries de modèles LLaMA3, sans modifier la distribution initiale des phrases générées.",
      "upvotes": 10,
      "discussionId": "683d29db83edd521f1164482",
      "ai_summary": "CLaSp, an in-context layer-skipping strategy for self-speculative decoding, accelerates Large Language Model decoding without additional modules or training, achieving a 1.3x to 1.7x speedup on LLaMA3 models.",
      "ai_keywords": [
        "speculative decoding",
        "large language models",
        "draft model",
        "verify model",
        "in-context layer-skipping",
        "dynamic programming algorithm",
        "hidden states",
        "verification stage"
      ]
    },
    "publishedAt": "2025-05-30T00:15:06.000Z",
    "title": "CLaSp: In-Context Layer Skip for Self-Speculative Decoding",
    "summary": "Speculative decoding (SD) is a promising method for accelerating the decoding\nprocess of Large Language Models (LLMs). The efficiency of SD primarily hinges\non the consistency between the draft model and the verify model. However,\nexisting drafting approaches typically require additional modules to be\ntrained, which can be challenging to implement and ensure compatibility across\nvarious LLMs. In this paper, we propose CLaSp, an in-context layer-skipping\nstrategy for self-speculative decoding. Unlike prior methods, CLaSp does not\nrequire additional drafting modules or extra training. Instead, it employs a\nplug-and-play mechanism by skipping intermediate layers of the verify model to\nconstruct a compressed draft model. Specifically, we develop a dynamic\nprogramming algorithm that optimizes the layer-skipping process by leveraging\nthe complete hidden states from the last verification stage as an objective.\nThis enables CLaSp to dynamically adjust its layer-skipping strategy after each\nverification stage, without relying on pre-optimized sets of skipped layers.\nExperimental results across diverse downstream tasks demonstrate that CLaSp\nachieves a speedup of 1.3x ~ 1.7x on LLaMA3 series models without altering the\noriginal distribution of the generated text.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24196.png",
    "numComments": 5,
    "submittedBy": {
      "_id": "64c7b4d1c547ed5243c07b6c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c7b4d1c547ed5243c07b6c/h96CLBj6dcm01soK2UJzr.jpeg",
      "fullname": "Longze Chen",
      "name": "lzchen2001",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.23941",
      "authors": [
        {
          "_id": "683cf4405810d395f0a3788b",
          "name": "An Vo",
          "hidden": false
        },
        {
          "_id": "683cf4405810d395f0a3788c",
          "name": "Khai-Nguyen Nguyen",
          "hidden": false
        },
        {
          "_id": "683cf4405810d395f0a3788d",
          "user": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "name": "Mohammad Reza Taesiri",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-06-02T00:52:37.933Z",
          "hidden": false
        },
        {
          "_id": "683cf4405810d395f0a3788e",
          "name": "Vy Tuong Dang",
          "hidden": false
        },
        {
          "_id": "683cf4405810d395f0a3788f",
          "user": {
            "_id": "653194a4c8da3465f4701ad1",
            "avatarUrl": "/avatars/6682164fcaf1d339ce9ac82ba131af5e.svg",
            "isPro": true,
            "fullname": "Khai-Nguyen Nguyen",
            "user": "knguyennguyen",
            "type": "user"
          },
          "name": "Anh Totti Nguyen",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-02T00:45:56.803Z",
          "hidden": false
        },
        {
          "_id": "683cf4405810d395f0a37890",
          "name": "Daeyoung Kim",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T18:47:58.000Z",
      "submittedOnDailyAt": "2025-06-02T03:28:19.444Z",
      "title": "Vision de Long Range Group est défavorisée.",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Les modèles de langue générale (LLMs) se souviennent de nombreuses connaissances sur la réseau, ce qui peut aider aux tâches ultérieures mais peut également être affecté par des biais et des réponses incorrectes. Dans cette étude, nous avons investigué comment le modèle de langue visuelle (VLMs) a un biais dans des tâches visuelles standards et objectifs (détection d'objets et identification) et comment cela affecte la précision du modèle avec des connaissances populaires sur la réseau. De cette manière, il a été clairement démontré que les meilleurs VLMs ont un fort biais. Par exemple, nous avons montré que le reconnaissance de « ADVISOR » dans le modèle « ADVISOR » avec 4 flux n'est pas possible, et la précision moyenne de la détection d'objets dans 7 différentes zones (animaux, logos, échecs, jeux, tricks optiques, tableaux avec motifs) est de 17.05%. En ajoutant du texte (par exemple, « ADVISOR ») à des images réelles, la précision des VLMs diminue encore plus. Le biais des VLMs est fort, et même si le modèle est indiqué pour vérifier les résultats ou seulement croire aux détails de l'image, la précision moyenne ne peut pas être augmentée. Cette étude montre un intéressant mode de défaillance des VLMs et fournit un cadre de travail pour automatiser le biais des VLMs. Les codes et les données sont disponibles sur vlmsarebiased.github.io.",
      "upvotes": 10,
      "discussionId": "683cf4445810d395f0a37983",
      "projectPage": "https://vlmsarebiased.github.io/",
      "githubRepo": "https://github.com/anvo25/vlms-are-biased",
      "ai_summary": "Vision language models exhibit strong biases in counting and identification tasks, demonstrating a failure mode that persist even with additional instructions or context.",
      "ai_keywords": [
        "large language models",
        "vision language models",
        "downstream tasks",
        "popular subjects",
        "accuracy",
        "visual tasks",
        "counting",
        "identification",
        "biases",
        "counterfactual image",
        "automated framework"
      ]
    },
    "publishedAt": "2025-05-29T14:47:58.000Z",
    "title": "Vision Language Models are Biased",
    "summary": "Large language models (LLMs) memorize a vast amount of prior knowledge from\nthe Internet that help them on downstream tasks but also may notoriously sway\ntheir outputs towards wrong or biased answers. In this work, we test how the\nknowledge about popular subjects hurt the accuracy of vision language models\n(VLMs) on standard, objective visual tasks of counting and identification. We\nfind that state-of-the-art VLMs are strongly biased (e.g, unable to recognize a\nfourth stripe has been added to a 3-stripe Adidas logo) scoring an average of\n17.05% accuracy in counting (e.g., counting stripes in an Adidas-like logo)\nacross 7 diverse domains from animals, logos, chess, board games, optical\nillusions, to patterned grids. Insert text (e.g., \"Adidas\") describing the\nsubject name into the counterfactual image further decreases VLM accuracy. The\nbiases in VLMs are so strong that instructing them to double-check their\nresults or rely exclusively on image details to answer improves counting\naccuracy by only +2 points, on average. Our work presents an interesting\nfailure mode in VLMs and an automated framework for testing VLM biases. Code\nand data are available at: vlmsarebiased.github.io.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23941.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 84
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.24521",
      "authors": [
        {
          "_id": "683d11d1495f0b58f2fd49a9",
          "name": "Yang-Tian Sun",
          "hidden": false
        },
        {
          "_id": "683d11d1495f0b58f2fd49aa",
          "name": "Xin Yu",
          "hidden": false
        },
        {
          "_id": "683d11d1495f0b58f2fd49ab",
          "name": "Zehuan Huang",
          "hidden": false
        },
        {
          "_id": "683d11d1495f0b58f2fd49ac",
          "name": "Yi-Hua Huang",
          "hidden": false
        },
        {
          "_id": "683d11d1495f0b58f2fd49ad",
          "name": "Yuan-Chen Guo",
          "hidden": false
        },
        {
          "_id": "683d11d1495f0b58f2fd49ae",
          "name": "Ziyi Yang",
          "hidden": false
        },
        {
          "_id": "683d11d1495f0b58f2fd49af",
          "name": "Yan-Pei Cao",
          "hidden": false
        },
        {
          "_id": "683d11d1495f0b58f2fd49b0",
          "name": "Xiaojuan Qi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-30T12:31:59.000Z",
      "submittedOnDailyAt": "2025-06-02T01:25:45.570Z",
      "title": "UniGeo : Contrôle de la diffusion générique de vidéos unifiée",
      "submittedOnDailyBy": {
        "_id": "6375d136dee28348a9c63cbf",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6375d136dee28348a9c63cbf/gK465HBrQWIOZ-qtHb-Vh.jpeg",
        "isPro": false,
        "fullname": "zehuan-huang",
        "user": "huanngzh",
        "type": "user"
      },
      "summary": "Récemment, les méthodes qui utilisent des modèles de dérivation pour aider à mesurer la structure des monologues (par exemple, la profondeur et la normalité) ont attiré l'attention grâce à leur puissante capacité de généralisation. Cependant, de nombreux études antérieures se concentrent sur la mesure de caractéristiques structurales dans chaque cadre de vidéo à l'intérieur de la coordonnée de la caméra, ignorant la capacité des modèles de dérivation pour déterminer les relations de correspondance entre les cadres dans une structure unique. Dans cet article, nous montrons comment, grâce à un design approprié et à des ajustements, il est possible d'utiliser efficacement la cohérence propre des modèles de génération de vidéo pour mesurer des structures. Spécifiquement, 1) nous choisissons comme objectif de prédiction des vidéos de cadres qui partagent une relation de correspondance commune dans la même coordonnée, pour sélectionner des caractéristiques structurales, 2) nous introduisons une nouvelle forme de normalisation conditionnelle basée sur le réutilisation de données de positionnement, et 3) nous améliorons le rendement en entraînant en parallèle plusieurs caractéristiques structurales qui partagent la même relation de correspondance. Nos résultats montrent que nous atteignons une excellente prédiction des caractéristiques structurales en ensemble, ce qui peut être directement appliqué aux tâches de reconstruction. Si nous entraînons uniquement avec des données de vidéo statiques, notre approche fonctionne également efficacement en généralisation à des séquences de vidéo dynamiques.",
      "upvotes": 8,
      "discussionId": "683d11d3495f0b58f2fd4a95",
      "projectPage": "https://sunyangtian.github.io/UniGeo-web/",
      "githubRepo": "https://github.com/SunYangtian/UniGeo",
      "ai_summary": "Video generation models leveraging diffusion priors achieve superior global geometric attribute estimation and reconstructions, benefiting from inter-frame consistency and joint training on shared attributes.",
      "ai_keywords": [
        "diffusion models",
        "monocular geometric estimation",
        "depth",
        "normal",
        "camera coordinate system",
        "intrinsic consistency",
        "video generation models",
        "global coordinate system",
        "positional encodings",
        "joint training",
        "static video data",
        "dynamic video scenes"
      ]
    },
    "publishedAt": "2025-05-30T08:31:59.000Z",
    "title": "UniGeo: Taming Video Diffusion for Unified Consistent Geometry\n  Estimation",
    "summary": "Recently, methods leveraging diffusion model priors to assist monocular\ngeometric estimation (e.g., depth and normal) have gained significant attention\ndue to their strong generalization ability. However, most existing works focus\non estimating geometric properties within the camera coordinate system of\nindividual video frames, neglecting the inherent ability of diffusion models to\ndetermine inter-frame correspondence. In this work, we demonstrate that,\nthrough appropriate design and fine-tuning, the intrinsic consistency of video\ngeneration models can be effectively harnessed for consistent geometric\nestimation. Specifically, we 1) select geometric attributes in the global\ncoordinate system that share the same correspondence with video frames as the\nprediction targets, 2) introduce a novel and efficient conditioning method by\nreusing positional encodings, and 3) enhance performance through joint training\non multiple geometric attributes that share the same correspondence. Our\nresults achieve superior performance in predicting global geometric attributes\nin videos and can be directly applied to reconstruction tasks. Even when\ntrained solely on static video data, our approach exhibits the potential to\ngeneralize to dynamic video scenes.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24521.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6375d136dee28348a9c63cbf",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6375d136dee28348a9c63cbf/gK465HBrQWIOZ-qtHb-Vh.jpeg",
      "fullname": "zehuan-huang",
      "name": "huanngzh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 32
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.24858",
      "authors": [
        {
          "_id": "683d2a3651706d12b2cc8ace",
          "name": "Gabrielle Kaili-May Liu",
          "hidden": false
        },
        {
          "_id": "683d2a3651706d12b2cc8acf",
          "name": "Gal Yona",
          "hidden": false
        },
        {
          "_id": "683d2a3651706d12b2cc8ad0",
          "name": "Avi Caciularu",
          "hidden": false
        },
        {
          "_id": "683d2a3651706d12b2cc8ad1",
          "name": "Idan Szpektor",
          "hidden": false
        },
        {
          "_id": "683d2a3651706d12b2cc8ad2",
          "name": "Tim G. J. Rudner",
          "hidden": false
        },
        {
          "_id": "683d2a3651706d12b2cc8ad3",
          "name": "Arman Cohan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-30T17:54:08.000Z",
      "submittedOnDailyAt": "2025-06-02T03:13:37.735Z",
      "title": "MetaFaith : L'expression de l'incertitude véritable de la nature du langage naturel dans un modèle de langage génératif",
      "submittedOnDailyBy": {
        "_id": "64f1ca1d5b8a6a5d39d75771",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f1ca1d5b8a6a5d39d75771/Caq_Ahp7Qkm1nHyhBcztE.jpeg",
        "isPro": false,
        "fullname": "John Chih Liu",
        "user": "johncliu",
        "type": "user"
      },
      "summary": "Un des éléments importants pour la confiance dans les LLMs est la transmission de l'incertitude qui peut être confiée, mais les LLMs utilisent des expressions de confiance dans la transmission de déclarations fausses, ce qui peut conduire à des confiances exagérées ou à la rupture de la confiance. Nous proposons un premier étude systématique pour évaluer la capacité des LLMs à utiliser le langage qui reflète l'incertitude propre, dans un large éventail de modèles, datasets et stratégies de prompts. Nos résultats montrent que les LLMs échouent significativement à cette tâche et que les solutions existantes ne sont pas suffisantes : le méthode standard de prompts fournit seulement quelques avantages, et les méthodes d'ajustement basées sur des faits peuvent également causer des dommages en termes de précision. Pour résoudre ces importantes limitations, nous proposons un nouvel approche d'ajustement basée sur la métacognition humaine, appelée MetaFaith. MetaFaith améliore significativement la précision dans les ajustements sur différents modèles et domaines de tâches, atteignant un amélioration maximale de 61% en précision et une augmentation de 83% en probabilité d'être évalué comme généré initialement par un humain.",
      "upvotes": 7,
      "discussionId": "683d2a3751706d12b2cc8b0a",
      "ai_summary": "A study reveals that Large Language Models (LLMs) struggle with expressing uncertainty accurately and introduces MetaFaith, a prompt-based method that enhances their calibration significantly.",
      "ai_keywords": [
        "faithful confidence calibration",
        "linguistic expressions of uncertainty",
        "intrinsic uncertainty",
        "prompting strategies",
        "metacognition"
      ]
    },
    "publishedAt": "2025-05-30T13:54:08.000Z",
    "title": "MetaFaith: Faithful Natural Language Uncertainty Expression in LLMs",
    "summary": "A critical component in the trustworthiness of LLMs is reliable uncertainty\ncommunication, yet LLMs often use assertive language when conveying false\nclaims, leading to over-reliance and eroded trust. We present the first\nsystematic study of faithful confidence calibration of LLMs,\nbenchmarking models' ability to use linguistic expressions of uncertainty that\nfaithfully reflect their intrinsic uncertainty, across a\ncomprehensive array of models, datasets, and prompting strategies. Our results\ndemonstrate that LLMs largely fail at this task, and that existing\ninterventions are insufficient: standard prompt approaches provide only\nmarginal gains, and existing, factuality-based calibration techniques can even\nharm faithful calibration. To address this critical gap, we introduce\nMetaFaith, a novel prompt-based calibration approach inspired by human\nmetacognition. We show that MetaFaith robustly improves faithful calibration\nacross diverse models and task domains, enabling up to 61% improvement in\nfaithfulness and achieving an 83% win rate over original generations as judged\nby humans.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24858.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64f1ca1d5b8a6a5d39d75771",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f1ca1d5b8a6a5d39d75771/Caq_Ahp7Qkm1nHyhBcztE.jpeg",
      "fullname": "John Chih Liu",
      "name": "johncliu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.24417",
      "authors": [
        {
          "_id": "683d0b6c5810d395f0a9a49e",
          "name": "Runnan Lu",
          "hidden": false
        },
        {
          "_id": "683d0b6c5810d395f0a9a49f",
          "name": "Yuxuan Zhang",
          "hidden": false
        },
        {
          "_id": "683d0b6c5810d395f0a9a4a0",
          "name": "Jailing Liu",
          "hidden": false
        },
        {
          "_id": "683d0b6c5810d395f0a9a4a1",
          "name": "Haifa Wang",
          "hidden": false
        },
        {
          "_id": "683d0b6c5810d395f0a9a4a2",
          "name": "Yiren Song",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-30T09:55:39.000Z",
      "submittedOnDailyAt": "2025-06-02T00:55:24.254Z",
      "title": "EasyText: Transformateur de Diffusion Contrôlable pour Texte Multilingue\nRéseau",
      "submittedOnDailyBy": {
        "_id": "64311a95034ecbefddd141ef",
        "avatarUrl": "/avatars/b6dc5ca373bedbaa368208517954c375.svg",
        "isPro": true,
        "fullname": "Yiren Song",
        "user": "yiren98",
        "type": "user"
      },
      "summary": "Utiliser des modèles de diffusion pour générer du texte précis a été une attente à long terme, mais jusqu'à présent, il a été considéré difficile. Les méthodes récentes ont amélioré la visualisation de textes dans la même langue, mais la visualisation de textes dans des langues arbitraires n'a pas encore été explorée. Dans cet article, nous proposons un cadre de visualisation de texte basé sur DiT (Diffusion Transformer), appelé EasyText, qui combine l'élimination du bruit et le codage de tokens de caractères dans plusieurs langues pour effectuer la visualisation de texte de manière précise et contrôlée. De plus, nous proposons un approche pour codifier l'information de position des caractères et une interprétation de l'information de position, avec l'objectif de améliorer la visualisation de texte. Nous avons construit des ensembles de données importants, comprenant un million d'annotations d'images-texte dans plusieurs langues et 20 000 images de haute qualité annotées, pour les utiliser dans l'entraînement préalable et le fine-tuning. Nous avons effectué des expériences et des évaluations détaillées pour montrer la visualisation de texte dans plusieurs langues, la qualité de la visualisation et l'effet de l'intégration du texte dans le routage vers l'avenir.",
      "upvotes": 7,
      "discussionId": "683d0b6f5810d395f0a9a57b",
      "ai_summary": "The paper presents EasyText, a multilingual text rendering framework using DiT that enhances rendering precision and visual quality with large datasets.",
      "ai_keywords": [
        "DiT (Diffusion Transformer)",
        "denoising latents",
        "multilingual character tokens",
        "character positioning encoding",
        "position encoding interpolation",
        "synthetic text image dataset",
        "pretraining",
        "fine-tuning",
        "multilingual text rendering",
        "visual quality",
        "layout-aware text integration"
      ]
    },
    "publishedAt": "2025-05-30T05:55:39.000Z",
    "title": "EasyText: Controllable Diffusion Transformer for Multilingual Text\n  Rendering",
    "summary": "Generating accurate multilingual text with diffusion models has long been\ndesired but remains challenging. Recent methods have made progress in rendering\ntext in a single language, but rendering arbitrary languages is still an\nunexplored area. This paper introduces EasyText, a text rendering framework\nbased on DiT (Diffusion Transformer), which connects denoising latents with\nmultilingual character tokens encoded as character tokens. We propose character\npositioning encoding and position encoding interpolation techniques to achieve\ncontrollable and precise text rendering. Additionally, we construct a\nlarge-scale synthetic text image dataset with 1 million multilingual image-text\nannotations as well as a high-quality dataset of 20K annotated images, which\nare used for pretraining and fine-tuning respectively. Extensive experiments\nand evaluations demonstrate the effectiveness and advancement of our approach\nin multilingual text rendering, visual quality, and layout-aware text\nintegration.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24417.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64311a95034ecbefddd141ef",
      "avatarUrl": "/avatars/b6dc5ca373bedbaa368208517954c375.svg",
      "fullname": "Yiren Song",
      "name": "yiren98",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 17
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.20873",
      "authors": [
        {
          "_id": "68395a548ead63ba096bba45",
          "user": {
            "_id": "6770efb5b673f241332fc4a7",
            "avatarUrl": "/avatars/7aca599492233e6a51c2d6a8f52c644e.svg",
            "isPro": false,
            "fullname": "Chaeyoung Jung",
            "user": "Chae0",
            "type": "user"
          },
          "name": "Chaeyoung Jung",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-02T07:46:20.597Z",
          "hidden": false
        },
        {
          "_id": "68395a548ead63ba096bba46",
          "name": "Youngjoon Jang",
          "hidden": false
        },
        {
          "_id": "68395a548ead63ba096bba47",
          "name": "Jongmin Choi",
          "hidden": false
        },
        {
          "_id": "68395a548ead63ba096bba48",
          "name": "Joon Son Chung",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-27T08:22:56.000Z",
      "submittedOnDailyAt": "2025-06-02T06:33:06.810Z",
      "title": "Décodage du Labyrinthe Porcin : Modèle de Langage Large-échelle pour Améliorer la Compréhension de la Structure Multimodale dans les Données Audio-Visuelles",
      "submittedOnDailyBy": {
        "_id": "6770efb5b673f241332fc4a7",
        "avatarUrl": "/avatars/7aca599492233e6a51c2d6a8f52c644e.svg",
        "isPro": false,
        "fullname": "Chaeyoung Jung",
        "user": "Chae0",
        "type": "user"
      },
      "summary": "L'objectif de cette recherche est d'ajuster le modèle pour éviter qu'il ne se produise un \"bounce\" dans les modèles de langue audio et vidéo (AV-LLMs), améliorant ainsi la compréhension multimodale équilibrée. Dans les actuels AV-LLMs, les caractéristiques audio et vidéo sont généralement traitées ensemble dans le décodificateur. Cette stratégie favorise une compréhension multimodale unifiée, mais elle présente également le risque que le modèle dépende excessivement d'une entraînement déséquilibré, ce qui peut provoquer un \"bounce\" dans le modèle. Pour atténuer cela, on propose une stratégie simple et efficace appelée Fork-Merge Decoding (FMD). FMD traite uniquement l'audio ou uniquement le vidéo dans les premières couches du décodificateur, ajustant ainsi la production du \"bounce\" et améliorant la compréhension multimodale équilibrée dans les AV-LLMs.",
      "upvotes": 6,
      "discussionId": "68395a558ead63ba096bba7b",
      "ai_summary": "The Fork-Merge Decoding strategy improves balanced multimodal understanding in audio-visual large language models by separating and then combining modality-specific reasoning.",
      "ai_keywords": [
        "fork-merge decoding",
        "AU-LLMs",
        "modality bias",
        "audio-visual large language models",
        "VideoLLaMA2",
        "video-SALMONN",
        "benchmark datasets",
        "audio-visual reasoning"
      ]
    },
    "publishedAt": "2025-05-27T04:22:56.000Z",
    "title": "Fork-Merge Decoding: Enhancing Multimodal Understanding in Audio-Visual\n  Large Language Models",
    "summary": "The goal of this work is to enhance balanced multimodal understanding in\naudio-visual large language models (AV-LLMs) by addressing modality bias\nwithout requiring additional training. In current AV-LLMs, audio and video\nfeatures are typically processed jointly in the decoder. While this strategy\nfacilitates unified multimodal understanding, it may introduce modality bias,\nwhere the model tends to over-rely on one modality due to imbalanced training\nsignals. To mitigate this, we propose Fork-Merge Decoding (FMD), a simple yet\neffective inference-time strategy that requires no additional training or\narchitectural modifications. FMD first performs modality-specific reasoning by\nprocessing audio-only and video-only inputs through the early decoder layers (a\nfork phase), and then merges the resulting hidden states for joint reasoning in\nthe remaining layers (a merge phase). This approach promotes balanced modality\ncontributions and leverages complementary information across modalities. We\nevaluate our method on two representative AV-LLMs, VideoLLaMA2 and\nvideo-SALMONN, using three benchmark datasets. Experimental results demonstrate\nconsistent performance improvements on tasks focused on audio, video, and\ncombined audio-visual reasoning, demonstrating the effectiveness of\ninference-time interventions for robust multimodal understanding.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20873.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6770efb5b673f241332fc4a7",
      "avatarUrl": "/avatars/7aca599492233e6a51c2d6a8f52c644e.svg",
      "fullname": "Chaeyoung Jung",
      "name": "Chae0",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.24850",
      "authors": [
        {
          "_id": "683d0ffbe41c42faceda19b2",
          "user": {
            "_id": "6587e5a4b2177de3967ff434",
            "avatarUrl": "/avatars/f2dfbc44eb2bff8d8d66d26db8539708.svg",
            "isPro": false,
            "fullname": "Shuyao Xu",
            "user": "Tim-Xu",
            "type": "user"
          },
          "name": "Shuyao Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-02T07:40:56.229Z",
          "hidden": false
        },
        {
          "_id": "683d0ffbe41c42faceda19b3",
          "name": "Cheng Peng",
          "hidden": false
        },
        {
          "_id": "683d0ffbe41c42faceda19b4",
          "name": "Jiangxuan Long",
          "hidden": false
        },
        {
          "_id": "683d0ffbe41c42faceda19b5",
          "name": "Weidi Xu",
          "hidden": false
        },
        {
          "_id": "683d0ffbe41c42faceda19b6",
          "name": "Wei Chu",
          "hidden": false
        },
        {
          "_id": "683d0ffbe41c42faceda19b7",
          "name": "Yuan Qi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-30T17:47:17.000Z",
      "submittedOnDailyAt": "2025-06-02T02:07:38.924Z",
      "title": "Introduction of negative signals: data on the logical theory of strengthening artificial intelligence in LLM models from professors.",
      "submittedOnDailyBy": {
        "_id": "66e83ec5deb449d8d856e78d",
        "avatarUrl": "/avatars/c5e56be65fcacb3192ce10ba6d8f48e2.svg",
        "isPro": false,
        "fullname": "Tongyan Hu",
        "user": "entropyhu",
        "type": "user"
      },
      "summary": "Récemment, les modèles de haut niveau de logique (par exemple, DeepSeek-R1, o1 de OpenAI) ont démontré que les données obtenues peuvent être transmises efficacement à des petits modèles apprenants avec une grande efficacité. Cependant, les entraînements standard utilisent un rejet d'échantillons qui suppriment des exemples de logique négative, bien qu'ils aient du valeur mais ne soient pas utilisés de manière efficace. Cet article vise à aborder des questions cruciales pour maximiser le rendement logique des modèles de langage grands (LLM). Pour cela, nous proposons un cadre à deux étapes appelé Reinforcement Distillation (REDI). Dans la première étape, nous entraînons avec Supervised Fine-Tuning (SFT) pour apprendre des traces positives. Dans la deuxième étape, nous utilisons une fonction d'objectif proposée par nous, REDI, pour améliorer le modèle en utilisant à la fois des traces positives et négatives. Cette nouvelle fonction d'objectif est plus simple que DPO ou SimPO et s'adapte directement à la fonction de perte sans nécessiter de références. Notre évaluation expérimentale montre que REDI montre un excellent rendement dans des tests mathématiques logiques, dépassant les entraînements de SFT avec rejet d'échantillons ou des combinaisons de SFT avec DPO/SimPO. Spécifiquement, le modèle Qwen-REDI-1.5B, après avoir été amélioré avec 131k exemples positifs et négatifs dans le jeu de données Open-R1, a atteint un 83.1% sur MATH-500 (pass@1). Ce rendement est comparable au de DeepSeek-R1-Distill-Qwen-1.5B (modèle amélioré avec 800k données propriétaires) et améliore ou atteint le rendement des modèles améliorés avec des données publiques, devenant le nouveau leader des modèles de 1.5B avec des données publiques.",
      "upvotes": 5,
      "discussionId": "683d0ffce41c42faceda19da",
      "githubRepo": "https://github.com/Tim-Siu/reinforcement-distillation",
      "ai_summary": "Reinforcement Distillation (REDI) leverages both positive and negative traces to enhance large language model reasoning performance offline, outperforming traditional methods and achieving state-of-the-art results with limited open data.",
      "ai_keywords": [
        "model distillation",
        "DeepSeek-R1",
        "OpenAI's o1",
        "Reinforcement Distillation (REDI)",
        "Supervised Fine-Tuning (SFT)",
        "REDI objective",
        "DPO",
        "SimPO",
        "mathematical reasoning tasks",
        "MATH-500",
        "Qwen-REDI-1.5B",
        "DeepSeek-R1-Distill-Qwen-1.5B"
      ]
    },
    "publishedAt": "2025-05-30T13:47:17.000Z",
    "title": "Harnessing Negative Signals: Reinforcement Distillation from Teacher\n  Data for LLM Reasoning",
    "summary": "Recent advances in model distillation demonstrate that data from advanced\nreasoning models (e.g., DeepSeek-R1, OpenAI's o1) can effectively transfer\ncomplex reasoning abilities to smaller, efficient student models. However,\nstandard practices employ rejection sampling, discarding incorrect reasoning\nexamples -- valuable, yet often underutilized data. This paper addresses the\ncritical question: How can both positive and negative distilled reasoning\ntraces be effectively leveraged to maximize LLM reasoning performance in an\noffline setting? To this end, We propose Reinforcement Distillation (REDI), a\ntwo-stage framework. Stage 1 learns from positive traces via Supervised\nFine-Tuning (SFT). Stage 2 further refines the model using both positive and\nnegative traces through our proposed REDI objective. This novel objective is a\nsimple, reference-free loss function that outperforms established methods like\nDPO and SimPO in this distillation context. Our empirical evaluations\ndemonstrate REDI's superiority over baseline Rejection Sampling SFT or SFT\ncombined with DPO/SimPO on mathematical reasoning tasks. Notably, the\nQwen-REDI-1.5B model, post-trained on just 131k positive and negative examples\nfrom the open Open-R1 dataset, achieves an 83.1% score on MATH-500 (pass@1).\nIts performance matches or surpasses that of DeepSeek-R1-Distill-Qwen-1.5B (a\nmodel post-trained on 800k proprietary data) across various mathematical\nreasoning benchmarks, establishing a new state-of-the-art for 1.5B models\npost-trained offline with openly available data.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24850.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "66e83ec5deb449d8d856e78d",
      "avatarUrl": "/avatars/c5e56be65fcacb3192ce10ba6d8f48e2.svg",
      "fullname": "Tongyan Hu",
      "name": "entropyhu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.24293",
      "authors": [
        {
          "_id": "683d01ec446fd0c8ff323010",
          "user": {
            "_id": "6658f863ce1b283888625af3",
            "avatarUrl": "/avatars/e4b2c7df0f398eb68c0566031ceac99e.svg",
            "isPro": false,
            "fullname": "James Golden",
            "user": "jamesgolden1",
            "type": "user"
          },
          "name": "James R. Golden",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-06-02T02:38:10.635Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6658f863ce1b283888625af3/uulwAnV1EYXXSdsC-eDMW.png"
      ],
      "publishedAt": "2025-05-30T07:08:33.000Z",
      "submittedOnDailyAt": "2025-06-02T01:02:58.980Z",
      "title": "Les modèles de langage globaux sont des projections linéaires locales.",
      "submittedOnDailyBy": {
        "_id": "6658f863ce1b283888625af3",
        "avatarUrl": "/avatars/e4b2c7df0f398eb68c0566031ceac99e.svg",
        "isPro": false,
        "fullname": "James Golden",
        "user": "jamesgolden1",
        "type": "user"
      },
      "summary": "Nous montrons que nous pouvons cartographier l'inférence d'opérations importantes de modèles de langage de haut rendement (LLM) avec des poids publics sans modifier les séquences d'entrée ni changer les poids du modèle ou les prédictions de sortie. Nous avons étendu les techniques qui montrent la linéarité locale ou partielle de modèles de diffusion d'images pour calculer de manière stratégique le gradient pour la prédiction du token suivant dans une séquence d'entrée donnée, de sorte que le Jacobian du modèle soit propagé presque exactement à un système linéaire. Cette méthodologie a été appliquée à plusieurs modèles (Llama 3, Gemma 3, Qwen 3, Phi 4, Mistral Ministral, OLMo 2, et le plus grand Llama 3.3 70B Q4), et a été confirmée par la décomposition en valeurs singulières (SVD) que ces LLMs fonctionnent dans un espace de très faible dimension. Dans cet espace, de nombreux vecteurs singuliers plus grands codifient des concepts liés aux tokens de sortie les plus probables. Cette approche permet de voir presque exactement comment fonctionne chaque couche continue (et ses composants d'attention et de MLP internes) et d'observer l'apparition de concepts contextuels. Bien que les moderns LLMs aient une grande expressivité et une non-linéarité globale, ils peuvent être interprétés comme des approximations presque exactes d'un système linéaire local, révélant des structures contextuelles compréhensibles dans le processus de prédiction du token suivant et fournissant une vision interne des représentations du modèle.",
      "upvotes": 3,
      "discussionId": "683d01ee446fd0c8ff323087",
      "githubRepo": "https://github.com/jamesgolden1/llms-are-llms/",
      "ai_summary": "LLMs can be approximated as linear systems for inference, offering insights into their internal representations and semantic structures without altering the models or their predictions.",
      "ai_keywords": [
        "large language models (LLMs)",
        "inference operations",
        "linear system",
        "gradient computation",
        "Jacobian",
        "singular value decomposition",
        "low-dimensional subspaces",
        "semantic concepts",
        "attention components",
        "MLP components",
        "locally linear decompositions"
      ]
    },
    "publishedAt": "2025-05-30T03:08:33.000Z",
    "title": "Large Language Models are Locally Linear Mappings",
    "summary": "We demonstrate that the inference operations of several open-weight large\nlanguage models (LLMs) can be mapped to an exactly equivalent linear system for\nan input sequence without modifying the model weights or altering output\npredictions. Extending techniques from image diffusion models that exhibit\nlocal or piecewise linearity, we strategically alter the gradient computation\nwith respect to a given input sequence for a next-token prediction such that\nthe Jacobian of the model nearly exactly reproduces the forward prediction with\na linear system. We demonstrate this approach across models (Llama 3, Gemma 3,\nQwen 3, Phi 4, Mistral Ministral and OLMo 2, up to Llama 3.3 70B Q4) and show\nthrough the singular value decomposition of the detached Jacobian that these\nLLMs operate in extremely low-dimensional subspaces where many of the largest\nsingular vectors decode to concepts related to the most-likely output token.\nThis approach also allows us to examine the operation of each successive layer\n(and its attention and MLP components) as nearly-exact linear systems and\nobserve the emergence of semantic concepts. Despite their expressive power and\nglobal nonlinearity, modern LLMs can be interpreted through nearly-exact\nlocally linear decompositions that provide insights into their internal\nrepresentations and reveal interpretable semantic structures in the next-token\nprediction process.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6658f863ce1b283888625af3/uulwAnV1EYXXSdsC-eDMW.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24293.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6658f863ce1b283888625af3",
      "avatarUrl": "/avatars/e4b2c7df0f398eb68c0566031ceac99e.svg",
      "fullname": "James Golden",
      "name": "jamesgolden1",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.23926",
      "authors": [
        {
          "_id": "683d33be277ad05e5a672f79",
          "name": "Xuweiyi Chen",
          "hidden": false
        },
        {
          "_id": "683d33be277ad05e5a672f7a",
          "name": "Wentao Zhou",
          "hidden": false
        },
        {
          "_id": "683d33be277ad05e5a672f7b",
          "name": "Aruni RoyChowdhury",
          "hidden": false
        },
        {
          "_id": "683d33be277ad05e5a672f7c",
          "name": "Zezhou Cheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T18:21:47.000Z",
      "submittedOnDailyAt": "2025-06-02T03:49:08.677Z",
      "title": "Point-MoE : Mélange d'Experts pour la Généralisation Croisée de la Segmentation Sémantique 3D",
      "submittedOnDailyBy": {
        "_id": "634632aaac1cb29fb2ac9f14",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/634632aaac1cb29fb2ac9f14/nGZ2TzKOOcKMAR_NFYKkR.jpeg",
        "isPro": false,
        "fullname": "Xuweiyi Chen",
        "user": "Xuweiyi",
        "type": "user"
      },
      "summary": "Le texte explique pourquoi l'compréhension des clusters de points n'a pas atteint le niveau des escalateurs et propose des solutions. Voici la traduction en français du texte original :\n\nLes escalateurs ont été affectés par le traitement du langage naturel et la vision par ordinateur, mais l'compréhension des clusters de points n'a pas encore atteint ce niveau. Cela est dû à l'absence relative de la échelle des ensembles de données 3D et à la diversité de leurs sources. Les clusters de points sont capturés dans différentes zones, de l'intérieur à l'extérieur, en utilisant différents capteurs (par exemple : caméras de profondeur, Lidar). Chaque capteur introduit des patrons de balayage uniques, des densités d'échantillonnage et des biais sémantiques. Ces différences entre zones sont des obstacles cruciaux pour l'entraînement de modèles uniformes à grande échelle. En particulier, ce problème est particulièrement pertinent en raison des restrictions réelles qui empêchent l'étiquetage des zones lors du processus d'inférence. Dans cet article, on propose une architecture Mixture-of-Experts qui permet une généralisation à grande échelle et entre zones. Point-MoE, qui utilise des données de clusters de points comme base, est considérablement affecté dans son rendement. Cependant, Point-MoE peut spécialiser automatiquement ses experts même si les données étiquetées ne sont pas disponibles, en utilisant une stratégie simple de routage top-k. Les expérimentations montrent que Point-MoE dépasse les clusters de points standard et a un bon rendement en généralisation vers des zones non vues précédemment. Cette recherche montre un chemin échellable pour le compréhension 3D. Le modèle privilégie le découverte automatique de la structure de différents données 3D et remplace l'édition automatique ou la sous-offre de zones.",
      "upvotes": 3,
      "discussionId": "683d33c4277ad05e5a67310e",
      "ai_summary": "Point-MoE, a Mixture-of-Experts architecture, enables large-scale, cross-domain generalization in 3D perception by automatically specializing experts without domain labels.",
      "ai_keywords": [
        "Mixture-of-Experts",
        "Point-MoE",
        "point cloud backbones",
        "3D perception",
        "domain heterogeneity",
        "domain labels",
        "top-k routing",
        "multi-domain baselines"
      ]
    },
    "publishedAt": "2025-05-29T14:21:47.000Z",
    "title": "Point-MoE: Towards Cross-Domain Generalization in 3D Semantic\n  Segmentation via Mixture-of-Experts",
    "summary": "While scaling laws have transformed natural language processing and computer\nvision, 3D point cloud understanding has yet to reach that stage. This can be\nattributed to both the comparatively smaller scale of 3D datasets, as well as\nthe disparate sources of the data itself. Point clouds are captured by diverse\nsensors (e.g., depth cameras, LiDAR) across varied domains (e.g., indoor,\noutdoor), each introducing unique scanning patterns, sampling densities, and\nsemantic biases. Such domain heterogeneity poses a major barrier towards\ntraining unified models at scale, especially under the realistic constraint\nthat domain labels are typically inaccessible at inference time. In this work,\nwe propose Point-MoE, a Mixture-of-Experts architecture designed to enable\nlarge-scale, cross-domain generalization in 3D perception. We show that\nstandard point cloud backbones degrade significantly in performance when\ntrained on mixed-domain data, whereas Point-MoE with a simple top-k routing\nstrategy can automatically specialize experts, even without access to domain\nlabels. Our experiments demonstrate that Point-MoE not only outperforms strong\nmulti-domain baselines but also generalizes better to unseen domains. This work\nhighlights a scalable path forward for 3D understanding: letting the model\ndiscover structure in diverse 3D data, rather than imposing it via manual\ncuration or domain supervision.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23926.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "634632aaac1cb29fb2ac9f14",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/634632aaac1cb29fb2ac9f14/nGZ2TzKOOcKMAR_NFYKkR.jpeg",
      "fullname": "Xuweiyi Chen",
      "name": "Xuweiyi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.24615",
      "authors": [
        {
          "_id": "683d4295c31058e5bf2e2b0b",
          "name": "Yan Liu",
          "hidden": false
        },
        {
          "_id": "683d4295c31058e5bf2e2b0c",
          "user": {
            "_id": "646a11791556443f24b582e9",
            "avatarUrl": "/avatars/b54d416ddca2f124492ba51bc4b95fd2.svg",
            "isPro": false,
            "fullname": "Zonglin Yang",
            "user": "ZonglinY",
            "type": "user"
          },
          "name": "Zonglin Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-02T07:40:15.714Z",
          "hidden": false
        },
        {
          "_id": "683d4295c31058e5bf2e2b0d",
          "name": "Soujanya Poria",
          "hidden": false
        },
        {
          "_id": "683d4295c31058e5bf2e2b0e",
          "name": "Thanh-Son Nguyen",
          "hidden": false
        },
        {
          "_id": "683d4295c31058e5bf2e2b0f",
          "name": "Erik Cambria",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-30T14:08:13.000Z",
      "submittedOnDailyAt": "2025-06-02T04:51:23.329Z",
      "title": "Utiliser le langage naturel pour explorer un nouvel élément de la science",
      "submittedOnDailyBy": {
        "_id": "646a11791556443f24b582e9",
        "avatarUrl": "/avatars/b54d416ddca2f124492ba51bc4b95fd2.svg",
        "isPro": false,
        "fullname": "Zonglin Yang",
        "user": "ZonglinY",
        "type": "user"
      },
      "summary": "Dans l'ère de la croissance de la science, la spécificité des nouvelles idées de recherche est importante mais peut également être difficile dans l'environnement académique. Malgré les possibilités potentielles, la manque de jeux de données de référence appropriés empêche l'avancement dans la recherche de nouvelles fonctions. Le plus important est que, l'utilisation simple des technologies actuelles de Traitement du Langage Naturel (NLP), comme dans les recherches et vérifications, ne peuvent pas être appliquées de manière cohérente avec la similitude des phrases et la conceptualisation des idées. Dans cet article, on propose l'utilisation de modèles de langage grands (LLMs) pour la détection de nouvelles fonctions (ND) en science, et on prépare deux nouveaux jeux de données liés au marché et au domaine du NLP. Pour construire des jeux de données appropriés pour la ND, on extrait des jeux fermés selon les références de l'article et on propose le résumé des idées principales à travers les LLMs. Pour conceptualiser les idées, on propose que les LLMs soient utilisés pour obtenir avec précision le savoir au niveau des idées et qu'ils soient ajustés à des concepts similaires, et on propose d'entraîner des modèles de recherche légers pour une recherche efficace et précise des idées. Les expérimentations montrent que les méthodes proposées présentent un rendement cohérent dans la recherche d'idées et dans la ND, comparé à d'autres méthodes. Les codes et les données sont disponibles sur https://anonymous.4open.science/r/NoveltyDetection-10FB/.",
      "upvotes": 2,
      "discussionId": "683d4296c31058e5bf2e2b63",
      "ai_summary": "A method utilizing large language models to detect scientific novelty by distilling idea-level knowledge and constructing specialized datasets in marketing and NLP domains.",
      "ai_keywords": [
        "large language models",
        "scientific novelty detection",
        "closure sets",
        "idea retrieval",
        "idea conception",
        "lightweight retriever",
        "knowledge distillation"
      ]
    },
    "publishedAt": "2025-05-30T10:08:13.000Z",
    "title": "Harnessing Large Language Models for Scientific Novelty Detection",
    "summary": "In an era of exponential scientific growth, identifying novel research ideas\nis crucial and challenging in academia. Despite potential, the lack of an\nappropriate benchmark dataset hinders the research of novelty detection. More\nimportantly, simply adopting existing NLP technologies, e.g., retrieving and\nthen cross-checking, is not a one-size-fits-all solution due to the gap between\ntextual similarity and idea conception. In this paper, we propose to harness\nlarge language models (LLMs) for scientific novelty detection (ND), associated\nwith two new datasets in marketing and NLP domains. To construct the\nconsiderate datasets for ND, we propose to extract closure sets of papers based\non their relationship, and then summarize their main ideas based on LLMs. To\ncapture idea conception, we propose to train a lightweight retriever by\ndistilling the idea-level knowledge from LLMs to align ideas with similar\nconception, enabling efficient and accurate idea retrieval for LLM novelty\ndetection. Experiments show our method consistently outperforms others on the\nproposed benchmark datasets for idea retrieval and ND tasks. Codes and data are\navailable at https://anonymous.4open.science/r/NoveltyDetection-10FB/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24615.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "646a11791556443f24b582e9",
      "avatarUrl": "/avatars/b54d416ddca2f124492ba51bc4b95fd2.svg",
      "fullname": "Zonglin Yang",
      "name": "ZonglinY",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.24517",
      "authors": [
        {
          "_id": "683d3f3100c71614babecb8c",
          "user": {
            "_id": "64395702bb7ded0a0fee8889",
            "avatarUrl": "/avatars/afe8f9b6de358497b0db8a03f8a3a704.svg",
            "isPro": false,
            "fullname": "Yinqi Li",
            "user": "yinqi",
            "type": "user"
          },
          "name": "Yinqi Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-02T07:40:19.037Z",
          "hidden": false
        },
        {
          "_id": "683d3f3100c71614babecb8d",
          "name": "Jiahe Zhao",
          "hidden": false
        },
        {
          "_id": "683d3f3100c71614babecb8e",
          "name": "Hong Chang",
          "hidden": false
        },
        {
          "_id": "683d3f3100c71614babecb8f",
          "name": "Ruibing Hou",
          "hidden": false
        },
        {
          "_id": "683d3f3100c71614babecb90",
          "name": "Shiguang Shan",
          "hidden": false
        },
        {
          "_id": "683d3f3100c71614babecb91",
          "name": "Xilin Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-30T12:29:38.000Z",
      "submittedOnDailyAt": "2025-06-02T04:55:28.985Z",
      "title": "un^2CLIP: CLIP: Un méthode de rétrocès pour mieux comprendre les détails visuels de CLIP",
      "submittedOnDailyBy": {
        "_id": "64395702bb7ded0a0fee8889",
        "avatarUrl": "/avatars/afe8f9b6de358497b0db8a03f8a3a704.svg",
        "isPro": false,
        "fullname": "Yinqi Li",
        "user": "yinqi",
        "type": "user"
      },
      "summary": "Contrastive Language-Image Pre-training (CLIP) est utilisé comme modèle de base et des applications diverses sont appliquées aux tâches visuelles et variées. Cependant, des études récentes montrent que CLIP a des limitations dans sa capacité à différencier des différences détaillées dans les images et ne montre pas le meilleur rendement dans les tâches de prédiction dense et visuelle. Par conséquent, cet article vise à améliorer le modèle CLIP et à capturer les informations visuelles détaillées possibles dans les images. Nous avons découvert que un modèle génératif spécifique appelé unCLIP fournit un cadre approprié pour atteindre cet objectif. En particulier, unCLIP entraîne un générateur d'images basé sur les ambassadeurs de CLIP. C'est-à-dire, nous inversons l'encodeur de CLIP. Comparé à CLIP comme modèle discriminant, le modèle génératif montre un meilleur rendement dans la capture d'informations détaillées des images. De plus, l'espace d'entrée conditionnel de unCLIP coïncide avec l'espace original des ambassadeurs de CLIP (images-texte). Par conséquent, nous proposons d'inverser unCLIP (appelé un^2CLIP) pour améliorer le modèle CLIP. Cette amélioration de l'encodeur d'images permet de capturer des informations visuelles détaillées tout en maintenant la coïncidence avec l'encodeur de texte original. Nous évaluons le CLIP amélioré dans diverses tâches où il est appliqué, en particulier dans le benchmark difficile MMVP-VLM, la tâche de segmentation des étiquettes de boîtes ouvertes pour la prédiction dense, et diverses tâches de modèle contre le langage. Les expériences montrent que un^2CLIP améliore significativement CLIP original et les autres méthodes d'amélioration précédentes. Les codes et modèles sont disponibles sur https://github.com/LiYinqi/un2CLIP.",
      "upvotes": 2,
      "discussionId": "683d3f3200c71614babecbe3",
      "githubRepo": "https://github.com/LiYinqi/un2CLIP",
      "ai_summary": "A generative model framework, unCLIP, is inverted to improve CLIP's ability to capture detailed visual information while maintaining text alignment.",
      "ai_keywords": [
        "Contrastive Language-Image Pre-training",
        "CLIP",
        "unCLIP",
        "image generator",
        "image encoding",
        "data distribution",
        "dense-prediction",
        "vision-centric",
        "multimodal",
        "open-vocabulary segmentation",
        "multimodal large language model",
        "MMVP-VLM benchmark"
      ]
    },
    "publishedAt": "2025-05-30T08:29:38.000Z",
    "title": "un^2CLIP: Improving CLIP's Visual Detail Capturing Ability via\n  Inverting unCLIP",
    "summary": "Contrastive Language-Image Pre-training (CLIP) has become a foundation model\nand has been applied to various vision and multimodal tasks. However, recent\nworks indicate that CLIP falls short in distinguishing detailed differences in\nimages and shows suboptimal performance on dense-prediction and vision-centric\nmultimodal tasks. Therefore, this work focuses on improving existing CLIP\nmodels, aiming to capture as many visual details in images as possible. We find\nthat a specific type of generative models, unCLIP, provides a suitable\nframework for achieving our goal. Specifically, unCLIP trains an image\ngenerator conditioned on the CLIP image embedding. In other words, it inverts\nthe CLIP image encoder. Compared to discriminative models like CLIP, generative\nmodels are better at capturing image details because they are trained to learn\nthe data distribution of images. Additionally, the conditional input space of\nunCLIP aligns with CLIP's original image-text embedding space. Therefore, we\npropose to invert unCLIP (dubbed un^2CLIP) to improve the CLIP model. In this\nway, the improved image encoder can gain unCLIP's visual detail capturing\nability while preserving its alignment with the original text encoder\nsimultaneously. We evaluate our improved CLIP across various tasks to which\nCLIP has been applied, including the challenging MMVP-VLM benchmark, the\ndense-prediction open-vocabulary segmentation task, and multimodal large\nlanguage model tasks. Experiments show that un^2CLIP significantly improves\nthe original CLIP and previous CLIP improvement methods. Code and models will\nbe available at https://github.com/LiYinqi/un2CLIP.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24517.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64395702bb7ded0a0fee8889",
      "avatarUrl": "/avatars/afe8f9b6de358497b0db8a03f8a3a704.svg",
      "fullname": "Yinqi Li",
      "name": "yinqi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.23009",
      "authors": [
        {
          "_id": "683916c60df60182c0dee89d",
          "user": {
            "_id": "66958c29d4ca2767b9c41005",
            "avatarUrl": "/avatars/c81b65c1ad345c48c252773ea78b7607.svg",
            "isPro": true,
            "fullname": "Ruskin Raj Manku",
            "user": "ruskinmanku",
            "type": "user"
          },
          "name": "Ruskin Raj Manku",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-02T07:47:12.390Z",
          "hidden": false
        },
        {
          "_id": "683916c60df60182c0dee89e",
          "name": "Yuzhi Tang",
          "hidden": false
        },
        {
          "_id": "683916c60df60182c0dee89f",
          "name": "Xingjian Shi",
          "hidden": false
        },
        {
          "_id": "683916c60df60182c0dee8a0",
          "name": "Mu Li",
          "hidden": false
        },
        {
          "_id": "683916c60df60182c0dee8a1",
          "name": "Alex Smola",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T02:36:24.000Z",
      "submittedOnDailyAt": "2025-06-02T01:24:21.995Z",
      "title": "EmergentTTS-Eval : L'évaluation des modèles TTS en face de problèmes complexes de prononciation, d'expression et de langage est réalisée en utilisant le Modèle-en-Juge.",
      "submittedOnDailyBy": {
        "_id": "66958c29d4ca2767b9c41005",
        "avatarUrl": "/avatars/c81b65c1ad345c48c252773ea78b7607.svg",
        "isPro": true,
        "fullname": "Ruskin Raj Manku",
        "user": "ruskinmanku",
        "type": "user"
      },
      "summary": "Text-to-Speech (TTS) benchmarks incluyen varios desafíos pour les modèles qui traiteront des textes plus complexes de manière plus efficace. On présente EmergentTTS-Eval, un cadre basé sur EmergentTTS. Ce cadre fournit six scénarios TTS difficiles qui incluent des émotions, du parallélisme, des langues étrangères, une complexité syntaxique, des prononciations complexes (comme les URLs et les formules) et des questions. Une des caractéristiques importantes est qu'il automatise la génération de cas de test et l'évaluation, facilitant l'extension du benchmark. Initialement, on utilise de petits ensembles de prompts écrits par humains pour élargir progressivement, en utilisant des modèles de langage grands (LLMs), des cas de test qui présentent des défis structurels, acoustiques et prononciationnels. Cela génère 1,645 cas de test divers. De plus, les modèles utilisent la structure d'évaluation pour évaluer l'émotion, la prononciation, le ton et la précision de la prononciation avec un modèle de langage grand d'audio (LALM). EmergentTTS-Eval évalue les systèmes TTS ouverts et propriétaires (comme 11Labs, Deepgram et OpenAI's 4o-mini-TTS) et montre leurs différences de performance. Les résultats montrent que le méthode utilisant la structure d'évaluation fournit une évaluation robuste de TTS et montre une forte corrélation avec les préférences humaines. Les codes d'évaluation et les ensembles de données de EmergentTTS-Eval sont disponibles sur https://github.com/boson-ai/EmergentTTS-Eval-public{code} et https://huggingface.co/datasets/bosonai/EmergentTTS-Eval{dataset}.",
      "upvotes": 2,
      "discussionId": "683916c70df60182c0dee8dc",
      "ai_summary": "A comprehensive TTS benchmark, EmergentTTS-Eval, automates test-case generation and evaluation using LLMs and LALM to assess nuanced and semantically complex text in speech outputs.",
      "ai_keywords": [
        "EmergentTTS-Eval",
        "LLMs",
        "Large Audio Language Model (LALM)",
        "expressed emotion",
        "prosodic",
        "intonational",
        "pronunciation accuracy",
        "TTS systems",
        "model-as-a-judge"
      ]
    },
    "publishedAt": "2025-05-28T22:36:24.000Z",
    "title": "EmergentTTS-Eval: Evaluating TTS Models on Complex Prosodic,\n  Expressiveness, and Linguistic Challenges Using Model-as-a-Judge",
    "summary": "Text-to-Speech (TTS) benchmarks often fail to capture how well models handle\nnuanced and semantically complex text. Building on EmergentTTS, we\nintroduce EmergentTTS-Eval, a comprehensive benchmark covering six\nchallenging TTS scenarios: emotions, paralinguistics, foreign words, syntactic\ncomplexity, complex pronunciation (e.g. URLs, formulas), and questions.\nCrucially, our framework automates both test-case generation and evaluation,\nmaking the benchmark easily extensible. Starting from a small set of\nhuman-written seed prompts, we iteratively extend them using LLMs to target\nspecific structural, phonetic and prosodic challenges, resulting in 1,645\ndiverse test cases. Moreover, we employ a model-as-a-judge approach, using a\nLarge Audio Language Model (LALM) to assess the speech across multiple\ndimensions such as expressed emotion, prosodic, intonational, and pronunciation\naccuracy. We evaluate state-of-the-art open-source and proprietary TTS systems,\nsuch as 11Labs, Deepgram, and OpenAI's 4o-mini-TTS, on EmergentTTS-Eval,\ndemonstrating its ability to reveal fine-grained performance differences.\nResults show that the model-as-a-judge approach offers robust TTS assessment\nand a high correlation with human preferences. We open source the evaluation\nhttps://github.com/boson-ai/EmergentTTS-Eval-public{code} and the\nhttps://huggingface.co/datasets/bosonai/EmergentTTS-Eval{dataset}.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23009.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66958c29d4ca2767b9c41005",
      "avatarUrl": "/avatars/c81b65c1ad345c48c252773ea78b7607.svg",
      "fullname": "Ruskin Raj Manku",
      "name": "ruskinmanku",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.23844",
      "authors": [
        {
          "_id": "683d0ac47852d920b7dc3dc5",
          "name": "Zhenglun Kong",
          "hidden": false
        },
        {
          "_id": "683d0ac47852d920b7dc3dc6",
          "name": "Zheng Zhan",
          "hidden": false
        },
        {
          "_id": "683d0ac47852d920b7dc3dc7",
          "name": "Shiyue Hou",
          "hidden": false
        },
        {
          "_id": "683d0ac47852d920b7dc3dc8",
          "name": "Yifan Gong",
          "hidden": false
        },
        {
          "_id": "683d0ac47852d920b7dc3dc9",
          "name": "Xin Meng",
          "hidden": false
        },
        {
          "_id": "683d0ac47852d920b7dc3dca",
          "name": "Pengwei Sui",
          "hidden": false
        },
        {
          "_id": "683d0ac47852d920b7dc3dcb",
          "name": "Peiyan Dong",
          "hidden": false
        },
        {
          "_id": "683d0ac47852d920b7dc3dcc",
          "name": "Xuan Shen",
          "hidden": false
        },
        {
          "_id": "683d0ac47852d920b7dc3dcd",
          "name": "Zifeng Wang",
          "hidden": false
        },
        {
          "_id": "683d0ac47852d920b7dc3dce",
          "name": "Pu Zhao",
          "hidden": false
        },
        {
          "_id": "683d0ac47852d920b7dc3dcf",
          "name": "Hao Tang",
          "hidden": false
        },
        {
          "_id": "683d0ac47852d920b7dc3dd0",
          "name": "Stratis Ioannidis",
          "hidden": false
        },
        {
          "_id": "683d0ac47852d920b7dc3dd1",
          "name": "Yanzhi Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T16:24:50.000Z",
      "submittedOnDailyAt": "2025-06-02T00:52:55.266Z",
      "title": "Implémenter le cadre de flexibilité pour faciliter la concentration des connaissances échelonnables.",
      "submittedOnDailyBy": {
        "_id": "5f2c36551ebc8c6ede2f0e53",
        "avatarUrl": "/avatars/e3ddbd15f50b86958377b5fc2460a57e.svg",
        "isPro": false,
        "fullname": "Tony Kong",
        "user": "TonyK",
        "type": "user"
      },
      "summary": "Les modèles de langue grand (LLMs) montrent un grand potentiel mais présentent des défis pour améliorer de manière continue, surtout lorsqu'il s'agit d'intégrer des fonctions de d'autres LLMs spécifiques. Les méthodes populaires comme l'intégration sur serveur et l'intégration de poids nécessitent beaucoup de mémoire et ont des difficultés à s'adapter aux environnements de données variables. Récemment, des efforts ont été faits pour collecter le savoir de plusieurs LLMs et le déplacer vers un seul modèle de langue. Cependant, on a observé des interférences de connaissance et une perte de fonctionnalités, ce qui est principalement attribué à la sélection des candidats et à la flexibilité limitée du flux d'apprentissage. Pour aborder ces problèmes, nous proposons un cadre qui sélectionne et concentre le savoir de différents LLMs pour construire un seul modèle robuste. Ce cadre évite l'importante surcharge mémoire de l'intégration sur serveur et de poids, et cherche à mitiger la flexibilité limitée de la sélection. En particulier, nous concevons une réseau de sélection adaptative qui identifie les LLMs sources les plus pertinents et réduit les interférences de connaissance. De plus, nous proposons une stratégie dynamique d'intégration de poids qui considère les forces uniques des LLMs candidats et une fonction de perte rétroactionnelle qui empêche le sélectionneur de se concentrer sur un seul ensemble de sources. Les résultats des expériences montrent que notre méthode réduit les interférences de connaissance de 50% et permet un processus d'intégration de connaissance stable et scalable. Le code est disponible sur : https://github.com/ZLKong/LLM_Integration.",
      "upvotes": 2,
      "discussionId": "683d0ac57852d920b7dc3e20",
      "projectPage": "https://github.com/ZLKong/LLM_Integration/tree/main",
      "githubRepo": "https://github.com/ZLKong/LLM_Integration/tree/main",
      "ai_summary": "A framework for adaptive selection and dynamic weighted fusion of knowledge from multiple LLMs reduces interference and improves scalability in knowledge aggregation.",
      "ai_keywords": [
        "large language models",
        "fine-tuning",
        "ensemble",
        "weight merging",
        "adaptive selection network",
        "dynamic weighted fusion",
        "feedback-driven loss function",
        "knowledge interference"
      ]
    },
    "publishedAt": "2025-05-28T12:24:50.000Z",
    "title": "Enabling Flexible Multi-LLM Integration for Scalable Knowledge\n  Aggregation",
    "summary": "Large language models (LLMs) have shown remarkable promise but remain\nchallenging to continually improve through traditional finetuning, particularly\nwhen integrating capabilities from other specialized LLMs. Popular methods like\nensemble and weight merging require substantial memory and struggle to adapt to\nchanging data environments. Recent efforts have transferred knowledge from\nmultiple LLMs into a single target model; however, they suffer from\ninterference and degraded performance among tasks, largely due to limited\nflexibility in candidate selection and training pipelines. To address these\nissues, we propose a framework that adaptively selects and aggregates knowledge\nfrom diverse LLMs to build a single, stronger model, avoiding the high memory\noverhead of ensemble and inflexible weight merging. Specifically, we design an\nadaptive selection network that identifies the most relevant source LLMs based\non their scores, thereby reducing knowledge interference. We further propose a\ndynamic weighted fusion strategy that accounts for the inherent strengths of\ncandidate LLMs, along with a feedback-driven loss function that prevents the\nselector from converging on a single subset of sources. Experimental results\ndemonstrate that our method can enable a more stable and scalable knowledge\naggregation process while reducing knowledge interference by up to 50% compared\nto existing approaches. Code is avaliable at\nhttps://github.com/ZLKong/LLM_Integration",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23844.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5f2c36551ebc8c6ede2f0e53",
      "avatarUrl": "/avatars/e3ddbd15f50b86958377b5fc2460a57e.svg",
      "fullname": "Tony Kong",
      "name": "TonyK",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.21864",
      "authors": [
        {
          "_id": "683b8af5091615f46fabadde",
          "user": {
            "_id": "655a50a850b9a14799165d53",
            "avatarUrl": "/avatars/6bb37ac0b6840771ff7a6a6da4192ace.svg",
            "isPro": false,
            "fullname": "Mengda Xu",
            "user": "mengdaxu",
            "type": "user"
          },
          "name": "Mengda Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-02T07:42:10.223Z",
          "hidden": false
        },
        {
          "_id": "683b8af5091615f46fabaddf",
          "name": "Han Zhang",
          "hidden": false
        },
        {
          "_id": "683b8af5091615f46fabade0",
          "name": "Yifan Hou",
          "hidden": false
        },
        {
          "_id": "683b8af5091615f46fabade1",
          "name": "Zhenjia Xu",
          "hidden": false
        },
        {
          "_id": "683b8af5091615f46fabade2",
          "name": "Linxi Fan",
          "hidden": false
        },
        {
          "_id": "683b8af5091615f46fabade3",
          "name": "Manuela Veloso",
          "hidden": false
        },
        {
          "_id": "683b8af5091615f46fabade4",
          "name": "Shuran Song",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/655a50a850b9a14799165d53/IaKGx3B7I2pvk3nOzX9s2.mp4"
      ],
      "publishedAt": "2025-05-28T01:25:27.000Z",
      "submittedOnDailyAt": "2025-06-02T06:21:16.447Z",
      "title": "DexUMI : Manipulation des mains humaines pour une interface de manipulation généralisée",
      "submittedOnDailyBy": {
        "_id": "655a50a850b9a14799165d53",
        "avatarUrl": "/avatars/6bb37ac0b6840771ff7a6a6da4192ace.svg",
        "isPro": false,
        "fullname": "Mengda Xu",
        "user": "mengdaxu",
        "type": "user"
      },
      "summary": "DexUMI est présenté. Il s'agit d'un cadre de travail pour la collecte de données et l'apprentissage de politiques qui permet à des mains robotiques différentes d'apprendre des compétences de mouvement en détail grâce à une interface naturelle des mains humaines. DexUMI minimise les différences physiques entre les mains humaines et celles de différents robots grâce à des ajustements en matière de matériel et logiciel. Les ajustements en matière de matériel comprennent l'utilisation d'actuateurs scalables pour réduire les différences de mobilité, ce qui permet de fournir une rétroaction directe lors de la collecte de données de mouvement, facilitant ainsi l'application de mouvements humains aux mains de robot. Les ajustements en logiciel réduisent les différences visuelles en transformant les données d'image pour que les mains humaines soient remplacées par des mains de robot de haute qualité en utilisant des réseaux de neurones artificiels. Pour démontrer ses capacités, des expériences ont été réalisées sur deux plateformes de matériel de deux types de mains de robot de haute précision, atteignant un taux moyen de succès de 86% pour des tâches.",
      "upvotes": 1,
      "discussionId": "683b8af8091615f46fabaf00",
      "projectPage": "https://dex-umi.github.io/",
      "githubRepo": "https://github.com/real-stanford/DexUMI",
      "ai_summary": "DexUMI framework utilizes a wearable hand exoskeleton and high-fidelity robot hand inpainting to transfer dexterous manipulation skills from human hands to robot hands, achieving high task success rates.",
      "ai_keywords": [
        "wearable hand exoskeleton",
        "haptic feedback",
        "robot hand inpainting",
        "dexterous manipulation",
        "kinematics",
        "visual gap"
      ]
    },
    "publishedAt": "2025-05-27T21:25:27.000Z",
    "title": "DexUMI: Using Human Hand as the Universal Manipulation Interface for\n  Dexterous Manipulation",
    "summary": "We present DexUMI - a data collection and policy learning framework that uses\nthe human hand as the natural interface to transfer dexterous manipulation\nskills to various robot hands. DexUMI includes hardware and software\nadaptations to minimize the embodiment gap between the human hand and various\nrobot hands. The hardware adaptation bridges the kinematics gap using a\nwearable hand exoskeleton. It allows direct haptic feedback in manipulation\ndata collection and adapts human motion to feasible robot hand motion. The\nsoftware adaptation bridges the visual gap by replacing the human hand in video\ndata with high-fidelity robot hand inpainting. We demonstrate DexUMI's\ncapabilities through comprehensive real-world experiments on two different\ndexterous robot hand hardware platforms, achieving an average task success rate\nof 86%.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/655a50a850b9a14799165d53/IaKGx3B7I2pvk3nOzX9s2.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21864.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "655a50a850b9a14799165d53",
      "avatarUrl": "/avatars/6bb37ac0b6840771ff7a6a6da4192ace.svg",
      "fullname": "Mengda Xu",
      "name": "mengdaxu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.13157",
      "authors": [
        {
          "_id": "683b58eb84fbd4b28d8d891e",
          "user": {
            "_id": "6469408ab2321e47d3294414",
            "avatarUrl": "/avatars/05621d33f9f6337bc66e59d9e81d05ef.svg",
            "isPro": false,
            "fullname": "Yassine El Boudouri",
            "user": "yelboudouri",
            "type": "user"
          },
          "name": "Yassine El Boudouri",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-02T07:42:17.658Z",
          "hidden": false
        },
        {
          "_id": "683b58eb84fbd4b28d8d891f",
          "name": "Walter Nuninger",
          "hidden": false
        },
        {
          "_id": "683b58eb84fbd4b28d8d8920",
          "name": "Julian Alvarez",
          "hidden": false
        },
        {
          "_id": "683b58eb84fbd4b28d8d8921",
          "name": "Yvan Peter",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-19T14:18:16.000Z",
      "submittedOnDailyAt": "2025-06-02T07:13:19.898Z",
      "title": "Évaluation du Modèle de Platage de Roles dans les Grands Modèles de Langue",
      "submittedOnDailyBy": {
        "_id": "6469408ab2321e47d3294414",
        "avatarUrl": "/avatars/05621d33f9f6337bc66e59d9e81d05ef.svg",
        "isPro": false,
        "fullname": "Yassine El Boudouri",
        "user": "yelboudouri",
        "type": "user"
      },
      "summary": "Les modèles de langage à grande échelle (LLMs) montrent des capacités qui peuvent s'appliquer autant aux joueurs de rôle qu'aux joueurs professionnels. Cependant, l'évaluation de ces capacités peut constituer un grand défi, car l'évaluation humaine peut être très coûteuse et les méthodes automatiques peuvent inclure des biais. Pour relever ces défis, nous présentons un nouvel indice d'évaluation pour les habiletés des joueurs de rôle dans les LLMs, appelé \"Role-Playing Eval (RPEval)\". Cette évaluation a été conçue sur quatre dimensions : compréhension émotionnelle, politique de décision, cohérence éthique et cohérence du personnage. Dans cet article, nous détaillons la construction et les critères d'évaluation de RPEval. Notre code et dataset sont disponibles pour l'utilisation sur https://github.com/yelboudouri/RPEval.",
      "upvotes": 1,
      "discussionId": "683b58ec84fbd4b28d8d8935",
      "githubRepo": "https://github.com/yelboudouri/RPEval",
      "ai_summary": "A benchmark called Role-Playing Eval assesses Large Language Models in role-playing across emotional understanding, decision-making, moral alignment, and in-character consistency.",
      "ai_keywords": [
        "Large Language Models",
        "Role-Playing Eval",
        "emotional understanding",
        "decision-making",
        "moral alignment",
        "in-character consistency"
      ]
    },
    "publishedAt": "2025-05-19T10:18:16.000Z",
    "title": "Role-Playing Evaluation for Large Language Models",
    "summary": "Large Language Models (LLMs) demonstrate a notable capacity for adopting\npersonas and engaging in role-playing. However, evaluating this ability\npresents significant challenges, as human assessments are resource-intensive\nand automated evaluations can be biased. To address this, we introduce\nRole-Playing Eval (RPEval), a novel benchmark designed to assess LLM\nrole-playing capabilities across four key dimensions: emotional understanding,\ndecision-making, moral alignment, and in-character consistency. This article\ndetails the construction of RPEval and presents baseline evaluations. Our code\nand dataset are available at https://github.com/yelboudouri/RPEval",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.13157.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6469408ab2321e47d3294414",
      "avatarUrl": "/avatars/05621d33f9f6337bc66e59d9e81d05ef.svg",
      "fullname": "Yassine El Boudouri",
      "name": "yelboudouri",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.23832",
      "authors": [
        {
          "_id": "683d67343f97feb881211cf8",
          "name": "Chaeeun Kim",
          "hidden": false
        },
        {
          "_id": "683d67343f97feb881211cf9",
          "name": "Jinu Lee",
          "hidden": false
        },
        {
          "_id": "683d67343f97feb881211cfa",
          "name": "Wonseok Hwang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T09:02:41.000Z",
      "submittedOnDailyAt": "2025-06-02T07:26:28.412Z",
      "title": "Révision de cas juridiques : Révision d'éléments juridiques dans la recherche de cas juridiques révisés",
      "submittedOnDailyBy": {
        "_id": "614c9487cbb5e52274a4024d",
        "avatarUrl": "/avatars/a923db5ea27c4184ed2ce84738860203.svg",
        "isPro": false,
        "fullname": "Chaeeun Kim",
        "user": "Chaeeun-Kim",
        "type": "user"
      },
      "summary": "La recherche de cas juridiques (RCJ), la recherche de cas liés à partir de mots-clés est une tâche de base pour les experts en droit. Cependant, actuellement, il existe deux grandes limitations dans la recherche. L'une est que le corpus de recherche utilisé pour évaluer est d'une échelle relativement petite (par exemple, 100-55K cas) et le spectre de types de mots-clés pénales est étroit, ce qui ne permet pas de refléter suffisamment la complexité des scénarios de recherche de droit dans le monde réel. L'autre est que les méthodes d'embedding ou de coïncidence de mots limitent la coïncidence aux expressions juridiques non pertinentes. Pour faire face à ces problèmes, sont présentés LEGAR BENCH et LegalSearchLM. LEGAR BENCH est le premier cadre de référence à grande échelle de RCJ en Corée, avec 120 000 cas judiciaires et 411 types de crimes différents. LegalSearchLM applique le principe judiciaire aux cas de recherche et génère directement le contenu basé sur le cas cible par décodage restreint. Les résultats des expériences montrent que sur LEGAR BENCH, ils dépassent le point de référence de 6 à 20% et atteignent un niveau avancé de performance, et dépassent de plus de 15% le modèle AUTOMATOPRIME, entraîné sur des données.",
      "upvotes": 0,
      "discussionId": "683d67353f97feb881211d6a"
    },
    "publishedAt": "2025-05-28T05:02:41.000Z",
    "title": "LegalSearchLM: Rethinking Legal Case Retrieval as Legal Elements\n  Generation",
    "summary": "Legal Case Retrieval (LCR), which retrieves relevant cases from a query case,\nis a fundamental task for legal professionals in research and decision-making.\nHowever, existing studies on LCR face two major limitations. First, they are\nevaluated on relatively small-scale retrieval corpora (e.g., 100-55K cases) and\nuse a narrow range of criminal query types, which cannot sufficiently reflect\nthe complexity of real-world legal retrieval scenarios. Second, their reliance\non embedding-based or lexical matching methods often results in limited\nrepresentations and legally irrelevant matches. To address these issues, we\npresent: (1) LEGAR BENCH, the first large-scale Korean LCR benchmark, covering\n411 diverse crime types in queries over 1.2M legal cases; and (2)\nLegalSearchLM, a retrieval model that performs legal element reasoning over the\nquery case and directly generates content grounded in the target cases through\nconstrained decoding. Experimental results show that LegalSearchLM outperforms\nbaselines by 6-20% on LEGAR BENCH, achieving state-of-the-art performance. It\nalso demonstrates strong generalization to out-of-domain cases, outperforming\nnaive generative models trained on in-domain data by 15%.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23832.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "614c9487cbb5e52274a4024d",
      "avatarUrl": "/avatars/a923db5ea27c4184ed2ce84738860203.svg",
      "fullname": "Chaeeun Kim",
      "name": "Chaeeun-Kim",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  }
]