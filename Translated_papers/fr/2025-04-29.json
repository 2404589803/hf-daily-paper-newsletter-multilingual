[
  {
    "paper": {
      "id": "2504.19724",
      "authors": [
        {
          "_id": "68104dd9ec94d9d54ebde2c8",
          "user": {
            "_id": "637745113a63a2983ffbde13",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669187672174-637745113a63a2983ffbde13.jpeg",
            "isPro": false,
            "fullname": "Haofan Wang",
            "user": "wanghaofan",
            "type": "user"
          },
          "name": "Haofan Wang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-29T03:56:15.248Z",
          "hidden": false
        },
        {
          "_id": "68104dd9ec94d9d54ebde2c9",
          "user": {
            "_id": "66471d8f4356b3b33548ee95",
            "avatarUrl": "/avatars/783beebc837d91684f8a959733b48e5b.svg",
            "isPro": false,
            "fullname": "Yujia Xu",
            "user": "YujiaX",
            "type": "user"
          },
          "name": "Yujia Xu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-29T09:20:54.897Z",
          "hidden": false
        },
        {
          "_id": "68104dd9ec94d9d54ebde2ca",
          "name": "Yimeng Li",
          "hidden": false
        },
        {
          "_id": "68104dd9ec94d9d54ebde2cb",
          "name": "Junchen Li",
          "hidden": false
        },
        {
          "_id": "68104dd9ec94d9d54ebde2cc",
          "name": "Chaowei Zhang",
          "hidden": false
        },
        {
          "_id": "68104dd9ec94d9d54ebde2cd",
          "user": {
            "_id": "6649b84af50d4711191ab04c",
            "avatarUrl": "/avatars/dfe85eb28ae970e718c37cc6bc459457.svg",
            "isPro": false,
            "fullname": "WJ",
            "user": "SNOWAI",
            "type": "user"
          },
          "name": "Jing Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-29T08:03:03.819Z",
          "hidden": false
        },
        {
          "_id": "68104dd9ec94d9d54ebde2ce",
          "name": "Kejia Yang",
          "hidden": false
        },
        {
          "_id": "68104dd9ec94d9d54ebde2cf",
          "name": "Zhibo Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-28T12:19:53.000Z",
      "submittedOnDailyAt": "2025-04-29T02:27:56.443Z",
      "title": "Replica Virtuelle Texte Affichage Virtuel",
      "submittedOnDailyBy": {
        "_id": "637745113a63a2983ffbde13",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669187672174-637745113a63a2983ffbde13.jpeg",
        "isPro": false,
        "fullname": "Haofan Wang",
        "user": "wanghaofan",
        "type": "user"
      },
      "summary": "Le modèle de génération d'images à partir de texte moderne a réalisé une grande révolution dans la création d'images belles, mais sa capacité à générer des éléments typographiques précis et flexibles, surtout pour des caractères qui ne sont pas des alphabets latins, est limitée. Pour résoudre cette limitation, nous avons proposé RepText, en nous basant sur l'hypothèse douteuse selon laquelle la compréhension du texte n'est pas une condition nécessaire mais une condition supplémentaire pour la génération de texte graphique. RepText vise à ce que le modèle de génération d'images à partir de texte puisse graphiser des textes visuels de différents langues avec précision, en utilisant différentes sources de caractères que l'utilisateur puisse spécifier. Concrètement, nous appliquons des configurations de ControlNet pour créer des textes visuels harmonieux, y compris des caractères et des positions indépendamment du langage, et nous permettons à l'utilisateur de personnaliser le contenu du texte, la fonte et sa position. Pour améliorer la précision, nous combinons la perte de texte visuelle et la perte de diffusion. De plus, nous initialisons directement les valeurs de potentiel des caractères pendant la phase d'inférence pour stabiliser le processus de génération et nous évitons la distorsion du fond en utilisant une masque d'aire pour limiter l'injection de caractéristiques dans les régions de texte. Pour vérifier les effets de RepText, nous avons effectué des expériences extensives qui ont montré un rendement considérablement supérieur à celui des outils actuels et ont abouti à des résultats similaires à ceux des modèles multilingues naturels non-source. Plus justement, nous avons discuté en détail ces limitations finalement.",
      "upvotes": 16,
      "discussionId": "68104ddfec94d9d54ebde3f3",
      "projectPage": "https://reptext.github.io/",
      "githubRepo": "https://github.com/Shakker-Labs/RepText"
    },
    "publishedAt": "2025-04-28T08:19:53.000Z",
    "title": "RepText: Rendering Visual Text via Replicating",
    "summary": "Although contemporary text-to-image generation models have achieved\nremarkable breakthroughs in producing visually appealing images, their capacity\nto generate precise and flexible typographic elements, especially non-Latin\nalphabets, remains constrained. To address these limitations, we start from an\nnaive assumption that text understanding is only a sufficient condition for\ntext rendering, but not a necessary condition. Based on this, we present\nRepText, which aims to empower pre-trained monolingual text-to-image generation\nmodels with the ability to accurately render, or more precisely, replicate,\nmultilingual visual text in user-specified fonts, without the need to really\nunderstand them. Specifically, we adopt the setting from ControlNet and\nadditionally integrate language agnostic glyph and position of rendered text to\nenable generating harmonized visual text, allowing users to customize text\ncontent, font and position on their needs. To improve accuracy, a text\nperceptual loss is employed along with the diffusion loss. Furthermore, to\nstabilize rendering process, at the inference phase, we directly initialize\nwith noisy glyph latent instead of random initialization, and adopt region\nmasks to restrict the feature injection to only the text region to avoid\ndistortion of the background. We conducted extensive experiments to verify the\neffectiveness of our RepText relative to existing works, our approach\noutperforms existing open-source methods and achieves comparable results to\nnative multi-language closed-source models. To be more fair, we also\nexhaustively discuss its limitations in the end.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.19724.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "637745113a63a2983ffbde13",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669187672174-637745113a63a2983ffbde13.jpeg",
      "fullname": "Haofan Wang",
      "name": "wanghaofan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 79
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.19838",
      "authors": [
        {
          "_id": "6810317e007d579cbf5200ba",
          "name": "Guangyi Liu",
          "hidden": false
        },
        {
          "_id": "6810317e007d579cbf5200bb",
          "user": {
            "_id": "65a088f4300957620ba45c70",
            "avatarUrl": "/avatars/56ed45e10d3455531979f30881b2d3f9.svg",
            "isPro": false,
            "fullname": "pengxiang zhao",
            "user": "Pengxiangzhao",
            "type": "user"
          },
          "name": "Pengxiang Zhao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-29T07:59:51.686Z",
          "hidden": false
        },
        {
          "_id": "6810317e007d579cbf5200bc",
          "user": {
            "_id": "667b91162bfe908436900faa",
            "avatarUrl": "/avatars/daeaf058ec1df5307996895a5cbba052.svg",
            "isPro": false,
            "fullname": "Liang Liu",
            "user": "melpancake",
            "type": "user"
          },
          "name": "Liang Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-29T08:01:16.707Z",
          "hidden": false
        },
        {
          "_id": "6810317e007d579cbf5200bd",
          "name": "Yaxuan Guo",
          "hidden": false
        },
        {
          "_id": "6810317e007d579cbf5200be",
          "name": "Han Xiao",
          "hidden": false
        },
        {
          "_id": "6810317e007d579cbf5200bf",
          "name": "Weifeng Lin",
          "hidden": false
        },
        {
          "_id": "6810317e007d579cbf5200c0",
          "user": {
            "_id": "6458ce236fa580137af5aa95",
            "avatarUrl": "/avatars/db65a7332e375eb5daad5c1b076b1e3b.svg",
            "isPro": false,
            "fullname": "Yuxiang Chai",
            "user": "Yuxiang007",
            "type": "user"
          },
          "name": "Yuxiang Chai",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-29T08:01:19.091Z",
          "hidden": false
        },
        {
          "_id": "6810317e007d579cbf5200c1",
          "name": "Yue Han",
          "hidden": false
        },
        {
          "_id": "6810317e007d579cbf5200c2",
          "name": "Shuai Ren",
          "hidden": false
        },
        {
          "_id": "6810317e007d579cbf5200c3",
          "name": "Hao Wang",
          "hidden": false
        },
        {
          "_id": "6810317e007d579cbf5200c4",
          "name": "Xiaoyu Liang",
          "hidden": false
        },
        {
          "_id": "6810317e007d579cbf5200c5",
          "name": "Wenhao Wang",
          "hidden": false
        },
        {
          "_id": "6810317e007d579cbf5200c6",
          "name": "Tianze Wu",
          "hidden": false
        },
        {
          "_id": "6810317e007d579cbf5200c7",
          "name": "Linghao Li",
          "hidden": false
        },
        {
          "_id": "6810317e007d579cbf5200c8",
          "name": "Hao Wang",
          "hidden": false
        },
        {
          "_id": "6810317e007d579cbf5200c9",
          "name": "Guanjing Xiong",
          "hidden": false
        },
        {
          "_id": "6810317e007d579cbf5200ca",
          "name": "Yong Liu",
          "hidden": false
        },
        {
          "_id": "6810317e007d579cbf5200cb",
          "name": "Hongsheng Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-28T14:39:25.000Z",
      "submittedOnDailyAt": "2025-04-29T00:30:25.482Z",
      "title": "LLM Droving GUI Agent's Call Automation: Développement et Perspectives Futures",
      "submittedOnDailyBy": {
        "_id": "64d761b98ebc40443831f82a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64d761b98ebc40443831f82a/DHBOtOstiFp2-lDY6b9gb.png",
        "isPro": false,
        "fullname": "lgy0404",
        "user": "lgy0404",
        "type": "user"
      },
      "summary": "Reviser de manière systématique l'output GUI de lignes de code guidée par un modèle de langage, mettant en avant l'évolution depuis l'automatisme basé sur des scripts jusqu'à des systèmes cognitifs et adaptatifs. Tout d'abord, on explique les principaux problèmes : (i) la généralité, (ii) le haut coût de maintenance, (iii) la déficience dans la compréhension des intentions, et on présente les solutions que le modèle de langage offre pour les résoudre. Ensuite, on propose une technologie qui inclut les outputs de base et les cadres (output unique, multiples outputs, exécution planifiée), les approches de modélisation (ingénierie de prédictions, basée sur l'apprentissage), et la configuration de jeux de données originaux et de références. De plus, on détaille des architectures spécifiques pour des tâches, l'ajustement avec des instructeurs, et des stratégies d'apprentissage par renforcement, avec l'objectif de relier les intentions de l'utilisateur au comportement de la GUI. Enfin, on discute des problèmes ouverts tels que la diversité des jeux de données, l'efficacité de l'implémentation sur des dispositifs, l'adaptabilité centrée sur l'utilisateur, et les préoccupations de sécurité, et on offre des perspectives avancées pour ce domaine en constante évolution. Cet article est une source précieuse d'information pour les chercheurs et les praticiens qui cherchent à concevoir des outputs de GUI de lignes de code échellables et utilisateur-amiables en utilisant un modèle de langage.",
      "upvotes": 15,
      "discussionId": "68103184007d579cbf5202d9",
      "projectPage": "https://github.com/PhoneLLM/Awesome-LLM-Powered-Phone-GUI-Agents"
    },
    "publishedAt": "2025-04-28T10:39:25.000Z",
    "title": "LLM-Powered GUI Agents in Phone Automation: Surveying Progress and\n  Prospects",
    "summary": "With the rapid rise of large language models (LLMs), phone automation has\nundergone transformative changes. This paper systematically reviews LLM-driven\nphone GUI agents, highlighting their evolution from script-based automation to\nintelligent, adaptive systems. We first contextualize key challenges, (i)\nlimited generality, (ii) high maintenance overhead, and (iii) weak intent\ncomprehension, and show how LLMs address these issues through advanced language\nunderstanding, multimodal perception, and robust decision-making. We then\npropose a taxonomy covering fundamental agent frameworks (single-agent,\nmulti-agent, plan-then-act), modeling approaches (prompt engineering,\ntraining-based), and essential datasets and benchmarks. Furthermore, we detail\ntask-specific architectures, supervised fine-tuning, and reinforcement learning\nstrategies that bridge user intent and GUI operations. Finally, we discuss open\nchallenges such as dataset diversity, on-device deployment efficiency,\nuser-centric adaptation, and security concerns, offering forward-looking\ninsights into this rapidly evolving field. By providing a structured overview\nand identifying pressing research gaps, this paper serves as a definitive\nreference for researchers and practitioners seeking to harness LLMs in\ndesigning scalable, user-friendly phone GUI agents.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.19838.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "64d761b98ebc40443831f82a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64d761b98ebc40443831f82a/DHBOtOstiFp2-lDY6b9gb.png",
      "fullname": "lgy0404",
      "name": "lgy0404",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.19093",
      "authors": [
        {
          "_id": "6810356ab91a093e4f4cc262",
          "user": {
            "_id": "671b852aa4fa4f8f5fb5404c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/671b852aa4fa4f8f5fb5404c/TDLsgP8WgKW-qaA8Ys-iJ.jpeg",
            "isPro": false,
            "fullname": "YU LI",
            "user": "yu0226",
            "type": "user"
          },
          "name": "Yu Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-29T08:01:23.352Z",
          "hidden": false
        },
        {
          "_id": "6810356ab91a093e4f4cc263",
          "name": "Qizhi Pei",
          "hidden": false
        },
        {
          "_id": "6810356ab91a093e4f4cc264",
          "user": {
            "_id": "67ad790c2b28204981be8e24",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67ad790c2b28204981be8e24/KstE5e5bUXXIvgPJqMO2B.jpeg",
            "isPro": false,
            "fullname": "Mengyuan Sun",
            "user": "blue01223",
            "type": "user"
          },
          "name": "Mengyuan Sun",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-29T08:01:29.740Z",
          "hidden": false
        },
        {
          "_id": "6810356ab91a093e4f4cc265",
          "name": "Honglin Lin",
          "hidden": false
        },
        {
          "_id": "6810356ab91a093e4f4cc266",
          "name": "Chenlin Ming",
          "hidden": false
        },
        {
          "_id": "6810356ab91a093e4f4cc267",
          "name": "Xin Gao",
          "hidden": false
        },
        {
          "_id": "6810356ab91a093e4f4cc268",
          "name": "Jiang Wu",
          "hidden": false
        },
        {
          "_id": "6810356ab91a093e4f4cc269",
          "name": "Conghui He",
          "hidden": false
        },
        {
          "_id": "6810356ab91a093e4f4cc26a",
          "name": "Lijun Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-27T03:41:17.000Z",
      "submittedOnDailyAt": "2025-04-29T04:39:29.218Z",
      "title": "CipherBank : Motifs pour déterminer les limites de la compréhension d'un LLM dans les défis de cryptanalyse",
      "submittedOnDailyBy": {
        "_id": "6397f6081323f19c578f142e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6397f6081323f19c578f142e/it7FYYKjlLX8wSsMLm8EO.jpeg",
        "isPro": false,
        "fullname": "QizhiPei",
        "user": "QizhiPei",
        "type": "user"
      },
      "summary": "Les modèles de langage artificiel (LLMs) ont démontré des capacités impressionnantes grâce au développement récent de l'intelligence artificielle (par exemple, o1 et o3). Malgré leurs réussites notables dans des domaines comme la mathématique ou le code, l'intelligence artificielle des LLMs n'a pas encore été exhaustivement étudiée dans le domaine de la technologie cryptographique. Dans cet article, nous présentons le CipherBank, un ensemble de 2,358 évaluations benchmark développé de près pour évaluer l'intelligence artificielle des LLMs dans des tâches de cryptage et de décryptage. Le CipherBank est constitué de 5 blocs et 14 sous-blocs, couvrant 262 textes plans uniques, avec un accent sur les situations de confidentialité et de cryptage dans des contextes réels. À partir d'une perspective de cryptage, le CipherBank combine trois principales méthodes de cryptage et neuf algorithmes supplémentaires, y compris des techniques personnalisées, à l'exception du cryptage classique. Dans le CipherBank, les meilleurs LLMs sont évalués, dont incluent GPT-4o, DeepSeek-V3, et les modèles avancés qui mettent l'accent sur l'intelligence artificielle, comme o1 et DeepSeek-R1. Nos résultats confirment les différences entre les LLMs généraux et ceux qui mettent l'accent sur l'intelligence artificielle dans des tâches de cryptage et de décryptage, et montrent clairement les résultats des modèles qui mettent l'accent sur l'intelligence artificielle dans des tâches de cryptage et de décryptage. Ces résultats sont analysés en profondeur, et des tests d'erreur sont effectués pour identifier les limitations et les domaines d'amélioration de l'intelligence artificielle des LLMs dans le domaine de la cryptage. Ces résultats soulignent la nécessité de poursuivre le développement de l'intelligence artificielle des LLMs.",
      "upvotes": 9,
      "discussionId": "68103574b91a093e4f4cc57a",
      "projectPage": "https://cipherbankeva.github.io/",
      "githubRepo": "https://github.com/Goodman-liyu/CipherBank"
    },
    "publishedAt": "2025-04-26T23:41:17.000Z",
    "title": "CipherBank: Exploring the Boundary of LLM Reasoning Capabilities through\n  Cryptography Challenges",
    "summary": "Large language models (LLMs) have demonstrated remarkable capabilities,\nespecially the recent advancements in reasoning, such as o1 and o3, pushing the\nboundaries of AI. Despite these impressive achievements in mathematics and\ncoding, the reasoning abilities of LLMs in domains requiring cryptographic\nexpertise remain underexplored. In this paper, we introduce CipherBank, a\ncomprehensive benchmark designed to evaluate the reasoning capabilities of LLMs\nin cryptographic decryption tasks. CipherBank comprises 2,358 meticulously\ncrafted problems, covering 262 unique plaintexts across 5 domains and 14\nsubdomains, with a focus on privacy-sensitive and real-world scenarios that\nnecessitate encryption. From a cryptographic perspective, CipherBank\nincorporates 3 major categories of encryption methods, spanning 9 distinct\nalgorithms, ranging from classical ciphers to custom cryptographic techniques.\nWe evaluate state-of-the-art LLMs on CipherBank, e.g., GPT-4o, DeepSeek-V3, and\ncutting-edge reasoning-focused models such as o1 and DeepSeek-R1. Our results\nreveal significant gaps in reasoning abilities not only between general-purpose\nchat LLMs and reasoning-focused LLMs but also in the performance of current\nreasoning-focused models when applied to classical cryptographic decryption\ntasks, highlighting the challenges these models face in understanding and\nmanipulating encrypted data. Through detailed analysis and error\ninvestigations, we provide several key observations that shed light on the\nlimitations and potential improvement areas for LLMs in cryptographic\nreasoning. These findings underscore the need for continuous advancements in\nLLM reasoning capabilities.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.19093.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "6397f6081323f19c578f142e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6397f6081323f19c578f142e/it7FYYKjlLX8wSsMLm8EO.jpeg",
      "fullname": "QizhiPei",
      "name": "QizhiPei",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 23
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.18919",
      "authors": [
        {
          "_id": "681039b2b02c157249d046b0",
          "name": "Andrew M. Bean",
          "hidden": false
        },
        {
          "_id": "681039b2b02c157249d046b1",
          "name": "Rebecca Payne",
          "hidden": false
        },
        {
          "_id": "681039b2b02c157249d046b2",
          "name": "Guy Parsons",
          "hidden": false
        },
        {
          "_id": "681039b2b02c157249d046b3",
          "name": "Hannah Rose Kirk",
          "hidden": false
        },
        {
          "_id": "681039b2b02c157249d046b4",
          "name": "Juan Ciro",
          "hidden": false
        },
        {
          "_id": "681039b2b02c157249d046b5",
          "name": "Rafael Mosquera",
          "hidden": false
        },
        {
          "_id": "681039b2b02c157249d046b6",
          "name": "Sara Hincapié Monsalve",
          "hidden": false
        },
        {
          "_id": "681039b2b02c157249d046b7",
          "name": "Aruna S. Ekanayaka",
          "hidden": false
        },
        {
          "_id": "681039b2b02c157249d046b8",
          "name": "Lionel Tarassenko",
          "hidden": false
        },
        {
          "_id": "681039b2b02c157249d046b9",
          "name": "Luc Rocher",
          "hidden": false
        },
        {
          "_id": "681039b2b02c157249d046ba",
          "name": "Adam Mahdi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-26T13:32:49.000Z",
      "submittedOnDailyAt": "2025-04-29T01:01:48.840Z",
      "title": "Le connaissance clinique des LLM ne s'adapte pas à l'interaction avec les humains.",
      "submittedOnDailyBy": {
        "_id": "659bec4728676374f33ef921",
        "avatarUrl": "/avatars/217ae547d6460e65c6d2a23012741830.svg",
        "isPro": false,
        "fullname": "Andrew Bean",
        "user": "ambean",
        "type": "user"
      },
      "summary": "Les fournisseurs de services médicaux mondiaux sont en train d'étudier comment utiliser des modèles de langage grand (LLMs) pour fournir des conseils médicaux à la communauté. Les LLMs ont obtenu des scores presque parfaits dans les examens de médecine, mais il n'est pas possible de garantir que leur performance soit la même en réalité. Une étude contrôlée avec 1,298 participants a été menée pour évaluer si les LLMs peuvent aider les participants à identifier des états de santé potentiels et à choisir des plans d'action (traitement). Les participants ont reçu l'aide d'un LLM (GPT-4o, Llama 3, Command R+) ou d'une source sélectionnée (Conseil National de Santé). Dans le test individuel, les LLMs ont pu identifier avec une précision moyenne de 94,9% les états et de 56,3% les traitements. Cependant, les participants qui ont utilisé les mêmes LLMs ont identifié avec moins de 34,5% de précision les états et avec moins de 44,2% de précision les traitements, ce qui est moins précis que la source sélectionnée. Il a été découvert que l'interaction avec les utilisateurs est l'un des défis pour l'application des conseils médicaux des LLMs. L'interaction avec des patients simulés n'a pas pu prédire les erreurs observées chez les participants humains. Il est proposé de réaliser des tests systématiques de participants humains avant la mise en œuvre publique dans le secteur médical pour évaluer la capacité d'interaction.",
      "upvotes": 4,
      "discussionId": "681039b5b02c157249d04787"
    },
    "publishedAt": "2025-04-26T09:32:49.000Z",
    "title": "Clinical knowledge in LLMs does not translate to human interactions",
    "summary": "Global healthcare providers are exploring use of large language models (LLMs)\nto provide medical advice to the public. LLMs now achieve nearly perfect scores\non medical licensing exams, but this does not necessarily translate to accurate\nperformance in real-world settings. We tested if LLMs can assist members of the\npublic in identifying underlying conditions and choosing a course of action\n(disposition) in ten medical scenarios in a controlled study with 1,298\nparticipants. Participants were randomly assigned to receive assistance from an\nLLM (GPT-4o, Llama 3, Command R+) or a source of their choice (control). Tested\nalone, LLMs complete the scenarios accurately, correctly identifying conditions\nin 94.9% of cases and disposition in 56.3% on average. However, participants\nusing the same LLMs identified relevant conditions in less than 34.5% of cases\nand disposition in less than 44.2%, both no better than the control group. We\nidentify user interactions as a challenge to the deployment of LLMs for medical\nadvice. Standard benchmarks for medical knowledge and simulated patient\ninteractions do not predict the failures we find with human participants.\nMoving forward, we recommend systematic human user testing to evaluate\ninteractive capabilities prior to public deployments in healthcare.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.18919.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "659bec4728676374f33ef921",
      "avatarUrl": "/avatars/217ae547d6460e65c6d2a23012741830.svg",
      "fullname": "Andrew Bean",
      "name": "ambean",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.17258",
      "authors": [
        {
          "_id": "680edc612488a3b6b9feb9d0",
          "user": {
            "_id": "661e07e02a8496916011c08a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/6vIkL4LJ0bLeJL99E20F_.jpeg",
            "isPro": false,
            "fullname": "Md Ashiqur Rahman",
            "user": "ashiq24",
            "type": "user"
          },
          "name": "Md Ashiqur Rahman",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-04-28T03:56:25.436Z",
          "hidden": false
        },
        {
          "_id": "680edc612488a3b6b9feb9d1",
          "name": "Raymond A. Yeh",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/661e07e02a8496916011c08a/_lIAAwBEVKqioepM0Mmfl.png",
        "https://cdn-uploads.huggingface.co/production/uploads/661e07e02a8496916011c08a/UUaV0uoeUWOTbfJopBnbV.png"
      ],
      "publishedAt": "2025-04-24T05:29:51.000Z",
      "submittedOnDailyAt": "2025-04-29T02:41:40.480Z",
      "title": "Groupe Désampling et Sans Alias",
      "submittedOnDailyBy": {
        "_id": "661e07e02a8496916011c08a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/6vIkL4LJ0bLeJL99E20F_.jpeg",
        "isPro": false,
        "fullname": "Md Ashiqur Rahman",
        "user": "ashiq24",
        "type": "user"
      },
      "summary": "La couche de sous-échantillonnage est un élément important de l'architecture de CNN, contribuant à l'expansion du champ de réception grâce au apprentissage de caractéristiques de haut niveau et à la réduction du consommation de mémoire et du calcul du modèle. Dans cet article, on étudie la généralisation d'une couche de sous-échantillonnage uniforme applicable à des architectures comme les G-CNN, qui possèdent des symétries dans des groupes. On cherche à sous-échantillonner des signaux (cartes de caractéristiques) sur des groupes finis de manière à obtenir un effet anti-aliasing. Pour cela, on inclut les points suivants :\n\n(a) On propose un algorithme pour sélectionner un sous-groupe approprié lorsque le groupe fini et la proportion de sous-échantillonnage sont connus.\n(b) On étudie les méthodes pour exécuter la restriction de bande et l'effet anti-aliasing lorsque le groupe et le sous-groupe sont connus.\n\nSpécifiquement, notre méthode généralise le concept de sous-échantillonnage basé sur la théorie traditionnelle de sous-échantillonnage. Dans les cas où la signaux est sur un groupe cyclique, notre méthode se transforme en la norme de l'opération de sous-échantillonnage d'un filtre idéal de basse fréquence pour récupérer le sous-échantillonnage standard. Enfin, des expériences sont réalisées sur des tâches de classification d'images, montrant que l'opération de sous-échantillonnage proposée améliore la précision, maintient mieux la symétrie dans les groupes et permet de réduire le taille du modèle lorsqu'elle est introduite dans des réseaux G-symétriques.",
      "upvotes": 4,
      "discussionId": "680edc622488a3b6b9feba0e",
      "projectPage": "https://github.com/ashiq24/Group_Sampling",
      "githubRepo": "https://github.com/ashiq24/Group_Sampling",
      "ai_keywords": [
        "downsampling layers",
        "CNN architectures",
        "receptive field",
        "high-level features",
        "memory/computation",
        "group equivariant architectures",
        "G-CNNs",
        "finite groups",
        "downsampling rate",
        "subgroup",
        "bandlimited-ness",
        "anti-aliasing",
        "classical sampling theory",
        "cyclic group",
        "periodic",
        "ideal low-pass filter",
        "subsampling operation",
        "image classification tasks",
        "equivariance",
        "model size",
        "G-equivariant networks"
      ]
    },
    "publishedAt": "2025-04-24T01:29:51.000Z",
    "title": "Group Downsampling with Equivariant Anti-aliasing",
    "summary": "Downsampling layers are crucial building blocks in CNN architectures, which\nhelp to increase the receptive field for learning high-level features and\nreduce the amount of memory/computation in the model. In this work, we study\nthe generalization of the uniform downsampling layer for group equivariant\narchitectures, e.g., G-CNNs. That is, we aim to downsample signals (feature\nmaps) on general finite groups with anti-aliasing. This involves the following:\n(a) Given a finite group and a downsampling rate, we present an algorithm to\nform a suitable choice of subgroup. (b) Given a group and a subgroup, we study\nthe notion of bandlimited-ness and propose how to perform anti-aliasing.\nNotably, our method generalizes the notion of downsampling based on classical\nsampling theory. When the signal is on a cyclic group, i.e., periodic, our\nmethod recovers the standard downsampling of an ideal low-pass filter followed\nby a subsampling operation. Finally, we conducted experiments on image\nclassification tasks demonstrating that the proposed downsampling operation\nimproves accuracy, better preserves equivariance, and reduces model size when\nincorporated into G-equivariant networks",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/661e07e02a8496916011c08a/_lIAAwBEVKqioepM0Mmfl.png",
      "https://cdn-uploads.huggingface.co/production/uploads/661e07e02a8496916011c08a/UUaV0uoeUWOTbfJopBnbV.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17258.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "661e07e02a8496916011c08a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/6vIkL4LJ0bLeJL99E20F_.jpeg",
      "fullname": "Md Ashiqur Rahman",
      "name": "ashiq24",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.15780",
      "authors": [
        {
          "_id": "68104f85b442ffc234b670cd",
          "name": "Daocheng Fu",
          "hidden": false
        },
        {
          "_id": "68104f85b442ffc234b670ce",
          "name": "Zijun Chen",
          "hidden": false
        },
        {
          "_id": "68104f85b442ffc234b670cf",
          "name": "Renqiu Xia",
          "hidden": false
        },
        {
          "_id": "68104f85b442ffc234b670d0",
          "name": "Qi Liu",
          "hidden": false
        },
        {
          "_id": "68104f85b442ffc234b670d1",
          "name": "Yuan Feng",
          "hidden": false
        },
        {
          "_id": "68104f85b442ffc234b670d2",
          "name": "Hongbin Zhou",
          "hidden": false
        },
        {
          "_id": "68104f85b442ffc234b670d3",
          "name": "Renrui Zhang",
          "hidden": false
        },
        {
          "_id": "68104f85b442ffc234b670d4",
          "name": "Shiyang Feng",
          "hidden": false
        },
        {
          "_id": "68104f85b442ffc234b670d5",
          "name": "Peng Gao",
          "hidden": false
        },
        {
          "_id": "68104f85b442ffc234b670d6",
          "name": "Junchi Yan",
          "hidden": false
        },
        {
          "_id": "68104f85b442ffc234b670d7",
          "name": "Botian Shi",
          "hidden": false
        },
        {
          "_id": "68104f85b442ffc234b670d8",
          "name": "Bo Zhang",
          "hidden": false
        },
        {
          "_id": "68104f85b442ffc234b670d9",
          "name": "Yu Qiao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-22T10:45:23.000Z",
      "submittedOnDailyAt": "2025-04-29T02:36:15.881Z",
      "title": "TrustGeoGen : Moteur de données échelonnable pour la démonstration formelle de solutions de problèmes géométriques multimodales fiables",
      "submittedOnDailyBy": {
        "_id": "65b7ae76768464877cdb2e39",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65b7ae76768464877cdb2e39/uAWPo4tpkqbZoDeEkc7y0.jpeg",
        "isPro": false,
        "fullname": "Renqiu Xia",
        "user": "renqiux0302",
        "type": "user"
      },
      "summary": "La résolution de problèmes géométriques (GPS) nécessite l'intégration efficace de diverses informations et la nécessité d'une logique démonstrative. Bien que le rapide développement de grands modèles de langage dans la résolution de problèmes généraux, il reste des parties non résolues dans les méthodes et les benchmarks, et les benchmarks de GPS synthétiques existants ne permettent pas de tests automatiques des modèles de langage et sont chargés de bruit et d'informations autologiques. Dans cet article, nous proposons un moteur de données scalable pour la génération de problèmes appelé TrustGeoGen, et nous proposons de fournir un benchmark fondamental basé sur des tests formels pour encourager le développement de méthodes en GPS. Ce moteur génère des données géométriques de manière innovante à travers 4 innovations clés : 1) l'alignement de la diversité de dessins, descriptions textuelles et solutions pas à pas, 2) la garantie des chemins de raisonnement réguliers à travers des tests formels, 3) l'amélioration de la complexité évolutive par la structure de processus foliaires, et 4) la génération simultanée de variations et de rétrocours réflexifs de multiples solutions par les algorithmes de la série GeoExplore. A travers des tests logiques formels, TrustGeoGen génère le jeu de données GeoTrust-200K, garantissant la profondeur des modèles et proposant un benchmark fondamental pour le développement de méthodes en GPS.",
      "upvotes": 4,
      "discussionId": "68104f86b442ffc234b67113"
    },
    "publishedAt": "2025-04-22T06:45:23.000Z",
    "title": "TrustGeoGen: Scalable and Formal-Verified Data Engine for Trustworthy\n  Multi-modal Geometric Problem Solving",
    "summary": "Mathematical geometric problem solving (GPS) often requires effective\nintegration of multimodal information and verifiable logical coherence. Despite\nthe fast development of large language models in general problem solving, it\nremains unresolved regarding with both methodology and benchmarks, especially\ngiven the fact that exiting synthetic GPS benchmarks are often not\nself-verified and contain noise and self-contradicted information due to the\nillusion of LLMs. In this paper, we propose a scalable data engine called\nTrustGeoGen for problem generation, with formal verification to provide a\nprincipled benchmark, which we believe lays the foundation for the further\ndevelopment of methods for GPS. The engine synthesizes geometric data through\nfour key innovations: 1) multimodal-aligned generation of diagrams, textual\ndescriptions, and stepwise solutions; 2) formal verification ensuring\nrule-compliant reasoning paths; 3) a bootstrapping mechanism enabling\ncomplexity escalation via recursive state generation and 4) our devised\nGeoExplore series algorithms simultaneously produce multi-solution variants and\nself-reflective backtracking traces. By formal logical verification,\nTrustGeoGen produces GeoTrust-200K dataset with guaranteed modality integrity,\nalong with GeoTrust-test testset. Experiments reveal the state-of-the-art\nmodels achieve only 49.17\\% accuracy on GeoTrust-test, demonstrating its\nevaluation stringency. Crucially, models trained on GeoTrust achieve OOD\ngeneralization on GeoQA, significantly reducing logical inconsistencies\nrelative to pseudo-label annotated by OpenAI-o1. Our code is available at\nhttps://github.com/Alpha-Innovator/TrustGeoGen",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15780.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65b7ae76768464877cdb2e39",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65b7ae76768464877cdb2e39/uAWPo4tpkqbZoDeEkc7y0.jpeg",
      "fullname": "Renqiu Xia",
      "name": "renqiux0302",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.16083",
      "authors": [
        {
          "_id": "68105d8632d635f02bc2976e",
          "name": "Yucheng Li",
          "hidden": false
        },
        {
          "_id": "68105d8632d635f02bc2976f",
          "user": {
            "_id": "6278bd42541f3d2dfa77ea70",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6278bd42541f3d2dfa77ea70/ejn49eapnB3UXQckAYdTd.jpeg",
            "isPro": true,
            "fullname": "Huiqiang Jiang",
            "user": "iofu728",
            "type": "user"
          },
          "name": "Huiqiang Jiang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-29T05:03:03.400Z",
          "hidden": false
        },
        {
          "_id": "68105d8632d635f02bc29770",
          "name": "Chengruidong Zhang",
          "hidden": false
        },
        {
          "_id": "68105d8632d635f02bc29771",
          "name": "Qianhui Wu",
          "hidden": false
        },
        {
          "_id": "68105d8632d635f02bc29772",
          "name": "Xufang Luo",
          "hidden": false
        },
        {
          "_id": "68105d8632d635f02bc29773",
          "name": "Surin Ahn",
          "hidden": false
        },
        {
          "_id": "68105d8632d635f02bc29774",
          "name": "Amir H. Abdi",
          "hidden": false
        },
        {
          "_id": "68105d8632d635f02bc29775",
          "name": "Dongsheng Li",
          "hidden": false
        },
        {
          "_id": "68105d8632d635f02bc29776",
          "name": "Jianfeng Gao",
          "hidden": false
        },
        {
          "_id": "68105d8632d635f02bc29777",
          "name": "Yuqing Yang",
          "hidden": false
        },
        {
          "_id": "68105d8632d635f02bc29778",
          "name": "Lili Qiu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6278bd42541f3d2dfa77ea70/K2pFSyL6PhhzhfK3ar9Ok.jpeg"
      ],
      "publishedAt": "2025-04-22T17:59:51.000Z",
      "submittedOnDailyAt": "2025-04-29T03:34:32.012Z",
      "title": "MMInference : Accélération du Pré-entraînement de VLMs à Long Context Utilisant une Substitution d'Actions Sparses Basée sur le Modèle",
      "submittedOnDailyBy": {
        "_id": "6278bd42541f3d2dfa77ea70",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6278bd42541f3d2dfa77ea70/ejn49eapnB3UXQckAYdTd.jpeg",
        "isPro": true,
        "fullname": "Huiqiang Jiang",
        "user": "iofu728",
        "type": "user"
      },
      "summary": "La mise en œuvre de compétences de contexte long et de compréhension visuelle ouvre des perspectives sans précédent pour les modèles de langage visuo-linguistiques (VLMs). Cependant, la complexité bidimensionnelle de l'attention remplie de réserves constitue un grand obstacle pour des fonctions réalistes. Pour surmonter cette limitation, nous présentons MMInference (Inférence Multimodalité de Million de Tokens), une technique d'attention dynamique et éparsée pour traiter rapidement différentes entrées de contexte long. Tout d'abord, notre analyse extrait la localisation temporelle et spatiale des entrées vidéo dans un modèle épars caractéristique, le Grid. En même temps, les VLMs présentent une distribution éparsée clairement différente de ceux d'autres modèles. Nous utilisons un approche basée sur les permutations pour résoudre la diversité du modèle, explorant des motifs épars optimaux dans chaque tête dans le scénario en ligne, et MMInference construit une distribution éparsée dynamiquement selon l'entrée. De plus, il offre un canal GPU optimisé pour des calculs épars efficaces. En particulier, MMInference permet l'intégration sécurisée sans modifications ou fine-tuning, évitant que des erreurs soient ignorées dans les VLMs actuels. A travers des expérimentations sur différents types de benchmarks, comme QA vidéo, Captionning, VisionNIAH et Mixed-Modalité NIAH, en utilisant les VLMs de contexte long les plus avancés (LongVila, LlavaVideo, VideoChat-Flash, Qwen2.5-VL), MMInference réussit à accélérer le passage épars de 1M tokens de 8.3 fois, tout en maintenant la précision. Le code est disponible sur https://aka.ms/MMInference.",
      "upvotes": 3,
      "discussionId": "68105d8732d635f02bc297bb"
    },
    "publishedAt": "2025-04-22T13:59:51.000Z",
    "title": "MMInference: Accelerating Pre-filling for Long-Context VLMs via\n  Modality-Aware Permutation Sparse Attention",
    "summary": "The integration of long-context capabilities with visual understanding\nunlocks unprecedented potential for Vision Language Models (VLMs). However, the\nquadratic attention complexity during the pre-filling phase remains a\nsignificant obstacle to real-world deployment. To overcome this limitation, we\nintroduce MMInference (Multimodality Million tokens Inference), a dynamic\nsparse attention method that accelerates the prefilling stage for long-context\nmulti-modal inputs. First, our analysis reveals that the temporal and spatial\nlocality of video input leads to a unique sparse pattern, the Grid pattern.\nSimultaneously, VLMs exhibit markedly different sparse distributions across\ndifferent modalities. We introduce a permutation-based method to leverage the\nunique Grid pattern and handle modality boundary issues. By offline search the\noptimal sparse patterns for each head, MMInference constructs the sparse\ndistribution dynamically based on the input. We also provide optimized GPU\nkernels for efficient sparse computations. Notably, MMInference integrates\nseamlessly into existing VLM pipelines without any model modifications or\nfine-tuning. Experiments on multi-modal benchmarks-including Video QA,\nCaptioning, VisionNIAH, and Mixed-Modality NIAH-with state-of-the-art\nlong-context VLMs (LongVila, LlavaVideo, VideoChat-Flash, Qwen2.5-VL) show that\nMMInference accelerates the pre-filling stage by up to 8.3x at 1M tokens while\nmaintaining accuracy. Our code is available at https://aka.ms/MMInference.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6278bd42541f3d2dfa77ea70/K2pFSyL6PhhzhfK3ar9Ok.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16083.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6278bd42541f3d2dfa77ea70",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6278bd42541f3d2dfa77ea70/ejn49eapnB3UXQckAYdTd.jpeg",
      "fullname": "Huiqiang Jiang",
      "name": "iofu728",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.18589",
      "authors": [
        {
          "_id": "68106d0daa36fca4aeebb34a",
          "user": {
            "_id": "67132d16659c7cd704867365",
            "avatarUrl": "/avatars/cab345716718965b87e29ac248fbc7e4.svg",
            "isPro": false,
            "fullname": "zhikai wang",
            "user": "cloudcatcher2",
            "type": "user"
          },
          "name": "Zhikai Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-29T07:57:55.121Z",
          "hidden": false
        },
        {
          "_id": "68106d0daa36fca4aeebb34b",
          "name": "Jiashuo Sun",
          "hidden": false
        },
        {
          "_id": "68106d0daa36fca4aeebb34c",
          "name": "Wenqi Zhang",
          "hidden": false
        },
        {
          "_id": "68106d0daa36fca4aeebb34d",
          "name": "Zhiqiang Hu",
          "hidden": false
        },
        {
          "_id": "68106d0daa36fca4aeebb34e",
          "name": "Xin Li",
          "hidden": false
        },
        {
          "_id": "68106d0daa36fca4aeebb34f",
          "name": "Fan Wang",
          "hidden": false
        },
        {
          "_id": "68106d0daa36fca4aeebb350",
          "name": "Deli Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-24T06:16:38.000Z",
      "submittedOnDailyAt": "2025-04-29T06:53:11.886Z",
      "title": "Modèles de Mathématiques de Logique pour Marquer Dépendances Visuelles",
      "submittedOnDailyBy": {
        "_id": "67132d16659c7cd704867365",
        "avatarUrl": "/avatars/cab345716718965b87e29ac248fbc7e4.svg",
        "isPro": false,
        "fullname": "zhikai wang",
        "user": "cloudcatcher2",
        "type": "user"
      },
      "summary": "Le développement récent des grands modèles de langue et vision (LVLMs) a considérablement amélioré la capacité d'intégration de l'information visuelle et linguistique, atteignant des compétences spécialisées en reconnaissance d'objets, captation, réponses à des questions visuelles et plus. Cependant, les cadres actuels d'évaluation se concentrent principalement sur l'évaluation du contenu de connaissances, ce qui entraîne une absence d'évaluation des connaissances spécialisées par domaine, des éléments de base de mathématiques et la capacité pour comprendre, intégrer et justifier des concepts visuels. Nous avons identifié des problèmes dans l'évaluation des problèmes de base de mathématiques et reconnu l'importance que les modèles comprennent, intègrent et justifient les relations explicites de dépendance visuelle. Pour aborder ces problèmes, nous avons introduit VCBENCH, un nouveau standard d'évaluation de la raisonnement mathématique basé sur les relations explicites de dépendance visuelle. VCBENCH comprend 6 domaines cognitifs, 1 720 problèmes et 6 697 images (moyenne de 3,9 par problème), garantissant la capacité de justifier avec plusieurs images. Évaluant 26 des LVLMs les plus avancés avec VCBENCH, nous avons observé des différences significatives dans le rendement, et même les modèles les plus avancés ne dépassent pas la précision de 50%. Nos résultats clairement montrent le problème d'intégration de la vision et de la mathématiques et offrent une vision de la possibilité du développement futur des LVLMs.",
      "upvotes": 2,
      "discussionId": "68106d0faa36fca4aeebb3a2",
      "projectPage": "https://alibaba-damo-academy.github.io/VCBench/",
      "githubRepo": "https://github.com/alibaba-damo-academy/VCBench"
    },
    "publishedAt": "2025-04-24T02:16:38.000Z",
    "title": "Benchmarking Multimodal Mathematical Reasoning with Explicit Visual\n  Dependency",
    "summary": "Recent advancements in Large Vision-Language Models (LVLMs) have\nsignificantly enhanced their ability to integrate visual and linguistic\ninformation, achieving near-human proficiency in tasks like object recognition,\ncaptioning, and visual question answering. However, current benchmarks\ntypically focus on knowledge-centric evaluations that assess domain-specific\nexpertise, often neglecting the core ability to reason about fundamental\nmathematical elements and visual concepts. We identify a gap in evaluating\nelementary-level math problems, which rely on explicit visual\ndependencies-requiring models to discern, integrate, and reason across multiple\nimages while incorporating commonsense knowledge, all of which are crucial for\nadvancing toward broader AGI capabilities. To address this gap, we introduce\nVCBENCH, a comprehensive benchmark for multimodal mathematical reasoning with\nexplicit visual dependencies. VCBENCH includes 1,720 problems across six\ncognitive domains, featuring 6,697 images (averaging 3.9 per question) to\nensure multi-image reasoning. We evaluate 26 state-of-the-art LVLMs on VCBENCH,\nrevealing substantial performance disparities, with even the top models unable\nto exceed 50% accuracy. Our findings highlight the ongoing challenges in\nvisual-mathematical integration and suggest avenues for future LVLM\nadvancements.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.18589.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67132d16659c7cd704867365",
      "avatarUrl": "/avatars/cab345716718965b87e29ac248fbc7e4.svg",
      "fullname": "zhikai wang",
      "name": "cloudcatcher2",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.19395",
      "authors": [
        {
          "_id": "6810767a0f244cf14e5a3060",
          "user": {
            "_id": "6675c9305eaa9dd299dcdca0",
            "avatarUrl": "/avatars/504a259033605e489809c8f202538d75.svg",
            "isPro": false,
            "fullname": "Zhouxiang Fang",
            "user": "FocusV857",
            "type": "user"
          },
          "name": "Zhouxiang Fang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-29T08:01:43.304Z",
          "hidden": false
        },
        {
          "_id": "6810767a0f244cf14e5a3061",
          "name": "Aayush Mishra",
          "hidden": false
        },
        {
          "_id": "6810767a0f244cf14e5a3062",
          "name": "Muhan Gao",
          "hidden": false
        },
        {
          "_id": "6810767a0f244cf14e5a3063",
          "name": "Anqi Liu",
          "hidden": false
        },
        {
          "_id": "6810767a0f244cf14e5a3064",
          "name": "Daniel Khashabi",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6675c9305eaa9dd299dcdca0/DAhhWMnvctlmAuzzjp97v.png"
      ],
      "publishedAt": "2025-04-28T00:05:29.000Z",
      "submittedOnDailyAt": "2025-04-29T05:22:18.705Z",
      "title": "ICL CIPHERS : La quantification de la personnalité \"apprentissage\" comme un méthode de chiffrement par apprentissage en contexte",
      "submittedOnDailyBy": {
        "_id": "6675c9305eaa9dd299dcdca0",
        "avatarUrl": "/avatars/504a259033605e489809c8f202538d75.svg",
        "isPro": false,
        "fullname": "Zhouxiang Fang",
        "user": "FocusV857",
        "type": "user"
      },
      "summary": "Les derniers études ont montré que l'Apprentissage en Contexte (ICL) fonctionne de deux manières. Ce processus est composé de la recherche de motifs appris lors de l'entraînement et d'un \"apprentissage\" basé sur des instructions. Cependant, distinguer ces deux modes est un objectif difficile. Nous avons introduit une classe de remplacement de tâches basée sur le chiffrement de messages en anglais, une technique classique de cryptographie. Dans cette approche, nous remplaçons certains tokens des données contextuelles par d'autres non pertinents pour réduire la lisibilité du message en anglais. Cependant, de manière conçue, ce remplacement peut introduire des motifs fixes et être réversible. Ce chiffrement réversible (chiffrement invisible) maintient la tâche dans son sens abstrait, indépendamment des transformations. C'est un problème intéressant d'explorer si les Modèles de Langue Grands (LLMs) peuvent interpréter les CIPHERS d'ICL comme une bijection. Nous montrons que les LLMs peuvent interpréter les CIPHERS d'ICL mieux avec une bijection que avec une ligne basée sur une non-bijection (irréversible), offrant une nouvelle approche pour quantifier l'«apprentissage» d'ICL. Cette erreur est faible mais est confirmée par 4 ensembles de données et 6 modèles à travers le contexte. Enfin, nous avons examiné les représentations internes des LLMs et avons découvert leur capacité à interpréter des entrées chiffrées.",
      "upvotes": 1,
      "discussionId": "6810767b0f244cf14e5a30dc",
      "githubRepo": "https://github.com/jhu-CLSP/icl-ciphers"
    },
    "publishedAt": "2025-04-27T20:05:29.000Z",
    "title": "ICL CIPHERS: Quantifying \"Learning'' in In-Context Learning via\n  Substitution Ciphers",
    "summary": "Recent works have suggested that In-Context Learning (ICL) operates in dual\nmodes, i.e. task retrieval (remember learned patterns from pre-training) and\ntask learning (inference-time ``learning'' from demonstrations). However,\ndisentangling these the two modes remains a challenging goal. We introduce ICL\nCIPHERS, a class of task reformulations based on substitution ciphers borrowed\nfrom classic cryptography. In this approach, a subset of tokens in the\nin-context inputs are substituted with other (irrelevant) tokens, rendering\nEnglish sentences less comprehensible to human eye. However, by design, there\nis a latent, fixed pattern to this substitution, making it reversible. This\nbijective (reversible) cipher ensures that the task remains a well-defined task\nin some abstract sense, despite the transformations. It is a curious question\nif LLMs can solve ICL CIPHERS with a BIJECTIVE mapping, which requires\ndeciphering the latent cipher. We show that LLMs are better at solving ICL\nCIPHERS with BIJECTIVE mappings than the NON-BIJECTIVE (irreversible) baseline,\nproviding a novel approach to quantify ``learning'' in ICL. While this gap is\nsmall, it is consistent across the board on four datasets and six models.\nFinally, we examine LLMs' internal representations and identify evidence in\ntheir ability to decode the ciphered inputs.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6675c9305eaa9dd299dcdca0/DAhhWMnvctlmAuzzjp97v.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.19395.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6675c9305eaa9dd299dcdca0",
      "avatarUrl": "/avatars/504a259033605e489809c8f202538d75.svg",
      "fullname": "Zhouxiang Fang",
      "name": "FocusV857",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.19162",
      "authors": [
        {
          "_id": "6810799e10b86ba322c27fb8",
          "user": {
            "_id": "61b859ddbdf1fac5ed499992",
            "avatarUrl": "/avatars/2387fb9b8a46840bfc75248462f0a410.svg",
            "isPro": false,
            "fullname": "Jiaqi Chen",
            "user": "judge",
            "type": "user"
          },
          "name": "Jiaqi Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-29T07:58:59.273Z",
          "hidden": false
        },
        {
          "_id": "6810799e10b86ba322c27fb9",
          "name": "Bang Zhang",
          "hidden": false
        },
        {
          "_id": "6810799e10b86ba322c27fba",
          "name": "Ruotian Ma",
          "hidden": false
        },
        {
          "_id": "6810799e10b86ba322c27fbb",
          "name": "Peisong Wang",
          "hidden": false
        },
        {
          "_id": "6810799e10b86ba322c27fbc",
          "name": "Xiaodan Liang",
          "hidden": false
        },
        {
          "_id": "6810799e10b86ba322c27fbd",
          "name": "Zhaopeng Tu",
          "hidden": false
        },
        {
          "_id": "6810799e10b86ba322c27fbe",
          "name": "Xiaolong Li",
          "hidden": false
        },
        {
          "_id": "6810799e10b86ba322c27fbf",
          "name": "Kwan-Yee K. Wong",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-27T08:45:06.000Z",
      "submittedOnDailyAt": "2025-04-29T06:46:57.300Z",
      "title": "SPC : Évaluation de la compétence autonome par des jeux adversaires pour l'évolution de l'inférence dans les modèles de langage d'intelligence artificielle",
      "submittedOnDailyBy": {
        "_id": "61b859ddbdf1fac5ed499992",
        "avatarUrl": "/avatars/2387fb9b8a46840bfc75248462f0a410.svg",
        "isPro": false,
        "fullname": "Jiaqi Chen",
        "user": "judge",
        "type": "user"
      },
      "summary": "L'évaluation de la confiance à différentes étapes de modèles de langage grands (LLM) présente des difficultés pour obtenir sous-ensembles de haute qualité à chaque niveau, ainsi que pour l'évaluation d'étapes d'inférence logique comme le Chain-of-Thought. Dans cet article, nous présentons une nouvelle approche appelée Self-Play Critic (SPC), qui permet d'évaluer la précision des étapes logiques sans nécessiter de noter manuellement chaque niveau. SPC évolue sa capacité à évaluer la précision des étapes logiques à travers des jeux compétitifs dans ses propres jeux. SPC est composé de deux modèles micro-ajustés : un \"générateur de snippets\" et un \"évaluateur\". Le \"générateur de snippets\" crée des étapes avec des erreurs intentionnelles et conçues pour être difficiles à détecter, tandis que l'«évaluateur» analyse la précision des étapes logiques. Ces deux modèles se confrontent dans des jeux compétitifs, où le vainqueur reçoit une récompense positive et le perdant une négative. A travers l'apprentissage par refroidissement, les modèles sont améliorés de manière itérative, et chaque vainqueur d'une compétition reçoit une récompense positive tandis que le perdant reçoit une négative. Les résultats des expérimentations sur trois référentiels de traitement logique, ProcessBench, PRM800K et DeltaBench, montrent que SPC améliore significativement la capacité de détection d'erreurs (par exemple, augmentant la précision de 70,8% à 77,7% sur ProcessBench). De plus, SPC guide la recherche dans les tests de différents LLM, améliorant de manière significative le rendement logique mathématique sur MATH500 et AIME2024 et dépassant les modèles de récompense de traitement les plus récents.",
      "upvotes": 1,
      "discussionId": "6810799f10b86ba322c27fea",
      "ai_keywords": [
        "Self-Play Critic (SPC)",
        "adversarial self-play games",
        "\"sneaky generator\"",
        "\"critic\"",
        "reinforcement learning",
        "ProcessBench",
        "PRM800K",
        "DeltaBench",
        "parameter-efficient fine-tuning",
        "distilled R1 model",
        "MATH500",
        "AIME2024"
      ]
    },
    "publishedAt": "2025-04-27T04:45:06.000Z",
    "title": "SPC: Evolving Self-Play Critic via Adversarial Games for LLM Reasoning",
    "summary": "Evaluating the step-by-step reliability of large language model (LLM)\nreasoning, such as Chain-of-Thought, remains challenging due to the difficulty\nand cost of obtaining high-quality step-level supervision. In this paper, we\nintroduce Self-Play Critic (SPC), a novel approach where a critic model evolves\nits ability to assess reasoning steps through adversarial self-play games,\neliminating the need for manual step-level annotation. SPC involves fine-tuning\ntwo copies of a base model to play two roles, namely a \"sneaky generator\" that\ndeliberately produces erroneous steps designed to be difficult to detect, and a\n\"critic\" that analyzes the correctness of reasoning steps. These two models\nengage in an adversarial game in which the generator aims to fool the critic,\nwhile the critic model seeks to identify the generator's errors. Using\nreinforcement learning based on the game outcomes, the models iteratively\nimprove; the winner of each confrontation receives a positive reward and the\nloser receives a negative reward, driving continuous self-evolution.\nExperiments on three reasoning process benchmarks (ProcessBench, PRM800K,\nDeltaBench) demonstrate that our SPC progressively enhances its error detection\ncapabilities (e.g., accuracy increases from 70.8% to 77.7% on ProcessBench) and\nsurpasses strong baselines, including distilled R1 model. Furthermore, applying\nSPC to guide the test-time search of diverse LLMs significantly improves their\nmathematical reasoning performance on MATH500 and AIME2024, outperforming\nstate-of-the-art process reward models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.19162.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "61b859ddbdf1fac5ed499992",
      "avatarUrl": "/avatars/2387fb9b8a46840bfc75248462f0a410.svg",
      "fullname": "Jiaqi Chen",
      "name": "judge",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]