[
  {
    "paper": {
      "id": "2505.02567",
      "authors": [
        {
          "_id": "681c7895c7211b7efbc49f17",
          "name": "Xinjie Zhang",
          "hidden": false
        },
        {
          "_id": "681c7895c7211b7efbc49f18",
          "name": "Jintao Guo",
          "hidden": false
        },
        {
          "_id": "681c7895c7211b7efbc49f19",
          "user": {
            "_id": "66ab4c8a1703f12f49583c6d",
            "avatarUrl": "/avatars/59c77d4556edc049bb410e180813d5e3.svg",
            "isPro": false,
            "fullname": "zss",
            "user": "Suikong",
            "type": "user"
          },
          "name": "Shanshan Zhao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-08T10:07:03.107Z",
          "hidden": false
        },
        {
          "_id": "681c7895c7211b7efbc49f1a",
          "name": "Minghao Fu",
          "hidden": false
        },
        {
          "_id": "681c7895c7211b7efbc49f1b",
          "name": "Lunhao Duan",
          "hidden": false
        },
        {
          "_id": "681c7895c7211b7efbc49f1c",
          "user": {
            "_id": "636f4c6b5d2050767e4a1491",
            "avatarUrl": "/avatars/630c2ae12937fdb16ccd3280bc05729d.svg",
            "isPro": false,
            "fullname": "Guo-Hua Wang",
            "user": "Flourish",
            "type": "user"
          },
          "name": "Guo-Hua Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-08T09:58:04.757Z",
          "hidden": false
        },
        {
          "_id": "681c7895c7211b7efbc49f1d",
          "name": "Qing-Guo Chen",
          "hidden": false
        },
        {
          "_id": "681c7895c7211b7efbc49f1e",
          "name": "Zhao Xu",
          "hidden": false
        },
        {
          "_id": "681c7895c7211b7efbc49f1f",
          "name": "Weihua Luo",
          "hidden": false
        },
        {
          "_id": "681c7895c7211b7efbc49f20",
          "name": "Kaifu Zhang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/658a8a837959448ef5500ce5/DwtDeTTjSrZoe2r4sL-a4.png"
      ],
      "publishedAt": "2025-05-05T11:18:03.000Z",
      "submittedOnDailyAt": "2025-05-08T07:57:47.854Z",
      "title": "La compréhension du développement de la dimension de l'unité et de la génération de la dimension de la création, ainsi que les défis et les opportunités.",
      "submittedOnDailyBy": {
        "_id": "658a8a837959448ef5500ce5",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658a8a837959448ef5500ce5/4rb2RXrsgCDH80VnwZsEt.jpeg",
        "isPro": false,
        "fullname": "Shiyin Lu",
        "user": "runninglsy",
        "type": "user"
      },
      "summary": "Récemment, on observe un progrès extraordinaire dans les deux domaines : le modèle de compréhension multimodal et le modèle de génération d'images. En plus de l'excellence dans chaque domaine, les deux ont été développés de manière indépendante et ont formé des architectures paradigmatiques différentes : l'architecture basée sur la régression automatique est principalement utilisée dans la compréhension multimodal, tandis que l'architecture basée sur la distribution est la base de la génération d'images. Dans les derniers temps, l'intérêt pour le développement de cadres de travail qui intègrent les deux domaines a augmenté. Les nouvelles fonctions de GPT-4 sont un exemple de ce tendance vers l'intégration. Cependant, les différences architecturales entre ces domaines posent de grands défis. Cette recherche présente les efforts actuels pour clairement présenter les problèmes et guider les futures recherches. Tout d'abord, les concepts fondamentaux et les avancées récentes en compréhension multimodal et dans les modèles de génération d'images à partir de texte sont présentés. Ensuite, les modèles intégrés sont classifiés en trois architectures principales et les innovations en conception et en recherche sont analysées. De plus, des ensembles de données et des cadres d'évaluation appropriés pour les modèles intégrés sont rassemblés, fournissant des ressources pour les discussions futures. Enfin, les principales questions dans ce nouveau domaine sont discutées, y compris les stratégies de tokenisation, l'attention croisée modal et les données. Ce domaine se trouve dans une étape initiale et on attend un progrès rapide, avec cette recherche mise à jour régulièrement. Notre objectif est de favoriser la recherche et de fournir des ressources efficaces à la communauté. Les références liées à cette recherche peuvent être accédées sur GitHub (https://github.com/AIDC-AI/Awesome-Unified-Multimodal-Models).",
      "upvotes": 33,
      "discussionId": "681c7896c7211b7efbc49f76",
      "githubRepo": "https://github.com/AIDC-AI/Awesome-Unified-Multimodal-Models",
      "ai_keywords": [
        "autoregressive-based architectures",
        "diffusion-based models",
        "unified frameworks",
        "GPT-4o",
        "multimodal understanding",
        "text-to-image generation models",
        "diffusion-based",
        "autoregressive-based",
        "hybrid approaches",
        "cross-modal attention"
      ]
    },
    "publishedAt": "2025-05-05T07:18:03.000Z",
    "title": "Unified Multimodal Understanding and Generation Models: Advances,\n  Challenges, and Opportunities",
    "summary": "Recent years have seen remarkable progress in both multimodal understanding\nmodels and image generation models. Despite their respective successes, these\ntwo domains have evolved independently, leading to distinct architectural\nparadigms: While autoregressive-based architectures have dominated multimodal\nunderstanding, diffusion-based models have become the cornerstone of image\ngeneration. Recently, there has been growing interest in developing unified\nframeworks that integrate these tasks. The emergence of GPT-4o's new\ncapabilities exemplifies this trend, highlighting the potential for\nunification. However, the architectural differences between the two domains\npose significant challenges. To provide a clear overview of current efforts\ntoward unification, we present a comprehensive survey aimed at guiding future\nresearch. First, we introduce the foundational concepts and recent advancements\nin multimodal understanding and text-to-image generation models. Next, we\nreview existing unified models, categorizing them into three main architectural\nparadigms: diffusion-based, autoregressive-based, and hybrid approaches that\nfuse autoregressive and diffusion mechanisms. For each category, we analyze the\nstructural designs and innovations introduced by related works. Additionally,\nwe compile datasets and benchmarks tailored for unified models, offering\nresources for future exploration. Finally, we discuss the key challenges facing\nthis nascent field, including tokenization strategy, cross-modal attention, and\ndata. As this area is still in its early stages, we anticipate rapid\nadvancements and will regularly update this survey. Our goal is to inspire\nfurther research and provide a valuable reference for the community. The\nreferences associated with this survey are available on GitHub\n(https://github.com/AIDC-AI/Awesome-Unified-Multimodal-Models).",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/658a8a837959448ef5500ce5/DwtDeTTjSrZoe2r4sL-a4.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02567.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "658a8a837959448ef5500ce5",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658a8a837959448ef5500ce5/4rb2RXrsgCDH80VnwZsEt.jpeg",
      "fullname": "Shiyin Lu",
      "name": "runninglsy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.04588",
      "authors": [
        {
          "_id": "681c15ab84d0a008fcdb1ee8",
          "name": "Hao Sun",
          "hidden": false
        },
        {
          "_id": "681c15ab84d0a008fcdb1ee9",
          "name": "Zile Qiao",
          "hidden": false
        },
        {
          "_id": "681c15ab84d0a008fcdb1eea",
          "user": {
            "_id": "66224557c61c7fbd98099079",
            "avatarUrl": "/avatars/a4f2144585c808865c73b5b7f0087c1f.svg",
            "isPro": false,
            "fullname": "GJ",
            "user": "SpaceProduct",
            "type": "user"
          },
          "name": "Jiayan Guo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-08T08:59:01.834Z",
          "hidden": false
        },
        {
          "_id": "681c15ab84d0a008fcdb1eeb",
          "name": "Xuanbo Fan",
          "hidden": false
        },
        {
          "_id": "681c15ab84d0a008fcdb1eec",
          "name": "Yingyan Hou",
          "hidden": false
        },
        {
          "_id": "681c15ab84d0a008fcdb1eed",
          "name": "Yong Jiang",
          "hidden": false
        },
        {
          "_id": "681c15ab84d0a008fcdb1eee",
          "name": "Pengjun Xie",
          "hidden": false
        },
        {
          "_id": "681c15ab84d0a008fcdb1eef",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "681c15ab84d0a008fcdb1ef0",
          "name": "Yan Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-07T17:30:22.000Z",
      "submittedOnDailyAt": "2025-05-08T00:54:07.103Z",
      "title": "ZeroSearch : Il n'est pas nécessaire de faire de recherche pour améliorer la capacité de recherche.",
      "submittedOnDailyBy": {
        "_id": "66224557c61c7fbd98099079",
        "avatarUrl": "/avatars/a4f2144585c808865c73b5b7f0087c1f.svg",
        "isPro": false,
        "fullname": "GJ",
        "user": "SpaceProduct",
        "type": "user"
      },
      "summary": "La recherche d'information efficace est cruciale pour améliorer les capacités de logique et de génération des grands modèles de langue (LLMs). Dans les derniers études, l'apprentissage cognitif par renforcement (RL) a été revu pour améliorer la capacité de recherche des LLMs dans des environnements réels en interagissant avec des moteurs de recherche en temps réel. Cette approche montre des résultats attendus, mais présente deux problèmes majeurs : (1) le contrôle de la qualité des documents est impossible : la qualité des documents retournés par le moteur de recherche est difficile à prédire et entraîne des bruits et des instabilités dans le processus d'apprentissage ; (2) l'augmentation drastique des coûts des API : l'apprentissage par RL nécessite des téléchargements fréquents et des millions de requêtes de recherche, ce qui signifie un augmentation significative des coûts des API et limite l'échelle. Pour résoudre ces problèmes, nous présentons le cadre d'apprentissage cognitif par renforcement ZeroSearch. Cette approche vise à améliorer la capacité de recherche des LLMs sans interagir avec des moteurs de recherche dans des environnements réels. Notre approche commence par un ajustement léger supervisé pour transformer les LLMs en modules de recherche. Ensuite, lors de l'apprentissage par RL, une stratégie de téléchargements basée sur des collecteurs est utilisée, et la qualité progressivement des documents générés est éliminée, exposant le modèle à des scénarios de recherche qui le rendent difficiles pour développer sa capacité de logique. Des expériences extensives montrent que ZeroSearch est efficace pour améliorer la capacité de recherche d'un LLM de 3B, atteignant un rendement comparable à un moteur de recherche réel avec un module de 7B et le dépassant avec un module de 14B. De plus, il est possible d'étendre ces modèles tant les modèles de base que les modèles d'apprentissage d'instance avec des tailles de paramètres différentes, et il a une bonne compatibilité avec une large gamme d'algorithmes de RL.",
      "upvotes": 23,
      "discussionId": "681c15ac84d0a008fcdb1f21",
      "projectPage": "https://alibaba-nlp.github.io/ZeroSearch/",
      "githubRepo": "https://github.com/Alibaba-nlp/ZeroSearch",
      "ai_keywords": [
        "reinforcement learning (RL)",
        "large language models (LLMs)",
        "search capabilities",
        "live search engines",
        "real-world environments",
        "document quality",
        "noise",
        "instability",
        "training process",
        "API costs",
        "rollouts",
        "search requests",
        "ZeroSearch",
        "lightweight supervised fine-tuning",
        "retrieval module",
        "relevant documents",
        "noisy documents",
        "query",
        "curriculum-based rollout strategy",
        "reasoning ability",
        "retrieval scenarios",
        "base models",
        "instruction-tuned models",
        "parameter sizes",
        "RL algorithms"
      ]
    },
    "publishedAt": "2025-05-07T13:30:22.000Z",
    "title": "ZeroSearch: Incentivize the Search Capability of LLMs without Searching",
    "summary": "Effective information searching is essential for enhancing the reasoning and\ngeneration capabilities of large language models (LLMs). Recent research has\nexplored using reinforcement learning (RL) to improve LLMs' search capabilities\nby interacting with live search engines in real-world environments. While these\napproaches show promising results, they face two major challenges: (1)\nUncontrolled Document Quality: The quality of documents returned by search\nengines is often unpredictable, introducing noise and instability into the\ntraining process. (2) Prohibitively High API Costs: RL training requires\nfrequent rollouts, potentially involving hundreds of thousands of search\nrequests, which incur substantial API expenses and severely constrain\nscalability. To address these challenges, we introduce ZeroSearch, a\nreinforcement learning framework that incentivizes the search capabilities of\nLLMs without interacting with real search engines. Our approach begins with\nlightweight supervised fine-tuning to transform the LLM into a retrieval module\ncapable of generating both relevant and noisy documents in response to a query.\nDuring RL training, we employ a curriculum-based rollout strategy that\nincrementally degrades the quality of generated documents, progressively\neliciting the model's reasoning ability by exposing it to increasingly\nchallenging retrieval scenarios. Extensive experiments demonstrate that\nZeroSearch effectively incentivizes the search capabilities of LLMs using a 3B\nLLM as the retrieval module. Remarkably, a 7B retrieval module achieves\ncomparable performance to the real search engine, while a 14B retrieval module\neven surpasses it. Furthermore, it generalizes well across both base and\ninstruction-tuned models of various parameter sizes and is compatible with a\nwide range of RL algorithms.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.04588.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66224557c61c7fbd98099079",
      "avatarUrl": "/avatars/a4f2144585c808865c73b5b7f0087c1f.svg",
      "fullname": "GJ",
      "name": "SpaceProduct",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.04512",
      "authors": [
        {
          "_id": "681c546817fc8222efed5318",
          "name": "Teng Hu",
          "hidden": false
        },
        {
          "_id": "681c546817fc8222efed5319",
          "name": "Zhentao Yu",
          "hidden": false
        },
        {
          "_id": "681c546817fc8222efed531a",
          "name": "Zhengguang Zhou",
          "hidden": false
        },
        {
          "_id": "681c546817fc8222efed531b",
          "name": "Sen Liang",
          "hidden": false
        },
        {
          "_id": "681c546817fc8222efed531c",
          "name": "Yuan Zhou",
          "hidden": false
        },
        {
          "_id": "681c546817fc8222efed531d",
          "name": "Qin Lin",
          "hidden": false
        },
        {
          "_id": "681c546817fc8222efed531e",
          "name": "Qinglin Lu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-07T15:33:18.000Z",
      "submittedOnDailyAt": "2025-05-08T05:21:39.978Z",
      "title": "Formas de Customización : Architecture de la Direction par la Diversité des Vidéos Personnalisées",
      "submittedOnDailyBy": {
        "_id": "63468720dd6d90d82ccf3450",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
        "isPro": false,
        "fullname": "YSH",
        "user": "BestWishYsh",
        "type": "user"
      },
      "summary": "La génération de vidéos personnalisées a pour objectif de créer des vidéos qui incluent un thème spécifique sous des conditions définies de manière flexible par l'utilisateur. Cependant, les méthodes actuelles rencontrent des défis en termes de cohérence d'identité et de limitations du modèle d'entrée. Dans cet article, nous proposons le cadre de travail HunyuanCustom, un système basé sur plusieurs modèles pour la génération de vidéos personnalisées. Ce cadre vise à prioriser la cohérence du thème, en soutenant des conditions d'image, de voix, de vidéo et de texte. Le modèle construit dans HunyuanVideo introduit un module de fusion texte-image basé sur LLaVA et un module de renforcement de l'ID d'image pour aborder la tâche de génération conditionnée par image et texte. De plus, nous proposons une structure d'injection de conditions basée sur des modèles pour permettre la génération conditionnée par voix et vidéo. Le module AudioNet implémente une attention spatiale pour effectuer un focalisation d'attention hiérarchique, tandis que le module de génération conditionné par vidéo utilise une réseau de correspondance de caractéristiques basée sur des blocs pour intégrer des vidéos conditionnées de manière potentielle. Les expériences sur des thèmes uniques et multiples montrent que les méthodes actuelles d'identité, de réalisme et de focalisation texte-vidéo sont significativement dépassées par les méthodes avancées ouvertes et fermées. De plus, nous validons l'association avec des tâches de téléchargement et nous montrons une excellente performance dans la génération personnalisée de vidéos avec voix et mouvement. Les résultats démontrent que la génération conditionnée de plusieurs modèles et la stratégie de maintien de l'identité sont essentielles pour le développement de la génération de vidéos contrôlables. Tout le code et les modèles sont disponibles sur https://hunyuancustom.github.io.",
      "upvotes": 8,
      "discussionId": "681c546e17fc8222efed54ce",
      "ai_keywords": [
        "LLaVA",
        "text-image fusion module",
        "image ID enhancement module",
        "temporal concatenation",
        "modality-specific condition injection mechanisms",
        "AudioNet module",
        "spatial cross-attention",
        "video-driven injection module",
        "latent-compressed conditional video",
        "patchify-based feature-alignment network",
        "ID consistency",
        "text-video alignment",
        "controllable video generation"
      ]
    },
    "publishedAt": "2025-05-07T11:33:18.000Z",
    "title": "HunyuanCustom: A Multimodal-Driven Architecture for Customized Video\n  Generation",
    "summary": "Customized video generation aims to produce videos featuring specific\nsubjects under flexible user-defined conditions, yet existing methods often\nstruggle with identity consistency and limited input modalities. In this paper,\nwe propose HunyuanCustom, a multi-modal customized video generation framework\nthat emphasizes subject consistency while supporting image, audio, video, and\ntext conditions. Built upon HunyuanVideo, our model first addresses the\nimage-text conditioned generation task by introducing a text-image fusion\nmodule based on LLaVA for enhanced multi-modal understanding, along with an\nimage ID enhancement module that leverages temporal concatenation to reinforce\nidentity features across frames. To enable audio- and video-conditioned\ngeneration, we further propose modality-specific condition injection\nmechanisms: an AudioNet module that achieves hierarchical alignment via spatial\ncross-attention, and a video-driven injection module that integrates\nlatent-compressed conditional video through a patchify-based feature-alignment\nnetwork. Extensive experiments on single- and multi-subject scenarios\ndemonstrate that HunyuanCustom significantly outperforms state-of-the-art open-\nand closed-source methods in terms of ID consistency, realism, and text-video\nalignment. Moreover, we validate its robustness across downstream tasks,\nincluding audio and video-driven customized video generation. Our results\nhighlight the effectiveness of multi-modal conditioning and identity-preserving\nstrategies in advancing controllable video generation. All the code and models\nare available at https://hunyuancustom.github.io.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.04512.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "63468720dd6d90d82ccf3450",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
      "fullname": "YSH",
      "name": "BestWishYsh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 50
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.04622",
      "authors": [
        {
          "_id": "681c03418ff29a163ef5f370",
          "name": "Jingwen Ye",
          "hidden": false
        },
        {
          "_id": "681c03418ff29a163ef5f371",
          "user": {
            "_id": "64c903957b4d0d947ce86bc6",
            "avatarUrl": "/avatars/61d70a3ba00c83a5950f5c909a1a06f8.svg",
            "isPro": false,
            "fullname": "Yuze He",
            "user": "hyz317",
            "type": "user"
          },
          "name": "Yuze He",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-08T08:59:10.350Z",
          "hidden": false
        },
        {
          "_id": "681c03418ff29a163ef5f372",
          "name": "Yanning Zhou",
          "hidden": false
        },
        {
          "_id": "681c03418ff29a163ef5f373",
          "name": "Yiqin Zhu",
          "hidden": false
        },
        {
          "_id": "681c03418ff29a163ef5f374",
          "user": {
            "_id": "6441491c5d600fb0951cd872",
            "avatarUrl": "/avatars/d98892f3b52d87c2328201efa9897110.svg",
            "isPro": false,
            "fullname": "Kaiwen Xiao",
            "user": "loktarxiao",
            "type": "user"
          },
          "name": "Kaiwen Xiao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-08T08:59:12.445Z",
          "hidden": false
        },
        {
          "_id": "681c03418ff29a163ef5f375",
          "name": "Yong-Jin Liu",
          "hidden": false
        },
        {
          "_id": "681c03418ff29a163ef5f376",
          "name": "Wei Yang",
          "hidden": false
        },
        {
          "_id": "681c03418ff29a163ef5f377",
          "name": "Xiao Han",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-07T17:59:46.000Z",
      "submittedOnDailyAt": "2025-05-08T05:41:14.360Z",
      "title": "Primitive Aniki : Génération d'Assemblées 3D de Primitifs Humanisés avec Transformateur Auto-régressif",
      "submittedOnDailyBy": {
        "_id": "64c903957b4d0d947ce86bc6",
        "avatarUrl": "/avatars/61d70a3ba00c83a5950f5c909a1a06f8.svg",
        "isPro": false,
        "fullname": "Yuze He",
        "user": "hyz317",
        "type": "user"
      },
      "summary": "La abstraction d'éléments essentiels joue un rôle crucial dans la décomposition de modèles 3D complexes en éléments géométriques simples, influençant significativement la perception visuelle humaine et présentant une large gamme d'applications dans le domaine de la vision par ordinateur et de la graphique. Bien que le développement récent du contenu 3D ait démontré une évolution impressionnante, les méthodes actuelles d'abstraction d'éléments dépendent d'optimisations géométriques et ont une compréhension limitée de la signification, étant entraînées sur de petits ensembles de données par classe, ce qui rend difficile la généralisation entre différentes classes de formes. Nous présentons un nouveau cadre de travail appelé PrimitiveAnything. Ce cadre redefinit l'abstraction d'éléments comme une tâche de génération de combinaisons d'éléments, incluant également la transition d'éléments qui comprend des conditions de forme. PrimitiveAnything inclut un méthode de paramétrisation sans erreur qui représente de manière cohérente différents types d'éléments, ainsi qu'un méthode de transition d'éléments pour générer automatiquement des combinaisons d'éléments. Le cadre proposé permet au système d'apprendre directement les combinaisons d'éléments à travers de grandes abstractions humaines, ce qui permet de comprendre comment les personnes décomposent des formes complexes en éléments. Au travers d'expériences extensives, PrimitiveAnything a démontré sa capacité à maintenir la précision géométrique dans différentes classes de formes, générant des combinaisons d'éléments qui se alignent avec les observations humaines, et a démontré son potentiel dans divers applications de projets 3D, ainsi que dans la possibilité de contenu basé sur des éléments dans le jeu. Page du projet : https://primitiveanything.github.io",
      "upvotes": 7,
      "discussionId": "681c03468ff29a163ef5f4d7",
      "projectPage": "https://primitiveanything.github.io/",
      "githubRepo": "https://github.com/PrimitiveAnything/PrimitiveAnything",
      "ai_keywords": [
        "shape primitive abstraction",
        "geometric elements",
        "human visual cognition",
        "computer vision",
        "graphics",
        "3D content generation",
        "geometric optimization",
        "semantic understanding",
        "category-specific datasets",
        "primitive assembly generation task",
        "shape-conditioned primitive transformer",
        "auto-regressive generation",
        "ambiguity-free parameterization scheme",
        "human-crafted abstractions",
        "high-quality primitive assemblies",
        "human perception",
        "geometric fidelity",
        "3D applications",
        "user-generated content (UGC)"
      ]
    },
    "publishedAt": "2025-05-07T13:59:46.000Z",
    "title": "PrimitiveAnything: Human-Crafted 3D Primitive Assembly Generation with\n  Auto-Regressive Transformer",
    "summary": "Shape primitive abstraction, which decomposes complex 3D shapes into simple\ngeometric elements, plays a crucial role in human visual cognition and has\nbroad applications in computer vision and graphics. While recent advances in 3D\ncontent generation have shown remarkable progress, existing primitive\nabstraction methods either rely on geometric optimization with limited semantic\nunderstanding or learn from small-scale, category-specific datasets, struggling\nto generalize across diverse shape categories. We present PrimitiveAnything, a\nnovel framework that reformulates shape primitive abstraction as a primitive\nassembly generation task. PrimitiveAnything includes a shape-conditioned\nprimitive transformer for auto-regressive generation and an ambiguity-free\nparameterization scheme to represent multiple types of primitives in a unified\nmanner. The proposed framework directly learns the process of primitive\nassembly from large-scale human-crafted abstractions, enabling it to capture\nhow humans decompose complex shapes into primitive elements. Through extensive\nexperiments, we demonstrate that PrimitiveAnything can generate high-quality\nprimitive assemblies that better align with human perception while maintaining\ngeometric fidelity across diverse shape categories. It benefits various 3D\napplications and shows potential for enabling primitive-based user-generated\ncontent (UGC) in games. Project page: https://primitiveanything.github.io",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.04622.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64c903957b4d0d947ce86bc6",
      "avatarUrl": "/avatars/61d70a3ba00c83a5950f5c909a1a06f8.svg",
      "fullname": "Yuze He",
      "name": "hyz317",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.04364",
      "authors": [
        {
          "_id": "681c189c791c72783efe5a94",
          "user": {
            "_id": "6205fefd3f1dc8a642d70b10",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1673948194938-6205fefd3f1dc8a642d70b10.jpeg",
            "isPro": false,
            "fullname": "Kai Ruan",
            "user": "6cf",
            "type": "user"
          },
          "name": "Kai Ruan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-08T08:58:58.134Z",
          "hidden": false
        },
        {
          "_id": "681c189c791c72783efe5a95",
          "name": "Mowen Huang",
          "hidden": false
        },
        {
          "_id": "681c189c791c72783efe5a96",
          "name": "Ji-Rong Wen",
          "hidden": false
        },
        {
          "_id": "681c189c791c72783efe5a97",
          "name": "Hao Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-07T12:32:01.000Z",
      "submittedOnDailyAt": "2025-05-08T01:06:26.256Z",
      "title": "Benchmark du Connaissance Collaborative de LLM",
      "submittedOnDailyBy": {
        "_id": "6205fefd3f1dc8a642d70b10",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1673948194938-6205fefd3f1dc8a642d70b10.jpeg",
        "isPro": false,
        "fullname": "Kai Ruan",
        "user": "6cf",
        "type": "user"
      },
      "summary": "Les modèles de langage grands (LLMs) montrent la possibilité de logiques complexes, mais la fonction de la collaboration épisodique dans les systèmes d'agents multi (MAS) sous des caractéristiques spécifiques des groupes naturels (par exemple, la cognition et la communication localement limitées) a été peu étudiée. Les benchmarks actuels ne parviennent pas à comprendre complètement les problèmes caractéristiques de la collaboration distribuée lorsque les agents agissent en se basant sur des informations spatiales-temporelles incomplètes. Pour combler cette lacune, on introduit SwarmBench, un nouveau benchmark qui évalue la capacité de connaissance collective des LLMs en tant qu'agents distribués. SwarmBench présente 5 tâches de collaboration basiques sur un réseau 2D, conçues pour que les agents dépendent principalement de perceptions locales (une zone visuelle de k×k) et de communication locale. Des indicateurs pour l'efficacité de la collaboration sont proposés, et les dynamiques de la collectivité épisodique sont analysées. Plusieurs LLMs avancés sont évalués dans 0 shot, observant des différences significatives pour chaque tâche. Les problèmes découlant de la limitation de l'information locale sont clairement mis en évidence. La collaboration épisodique apparaît, mais est limitée dans la formation de plans et stratégies robustes face à l'incertitude. Évaluer le rendement des LLMs dans des conditions collectives est crucial pour l'implémentation de futurs systèmes distribués. SwarmBench est construit comme un système physique échellable avec des caractéristiques mécaniques prédéfinies, offrant des environnements, des prompts, des scripts d'évaluation et des ensembles de données expérimentaux détaillés, contribuant à la recherche reproductible de la collaboration basée sur les MAS et à la base théorique des MAS. Le dépôt de code est disponible sur https://github.com/x66ccff/swarmbench.",
      "upvotes": 7,
      "discussionId": "681c189e791c72783efe5b2d",
      "githubRepo": "https://github.com/x66ccff/swarmbench",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "Multi-Agent Systems (MAS)",
        "swarm intelligence",
        "decentralized coordination",
        "spatio-temporal information",
        "SwarmBench",
        "foundational MAS coordination tasks",
        "2D grid environment",
        "local sensory input",
        "local communication",
        "coordination effectiveness",
        "emergent group dynamics",
        "zero-shot setting",
        "robust planning",
        "strategy formation",
        "uncertainty",
        "decentralized scenarios",
        "Embodied MAS"
      ]
    },
    "publishedAt": "2025-05-07T08:32:01.000Z",
    "title": "Benchmarking LLMs' Swarm intelligence",
    "summary": "Large Language Models (LLMs) show potential for complex reasoning, yet their\ncapacity for emergent coordination in Multi-Agent Systems (MAS) when operating\nunder strict constraints-such as limited local perception and communication,\ncharacteristic of natural swarms-remains largely unexplored, particularly\nconcerning the nuances of swarm intelligence. Existing benchmarks often do not\nfully capture the unique challenges of decentralized coordination that arise\nwhen agents operate with incomplete spatio-temporal information. To bridge this\ngap, we introduce SwarmBench, a novel benchmark designed to systematically\nevaluate the swarm intelligence capabilities of LLMs acting as decentralized\nagents. SwarmBench features five foundational MAS coordination tasks within a\nconfigurable 2D grid environment, forcing agents to rely primarily on local\nsensory input (k x k view) and local communication. We propose metrics for\ncoordination effectiveness and analyze emergent group dynamics. Evaluating\nseveral leading LLMs in a zero-shot setting, we find significant performance\nvariations across tasks, highlighting the difficulties posed by local\ninformation constraints. While some coordination emerges, results indicate\nlimitations in robust planning and strategy formation under uncertainty in\nthese decentralized scenarios. Assessing LLMs under swarm-like conditions is\ncrucial for realizing their potential in future decentralized systems. We\nrelease SwarmBench as an open, extensible toolkit-built upon a customizable and\nscalable physical system with defined mechanical properties. It provides\nenvironments, prompts, evaluation scripts, and the comprehensive experimental\ndatasets generated, aiming to foster reproducible research into LLM-based MAS\ncoordination and the theoretical underpinnings of Embodied MAS. Our code\nrepository is available at https://github.com/x66ccff/swarmbench.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.04364.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "6205fefd3f1dc8a642d70b10",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1673948194938-6205fefd3f1dc8a642d70b10.jpeg",
      "fullname": "Kai Ruan",
      "name": "6cf",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.04528",
      "authors": [
        {
          "_id": "681c5152c7211b7efbba4b73",
          "user": {
            "_id": "641aef7b1911d3be67425338",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/641aef7b1911d3be67425338/CmCbWWB6NxkAaus59q31w.jpeg",
            "isPro": false,
            "fullname": "Qi Liu",
            "user": "purewhite42",
            "type": "user"
          },
          "name": "Qi Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-08T08:58:55.624Z",
          "hidden": false
        },
        {
          "_id": "681c5152c7211b7efbba4b74",
          "name": "Xinhao Zheng",
          "hidden": false
        },
        {
          "_id": "681c5152c7211b7efbba4b75",
          "name": "Renqiu Xia",
          "hidden": false
        },
        {
          "_id": "681c5152c7211b7efbba4b76",
          "name": "Xingzhi Qi",
          "hidden": false
        },
        {
          "_id": "681c5152c7211b7efbba4b77",
          "name": "Qinxiang Cao",
          "hidden": false
        },
        {
          "_id": "681c5152c7211b7efbba4b78",
          "name": "Junchi Yan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-07T16:02:14.000Z",
      "submittedOnDailyAt": "2025-05-08T05:14:50.449Z",
      "title": "Identification et résolution de problèmes par formalisation, cadres de travail et benchmarks officiels",
      "submittedOnDailyBy": {
        "_id": "65b7ae76768464877cdb2e39",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65b7ae76768464877cdb2e39/uAWPo4tpkqbZoDeEkc7y0.jpeg",
        "isPro": false,
        "fullname": "Renqiu Xia",
        "user": "renqiux0302",
        "type": "user"
      },
      "summary": "La résolution de problèmes est une tâche claire en soi et une composante importante de la science et de l'ingénierie. Cependant, il manque une quantité suffisante de règles spécifiques pour la résolution de problèmes. Avec le développement récent des agents basés sur l'IA pour la résolution de problèmes, un intérêt croissant s'est développé pour la possibilité de vérifier le processus de résolution de problèmes au niveau de processus, ce qui n'a pas encore été suffisamment étudié. Pour compléter ces lacunes, nous proposons de mettre en place des principes de base de résolution de problèmes en termes de processus déterministes Markoviens (MDP) et FPS (Formal Problem-Solving). FPS est un nouveau cadre de travail pour la résolution de problèmes vérifiée basée sur l'environnement de FTP (Formal Theorem Proving). De plus, dans D-FPS (Deductive FPS), la vérification de la solution est séparée de la solution elle-même, avec l'objectif de rendre la résolution plus humaine. L'expressivité, la précision et la complétude de ces cadres de travail ont été démontrées. Nous avons construit trois cadres de référence pour la résolution de problèmes : FormalMath500 est une partie formalisée du benchmark MATH500. MiniF2F-Solving et PutnamBench-Solving sont des versions améliorées des benchmarks FTP MiniF2F et PutnamBench. Nous proposons RPE (Restricted Propositional Equivalence), un approche syntaxique utilisant des démonstrations formelles, avec l'objectif de évaluer avec précision, analytiquement et de manière humaine. RPE est un méthode pour déterminer la précision de la réponse par des démonstrations formelles. Sur la base de 4 modèles communs de FTP et 2 méthodes de programmation avancées, FPS peut résoudre jusqu'à 23,77% du maximum de FormalMath500, 27,47% du maximum de MiniF2F-Solving et 0,31% du maximum de PutnamBench-Solving.",
      "upvotes": 5,
      "discussionId": "681c5153c7211b7efbba4bb4",
      "githubRepo": "https://github.com/Purewhite2019/formal_problem_solving_main",
      "ai_keywords": [
        "Markov decision process",
        "FPS (Formal Problem-Solving)",
        "FTP (formal theorem proving)",
        "D-FPS (Deductive FPS)",
        "FormalMath500",
        "MiniF2F-Solving",
        "PutnamBench-Solving",
        "RPE (Restricted Propositional Equivalence)"
      ]
    },
    "publishedAt": "2025-05-07T12:02:14.000Z",
    "title": "Beyond Theorem Proving: Formulation, Framework and Benchmark for Formal\n  Problem-Solving",
    "summary": "As a seemingly self-explanatory task, problem-solving has been a significant\ncomponent of science and engineering. However, a general yet concrete\nformulation of problem-solving itself is missing. With the recent development\nof AI-based problem-solving agents, the demand for process-level verifiability\nis rapidly increasing yet underexplored. To fill these gaps, we present a\nprincipled formulation of problem-solving as a deterministic Markov decision\nprocess; a novel framework, FPS (Formal Problem-Solving), which utilizes\nexisting FTP (formal theorem proving) environments to perform process-verified\nproblem-solving; and D-FPS (Deductive FPS), decoupling solving and answer\nverification for better human-alignment. The expressiveness, soundness and\ncompleteness of the frameworks are proven. We construct three benchmarks on\nproblem-solving: FormalMath500, a formalization of a subset of the MATH500\nbenchmark; MiniF2F-Solving and PutnamBench-Solving, adaptations of FTP\nbenchmarks MiniF2F and PutnamBench. For faithful, interpretable, and\nhuman-aligned evaluation, we propose RPE (Restricted Propositional\nEquivalence), a symbolic approach to determine the correctness of answers by\nformal verification. We evaluate four prevalent FTP models and two prompting\nmethods as baselines, solving at most 23.77% of FormalMath500, 27.47% of\nMiniF2F-Solving, and 0.31% of PutnamBench-Solving.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.04528.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65b7ae76768464877cdb2e39",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65b7ae76768464877cdb2e39/uAWPo4tpkqbZoDeEkc7y0.jpeg",
      "fullname": "Renqiu Xia",
      "name": "renqiux0302",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.03912",
      "authors": [
        {
          "_id": "681c549cb322a2fe864c8b0d",
          "name": "Can Cui",
          "hidden": false
        },
        {
          "_id": "681c549cb322a2fe864c8b0e",
          "name": "Pengxiang Ding",
          "hidden": false
        },
        {
          "_id": "681c549cb322a2fe864c8b0f",
          "name": "Wenxuan Song",
          "hidden": false
        },
        {
          "_id": "681c549cb322a2fe864c8b10",
          "name": "Shuanghao Bai",
          "hidden": false
        },
        {
          "_id": "681c549cb322a2fe864c8b11",
          "name": "Xinyang Tong",
          "hidden": false
        },
        {
          "_id": "681c549cb322a2fe864c8b12",
          "name": "Zirui Ge",
          "hidden": false
        },
        {
          "_id": "681c549cb322a2fe864c8b13",
          "name": "Runze Suo",
          "hidden": false
        },
        {
          "_id": "681c549cb322a2fe864c8b14",
          "name": "Wanqi Zhou",
          "hidden": false
        },
        {
          "_id": "681c549cb322a2fe864c8b15",
          "name": "Yang Liu",
          "hidden": false
        },
        {
          "_id": "681c549cb322a2fe864c8b16",
          "name": "Bofang Jia",
          "hidden": false
        },
        {
          "_id": "681c549cb322a2fe864c8b17",
          "name": "Han Zhao",
          "hidden": false
        },
        {
          "_id": "681c549cb322a2fe864c8b18",
          "name": "Siteng Huang",
          "hidden": false
        },
        {
          "_id": "681c549cb322a2fe864c8b19",
          "name": "Donglin Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-06T18:35:07.000Z",
      "submittedOnDailyAt": "2025-05-08T05:23:33.004Z",
      "title": "OpenHelix : Service court, analyse expérimentale, code ouvert\nModèle de système double VLA pour la manipulation de robots",
      "submittedOnDailyBy": {
        "_id": "65fd82762bf2cd20ddaa193f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/yBYbWp_mT7UusYdkqtAvw.png",
        "isPro": false,
        "fullname": "Siteng Huang",
        "user": "huangsiteng",
        "type": "user"
      },
      "summary": "La architecture VLA (Vision-Langue-Action) des systèmes doubles a transformé en un sujet d'intérêt croissant dans la recherche en intelligence artificielle, mais manque de travaux de code ouverts suffisamment pour l'analyse et l'optimisation de leur performance avancée. Pour aborder ce problème, dans ce travail, on résume le design de l'architecture existante des systèmes doubles et on effectue une évaluation expérimentale systématique de ses éléments de design fondamentaux. Enfin, on propose de fournir un modèle ouvert de faible coût pour des explorations avancées. Naturellement, ce projet continuera d'ajouter des résultats d'expériences et des modèles de code ouverts qui améliorent la performance, de manière que tout le monde puisse les utiliser comme options disponibles. Page du projet : https://openhelix-robot.github.io/.",
      "upvotes": 3,
      "discussionId": "681c549eb322a2fe864c8b6e"
    },
    "publishedAt": "2025-05-06T14:35:07.000Z",
    "title": "OpenHelix: A Short Survey, Empirical Analysis, and Open-Source\n  Dual-System VLA Model for Robotic Manipulation",
    "summary": "Dual-system VLA (Vision-Language-Action) architectures have become a hot\ntopic in embodied intelligence research, but there is a lack of sufficient\nopen-source work for further performance analysis and optimization. To address\nthis problem, this paper will summarize and compare the structural designs of\nexisting dual-system architectures, and conduct systematic empirical\nevaluations on the core design elements of existing dual-system architectures.\nUltimately, it will provide a low-cost open-source model for further\nexploration. Of course, this project will continue to update with more\nexperimental conclusions and open-source models with improved performance for\neveryone to choose from. Project page: https://openhelix-robot.github.io/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03912.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65fd82762bf2cd20ddaa193f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/yBYbWp_mT7UusYdkqtAvw.png",
      "fullname": "Siteng Huang",
      "name": "huangsiteng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.03418",
      "authors": [
        {
          "_id": "681c4d5b5971460af345032a",
          "name": "Da Zheng",
          "hidden": false
        },
        {
          "_id": "681c4d5b5971460af345032b",
          "name": "Lun Du",
          "hidden": false
        },
        {
          "_id": "681c4d5b5971460af345032c",
          "name": "Junwei Su",
          "hidden": false
        },
        {
          "_id": "681c4d5b5971460af345032d",
          "name": "Yuchen Tian",
          "hidden": false
        },
        {
          "_id": "681c4d5b5971460af345032e",
          "name": "Yuqi Zhu",
          "hidden": false
        },
        {
          "_id": "681c4d5b5971460af345032f",
          "name": "Jintian Zhang",
          "hidden": false
        },
        {
          "_id": "681c4d5b5971460af3450330",
          "name": "Lanning Wei",
          "hidden": false
        },
        {
          "_id": "681c4d5b5971460af3450331",
          "name": "Ningyu Zhang",
          "hidden": false
        },
        {
          "_id": "681c4d5b5971460af3450332",
          "name": "Huajun Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-06T10:53:58.000Z",
      "submittedOnDailyAt": "2025-05-08T04:51:36.213Z",
      "title": "Recherche sur l'implémentation de solutions complexes pour la résolution de problèmes de calcul de connaissances à l'aide de modèles de langage à grande échelle",
      "submittedOnDailyBy": {
        "_id": "620b3bbb0668e435407c8d0a",
        "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
        "isPro": false,
        "fullname": "Ningyu Zhang",
        "user": "Ningyu",
        "type": "user"
      },
      "summary": "La résolution de problèmes a été reconnue comme une motivation fondamentale pour le progrès humain dans divers domaines, ce qui a été largement reconnu dans l'histoire. Avec le développement de l'intelligence artificielle, les modèles de langage à grande échelle (LLMs) ont émergé comme des outils puissants pour résoudre des problèmes complexes dans divers domaines. À différence des systèmes de calcul traditionnels, les LLMs combinent une puissance de calcul excessive et une approximation à la logique humaine pour générer des solutions, effectuer des inférences et utiliser des outils externes de calcul. Cependant, lorsqu'ils sont appliqués aux problèmes à grande échelle dans le monde réel, ils rencontrent de grands défis, tels que la nécessité d'explications logiques pas à pas, l'intégration de connaissances de diverses domaines et la vérification des résultats. Cette étude investigate les capacités et les limites des LLMs dans la résolution de problèmes complexes, en examinant des techniques comme l'explication logique pas à pas (Chain-of-Thought, CoT), l'expansion de connaissances et des méthodes de vérification basées sur les LLMs et les outils. De plus, des problèmes spécifiques dans des domaines tels que le développement du logiciel, l'explication et la démonstration logique mathématique, l'analyse de données et le modélisation, et la recherche scientifique sont abordés. Cet article discute les limites fondamentales des solutions actuelles basées sur les LLMs et propose des directions futures pour la résolution de problèmes complexes basées sur les LLMs, à partir de la perspective d'explications logiques pas à pas, l'intégration de connaissances de diverses domaines et la vérification des résultats.",
      "upvotes": 2,
      "discussionId": "681c4d5f5971460af3450465",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "Chain-of-Thought (CoT) reasoning",
        "knowledge augmentation",
        "verification techniques",
        "software engineering",
        "mathematical reasoning and proving",
        "data analysis and modeling",
        "scientific research",
        "multi-step reasoning",
        "domain knowledge integration",
        "result verification"
      ]
    },
    "publishedAt": "2025-05-06T06:53:58.000Z",
    "title": "Knowledge Augmented Complex Problem Solving with Large Language Models:\n  A Survey",
    "summary": "Problem-solving has been a fundamental driver of human progress in numerous\ndomains. With advancements in artificial intelligence, Large Language Models\n(LLMs) have emerged as powerful tools capable of tackling complex problems\nacross diverse domains. Unlike traditional computational systems, LLMs combine\nraw computational power with an approximation of human reasoning, allowing them\nto generate solutions, make inferences, and even leverage external\ncomputational tools. However, applying LLMs to real-world problem-solving\npresents significant challenges, including multi-step reasoning, domain\nknowledge integration, and result verification. This survey explores the\ncapabilities and limitations of LLMs in complex problem-solving, examining\ntechniques including Chain-of-Thought (CoT) reasoning, knowledge augmentation,\nand various LLM-based and tool-based verification techniques. Additionally, we\nhighlight domain-specific challenges in various domains, such as software\nengineering, mathematical reasoning and proving, data analysis and modeling,\nand scientific research. The paper further discusses the fundamental\nlimitations of the current LLM solutions and the future directions of LLM-based\ncomplex problems solving from the perspective of multi-step reasoning, domain\nknowledge integration and result verification.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03418.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "620b3bbb0668e435407c8d0a",
      "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
      "fullname": "Ningyu Zhang",
      "name": "Ningyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 22
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.03821",
      "authors": [
        {
          "_id": "681c7a3829ba66a745217db5",
          "user": {
            "_id": "63caf7ce9f78909f9f81eb72",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63caf7ce9f78909f9f81eb72/2mLUr_DdEom28DjodWimv.jpeg",
            "isPro": true,
            "fullname": "Gracjan Goral",
            "user": "Gracjan",
            "type": "user"
          },
          "name": "Gracjan Góral",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-08T09:58:02.558Z",
          "hidden": false
        },
        {
          "_id": "681c7a3829ba66a745217db6",
          "name": "Alicja Ziarko",
          "hidden": false
        },
        {
          "_id": "681c7a3829ba66a745217db7",
          "name": "Piotr Miłoś",
          "hidden": false
        },
        {
          "_id": "681c7a3829ba66a745217db8",
          "name": "Michał Nauman",
          "hidden": false
        },
        {
          "_id": "681c7a3829ba66a745217db9",
          "name": "Maciej Wołczyk",
          "hidden": false
        },
        {
          "_id": "681c7a3829ba66a745217dba",
          "name": "Michał Kosiński",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63caf7ce9f78909f9f81eb72/b8OMQONPXPyBtcitD1Yg7.jpeg"
      ],
      "publishedAt": "2025-05-03T00:10:41.000Z",
      "submittedOnDailyAt": "2025-05-08T08:19:59.040Z",
      "title": "Vision : Évaluation de la perspective visuelle du modèle de vision",
      "submittedOnDailyBy": {
        "_id": "63caf7ce9f78909f9f81eb72",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63caf7ce9f78909f9f81eb72/2mLUr_DdEom28DjodWimv.jpeg",
        "isPro": true,
        "fullname": "Gracjan Goral",
        "user": "Gracjan",
        "type": "user"
      },
      "summary": "La Vision de Langue et les Modèles de Vision (VLMs) sont étudiés pour évaluer leurs habiletés en représentation visuelle. Des nouvelles tâches visuelles sont utilisées qui ressemblent plus à des tests humains. Notre approche combine un jouet mini et un objet pour configurer des scénarios ajustés. La position de l'objet et la direction du jouet mini sont systématiquement changées, et les perspectives d'un oiseau ainsi qu'une perspective superficielle sont utilisées, créant 144 tâches visuelles uniques. Chaque tâche visuelle est composée de 7 séries de questions diagnostiques conçues pour évaluer la compréhension du scénario, la raison spatiale et la représentation visuelle. Des modèles récents tels que GPT-4-Turbo, GPT-4o, Llama-3.2-11B-Vision-Instruct et Claude Sonnet ont été évalués. Ces modèles ont montré une excellence dans la compréhension du scénario, mais leur rendement en raison spatiale a été significativement plus faible et leur représentation visuelle a dégénéré encore plus. Notre analyse montre la différence entre la reconnaissance d'objets au niveau superficiel et la nécessité d'une représentation spatiale et visuelle profonde, soutenant que l'intégration d'une représentation géométrique claire et des protocoles d'entraînement est essentielle pour l'avenir des VLMs.",
      "upvotes": 2,
      "discussionId": "681c7a3e29ba66a745217f0c"
    },
    "publishedAt": "2025-05-02T20:10:41.000Z",
    "title": "Beyond Recognition: Evaluating Visual Perspective Taking in Vision\n  Language Models",
    "summary": "We investigate the ability of Vision Language Models (VLMs) to perform visual\nperspective taking using a novel set of visual tasks inspired by established\nhuman tests. Our approach leverages carefully controlled scenes, in which a\nsingle humanoid minifigure is paired with a single object. By systematically\nvarying spatial configurations - such as object position relative to the\nhumanoid minifigure and the humanoid minifigure's orientation - and using both\nbird's-eye and surface-level views, we created 144 unique visual tasks. Each\nvisual task is paired with a series of 7 diagnostic questions designed to\nassess three levels of visual cognition: scene understanding, spatial\nreasoning, and visual perspective taking. Our evaluation of several\nstate-of-the-art models, including GPT-4-Turbo, GPT-4o,\nLlama-3.2-11B-Vision-Instruct, and variants of Claude Sonnet, reveals that\nwhile they excel in scene understanding, the performance declines significantly\non spatial reasoning and further deteriorates on perspective-taking. Our\nanalysis suggests a gap between surface-level object recognition and the deeper\nspatial and perspective reasoning required for complex visual tasks, pointing\nto the need for integrating explicit geometric representations and tailored\ntraining protocols in future VLM development.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63caf7ce9f78909f9f81eb72/b8OMQONPXPyBtcitD1Yg7.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03821.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63caf7ce9f78909f9f81eb72",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63caf7ce9f78909f9f81eb72/2mLUr_DdEom28DjodWimv.jpeg",
      "fullname": "Gracjan Goral",
      "name": "Gracjan",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.00358",
      "authors": [
        {
          "_id": "68154d77c8ab88a66b8d81a7",
          "name": "Albert Ge",
          "hidden": false
        },
        {
          "_id": "68154d77c8ab88a66b8d81a8",
          "name": "Tzu-Heng Huang",
          "hidden": false
        },
        {
          "_id": "68154d77c8ab88a66b8d81a9",
          "name": "John Cooper",
          "hidden": false
        },
        {
          "_id": "68154d77c8ab88a66b8d81aa",
          "name": "Avi Trost",
          "hidden": false
        },
        {
          "_id": "68154d77c8ab88a66b8d81ab",
          "name": "Ziyi Chu",
          "hidden": false
        },
        {
          "_id": "68154d77c8ab88a66b8d81ac",
          "name": "Satya Sai Srinath Namburi GNVV",
          "hidden": false
        },
        {
          "_id": "68154d77c8ab88a66b8d81ad",
          "name": "Ziyang Cai",
          "hidden": false
        },
        {
          "_id": "68154d77c8ab88a66b8d81ae",
          "name": "Kendall Park",
          "hidden": false
        },
        {
          "_id": "68154d77c8ab88a66b8d81af",
          "name": "Nicholas Roberts",
          "hidden": false
        },
        {
          "_id": "68154d77c8ab88a66b8d81b0",
          "name": "Frederic Sala",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-01T07:08:19.000Z",
      "submittedOnDailyAt": "2025-05-08T05:38:42.650Z",
      "title": "R&B : Réorganisation du domaine et équilibre de la confusion de données pour l'entraînement efficace de modèles de base",
      "submittedOnDailyBy": {
        "_id": "650263c89a612aa33a018383",
        "avatarUrl": "/avatars/a1415a4696da98ffebc1ac798cbab052.svg",
        "isPro": false,
        "fullname": "Albert Ge",
        "user": "albertge",
        "type": "user"
      },
      "summary": "Le système de mélange de données réduit les coûts liés à l'entraînement de modèles de langage. Ces méthodes présentent deux inconvénients. Premièrement, le domaine des données (par exemple, sources de données, types de tâches) est déterminé avant, ce qui empêche de comprendre les nuances linguistiques importantes et de les relier à l'amélioration de la qualité. Deuxièmement, ces méthodes peuvent être complexes en termes de calcul. Avec l'augmentation du nombre de domaines, la quantité de calculs augmente. Pour faire face à ces défis, on utilise le cadre R&B pour répartir de nouveau les données d'entraînement en fonction de la similitude linguistique, créer des domaines plus petits et utiliser l'expérience du domaine acquise lors du processus d'entraînement pour optimiser la structure des données avec la matrice Gram. Au contraire des études précédentes, il n'est pas nécessaire de calculer des informations d'évaluation ou des pertes/gradients supplémentaires. Une analyse de cette technique est effectuée sous des conditions standards et la efficacité du R&B est comparée à un approche de mélange non appropriée, fournissant une conformité théorique. Expérimentalement, on montre l'effet du R&B sur cinq ensembles de données différents de traitement de langage, inférence et tâches de données multiformes. Le R&B peut dépasser ou égaler le meilleur rendement d'un système de mélange de données de haut niveau, seul avec un surcoût de calcul supplémentaire de 0.01%.",
      "upvotes": 2,
      "discussionId": "68154d78c8ab88a66b8d820c",
      "ai_keywords": [
        "semantic similarity",
        "Gram matrix",
        "domain gradients"
      ]
    },
    "publishedAt": "2025-05-01T03:08:19.000Z",
    "title": "R&B: Domain Regrouping and Data Mixture Balancing for Efficient\n  Foundation Model Training",
    "summary": "Data mixing strategies have successfully reduced the costs involved in\ntraining language models. While promising, such methods suffer from two flaws.\nFirst, they rely on predetermined data domains (e.g., data sources, task\ntypes), which may fail to capture critical semantic nuances, leaving\nperformance on the table. Second, these methods scale with the number of\ndomains in a computationally prohibitive way. We address these challenges via\nR&B, a framework that re-partitions training data based on semantic similarity\n(Regroup) to create finer-grained domains, and efficiently optimizes the data\ncomposition (Balance) by leveraging a Gram matrix induced by domain gradients\nobtained throughout training. Unlike prior works, it removes the need for\nadditional compute to obtain evaluation information such as losses or\ngradients. We analyze this technique under standard regularity conditions and\nprovide theoretical insights that justify R&B's effectiveness compared to\nnon-adaptive mixing approaches. Empirically, we demonstrate the effectiveness\nof R&B on five diverse datasets ranging from natural language to reasoning and\nmultimodal tasks. With as little as 0.01% additional compute overhead, R&B\nmatches or exceeds the performance of state-of-the-art data mixing strategies.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.00358.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "650263c89a612aa33a018383",
      "avatarUrl": "/avatars/a1415a4696da98ffebc1ac798cbab052.svg",
      "fullname": "Albert Ge",
      "name": "albertge",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.03570",
      "authors": [
        {
          "_id": "681b518bf497fd5e45b55eeb",
          "user": {
            "_id": "667ed2bf12e48bee0e972ccc",
            "avatarUrl": "/avatars/9489dba960a63127397f4ca13e507078.svg",
            "isPro": false,
            "fullname": "Mariya Davydova",
            "user": "mariya-davydova",
            "type": "user"
          },
          "name": "Mariya Davydova",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-08T08:59:24.254Z",
          "hidden": false
        },
        {
          "_id": "681b518bf497fd5e45b55eec",
          "name": "Daniel Jeffries",
          "hidden": false
        },
        {
          "_id": "681b518bf497fd5e45b55eed",
          "name": "Patrick Barker",
          "hidden": false
        },
        {
          "_id": "681b518bf497fd5e45b55eee",
          "name": "Arturo Márquez Flores",
          "hidden": false
        },
        {
          "_id": "681b518bf497fd5e45b55eef",
          "name": "Sinéad Ryan",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/667ed2bf12e48bee0e972ccc/fe49DABHt5h8Uw-_GCzaB.png",
        "https://cdn-uploads.huggingface.co/production/uploads/667ed2bf12e48bee0e972ccc/yfxA_jM6W5khwa_hHL_mF.png"
      ],
      "publishedAt": "2025-05-06T14:29:47.000Z",
      "submittedOnDailyAt": "2025-05-08T07:36:52.978Z",
      "title": "OSUniverse: Benchmark de la GUI de Navigation AI de DamoDal",
      "submittedOnDailyBy": {
        "_id": "667ed2bf12e48bee0e972ccc",
        "avatarUrl": "/avatars/9489dba960a63127397f4ca13e507078.svg",
        "isPro": false,
        "fullname": "Mariya Davydova",
        "user": "mariya-davydova",
        "type": "user"
      },
      "summary": "Dans cet article, un cadre de test pour OSUniverse : desktop est présenté, qui évalue une large gamme de tâches complexes et divers. Ce cadre de test se concentre sur l'utilisateur amiable, l'extensibilité, la couverture complète des cas de test et la vérification automatique, exige la capacité, la précision et la clarté de pensée d'un agent IA qui doit réaliser des actions depuis la pinceau de base de précision jusqu'aux tests de multiples applications. Les contenus présentés dans la version 1 du cadre de test sont inclus ici. La complexité des cas de test est ajustée de manière que les agents IA de l'état de l'art au moment de leur publication ne réussissent pas à obtenir plus de 50% des résultats, et il est vérifié si le niveau moyen des employés peut effectuer ces tâches avec une précision totale. Le cadre de test permet de qualifier de manière ponctuelle et de présenter une structure de vérification automatique avec un taux d'erreur moyen inférieur à 2%. Cette outil fournit une base solide pour mesurer le développement de l'automatisation complète, la capacité et l'efficacité des agents IA de navigation de GUI à court et à long terme. Le code source du cadre de test est disponible sur https://github.com/agentsea/osuniverse.",
      "upvotes": 1,
      "discussionId": "681b518cf497fd5e45b55f0f",
      "projectPage": "https://agentsea.github.io/osuniverse/",
      "githubRepo": "https://github.com/agentsea/osuniverse"
    },
    "publishedAt": "2025-05-06T10:29:47.000Z",
    "title": "OSUniverse: Benchmark for Multimodal GUI-navigation AI Agents",
    "summary": "In this paper, we introduce OSUniverse: a benchmark of complex, multimodal\ndesktop-oriented tasks for advanced GUI-navigation AI agents that focuses on\nease of use, extensibility, comprehensive coverage of test cases, and automated\nvalidation. We divide the tasks in increasing levels of complexity, from basic\nprecision clicking to multistep, multiapplication tests requiring dexterity,\nprecision, and clear thinking from the agent. In version one of the benchmark,\npresented here, we have calibrated the complexity of the benchmark test cases\nto ensure that the SOTA (State of the Art) agents (at the time of publication)\ndo not achieve results higher than 50%, while the average white collar worker\ncan perform all these tasks with perfect accuracy. The benchmark can be scored\nmanually, but we also introduce an automated validation mechanism that has an\naverage error rate less than 2%. Therefore, this benchmark presents solid\nground for fully automated measuring of progress, capabilities and the\neffectiveness of GUI-navigation AI agents over the short and medium-term\nhorizon. The source code of the benchmark is available at\nhttps://github.com/agentsea/osuniverse.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/667ed2bf12e48bee0e972ccc/fe49DABHt5h8Uw-_GCzaB.png",
      "https://cdn-uploads.huggingface.co/production/uploads/667ed2bf12e48bee0e972ccc/yfxA_jM6W5khwa_hHL_mF.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03570.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "667ed2bf12e48bee0e972ccc",
      "avatarUrl": "/avatars/9489dba960a63127397f4ca13e507078.svg",
      "fullname": "Mariya Davydova",
      "name": "mariya-davydova",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.02393",
      "authors": [
        {
          "_id": "681c423f198e1dea5c26f2f4",
          "user": {
            "_id": "6445e9bd1cfc9ae6bb40985c",
            "avatarUrl": "/avatars/33f48c1c820e592cbf4ea4af479dd378.svg",
            "isPro": false,
            "fullname": "Evan Jeong",
            "user": "Eavn",
            "type": "user"
          },
          "name": "Sungheon Jeong",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-08T09:38:37.142Z",
          "hidden": false
        },
        {
          "_id": "681c423f198e1dea5c26f2f5",
          "user": {
            "_id": "646b57c6e5abcbf6709fabf6",
            "avatarUrl": "/avatars/e9749acf7866eeaf017f0a43351794fc.svg",
            "isPro": false,
            "fullname": "Jihong Park",
            "user": "Paper9795",
            "type": "user"
          },
          "name": "Jihong Park",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-08T05:36:09.797Z",
          "hidden": false
        },
        {
          "_id": "681c423f198e1dea5c26f2f6",
          "name": "Mohsen Imani",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-05T06:33:20.000Z",
      "submittedOnDailyAt": "2025-05-08T04:12:55.976Z",
      "title": "Détection de l'exceptionnalité dans les vidéos en utilisant la fusion d'images et d'événements avec une emphase sur l'incertitude",
      "submittedOnDailyBy": {
        "_id": "6445e9bd1cfc9ae6bb40985c",
        "avatarUrl": "/avatars/33f48c1c820e592cbf4ea4af479dd378.svg",
        "isPro": false,
        "fullname": "Evan Jeong",
        "user": "Eavn",
        "type": "user"
      },
      "summary": "Varios systèmes de détection vidéo actuels dépendent uniquement des marqueurs RGB, mais cela les rendent incapables de la capacité analytique temporelle nécessaire pour détecter des mouvements rapides ou instantanés, ce qui les transforme en indicateurs d'événements étranges. Pour résoudre cette limitation, nous proposons un cadre de travail pour la détection d'événements dans les vidéos \"Fusion d'événements et d'images pour la détection de vidéos d'anomalies (IEF-VAD)\" qui combine la représentation d'événements et les caractéristiques d'images dans les vidéos RGB. Ce cadre de travail principalement effectue la fusion directe des marqueurs d'images et des caractéristiques d'images lors du traitement de l'incertitude. Ce système : (i) modélise le bruit sensoriel par la correction de la distribution de Student's-t et obtient des poids de déviation inverse au niveau de la valeur par l'approximation de Laplace ; (ii) applique des mises à jour de type Kalman pour ajuster l'équilibre du modèle temporel ; (iii) raffine répétitivement l'état potentiel et élimine le bruit croisé des autres modalités. IEF-VAD ne nécessite pas de capteurs professionnels d'événements ni de marqueurs, et établit le niveau de l'art sur plusieurs benchmarks de détection d'anomalies dans le monde réel. Ces résultats démontrent l'utilité de la représentation d'événements synthétiques en RGB pour mettre en évidence des codes de mouvement généralement non représentés, et permettent d'atteindre une compréhension précise et robuste des vidéos dans diverses applications. Le code et le modèle sont disponibles sur https://github.com/EavnJeong/IEF-VAD.",
      "upvotes": 1,
      "discussionId": "681c4243198e1dea5c26f3cd",
      "githubRepo": "https://github.com/EavnJeong/IEF-VAD",
      "ai_keywords": [
        "Image-Event Fusion",
        "Video Anomaly Detection",
        "event representations",
        "Student`s-t likelihood",
        "Laplace approximation",
        "Kalman-style frame-wise updates",
        "fused latent state",
        "cross-modal noise"
      ]
    },
    "publishedAt": "2025-05-05T02:33:20.000Z",
    "title": "Uncertainty-Weighted Image-Event Multimodal Fusion for Video Anomaly\n  Detection",
    "summary": "Most existing video anomaly detectors rely solely on RGB frames, which lack\nthe temporal resolution needed to capture abrupt or transient motion cues, key\nindicators of anomalous events. To address this limitation, we propose\nImage-Event Fusion for Video Anomaly Detection (IEF-VAD), a framework that\nsynthesizes event representations directly from RGB videos and fuses them with\nimage features through a principled, uncertainty-aware process. The system (i)\nmodels heavy-tailed sensor noise with a Student`s-t likelihood, deriving\nvalue-level inverse-variance weights via a Laplace approximation; (ii) applies\nKalman-style frame-wise updates to balance modalities over time; and (iii)\niteratively refines the fused latent state to erase residual cross-modal noise.\nWithout any dedicated event sensor or frame-level labels, IEF-VAD sets a new\nstate of the art across multiple real-world anomaly detection benchmarks. These\nfindings highlight the utility of synthetic event representations in\nemphasizing motion cues that are often underrepresented in RGB frames, enabling\naccurate and robust video understanding across diverse applications without\nrequiring dedicated event sensors. Code and models are available at\nhttps://github.com/EavnJeong/IEF-VAD.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02393.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6445e9bd1cfc9ae6bb40985c",
      "avatarUrl": "/avatars/33f48c1c820e592cbf4ea4af479dd378.svg",
      "fullname": "Evan Jeong",
      "name": "Eavn",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]