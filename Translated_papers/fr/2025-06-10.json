[
  {
    "paper": {
      "id": "2506.08007",
      "authors": [
        {
          "_id": "684794553ec10bdd8ab4de1a",
          "name": "Qingxiu Dong",
          "hidden": false
        },
        {
          "_id": "684794553ec10bdd8ab4de1b",
          "user": {
            "_id": "5df85abada6d0311fd3d5408",
            "avatarUrl": "/avatars/2331cf703c1b5d3a62e2050b1a6eb108.svg",
            "isPro": false,
            "fullname": "Li Dong",
            "user": "unilm",
            "type": "user"
          },
          "name": "Li Dong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-10T08:44:23.723Z",
          "hidden": false
        },
        {
          "_id": "684794553ec10bdd8ab4de1c",
          "user": {
            "_id": "667119d6578448466d9531a6",
            "avatarUrl": "/avatars/72c31909a5584b1306b6404b94a22b2a.svg",
            "isPro": false,
            "fullname": "Yao Tang",
            "user": "YaoTang23",
            "type": "user"
          },
          "name": "Yao Tang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-10T08:44:20.414Z",
          "hidden": false
        },
        {
          "_id": "684794553ec10bdd8ab4de1d",
          "name": "Tianzhu Ye",
          "hidden": false
        },
        {
          "_id": "684794553ec10bdd8ab4de1e",
          "name": "Yutao Sun",
          "hidden": false
        },
        {
          "_id": "684794553ec10bdd8ab4de1f",
          "name": "Zhifang Sui",
          "hidden": false
        },
        {
          "_id": "684794553ec10bdd8ab4de20",
          "user": {
            "_id": "67ecd6178647cfa1775f75ed",
            "avatarUrl": "/avatars/98882cc58dc0a5de94df765d523d92c9.svg",
            "isPro": false,
            "fullname": "FW",
            "user": "frontierai",
            "type": "user"
          },
          "name": "Furu Wei",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-10T02:11:34.050Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/5df85abada6d0311fd3d5408/OeDc7c4QFJOxdkJWvdbWB.png"
      ],
      "publishedAt": "2025-06-09T17:59:53.000Z",
      "submittedOnDailyAt": "2025-06-10T00:43:01.816Z",
      "title": "Reinforcement Pré-Entraînement",
      "submittedOnDailyBy": {
        "_id": "5df85abada6d0311fd3d5408",
        "avatarUrl": "/avatars/2331cf703c1b5d3a62e2050b1a6eb108.svg",
        "isPro": false,
        "fullname": "Li Dong",
        "user": "unilm",
        "type": "user"
      },
      "summary": "Dans cette étude, nous présentons un nouveau paradigme d'accroissement appelé Reinforcement Pre-Training (RPT) pour les modèles de langage grands et l'apprentissage par renforcement (RL). Plus précisément, nous reconstruisons la tâche d'étiquetage logique en utilisant l'apprentissage par renforcement pour prédire le prochain token, et nous appliquons un méthode qui reçoit une récompense confirmable lorsque le prochain token est prédit avec précision dans un contexte existant. Le RPT, en contraste avec les cas généraux, propose un méthode d'accroissement indépendante de guides spécifiques dans une zone, en utilisant de grandes quantités de données de phrases. Cela permet un grand améliorament de la précision de la prédiction du prochain token. De plus, le RPT offre une forte base de pré-entraînement qui peut être développée à travers un ajustement supplémentaire d'apprentissage par renforcement. La courbe d'accroissement montre comment la précision de la prédiction du prochain token s'améliore de manière constante avec l'augmentation du calcul d'apprentissage. Ces résultats démontrent que le RPT est un paradigme efficace et souhaitable pour le pré-entraînement de modèles de langage.",
      "upvotes": 108,
      "discussionId": "684794553ec10bdd8ab4de21",
      "ai_summary": "Reinforcement Pre-Training (RPT) improves language model accuracy through reinforcement learning and offers a scalable method for leveraging text data for general-purpose RL.",
      "ai_keywords": [
        "Reinforcement Pre-Training (RPT)",
        "next-token prediction",
        "reasoning task",
        "reinforcement learning (RL)",
        "verifiable rewards",
        "language modeling accuracy",
        "reinforcement fine-tuning",
        "scaling curves"
      ]
    },
    "publishedAt": "2025-06-09T13:59:53.000Z",
    "title": "Reinforcement Pre-Training",
    "summary": "In this work, we introduce Reinforcement Pre-Training (RPT) as a new scaling\nparadigm for large language models and reinforcement learning (RL).\nSpecifically, we reframe next-token prediction as a reasoning task trained\nusing RL, where it receives verifiable rewards for correctly predicting the\nnext token for a given context. RPT offers a scalable method to leverage vast\namounts of text data for general-purpose RL, rather than relying on\ndomain-specific annotated answers. By incentivizing the capability of\nnext-token reasoning, RPT significantly improves the language modeling accuracy\nof predicting the next tokens. Moreover, RPT provides a strong pre-trained\nfoundation for further reinforcement fine-tuning. The scaling curves show that\nincreased training compute consistently improves the next-token prediction\naccuracy. The results position RPT as an effective and promising scaling\nparadigm to advance language model pre-training.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/5df85abada6d0311fd3d5408/OeDc7c4QFJOxdkJWvdbWB.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08007.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "5df85abada6d0311fd3d5408",
      "avatarUrl": "/avatars/2331cf703c1b5d3a62e2050b1a6eb108.svg",
      "fullname": "Li Dong",
      "name": "unilm",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 28
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.07044",
      "authors": [
        {
          "_id": "684795093ec10bdd8ab4de43",
          "name": "LASA Team",
          "hidden": false
        },
        {
          "_id": "684795093ec10bdd8ab4de44",
          "user": {
            "_id": "64118689756b9e455c7eac62",
            "avatarUrl": "/avatars/cdb3da22593facf545a0bafbf548b07e.svg",
            "isPro": false,
            "fullname": "Xu Weiwen",
            "user": "xww033",
            "type": "user"
          },
          "name": "Weiwen Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-10T08:44:07.459Z",
          "hidden": false
        },
        {
          "_id": "684795093ec10bdd8ab4de45",
          "user": {
            "_id": "604f67ef0fe8ff3ec13d71ef",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/604f67ef0fe8ff3ec13d71ef/KhUwWvZ3OJ9nEee3B-SXO.png",
            "isPro": false,
            "fullname": "Hou Pong (Ken) Chan",
            "user": "kenchan0226",
            "type": "user"
          },
          "name": "Hou Pong Chan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-10T08:44:05.163Z",
          "hidden": false
        },
        {
          "_id": "684795093ec10bdd8ab4de46",
          "name": "Long Li",
          "hidden": false
        },
        {
          "_id": "684795093ec10bdd8ab4de47",
          "name": "Mahani Aljunied",
          "hidden": false
        },
        {
          "_id": "684795093ec10bdd8ab4de48",
          "name": "Ruifeng Yuan",
          "hidden": false
        },
        {
          "_id": "684795093ec10bdd8ab4de49",
          "user": {
            "_id": "61e09ec13a1781f66b4e9ae2",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1642110635503-noauth.jpeg",
            "isPro": false,
            "fullname": "Jianyu Wang",
            "user": "Jianyu",
            "type": "user"
          },
          "name": "Jianyu Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-10T08:44:03.340Z",
          "hidden": false
        },
        {
          "_id": "684795093ec10bdd8ab4de4a",
          "name": "Chenghao Xiao",
          "hidden": false
        },
        {
          "_id": "684795093ec10bdd8ab4de4b",
          "name": "Guizhen Chen",
          "hidden": false
        },
        {
          "_id": "684795093ec10bdd8ab4de4c",
          "name": "Chaoqun Liu",
          "hidden": false
        },
        {
          "_id": "684795093ec10bdd8ab4de4d",
          "name": "Zhaodonghui Li",
          "hidden": false
        },
        {
          "_id": "684795093ec10bdd8ab4de4e",
          "name": "Yu Sun",
          "hidden": false
        },
        {
          "_id": "684795093ec10bdd8ab4de4f",
          "name": "Junao Shen",
          "hidden": false
        },
        {
          "_id": "684795093ec10bdd8ab4de50",
          "name": "Chaojun Wang",
          "hidden": false
        },
        {
          "_id": "684795093ec10bdd8ab4de51",
          "name": "Jie Tan",
          "hidden": false
        },
        {
          "_id": "684795093ec10bdd8ab4de52",
          "name": "Deli Zhao",
          "hidden": false
        },
        {
          "_id": "684795093ec10bdd8ab4de53",
          "name": "Tingyang Xu",
          "hidden": false
        },
        {
          "_id": "684795093ec10bdd8ab4de54",
          "name": "Hao Zhang",
          "hidden": false
        },
        {
          "_id": "684795093ec10bdd8ab4de55",
          "user": {
            "_id": "642eecbf9b2484d7d8526781",
            "avatarUrl": "/avatars/773606f4a37d48861ec4f0f2df8a956f.svg",
            "isPro": false,
            "fullname": "Yu Rong",
            "user": "Swrooy",
            "type": "user"
          },
          "name": "Yu Rong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-10T08:44:01.224Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/604f67ef0fe8ff3ec13d71ef/R3ajyza5JHjd8tOwwV2ht.png"
      ],
      "publishedAt": "2025-06-08T08:47:30.000Z",
      "submittedOnDailyAt": "2025-06-10T00:48:48.080Z",
      "title": "Rings : Modèle généralisé pour l'intégration d'un compréhension médicale multi-type et logique",
      "submittedOnDailyBy": {
        "_id": "604f67ef0fe8ff3ec13d71ef",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/604f67ef0fe8ff3ec13d71ef/KhUwWvZ3OJ9nEee3B-SXO.png",
        "isPro": false,
        "fullname": "Hou Pong (Ken) Chan",
        "user": "kenchan0226",
        "type": "user"
      },
      "summary": "Les modèles de langue multimodal (MLLMs) montrent une capacité impressionnante pour comprendre des éléments visuels généraux. Cela est possible grâce à des ensembles de données massives et à des stratégies d'apprentissage avancées. Cependant, l'efficacité de leur application dans le domaine médical est limitée par les différences intrinsèques aux données et aux tâches médicales, qui sont différentes de celles des domaines généraux. Spécifiquement, les MLLMs médicaux actuels sont restreints par les aspects principaux suivants : une largeur limitée de connaissances médicales en plus d'images médicales, un accroissement de la confusion due à des processus inadéquats de préparation des données et une insuffisance de capacités logiques pour des situations médicales complexes. Pour aborder ces problèmes, nous proposons un procédé détaillé de préparation des données qui comprend : commencer par des images médicales et obtenir de manière efficace des connaissances médicales à partir de documents médicaux et de données générales. Les captures médicales précises, les questions visuelles et réponses (VQA) et les échantillons logiques seront synthétisés. En conséquence, un ensemble de données riche en connaissances médicales sera construit. Avec ces données préparées, nous présentons un nouveau MLLM médical : Lingshu. Lingshu inclut des connaissances médicales spécialisées et améliore sa capacité à résoudre des tâches de manière évolutive grâce à un apprentissage en étapes. De plus, nous avons exploré la possibilité d'apprentissage par renforcement basé sur un paradigme de récompenses provable. Nous avons également développé MedEvalKit, un cadre d'évaluation permettant une évaluation standard, juste et efficace du modèle, en intégrant un ensemble d'évaluations de benchmark médical avec des données et des textes multimodaux. Lingshu a démontré surpasser les modèles multimodaux ouverts actuels, montrant des résultats supérieurs sur trois tâches médicales fondamentales : QA multimodal, QA basée sur le texte et génération de rapports médicaux...",
      "upvotes": 50,
      "discussionId": "684795093ec10bdd8ab4de56",
      "ai_summary": "A medical-specialized multimodal large language model, Lingshu, is introduced with enhanced data curation and reinforcement learning to address limitations in medical applications.",
      "ai_keywords": [
        "Multimodal Large Language Models",
        "MLLMs",
        "medical knowledge",
        "hallucinations",
        "data curation",
        "medical texts",
        "general-domain data",
        "accurate medical captions",
        "visual question answering",
        "VQA",
        "reasoning capabilities",
        "multi-stage training",
        "medical expertise",
        "reinforcement learning",
        "verifiable rewards paradigm",
        "MedEvalKit",
        "multimodal QA",
        "text-based QA",
        "medical report generation"
      ]
    },
    "publishedAt": "2025-06-08T04:47:30.000Z",
    "title": "Lingshu: A Generalist Foundation Model for Unified Multimodal Medical\n  Understanding and Reasoning",
    "summary": "Multimodal Large Language Models (MLLMs) have demonstrated impressive\ncapabilities in understanding common visual elements, largely due to their\nlarge-scale datasets and advanced training strategies. However, their\neffectiveness in medical applications remains limited due to the inherent\ndiscrepancies between data and tasks in medical scenarios and those in the\ngeneral domain. Concretely, existing medical MLLMs face the following critical\nlimitations: (1) limited coverage of medical knowledge beyond imaging, (2)\nheightened susceptibility to hallucinations due to suboptimal data curation\nprocesses, (3) lack of reasoning capabilities tailored for complex medical\nscenarios. To address these challenges, we first propose a comprehensive data\ncuration procedure that (1) efficiently acquires rich medical knowledge data\nnot only from medical imaging but also from extensive medical texts and\ngeneral-domain data; and (2) synthesizes accurate medical captions, visual\nquestion answering (VQA), and reasoning samples. As a result, we build a\nmultimodal dataset enriched with extensive medical knowledge. Building on the\ncurated data, we introduce our medical-specialized MLLM: Lingshu. Lingshu\nundergoes multi-stage training to embed medical expertise and enhance its\ntask-solving capabilities progressively. Besides, we preliminarily explore the\npotential of applying reinforcement learning with verifiable rewards paradigm\nto enhance Lingshu's medical reasoning ability. Additionally, we develop\nMedEvalKit, a unified evaluation framework that consolidates leading multimodal\nand textual medical benchmarks for standardized, fair, and efficient model\nassessment. We evaluate the performance of Lingshu on three fundamental medical\ntasks, multimodal QA, text-based QA, and medical report generation. The results\nshow that Lingshu consistently outperforms the existing open-source multimodal\nmodels on most tasks ...",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/604f67ef0fe8ff3ec13d71ef/R3ajyza5JHjd8tOwwV2ht.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07044.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "604f67ef0fe8ff3ec13d71ef",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/604f67ef0fe8ff3ec13d71ef/KhUwWvZ3OJ9nEee3B-SXO.png",
      "fullname": "Hou Pong (Ken) Chan",
      "name": "kenchan0226",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.07900",
      "authors": [
        {
          "_id": "6847924d3ec10bdd8ab4ddb9",
          "name": "MiniCPM Team",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddba",
          "name": "Chaojun Xiao",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddbb",
          "name": "Yuxuan Li",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddbc",
          "name": "Xu Han",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddbd",
          "name": "Yuzhuo Bai",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddbe",
          "name": "Jie Cai",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddbf",
          "name": "Haotian Chen",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddc0",
          "name": "Wentong Chen",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddc1",
          "name": "Xin Cong",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddc2",
          "name": "Ganqu Cui",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddc3",
          "name": "Ning Ding",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddc4",
          "name": "Shengdan Fan",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddc5",
          "name": "Yewei Fang",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddc6",
          "name": "Zixuan Fu",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddc7",
          "name": "Wenyu Guan",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddc8",
          "name": "Yitong Guan",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddc9",
          "name": "Junshao Guo",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddca",
          "name": "Yufeng Han",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddcb",
          "name": "Bingxiang He",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddcc",
          "name": "Yuxiang Huang",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddcd",
          "name": "Cunliang Kong",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddce",
          "name": "Qiuzuo Li",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddcf",
          "name": "Siyuan Li",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddd0",
          "name": "Wenhao Li",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddd1",
          "name": "Yanghao Li",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddd2",
          "name": "Yishan Li",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddd3",
          "name": "Zhen Li",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddd4",
          "name": "Dan Liu",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddd5",
          "name": "Biyuan Lin",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddd6",
          "name": "Yankai Lin",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddd7",
          "name": "Xiang Long",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddd8",
          "name": "Quanyu Lu",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddd9",
          "name": "Yaxi Lu",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddda",
          "name": "Peiyan Luo",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4dddb",
          "name": "Hongya Lyu",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4dddc",
          "name": "Litu Ou",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4dddd",
          "name": "Yinxu Pan",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddde",
          "name": "Zekai Qu",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4dddf",
          "name": "Qundong Shi",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4dde0",
          "name": "Zijun Song",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4dde1",
          "name": "Jiayuan Su",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4dde2",
          "name": "Zhou Su",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4dde3",
          "name": "Ao Sun",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4dde4",
          "name": "Xianghui Sun",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4dde5",
          "name": "Peijun Tang",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4dde6",
          "name": "Fangzheng Wang",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4dde7",
          "name": "Feng Wang",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4dde8",
          "name": "Shuo Wang",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4dde9",
          "user": {
            "_id": "63be286fb3b8c44f8cecc16f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63be286fb3b8c44f8cecc16f/1CIkfEKoTnBYdYDSuQ8AT.jpeg",
            "isPro": false,
            "fullname": "Yudong Wang",
            "user": "BigDong",
            "type": "user"
          },
          "name": "Yudong Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-10T08:45:33.890Z",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddea",
          "name": "Yesai Wu",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddeb",
          "name": "Zhenyu Xiao",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddec",
          "name": "Jie Xie",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4dded",
          "name": "Zihao Xie",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddee",
          "name": "Yukun Yan",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddef",
          "name": "Jiarui Yuan",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddf0",
          "name": "Kaihuo Zhang",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddf1",
          "name": "Lei Zhang",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddf2",
          "name": "Linyue Zhang",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddf3",
          "name": "Xueren Zhang",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddf4",
          "name": "Yudi Zhang",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddf5",
          "name": "Hengyu Zhao",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddf6",
          "name": "Weilin Zhao",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddf7",
          "name": "Weilun Zhao",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddf8",
          "name": "Yuanqian Zhao",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddf9",
          "name": "Zhi Zheng",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddfa",
          "name": "Ge Zhou",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddfb",
          "name": "Jie Zhou",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddfc",
          "name": "Wei Zhou",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddfd",
          "name": "Zihan Zhou",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddfe",
          "name": "Zixuan Zhou",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddff",
          "name": "Zhiyuan Liu",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4de00",
          "name": "Guoyang Zeng",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4de01",
          "name": "Chao Jia",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4de02",
          "name": "Dahai Li",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4de03",
          "name": "Maosong Sun",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/608f6d72283d0a8d7be9d1f9/NF1aHsqQbJ_Dl18__cn2H.qt"
      ],
      "publishedAt": "2025-06-09T16:16:50.000Z",
      "submittedOnDailyAt": "2025-06-10T00:50:56.021Z",
      "title": "MiniCPM4 : Implémentation de base de terminal efficace de modèles de langage grands (LLMs)",
      "submittedOnDailyBy": {
        "_id": "608f6d72283d0a8d7be9d1f9",
        "avatarUrl": "/avatars/7f499a37019359a3c488ba6cc11751fc.svg",
        "isPro": false,
        "fullname": "Chaojun XIAO",
        "user": "xcjthu",
        "type": "user"
      },
      "summary": "Dans cet article, on présente MiniCPM4, un modèle de langage de grande échelle efficace, comme un modèle pour des dispositifs terminaux. Cette efficacité a été atteinte grâce à des innovations systématiques dans quatre éléments principaux : architecture du modèle, données d'entraînement, algorithmes d'entraînement et système d'inférence. En particulier, on propose InfLLM v2 dans l'architecture du modèle et une structure d'attention rare et apprenable pour accélérer le traitement de phrases longues tant au pré-traitement que au post-traitement. Dans les données d'entraînement, on propose UltraClean et UltraChat v2 pour atteindre un rendement satisfaisant du modèle lors d'entraînements de 800 milliards de tokens. Dans l'algorithme d'entraînement, on propose ModelTunnel v2 et met en œuvre un apprentissage par renforcement équilibré en association avec des algorithmes d'entraînement efficaces sur les données, y compris BitCPM, un LLM de 3 minutes avec efficacité sur les données. Dans le système d'inférence, on propose CPM.cu, qui combine une attention rare, une quantification du modèle et un traitement prédictif pour implémenter efficacement le pré-traitement et le post-traitement. MiniCPM4 est disponible en deux versions avec 0,5B et 8B paramètres, et montre un rendement excellent sur de multiples benchmarks de la même échelle que d'autres modèles ouverts, démontrant clairement son efficacité et sa pertinence. En particulier, MiniCPM4-8B montre une amélioration notable de la vitesse du traitement de séquences longues par rapport à Qwen3-8B. De plus, MiniCPM4 a réussi dans diverses applications, démontrant sa capacité à générer des recherches fiables et à utiliser des protocoles de contexte du modèle pour une large gamme d'utilisations.",
      "upvotes": 45,
      "discussionId": "6847924e3ec10bdd8ab4de04",
      "projectPage": "https://huggingface.co/collections/openbmb/minicpm4-6841ab29d180257e940baa9b",
      "githubRepo": "https://github.com/openbmb/minicpm",
      "ai_summary": "MiniCPM4, a highly efficient large language model for end-side devices, achieves superior performance using innovations in sparse attention, pre-training datasets, training algorithms, and inference systems.",
      "ai_keywords": [
        "InfLLM v2",
        "sparse attention mechanism",
        "UltraClean",
        "UltraChat v2",
        "prefilling",
        "decoding",
        "long-context processing",
        "ModelTunnel v2",
        "chunk-wise rollout",
        "data-efficient tenary LLM",
        "BitCPM",
        "CPM.cu",
        "model quantization",
        "speculative sampling"
      ]
    },
    "publishedAt": "2025-06-09T12:16:50.000Z",
    "title": "MiniCPM4: Ultra-Efficient LLMs on End Devices",
    "summary": "This paper introduces MiniCPM4, a highly efficient large language model (LLM)\ndesigned explicitly for end-side devices. We achieve this efficiency through\nsystematic innovation in four key dimensions: model architecture, training\ndata, training algorithms, and inference systems. Specifically, in terms of\nmodel architecture, we propose InfLLM v2, a trainable sparse attention\nmechanism that accelerates both prefilling and decoding phases for long-context\nprocessing. Regarding training data, we propose UltraClean, an efficient and\naccurate pre-training data filtering and generation strategy, and UltraChat v2,\na comprehensive supervised fine-tuning dataset. These datasets enable\nsatisfactory model performance to be achieved using just 8 trillion training\ntokens. Regarding training algorithms, we propose ModelTunnel v2 for efficient\npre-training strategy search, and improve existing post-training methods by\nintroducing chunk-wise rollout for load-balanced reinforcement learning and\ndata-efficient tenary LLM, BitCPM. Regarding inference systems, we propose\nCPM.cu that integrates sparse attention, model quantization, and speculative\nsampling to achieve efficient prefilling and decoding. To meet diverse\non-device requirements, MiniCPM4 is available in two versions, with 0.5B and 8B\nparameters, respectively. Sufficient evaluation results show that MiniCPM4\noutperforms open-source models of similar size across multiple benchmarks,\nhighlighting both its efficiency and effectiveness. Notably, MiniCPM4-8B\ndemonstrates significant speed improvements over Qwen3-8B when processing long\nsequences. Through further adaptation, MiniCPM4 successfully powers diverse\napplications, including trustworthy survey generation and tool use with model\ncontext protocol, clearly showcasing its broad usability.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/608f6d72283d0a8d7be9d1f9/NF1aHsqQbJ_Dl18__cn2H.qt"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07900.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "608f6d72283d0a8d7be9d1f9",
      "avatarUrl": "/avatars/7f499a37019359a3c488ba6cc11751fc.svg",
      "fullname": "Chaojun XIAO",
      "name": "xcjthu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.06444",
      "authors": [
        {
          "_id": "68479c1e3ec10bdd8ab4de9d",
          "name": "Ruizhong Qiu",
          "hidden": false
        },
        {
          "_id": "68479c1e3ec10bdd8ab4de9e",
          "name": "Gaotang Li",
          "hidden": false
        },
        {
          "_id": "68479c1e3ec10bdd8ab4de9f",
          "name": "Tianxin Wei",
          "hidden": false
        },
        {
          "_id": "68479c1e3ec10bdd8ab4dea0",
          "name": "Jingrui He",
          "hidden": false
        },
        {
          "_id": "68479c1e3ec10bdd8ab4dea1",
          "name": "Hanghang Tong",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/65370d95019de94263ad34a7/Uhd5i-LpHkPBjdpr4KJLi.jpeg"
      ],
      "publishedAt": "2025-06-06T18:05:45.000Z",
      "submittedOnDailyAt": "2025-06-10T01:29:36.727Z",
      "title": "Bien sûr ! Voici la traduction du texte anglais en français, en maintenant la professionnalité et la précision :\n\n**Safe-1 : Paradigme d'échelle de sécurité des modèles de langue**\n\nJ'espère que cela soit utile. Si vous avez besoin d'une aide supplémentaire, n'hésitez pas à me le dire.",
      "submittedOnDailyBy": {
        "_id": "65370d95019de94263ad34a7",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65370d95019de94263ad34a7/fH9SoJfX7yifXup0DvXkm.jpeg",
        "isPro": false,
        "fullname": "Ruizhong Qiu",
        "user": "q-rz",
        "type": "user"
      },
      "summary": "La recherche actuelle en sécurité se concentre principalement sur l'ajuste pendant l'étape d'apprentissage, avec l'objectif de former les LLM à des comportements sécurisés. Cependant, des études récentes ont montré que ces méthodes sont vulnérables à différents attaques de \"paniers à freins\". De plus, l'échelle de l'inférence a considérablement amélioré la capacité logique des LLM, mais son impact sur la sécurité n'a pas été étudié en détail. Pour aborder ces lacunes, notre recherche développe une nouvelle échelle de l'inférence pour la sécurité des LLM face aux nouveaux risques. Nous avons démontré que les méthodes d'échelle de l'inférence existantes, qui fonctionnent bien pour des tâches logiques, présentent des rendements faibles dans des contextes sécurisés et sont affectées comparativement au méthode de sampling Best-of-N. Cette inadéquation est attribuée aux hauts coûts de calcul dans l'évaluation des modèles de récompense par processus (PRM), qui représente un dilemme entre exploration et efficacité. Pour surmonter ce dilemme, nous proposons le paradigme d'échelle de l'inférence SAFFRON. L'un des éléments clé de ce paradigme est l'introduction du modèle de récompense par division (MRM), qui réduit considérablement le nombre d'évaluations des modèles de récompense nécessaires. Pour mettre en œuvre ce paradigme, nous proposons trois actions : (i) l'entraînement d'objectifs de sous-sous-ensembles pour le MRM, (ii) des limitations de l'exploration conservatrices pour éviter l'exploration de distributions, et (iii) une stratégie de caching de clé-valeur basée sur des arbres pour promouvoir la comparaison de caches de clé-valeur entre séquences lors de la recherche d'arbres. Les tests d'extension ont démontré l'effet de notre méthode. De plus, nous avons publié un jeu de données de récompense de sécurité à l'échelle des tokens (Safety4M) lié au modèle de division de récompense entraîné (Saffron-1), avec l'objectif de favoriser la recherche sur la sécurité des LLM. Notre code, nos modèles et nos données sont disponibles sur https://github.com/q-rz/saffron, et notre page web du projet est sur https://q-rz.github.io/p/saffron.",
      "upvotes": 40,
      "discussionId": "68479c1e3ec10bdd8ab4dea2",
      "projectPage": "https://q-rz.github.io/p/saffron",
      "githubRepo": "https://github.com/q-rz/saffron",
      "ai_summary": "SAFFRON, a novel inference scaling paradigm, enhances LLM safety by reducing reward model evaluations through a multifurcation reward model and other optimizations.",
      "ai_keywords": [
        "LLMs",
        "inference scaling",
        "safety assurance",
        "jailbreak attacks",
        "Best-of-N Sampling",
        "process reward model",
        "exploration--efficiency dilemma",
        "multifurcation reward model",
        "partial supervision training",
        "conservative exploration constraint",
        "Trie-based key--value caching",
        "Safety4M dataset"
      ]
    },
    "publishedAt": "2025-06-06T14:05:45.000Z",
    "title": "Saffron-1: Towards an Inference Scaling Paradigm for LLM Safety\n  Assurance",
    "summary": "Existing safety assurance research has primarily focused on training-phase\nalignment to instill safe behaviors into LLMs. However, recent studies have\nexposed these methods' susceptibility to diverse jailbreak attacks.\nConcurrently, inference scaling has significantly advanced LLM reasoning\ncapabilities but remains unexplored in the context of safety assurance.\nAddressing this gap, our work pioneers inference scaling for robust and\neffective LLM safety against emerging threats. We reveal that conventional\ninference scaling techniques, despite their success in reasoning tasks, perform\npoorly in safety contexts, even falling short of basic approaches like\nBest-of-N Sampling. We attribute this inefficiency to a newly identified\nchallenge, the exploration--efficiency dilemma, arising from the high\ncomputational overhead associated with frequent process reward model (PRM)\nevaluations. To overcome this dilemma, we propose SAFFRON, a novel inference\nscaling paradigm tailored explicitly for safety assurance. Central to our\napproach is the introduction of a multifurcation reward model (MRM) that\nsignificantly reduces the required number of reward model evaluations. To\noperationalize this paradigm, we further propose: (i) a partial supervision\ntraining objective for MRM, (ii) a conservative exploration constraint to\nprevent out-of-distribution explorations, and (iii) a Trie-based key--value\ncaching strategy that facilitates cache sharing across sequences during tree\nsearch. Extensive experiments validate the effectiveness of our method.\nAdditionally, we publicly release our trained multifurcation reward model\n(Saffron-1) and the accompanying token-level safety reward dataset (Safety4M)\nto accelerate future research in LLM safety. Our code, model, and data are\npublicly available at https://github.com/q-rz/saffron , and our project\nhomepage is at https://q-rz.github.io/p/saffron .",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/65370d95019de94263ad34a7/Uhd5i-LpHkPBjdpr4KJLi.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.06444.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65370d95019de94263ad34a7",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65370d95019de94263ad34a7/fH9SoJfX7yifXup0DvXkm.jpeg",
      "fullname": "Ruizhong Qiu",
      "name": "q-rz",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.07977",
      "authors": [
        {
          "_id": "684792f03ec10bdd8ab4de06",
          "name": "Jingjing Chang",
          "hidden": false
        },
        {
          "_id": "684792f03ec10bdd8ab4de07",
          "user": {
            "_id": "647469b9a51711a3b58bda2b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647469b9a51711a3b58bda2b/yeDf8Sa8IDEQyney1dGC9.jpeg",
            "isPro": false,
            "fullname": "Yixiao Fang",
            "user": "fangyixiao",
            "type": "user"
          },
          "name": "Yixiao Fang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-10T08:44:54.679Z",
          "hidden": false
        },
        {
          "_id": "684792f03ec10bdd8ab4de08",
          "name": "Peng Xing",
          "hidden": false
        },
        {
          "_id": "684792f03ec10bdd8ab4de09",
          "name": "Shuhan Wu",
          "hidden": false
        },
        {
          "_id": "684792f03ec10bdd8ab4de0a",
          "user": {
            "_id": "64b914c8ace99c0723ad83a9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b914c8ace99c0723ad83a9/B4gxNByeVY_xaOcjwiN1j.jpeg",
            "isPro": false,
            "fullname": "Wei Cheng",
            "user": "wchengad",
            "type": "user"
          },
          "name": "Wei Cheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-10T08:45:23.322Z",
          "hidden": false
        },
        {
          "_id": "684792f03ec10bdd8ab4de0b",
          "name": "Rui Wang",
          "hidden": false
        },
        {
          "_id": "684792f03ec10bdd8ab4de0c",
          "name": "Xianfang Zeng",
          "hidden": false
        },
        {
          "_id": "684792f03ec10bdd8ab4de0d",
          "name": "Gang Yu",
          "hidden": false
        },
        {
          "_id": "684792f03ec10bdd8ab4de0e",
          "name": "Hai-Bao Chen",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64b914c8ace99c0723ad83a9/Qq0Ue6mPEkoDJkIMGjRJ0.jpeg",
        "https://cdn-uploads.huggingface.co/production/uploads/64b914c8ace99c0723ad83a9/ywhobM8HPQHIRy4CRBkLI.jpeg"
      ],
      "publishedAt": "2025-06-09T17:50:21.000Z",
      "submittedOnDailyAt": "2025-06-10T00:52:27.518Z",
      "title": "OneIG-Bench : Évaluation Complète de la Génération d'Images",
      "submittedOnDailyBy": {
        "_id": "64b914c8ace99c0723ad83a9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b914c8ace99c0723ad83a9/B4gxNByeVY_xaOcjwiN1j.jpeg",
        "isPro": false,
        "fullname": "Wei Cheng",
        "user": "wchengad",
        "type": "user"
      },
      "summary": "Texte traduit en français :\n\nLe texte se traduit en japonais.",
      "upvotes": 35,
      "discussionId": "684792f03ec10bdd8ab4de0f",
      "projectPage": "https://oneig-bench.github.io/",
      "githubRepo": "https://github.com/OneIG-Bench/OneIG-Benchmark",
      "ai_summary": "OneIG-Bench is a comprehensive benchmark framework for evaluating text-to-image models across multiple dimensions including reasoning, text rendering, and diversity.",
      "ai_keywords": [
        "text-to-image (T2I) models",
        "prompt-image alignment",
        "text rendering precision",
        "reasoning-generated content",
        "stylization",
        "diversity"
      ]
    },
    "publishedAt": "2025-06-09T13:50:21.000Z",
    "title": "OneIG-Bench: Omni-dimensional Nuanced Evaluation for Image Generation",
    "summary": "Text-to-image (T2I) models have garnered significant attention for generating\nhigh-quality images aligned with text prompts. However, rapid T2I model\nadvancements reveal limitations in early benchmarks, lacking comprehensive\nevaluations, for example, the evaluation on reasoning, text rendering and\nstyle. Notably, recent state-of-the-art models, with their rich knowledge\nmodeling capabilities, show promising results on the image generation problems\nrequiring strong reasoning ability, yet existing evaluation systems have not\nadequately addressed this frontier. To systematically address these gaps, we\nintroduce OneIG-Bench, a meticulously designed comprehensive benchmark\nframework for fine-grained evaluation of T2I models across multiple dimensions,\nincluding prompt-image alignment, text rendering precision, reasoning-generated\ncontent, stylization, and diversity. By structuring the evaluation, this\nbenchmark enables in-depth analysis of model performance, helping researchers\nand practitioners pinpoint strengths and bottlenecks in the full pipeline of\nimage generation. Specifically, OneIG-Bench enables flexible evaluation by\nallowing users to focus on a particular evaluation subset. Instead of\ngenerating images for the entire set of prompts, users can generate images only\nfor the prompts associated with the selected dimension and complete the\ncorresponding evaluation accordingly. Our codebase and dataset are now publicly\navailable to facilitate reproducible evaluation studies and cross-model\ncomparisons within the T2I research community.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64b914c8ace99c0723ad83a9/Qq0Ue6mPEkoDJkIMGjRJ0.jpeg",
      "https://cdn-uploads.huggingface.co/production/uploads/64b914c8ace99c0723ad83a9/ywhobM8HPQHIRy4CRBkLI.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07977.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b914c8ace99c0723ad83a9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b914c8ace99c0723ad83a9/B4gxNByeVY_xaOcjwiN1j.jpeg",
      "fullname": "Wei Cheng",
      "name": "wchengad",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.07491",
      "authors": [
        {
          "_id": "684799083ec10bdd8ab4de8a",
          "user": {
            "_id": "63efbb1efc92a63ac81126d0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676655314726-noauth.jpeg",
            "isPro": true,
            "fullname": "Yongsen Mao",
            "user": "ysmao",
            "type": "user"
          },
          "name": "Yongsen Mao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-10T08:43:08.018Z",
          "hidden": false
        },
        {
          "_id": "684799083ec10bdd8ab4de8b",
          "name": "Junhao Zhong",
          "hidden": false
        },
        {
          "_id": "684799083ec10bdd8ab4de8c",
          "name": "Chuan Fang",
          "hidden": false
        },
        {
          "_id": "684799083ec10bdd8ab4de8d",
          "user": {
            "_id": "6437c0ead38ce48bdd4b0067",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6437c0ead38ce48bdd4b0067/9HdcjbD0ugiPypLDtQl8E.png",
            "isPro": false,
            "fullname": "Jia Zheng",
            "user": "bertjiazheng",
            "type": "user"
          },
          "name": "Jia Zheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-10T08:43:11.939Z",
          "hidden": false
        },
        {
          "_id": "684799083ec10bdd8ab4de8e",
          "name": "Rui Tang",
          "hidden": false
        },
        {
          "_id": "684799083ec10bdd8ab4de8f",
          "name": "Hao Zhu",
          "hidden": false
        },
        {
          "_id": "684799083ec10bdd8ab4de90",
          "name": "Ping Tan",
          "hidden": false
        },
        {
          "_id": "684799083ec10bdd8ab4de91",
          "name": "Zihan Zhou",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6437c0ead38ce48bdd4b0067/0ppuxf3I81w8mWqDgei0W.mp4"
      ],
      "publishedAt": "2025-06-09T07:10:58.000Z",
      "submittedOnDailyAt": "2025-06-10T01:04:13.223Z",
      "title": "Spectrum LM : Entraînement de grands modèles de langage pour la modélisation structurée des intérieurs",
      "submittedOnDailyBy": {
        "_id": "6437c0ead38ce48bdd4b0067",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6437c0ead38ce48bdd4b0067/9HdcjbD0ugiPypLDtQl8E.png",
        "isPro": false,
        "fullname": "Jia Zheng",
        "user": "bertjiazheng",
        "type": "user"
      },
      "summary": "SpatialLM est un grand modèle de langage qui traite des données de points 3D et génère des sorties pour comprendre des espaces 3D structurés. Ces sorties comprennent des éléments architecturaux tels que les murs, les portes et les fenêtres, ainsi que des catégories significatives. Différent des méthodes précédentes, SpatialLM ne utilise pas des réseaux spécialisés pour des tâches, mais se ajuste directement à une architecture de modèles de langage de code ouvert, par un ajustement fin.\n\nPour l'entraînement de SpatialLM, on a collecté 12 328 espaces intérieurs (54 778 chambres) de haute qualité de données synthétiques, et des études détaillées sur les modèles et les décisions d'entraînement ont été réalisées. Dans les cadres de référence publics, notre modèle montre le meilleur rendement en estimation spatiale simultanée et détection d'objets 3D, ce qui démontre la possibilité d'améliorer la compréhension spatiale des modèles modernes de langage d'images et de robotique dans des applications telles que l'expansion d'images et les robots concrets.",
      "upvotes": 22,
      "discussionId": "684799083ec10bdd8ab4de92",
      "projectPage": "https://manycore-research.github.io/SpatialLM",
      "githubRepo": "https://github.com/manycore-research/SpatialLM/",
      "ai_summary": "SpatialLM, a multimodal large language model, processes 3D point cloud data to generate structured scene understanding outputs, achieving state-of-the-art performance in layout estimation and competitive results in 3D object detection.",
      "ai_keywords": [
        "large language model",
        "3D point cloud",
        "structured 3D scene understanding",
        "multimodal LLM",
        "fine-tuning",
        "synthetic dataset",
        "ground-truth 3D annotations",
        "layout estimation",
        "3D object detection",
        "augmented reality",
        "embodied robotics"
      ]
    },
    "publishedAt": "2025-06-09T03:10:58.000Z",
    "title": "SpatialLM: Training Large Language Models for Structured Indoor Modeling",
    "summary": "SpatialLM is a large language model designed to process 3D point cloud data\nand generate structured 3D scene understanding outputs. These outputs include\narchitectural elements like walls, doors, windows, and oriented object boxes\nwith their semantic categories. Unlike previous methods which exploit\ntask-specific network designs, our model adheres to the standard multimodal LLM\narchitecture and is fine-tuned directly from open-source LLMs.\n  To train SpatialLM, we collect a large-scale, high-quality synthetic dataset\nconsisting of the point clouds of 12,328 indoor scenes (54,778 rooms) with\nground-truth 3D annotations, and conduct a careful study on various modeling\nand training decisions. On public benchmarks, our model gives state-of-the-art\nperformance in layout estimation and competitive results in 3D object\ndetection. With that, we show a feasible path for enhancing the spatial\nunderstanding capabilities of modern LLMs for applications in augmented\nreality, embodied robotics, and more.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6437c0ead38ce48bdd4b0067/0ppuxf3I81w8mWqDgei0W.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07491.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6437c0ead38ce48bdd4b0067",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6437c0ead38ce48bdd4b0067/9HdcjbD0ugiPypLDtQl8E.png",
      "fullname": "Jia Zheng",
      "name": "bertjiazheng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 15
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.07986",
      "authors": [
        {
          "_id": "68479b0f3ec10bdd8ab4de94",
          "name": "Zhengyao Lv",
          "hidden": false
        },
        {
          "_id": "68479b0f3ec10bdd8ab4de95",
          "name": "Tianlin Pan",
          "hidden": false
        },
        {
          "_id": "68479b0f3ec10bdd8ab4de96",
          "user": {
            "_id": "635f8ed47c05eb9f59963d3a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/635f8ed47c05eb9f59963d3a/uQf4p9N9pSaFy87Wg9v4k.jpeg",
            "isPro": false,
            "fullname": "ChenyangSi",
            "user": "ChenyangSi",
            "type": "user"
          },
          "name": "Chenyang Si",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-10T08:43:05.835Z",
          "hidden": false
        },
        {
          "_id": "68479b0f3ec10bdd8ab4de97",
          "name": "Zhaoxi Chen",
          "hidden": false
        },
        {
          "_id": "68479b0f3ec10bdd8ab4de98",
          "name": "Wangmeng Zuo",
          "hidden": false
        },
        {
          "_id": "68479b0f3ec10bdd8ab4de99",
          "name": "Ziwei Liu",
          "hidden": false
        },
        {
          "_id": "68479b0f3ec10bdd8ab4de9a",
          "name": "Kwan-Yee K. Wong",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-09T17:54:04.000Z",
      "submittedOnDailyAt": "2025-06-10T01:15:51.787Z",
      "title": "Le nouvel approche de l'interaction entre les divers modèles d'intelligence artificielle",
      "submittedOnDailyBy": {
        "_id": "645aff5121ab438e732c47c1",
        "avatarUrl": "/avatars/23b2a853139b0f2ae1fa88e2bd4e0056.svg",
        "isPro": false,
        "fullname": "Zhengyao Lv",
        "user": "cszy98",
        "type": "user"
      },
      "summary": "MM-DiTs a atteint un développement impressionnant dans la génération visuelle basée sur le texte. Cependant, des modèles récents comme FLUX rencontrent des difficultés pour atteindre une correspondance précise entre les prompts de texte et le contenu généré. Deux problèmes importants ont été identifiés dans la structure d'attention de MM-DiT : 1) l'inhibition de l'attention croisée modale en raison du déséquilibre entre tokens d'image et de texte, et 2) la manque de poids d'attention en fonction du temps, ce qui entrave la correspondance. Pour résoudre ces problèmes, on propose l'attention croisée modale régulée par la température (TACA). Cette méthodologie est efficace en termes de paramètres et utilise l'échelle de température et des ajustements temporels pour réorganiser dynamiquement l'intersection multimodale. La combinaison avec le fine-tuning de LoRA améliore significativement la correspondance texte-image sur le benchmark T2I-CompBench avec un minimum de chargement calcul. TACA a été vérifiée sur des modèles récents comme FLUX et SD3.5, montrant des améliorations dans la correspondance contextuelle de l'image par rapport à l'apparence d'objets, la combinaison de caractéristiques et les relations spatiales. Nos résultats soulignent l'importance de l'équilibre de l'attention croisée modale pour améliorer la fidélité significative dans les modèles de diffusion d'images à partir de texte. Notre code est disponible sur https://github.com/Vchitect/TACA.",
      "upvotes": 11,
      "discussionId": "68479b0f3ec10bdd8ab4de9b",
      "projectPage": "https://vchitect.github.io/TACA/",
      "githubRepo": "https://github.com/Vchitect/TACA",
      "ai_summary": "Temperature-Adjusted Cross-modal Attention (TACA) enhances text-image alignment in diffusion models by dynamically rebalancing multimodal interactions through temperature scaling and timestep-dependent adjustment.",
      "ai_keywords": [
        "Temperature-Adjusted Cross-modal Attention",
        "TACA",
        "multimodal interactions",
        "temperature scaling",
        "timestep-dependent adjustment",
        "FLUX",
        "SD3.5",
        "T2I-CompBench",
        "semantic fidelity",
        "text-to-image diffusion models"
      ]
    },
    "publishedAt": "2025-06-09T13:54:04.000Z",
    "title": "Rethinking Cross-Modal Interaction in Multimodal Diffusion Transformers",
    "summary": "Multimodal Diffusion Transformers (MM-DiTs) have achieved remarkable progress\nin text-driven visual generation. However, even state-of-the-art MM-DiT models\nlike FLUX struggle with achieving precise alignment between text prompts and\ngenerated content. We identify two key issues in the attention mechanism of\nMM-DiT, namely 1) the suppression of cross-modal attention due to token\nimbalance between visual and textual modalities and 2) the lack of\ntimestep-aware attention weighting, which hinder the alignment. To address\nthese issues, we propose Temperature-Adjusted Cross-modal Attention\n(TACA), a parameter-efficient method that dynamically rebalances multimodal\ninteractions through temperature scaling and timestep-dependent adjustment.\nWhen combined with LoRA fine-tuning, TACA significantly enhances text-image\nalignment on the T2I-CompBench benchmark with minimal computational overhead.\nWe tested TACA on state-of-the-art models like FLUX and SD3.5, demonstrating\nits ability to improve image-text alignment in terms of object appearance,\nattribute binding, and spatial relationships. Our findings highlight the\nimportance of balancing cross-modal attention in improving semantic fidelity in\ntext-to-image diffusion models. Our codes are publicly available at\nhttps://github.com/Vchitect/TACA",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07986.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645aff5121ab438e732c47c1",
      "avatarUrl": "/avatars/23b2a853139b0f2ae1fa88e2bd4e0056.svg",
      "fullname": "Zhengyao Lv",
      "name": "cszy98",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.07553",
      "authors": [
        {
          "_id": "684794a43ec10bdd8ab4de24",
          "user": {
            "_id": "65eaa07cb6c760d77468b4b6",
            "avatarUrl": "/avatars/4a1aae58986b40444351e0a167ca807c.svg",
            "isPro": false,
            "fullname": "Jingchao Wang",
            "user": "jcwang0602",
            "type": "user"
          },
          "name": "Jingchao Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-10T08:44:15.857Z",
          "hidden": false
        },
        {
          "_id": "684794a43ec10bdd8ab4de25",
          "user": {
            "_id": "65fd45473ccf43503350d837",
            "avatarUrl": "/avatars/11b9679945b3c89b142f0d62a312f362.svg",
            "isPro": false,
            "fullname": "Haote Yang",
            "user": "Hoter",
            "type": "user"
          },
          "name": "Haote Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-10T08:44:18.060Z",
          "hidden": false
        },
        {
          "_id": "684794a43ec10bdd8ab4de26",
          "name": "Jiang Wu",
          "hidden": false
        },
        {
          "_id": "684794a43ec10bdd8ab4de27",
          "name": "Yifan He",
          "hidden": false
        },
        {
          "_id": "684794a43ec10bdd8ab4de28",
          "name": "Xingjian Wei",
          "hidden": false
        },
        {
          "_id": "684794a43ec10bdd8ab4de29",
          "name": "Yinfan Wang",
          "hidden": false
        },
        {
          "_id": "684794a43ec10bdd8ab4de2a",
          "name": "Chengjin Liu",
          "hidden": false
        },
        {
          "_id": "684794a43ec10bdd8ab4de2b",
          "name": "Lingli Ge",
          "hidden": false
        },
        {
          "_id": "684794a43ec10bdd8ab4de2c",
          "name": "Lijun Wu",
          "hidden": false
        },
        {
          "_id": "684794a43ec10bdd8ab4de2d",
          "name": "Bin Wang",
          "hidden": false
        },
        {
          "_id": "684794a43ec10bdd8ab4de2e",
          "name": "Dahua Lin",
          "hidden": false
        },
        {
          "_id": "684794a43ec10bdd8ab4de2f",
          "name": "Conghui He",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-09T08:47:10.000Z",
      "submittedOnDailyAt": "2025-06-10T00:49:58.326Z",
      "title": "GTR-CoT : Trajectoire d'Exploration de Graphes de Scout Foray Microkernel\nReconnaissance de Structures",
      "submittedOnDailyBy": {
        "_id": "65fd45473ccf43503350d837",
        "avatarUrl": "/avatars/11b9679945b3c89b142f0d62a312f362.svg",
        "isPro": false,
        "fullname": "Haote Yang",
        "user": "Hoter",
        "type": "user"
      },
      "summary": "La Reconnaissance des Structures Chimiques (OCSR) joue un rôle crucial dans la digitalisation du savoir chimique, transformant des images de molécules en formats lisibles par la machine. Les modèles de langage visuel et de langage (VLMs) récents ont abordé ces problèmes, mais la capture d'images de structures complexes et de descriptions instables souvent présentent des défis. Pour relever ces défis, nous présentons le GTR-Mol-VLM. Ce modèle présente deux innovations principales : 1) une architecture de Visual Coshot Escape qui imite la théorie utilisée par les humains pour prédire séquentiellement les liaisons d'atomes et analyser progressivement la structure des molécules. 2) un approche axée sur les données pour résoudre l'inégalité entre les structures simples des images et les descriptions étendues. Pour le développement du modèle, nous avons construit le GTR-CoT-1.3M. Ce jeu de données contient un grand nombre de jeux d'entraînement d'instructions avec des descriptions minutieusement modifiées. De plus, nous présentons MolRec-Bench, le premier benchmark pour évaluer la précision de l'analyse de graphes en OCSR. Les expériences détaillées montrent que le GTR-Mol-VLM obtient des résultats supérieurs aux modèles spécialisés, aux VLMs en chimie et aux VLMs généraux commercials. En particulier, pour des images de molécules qui incluent des structures simples de groupes fonctionnels, le GTR-Mol-VLM améliore d'environ 14% en termes de scores SMILE et de métriques basées sur les graphes. Nous espérons que cette recherche contribue à répondre à la nécessité réelle de la technologie OCSR et encourage le développement de la informatique chimique et de l'IA pour les sciences. Nous publions le GTR-CoT.",
      "upvotes": 11,
      "discussionId": "684794a43ec10bdd8ab4de30",
      "ai_summary": "GTR-Mol-VLM, featuring graph traversal and data-centric principles, outperforms existing models in Optical Chemical Structure Recognition by accurately parsing molecular graphs and handling abbreviated structures.",
      "ai_keywords": [
        "Graph Traversal as Visual Chain of Thought",
        "Faithfully Recognize What You've Seen",
        "GTR-CoT-1.3M",
        "MolRec-Bench",
        "graph-parsing accuracy",
        "Optical Chemical Structure Recognition",
        "VLMs",
        "SMILES-based",
        "graph-based metrics"
      ]
    },
    "publishedAt": "2025-06-09T04:47:10.000Z",
    "title": "GTR-CoT: Graph Traversal as Visual Chain of Thought for Molecular\n  Structure Recognition",
    "summary": "Optical Chemical Structure Recognition (OCSR) is crucial for digitizing\nchemical knowledge by converting molecular images into machine-readable\nformats. While recent vision-language models (VLMs) have shown potential in\nthis task, their image-captioning approach often struggles with complex\nmolecular structures and inconsistent annotations. To overcome these\nchallenges, we introduce GTR-Mol-VLM, a novel framework featuring two key\ninnovations: (1) the Graph Traversal as Visual Chain of Thought\nmechanism that emulates human reasoning by incrementally parsing molecular\ngraphs through sequential atom-bond predictions, and (2) the data-centric\nprinciple of Faithfully Recognize What You've Seen, which addresses\nthe mismatch between abbreviated structures in images and their expanded\nannotations. To support model development, we constructed GTR-CoT-1.3M, a\nlarge-scale instruction-tuning dataset with meticulously corrected annotations,\nand introduced MolRec-Bench, the first benchmark designed for a fine-grained\nevaluation of graph-parsing accuracy in OCSR. Comprehensive experiments\ndemonstrate that GTR-Mol-VLM achieves superior results compared to specialist\nmodels, chemistry-domain VLMs, and commercial general-purpose VLMs. Notably, in\nscenarios involving molecular images with functional group abbreviations,\nGTR-Mol-VLM outperforms the second-best baseline by approximately 14 percentage\npoints, both in SMILES-based and graph-based metrics. We hope that this work\nwill drive OCSR technology to more effectively meet real-world needs, thereby\nadvancing the fields of cheminformatics and AI for Science. We will release\nGTR-CoT at https://github.com/opendatalab/GTR-CoT.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07553.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65fd45473ccf43503350d837",
      "avatarUrl": "/avatars/11b9679945b3c89b142f0d62a312f362.svg",
      "fullname": "Haote Yang",
      "name": "Hoter",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.07712",
      "authors": [
        {
          "_id": "684790cd3ec10bdd8ab4ddaa",
          "user": {
            "_id": "66dfb6bac93721c02f75f37e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/zzoX0tpkHCNsFGjtnATSt.png",
            "isPro": false,
            "fullname": "Renjie",
            "user": "RogerLos",
            "type": "user"
          },
          "name": "Renjie Luo",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-10T01:56:30.026Z",
          "hidden": false
        },
        {
          "_id": "684790cd3ec10bdd8ab4ddab",
          "name": "Jiaxi Li",
          "hidden": false
        },
        {
          "_id": "684790cd3ec10bdd8ab4ddac",
          "user": {
            "_id": "65d7b983baa72790a1151923",
            "avatarUrl": "/avatars/938531e84ca01a0c5a2a174057e3e9c5.svg",
            "isPro": false,
            "fullname": "Chen Huang",
            "user": "Albus-Chen",
            "type": "user"
          },
          "name": "Chen Huang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-10T09:19:43.446Z",
          "hidden": false
        },
        {
          "_id": "684790cd3ec10bdd8ab4ddad",
          "name": "Wei Lu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-09T12:56:41.000Z",
      "submittedOnDailyAt": "2025-06-10T00:30:07.286Z",
      "title": "Parcourant le bateau : Le chemin pour l'entraînement efficace à long terme dans les modèles de langage de petite taille",
      "submittedOnDailyBy": {
        "_id": "66dfb6bac93721c02f75f37e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/zzoX0tpkHCNsFGjtnATSt.png",
        "isPro": false,
        "fullname": "Renjie",
        "user": "RogerLos",
        "type": "user"
      },
      "summary": "Le contrôle de CoT long (Long Concept) est souvent utilisé pour renforcer la logique des modèles de langage. Il est efficace pour des modèles grands, mais lorsqu'on utilise des données de CoT long limitées pour entraîner des petits modèles de langage (SLMs; moins de 3B paramètres), on observe un déclin significatif du rendement logique, appelé \"déclin de CoT long\". Ce phénomène a été observé dans de nombreuses tests avec des familles telles que Qwen2.5, LLaMA3 et Gemma3. Dans ce contexte, les modèles entraînés avec des exemples de CoT long de 8k perdent environ 25% de leur rendement avant l'ajustement. De plus, pour des petits modèles, même en entraînant des exemples de 220k de CoT long, il n'est pas possible de récupérer ou d'excéder le rendement avant l'ajustement. Ce phénomène est analysé comme une accumulation d'erreurs, car tandis que les réponses longues renforcent la fonction logique à chaque étape, le risque de répéter des erreurs augmente. De plus, le déclin de CoT long a également un impact négatif sur l'entraînement de renforcement (RL) lors de la phase postérieure. Il a été confirmé que ce phénomène peut être mitigé avec des ajustements de microcontrôle suffisamment grands (SFT). Ces résultats remettent en question l'hypothèse générale sur les avantages de l'entraînement de SLMs avec CoT long et fournissent une ligne directrice pratique pour la construction de modèles logiques de petite taille.",
      "upvotes": 10,
      "discussionId": "684790cd3ec10bdd8ab4ddae",
      "ai_summary": "Small language models experience significant performance declines when trained on long chain-of-thought data due to error accumulation, impacting downstream reinforcement learning but potentially mitigated by extensive supervised fine-tuning.",
      "ai_keywords": [
        "Long chain-of-thought",
        "Long CoT Degradation",
        "small language models",
        "SLMs",
        "Qwen2.5",
        "LLaMA3",
        "Gemma3",
        "error accumulation",
        "supervised fine-tuning",
        "SFT",
        "reinforcement learning"
      ]
    },
    "publishedAt": "2025-06-09T08:56:41.000Z",
    "title": "Through the Valley: Path to Effective Long CoT Training for Small\n  Language Models",
    "summary": "Long chain-of-thought (CoT) supervision has become a common strategy to\nenhance reasoning in language models. While effective for large models, we\nidentify a phenomenon we call Long CoT Degradation, in which small language\nmodels (SLMs; <=3B parameters) trained on limited long CoT data experience\nsignificant performance deterioration. Through extensive experiments on the\nQwen2.5, LLaMA3 and Gemma3 families, we demonstrate that this degradation is\nwidespread across SLMs. In some settings, models trained on only 8k long CoT\nexamples lose up to 75% of their original performance before fine-tuning.\nStrikingly, we further observe that for some particularly small models, even\ntraining on 220k long CoT examples fails to recover or surpass their original\nperformance prior to fine-tuning. Our analysis attributes this effect to error\naccumulation: while longer responses increase the capacity for multi-step\nreasoning, they also amplify the risk of compounding mistakes. Furthermore, we\nfind that Long CoT Degradation may negatively impacts downstream reinforcement\nlearning (RL), although this can be alleviated by sufficiently scaled\nsupervised fine-tuning (SFT). Our findings challenge common assumptions about\nthe benefits of long CoT training for SLMs and offer practical guidance for\nbuilding more effective small-scale reasoning models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07712.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66dfb6bac93721c02f75f37e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/zzoX0tpkHCNsFGjtnATSt.png",
      "fullname": "Renjie",
      "name": "RogerLos",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.07530",
      "authors": [
        {
          "_id": "68478dae3ec10bdd8ab4dd9b",
          "name": "Hongyu Wang",
          "hidden": false
        },
        {
          "_id": "68478dae3ec10bdd8ab4dd9c",
          "name": "Chuyan Xiong",
          "hidden": false
        },
        {
          "_id": "68478dae3ec10bdd8ab4dd9d",
          "name": "Ruiping Wang",
          "hidden": false
        },
        {
          "_id": "68478dae3ec10bdd8ab4dd9e",
          "name": "Xilin Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-09T08:15:11.000Z",
      "submittedOnDailyAt": "2025-06-10T00:14:07.356Z",
      "title": "BitVLA : Formula de Manipulación de Modelos de Acciones de Larga Distancia de Visión 1 Bit",
      "submittedOnDailyBy": {
        "_id": "63f71771d36951307fcb4dcd",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f71771d36951307fcb4dcd/1c-GSQmSSuDXyVWjxZFy_.jpeg",
        "isPro": false,
        "fullname": "Hongyu Wang",
        "user": "hongyuw",
        "type": "user"
      },
      "summary": "Le modèle d'action VLA (Vision Language Action) montre des capacités impressionnantes pour différentes tâches complexes de l'automatisation. Cependant, l'augmentation du taille du modèle a devenu un problème important pour les systèmes de machines à faibles ressources. Il a été démontré que la prédiction d'un bit est efficace pour améliorer l'efficacité de l'inférence dans des modèles de langage grands sans perdre significativement de performance, mais son application à des modèles VLA a encore été peu explorée. Dans cette étude, nous présentons le premier modèle VLA à 1 bit pour l'automatisation, nommé \"BitVLA\". Dans ce modèle, tous les paramètres sont composés de trois valeurs : {-1, 0, 1}. De plus, nous proposons une stratégie d'entraînement pour réduire la mémoire de l'encodeur visuel par le processus de distillation. Dans ce processus, l'encodeur complet peut ajuster mieux les représentations potentielles du modèle cible. Par conséquent, BitVLA, au lieu de prédire avec des bits de plus de 4, atteint un rendement comparable aux modèles leaders comme OpenVLA-OFT sur le benchmark LIBERO, en utilisant seulement le 29,8% de la mémoire. Ces résultats montrent que BitVLA peut améliorer la possibilité de traitement dans les dispositifs de bord à faibles ressources en mémoire. Les codes et les poids du modèle sont disponibles sur GitHub : https://github.com/ustcwhy/BitVLA.",
      "upvotes": 9,
      "discussionId": "68478dae3ec10bdd8ab4dd9f",
      "githubRepo": "https://github.com/ustcwhy/BitVLA",
      "ai_summary": "BitVLA, a 1-bit VLA model with ternary parameters, achieves comparable performance to OpenVLA-OFT on LIBERO while using 29.8% less memory through distillation-aware training.",
      "ai_keywords": [
        "VLA models",
        "1-bit pretraining",
        "ternary parameters",
        "distillation-aware training",
        "vision encoder",
        "full-precision encoder",
        "latent representations",
        "memory footprint",
        "robotics manipulation",
        "OpenVLA-OFT",
        "LIBERO benchmark",
        "memory-constrained edge devices"
      ]
    },
    "publishedAt": "2025-06-09T04:15:11.000Z",
    "title": "BitVLA: 1-bit Vision-Language-Action Models for Robotics Manipulation",
    "summary": "Vision-Language-Action (VLA) models have shown impressive capabilities across\na wide range of robotics manipulation tasks. However, their growing model size\nposes significant challenges for deployment on resource-constrained robotic\nsystems. While 1-bit pretraining has proven effective for enhancing the\ninference efficiency of large language models with minimal performance loss,\nits application to VLA models remains underexplored. In this work, we present\nBitVLA, the first 1-bit VLA model for robotics manipulation, in which every\nparameter is ternary, i.e., {-1, 0, 1}. To further reduce the memory footprint\nof the vision encoder, we propose the distillation-aware training strategy that\ncompresses the full-precision encoder to 1.58-bit weights. During this process,\na full-precision encoder serves as a teacher model to better align latent\nrepresentations. Despite the lack of large-scale robotics pretraining, BitVLA\nachieves performance comparable to the state-of-the-art model OpenVLA-OFT with\n4-bit post-training quantization on the LIBERO benchmark, while consuming only\n29.8% of the memory. These results highlight BitVLA's promise for deployment on\nmemory-constrained edge devices. We release the code and model weights in\nhttps://github.com/ustcwhy/BitVLA.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07530.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63f71771d36951307fcb4dcd",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f71771d36951307fcb4dcd/1c-GSQmSSuDXyVWjxZFy_.jpeg",
      "fullname": "Hongyu Wang",
      "name": "hongyuw",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 18
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.07298",
      "authors": [
        {
          "_id": "684795e83ec10bdd8ab4de6a",
          "user": {
            "_id": "62de9e6fdcdc9043efa8b756",
            "avatarUrl": "/avatars/c26974c740633d143f7382f0858ea99a.svg",
            "isPro": false,
            "fullname": "Yijia Dai",
            "user": "DaiYijia",
            "type": "user"
          },
          "name": "Yijia Dai",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-10T08:43:53.562Z",
          "hidden": false
        },
        {
          "_id": "684795e83ec10bdd8ab4de6b",
          "name": "Zhaolin Gao",
          "hidden": false
        },
        {
          "_id": "684795e83ec10bdd8ab4de6c",
          "name": "Yahya Satter",
          "hidden": false
        },
        {
          "_id": "684795e83ec10bdd8ab4de6d",
          "user": {
            "_id": "664f92095a60ca2484b90d7a",
            "avatarUrl": "/avatars/3232bb702ed479ac821b7a5dfb457d0b.svg",
            "isPro": false,
            "fullname": "Sarah Dean",
            "user": "sarahdean",
            "type": "user"
          },
          "name": "Sarah Dean",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-10T02:18:19.596Z",
          "hidden": false
        },
        {
          "_id": "684795e83ec10bdd8ab4de6e",
          "name": "Jennifer J. Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-08T21:49:38.000Z",
      "submittedOnDailyAt": "2025-06-10T01:00:39.516Z",
      "title": "Les modèles de langage à grande échelle d'apprentissage prédictif apprennent le modèle de Markov caché dans le contexte.",
      "submittedOnDailyBy": {
        "_id": "652eec0aabc673c4204c459e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652eec0aabc673c4204c459e/9otSQFP8G3S8zarR1Y5rE.jpeg",
        "isPro": false,
        "fullname": "Zhaolin Gao",
        "user": "GitBag",
        "type": "user"
      },
      "summary": "Les modèles de Markov cachés (HMMs) sont des outils de base pour modéliser des données de séquence, mais ils sont complexes à calculer pour s'adapter aux données du monde réel. Dans cet article, nous montrons que les modèles de langage à grande échelle (LLMs) entraînés précédemment ont la capacité d'inférer des motifs à partir d'exemples dans le prompt et peuvent modéliser efficacement les données générées par les HMMs. Dans un ensemble de HMMs synthétiques, les LLMs atteignent une précision de prédiction proche de l'optimale théorique. Des nouveaux tendances d'échelle en fonction des caractéristiques des HMMs sont découvertes et des prédictions théoriques sur ces observations expérimentales sont fournies. De plus, nous offrons des astuces pratiques pour les scientifiques pour utiliser l'apprentissage en contexte (ICL) comme outil de diagnostic. Dans des tâches déterministes d'animaux du monde réel, l'ICL atteint un rendement compétitif avec les modèles conçus par des experts humains. Selon notre information, cela est un exemple qui montre pour la première fois que l'ICL peut apprendre des séquences générées par les HMMs pour faire des prédictions, établissant ainsi le potentiel possible des LLMs pour comprendre l'apprentissage en contexte et développer des outils robustes pour l'étude de structures cachées dans des données scientifiques complexes.",
      "upvotes": 8,
      "discussionId": "684795e93ec10bdd8ab4de6f",
      "githubRepo": "https://github.com/DaiYijia02/icl-hmm",
      "ai_summary": "In-context learning in large language models can effectively model sequences generated by hidden Markov models, achieving predictive accuracy and uncovering scaling trends, thus demonstrating its potential as a diagnostic tool for complex scientific data.",
      "ai_keywords": [
        "hidden Markov models",
        "HMMs",
        "large language models",
        "LLMs",
        "in-context learning",
        "IC",
        "predictive accuracy",
        "theoretical optimum",
        "synthetic HMMs",
        "scaling trends",
        "empirical observations",
        "animal decision-making tasks",
        "human experts"
      ]
    },
    "publishedAt": "2025-06-08T17:49:38.000Z",
    "title": "Pre-trained Large Language Models Learn Hidden Markov Models In-context",
    "summary": "Hidden Markov Models (HMMs) are foundational tools for modeling sequential\ndata with latent Markovian structure, yet fitting them to real-world data\nremains computationally challenging. In this work, we show that pre-trained\nlarge language models (LLMs) can effectively model data generated by HMMs via\nin-context learning (ICL)x2013their ability to infer patterns from\nexamples within a prompt. On a diverse set of synthetic HMMs, LLMs achieve\npredictive accuracy approaching the theoretical optimum. We uncover novel\nscaling trends influenced by HMM properties, and offer theoretical conjectures\nfor these empirical observations. We also provide practical guidelines for\nscientists on using ICL as a diagnostic tool for complex data. On real-world\nanimal decision-making tasks, ICL achieves competitive performance with models\ndesigned by human experts. To our knowledge, this is the first demonstration\nthat ICL can learn and predict HMM-generated sequencesx2013an\nadvance that deepens our understanding of in-context learning in LLMs and\nestablishes its potential as a powerful tool for uncovering hidden structure in\ncomplex scientific data.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07298.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "652eec0aabc673c4204c459e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652eec0aabc673c4204c459e/9otSQFP8G3S8zarR1Y5rE.jpeg",
      "fullname": "Zhaolin Gao",
      "name": "GitBag",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.07463",
      "authors": [
        {
          "_id": "68478a493ec10bdd8ab4dd90",
          "user": {
            "_id": "632c234f42c386ebd2710434",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/632c234f42c386ebd2710434/HyWRWi063S69JTy_IMjoe.jpeg",
            "isPro": false,
            "fullname": "Guang Liu",
            "user": "ZacLiu",
            "type": "user"
          },
          "name": "Guang Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-10T09:19:47.891Z",
          "hidden": false
        },
        {
          "_id": "68478a493ec10bdd8ab4dd91",
          "user": {
            "_id": "63a11ce02fabbbb899a01d58",
            "avatarUrl": "/avatars/ee3d4088b6d32b2c18b8be91913e90dd.svg",
            "isPro": false,
            "fullname": "ldwang",
            "user": "ldwang",
            "type": "user"
          },
          "name": "Liangdong Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-10T09:19:50.398Z",
          "hidden": false
        },
        {
          "_id": "68478a493ec10bdd8ab4dd92",
          "name": "Jijie Li",
          "hidden": false
        },
        {
          "_id": "68478a493ec10bdd8ab4dd93",
          "name": "Yang Yu",
          "hidden": false
        },
        {
          "_id": "68478a493ec10bdd8ab4dd94",
          "name": "Yao Xu",
          "hidden": false
        },
        {
          "_id": "68478a493ec10bdd8ab4dd95",
          "name": "Jiabei Chen",
          "hidden": false
        },
        {
          "_id": "68478a493ec10bdd8ab4dd96",
          "name": "Yu Bai",
          "hidden": false
        },
        {
          "_id": "68478a493ec10bdd8ab4dd97",
          "name": "Feng Liao",
          "hidden": false
        },
        {
          "_id": "68478a493ec10bdd8ab4dd98",
          "user": {
            "_id": "629aa3155ab4232a3fe0893e",
            "avatarUrl": "/avatars/cf2d4a9295b5da9e2e4d2278bbb36040.svg",
            "isPro": false,
            "fullname": "Yonghua Lin",
            "user": "Yonghua",
            "type": "user"
          },
          "name": "Yonghua Lin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-10T09:39:44.976Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-09T06:14:19.000Z",
      "submittedOnDailyAt": "2025-06-10T01:15:00.564Z",
      "title": "CCI4.0 : Renforcement de la théorie du langage dans les modèles de grands langages par l'entraînement préliminaire de bibliothèques",
      "submittedOnDailyBy": {
        "_id": "632c234f42c386ebd2710434",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/632c234f42c386ebd2710434/HyWRWi063S69JTy_IMjoe.jpeg",
        "isPro": false,
        "fullname": "Guang Liu",
        "user": "ZacLiu",
        "type": "user"
      },
      "summary": "CCI4.0 se présente. Il s'agit d'un ensemble de données de prédiction et d'édition pour deux langages à grande échelle développé pour atteindre une qualité de données urgente et les diverses tendances logiques du être humain. CCI4.0 occupe environ 35 TB d'espace disque et est constitué de deux sous-ensembles de données : CCI4.0-M2-Base et CCI4.0-M2-CoT. CCI4.0-M2-Base intègre un ensemble de données en anglais de 22,5 TB de Nemotron-CC, ainsi que des données en mathématiques, Wiki, arXiv et des codes de diverses sources, totalisant 5,2 TB. Ces données sont principalement fournies à partir de jeux de données traités, bien que les normes de qualité dans chaque domaine varient dynamiquement et nécessitent l'expérience et le travail de plusieurs experts. Par conséquent, un nouveau flux de travail est proposé pour discuter de la qualité des données basé sur des modèles. Ce flux de travail se réalise à travers deux étapes d'élimination, un score de qualité pour un classifieur multiclasse et un filtrage fluyent par domaine. Il est extrait 4,5 milliards de pages de modèles de CoT (Chain-of-Thought) et nommé CCI4.0-M2-CoT. Cela diffère de l'amélioration de CoT dans les modèles grands et montre une réduction significative de la possibilité d'hallucination. Dans des évaluations expérimentales, les LLMs entraînés sur CCI4.0 reçoivent des signaux d'entraînement relativement fiables, surtout dans les tâches de mathématiques et de code, montrant une amélioration constante. Nos résultats soulignent l'importance de l'édition stricte des données et des modèles de pensée humains, et jouent un rôle crucial dans l'amélioration des LLMs, ainsi qu'ils offrent plusieurs lumières sur le traitement automatique de corpus préalablement entraînés.",
      "upvotes": 7,
      "discussionId": "68478a493ec10bdd8ab4dd99",
      "projectPage": "https://openseek.baai.ac.cn/",
      "githubRepo": "https://github.com/FlagAI-Open/OpenSeek",
      "ai_summary": "A large-scale bilingual pre-training dataset, CCI4.0, enhances data quality and diverse reasoning patterns for language models, leading to improved performance in downstream tasks like math and code reflection.",
      "ai_keywords": [
        "pre-training dataset",
        "bilingual pre-training",
        "data quality",
        "reasoning trajectory",
        "deduplication",
        "multiclassifier quality scoring",
        "domain-aware fluency filtering",
        "Chain-of-Thought",
        "CoT extraction",
        "language models",
        "LLMs",
        "downstream tasks",
        "math tasks",
        "code reflection tasks",
        "data curation",
        "human thinking templates"
      ]
    },
    "publishedAt": "2025-06-09T02:14:19.000Z",
    "title": "CCI4.0: A Bilingual Pretraining Dataset for Enhancing Reasoning in Large\n  Language Models",
    "summary": "We introduce CCI4.0, a large-scale bilingual pre-training dataset engineered\nfor superior data quality and diverse human-like reasoning trajectory. CCI4.0\noccupies roughly 35 TB of disk space and comprises two sub-datasets:\nCCI4.0-M2-Base and CCI4.0-M2-CoT. CCI4.0-M2-Base combines a 5.2 TB carefully\ncurated Chinese web corpus, a 22.5 TB English subset from Nemotron-CC, and\ndiverse sources from math, wiki, arxiv, and code. Although these data are\nmostly sourced from well-processed datasets, the quality standards of various\ndomains are dynamic and require extensive expert experience and labor to\nprocess. So, we propose a novel pipeline justifying data quality mainly based\non models through two-stage deduplication, multiclassifier quality scoring, and\ndomain-aware fluency filtering. We extract 4.5 billion pieces of\nCoT(Chain-of-Thought) templates, named CCI4.0-M2-CoT. Differing from the\ndistillation of CoT from larger models, our proposed staged CoT extraction\nexemplifies diverse reasoning patterns and significantly decreases the\npossibility of hallucination. Empirical evaluations demonstrate that LLMs\npre-trained in CCI4.0 benefit from cleaner, more reliable training signals,\nyielding consistent improvements in downstream tasks, especially in math and\ncode reflection tasks. Our results underscore the critical role of rigorous\ndata curation and human thinking templates in advancing LLM performance,\nshedding some light on automatically processing pretraining corpora.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07463.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "632c234f42c386ebd2710434",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/632c234f42c386ebd2710434/HyWRWi063S69JTy_IMjoe.jpeg",
      "fullname": "Guang Liu",
      "name": "ZacLiu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.07434",
      "authors": [
        {
          "_id": "684797f33ec10bdd8ab4de7a",
          "user": {
            "_id": "6447ca6ca478b20f1755b294",
            "avatarUrl": "/avatars/5049856b5ed1b74533fff902e14b4c7c.svg",
            "isPro": false,
            "fullname": "Feifan Song",
            "user": "songff",
            "type": "user"
          },
          "name": "Feifan Song",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-10T08:43:29.497Z",
          "hidden": false
        },
        {
          "_id": "684797f33ec10bdd8ab4de7b",
          "user": {
            "_id": "67244a81aa8556c561925ab6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/w-vZ0uwYACagrNq-H1oyO.jpeg",
            "isPro": false,
            "fullname": "Shaohang Wei",
            "user": "SylvainWei",
            "type": "user"
          },
          "name": "Shaohang Wei",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-10T08:43:27.498Z",
          "hidden": false
        },
        {
          "_id": "684797f33ec10bdd8ab4de7c",
          "name": "Wen Luo",
          "hidden": false
        },
        {
          "_id": "684797f33ec10bdd8ab4de7d",
          "name": "Yuxuan Fan",
          "hidden": false
        },
        {
          "_id": "684797f33ec10bdd8ab4de7e",
          "name": "Tianyu Liu",
          "hidden": false
        },
        {
          "_id": "684797f33ec10bdd8ab4de7f",
          "name": "Guoyin Wang",
          "hidden": false
        },
        {
          "_id": "684797f33ec10bdd8ab4de80",
          "name": "Houfeng Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-09T05:21:22.000Z",
      "submittedOnDailyAt": "2025-06-10T00:59:10.518Z",
      "title": "Commence par une petite moitié facile et termine : ajustement de l'orientation des ressources faibles en interprétant la transition d'un point faible à un point fort.",
      "submittedOnDailyBy": {
        "_id": "6447ca6ca478b20f1755b294",
        "avatarUrl": "/avatars/5049856b5ed1b74533fff902e14b4c7c.svg",
        "isPro": false,
        "fullname": "Feifan Song",
        "user": "songff",
        "type": "user"
      },
      "summary": "Les modèles de langage grands (LLMs) doivent éviter de créer du contenu qui ne convient pas aux préoccupations humaines, qui soit initial, inadéquat ou sans sens. Récemment, des méthodes ont été développées pour répondre à ces modèles avec peu de ressources, mais obtenir du contenu de haute qualité reste un défi. Après avoir observé que la difficulté de la réponse se concentrait sur le début de l'interprétation, nous proposons un nouveau cadre de travail \"Decoding from Weak to Strong (WSD)\" pour améliorer la capacité de réponse. Un petit modèle de réponse élimine la partie initiale correspondante avant que le modèle de base génère la réponse, et ensuite, il est contrôlé automatiquement par une structure de switch similaire à celle d'un programme de machine. De plus, nous avons collecté un nouveau ensemble de données appelé \"GenerAlign\" et avons fine-tuné un petit modèle de 700-3B pour l'utiliser comme modèle de rejet. Ce modèle améliore efficacement le modèle de base sous le cadre de travail WSD et montre un rendement supérieur à tous les méthodes de base, évitant ainsi des pertes dans les tâches de décharge. De plus, nous avons analysé l'impact de différentes configurations et l'efficacité en temps, ainsi que la structure interne de WSD en détail.",
      "upvotes": 7,
      "discussionId": "684797f33ec10bdd8ab4de81",
      "githubRepo": "https://github.com/F2-Song/Weak-to-Strong-Decoding",
      "ai_summary": "A new decoding framework (Weak-to-Strong Decoding, WSD) enhances the alignment of large language models by using a small aligned model to draft responses, followed by the base model, with a design to prevent degradation in performance on downstream tasks.",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "LLM alignment",
        "human preferences",
        "low-resource methods",
        "decoding",
        "small aligned model",
        "auto-switch mechanism",
        "GenerAlign",
        "Pilot-3B",
        "draft model",
        "alignment tax",
        "intrinsic mechanisms"
      ]
    },
    "publishedAt": "2025-06-09T01:21:22.000Z",
    "title": "Well Begun is Half Done: Low-resource Preference Alignment by\n  Weak-to-Strong Decoding",
    "summary": "Large Language Models (LLMs) require alignment with human preferences to\navoid generating offensive, false, or meaningless content. Recently,\nlow-resource methods for LLM alignment have been popular, while still facing\nchallenges in obtaining both high-quality and aligned content. Motivated by the\nobservation that the difficulty of generating aligned responses is concentrated\nat the beginning of decoding, we propose a novel framework, Weak-to-Strong\nDecoding (WSD), to enhance the alignment ability of base models by the guidance\nof a small aligned model. The small model first drafts well-aligned beginnings,\nfollowed by the large base model to continue the rest, controlled by a\nwell-designed auto-switch mechanism. We also collect a new dataset, GenerAlign,\nto fine-tune a small-sized Pilot-3B as the draft model, which effectively\nenhances different base models under the WSD framework to outperform all\nbaseline methods, while avoiding degradation on downstream tasks, termed as the\nalignment tax. Extensive experiments are further conducted to examine the\nimpact of different settings and time efficiency, as well as analyses on the\nintrinsic mechanisms of WSD in depth.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07434.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6447ca6ca478b20f1755b294",
      "avatarUrl": "/avatars/5049856b5ed1b74533fff902e14b4c7c.svg",
      "fullname": "Feifan Song",
      "name": "songff",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.06941",
      "authors": [
        {
          "_id": "684797863ec10bdd8ab4de72",
          "user": {
            "_id": "6520621836008ecc88699622",
            "avatarUrl": "/avatars/b08c00af00f1736a4f4938443e575b0e.svg",
            "isPro": false,
            "fullname": "Parshin Shojaee",
            "user": "parshinsh",
            "type": "user"
          },
          "name": "Parshin Shojaee",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-10T08:43:32.697Z",
          "hidden": false
        },
        {
          "_id": "684797863ec10bdd8ab4de73",
          "name": "Iman Mirzadeh",
          "hidden": false
        },
        {
          "_id": "684797863ec10bdd8ab4de74",
          "name": "Keivan Alizadeh",
          "hidden": false
        },
        {
          "_id": "684797863ec10bdd8ab4de75",
          "name": "Maxwell Horton",
          "hidden": false
        },
        {
          "_id": "684797863ec10bdd8ab4de76",
          "name": "Samy Bengio",
          "hidden": false
        },
        {
          "_id": "684797863ec10bdd8ab4de77",
          "name": "Mehrdad Farajtabar",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-07T22:42:29.000Z",
      "submittedOnDailyAt": "2025-06-10T01:00:21.500Z",
      "title": "«Contexte de la Mémoire : Forces et Limites des Modèles Théoriques qui Considèrent des Problèmes Complexes»",
      "submittedOnDailyBy": {
        "_id": "6520621836008ecc88699622",
        "avatarUrl": "/avatars/b08c00af00f1736a4f4938443e575b0e.svg",
        "isPro": false,
        "fullname": "Parshin Shojaee",
        "user": "parshinsh",
        "type": "user"
      },
      "summary": "Récemment, les modèles de langue ont introduit des modèles de grande raisonnance (LRMs) pour générer des processus de pensée spécifiques avant de fournir une réponse. Ces modèles ont démontré des améliorations dans les référentiels de fondements logiques, mais leurs capacités de base, leurs caractéristiques d'échelle et leurs limites n'ont pas été suffisamment compréhenues. Actuellement, les évaluations se concentrent principalement sur les référentiels de mathématiques et de programmation, favorisant la précision de la réponse finale. Cependant, ce paradigme d'évaluation ne peut pas comprendre les fondements logiques, ce qui peut les contaminer. Dans cette étude, on a investigué ces défauts structurels, et on a inspiré-se d'environnements de puzzles contrôlables qui permettent de manipuler la complexité structurelle, pour étudier les défauts du paradigme d'évaluation. Ce environnement permet d'analyser non seulement la réponse finale mais aussi les fondements logiques internes, et de comprendre comment les LRMs pensent. À travers d'expériences étendues, on a démontré que les LRMs perdent complètement leur performance lorsqu'ils dépassent une certaine complexité. De plus, on a observé des limites de scalabilité non strictes : l'effort logique augmente à un niveau constant avec la complexité de la tâche, mais le reste des buckets de tokens ne s'augmente pas. En comparant les LRMs aux modèles de langue standards utilisant la même computation d'inférence, on a identifié trois directions de performance : (1) dans les tâches de faible complexité, les modèles standards dépassent les LRMs, (2) dans les tâches de complexité moyenne, les LRMs ont une avantage, et (3) dans les tâches de haute complexité, les deux modèles perdent complètement. Les LRMs sont limités dans les calculs précis et montrent des comportements inadéquats en échelle, sans un algorithme clair. De plus, on a étudié plus profondément les fondements logiques, on a examiné les motifs de solutions trouvées et on a analysé les comportements de calcul du modèle, révélant ses forces et ses limites et soulevant des questions sur sa capacité.",
      "upvotes": 7,
      "discussionId": "684797863ec10bdd8ab4de78",
      "ai_summary": "Large Reasoning Models (LRMs) exhibit varying performance across task complexities, with limitations in exact computation and inconsistent reasoning, as assessed using controllable puzzle environments.",
      "ai_keywords": [
        "Large Reasoning Models",
        "LRMs",
        "controllable puzzle environments",
        "reasoning traces",
        "standard LLMs",
        "performance regimes",
        "exact computation",
        "reasoning capabilities"
      ]
    },
    "publishedAt": "2025-06-07T18:42:29.000Z",
    "title": "The Illusion of Thinking: Understanding the Strengths and Limitations of\n  Reasoning Models via the Lens of Problem Complexity",
    "summary": "Recent generations of language models have introduced Large Reasoning Models\n(LRMs) that generate detailed thinking processes before providing answers.\nWhile these models demonstrate improved performance on reasoning benchmarks,\ntheir fundamental capabilities, scaling properties, and limitations remain\ninsufficiently understood. Current evaluations primarily focus on established\nmath and coding benchmarks, emphasizing final answer accuracy. However, this\nevaluation paradigm often suffers from contamination and does not provide\ninsights into the reasoning traces. In this work, we systematically investigate\nthese gaps with the help of controllable puzzle environments that allow precise\nmanipulation of complexity while maintaining consistent logical structures.\nThis setup enables the analysis of not only final answers but also the internal\nreasoning traces, offering insights into how LRMs think. Through extensive\nexperiments, we show that LRMs face a complete accuracy collapse beyond certain\ncomplexities. Moreover, they exhibit a counterintuitive scaling limit: their\nreasoning effort increases with problem complexity up to a point, then declines\ndespite having remaining token budget. By comparing LRMs with their standard\nLLM counterparts under same inference compute, we identify three performance\nregimes: (1) low-complexity tasks where standard models outperform LRMs, (2)\nmedium-complexity tasks where LRMs demonstrates advantage, and (3)\nhigh-complexity tasks where both models face complete collapse. We found that\nLRMs have limitations in exact computation: they fail to use explicit\nalgorithms and reason inconsistently across scales. We also investigate the\nreasoning traces in more depth, studying the patterns of explored solutions and\nanalyzing the models' computational behavior, shedding light on their\nstrengths, limitations, and raising questions about their reasoning\ncapabilities.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.06941.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6520621836008ecc88699622",
      "avatarUrl": "/avatars/b08c00af00f1736a4f4938443e575b0e.svg",
      "fullname": "Parshin Shojaee",
      "name": "parshinsh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.06205",
      "authors": [
        {
          "_id": "6846ca9b3ec10bdd8ab4dbf4",
          "user": {
            "_id": "66727b038171db46e7f4f242",
            "avatarUrl": "/avatars/3f8ab656006533fe1a24dd48c9b0a1b6.svg",
            "isPro": false,
            "fullname": "sc",
            "user": "sc-bd",
            "type": "user"
          },
          "name": "Sheng Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-10T09:21:08.452Z",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dbf5",
          "name": "Peiyu He",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dbf6",
          "name": "Jiaxin Hu",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dbf7",
          "name": "Ziyang Liu",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dbf8",
          "name": "Yansheng Wang",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dbf9",
          "name": "Tao Xu",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dbfa",
          "name": "Chi Zhang",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dbfb",
          "name": "Chongchong Zhang",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dbfc",
          "name": "Chao An",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dbfd",
          "name": "Shiyu Cai",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dbfe",
          "name": "Duo Cao",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dbff",
          "name": "Kangping Chen",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc00",
          "name": "Shuai Chu",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc01",
          "name": "Tianwei Chu",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc02",
          "name": "Mingdi Dan",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc03",
          "name": "Min Du",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc04",
          "name": "Weiwei Fang",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc05",
          "name": "Pengyou Fu",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc06",
          "name": "Junkai Hu",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc07",
          "name": "Xiaowei Jiang",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc08",
          "name": "Zhaodi Jiang",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc09",
          "name": "Fuxuan Li",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc0a",
          "name": "Jun Li",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc0b",
          "name": "Minghui Li",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc0c",
          "name": "Mingyao Li",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc0d",
          "name": "Yanchang Li",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc0e",
          "name": "Zhibin Li",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc0f",
          "name": "Guangming Liu",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc10",
          "name": "Kairui Liu",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc11",
          "name": "Lihao Liu",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc12",
          "name": "Weizhi Liu",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc13",
          "name": "Xiaoshun Liu",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc14",
          "name": "Yufei Liu",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc15",
          "name": "Yunfei Liu",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc16",
          "name": "Qiang Lu",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc17",
          "name": "Yuanfei Luo",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc18",
          "name": "Xiang Lv",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc19",
          "name": "Hongying Ma",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc1a",
          "name": "Sai Ma",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc1b",
          "name": "Lingxian Mi",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc1c",
          "name": "Sha Sa",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc1d",
          "name": "Hongxiang Shu",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc1e",
          "name": "Lei Tian",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc1f",
          "name": "Chengzhi Wang",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc20",
          "name": "Jiayu Wang",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc21",
          "name": "Kaijie Wang",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc22",
          "name": "Qingyi Wang",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc23",
          "name": "Renwen Wang",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc24",
          "name": "Tao Wang",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc25",
          "name": "Wei Wang",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc26",
          "name": "Xirui Wang",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc27",
          "name": "Chao Wei",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc28",
          "name": "Xuguang Wei",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc29",
          "name": "Zijun Xia",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc2a",
          "name": "Zhaohao Xiao",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc2b",
          "name": "Tingshuai Yan",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc2c",
          "name": "Liyan Yang",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc2d",
          "name": "Yifan Yang",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc2e",
          "name": "Zhikai Yang",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc2f",
          "name": "Zhong Yin",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc30",
          "name": "Li Yuan",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc31",
          "name": "Liuchun Yuan",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc32",
          "name": "Chi Zhang",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc33",
          "name": "Jinyang Zhang",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc34",
          "name": "Junhui Zhang",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc35",
          "name": "Linge Zhang",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc36",
          "name": "Zhenyi Zhang",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc37",
          "name": "Zheyu Zhang",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc38",
          "name": "Dongjie Zhu",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc39",
          "name": "Hang Li",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc3a",
          "name": "Yangang Zhang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/66727b038171db46e7f4f242/nrwuIsn9tQaR75nAuym-R.mp4"
      ],
      "publishedAt": "2025-06-06T16:08:47.000Z",
      "submittedOnDailyAt": "2025-06-10T07:53:05.305Z",
      "title": "Astra: \"Récipe de Hoyo pour l'Apprentissage Multimodal de Manière Générale dans les Robots Mobiles de Usage Courant\"",
      "submittedOnDailyBy": {
        "_id": "66727b038171db46e7f4f242",
        "avatarUrl": "/avatars/3f8ab656006533fe1a24dd48c9b0a1b6.svg",
        "isPro": false,
        "fullname": "sc",
        "user": "sc-bd",
        "type": "user"
      },
      "summary": "Le système de navigation de robots modernes pose plusieurs problèmes dans des environnements intérieurs complexes et variés. Les méthodes traditionnelles s'appuient sur de multiples modules avec des petits modèles ou sur des systèmes basés sur des règles, ce qui les empêche d'adapter à de nouveaux environnements. En réponse à cette situation, nous avons développé une architecture fonctionnelle de double modèle pour la navigation de robots mobiles, nommée Astra-Global et Astra-Local. Astra-Global traite des entrées visuelles et linguistiques en utilisant un modèle de langage grand multimodal, et détermine la position du robot et son objectif en utilisant un graphe sémantique-thématique mixte, dépassant ainsi les méthodes traditionnelles de reconnaissance visuelle de lieux. Astra-Local est une réseau de neurones capable de gérer plusieurs tâches, gérant la planification de pas locaux et la mesure de l'odométrie. L'encodeur de l'espace-temps 4D est entraîné par observation personnelle, générant des caractéristiques robustes en 4D pour des tâches ultérieures. Le tête de planification utilise le méthode de matching de flux et une perte d'ESDF avec masque pour minimiser le risque de collision et générer des projets locaux. Le tête d'odométrie intègre divers entrées de capteurs via un encodeur de canaux, prédisant la position relative du robot. L'implémentation d'Astra dans l'intérieur des Minas Réales atteint un haut pourcentage de succès dans des missions dans divers environnements intérieurs.",
      "upvotes": 7,
      "discussionId": "6846ca9b3ec10bdd8ab4dc3b",
      "ai_summary": "Astra, a dual-model architecture for mobile robot navigation, uses a multimodal LLM for global localization and a multitask network for local path planning and odometry estimation, achieving high success rates in diverse indoor environments.",
      "ai_keywords": [
        "LLM",
        "self and goal localization",
        "hybrid topological-semantic graph",
        "multimodal LLM",
        "multitask network",
        "4D spatial-temporal encoder",
        "self-supervised learning",
        "4D features",
        "flow matching",
        "masked ESDF loss",
        "local trajectories",
        "transformer encoder",
        "relative pose prediction"
      ]
    },
    "publishedAt": "2025-06-06T12:08:47.000Z",
    "title": "Astra: Toward General-Purpose Mobile Robots via Hierarchical Multimodal\n  Learning",
    "summary": "Modern robot navigation systems encounter difficulties in diverse and complex\nindoor environments. Traditional approaches rely on multiple modules with small\nmodels or rule-based systems and thus lack adaptability to new environments. To\naddress this, we developed Astra, a comprehensive dual-model architecture,\nAstra-Global and Astra-Local, for mobile robot navigation. Astra-Global, a\nmultimodal LLM, processes vision and language inputs to perform self and goal\nlocalization using a hybrid topological-semantic graph as the global map, and\noutperforms traditional visual place recognition methods. Astra-Local, a\nmultitask network, handles local path planning and odometry estimation. Its 4D\nspatial-temporal encoder, trained through self-supervised learning, generates\nrobust 4D features for downstream tasks. The planning head utilizes flow\nmatching and a novel masked ESDF loss to minimize collision risks for\ngenerating local trajectories, and the odometry head integrates multi-sensor\ninputs via a transformer encoder to predict the relative pose of the robot.\nDeployed on real in-house mobile robots, Astra achieves high end-to-end mission\nsuccess rate across diverse indoor environments.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/66727b038171db46e7f4f242/nrwuIsn9tQaR75nAuym-R.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.06205.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66727b038171db46e7f4f242",
      "avatarUrl": "/avatars/3f8ab656006533fe1a24dd48c9b0a1b6.svg",
      "fullname": "sc",
      "name": "sc-bd",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.08012",
      "authors": [
        {
          "_id": "684791b63ec10bdd8ab4ddb1",
          "name": "Penghao Wu",
          "hidden": false
        },
        {
          "_id": "684791b63ec10bdd8ab4ddb2",
          "name": "Shengnan Ma",
          "hidden": false
        },
        {
          "_id": "684791b63ec10bdd8ab4ddb3",
          "name": "Bo Wang",
          "hidden": false
        },
        {
          "_id": "684791b63ec10bdd8ab4ddb4",
          "name": "Jiaheng Yu",
          "hidden": false
        },
        {
          "_id": "684791b63ec10bdd8ab4ddb5",
          "name": "Lewei Lu",
          "hidden": false
        },
        {
          "_id": "684791b63ec10bdd8ab4ddb6",
          "name": "Ziwei Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-09T17:59:57.000Z",
      "submittedOnDailyAt": "2025-06-10T00:35:58.769Z",
      "title": "GUI-Reflection : Utilité Graphique de Réflexion Autonome\nGUI-Reflection : Modèle d'Utilité Graphique renforcé par Réflexion Autonome\nGUI-Reflection : Création d'un Modèle de GUI Multimodal avec Réflexion Autonome\nGUI-Reflection : Renforcement du Modèle d'Utilité Graphique par Réflexion Autonome de Comportement",
      "submittedOnDailyBy": {
        "_id": "64101f81b27543634e377fc1",
        "avatarUrl": "/avatars/557dd9d4707e3b38e0805dfb87c08004.svg",
        "isPro": false,
        "fullname": "Penghao Wu",
        "user": "craigwu",
        "type": "user"
      },
      "summary": "Les modèles de grands langages multimodal (MLLMs) démontrent une possibilité innovante dans l'automatisation des interfaces graphiques (GUI). Cependant, les modèles actuels de GUI ont été entraînés principalement sur des trajets en ligne avec quasiment aucune erreur, ce qui a limité leur capacité à la rapidité et à la correction d'erreurs. Pour corriger cela, nous proposons un nouveau cadre de travail appelé GUI-Reflection. Ce cadre met en place trois étapes : l'entraînement préalable de la GUI, l'ajustement de sous-sous-réduction en ligne (SFT) et l'entraînement de réflexion en ligne, ce qui permet aux modèles de GUI d'acquérir clairement la capacité d'automatiser la réflexion et la correction d'erreurs.\n\nGUI-Reflection utilise un processus de génération et d'apprentissage de données completament automatisés pour favoriser l'apparition d'actions de réflexion automatisées. Spécifiquement, 1) nous proposons un pipeline échelonnable de données pour construire automatiquement des données de réflexion et de correction d'erreurs à partir des trajets actuels de réussite. Les modèles de GUI actuels se concentrent principalement sur la base graphique et la compréhension de l'interface utilisateur, pourquoi nous proposons le GUI-Reflection Task Suite pour enseigner et évaluer clairement les compétences axées sur la réflexion. 2) De plus, nous avons construit divers environnements efficaces pour l'entraînement en ligne et la collecte de données. 3) En utilisant ces environnements, nous proposons un algorithme d'entraînement de réflexion en ligne itératif, ce qui permet aux modèles de continuer à améliorer leur capacité à la réflexion et à la correction d'erreurs. Notre cadre de travail confère aux agents de GUI la capacité de réflexion automatique et de correction d'erreurs, ouvrant le chemin vers une automatisation de GUI plus robuste, adaptable et intelligente. Nous sommes prêts à publier gratuitement tous les données, modèles, environnements et outils.",
      "upvotes": 6,
      "discussionId": "684791b63ec10bdd8ab4ddb7",
      "projectPage": "https://penghao-wu.github.io/GUI_Reflection/",
      "githubRepo": "https://github.com/penghao-wu/GUI_Reflection",
      "ai_summary": "GUI-Reflection enhances GUI automation by integrating self-reflection and error correction through scalable data pipelines and an iterative online tuning framework.",
      "ai_keywords": [
        "multimodal large language models",
        "graphical user interface",
        "GUI automation",
        "self-reflection",
        "error correction",
        "GUI-specific pre-training",
        "supervised fine-tuning",
        "online reflection tuning",
        "reflection-oriented abilities",
        "iterative online reflection tuning algorithm"
      ]
    },
    "publishedAt": "2025-06-09T13:59:57.000Z",
    "title": "GUI-Reflection: Empowering Multimodal GUI Models with Self-Reflection\n  Behavior",
    "summary": "Multimodal Large Language Models (MLLMs) have shown great potential in\nrevolutionizing Graphical User Interface (GUI) automation. However, existing\nGUI models mostly rely on learning from nearly error-free offline trajectories,\nthus lacking reflection and error recovery capabilities. To bridge this gap, we\npropose GUI-Reflection, a novel framework that explicitly integrates\nself-reflection and error correction capabilities into end-to-end multimodal\nGUI models throughout dedicated training stages: GUI-specific pre-training,\noffline supervised fine-tuning (SFT), and online reflection tuning.\nGUI-reflection enables self-reflection behavior emergence with fully automated\ndata generation and learning processes without requiring any human annotation.\nSpecifically, 1) we first propose scalable data pipelines to automatically\nconstruct reflection and error correction data from existing successful\ntrajectories. While existing GUI models mainly focus on grounding and UI\nunderstanding ability, we propose the GUI-Reflection Task Suite to learn and\nevaluate reflection-oriented abilities explicitly. 2) Furthermore, we built a\ndiverse and efficient environment for online training and data collection of\nGUI models on mobile devices. 3) We also present an iterative online reflection\ntuning algorithm leveraging the proposed environment, enabling the model to\ncontinuously enhance its reflection and error correction abilities. Our\nframework equips GUI agents with self-reflection and correction capabilities,\npaving the way for more robust, adaptable, and intelligent GUI automation, with\nall data, models, environments, and tools to be released publicly.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08012.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64101f81b27543634e377fc1",
      "avatarUrl": "/avatars/557dd9d4707e3b38e0805dfb87c08004.svg",
      "fullname": "Penghao Wu",
      "name": "craigwu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 15
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.07309",
      "authors": [
        {
          "_id": "6847b8793ec10bdd8ab4df4f",
          "user": {
            "_id": "67f42bd98752b56bd349a9db",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/_1jLlx7qqMDYb2ylZ2TCa.png",
            "isPro": false,
            "fullname": "Yin Huang",
            "user": "MaggieHuang",
            "type": "user"
          },
          "name": "Yin Huang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-10T04:45:46.729Z",
          "hidden": false
        },
        {
          "_id": "6847b8793ec10bdd8ab4df50",
          "name": "Yifan Ethan Xu",
          "hidden": false
        },
        {
          "_id": "6847b8793ec10bdd8ab4df51",
          "name": "Kai Sun",
          "hidden": false
        },
        {
          "_id": "6847b8793ec10bdd8ab4df52",
          "name": "Vera Yan",
          "hidden": false
        },
        {
          "_id": "6847b8793ec10bdd8ab4df53",
          "name": "Alicia Sun",
          "hidden": false
        },
        {
          "_id": "6847b8793ec10bdd8ab4df54",
          "name": "Haidar Khan",
          "hidden": false
        },
        {
          "_id": "6847b8793ec10bdd8ab4df55",
          "name": "Jimmy Nguyen",
          "hidden": false
        },
        {
          "_id": "6847b8793ec10bdd8ab4df56",
          "name": "Mohammad Kachuee",
          "hidden": false
        },
        {
          "_id": "6847b8793ec10bdd8ab4df57",
          "name": "Zhaojiang Lin",
          "hidden": false
        },
        {
          "_id": "6847b8793ec10bdd8ab4df58",
          "name": "Yue Liu",
          "hidden": false
        },
        {
          "_id": "6847b8793ec10bdd8ab4df59",
          "name": "Aaron Colak",
          "hidden": false
        },
        {
          "_id": "6847b8793ec10bdd8ab4df5a",
          "name": "Anuj Kumar",
          "hidden": false
        },
        {
          "_id": "6847b8793ec10bdd8ab4df5b",
          "name": "Wen-tau Yih",
          "hidden": false
        },
        {
          "_id": "6847b8793ec10bdd8ab4df5c",
          "name": "Xin Luna Dong",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-08T22:51:46.000Z",
      "submittedOnDailyAt": "2025-06-10T03:17:46.849Z",
      "title": "ConfQA : Répondez seulement à ce que tu es sûr de savoir.",
      "submittedOnDailyBy": {
        "_id": "67f42bd98752b56bd349a9db",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/_1jLlx7qqMDYb2ylZ2TCa.png",
        "isPro": false,
        "fullname": "Yin Huang",
        "user": "MaggieHuang",
        "type": "user"
      },
      "summary": "LLM se demande si peut éviter la session des cinturons avec des explications vraies. Dans cet article, on propose un pas d'ajuste appelé ConfQA, qui réduit le pourcentage de session des cinturons de 20-40% à moins de 5% sur de multiples cadres de test de vérité. L'idée clé est simple : il faut entraîner le LLM pour qu'il continue d'apprendre des réponses précises et reconnaît \"je ne suis pas sûr\" lorsque cela n'est pas le cas. Cependant, deux éléments principaux contribuent à un haut effet d'entraînement. Premièrement, on ajoute un modèle de réponse \"répondre seulement quand on est sûr\", ce qui maintient la session des cinturons dans un intervalle de 15-25% si ce modèle n'est pas inclus. Deuxièmement, on utilise des explications simples et véritables, en particulier les valeurs de propriétés dans des graphes de connaissance pour ajuster la confiance du LLM et atteindre une forte généralisation selon le domaine et le type de question. D'une telle perspective, on propose un cadre de connaissance neuronale dual pour choisir un système de réseau neuronal paramétrique interne basé sur la confiance et un système externe de champs de signes enregistrés. Ce cadre permet d'améliorer la précision de 30% ou plus, tout en augmentant la précision de 95% ou plus.",
      "upvotes": 6,
      "discussionId": "6847b87a3ec10bdd8ab4df5d",
      "ai_summary": "ConfQA fine-tuning strategy reduces factual statement hallucination in LLMs by 80%, using a dampening prompt and factual statements from knowledge graphs to improve confidence calibration and knowledge selection.",
      "ai_keywords": [
        "Large Language Models",
        "LLMs",
        "fine-tuning",
        "ConfQA",
        "hallucination",
        "factuality benchmarks",
        "dampening prompt",
        "factual statements",
        "knowledge graphs",
        "confidence calibration",
        "Dual Neural Knowledge framework",
        "neural knowledge",
        "symbolic knowledge",
        "accuracy gains",
        "external retrievals"
      ]
    },
    "publishedAt": "2025-06-08T18:51:46.000Z",
    "title": "ConfQA: Answer Only If You Are Confident",
    "summary": "Can we teach Large Language Models (LLMs) to refrain from hallucinating\nfactual statements? In this paper we present a fine-tuning strategy that we\ncall ConfQA, which can reduce hallucination rate from 20-40% to under 5% across\nmultiple factuality benchmarks. The core idea is simple: when the LLM answers a\nquestion correctly, it is trained to continue with the answer; otherwise, it is\ntrained to admit \"I am unsure\". But there are two key factors that make the\ntraining highly effective. First, we introduce a dampening prompt \"answer only\nif you are confident\" to explicitly guide the behavior, without which\nhallucination remains high as 15%-25%. Second, we leverage simple factual\nstatements, specifically attribute values from knowledge graphs, to help LLMs\ncalibrate the confidence, resulting in robust generalization across domains and\nquestion types. Building on this insight, we propose the Dual Neural Knowledge\nframework, which seamlessly select between internally parameterized neural\nknowledge and externally recorded symbolic knowledge based on ConfQA's\nconfidence. The framework enables potential accuracy gains to beyond 95%, while\nreducing unnecessary external retrievals by over 30%.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07309.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67f42bd98752b56bd349a9db",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/_1jLlx7qqMDYb2ylZ2TCa.png",
      "fullname": "Yin Huang",
      "name": "MaggieHuang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.08010",
      "authors": [
        {
          "_id": "6847ad3b3ec10bdd8ab4df06",
          "name": "Nick Jiang",
          "hidden": false
        },
        {
          "_id": "6847ad3b3ec10bdd8ab4df07",
          "name": "Amil Dravid",
          "hidden": false
        },
        {
          "_id": "6847ad3b3ec10bdd8ab4df08",
          "name": "Alexei Efros",
          "hidden": false
        },
        {
          "_id": "6847ad3b3ec10bdd8ab4df09",
          "name": "Yossi Gandelsman",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-09T17:59:57.000Z",
      "submittedOnDailyAt": "2025-06-10T04:42:01.785Z",
      "title": "Visions Transformers Ne Requièrent Pas de Données d'Entraînement Préalables",
      "submittedOnDailyBy": {
        "_id": "6398d9d168e3392256aaf952",
        "avatarUrl": "/avatars/062363a9b5ebb26603c543a7fc3ee4ec.svg",
        "isPro": false,
        "fullname": "Nick",
        "user": "nickjiang",
        "type": "user"
      },
      "summary": "Nous étudions les mécanismes des phénomènes qui ont été reconnus précédemment. Ce phénomène consiste à générer des cartes d'attention qui ajoutent du bruit aux tokens à grande échelle. Nous avons observé dans divers modèles (comme CLIP, DINOv2) que certaines neurones rares se concentrent sur l'activation à grande échelle dans les tokens de sortie, générant des motifs d'attention discontinus et détériorant le traitement visuel subséquent. Actuellement, la solution est de réentraîner le modèle en utilisant des registrateurs supplémentaires pour éliminer ces sorties de sortie. Cependant, basés sur nos résultats, nous proposons une approche sans entraînement supplémentaire pour reproduire l'effet des registrateurs. Nous déplaceons l'activation à grande échelle des neurones registrateurs trouvés sans entraînement supplémentaire, et nous créons l'effet des registrateurs comme si le modèle était entraîné sans les utiliser. Notre méthode génère des cartes d'attention et des cartes de caractéristiques, améliore le rendement du modèle de base sur diverses tâches visuelles, et effectue des comparaisons avec des modèles entraînés explicitement avec des registrateurs. En second lieu, nous étendons le registrateur de temps de test à un modèle de vision temporelle, et nous améliorons sa compréhensibilité. Nos résultats montrent que le registrateur de temps de test effectue le même rôle que les registrateurs en temps de test, et fournit une solution sans limites d'entraînement, indépendamment du fait que le modèle de prédiction inclue des registrateurs ou non.",
      "upvotes": 5,
      "discussionId": "6847ad3c3ec10bdd8ab4df0a",
      "ai_summary": "A training-free method shifts high-norm activations in Vision Transformers to an untrained token, enhancing attention maps and performance across visual tasks, and improving interpretability in vision-language models.",
      "ai_keywords": [
        "Vision Transformers",
        "high-norm tokens",
        "noisy attention maps",
        "activations",
        "neurons",
        "irregular attention patterns",
        "downstream visual processing",
        "register tokens",
        "feature maps",
        "vision-language models",
        "interpretability",
        "test-time registers"
      ]
    },
    "publishedAt": "2025-06-09T13:59:57.000Z",
    "title": "Vision Transformers Don't Need Trained Registers",
    "summary": "We investigate the mechanism underlying a previously identified phenomenon in\nVision Transformers -- the emergence of high-norm tokens that lead to noisy\nattention maps. We observe that in multiple models (e.g., CLIP, DINOv2), a\nsparse set of neurons is responsible for concentrating high-norm activations on\noutlier tokens, leading to irregular attention patterns and degrading\ndownstream visual processing. While the existing solution for removing these\noutliers involves retraining models from scratch with additional learned\nregister tokens, we use our findings to create a training-free approach to\nmitigate these artifacts. By shifting the high-norm activations from our\ndiscovered register neurons into an additional untrained token, we can mimic\nthe effect of register tokens on a model already trained without registers. We\ndemonstrate that our method produces cleaner attention and feature maps,\nenhances performance over base models across multiple downstream visual tasks,\nand achieves results comparable to models explicitly trained with register\ntokens. We then extend test-time registers to off-the-shelf vision-language\nmodels to improve their interpretability. Our results suggest that test-time\nregisters effectively take on the role of register tokens at test-time,\noffering a training-free solution for any pre-trained model released without\nthem.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08010.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6398d9d168e3392256aaf952",
      "avatarUrl": "/avatars/062363a9b5ebb26603c543a7fc3ee4ec.svg",
      "fullname": "Nick",
      "name": "nickjiang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.08006",
      "authors": [
        {
          "_id": "6847ae533ec10bdd8ab4df0c",
          "name": "Sicheng Mo",
          "hidden": false
        },
        {
          "_id": "6847ae533ec10bdd8ab4df0d",
          "name": "Ziyang Leng",
          "hidden": false
        },
        {
          "_id": "6847ae533ec10bdd8ab4df0e",
          "name": "Leon Liu",
          "hidden": false
        },
        {
          "_id": "6847ae533ec10bdd8ab4df0f",
          "name": "Weizhen Wang",
          "hidden": false
        },
        {
          "_id": "6847ae533ec10bdd8ab4df10",
          "name": "Honglin He",
          "hidden": false
        },
        {
          "_id": "6847ae533ec10bdd8ab4df11",
          "name": "Bolei Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-09T17:59:52.000Z",
      "submittedOnDailyAt": "2025-06-10T02:33:18.764Z",
      "title": "Driepland : Un monde configurable avec des modèles de simulation et de génération",
      "submittedOnDailyBy": {
        "_id": "637c94d3f219c71f93eda9ad",
        "avatarUrl": "/avatars/6dae0c30755196ccc0a5a06b3981c47f.svg",
        "isPro": true,
        "fullname": "Sicheng Mo",
        "user": "Sichengmo",
        "type": "user"
      },
      "summary": "Les modèles de génération de vidéos à grande échelle sont adaptés pour la composition de contenu visuel dynamique qui constitue un monde dynamique, mais leur manque de contrôle individuel de chaque élément les rend difficiles à utiliser dans l'édition de scènes ou dans l'entraînement d'agents AI spécifiques. Nous proposons le cadre de travail hybride de génération de mondes \"Dreamland\", qui intègre le contrôle d'ordre élevé de simulateurs basés sur la physique et le contenu vivant des modèles d'apprentissage à grande échelle. Spécifiquement, nous avons conçu une abstraction du monde avec des couches qui incluent la signification et la généralité au niveau de pixels et d'objets, et nous utilisons une représentation intermédiaire pour relier le simulateur au modèle génératif. Cette méthodologie améliore la possibilité de contrôle et minimise le coût d'adaptation à la distribution réelle et au ajustement initial, soutenant l'utilisation des modèles d'apprentissage préalable ouverts tant pour les existants que futurs. De plus, nous avons construit le jeu de données D3Sim pour encourager l'entraînement et l'évaluation de la chaîne de génération hybride. Les expériences montrent que Dreamland améliore la qualité du contenu de 50,8 % et augmente la possibilité de contrôle de 17,9 %, démontrant également son grand potentiel pour l'entraînement d'agents AI spécifiques. Les codes et les données sont disponibles pour leur utilisation.",
      "upvotes": 4,
      "discussionId": "6847ae533ec10bdd8ab4df12",
      "projectPage": "https://metadriverse.github.io/dreamland/",
      "ai_summary": "Dreamland, a hybrid framework, combines physics-based simulators and generative models to improve controllability and image quality in video generation.",
      "ai_keywords": [
        "video generative models",
        "physics-based simulator",
        "photorealistic content",
        "world abstraction",
        "pixel-level semantics",
        "object-level semantics",
        "geometry",
        "layered world abstraction",
        "early alignment",
        "D3Sim dataset",
        "embodied agent training"
      ]
    },
    "publishedAt": "2025-06-09T13:59:52.000Z",
    "title": "Dreamland: Controllable World Creation with Simulator and Generative\n  Models",
    "summary": "Large-scale video generative models can synthesize diverse and realistic\nvisual content for dynamic world creation, but they often lack element-wise\ncontrollability, hindering their use in editing scenes and training embodied AI\nagents. We propose Dreamland, a hybrid world generation framework combining the\ngranular control of a physics-based simulator and the photorealistic content\noutput of large-scale pretrained generative models. In particular, we design a\nlayered world abstraction that encodes both pixel-level and object-level\nsemantics and geometry as an intermediate representation to bridge the\nsimulator and the generative model. This approach enhances controllability,\nminimizes adaptation cost through early alignment with real-world\ndistributions, and supports off-the-shelf use of existing and future pretrained\ngenerative models. We further construct a D3Sim dataset to facilitate the\ntraining and evaluation of hybrid generation pipelines. Experiments demonstrate\nthat Dreamland outperforms existing baselines with 50.8% improved image\nquality, 17.9% stronger controllability, and has great potential to enhance\nembodied agent training. Code and data will be made available.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08006.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "637c94d3f219c71f93eda9ad",
      "avatarUrl": "/avatars/6dae0c30755196ccc0a5a06b3981c47f.svg",
      "fullname": "Sicheng Mo",
      "name": "Sichengmo",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.06266",
      "authors": [
        {
          "_id": "6847b4b43ec10bdd8ab4df33",
          "user": {
            "_id": "6337537b267cee4d068f604d",
            "avatarUrl": "/avatars/15267f0759a6570c98ee6a150558fcc0.svg",
            "isPro": false,
            "fullname": "Sabri Eyuboglu",
            "user": "sabrieyuboglu",
            "type": "user"
          },
          "name": "Sabri Eyuboglu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-10T04:29:41.818Z",
          "hidden": false
        },
        {
          "_id": "6847b4b43ec10bdd8ab4df34",
          "name": "Ryan Ehrlich",
          "hidden": false
        },
        {
          "_id": "6847b4b43ec10bdd8ab4df35",
          "name": "Simran Arora",
          "hidden": false
        },
        {
          "_id": "6847b4b43ec10bdd8ab4df36",
          "name": "Neel Guha",
          "hidden": false
        },
        {
          "_id": "6847b4b43ec10bdd8ab4df37",
          "name": "Dylan Zinsley",
          "hidden": false
        },
        {
          "_id": "6847b4b43ec10bdd8ab4df38",
          "name": "Emily Liu",
          "hidden": false
        },
        {
          "_id": "6847b4b43ec10bdd8ab4df39",
          "name": "Will Tennien",
          "hidden": false
        },
        {
          "_id": "6847b4b43ec10bdd8ab4df3a",
          "name": "Atri Rudra",
          "hidden": false
        },
        {
          "_id": "6847b4b43ec10bdd8ab4df3b",
          "name": "James Zou",
          "hidden": false
        },
        {
          "_id": "6847b4b43ec10bdd8ab4df3c",
          "name": "Azalia Mirhoseini",
          "hidden": false
        },
        {
          "_id": "6847b4b43ec10bdd8ab4df3d",
          "name": "Christopher Re",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-06T17:48:23.000Z",
      "submittedOnDailyAt": "2025-06-10T03:02:08.278Z",
      "title": "Caractéristique : Modèles légers et représentations communes de grand contexte peuvent être appris.",
      "submittedOnDailyBy": {
        "_id": "6337537b267cee4d068f604d",
        "avatarUrl": "/avatars/15267f0759a6570c98ee6a150558fcc0.svg",
        "isPro": false,
        "fullname": "Sabri Eyuboglu",
        "user": "sabrieyuboglu",
        "type": "user"
      },
      "summary": "Les modèles de langage grands répondent à des requêtes basées sur de grands corpus de texte tels que des bases de code, des documents juridiques et des registres de chat, en utilisant l'apprentissage en contexte (ICL) en placeant tout le corpus dans une fenêtre de contexte. Actuellement, les modèles supportent des contextes de 100K-1M tokens, mais cette configuration augmente le consommateur de mémoire de cache KV proportionnellement à la longueur de l'entrée, ce qui implique des coûts pour le service. Nous examinons une configuration alternative d'entraînement en cache KV petit sur la réseau en ligne par corpus. Pendant l'inférence, le cache KV entraîné est appelé \"cartographie\" et est lu pour valider la réponse. Un point important est que le coût d'entraînement de la cartographie peut être attribué à toutes les demandes qui référencent le même corpus. Cependant, l'entraînement de la cartographie en utilisant la prédiction du token suivant dans le corpus n'a pas abouti à des résultats meilleurs que l'ICL. Au lieu de cela, nous proposons de générer un convertisseur synthétique pour le corpus et d'entraîner la cartographie en utilisant les objets distillés du contexte pour un méthode d'entraînement appelée \"autodidactique\". La cartographie entraînée avec autodidactique récrée les fonctions de l'ICL et peut réduire significativement les coûts du service. Dans les benchmarks de contextes longs et difficiles, la cartographie entraînée avec autodidactique peut concourir avec l'ICL, réduisant l'utilisation de mémoire en 38,6 fois et augmentant le flux transformer en 26,4 fois. De plus, l'autodidactique peut efficacement étendre la longueur du contexte du modèle (par exemple, de 128k tokens à 484k en MTOB), et en conséquence, permet de configurer la cartographie pendant l'inférence.",
      "upvotes": 4,
      "discussionId": "6847b4b43ec10bdd8ab4df3e",
      "projectPage": "https://hazyresearch.stanford.edu/blog/2025-06-08-cartridges",
      "githubRepo": "https://github.com/HazyResearch/cartridges",
      "ai_summary": "Training a smaller, offline KV cache (Cartridge) with a context-distillation objective (self-study) for large language models reduces serving costs, matches ICL performance, and extends effective context length.",
      "ai_keywords": [
        "KV cache",
        "Cartridge",
        "in-context learning (ICL)",
        "self-study",
        "context-distillation objective",
        "MTOB"
      ]
    },
    "publishedAt": "2025-06-06T13:48:23.000Z",
    "title": "Cartridges: Lightweight and general-purpose long context representations\n  via self-study",
    "summary": "Large language models are often used to answer queries grounded in large text\ncorpora (e.g. codebases, legal documents, or chat histories) by placing the\nentire corpus in the context window and leveraging in-context learning (ICL).\nAlthough current models support contexts of 100K-1M tokens, this setup is\ncostly to serve because the memory consumption of the KV cache scales with\ninput length. We explore an alternative: training a smaller KV cache offline on\neach corpus. At inference time, we load this trained KV cache, which we call a\nCartridge, and decode a response. Critically, the cost of training a Cartridge\ncan be amortized across all the queries referencing the same corpus. However,\nwe find that the naive approach of training the Cartridge with next-token\nprediction on the corpus is not competitive with ICL. Instead, we propose\nself-study, a training recipe in which we generate synthetic conversations\nabout the corpus and train the Cartridge with a context-distillation objective.\nWe find that Cartridges trained with self-study replicate the functionality of\nICL, while being significantly cheaper to serve. On challenging long-context\nbenchmarks, Cartridges trained with self-study match ICL performance while\nusing 38.6x less memory and enabling 26.4x higher throughput. Self-study also\nextends the model's effective context length (e.g. from 128k to 484k tokens on\nMTOB) and surprisingly, leads to Cartridges that can be composed at inference\ntime without retraining.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.06266.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6337537b267cee4d068f604d",
      "avatarUrl": "/avatars/15267f0759a6570c98ee6a150558fcc0.svg",
      "fullname": "Sabri Eyuboglu",
      "name": "sabrieyuboglu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.07848",
      "authors": [
        {
          "_id": "6847de223ec10bdd8ab4e02a",
          "name": "Teng Hu",
          "hidden": false
        },
        {
          "_id": "6847de223ec10bdd8ab4e02b",
          "name": "Zhentao Yu",
          "hidden": false
        },
        {
          "_id": "6847de223ec10bdd8ab4e02c",
          "name": "Zhengguang Zhou",
          "hidden": false
        },
        {
          "_id": "6847de223ec10bdd8ab4e02d",
          "name": "Jiangning Zhang",
          "hidden": false
        },
        {
          "_id": "6847de223ec10bdd8ab4e02e",
          "name": "Yuan Zhou",
          "hidden": false
        },
        {
          "_id": "6847de223ec10bdd8ab4e02f",
          "name": "Qinglin Lu",
          "hidden": false
        },
        {
          "_id": "6847de223ec10bdd8ab4e030",
          "name": "Ran Yi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-09T15:11:09.000Z",
      "submittedOnDailyAt": "2025-06-10T05:57:02.723Z",
      "title": "PolyVivid : Interaction et mise à jour multicanals pour la génération de vidéos de rencontre de vidéos",
      "submittedOnDailyBy": {
        "_id": "63468720dd6d90d82ccf3450",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
        "isPro": false,
        "fullname": "YSH",
        "user": "BestWishYsh",
        "type": "user"
      },
      "summary": "Récemment, le développement de la génération d'images a conduit les modèles actuels à manquer particulièrement de contrôle précis sur l'interaction et l'identité de plusieurs personnages. Dans cet article, nous proposons un cadre de travail pour la personnalisation d'images multiples nommé \"PolyVivid\", qui permet de maintenir l'identité des personnages tout en facilitant la génération flexible. Pour établir une correspondance précise entre texte et image, nous concevons un module de fusion texte-image basé sur VLLM, ce qui permet d'insérer des identifiants visuels dans l'espace de texte, fournissant une base précise. De plus, pour préserver l'identité et promouvoir l'interaction entre personnages, nous proposons un module d'extension basé sur 3D-RoPE, ce qui permet une fusion bidirectionnelle structurée de texte et image. De plus, nous développons un module pour insérer des identifiants qui maintiennent la cohérence et évitent la perte d'identité lors de la génération d'images. Enfin, nous construisons une chaîne de données basée sur MLLM et nous combinons une stratégie d'intégration de personnages basée sur MLLM, basique, segmentation et clic pour générer des données de haute qualité et réduire l'ambiguïté et l'identifiabilité lors de la génération ultérieure. Les expériences étendues montrent que PolyVivid dépasse les limites actuels tant des codes ouverts que des lignes commerciales, démontrant des résultats exceptionnels en termes de fidélité de l'identité, de réalisme des images et d'alignement des personnages.",
      "upvotes": 2,
      "discussionId": "6847de223ec10bdd8ab4e031",
      "projectPage": "https://sjtuplayer.github.io/projects/PolyVivid/",
      "ai_summary": "PolyVivid is a multi-subject video customization framework that uses text-image fusion, 3D-RoPE enhancement, attention-inherited identity injection, and MLLM-based data processing to ensure identity consistency and realistic video generation.",
      "ai_keywords": [
        "VLLM-based text-image fusion",
        "3D-RoPE-based enhancement",
        "attention-inherited identity injection",
        "MLLM-based data pipeline",
        "identity fidelity",
        "video realism",
        "subject alignment"
      ]
    },
    "publishedAt": "2025-06-09T11:11:09.000Z",
    "title": "PolyVivid: Vivid Multi-Subject Video Generation with Cross-Modal\n  Interaction and Enhancement",
    "summary": "Despite recent advances in video generation, existing models still lack\nfine-grained controllability, especially for multi-subject customization with\nconsistent identity and interaction. In this paper, we propose PolyVivid, a\nmulti-subject video customization framework that enables flexible and\nidentity-consistent generation. To establish accurate correspondences between\nsubject images and textual entities, we design a VLLM-based text-image fusion\nmodule that embeds visual identities into the textual space for precise\ngrounding. To further enhance identity preservation and subject interaction, we\npropose a 3D-RoPE-based enhancement module that enables structured\nbidirectional fusion between text and image embeddings. Moreover, we develop an\nattention-inherited identity injection module to effectively inject fused\nidentity features into the video generation process, mitigating identity drift.\nFinally, we construct an MLLM-based data pipeline that combines MLLM-based\ngrounding, segmentation, and a clique-based subject consolidation strategy to\nproduce high-quality multi-subject data, effectively enhancing subject\ndistinction and reducing ambiguity in downstream video generation. Extensive\nexperiments demonstrate that PolyVivid achieves superior performance in\nidentity fidelity, video realism, and subject alignment, outperforming existing\nopen-source and commercial baselines.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07848.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63468720dd6d90d82ccf3450",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
      "fullname": "YSH",
      "name": "BestWishYsh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 56
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.07527",
      "authors": [
        {
          "_id": "6847dce63ec10bdd8ab4e011",
          "user": {
            "_id": "659e3ea885956d2cccda2b9e",
            "avatarUrl": "/avatars/f26d415f7c1b39550af2075769f38a91.svg",
            "isPro": false,
            "fullname": "马路",
            "user": "RoadQAQ",
            "type": "user"
          },
          "name": "Lu Ma",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-10T07:21:11.443Z",
          "hidden": false
        },
        {
          "_id": "6847dce63ec10bdd8ab4e012",
          "name": "Hao Liang",
          "hidden": false
        },
        {
          "_id": "6847dce63ec10bdd8ab4e013",
          "name": "Meiyi Qiang",
          "hidden": false
        },
        {
          "_id": "6847dce63ec10bdd8ab4e014",
          "name": "Lexiang Tang",
          "hidden": false
        },
        {
          "_id": "6847dce63ec10bdd8ab4e015",
          "name": "Xiaochen Ma",
          "hidden": false
        },
        {
          "_id": "6847dce63ec10bdd8ab4e016",
          "name": "Zhen Hao Wong",
          "hidden": false
        },
        {
          "_id": "6847dce63ec10bdd8ab4e017",
          "name": "Junbo Niu",
          "hidden": false
        },
        {
          "_id": "6847dce63ec10bdd8ab4e018",
          "name": "Chengyu Shen",
          "hidden": false
        },
        {
          "_id": "6847dce63ec10bdd8ab4e019",
          "name": "Runming He",
          "hidden": false
        },
        {
          "_id": "6847dce63ec10bdd8ab4e01a",
          "name": "Bin Cui",
          "hidden": false
        },
        {
          "_id": "6847dce63ec10bdd8ab4e01b",
          "name": "Wentao Zhang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/659e3ea885956d2cccda2b9e/Q_orxNfgmdXXT6I2ahrDs.jpeg"
      ],
      "publishedAt": "2025-06-09T08:11:20.000Z",
      "submittedOnDailyAt": "2025-06-10T05:52:14.746Z",
      "title": "Apprenant depuis DeepLimit : Ajustes en ligne pour les problèmes d'entrée en ligne pour les plus difficiles problèmes",
      "submittedOnDailyBy": {
        "_id": "659e3ea885956d2cccda2b9e",
        "avatarUrl": "/avatars/f26d415f7c1b39550af2075769f38a91.svg",
        "isPro": false,
        "fullname": "马路",
        "user": "RoadQAQ",
        "type": "user"
      },
      "summary": "Récemment, les modèles de langage grand (LLM) montrent comment les apprentissages par renforcement (RL) peuvent générer des actions complexes comme la planification et la réflexion autonome. Cependant, le développement actuel de RL ne dépasse pas les limites des modèles de base, car ils sont optimisés sur la base de l'information actuelle et ne peuvent pas s'adapter à de nouvelles informations. Pour surmonter ces limites, nous utilisons des ajustements de fine-tuning (SFT) pour entraîner des parties de RL qui ne peuvent pas apprendre. Cela nous permet de combiner des données de supervision de haute qualité et des motifs de nouvelles connaissances et raisons. En analysant la dynamique d'entraînement de RL et SFT, nous avons découvert que RL se concentre sur l'amélioration de l'efficacité et de la sécurité des actions, tandis que SFT est efficace pour promouvoir l'apprentissage dans des contextes hors de l'étendue actuelle du modèle. Sur base de ces points forts, nous présentons un nouvel approche d'entraînement appelé ReLIFT (Reinforcement Learning Interleaved with Online Fine-Tuning). Dans ReLIFT, l'entraînement est principalement effectué avec RL, mais lorsque des questions difficiles sont rencontrées, des solutions de haute qualité sont ajustées et des entraînements de croisement entre RL et fine-tuning sont réalisés pour améliorer la capacité de raisonnement du modèle. En comparaison avec d'autres modèles de RL, ReLIFT a obtenu un accroissement moyen de plus de +5.2 points sur 5 marqueurs de compétence et 1 benchmark de distribution hors de l'étendue. De plus, avec seulement 13% de données de supervision détaillée, ReLIFT dépasse à la fois l'efficacité de RL et de SFT et met en avant sa scalabilité. Ces résultats montrent que ReLIFT dépasse les limites fondamentales de RL et met en avant son potentiel important.",
      "upvotes": 2,
      "discussionId": "6847dce73ec10bdd8ab4e01c",
      "githubRepo": "https://github.com/TheRoadQaQ/ReLIFT",
      "ai_summary": "ReLIFT, a method combining reinforcement learning and supervised fine-tuning, enhances large language model reasoning by addressing limitations of RL through interleaved training, improving performance across benchmarks with minimal data.",
      "ai_keywords": [
        "reinforcement learning",
        "supervised fine-tuning",
        "ReLIFT",
        "large language model",
        "reasoning",
        "training dynamics",
        "zero-RL models",
        "competition-level benchmarks",
        "out-of-distribution benchmark"
      ]
    },
    "publishedAt": "2025-06-09T04:11:20.000Z",
    "title": "Learning What Reinforcement Learning Can't: Interleaved Online\n  Fine-Tuning for Hardest Questions",
    "summary": "Recent advances in large language model (LLM) reasoning have shown that\nsophisticated behaviors such as planning and self-reflection can emerge through\nreinforcement learning (RL). However, despite these successes, RL in its\ncurrent form remains insufficient to induce capabilities that exceed the\nlimitations of the base model, as it is primarily optimized based on existing\nknowledge of the model rather than facilitating the acquisition of new\ninformation. To address this limitation, we employ supervised fine-tuning (SFT)\nto learn what RL cannot, which enables the incorporation of new knowledge and\nreasoning patterns by leveraging high-quality demonstration data. We analyze\nthe training dynamics of RL and SFT for LLM reasoning and find that RL excels\nat maintaining and improving performance on questions within the model's\noriginal capabilities, while SFT is more effective at enabling progress on\nquestions beyond the current scope of the model. Motivated by the complementary\nstrengths of RL and SFT, we introduce a novel training approach,\nReLIFT (Reinforcement Learning Interleaved\nwith Online Fine-Tuning). In ReLIFT, the model is primarily\ntrained using RL, but when it encounters challenging questions, high-quality\nsolutions are collected for fine-tuning, and the training process alternates\nbetween RL and fine-tuning to enhance the model's reasoning abilities. ReLIFT\nachieves an average improvement of over +5.2 points across five\ncompetition-level benchmarks and one out-of-distribution benchmark compared to\nother zero-RL models. Furthermore, we demonstrate that ReLIFT outperforms both\nRL and SFT while using only 13\\% of the detailed demonstration data,\nhighlighting its scalability. These results provide compelling evidence that\nReLIFT overcomes the fundamental limitations of RL and underscores the\nsignificant potential.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/659e3ea885956d2cccda2b9e/Q_orxNfgmdXXT6I2ahrDs.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07527.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "659e3ea885956d2cccda2b9e",
      "avatarUrl": "/avatars/f26d415f7c1b39550af2075769f38a91.svg",
      "fullname": "马路",
      "name": "RoadQAQ",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.07240",
      "authors": [
        {
          "_id": "6847b6513ec10bdd8ab4df49",
          "user": {
            "_id": "600bde0c2b417b1d53669bd0",
            "avatarUrl": "/avatars/2d9704713630e96458368b47179c039c.svg",
            "isPro": false,
            "fullname": "Roy Eisenstadt",
            "user": "royeis",
            "type": "user"
          },
          "name": "Roy Eisenstadt",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-10T04:36:34.797Z",
          "hidden": false
        },
        {
          "_id": "6847b6513ec10bdd8ab4df4a",
          "name": "Itamar Zimerman",
          "hidden": false
        },
        {
          "_id": "6847b6513ec10bdd8ab4df4b",
          "name": "Lior Wolf",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/65376feed325b3f02fb92c69/TYxLNOb6ac6HH-pR04f7W.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/65376feed325b3f02fb92c69/CVHB0FrpM5va85IVFh2bT.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/65376feed325b3f02fb92c69/c-o1T8-RuSJ222Fj79LDN.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/65376feed325b3f02fb92c69/eKIDa2-ZEMdQdYOimgSpC.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/65376feed325b3f02fb92c69/VI5bYWw7ZDAruk6rqNrMh.jpeg"
      ],
      "publishedAt": "2025-06-08T17:54:33.000Z",
      "submittedOnDailyAt": "2025-06-10T03:10:26.281Z",
      "title": "Observation et contrôle de la longueur du chemin de pensée dans les LM",
      "submittedOnDailyBy": {
        "_id": "65376feed325b3f02fb92c69",
        "avatarUrl": "/avatars/e952918cf434d5302e9b1a404eccaf0e.svg",
        "isPro": false,
        "fullname": "Itamar Zimerman",
        "user": "ItamarZ",
        "type": "user"
      },
      "summary": "Récemment, la théorie de la structure explicite a démontré que le raisonnement du modèle peut différencier clairement les processus internes du modèle et les réponses finales. Dans ce contexte, l'une des variables qui affectent la qualité des réponses est la longueur du raisonnement. Si le raisonnement est trop court, le modèle peut ne pas comprendre la complexité du travail, tandis que si il est trop long, le modèle peut penser trop et effectuer des calculs inutiles, ce qui peut diminuer son rendement. Cet article investigate et utilise une structure de base pour que les modèles de langage grands (LLM) comprennent et ajustent la longueur du raisonnement dans le processus de pensée explicite. Tout d'abord, nous codifions comment le modèle réalise le processus de raisonnement, puis nous visualisons le plan du modèle pour fournir une compréhension de sa dynamique de planification. Ensuite, nous manipulons le codé de la progression interne pendant l'inférence pour réduire les étapes inutiles et générer une séquence de pensée plus claire et décisive. Selon nos résultats expérimentaux, ce méthode \"overclock\" peut réduire le pensée excessive, améliorer la précision des réponses et réduire le temps d'inférence. Notre code est disponible pour l'utilisation publique.",
      "upvotes": 2,
      "discussionId": "6847b6513ec10bdd8ab4df4c",
      "projectPage": "https://royeisen.github.io/OverclockingLLMReasoning-paper/",
      "githubRepo": "https://github.com/royeisen/reasoning_loading_bar",
      "ai_summary": "LLMs regulate reasoning length through progress encoding, and manipulating this encoding improves accuracy and reduces inference time.",
      "ai_keywords": [
        "explicit structured reasoning",
        "LLMs",
        "reasoning process",
        "progress bar visualization",
        "progress encoding",
        "inference",
        "overclocking",
        "overthinking",
        "answer accuracy",
        "inference latency"
      ]
    },
    "publishedAt": "2025-06-08T13:54:33.000Z",
    "title": "Overclocking LLM Reasoning: Monitoring and Controlling Thinking Path\n  Lengths in LLMs",
    "summary": "Recently, techniques such as explicit structured reasoning have demonstrated\nstrong test-time scaling behavior by enforcing a separation between the model's\ninternal \"thinking\" process and the final response. A key factor influencing\nanswer quality in this setting is the length of the thinking stage. When the\nreasoning is too short, the model may fail to capture the complexity of the\ntask. Conversely, when it is too long, the model may overthink, leading to\nunnecessary computation and degraded performance. This paper explores and\nexploits the underlying mechanisms by which LLMs understand and regulate the\nlength of their reasoning during explicit thought processes. First, we show\nthat LLMs encode their progress through the reasoning process and introduce an\ninteractive progress bar visualization, which is then used to reveal insights\non the model's planning dynamics. Second, we manipulate the internal progress\nencoding during inference to reduce unnecessary steps and generate a more\nconcise and decisive chain of thoughts. Our empirical results demonstrate that\nthis \"overclocking\" method mitigates overthinking, improves answer accuracy,\nand reduces inference latency. Our code is publicly available.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/65376feed325b3f02fb92c69/TYxLNOb6ac6HH-pR04f7W.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/65376feed325b3f02fb92c69/CVHB0FrpM5va85IVFh2bT.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/65376feed325b3f02fb92c69/c-o1T8-RuSJ222Fj79LDN.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/65376feed325b3f02fb92c69/eKIDa2-ZEMdQdYOimgSpC.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/65376feed325b3f02fb92c69/VI5bYWw7ZDAruk6rqNrMh.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07240.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65376feed325b3f02fb92c69",
      "avatarUrl": "/avatars/e952918cf434d5302e9b1a404eccaf0e.svg",
      "fullname": "Itamar Zimerman",
      "name": "ItamarZ",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.07160",
      "authors": [
        {
          "_id": "6847c0263ec10bdd8ab4df60",
          "user": {
            "_id": "627b73728b6ecd7ece822825",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/627b73728b6ecd7ece822825/QV-sT0vwupGZYg-loLPRw.jpeg",
            "isPro": false,
            "fullname": "Yikun Wang",
            "user": "LibraTree",
            "type": "user"
          },
          "name": "Yikun Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-10T08:42:47.991Z",
          "hidden": false
        },
        {
          "_id": "6847c0263ec10bdd8ab4df61",
          "name": "Yibin Wang",
          "hidden": false
        },
        {
          "_id": "6847c0263ec10bdd8ab4df62",
          "name": "Dianyi Wang",
          "hidden": false
        },
        {
          "_id": "6847c0263ec10bdd8ab4df63",
          "name": "Zimian Peng",
          "hidden": false
        },
        {
          "_id": "6847c0263ec10bdd8ab4df64",
          "name": "Qipeng Guo",
          "hidden": false
        },
        {
          "_id": "6847c0263ec10bdd8ab4df65",
          "name": "Dacheng Tao",
          "hidden": false
        },
        {
          "_id": "6847c0263ec10bdd8ab4df66",
          "name": "Jiaqi Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-08T14:18:15.000Z",
      "submittedOnDailyAt": "2025-06-10T04:04:49.808Z",
      "title": "GeometryZero : Amélioration du méthode géométrique de résolution de problèmes de LLM par une optimisation stratégique des groupes",
      "submittedOnDailyBy": {
        "_id": "627b73728b6ecd7ece822825",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/627b73728b6ecd7ece822825/QV-sT0vwupGZYg-loLPRw.jpeg",
        "isPro": false,
        "fullname": "Yikun Wang",
        "user": "LibraTree",
        "type": "user"
      },
      "summary": "Récemment, le développement de grands modèles de langage (LLMs) a démontré des capacités exceptionnelles dans divers domaines, y compris la résolution de problèmes mathématiques. En particulier, la résolution de problèmes géométriques est un domaine difficile où la composition de l'explication joue un rôle crucial. Les méthodes actuelles ne parviennent pas à atteindre le rendement optimal ou dépendent de grands LLMs (comme GPT-4), ce qui implique des coûts de calcul très élevés. Nous avons identifié un chemin prometteur : l'apprentissage par renforcement basé sur des compensations provable (comme GRPO), qui peut permettre l'apprentissage de petits modèles. Cet approche a le potentiel de intégrer la composition de l'explication avec des capacités fortes d'inférence géométrique. Cependant, l'application directe de GRPO à l'inférence géométrique implique des limitations inhérentes, car elle dépend absolument des signaux de compensation, ce qui peut générer une variété de formes de composition de l'explication. Pour résoudre ces problèmes, nous proposons l'Optimisation de Politiques en Comparaison avec des Groupes (GCPO). GCPO se distingue par deux innovations clés : (1) Masquage de Comparaison de Groupes : fournit des signaux de compensation positive et négative appropriés pour la composition de l'explication. (2) Compensation par Longueur : incite les chaînes d'inférence longues. En se basant sur GCPO, nous avons développé la famille de modèles GeometryZero. GeometryZero offre des modèles d'inférence géométrique qui réduisent les coûts de calcul en évaluant de manière adéquate la composition de l'explication. À travers de nombreux expériments sur différents benchmarks comme Geometry3K et MathVista, les modèles de GeometryZero dépassent les standards (comme GRPO) et atteignent un accroissement moyen de 4,29% sur tous les benchmarks.",
      "upvotes": 2,
      "discussionId": "6847c0273ec10bdd8ab4df67",
      "ai_summary": "A new reinforcement learning framework, Group Contrastive Policy Optimization (GCPO), enhances geometric reasoning in large language models with judicious auxiliary constructions, outperforming existing methods on benchmarks.",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "mathematical reasoning",
        "geometry problem solving",
        "reinforcement learning",
        "verifiable reward",
        "GRPO",
        "Group Contrastive Policy Optimization (GCPO)",
        "Group Contrastive Masking",
        "length reward",
        "GeometryZero",
        "Geometry3K",
        "MathVista"
      ]
    },
    "publishedAt": "2025-06-08T10:18:15.000Z",
    "title": "GeometryZero: Improving Geometry Solving for LLM with Group Contrastive\n  Policy Optimization",
    "summary": "Recent advances in large language models (LLMs) have demonstrated remarkable\ncapabilities across diverse domains, particularly in mathematical reasoning,\namid which geometry problem solving remains a challenging area where auxiliary\nconstruction plays a enssential role. Existing approaches either achieve\nsuboptimal performance or rely on massive LLMs (e.g., GPT-4o), incurring\nmassive computational costs. We posit that reinforcement learning with\nverifiable reward (e.g., GRPO) offers a promising direction for training\nsmaller models that effectively combine auxiliary construction with robust\ngeometric reasoning. However, directly applying GRPO to geometric reasoning\npresents fundamental limitations due to its dependence on unconditional\nrewards, which leads to indiscriminate and counterproductive auxiliary\nconstructions. To address these challenges, we propose Group Contrastive Policy\nOptimization (GCPO), a novel reinforcement learning framework featuring two key\ninnovations: (1) Group Contrastive Masking, which adaptively provides positive\nor negative reward signals for auxiliary construction based on contextual\nutility, and a (2) length reward that promotes longer reasoning chains.\nBuilding on GCPO, we develop GeometryZero, a family of affordable-size\ngeometric reasoning models that judiciously determine when to employ auxiliary\nconstruction. Our extensive empirical evaluation across popular geometric\nbenchmarks (Geometry3K, MathVista) demonstrates that GeometryZero models\nconsistently outperform baselines (e.g. GRPO), achieving an average improvement\nof 4.29% across all benchmarks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07160.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "627b73728b6ecd7ece822825",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/627b73728b6ecd7ece822825/QV-sT0vwupGZYg-loLPRw.jpeg",
      "fullname": "Yikun Wang",
      "name": "LibraTree",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.03690",
      "authors": [
        {
          "_id": "684664f13ec10bdd8ab4dac0",
          "user": {
            "_id": "64e6c617ecce34cb442cb208",
            "avatarUrl": "/avatars/ebc61bf6a043314cb2089b1efd5e6a18.svg",
            "isPro": false,
            "fullname": "JieSun",
            "user": "Sunshine279",
            "type": "user"
          },
          "name": "Jie Sun",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-09T10:11:19.474Z",
          "hidden": false
        },
        {
          "_id": "684664f13ec10bdd8ab4dac1",
          "name": "Junkang Wu",
          "hidden": false
        },
        {
          "_id": "684664f13ec10bdd8ab4dac2",
          "name": "Jiancan Wu",
          "hidden": false
        },
        {
          "_id": "684664f13ec10bdd8ab4dac3",
          "name": "Zhibo Zhu",
          "hidden": false
        },
        {
          "_id": "684664f13ec10bdd8ab4dac4",
          "name": "Xingyu Lu",
          "hidden": false
        },
        {
          "_id": "684664f13ec10bdd8ab4dac5",
          "name": "Jun Zhou",
          "hidden": false
        },
        {
          "_id": "684664f13ec10bdd8ab4dac6",
          "name": "Lintao Ma",
          "hidden": false
        },
        {
          "_id": "684664f13ec10bdd8ab4dac7",
          "name": "Xiang Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-04T08:19:37.000Z",
      "submittedOnDailyAt": "2025-06-10T00:51:18.490Z",
      "title": "Optimisons le style solide grâce à l'utilisation de marges de méthodes dynamiques.",
      "submittedOnDailyBy": {
        "_id": "64e6c617ecce34cb442cb208",
        "avatarUrl": "/avatars/ebc61bf6a043314cb2089b1efd5e6a18.svg",
        "isPro": false,
        "fullname": "JieSun",
        "user": "Sunshine279",
        "type": "user"
      },
      "summary": "La mise à jour d'un modèle de Langage Large (LLM) est essentielle pour garantir la sécurité et la confiance dans les applications pratiques. La Direct Preference Optimization (DPO) est un méthode qui utilise des paires de préférences pour optimiser efficacement le modèle. Ce méthode peut réduire considérablement les besoins en ressources. Cependant, le rendement de DPO est considérablement affecté par la qualité des données. Les données peuvent subir des dégâts par bruit. Dans cet article, nous proposons un algorithme d'optimisation de préférences avec marge dynamique (gamma-PO) qui ajuste dynamiquement la marge de compensation au niveau des paires en utilisant la technique de calibration de marge spécifique à l'instance. Le gamma-PO priorise stratégiquement les paires à haute confiance (paires où la marge de compensation est élevée) et réduit le bruit dans les paires incertaines, s'adaptant à la version de DPO. Dans les benchmarks comme AlpacaEval2 et Arena-Hard, le gamma-PO a atteint des améliorations moyennes de 4,4%, établissant de nouveaux standards de rendement. De plus, le gamma-PO nécessite seulement des changements minimaux dans le code et ne touche pas l'environnement d'entraînement. C'est une solution puissante pour renforcer la mise à jour d'un LLM. Le code est disponible sur https://github.com/sunjie279/gammaPO.",
      "upvotes": 2,
      "discussionId": "684664f13ec10bdd8ab4dac8",
      "ai_summary": "The paper introduces γ-PO, a dynamic target margin preference optimization algorithm that enhances Large Language Models' alignment by adjusting reward margins at the pairwise level, leading to improved performance with minimal impact on training.",
      "ai_keywords": [
        "Large Language Models",
        "LLMs",
        "Direct Preference Optimization",
        "DPO",
        "preference pairs",
        "γ-PO",
        "instance-specific margin calibration",
        "reward margins",
        "AlpacaEval2",
        "Arena-Hard",
        "state-of-the-art performance"
      ]
    },
    "publishedAt": "2025-06-04T04:19:37.000Z",
    "title": "Robust Preference Optimization via Dynamic Target Margins",
    "summary": "The alignment of Large Language Models (LLMs) is crucial for ensuring their\nsafety and reliability in practical applications. Direct Preference\nOptimization (DPO) has emerged as an efficient method that directly optimizes\nmodels using preference pairs, significantly reducing resource demands.\nHowever, the effectiveness of DPO heavily depends on the data quality, which is\nfrequently compromised by noise. In this work, we propose gamma-PO, a\ndynamic target margin preference optimization algorithm that adjust reward\nmargins at the pairwise level. By introducing instance-specific margin\ncalibration, gamma-PO strategically prioritizes high-confidence pairs (those\ndemonstrating higher reward margins) while suppressing potential noise from\nambiguous pairs. Moreover, gamma-PO is a plug-and-play method, compatible\nwith variants of DPO that rely on reward margin between preference pairs.\nAcross benchmarks such as AlpacaEval2 and Arena-Hard, gamma-PO achieves an\naverage 4.4\\% improvement over other baselines, setting new benchmarks for\nstate-of-the-art performance. Additionally, gamma-PO requires minimal code\nchanges and has a negligible impact on training efficiency, making it a robust\nsolution for enhancing LLMs alignment. Our codes are available at\nhttps://github.com/sunjie279/gammaPO{https://github.com/sunjie279/gammaPO}.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03690.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64e6c617ecce34cb442cb208",
      "avatarUrl": "/avatars/ebc61bf6a043314cb2089b1efd5e6a18.svg",
      "fullname": "JieSun",
      "name": "Sunshine279",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.07803",
      "authors": [
        {
          "_id": "6847d4103ec10bdd8ab4dfb1",
          "user": {
            "_id": "6437d0a951c7ebfc813c735b",
            "avatarUrl": "/avatars/6cbac4e4be5029655702c5d8b9046b90.svg",
            "isPro": false,
            "fullname": "Allakhverdov Eduard",
            "user": "combat-helicopter",
            "type": "user"
          },
          "name": "Eduard Allakhverdov",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-10T08:42:38.549Z",
          "hidden": false
        },
        {
          "_id": "6847d4103ec10bdd8ab4dfb2",
          "name": "Dmitrii Tarasov",
          "hidden": false
        },
        {
          "_id": "6847d4103ec10bdd8ab4dfb3",
          "name": "Elizaveta Goncharova",
          "hidden": false
        },
        {
          "_id": "6847d4103ec10bdd8ab4dfb4",
          "name": "Andrey Kuznetsov",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-09T14:32:18.000Z",
      "submittedOnDailyAt": "2025-06-10T05:17:41.513Z",
      "title": "Reconstruction d'images est un instrument pour l'analyse de caractéristiques.",
      "submittedOnDailyBy": {
        "_id": "6310ff34bc152fa3e810c186",
        "avatarUrl": "/avatars/bfd63bcd81548283f5e496e3693bf143.svg",
        "isPro": false,
        "fullname": "Elizaveta Goncharova",
        "user": "Elizaveta",
        "type": "user"
      },
      "summary": "Le Vision Encoder augmente son utilisation dans des applications modernes comme des modèles de vision et des systèmes multimodal. Malgré ses succès impressionnants, la façon dont cette architecture représente des caractéristiques internes n'est pas claire. Dans ce travail, nous proposons un nouvel approche pour interpréter des caractéristiques visuelles à travers la reconstruction d'images. Nous comparons deux familles de modèles liés, SigLIP et SigLIP2, qui, bien que possédent des objectifs d'apprentissage différents, montrent que l'encoder pré-entraîné sur des tâches de version d'images conserve plus d'information d'images qu'un entraîné sur des tâches de version de vidéo. De plus, nous appliquons cette méthodologie à divers encodeurs de vision et ordonnons leur représentation des caractéristiques en fonction de la quantité d'information qu'ils contiennent. Enfin, nous démontrons que la manipulation des espaces de caractéristiques produit des changements prédictibles dans les images reconstruites et que la rotation orthogonale (plutôt que des transformations spatiales) contrôle l'encodage du couleur. Notre approche est applicable à tous les encodeurs de vidéo et révèle la structure interne de leurs espaces de caractéristiques. Les codes et les poids de modèles pour reproduire les expériences sont disponibles sur GitHub.",
      "upvotes": 0,
      "discussionId": "6847d4103ec10bdd8ab4dfb5",
      "projectPage": "https://fusionbrainlab.github.io/feature_analysis/",
      "githubRepo": "https://github.com/FusionBrainLab/feature_analysis",
      "ai_summary": "Image reconstruction reveals that vision encoders retain more image information after image-based tasks and that orthogonal rotations in feature space control color encoding.",
      "ai_keywords": [
        "SigLIP",
        "SigLIP2",
        "vision encoders",
        "image reconstruction",
        "contrastive learning",
        "feature representations"
      ]
    },
    "publishedAt": "2025-06-09T10:32:18.000Z",
    "title": "Image Reconstruction as a Tool for Feature Analysis",
    "summary": "Vision encoders are increasingly used in modern applications, from\nvision-only models to multimodal systems such as vision-language models.\nDespite their remarkable success, it remains unclear how these architectures\nrepresent features internally. Here, we propose a novel approach for\ninterpreting vision features via image reconstruction. We compare two related\nmodel families, SigLIP and SigLIP2, which differ only in their training\nobjective, and show that encoders pre-trained on image-based tasks retain\nsignificantly more image information than those trained on non-image tasks such\nas contrastive learning. We further apply our method to a range of vision\nencoders, ranking them by the informativeness of their feature representations.\nFinally, we demonstrate that manipulating the feature space yields predictable\nchanges in reconstructed images, revealing that orthogonal rotations (rather\nthan spatial transformations) control color encoding. Our approach can be\napplied to any vision encoder, shedding light on the inner structure of its\nfeature space. The code and model weights to reproduce the experiments are\navailable in GitHub.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07803.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6310ff34bc152fa3e810c186",
      "avatarUrl": "/avatars/bfd63bcd81548283f5e496e3693bf143.svg",
      "fullname": "Elizaveta Goncharova",
      "name": "Elizaveta",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.07645",
      "authors": [
        {
          "_id": "6847ea583ec10bdd8ab4e05a",
          "user": {
            "_id": "635270e36cfb8f14981312e7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/635270e36cfb8f14981312e7/FpqjdhABEDuxWR8k2zOwA.jpeg",
            "isPro": false,
            "fullname": "Maciej Chrabąszcz",
            "user": "mchraba",
            "type": "user"
          },
          "name": "Maciej Chrabąszcz",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-10T08:42:31.860Z",
          "hidden": false
        },
        {
          "_id": "6847ea583ec10bdd8ab4e05b",
          "user": {
            "_id": "66dab47f8506f9b6cf5f08ed",
            "avatarUrl": "/avatars/e6ba87adbaacdeccf8c4818596c655d0.svg",
            "isPro": false,
            "fullname": "LLM Attack",
            "user": "llmAttack",
            "type": "user"
          },
          "name": "Katarzyna Lorenc",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-10T08:18:33.059Z",
          "hidden": false
        },
        {
          "_id": "6847ea583ec10bdd8ab4e05c",
          "name": "Karolina Seweryn",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-09T11:09:39.000Z",
      "submittedOnDailyAt": "2025-06-10T06:49:10.646Z",
      "title": "Utilisation du modèle de Processus dans l'évaluation de la robustesse dans les langues riches en ressources de LLM",
      "submittedOnDailyBy": {
        "_id": "635270e36cfb8f14981312e7",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/635270e36cfb8f14981312e7/FpqjdhABEDuxWR8k2zOwA.jpeg",
        "isPro": false,
        "fullname": "Maciej Chrabąszcz",
        "user": "mchraba",
        "type": "user"
      },
      "summary": "Les modèles de langage général (LLMs) ont démontré récemment des capacités impressionnantes dans diverses tâches de traitement du langage naturel (NLP). Cependant, ils sont vulnérables aux \"jailbreaks\" et \"perturbations\" et nécessitent des évaluations supplémentaires. De nombreux LLMs sont constitués de multiples langues, mais les données d'entraînement liées à la sécurité sont principalement incluses dans des langues de haute richesse tel que l'anglais. Cela montre la vulnérabilité à la destruction des langues de faible richesse, comme le polonais. Nous avons montré que des attaques puissantes peuvent être créées avec seulement des changements de caractères et l'utilisation de petits modèles de traitement des mots pour calculer leur importance, ce qui peut les rendre plus économiques. Ces attaques à l'échelle des caractères et des mots peuvent modifier significativement les prédictions de différents LLMs et éviter les fonctions de sécurité internes. Nous avons vérifié et montré la vulnérabilité de ces attaques dans le polonais (une langue de faible richesse) et nous avons également présenté des méthodes pour les étendre à d'autres langues. Nous fournissons notre ensemble de données et de code pour une recherche supplémentaire.",
      "upvotes": 0,
      "discussionId": "6847ea583ec10bdd8ab4e05d",
      "ai_summary": "Character and word-level attacks using a proxy model reveal vulnerabilities in LLMs across languages, particularly in low-resource languages like Polish.",
      "ai_keywords": [
        "large language models",
        "natural language processing",
        "jailbreaks",
        "perturbations",
        "multilingual",
        "safety-related training data",
        "high-resource languages",
        "low-resource languages",
        "character-level attacks",
        "word-level attacks",
        "word importance calculation",
        "internal safety mechanisms"
      ]
    },
    "publishedAt": "2025-06-09T07:09:39.000Z",
    "title": "Evaluating LLMs Robustness in Less Resourced Languages with Proxy Models",
    "summary": "Large language models (LLMs) have demonstrated impressive capabilities across\nvarious natural language processing (NLP) tasks in recent years. However, their\nsusceptibility to jailbreaks and perturbations necessitates additional\nevaluations. Many LLMs are multilingual, but safety-related training data\ncontains mainly high-resource languages like English. This can leave them\nvulnerable to perturbations in low-resource languages such as Polish. We show\nhow surprisingly strong attacks can be cheaply created by altering just a few\ncharacters and using a small proxy model for word importance calculation. We\nfind that these character and word-level attacks drastically alter the\npredictions of different LLMs, suggesting a potential vulnerability that can be\nused to circumvent their internal safety mechanisms. We validate our attack\nconstruction methodology on Polish, a low-resource language, and find potential\nvulnerabilities of LLMs in this language. Additionally, we show how it can be\nextended to other languages. We release the created datasets and code for\nfurther research.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07645.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "635270e36cfb8f14981312e7",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/635270e36cfb8f14981312e7/FpqjdhABEDuxWR8k2zOwA.jpeg",
      "fullname": "Maciej Chrabąszcz",
      "name": "mchraba",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.05904",
      "authors": [
        {
          "_id": "6847e05a3ec10bdd8ab4e03d",
          "user": {
            "_id": "6369b1d456d1f93498130a8a",
            "avatarUrl": "/avatars/8ec228aa6f171715652511f948765db9.svg",
            "isPro": false,
            "fullname": "Yichi Zhang",
            "user": "594zyc",
            "type": "user"
          },
          "name": "Yichi Zhang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-10T07:35:55.259Z",
          "hidden": false
        },
        {
          "_id": "6847e05a3ec10bdd8ab4e03e",
          "name": "Xin Luna Dong",
          "hidden": false
        },
        {
          "_id": "6847e05a3ec10bdd8ab4e03f",
          "name": "Zhaojiang Lin",
          "hidden": false
        },
        {
          "_id": "6847e05a3ec10bdd8ab4e040",
          "name": "Andrea Madotto",
          "hidden": false
        },
        {
          "_id": "6847e05a3ec10bdd8ab4e041",
          "name": "Anuj Kumar",
          "hidden": false
        },
        {
          "_id": "6847e05a3ec10bdd8ab4e042",
          "name": "Babak Damavandi",
          "hidden": false
        },
        {
          "_id": "6847e05a3ec10bdd8ab4e043",
          "name": "Joyce Chai",
          "hidden": false
        },
        {
          "_id": "6847e05a3ec10bdd8ab4e044",
          "name": "Seungwhan Moon",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-06T09:23:29.000Z",
      "submittedOnDailyAt": "2025-06-10T06:07:28.858Z",
      "title": "Génération de dialogues assistés d'actions à partir de vidéos subjectives",
      "submittedOnDailyBy": {
        "_id": "6369b1d456d1f93498130a8a",
        "avatarUrl": "/avatars/8ec228aa6f171715652511f948765db9.svg",
        "isPro": false,
        "fullname": "Yichi Zhang",
        "user": "594zyc",
        "type": "user"
      },
      "summary": "Le développement de l'intelligence artificielle dans les conversations a été considérablement grand, mais le développement de systèmes en temps réel pour des tâches visuelles est confronté à de grands défis. Ces systèmes doivent fournir des conseils interactifs et actifs basés sur des entrées visuelles en temps réel, mais leur développement est limité par des processus coûteux de collecte de données et d'évaluation du système. Pour résoudre ces limitations, nous présentons trois principales contributions à travers un cadre informatique basé :\n\nPremièrement, nous introduisons un nouveau plan de collecte de données pour générer des ensembles de données par synthèse de diarologues à partir de captures directes, ce qui résulte en des ensembles de données de diarologues synthétiques à grande échelle et divers domaines.\n\nDeuxièmement, nous développons des métriques d'évaluation automatiques validées par diverses recherches humaines.\n\nTroisièmement, nous proposons des modèles qui traitent des entrées vidéos dynamiques de manière efficace et génèrent des réponses appropriées du début au bout du flux. Ce modèle utilise de nouvelles techniques pour gérer l'inégalité des données et des vidéos de longue durée.\n\nCette étude est basée sur le développement d'assistants AI en temps réel et actifs qui peuvent guider les utilisateurs dans diverses tâches. Page du projet : https://pro-assist.github.io/",
      "upvotes": 0,
      "discussionId": "6847e05a3ec10bdd8ab4e045",
      "projectPage": "https://pro-assist.github.io/",
      "ai_summary": "A framework provides automated data synthesis, evaluation metrics, and an end-to-end model for real-time, proactive conversational AI task guidance using streaming video inputs.",
      "ai_keywords": [
        "data curation pipeline",
        "synthetic dialogue dataset",
        "automatic evaluation metrics",
        "end-to-end model",
        "data imbalance",
        "long-duration videos"
      ]
    },
    "publishedAt": "2025-06-06T05:23:29.000Z",
    "title": "Proactive Assistant Dialogue Generation from Streaming Egocentric Videos",
    "summary": "Recent advances in conversational AI have been substantial, but developing\nreal-time systems for perceptual task guidance remains challenging. These\nsystems must provide interactive, proactive assistance based on streaming\nvisual inputs, yet their development is constrained by the costly and\nlabor-intensive process of data collection and system evaluation. To address\nthese limitations, we present a comprehensive framework with three key\ncontributions. First, we introduce a novel data curation pipeline that\nsynthesizes dialogues from annotated egocentric videos, resulting in \\dataset,\na large-scale synthetic dialogue dataset spanning multiple domains. Second, we\ndevelop a suite of automatic evaluation metrics, validated through extensive\nhuman studies. Third, we propose an end-to-end model that processes streaming\nvideo inputs to generate contextually appropriate responses, incorporating\nnovel techniques for handling data imbalance and long-duration videos. This\nwork lays the foundation for developing real-time, proactive AI assistants\ncapable of guiding users through diverse tasks. Project page:\nhttps://pro-assist.github.io/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05904.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6369b1d456d1f93498130a8a",
      "avatarUrl": "/avatars/8ec228aa6f171715652511f948765db9.svg",
      "fullname": "Yichi Zhang",
      "name": "594zyc",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.04807",
      "authors": [
        {
          "_id": "6847c0983ec10bdd8ab4df69",
          "user": {
            "_id": "65fba5700b78c48c9e393a3e",
            "avatarUrl": "/avatars/795cc8167460d8e89ff91d27c5da9fb2.svg",
            "isPro": false,
            "fullname": "Yuyi Zhang",
            "user": "ZZXF",
            "type": "user"
          },
          "name": "Yuyi Zhang",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-06-10T05:42:47.055Z",
          "hidden": false
        },
        {
          "_id": "6847c0983ec10bdd8ab4df6a",
          "user": {
            "_id": "6616c9e090d2013d26a54b47",
            "avatarUrl": "/avatars/573064303dcdcf778e1fbbfcff3c9a2b.svg",
            "isPro": false,
            "fullname": "Shi",
            "user": "shiyx1",
            "type": "user"
          },
          "name": "Yongxin Shi",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-10T05:20:28.050Z",
          "hidden": false
        },
        {
          "_id": "6847c0983ec10bdd8ab4df6b",
          "name": "Peirong Zhang",
          "hidden": false
        },
        {
          "_id": "6847c0983ec10bdd8ab4df6c",
          "name": "Yixin Zhao",
          "hidden": false
        },
        {
          "_id": "6847c0983ec10bdd8ab4df6d",
          "name": "Zhenhua Yang",
          "hidden": false
        },
        {
          "_id": "6847c0983ec10bdd8ab4df6e",
          "user": {
            "_id": "66a102960072f5db18e860e3",
            "avatarUrl": "/avatars/7679eddb31153c6b868cf496833551d6.svg",
            "isPro": false,
            "fullname": "Lianwen Jin",
            "user": "lianwen",
            "type": "user"
          },
          "name": "Lianwen Jin",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-10T05:20:28.050Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-05T09:33:06.000Z",
      "submittedOnDailyAt": "2025-06-10T04:16:00.729Z",
      "title": "Megahan 97K : Megakatégorie Reconnaissance de Caractères Chinois pour un Grand Dataset (Plus de 97K Catégories)",
      "submittedOnDailyBy": {
        "_id": "65fba5700b78c48c9e393a3e",
        "avatarUrl": "/avatars/795cc8167460d8e89ff91d27c5da9fb2.svg",
        "isPro": false,
        "fullname": "Yuyi Zhang",
        "user": "ZZXF",
        "type": "user"
      },
      "summary": "La écriture chinoise, fondamentale pour la langue et la culture chinoises, est très vaste et diversifiée, et le récent standard GB18030-2022 inclut 87,887 catégories. La précision dans la reconnaissance de ces grandes lettres, connue sous le nom de \"Reconnaissance de Caractères Traditionnels\", est un grand défi pour la préservation de la culture et l'application numérique. Bien que l'OCR (Reconnaissance Optique de Caractères) ait progressé, la reconnaissance de caractères traditionnels reste peu explorée en raison de la rareté des ensembles de données. Pour combler ce domaine important, nous présentons MegaHan97K, un ensemble de données de caractères traditionnels et à grande échelle. Notre travail offre trois contributions principales : 1) MegaHan97K est le premier ensemble de données qui respecte complètement le standard GB18030-2022 et inclut au moins six fois plus de catégories que les ensembles de données actuels ; 2) résout efficacement le problème de la distribution en queue longue, fournissant des échantillons équilibrés pour toutes les catégories à travers trois sous-ensembles différents : lettres manuscrites, historiques et synthétiques ; 3) révèle de nouveaux défis dans la reconnaissance de caractères traditionnels, notamment l'augmentation de la charge de stockage, la reconnaissance de caractères similaires et les défis d'apprentissage sans exemples. Malgré nos limites de connaissances, MegaHan97K est attendu comme l'ensemble de données la plus vaste dans le domaine de l'OCR et dans de nombreuses zones de reconnaissance de motifs. L'ensemble de données est disponible sur https://github.com/SCUT-DLVCLab/MegaHan97K.",
      "upvotes": 0,
      "discussionId": "6847c0993ec10bdd8ab4df6f",
      "ai_summary": "MegaHan97K, a large-scale dataset for recognizing over 97,000 Chinese characters, addresses the long-tail distribution problem and reveals new challenges in mega-category OCR.",
      "ai_keywords": [
        "Optical Character Recognition (OCR)",
        "mega-category recognition",
        "MegaHan97K",
        "long-tail distribution",
        "zero-shot learning"
      ]
    },
    "publishedAt": "2025-06-05T05:33:06.000Z",
    "title": "MegaHan97K: A Large-Scale Dataset for Mega-Category Chinese Character\n  Recognition with over 97K Categories",
    "summary": "Foundational to the Chinese language and culture, Chinese characters\nencompass extraordinarily extensive and ever-expanding categories, with the\nlatest Chinese GB18030-2022 standard containing 87,887 categories. The accurate\nrecognition of this vast number of characters, termed mega-category\nrecognition, presents a formidable yet crucial challenge for cultural heritage\npreservation and digital applications. Despite significant advances in Optical\nCharacter Recognition (OCR), mega-category recognition remains unexplored due\nto the absence of comprehensive datasets, with the largest existing dataset\ncontaining merely 16,151 categories. To bridge this critical gap, we introduce\nMegaHan97K, a mega-category, large-scale dataset covering an unprecedented\n97,455 categories of Chinese characters. Our work offers three major\ncontributions: (1) MegaHan97K is the first dataset to fully support the latest\nGB18030-2022 standard, providing at least six times more categories than\nexisting datasets; (2) It effectively addresses the long-tail distribution\nproblem by providing balanced samples across all categories through its three\ndistinct subsets: handwritten, historical and synthetic subsets; (3)\nComprehensive benchmarking experiments reveal new challenges in mega-category\nscenarios, including increased storage demands, morphologically similar\ncharacter recognition, and zero-shot learning difficulties, while also\nunlocking substantial opportunities for future research. To the best of our\nknowledge, the MetaHan97K is likely the dataset with the largest classes not\nonly in the field of OCR but may also in the broader domain of pattern\nrecognition. The dataset is available at\nhttps://github.com/SCUT-DLVCLab/MegaHan97K.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04807.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65fba5700b78c48c9e393a3e",
      "avatarUrl": "/avatars/795cc8167460d8e89ff91d27c5da9fb2.svg",
      "fullname": "Yuyi Zhang",
      "name": "ZZXF",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.23473",
      "authors": [
        {
          "_id": "6847c9693ec10bdd8ab4df91",
          "name": "Xiaorui Wu",
          "hidden": false
        },
        {
          "_id": "6847c9693ec10bdd8ab4df92",
          "name": "Xiaofeng Mao",
          "hidden": false
        },
        {
          "_id": "6847c9693ec10bdd8ab4df93",
          "name": "Xin Zhang",
          "hidden": false
        },
        {
          "_id": "6847c9693ec10bdd8ab4df94",
          "name": "Fei Li",
          "hidden": false
        },
        {
          "_id": "6847c9693ec10bdd8ab4df95",
          "name": "Chong Teng",
          "hidden": false
        },
        {
          "_id": "6847c9693ec10bdd8ab4df96",
          "name": "Yuxiang Peng",
          "hidden": false
        },
        {
          "_id": "6847c9693ec10bdd8ab4df97",
          "name": "Li Zheng",
          "hidden": false
        },
        {
          "_id": "6847c9693ec10bdd8ab4df98",
          "name": "Donghong Ji",
          "hidden": false
        },
        {
          "_id": "6847c9693ec10bdd8ab4df99",
          "name": "Zhuang Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T14:26:46.000Z",
      "submittedOnDailyAt": "2025-06-10T04:28:59.228Z",
      "title": "EVOREFUSE : Évaluation et mesures de réponse face à la réponse exagérée de rejet d'un modèle de langage par l'optimisation évolutive de Prompts",
      "submittedOnDailyBy": {
        "_id": "63d159132036e44c44f87a91",
        "avatarUrl": "/avatars/6a2c9e5b3b25cf1949277d8c40c0070b.svg",
        "isPro": false,
        "fullname": "Zhuang Li",
        "user": "lizhuang144",
        "type": "user"
      },
      "summary": "Les grands modèles de langue (LLMs) rejettent souvent des structures de requêtes de papillon : des requêtes innocentes synthétiques sont rejetées inutilement par des lignes de sécurité conservatrices, ce qui cause un grand dommage à l'expérience utilisateur. Cette concentration des commandes évalue et atténue le rejet excessif, mais les méthodes actuelles de configuration des commandes, la génération automatique ou les changements de commandes ne sont pas scalables et ne peuvent générer une diversité et une efficacité suffisantes dans la génération de rejets. Pour résoudre ces limitations, nous présentons EVOREFUSE. EVOREFUSE est un approche optimiseur de programmation pour générer des commandes cohérentes de papillon. EVOREFUSE explore un espace de commandes plus diversifié que les méthodes actuelles, en utilisant des stratégies de mutation et de recombinaison pour maximiser la probabilité inférieure de rejet d'un LLM, ce qui le rend plus fiable. Avec EVOREFUSE, nous avons créé deux nouveaux ensembles de données : EVOREFUSE-TEST, qui agit comme un benchmark avec 582 commandes de papillon, avec un rejet moyen de 140.41% sur 9 modèles de LLM, une diversité précédente de 34.86% et un accroissement de 40.03% dans le score de confiance de rejet, dépassant le meilleur benchmark disponible. EVOREFUSE-ALIGN fournit 3 000 commandes et réponses de papillon pour l'entraînement de service et l'entraînement de lignes de sécurité basées sur des préférences. Le modèle LLAMA3.1-8B-INSTRUCT, entraîné avec la régulation de EVOREFUSE-ALIGN, atténue le rejet excessif de 14.31% moins comparé au meilleur ensemble de données de ligne de sécurité. Dans l'analyse de EVOREFUSE-TEST, il est clairement démontré que le modèle se concentre trop sur des mots-clés sensibles et ignore largement le contexte, ce qui provoque un rejet excessif.",
      "upvotes": 0,
      "discussionId": "6847c9693ec10bdd8ab4df9a",
      "ai_summary": "EVOREFUSE, an evolutionary algorithm, generates diverse pseudo-malicious instructions to optimize LLM refusal training, improving user experience without compromising safety.",
      "ai_keywords": [
        "large language models",
        "pseudo-malicious instructions",
        "safety alignment",
        "instruction optimization",
        "evolutionary algorithm",
        "mutation strategies",
        "recombination",
        "evidence lower bound",
        "refusal probability",
        "lexical diversity",
        "LLM response confidence scores",
        "over-refusals",
        "supervised fine-tuning",
        "preference-based alignment training"
      ]
    },
    "publishedAt": "2025-05-29T10:26:46.000Z",
    "title": "EVOREFUSE: Evolutionary Prompt Optimization for Evaluation and\n  Mitigation of LLM Over-Refusal to Pseudo-Malicious Instructions",
    "summary": "Large language models (LLMs) frequently refuse to respond to pseudo-malicious\ninstructions: semantically harmless input queries triggering unnecessary LLM\nrefusals due to conservative safety alignment, significantly impairing user\nexperience. Collecting such instructions is crucial for evaluating and\nmitigating over-refusals, but existing instruction curation methods, like\nmanual creation or instruction rewriting, either lack scalability or fail to\nproduce sufficiently diverse and effective refusal-inducing prompts. To address\nthese limitations, we introduce EVOREFUSE, a prompt optimization approach that\ngenerates diverse pseudo-malicious instructions consistently eliciting\nconfident refusals across LLMs. EVOREFUSE employs an evolutionary algorithm\nexploring the instruction space in more diverse directions than existing\nmethods via mutation strategies and recombination, and iteratively evolves seed\ninstructions to maximize evidence lower bound on LLM refusal probability. Using\nEVOREFUSE, we create two novel datasets: EVOREFUSE-TEST, a benchmark of 582\npseudo-malicious instructions that outperforms the next-best benchmark with\n140.41% higher average refusal triggering rate across 9 LLMs, 34.86% greater\nlexical diversity, and 40.03% improved LLM response confidence scores; and\nEVOREFUSE-ALIGN, which provides 3,000 pseudo-malicious instructions with\nresponses for supervised and preference-based alignment training.\nLLAMA3.1-8B-INSTRUCT supervisedly fine-tuned on EVOREFUSE-ALIGN achieves up to\n14.31% fewer over-refusals than models trained on the second-best alignment\ndataset, without compromising safety. Our analysis with EVOREFUSE-TEST reveals\nmodels trigger over-refusals by overly focusing on sensitive keywords while\nignoring broader context.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23473.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63d159132036e44c44f87a91",
      "avatarUrl": "/avatars/6a2c9e5b3b25cf1949277d8c40c0070b.svg",
      "fullname": "Zhuang Li",
      "name": "lizhuang144",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  }
]