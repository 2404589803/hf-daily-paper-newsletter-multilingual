[
  {
    "paper": {
      "id": "2507.09477",
      "authors": [
        {
          "_id": "68787030001546c83aa4f9ae",
          "name": "Yangning Li",
          "hidden": false
        },
        {
          "_id": "68787030001546c83aa4f9af",
          "user": {
            "_id": "6667e801fd95ddf66cac84ff",
            "avatarUrl": "/avatars/b75f6253160c81f558e6b40ed2ca1755.svg",
            "isPro": false,
            "fullname": "Weizhi Zhang",
            "user": "WZDavid",
            "type": "user"
          },
          "name": "Weizhi Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-17T08:22:48.379Z",
          "hidden": false
        },
        {
          "_id": "68787030001546c83aa4f9b0",
          "name": "Yuyao Yang",
          "hidden": false
        },
        {
          "_id": "68787030001546c83aa4f9b1",
          "name": "Wei-Chieh Huang",
          "hidden": false
        },
        {
          "_id": "68787030001546c83aa4f9b2",
          "name": "Yaozu Wu",
          "hidden": false
        },
        {
          "_id": "68787030001546c83aa4f9b3",
          "name": "Junyu Luo",
          "hidden": false
        },
        {
          "_id": "68787030001546c83aa4f9b4",
          "name": "Yuanchen Bei",
          "hidden": false
        },
        {
          "_id": "68787030001546c83aa4f9b5",
          "user": {
            "_id": "633f112013e836a0fc4fa567",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1665077519962-noauth.jpeg",
            "isPro": false,
            "fullname": "Henry Peng Zou",
            "user": "TreeForest",
            "type": "user"
          },
          "name": "Henry Peng Zou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-17T08:22:46.466Z",
          "hidden": false
        },
        {
          "_id": "68787030001546c83aa4f9b6",
          "name": "Xiao Luo",
          "hidden": false
        },
        {
          "_id": "68787030001546c83aa4f9b7",
          "name": "Yusheng Zhao",
          "hidden": false
        },
        {
          "_id": "68787030001546c83aa4f9b8",
          "name": "Chunkit Chan",
          "hidden": false
        },
        {
          "_id": "68787030001546c83aa4f9b9",
          "name": "Yankai Chen",
          "hidden": false
        },
        {
          "_id": "68787030001546c83aa4f9ba",
          "name": "Zhongfen Deng",
          "hidden": false
        },
        {
          "_id": "68787030001546c83aa4f9bb",
          "name": "Yinghui Li",
          "hidden": false
        },
        {
          "_id": "68787030001546c83aa4f9bc",
          "name": "Hai-Tao Zheng",
          "hidden": false
        },
        {
          "_id": "68787030001546c83aa4f9bd",
          "name": "Dongyuan Li",
          "hidden": false
        },
        {
          "_id": "68787030001546c83aa4f9be",
          "name": "Renhe Jiang",
          "hidden": false
        },
        {
          "_id": "68787030001546c83aa4f9bf",
          "name": "Ming Zhang",
          "hidden": false
        },
        {
          "_id": "68787030001546c83aa4f9c0",
          "name": "Yangqiu Song",
          "hidden": false
        },
        {
          "_id": "68787030001546c83aa4f9c1",
          "name": "Philip S. Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-13T03:29:41.000Z",
      "submittedOnDailyAt": "2025-07-17T02:27:52.541Z",
      "title": "Résumé du système RAG-ronron dans un modèle de langage : complexité du système pour atteindre l'agent-RAG et profondeur du ronron",
      "submittedOnDailyBy": {
        "_id": "6667e801fd95ddf66cac84ff",
        "avatarUrl": "/avatars/b75f6253160c81f558e6b40ed2ca1755.svg",
        "isPro": false,
        "fullname": "Weizhi Zhang",
        "user": "WZDavid",
        "type": "user"
      },
      "summary": "Le REVIEW ARGUMENTATION (RAG) injecte des connaissances externes pour améliorer la factibilité des grands modèles de langue (LLMs), mais a des limitations pour les problèmes d'inférence multi-catégoriels. En revanche, un approche purement basée sur le raisonnement peut entraîner des hallucinations ou des bases de faits incorrectes. Cette recherche intègre les deux approches depuis une perspective de raisonnement et de révision. Tout d'abord, elle optimise chaque étape de RAG en intégrant du raisonnement enrichi (Reasoning-Enhanced RAG). Ensuite, elle complète les connaissances de type différents et élargit le contexte des inférences complexes (RAG-Enhanced Reasoning). Enfin, elle met l'accent sur un cadre simplifié pour la coordination du raisonnement et de la révision, axé sur l'exploration et le raisonnement en arrière (agentic) pour les modèles de LLMs, avec l'objectif de atteindre le meilleur rendement dans des cadres de tests intelligents. Les méthodes, les ensembles de données et les défis ouverts sont classifiés, établissant des ponts pour des recherches en profondeur, efficacité, adaptabilité, fiabilité et centre sur l'être humain. Ce ensemble est accessible à https://github.com/DavidZWZ/Awesome-RAG-Reasoning.",
      "upvotes": 35,
      "discussionId": "68787031001546c83aa4f9c2",
      "githubRepo": "https://github.com/DavidZWZ/Awesome-RAG-Reasoning",
      "ai_summary": "This survey integrates reasoning and retrieval in Large Language Models to improve factuality and multi-step inference, highlighting Synergized RAG-Reasoning frameworks and outlining future research directions.",
      "ai_keywords": [
        "Retrieval-Augmented Generation",
        "Large Language Models",
        "Reasoning-Enhanced RAG",
        "RAG-Enhanced Reasoning",
        "Synergized RAG-Reasoning",
        "knowledge-intensive benchmarks",
        "multimodally-adaptive",
        "trustworthy",
        "human-centric"
      ],
      "githubStars": 46
    },
    "publishedAt": "2025-07-12T23:29:41.000Z",
    "title": "Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning\n  Systems in LLMs",
    "summary": "Retrieval-Augmented Generation (RAG) lifts the factuality of Large Language\nModels (LLMs) by injecting external knowledge, yet it falls short on problems\nthat demand multi-step inference; conversely, purely reasoning-oriented\napproaches often hallucinate or mis-ground facts. This survey synthesizes both\nstrands under a unified reasoning-retrieval perspective. We first map how\nadvanced reasoning optimizes each stage of RAG (Reasoning-Enhanced RAG). Then,\nwe show how retrieved knowledge of different type supply missing premises and\nexpand context for complex inference (RAG-Enhanced Reasoning). Finally, we\nspotlight emerging Synergized RAG-Reasoning frameworks, where (agentic) LLMs\niteratively interleave search and reasoning to achieve state-of-the-art\nperformance across knowledge-intensive benchmarks. We categorize methods,\ndatasets, and open challenges, and outline research avenues toward deeper\nRAG-Reasoning systems that are more effective, multimodally-adaptive,\ntrustworthy, and human-centric. The collection is available at\nhttps://github.com/DavidZWZ/Awesome-RAG-Reasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.09477.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6667e801fd95ddf66cac84ff",
      "avatarUrl": "/avatars/b75f6253160c81f558e6b40ed2ca1755.svg",
      "fullname": "Weizhi Zhang",
      "name": "WZDavid",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.12465",
      "authors": [
        {
          "_id": "6878635e001546c83aa4f979",
          "user": {
            "_id": "65af6f6b52e1b2aae437af2e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65af6f6b52e1b2aae437af2e/sFC98zLL_ZPS9fvZFi01W.jpeg",
            "isPro": false,
            "fullname": "Ziang Cao",
            "user": "Caoza",
            "type": "user"
          },
          "name": "Ziang Cao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-17T08:22:44.605Z",
          "hidden": false
        },
        {
          "_id": "6878635e001546c83aa4f97a",
          "user": {
            "_id": "62fc8cf7ee999004b5a8b982",
            "avatarUrl": "/avatars/6c5dda9e58747054a989f077a078f3dc.svg",
            "isPro": false,
            "fullname": "Zhaoxi Chen",
            "user": "FrozenBurning",
            "type": "user"
          },
          "name": "Zhaoxi Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-07-17T09:22:11.615Z",
          "hidden": false
        },
        {
          "_id": "6878635e001546c83aa4f97b",
          "name": "Linag Pan",
          "hidden": false
        },
        {
          "_id": "6878635e001546c83aa4f97c",
          "user": {
            "_id": "62ab1ac1d48b4d8b048a3473",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656826685333-62ab1ac1d48b4d8b048a3473.png",
            "isPro": false,
            "fullname": "Ziwei Liu",
            "user": "liuziwei7",
            "type": "user"
          },
          "name": "Ziwei Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-07-17T09:21:56.595Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/65af6f6b52e1b2aae437af2e/nrL7wGaZ1Z5FZWuu0GTbg.mp4"
      ],
      "publishedAt": "2025-07-16T17:59:35.000Z",
      "submittedOnDailyAt": "2025-07-17T01:26:11.001Z",
      "title": "PhysX : Physique basée sur des ressources 3D",
      "submittedOnDailyBy": {
        "_id": "65af6f6b52e1b2aae437af2e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65af6f6b52e1b2aae437af2e/sFC98zLL_ZPS9fvZFi01W.jpeg",
        "isPro": false,
        "fullname": "Ziang Cao",
        "user": "Caoza",
        "type": "user"
      },
      "summary": "3D modélisation permet un mouvement physique dans le virtuel. Actuellement, la génération 3D se base sur la géométrie et les textures, dépassant le modélisation physique. En conséquence, les modèles 3D générés évoluent rapidement, mais les actifs 3D synthétisés dépassent leurs propriétés physiques riches et importantes, empêchent la simulation dans le domaine physique ou l'application de l'IA dans la réalité concrète. Pour résoudre ces défis, nous proposons le paradigme unifié de la génération d'actifs 3D basée sur la physique, PhysX.\n\n1) Nous passons sur les erreurs importantes des ensembles de données 3D avec peu d'annotations physiques, présentant PhysXNet, le premier ensemble de données 3D annoté de manière systématique dans cinq dimensions physiques fondamentales. En particulier, nous avons développé un processus d'annotation humaine scalable basé sur des modèles de langage visuel, ce qui permet une production efficace d'actifs.\n\n2) De plus, nous proposons PhysXGen, un cadre de travail pour la génération d'actifs 3D à partir d'images physiques, où l'on injecte des connaissances physiques préalablement entraînées dans des espaces 3D pré-entraînés. En particulier, PhysXGen modélise clairement la corrélation potentielle entre la structure 3D et les propriétés physiques, génère des actifs 3D avec des prédictions physiques raisonnables et maintient la qualité géométrique naturelle. Des expériences extensives démontrent la haute efficacité et la capacité de généralisation de notre cadre. Tout code, données et modèles sont disponibles pour encourager futures recherches en IA physique générative.",
      "upvotes": 19,
      "discussionId": "6878635e001546c83aa4f97d",
      "projectPage": "https://physx-3d.github.io/",
      "githubRepo": "https://github.com/ziangcao0312/PhysX",
      "ai_summary": "PhysX addresses the lack of physical properties in 3D generative models by introducing PhysXNet, a physics-annotated dataset, and PhysXGen, a framework that integrates physical knowledge into 3D asset generation.",
      "ai_keywords": [
        "physics-grounded 3D asset generation",
        "PhysXNet",
        "physics-annotated 3D datasets",
        "vision-language models",
        "human-in-the-loop annotation pipeline",
        "PhysXGen",
        "feed-forward framework",
        "dual-branch architecture",
        "latent correlations",
        "physical predictions",
        "geometry quality"
      ],
      "githubStars": 20
    },
    "publishedAt": "2025-07-16T13:59:35.000Z",
    "title": "PhysX: Physical-Grounded 3D Asset Generation",
    "summary": "3D modeling is moving from virtual to physical. Existing 3D generation\nprimarily emphasizes geometries and textures while neglecting physical-grounded\nmodeling. Consequently, despite the rapid development of 3D generative models,\nthe synthesized 3D assets often overlook rich and important physical\nproperties, hampering their real-world application in physical domains like\nsimulation and embodied AI. As an initial attempt to address this challenge, we\npropose PhysX, an end-to-end paradigm for physical-grounded 3D asset\ngeneration. 1) To bridge the critical gap in physics-annotated 3D datasets, we\npresent PhysXNet - the first physics-grounded 3D dataset systematically\nannotated across five foundational dimensions: absolute scale, material,\naffordance, kinematics, and function description. In particular, we devise a\nscalable human-in-the-loop annotation pipeline based on vision-language models,\nwhich enables efficient creation of physics-first assets from raw 3D assets.2)\nFurthermore, we propose PhysXGen, a feed-forward framework for\nphysics-grounded image-to-3D asset generation, injecting physical knowledge\ninto the pre-trained 3D structural space. Specifically, PhysXGen employs a\ndual-branch architecture to explicitly model the latent correlations between 3D\nstructures and physical properties, thereby producing 3D assets with plausible\nphysical predictions while preserving the native geometry quality. Extensive\nexperiments validate the superior performance and promising generalization\ncapability of our framework. All the code, data, and models will be released to\nfacilitate future research in generative physical AI.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/65af6f6b52e1b2aae437af2e/nrL7wGaZ1Z5FZWuu0GTbg.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.12465.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65af6f6b52e1b2aae437af2e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65af6f6b52e1b2aae437af2e/sFC98zLL_ZPS9fvZFi01W.jpeg",
      "fullname": "Ziang Cao",
      "name": "Caoza",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.11949",
      "authors": [
        {
          "_id": "687872c3001546c83aa4f9cf",
          "user": {
            "_id": "683d94e5ba11bab2cc848aab",
            "avatarUrl": "/avatars/4a1915ad48c78b3a71733b3282f2f93c.svg",
            "isPro": false,
            "fullname": "Shuyang Xu",
            "user": "JimSYXu",
            "type": "user"
          },
          "name": "Shuyang Xu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-07-17T09:22:22.253Z",
          "hidden": false
        },
        {
          "_id": "687872c3001546c83aa4f9d0",
          "user": {
            "_id": "645223fb01d7bd9555ea399a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645223fb01d7bd9555ea399a/fVR7XmGg6pMSRKx9sEdvT.png",
            "isPro": false,
            "fullname": "Zhiyang Dou",
            "user": "frankzydou",
            "type": "user"
          },
          "name": "Zhiyang Dou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-17T08:22:59.825Z",
          "hidden": false
        },
        {
          "_id": "687872c3001546c83aa4f9d1",
          "name": "Mingyi Shi",
          "hidden": false
        },
        {
          "_id": "687872c3001546c83aa4f9d2",
          "name": "Liang Pan",
          "hidden": false
        },
        {
          "_id": "687872c3001546c83aa4f9d3",
          "name": "Leo Ho",
          "hidden": false
        },
        {
          "_id": "687872c3001546c83aa4f9d4",
          "name": "Jingbo Wang",
          "hidden": false
        },
        {
          "_id": "687872c3001546c83aa4f9d5",
          "name": "Yuan Liu",
          "hidden": false
        },
        {
          "_id": "687872c3001546c83aa4f9d6",
          "name": "Cheng Lin",
          "hidden": false
        },
        {
          "_id": "687872c3001546c83aa4f9d7",
          "name": "Yuexin Ma",
          "hidden": false
        },
        {
          "_id": "687872c3001546c83aa4f9d8",
          "name": "Wenping Wang",
          "hidden": false
        },
        {
          "_id": "687872c3001546c83aa4f9d9",
          "name": "Taku Komura",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/645223fb01d7bd9555ea399a/KoPBJkGwyft7hfVmM8ikZ.mp4"
      ],
      "publishedAt": "2025-07-16T06:33:11.000Z",
      "submittedOnDailyAt": "2025-07-17T02:48:15.112Z",
      "title": "MOSPA : Génération d'Actions Humaines par l'Acoustique Spatiale",
      "submittedOnDailyBy": {
        "_id": "645223fb01d7bd9555ea399a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645223fb01d7bd9555ea399a/fVR7XmGg6pMSRKx9sEdvT.png",
        "isPro": false,
        "fullname": "Zhiyang Dou",
        "user": "frankzydou",
        "type": "user"
      },
      "summary": "Les humains virtuels implémenter des réactions dynamiques et réalistes à diverses stimuli auditifs est un défi important dans l'animation de personnages, ce qui exige l'intégration de modèles cognitifs observateurs et de synthèse de mouvements. Bien qu'il soit crucial, ce sujet a été peu étudié. Les études précédentes ont principalement porté sur la modélisation des habiletés comme le parler, le son et la musique, adaptant ces modèles aux fonctions et mouvements. Les caractéristiques spatiales du son n'ont pas encore été considérées dans ces modèles. Pour corriger cela et faciliter la modélisation de mouvements de personnages de haute qualité à partir du son spatial, nous présentons le premier ensemble de données détaillé de personnages animés animés par son spatial (SAM). Cet ensemble de données inclut des données sonores spatiales et de mouvement de haute qualité et variés. Pour établir un benchmark, nous avons développé un cadre génératif basé sur une diffusion efficace et efficace, que nous nommons MOSPA (Mouvements de Personnages à Partir du Son Spatial). Ce cadre utilise une structure de fusion efficace pour comprendre de manière fiable la relation entre les mouvements corporels et le son spatial. Après l'entraînement, MOSPA peut générer divers mouvements réalistes de personnages à partir d'entrées de son spatial. Notre ensemble de données et nos méthodes ont atteint les meilleurs résultats dans ce travail. Notre modèle et ensemble de données seront disponibles à la demande. Pour plus de détails, consultez le vidéo du sous-projet.",
      "upvotes": 10,
      "discussionId": "687872c9001546c83aa4f9da",
      "ai_summary": "A diffusion-based generative framework, MOSPA, is introduced to model human motion in response to spatial audio, achieving state-of-the-art performance using the newly created SAM dataset.",
      "ai_keywords": [
        "diffusion-based generative framework",
        "spatial audio",
        "human motion",
        "SAM dataset",
        "MOSPA",
        "spatial features",
        "perceptual modeling",
        "motion synthesis"
      ]
    },
    "publishedAt": "2025-07-16T02:33:11.000Z",
    "title": "MOSPA: Human Motion Generation Driven by Spatial Audio",
    "summary": "Enabling virtual humans to dynamically and realistically respond to diverse\nauditory stimuli remains a key challenge in character animation, demanding the\nintegration of perceptual modeling and motion synthesis. Despite its\nsignificance, this task remains largely unexplored. Most previous works have\nprimarily focused on mapping modalities like speech, audio, and music to\ngenerate human motion. As of yet, these models typically overlook the impact of\nspatial features encoded in spatial audio signals on human motion. To bridge\nthis gap and enable high-quality modeling of human movements in response to\nspatial audio, we introduce the first comprehensive Spatial Audio-Driven Human\nMotion (SAM) dataset, which contains diverse and high-quality spatial audio and\nmotion data. For benchmarking, we develop a simple yet effective\ndiffusion-based generative framework for human MOtion generation driven by\nSPatial Audio, termed MOSPA, which faithfully captures the relationship between\nbody motion and spatial audio through an effective fusion mechanism. Once\ntrained, MOSPA could generate diverse realistic human motions conditioned on\nvarying spatial audio inputs. We perform a thorough investigation of the\nproposed dataset and conduct extensive experiments for benchmarking, where our\nmethod achieves state-of-the-art performance on this task. Our model and\ndataset will be open-sourced upon acceptance. Please refer to our supplementary\nvideo for more details.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/645223fb01d7bd9555ea399a/KoPBJkGwyft7hfVmM8ikZ.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.11949.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645223fb01d7bd9555ea399a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645223fb01d7bd9555ea399a/fVR7XmGg6pMSRKx9sEdvT.png",
      "fullname": "Zhiyang Dou",
      "name": "frankzydou",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.12463",
      "authors": [
        {
          "_id": "68788789001546c83aa4f9e4",
          "name": "Renjie Li",
          "hidden": false
        },
        {
          "_id": "68788789001546c83aa4f9e5",
          "user": {
            "_id": "6825fc0e58cf56d164cb339d",
            "avatarUrl": "/avatars/4ade4a5bbb0a805a92a83bfb233f805f.svg",
            "isPro": false,
            "fullname": "Ruijie Ye",
            "user": "jerryye0110",
            "type": "user"
          },
          "name": "Ruijie Ye",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-17T08:17:53.937Z",
          "hidden": false
        },
        {
          "_id": "68788789001546c83aa4f9e6",
          "name": "Mingyang Wu",
          "hidden": false
        },
        {
          "_id": "68788789001546c83aa4f9e7",
          "name": "Hao Frank Yang",
          "hidden": false
        },
        {
          "_id": "68788789001546c83aa4f9e8",
          "user": {
            "_id": "63b354bb7091e602f1a0e2e8",
            "avatarUrl": "/avatars/a388d93c0af2f57eadb6fa60d6789041.svg",
            "isPro": false,
            "fullname": "wayne",
            "user": "waynefan",
            "type": "user"
          },
          "name": "Zhiwen Fan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-17T08:23:18.102Z",
          "hidden": false
        },
        {
          "_id": "68788789001546c83aa4f9e9",
          "name": "Hezhen Hu",
          "hidden": false
        },
        {
          "_id": "68788789001546c83aa4f9ea",
          "user": {
            "_id": "62548d5fef3debb2ddf91217",
            "avatarUrl": "/avatars/14975b45568f9c399c92c3986b6ce83e.svg",
            "isPro": false,
            "fullname": "Zhengzhong Tu",
            "user": "vztu",
            "type": "user"
          },
          "name": "Zhengzhong Tu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-17T08:23:16.047Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/62548d5fef3debb2ddf91217/ARblfCBCEmNaS2zzbTJLm.jpeg"
      ],
      "publishedAt": "2025-07-16T17:59:30.000Z",
      "submittedOnDailyAt": "2025-07-17T03:48:41.381Z",
      "title": "MMHU : Comprendre le comportement humain dans un cadre de référence de modèles à grande échelle",
      "submittedOnDailyBy": {
        "_id": "62548d5fef3debb2ddf91217",
        "avatarUrl": "/avatars/14975b45568f9c399c92c3986b6ce83e.svg",
        "isPro": false,
        "fullname": "Zhengzhong Tu",
        "user": "vztu",
        "type": "user"
      },
      "summary": "L'être humain est un élément essentiel du système de transport et il est crucial de comprendre son comportement pour le développement de systèmes d'opération sûrs. L'histoire de la recherche a examiné différents aspects du comportement humain, en analysant les mouvements, les routes et les intentions, mais il n'existe pas encore un cadre de référence consolidé pour comprendre le comportement humain en conduite autonome. Dans cette étude, on propose le MMHU, un cadre de référence d'analyse du comportement humain à grande échelle, qui inclut des commentaires détaillés sur le mouvement humain et les routes, des explications de comportement, des intentions humaines et des étiquettes d'actions importantes liées à la sécurité de la conduite. Le jeu de données est composé de données d'opération existantes de Waymo, de vidéos 'en champ' obtenues sur YouTube et de données de récolte automatique, totalisant 57k clips de mouvements de personnes et 1,73M cadres. Grâce au développement d'un système de capture de comportement riche par commentaire, le jeu de données fournit un analyse détaillée et des cadres de référence pour diverses tâches, offrant un système d'évaluation large, allant des prédictions de mouvements aux générations de mouvements, y compris des évaluations larges du comportement humain, de la réponse aux questions à divers systèmes d'évaluation. Page du projet : https://MMHU-Benchmark.github.io.",
      "upvotes": 9,
      "discussionId": "6878878a001546c83aa4f9eb",
      "projectPage": "https://mmhu-benchmark.github.io/",
      "ai_summary": "A large-scale benchmark, MMHU, is proposed for human behavior analysis in autonomous driving, featuring rich annotations and diverse data sources, and benchmarking multiple tasks including motion prediction and behavior question answering.",
      "ai_keywords": [
        "human behavior analysis",
        "motion prediction",
        "motion generation",
        "human behavior question answering",
        "human-in-the-loop annotation",
        "Waymo",
        "YouTube",
        "self-collected data"
      ]
    },
    "publishedAt": "2025-07-16T13:59:30.000Z",
    "title": "MMHU: A Massive-Scale Multimodal Benchmark for Human Behavior\n  Understanding",
    "summary": "Humans are integral components of the transportation ecosystem, and\nunderstanding their behaviors is crucial to facilitating the development of\nsafe driving systems. Although recent progress has explored various aspects of\nhuman behaviorx2014such as motion, trajectories, and\nintentionx2014a comprehensive benchmark for evaluating human\nbehavior understanding in autonomous driving remains unavailable. In this work,\nwe propose MMHU, a large-scale benchmark for human behavior analysis\nfeaturing rich annotations, such as human motion and trajectories, text\ndescription for human motions, human intention, and critical behavior labels\nrelevant to driving safety. Our dataset encompasses 57k human motion clips and\n1.73M frames gathered from diverse sources, including established driving\ndatasets such as Waymo, in-the-wild videos from YouTube, and self-collected\ndata. A human-in-the-loop annotation pipeline is developed to generate rich\nbehavior captions. We provide a thorough dataset analysis and benchmark\nmultiple tasksx2014ranging from motion prediction to motion\ngeneration and human behavior question answeringx2014thereby\noffering a broad evaluation suite. Project page :\nhttps://MMHU-Benchmark.github.io.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62548d5fef3debb2ddf91217/ARblfCBCEmNaS2zzbTJLm.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.12463.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62548d5fef3debb2ddf91217",
      "avatarUrl": "/avatars/14975b45568f9c399c92c3986b6ce83e.svg",
      "fullname": "Zhengzhong Tu",
      "name": "vztu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.12415",
      "authors": [
        {
          "_id": "68788b9b001546c83aa4f9ed",
          "name": "Xinyi He",
          "hidden": false
        },
        {
          "_id": "68788b9b001546c83aa4f9ee",
          "user": {
            "_id": "612ee6a7b960e78c6d2319d4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/612ee6a7b960e78c6d2319d4/2Hu9BaAyXbyh1vt0v1Qui.jpeg",
            "isPro": false,
            "fullname": "Qian Liu",
            "user": "SivilTaram",
            "type": "user"
          },
          "name": "Qian Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-17T08:17:46.375Z",
          "hidden": false
        },
        {
          "_id": "68788b9b001546c83aa4f9ef",
          "user": {
            "_id": "61711f02e0b1ddb56eb9b526",
            "avatarUrl": "/avatars/3e2fdf774f5bc1f73b450486d6da42d4.svg",
            "isPro": true,
            "fullname": "Mingzhe Du",
            "user": "Elfsong",
            "type": "user"
          },
          "name": "Mingzhe Du",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-17T09:08:30.259Z",
          "hidden": false
        },
        {
          "_id": "68788b9b001546c83aa4f9f0",
          "name": "Lin Yan",
          "hidden": false
        },
        {
          "_id": "68788b9b001546c83aa4f9f1",
          "name": "Zhijie Fan",
          "hidden": false
        },
        {
          "_id": "68788b9b001546c83aa4f9f2",
          "name": "Yiming Huang",
          "hidden": false
        },
        {
          "_id": "68788b9b001546c83aa4f9f3",
          "name": "Zejian Yuan",
          "hidden": false
        },
        {
          "_id": "68788b9b001546c83aa4f9f4",
          "name": "Zejun Ma",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/612ee6a7b960e78c6d2319d4/_XkK-c8Xm-G1Ui5AwtGdu.png"
      ],
      "publishedAt": "2025-07-16T17:05:17.000Z",
      "submittedOnDailyAt": "2025-07-17T04:10:30.408Z",
      "title": "SWE-Perf : Est-ce que le modèle de langage peut optimiser le rendement du code dans des dépôts réels ?",
      "submittedOnDailyBy": {
        "_id": "612ee6a7b960e78c6d2319d4",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/612ee6a7b960e78c6d2319d4/2Hu9BaAyXbyh1vt0v1Qui.jpeg",
        "isPro": false,
        "fullname": "Qian Liu",
        "user": "SivilTaram",
        "type": "user"
      },
      "summary": "L'efficacité du code est la plus importante dans le développement du logiciel mondial et est particulièrement cruciale dans les systèmes de production. Les modèles de langage de code (LLMs) ont démontré des habiletés exceptionnelles dans la génération de code et la correction d'erreurs, mais leur capacité à optimiser le code au niveau de la base de données n'a pas été explorée en grande mesure. Pour corriger cette lacune, nous présentons SWE-Perf. C'est le premier cadre de référence conçu spécifiquement pour que les LLMs puissent effectuer des évaluations systématiques de l'optimisation du code dans des contextes de base de données réels. SWE-Perf est composé de 140 Demandez de Pull Requests qui comprennent des améliorations en efficacité, et chaque instance de benchmark inclut le code source relatif, la fonction objectif, les tests de performance, les patchs d'expérience des porteurs et un environnement executable. Par l'évaluation exhaustive des méthodes d'accès au niveau de fichier et de base de données (par exemple, Agentless et OpenHands), SWE-Perf souligne clairement la grande différence entre les capacités actuelles des LLMs et le rendement d'optimisation de l'expérience, et souligne les opportunités de recherche importantes dans ce nouveau domaine.",
      "upvotes": 8,
      "discussionId": "68788b9b001546c83aa4f9f5",
      "projectPage": "https://swe-perf.github.io/",
      "githubRepo": "https://github.com/SWE-Perf/SWE-Perf",
      "ai_summary": "SWE-Perf is a benchmark for evaluating Large Language Models in code performance optimization using real-world repository data.",
      "ai_keywords": [
        "Large Language Models",
        "code performance optimization",
        "benchmark",
        "performance-improving pull requests",
        "codebase",
        "target functions",
        "performance-related tests",
        "expert-authored patches",
        "executable environments",
        "Agentless",
        "OpenHands"
      ],
      "githubStars": 4
    },
    "publishedAt": "2025-07-16T13:05:17.000Z",
    "title": "SWE-Perf: Can Language Models Optimize Code Performance on Real-World\n  Repositories?",
    "summary": "Code performance optimization is paramount in real-world software engineering\nand critical for production-level systems. While Large Language Models (LLMs)\nhave demonstrated impressive capabilities in code generation and bug fixing,\ntheir proficiency in enhancing code performance at the repository level remains\nlargely unexplored. To address this gap, we introduce SWE-Perf, the first\nbenchmark specifically designed to systematically evaluate LLMs on code\nperformance optimization tasks within authentic repository contexts. SWE-Perf\ncomprises 140 carefully curated instances, each derived from\nperformance-improving pull requests from popular GitHub repositories. Each\nbenchmark instance includes the relevant codebase, target functions,\nperformance-related tests, expert-authored patches, and executable\nenvironments. Through a comprehensive evaluation of representative methods that\nspan file-level and repo-level approaches (e.g., Agentless and OpenHands), we\nreveal a substantial capability gap between existing LLMs and expert-level\noptimization performance, highlighting critical research opportunities in this\nemerging field.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/612ee6a7b960e78c6d2319d4/_XkK-c8Xm-G1Ui5AwtGdu.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.12415.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "612ee6a7b960e78c6d2319d4",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/612ee6a7b960e78c6d2319d4/2Hu9BaAyXbyh1vt0v1Qui.jpeg",
      "fullname": "Qian Liu",
      "name": "SivilTaram",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 85
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.11527",
      "authors": [
        {
          "_id": "68785ee1001546c83aa4f967",
          "user": {
            "_id": "65c950ebd908bd52a4477116",
            "avatarUrl": "/avatars/bc6ba0dd2903c7bea37f7b9c40857718.svg",
            "isPro": false,
            "fullname": "Yinsheng Li",
            "user": "Eason666",
            "type": "user"
          },
          "name": "Yinsheng Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-17T08:22:42.470Z",
          "hidden": false
        },
        {
          "_id": "68785ee1001546c83aa4f968",
          "user": {
            "_id": "643ba2f725681c3afaa8f05e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643ba2f725681c3afaa8f05e/2RnOdmbBM8WHYhiTGG-Cd.jpeg",
            "isPro": false,
            "fullname": "Zhen Dong",
            "user": "zhendongucb",
            "type": "user"
          },
          "name": "Zhen Dong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-17T08:22:37.102Z",
          "hidden": false
        },
        {
          "_id": "68785ee1001546c83aa4f969",
          "name": "Yi Shao",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/643ba2f725681c3afaa8f05e/2T1jxRrhqMGVLESbEDwJT.png"
      ],
      "publishedAt": "2025-07-15T17:56:04.000Z",
      "submittedOnDailyAt": "2025-07-17T01:30:58.515Z",
      "title": "DrafterBench : Marceaux de Tests pour l'Automatisation des Tâches en Ingénierie Civile des Infrastructures",
      "submittedOnDailyBy": {
        "_id": "643ba2f725681c3afaa8f05e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643ba2f725681c3afaa8f05e/2RnOdmbBM8WHYhiTGG-Cd.jpeg",
        "isPro": false,
        "fullname": "Zhen Dong",
        "user": "zhendongucb",
        "type": "user"
      },
      "summary": "Les modèles de langage grands (LLM) agents montrent un grand potentiel pour résoudre des problèmes réels et présentent des possibilités pour l'automatisation des tâches dans l'industrie. Cependant, pour évaluer l'automatisation des agents à partir d'une perspective industrielle, il faut plus de référentiels. Nous proposons donc DrafterBench comme un référentiel pour l'évaluation détaillée des agents de LLM dans la tâche de modification de dessins techniques en génie civil. DrafterBench intègre 12 tâches basées sur des fichiers de dessins réels et comprend 46 fonctions/outils de définition d'utilisateur et 1920 tâches. Il est un référentiel strict qui évalue la capacité des agents à comprendre des commandes longues et complexes, l'utilisation de connaissances avancées et la perception potentielle de politiques dans la qualité des ordres dynamiques. L'ensemble des outils évalue une série de compétences, y compris la compréhension de données structurées, l'exécution de fonctions, le suivi des commandes et le raisonnement critique. DrafterBench analyse avec précision l'exactitude du travail et les statistiques d'erreurs, offrant une vision profonde des capacités des agents et aidant à établir des objectifs d'amélioration de l'intégration de LLM dans les applications de génie civil. Notre référentiel est disponible sur https://github.com/Eason-Li-AIS/DrafterBench et le ensemble de tests est téléchargeable sur https://huggingface.co/datasets/Eason666/DrafterBench.",
      "upvotes": 8,
      "discussionId": "68785ee1001546c83aa4f96a",
      "githubRepo": "https://github.com/Eason-Li-AIS/DrafterBench",
      "ai_summary": "DrafterBench is an open-source benchmark for evaluating LLM agents in technical drawing revision, assessing their capabilities in structured data comprehension, function execution, instruction following, and critical reasoning.",
      "ai_keywords": [
        "Large Language Model (LLM)",
        "technical drawing revision",
        "structured data comprehension",
        "function execution",
        "instruction following",
        "critical reasoning",
        "benchmark",
        "open-source",
        "implicit policy awareness"
      ],
      "githubStars": 29
    },
    "publishedAt": "2025-07-15T13:56:04.000Z",
    "title": "DrafterBench: Benchmarking Large Language Models for Tasks Automation in\n  Civil Engineering",
    "summary": "Large Language Model (LLM) agents have shown great potential for solving\nreal-world problems and promise to be a solution for tasks automation in\nindustry. However, more benchmarks are needed to systematically evaluate\nautomation agents from an industrial perspective, for example, in Civil\nEngineering. Therefore, we propose DrafterBench for the comprehensive\nevaluation of LLM agents in the context of technical drawing revision, a\nrepresentation task in civil engineering. DrafterBench contains twelve types of\ntasks summarized from real-world drawing files, with 46 customized\nfunctions/tools and 1920 tasks in total. DrafterBench is an open-source\nbenchmark to rigorously test AI agents' proficiency in interpreting intricate\nand long-context instructions, leveraging prior knowledge, and adapting to\ndynamic instruction quality via implicit policy awareness. The toolkit\ncomprehensively assesses distinct capabilities in structured data\ncomprehension, function execution, instruction following, and critical\nreasoning. DrafterBench offers detailed analysis of task accuracy and error\nstatistics, aiming to provide deeper insight into agent capabilities and\nidentify improvement targets for integrating LLMs in engineering applications.\nOur benchmark is available at https://github.com/Eason-Li-AIS/DrafterBench,\nwith the test set hosted at\nhttps://huggingface.co/datasets/Eason666/DrafterBench.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/643ba2f725681c3afaa8f05e/2T1jxRrhqMGVLESbEDwJT.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.11527.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "643ba2f725681c3afaa8f05e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643ba2f725681c3afaa8f05e/2RnOdmbBM8WHYhiTGG-Cd.jpeg",
      "fullname": "Zhen Dong",
      "name": "zhendongucb",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.02857",
      "authors": [
        {
          "_id": "68788cd9001546c83aa4f9f7",
          "user": {
            "_id": "66aef8691dd7d0a8c6584724",
            "avatarUrl": "/avatars/df9c2a56f3d0746cf64a330137a105b4.svg",
            "isPro": false,
            "fullname": "Ziye Li",
            "user": "TribeRinb",
            "type": "user"
          },
          "name": "Ziye Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-07-17T09:48:49.437Z",
          "hidden": false
        },
        {
          "_id": "68788cd9001546c83aa4f9f8",
          "name": "Hao Luo",
          "hidden": false
        },
        {
          "_id": "68788cd9001546c83aa4f9f9",
          "user": {
            "_id": "6335c7fa2db86a181cca723f",
            "avatarUrl": "/avatars/13f04af01914f8473f9939f49b4eecd4.svg",
            "isPro": false,
            "fullname": "XC",
            "user": "XinchengShuai",
            "type": "user"
          },
          "name": "Xincheng Shuai",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-07-17T09:48:34.400Z",
          "hidden": false
        },
        {
          "_id": "68788cd9001546c83aa4f9fa",
          "user": {
            "_id": "67ff29ecbf6889a333c69c7a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/fYIJFtF0uciB-1wb0lmTY.png",
            "isPro": false,
            "fullname": "Henghui Ding",
            "user": "HenghuiDing",
            "type": "user"
          },
          "name": "Henghui Ding",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-07-17T09:48:26.454Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-03T17:59:02.000Z",
      "submittedOnDailyAt": "2025-07-17T04:11:55.501Z",
      "title": "Selon les conditions, l'image est traitée pour contrôler l'animation.",
      "submittedOnDailyBy": {
        "_id": "67ff29ecbf6889a333c69c7a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/fYIJFtF0uciB-1wb0lmTY.png",
        "isPro": false,
        "fullname": "Henghui Ding",
        "user": "HenghuiDing",
        "type": "user"
      },
      "summary": "Le développement récent de la génération d'images, notamment dans les modèles distribués, a conduit à un progrès significatif dans la synthèse de texte à image (T2V) et d'image à image (I2V). Cependant, des problèmes persistent dans l'intégration efficace de signaux dynamiques de mouvement et de contraintes spatiales flexibles. Les méthodes actuelles de T2V dépendent généralement de modèles de texte et peuvent rencontrer des difficultés pour contrôler précisément la position spatiale du contenu généré. D'autre part, les méthodes de I2V dépendent d'images réelles, ce qui limite la possibilité d'éditer le contenu synthétique. Certains méthodes ont introduit ControlNet pour introduire conditions basées sur des images, mais souvent manquent de contrôles clairs de mouvement et présentent une charge de calcul significative. En réponse à ces limites, nous proposons le cadre sans limites pour animer des images conditionnées avec des trajectoires de mouvement définies par l'utilisateur, appelé AnyI2V. AnyI2V supporte un large type de données que ControlNet ne couvre pas, comme ensembles de données ou points centraux, permettant un contrôle plus flexible et varié dans la génération d'animations. De plus, il supporte des entrées conditionnées de données mixtes et permet des états transitoires et éditions via LoRA et modèles de texte. À travers des expériences larges, la proposition de AnyI2V offre une nouvelle perspective sur le contrôle de l'espace et du mouvement, démontrant des résultats excellents. Le code est disponible sur https://henghuiding.com/AnyI2V/.",
      "upvotes": 3,
      "discussionId": "68788cda001546c83aa4f9fb",
      "projectPage": "https://henghuiding.com/AnyI2V/",
      "githubRepo": "https://github.com/FudanCVL/AnyI2V",
      "ai_summary": "AnyI2V is a training-free framework that animates conditional images with user-defined motion trajectories, supporting various data types and enabling flexible video generation.",
      "ai_keywords": [
        "diffusion models",
        "text-to-video",
        "image-to-video",
        "ControlNet",
        "motion trajectories",
        "conditional images",
        "meshes",
        "point clouds",
        "mixed conditional inputs",
        "style transfer",
        "LoRA",
        "text prompts"
      ],
      "githubStars": 83
    },
    "publishedAt": "2025-07-03T13:59:02.000Z",
    "title": "AnyI2V: Animating Any Conditional Image with Motion Control",
    "summary": "Recent advancements in video generation, particularly in diffusion models,\nhave driven notable progress in text-to-video (T2V) and image-to-video (I2V)\nsynthesis. However, challenges remain in effectively integrating dynamic motion\nsignals and flexible spatial constraints. Existing T2V methods typically rely\non text prompts, which inherently lack precise control over the spatial layout\nof generated content. In contrast, I2V methods are limited by their dependence\non real images, which restricts the editability of the synthesized content.\nAlthough some methods incorporate ControlNet to introduce image-based\nconditioning, they often lack explicit motion control and require\ncomputationally expensive training. To address these limitations, we propose\nAnyI2V, a training-free framework that animates any conditional images with\nuser-defined motion trajectories. AnyI2V supports a broader range of modalities\nas the conditional image, including data types such as meshes and point clouds\nthat are not supported by ControlNet, enabling more flexible and versatile\nvideo generation. Additionally, it supports mixed conditional inputs and\nenables style transfer and editing via LoRA and text prompts. Extensive\nexperiments demonstrate that the proposed AnyI2V achieves superior performance\nand provides a new perspective in spatial- and motion-controlled video\ngeneration. Code is available at https://henghuiding.com/AnyI2V/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.02857.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67ff29ecbf6889a333c69c7a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/fYIJFtF0uciB-1wb0lmTY.png",
      "fullname": "Henghui Ding",
      "name": "HenghuiDing",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.12462",
      "authors": [
        {
          "_id": "68785eb6001546c83aa4f95b",
          "name": "Yuxi Xiao",
          "hidden": false
        },
        {
          "_id": "68785eb6001546c83aa4f95c",
          "user": {
            "_id": "649bf403fd9cea8366d669ad",
            "avatarUrl": "/avatars/27bd8ca9a948ec38fee950b64f669ce3.svg",
            "isPro": false,
            "fullname": "Jianyuan Wang",
            "user": "JianyuanWang",
            "type": "user"
          },
          "name": "Jianyuan Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-07-17T09:37:32.434Z",
          "hidden": false
        },
        {
          "_id": "68785eb6001546c83aa4f95d",
          "user": {
            "_id": "6485ce5ec7f19728a49df17a",
            "avatarUrl": "/avatars/e83966e6906c1d0f151300981e30f85a.svg",
            "isPro": true,
            "fullname": "Nan",
            "user": "cherubicxn",
            "type": "user"
          },
          "name": "Nan Xue",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-17T08:15:41.525Z",
          "hidden": false
        },
        {
          "_id": "68785eb6001546c83aa4f95e",
          "user": {
            "_id": "6393a73584c565d2c3416cb9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6393a73584c565d2c3416cb9/OGtX-i-_MLg5--qA054ti.jpeg",
            "isPro": true,
            "fullname": "Nikita Karaev",
            "user": "nikkar",
            "type": "user"
          },
          "name": "Nikita Karaev",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-07-17T09:40:30.784Z",
          "hidden": false
        },
        {
          "_id": "68785eb6001546c83aa4f95f",
          "name": "Yuri Makarov",
          "hidden": false
        },
        {
          "_id": "68785eb6001546c83aa4f960",
          "user": {
            "_id": "647b5fef6a79fbf5e996c47c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647b5fef6a79fbf5e996c47c/IkSMnDsCY_CyEFCiMDuxe.jpeg",
            "isPro": false,
            "fullname": "Bingyi Kang",
            "user": "bykang",
            "type": "user"
          },
          "name": "Bingyi Kang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-07-17T09:40:39.614Z",
          "hidden": false
        },
        {
          "_id": "68785eb6001546c83aa4f961",
          "name": "Xing Zhu",
          "hidden": false
        },
        {
          "_id": "68785eb6001546c83aa4f962",
          "name": "Hujun Bao",
          "hidden": false
        },
        {
          "_id": "68785eb6001546c83aa4f963",
          "name": "Yujun Shen",
          "hidden": false
        },
        {
          "_id": "68785eb6001546c83aa4f964",
          "name": "Xiaowei Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-16T17:59:03.000Z",
      "submittedOnDailyAt": "2025-07-17T06:55:04.439Z",
      "title": "Spectral Tracker V2 : Logiciel pour accéder facilement à la traceur de points 3D",
      "submittedOnDailyBy": {
        "_id": "6688a8f30bf195d6e53ac28d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6688a8f30bf195d6e53ac28d/5izbVmdCWWwA1wBjcxZPB.jpeg",
        "isPro": true,
        "fullname": "Yuxi Xiao",
        "user": "Yuxihenry",
        "type": "user"
      },
      "summary": "Introduis le Space Tracker V2. C'est un méthode de suivi de points 3D pour caméras avec défaut. Elle a été développée comme une amélioration des méthodes de suivi 3D basées sur la modélisation de composants complets, en intégrant une connexion intrinsèque entre le suivi de points, la profondeur des caméras avec défaut et l'estimation de la posture des caméras, ce qui résulte en un suivi de points 3D de haute efficacité. Le mouvement dans l'espace 3D est décomposé en scènes dynamiques, mouvements automatiques de la caméra et mouvements d'objets en pixels, adoptant une architecture end-to-end entièrement différenciable. Cela permet un entraînement scalable à travers une large collection de données qui inclut des séquences de vidéo RGB-D avec posture, des fichiers de pré-land avec étiquettes absentes et d'autres. En apprenant la structure et le mouvement en ensemble à partir de diverses sources de données, le Space Tracker V2 dépasse d'efficience au moins de 30% des méthodes de suivi 3D existantes, dépasse la précision des méthodes avancées de reconstruction 3D dynamique et fonctionne 50 fois plus rapidement.",
      "upvotes": 2,
      "discussionId": "68785eb7001546c83aa4f965",
      "projectPage": "https://spatialtracker.github.io",
      "githubRepo": "https://github.com/henry123-boy/SpaTrackerV2",
      "ai_summary": "SpatialTrackerV2 is a feed-forward 3D point tracking method for monocular videos that integrates point tracking, monocular depth, and camera pose estimation into a unified, end-to-end architecture, achieving high performance and speed.",
      "ai_keywords": [
        "feed-forward",
        "3D point tracking",
        "monocular videos",
        "intrinsic connections",
        "monocular depth",
        "camera pose estimation",
        "fully differentiable",
        "end-to-end architecture",
        "scene geometry",
        "camera ego-motion",
        "pixel-wise object motion",
        "synthetic sequences",
        "posed RGB-D videos",
        "unlabeled in-the-wild footage",
        "dynamic 3D reconstruction"
      ],
      "githubStars": 467
    },
    "publishedAt": "2025-07-16T13:59:03.000Z",
    "title": "SpatialTrackerV2: 3D Point Tracking Made Easy",
    "summary": "We present SpatialTrackerV2, a feed-forward 3D point tracking method for\nmonocular videos. Going beyond modular pipelines built on off-the-shelf\ncomponents for 3D tracking, our approach unifies the intrinsic connections\nbetween point tracking, monocular depth, and camera pose estimation into a\nhigh-performing and feedforward 3D point tracker. It decomposes world-space 3D\nmotion into scene geometry, camera ego-motion, and pixel-wise object motion,\nwith a fully differentiable and end-to-end architecture, allowing scalable\ntraining across a wide range of datasets, including synthetic sequences, posed\nRGB-D videos, and unlabeled in-the-wild footage. By learning geometry and\nmotion jointly from such heterogeneous data, SpatialTrackerV2 outperforms\nexisting 3D tracking methods by 30%, and matches the accuracy of leading\ndynamic 3D reconstruction approaches while running 50times faster.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.12462.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6688a8f30bf195d6e53ac28d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6688a8f30bf195d6e53ac28d/5izbVmdCWWwA1wBjcxZPB.jpeg",
      "fullname": "Yuxi Xiao",
      "name": "Yuxihenry",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.09025",
      "authors": [
        {
          "_id": "68786e45001546c83aa4f9a0",
          "name": "Chien Van Nguyen",
          "hidden": false
        },
        {
          "_id": "68786e45001546c83aa4f9a1",
          "name": "Ruiyi Zhang",
          "hidden": false
        },
        {
          "_id": "68786e45001546c83aa4f9a2",
          "user": {
            "_id": "652767bfbdcf00b9b9ac9a74",
            "avatarUrl": "/avatars/2cc8e9167562f364f0c25410f13a9d62.svg",
            "isPro": false,
            "fullname": "Hanieh Deilamsalehy",
            "user": "haniehds",
            "type": "user"
          },
          "name": "Hanieh Deilamsalehy",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-07-17T09:36:52.637Z",
          "hidden": false
        },
        {
          "_id": "68786e45001546c83aa4f9a3",
          "name": "Puneet Mathur",
          "hidden": false
        },
        {
          "_id": "68786e45001546c83aa4f9a4",
          "name": "Viet Dac Lai",
          "hidden": false
        },
        {
          "_id": "68786e45001546c83aa4f9a5",
          "name": "Haoliang Wang",
          "hidden": false
        },
        {
          "_id": "68786e45001546c83aa4f9a6",
          "user": {
            "_id": "66d32a678819c81cce2052f4",
            "avatarUrl": "/avatars/3a5b40ef9e9e73512743756d1c24ab6c.svg",
            "isPro": false,
            "fullname": "Jayakumar Subramanian",
            "user": "jasubram",
            "type": "user"
          },
          "name": "Jayakumar Subramanian",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-07-17T09:37:03.196Z",
          "hidden": false
        },
        {
          "_id": "68786e45001546c83aa4f9a7",
          "name": "Ryan A. Rossi",
          "hidden": false
        },
        {
          "_id": "68786e45001546c83aa4f9a8",
          "user": {
            "_id": "67f1035155fe17a33dc71f23",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/ohlRrzUI8VwvyYYxFXJuY.png",
            "isPro": false,
            "fullname": "Trung Bui",
            "user": "TrungBui1111",
            "type": "user"
          },
          "name": "Trung Bui",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-07-17T09:37:13.349Z",
          "hidden": false
        },
        {
          "_id": "68786e45001546c83aa4f9a9",
          "user": {
            "_id": "675346f0ab1d47a36ca60b89",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/mqWUUxAuFn3DsgIqoF_ah.png",
            "isPro": false,
            "fullname": "Nikos Vlassis",
            "user": "Nikosapa",
            "type": "user"
          },
          "name": "Nikos Vlassis",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-07-17T09:36:17.107Z",
          "hidden": false
        },
        {
          "_id": "68786e45001546c83aa4f9aa",
          "user": {
            "_id": "62c5947524171688a9feb992",
            "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
            "isPro": false,
            "fullname": "Franck Dernoncourt",
            "user": "Franck-Dernoncourt",
            "type": "user"
          },
          "name": "Franck Dernoncourt",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-17T08:14:23.916Z",
          "hidden": false
        },
        {
          "_id": "68786e45001546c83aa4f9ab",
          "user": {
            "_id": "64804fad8c6a3b8f11f73912",
            "avatarUrl": "/avatars/61e37a91d4bba35fda9bf52aadd87745.svg",
            "isPro": false,
            "fullname": "Thien Huu Nguyen",
            "user": "anoperson",
            "type": "user"
          },
          "name": "Thien Huu Nguyen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-07-17T09:36:11.546Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-11T21:19:18.000Z",
      "submittedOnDailyAt": "2025-07-17T02:00:25.332Z",
      "title": "Lizard : Marco de linearización eficiente pour modèles de langage de grande échelle",
      "submittedOnDailyBy": {
        "_id": "62c5947524171688a9feb992",
        "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
        "isPro": false,
        "fullname": "Franck Dernoncourt",
        "user": "Franck-Dernoncourt",
        "type": "user"
      },
      "summary": "Lite est un cadre de travail basé sur le Transformer qui transforme grands modèles de langage naturel (LLMs) en structures linéaires pour la génération de contextes infinis. Les modèles de LLMs basés sur le Transformer souvent rencontrent des problèmes de mémoire et de calcul lorsque la longueur du contexte augmente, en raison de la complexité quadratique de l'attention softmax et de l'augmentation du cache des clés et des valeurs (KV). Lite approxime l'attention softmax tout en maintenant la qualité des résultats, introduisant une structure d'attention carrée pour surmonter ces limitations. A différence de d'autres méthodes de linéarisation, qui étaient limitées par la structure du modèle, Lite inclut des modules de gestion basés sur des modèles linéaires récents, permettant un contrôle adaptatif de la mémoire, une inférence constante, une forte généralisation et une plus grande flexibilité dans le design du modèle. Lite combine l'attention globale linéaire avec l'attention glissante basée sur la métamémoire pour former une structure hybride qui capture à la fois des relations à longue distance et des interactions locales détaillées. De plus, il introduit des algorithmes de hardware pour accélérer l'entraînement du modèle. Les expériences élargies montrent que Lite récupère la performance des modèles entraînés sans perte dans des tâches de modélisation de langage standard et dépasse considérablement d'autres méthodes de linéarisation. Dans le benchmark MMLU de 5 exemples, Lite améliore de plus de 18 points par rapport aux modèles leaders et montre également des améliorations significatives dans des tâches de mémoire associative.",
      "upvotes": 2,
      "discussionId": "68786e45001546c83aa4f9ac",
      "ai_summary": "Lizard is a linearization framework that transforms Transformer-based LLMs into subquadratic architectures for efficient infinite-context generation, using a hybrid attention mechanism and hardware-aware training.",
      "ai_keywords": [
        "Transformer-based LLMs",
        "subquadratic architectures",
        "softmax attention",
        "key-value (KV) cache",
        "subquadratic attention mechanism",
        "gating module",
        "adaptive memory control",
        "constant-memory inference",
        "length generalization",
        "gated linear attention",
        "sliding window attention",
        "meta memory",
        "hardware-aware algorithm",
        "MMLU benchmark",
        "associative recall tasks"
      ]
    },
    "publishedAt": "2025-07-11T17:19:18.000Z",
    "title": "Lizard: An Efficient Linearization Framework for Large Language Models",
    "summary": "We propose Lizard, a linearization framework that transforms pretrained\nTransformer-based Large Language Models (LLMs) into flexible, subquadratic\narchitectures for infinite-context generation. Transformer-based LLMs face\nsignificant memory and computational bottlenecks as context lengths increase,\ndue to the quadratic complexity of softmax attention and the growing key-value\n(KV) cache. Lizard addresses these limitations by introducing a subquadratic\nattention mechanism that closely approximates softmax attention while\npreserving the output quality. Unlike previous linearization methods, which are\noften limited by fixed model structures and therefore exclude gating\nmechanisms, Lizard incorporates a gating module inspired by recent\nstate-of-the-art linear models. This enables adaptive memory control, supports\nconstant-memory inference, offers strong length generalization, and allows more\nflexible model design. Lizard combines gated linear attention for global\ncontext compression with sliding window attention enhanced by meta memory,\nforming a hybrid mechanism that captures both long-range dependencies and\nfine-grained local interactions. Moreover, we introduce a hardware-aware\nalgorithm that accelerates the training speed of our models. Extensive\nexperiments show that Lizard achieves near-lossless recovery of the teacher\nmodel's performance across standard language modeling tasks, while\nsignificantly outperforming previous linearization methods. On the 5-shot MMLU\nbenchmark, Lizard improves over prior models by 18 points and shows significant\nimprovements on associative recall tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.09025.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62c5947524171688a9feb992",
      "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
      "fullname": "Franck Dernoncourt",
      "name": "Franck-Dernoncourt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.07451",
      "authors": [
        {
          "_id": "68747204257d4f04353702de",
          "user": {
            "_id": "6474b290d815855e4ef59b05",
            "avatarUrl": "/avatars/decf3cfe8a5b2203f0520cdb3d26ee40.svg",
            "isPro": false,
            "fullname": "Hongzhi Zhang",
            "user": "hongzhizhang",
            "type": "user"
          },
          "name": "Hongzhi Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-17T08:21:31.114Z",
          "hidden": false
        },
        {
          "_id": "68747204257d4f04353702df",
          "name": "Jia Fu",
          "hidden": false
        },
        {
          "_id": "68747204257d4f04353702e0",
          "name": "Jingyuan Zhang",
          "hidden": false
        },
        {
          "_id": "68747204257d4f04353702e1",
          "name": "Kai Fu",
          "hidden": false
        },
        {
          "_id": "68747204257d4f04353702e2",
          "name": "Qi Wang",
          "hidden": false
        },
        {
          "_id": "68747204257d4f04353702e3",
          "user": {
            "_id": "67c5945da1661d5fa6f29adb",
            "avatarUrl": "/avatars/62561f3875c0c251cae949acc38d72dc.svg",
            "isPro": false,
            "fullname": "Fuzheng Zhang",
            "user": "Edrex",
            "type": "user"
          },
          "name": "Fuzheng Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-07-17T09:35:13.823Z",
          "hidden": false
        },
        {
          "_id": "68747204257d4f04353702e4",
          "user": {
            "_id": "67c6c570cf87e2d2ebfc81aa",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67c6c570cf87e2d2ebfc81aa/7qAstZtIT86Uwrz3u_anv.jpeg",
            "isPro": false,
            "fullname": "Guorui Zhou",
            "user": "GuoruiZhou",
            "type": "user"
          },
          "name": "Guorui Zhou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-07-17T09:34:55.007Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-10T05:58:55.000Z",
      "submittedOnDailyAt": "2025-07-17T07:36:19.087Z",
      "title": "RLEP : Logique d'apprentissage par renforcement utilisant la réutilisation d'expériences pour les LLM",
      "submittedOnDailyBy": {
        "_id": "6474b290d815855e4ef59b05",
        "avatarUrl": "/avatars/decf3cfe8a5b2203f0520cdb3d26ee40.svg",
        "isPro": false,
        "fullname": "Hongzhi Zhang",
        "user": "hongzhizhang",
        "type": "user"
      },
      "summary": "Appliquer l'apprentissage par renforcement (RL) à des modèles de langage de grande taille peut être une perte énergique importante : l'entraînement peut être instable et les politiques peuvent s'effondrer progressivement depuis les poids initiaux. Nous présentons Reinforcement Learning with Experience Replay (RLEP), un approche basée sur la réplication d'expériences. RLEP est un cadre à deux étapes : d'abord, des cartes testées sont collectées, puis elles sont répliquées lors de l'entraînement ultérieur. Chaque fois qu'un update se produit, la politique est optimisée à l'aide d'une mélange de nouveaux rollouts et de ces succès répliqués dans des mini-batches. En favorisant la création de haute qualité, RLEP évite l'exploration inutile dans le modèle, se concentre sur des chemins possibles et offre une convergence rapide et un rendement solide. Avec un modèle basé sur Qwen2.5-Math-7B, RLEP peut atteindre la précision maximale du standard et le réaliser sans de grandes mises à jour, améliorant la précision de AIME-2024 de 38.2% à 39.9%, de AIME-2025 de 19.8% à 22.3% et de AMC-2023 de 77.0% à 82.2%. Notre code, ensembles de données et points de vérification sont disponibles sur https://github.com/Kwai-Klear/RLEP, avec l'objectif de favoriser la reproductibilité et le développement de la recherche.",
      "upvotes": 1,
      "discussionId": "68747204257d4f04353702e5",
      "ai_summary": "RLEP, a reinforcement learning framework with experience replay, enhances large language model training by focusing on high-quality examples, leading to faster convergence and improved performance on math-related benchmarks.",
      "ai_keywords": [
        "reinforcement learning",
        "experience replay",
        "trajectories",
        "rollouts",
        "policy optimization",
        "convergence",
        "performance",
        "Qwen2.5-Math-7B",
        "AIME-2024",
        "AIME-2025",
        "AMC-2023"
      ]
    },
    "publishedAt": "2025-07-10T01:58:55.000Z",
    "title": "RLEP: Reinforcement Learning with Experience Replay for LLM Reasoning",
    "summary": "Reinforcement learning (RL) for large language models is an energy-intensive\nendeavor: training can be unstable, and the policy may gradually drift away\nfrom its pretrained weights. We present RLEP\\, -- \\,Reinforcement\nLearning with Experience rePlay\\, -- \\,a two-phase framework that first\ncollects verified trajectories and then replays them during subsequent\ntraining. At every update step, the policy is optimized on mini-batches that\nblend newly generated rollouts with these replayed successes. By replaying\nhigh-quality examples, RLEP steers the model away from fruitless exploration,\nfocuses learning on promising reasoning paths, and delivers both faster\nconvergence and stronger final performance. On the Qwen2.5-Math-7B base model,\nRLEP reaches baseline peak accuracy with substantially fewer updates and\nultimately surpasses it, improving accuracy on AIME-2024 from 38.2% to 39.9%,\non AIME-2025 from 19.8% to 22.3%, and on AMC-2023 from 77.0% to 82.2%. Our\ncode, datasets, and checkpoints are publicly available at\nhttps://github.com/Kwai-Klear/RLEP to facilitate reproducibility and further\nresearch.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.07451.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6474b290d815855e4ef59b05",
      "avatarUrl": "/avatars/decf3cfe8a5b2203f0520cdb3d26ee40.svg",
      "fullname": "Hongzhi Zhang",
      "name": "hongzhizhang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.05065",
      "authors": [
        {
          "_id": "68776e57ff8f47a7f86442bd",
          "user": {
            "_id": "669e707ec517d804cfce91c5",
            "avatarUrl": "/avatars/fc18e64e100cbf28763db04b44242747.svg",
            "isPro": false,
            "fullname": "Corrado Rainone",
            "user": "crainone",
            "type": "user"
          },
          "name": "Corrado Rainone",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-16T15:05:55.722Z",
          "hidden": false
        },
        {
          "_id": "68776e57ff8f47a7f86442be",
          "name": "Tim Bakker",
          "hidden": false
        },
        {
          "_id": "68776e57ff8f47a7f86442bf",
          "name": "Roland Memisevic",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-07T14:49:18.000Z",
      "submittedOnDailyAt": "2025-07-17T06:25:52.334Z",
      "title": "Une des méthodes pour rendre la théorie de la logique possible dans des modèles de langage de petit taille est de remplacer le pensée par l'utilisation de outils.",
      "submittedOnDailyBy": {
        "_id": "669e707ec517d804cfce91c5",
        "avatarUrl": "/avatars/fc18e64e100cbf28763db04b44242747.svg",
        "isPro": false,
        "fullname": "Corrado Rainone",
        "user": "crainone",
        "type": "user"
      },
      "summary": "Le dernier développement a établi un nouveau paradigme d'apprentissage automatique basé sur l'augmentation du calcul pour l'inférence et l'apprentissage. Dans le cadre de cette recherche, on combine le Supervised Fine-Tuning (SFT) pour des exemples synthétiques et le Reinforcement Learning avec des récompenses provable (RLVR) pour entraîner des modèles de langage à grande échelle, avec l'objectif de richifier la façon de \"penser\" exprimée en nature dans l'inférence, en augmentant ainsi le calcul nécessaire. Dans cet article, on propose la configuration d'interaction en tours entre tokens et outils de maintenance de l'état. À chaque tour, le nouvel état de l'outil est ajouté au contexte du modèle, et l'objectif du travail du modèle est de créer des tokens basés sur un langage de domaine spécifique (DSL) pour contrôler l'outil. Ce méthode est évaluée avec des problèmes de modification de code en Python, montrant que, dans un environnement limité, on peut collecter rapidement des expériences, obtenir des signaux de récompense denses et, en outre, entraîner un modèle de 3B paramètres.",
      "upvotes": 1,
      "discussionId": "68776e58ff8f47a7f86442c0",
      "ai_summary": "A new approach formats tokens as a multi-turn interaction trace with a stateful tool for training Large Language Models, enabling faster sampling and denser reward signals for tasks like repairing Python code.",
      "ai_keywords": [
        "Supervised Fine-Tuning",
        "Reinforcement Learning with Verifiable Rewards",
        "Large Language Models",
        "multi-turn interaction trace",
        "stateful tool",
        "custom DSL"
      ]
    },
    "publishedAt": "2025-07-07T10:49:18.000Z",
    "title": "Replacing thinking with tool usage enables reasoning in small language\n  models",
    "summary": "Recent advances have established a new machine learning paradigm based on\nscaling up compute at inference time as well as at training time. In that line\nof work, a combination of Supervised Fine-Tuning (SFT) on synthetic\ndemonstrations and Reinforcement Learning with Verifiable Rewards (RLVR) is\nused for training Large Language Models to expend extra compute during\ninference in the form of \"thoughts\" expressed in natural language. In this\npaper, we propose to instead format these tokens as a multi-turn interaction\ntrace with a stateful tool. At each turn, the new state of the tool is appended\nto the context of the model, whose job is to generate the tokens necessary to\ncontrol the tool via a custom DSL. We benchmark this approach on the problem of\nrepairing malfunctioning Python code, and show that this constrained setup\nallows for faster sampling of experience and a denser reward signal, allowing\neven models of size up to 3B parameters to learn how to proficiently expend\nadditional compute on the task.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.05065.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "669e707ec517d804cfce91c5",
      "avatarUrl": "/avatars/fc18e64e100cbf28763db04b44242747.svg",
      "fullname": "Corrado Rainone",
      "name": "crainone",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]