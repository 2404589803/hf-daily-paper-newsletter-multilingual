[
  {
    "paper": {
      "id": "2503.07920",
      "authors": [
        {
          "_id": "67d0f9c95f0fcc0c38902b8e",
          "name": "Samuel Cahyawijaya",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902b8f",
          "name": "Holy Lovenia",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902b90",
          "name": "Joel Ruben Antony Moniz",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902b91",
          "name": "Tack Hwa Wong",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902b92",
          "name": "Mohammad Rifqi Farhansyah",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902b93",
          "name": "Thant Thiri Maung",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902b94",
          "name": "Frederikus Hudi",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902b95",
          "name": "David Anugraha",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902b96",
          "user": {
            "_id": "63ddfced5ea8577c8d5fb421",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1677144169806-63ddfced5ea8577c8d5fb421.jpeg",
            "isPro": false,
            "fullname": "Muhammad Ravi Shulthan Habibi",
            "user": "muhammadravi251001",
            "type": "user"
          },
          "name": "Muhammad Ravi Shulthan Habibi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:38:20.672Z",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902b97",
          "name": "Muhammad Reza Qorib",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902b98",
          "name": "Amit Agarwal",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902b99",
          "name": "Joseph Marvin Imperial",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902b9a",
          "name": "Hitesh Laxmichand Patel",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902b9b",
          "user": {
            "_id": "67d1039a3e0dca11407f9460",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/tN5K_Gc8oAlw0ADYuyc1s.png",
            "isPro": false,
            "fullname": "Vicky Feliren",
            "user": "feliren",
            "type": "user"
          },
          "name": "Vicky Feliren",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:38:30.804Z",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902b9c",
          "name": "Bahrul Ilmi Nasution",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902b9d",
          "user": {
            "_id": "67559e52860bd4d8f4e9beeb",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/I_SNcfwTifgHtL9NFLLli.jpeg",
            "isPro": false,
            "fullname": "Manuel Antonio Rufino",
            "user": "antonrufino",
            "type": "user"
          },
          "name": "Manuel Antonio Rufino",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:38:33.476Z",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902b9e",
          "name": "Genta Indra Winata",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902b9f",
          "name": "Rian Adam Rajagede",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902ba0",
          "name": "Carlos Rafael Catalan",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902ba1",
          "name": "Mohamed Fazli Imam",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902ba2",
          "name": "Priyaranjan Pattnayak",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902ba3",
          "name": "Salsabila Zahirah Pranida",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902ba4",
          "name": "Kevin Pratama",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902ba5",
          "name": "Yeshil Bangera",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902ba6",
          "user": {
            "_id": "66d03a984505b8d635183aaa",
            "avatarUrl": "/avatars/0eab10dfad243d9dc19318b0f88de496.svg",
            "isPro": false,
            "fullname": "Adisai Na-Thalang",
            "user": "ensmart72",
            "type": "user"
          },
          "name": "Adisai Na-Thalang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:50:17.122Z",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902ba7",
          "name": "Patricia Nicole Monderin",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902ba8",
          "name": "Yueqi Song",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902ba9",
          "name": "Christian Simon",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902baa",
          "name": "Lynnette Hui Xian Ng",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bab",
          "name": "Richardy Lobo' Sapan",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bac",
          "name": "Taki Hasan Rafi",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bad",
          "name": "Bin Wang",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bae",
          "name": "Supryadi",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902baf",
          "name": "Kanyakorn Veerakanjana",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bb0",
          "name": "Piyalitt Ittichaiwong",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bb1",
          "name": "Matthew Theodore Roque",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bb2",
          "name": "Karissa Vincentio",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bb3",
          "name": "Takdanai Kreangphet",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bb4",
          "user": {
            "_id": "631a4855300a072a8da70abd",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/631a4855300a072a8da70abd/jRnzdW5JBjICYKCmkUFI-.jpeg",
            "isPro": false,
            "fullname": "phakphum artkaew",
            "user": "pakphum",
            "type": "user"
          },
          "name": "Phakphum Artkaew",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:38:41.811Z",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bb5",
          "name": "Kadek Hendrawan Palgunadi",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bb6",
          "name": "Yanzhi Yu",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bb7",
          "name": "Rochana Prih Hastuti",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bb8",
          "name": "William Nixon",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bb9",
          "name": "Mithil Bangera",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bba",
          "name": "Adrian Xuan Wei Lim",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bbb",
          "user": {
            "_id": "64f2e3b87244601d8f4365cf",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f2e3b87244601d8f4365cf/QHKB8DOMBoKSXgMo6nY6z.jpeg",
            "isPro": false,
            "fullname": "Aye Hninn Khine",
            "user": "ayehninnkhine",
            "type": "user"
          },
          "name": "Aye Hninn Khine",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:38:27.900Z",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bbc",
          "name": "Hanif Muhammad Zhafran",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bbd",
          "name": "Teddy Ferdinan",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bbe",
          "name": "Audra Aurora Izzani",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bbf",
          "name": "Ayushman Singh",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bc0",
          "name": "Evan",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bc1",
          "name": "Jauza Akbar Krito",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bc2",
          "name": "Michael Anugraha",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bc3",
          "name": "Fenal Ashokbhai Ilasariya",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bc4",
          "name": "Haochen Li",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bc5",
          "name": "John Amadeo Daniswara",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bc6",
          "name": "Filbert Aurelian Tjiaranata",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bc7",
          "name": "Eryawan Presma Yulianrifat",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bc8",
          "name": "Can Udomcharoenchaikit",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bc9",
          "name": "Fadil Risdian Ansori",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bca",
          "name": "Mahardika Krisna Ihsani",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bcb",
          "name": "Giang Nguyen",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bcc",
          "name": "Anab Maulana Barik",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bcd",
          "name": "Dan John Velasco",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bce",
          "name": "Rifo Ahmad Genadi",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bcf",
          "name": "Saptarshi Saha",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bd0",
          "user": {
            "_id": "66a31819b839c8994e5c3815",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66a31819b839c8994e5c3815/ARVZtfxJYyGvZ0zHyaaBP.png",
            "isPro": false,
            "fullname": "Chengwei Wei",
            "user": "amao0o0",
            "type": "user"
          },
          "name": "Chengwei Wei",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:38:39.101Z",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bd1",
          "name": "Isaiah Flores",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bd2",
          "name": "Kenneth Ko Han Chen",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bd3",
          "name": "Anjela Gail Santos",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bd4",
          "name": "Wan Shen Lim",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bd5",
          "name": "Kaung Si Phyo",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bd6",
          "name": "Tim Santos",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bd7",
          "name": "Meisyarah Dwiastuti",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bd8",
          "name": "Jiayun Luo",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bd9",
          "name": "Jan Christian Blaise Cruz",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bda",
          "name": "Ming Shan Hee",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bdb",
          "name": "Ikhlasul Akmal Hanif",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bdc",
          "name": "M. Alif Al Hakim",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bdd",
          "name": "Muhammad Rizky Sya'ban",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bde",
          "name": "Kun Kerdthaisong",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bdf",
          "name": "Lester James V. Miranda",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902be0",
          "name": "Fajri Koto",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902be1",
          "name": "Tirana Noor Fatyanosa",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902be2",
          "name": "Alham Fikri Aji",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902be3",
          "name": "Jostin Jerico Rosal",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902be4",
          "name": "Jun Kevin",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902be5",
          "user": {
            "_id": "640ead243830fd441c2e9838",
            "avatarUrl": "/avatars/4083942ce6b432a4cfb3524f72bcffb0.svg",
            "isPro": false,
            "fullname": "Robert Wijaya",
            "user": "wijayarobert",
            "type": "user"
          },
          "name": "Robert Wijaya",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:38:17.433Z",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902be6",
          "name": "Onno P. Kampman",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902be7",
          "name": "Ruochen Zhang",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902be8",
          "user": {
            "_id": "61e52be53d6dbb1da842316a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61e52be53d6dbb1da842316a/gx0WGPcOCClXPymoKglc4.jpeg",
            "isPro": false,
            "fullname": "Börje Karlsson",
            "user": "tellarin",
            "type": "user"
          },
          "name": "Börje F. Karlsson",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:38:36.440Z",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902be9",
          "name": "Peerat Limkonchotiwat",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T23:54:52.000Z",
      "title": "Cloud source, Croil, or generative? SEA-VL, un ensemble de données de langage visuel pour plusieurs caractéristiques de la région sud-est asiatique.",
      "summary": "Sure, here is the translation of the provided text into French:\n\n\"L'Asie du Sud-Est (ASE) est caractérisée par la diversité des langues et des cultures, mais il ne constitue pas un endroit représentatif pour l'étude des langues visuelles (LV). Cela est dû au fait que les modèles d'IA ne peuvent pas comprendre les nuances subtiles de la culture ASE, ce qui entraîne des erreurs. Pour corriger cela, on propose le projet open-source ASE-LV. Ce projet vise à développer des données de haute qualité liées aux langues de l'ASE. ASE-LV garantit la contribution des participants des pays de l'ASE, assurant la pertinence culturelle et la diversité, et promeut l'expansion des langues représentatives dans l'étude des LV. Il est plus coût-efficace que les solutions de nuage, atteignant environ le 85% de la pertinence culturelle, mais il est également noté que les modèles génératifs d'images ne peuvent pas refléter précisément la culture ASE. Les images générées ne peuvent pas refléter le contexte des traditions et des cultures locales. Dans ASE-LV, l'objectif est de recueillir 1,28 millions d'images liées à la culture ASE, avec un taille plus grande de cinq fois que d'autres ensembles de données. ASE-LV vise à combler les lacunes représentatives de l'ASE et à encourager le développement d'un système d'IA qui représente de manière plus large et diversifiée des cultures.\"",
      "upvotes": 44,
      "discussionId": "67d0f9cd5f0fcc0c38902cdf",
      "ai_keywords": [
        "vision-language (VL) research",
        "cultural relevance",
        "crowdsourcing",
        "image crawling",
        "image generation",
        "generative vision models",
        "synthesized images",
        "datasets"
      ]
    },
    "publishedAt": "2025-03-10T19:54:52.000Z",
    "title": "Crowdsource, Crawl, or Generate? Creating SEA-VL, a Multicultural\n  Vision-Language Dataset for Southeast Asia",
    "summary": "Southeast Asia (SEA) is a region of extraordinary linguistic and cultural\ndiversity, yet it remains significantly underrepresented in vision-language\n(VL) research. This often results in artificial intelligence (AI) models that\nfail to capture SEA cultural nuances. To fill this gap, we present SEA-VL, an\nopen-source initiative dedicated to developing high-quality, culturally\nrelevant data for SEA languages. By involving contributors from SEA countries,\nSEA-VL aims to ensure better cultural relevance and diversity, fostering\ngreater inclusivity of underrepresented languages in VL research. Beyond\ncrowdsourcing, our initiative goes one step further in the exploration of the\nautomatic collection of culturally relevant images through crawling and image\ngeneration. First, we find that image crawling achieves approximately ~85%\ncultural relevance while being more cost- and time-efficient than\ncrowdsourcing. Second, despite the substantial progress in generative vision\nmodels, synthetic images remain unreliable in accurately reflecting SEA\ncultures. The generated images often fail to reflect the nuanced traditions and\ncultural contexts of the region. Collectively, we gather 1.28M SEA\nculturally-relevant images, more than 50 times larger than other existing\ndatasets. Through SEA-VL, we aim to bridge the representation gap in SEA,\nfostering the development of more inclusive AI systems that authentically\nrepresent diverse cultures across SEA.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07920.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.07536",
      "authors": [
        {
          "_id": "67d04f248f79213c2fc0ba04",
          "name": "Yingzhe Peng",
          "hidden": false
        },
        {
          "_id": "67d04f248f79213c2fc0ba05",
          "name": "Gongrui Zhang",
          "hidden": false
        },
        {
          "_id": "67d04f248f79213c2fc0ba06",
          "name": "Miaosen Zhang",
          "hidden": false
        },
        {
          "_id": "67d04f248f79213c2fc0ba07",
          "name": "Zhiyuan You",
          "hidden": false
        },
        {
          "_id": "67d04f248f79213c2fc0ba08",
          "name": "Jie Liu",
          "hidden": false
        },
        {
          "_id": "67d04f248f79213c2fc0ba09",
          "name": "Qipeng Zhu",
          "hidden": false
        },
        {
          "_id": "67d04f248f79213c2fc0ba0a",
          "name": "Kai Yang",
          "hidden": false
        },
        {
          "_id": "67d04f248f79213c2fc0ba0b",
          "name": "Xingzhong Xu",
          "hidden": false
        },
        {
          "_id": "67d04f248f79213c2fc0ba0c",
          "name": "Xin Geng",
          "hidden": false
        },
        {
          "_id": "67d04f248f79213c2fc0ba0d",
          "name": "Xu Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T17:04:14.000Z",
      "title": "LMM-R1 : Un modèle de 30 millions de mots se renforce avec des capacités de logique fortes basées sur un apprentissage par renforcement à deux étapes.",
      "summary": "Pour renforcer la théorie des modèles de mémoire multimodales (LMMs), il est nécessaire de résoudre les problèmes inhérents à l'interaction complexe entre le reconnaissance visuelle et la théorie rationnelle, surtout dans des architectures à 300 millions de paramètres qui limitent la capacité théorique du modèle.\n\nL'apprentissage par renforcement basé sur les règles (RL) montre des résultats exceptionnels dans des domaines contextuels, mais rencontre deux grands obstacles lorsqu'il est appliqué aux modèles de mémoire multimodales : 1. La limitation des données qui empêche d'obtenir des réponses incertaines ou des exemples complexes de théorie rationnelle. 2. La dégradation de la théorie rationnelle due à l'apprentissage précédent du modèle.\n\nPour résoudre ces problèmes, nous proposons un cadre de travail à deux étapes appelé \\method. Ce cadre améliore la théorie rationnelle par l'apprentissage par renforcement (FRE) et généralise l'apprentissage de la mémoire multimodale (MGT). Dans l'étape de FRE, la capacité théorique est améliorée à l'aide de données contextuelles, et dans l'étape de MGT, cette capacité est généralisée aux domaines de la mémoire multimodale.\n\nLes expériences sur Qwen2.5-VL-Instruct-3B ont montré que \\method a réalisé un accroissement moyen de 4,83% sur les benchmarks de la mémoire multimodale et de 4,5% sur les benchmarks cérébraux, et un effet de 3,63% sur des tâches de jeu de football complexes. Ces résultats prouvent que l'amélioration de la théorie rationnelle basée sur la mémoire est efficace pour la généralisation de la mémoire multimodale et offre un approche efficace des données qui évite le haut coût d'obtenir des données de haute qualité.",
      "upvotes": 41,
      "discussionId": "67d04f268f79213c2fc0ba8b",
      "projectPage": "https://forjadeforest.github.io/LMM-R1-ProjectPage",
      "githubRepo": "https://github.com/TideDra/lmm-r1",
      "ai_keywords": [
        "Large Multimodal Models (LMMs)",
        "visual perception",
        "logical reasoning",
        "3B-parameter architectures",
        "rule-based reinforcement learning (RL)",
        "multimodal extension",
        "ambiguous answers",
        "complex reasoning examples",
        "degraded foundational reasoning",
        "multimodal pretraining",
        "Foundational Reasoning Enhancement (FRE)",
        "Multimodal Generalization Training (MGT)",
        "Qwen2.5-VL-Instruct-3B",
        "multimodal benchmarks",
        "text-only benchmarks",
        "complex Football Game tasks",
        "text-based reasoning enhancement",
        "data-efficient paradigm"
      ]
    },
    "publishedAt": "2025-03-10T13:04:14.000Z",
    "title": "LMM-R1: Empowering 3B LMMs with Strong Reasoning Abilities Through\n  Two-Stage Rule-Based RL",
    "summary": "Enhancing reasoning in Large Multimodal Models (LMMs) faces unique challenges\nfrom the complex interplay between visual perception and logical reasoning,\nparticularly in compact 3B-parameter architectures where architectural\nconstraints limit reasoning capacity and modality alignment.\n  While rule-based reinforcement learning (RL) excels in text-only domains, its\nmultimodal extension confronts two critical barriers: (1) data limitations due\nto ambiguous answers and scarce complex reasoning examples, and (2) degraded\nfoundational reasoning induced by multimodal pretraining.\n  To address these challenges, we propose \\method, a two-stage\nframework adapting rule-based RL for multimodal reasoning through\nFoundational Reasoning Enhancement (FRE) followed by\nMultimodal Generalization Training (MGT). The FRE stage first\nstrengthens reasoning abilities using text-only data with rule-based RL, then\nthe MGT stage generalizes these reasoning capabilities to multimodal domains.\n  Experiments on Qwen2.5-VL-Instruct-3B demonstrate that \\method achieves\n4.83\\% and 4.5\\% average improvements over baselines in multimodal and\ntext-only benchmarks, respectively, with a 3.63\\% gain in complex Football Game\ntasks. These results validate that text-based reasoning enhancement enables\neffective multimodal generalization, offering a data-efficient paradigm that\nbypasses costly high-quality multimodal training data.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07536.png",
    "numComments": 2,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.08638",
      "authors": [
        {
          "_id": "67d1027435066eade61549ae",
          "user": {
            "_id": "5fd6f670053c8345eddc1b68",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5fd6f670053c8345eddc1b68/cuTsu2krRYHC6zYGD2dpQ.jpeg",
            "isPro": false,
            "fullname": "Ruibin Yuan",
            "user": "a43992899",
            "type": "user"
          },
          "name": "Ruibin Yuan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:36:33.054Z",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549af",
          "name": "Hanfeng Lin",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549b0",
          "name": "Shuyue Guo",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549b1",
          "user": {
            "_id": "638efcf4c67af472d316d424",
            "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
            "isPro": false,
            "fullname": "Ge Zhang",
            "user": "zhangysk",
            "type": "user"
          },
          "name": "Ge Zhang",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-03-12T06:24:13.961Z",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549b2",
          "name": "Jiahao Pan",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549b3",
          "name": "Yongyi Zang",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549b4",
          "name": "Haohe Liu",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549b5",
          "name": "Yiming Liang",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549b6",
          "name": "Wenye Ma",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549b7",
          "user": {
            "_id": "654907a4a1faff97850c4eff",
            "avatarUrl": "/avatars/458c90151614bc7f116943b6e67d6b8a.svg",
            "isPro": false,
            "fullname": "du",
            "user": "dododododo",
            "type": "user"
          },
          "name": "Xingjian Du",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:36:36.330Z",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549b8",
          "name": "Xinrun Du",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549b9",
          "name": "Zhen Ye",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549ba",
          "name": "Tianyu Zheng",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549bb",
          "name": "Yinghao Ma",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549bc",
          "user": {
            "_id": "6417d9ea8f689506e7148417",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6417d9ea8f689506e7148417/bAYcruWNw4WvmuQcGgcwC.jpeg",
            "isPro": false,
            "fullname": "minghao",
            "user": "Liam-Liu",
            "type": "user"
          },
          "name": "Minghao Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:36:39.193Z",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549bd",
          "name": "Zeyue Tian",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549be",
          "name": "Ziya Zhou",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549bf",
          "name": "Liumeng Xue",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549c0",
          "user": {
            "_id": "6628adb14277eae0da5eee28",
            "avatarUrl": "/avatars/6cb41b80cc5e014e455dfc2a22682e64.svg",
            "isPro": true,
            "fullname": "HKUST Audio",
            "user": "HKUST-Audio",
            "type": "user"
          },
          "name": "Xingwei Qu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-12T03:41:43.139Z",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549c1",
          "name": "Yizhi Li",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549c2",
          "name": "Shangda Wu",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549c3",
          "name": "Tianhao Shen",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549c4",
          "name": "Ziyang Ma",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549c5",
          "name": "Jun Zhan",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549c6",
          "name": "Chunhui Wang",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549c7",
          "name": "Yatian Wang",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549c8",
          "name": "Xiaowei Chi",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549c9",
          "name": "Xinyue Zhang",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549ca",
          "name": "Zhenzhu Yang",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549cb",
          "name": "Xiangzhou Wang",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549cc",
          "name": "Shansong Liu",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549cd",
          "name": "Lingrui Mei",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549ce",
          "name": "Peng Li",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549cf",
          "name": "Junjie Wang",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549d0",
          "name": "Jianwei Yu",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549d1",
          "name": "Guojian Pang",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549d2",
          "name": "Xu Li",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549d3",
          "name": "Zihao Wang",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549d4",
          "name": "Xiaohuan Zhou",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549d5",
          "name": "Lijun Yu",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549d6",
          "name": "Emmanouil Benetos",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549d7",
          "name": "Yong Chen",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549d8",
          "name": "Chenghua Lin",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549d9",
          "name": "Xie Chen",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549da",
          "name": "Gus Xia",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549db",
          "name": "Zhaoxiang Zhang",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549dc",
          "name": "Chao Zhang",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549dd",
          "name": "Wenhu Chen",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549de",
          "name": "Xinyu Zhou",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549df",
          "name": "Xipeng Qiu",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549e0",
          "name": "Roger Dannenberg",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549e1",
          "name": "Jiaheng Liu",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549e2",
          "name": "Jian Yang",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549e3",
          "name": "Wenhao Huang",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549e4",
          "name": "Wei Xue",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549e5",
          "name": "Xu Tan",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549e6",
          "name": "Yike Guo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-11T17:26:50.000Z",
      "title": "Scaling Open-Ended Foundation Models for Music Generation",
      "summary": "Nous présentons \"YuE\", une famille de modèles basés sur l'architecture de la base ouverte LLaMA2, conçus pour aborder les défis spécifiques de la génération musicale de longues phrases comme des chansons. Spécifiquement, YuE dépasse les trois milliards de tokens et peut générer de la musique qui occupe 5% d'un disque, tout en maintenant la cohérence de la parole, la structure musicale cohérente et une mélodie bocaralla émotionnelle. Ceci est réalisé grâce à : (1) la prédiction du token suivant pour les séparations de trajectoire, (2) l'amélioration de la cohérence de la parole dans des contextes longs grâce à des conditions avancées, et (3) l'apprentissage préalable multi-tâche et multi-étape. De plus, YuE réinvente les techniques de génération musicale et permet des transformations de style bidirectionnelles (par exemple, convertir de la musique pop de la ville japonaise en rap en anglais tout en maintenant le support original). Selon des évaluations détaillées, YuE dépasse certains systèmes propres dans la flexibilité musicale et la bocaralla. De plus, lors de l'ajustement fin de YuE, on peut obtenir des contrôles supplémentaires et du support pour des langues supplémentaires. De plus, les résultats de génération montrent que les représentations apprises de YuE démontrent également des résultats excellents dans des tâches de compréhension musicale, dépassant les meilleurs méthodes dans le cadre de référence MARBLE. Mots clés : parole à chanson, génération de musique, longues phrases, modèle de base, génération musicale.",
      "upvotes": 39,
      "discussionId": "67d1027735066eade6154a7e",
      "ai_keywords": [
        "track-decoupled next-token prediction",
        "dense mixture signals",
        "structural progressive conditioning",
        "long-context lyrical alignment",
        "multitask, multiphase pre-training",
        "in-context learning",
        "versatile style transfer",
        "bidirectional generation",
        "musicality",
        "vocal agility",
        "tail languages",
        "music understanding tasks",
        "MARBLE benchmark"
      ]
    },
    "publishedAt": "2025-03-11T13:26:50.000Z",
    "title": "YuE: Scaling Open Foundation Models for Long-Form Music Generation",
    "summary": "We tackle the task of long-form music generation--particularly the\nchallenging lyrics-to-song problem--by introducing YuE, a family of\nopen foundation models based on the LLaMA2 architecture. Specifically, YuE\nscales to trillions of tokens and generates up to five minutes of music while\nmaintaining lyrical alignment, coherent musical structure, and engaging vocal\nmelodies with appropriate accompaniment. It achieves this through (1)\ntrack-decoupled next-token prediction to overcome dense mixture signals, (2)\nstructural progressive conditioning for long-context lyrical alignment, and (3)\na multitask, multiphase pre-training recipe to converge and generalize. In\naddition, we redesign the in-context learning technique for music generation,\nenabling versatile style transfer (e.g., converting Japanese city pop into an\nEnglish rap while preserving the original accompaniment) and bidirectional\ngeneration. Through extensive evaluation, we demonstrate that YuE matches or\neven surpasses some of the proprietary systems in musicality and vocal agility.\nIn addition, fine-tuning YuE enables additional controls and enhanced support\nfor tail languages. Furthermore, beyond generation, we show that YuE's learned\nrepresentations can perform well on music understanding tasks, where the\nresults of YuE match or exceed state-of-the-art methods on the MARBLE\nbenchmark. Keywords: lyrics2song, song generation, long-form, foundation model,\nmusic generation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.08638.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.08120",
      "authors": [
        {
          "_id": "67d0f5ace3c8042929eea946",
          "user": {
            "_id": "64c860d23a3f428da65ea499",
            "avatarUrl": "/avatars/f0bcc6ae7e558babe691b6bbf1059c9d.svg",
            "isPro": false,
            "fullname": "lijunzhe",
            "user": "tulvgengenr",
            "type": "user"
          },
          "name": "Junzhe Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:39:26.197Z",
          "hidden": false
        },
        {
          "_id": "67d0f5ace3c8042929eea947",
          "name": "Xuerui Qiu",
          "hidden": false
        },
        {
          "_id": "67d0f5ace3c8042929eea948",
          "name": "Linrui Xu",
          "hidden": false
        },
        {
          "_id": "67d0f5ace3c8042929eea949",
          "name": "Liya Guo",
          "hidden": false
        },
        {
          "_id": "67d0f5ace3c8042929eea94a",
          "user": {
            "_id": "64daecec888b7e9c400f59b5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64daecec888b7e9c400f59b5/f4pfOfWk6jYJX-Nf2-qHn.png",
            "isPro": false,
            "fullname": "Delin Qu",
            "user": "delinqu",
            "type": "user"
          },
          "name": "Delin Qu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:39:29.349Z",
          "hidden": false
        },
        {
          "_id": "67d0f5ace3c8042929eea94b",
          "name": "Tingting Long",
          "hidden": false
        },
        {
          "_id": "67d0f5ace3c8042929eea94c",
          "name": "Chun Fan",
          "hidden": false
        },
        {
          "_id": "67d0f5ace3c8042929eea94d",
          "name": "Ming Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-11T07:34:59.000Z",
      "title": "UniF^2ace : Modèle unifié monomodal pour la compréhension et la génération de visages en nuage",
      "summary": "Les modèles multimodal unifiés (UMMs) jouent un rôle crucial comme paradigme fondamental dans la recherche en vision par ordinateur, démontrant une puissance significative dans la compréhension et la génération d'images. Cependant, les recherches antérieures dans le domaine des caractéristiques faciales ont concentré leur approche sur la compréhension de caractéristiques faciales basiques, limitant ainsi leur capacité à traiter des caractéristiques faciales subtiles, ce qui a été considéré comme un problème dans leur capacité à générer des informations. Pour surmonter ces limites, nous proposons UniF^2ace, le premier UMM spécialisé dans la compréhension et la génération de caractéristiques faciales subtiles. En général, nous utilisons deux méthodes de division qui offrent deux avantages mutuellement et une architecture d'explorateur de micros en deux étapes pour entraîner UniF^2ace avec un ensemble de données spécialement construit. En particulier, nous avons construit le premier ensemble de données faciales à grande échelle, UniF^2ace-130K, qui comprend 130K paires d'images-texte, et cet ensemble a été élargi avec 1 million de paires de questions-réponses pour étendre diverses caractéristiques faciales. De plus, nous avons établi la connexion théorique entre la méthode de division de point et le modèle de génération avec masque, optimisant les deux limites inférieures pour améliorer significativement la capacité du modèle à synthétiser des informations détaillées des visages. Enfin, nous avons introduit des exploreurs de micros au niveau de token et de séquence, ce qui permet l'apprentissage de représentations subtilesment efficaces pour les deux tâches de compréhension et de génération. Les expérimentations étendues sur UniF^2ace-130K montrent que UniF^2ace dépasse les UMMs et les modèles de génération existants, en atteignant un excellent rendement dans les deux tâches de compréhension et de génération.",
      "upvotes": 23,
      "discussionId": "67d0f5b4e3c8042929eeab49",
      "ai_keywords": [
        "diffusion score matching",
        "masked generative models",
        "evidence lower bounds",
        "mixture-of-experts",
        "token-level",
        "sequence-level"
      ]
    },
    "publishedAt": "2025-03-11T03:34:59.000Z",
    "title": "UniF^2ace: Fine-grained Face Understanding and Generation\n  with Unified Multimodal Models",
    "summary": "Unified multimodal models (UMMs) have emerged as a powerful paradigm in\nfoundational computer vision research, demonstrating significant potential in\nboth image understanding and generation. However, existing research in the face\ndomain primarily focuses on coarse facial attribute understanding,\nwith limited capacity to handle fine-grained facial attributes and\nwithout addressing generation capabilities. To overcome these limitations, we\npropose UniF^2ace, the first UMM tailored specifically for\nfine-grained face understanding and generation. In general, we train\nUniF^2ace on a self-constructed, specialized dataset utilizing two\nmutually beneficial diffusion techniques and a two-level mixture-of-experts\narchitecture. Specifically, we first build a large-scale facial dataset,\nUniF^2ace-130K, which contains 130K image-text pairs with one\nmillion question-answering pairs that span a wide range of facial attributes.\nSecond, we establish a theoretical connection between discrete diffusion score\nmatching and masked generative models, optimizing both evidence lower bounds\nsimultaneously, which significantly improves the model's ability to synthesize\nfacial details. Finally, we introduce both token-level and sequence-level\nmixture-of-experts, enabling efficient fine-grained representation learning for\nboth understanding and generation tasks. Extensive experiments on\nUniF^2ace-130K demonstrate that UniF^2ace outperforms\nexisting UMMs and generative models, achieving superior performance across both\nunderstanding and generation tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.08120.png",
    "numComments": 2,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.07703",
      "authors": [
        {
          "_id": "67d0f422a3158b8e55d3562f",
          "name": "Lixue Gong",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35630",
          "name": "Xiaoxia Hou",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35631",
          "name": "Fanshi Li",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35632",
          "name": "Liang Li",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35633",
          "name": "Xiaochen Lian",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35634",
          "name": "Fei Liu",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35635",
          "name": "Liyang Liu",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35636",
          "name": "Wei Liu",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35637",
          "name": "Wei Lu",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35638",
          "name": "Yichun Shi",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35639",
          "name": "Shiqi Sun",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d3563a",
          "name": "Yu Tian",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d3563b",
          "name": "Zhi Tian",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d3563c",
          "name": "Peng Wang",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d3563d",
          "name": "Xun Wang",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d3563e",
          "name": "Ye Wang",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d3563f",
          "name": "Guofeng Wu",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35640",
          "user": {
            "_id": "6381c5d63680a7cf34e08ca9",
            "avatarUrl": "/avatars/731467e2d80d0ae163c4a00a9e3ff9e5.svg",
            "isPro": false,
            "fullname": "wujie10558@gmail.com",
            "user": "wujie10",
            "type": "user"
          },
          "name": "Jie Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:40:44.088Z",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35641",
          "name": "Xin Xia",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35642",
          "name": "Xuefeng Xiao",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35643",
          "name": "Linjie Yang",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35644",
          "name": "Zhonghua Zhai",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35645",
          "name": "Xinyu Zhang",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35646",
          "name": "Qi Zhang",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35647",
          "name": "Yuwei Zhang",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35648",
          "name": "Shijia Zhao",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35649",
          "name": "Jianchao Yang",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d3564a",
          "name": "Weilin Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T17:58:33.000Z",
      "title": "Seedream 2.0 : Seedream 2.0 : Modèle de génération d'images entre langues chinoises et anglaises",
      "summary": "L'avancement des modèles de différenciation rapide stimule un progrès impressionnant dans le domaine de la génération d'images. Cependant, les modèles généraux comme Flux, SD3.5 et Midjourney rencontrent des problèmes tels que le biais du modèle, des capacités de formation textuelle limitées et une compréhension insuffisante de la nuance culturelle chinoise. Pour résoudre ces limites, nous présentons Seedream 2.0, un modèle de génération d'images bilingue sur plusieurs dimensions qui excelle en maternelle et anglais. Ce modèle supporte la génération d'images bilingues et l'entraînement textuel dans les deux langues, permettant une gestion efficace des prompts. Nous avons développé un système de fusion de connaissances puissant et un système de capture qui équilibre la précision et la richesse de l'explication des images. Seedream intègre un modèle de langue bilingue autonome comme encodeur de texte, ce qui lui permet d'apprendre directement le savoir maternel du langage à partir des données massives. De cette manière, il peut générer avec une haute précision la nuance et l'expression artistique de la culture en chinois ou en anglais. De plus, Glyph-Aligned ByT5 soutient un entraînement flexible au niveau de mot et Scaled ROPE offre une bonne extensibilité pour des résolutions non entraînées. La forte édition du modèle final, qui comprend des optimisations postérieures multi-étapes et des itérations de SFT et RLHF, a permis d'atteindre un rendement leader sur plusieurs aspects, tels que la conformité aux prompts, la beauté, la précision textuelle et l'exactitude structurale. A travers de nombreux expériments, Seedream 2.0 a démontré sa capacité d'atteindre un rendement leader sur plusieurs aspects, y compris la conformité aux prompts, la beauté, la précision textuelle et l'exactitude structurale, ce qui la rend un modèle de haut rendement sur plusieurs aspects.",
      "upvotes": 21,
      "discussionId": "67d0f42fa3158b8e55d358ea",
      "projectPage": "https://team.doubao.com/zh/tech/seedream",
      "ai_keywords": [
        "diffusion models",
        "Flux",
        "SD3.5",
        "Midjourney",
        "model bias",
        "Seedream 2.0",
        "bilingual image generation",
        "text prompt",
        "data system",
        "caption system",
        "bilingual large language model",
        "high-fidelity images",
        "cultural nuances",
        "aesthetic expressions",
        "Glyph-Aligned ByT5",
        "character-level text rendering",
        "Scaled ROPE",
        "multi-phase post-training optimizations",
        "SFT",
        "RLHF",
        "prompt-following",
        "structural correctness",
        "ELO score",
        "instruction-based image editing model",
        "SeedEdit"
      ]
    },
    "publishedAt": "2025-03-10T13:58:33.000Z",
    "title": "Seedream 2.0: A Native Chinese-English Bilingual Image Generation\n  Foundation Model",
    "summary": "Rapid advancement of diffusion models has catalyzed remarkable progress in\nthe field of image generation. However, prevalent models such as Flux, SD3.5\nand Midjourney, still grapple with issues like model bias, limited text\nrendering capabilities, and insufficient understanding of Chinese cultural\nnuances. To address these limitations, we present Seedream 2.0, a native\nChinese-English bilingual image generation foundation model that excels across\ndiverse dimensions, which adeptly manages text prompt in both Chinese and\nEnglish, supporting bilingual image generation and text rendering. We develop a\npowerful data system that facilitates knowledge integration, and a caption\nsystem that balances the accuracy and richness for image description.\nParticularly, Seedream is integrated with a self-developed bilingual large\nlanguage model as a text encoder, allowing it to learn native knowledge\ndirectly from massive data. This enable it to generate high-fidelity images\nwith accurate cultural nuances and aesthetic expressions described in either\nChinese or English. Beside, Glyph-Aligned ByT5 is applied for flexible\ncharacter-level text rendering, while a Scaled ROPE generalizes well to\nuntrained resolutions. Multi-phase post-training optimizations, including SFT\nand RLHF iterations, further improve the overall capability. Through extensive\nexperimentation, we demonstrate that Seedream 2.0 achieves state-of-the-art\nperformance across multiple aspects, including prompt-following, aesthetics,\ntext rendering, and structural correctness. Furthermore, Seedream 2.0 has been\noptimized through multiple RLHF iterations to closely align its output with\nhuman preferences, as revealed by its outstanding ELO score. In addition, it\ncan be readily adapted to an instruction-based image editing model, such as\nSeedEdit, with strong editing capability that balances instruction-following\nand image consistency.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07703.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.05978",
      "authors": [
        {
          "_id": "67d129d732b4bbfb938321a1",
          "name": "Hongwei Yi",
          "hidden": false
        },
        {
          "_id": "67d129d732b4bbfb938321a2",
          "user": {
            "_id": "66015e8aa4d296af07de538e",
            "avatarUrl": "/avatars/a1295c631cc2646282c545859975ce4c.svg",
            "isPro": false,
            "fullname": "Ye",
            "user": "Owen777",
            "type": "user"
          },
          "name": "Tian Ye",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:50:12.319Z",
          "hidden": false
        },
        {
          "_id": "67d129d732b4bbfb938321a3",
          "name": "Shitong Shao",
          "hidden": false
        },
        {
          "_id": "67d129d732b4bbfb938321a4",
          "name": "Xuancheng Yang",
          "hidden": false
        },
        {
          "_id": "67d129d732b4bbfb938321a5",
          "name": "Jiantong Zhao",
          "hidden": false
        },
        {
          "_id": "67d129d732b4bbfb938321a6",
          "name": "Hanzhong Guo",
          "hidden": false
        },
        {
          "_id": "67d129d732b4bbfb938321a7",
          "name": "Terrance Wang",
          "hidden": false
        },
        {
          "_id": "67d129d732b4bbfb938321a8",
          "name": "Qingyu Yin",
          "hidden": false
        },
        {
          "_id": "67d129d732b4bbfb938321a9",
          "name": "Zeke Xie",
          "hidden": false
        },
        {
          "_id": "67d129d732b4bbfb938321aa",
          "name": "Lei Zhu",
          "hidden": false
        },
        {
          "_id": "67d129d732b4bbfb938321ab",
          "name": "Wei Li",
          "hidden": false
        },
        {
          "_id": "67d129d732b4bbfb938321ac",
          "name": "Michael Lingelbach",
          "hidden": false
        },
        {
          "_id": "67d129d732b4bbfb938321ad",
          "name": "Daquan Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-07T23:21:11.000Z",
      "title": "Crée des images d'infinités histoires avec des mots et de la voix.",
      "summary": "MagicInfinity utilise le cadre de travail Diffusion Transformer (DiT) pour surmonter les limitations des animations de personnages précédentes et offre des résultats de haute qualité pour différents types de personnages. Cela inclut des personnages humains réalistes, des personnages de pièce, et des personnages animés stylisés. Cette fonction supporte des variations faciales, des perspectives panoramiques et permet une spécification précise des déterminants de l'animation de plusieurs personnages en utilisant une masque d'entrée. Notre approche résout les principales problèmes grâce à trois innovations implémentations : 1. La fonction d'attention 3D et le désaccord de fenêtres de fenêtres de glissement pour générer des vidéos de longue durée, en maintenant la coopération séquentielle et la qualité visuelle de différents styles de personnages. 2. Introduction du apprentissage de classification à deux étapes pour permettre un contrôle multimodal à long terme, en utilisant la synchronisation de la voix avec les lèvres, des actions représentatives de phrases et la préservation de l'identité à travers des images de référence. 3. Utilisation de masques par zones et de fonctions de perte adaptatives pour équilibrer le contrôle global de la phrase et le guide vocal local, et soutenir l'animation de certains éléments. L'optimisation est réalisée grâce à notre technique innovante de passage intégré et distillation de cfg, atteignant une vitesse d'inférence 20 fois plus rapide que le modèle de base. Avec 8 GPU H100, on peut générer des vidéos de 540x540p en 10 secondes et des vidéos de 720x720p en 30 secondes, sans perte de qualité. Nouvelles évaluations de benchmark montrent des résultats excellents en synchronisation de la voix et des lèvres, préservation de l'identité et de mouvements naturels, et sont disponibles pour l'utilisation publique. https://www.hedra.com/, exemple : https://magicinfinite.github.io/",
      "upvotes": 19,
      "discussionId": "67d129e332b4bbfb938324a0",
      "projectPage": "https://magicinfinite.github.io/",
      "ai_keywords": [
        "diffusion Transformer (DiT)",
        "3D full-attention mechanisms",
        "sliding window denoising strategy",
        "infinite video generation",
        "temporal coherence",
        "two-stage curriculum learning scheme",
        "audio for lip sync",
        "text for expressive dynamics",
        "reference images for identity preservation",
        "region-specific masks",
        "adaptive loss functions",
        "unified step and cfg distillation techniques",
        "inference speed",
        "audio-lip synchronization",
        "identity preservation",
        "motion naturalness"
      ]
    },
    "publishedAt": "2025-03-07T18:21:11.000Z",
    "title": "MagicInfinite: Generating Infinite Talking Videos with Your Words and\n  Voice",
    "summary": "We present MagicInfinite, a novel diffusion Transformer (DiT) framework that\novercomes traditional portrait animation limitations, delivering high-fidelity\nresults across diverse character types-realistic humans, full-body figures, and\nstylized anime characters. It supports varied facial poses, including\nback-facing views, and animates single or multiple characters with input masks\nfor precise speaker designation in multi-character scenes. Our approach tackles\nkey challenges with three innovations: (1) 3D full-attention mechanisms with a\nsliding window denoising strategy, enabling infinite video generation with\ntemporal coherence and visual quality across diverse character styles; (2) a\ntwo-stage curriculum learning scheme, integrating audio for lip sync, text for\nexpressive dynamics, and reference images for identity preservation, enabling\nflexible multi-modal control over long sequences; and (3) region-specific masks\nwith adaptive loss functions to balance global textual control and local audio\nguidance, supporting speaker-specific animations. Efficiency is enhanced via\nour innovative unified step and cfg distillation techniques, achieving a 20x\ninference speed boost over the basemodel: generating a 10 second 540x540p video\nin 10 seconds or 720x720p in 30 seconds on 8 H100 GPUs, without quality loss.\nEvaluations on our new benchmark demonstrate MagicInfinite's superiority in\naudio-lip synchronization, identity preservation, and motion naturalness across\ndiverse scenarios. It is publicly available at https://www.hedra.com/, with\nexamples at https://magicinfinite.github.io/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05978.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.08625",
      "authors": [
        {
          "_id": "67d0fd74f8595b656f921a48",
          "user": {
            "_id": "632179745fc60c44fd91fc33",
            "avatarUrl": "/avatars/37d4fefbcc19f091dccffefec9706de2.svg",
            "isPro": false,
            "fullname": "zhumuzhi",
            "user": "Z-MU-Z",
            "type": "user"
          },
          "name": "Muzhi Zhu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:36:42.495Z",
          "hidden": false
        },
        {
          "_id": "67d0fd74f8595b656f921a49",
          "name": "Yuzhuo Tian",
          "hidden": false
        },
        {
          "_id": "67d0fd74f8595b656f921a4a",
          "name": "Hao Chen",
          "hidden": false
        },
        {
          "_id": "67d0fd74f8595b656f921a4b",
          "name": "Chunluan Zhou",
          "hidden": false
        },
        {
          "_id": "67d0fd74f8595b656f921a4c",
          "name": "Qingpei Guo",
          "hidden": false
        },
        {
          "_id": "67d0fd74f8595b656f921a4d",
          "name": "Yang Liu",
          "hidden": false
        },
        {
          "_id": "67d0fd74f8595b656f921a4e",
          "name": "Ming Yang",
          "hidden": false
        },
        {
          "_id": "67d0fd74f8595b656f921a4f",
          "name": "Chunhua Shen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-11T17:08:54.000Z",
      "title": "SegAgent : Pour explorer l'compréhension des pixels dans le cadre des modèles de multiple modalité, on imite le chemin du Humain Annotateur.",
      "summary": "Les modèles de ligne de personnages multiples (MLLMs) montrent des capacités de compréhension d'images, mais sont limités dans leur application pratique en raison des difficultés de compréhension au niveau des pixels. Actuellement, les tâches d'évaluation telles que VQA et la géoréférenciation visuelle évaluent excessivement la compréhension au niveau des pixels, ce qui ne reflète pas la capacité réelle des MLLMs. La base de la compréhension au niveau des pixels est la segmentation, mais les méthodes actuelles nécessitent que les MLLMs génèrent des tokens invisibles interprétables par un décodificateur externe de pixels. Cette approche détruit l'espace de sortie textuel des MLLMs, peut perdre la capacité linguistique potentielle, réduit l'efficacité et l'extensibilité du modèle, et ne reflète pas sa propre compréhension au niveau des pixels.\n\nPourquoi, nous présentons la tâche d'analyse de masques humains (HLMAT). C'est un nouveau paradigme dans lequel les MLLMs se comportent comme des anonymisateurs humains, utilisant des outils d'analyse interactif pour effectuer des analyses. La segmentation est modélisée dans un processus de décision markovien multiniveau, et HLMAT modélise des changements structurels ou utilise des tokens invisibles, permettant aux MLLMs de générer des points de clic basés sur le texte pour créer des masques de haute qualité. Dans ce système, nous avons développé le SegAgent, un modèle finalement entraîné comme un anonymisateur humain. Ce modèle atteint des performances de la technologie la plus récente (SOTA) et soutient des tâches supplémentaires telles que l'optimisation des masques et le filtrage d'analyse.\n\nHLMAT fournit un protocole pour évaluer la compréhension au niveau des pixels des MLLMs, introduisant des tâches de décision multiniveau visuelles et aidant à explorer la capacité d'inférence visuelle des MLLMs. L'amélioration de la recherche d'arbres de notre politique de meilleurement StaR et le guide PRM améliorent la robustesse du modèle dans des tâches complexes de segmentation, et sont basés sur la précision du reconnaissance visuelle et l'avenir des systèmes de décision multiniveau des MLLMs.",
      "upvotes": 18,
      "discussionId": "67d0fd76f8595b656f921ae8",
      "projectPage": "https://aim-uofa.github.io/SegAgent/",
      "githubRepo": "https://github.com/aim-uofa/SegAgent",
      "ai_keywords": [
        "Human-Like Mask Annotation Task (HLMAT)",
        "Markov Decision Process",
        "multi-step decision-making",
        "click points",
        "masks",
        "policy improvement method StaR",
        "PRM-guided tree search",
        "mask refinement",
        "annotation filtering",
        "fine-grained pixel understanding",
        "vision-centric task",
        "visual reasoning abilities"
      ]
    },
    "publishedAt": "2025-03-11T13:08:54.000Z",
    "title": "SegAgent: Exploring Pixel Understanding Capabilities in MLLMs by\n  Imitating Human Annotator Trajectories",
    "summary": "While MLLMs have demonstrated adequate image understanding capabilities, they\nstill struggle with pixel-level comprehension, limiting their practical\napplications. Current evaluation tasks like VQA and visual grounding remain too\ncoarse to assess fine-grained pixel comprehension accurately. Though\nsegmentation is foundational for pixel-level understanding, existing methods\noften require MLLMs to generate implicit tokens, decoded through external pixel\ndecoders. This approach disrupts the MLLM's text output space, potentially\ncompromising language capabilities and reducing flexibility and extensibility,\nwhile failing to reflect the model's intrinsic pixel-level understanding.\n  Thus, we introduce the Human-Like Mask Annotation Task (HLMAT), a new\nparadigm where MLLMs mimic human annotators using interactive segmentation\ntools. Modeling segmentation as a multi-step Markov Decision Process, HLMAT\nenables MLLMs to iteratively generate text-based click points, achieving\nhigh-quality masks without architectural changes or implicit tokens. Through\nthis setup, we develop SegAgent, a model fine-tuned on human-like annotation\ntrajectories, which achieves performance comparable to state-of-the-art (SOTA)\nmethods and supports additional tasks like mask refinement and annotation\nfiltering.\n  HLMAT provides a protocol for assessing fine-grained pixel understanding in\nMLLMs and introduces a vision-centric, multi-step decision-making task that\nfacilitates exploration of MLLMs' visual reasoning abilities. Our adaptations\nof policy improvement method StaR and PRM-guided tree search further enhance\nmodel robustness in complex segmentation tasks, laying a foundation for future\nadvancements in fine-grained visual perception and multi-step decision-making\nfor MLLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.08625.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.07604",
      "authors": [
        {
          "_id": "67cfa4ecd8cb8688d7d6d8b5",
          "name": "Tianhe Lin",
          "hidden": false
        },
        {
          "_id": "67cfa4ecd8cb8688d7d6d8b6",
          "user": {
            "_id": "62d65139667051e0a29bffe7",
            "avatarUrl": "/avatars/0252aa2bcd4cf1c8e4b87e5f164b6da5.svg",
            "isPro": false,
            "fullname": "Jian Xie",
            "user": "hsaest",
            "type": "user"
          },
          "name": "Jian Xie",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:42:36.765Z",
          "hidden": false
        },
        {
          "_id": "67cfa4ecd8cb8688d7d6d8b7",
          "name": "Siyu Yuan",
          "hidden": false
        },
        {
          "_id": "67cfa4ecd8cb8688d7d6d8b8",
          "name": "Deqing Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T17:58:31.000Z",
      "title": "L'initialisation des entrées dans Transformer est par inférence par tri.",
      "summary": "Les calculs pendant le test ont été identifiés comme un nouveau paradigme qui améliore la capacité d'inférence multi-niveau complexe du modèle de langage. Cela a été démontré clairement avec le succès de OpenAI's o1, o3 et DeepSeek's R1. En comparaison avec l'inférence explicite pendant le test, l'inférence implicite est plus efficace et génère moins de tokens. Cependant, la raison pour laquelle la capacité d'inférence évolutive apparaît de manière implicite n'est pas claire. Dans cet article, des expériences analytiques sont réalisées pour explorer comment fonctionne l'inférence implicite dans des tâches multi-niveau, en entraînant le modèle GPT-2 avec un ensemble de données de mathématiques multi-niveau. Nos résultats sont les suivants : 1) Les modèles de langage peuvent atteindre des précisions élevées dans les tests à l'intérieur et à l'extérieur du domaine, en entraînant avec des données de modèles fixes et en effectuant l'inférence en étapes. Cependant, cette capacité n'apparaît que pour les données de modèles fixes. 2) D'autre part, la capacité d'inférence implicite entraînée avec des modèles instables s'adapte trop fortement à ces modèles et échoue dans les applications évolutives. Ce limite est également observé dans les modèles de langage les plus avancés à grande échelle. Ces résultats montrent que les modèles de langage peuvent obtenir une inférence implicite par apprentissage court, gagnant un excellent rendement dans des tâches similaires mais perdant des capacités de généralisation.",
      "upvotes": 14,
      "discussionId": "67cfa4edd8cb8688d7d6d908",
      "githubRepo": "https://github.com/TianheL/LM-Implicit-Reasoning",
      "ai_keywords": [
        "test-time compute",
        "multi-step reasoning",
        "OpenAI's o1",
        "OpenAI's o3",
        "DeepSeek's R1",
        "implicit reasoning",
        "inference-efficient",
        "generated tokens",
        "explicit reasoning",
        "GPT-2",
        "multi-step mathematical reasoning dataset",
        "step-by-step reasoning",
        "in-domain tests",
        "out-of-domain tests",
        "fixed-pattern data",
        "unfixed-pattern data",
        "overfit",
        "generalization",
        "shortcut learning"
      ]
    },
    "publishedAt": "2025-03-10T13:58:31.000Z",
    "title": "Implicit Reasoning in Transformers is Reasoning through Shortcuts",
    "summary": "Test-time compute is emerging as a new paradigm for enhancing language\nmodels' complex multi-step reasoning capabilities, as demonstrated by the\nsuccess of OpenAI's o1 and o3, as well as DeepSeek's R1. Compared to explicit\nreasoning in test-time compute, implicit reasoning is more inference-efficient,\nrequiring fewer generated tokens. However, why does the advanced reasoning\ncapability fail to emerge in the implicit reasoning style? In this work, we\ntrain GPT-2 from scratch on a curated multi-step mathematical reasoning dataset\nand conduct analytical experiments to investigate how language models perform\nimplicit reasoning in multi-step tasks. Our findings reveal: 1) Language models\ncan perform step-by-step reasoning and achieve high accuracy in both in-domain\nand out-of-domain tests via implicit reasoning. However, this capability only\nemerges when trained on fixed-pattern data. 2) Conversely, implicit reasoning\nabilities emerging from training on unfixed-pattern data tend to overfit a\nspecific pattern and fail to generalize further. Notably, this limitation is\nalso observed in state-of-the-art large language models. These findings suggest\nthat language models acquire implicit reasoning through shortcut learning,\nenabling strong performance on tasks with similar patterns while lacking\ngeneralization.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07604.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.08605",
      "authors": [
        {
          "_id": "67d0ed0877b0c8ac3f304ef1",
          "name": "Subin Kim",
          "hidden": false
        },
        {
          "_id": "67d0ed0877b0c8ac3f304ef2",
          "name": "Seoung Wug Oh",
          "hidden": false
        },
        {
          "_id": "67d0ed0877b0c8ac3f304ef3",
          "name": "Jui-Hsien Wang",
          "hidden": false
        },
        {
          "_id": "67d0ed0877b0c8ac3f304ef4",
          "name": "Joon-Young Lee",
          "hidden": false
        },
        {
          "_id": "67d0ed0877b0c8ac3f304ef5",
          "name": "Jinwoo Shin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-11T16:43:45.000Z",
      "title": "Génération de vidéos d'événement pour amortissement de copie synchrone avec ajustements libres",
      "summary": "Récemment, le développement de modèles d'expansion vidéo a permis la génération de vidéos de haute qualité à partir d'un seul prompt, mais la quantité limitée de données et les coûts élevés de calcul rendent la génération de vidéos longues une tâche difficile. En réponse à cette problématique, plusieurs études ont proposé un approche sans tuning, en étendant des modèles existants pour la génération de vidéos longues et en permettant des changements dynamiques de contenu via plusieurs prompts. Cependant, ces méthodes se concentrent principalement sur la garantie d'un mouvement lisse entre les frames adjacentes, ce qui entraîne une perte progressive de cohérence significative dans les séquences longues, ce qui constitue un problème majeur. Pour résoudre cette difficulté, nous proposons l'approche Synchronized Coupled Sampling (SynCoS). SynCoS synchronise le pas d'expansion de toute la vidéo et introduit un nouveau méthode d'inférence qui assure la cohérence à des distances considérables, non seulement entre les frames adjacentes. Notre approche combine deux stratégies de sampling : sampling inverse et sampling basé sur l'optimisation, garantissant un mouvement lisse entre les frames adjacentes et enforçant une cohérence globale. Cependant, l'échange direct de ces samples peut créer des défauts d'expansion asymétriques, détruire le prompt guide et générer des changements de contenu imprévisibles. Pour résoudre ce problème, SynCoS utilise des pas de temps basiques et des bruits fixes pour synchroniser le sampling, garantissant que les échantillons soient cohérents et que les défauts d'expansion se ajustent au prompt guide. Les expériences extensives montrent que SynCoS améliore significativement la génération de vidéos longues, atteint un mouvement lisse et une cohérence à des distances considérables, dépassant notablement les méthodes précédentes.",
      "upvotes": 13,
      "discussionId": "67d0ed0b77b0c8ac3f304f7c",
      "projectPage": "https://syncos2025.github.io/",
      "githubRepo": "https://github.com/subin-kim-cv/SynCoS",
      "ai_keywords": [
        "text-to-video diffusion models",
        "high-quality short video generation",
        "long video generation",
        "tuning-free approaches",
        "multiple prompts",
        "dynamic content changes",
        "smooth transitions",
        "content drift",
        "semantic coherence",
        "Synchronized Coupled Sampling (SynCoS)",
        "denoising paths",
        "reverse sampling",
        "optimization-based sampling",
        "seamless local transitions",
        "global coherence",
        "grounded timestep",
        "fixed baseline noise",
        "multi-event long video generation",
        "long-range consistency"
      ]
    },
    "publishedAt": "2025-03-11T12:43:45.000Z",
    "title": "Tuning-Free Multi-Event Long Video Generation via Synchronized Coupled\n  Sampling",
    "summary": "While recent advancements in text-to-video diffusion models enable\nhigh-quality short video generation from a single prompt, generating real-world\nlong videos in a single pass remains challenging due to limited data and high\ncomputational costs. To address this, several works propose tuning-free\napproaches, i.e., extending existing models for long video generation,\nspecifically using multiple prompts to allow for dynamic and controlled content\nchanges. However, these methods primarily focus on ensuring smooth transitions\nbetween adjacent frames, often leading to content drift and a gradual loss of\nsemantic coherence over longer sequences. To tackle such an issue, we propose\nSynchronized Coupled Sampling (SynCoS), a novel inference framework that\nsynchronizes denoising paths across the entire video, ensuring long-range\nconsistency across both adjacent and distant frames. Our approach combines two\ncomplementary sampling strategies: reverse and optimization-based sampling,\nwhich ensure seamless local transitions and enforce global coherence,\nrespectively. However, directly alternating between these samplings misaligns\ndenoising trajectories, disrupting prompt guidance and introducing unintended\ncontent changes as they operate independently. To resolve this, SynCoS\nsynchronizes them through a grounded timestep and a fixed baseline noise,\nensuring fully coupled sampling with aligned denoising paths. Extensive\nexperiments show that SynCoS significantly improves multi-event long video\ngeneration, achieving smoother transitions and superior long-range coherence,\noutperforming previous approaches both quantitatively and qualitatively.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.08605.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.07891",
      "authors": [
        {
          "_id": "67d108c56bd6c57bab0b6f07",
          "name": "Jinhyuk Lee",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f08",
          "name": "Feiyang Chen",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f09",
          "name": "Sahil Dua",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f0a",
          "name": "Daniel Cer",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f0b",
          "name": "Madhuri Shanbhogue",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f0c",
          "name": "Iftekhar Naim",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f0d",
          "name": "Gustavo Hernández Ábrego",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f0e",
          "name": "Zhe Li",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f0f",
          "name": "Kaifeng Chen",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f10",
          "name": "Henrique Schechter Vera",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f11",
          "name": "Xiaoqi Ren",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f12",
          "name": "Shanfeng Zhang",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f13",
          "name": "Daniel Salz",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f14",
          "name": "Michael Boratko",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f15",
          "name": "Jay Han",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f16",
          "name": "Blair Chen",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f17",
          "name": "Shuo Huang",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f18",
          "name": "Vikram Rao",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f19",
          "name": "Paul Suganthan",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f1a",
          "name": "Feng Han",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f1b",
          "name": "Andreas Doumanoglou",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f1c",
          "name": "Nithi Gupta",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f1d",
          "name": "Fedor Moiseev",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f1e",
          "name": "Cathy Yip",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f1f",
          "name": "Aashi Jain",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f20",
          "name": "Simon Baumgartner",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f21",
          "name": "Shahrokh Shahi",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f22",
          "name": "Frank Palma Gomez",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f23",
          "name": "Sandeep Mariserla",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f24",
          "name": "Min Choi",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f25",
          "name": "Parashar Shah",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f26",
          "name": "Sonam Goenka",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f27",
          "name": "Ke Chen",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f28",
          "name": "Ye Xia",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f29",
          "name": "Koert Chen",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f2a",
          "name": "Sai Meher Karthik Duddu",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f2b",
          "name": "Yichang Chen",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f2c",
          "name": "Trevor Walker",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f2d",
          "name": "Wenlei Zhou",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f2e",
          "name": "Rakesh Ghiya",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f2f",
          "name": "Zach Gleicher",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f30",
          "name": "Karan Gill",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f31",
          "name": "Zhe Dong",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f32",
          "name": "Mojtaba Seyedhosseini",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f33",
          "name": "Yunhsuan Sung",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f34",
          "name": "Raphael Hoffmann",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f35",
          "name": "Tom Duerig",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T22:16:45.000Z",
      "title": "Gemini Embedding : Embedding d'Extensibilité de Gemini",
      "summary": "Dans ce rapport, nous présentons la plus avancée et puissante outil de modélisation interne de Google, appelé \"Gemini Embedding\". Cette outil exploite les capacités uniques de compréhension multilingue et de compréhension de code de Gemini pour générer une représentation à haut niveau de généralisation dans des textes multilingues et de différents contextes. Les représentations générées par Gemini Embedding peuvent être appliquées à diverses tâches de traitement postérieur, comme la classification, la similitude, le clustering, l'ordonnancement et la recherche. Évaluée sur le benchmark de text embedding multilingue à grande échelle (MMTEB), ce modèle montre un grand avancement par rapport aux précédents, car le benchmark inclut plus de 100 tâches dans plus de 250 langues. Gemini Embedding a démontré être le meilleur dans les benchmarks multilingue, anglais et de code, et notre modèle d'intégration montre une force significative dans une large gamme de tâches, dépassant les modèles de domaine spécifique.",
      "upvotes": 12,
      "discussionId": "67d108c66bd6c57bab0b6f6e",
      "ai_keywords": [
        "Gemini Embedding",
        "large language model",
        "multilingual",
        "code understanding",
        "representations",
        "downstream tasks",
        "classification",
        "similarity",
        "clustering",
        "ranking",
        "retrieval",
        "Massive Multilingual Text Embedding Benchmark (MMTEB)",
        "embedding quality",
        "specialized domain-specific models"
      ]
    },
    "publishedAt": "2025-03-10T18:16:45.000Z",
    "title": "Gemini Embedding: Generalizable Embeddings from Gemini",
    "summary": "In this report, we introduce Gemini Embedding, a state-of-the-art embedding\nmodel leveraging the power of Gemini, Google's most capable large language\nmodel. Capitalizing on Gemini's inherent multilingual and code understanding\ncapabilities, Gemini Embedding produces highly generalizable embeddings for\ntext spanning numerous languages and textual modalities. The representations\ngenerated by Gemini Embedding can be precomputed and applied to a variety of\ndownstream tasks including classification, similarity, clustering, ranking, and\nretrieval. Evaluated on the Massive Multilingual Text Embedding Benchmark\n(MMTEB), which includes over one hundred tasks across 250+ languages, Gemini\nEmbedding substantially outperforms prior state-of-the-art models,\ndemonstrating considerable improvements in embedding quality. Achieving\nstate-of-the-art performance across MMTEB's multilingual, English, and code\nbenchmarks, our unified model demonstrates strong capabilities across a broad\nselection of tasks and surpasses specialized domain-specific models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07891.png",
    "numComments": 2,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.08619",
      "authors": [
        {
          "_id": "67d0eb9cec69694dca382208",
          "name": "Xianfeng Wu",
          "hidden": false
        },
        {
          "_id": "67d0eb9cec69694dca382209",
          "name": "Yajing Bai",
          "hidden": false
        },
        {
          "_id": "67d0eb9cec69694dca38220a",
          "name": "Haoze Zheng",
          "hidden": false
        },
        {
          "_id": "67d0eb9cec69694dca38220b",
          "name": "Harold Haodong Chen",
          "hidden": false
        },
        {
          "_id": "67d0eb9cec69694dca38220c",
          "name": "Yexin Liu",
          "hidden": false
        },
        {
          "_id": "67d0eb9cec69694dca38220d",
          "name": "Zihao Wang",
          "hidden": false
        },
        {
          "_id": "67d0eb9cec69694dca38220e",
          "name": "Xuran Ma",
          "hidden": false
        },
        {
          "_id": "67d0eb9cec69694dca38220f",
          "name": "Wen-Jie Shu",
          "hidden": false
        },
        {
          "_id": "67d0eb9cec69694dca382210",
          "name": "Xianzu Wu",
          "hidden": false
        },
        {
          "_id": "67d0eb9cec69694dca382211",
          "name": "Harry Yang",
          "hidden": false
        },
        {
          "_id": "67d0eb9cec69694dca382212",
          "name": "Ser-Nam Lim",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-11T16:58:02.000Z",
      "title": "LightGen : Transfert de connaissances et optimisation des préférences directes pour la génération efficace d'images",
      "summary": "Le développement de la technologie transformant le texte en images a été principalement poussé par la disponibilité de grands ensembles de données et l'architecture riche en paramètres. Ces nécessités imposent un strict limite d'accessibilité pour les chercheurs ou les professionnels avec des ressources informatiques limitées. Dans cet article, nous présentons LightGen, un modèle d'apprentissage efficace développé en utilisant le savoir de distillation (KD) et l'optimisation de préférences directes (DPO). LightGen adopte le succès de la technique de KD dans les modèles de langage grands de multiples types (MLLM) pour absorber le savoir du modèle de transformation texte en image de haut rendement (SOTA) dans une architecture simple et peu paramétrée, la Masked Autoregressive Model (MAR). Il utilise un ensemble de données de synthèse simple composé de 2M images de haute qualité, démontrant que la diversité des données est plus importante que sa quantité pour déterminer le rendement du modèle. Cette stratégie réduit significativement les besoins en ressources informatiques et peut limiter le temps d'apprentissage précédent à 88 jours GPU au lieu de milliers de jours GPU. De plus, on intègre la technique DPO pour améliorer la fidélité et la précision de la position des images, résolvant les défauts caractéristiques des données de synthèse. Les expériences détaillées montrent que LightGen, en réduisant significativement les ressources informatiques et en étendant l'accès dans des environnements avec des limitations de ressources, atteint une qualité de génération d'images équivalente aux modèles de SOTA. Le code est disponible sur https://github.com/XianfengWu01/LightGen.",
      "upvotes": 11,
      "discussionId": "67d0eba3ec69694dca3823a0",
      "ai_keywords": [
        "knowledge distillation (KD)",
        "Direct Preference Optimization (DPO)",
        "Multi-Modal Large Language Models (MLLMs)",
        "Masked Autoregressive (MAR)",
        "synthetic dataset",
        "data diversity",
        "data volume",
        "model performance",
        "computational demands",
        "pre-training time",
        "synthetic data",
        "high-frequency details",
        "spatial inaccuracies",
        "image fidelity",
        "positional accuracy"
      ]
    },
    "publishedAt": "2025-03-11T12:58:02.000Z",
    "title": "LightGen: Efficient Image Generation through Knowledge Distillation and\n  Direct Preference Optimization",
    "summary": "Recent advances in text-to-image generation have primarily relied on\nextensive datasets and parameter-heavy architectures. These requirements\nseverely limit accessibility for researchers and practitioners who lack\nsubstantial computational resources. In this paper, we introduce \\model, an\nefficient training paradigm for image generation models that uses knowledge\ndistillation (KD) and Direct Preference Optimization (DPO). Drawing inspiration\nfrom the success of data KD techniques widely adopted in Multi-Modal Large\nLanguage Models (MLLMs), LightGen distills knowledge from state-of-the-art\n(SOTA) text-to-image models into a compact Masked Autoregressive (MAR)\narchitecture with only 0.7B parameters. Using a compact synthetic dataset of\njust 2M high-quality images generated from varied captions, we demonstrate\nthat data diversity significantly outweighs data volume in determining model\nperformance. This strategy dramatically reduces computational demands and\nreduces pre-training time from potentially thousands of GPU-days to merely 88\nGPU-days. Furthermore, to address the inherent shortcomings of synthetic data,\nparticularly poor high-frequency details and spatial inaccuracies, we integrate\nthe DPO technique that refines image fidelity and positional accuracy.\nComprehensive experiments confirm that LightGen achieves image generation\nquality comparable to SOTA models while significantly reducing computational\nresources and expanding accessibility for resource-constrained environments.\nCode is available at https://github.com/XianfengWu01/LightGen",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.08619.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.08686",
      "authors": [
        {
          "_id": "67d0f892a189f3978638e154",
          "name": "Jialv Zou",
          "hidden": false
        },
        {
          "_id": "67d0f892a189f3978638e155",
          "name": "Bencheng Liao",
          "hidden": false
        },
        {
          "_id": "67d0f892a189f3978638e156",
          "name": "Qian Zhang",
          "hidden": false
        },
        {
          "_id": "67d0f892a189f3978638e157",
          "name": "Wenyu Liu",
          "hidden": false
        },
        {
          "_id": "67d0f892a189f3978638e158",
          "name": "Xinggang Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-11T17:59:46.000Z",
      "title": "OmniMamba : Modèle de Unités Efficaces basé sur des Modèles d'Espace-Temps pour la Compréhension et la Génération de Démonstrations",
      "summary": "Récemment, le développement de modèles de génération de compréhension et de visualisation monomodaux (ou génération monomodulaire) a été limité par la charge de complexité informatique excessive et la dépendance de grands ensembles de données d'entraînement. Nous présentons le modèle de génération monomodulaire \"OmniMamba\", basé sur une architecture linéaire. Ce modèle génère du texte et des images simultanément à travers un seul pas de prédiction du token suivant. OmniMamba maximise l'efficacité en termes de calcul et de mémoire de Mamba-2 pour étendre la génération monomodulaire à la génération de texte. Pour aborder les problèmes d'efficacité des données dans les modèles intégrés, nous proposons deux idées clés : (1) séparer la génération de modules guidés par des modèles non-Occasionnels et (2) appliquer LoRA de manière efficace sur les séquences de tâches. De plus, nous introduisons une stratégie d'apprentissage en deux étapes pour atténuer l'inégalité des données entre deux tâches. Avec ces technologies, OmniMamba a démontré un rendement compétitif par rapport à JanusFlow et a dépassé Show-o, entraîné avec seulement 2M paires d'images et de texte, ce qui représente 1/1000 de la quantité de données de Show-o. En particulier, OmniMamba présente une vitesse d'inférence 119,2 fois plus rapide et une réduction de 63% en mémoire GPU par rapport aux modèles compétitifs basés sur Transformer. Le code et le modèle sont disponibles sur https://github.com/hustvl/OmniMamba.",
      "upvotes": 9,
      "discussionId": "67d0f894a189f3978638e1b7",
      "ai_keywords": [
        "OmniMamba",
        "linear-architecture-based multimodal generation model",
        "unified next-token prediction paradigm",
        "Mamba-2",
        "computational efficiency",
        "memory efficiency",
        "decoupled vocabularies",
        "task-specific LoRA",
        "parameter-efficient adaptation",
        "decoupled two-stage training strategy",
        "data imbalance",
        "Show-o",
        "benchmark",
        "inference efficiency",
        "Transformer-based counterparts"
      ]
    },
    "publishedAt": "2025-03-11T13:59:46.000Z",
    "title": "OmniMamba: Efficient and Unified Multimodal Understanding and Generation\n  via State Space Models",
    "summary": "Recent advancements in unified multimodal understanding and visual generation\n(or multimodal generation) models have been hindered by their quadratic\ncomputational complexity and dependence on large-scale training data. We\npresent OmniMamba, the first linear-architecture-based multimodal generation\nmodel that generates both text and images through a unified next-token\nprediction paradigm. The model fully leverages Mamba-2's high computational and\nmemory efficiency, extending its capabilities from text generation to\nmultimodal generation. To address the data inefficiency of existing unified\nmodels, we propose two key innovations: (1) decoupled vocabularies to guide\nmodality-specific generation, and (2) task-specific LoRA for\nparameter-efficient adaptation. Furthermore, we introduce a decoupled two-stage\ntraining strategy to mitigate data imbalance between two tasks. Equipped with\nthese techniques, OmniMamba achieves competitive performance with JanusFlow\nwhile surpassing Show-o across benchmarks, despite being trained on merely 2M\nimage-text pairs, which is 1,000 times fewer than Show-o. Notably, OmniMamba\nstands out with outstanding inference efficiency, achieving up to a 119.2 times\nspeedup and 63% GPU memory reduction for long-sequence generation compared to\nTransformer-based counterparts. Code and models are released at\nhttps://github.com/hustvl/OmniMamba",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.08686.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.07860",
      "authors": [
        {
          "_id": "67d0e915d0038007e5a75178",
          "user": {
            "_id": "650871aeb44445e9b3625c7b",
            "avatarUrl": "/avatars/c35bd3e4a851389a4b6898a5a51e2219.svg",
            "isPro": false,
            "fullname": "James Burgess",
            "user": "jmhb",
            "type": "user"
          },
          "name": "James Burgess",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:41:30.547Z",
          "hidden": false
        },
        {
          "_id": "67d0e915d0038007e5a75179",
          "user": {
            "_id": "65703fab7f50602340d23704",
            "avatarUrl": "/avatars/324c45f5fba9cd8c38a89b30427c06b4.svg",
            "isPro": false,
            "fullname": "Xiaohan Wang",
            "user": "nicholswang",
            "type": "user"
          },
          "name": "Xiaohan Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:40:56.564Z",
          "hidden": false
        },
        {
          "_id": "67d0e915d0038007e5a7517a",
          "name": "Yuhui Zhang",
          "hidden": false
        },
        {
          "_id": "67d0e915d0038007e5a7517b",
          "name": "Anita Rau",
          "hidden": false
        },
        {
          "_id": "67d0e915d0038007e5a7517c",
          "name": "Alejandro Lozano",
          "hidden": false
        },
        {
          "_id": "67d0e915d0038007e5a7517d",
          "name": "Lisa Dunlap",
          "hidden": false
        },
        {
          "_id": "67d0e915d0038007e5a7517e",
          "name": "Trevor Darrell",
          "hidden": false
        },
        {
          "_id": "67d0e915d0038007e5a7517f",
          "name": "Serena Yeung-Levy",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T21:18:32.000Z",
      "title": "Vidéo Action Diffusion",
      "summary": "Que méthodes son utilisées pour vérifier les différences qui apparaissent dans un vidéo lorsque deux personnes réalisent la même action ? Dans cet article, on présente un nouveau défi appelé \"Video Action Differencing (VidDiff)\" pour reconnaître les petites différences dans des vidéos de la même action. Ce défi a des applications dans diverses domaines comme l'entraînement de joueurs ou la cuisine. Pour faciliter le développement de ce nouveau défi, des données VidDiffBench ont été créées. Ce jeu de données comprend 549 paires de vidéos, avec 4,469 différences spécifiques d'actions et 2,075 explications humaines des moments où ces différences se produisent. Nos expériences montrent que les modèles grands de multimodalité (LMMs) comme GPT-4o et Qwen2-VL sont significativement affectés par VidDiffBench. En analysant les cas d'échec des LMMs dans VidDiffBench, deux défis importants sont identifiés : la localisation et la comparaison des sous-actions liées à deux vidéos et la comparaison de détails dans les frames. Pour aborder ces défis, on propose le méthode VidDiff, qui est composée de trois étapes : la proposition des différences d'action, la localisation des frames clés et la comparaison de frames. Chaque étape utilise un modèle de base spécialisé. Pour promouvoir futures recherches dans ce nouveau défi, le benchmark est accessible à https://huggingface.co/datasets/jmhb/VidDiffBench et le code à http://jmhb0.github.io/viddiff.",
      "upvotes": 9,
      "discussionId": "67d0e917d0038007e5a751e9",
      "projectPage": "https://jmhb0.github.io/viddiff/",
      "githubRepo": "https://github.com/jmhb0/viddiff",
      "ai_keywords": [
        "Video Action Differencing (VidDiff)",
        "VidDiffBench",
        "multimodal models (LMMs)",
        "GPT-4o",
        "Qwen2-VL",
        "action difference proposal",
        "keyframe localization",
        "frame differencing",
        "agentic workflow",
        "fine-grained action differences",
        "localization timestamps"
      ]
    },
    "publishedAt": "2025-03-10T17:18:32.000Z",
    "title": "Video Action Differencing",
    "summary": "How do two individuals differ when performing the same action? In this work,\nwe introduce Video Action Differencing (VidDiff), the novel task of identifying\nsubtle differences between videos of the same action, which has many\napplications, such as coaching and skill learning. To enable development on\nthis new task, we first create VidDiffBench, a benchmark dataset containing 549\nvideo pairs, with human annotations of 4,469 fine-grained action differences\nand 2,075 localization timestamps indicating where these differences occur. Our\nexperiments demonstrate that VidDiffBench poses a significant challenge for\nstate-of-the-art large multimodal models (LMMs), such as GPT-4o and Qwen2-VL.\nBy analyzing failure cases of LMMs on VidDiffBench, we highlight two key\nchallenges for this task: localizing relevant sub-actions over two videos and\nfine-grained frame comparison. To overcome these, we propose the VidDiff\nmethod, an agentic workflow that breaks the task into three stages: action\ndifference proposal, keyframe localization, and frame differencing, each stage\nutilizing specialized foundation models. To encourage future research in this\nnew task, we release the benchmark at\nhttps://huggingface.co/datasets/jmhb/VidDiffBench and code at\nhttp://jmhb0.github.io/viddiff.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07860.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.07572",
      "authors": [
        {
          "_id": "67d0e38171b6b577dbb8c72c",
          "user": {
            "_id": "6500bbf5e102da55f9ed43fc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6500bbf5e102da55f9ed43fc/QZ6EAFV2CStFsILmTJw5D.jpeg",
            "isPro": true,
            "fullname": "Yuxiao Qu",
            "user": "CohenQu",
            "type": "user"
          },
          "name": "Yuxiao Qu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:41:33.926Z",
          "hidden": false
        },
        {
          "_id": "67d0e38171b6b577dbb8c72d",
          "name": "Matthew Y. R. Yang",
          "hidden": false
        },
        {
          "_id": "67d0e38171b6b577dbb8c72e",
          "name": "Amrith Setlur",
          "hidden": false
        },
        {
          "_id": "67d0e38171b6b577dbb8c72f",
          "name": "Lewis Tunstall",
          "hidden": false
        },
        {
          "_id": "67d0e38171b6b577dbb8c730",
          "name": "Edward Emanuel Beeching",
          "hidden": false
        },
        {
          "_id": "67d0e38171b6b577dbb8c731",
          "name": "Ruslan Salakhutdinov",
          "hidden": false
        },
        {
          "_id": "67d0e38171b6b577dbb8c732",
          "name": "Aviral Kumar",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T17:40:43.000Z",
      "title": "Optimisation des calculs dans les tests par des ajustements micro-régulations méta-récompenses",
      "summary": "L'utilisation efficace du calcul informatique lors de l'entraînement de modèles est cruciale pour améliorer le rendement logique des LLMs. Actuellement, les méthodes dominantes se concentrent sur le fine-tuning sur des flux de recherche ou sur l'apprentissage par renforcement (RL) avec des résultats binaires (0/1), mais il n'est pas clair si ces approches efficacement utilisent le calcul informatique lors du test ou si elles peuvent être étendues lorsque la gestion est améliorée. Dans cet article, nous répondons à ces questions. Nous formulons le problème d'optimisation du calcul informatique lors du test comme un problème d'apprentissage méta-récursif (RL), ce qui nous permet de voir de manière fondamentale comment le calcul informatique est utilisé lors du test. Dès cette perspective, nous pouvons configurer les longues séquences de sortie générées par les LLMs lors du test comme plusieurs épisodes d'entraînement, évaluant l'efficacité du calcul informatique par la perte cumulée des tokens. L'algorithme RL peut aider à trouver un équilibre entre l'exploration optimale et l'utilisation. La minimisation de la perte cumulée fournit un équilibre entre l'exploration et l'utilisation des tokens, montrant que les modèles les plus récents peuvent réduire la perte, et en combinant le RL avec un bonus de récompense dense, nous pouvons réaliser ces avantages. Ce bonus représente le \"progress\" dans chaque bloc subséquent de la séquence de sortie et quantifie la variation dans la probabilité de succès final. En utilisant cette perspective, nous développons une nouvelle classe de méthodes de fine-tuning pour optimiser le calcul informatique lors du test, appelées Meta-Recursive Training Fine-tuning (MRT). Le MRT fournit un effet relatif de 2-3 fois meilleur que le RL de récompenses, et un effet d'environ 1,5 fois meilleur en termes d'efficacité des tokens mathématiques.",
      "upvotes": 8,
      "discussionId": "67d0e38271b6b577dbb8c7b7",
      "projectPage": "https://cohenqu.github.io/mrt.github.io/",
      "ai_keywords": [
        "meta-reinforcement learning (RL)",
        "cumulative regret",
        "token stream",
        "exploration and exploitation",
        "dense reward bonus",
        "likelihood of eventual success",
        "Meta Reinforcement Fine-Tuning (MRT)"
      ]
    },
    "publishedAt": "2025-03-10T13:40:43.000Z",
    "title": "Optimizing Test-Time Compute via Meta Reinforcement Fine-Tuning",
    "summary": "Training models to effectively use test-time compute is crucial for improving\nthe reasoning performance of LLMs. Current methods mostly do so via fine-tuning\non search traces or running RL with 0/1 outcome reward, but do these approaches\nefficiently utilize test-time compute? Would these approaches continue to scale\nas the budget improves? In this paper, we try to answer these questions. We\nformalize the problem of optimizing test-time compute as a meta-reinforcement\nlearning (RL) problem, which provides a principled perspective on spending\ntest-time compute. This perspective enables us to view the long output stream\nfrom the LLM as consisting of several episodes run at test time and leads us to\nuse a notion of cumulative regret over output tokens as a way to measure the\nefficacy of test-time compute. Akin to how RL algorithms can best tradeoff\nexploration and exploitation over training, minimizing cumulative regret would\nalso provide the best balance between exploration and exploitation in the token\nstream. While we show that state-of-the-art models do not minimize regret, one\ncan do so by maximizing a dense reward bonus in conjunction with the outcome\n0/1 reward RL. This bonus is the ''progress'' made by each subsequent block in\nthe output stream, quantified by the change in the likelihood of eventual\nsuccess. Using these insights, we develop Meta Reinforcement Fine-Tuning, or\nMRT, a new class of fine-tuning methods for optimizing test-time compute. MRT\nleads to a 2-3x relative gain in performance and roughly a 1.5x gain in token\nefficiency for math reasoning compared to outcome-reward RL.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07572.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.08588",
      "authors": [
        {
          "_id": "67d125362da8f91f8ef0412f",
          "user": {
            "_id": "6190ab805ca89a28e9f66873",
            "avatarUrl": "/avatars/a677a8401360be473895494e5fb267bb.svg",
            "isPro": false,
            "fullname": "Xin Xu",
            "user": "XinXuNLPer",
            "type": "user"
          },
          "name": "Xin Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:36:07.130Z",
          "hidden": false
        },
        {
          "_id": "67d125362da8f91f8ef04130",
          "name": "Wei Xu",
          "hidden": false
        },
        {
          "_id": "67d125362da8f91f8ef04131",
          "name": "Ningyu Zhang",
          "hidden": false
        },
        {
          "_id": "67d125362da8f91f8ef04132",
          "name": "Julian McAuley",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-11T16:25:36.000Z",
      "title": "BiasEdit : Méthode pour modifier les modèles de langage entraînés pour éliminer les biais",
      "summary": "Dans les études précédentes, il a été clairement confirmé que les modèles de langue présentent des biais de type échelle dans leurs unités de traitement. Dans la stratégie actuelle de biais de dispositif dans les unités de traitement, la réentraînement des modèles en utilisant des facteurs de configuration, la projection de représentations et la génération ne sont pas efficaces pour éliminer les biais ou modifier directement les représentations internes biaisées du modèle. Pour faire face à ces problèmes, nous proposons un méthode efficace pour éditer des modèles qui supprime les biais de type échelle appelée BiasEdit. BiasEdit utilise une réseau éditeur qui exécute des éditions de modèle à travers une réseau légère, générant des mises à jour de paramètres pour éliminer le biais. BiasEdit maintient la capacité de modéliser la langue tout en éditant partiellement certains paramètres du modèle de langue en utilisant la perte de biais de dispositif. Dans les expériences avec StereoSet et Crows-Pairs, nous avons démontré l'efficacité, l'efficience et la robustesse de l'élimination des biais, et que l'édition des biais dans le modèle de langue a un impact très réduit sur sa capacité générale. De plus, nous avons effectué des entraînements de biais pour explorer les biais de chaque module et étudier l'impact de l'édition des biais sur différentes composantes du modèle de langue.",
      "upvotes": 5,
      "discussionId": "67d125382da8f91f8ef041d0",
      "githubRepo": "https://github.com/zjunlp/BiasEdit",
      "ai_keywords": [
        "language models",
        "counterfactual data",
        "representation projection",
        "prompting",
        "BiasEdit",
        "parameter updates",
        "debiasing loss",
        "retention loss",
        "StereoSet",
        "Crows-Pairs",
        "language modeling abilities",
        "bias tracing"
      ]
    },
    "publishedAt": "2025-03-11T12:25:36.000Z",
    "title": "BiasEdit: Debiasing Stereotyped Language Models via Model Editing",
    "summary": "Previous studies have established that language models manifest stereotyped\nbiases. Existing debiasing strategies, such as retraining a model with\ncounterfactual data, representation projection, and prompting often fail to\nefficiently eliminate bias or directly alter the models' biased internal\nrepresentations. To address these issues, we propose BiasEdit, an efficient\nmodel editing method to remove stereotypical bias from language models through\nlightweight networks that act as editors to generate parameter updates.\nBiasEdit employs a debiasing loss guiding editor networks to conduct local\nedits on partial parameters of a language model for debiasing while preserving\nthe language modeling abilities during editing through a retention loss.\nExperiments on StereoSet and Crows-Pairs demonstrate the effectiveness,\nefficiency, and robustness of BiasEdit in eliminating bias compared to\ntangental debiasing baselines and little to no impact on the language models'\ngeneral capabilities. In addition, we conduct bias tracing to probe bias in\nvarious modules and explore bias editing impacts on different components of\nlanguage models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.08588.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.08689",
      "authors": [
        {
          "_id": "67d0f759cb5bf46c22ac8af1",
          "name": "Yongdong Luo",
          "hidden": false
        },
        {
          "_id": "67d0f759cb5bf46c22ac8af2",
          "name": "Wang Chen",
          "hidden": false
        },
        {
          "_id": "67d0f759cb5bf46c22ac8af3",
          "name": "Xiawu Zheng",
          "hidden": false
        },
        {
          "_id": "67d0f759cb5bf46c22ac8af4",
          "name": "Weizhong Huang",
          "hidden": false
        },
        {
          "_id": "67d0f759cb5bf46c22ac8af5",
          "name": "Shukang Yin",
          "hidden": false
        },
        {
          "_id": "67d0f759cb5bf46c22ac8af6",
          "name": "Haojia Lin",
          "hidden": false
        },
        {
          "_id": "67d0f759cb5bf46c22ac8af7",
          "name": "Chaoyou Fu",
          "hidden": false
        },
        {
          "_id": "67d0f759cb5bf46c22ac8af8",
          "name": "Jinfa Huang",
          "hidden": false
        },
        {
          "_id": "67d0f759cb5bf46c22ac8af9",
          "name": "Jiayi Ji",
          "hidden": false
        },
        {
          "_id": "67d0f759cb5bf46c22ac8afa",
          "name": "Jiebo Luo",
          "hidden": false
        },
        {
          "_id": "67d0f759cb5bf46c22ac8afb",
          "name": "Rongrong Ji",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-11T17:59:57.000Z",
      "title": "Pour réaliser une structure de distribution de tokens pour la consultation, on s'appuie sur la compréhension de longs vidéos et on met en place un contexte.",
      "summary": "L'évolution récente de l'compréhension de longs vidéos se fait par la réduction des tokens visuels, ce qui réduit l'information non nécessaire visuelle. Cependant, les méthodes actuelles utilisent la réduction des tokens de réponse ultérieure dans les couches de décodage, mais ne considèrent pas la relation significative entre les tokens visuels et les requêtes (query) au niveau d'entrée. Dans cet article, nous proposons un nouveau module d'entraînement avec contraintes logiques précédentes appelé \"QuoTA\" pour étendre les grands modèles de langage vidéo (LVLMs). Ce module étend les LVLMs en utilisant l'attribution de tokens visuels basée sur l'évaluation du niveau d'importance des frames liées à la requête. La sélection de tokens est cruciale, et le processus optimise l'utilisation du bucket de tokens pour le traitement visuel, en conservant du contenu significatif. Spécifiquement, (i) QuoTA attribue des scores d'importance de frames basés sur la relation avec la requête, permettant une attribution de tokens visuels avant l'interaction croisée des modes dans la couche de décodage, (ii) sépare la requête en une inférence de \"Chain-of-Thoughts\" pour évaluer de manière plus précise l'importance des frames en utilisant le LVLM, et (iii) fournit des fonctions de port et de tube pour étendre les LVLMs. Les résultats expérimentaux étendus, combinés avec LLaVA-Video-7B, montrent un augmentation moyenne du rendement de 6 benchmarks (y compris Video-MME et MLVU) de 3,2%, en utilisant le même bucket de tokens visuels. Le code est disponible sur https://github.com/MAC-AutoML/QuoTA.",
      "upvotes": 4,
      "discussionId": "67d0f75bcb5bf46c22ac8b70",
      "githubRepo": "https://github.com/MAC-AutoML/QuoTA",
      "ai_keywords": [
        "QuoTA",
        "ante-hoc",
        "training-free",
        "modular",
        "long video understanding",
        "visual token pruning",
        "attention distribution",
        "decoder layers",
        "input-level semantic correlation",
        "visual tokens",
        "instructions",
        "query",
        "frame-level importance assessment",
        "task-specific requirements",
        "token budget utilization",
        "semantically relevant content",
        "Chain-of-Thoughts reasoning",
        "cross-modal interactions",
        "plug-and-play functionality",
        "LLaVA-Video-7B",
        "Video-MME",
        "MLVU"
      ]
    },
    "publishedAt": "2025-03-11T13:59:57.000Z",
    "title": "QuoTA: Query-oriented Token Assignment via CoT Query Decouple for Long\n  Video Comprehension",
    "summary": "Recent advances in long video understanding typically mitigate visual\nredundancy through visual token pruning based on attention distribution.\nHowever, while existing methods employ post-hoc low-response token pruning in\ndecoder layers, they overlook the input-level semantic correlation between\nvisual tokens and instructions (query). In this paper, we propose QuoTA, an\nante-hoc training-free modular that extends existing large video-language\nmodels (LVLMs) for visual token assignment based on query-oriented frame-level\nimportance assessment. The query-oriented token selection is crucial as it\naligns visual processing with task-specific requirements, optimizing token\nbudget utilization while preserving semantically relevant content.\nSpecifically, (i) QuoTA strategically allocates frame-level importance scores\nbased on query relevance, enabling one-time visual token assignment before\ncross-modal interactions in decoder layers, (ii) we decouple the query through\nChain-of-Thoughts reasoning to facilitate more precise LVLM-based frame\nimportance scoring, and (iii) QuoTA offers a plug-and-play functionality that\nextends to existing LVLMs. Extensive experimental results demonstrate that\nimplementing QuoTA with LLaVA-Video-7B yields an average performance\nimprovement of 3.2% across six benchmarks (including Video-MME and MLVU) while\noperating within an identical visual token budget as the baseline. Codes are\nopen-sourced at https://github.com/MAC-AutoML/QuoTA.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.08689.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.08685",
      "authors": [
        {
          "_id": "67d0f7032eaba9be7bf76e0e",
          "user": {
            "_id": "63483629ac5172169929da0e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1665676793089-noauth.jpeg",
            "isPro": false,
            "fullname": "Xin Wen",
            "user": "xwen99",
            "type": "user"
          },
          "name": "Xin Wen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:39:00.455Z",
          "hidden": false
        },
        {
          "_id": "67d0f7032eaba9be7bf76e0f",
          "user": {
            "_id": "62dcd71075e9787ec5aa41ba",
            "avatarUrl": "/avatars/f37ce036b76180ed0fa004f9c8c09363.svg",
            "isPro": true,
            "fullname": "Bingchen Zhao",
            "user": "tennant",
            "type": "user"
          },
          "name": "Bingchen Zhao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:38:56.945Z",
          "hidden": false
        },
        {
          "_id": "67d0f7032eaba9be7bf76e10",
          "name": "Ismail Elezi",
          "hidden": false
        },
        {
          "_id": "67d0f7032eaba9be7bf76e11",
          "name": "Jiankang Deng",
          "hidden": false
        },
        {
          "_id": "67d0f7032eaba9be7bf76e12",
          "name": "Xiaojuan Qi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-11T17:59:41.000Z",
      "title": "\"Analyse en composantes principales\" permet aux images d'avoir un nouveau langage.",
      "summary": "Nous présentons un nouveau cadre de travail de tokenisation visuelle. Ce cadre de travail introduit une structure similaire à la PCA provable dans l'espace de tokens potentiels. La tokenisation visuelle existante se concentre principalement sur l'optimisation de la précision de reconstruction, mais fréquemment ignore les caractéristiques structurales de l'espace potentiel, ce qui est un élément important pour l'interprétation et les tâches ultérieures. Notre méthode génère une séquence de tokens causales unidimensionnels pour les images. Chaque token continu a une variabilité expliquée décroissante mathématiquement garantie, fournissant de l'information non répétitive. Cette structure est similaire à l'analyse des composantes principales. Cette restriction structurelle garantit que les tokens extraient d'abord les caractéristiques visuelles les plus salientes, et chaque token suivant ajoute de l'information complémentaire de manière graduelle décroissante. De plus, nous avons identifié et résolu l'effet de la combinaison sémantique spectrale, où le contenu sémantique d'un haut niveau et les détails spectrales d'un bas niveau se mélangent de manière inattendue dans les tokens. Les expérimentations montrent que notre approche atteint un rendement de reconstruction supérieur au courant et améliore l'interprétabilité pour mieux s'adapter au système visuel humain. De plus, les modèles automatiques de régression entraînés et inférées en utilisant notre séquence de tokens atteignent le rendement le plus élevé actuel mais nécessitent moins de tokens.",
      "upvotes": 4,
      "discussionId": "67d0f7052eaba9be7bf76eac",
      "projectPage": "https://visual-gen.github.io/semanticist/",
      "githubRepo": "https://github.com/visual-gen/semanticist",
      "ai_keywords": [
        "visual tokenization framework",
        "PCA-like structure",
        "latent token space",
        "reconstruction fidelity",
        "structural properties",
        "interpretabiliy",
        "1D causal token sequence",
        "explained variance",
        "principal component analysis",
        "salient visual features",
        "semantic-spectrum coupling effect",
        "diffusion decoder",
        "reconstruction performance",
        "human vision system",
        "auto-regressive models",
        "state-of-the-art methods"
      ]
    },
    "publishedAt": "2025-03-11T13:59:41.000Z",
    "title": "\"Principal Components\" Enable A New Language of Images",
    "summary": "We introduce a novel visual tokenization framework that embeds a provable\nPCA-like structure into the latent token space. While existing visual\ntokenizers primarily optimize for reconstruction fidelity, they often neglect\nthe structural properties of the latent space -- a critical factor for both\ninterpretability and downstream tasks. Our method generates a 1D causal token\nsequence for images, where each successive token contributes non-overlapping\ninformation with mathematically guaranteed decreasing explained variance,\nanalogous to principal component analysis. This structural constraint ensures\nthe tokenizer extracts the most salient visual features first, with each\nsubsequent token adding diminishing yet complementary information.\nAdditionally, we identified and resolved a semantic-spectrum coupling effect\nthat causes the unwanted entanglement of high-level semantic content and\nlow-level spectral details in the tokens by leveraging a diffusion decoder.\nExperiments demonstrate that our approach achieves state-of-the-art\nreconstruction performance and enables better interpretability to align with\nthe human vision system. Moreover, auto-regressive models trained on our token\nsequences achieve performance comparable to current state-of-the-art methods\nwhile requiring fewer tokens for training and inference.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.08685.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.07699",
      "authors": [
        {
          "_id": "67d114912264403cbf39d0ba",
          "name": "Huiyang Shao",
          "hidden": false
        },
        {
          "_id": "67d114912264403cbf39d0bb",
          "name": "Xin Xia",
          "hidden": false
        },
        {
          "_id": "67d114912264403cbf39d0bc",
          "name": "Yuhong Yang",
          "hidden": false
        },
        {
          "_id": "67d114912264403cbf39d0bd",
          "name": "Yuxi Ren",
          "hidden": false
        },
        {
          "_id": "67d114912264403cbf39d0be",
          "name": "Xing Wang",
          "hidden": false
        },
        {
          "_id": "67d114912264403cbf39d0bf",
          "name": "Xuefeng Xiao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T17:20:52.000Z",
      "title": "RayFlow: Reconnaître les informations d'instances pour accélérer le traitement du trafic de manière adaptative",
      "summary": "Le modèle de diffusion a réussi dans de nombreuses domaines de manière impressionnante. Cependant, la lenteur de la génération reste un problème important. Les méthodes d'accélération actuelles perdent la qualité des échantillons ou introduisent des complexités dans l'entraînement. Dans ce contexte, on propose RayFlow, un nouveau cadre de travail de diffusion. RayFlow guide chaque échantillon à travers différentes routes correspondant à des distributions cibles uniques pour chaque instance, ce qui maintient la diversité et l'stabilité de la génération tout en minimisant les étapes d'échantillonnage. De plus, on présente le méthode d'échantillonnage Time Sampler, qui se concentre sur améliorer l'efficacité de l'entraînement en se focalisant sur les moments temporels les plus importants. Les expériences larges montrent que RayFlow améliore la vitesse, le contrôle et l'efficacité de l'entraînement par rapport aux méthodes d'accélération existantes, démontrant sa capacité à générer des images de haute qualité.",
      "upvotes": 3,
      "discussionId": "67d114922264403cbf39d0f8",
      "ai_keywords": [
        "diffusion models",
        "generation speed",
        "RayFlow",
        "instance-specific target distribution",
        "sampling steps",
        "generation diversity",
        "stability",
        "Time Sampler",
        "importance sampling",
        "training efficiency",
        "high-quality images"
      ]
    },
    "publishedAt": "2025-03-10T13:20:52.000Z",
    "title": "RayFlow: Instance-Aware Diffusion Acceleration via Adaptive Flow\n  Trajectories",
    "summary": "Diffusion models have achieved remarkable success across various domains.\nHowever, their slow generation speed remains a critical challenge. Existing\nacceleration methods, while aiming to reduce steps, often compromise sample\nquality, controllability, or introduce training complexities. Therefore, we\npropose RayFlow, a novel diffusion framework that addresses these limitations.\nUnlike previous methods, RayFlow guides each sample along a unique path towards\nan instance-specific target distribution. This method minimizes sampling steps\nwhile preserving generation diversity and stability. Furthermore, we introduce\nTime Sampler, an importance sampling technique to enhance training efficiency\nby focusing on crucial timesteps. Extensive experiments demonstrate RayFlow's\nsuperiority in generating high-quality images with improved speed, control, and\ntraining efficiency compared to existing acceleration techniques.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07699.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.18858",
      "authors": [
        {
          "_id": "67d1080b2264403cbf36b0ad",
          "name": "Jingtao Zhan",
          "hidden": false
        },
        {
          "_id": "67d1080b2264403cbf36b0ae",
          "name": "Jiahao Zhao",
          "hidden": false
        },
        {
          "_id": "67d1080b2264403cbf36b0af",
          "name": "Jiayu Li",
          "hidden": false
        },
        {
          "_id": "67d1080b2264403cbf36b0b0",
          "name": "Yiqun Liu",
          "hidden": false
        },
        {
          "_id": "67d1080b2264403cbf36b0b1",
          "name": "Bo Zhang",
          "hidden": false
        },
        {
          "_id": "67d1080b2264403cbf36b0b2",
          "name": "Qingyao Ai",
          "hidden": false
        },
        {
          "_id": "67d1080b2264403cbf36b0b3",
          "name": "Jiaxin Mao",
          "hidden": false
        },
        {
          "_id": "67d1080b2264403cbf36b0b4",
          "name": "Hongning Wang",
          "hidden": false
        },
        {
          "_id": "67d1080b2264403cbf36b0b5",
          "name": "Min Zhang",
          "hidden": false
        },
        {
          "_id": "67d1080b2264403cbf36b0b6",
          "name": "Shaoping Ma",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-26T05:59:45.000Z",
      "title": "Évaluation de l'Intelligence par des Tests et des Erreurs",
      "summary": "L'intelligence est une caractéristique importante qui limite les erreurs et cherche des solutions. En se basant sur ce concept, nous proposons un jeu de survie (Survival Game) pour évaluer l'intelligence en fonction de la quantité d'erreurs. Une plus faible quantité d'erreurs indique une plus grande intelligence. L'attente et la variance des erreurs sont limitées, ce qui reflète la capacité de résoudre des problèmes de manière continue et est définie comme le niveau autonome (Autonomous Level). En utilisant ce jeu, nous évaluons structurellement les systèmes AI actuels. Il apparaît que les systèmes AI atteignent le niveau autonome dans des tâches simples, mais sont loin d'y parvenir dans des tâches complexes telles que la vision, la recherche, le reconnaissance d'images, le langage et plus encore. Pour atteindre le niveau autonome dans des tâches générales, il est prédit qu'il faudrait 10^26 paramètres. Cela implique que le nombre de GPU H100 nécessaires est excessivement grand, et le coût total est 10^7 fois plus élevé que le prix de marché d'Apple Inc. Selon la loi de Moore, il serait nécessaire de 70 ans pour soutenir cette échelle de paramètres. Ces hauts coûts mettent en évidence la complexité des tâches humaines et l'insuffisance des technologies AI actuelles. Pour approfondir ces phénomènes, nous avons effectué un analyse théorique et des expérimentations du jeu de survie. Nos résultats montrent que les tâches humaines sont extrêmement complexes et nécessitent des modèles à grande échelle. Cette complexité est évidente dans la nécessité de modèles à grande échelle, ce qui implique des coûts astronomiques et une faible capacité à résoudre des problèmes complexes dans les technologies actuelles.",
      "upvotes": 3,
      "discussionId": "67d108112264403cbf36b1e9",
      "githubRepo": "https://github.com/jingtaozhan/IntelligenceTest"
    },
    "publishedAt": "2025-02-26T00:59:45.000Z",
    "title": "Evaluating Intelligence via Trial and Error",
    "summary": "Intelligence is a crucial trait for species to find solutions within a\nlimited number of trial-and-error attempts. Building on this idea, we introduce\nSurvival Game as a framework to evaluate intelligence based on the number of\nfailed attempts in a trial-and-error process. Fewer failures indicate higher\nintelligence. When the expectation and variance of failure counts are both\nfinite, it signals the ability to consistently find solutions to new\nchallenges, which we define as the Autonomous Level of intelligence. Using\nSurvival Game, we comprehensively evaluate existing AI systems. Our results\nshow that while AI systems achieve the Autonomous Level in simple tasks, they\nare still far from it in more complex tasks, such as vision, search,\nrecommendation, and language. While scaling current AI technologies might help,\nthis would come at an astronomical cost. Projections suggest that achieving the\nAutonomous Level for general tasks would require 10^{26} parameters. To put\nthis into perspective, loading such a massive model requires so many H100 GPUs\nthat their total value is 10^{7} times that of Apple Inc.'s market value.\nEven with Moore's Law, supporting such a parameter scale would take 70 years.\nThis staggering cost highlights the complexity of human tasks and the\ninadequacies of current AI technologies. To further investigate this\nphenomenon, we conduct a theoretical analysis of Survival Game and its\nexperimental results. Our findings suggest that human tasks possess a\ncriticality property. As a result, Autonomous Level requires a deep\nunderstanding of the task's underlying mechanisms. Current AI systems, however,\ndo not fully grasp these mechanisms and instead rely on superficial mimicry,\nmaking it difficult for them to reach an autonomous level. We believe Survival\nGame can not only guide the future development of AI but also offer profound\ninsights into human intelligence.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.18858.png",
    "numComments": 2,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.07639",
      "authors": [
        {
          "_id": "67d0e3ede3afecf451915d0a",
          "name": "Xingyi Yang",
          "hidden": false
        },
        {
          "_id": "67d0e3ede3afecf451915d0b",
          "name": "Constantin Venhoff",
          "hidden": false
        },
        {
          "_id": "67d0e3ede3afecf451915d0c",
          "name": "Ashkan Khakzar",
          "hidden": false
        },
        {
          "_id": "67d0e3ede3afecf451915d0d",
          "name": "Christian Schroeder de Witt",
          "hidden": false
        },
        {
          "_id": "67d0e3ede3afecf451915d0e",
          "name": "Puneet K. Dokania",
          "hidden": false
        },
        {
          "_id": "67d0e3ede3afecf451915d0f",
          "name": "Adel Bibi",
          "hidden": false
        },
        {
          "_id": "67d0e3ede3afecf451915d10",
          "name": "Philip Torr",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-05T17:40:54.000Z",
      "title": "Mixture of Experts Made Intrinsically Interpretable",
      "summary": "Le neurone représente plusieurs significations dans des modèles de langage à grande échelle et fréquemment codifie des concepts irrélevants simultanément, en masquant son interprétabilité. Au lieu de dépendre de méthodes de post-traitement, nous proposons un modèle de langage Experts Mixte (MoE) qui possède une interprétabilité propre. Notre approche s'appuie sur l'observation que des réseaux étendus avec des activités rares dans le modèle de langage sont faciles à détecter des causes interprétatives. Cependant, l'entraînement de ces grandes réseaux rares directement est computationally impraticable. L'architecture MoE active certains experts pour traiter les entrées de manière échelonnable, ce qui concorde avec notre objectif d'interprétabilité. Dans MoE-X, nous remplaçons les couches MoE par des MLPs équivalents rares, établissant cette connexion. Cette approximation permet une échelonnabilité efficace de la dimension d'entrée et maintient la rarité. De plus, pour améliorer l'interprétabilité, nous forceons des activités rares dans chaque expert, redésignons la structure des racines et privilégions les experts avec l'activité la plus rare. Cela garantit que seules des caractéristiques claires soient traitées. MoE-X a été évalué sur des tâches de jeu d'échecs et de traitement du langage naturel, montrant un rendement comparable aux modèles denses et un grand amélioration de l'interprétabilité. Comparé à GPT-2, MoE-X est plus permutatif et sa interprétabilité est supérieure aux approches basées sur les codificateurs autochtones rares (SAE).",
      "upvotes": 2,
      "discussionId": "67d0e3f0e3afecf451915dfa",
      "ai_keywords": [
        "polysemanticity",
        "Mixture-of-Experts (MoE)",
        "interpretable",
        "sparse activations",
        "sparsity",
        "sparse networks",
        "hidden size",
        "sparse activation",
        "routing mechanism",
        "salient features",
        "perplexity",
        "GPT-2",
        "sparse autoencoder (SAE)"
      ]
    },
    "publishedAt": "2025-03-05T12:40:54.000Z",
    "title": "Mixture of Experts Made Intrinsically Interpretable",
    "summary": "Neurons in large language models often exhibit polysemanticity,\nsimultaneously encoding multiple unrelated concepts and obscuring\ninterpretability. Instead of relying on post-hoc methods, we present\nMoE-X, a Mixture-of-Experts (MoE) language model designed to be\nintrinsically interpretable. Our approach is motivated by the\nobservation that, in language models, wider networks with sparse activations\nare more likely to capture interpretable factors. However, directly training\nsuch large sparse networks is computationally prohibitive. MoE architectures\noffer a scalable alternative by activating only a subset of experts for any\ngiven input, inherently aligning with interpretability objectives. In MoE-X, we\nestablish this connection by rewriting the MoE layer as an equivalent sparse,\nlarge MLP. This approach enables efficient scaling of the hidden size while\nmaintaining sparsity. To further enhance interpretability, we enforce sparse\nactivation within each expert and redesign the routing mechanism to prioritize\nexperts with the highest activation sparsity. These designs ensure that only\nthe most salient features are routed and processed by the experts. We evaluate\nMoE-X on chess and natural language tasks, showing that it achieves performance\ncomparable to dense models while significantly improving interpretability.\nMoE-X achieves a perplexity better than GPT-2, with interpretability surpassing\neven sparse autoencoder (SAE)-based approaches.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07639.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.08507",
      "authors": [
        {
          "_id": "67d15293e3afecf451aceab7",
          "name": "Qing Jiang",
          "hidden": false
        },
        {
          "_id": "67d15293e3afecf451aceab8",
          "name": "Lin Wu",
          "hidden": false
        },
        {
          "_id": "67d15293e3afecf451aceab9",
          "name": "Zhaoyang Zeng",
          "hidden": false
        },
        {
          "_id": "67d15293e3afecf451aceaba",
          "name": "Tianhe Ren",
          "hidden": false
        },
        {
          "_id": "67d15293e3afecf451aceabb",
          "name": "Yuda Xiong",
          "hidden": false
        },
        {
          "_id": "67d15293e3afecf451aceabc",
          "name": "Yihao Chen",
          "hidden": false
        },
        {
          "_id": "67d15293e3afecf451aceabd",
          "name": "Qin Liu",
          "hidden": false
        },
        {
          "_id": "67d15293e3afecf451aceabe",
          "name": "Lei Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-11T14:57:14.000Z",
      "title": "「Sans importance de qui」",
      "summary": "L'être humain est le participant le plus important dans la vision par ordinateur, et le travail de détection de certaines personnes définies par la capacité à comprendre le langage naturel a un sens pratique. Cependant, les modèles actuels souvent ne parviennent pas à atteindre la possibilité d'utilisation réelle, et les benchmarks actuels empêchent son développement en se concentrant sur un seul critère. Dans cet article, trois aspects cruciaux de ce travail ont été examinés : la définition du travail, le design des jeux de données et l'architecture du modèle. Tout d'abord, les cinq caractéristiques possibles d'entités et les trois propriétés du travail ont été identifiées, et ensuite, l'objectif a été de surmonter ces problèmes et de améliorer la simulation d'applications réelles en designant un nouveau jeu de données appelé HumanRef. Du point de vue du design du modèle, des modèles de langage et un cadre de travail pour la détection d'objets ont été intégrés pour construire un modèle de référence fort appelé RexSeek. Les résultats des expériences montrent que les modèles les plus avancés qui se comportent bien généralement (RefCOCO/+/g) échouent à détecter de nombreuses personnes dans HumanRef. D'autre part, RexSeek a démontré un excellent rendement en critères humains, tout en pouvant être étendu efficacement aux objets généraux et s'appliquer largement à différentes tâches d'observation. Le code est disponible sur https://github.com/IDEA-Research/RexSeek.",
      "upvotes": 1,
      "discussionId": "67d15294e3afecf451aceb29",
      "ai_keywords": [
        "referable entities",
        "multimodal large language model",
        "object detection framework",
        "HumanRef",
        "RexSeek",
        "RefCOCO/+/g"
      ]
    },
    "publishedAt": "2025-03-11T10:57:14.000Z",
    "title": "Referring to Any Person",
    "summary": "Humans are undoubtedly the most important participants in computer vision,\nand the ability to detect any individual given a natural language description,\na task we define as referring to any person, holds substantial practical value.\nHowever, we find that existing models generally fail to achieve real-world\nusability, and current benchmarks are limited by their focus on one-to-one\nreferring, that hinder progress in this area. In this work, we revisit this\ntask from three critical perspectives: task definition, dataset design, and\nmodel architecture. We first identify five aspects of referable entities and\nthree distinctive characteristics of this task. Next, we introduce HumanRef, a\nnovel dataset designed to tackle these challenges and better reflect real-world\napplications. From a model design perspective, we integrate a multimodal large\nlanguage model with an object detection framework, constructing a robust\nreferring model named RexSeek. Experimental results reveal that\nstate-of-the-art models, which perform well on commonly used benchmarks like\nRefCOCO/+/g, struggle with HumanRef due to their inability to detect multiple\nindividuals. In contrast, RexSeek not only excels in human referring but also\ngeneralizes effectively to common object referring, making it broadly\napplicable across various perception tasks. Code is available at\nhttps://github.com/IDEA-Research/RexSeek",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.08507.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.08478",
      "authors": [
        {
          "_id": "67d12d0b44be28339053b965",
          "user": {
            "_id": "64a3eb280111d5ff6c4849fd",
            "avatarUrl": "/avatars/3a9000393b8d200418bae5fe7d902e4d.svg",
            "isPro": false,
            "fullname": "Han-Wei Kung",
            "user": "hkung",
            "type": "user"
          },
          "name": "Han-Wei Kung",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:50:08.946Z",
          "hidden": false
        },
        {
          "_id": "67d12d0b44be28339053b966",
          "name": "Tuomas Varanka",
          "hidden": false
        },
        {
          "_id": "67d12d0b44be28339053b967",
          "name": "Terence Sim",
          "hidden": false
        },
        {
          "_id": "67d12d0b44be28339053b968",
          "name": "Nicu Sebe",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-11T14:29:37.000Z",
      "title": "NullFace : Approche non supervisée de la localisation de la face",
      "summary": "La quantité de caméras continue d'augmenter, et avec cela, les préoccupations de la vie privée dans l'ère numérique actuelle augmentent. Les méthodes d'anti-identification existantes peuvent masquer l'information personnelle des personnes, mais maintenir l'utilité des images est difficile. Cet article présente une méthodologie permettant l'animation de la face tout en maintenant la propriété des caractéristiques non liées, appliquée sans entraînement. Notre approche utilise un modèle étendu à partir d'images de texte pré-entraîné, sans nécessité d'optimisation ou d'entraînement supplémentaire. Tout d'abord, on reconstruit le bruit initial dans les images entrées. Ensuite, on réduit le bruit caché pour maintenir les caractéristiques du visage, et on utilise le bruit caché modifié du visage pour réduire le bruit caché. Notre approche permet l'animation sélective de la région faciale, et les utilisateurs peuvent contrôler si l'animation est appliquée ou si la région faciale est maintenue. Comparé aux méthodes les plus récentes, notre approche dépasse en termes d'animation, de maintenance des caractéristiques du visage et de qualité de l'image. Sa flexibilité, sa robustesse et ses caractéristiques pratiques le rendent adapté aux applications réalistes. Le code et les données peuvent être trouvés sur https://github.com/hanweikung/nullface.",
      "upvotes": 1,
      "discussionId": "67d12d1044be28339053baab",
      "ai_keywords": [
        "text-to-image diffusion model",
        "identity-conditioned diffusion",
        "identity embeddings",
        "localized anonymization"
      ]
    },
    "publishedAt": "2025-03-11T10:29:37.000Z",
    "title": "NullFace: Training-Free Localized Face Anonymization",
    "summary": "Privacy concerns around ever increasing number of cameras are increasing in\ntoday's digital age. Although existing anonymization methods are able to\nobscure identity information, they often struggle to preserve the utility of\nthe images. In this work, we introduce a training-free method for face\nanonymization that preserves key non-identity-related attributes. Our approach\nutilizes a pre-trained text-to-image diffusion model without requiring\noptimization or training. It begins by inverting the input image to recover its\ninitial noise. The noise is then denoised through an identity-conditioned\ndiffusion process, where modified identity embeddings ensure the anonymized\nface is distinct from the original identity. Our approach also supports\nlocalized anonymization, giving users control over which facial regions are\nanonymized or kept intact. Comprehensive evaluations against state-of-the-art\nmethods show our approach excels in anonymization, attribute preservation, and\nimage quality. Its flexibility, robustness, and practicality make it\nwell-suited for real-world applications. Code and data can be found at\nhttps://github.com/hanweikung/nullface .",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.08478.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.08307",
      "authors": [
        {
          "_id": "67d140378cb4592900a1a75e",
          "user": {
            "_id": "66895b3d41fcf83c026b5dca",
            "avatarUrl": "/avatars/f1104041ee3445024f05d5d0b4d1550b.svg",
            "isPro": false,
            "fullname": "Alex Ergasti",
            "user": "MaverickAlex",
            "type": "user"
          },
          "name": "Alex Ergasti",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:36:03.978Z",
          "hidden": false
        },
        {
          "_id": "67d140378cb4592900a1a75f",
          "name": "Giuseppe Gabriele Tarollo",
          "hidden": false
        },
        {
          "_id": "67d140378cb4592900a1a760",
          "name": "Filippo Botti",
          "hidden": false
        },
        {
          "_id": "67d140378cb4592900a1a761",
          "name": "Tomaso Fontanini",
          "hidden": false
        },
        {
          "_id": "67d140378cb4592900a1a762",
          "name": "Claudio Ferrari",
          "hidden": false
        },
        {
          "_id": "67d140378cb4592900a1a763",
          "name": "Massimo Bertozzi",
          "hidden": false
        },
        {
          "_id": "67d140378cb4592900a1a764",
          "name": "Andrea Prati",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-11T11:18:47.000Z",
      "title": "RFLAV : Flux de flux d'enregistrement pour la génération de vidéos de voix infinie",
      "summary": "La génération d'AV reste un problème important dans la génération de l'IA, complexé principalement par trois facteurs cruciaux : la qualité des exemples générés, la synchronisation motivée par la motivation et la cohérence temporelle, et la nécessité que le tracé sonore coïncide ou soit opposé aux données visuelles, ou coïncide sans limites pendant de longs temps de vidéo. Dans cet article, nous présentons une nouvelle architecture basée sur les transformers pour résoudre tous ces problèmes importants dans la génération d'AV. Nous revisitrons trois modules d'interaction croisés différents, et nous évaluons que le module léger de fonctions temporelles est la forme la plus efficace et efficace en termes de calcul pour la configuration des modules sonores et visuels. Les résultats des expérimentations montrent que cette architecture dépasse les modèles les plus avancés actuels dans les tâches de génération d'AV multi-modules. Les codes et les points de vérification sont disponibles sur https://github.com/ErgastiAlex/R-FLAV.",
      "upvotes": 1,
      "discussionId": "67d1403b8cb4592900a1a868",
      "githubRepo": "https://github.com/ErgastiAlex/R-FLAV",
      "ai_keywords": [
        "transformer-based architecture",
        "cross modality interaction modules",
        "lightweight temporal fusion module",
        "audio and visual modalities",
        "multimodal AV generation tasks"
      ]
    },
    "publishedAt": "2025-03-11T07:18:47.000Z",
    "title": "^RFLAV: Rolling Flow matching for infinite Audio Video generation",
    "summary": "Joint audio-video (AV) generation is still a significant challenge in\ngenerative AI, primarily due to three critical requirements: quality of the\ngenerated samples, seamless multimodal synchronization and temporal coherence,\nwith audio tracks that match the visual data and vice versa, and limitless\nvideo duration. In this paper, we present , a novel transformer-based\narchitecture that addresses all the key challenges of AV generation. We explore\nthree distinct cross modality interaction modules, with our lightweight\ntemporal fusion module emerging as the most effective and computationally\nefficient approach for aligning audio and visual modalities. Our experimental\nresults demonstrate that  outperforms existing state-of-the-art models\nin multimodal AV generation tasks. Our code and checkpoints are available at\nhttps://github.com/ErgastiAlex/R-FLAV.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.08307.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.08102",
      "authors": [
        {
          "_id": "67d12c32428a3d8d5281f310",
          "name": "Jiale Wei",
          "hidden": false
        },
        {
          "_id": "67d12c32428a3d8d5281f311",
          "name": "Xiang Ying",
          "hidden": false
        },
        {
          "_id": "67d12c32428a3d8d5281f312",
          "name": "Tao Gao",
          "hidden": false
        },
        {
          "_id": "67d12c32428a3d8d5281f313",
          "name": "Felix Tao",
          "hidden": false
        },
        {
          "_id": "67d12c32428a3d8d5281f314",
          "name": "Jingbo Shang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-11T07:05:52.000Z",
      "title": "AI-native Memory 2.0 : Le Deuxième Moment",
      "summary": "La humanité échange des souvenirs personnels à travers l'interaction avec le monde extérieur. Cette interaction comprend l'interaction avec d'autres personnes, des sites Web, des applications et futurs agents d'IA. Une partie de cette interaction est que les utilisateurs doivent fournir la même information répétitivement dans différents contextes, ce qui peut rendre l'information complexe en raison de données inutiles. Les solutions actuelles, comme la sauvegarde de l'information d'authentification dans les navigateurs, les structures d'autocompletion ou les systèmes d'authentification intégrés, aident à réduire la quantité d'information inutile en stockant et en extrayant des données de l'utilisateur. Le développement de modèles de langage grands (LLM) offre une opportunité pour réorganiser la gestion de la mémoire dans le paradigme de la base de connaissance de l'IA, et SECOND ME exploite cette opportunité pour promouvoir un approche plus systématique et intelligente dans la gestion de la mémoire. SECOND ME est un système de charge de mémoire intelligent et continu qui maintient, organise et utilise des connaissances dynamiquement en relation avec le savoir de l'utilisateur. Il agit comme un médiateur dans les interactions de l'utilisateur, génère des réponses automatiques dans des contextes appropriés, prépare les informations nécessaires et promeut la communication sans intermédiaires avec des systèmes externes, réduisant significativement le fardeau cognitif et la friction dans les interactions. En comparaison avec les solutions traditionnelles de stockage de la mémoire, SECOND ME utilise la paramétrisation de la mémoire basée sur les LLM pour permettre une organisation structurée, des connexions de contexte et une recherche de connaissances adaptative, ce qui favorise un approche plus systématique et intelligente dans la gestion de la mémoire. En tant qu'agent personnel d'IA, SECOND ME s'intègre davantage dans l'écosystème numérique et se présente comme un pas important pour renforcer l'interaction entre l'humanité et le monde. Nous sommes en train de compléter complètement le système de dépôt distribué localisable sur GitHub : https://github.com/Mindverse/Second-Me.",
      "upvotes": 1,
      "discussionId": "67d12c33428a3d8d5281f346",
      "ai_keywords": [
        "large language models (LLMs)",
        "intelligent, persistent memory offload system",
        "context-aware responses",
        "structured organization",
        "contextual reasoning",
        "adaptive knowledge retrieval",
        "self-optimizing memory systems"
      ]
    },
    "publishedAt": "2025-03-11T03:05:52.000Z",
    "title": "AI-native Memory 2.0: Second Me",
    "summary": "Human interaction with the external world fundamentally involves the exchange\nof personal memory, whether with other individuals, websites, applications, or,\nin the future, AI agents. A significant portion of this interaction is\nredundant, requiring users to repeatedly provide the same information across\ndifferent contexts. Existing solutions, such as browser-stored credentials,\nautofill mechanisms, and unified authentication systems, have aimed to mitigate\nthis redundancy by serving as intermediaries that store and retrieve commonly\nused user data. The advent of large language models (LLMs) presents an\nopportunity to redefine memory management through an AI-native paradigm: SECOND\nME. SECOND ME acts as an intelligent, persistent memory offload system that\nretains, organizes, and dynamically utilizes user-specific knowledge. By\nserving as an intermediary in user interactions, it can autonomously generate\ncontext-aware responses, prefill required information, and facilitate seamless\ncommunication with external systems, significantly reducing cognitive load and\ninteraction friction. Unlike traditional memory storage solutions, SECOND ME\nextends beyond static data retention by leveraging LLM-based memory\nparameterization. This enables structured organization, contextual reasoning,\nand adaptive knowledge retrieval, facilitating a more systematic and\nintelligent approach to memory management. As AI-driven personal agents like\nSECOND ME become increasingly integrated into digital ecosystems, SECOND ME\nfurther represents a critical step toward augmenting human-world interaction\nwith persistent, contextually aware, and self-optimizing memory systems. We\nhave open-sourced the fully localizable deployment system at GitHub:\nhttps://github.com/Mindverse/Second-Me.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.08102.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.06594",
      "authors": [
        {
          "_id": "67cfd77ff8ee57c14450221b",
          "user": {
            "_id": "6440b38d3e0374802e1acc5e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6440b38d3e0374802e1acc5e/w-ZpW_9gCSHUeDKyGSeMt.jpeg",
            "isPro": false,
            "fullname": "luoyingfeng",
            "user": "luoyingfeng",
            "type": "user"
          },
          "name": "Yingfeng Luo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:42:33.649Z",
          "hidden": false
        },
        {
          "_id": "67cfd77ff8ee57c14450221c",
          "name": "Tong Zheng",
          "hidden": false
        },
        {
          "_id": "67cfd77ff8ee57c14450221d",
          "name": "Yongyu Mu",
          "hidden": false
        },
        {
          "_id": "67cfd77ff8ee57c14450221e",
          "name": "Bei Li",
          "hidden": false
        },
        {
          "_id": "67cfd77ff8ee57c14450221f",
          "name": "Qinghong Zhang",
          "hidden": false
        },
        {
          "_id": "67cfd77ff8ee57c144502220",
          "name": "Yongqi Gao",
          "hidden": false
        },
        {
          "_id": "67cfd77ff8ee57c144502221",
          "name": "Ziqiang Xu",
          "hidden": false
        },
        {
          "_id": "67cfd77ff8ee57c144502222",
          "name": "Peinan Feng",
          "hidden": false
        },
        {
          "_id": "67cfd77ff8ee57c144502223",
          "name": "Xiaoqian Liu",
          "hidden": false
        },
        {
          "_id": "67cfd77ff8ee57c144502224",
          "name": "Tong Xiao",
          "hidden": false
        },
        {
          "_id": "67cfd77ff8ee57c144502225",
          "name": "Jingbo Zhu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-09T12:54:05.000Z",
      "title": "Pas seulement décodeur : les modèles de langage grands peuvent être utilisés comme encodeurs pour la traduction automatique.",
      "summary": "L'domaine de la traduction automatique basée sur les réseaux neuronaux (NMT) a connu des changements avec l'apparition de modèles de langage grand (LLM). Dans les derniers débats sur le traitement du langage naturel (NLP), l'accent a été mis sur l'utilisation uniquement de modèles transformateurs pré-entraînés pour la traduction automatique et la modélisation de diverses tâches. Dans les modèles NMT de la semaine dernière, l'architecture standard encoder-decoder a reçu moins d'attention. Dans cet article, nous examinons comment relire le monde des LLM et NMT pour développer des modèles généraux, efficaces et faciles à optimiser. Nous appliquons un LLM comme encodeur dans NMT, tandis que le décodeur NMT reste intact. De plus, nous développons des méthodes pour améliorer la collaboration entre l'LLM et le décodeur NMT. Nous construisons un nouveau jeu de données qui inclut diverses tâches pour évaluer la capacité d'extension des systèmes de traduction automatique dans plusieurs tâches. Dans les évaluations de WMT et notre jeu de données, les résultats obtenus par notre méthode sont comparables ou meilleurs en termes de qualité de traduction, avec un accroissement de la vitesse d'inférence de 2,4 à 6,5 fois et une réduction de 75% de l'utilisation de la mémoire par cache KV. De plus, il montre une forte capacité d'extension dans diverses tâches liées à la traduction.",
      "upvotes": 1,
      "discussionId": "67cfd780f8ee57c144502268",
      "githubRepo": "https://github.com/NiuTrans/LaMaTE/",
      "ai_keywords": [
        "large language models (LLMs)",
        "neural machine translation (NMT)",
        "natural language processing (NLP)",
        "Transformer decoder",
        "encoder-decoder architectures",
        "pre-trained Transformer decoder",
        "LLMs",
        "NMT encoding",
        "NMT decoder",
        "KV cache"
      ]
    },
    "publishedAt": "2025-03-09T08:54:05.000Z",
    "title": "Beyond Decoder-only: Large Language Models Can be Good Encoders for\n  Machine Translation",
    "summary": "The field of neural machine translation (NMT) has changed with the advent of\nlarge language models (LLMs). Much of the recent emphasis in natural language\nprocessing (NLP) has been on modeling machine translation and many other\nproblems using a single pre-trained Transformer decoder, while encoder-decoder\narchitectures, which were the standard in earlier NMT models, have received\nrelatively less attention. In this paper, we explore translation models that\nare universal, efficient, and easy to optimize, by marrying the world of LLMs\nwith the world of NMT. We apply LLMs to NMT encoding and leave the NMT decoder\nunchanged. We also develop methods for adapting LLMs to work better with the\nNMT decoder. Furthermore, we construct a new dataset involving multiple tasks\nto assess how well the machine translation system generalizes across various\ntasks. Evaluations on the WMT and our datasets show that results using our\nmethod match or surpass a range of baselines in terms of translation quality,\nbut achieve 2.4 sim 6.5 times inference speedups and a 75% reduction in\nthe memory footprint of the KV cache. It also demonstrates strong\ngeneralization across a variety of translation-related tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.06594.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.06492",
      "authors": [
        {
          "_id": "67cfe557ad91643b5cb7d2c6",
          "user": {
            "_id": "67cd327432668b04f4555270",
            "avatarUrl": "/avatars/15e2cef976cbe05c4c5858c88dccf4af.svg",
            "isPro": false,
            "fullname": "Yanling Wang",
            "user": "WYLing",
            "type": "user"
          },
          "name": "Yanling Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-11T16:09:28.071Z",
          "hidden": false
        },
        {
          "_id": "67cfe557ad91643b5cb7d2c7",
          "name": "Yihan Zhao",
          "hidden": false
        },
        {
          "_id": "67cfe557ad91643b5cb7d2c8",
          "name": "Xiaodong Chen",
          "hidden": false
        },
        {
          "_id": "67cfe557ad91643b5cb7d2c9",
          "name": "Shasha Guo",
          "hidden": false
        },
        {
          "_id": "67cfe557ad91643b5cb7d2ca",
          "name": "Lixin Liu",
          "hidden": false
        },
        {
          "_id": "67cfe557ad91643b5cb7d2cb",
          "name": "Haoyang Li",
          "hidden": false
        },
        {
          "_id": "67cfe557ad91643b5cb7d2cc",
          "name": "Yong Xiao",
          "hidden": false
        },
        {
          "_id": "67cfe557ad91643b5cb7d2cd",
          "name": "Jing Zhang",
          "hidden": false
        },
        {
          "_id": "67cfe557ad91643b5cb7d2ce",
          "name": "Qi Li",
          "hidden": false
        },
        {
          "_id": "67cfe557ad91643b5cb7d2cf",
          "name": "Ke Xu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-09T07:25:32.000Z",
      "title": "VisualSimpleQA : Normes d'Évaluation pour l'Exploration de Faits dans les Questions de Modèles de Vision-Langue de Grande Échelle",
      "summary": "Les modèles de langue visuelle et linguistique (LVLMs) ont montré des résultats impressionnants, mais il reste de nombreux problèmes dans la génération de réponses factuelles face aux requêtes d'exploration de faits. Les actuales cadres d'évaluation d'exploration de faits multimodals se concentrent principalement sur la comparaison entre le sortie du modèle et les réponses réelles, ce qui limite la compréhension spécifique de leur performance. Pour résoudre ce problème, nous présentons le cadre d'évaluation d'exploration de faits multimodal appelé VisualSimpleQA. Ce cadre a deux caractéristiques marquantes : 1. Permet d'évaluer le flux et la charge du décodificateur des modèles visuels et linguistiques de LVLMs. 2. Définit clairement la difficulté et guide l'annotation humaine, en extrayant un sous-ensemble appelé VisualSimpleQA-hard. Selon les expériences avec 15 modèles, y compris des modèles comme GPT-4o, la précision dans l'exploration de faits multimodale de VisualSimpleQA est supérieure à 60%, et pour VisualSimpleQA-hard à 30%. De plus, l'évaluation de la charge du décodificateur montre une grande possibilité d'amélioration dans les deux modules visuel et linguistique. Le dataset est disponible sur https://huggingface.co/datasets/WYLing/VisualSimpleQA.",
      "upvotes": 1,
      "discussionId": "67cfe55bad91643b5cb7d3fb",
      "ai_keywords": [
        "Large vision-language models",
        "fact-seeking question answering",
        "multimodal benchmarks",
        "visual modality",
        "linguistic modality",
        "VisualSimpleQA",
        "VisualSimpleQA-hard",
        "GPT-4",
        "multimodal fact-seeking QA",
        "decoupled evaluation"
      ]
    },
    "publishedAt": "2025-03-09T03:25:32.000Z",
    "title": "VisualSimpleQA: A Benchmark for Decoupled Evaluation of Large\n  Vision-Language Models in Fact-Seeking Question Answering",
    "summary": "Large vision-language models (LVLMs) have demonstrated remarkable\nachievements, yet the generation of non-factual responses remains prevalent in\nfact-seeking question answering (QA). Current multimodal fact-seeking\nbenchmarks primarily focus on comparing model outputs to ground truth answers,\nproviding limited insights into the performance of modality-specific modules.\nTo bridge this gap, we introduce VisualSimpleQA, a multimodal fact-seeking\nbenchmark with two key features. First, it enables streamlined and decoupled\nevaluation of LVLMs in visual and linguistic modalities. Second, it\nincorporates well-defined difficulty criteria to guide human annotation and\nfacilitates the extraction of a challenging subset, VisualSimpleQA-hard.\nExperiments on 15 LVLMs show that even state-of-the-art models such as GPT-4o\nachieve merely 60%+ correctness in multimodal fact-seeking QA on VisualSimpleQA\nand 30%+ on VisualSimpleQA-hard. Furthermore, the decoupled evaluation across\nthese models highlights substantial opportunities for improvement in both\nvisual and linguistic modules. The dataset is available at\nhttps://huggingface.co/datasets/WYLing/VisualSimpleQA.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.06492.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.05860",
      "authors": [
        {
          "_id": "67d0a239967ead9b5aff9883",
          "user": {
            "_id": "655a627aab0644b531a02eb1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/9rW6X1idfx1p5omky67D6.jpeg",
            "isPro": false,
            "fullname": "Roham Koohestani",
            "user": "RohamKoohestani",
            "type": "user"
          },
          "name": "Roham Koohestani",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:41:54.151Z",
          "hidden": false
        },
        {
          "_id": "67d0a239967ead9b5aff9884",
          "user": {
            "_id": "655213d1968a2554a5e8212a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/3XM_b9imWk-pwoueJwAZB.jpeg",
            "isPro": false,
            "fullname": "Philippe de Bekker",
            "user": "philippedebekker",
            "type": "user"
          },
          "name": "Philippe de Bekker",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-11T20:51:06.669Z",
          "hidden": false
        },
        {
          "_id": "67d0a239967ead9b5aff9885",
          "name": "Maliheh Izadi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-07T18:44:32.000Z",
      "title": "Les Marques de Performance des Modèles d'Intelligence Artificielle en Ingénierie Logicielle : Révision, Outils de Recherche et Protocoles d'Extension",
      "summary": "Le benchmark est essentiel pour des évaluations cohérentes et pour garantir la reproductibilité. L'intégration de l'intelligence artificielle (IA) dans le domaine du logiciel (SE) a considérablement augmenté le nombre de benchmarks pour des tâches telles que la génération de code ou la correction d'erreurs. Cependant, cette augmentation a associé avec plusieurs problèmes : 1. la dispersion des connaissances sur les benchmarks pour chaque tâche, 2. la difficulté de sélectionner des benchmarks pertinents, 3. la manque de standards uniformes dans le développement de benchmarks, et 4. les limites actuelles des benchmarks. Dans cet article, 173 études ont été examinées et 204 benchmarks d'IA pour le SE ont été identifiés. Ces benchmarks ont été classifiés, analysés et leurs déficiences pratiques ont été révélées. Sur la base de cette étude, \"BenchScout\" a été développé, une outil de recherche sémantique pour trouver des contextes pertinents en utilisant un clustering automatique. L'accessibilité, l'efficacité et l'intuitivité de BenchScout ont été évaluées avec 22 participants, obtenant des scores moyens de 4,5, 4,0 et 4,1 pour chaque critère. Pour améliorer les normes de benchmarks, BenchFrame est proposé, un méthode unique. Dans un cas d'étude, BenchFrame a été appliqué au benchmark HumanEval, résolvant principalement ses limites. En conséquence, HumanEvalNext a acquis les caractéristiques suivantes : 1. amélioration de la correction d'erreurs, 2. amélioration de la conversion de langage, 3. extension de la couverture des tests, et 4. amélioration du niveau de difficulté. Ensuite, HumanEval, HumanEvalPlus et HumanEvalNext ont été évalués avec 10 modèles de langage de code de pointe. Dans HumanEvalNext, les scores pour HumanEval et HumanEvalPlus ont diminué à 31,22% et 19,94% respectivement.",
      "upvotes": 1,
      "discussionId": "67d0a23a967ead9b5aff98da",
      "projectPage": "https://evalpro.online/",
      "githubRepo": "https://github.com/AISE-TUDelft/AI4SE-benchmarks",
      "ai_keywords": [
        "AI4SE (Artificial Intelligence in Software Engineering)",
        "BenchScout",
        "semantic search tool",
        "automated clustering",
        "BenchFrame",
        "HumanEval",
        "HumanEvalNext",
        "pass@1 score"
      ]
    },
    "publishedAt": "2025-03-07T13:44:32.000Z",
    "title": "Benchmarking AI Models in Software Engineering: A Review, Search Tool,\n  and Enhancement Protocol",
    "summary": "Benchmarks are essential for consistent evaluation and reproducibility. The\nintegration of Artificial Intelligence into Software Engineering (AI4SE) has\ngiven rise to numerous benchmarks for tasks such as code generation and bug\nfixing. However, this surge presents challenges: (1) scattered benchmark\nknowledge across tasks, (2) difficulty in selecting relevant benchmarks, (3)\nthe absence of a uniform standard for benchmark development, and (4)\nlimitations of existing benchmarks. In this paper, we review 173 studies and\nidentify 204 AI4SE benchmarks. We classify these benchmarks, analyze their\nlimitations, and expose gaps in practices. Based on our review, we created\nBenchScout, a semantic search tool to find relevant benchmarks, using automated\nclustering of the contexts from associated studies. We conducted a user study\nwith 22 participants to evaluate BenchScout's usability, effectiveness, and\nintuitiveness which resulted in average scores of 4.5, 4.0, and 4.1 out of 5.\nTo advance benchmarking standards, we propose BenchFrame, a unified method to\nenhance benchmark quality. As a case study, we applied BenchFrame to the\nHumanEval benchmark and addressed its main limitations. This led to\nHumanEvalNext, featuring (1) corrected errors, (2) improved language\nconversion, (3) expanded test coverage, and (4) increased difficulty. We then\nevaluated ten state-of-the-art code language models on HumanEval,\nHumanEvalPlus, and HumanEvalNext. On HumanEvalNext, models showed a pass@1\nscore reduction of 31.22% and 19.94% compared to HumanEval and HumanEvalPlus,\nrespectively.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05860.png",
    "numComments": 1,
    "isAuthorParticipating": true
  }
]