[
  {
    "paper": {
      "id": "2504.10514",
      "authors": [
        {
          "_id": "67ffedb8b0c26d6ec0b608cf",
          "name": "Yijun Liang",
          "hidden": false
        },
        {
          "_id": "67ffedb8b0c26d6ec0b608d0",
          "name": "Ming Li",
          "hidden": false
        },
        {
          "_id": "67ffedb8b0c26d6ec0b608d1",
          "user": {
            "_id": "64a8121e35fab7cd04c30ed0",
            "avatarUrl": "/avatars/48849b84703158772f1022932331b143.svg",
            "isPro": false,
            "fullname": "Chenrui Fan",
            "user": "Fcr09",
            "type": "user"
          },
          "name": "Chenrui Fan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-17T08:05:05.662Z",
          "hidden": false
        },
        {
          "_id": "67ffedb8b0c26d6ec0b608d2",
          "name": "Ziyue Li",
          "hidden": false
        },
        {
          "_id": "67ffedb8b0c26d6ec0b608d3",
          "name": "Dang Nguyen",
          "hidden": false
        },
        {
          "_id": "67ffedb8b0c26d6ec0b608d4",
          "user": {
            "_id": "63f546e0fcf95ecac2b0ee3e",
            "avatarUrl": "/avatars/02a401bcff91cc473d9946bbb771a985.svg",
            "isPro": false,
            "fullname": "Kwesi Cobbina",
            "user": "kweCobi",
            "type": "user"
          },
          "name": "Kwesi Cobbina",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:08:36.225Z",
          "hidden": false
        },
        {
          "_id": "67ffedb8b0c26d6ec0b608d5",
          "user": {
            "_id": "639d4b8d860db464ae35c3ab",
            "avatarUrl": "/avatars/ec0fa3e91593a03fc9fb611e66b30553.svg",
            "isPro": false,
            "fullname": "Shweta Bhardwaj",
            "user": "shweta12",
            "type": "user"
          },
          "name": "Shweta Bhardwaj",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:08:29.680Z",
          "hidden": false
        },
        {
          "_id": "67ffedb8b0c26d6ec0b608d6",
          "user": {
            "_id": "6393847e3e30234ae798b7be",
            "avatarUrl": "/avatars/daeb8c37dff4432d837a69b87c196521.svg",
            "isPro": true,
            "fullname": "JiuhaiChen",
            "user": "jiuhai",
            "type": "user"
          },
          "name": "Jiuhai Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:08:22.786Z",
          "hidden": false
        },
        {
          "_id": "67ffedb8b0c26d6ec0b608d7",
          "name": "Fuxiao Liu",
          "hidden": false
        },
        {
          "_id": "67ffedb8b0c26d6ec0b608d8",
          "user": {
            "_id": "647f5af5b0e96764589f3b2a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
            "isPro": false,
            "fullname": "Tianyi Zhou",
            "user": "zhoutianyi",
            "type": "user"
          },
          "name": "Tianyi Zhou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-17T08:05:07.396Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-10T16:36:26.000Z",
      "submittedOnDailyAt": "2025-04-17T00:58:33.032Z",
      "title": "ColorBench : Les VLMs peuvent voir et comprendre le monde des couleurs ? Prédiction du couleur, théorie et critères de robustesse d'évaluation",
      "submittedOnDailyBy": {
        "_id": "647f5af5b0e96764589f3b2a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
        "isPro": false,
        "fullname": "Tianyi Zhou",
        "user": "zhoutianyi",
        "type": "user"
      },
      "summary": "Le couleur joue un rôle crucial dans la perception humaine et généralement fournit une importante orientation dans les inférences visuelles. Cependant, il n'est pas clair si les modèles de langage visuel (VLMs) peuvent reconnaître et comprendre le couleur de la même manière que l'humain. Dans cet article, nous présentons un nouveau benchmark appelé \"ColorBench\" pour évaluer la capacité de reconnaissance de couleurs. Ce benchmark a été conçu en détail pour évaluer la compréhension du couleur, la justification de la perception et la cohérence dans la transformation des couleurs. Des scénarios de test différents ont été sélectionnés et des données ont été collectées sur la base d'applications réelles pour évaluer comment ces modèles reconnaissent le couleur, infèrent le sens par le biais de codes de couleur et maintiennent un rendement constant dans la transformation des couleurs. Composé de 32 VLMs, chacun constitué de différents modèles de langage et de codificateurs visuels, des évaluations détaillées ont été effectuées, et dans cet article, nous soulignons les nouveaux résultats suivants : (i) l'échelle (les modèles grands sont meilleurs) est maintenue dans ColorBench, et le modèle de langage joue un rôle plus important que celui du codificateur visuel. (ii) Cependant, la différence de performance entre les modèles est relativement faible, démontrant que les VLMs actuels ont atteint un bon rendement dans la compréhension du couleur. (iii) L'inférence de type CoT améliore la précision et la robustesse de la compréhension du couleur, mais c'est un processus cognitif. (iv) Les VLMs utilisent des codes de couleur dans ColorBench, mais parfois ces codes peuvent conduire à des erreurs dans le modèle. Ces résultats clairement démontrent les limites importantes des VLMs actuels et soulignent la nécessité d'améliorer la compréhension du couleur. ColorBench serve de outil fondamental pour encourager la recherche à l'échelle humaine dans la compréhension du couleur de différentes IAs.",
      "upvotes": 18,
      "discussionId": "67ffedbeb0c26d6ec0b60a5b",
      "projectPage": "https://huggingface.co/datasets/umd-zhou-lab/ColorBench",
      "githubRepo": "https://github.com/tianyi-lab/ColorBench",
      "ai_keywords": [
        "vision-language models (VLMs)",
        "color understanding",
        "color perception",
        "color-based cues",
        "color transformations",
        "scaling law",
        "language model",
        "vision encoder",
        "CoT reasoning",
        "multimodal AI"
      ]
    },
    "publishedAt": "2025-04-10T12:36:26.000Z",
    "title": "ColorBench: Can VLMs See and Understand the Colorful World? A\n  Comprehensive Benchmark for Color Perception, Reasoning, and Robustness",
    "summary": "Color plays an important role in human perception and usually provides\ncritical clues in visual reasoning. However, it is unclear whether and how\nvision-language models (VLMs) can perceive, understand, and leverage color as\nhumans. This paper introduces ColorBench, an innovative benchmark meticulously\ncrafted to assess the capabilities of VLMs in color understanding, including\ncolor perception, reasoning, and robustness. By curating a suite of diverse\ntest scenarios, with grounding in real applications, ColorBench evaluates how\nthese models perceive colors, infer meanings from color-based cues, and\nmaintain consistent performance under varying color transformations. Through an\nextensive evaluation of 32 VLMs with varying language models and vision\nencoders, our paper reveals some undiscovered findings: (i) The scaling law\n(larger models are better) still holds on ColorBench, while the language model\nplays a more important role than the vision encoder. (ii) However, the\nperformance gaps across models are relatively small, indicating that color\nunderstanding has been largely neglected by existing VLMs. (iii) CoT reasoning\nimproves color understanding accuracies and robustness, though they are\nvision-centric tasks. (iv) Color clues are indeed leveraged by VLMs on\nColorBench but they can also mislead models in some tasks. These findings\nhighlight the critical limitations of current VLMs and underscore the need to\nenhance color comprehension. Our ColorBenchcan serve as a foundational tool for\nadvancing the study of human-level color understanding of multimodal AI.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10514.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "647f5af5b0e96764589f3b2a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
      "fullname": "Tianyi Zhou",
      "name": "zhoutianyi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.12240",
      "authors": [
        {
          "_id": "680057b49031335df49732fc",
          "user": {
            "_id": "64970d3d9c3b29dca8633f87",
            "avatarUrl": "/avatars/11e3c9c66d28490d6d09925f9aa47cd1.svg",
            "isPro": false,
            "fullname": "JunhaoZhuang",
            "user": "JunhaoZhuang",
            "type": "user"
          },
          "name": "Junhao Zhuang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-17T08:04:09.222Z",
          "hidden": false
        },
        {
          "_id": "680057b49031335df49732fd",
          "user": {
            "_id": "66837d3c48edefb453b0640a",
            "avatarUrl": "/avatars/b16385eaa612578728e2c6460a76b38f.svg",
            "isPro": false,
            "fullname": "Lingen Li",
            "user": "l-li",
            "type": "user"
          },
          "name": "Lingen Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:09:48.492Z",
          "hidden": false
        },
        {
          "_id": "680057b49031335df49732fe",
          "user": {
            "_id": "62d4577bc85b0fcf7fde39bb",
            "avatarUrl": "/avatars/a3a5729e33ae89ce9ba408830db3c835.svg",
            "isPro": false,
            "fullname": "Xuan Ju",
            "user": "juxuan27",
            "type": "user"
          },
          "name": "Xuan Ju",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:09:55.870Z",
          "hidden": false
        },
        {
          "_id": "680057b49031335df49732ff",
          "name": "Zhaoyang Zhang",
          "hidden": false
        },
        {
          "_id": "680057b49031335df4973300",
          "name": "Chun Yuan",
          "hidden": false
        },
        {
          "_id": "680057b49031335df4973301",
          "user": {
            "_id": "63ca3ddc04c979828310bfcb",
            "avatarUrl": "/avatars/615e0d8622950b4408b40d550f02a894.svg",
            "isPro": false,
            "fullname": "Ying Shan",
            "user": "yshan2u",
            "type": "user"
          },
          "name": "Ying Shan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:12:47.014Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64970d3d9c3b29dca8633f87/VweYE_xmPVmixUo3dh0Wu.png",
        "https://cdn-uploads.huggingface.co/production/uploads/64970d3d9c3b29dca8633f87/J9PY5rVuEtD_-ZLibCR1W.png"
      ],
      "publishedAt": "2025-04-16T16:45:19.000Z",
      "submittedOnDailyAt": "2025-04-17T00:32:24.205Z",
      "title": "Cobra : Technique de peinture de design efficace en utilisant des références larges",
      "submittedOnDailyBy": {
        "_id": "64970d3d9c3b29dca8633f87",
        "avatarUrl": "/avatars/11e3c9c66d28490d6d09925f9aa47cd1.svg",
        "isPro": false,
        "fullname": "JunhaoZhuang",
        "user": "JunhaoZhuang",
        "type": "user"
      },
      "summary": "Dans le secteur de l'industrie des bandes dessinées, des critères d'exactitude élevée, d'efficacité, de cohérence du contexte et de contrôle flexible sont requis pour le coloriage des pages. Une page de bande dessinée comprend divers personnages, objets et fonds, ce qui rend le processus de coloriage complexe. Bien que des avancées aient été réalisées dans les modèles de diffusion pour la génération d'images, leur application au coloriage des bandes dessinées est limitée. L'application des modèles de diffusion est limitée par le fait que ces modèles ne s'adaptent pas facilement aux exigences spécifiques du coloriage des bandes dessinées, qui requièrent un niveau de détail et de cohérence que les modèles de diffusion parfois ne peuvent atteindre.",
      "upvotes": 16,
      "discussionId": "680057b89031335df497343e",
      "projectPage": "https://zhuang2002.github.io/Cobra/",
      "githubRepo": "https://github.com/zhuang2002/Cobra",
      "ai_keywords": [
        "diffusion models",
        "line art colorization",
        "contextual image guidance",
        "color hints",
        "Causal Sparse DiT architecture",
        "positional encodings",
        "causal sparse attention",
        "Key-Value Cache",
        "long-context references",
        "color identity consistency"
      ]
    },
    "publishedAt": "2025-04-16T12:45:19.000Z",
    "title": "Cobra: Efficient Line Art COlorization with BRoAder References",
    "summary": "The comic production industry requires reference-based line art colorization\nwith high accuracy, efficiency, contextual consistency, and flexible control. A\ncomic page often involves diverse characters, objects, and backgrounds, which\ncomplicates the coloring process. Despite advancements in diffusion models for\nimage generation, their application in line art colorization remains limited,\nfacing challenges related to handling extensive reference images,\ntime-consuming inference, and flexible control. We investigate the necessity of\nextensive contextual image guidance on the quality of line art colorization. To\naddress these challenges, we introduce Cobra, an efficient and versatile method\nthat supports color hints and utilizes over 200 reference images while\nmaintaining low latency. Central to Cobra is a Causal Sparse DiT architecture,\nwhich leverages specially designed positional encodings, causal sparse\nattention, and Key-Value Cache to effectively manage long-context references\nand ensure color identity consistency. Results demonstrate that Cobra achieves\naccurate line art colorization through extensive contextual reference,\nsignificantly enhancing inference speed and interactivity, thereby meeting\ncritical industrial demands. We release our codes and models on our project\npage: https://zhuang2002.github.io/Cobra/.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64970d3d9c3b29dca8633f87/VweYE_xmPVmixUo3dh0Wu.png",
      "https://cdn-uploads.huggingface.co/production/uploads/64970d3d9c3b29dca8633f87/J9PY5rVuEtD_-ZLibCR1W.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.12240.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64970d3d9c3b29dca8633f87",
      "avatarUrl": "/avatars/11e3c9c66d28490d6d09925f9aa47cd1.svg",
      "fullname": "JunhaoZhuang",
      "name": "JunhaoZhuang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 32
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.12285",
      "authors": [
        {
          "_id": "68006e6e175c8dce4ec17f7a",
          "user": {
            "_id": "613f07f40153aafa379775f2",
            "avatarUrl": "/avatars/3965175b320d753d9a5ccb0c7d9298a4.svg",
            "isPro": false,
            "fullname": "Shuming Ma",
            "user": "shumingma",
            "type": "user"
          },
          "name": "Shuming Ma",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:13:17.676Z",
          "hidden": false
        },
        {
          "_id": "68006e6e175c8dce4ec17f7b",
          "user": {
            "_id": "63f71771d36951307fcb4dcd",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f71771d36951307fcb4dcd/1c-GSQmSSuDXyVWjxZFy_.jpeg",
            "isPro": false,
            "fullname": "Hongyu Wang",
            "user": "hongyuw",
            "type": "user"
          },
          "name": "Hongyu Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:14:45.597Z",
          "hidden": false
        },
        {
          "_id": "68006e6e175c8dce4ec17f7c",
          "user": {
            "_id": "632bd2f72d6a805eeb4bc601",
            "avatarUrl": "/avatars/6e1533e8a599f3068290aa69ac82cab7.svg",
            "isPro": false,
            "fullname": "HUANG SHAOHAN",
            "user": "buaahsh",
            "type": "user"
          },
          "name": "Shaohan Huang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:14:59.750Z",
          "hidden": false
        },
        {
          "_id": "68006e6e175c8dce4ec17f7d",
          "user": {
            "_id": "64abbcff6cadc7aca584f71b",
            "avatarUrl": "/avatars/fc6e85ad4a8befd133a37b411712c648.svg",
            "isPro": false,
            "fullname": "Xingxing Zhang",
            "user": "THU-CHUNXIA",
            "type": "user"
          },
          "name": "Xingxing Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:15:06.359Z",
          "hidden": false
        },
        {
          "_id": "68006e6e175c8dce4ec17f7e",
          "name": "Ying Hu",
          "hidden": false
        },
        {
          "_id": "68006e6e175c8dce4ec17f7f",
          "name": "Ting Song",
          "hidden": false
        },
        {
          "_id": "68006e6e175c8dce4ec17f80",
          "name": "Yan Xia",
          "hidden": false
        },
        {
          "_id": "68006e6e175c8dce4ec17f81",
          "user": {
            "_id": "6368c512fbfe97c16a40baba",
            "avatarUrl": "/avatars/1c23bc7c0b6d9225699ce27647623d7a.svg",
            "isPro": false,
            "fullname": "Furu Wei",
            "user": "thegenerality",
            "type": "user"
          },
          "name": "Furu Wei",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:15:42.250Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-16T17:51:43.000Z",
      "submittedOnDailyAt": "2025-04-17T01:29:56.744Z",
      "title": "BitNet b1.58 2B4T Informe Technique",
      "submittedOnDailyBy": {
        "_id": "63f71771d36951307fcb4dcd",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f71771d36951307fcb4dcd/1c-GSQmSSuDXyVWjxZFy_.jpeg",
        "isPro": false,
        "fullname": "Hongyu Wang",
        "user": "hongyuw",
        "type": "user"
      },
      "summary": "Bienet b1.58 2B4T introduit le premier modèle de langage large (LLM) d'origine primaire et d'accès libre avec un seul bit. Ce modèle dispose d'une échelle de 2 milliards de paramètres et a été entraîné sur un corpus de 400 milliards de tokens. Le modèle a publié des référentiels pour la compréhension linguistique, l'inférence mathématique, les capacités de programmation et les habiletés de dialogue, et a été soumis à une évaluation rigoureuse. Nos résultats montrent que bienet b1.58 2B4T atteint le rendement des meilleurs modèles de langage large d'accès libre de la même échelle, offrant une grande avantage en termes d'efficacité de calcul. En particulier, il réduit significativement l'utilisation de la mémoire, le consommation d'énergie et le temps de décision. De plus, pour encourager la recherche et l'adoption, les poids du modèle sont disponibles sur Hugging Face, et des implémentations ouvertes d'inférence sont également fournies pour deux architectures de GPU et de CPU.",
      "upvotes": 12,
      "discussionId": "68006e70175c8dce4ec17fc0",
      "ai_keywords": [
        "BitNet b1.58 2B4T",
        "Large Language Model (LLM)",
        "1-bit Large Language Model",
        "Train",
        "Corpus",
        "4 trillion tokens",
        "Benchmarks",
        "Language understanding",
        "Mathematical reasoning",
        "Coding proficiency",
        "Conversational ability"
      ]
    },
    "publishedAt": "2025-04-16T13:51:43.000Z",
    "title": "BitNet b1.58 2B4T Technical Report",
    "summary": "We introduce BitNet b1.58 2B4T, the first open-source, native 1-bit Large\nLanguage Model (LLM) at the 2-billion parameter scale. Trained on a corpus of 4\ntrillion tokens, the model has been rigorously evaluated across benchmarks\ncovering language understanding, mathematical reasoning, coding proficiency,\nand conversational ability. Our results demonstrate that BitNet b1.58 2B4T\nachieves performance on par with leading open-weight, full-precision LLMs of\nsimilar size, while offering significant advantages in computational\nefficiency, including substantially reduced memory footprint, energy\nconsumption, and decoding latency. To facilitate further research and adoption,\nthe model weights are released via Hugging Face along with open-source\ninference implementations for both GPU and CPU architectures.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.12285.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63f71771d36951307fcb4dcd",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f71771d36951307fcb4dcd/1c-GSQmSSuDXyVWjxZFy_.jpeg",
      "fullname": "Hongyu Wang",
      "name": "hongyuw",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 13
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.10326",
      "authors": [
        {
          "_id": "67ff772061373fdf16ce1d38",
          "user": {
            "_id": "66486ba1640bc89c93bcc8a2",
            "avatarUrl": "/avatars/07e07bd768339495e55b7b17f2f8bc59.svg",
            "isPro": false,
            "fullname": "Yangshen Deng",
            "user": "YangshenDeng",
            "type": "user"
          },
          "name": "Yangshen Deng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-16T11:57:58.302Z",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d39",
          "name": "Zhengxin You",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d3a",
          "name": "Long Xiang",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d3b",
          "user": {
            "_id": "66d6be0bddf54fd90923c727",
            "avatarUrl": "/avatars/7bb82c8c339db944d79d47b3b9b35aa8.svg",
            "isPro": false,
            "fullname": "Li",
            "user": "Qilong00",
            "type": "user"
          },
          "name": "Qilong Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:16:16.665Z",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d3c",
          "user": {
            "_id": "66efe50667c4ce2c9024cd45",
            "avatarUrl": "/avatars/7b7f650953c371f08a5beecc500b6a43.svg",
            "isPro": false,
            "fullname": "peiqiyuan",
            "user": "YuanPeiqi",
            "type": "user"
          },
          "name": "Peiqi Yuan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:16:22.829Z",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d3d",
          "name": "Zhaoyang Hong",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d3e",
          "user": {
            "_id": "66d6c339b61dd11022907252",
            "avatarUrl": "/avatars/d2ed3cc003e94b2e5204ce0f8a481dcf.svg",
            "isPro": false,
            "fullname": "Yitao Zheng",
            "user": "FeTieTer",
            "type": "user"
          },
          "name": "Yitao Zheng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:16:36.808Z",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d3f",
          "name": "Wanting Li",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d40",
          "user": {
            "_id": "671a7bce2b10d343bab18637",
            "avatarUrl": "/avatars/af1c10a59236d953b42e67d3955eecc4.svg",
            "isPro": false,
            "fullname": "runzhong",
            "user": "runzhongli",
            "type": "user"
          },
          "name": "Runzhong Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:17:02.031Z",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d41",
          "user": {
            "_id": "63898b61ec1f539adc0f4da2",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674248167280-63898b61ec1f539adc0f4da2.jpeg",
            "isPro": false,
            "fullname": "Haotian Liu",
            "user": "liuhaotian",
            "type": "user"
          },
          "name": "Haotian Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:17:25.728Z",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d42",
          "name": "Kyriakos Mouratidis",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d43",
          "name": "Man Lung Yiu",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d44",
          "name": "Huan Li",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d45",
          "name": "Qiaomu Shen",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d46",
          "name": "Rui Mao",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d47",
          "user": {
            "_id": "66b02a2642c34e7a212133c0",
            "avatarUrl": "/avatars/737a69b095e8c427ecd08f870b173635.svg",
            "isPro": false,
            "fullname": "Bo Tang",
            "user": "BO1022",
            "type": "user"
          },
          "name": "Bo Tang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:18:01.460Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-14T15:34:26.000Z",
      "submittedOnDailyAt": "2025-04-17T06:00:20.705Z",
      "title": "AlayaDB : Inférence efficace et efficace basée sur les données pour le contexte long des modèles de langue LLM",
      "submittedOnDailyBy": {
        "_id": "66486ba1640bc89c93bcc8a2",
        "avatarUrl": "/avatars/07e07bd768339495e55b7b17f2f8bc59.svg",
        "isPro": false,
        "fullname": "Yangshen Deng",
        "user": "YangshenDeng",
        "type": "user"
      },
      "summary": "AlibabaDB est un système de base de données vectorielle de la dernière génération conçu pour exécuter efficacement et de manière efficace l'inférence de modèles de langage de grande échelle (LLMs) avec des contextes longs. En particulier, la cache de KV et le calcul de l'attention dans le système d'inférence des LLMs sont séparés et intégrés dans le nouveau système de base de données vectorielle. En comparaison avec les solutions actuelles (par exemple, l'isolation de la cache de KV et le calcul de l'attention basé sur la recherche), AlibabaDB offre un consommation de ressources plus faible et une génération de haute qualité pour des chargements de travail et des objectifs de niveau de service (SLOs) variés. La clé d'AlibabaDB est l'abstraction des calculs d'attention et la gestion de la cache dans le processus de traitement des requêtes, ce qui optimise le rendement avec des opérateurs de requêtes naturels. Dans cet article, trois études de cas et des résultats d'expériences prolongées de benchmarking d'inférence des LLMs sont présentés, démontrant l'efficacité d'AlibabaDB.",
      "upvotes": 12,
      "discussionId": "67ff772261373fdf16ce1d93",
      "ai_keywords": [
        "vector database",
        "long-context inference",
        "Large Language Models (LLMs)",
        "KV cache",
        "attention computation",
        "Model as a Service (MaaS)",
        "Service Level Objectives (SLOs)",
        "KV cache disaggregation",
        "retrieval-based sparse attention",
        "query processing procedure",
        "native query optimizer",
        "LLM inference benchmarks"
      ]
    },
    "publishedAt": "2025-04-14T11:34:26.000Z",
    "title": "AlayaDB: The Data Foundation for Efficient and Effective Long-context\n  LLM Inference",
    "summary": "AlayaDB is a cutting-edge vector database system natively architected for\nefficient and effective long-context inference for Large Language Models (LLMs)\nat AlayaDB AI. Specifically, it decouples the KV cache and attention\ncomputation from the LLM inference systems, and encapsulates them into a novel\nvector database system. For the Model as a Service providers (MaaS), AlayaDB\nconsumes fewer hardware resources and offers higher generation quality for\nvarious workloads with different kinds of Service Level Objectives (SLOs), when\ncomparing with the existing alternative solutions (e.g., KV cache\ndisaggregation, retrieval-based sparse attention). The crux of AlayaDB is that\nit abstracts the attention computation and cache management for LLM inference\ninto a query processing procedure, and optimizes the performance via a native\nquery optimizer. In this work, we demonstrate the effectiveness of AlayaDB via\n(i) three use cases from our industry partners, and (ii) extensive experimental\nresults on LLM inference benchmarks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10326.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66486ba1640bc89c93bcc8a2",
      "avatarUrl": "/avatars/07e07bd768339495e55b7b17f2f8bc59.svg",
      "fullname": "Yangshen Deng",
      "name": "YangshenDeng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.09081",
      "authors": [
        {
          "_id": "67fdbfcccaa65039e8c3d8ae",
          "user": {
            "_id": "67dcd93f73e2178fe917b893",
            "avatarUrl": "/avatars/86ec25d32c45da3937280ef9a619f19e.svg",
            "isPro": false,
            "fullname": "Prabhat Pandey",
            "user": "panprabh",
            "type": "user"
          },
          "name": "Prabhat Pandey",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-16T09:23:52.370Z",
          "hidden": false
        },
        {
          "_id": "67fdbfcccaa65039e8c3d8af",
          "name": "Rupak Vignesh Swaminathan",
          "hidden": false
        },
        {
          "_id": "67fdbfcccaa65039e8c3d8b0",
          "user": {
            "_id": "66279810009f1bfdc6bf71bf",
            "avatarUrl": "/avatars/97f561c91b92e1abb4fe6f0b5c688126.svg",
            "isPro": false,
            "fullname": "Girish",
            "user": "vijaygirish2001",
            "type": "user"
          },
          "name": "K V Vijay Girish",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-16T09:23:49.809Z",
          "hidden": false
        },
        {
          "_id": "67fdbfcccaa65039e8c3d8b1",
          "user": {
            "_id": "66367dfb2d6b86ff193dbbe0",
            "avatarUrl": "/avatars/0a0c230bb5fc81a28a166691146cf807.svg",
            "isPro": false,
            "fullname": "Arunasish Sen",
            "user": "svinxz",
            "type": "user"
          },
          "name": "Arunasish Sen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-16T11:58:01.767Z",
          "hidden": false
        },
        {
          "_id": "67fdbfcccaa65039e8c3d8b2",
          "name": "Jian Xie",
          "hidden": false
        },
        {
          "_id": "67fdbfcccaa65039e8c3d8b3",
          "name": "Grant P. Strimel",
          "hidden": false
        },
        {
          "_id": "67fdbfcccaa65039e8c3d8b4",
          "name": "Andreas Schwarz",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-12T04:45:48.000Z",
      "submittedOnDailyAt": "2025-04-17T07:32:43.212Z",
      "title": "SIFT-50M : Réajustement en grand ensemble de données multilingues pour des commandes vocales",
      "submittedOnDailyBy": {
        "_id": "67dcd93f73e2178fe917b893",
        "avatarUrl": "/avatars/86ec25d32c45da3937280ef9a619f19e.svg",
        "isPro": false,
        "fullname": "Prabhat Pandey",
        "user": "panprabh",
        "type": "user"
      },
      "summary": "SIFT (Fine-tuning Instruction) est apparu. SIFT-50M est un ensemble de données de 50M qui sont adaptés au fine-tuning d'instructions et au pré-entraînement de grands modèles de langage de parole et de texte (LLMs). Construit comme un corpus de parole public, SIFT-50M comprend 14 000 heures de parole et est utilisé avec des modèles professionnels de parole et de texte. Cet ensemble de données a été élargi dans 5 langues et inclut différents types d'instructions pour comprendre et contrôler la parole. En utilisant SIFT-50M, SIFT-LLM a été entraîné, dépassant les LLMs actuels de parole et de texte dans le cadre de tests d'instructions suivies et montrant des résultats exceptionnels dans des tâches de base de parole. De plus, EvalSIFT est présenté pour évaluer la capacité de suivi d'instructions dans les LLMs de parole et de texte, conçu spécifiquement pour cette finalité.",
      "upvotes": 9,
      "discussionId": "67fdbfe0caa65039e8c3de4b",
      "ai_keywords": [
        "SIFT (Speech Instruction Fine-Tuning)",
        "speech-text large language models (LLMs)",
        "instruction fine-tuning",
        "pre-training",
        "speech corpora",
        "off-the-shelf expert models",
        "speech understanding",
        "controllable speech generation",
        "SIFT-LLM",
        "instruction-following benchmarks",
        "foundational speech tasks",
        "EvalSIFT"
      ]
    },
    "publishedAt": "2025-04-12T00:45:48.000Z",
    "title": "SIFT-50M: A Large-Scale Multilingual Dataset for Speech Instruction\n  Fine-Tuning",
    "summary": "We introduce SIFT (Speech Instruction Fine-Tuning), a 50M-example dataset\ndesigned for instruction fine-tuning and pre-training of speech-text large\nlanguage models (LLMs). SIFT-50M is built from publicly available speech\ncorpora, which collectively contain 14K hours of speech, and leverages LLMs\nalong with off-the-shelf expert models. The dataset spans five languages,\nencompassing a diverse range of speech understanding as well as controllable\nspeech generation instructions. Using SIFT-50M, we train SIFT-LLM, which\noutperforms existing speech-text LLMs on instruction-following benchmarks while\nachieving competitive performance on foundational speech tasks. To support\nfurther research, we also introduce EvalSIFT, a benchmark dataset specifically\ndesigned to evaluate the instruction-following capabilities of speech-text\nLLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.09081.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67dcd93f73e2178fe917b893",
      "avatarUrl": "/avatars/86ec25d32c45da3937280ef9a619f19e.svg",
      "fullname": "Prabhat Pandey",
      "name": "panprabh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.10483",
      "authors": [
        {
          "_id": "67fdd3e3913c97aa32f94e9b",
          "user": {
            "_id": "641480554b1701c01cdb36c4",
            "avatarUrl": "/avatars/f1f6b294e0236d76a68c099164c81f36.svg",
            "isPro": false,
            "fullname": "Xingjian Leng",
            "user": "xingjianleng",
            "type": "user"
          },
          "name": "Xingjian Leng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-16T09:23:36.449Z",
          "hidden": false
        },
        {
          "_id": "67fdd3e3913c97aa32f94e9c",
          "name": "Jaskirat Singh",
          "hidden": false
        },
        {
          "_id": "67fdd3e3913c97aa32f94e9d",
          "user": {
            "_id": "6752870ec63bc5b670b1b27e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6752870ec63bc5b670b1b27e/3CdHxnyTKbGup-1V67nEV.jpeg",
            "isPro": false,
            "fullname": "Yunzhong Hou",
            "user": "yunzhong-hou",
            "type": "user"
          },
          "name": "Yunzhong Hou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:34:11.946Z",
          "hidden": false
        },
        {
          "_id": "67fdd3e3913c97aa32f94e9e",
          "user": {
            "_id": "63bf533f4a2beec65568f813",
            "avatarUrl": "/avatars/bba583653b5cada8dd4a3ff2281e9dec.svg",
            "isPro": false,
            "fullname": "Xing",
            "user": "Zhenchang",
            "type": "user"
          },
          "name": "Zhenchang Xing",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:34:21.190Z",
          "hidden": false
        },
        {
          "_id": "67fdd3e3913c97aa32f94e9f",
          "user": {
            "_id": "6596422646624a86ff3b3bda",
            "avatarUrl": "/avatars/216e12b77e45ac5f1fa20932f5745411.svg",
            "isPro": false,
            "fullname": "Saining Xie",
            "user": "sainx",
            "type": "user"
          },
          "name": "Saining Xie",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:34:39.700Z",
          "hidden": false
        },
        {
          "_id": "67fdd3e3913c97aa32f94ea0",
          "user": {
            "_id": "666351ebd86c026caa135e5c",
            "avatarUrl": "/avatars/50a37f7e999f660c69f518b71577eb7d.svg",
            "isPro": false,
            "fullname": "Liang Zheng",
            "user": "liangzheng06",
            "type": "user"
          },
          "name": "Liang Zheng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:34:56.041Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-14T17:59:53.000Z",
      "submittedOnDailyAt": "2025-04-17T05:08:17.990Z",
      "title": "REPA-E : Méthode pour libérer la VAE dans l'ajustement à l'extrémité à l'extrémité en utilisant des diffuseurs potentiels\nTransformers\n\n(注意 : ici, \"Transformers\" est traduit directement par \"Transformers\" en français, car \"Transformers\" est le terme courant en français pour ce concept. Si un traduction plus formelle ou académique est souhaitée, on pourrait considérer \"Modèles de Transformers\" ou \"Techniques de Transformers\".)",
      "submittedOnDailyBy": {
        "_id": "5f1158120c833276f61f1a84",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
        "isPro": false,
        "fullname": "Niels Rogge",
        "user": "nielsr",
        "type": "user"
      },
      "summary": "Cet article traite d'un problème fondamental : \"Est-il possible d'entraîner simultanément un modèle de diffusion potentiel et un tokenisateur d'autoencodeur (VAE) de manière intégrale de la fin à la fin ?\" Le savoir traditionnel de l'apprentissage profond suggère qu'il est mieux d'entraîner de manière intégrale de la fin à la fin. Cependant, dans le cas d'un transformateur de diffusion potentiel, il a été démontré que l'entraînement de manière intégrale de la fin à la fin en utilisant la perte de diffusion standard avec VAE et modèle de diffusion entraîne une perte de performance finale. Nous montrons que la perte de diffusion n'est pas efficace et que l'utilisation de la perte de REPA pour entraîner de manière intégrale est possible. Le processus d'entraînement simple de REPA-E peut entraîner le modèle de diffusion 17 fois et 45 fois plus rapidement que le processus d'entraînement de REPA et de BERT. Intéressamment, l'entraînement de manière intégrale en REPA-E améliore également le VAE lui-même, améliorant la structure de l'espace potentiel et le rendement de génération dans le flux descendant. En termes de rendement final, notre approche atteint un nouvel état de l'art. Sur ImageNet 256 x 256, sans un générateur de jeu de classe et avec un générateur de jeu de classe, on atteint les valeurs de FID de 1,26 et 1,83, respectivement. Le code est disponible sur https://end2end-diffusion.github.io.",
      "upvotes": 3,
      "discussionId": "67fdd3e5913c97aa32f94ee3",
      "projectPage": "https://end2end-diffusion.github.io/",
      "githubRepo": "https://github.com/End2End-Diffusion/REPA-E",
      "ai_keywords": [
        "latent diffusion models",
        "variational auto-encoder (VAE) tokenizer",
        "end-to-end training",
        "diffusion-loss",
        "representation-alignment (REPA) loss",
        "diffusion model training",
        "VAE",
        "latent space structure",
        "downstream generation performance",
        "FID",
        "ImageNet 256 x 256"
      ]
    },
    "publishedAt": "2025-04-14T13:59:53.000Z",
    "title": "REPA-E: Unlocking VAE for End-to-End Tuning with Latent Diffusion\n  Transformers",
    "summary": "In this paper we tackle a fundamental question: \"Can we train latent\ndiffusion models together with the variational auto-encoder (VAE) tokenizer in\nan end-to-end manner?\" Traditional deep-learning wisdom dictates that\nend-to-end training is often preferable when possible. However, for latent\ndiffusion transformers, it is observed that end-to-end training both VAE and\ndiffusion-model using standard diffusion-loss is ineffective, even causing a\ndegradation in final performance. We show that while diffusion loss is\nineffective, end-to-end training can be unlocked through the\nrepresentation-alignment (REPA) loss -- allowing both VAE and diffusion model\nto be jointly tuned during the training process. Despite its simplicity, the\nproposed training recipe (REPA-E) shows remarkable performance; speeding up\ndiffusion model training by over 17x and 45x over REPA and vanilla training\nrecipes, respectively. Interestingly, we observe that end-to-end tuning with\nREPA-E also improves the VAE itself; leading to improved latent space structure\nand downstream generation performance. In terms of final performance, our\napproach sets a new state-of-the-art; achieving FID of 1.26 and 1.83 with and\nwithout classifier-free guidance on ImageNet 256 x 256. Code is available at\nhttps://end2end-diffusion.github.io.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10483.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5f1158120c833276f61f1a84",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
      "fullname": "Niels Rogge",
      "name": "nielsr",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 820
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.11952",
      "authors": [
        {
          "_id": "6800646e0679d4ec4b9d01a7",
          "user": {
            "_id": "645c60dd7d655680b57ddbff",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645c60dd7d655680b57ddbff/MTMtkV6sy44oD-zwkGX0E.png",
            "isPro": true,
            "fullname": "Ram Kadiyala",
            "user": "1024m",
            "type": "user"
          },
          "name": "Ram Mohan Rao Kadiyala",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-17T08:04:05.627Z",
          "hidden": false
        },
        {
          "_id": "6800646e0679d4ec4b9d01a8",
          "user": {
            "_id": "63da8ba3f03c3d71ef32408c",
            "avatarUrl": "/avatars/1b2e6f3ea2bac5ab35dbd53edb7f8cf2.svg",
            "isPro": false,
            "fullname": "Siddartha Pullakhandam",
            "user": "Siddartha10",
            "type": "user"
          },
          "name": "Siddartha Pullakhandam",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:18:54.938Z",
          "hidden": false
        },
        {
          "_id": "6800646e0679d4ec4b9d01a9",
          "name": "Kanwal Mehreen",
          "hidden": false
        },
        {
          "_id": "6800646e0679d4ec4b9d01aa",
          "user": {
            "_id": "618c1ad1c74578e0a4a4d074",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/618c1ad1c74578e0a4a4d074/8u_AkeHt4d6xtQ8hzaffU.jpeg",
            "isPro": true,
            "fullname": "Drishti Sharma",
            "user": "DrishtiSharma",
            "type": "user"
          },
          "name": "Drishti Sharma",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:19:22.573Z",
          "hidden": false
        },
        {
          "_id": "6800646e0679d4ec4b9d01ab",
          "name": "Siddhant Gupta",
          "hidden": false
        },
        {
          "_id": "6800646e0679d4ec4b9d01ac",
          "user": {
            "_id": "66c578770a22b2f9ab575847",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c578770a22b2f9ab575847/-zfNho1DR3yZHDazq669-.png",
            "isPro": false,
            "fullname": "Jebish Purbey",
            "user": "jebish7",
            "type": "user"
          },
          "name": "Jebish Purbey",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:23:00.066Z",
          "hidden": false
        },
        {
          "_id": "6800646e0679d4ec4b9d01ad",
          "user": {
            "_id": "653d84f13fc9c706fa755d03",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653d84f13fc9c706fa755d03/F_jYbeuLLM9EX8hKXcbHD.png",
            "isPro": false,
            "fullname": "Ashay Srivastava",
            "user": "ashay-sriv",
            "type": "user"
          },
          "name": "Ashay Srivastava",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:23:15.588Z",
          "hidden": false
        },
        {
          "_id": "6800646e0679d4ec4b9d01ae",
          "user": {
            "_id": "669a745a4bbe8ad52ee287cf",
            "avatarUrl": "/avatars/245644aa638b45a17ff71124bd5bbe0f.svg",
            "isPro": false,
            "fullname": "Subhasya Tippareddy",
            "user": "subhasyar",
            "type": "user"
          },
          "name": "Subhasya TippaReddy",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:23:23.946Z",
          "hidden": false
        },
        {
          "_id": "6800646e0679d4ec4b9d01af",
          "user": {
            "_id": "669a7383c9111326dc596f5e",
            "avatarUrl": "/avatars/8bbd307fd4bb2d7055b2c8fc9140dc81.svg",
            "isPro": false,
            "fullname": "Arvind Reddy Bobbili",
            "user": "Arvindreddy",
            "type": "user"
          },
          "name": "Arvind Reddy Bobbili",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:23:32.804Z",
          "hidden": false
        },
        {
          "_id": "6800646e0679d4ec4b9d01b0",
          "name": "Suraj Telugara Chandrashekhar",
          "hidden": false
        },
        {
          "_id": "6800646e0679d4ec4b9d01b1",
          "user": {
            "_id": "668db0bfb09a05f3d7cc796f",
            "avatarUrl": "/avatars/969c511cca4d129b99eca6252a468385.svg",
            "isPro": false,
            "fullname": "Modabbir Adeeb",
            "user": "moda10",
            "type": "user"
          },
          "name": "Modabbir Adeeb",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:23:46.461Z",
          "hidden": false
        },
        {
          "_id": "6800646e0679d4ec4b9d01b2",
          "user": {
            "_id": "641c3337f0b71a9743629985",
            "avatarUrl": "/avatars/820b124887f173c263a675728baf99c6.svg",
            "isPro": false,
            "fullname": "Srinadh Vura",
            "user": "SriV",
            "type": "user"
          },
          "name": "Srinadh Vura",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:23:53.315Z",
          "hidden": false
        },
        {
          "_id": "6800646e0679d4ec4b9d01b3",
          "name": "Hamza Farooq",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-16T10:29:30.000Z",
      "submittedOnDailyAt": "2025-04-17T00:47:10.039Z",
      "title": "Robuste Détection de Texte Généré par l'IA par Pinguin Detector",
      "submittedOnDailyBy": {
        "_id": "645c60dd7d655680b57ddbff",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645c60dd7d655680b57ddbff/MTMtkV6sy44oD-zwkGX0E.png",
        "isPro": true,
        "fullname": "Ram Kadiyala",
        "user": "1024m",
        "type": "user"
      },
      "summary": "Un système idéal pour détecter du contenu généré par des machines devrait fonctionner bien avec plusieurs générateurs. Les systèmes existants rencontrent des difficultés à identifier de manière précise du contenu généré par l'IA avec des textes courts. De plus, pas tous les textes sont entièrement écrits par des personnes ou par des modèles de langage grands (LLM). Dans notre article, nous présentons un ensemble de modèles construits pour la tâche de classification de tokens et démontrons que ces modèles fonctionnent bien dans des domaines et générateurs qui n'ont jamais été vus, dans des textes de parole non natif et dans des entrées adversariales. De plus, nous présentons un ensemble de données de 2,4M de textes qui incluent presque tous les textes générés par un LLM multilingue dans 23 langues. Nous démontrons également la performance des modèles dans chaque domaine et générateur, comparons leur performance face aux méthodes adversariales, et comparons la longueur des textes d'entrée, des textes générés et des textes originaux humains.",
      "upvotes": 2,
      "discussionId": "680064710679d4ec4b9d0224",
      "ai_keywords": [
        "token classification",
        "adversarial inputs"
      ]
    },
    "publishedAt": "2025-04-16T06:29:30.000Z",
    "title": "Robust and Fine-Grained Detection of AI Generated Texts",
    "summary": "An ideal detection system for machine generated content is supposed to work\nwell on any generator as many more advanced LLMs come into existence day by\nday. Existing systems often struggle with accurately identifying AI-generated\ncontent over shorter texts. Further, not all texts might be entirely authored\nby a human or LLM, hence we focused more over partial cases i.e human-LLM\nco-authored texts. Our paper introduces a set of models built for the task of\ntoken classification which are trained on an extensive collection of\nhuman-machine co-authored texts, which performed well over texts of unseen\ndomains, unseen generators, texts by non-native speakers and those with\nadversarial inputs. We also introduce a new dataset of over 2.4M such texts\nmostly co-authored by several popular proprietary LLMs over 23 languages. We\nalso present findings of our models' performance over each texts of each domain\nand generator. Additional findings include comparison of performance against\neach adversarial method, length of input texts and characteristics of generated\ntexts compared to the original human authored texts.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11952.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645c60dd7d655680b57ddbff",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645c60dd7d655680b57ddbff/MTMtkV6sy44oD-zwkGX0E.png",
      "fullname": "Ram Kadiyala",
      "name": "1024m",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 19
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.11092",
      "authors": [
        {
          "_id": "6800a43e1fd95d7dc21d6b83",
          "user": {
            "_id": "63e367d3fae035bdc4c347fc",
            "avatarUrl": "/avatars/239da0c9287bf965348aff4890e94722.svg",
            "isPro": false,
            "fullname": "Jiaxin Huang",
            "user": "JaceyH919",
            "type": "user"
          },
          "name": "Jiaxin Huang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-17T08:03:59.476Z",
          "hidden": false
        },
        {
          "_id": "6800a43e1fd95d7dc21d6b84",
          "name": "Sheng Miao",
          "hidden": false
        },
        {
          "_id": "6800a43e1fd95d7dc21d6b85",
          "name": "BangBnag Yang",
          "hidden": false
        },
        {
          "_id": "6800a43e1fd95d7dc21d6b86",
          "name": "Yuewen Ma",
          "hidden": false
        },
        {
          "_id": "6800a43e1fd95d7dc21d6b87",
          "name": "Yiyi Liao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-15T11:38:14.000Z",
      "submittedOnDailyAt": "2025-04-17T07:32:38.400Z",
      "title": "Vivid4D : Vidéo-basée Caméra Unitaire Vidéo pour Améliorer la Réconstruction 4D",
      "submittedOnDailyBy": {
        "_id": "63e367d3fae035bdc4c347fc",
        "avatarUrl": "/avatars/239da0c9287bf965348aff4890e94722.svg",
        "isPro": false,
        "fullname": "Jiaxin Huang",
        "user": "JaceyH919",
        "type": "user"
      },
      "summary": "Reconstruirer une écran dynamique 4D à partir d'un vidéo de séquences simples est précieux mais très difficile. Chaque instant dans une feuille temporelle peut être observé depuis un seul point de vue. Nous présentons un nouvel approche appelée Vivid4D, qui se concentre sur renforcer la perspective d'observation et améliorer la synthèse de vidéos 4D de séquences. Une différence avec les méthodes existantes est qu'elle utilise la priorité de profondeur pour renforcer la perspective d'observation et synthétise des vidéos à partir de différents points de vue. Ce méthode reconstruit la vidéo comme si elle était en mouvement depuis un nouveau point de vue, en utilisant l'édition des coupures du vidéo. Pour cela, un modèle d'édition de coupures de vidéo web immobile est entraîné en utilisant des masques synthétiques. Cela garantit la continuité spatiale et temporelle. De plus, pour réduire l'incertitude de profondeur de l'objet, un approche itérative est introduite pour renforcer la perspective d'observation et une perte de reconstruction forte. Les expériences montrent que notre méthode améliore effectivement la reconstruction et la complétude des écrans 4D de séquences.",
      "upvotes": 2,
      "discussionId": "6800a4441fd95d7dc21d6d46",
      "projectPage": "https://xdimlab.github.io/Vivid4D/",
      "ai_keywords": [
        "Vivid4D",
        "4D monocular video synthesis",
        "view augmentation",
        "video inpainting",
        "monocular depth priors",
        "unposed web videos",
        "synthetically generated masks",
        "iterative view augmentation strategy",
        "robust reconstruction loss"
      ]
    },
    "publishedAt": "2025-04-15T07:38:14.000Z",
    "title": "Vivid4D: Improving 4D Reconstruction from Monocular Video by Video\n  Inpainting",
    "summary": "Reconstructing 4D dynamic scenes from casually captured monocular videos is\nvaluable but highly challenging, as each timestamp is observed from a single\nviewpoint. We introduce Vivid4D, a novel approach that enhances 4D monocular\nvideo synthesis by augmenting observation views - synthesizing multi-view\nvideos from a monocular input. Unlike existing methods that either solely\nleverage geometric priors for supervision or use generative priors while\noverlooking geometry, we integrate both. This reformulates view augmentation as\na video inpainting task, where observed views are warped into new viewpoints\nbased on monocular depth priors. To achieve this, we train a video inpainting\nmodel on unposed web videos with synthetically generated masks that mimic\nwarping occlusions, ensuring spatially and temporally consistent completion of\nmissing regions. To further mitigate inaccuracies in monocular depth priors, we\nintroduce an iterative view augmentation strategy and a robust reconstruction\nloss. Experiments demonstrate that our method effectively improves monocular 4D\nscene reconstruction and completion.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11092.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63e367d3fae035bdc4c347fc",
      "avatarUrl": "/avatars/239da0c9287bf965348aff4890e94722.svg",
      "fullname": "Jiaxin Huang",
      "name": "JaceyH919",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.11536",
      "authors": [
        {
          "_id": "6800cc7159e20f50cc282e87",
          "name": "Jiazhan Feng",
          "hidden": false
        },
        {
          "_id": "6800cc7159e20f50cc282e88",
          "name": "Shijue Huang",
          "hidden": false
        },
        {
          "_id": "6800cc7159e20f50cc282e89",
          "name": "Xingwei Qu",
          "hidden": false
        },
        {
          "_id": "6800cc7159e20f50cc282e8a",
          "name": "Ge Zhang",
          "hidden": false
        },
        {
          "_id": "6800cc7159e20f50cc282e8b",
          "name": "Yujia Qin",
          "hidden": false
        },
        {
          "_id": "6800cc7159e20f50cc282e8c",
          "name": "Baoquan Zhong",
          "hidden": false
        },
        {
          "_id": "6800cc7159e20f50cc282e8d",
          "name": "Chengquan Jiang",
          "hidden": false
        },
        {
          "_id": "6800cc7159e20f50cc282e8e",
          "name": "Jinxin Chi",
          "hidden": false
        },
        {
          "_id": "6800cc7159e20f50cc282e8f",
          "name": "Wanjun Zhong",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-15T18:10:22.000Z",
      "submittedOnDailyAt": "2025-04-17T08:10:39.383Z",
      "title": "ReTool : Apprentissage Automatique pour la Stratégie de Mise en œuvre de Outils",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Le modèle d'audition (par exemple, DeepSeek R1) est entraîné à l'aide d'apprentissage par renforcement (RL) et est familiarisé avec les logiques logiques du langage naturel, mais il n'est pas adapté pour résoudre des problèmes structurés. Par exemple, dans des cas comme les logiques géométriques, les calculs simples et les solutions d'équations complexes, un interpréteur de code (IC) fonctionne mieux comme une outil de calcul. Pour corriger cela, on propose ReTool. ReTool renforce les logiques logiques de longues phrases qui nécessitent l'utilisation de outils intégrés grâce à l'entraînement, et comporte deux fonctions principales : 1. L'intervalle dynamique d'exécution de code en temps réel lors du traitement de logiques logiques du langage naturel, et 2. Permet la politique de politiques de rétroalimentation dans l'entraînement automatique, qui inclut l'exécution de code en temps réel, et qui permet au modèle d'apprendre comment appeler les outils. ReTool commence par la génération de données froides synthétiques, crée la traçage logique de longues phrases pour l'ajout de code, et effectue des ajustements micro du modèle de base. Ensuite, l'entraînement par rétroalimentation améliore continuellement la stratégie d'utilisation des outils du modèle, en utilisant les résultats comme récompenses, et peut découvrir automatiquement des patrons optimaux d'appel à des outils sans nécessiter de connaître des connaissances humaines préalables. Les expériences dans l'AIME des cadres de référence stricts de MATH OLYMPIAD montrent des résultats excellents pour ReTool : notre modèle de 32B atteint une précision de 67% en 400 pas d'entraînement, dépasse la ligne de RL basée sur le texte (40% de précision, 1080 pas), et est plus efficace et efficace. En particulier, ReTool-32B atteint une précision de 72,5% dans des configurations étendues, dépassant o1-preview de OpenAI d'un marge de 27,9%. Une analyse supplémentaire révèle des comportements similaires à l'ajustement automatique de code, où le modèle apprend automatiquement à utiliser des outils adaptatifs, connus sous le nom du \"moment 'maintenant'\". Ces résultats démontrent la possibilité d'intégration des outils basées sur les résultats et fournissent une nouvelle perspective sur le progrès dans la résolution de logiques mathématiques complexes.",
      "upvotes": 0,
      "discussionId": "6800cc7359e20f50cc282f43",
      "ai_keywords": [
        "reinforcement learning",
        "dynamic interleaving",
        "real-time code execution",
        "natural language reasoning processes",
        "automated RL paradigm",
        "policy rollouts",
        "multi-turn real-time code execution",
        "synthetic cold-start data generation",
        "code-augmented long-form reasoning traces",
        "fine-tuning",
        "RL training",
        "task outcomes as rewards",
        "autonomous discovery",
        "optimal tool invocation patterns",
        "MATH Olympiad benchmark",
        "AIME",
        "accuracy",
        "training steps",
        "OpenAI's o1-preview",
        "code self-correction",
        "adaptive tool use",
        "hybrid neuro-symbolic systems"
      ]
    },
    "publishedAt": "2025-04-15T14:10:22.000Z",
    "title": "ReTool: Reinforcement Learning for Strategic Tool Use in LLMs",
    "summary": "While reasoning models (e.g., DeepSeek R1) trained with reinforcement\nlearning (RL), excel in textual reasoning, they struggle in scenarios requiring\nstructured problem-solving, such as geometric reasoning, concise computation,\nor complex equation solving-areas where computational tools like code\ninterpreters (CI) demonstrate distinct advantages. To bridge this gap, we\npropose ReTool, which enhances long-form reasoning with tool-integrated\nlearning, including two key features: (1) dynamic interleaving of real-time\ncode execution within natural language reasoning processes, and (2) an\nautomated RL paradigm that allows policy rollouts with multi-turn real-time\ncode execution and teaches the model in learning when and how to invoke tools\nbased on outcome feedback. ReTool employs a systematic training framework,\nbeginning with synthetic cold-start data generation to produce code-augmented\nlong-form reasoning traces for fine-tuning base models. Subsequent RL training\nleverages task outcomes as rewards to iteratively refine the model's tool use\nstrategy, enabling autonomous discovery of optimal tool invocation patterns\nwithout human priors. Experiments on the challenging MATH Olympiad benchmark\nAIME demonstrate ReTool's superiority: Our 32B model achieves 67% accuracy with\n400 training steps, outperforming text-based RL baseline (40% accuracy, 1080\nsteps) in efficiency and performance. Remarkably, ReTool-32B attains 72.5%\naccuracy in extended settings, surpassing OpenAI's o1-preview by 27.9%. Further\nanalysis reveals emergent behaviors such as code self-correction, signaling an\n''aha moment'' in which the model autonomously masters adaptive tool use. These\nfindings highlight the promise of outcome-driven tool integration for advancing\ncomplex mathematical reasoning and offer new insights into hybrid\nneuro-symbolic systems.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11536.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6669
    },
    "isAuthorParticipating": false
  }
]