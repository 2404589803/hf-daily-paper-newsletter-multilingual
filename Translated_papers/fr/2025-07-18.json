[
  {
    "paper": {
      "id": "2507.13334",
      "authors": [
        {
          "_id": "6879aad021b37e676c8e406b",
          "user": {
            "_id": "63120517ae8896941da4c5da",
            "avatarUrl": "/avatars/10e1be026035f3e24225e6782a710083.svg",
            "isPro": false,
            "fullname": "Lingrui Mei",
            "user": "Chevalier",
            "type": "user"
          },
          "name": "Lingrui Mei",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-18T07:49:58.423Z",
          "hidden": false
        },
        {
          "_id": "6879aad021b37e676c8e406c",
          "user": {
            "_id": "671f9cd9ff056a1b49444f37",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/B3Z9oiFb79Gi-_YXKP13u.png",
            "isPro": false,
            "fullname": "duoduo yao",
            "user": "Theodyy",
            "type": "user"
          },
          "name": "Jiayu Yao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-18T07:49:48.326Z",
          "hidden": false
        },
        {
          "_id": "6879aad021b37e676c8e406d",
          "user": {
            "_id": "656ad93853703dd78f3de7b8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656ad93853703dd78f3de7b8/r6VB6ICND1td3wFOy8pnz.jpeg",
            "isPro": false,
            "fullname": "YuyaoGe",
            "user": "YuyaoGe",
            "type": "user"
          },
          "name": "Yuyao Ge",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-18T07:49:54.689Z",
          "hidden": false
        },
        {
          "_id": "6879aad021b37e676c8e406e",
          "name": "Yiwei Wang",
          "hidden": false
        },
        {
          "_id": "6879aad021b37e676c8e406f",
          "name": "Baolong Bi",
          "hidden": false
        },
        {
          "_id": "6879aad021b37e676c8e4070",
          "name": "Yujun Cai",
          "hidden": false
        },
        {
          "_id": "6879aad021b37e676c8e4071",
          "name": "Jiazhi Liu",
          "hidden": false
        },
        {
          "_id": "6879aad021b37e676c8e4072",
          "user": {
            "_id": "6720cf97a0396f933ec93ab8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/NAdPIISe9-TYGGM0nAgsb.png",
            "isPro": false,
            "fullname": "Li Max",
            "user": "LImax72",
            "type": "user"
          },
          "name": "Mingyu Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-18T07:49:56.619Z",
          "hidden": false
        },
        {
          "_id": "6879aad021b37e676c8e4073",
          "name": "Zhong-Zhi Li",
          "hidden": false
        },
        {
          "_id": "6879aad021b37e676c8e4074",
          "user": {
            "_id": "662383a20edeabfe3b64a6a5",
            "avatarUrl": "/avatars/a76da726002d853dd08a51a6af6311d9.svg",
            "isPro": false,
            "fullname": "Duzhen Zhang",
            "user": "ShowerMaker",
            "type": "user"
          },
          "name": "Duzhen Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-18T07:49:50.488Z",
          "hidden": false
        },
        {
          "_id": "6879aad021b37e676c8e4075",
          "user": {
            "_id": "669e27902dbf53ccd23ae47f",
            "avatarUrl": "/avatars/5c193b542dcfe2e64467fa5c686f3e20.svg",
            "isPro": false,
            "fullname": "chenlin",
            "user": "tvstfe",
            "type": "user"
          },
          "name": "Chenlin Zhou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-18T07:49:52.410Z",
          "hidden": false
        },
        {
          "_id": "6879aad021b37e676c8e4076",
          "name": "Jiayi Mao",
          "hidden": false
        },
        {
          "_id": "6879aad021b37e676c8e4077",
          "name": "Tianze Xia",
          "hidden": false
        },
        {
          "_id": "6879aad021b37e676c8e4078",
          "name": "Jiafeng Guo",
          "hidden": false
        },
        {
          "_id": "6879aad021b37e676c8e4079",
          "name": "Shenghua Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-17T17:50:36.000Z",
      "submittedOnDailyAt": "2025-07-18T00:52:14.092Z",
      "title": "Étude de l'apprentissage du contexte dans les modèles de langue de Larzaret",
      "submittedOnDailyBy": {
        "_id": "63120517ae8896941da4c5da",
        "avatarUrl": "/avatars/10e1be026035f3e24225e6782a710083.svg",
        "isPro": false,
        "fullname": "Lingrui Mei",
        "user": "Chevalier",
        "type": "user"
      },
      "summary": "Le rendement d'un LLM est essentiellement déterminé en fonction de l'information contextuelle fournie pendant l'inférence. Dans cette étude, on présente une introduction à l'« Ingénierie du Contexte », une discipline formelle qui transcende le design simple des prompts et intègre l'optimisation systématique de la charge d'information d'un LLM. On propose une classification détaillée de l'Ingénierie du Contexte, qui est divisée en ses éléments fondamentaux et sa mise en œuvre complexe. Tout d'abord, on passe en revue les éléments fondamentaux : recherche et génération du contexte, traitement et gestion du contexte. Ensuite, on étudie comment ces éléments s'intègrent structurellement pour créer des systèmes complexes : RAG (Recherche d'Archives et Génération de Texte), systèmes de mémoire et logique d'outils, et systèmes de multiples assemblées. Après avoir analysé systématiquement plus de 1300 articles de recherche, cette étude construit un programme technique qui montre que les modèles évolués par l'Ingénierie du Contexte présentent une excellente efficacité pour comprendre des contextes complexes, tout en révélant des limites claires dans la génération de longues phrases complexes. La solution de ces limites est une priorité claire pour les futures recherches. Enfin, cette étude fournit un cadre intégral pour les chercheurs et ingénieurs qui souhaitent soutenir le développement de l'IA capable de lire des contextes.",
      "upvotes": 65,
      "discussionId": "6879aad021b37e676c8e407a",
      "githubRepo": "https://github.com/Meirtz/Awesome-Context-Engineering",
      "ai_summary": "Context Engineering systematically optimizes information payloads for Large Language Models, addressing gaps in generating sophisticated, long-form outputs.",
      "ai_keywords": [
        "Context Engineering",
        "context retrieval",
        "context generation",
        "context processing",
        "context management",
        "retrieval-augmented generation",
        "memory systems",
        "tool-integrated reasoning",
        "multi-agent systems"
      ],
      "githubStars": 164
    },
    "publishedAt": "2025-07-17T13:50:36.000Z",
    "title": "A Survey of Context Engineering for Large Language Models",
    "summary": "The performance of Large Language Models (LLMs) is fundamentally determined\nby the contextual information provided during inference. This survey introduces\nContext Engineering, a formal discipline that transcends simple prompt design\nto encompass the systematic optimization of information payloads for LLMs. We\npresent a comprehensive taxonomy decomposing Context Engineering into its\nfoundational components and the sophisticated implementations that integrate\nthem into intelligent systems. We first examine the foundational components:\ncontext retrieval and generation, context processing and context management. We\nthen explore how these components are architecturally integrated to create\nsophisticated system implementations: retrieval-augmented generation (RAG),\nmemory systems and tool-integrated reasoning, and multi-agent systems. Through\nthis systematic analysis of over 1300 research papers, our survey not only\nestablishes a technical roadmap for the field but also reveals a critical\nresearch gap: a fundamental asymmetry exists between model capabilities. While\ncurrent models, augmented by advanced context engineering, demonstrate\nremarkable proficiency in understanding complex contexts, they exhibit\npronounced limitations in generating equally sophisticated, long-form outputs.\nAddressing this gap is a defining priority for future research. Ultimately,\nthis survey provides a unified framework for both researchers and engineers\nadvancing context-aware AI.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.13334.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63120517ae8896941da4c5da",
      "avatarUrl": "/avatars/10e1be026035f3e24225e6782a710083.svg",
      "fullname": "Lingrui Mei",
      "name": "Chevalier",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.13348",
      "authors": [
        {
          "_id": "6879ba2021b37e676c8e40c9",
          "name": "Senqiao Yang",
          "hidden": false
        },
        {
          "_id": "6879ba2021b37e676c8e40ca",
          "name": "Junyi Li",
          "hidden": false
        },
        {
          "_id": "6879ba2021b37e676c8e40cb",
          "name": "Xin Lai",
          "hidden": false
        },
        {
          "_id": "6879ba2021b37e676c8e40cc",
          "name": "Bei Yu",
          "hidden": false
        },
        {
          "_id": "6879ba2021b37e676c8e40cd",
          "name": "Hengshuang Zhao",
          "hidden": false
        },
        {
          "_id": "6879ba2021b37e676c8e40ce",
          "name": "Jiaya Jia",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-17T17:59:55.000Z",
      "submittedOnDailyAt": "2025-07-18T01:47:35.200Z",
      "title": "VisionThink : Modèle de langage visuel intelligent et efficace basé sur la théorie de Renors",
      "submittedOnDailyBy": {
        "_id": "6527b7280ae663e384eb8499",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6527b7280ae663e384eb8499/73yF3eu2cUx7jVZrhXnXx.jpeg",
        "isPro": false,
        "fullname": "Senqiao Yang",
        "user": "Senqiao",
        "type": "user"
      },
      "summary": "Le développement récent des modèles de langue visuelle (VLMs) a augmenté la quantité de tokens visuels et a rendu ces tokens plus longs que les tokens contextuels, contribuant à améliorer le rendement. Cependant, dans la plupart des scénarios réels, il n'est pas nécessaire que ces longs tokens visuels. Bien que le rendement soit significativement affecté dans certaines tâches liées à l'OCR, ces tokens fonctionnent adéquatement dans des tâches VQA générales à une résolution de quatre parties. Nous proposons donc un nouveau paradigme de traitement dynamique pour différentes résolutions et de nouvelles techniques de compression de tokens visuels. Ce méthode commence avec des images sous-échantillonnées et évalue intelligemment si elles sont suffisantes pour résoudre le problème. Si non, le modèle émet un token spécial demandant des images à haute résolution. Comparé aux méthodes efficaces existantes, VisionThink décide automatiquement si chaque token est compressé. Cette capacité est particulièrement forte dans les tâches liées à l'OCR et permet de réduire significativement les tokens visuels dans des tâches simples. Nous proposons une stratégie d'apprentissage par renforcement utilisant l'approche LLM-as-Judge et l'appliquons dans des tâches VQA générales. De plus, nous concevons rigoureusement des fonctions de récompense et des structures de récompense pour atteindre une proportion stable de demandes de rééchelle de résolution. Les tests d'extension montrent l'efficacité, l'efficience et l'efficacité de notre méthode. Le code est disponible sur https://github.com/dvlab-research/VisionThink.",
      "upvotes": 38,
      "discussionId": "6879ba2121b37e676c8e40cf",
      "githubRepo": "https://github.com/dvlab-research/VisionThink",
      "ai_summary": "VisionThink dynamically adjusts image resolution and visual token processing for efficient and effective vision-language tasks, improving performance on OCR tasks while reducing token usage in simpler tasks.",
      "ai_keywords": [
        "vision-language models",
        "visual tokens",
        "text tokens",
        "downsampled image",
        "smart decision-making",
        "special token",
        "Efficient VLM",
        "token compression",
        "reinforcement learning",
        "LLM-as-Judge",
        "reward function",
        "penalty mechanism",
        "image resize call ratio"
      ],
      "githubStars": 25
    },
    "publishedAt": "2025-07-17T13:59:55.000Z",
    "title": "VisionThink: Smart and Efficient Vision Language Model via Reinforcement\n  Learning",
    "summary": "Recent advancements in vision-language models (VLMs) have improved\nperformance by increasing the number of visual tokens, which are often\nsignificantly longer than text tokens. However, we observe that most real-world\nscenarios do not require such an extensive number of visual tokens. While the\nperformance drops significantly in a small subset of OCR-related tasks, models\nstill perform accurately in most other general VQA tasks with only 1/4\nresolution. Therefore, we propose to dynamically process distinct samples with\ndifferent resolutions, and present a new paradigm for visual token compression,\nnamely, VisionThink. It starts with a downsampled image and smartly decides\nwhether it is sufficient for problem solving. Otherwise, the model could output\na special token to request the higher-resolution image. Compared to existing\nEfficient VLM methods that compress tokens using fixed pruning ratios or\nthresholds, VisionThink autonomously decides whether to compress tokens case by\ncase. As a result, it demonstrates strong fine-grained visual understanding\ncapability on OCR-related tasks, and meanwhile saves substantial visual tokens\non simpler tasks. We adopt reinforcement learning and propose the LLM-as-Judge\nstrategy to successfully apply RL to general VQA tasks. Moreover, we carefully\ndesign a reward function and penalty mechanism to achieve a stable and\nreasonable image resize call ratio. Extensive experiments demonstrate the\nsuperiority, efficiency, and effectiveness of our method. Our code is available\nat https://github.com/dvlab-research/VisionThink.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.13348.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6527b7280ae663e384eb8499",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6527b7280ae663e384eb8499/73yF3eu2cUx7jVZrhXnXx.jpeg",
      "fullname": "Senqiao Yang",
      "name": "Senqiao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.13347",
      "authors": [
        {
          "_id": "6879b78a21b37e676c8e40b1",
          "name": "Yifan Wang",
          "hidden": false
        },
        {
          "_id": "6879b78a21b37e676c8e40b2",
          "name": "Jianjun Zhou",
          "hidden": false
        },
        {
          "_id": "6879b78a21b37e676c8e40b3",
          "name": "Haoyi Zhu",
          "hidden": false
        },
        {
          "_id": "6879b78a21b37e676c8e40b4",
          "name": "Wenzheng Chang",
          "hidden": false
        },
        {
          "_id": "6879b78a21b37e676c8e40b5",
          "name": "Yang Zhou",
          "hidden": false
        },
        {
          "_id": "6879b78a21b37e676c8e40b6",
          "name": "Zizun Li",
          "hidden": false
        },
        {
          "_id": "6879b78a21b37e676c8e40b7",
          "name": "Junyi Chen",
          "hidden": false
        },
        {
          "_id": "6879b78a21b37e676c8e40b8",
          "name": "Jiangmiao Pang",
          "hidden": false
        },
        {
          "_id": "6879b78a21b37e676c8e40b9",
          "name": "Chunhua Shen",
          "hidden": false
        },
        {
          "_id": "6879b78a21b37e676c8e40ba",
          "name": "Tong He",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-17T17:59:53.000Z",
      "submittedOnDailyAt": "2025-07-18T01:26:57.410Z",
      "title": "π³ : Vérification de l'Égalité d'Ordre et de Valeur dans l'Apprentissage Visuel Géométrique",
      "submittedOnDailyBy": {
        "_id": "6747ede3a9c72aebe1322382",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/inILqQ05sESbYLdsEldJ_.png",
        "isPro": false,
        "fullname": "Tong He",
        "user": "tonghe90",
        "type": "user"
      },
      "summary": "pi^3 est une réseau de neurones de tête qui rompt la dépendance d'un point fixe décisif et offre une nouvelle approche pour la reconstruction géométrique visuelle. Les méthodes existantes doivent se fixer sur une reconstruction spécifique d'une vue, ce qui peut conduire à des instabilités ou à un échec lorsque le point de référence n'est pas optimal, introduisant ainsi des asymétries. En revanche, pi^3 ne utilise pas de cadre de référence et utilise une architecture complète avec une symétrie échangée pour prédire la position de la caméra avec une invariance affine et les points de référence locaux avec une invariance en échelle. Cette conception possède une forte robustesse face à la séquence d'entrée et une haute efficacité. En raison de ces excellents points, notre méthode simple et asymétrique développe les meilleurs résultats sur une large gamme de tâches, comme l'estimation de l'orientation de la caméra, la mesure de la profondeur monoscopique/vidéo et la reconstruction de cartes de points denses. Le code et le modèle sont disponibles publiquement.",
      "upvotes": 30,
      "discussionId": "6879b78b21b37e676c8e40bb",
      "projectPage": "https://yyfz.github.io/pi3/",
      "githubRepo": "https://github.com/yyfz/Pi3",
      "ai_summary": "A permutation-equivariant neural network, $\\pi^3$, reconstructs visual geometry without a fixed reference view, achieving state-of-the-art performance in camera pose estimation, depth estimation, and point map reconstruction.",
      "ai_keywords": [
        "feed-forward neural network",
        "permutation-equivariant architecture",
        "affine-invariant",
        "scale-invariant",
        "camera pose estimation",
        "monocular depth estimation",
        "video depth estimation",
        "dense point map reconstruction"
      ],
      "githubStars": 104
    },
    "publishedAt": "2025-07-17T13:59:53.000Z",
    "title": "π^3: Scalable Permutation-Equivariant Visual Geometry Learning",
    "summary": "We introduce pi^3, a feed-forward neural network that offers a novel\napproach to visual geometry reconstruction, breaking the reliance on a\nconventional fixed reference view. Previous methods often anchor their\nreconstructions to a designated viewpoint, an inductive bias that can lead to\ninstability and failures if the reference is suboptimal. In contrast, pi^3\nemploys a fully permutation-equivariant architecture to predict\naffine-invariant camera poses and scale-invariant local point maps without any\nreference frames. This design makes our model inherently robust to input\nordering and highly scalable. These advantages enable our simple and bias-free\napproach to achieve state-of-the-art performance on a wide range of tasks,\nincluding camera pose estimation, monocular/video depth estimation, and dense\npoint map reconstruction. Code and models are publicly available.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.13347.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6747ede3a9c72aebe1322382",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/inILqQ05sESbYLdsEldJ_.png",
      "fullname": "Tong He",
      "name": "tonghe90",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.13332",
      "authors": [
        {
          "_id": "6879b01621b37e676c8e40a8",
          "user": {
            "_id": "66214b4e4991d64ad0e28675",
            "avatarUrl": "/avatars/2574928aab7e45bc581c567d556a4cfd.svg",
            "isPro": false,
            "fullname": "Zhouqi Hua",
            "user": "ZhouqiHUA",
            "type": "user"
          },
          "name": "Zhouqi Hua",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-18T07:49:38.507Z",
          "hidden": false
        },
        {
          "_id": "6879b01621b37e676c8e40a9",
          "name": "Wenwei Zhang",
          "hidden": false
        },
        {
          "_id": "6879b01621b37e676c8e40aa",
          "name": "Chengqi Lyu",
          "hidden": false
        },
        {
          "_id": "6879b01621b37e676c8e40ab",
          "user": {
            "_id": "6601196cc91ba4c08ad6e270",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6601196cc91ba4c08ad6e270/X2YPNzUOQXBz5Gv-xR9LW.jpeg",
            "isPro": false,
            "fullname": "yuzhe gu",
            "user": "vanilla1116",
            "type": "user"
          },
          "name": "Yuzhe Gu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-18T07:49:36.203Z",
          "hidden": false
        },
        {
          "_id": "6879b01621b37e676c8e40ac",
          "name": "Songyang Gao",
          "hidden": false
        },
        {
          "_id": "6879b01621b37e676c8e40ad",
          "name": "Kuikun Liu",
          "hidden": false
        },
        {
          "_id": "6879b01621b37e676c8e40ae",
          "name": "Kai Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-17T17:50:07.000Z",
      "submittedOnDailyAt": "2025-07-18T00:59:46.053Z",
      "title": "« Le fabricant de jeux : Le fabricant de machines de tourelle est généralisable en longueur »",
      "submittedOnDailyBy": {
        "_id": "6601196cc91ba4c08ad6e270",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6601196cc91ba4c08ad6e270/X2YPNzUOQXBz5Gv-xR9LW.jpeg",
        "isPro": false,
        "fullname": "yuzhe gu",
        "user": "vanilla1116",
        "type": "user"
      },
      "summary": "La capacité de généralisation en longueur, c'est-à-dire le pouvoir de résoudre des problèmes de séquences longues non rencontrées lors du période d'entraînement, est un problème fondamental dans les grands modèles de langage basés sur les Transformers. Les études précédentes ont principalement se concentré sur un approche de données dirigée vers des calculs numériques ou des opérations symboliques, ce qui a limité le rendement généralisé dans des tâches spécifiques. Pour trouver des solutions plus générales, cette étude concentre son recherche sur des problèmes qui peuvent être résolus par des calculs calculables. À partir de cette perspective, on propose l'intégration de données de calculs calculatoires qui modélisent le processus d'exécution d'un Torraín, appelé Torraín Machine Embedded Learning (TAIL), avec l'objectif de améliorer la capacité de généralisation en longueur dans les modèles de langage grands (LLM). TAIL étend les calculs calculatoires de manière linéaire aux états atomiques, réduit l'entraînement à court terme et atténue les difficultés d'accès aux données de longue distance dynamiques, introduisant un mécanisme de pistes de mémoire explicite. Pour tester la fiabilité et la généralisation de TAIL, un ensemble de données complexes a été construit, comprenant 8 algorithmes et 18 tâches. En utilisant uniquement ces données, TAIL a considérablement amélioré la capacité de généralisation en longueur et le rendement dans diverses tâches, dépassant les méthodes existantes et DeepSeek-R1. Les résultats des expériences montrent clairement que le concept fondamental de Torraín Machine est essentiel pour la généralisation en longueur, plutôt que le style de pensée. Ce modèle présente un comportement de lecture et d'écriture qui correspond aux caractéristiques de Torraín Machine dans la couche d'attention. Cette étude fournit une direction adéquate pour futures recherches sur l'apprentissage de la raison dans les modèles de langage grands.",
      "upvotes": 30,
      "discussionId": "6879b01721b37e676c8e40af",
      "ai_summary": "TAIL, a method that imitates Turing Machine execution processes, enhances the length generalization and performance of LLMs by synthesizing chain-of-thought data and reducing shortcut learning.",
      "ai_keywords": [
        "Transformer-based large language models",
        "length generalization",
        "Turing MAchine Imitation Learning",
        "TAIL",
        "chain-of-thoughts",
        "Turing Machine",
        "synthetic dataset",
        "Qwen2.5-7B",
        "read-and-write behaviors",
        "attention layers"
      ]
    },
    "publishedAt": "2025-07-17T13:50:07.000Z",
    "title": "The Imitation Game: Turing Machine Imitator is Length Generalizable\n  Reasoner",
    "summary": "Length generalization, the ability to solve problems of longer sequences than\nthose observed during training, poses a core challenge of Transformer-based\nlarge language models (LLM). Although existing studies have predominantly\nfocused on data-driven approaches for arithmetic operations and symbolic\nmanipulation tasks, these approaches tend to be task-specific with limited\noverall performance. To pursue a more general solution, this paper focuses on a\nbroader case of reasoning problems that are computable, i.e., problems that\nalgorithms can solve, thus can be solved by the Turing Machine. From this\nperspective, this paper proposes Turing MAchine Imitation Learning (TAIL) to\nimprove the length generalization ability of LLMs. TAIL synthesizes\nchain-of-thoughts (CoT) data that imitate the execution process of a Turing\nMachine by computer programs, which linearly expands the reasoning steps into\natomic states to alleviate shortcut learning and explicit memory fetch\nmechanism to reduce the difficulties of dynamic and long-range data access in\nelementary operations. To validate the reliability and universality of TAIL, we\nconstruct a challenging synthetic dataset covering 8 classes of algorithms and\n18 tasks. Without bells and whistles, TAIL significantly improves the length\ngeneralization ability as well as the performance of Qwen2.5-7B on various\ntasks using only synthetic data, surpassing previous methods and DeepSeek-R1.\nThe experimental results reveal that the key concepts in the Turing Machine,\ninstead of the thinking styles, are indispensable for TAIL for length\ngeneralization, through which the model exhibits read-and-write behaviors\nconsistent with the properties of the Turing Machine in their attention layers.\nThis work provides a promising direction for future research in the learning of\nLLM reasoning from synthetic data.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.13332.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6601196cc91ba4c08ad6e270",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6601196cc91ba4c08ad6e270/X2YPNzUOQXBz5Gv-xR9LW.jpeg",
      "fullname": "yuzhe gu",
      "name": "vanilla1116",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.12841",
      "authors": [
        {
          "_id": "6879bba621b37e676c8e4195",
          "name": "Yiming Ren",
          "hidden": false
        },
        {
          "_id": "6879bba621b37e676c8e4196",
          "name": "Zhiqiang Lin",
          "hidden": false
        },
        {
          "_id": "6879bba621b37e676c8e4197",
          "name": "Yu Li",
          "hidden": false
        },
        {
          "_id": "6879bba621b37e676c8e4198",
          "name": "Gao Meng",
          "hidden": false
        },
        {
          "_id": "6879bba621b37e676c8e4199",
          "name": "Weiyun Wang",
          "hidden": false
        },
        {
          "_id": "6879bba621b37e676c8e419a",
          "name": "Junjie Wang",
          "hidden": false
        },
        {
          "_id": "6879bba621b37e676c8e419b",
          "name": "Zicheng Lin",
          "hidden": false
        },
        {
          "_id": "6879bba621b37e676c8e419c",
          "name": "Jifeng Dai",
          "hidden": false
        },
        {
          "_id": "6879bba621b37e676c8e419d",
          "name": "Yujiu Yang",
          "hidden": false
        },
        {
          "_id": "6879bba621b37e676c8e419e",
          "name": "Wenhai Wang",
          "hidden": false
        },
        {
          "_id": "6879bba621b37e676c8e419f",
          "user": {
            "_id": "642e3bcb958faf258a40e89c",
            "avatarUrl": "/avatars/213501def37dc53032cee17e37fcc4c1.svg",
            "isPro": false,
            "fullname": "Ruihang Chu",
            "user": "Ruihang",
            "type": "user"
          },
          "name": "Ruihang Chu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-18T07:49:30.538Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-17T07:04:05.000Z",
      "submittedOnDailyAt": "2025-07-18T02:04:48.913Z",
      "title": "Project AnyCap : Un seul cadre de travail, ensemble de données et benchmark, mais capture de visages de toutes les directions contrôlables.",
      "submittedOnDailyBy": {
        "_id": "642e3bcb958faf258a40e89c",
        "avatarUrl": "/avatars/213501def37dc53032cee17e37fcc4c1.svg",
        "isPro": false,
        "fullname": "Ruihang Chu",
        "user": "Ruihang",
        "type": "user"
      },
      "summary": "Le contrôlé caption nécessite une disposition précise des modèles et des instructions, mais actuellement, les modèles présentent des déficiences dans le contrôle fin et dans les protocoles d'évaluation fiables. Pour corriger cela, nous présentons le projet AnyCap, qui est une solution intégrée qui combine des modèles, des jeux de données et des évaluations. On présente le AnyCapModel (ACM), un cadre léger qui permet d'améliorer la possibilité de contrôle du modèle de base sans avoir à le réentraîner. ACM réutilise la caption originale des modèles de base et combine cela avec les instructions du utilisateur et les caractéristiques du modèle pour générer des captions améliorées. Pour compléter l'insuffisance de données de caption contrôlée, un AnyCapDataset (ACD) a été construit, qui enregistre 3 modèles, 28 types d'instructions du utilisateur et 300k entrées de haute qualité. De plus, on propose l'AnyCapEval, qui sépare la précision du contenu du respect de l'étiquette, offrant des critères d'évaluation fiables pour la caption contrôlée. ACM améliore significativement la qualité de la caption dans les modèles de base sur l'AnyCapEval. En particulier, ACM-8B augmente de plus de 45% le score de contenu et de plus de 12% le score d'étiquette de GPT-4o, et obtient également de bons résultats dans des cadres communs comme MIA-Bench et VidCapBench.",
      "upvotes": 27,
      "discussionId": "6879bba721b37e676c8e41a0",
      "githubRepo": "https://github.com/qishisuren123/AnyCap",
      "ai_summary": "The AnyCap Project introduces a framework, dataset, and evaluation protocol to enhance controllability and reliability in multimodal captioning.",
      "ai_keywords": [
        "AnyCapModel",
        "ACM",
        "omni-modal captioning",
        "AnyCapDataset",
        "ACD",
        "AnyCapEval",
        "content accuracy",
        "stylistic fidelity",
        "MIA-Bench",
        "VidCapBench"
      ],
      "githubStars": 27
    },
    "publishedAt": "2025-07-17T03:04:05.000Z",
    "title": "AnyCap Project: A Unified Framework, Dataset, and Benchmark for\n  Controllable Omni-modal Captioning",
    "summary": "Controllable captioning is essential for precise multimodal alignment and\ninstruction following, yet existing models often lack fine-grained control and\nreliable evaluation protocols. To address this gap, we present the AnyCap\nProject, an integrated solution spanning model, dataset, and evaluation. We\nintroduce AnyCapModel (ACM), a lightweight plug-and-play framework that\nenhances the controllability of existing foundation models for omni-modal\ncaptioning without retraining the base model. ACM reuses the original captions\nfrom base models while incorporating user instructions and modality features to\ngenerate improved captions. To remedy the data scarcity in controllable\nmultimodal captioning, we build AnyCapDataset (ACD), covering three modalities,\n28 user-instruction types, and 300\\,k high-quality data entries. We further\npropose AnyCapEval, a new benchmark that provides more reliable evaluation\nmetrics for controllable captioning by decoupling content accuracy and\nstylistic fidelity. ACM markedly improves caption quality across a diverse set\nof base models on AnyCapEval. Notably, ACM-8B raises GPT-4o\\'s content scores\nby 45\\% and style scores by 12\\%, and it also achieves substantial gains on\nwidely used benchmarks such as MIA-Bench and VidCapBench.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.12841.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642e3bcb958faf258a40e89c",
      "avatarUrl": "/avatars/213501def37dc53032cee17e37fcc4c1.svg",
      "fullname": "Ruihang Chu",
      "name": "Ruihang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.13344",
      "authors": [
        {
          "_id": "6879f3aa21b37e676c8e4202",
          "user": {
            "_id": "649958942ca6f96c8b8c1076",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649958942ca6f96c8b8c1076/olfLIqNryaog1nAnQPwkN.jpeg",
            "isPro": false,
            "fullname": "Yudong Jin",
            "user": "krahets",
            "type": "user"
          },
          "name": "Yudong Jin",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-18T07:49:22.520Z",
          "hidden": false
        },
        {
          "_id": "6879f3aa21b37e676c8e4203",
          "name": "Sida Peng",
          "hidden": false
        },
        {
          "_id": "6879f3aa21b37e676c8e4204",
          "name": "Xuan Wang",
          "hidden": false
        },
        {
          "_id": "6879f3aa21b37e676c8e4205",
          "name": "Tao Xie",
          "hidden": false
        },
        {
          "_id": "6879f3aa21b37e676c8e4206",
          "name": "Zhen Xu",
          "hidden": false
        },
        {
          "_id": "6879f3aa21b37e676c8e4207",
          "name": "Yifan Yang",
          "hidden": false
        },
        {
          "_id": "6879f3aa21b37e676c8e4208",
          "name": "Yujun Shen",
          "hidden": false
        },
        {
          "_id": "6879f3aa21b37e676c8e4209",
          "name": "Hujun Bao",
          "hidden": false
        },
        {
          "_id": "6879f3aa21b37e676c8e420a",
          "name": "Xiaowei Zhou",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/649958942ca6f96c8b8c1076/G5xpU1DSr8smrbqbom9hU.mp4"
      ],
      "publishedAt": "2025-07-17T17:59:17.000Z",
      "submittedOnDailyAt": "2025-07-18T06:34:43.522Z",
      "title": "Diffuman4D : Synthèse de vision humaine 4D en accord avec un vidéo de vision 2D dispersée en utilisant un modèle de dispersion temporelle spectrale.",
      "submittedOnDailyBy": {
        "_id": "649958942ca6f96c8b8c1076",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649958942ca6f96c8b8c1076/olfLIqNryaog1nAnQPwkN.jpeg",
        "isPro": false,
        "fullname": "Yudong Jin",
        "user": "krahets",
        "type": "user"
      },
      "summary": "Cet article aborde le problème de la synthèse visuelle de qualité élevée à partir de vidéos rares. Les méthodes existantes utilisent des modèles de Diffusion 4D pour générer de nouveaux vidéos à partir de nouvelles vues, ce qui résout le problème de la faible observabilité, mais les vidéos générées par ces modèles souvent perdent la cohérence spectrale dans le temps, ce qui affecte la qualité de la synthèse visuelle. Dans cet article, nous proposons un nouveau processus de dénivellement itératif avec un approche de fenêtre glissante pour améliorer la cohérence spectrale dans le temps du modèle de Diffusion 4D. Spécifiquement, nous définissons une grille potentielle qui inclut des vidéos, des états de caméra et des états humains, et nous utilisons des fenêtres glissantes à l'extrémité de chaque temps spectral pour échanger le dénivellement, enfin décodant un vidéo dans la vision cible. Dans l'approche itérative glissante, l'information traverse suffisamment à travers la largeur de la grille potentielle, permettant au modèle de Diffusion 4D une grande réception de champ et, par conséquent, une grande cohérence dans le temps 4D des résultats. De plus, cette approche permet de contrôler le consommation de mémoire du GPU. Les expériences avec les ensembles de données DNA-Rendering et ActorsHQ montrent que notre approche est capable de générer des vidéos de nouvelles vues de haute qualité et de cohérence, surpassant considérablement les méthodes actuelles. Vous pouvez voir un démo interactif et des résultats vidéo sur la page du projet : https://diffuman4d.github.io/.",
      "upvotes": 18,
      "discussionId": "6879f3ab21b37e676c8e420b",
      "projectPage": "https://diffuman4d.github.io/",
      "githubRepo": "https://github.com/zju3dv/Diffuman4D",
      "ai_summary": "A sliding iterative denoising process is proposed to enhance spatio-temporal consistency in 4D diffusion models for high-fidelity view synthesis from sparse-view videos.",
      "ai_keywords": [
        "4D diffusion models",
        "sliding iterative denoising",
        "latent grid",
        "image",
        "camera pose",
        "human pose",
        "spatio-temporal consistency",
        "GPU memory consumption",
        "DNA-Rendering",
        "ActorsHQ"
      ],
      "githubStars": 48
    },
    "publishedAt": "2025-07-17T13:59:17.000Z",
    "title": "Diffuman4D: 4D Consistent Human View Synthesis from Sparse-View Videos\n  with Spatio-Temporal Diffusion Models",
    "summary": "This paper addresses the challenge of high-fidelity view synthesis of humans\nwith sparse-view videos as input. Previous methods solve the issue of\ninsufficient observation by leveraging 4D diffusion models to generate videos\nat novel viewpoints. However, the generated videos from these models often lack\nspatio-temporal consistency, thus degrading view synthesis quality. In this\npaper, we propose a novel sliding iterative denoising process to enhance the\nspatio-temporal consistency of the 4D diffusion model. Specifically, we define\na latent grid in which each latent encodes the image, camera pose, and human\npose for a certain viewpoint and timestamp, then alternately denoising the\nlatent grid along spatial and temporal dimensions with a sliding window, and\nfinally decode the videos at target viewpoints from the corresponding denoised\nlatents. Through the iterative sliding, information flows sufficiently across\nthe latent grid, allowing the diffusion model to obtain a large receptive field\nand thus enhance the 4D consistency of the output, while making the GPU memory\nconsumption affordable. The experiments on the DNA-Rendering and ActorsHQ\ndatasets demonstrate that our method is able to synthesize high-quality and\nconsistent novel-view videos and significantly outperforms the existing\napproaches. See our project page for interactive demos and video results:\nhttps://diffuman4d.github.io/ .",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/649958942ca6f96c8b8c1076/G5xpU1DSr8smrbqbom9hU.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.13344.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "649958942ca6f96c8b8c1076",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649958942ca6f96c8b8c1076/olfLIqNryaog1nAnQPwkN.jpeg",
      "fullname": "Yudong Jin",
      "name": "krahets",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.12508",
      "authors": [
        {
          "_id": "6879af4f21b37e676c8e409b",
          "user": {
            "_id": "65e919332fd9300c7eb96556",
            "avatarUrl": "/avatars/a826f7e14acf34603aa68e4fc27f12af.svg",
            "isPro": false,
            "fullname": "Yuncong Yang",
            "user": "yyuncong",
            "type": "user"
          },
          "name": "Yuncong Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-18T07:49:42.036Z",
          "hidden": false
        },
        {
          "_id": "6879af4f21b37e676c8e409c",
          "name": "Jiageng Liu",
          "hidden": false
        },
        {
          "_id": "6879af4f21b37e676c8e409d",
          "name": "Zheyuan Zhang",
          "hidden": false
        },
        {
          "_id": "6879af4f21b37e676c8e409e",
          "name": "Siyuan Zhou",
          "hidden": false
        },
        {
          "_id": "6879af4f21b37e676c8e409f",
          "name": "Reuben Tan",
          "hidden": false
        },
        {
          "_id": "6879af4f21b37e676c8e40a0",
          "name": "Jianwei Yang",
          "hidden": false
        },
        {
          "_id": "6879af4f21b37e676c8e40a1",
          "name": "Yilun Du",
          "hidden": false
        },
        {
          "_id": "6879af4f21b37e676c8e40a2",
          "name": "Chuang Gan",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/65e919332fd9300c7eb96556/sRj5beNxspccadVwKC_vj.mp4"
      ],
      "publishedAt": "2025-07-16T17:59:36.000Z",
      "submittedOnDailyAt": "2025-07-18T00:56:28.122Z",
      "title": "Le Désir du Cerveau : Logique Spatiale par Escalage dans la Vérification avec des Modèles d'Intelligence Artificielle",
      "submittedOnDailyBy": {
        "_id": "65e919332fd9300c7eb96556",
        "avatarUrl": "/avatars/a826f7e14acf34603aa68e4fc27f12af.svg",
        "isPro": false,
        "fullname": "Yuncong Yang",
        "user": "yyuncong",
        "type": "user"
      },
      "summary": "L'espace spatiaux de l'inférence dans l'espace 3D est un aspect central de la cognition humaine et est essentiel pour des tâches telles que le cartographie et le gestionnement. Cependant, les modèles de langage visuel (VLMs) de pointe éprouvent des difficultés même pour des tâches simples : ils ne disposent pas d'un modèle interne de la dynamique 3D uniquement à partir d'images 2D. Par conséquent, nous proposons \"MindJourney\", un cadre d'échelle de temps de test basé sur la diffusion de vidéo qui combine avec des modèles de monde contrôlables pour fournir aux VLMs des compétences qui manquent. Les VLMs dessinent répétitivement la vision relative synthétisée par le modèle de monde à chaque étape, créant des raisons à partir de multiples preuves recueillies par exploration interactive. Sans entraînement final, notre MindJourney atteint un amélioration du rendement moyen de 8% sur les benchmarks spatiaux d'inférence SAT, démontrant que la combinaison de VLMs et de modèles de monde permet une échelle de temps de test de manière simple et puissante pour l'inférence 3D. De plus, notre méthode améliore les VLMs d'inférence de temps de test entraînés par apprentissage par répétition et montre la possibilité d'utiliser des modèles de monde dans l'échelle de temps de test.",
      "upvotes": 10,
      "discussionId": "6879af5021b37e676c8e40a3",
      "projectPage": "https://umass-embodied-agi.github.io/MindJourney/",
      "githubRepo": "https://github.com/UMass-Embodied-AGI/MindJourney",
      "ai_summary": "MindJourney enhances vision-language models with 3D reasoning by coupling them with a video diffusion-based world model, achieving improved performance on spatial reasoning tasks without fine-tuning.",
      "ai_keywords": [
        "vision-language models",
        "VLMs",
        "world model",
        "video diffusion",
        "camera trajectory",
        "multi-view evidence",
        "spatial reasoning",
        "SAT benchmark",
        "reinforcement learning"
      ],
      "githubStars": 9
    },
    "publishedAt": "2025-07-16T13:59:36.000Z",
    "title": "MindJourney: Test-Time Scaling with World Models for Spatial Reasoning",
    "summary": "Spatial reasoning in 3D space is central to human cognition and indispensable\nfor embodied tasks such as navigation and manipulation. However,\nstate-of-the-art vision-language models (VLMs) struggle frequently with tasks\nas simple as anticipating how a scene will look after an egocentric motion:\nthey perceive 2D images but lack an internal model of 3D dynamics. We therefore\npropose MindJourney, a test-time scaling framework that grants a VLM with this\nmissing capability by coupling it to a controllable world model based on video\ndiffusion. The VLM iteratively sketches a concise camera trajectory, while the\nworld model synthesizes the corresponding view at each step. The VLM then\nreasons over this multi-view evidence gathered during the interactive\nexploration. Without any fine-tuning, our MindJourney achieves over an average\n8% performance boost on the representative spatial reasoning benchmark SAT,\nshowing that pairing VLMs with world models for test-time scaling offers a\nsimple, plug-and-play route to robust 3D reasoning. Meanwhile, our method also\nimproves upon the test-time inference VLMs trained through reinforcement\nlearning, which demonstrates the potential of our method that utilizes world\nmodels for test-time scaling.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/65e919332fd9300c7eb96556/sRj5beNxspccadVwKC_vj.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.12508.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65e919332fd9300c7eb96556",
      "avatarUrl": "/avatars/a826f7e14acf34603aa68e4fc27f12af.svg",
      "fullname": "Yuncong Yang",
      "name": "yyuncong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.13300",
      "authors": [
        {
          "_id": "6879c27e21b37e676c8e41a9",
          "name": "Yilun Zhao",
          "hidden": false
        },
        {
          "_id": "6879c27e21b37e676c8e41aa",
          "name": "Weiyuan Chen",
          "hidden": false
        },
        {
          "_id": "6879c27e21b37e676c8e41ab",
          "name": "Zhijian Xu",
          "hidden": false
        },
        {
          "_id": "6879c27e21b37e676c8e41ac",
          "name": "Manasi Patwardhan",
          "hidden": false
        },
        {
          "_id": "6879c27e21b37e676c8e41ad",
          "name": "Yixin Liu",
          "hidden": false
        },
        {
          "_id": "6879c27e21b37e676c8e41ae",
          "name": "Chengye Wang",
          "hidden": false
        },
        {
          "_id": "6879c27e21b37e676c8e41af",
          "name": "Lovekesh Vig",
          "hidden": false
        },
        {
          "_id": "6879c27e21b37e676c8e41b0",
          "name": "Arman Cohan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-17T17:09:22.000Z",
      "submittedOnDailyAt": "2025-07-18T02:12:09.802Z",
      "title": "AbGen : Évaluation de modèles de langage grands dans le design et l'évaluation des tests d'élimination de recherches scientifiques",
      "submittedOnDailyBy": {
        "_id": "62f662bcc58915315c4eccea",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
        "isPro": true,
        "fullname": "Yilun Zhao",
        "user": "yilunzhao",
        "type": "user"
      },
      "summary": "Voici la traduction en français :\n\nCi-dessous est présenté le premier benchmark \"AbGen\" pour évaluer la capacité de conception de recherches sur l'extinction dans la recherche scientifique. AbGen est composé de 1 500 exemples évalués par des experts dans 807 articles de NLP. Dans ce benchmark, on présente à un LLM la tâche de générer des concepts de recherches sur l'extinction de modules ou de processus spécifiques dans le contexte d'une recherche donnée. Selon l'évaluation de modèles avancés tels que DeepSeek-R1-0528 et o4-mini, il a été confirmé qu'il existe des différences claires de performance entre les modèles et les experts humains en ce qui concerne l'importance, la fidélité et la pertinence du concept de recherche sur l'extinction. De plus, il a été observé que les méthodes d'évaluation automatique actuelles ont une confiance évidentement insuffisante par rapport à l'évaluation humaine. Pour explorer ce point plus en détail, un meta-benchmark \"AbGen-Eval\" a été développé pour évaluer la confiance des systèmes d'évaluation automatique. Dans AbGen-Eval, on étudie divers systèmes de jury basés sur un LLM pour mesurer la confiance des systèmes automatiques généraux d'évaluation de la performance d'un LLM dans des tâches scientifiques complexes, et on fournit des guides pour une recherche future qui peuvent aider à développer des systèmes d'évaluation basés sur un LLM efficaces et fiables.",
      "upvotes": 9,
      "discussionId": "6879c27f21b37e676c8e41b1",
      "githubRepo": "https://github.com/yale-nlp/AbGen",
      "ai_summary": "AbGen evaluates LLMs in designing ablation studies for scientific research, revealing performance gaps compared to human experts and highlighting the unreliability of current automated evaluation methods.",
      "ai_keywords": [
        "LLMs",
        "ablation studies",
        "NLP papers",
        "DeepSeek-R1-0528",
        "o4-mini",
        "AbGen-Eval",
        "LLM-as-Judge"
      ],
      "githubStars": 2
    },
    "publishedAt": "2025-07-17T13:09:22.000Z",
    "title": "AbGen: Evaluating Large Language Models in Ablation Study Design and\n  Evaluation for Scientific Research",
    "summary": "We introduce AbGen, the first benchmark designed to evaluate the capabilities\nof LLMs in designing ablation studies for scientific research. AbGen consists\nof 1,500 expert-annotated examples derived from 807 NLP papers. In this\nbenchmark, LLMs are tasked with generating detailed ablation study designs for\na specified module or process based on the given research context. Our\nevaluation of leading LLMs, such as DeepSeek-R1-0528 and o4-mini, highlights a\nsignificant performance gap between these models and human experts in terms of\nthe importance, faithfulness, and soundness of the ablation study designs.\nMoreover, we demonstrate that current automated evaluation methods are not\nreliable for our task, as they show a significant discrepancy when compared to\nhuman assessment. To better investigate this, we develop AbGen-Eval, a\nmeta-evaluation benchmark designed to assess the reliability of commonly used\nautomated evaluation systems in measuring LLM performance on our task. We\ninvestigate various LLM-as-Judge systems on AbGen-Eval, providing insights for\nfuture research on developing more effective and reliable LLM-based evaluation\nsystems for complex scientific tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.13300.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62f662bcc58915315c4eccea",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
      "fullname": "Yilun Zhao",
      "name": "yilunzhao",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.12956",
      "authors": [
        {
          "_id": "6879df6521b37e676c8e41cd",
          "user": {
            "_id": "653b195c5f1703225b2fd571",
            "avatarUrl": "/avatars/b7f376225cef6c13952c9c5540dd43be.svg",
            "isPro": false,
            "fullname": "wangqiang",
            "user": "wangqiang9",
            "type": "user"
          },
          "name": "Qiang Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-18T07:49:24.746Z",
          "hidden": false
        },
        {
          "_id": "6879df6521b37e676c8e41ce",
          "name": "Mengchao Wang",
          "hidden": false
        },
        {
          "_id": "6879df6521b37e676c8e41cf",
          "name": "Fan Jiang",
          "hidden": false
        },
        {
          "_id": "6879df6521b37e676c8e41d0",
          "name": "Yaqi Fan",
          "hidden": false
        },
        {
          "_id": "6879df6521b37e676c8e41d1",
          "name": "Yonggang Qi",
          "hidden": false
        },
        {
          "_id": "6879df6521b37e676c8e41d2",
          "name": "Mu Xu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/653b195c5f1703225b2fd571/vkwYxJilAke5PetjLV8lm.mp4"
      ],
      "publishedAt": "2025-07-17T09:50:43.000Z",
      "submittedOnDailyAt": "2025-07-18T04:28:50.046Z",
      "title": "Je cherche une traduction précise et professionnelle du texte anglais fourni. Voici la traduction en français :\n\n\"Questar Pop : Amélioration de l'animation de multiples personnages en utilisant un Transformer diffus qui améliore l'expression\"",
      "submittedOnDailyBy": {
        "_id": "653b195c5f1703225b2fd571",
        "avatarUrl": "/avatars/b7f376225cef6c13952c9c5540dd43be.svg",
        "isPro": false,
        "fullname": "wangqiang",
        "user": "wangqiang9",
        "type": "user"
      },
      "summary": "La génération d'animations de visages expressifs pendant la détention est un défi complexe. Les méthodes existantes s'appuient sur des concepts géométriques explicites et présentent des artefacts lors de la création, et elles ne capturent pas avec précision les émotions subtiles. De plus, elles ne supportent actuellement pas l'animation de plusieurs personnages. L'interférence entre les caractéristiques de différents individus complique les problèmes. Pour résoudre ces défis, nous proposons FantasyPortrait, un cadre de travail basé sur les canaux de diffusion qui génère des animations émotionnelles de haute qualité. Notre approche introduit une stratégie d'apprentissage qui élargit les expressions et utilise des représentations cachées pour capturer les mouvements faciaux, améliorant la capacité d'expression émotionnelle du modèle. Dans le contrôle de plusieurs personnages, nous avons conçu une structure d'attention avec des masques pour générer des expressions de manière dynamique et indépendante, évitant en particulier l'interférence entre les caractéristiques. Nous contribuons à ce domaine avec le Multi-Expr dataset et ExprBench, un ensemble de données et un cadre de référence spécialisés pour l'entraînement et l'évaluation de l'animation de plusieurs personnages. Les expériences étendues montrent que FantasyPortrait dépasse les méthodes les plus récentes, notamment en évaluations qualitatives et quantitatives, ainsi que dans la création et le contexte complexe de plusieurs personnages. Notre page du projet est disponible sur https://fantasy-amap.github.io/fantasy-portrait/.",
      "upvotes": 9,
      "discussionId": "6879df6521b37e676c8e41d3",
      "projectPage": "https://fantasy-amap.github.io/fantasy-portrait/",
      "githubRepo": "https://github.com/Fantasy-AMAP/fantasy-portrait",
      "ai_summary": "FantasyPortrait, a diffusion transformer framework, generates high-fidelity and emotion-rich facial animations for single and multi-character scenarios using implicit representations and a masked cross-attention mechanism.",
      "ai_keywords": [
        "diffusion transformer",
        "expression-augmented learning",
        "implicit representations",
        "masked cross-attention mechanism",
        "Multi-Expr dataset",
        "ExprBench"
      ],
      "githubStars": 6
    },
    "publishedAt": "2025-07-17T05:50:43.000Z",
    "title": "FantasyPortrait: Enhancing Multi-Character Portrait Animation with\n  Expression-Augmented Diffusion Transformers",
    "summary": "Producing expressive facial animations from static images is a challenging\ntask. Prior methods relying on explicit geometric priors (e.g., facial\nlandmarks or 3DMM) often suffer from artifacts in cross reenactment and\nstruggle to capture subtle emotions. Furthermore, existing approaches lack\nsupport for multi-character animation, as driving features from different\nindividuals frequently interfere with one another, complicating the task. To\naddress these challenges, we propose FantasyPortrait, a diffusion transformer\nbased framework capable of generating high-fidelity and emotion-rich animations\nfor both single- and multi-character scenarios. Our method introduces an\nexpression-augmented learning strategy that utilizes implicit representations\nto capture identity-agnostic facial dynamics, enhancing the model's ability to\nrender fine-grained emotions. For multi-character control, we design a masked\ncross-attention mechanism that ensures independent yet coordinated expression\ngeneration, effectively preventing feature interference. To advance research in\nthis area, we propose the Multi-Expr dataset and ExprBench, which are\nspecifically designed datasets and benchmarks for training and evaluating\nmulti-character portrait animations. Extensive experiments demonstrate that\nFantasyPortrait significantly outperforms state-of-the-art methods in both\nquantitative metrics and qualitative evaluations, excelling particularly in\nchallenging cross reenactment and multi-character contexts. Our project page is\nhttps://fantasy-amap.github.io/fantasy-portrait/.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/653b195c5f1703225b2fd571/vkwYxJilAke5PetjLV8lm.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.12956.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "653b195c5f1703225b2fd571",
      "avatarUrl": "/avatars/b7f376225cef6c13952c9c5540dd43be.svg",
      "fullname": "wangqiang",
      "name": "wangqiang9",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.12990",
      "authors": [
        {
          "_id": "6879fdc821b37e676c8e422b",
          "name": "Nikita Koriagin",
          "hidden": false
        },
        {
          "_id": "6879fdc821b37e676c8e422c",
          "name": "Yaroslav Aksenov",
          "hidden": false
        },
        {
          "_id": "6879fdc821b37e676c8e422d",
          "user": {
            "_id": "634c5f8cfb80cc6bcaf42c03",
            "avatarUrl": "/avatars/1f37db0e70cbaf9707f4c8cbcee37ca0.svg",
            "isPro": false,
            "fullname": "Daniil Laptev",
            "user": "dlaptev",
            "type": "user"
          },
          "name": "Daniil Laptev",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-18T08:32:16.310Z",
          "hidden": false
        },
        {
          "_id": "6879fdc821b37e676c8e422e",
          "name": "Gleb Gerasimov",
          "hidden": false
        },
        {
          "_id": "6879fdc821b37e676c8e422f",
          "user": {
            "_id": "60b364e7f88532cd79eaff7b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654185363389-60b364e7f88532cd79eaff7b.jpeg",
            "isPro": false,
            "fullname": "Nikita Balagansky",
            "user": "elephantmipt",
            "type": "user"
          },
          "name": "Nikita Balagansky",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-18T08:32:14.730Z",
          "hidden": false
        },
        {
          "_id": "6879fdc821b37e676c8e4230",
          "user": {
            "_id": "62a9c8edc19f92ae443ab37f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669110208492-62a9c8edc19f92ae443ab37f.png",
            "isPro": false,
            "fullname": "Daniil Gavrilov",
            "user": "kefirski",
            "type": "user"
          },
          "name": "Daniil Gavrilov",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-18T08:32:17.917Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/60b364e7f88532cd79eaff7b/puU_3A7T0uk5E0fGkoc1k.png"
      ],
      "publishedAt": "2025-07-17T10:57:49.000Z",
      "submittedOnDailyAt": "2025-07-18T06:31:25.084Z",
      "title": "Enseigne aux experts de l'état de l'art anciens de nouvelles techniques de domination avec Boosting\n\nCet article explique comment utiliser \"Boosting\" pour enseigner aux \"experts de l'état de l'art anciens\" de nouvelles techniques dans un domaine. \"Experts de l'état de l'art\" signifie \"experts de la technique de l'état de l'art\", tandis que \"Boosting\" est une technologie qui améliore le rendement dans l'apprentissage automatique. Cet article décrit comment les \"experts de l'état de l'art anciens\" peuvent améliorer leur rendement dans un nouveau domaine en utilisant \"Boosting\".",
      "submittedOnDailyBy": {
        "_id": "60b364e7f88532cd79eaff7b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654185363389-60b364e7f88532cd79eaff7b.jpeg",
        "isPro": false,
        "fullname": "Nikita Balagansky",
        "user": "elephantmipt",
        "type": "user"
      },
      "summary": "Les autoencodeurs spars ont apparu comme une outil puissant pour interpréter les représentations internes des grands modèles de langage, mais tendent à échouer en ne tirant pas des caractéristiques spécifiques. Dans cet article, on propose une approche d'apprentissage résiduel pour compléter ces lacunes et on propose un méthode qui évite la nécessité d'une reprendre l'entraînement complet. En particulier, on propose d'entraîner un SAE (Autoencodeur Spars) et de modéliser les erreurs de reconstruction des caractéristiques spécifiques d'un SAE pré-entraîné, permettant ainsi que le modèle principal capture efficacement les caractéristiques non détectées. En combinant les résultats de ces deux modèles lors de l'inférence, on observe une amélioration significative de l'entropie croisée et de la mesure de variation expliquable des grands modèles de langage. Les résultats des expériences montrent que ce méthode intègre efficacement de nouveaux membres à la membresie et maintient le rendement général des modèles. Cette approche permet de sélectionner de manière sélective l'amélioration de l'interprétation mécanique dans certains membres, ouvrant de nouvelles possibilités pour l'interprétation mécanique objective des grands modèles de langage.",
      "upvotes": 4,
      "discussionId": "6879fdc821b37e676c8e4231",
      "ai_summary": "A residual learning approach enhances Sparse Autoencoders to capture domain-specific features without retraining, improving interpretability and performance on specialized domains.",
      "ai_keywords": [
        "Sparse Autoencoders",
        "residual learning",
        "reconstruction error",
        "cross-entropy",
        "explained variance",
        "targeted mechanistic interpretability",
        "Large Language Models"
      ]
    },
    "publishedAt": "2025-07-17T06:57:49.000Z",
    "title": "Teach Old SAEs New Domain Tricks with Boosting",
    "summary": "Sparse Autoencoders have emerged as powerful tools for interpreting the\ninternal representations of Large Language Models, yet they often fail to\ncapture domain-specific features not prevalent in their training corpora. This\npaper introduces a residual learning approach that addresses this feature\nblindness without requiring complete retraining. We propose training a\nsecondary SAE specifically to model the reconstruction error of a pretrained\nSAE on domain-specific texts, effectively capturing features missed by the\nprimary model. By summing the outputs of both models during inference, we\ndemonstrate significant improvements in both LLM cross-entropy and explained\nvariance metrics across multiple specialized domains. Our experiments show that\nthis method efficiently incorporates new domain knowledge into existing SAEs\nwhile maintaining their performance on general tasks. This approach enables\nresearchers to selectively enhance SAE interpretability for specific domains of\ninterest, opening new possibilities for targeted mechanistic interpretability\nof LLMs.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/60b364e7f88532cd79eaff7b/puU_3A7T0uk5E0fGkoc1k.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.12990.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60b364e7f88532cd79eaff7b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654185363389-60b364e7f88532cd79eaff7b.jpeg",
      "fullname": "Nikita Balagansky",
      "name": "elephantmipt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.12720",
      "authors": [
        {
          "_id": "68799d0521b37e676c8e4060",
          "name": "Abraham Toluase Owodunni",
          "hidden": false
        },
        {
          "_id": "68799d0521b37e676c8e4061",
          "name": "Orevaoghene Ahia",
          "hidden": false
        },
        {
          "_id": "68799d0521b37e676c8e4062",
          "name": "Sachin Kumar",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-17T01:55:41.000Z",
      "submittedOnDailyAt": "2025-07-18T00:36:57.429Z",
      "title": "FLEXITOKENS : Modèles de langage évoluant pour une tokenisation flexible",
      "submittedOnDailyBy": {
        "_id": "626d1e1e72169e781945bf44",
        "avatarUrl": "/avatars/6bf9f35042b6d939f2ab525816ad0423.svg",
        "isPro": false,
        "fullname": "Abraham  Owodunni",
        "user": "Owos",
        "type": "user"
      },
      "summary": "Les modèles de langue (MLs) ont des difficultés à s'adapter à de nouvelles distributions de données avec un ajuste faible. Cela est dû à la rigidité des sous-tokenisateurs. Cette immutabilité généralement ne change pas pendant la période d'adaptation, ce qui fait que de nombreux processus de tokenisation inutiles sont attaqués en raison de nouvelles distributions de données, de langues non vues ou de fragments de scripts excessifs. Dans cette étude, des MLs à l'échelle des bytes avec un tokenisateur d'entrée adaptable sont développés, créant un processus de tokenisation qui s'adapte. Le modèle inclut un sous-module qui prédit les limites des séquences de bytes d'entrée, ce qui se transforme en segments de transformation. Les méthodes actuelles sans tokenisateur entraînent la prédiction des limites des tokenisateurs avec des pertes d'assistance et imposent une taux de compression fixe sur tout le corpus d'entraînement, conduisant à une nouvelle rigidité. On propose les FLEXITOKENS. Les FLEXITOKENS abordent l'inflexibilité qui augmente significativement pendant la période d'adaptation. Elles sont vérifiées sur de multiples évaluations multilingues et tâches structuralement diverses, montrant que les FLEXITOKENS réduisent la fragmentation excessive des tokens par rapport aux sous-tokenisateurs ou autres tokenisateurs basés sur les gradients, améliorant de 10% en efficacité pour les tâches ultérieures. Les codes des expérimentations et les données sont disponibles sur https://github.com/owos/flexitokens.",
      "upvotes": 4,
      "discussionId": "68799d0521b37e676c8e4063",
      "ai_summary": "FLEXITOKENS, a byte-level language model with a learnable tokenizer, reduces token over-fragmentation and improves performance across multilingual and morphologically diverse tasks.",
      "ai_keywords": [
        "byte-level LMs",
        "learnable tokenizers",
        "boundary predictor",
        "FLEXITOKENS",
        "token over-fragmentation",
        "subword tokenizers",
        "gradient-based tokenizers"
      ]
    },
    "publishedAt": "2025-07-16T21:55:41.000Z",
    "title": "FLEXITOKENS: Flexible Tokenization for Evolving Language Models",
    "summary": "Language models (LMs) are challenging to adapt to new data distributions by\nsimple finetuning. This is due to the rigidity of their subword tokenizers,\nwhich typically remain unchanged during adaptation. This inflexibility often\nleads to inefficient tokenization, causing overfragmentation of\nout-of-distribution domains, unseen languages, or scripts. In this work, we\ndevelop byte-level LMs with learnable tokenizers to make tokenization adaptive.\nOur models include a submodule that learns to predict boundaries between the\ninput byte sequence, encoding it into variable-length segments. Existing\ntokenizer-free methods train this boundary predictor using an auxiliary loss\nthat enforces a fixed compression rate across the training corpus, introducing\na new kind of rigidity. We propose FLEXITOKENS, a simplified training objective\nthat enables significantly greater flexibility during adaptation. Evaluating\nacross multiple multilingual benchmarks, morphologically diverse tasks, and\ndomains, we demonstrate that FLEXITOKENS consistently reduces token\nover-fragmentation and achieves up to 10\\% improvements on downstream task\nperformance compared to subword and other gradient-based tokenizers. Code and\ndata for our experiments will be released at\nhttps://github.com/owos/flexitokens",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.12720.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "626d1e1e72169e781945bf44",
      "avatarUrl": "/avatars/6bf9f35042b6d939f2ab525816ad0423.svg",
      "fullname": "Abraham  Owodunni",
      "name": "Owos",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.04984",
      "authors": [
        {
          "_id": "68783633001546c83aa4f928",
          "user": {
            "_id": "67abb26debe64eaa3a624bd7",
            "avatarUrl": "/avatars/39259308f5aeda7f80a5489335554913.svg",
            "isPro": false,
            "fullname": "Zonglin Lyu",
            "user": "ucfzl",
            "type": "user"
          },
          "name": "Zonglin Lyu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-17T08:22:35.041Z",
          "hidden": false
        },
        {
          "_id": "68783633001546c83aa4f929",
          "name": "Chen Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-07T13:25:32.000Z",
      "submittedOnDailyAt": "2025-07-18T01:45:16.974Z",
      "title": "TLB-VFI : Reconnaissance du Temps Potentiel de la Bridge de Brouwer Diffusion dans les Frames de Vidéo Inter-Traitement",
      "submittedOnDailyBy": {
        "_id": "67abb26debe64eaa3a624bd7",
        "avatarUrl": "/avatars/39259308f5aeda7f80a5489335554913.svg",
        "isPro": false,
        "fullname": "Zonglin Lyu",
        "user": "ucfzl",
        "type": "user"
      },
      "summary": "La Interpolation des Frèmes de Vidéo (VFI) vise à prédire un nouveau frème intermédiaire \\( I_n \\) entre deux frèmes consécutives \\( I_0 \\) et \\( I_1 \\), évitant ainsi la charge temporelle et le signe. Les derniers méthodes ont appliqué des modèles de diffusion basés sur des images et des vidéos pour atteindre un rendement robuste. Cependant, les modèles basés sur des images ne peuvent extraire d'information temporelle, ce qui les rend plus anciens par rapport aux méthodes non diffusives. D'autre part, les modèles basés sur des vidéos peuvent extraire d'information temporelle, mais cela affecte négativement l'échelle d'entraînement, le taille du modèle et le temps d'inférence. Pour atténuer ces problèmes, nous proposons le Temporal-Aware Latent Brownian Bridge Diffusion for Video Frame Interpolation (TLB-VFI), qui utilise un gestionnaire 3D-ondelette et un autoencodeur qui connaît l'information temporelle. Ce méthode extrait une grande quantité d'information temporelle des vidéos et améliore de 20% le rendement en termes de FID par rapport aux modèles de diffusion basés sur des images récents. De plus, avec l'information temporelle riche, ce méthode peut atteindre un rendement robuste avec un nombre de paramètres trois fois moins. Cette réduction de paramètres peut conduire à une amélioration de la vitesse de 2,3 fois. En utilisant une ligne de gradient de couleurs ouverte, ce méthode nécessite environ 9.000 fois moins de données d'entraînement et atteint un nombre de paramètres 20 fois moins par rapport aux modèles de diffusion basés sur des vidéos. Les codes et les résultats peuvent être trouvés sur la page du projet : https://zonglinl.github.io/tlbvfi_page.",
      "upvotes": 4,
      "discussionId": "68783634001546c83aa4f92a",
      "projectPage": "https://zonglinl.github.io/tlbvfi_page/",
      "githubRepo": "https://github.com/ZonglinL/TLBVFI",
      "ai_summary": "Temporal-Aware Latent Brownian Bridge Diffusion for Video Frame Interpolation (TLB-VFI) improves video frame interpolation by efficiently extracting temporal information, reducing parameters, and requiring less training data compared to existing methods.",
      "ai_keywords": [
        "diffusion models",
        "video frame interpolation",
        "temporal information",
        "3D-wavelet gating",
        "temporal-aware autoencoder",
        "FID",
        "optical flow guidance"
      ],
      "githubStars": 9
    },
    "publishedAt": "2025-07-07T09:25:32.000Z",
    "title": "TLB-VFI: Temporal-Aware Latent Brownian Bridge Diffusion for Video Frame\n  Interpolation",
    "summary": "Video Frame Interpolation (VFI) aims to predict the intermediate frame I_n\n(we use n to denote time in videos to avoid notation overload with the timestep\nt in diffusion models) based on two consecutive neighboring frames I_0 and\nI_1. Recent approaches apply diffusion models (both image-based and\nvideo-based) in this task and achieve strong performance. However, image-based\ndiffusion models are unable to extract temporal information and are relatively\ninefficient compared to non-diffusion methods. Video-based diffusion models can\nextract temporal information, but they are too large in terms of training\nscale, model size, and inference time. To mitigate the above issues, we propose\nTemporal-Aware Latent Brownian Bridge Diffusion for Video Frame Interpolation\n(TLB-VFI), an efficient video-based diffusion model. By extracting rich\ntemporal information from video inputs through our proposed 3D-wavelet gating\nand temporal-aware autoencoder, our method achieves 20% improvement in FID on\nthe most challenging datasets over recent SOTA of image-based diffusion models.\nMeanwhile, due to the existence of rich temporal information, our method\nachieves strong performance while having 3times fewer parameters. Such a\nparameter reduction results in 2.3x speed up. By incorporating optical flow\nguidance, our method requires 9000x less training data and achieves over 20x\nfewer parameters than video-based diffusion models. Codes and results are\navailable at our project page: https://zonglinl.github.io/tlbvfi_page.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.04984.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67abb26debe64eaa3a624bd7",
      "avatarUrl": "/avatars/39259308f5aeda7f80a5489335554913.svg",
      "fullname": "Zonglin Lyu",
      "name": "ucfzl",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.11589",
      "authors": [
        {
          "_id": "687902bccc15e42a72b01ad6",
          "name": "Sandeep Suresh Cranganore",
          "hidden": false
        },
        {
          "_id": "687902bccc15e42a72b01ad7",
          "user": {
            "_id": "66bdb0025bdd611f9a008bec",
            "avatarUrl": "/avatars/468ec3aebcf798bff601583adbc3bf88.svg",
            "isPro": false,
            "fullname": "Bodnar",
            "user": "AndreiB137",
            "type": "user"
          },
          "name": "Andrei Bodnar",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-17T14:59:15.181Z",
          "hidden": false
        },
        {
          "_id": "687902bccc15e42a72b01ad8",
          "name": "Arturs Berzins",
          "hidden": false
        },
        {
          "_id": "687902bccc15e42a72b01ad9",
          "name": "Johannes Brandstetter",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-15T14:55:39.000Z",
      "submittedOnDailyAt": "2025-07-18T08:27:07.754Z",
      "title": "Le champ d'Einšhtain : Portefeuille Neuronal de la Relativité Universelle Computacionale",
      "submittedOnDailyBy": {
        "_id": "66bdb0025bdd611f9a008bec",
        "avatarUrl": "/avatars/468ec3aebcf798bff601583adbc3bf88.svg",
        "isPro": false,
        "fullname": "Bodnar",
        "user": "AndreiB137",
        "type": "user"
      },
      "summary": "EinFields introduit des représentations neuronales pour compresser les poids neuronaux cachés organisés nécessaires pour des simulations numériques de relativité géométrique en 4D efficaces. En modélisant le champ tensoriel fondamental de la relativité géométrique, les quantités physiques peuvent être différenciées automatiquement. Cependant, contrairement aux champs neuronaux généraux (par exemple, champs de distance signée, occupation, champs de radiance), EinFields émergent naturellement en tant que champs tensoriels neuronaux qui produisent des dynamiques lorsque l'on transforme la géométrie de l'espace-temps de la relativité géométrique en une représentation de champ neuronal. EinFields montre un potentiel remarquable dans le modélisation continue en 4D de l'espace-temps, l'indépendance des messages, l'efficacité de stockage, l'exactitude de la différentiation et l'utilisabilité. Ils affrontent ces défis à partir du banc d'essai standard de la relativité géométrique, en lançant une bibliothèque open-source basée sur JAX pour explorer une approche plus scalable et expressive en relativité numérique. Le code est disponible à https://github.com/AndreiB137/EinFields.",
      "upvotes": 0,
      "discussionId": "687902bdcc15e42a72b01ada",
      "githubRepo": "https://github.com/AndreiB137/EinFields",
      "ai_summary": "Einstein Fields, a neural tensor field representation, compresses four-dimensional numerical relativity simulations into neural network weights, enabling automatic differentiation and natural emergence of dynamics.",
      "ai_keywords": [
        "Einstein Fields",
        "neural representation",
        "implicit neural network",
        "metric",
        "general relativity",
        "neural tensor fields",
        "spacetime geometry",
        "automatic differentiation",
        "numerical relativity",
        "JAX-based library"
      ],
      "githubStars": 23
    },
    "publishedAt": "2025-07-15T10:55:39.000Z",
    "title": "Einstein Fields: A Neural Perspective To Computational General\n  Relativity",
    "summary": "We introduce Einstein Fields, a neural representation that is designed to\ncompress computationally intensive four-dimensional numerical relativity\nsimulations into compact implicit neural network weights. By modeling the\nmetric, which is the core tensor field of general relativity, Einstein\nFields enable the derivation of physical quantities via automatic\ndifferentiation. However, unlike conventional neural fields (e.g., signed\ndistance, occupancy, or radiance fields), Einstein Fields are Neural\nTensor Fields with the key difference that when encoding the spacetime\ngeometry of general relativity into neural field representations, dynamics\nemerge naturally as a byproduct. Einstein Fields show remarkable potential,\nincluding continuum modeling of 4D spacetime, mesh-agnosticity, storage\nefficiency, derivative accuracy, and ease of use. We address these challenges\nacross several canonical test beds of general relativity and release an open\nsource JAX-based library, paving the way for more scalable and expressive\napproaches to numerical relativity. Code is made available at\nhttps://github.com/AndreiB137/EinFields",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.11589.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66bdb0025bdd611f9a008bec",
      "avatarUrl": "/avatars/468ec3aebcf798bff601583adbc3bf88.svg",
      "fullname": "Bodnar",
      "name": "AndreiB137",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]