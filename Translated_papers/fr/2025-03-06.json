[
  {
    "paper": {
      "id": "2503.00865",
      "authors": [
        {
          "_id": "67c666245e2443d7d5e9b76a",
          "user": {
            "_id": "64802face9ff472e30dc1ceb",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64802face9ff472e30dc1ceb/bcwTlgpaUrU7m2RMB5zCc.png",
            "isPro": false,
            "fullname": "Yiran Zhao",
            "user": "Yiran0924",
            "type": "user"
          },
          "name": "Yiran Zhao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-04T08:51:21.231Z",
          "hidden": false
        },
        {
          "_id": "67c666245e2443d7d5e9b76b",
          "user": {
            "_id": "61657b0b20606e5e73f611cc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61657b0b20606e5e73f611cc/6ZPne2GYlWkxrx35ND1P8.png",
            "isPro": false,
            "fullname": "CHAOQUN LIU",
            "user": "lukecq",
            "type": "user"
          },
          "name": "Chaoqun Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-06T09:27:33.956Z",
          "hidden": false
        },
        {
          "_id": "67c666245e2443d7d5e9b76c",
          "name": "Yue Deng",
          "hidden": false
        },
        {
          "_id": "67c666245e2443d7d5e9b76d",
          "user": {
            "_id": "671609f7664f44a151f1f0e8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/fEQLuH1kdW5Pd9Y_J64hN.png",
            "isPro": false,
            "fullname": "jiahao ying",
            "user": "jhying",
            "type": "user"
          },
          "name": "Jiahao Ying",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-06T09:35:39.926Z",
          "hidden": false
        },
        {
          "_id": "67c666245e2443d7d5e9b76e",
          "user": {
            "_id": "6539c87ba318a98bf0d15dd8",
            "avatarUrl": "/avatars/beb9ba6eeacb61addc5897836bd59f55.svg",
            "isPro": false,
            "fullname": "Mahani Aljunied",
            "user": "maljunied",
            "type": "user"
          },
          "name": "Mahani Aljunied",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-06T09:35:33.285Z",
          "hidden": false
        },
        {
          "_id": "67c666245e2443d7d5e9b76f",
          "name": "Zhaodonghui Li",
          "hidden": false
        },
        {
          "_id": "67c666245e2443d7d5e9b770",
          "user": {
            "_id": "6454685a548f22be598414c4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/eMjMWKJ-AouF7eY1-RzGF.jpeg",
            "isPro": false,
            "fullname": "Lidong Bing",
            "user": "LidongBing",
            "type": "user"
          },
          "name": "Lidong Bing",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-06T09:35:19.611Z",
          "hidden": false
        },
        {
          "_id": "67c666245e2443d7d5e9b771",
          "user": {
            "_id": "604f67ef0fe8ff3ec13d71ef",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/604f67ef0fe8ff3ec13d71ef/KhUwWvZ3OJ9nEee3B-SXO.png",
            "isPro": false,
            "fullname": "Hou Pong (Ken) Chan",
            "user": "kenchan0226",
            "type": "user"
          },
          "name": "Hou Pong Chan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-06T09:35:50.272Z",
          "hidden": false
        },
        {
          "_id": "67c666245e2443d7d5e9b772",
          "name": "Yu Rong",
          "hidden": false
        },
        {
          "_id": "67c666245e2443d7d5e9b773",
          "name": "Deli Zhao",
          "hidden": false
        },
        {
          "_id": "67c666245e2443d7d5e9b774",
          "user": {
            "_id": "60dff6ae19a362a8c27862aa",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60dff6ae19a362a8c27862aa/LIYzLB3cdPh-B3XIBgBCC.jpeg",
            "isPro": false,
            "fullname": "Wenxuan Zhang",
            "user": "isakzhang",
            "type": "user"
          },
          "name": "Wenxuan Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-06T09:27:36.769Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-02T11:53:55.000Z",
      "title": "Bella: Le modèle de langage multilingue de Google offre des services à plus de 90% des utilisateurs du monde.",
      "summary": "Les modèles de langage de langage (LLMs) ont apporté des innovations au traitement du langage naturel (NLP), mais les modèles multilingues de LLMs open-source sont rares, et les modèles existants sont limités dans leur gamme de langues. Ces modèles généralement favorisent les langues riches en ressources et sont largement utilisées, tandis que les langues pauvres en ressources sont rejetées. Pour résoudre cette disparité, nous présentons Babel, un modèle de LLMs multilingue open-source. Ce modèle couvre 25 langues et soutient 90% de la population mondiale, y compris de nombreuses langues qui ont été rejetées par d'autres modèles multilingues open-source. Ces modèles diffèrent des autres par leur approche générale de l'apprentissage prédictif continu. La capacité des paramètres est élargie à travers des technologies de couches pour élever les limites de la performance. Nous présentons deux versions de Babel : Babel-9B et Babel-83B. La première se concentre sur l'efficacité de l'inférence et du fine-tuning, tandis que la seconde établit de nouvelles références pour les modèles de LLMs multilingues open-source. Dans une large évaluation de tâches multilingues, Babel montre un rendement relativement élevé par rapport aux autres modèles de LLMs open-source de même taille. De plus, en utilisant le jeu de données de super-fine-tuning de Super-Bible Dining Infinity, Babel atteint un rendement exceptionnel. Babel-9B-Chat est le meilleur modèle parmi ceux de 10B de taille, tandis que Babel-83B-Chat établit de nouvelles références dans les tâches multilingues et atteint un niveau de rendement de modèle de business.",
      "upvotes": 32,
      "discussionId": "67c666255e2443d7d5e9b7b3",
      "projectPage": "https://babel-llm.github.io/babel-llm/",
      "githubRepo": "https://github.com/babel-llm/babel-llm"
    },
    "publishedAt": "2025-03-05T21:49:03.700Z",
    "title": "Babel: Open Multilingual Large Language Models Serving Over 90% of Global Speakers",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.00865.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64802face9ff472e30dc1ceb",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64802face9ff472e30dc1ceb/bcwTlgpaUrU7m2RMB5zCc.png",
      "fullname": "Yiran Zhao",
      "name": "Yiran0924",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.00329",
      "authors": [
        {
          "_id": "67c755f898a2e37274c62c96",
          "name": "Benjamin Schneider",
          "hidden": false
        },
        {
          "_id": "67c755f898a2e37274c62c97",
          "name": "Florian Kerschbaum",
          "hidden": false
        },
        {
          "_id": "67c755f898a2e37274c62c98",
          "user": {
            "_id": "6313a86154e6e5d9f0f94e04",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
            "isPro": false,
            "fullname": "Wenhu Chen",
            "user": "wenhu",
            "type": "user"
          },
          "name": "Wenhu Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-06T09:50:07.881Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-01T03:29:02.000Z",
      "title": "ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC",
      "summary": "Les modèles d'embbeding visuels sont excellents pour des tâches telles que la recherche visuelle et la classification, mais ne peuvent pas être utilisés pour des tâches qui nécessitent des idées ou des indications du utilisateur. Ces tâches nécessitent un modèle d'embbeding multimodal qui combine des entrées visuelles et linguistiques. L'approche basée sur CLIP sépare l'image et le texte pour les embbeder et fusionne ensuite les résultats. Nous avons observé que cela entraîne une interaction faible entre les modalités et un mauvais contrôle de l'expression de l'utilisateur. Nous présentons ABC, un modèle d'embbeding multimodal ouvert qui intègre profondément les caractéristiques des images et des indications de langage naturel. ABC atteint les meilleurs résultats dans la recherche de texte à partir d'images de MSCOCO et est excellent dans les tâches de classification et VQA dans le cadre des tests multimodales de MAGIC. Avec sa représentation forte visuelle, ABC peut résoudre des problèmes visuels ambigus ou avec des idées non claires en utilisant uniquement du langage naturel. Pour évaluer cette capacité, nous avons conçu CtrlBench, un cadre de test qui échange de texte d'instruction et de contenu d'image pour rechercher des résultats précis. ABC fournit une qualité élevée d'expression et un contrôle flexible du langage naturel, améliorant le niveau de l'art dans l'embbeding multimodal. Notre modèle et ensemble de données sont disponibles sur la page du projet.",
      "upvotes": 10,
      "discussionId": "67c7560298a2e37274c6311d",
      "projectPage": "https://tiger-ai-lab.github.io/ABC/",
      "githubRepo": "https://github.com/TIGER-AI-Lab/ABC"
    },
    "publishedAt": "2025-03-05T21:33:37.945Z",
    "title": "ABC: Achieving Better Control of Multimodal Embeddings using VLMs",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6313a86154e6e5d9f0f94e04/b2vg-4UWwvcEboAZgK-Sv.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.00329.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6313a86154e6e5d9f0f94e04",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
      "fullname": "Wenhu Chen",
      "name": "wenhu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 33
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.03278",
      "authors": [
        {
          "_id": "67c94e5f8c4ef8be73583f4b",
          "name": "Jun Li",
          "hidden": false
        },
        {
          "_id": "67c94e5f8c4ef8be73583f4c",
          "user": {
            "_id": "631b9ff5824f2502e3557c7e",
            "avatarUrl": "/avatars/076043c9dba07644a570692563ef8114.svg",
            "isPro": false,
            "fullname": "liu",
            "user": "che111",
            "type": "user"
          },
          "name": "Che Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-06T10:03:06.882Z",
          "hidden": false
        },
        {
          "_id": "67c94e5f8c4ef8be73583f4d",
          "name": "Wenjia Bai",
          "hidden": false
        },
        {
          "_id": "67c94e5f8c4ef8be73583f4e",
          "name": "Rossella Arcucci",
          "hidden": false
        },
        {
          "_id": "67c94e5f8c4ef8be73583f4f",
          "name": "Cosmin I. Bercea",
          "hidden": false
        },
        {
          "_id": "67c94e5f8c4ef8be73583f50",
          "name": "Julia A. Schnabel",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-05T09:02:33.000Z",
      "title": "Amélioration de la base de connaissances basées sur les anomalies dans les modèles de langage visuolinguistique",
      "summary": "Les modèles de langage visuel (VLMs) ont démontré des capacités impressionnantes dans des tâches basées sur la vision. Cependant, l'efficacité, et en particulier, la détection et la localisation d'anomalies dans les images médicales, restent des domaines qui nécessitent une profondeur accrue. L'un des principaux défis est la complexité et l'abstraction des termes médicaux, ainsi que la difficulté de créer une association directe entre les échantillons d'anomalies pathologiques et les caractéristiques visuelles. Dans cet article, nous proposons une nouvelle approche pour améliorer la détection et la localisation d'anomalies dans les VLMs en utilisant des connaissances médicales décomposées. Au lieu de reconnaître directement le modèle pour certaines anomalies, nous nous concentrons sur la décomposition des concepts médicaux en attributs basiques et en motifs visuels communs. Cette stratégie favorise une meilleure concordance entre l'explication et les caractéristiques visuelles, améliorant à la fois la détection d'anomalies et leur localisation dans les images médicales. Notre méthode a été évaluée sur le modèle Florence-2 de 0.23B, atteignant un rendement similaire à celui d'un VLM médical basé sur LLaVA de 7B. En particulier, ce modèle améliore la détection d'anomalies, même lorsqu'il est entraîné sur seulement 1.5% des données. Les résultats des expérimentations montrent que notre approche démontre une efficacité et une forte capacité de généralisation tant pour des anomalies connues que pour des anomalies jamais vues avant.",
      "upvotes": 9,
      "discussionId": "67c94e608c4ef8be73583f7b",
      "projectPage": "https://lijunrio.github.io/AG-KD/"
    },
    "publishedAt": "2025-03-06T02:29:15.964Z",
    "title": "Enhancing Abnormality Grounding for Vision Language Models with Knowledge Descriptions",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.03278.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "631b9ff5824f2502e3557c7e",
      "avatarUrl": "/avatars/076043c9dba07644a570692563ef8114.svg",
      "fullname": "liu",
      "name": "che111",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.03751",
      "authors": [
        {
          "_id": "67c912b1b5903dd437cc2370",
          "user": {
            "_id": "658529d61c461dfe88afe8e8",
            "avatarUrl": "/avatars/a22c1b07d28c2662833c462c6537d835.svg",
            "isPro": false,
            "fullname": "Xuanchi Ren",
            "user": "xrenaa",
            "type": "user"
          },
          "name": "Xuanchi Ren",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-06T09:55:04.321Z",
          "hidden": false
        },
        {
          "_id": "67c912b1b5903dd437cc2371",
          "name": "Tianchang Shen",
          "hidden": false
        },
        {
          "_id": "67c912b1b5903dd437cc2372",
          "name": "Jiahui Huang",
          "hidden": false
        },
        {
          "_id": "67c912b1b5903dd437cc2373",
          "name": "Huan Ling",
          "hidden": false
        },
        {
          "_id": "67c912b1b5903dd437cc2374",
          "name": "Yifan Lu",
          "hidden": false
        },
        {
          "_id": "67c912b1b5903dd437cc2375",
          "name": "Merlin Nimier-David",
          "hidden": false
        },
        {
          "_id": "67c912b1b5903dd437cc2376",
          "name": "Thomas Müller",
          "hidden": false
        },
        {
          "_id": "67c912b1b5903dd437cc2377",
          "name": "Alexander Keller",
          "hidden": false
        },
        {
          "_id": "67c912b1b5903dd437cc2378",
          "name": "Sanja Fidler",
          "hidden": false
        },
        {
          "_id": "67c912b1b5903dd437cc2379",
          "name": "Jun Gao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-05T18:59:50.000Z",
      "title": "GEN3C : Génération de mythologies mondiales à l'aide d'information 3D et de contrôle précis des caméras",
      "summary": "GEN3C est un modèle d'images génératives qui possède la caractéristique de contrôle structurel de la caméra et de cohérence 3D dans le temps. Les modèles d'images existants généraient des images réalistes, mais avec peu d'informations 3D et de nombreuses incertitudes sur l'existence des objets. Le contrôle de la caméra nécessite que les paramètres de la caméra soient entrée pour la réseau neuronal, ce qui rend la déduction de la manière dont l'image change par rapport à la caméra complexe et difficile à contrôler. GEN3C utilise un tampon de capture 3D comme guide : on prédit la profondeur pixel par pixel d'une image de seed ou d'un frame généré précédemment, et on l'utilise conjointement avec les nouvelles actions de caméra fournies par l'utilisateur et la rendu 2D du tampon 3D pour générer le prochain frame. Un point clé est que GEN3C ne nécessite pas mémoriser ce qui a été généré précédemment ni inférer la structure de l'image à partir de la position de la caméra, ce qui lui permet de concentrer sa génération sur des zones jamais vues avant. Le modèle exploite sa génération dans des zones jamais vues avant et peut continuer à simuler le prochain frame. Nos résultats montrent un contrôle de la caméra plus précis que les technologies existantes et présentent également des résultats de pointe en synthèse de nouvelles vues dans des vues rares. En particulier, il montre un excellent rendement même dans des configurations difficiles comme des scénarios de rotation et des images avec peu de variation. Les résultats sont plus clairement visibles dans l'image. Consultez notre page web : https://research.nvidia.com/labs/toronto-ai/GEN3C/",
      "upvotes": 9,
      "discussionId": "67c912b9b5903dd437cc2505"
    },
    "publishedAt": "2025-03-05T22:13:22.552Z",
    "title": "GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.03751.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6288
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.02951",
      "authors": [
        {
          "_id": "67c907ea7568a12737ad4535",
          "user": {
            "_id": "653df1323479e9ebbe3eb6cc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653df1323479e9ebbe3eb6cc/K_g-r1iMRNKj99LXPuYF3.jpeg",
            "isPro": true,
            "fullname": "Zhangchen Xu",
            "user": "flydust",
            "type": "user"
          },
          "name": "Zhangchen Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-06T09:26:50.636Z",
          "hidden": false
        },
        {
          "_id": "67c907ea7568a12737ad4536",
          "user": {
            "_id": "637c88b6d55081513c5690d8",
            "avatarUrl": "/avatars/6766e23ebf46b46d6c8b48351c571907.svg",
            "isPro": false,
            "fullname": "Yang Liu",
            "user": "nlpyang",
            "type": "user"
          },
          "name": "Yang Liu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-06T02:26:54.940Z",
          "hidden": false
        },
        {
          "_id": "67c907ea7568a12737ad4537",
          "user": {
            "_id": "605e8dfd5abeb13e714c4c18",
            "avatarUrl": "/avatars/bc27a0ed17b2bd4311e89d3028fa327b.svg",
            "isPro": false,
            "fullname": "yueqin yin",
            "user": "yyqoni",
            "type": "user"
          },
          "name": "Yueqin Yin",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-06T09:26:48.614Z",
          "hidden": false
        },
        {
          "_id": "67c907ea7568a12737ad4538",
          "user": {
            "_id": "653b2524b77b5e255f2d29d2",
            "avatarUrl": "/avatars/f69aea8de84c435295e7638bad5bd82e.svg",
            "isPro": false,
            "fullname": "Mingyuan Zhou",
            "user": "mingyuanzhou",
            "type": "user"
          },
          "name": "Mingyuan Zhou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-06T10:03:56.474Z",
          "hidden": false
        },
        {
          "_id": "67c907ea7568a12737ad4539",
          "name": "Radha Poovendran",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-04T19:17:36.000Z",
      "title": "KodCode : Diversité, difficulté, ensemble de données synthétiques vérifiables.",
      "summary": "KodCode est un ensemble de données de code synthétique conçu pour résoudre les problèmes à long terme de l'obtention de données d'entraînement de haute qualité pour l'apprentissage du modèle de langage de code de Long-Step Route. Les ressources actuelles axées sur la codification ne peuvent pas garantir autant une large couverture (de tarefs de code simples à des problèmes algorithmiques de haut niveau de difficulté) que une précision vérifiable (comme des tests unitaires). D'autre part, KodCode configure des problèmes-solutions-tests avec un processus de certification automatique systématique. Notre pipeline génère du code pour une grande variété de problèmes et distribue des expériences supplémentaires pour résoudre des problèmes plus difficiles, créant des solutions et des cas de tests. Enfin, la synthèse de données après l'entraînement change les requêtes et génère des réponses à travers un processus d'échantillonnage de échantillons rejetés de la base de tests de DeepSeek R1. Ce pipeline génère des ensembles de données de code large, robustes et divers. KodCode facilite les entraînements normaux et a un potentiel dans les tests de paires d'unités et l'ajustement de RL. Les expériences d'entraînement dans les référentiels de codification (HumanEval(+), MBPP(+), BigCodeBench, LiveCodeBench) montrent que les modèles ajustés avec KodCode atteignent le meilleur rendement et montrent un comportement supérieur à Qwen2.5-Coder-32B-Instruct et DeepSeek-R1-Distill-Llama-70B.",
      "upvotes": 7,
      "discussionId": "67c907ee7568a12737ad4633",
      "projectPage": "https://kodcode-ai.github.io/",
      "githubRepo": "https://github.com/KodCode-AI/kodcode"
    },
    "publishedAt": "2025-03-05T21:31:01.626Z",
    "title": "KodCode: A Diverse, Challenging, and Verifiable Synthetic Dataset for Coding",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.02951.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "653df1323479e9ebbe3eb6cc",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653df1323479e9ebbe3eb6cc/K_g-r1iMRNKj99LXPuYF3.jpeg",
      "fullname": "Zhangchen Xu",
      "name": "flydust",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isMod": false,
      "followerCount": 13
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.01836",
      "authors": [
        {
          "_id": "67c94c32dd505e6a4db201a2",
          "user": {
            "_id": "65d9a453f20e4fc19480afba",
            "avatarUrl": "/avatars/27bfa034a13a5e7cb5fc3b647515a201.svg",
            "isPro": false,
            "fullname": "yisen li",
            "user": "yisenL",
            "type": "user"
          },
          "name": "Yisen Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-06T10:04:09.687Z",
          "hidden": false
        },
        {
          "_id": "67c94c32dd505e6a4db201a3",
          "name": "Lingfeng Yang",
          "hidden": false
        },
        {
          "_id": "67c94c32dd505e6a4db201a4",
          "name": "Wenxuan Shen",
          "hidden": false
        },
        {
          "_id": "67c94c32dd505e6a4db201a5",
          "name": "Pan Zhou",
          "hidden": false
        },
        {
          "_id": "67c94c32dd505e6a4db201a6",
          "name": "Yao Wan",
          "hidden": false
        },
        {
          "_id": "67c94c32dd505e6a4db201a7",
          "name": "Weiwei Lin",
          "hidden": false
        },
        {
          "_id": "67c94c32dd505e6a4db201a8",
          "user": {
            "_id": "643be8879f5d314db2d9ed23",
            "avatarUrl": "/avatars/64e9bb2c4e10fbe03e2b81afedf40865.svg",
            "isPro": false,
            "fullname": "Chen Dongping",
            "user": "shuaishuaicdp",
            "type": "user"
          },
          "name": "Dongping Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-06T10:09:00.496Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-03T18:56:44.000Z",
      "title": "CrowdSelect : Sélection de données de commandes complexes pour le connaissance de plusieurs modèles de langage",
      "summary": "Une méthode qui utilise des sous-ensembles sélectionnés par leur capacité à suivre des règles d'orientation a devenu l'état de l'art dans l'entraînement de modèles. La stratégie actuelle de sélection des données d'orientation est principalement basée sur des signaux unidimensionnels (par exemple, des scores de récompense, de difficulté du modèle), ce qui ne capture pas la complexité de suivre des règles dans divers domaines. Nous avons donc abordé ce défi en proposant trois métriques fondamentales basées sur le savoir de plusieurs modèles de langage grands (Multi-LLM) et leur utilisation dans un approche axée sur le clustering appelée CrowdSelect. Nos résultats de validation montrent un amélioration de 4,81% dans Arena-Hard et de 11,1% sur MT-bench en utilisant Llama-3.2-3b-instruct. Notre travail fournit des domaines d'étude utiles pour des études futures. Le code est disponible sur https://github.com/listentm/crowdselect.",
      "upvotes": 6,
      "discussionId": "67c94c33dd505e6a4db201f6",
      "githubRepo": "https://github.com/listentm/crowdselect"
    },
    "publishedAt": "2025-03-06T02:20:38.735Z",
    "title": "CrowdSelect: Synthetic Instruction Data Selection with Multi-LLM Wisdom",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01836.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "643be8879f5d314db2d9ed23",
      "avatarUrl": "/avatars/64e9bb2c4e10fbe03e2b81afedf40865.svg",
      "fullname": "Chen Dongping",
      "name": "shuaishuaicdp",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.18860",
      "authors": [
        {
          "_id": "67c95acf88c3b4201c10b9e9",
          "name": "Md Mehrab Tanjim",
          "hidden": false
        },
        {
          "_id": "67c95acf88c3b4201c10b9ea",
          "name": "Ryan A. Rossi",
          "hidden": false
        },
        {
          "_id": "67c95acf88c3b4201c10b9eb",
          "name": "Mike Rimer",
          "hidden": false
        },
        {
          "_id": "67c95acf88c3b4201c10b9ec",
          "name": "Xiang Chen",
          "hidden": false
        },
        {
          "_id": "67c95acf88c3b4201c10b9ed",
          "name": "Sungchul Kim",
          "hidden": false
        },
        {
          "_id": "67c95acf88c3b4201c10b9ee",
          "name": "Vaishnavi Muppala",
          "hidden": false
        },
        {
          "_id": "67c95acf88c3b4201c10b9ef",
          "name": "Tong Yu",
          "hidden": false
        },
        {
          "_id": "67c95acf88c3b4201c10b9f0",
          "name": "Zhengmian Hu",
          "hidden": false
        },
        {
          "_id": "67c95acf88c3b4201c10b9f1",
          "name": "Ritwik Sinha",
          "hidden": false
        },
        {
          "_id": "67c95acf88c3b4201c10b9f2",
          "name": "Wei Zhang",
          "hidden": false
        },
        {
          "_id": "67c95acf88c3b4201c10b9f3",
          "name": "Iftikhar Ahamath Burhanuddin",
          "hidden": false
        },
        {
          "_id": "67c95acf88c3b4201c10b9f4",
          "user": {
            "_id": "62c5947524171688a9feb992",
            "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
            "isPro": false,
            "fullname": "Franck Dernoncourt",
            "user": "Franck-Dernoncourt",
            "type": "user"
          },
          "name": "Franck Dernoncourt",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-06T09:24:24.968Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-26T06:05:29.000Z",
      "title": "Réponse : Méthodes de rétroaction développées selon les différences dans les tâches de conversation",
      "summary": "Le conversateur socio parfois nécessite un algorithme de changement de questions pour offrir des réponses significatives (exactes) aux questions ou demandes de l'utilisateur, en utilisant une partie des interactions passées. Cependant, le mode précis de changement dépend des tâches que le conversateur socio soutient ou d'autres restrictions. Dans cet article, nous étudions systématiquement deux approches différentes dans deux tâches génératives essentiellement distinctes : \"changement\" et \"synthèse\". Ces tâches comprennent la génération de texte à partir de texte et la création d'images ou de tableaux de données en réponse à des questions de l'utilisateur, à partir de texte comme entrée. Nos résultats montrent que certaine approche de changement ou de synthèse est plus adaptée aux cas légers et aux tâches génératives. En particulier, l'approche de changement de questions est plus adaptée pour un conversateur socio, tandis que l'approche de synthèse de questions est plus adaptée pour un analyste de données socio. En particulier, dans les cas d'utilisation d'un analyste de données socio, l'étude de deux types de jeux de données courts et longs montre que la synthèse de questions montre toujours des meilleurs résultats, tandis que l'approche de changement de questions est la plus adaptée pour la réponse à des questions basées sur le texte de conversation.",
      "upvotes": 2,
      "discussionId": "67c95acf88c3b4201c10ba22"
    },
    "publishedAt": "2025-03-06T03:20:40.127Z",
    "title": "Exploring Rewriting Approaches for Different Conversational Tasks",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.18860.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62c5947524171688a9feb992",
      "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
      "fullname": "Franck Dernoncourt",
      "name": "Franck-Dernoncourt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.03044",
      "authors": [
        {
          "_id": "67c94e6ad325e95d82f23433",
          "user": {
            "_id": "5e7749883d77a72421292d07",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1670231290373-5e7749883d77a72421292d07.jpeg",
            "isPro": false,
            "fullname": "Gabriele Sarti",
            "user": "gsarti",
            "type": "user"
          },
          "name": "Gabriele Sarti",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-06T09:24:30.601Z",
          "hidden": false
        },
        {
          "_id": "67c94e6ad325e95d82f23434",
          "name": "Vilém Zouhar",
          "hidden": false
        },
        {
          "_id": "67c94e6ad325e95d82f23435",
          "name": "Grzegorz Chrupała",
          "hidden": false
        },
        {
          "_id": "67c94e6ad325e95d82f23436",
          "name": "Ana Guerberof-Arenas",
          "hidden": false
        },
        {
          "_id": "67c94e6ad325e95d82f23437",
          "name": "Malvina Nissim",
          "hidden": false
        },
        {
          "_id": "67c94e6ad325e95d82f23438",
          "name": "Arianna Bisazza",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-04T22:50:17.000Z",
      "title": "QE4PE : Outil d'évaluation de la qualité du niveau de langue pour corriger à la main ensuite",
      "summary": "L'évaluation de la qualité à l'échelle de la mot (QE) détecte des erreurs dans les traductions automatiques et guide et encourage l'édition humaine subséquente. La précision des systèmes de QE à l'échelle de la mot a été largement évaluée ; cependant, sa possibilité d'utilisation, son impact sur la vitesse, la qualité et les décisions de l'édition humaine nécessitent encore plus d'investigation. Notre équipe de recherche, QE4PE, étudie l'impact de la QE à l'échelle de la mot sur l'édition subséquente des traductions automatiques dans un environnement pratique, où participaient 42 éditeurs professionnels de 2 directions de traduction. Nous comparons différents modes de mise en lumière pour identifier les erreurs potentielles dans les sorties des modèles de MT les plus avancés. Nous évaluons la productivité de l'édition subséquente et l'amélioration de la qualité par analyse à l'échelle de la mot et de la phrase. Nous avons constaté que la vitesse de travail, le langage et les éditeurs sont des facteurs cruciaux pour l'effet des mises en lumière. De plus, nous avons observé qu'il y a une petite différence entre les mises en lumière humaines et celles automatisées, ce qui démontre l'intérêt de trouver un équilibre entre précision et efficacité.",
      "upvotes": 2,
      "discussionId": "67c94e6fd325e95d82f23524"
    },
    "publishedAt": "2025-03-06T02:30:17.431Z",
    "title": "QE4PE: Word-level Quality Estimation for Human Post-Editing",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.03044.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5e7749883d77a72421292d07",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1670231290373-5e7749883d77a72421292d07.jpeg",
      "fullname": "Gabriele Sarti",
      "name": "gsarti",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 213
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.20317",
      "authors": [
        {
          "_id": "67c95b2f1fcfdc62ba3a620b",
          "name": "Yongjia Lei",
          "hidden": false
        },
        {
          "_id": "67c95b2f1fcfdc62ba3a620c",
          "name": "Haoyu Han",
          "hidden": false
        },
        {
          "_id": "67c95b2f1fcfdc62ba3a620d",
          "name": "Ryan A. Rossi",
          "hidden": false
        },
        {
          "_id": "67c95b2f1fcfdc62ba3a620e",
          "user": {
            "_id": "62c5947524171688a9feb992",
            "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
            "isPro": false,
            "fullname": "Franck Dernoncourt",
            "user": "Franck-Dernoncourt",
            "type": "user"
          },
          "name": "Franck Dernoncourt",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-06T09:24:22.970Z",
          "hidden": false
        },
        {
          "_id": "67c95b2f1fcfdc62ba3a620f",
          "name": "Nedim Lipka",
          "hidden": false
        },
        {
          "_id": "67c95b2f1fcfdc62ba3a6210",
          "user": {
            "_id": "637c6d95a8716d642050b50f",
            "avatarUrl": "/avatars/0955a10113807348f24db968c7bd7c7a.svg",
            "isPro": false,
            "fullname": "Mahantesh Halappanavar",
            "user": "mhalappa",
            "type": "user"
          },
          "name": "Mahantesh M Halappanavar",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-06T08:22:09.090Z",
          "hidden": false
        },
        {
          "_id": "67c95b2f1fcfdc62ba3a6211",
          "name": "Jiliang Tang",
          "hidden": false
        },
        {
          "_id": "67c95b2f1fcfdc62ba3a6212",
          "name": "Yu Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-27T17:42:52.000Z",
      "title": "La fusion des structures de graphes et du contexte dans la recherche basée sur le savoir",
      "summary": "La Technique de Récupération Mixte de Structure et de Texte (MoR) propose de combiner la recherche structurale et textuelle pour aborder le problème de la réponse à des requêtes dans le contexte de connaissances basées sur des graphes (TG-KB). Actuellement, les méthodes de recherche considèrent séparément ces deux types de connaissances, ce qui limite leur efficacité. MoR utilise un cadre de planification, de raisonnement et d'organisation pour intégrer ces deux formes de recherche. Dans la phase de planification, MoR génère un graphe de planification pour clarifier la logique de la réponse à la requête. Dans la phase de raisonnement, il combine la recherche structurale et la recherche par coincidence textuelle pour obtenir des candidats dans le TG-KB. Finalement, dans la phase d'organisation, MoR réécale les candidats en fonction du trafic structurel. Les expérimentations montrent que MoR améliore l'efficacité de la recherche structurale et textuelle, réduit l'inégalité dans la qualité de la réponse et améliore la réécale des candidats grâce au trafic structurel. Le code est disponible sur GitHub : https://github.com/Yoega/MoR.",
      "upvotes": 1,
      "discussionId": "67c95b311fcfdc62ba3a62a5"
    },
    "publishedAt": "2025-03-06T03:22:14.664Z",
    "title": "Mixture of Structural-and-Textual Retrieval over Text-rich Graph Knowledge Bases",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.20317.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62c5947524171688a9feb992",
      "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
      "fullname": "Franck Dernoncourt",
      "name": "Franck-Dernoncourt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.01763",
      "authors": [
        {
          "_id": "67c92e9c746bbcdbdfa8ebd4",
          "name": "Zhengliang Shi",
          "hidden": false
        },
        {
          "_id": "67c92e9c746bbcdbdfa8ebd5",
          "name": "Yuhan Wang",
          "hidden": false
        },
        {
          "_id": "67c92e9c746bbcdbdfa8ebd6",
          "name": "Lingyong Yan",
          "hidden": false
        },
        {
          "_id": "67c92e9c746bbcdbdfa8ebd7",
          "name": "Pengjie Ren",
          "hidden": false
        },
        {
          "_id": "67c92e9c746bbcdbdfa8ebd8",
          "name": "Shuaiqiang Wang",
          "hidden": false
        },
        {
          "_id": "67c92e9c746bbcdbdfa8ebd9",
          "name": "Dawei Yin",
          "hidden": false
        },
        {
          "_id": "67c92e9c746bbcdbdfa8ebda",
          "name": "Zhaochun Ren",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-03T17:37:16.000Z",
      "title": "Les modèles d'évaluation n'ont pas suffisamment de connaissances sur un instrument spécifique : ils sont utilisés comme référentiels pour évaluer la qualité des modèles de langage grands.",
      "summary": "L'objectif de l'apprentissage par outils est d'utiliser de grands modèles de langage (LLMs) qui disposent d'une variété d'outils pour jouer le rôle d'agent résolvant des tâches pratiques. Il est important de considérer la limitation de la longueur du contexte des LLMs qui utilisent des outils, et de sélectionner des outils utiles à partir d'un grand ensemble d'outils en utilisant des modèles de recherche d'information (IR). Cependant, le rendement des modèles IR dans les tâches de recherche d'outils n'a pas été exhaustivement étudié. Dans les cadres d'évaluation d'outils, ces tâches sont simplifiées en utilisant de petites collections d'outils liés à chaque tâche, mais cela s'éloigne des scénarios réels. Dans cet article, nous proposons un nouveau cadre d'évaluation de recherche d'outils appelé \"ToolRet\", qui consiste en 7.6k tâches de recherche différentes et un ensemble de 43k outils. Dans ToolRet, six modèles sont évalués. Surprenant, les modèles qui montrent des résultats exceptionnels dans les cadres traditionnels d'IR présentent des rendements faibles dans ToolRet. Ce faible rendement de recherche affecte la taux de succès dans les tâches des LLMs qui utilisent des outils. De plus, nous fournissons un ensemble de données d'entraînement de plus de 200k instances pour améliorer significativement la capacité des modèles IR à la recherche d'outils.",
      "upvotes": 1,
      "discussionId": "67c92e9e746bbcdbdfa8ec57"
    },
    "publishedAt": "2025-03-06T00:12:07.867Z",
    "title": "Retrieval Models Aren't Tool-Savvy: Benchmarking Tool Retrieval for Large Language Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01763.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "640d3eaa3623f6a56dde856d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678589663024-640d3eaa3623f6a56dde856d.jpeg",
      "fullname": "vansin",
      "name": "vansin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.01729",
      "authors": [
        {
          "_id": "67c92e8b5650d7efeba5b48c",
          "name": "Santiago Bou Betran",
          "hidden": false
        },
        {
          "_id": "67c92e8b5650d7efeba5b48d",
          "name": "Alberta Longhini",
          "hidden": false
        },
        {
          "_id": "67c92e8b5650d7efeba5b48e",
          "name": "Miguel Vasco",
          "hidden": false
        },
        {
          "_id": "67c92e8b5650d7efeba5b48f",
          "name": "Yuchong Zhang",
          "hidden": false
        },
        {
          "_id": "67c92e8b5650d7efeba5b490",
          "name": "Danica Kragic",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-03T16:49:15.000Z",
      "title": "FLAME : Benchmark d'Apprentissage Fédéré pour la Manipulation de Robotique",
      "summary": "Le développement récent de la manipulation de robots est impulsé par des grands ensembles de données collectés dans différents environnements. Ce type de données a été utilisé pour entraîner les politiques de manipulation de robots, cette pratique étant jusqu'à présent axée sur des modèles motivés, avec des préoccupations sur l'échelle, l'adaptabilité et la confidentialité des données. D'autre part, l'apprentissage federé (federated learning) permet la distribution et la protection de la confidentialité pendant l'apprentissage, ce qui a permis son application dans le domaine de la manipulation de robots. Nous présentons FLAME (Federated Learning in Manipulation Environments), le premier benchmark approprié pour la manipulation de robots. FLAME comprend deux composants principaux : (i) un grand ensemble de données qui comprend plus de 160 000 démonstrations de tâches de manipulation d'experts ; (ii) un cadre de travail pour entraîner et évaluer les politiques de robots dans un environnement d'apprentissage federé. Dans FLAME, on évalue des algorithmes standards d'apprentissage federé, et on explore la possibilité d'entraînement de politiques distribuées, ainsi que des problèmes importants. Ce benchmark constitue la base pour la construction d'un apprentissage de robots échelonnable et adaptable.",
      "upvotes": 1,
      "discussionId": "67c92e8d5650d7efeba5b519"
    },
    "publishedAt": "2025-03-06T00:11:48.501Z",
    "title": "FLAME: A Federated Learning Benchmark for Robotic Manipulation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01729.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "640d3eaa3623f6a56dde856d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678589663024-640d3eaa3623f6a56dde856d.jpeg",
      "fullname": "vansin",
      "name": "vansin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.01449",
      "authors": [
        {
          "_id": "67c92e738d5fe8c860571103",
          "name": "Ting Zhang",
          "hidden": false
        },
        {
          "_id": "67c92e738d5fe8c860571104",
          "name": "Chengran Yang",
          "hidden": false
        },
        {
          "_id": "67c92e738d5fe8c860571105",
          "name": "Yindu Su",
          "hidden": false
        },
        {
          "_id": "67c92e738d5fe8c860571106",
          "name": "Martin Weyssow",
          "hidden": false
        },
        {
          "_id": "67c92e738d5fe8c860571107",
          "name": "Hung Nguyen",
          "hidden": false
        },
        {
          "_id": "67c92e738d5fe8c860571108",
          "name": "Tan Bui",
          "hidden": false
        },
        {
          "_id": "67c92e738d5fe8c860571109",
          "name": "Hong Jin Kang",
          "hidden": false
        },
        {
          "_id": "67c92e738d5fe8c86057110a",
          "name": "Yikun Li",
          "hidden": false
        },
        {
          "_id": "67c92e738d5fe8c86057110b",
          "name": "Eng Lieh Ouh",
          "hidden": false
        },
        {
          "_id": "67c92e738d5fe8c86057110c",
          "name": "Lwin Khin Shar",
          "hidden": false
        },
        {
          "_id": "67c92e738d5fe8c86057110d",
          "name": "David Lo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-03T11:56:00.000Z",
      "title": "Vérifiez la version du fichier. Téléchargez le fichier de vérification de la version.",
      "summary": "Le développement récent des modèles de langage génératifs (Generative AI) a démultiplié l'introduction de grands modèles de langage (LLMs) dans le domaine de l'ingénierie logicielle, résolvant des problèmes qui n'avaient pas été abordés depuis longtemps. Cependant, dans le domaine importante de la sécurité logicielle, la détection de vulnérabilités (SVD), la capacité des LLMs a été peu étudiée. Actuellement, la plupart des études évaluent les LLMs en utilisant des ensembles de données C/C++. Généralement, on examine un ou deux des aspects suivants : ingénierie de projets, entraînement de commandes, classification de l'ordre final. En conséquence de ces études, on constate une notable absence de connaissances sur l'efficacité des LLMs dans la détection de vulnérabilités dans divers langages de programmation. Pour aborder cette lacune, un étude expérimentale spécifique est menée pour évaluer le rendement des LLMs dans la SVD. Le projet est préparé pour évaluer 5 modèles de LLMs de petit taille final et 2 outils de sécurité statique de code de projets en utilisant diverses méthodologies, comme ingénierie de projets, entraînement de commandes et classification de l'ordre final. Ces modèles sont comparés à 5 modèles de LLMs final de petit taille et 2 outils de sécurité statique de code de projets. De plus, deux approches sont étudiées pour améliorer le rendement de la SVD dans les LLMs : la vision des données et la vision du modèle. Dans la vision des données, le modèle est réentraîné en utilisant des ensembles de données équilibrés obtenus par sous-échantillonnage. Dans la vision du modèle, on explore le méthode d'entraînement d'ensemble qui combine les prédictions de plusieurs modèles de LLMs. Les résultats spécifiques des expériences montrent que la SVD reste un défi pour les LLMs. Cette étude fournit une compréhension détaillée des rôles joués par les LLMs dans la SVD et offre des directives pratiques pour améliorer la pratique de sécurité logicielle en utilisant l'AI générative.",
      "upvotes": 1,
      "discussionId": "67c92e748d5fe8c860571142"
    },
    "publishedAt": "2025-03-06T00:11:25.013Z",
    "title": "Benchmarking Large Language Models for Multi-Language Software Vulnerability Detection",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01449.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "640d3eaa3623f6a56dde856d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678589663024-640d3eaa3623f6a56dde856d.jpeg",
      "fullname": "vansin",
      "name": "vansin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.01378",
      "authors": [
        {
          "_id": "67c92e537ae0115c7a7b9fa3",
          "name": "Artem Lykov",
          "hidden": false
        },
        {
          "_id": "67c92e537ae0115c7a7b9fa4",
          "name": "Valerii Serpiva",
          "hidden": false
        },
        {
          "_id": "67c92e537ae0115c7a7b9fa5",
          "name": "Muhammad Haris Khan",
          "hidden": false
        },
        {
          "_id": "67c92e537ae0115c7a7b9fa6",
          "name": "Oleg Sautenkov",
          "hidden": false
        },
        {
          "_id": "67c92e537ae0115c7a7b9fa7",
          "name": "Artyom Myshlyaev",
          "hidden": false
        },
        {
          "_id": "67c92e537ae0115c7a7b9fa8",
          "name": "Grik Tadevosyan",
          "hidden": false
        },
        {
          "_id": "67c92e537ae0115c7a7b9fa9",
          "name": "Yasheerah Yaqoot",
          "hidden": false
        },
        {
          "_id": "67c92e537ae0115c7a7b9faa",
          "name": "Dzmitry Tsetserukou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-03T10:21:36.000Z",
      "title": "Cocagoden : Tâches cognitives en temps réel dans les UAVs et le modèle VLA de logique et de cadres d'évaluation",
      "summary": "Dans cet article, nous présentons un nouveau modèle de Vision-Langue-Action (VLA) appelé \"CognitiveDrone\" adapté aux tâches complexes d'avions sans pilote (UAV). Ce modèle est basé sur un ensemble de données d'entraînement de plus de 8 000 exemples et génère des commandes d'action 4D séquentiellement selon des entrées visuelles et des indications textuelles, dans trois catégories : reconnaissance humaine, compréhension de symboles et logique. Pour améliorer son performance dans des scénarios complexes, il inclut un module de logique supplémentaire basé sur un modèle de Vision-Langue (VLM), et propose \"CognitiveDrone-R1\" pour simplifier les instructions avant le contrôle de haute fréquence. Selon les évaluations des expériences réalisées avec notre benchmark ouvert \"CognitiveDroneBench\", le modèle spécialisé pour la course (RaceVLA) a atteint un succès total de 31,3 %, tandis que le modèle de base de CognitiveDrone a atteint un succès de 59,6 % et CognitiveDrone-R1 un succès de 77,2 %. Ces résultats montrent une amélioration significative d'environ 30 % dans des tâches cognitives importantes et mettent en évidence l'effet de l'intégration de capacités de logique avancée dans les systèmes de contrôle d'avions sans pilote. Notre contribution comprend le développement d'un des modèles VLA les plus avancés et l'introduction du premier benchmark spécialisé pour l'évaluation des tâches cognitives dans la gestion d'avions sans pilote. Le dépôt complet est disponible sur cognitivedrone.github.io.",
      "upvotes": 1,
      "discussionId": "67c92e547ae0115c7a7b9fe6"
    },
    "publishedAt": "2025-03-06T00:10:56.364Z",
    "title": "CognitiveDrone: A VLA Model and Evaluation Benchmark for Real-Time Cognitive Task Solving and Reasoning in UAVs",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01378.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "640d3eaa3623f6a56dde856d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678589663024-640d3eaa3623f6a56dde856d.jpeg",
      "fullname": "vansin",
      "name": "vansin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.00502",
      "authors": [
        {
          "_id": "67c8427047c2aa135346dced",
          "user": {
            "_id": "66d3290364c1e9b73208af82",
            "avatarUrl": "/avatars/e0ea5f2b366927c7b146f248028a2e59.svg",
            "isPro": false,
            "fullname": "Shiyu Fang",
            "user": "FanGShiYuu",
            "type": "user"
          },
          "name": "Shiyu Fang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-05T15:46:48.691Z",
          "hidden": false
        },
        {
          "_id": "67c8427047c2aa135346dcee",
          "name": "Jiaqi Liu",
          "hidden": false
        },
        {
          "_id": "67c8427047c2aa135346dcef",
          "name": "Chengkai Xu",
          "hidden": false
        },
        {
          "_id": "67c8427047c2aa135346dcf0",
          "name": "Chen Lv",
          "hidden": false
        },
        {
          "_id": "67c8427047c2aa135346dcf1",
          "name": "Peng Hang",
          "hidden": false
        },
        {
          "_id": "67c8427047c2aa135346dcf2",
          "name": "Jian Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-01T14:15:52.000Z",
      "title": "Interagir, Instruire pour Améliorer : Marco de Systèmes de Réponses Doubles Encouragés par un Modèle de Langue Automatique pour Améliorer les Interactions des Véhicules Autonomes",
      "summary": "Les véhicules autonomes (VAs) entrent dans la phase d'commercialisation, mais leur capacité limitée d'interaction et d'expression d'intentions reste un problème dans leur interaction avec les véhicules humains (VHs). Les récents avancés des grands modèles de langage (GMLs) ont facilité la communication bidirectionnelle entre les humains et les machines, mais présentent des problèmes en raison de la lenteur de l'inférence et de la nécessité de prendre des décisions en temps réel. Pour résoudre ces problèmes, cet article présente un cadre de langage acteur-reasoner pour permettre une interaction claire et bidirectionnelle dans de multiples scénarios entre VAs et VHs. Tout d'abord, on introduit le nom d'« acteur » pour le registre de mémoire qui favorise l'interaction avec d'autres VHs et le registre de mémoire du GML. Ensuite, on introduit les modules de partitionnement de la mémoire et de la recherche de la mémoire à deux couches pour améliorer significativement la capacité de l'acteur de traiter différents VHs. La efficacité et la stabilité du cadre de langage acteur-reasonner ont été démontrées par comparaison avec des études d'inférence et des méthodes déterministes. Finalement, on combine les informations de l'interface externe de l'humain et de la machine (eHMI) et les décisions d'action applicables de l'acteur, basées sur le raisonnement du registre, pour confirmer l'effet du cadre de langage acteur-reasonner dans divers scénarios d'interaction sur le terrain. Le code est disponible sur https://github.com/FanGShiYuu/Actor-Reasoner.",
      "upvotes": 1,
      "discussionId": "67c8427247c2aa135346dd84",
      "projectPage": "https://fangshiyuu.github.io/Actor-Reasoner/",
      "githubRepo": "https://github.com/FanGShiYuu/Actor-Reasoner"
    },
    "publishedAt": "2025-03-05T21:37:18.981Z",
    "title": "Interact, Instruct to Improve: A LLM-Driven Parallel Actor-Reasoner Framework for Enhancing Autonomous Vehicle Interactions",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.00502.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66d3290364c1e9b73208af82",
      "avatarUrl": "/avatars/e0ea5f2b366927c7b146f248028a2e59.svg",
      "fullname": "Shiyu Fang",
      "name": "FanGShiYuu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.01372",
      "authors": [
        {
          "_id": "67c6bd6e8f3e7fd471affd06",
          "name": "Joel Niklaus",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd07",
          "name": "Jakob Merane",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd08",
          "name": "Luka Nenadic",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd09",
          "name": "Sina Ahmadi",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd0a",
          "name": "Yingqiang Gao",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd0b",
          "name": "Cyrill A. H. Chevalley",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd0c",
          "name": "Claude Humbel",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd0d",
          "name": "Christophe Gösken",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd0e",
          "name": "Lorenzo Tanzi",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd0f",
          "name": "Thomas Lüthi",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd10",
          "name": "Stefan Palombo",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd11",
          "name": "Spencer Poff",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd12",
          "name": "Boling Yang",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd13",
          "name": "Nan Wu",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd14",
          "name": "Matthew Guillod",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd15",
          "name": "Robin Mamié",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd16",
          "name": "Daniel Brunner",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd17",
          "name": "Julio Pereyra",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd18",
          "name": "Niko Grupen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-03T10:10:30.000Z",
      "title": "SwiLTra-Bench : Benchmark de Traduction de Règlements Complexes",
      "summary": "La traduction des lois en Suisse est particulièrement importante en raison des quatre langues officielles du pays et des exigences de documents multilingues. Cependant, ce processus dépend de spécialistes en droit et de traducteurs compétents, ce qui génère un embouteillage et affecte l'approche juste et efficace. Pour résoudre ces défis, nous présentons SwiLTra-Bench, un cadre de référence détaillé qui comprend plus de 180K paires de traductions correspondantes pour toutes les langues suisses (y compris l'anglais). Ce cadre de référence est conçu pour évaluer les systèmes de traduction basés sur des modèles de grande taille (LLM). Selon un analyse logique, les modèles avancés montrent un excellent rendement de traduction dans tous les types de documents, bien que le rendement des systèmes de traduction professionnels soit particulièrement élevé dans le domaine des lois mais diminue dans les notes de tête. Par une évaluation rigoureuse et l'évaluation de spécialistes humains, nous montrons que le fine-tuning des systèmes d'ouverture significativement améliore la qualité de la traduction, ce qui dépasse les modèles plus récents sans fine-tuning (par exemple, Claude-3.5-Sonnet). De plus, nous présentons SwiLTra-Judge, un système d'évaluation spécialisé de LLM qui coïncide davantage avec l'évaluation de spécialistes humains.",
      "upvotes": 0,
      "discussionId": "67c6bd708f3e7fd471affd5d"
    },
    "publishedAt": "2025-03-06T00:10:21.173Z",
    "title": "SwiLTra-Bench: The Swiss Legal Translation Benchmark",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01372.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "640d3eaa3623f6a56dde856d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678589663024-640d3eaa3623f6a56dde856d.jpeg",
      "fullname": "vansin",
      "name": "vansin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": false
  }
]