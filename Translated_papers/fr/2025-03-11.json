[
  {
    "paper": {
      "id": "2503.03601",
      "authors": [
        {
          "_id": "67cbfff12cc05acaab147f07",
          "name": "Kristian Kuznetsov",
          "hidden": false
        },
        {
          "_id": "67cbfff12cc05acaab147f08",
          "user": {
            "_id": "636254dc2691058b19d9276a",
            "avatarUrl": "/avatars/36eb0e27e0e321fb0ac513f0d4d67c95.svg",
            "isPro": false,
            "fullname": "Kushnareva",
            "user": "Kushnareva",
            "type": "user"
          },
          "name": "Laida Kushnareva",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-11T08:23:18.630Z",
          "hidden": false
        },
        {
          "_id": "67cbfff12cc05acaab147f09",
          "name": "Polina Druzhinina",
          "hidden": false
        },
        {
          "_id": "67cbfff12cc05acaab147f0a",
          "user": {
            "_id": "6172aaeec8e66e2aa84c06b9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6172aaeec8e66e2aa84c06b9/ZdRZSp3P1SU6CIDbvQwkv.jpeg",
            "isPro": false,
            "fullname": "Anton Razzhigaev",
            "user": "razzant",
            "type": "user"
          },
          "name": "Anton Razzhigaev",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-11T08:23:21.197Z",
          "hidden": false
        },
        {
          "_id": "67cbfff12cc05acaab147f0b",
          "name": "Anastasia Voznyuk",
          "hidden": false
        },
        {
          "_id": "67cbfff12cc05acaab147f0c",
          "name": "Irina Piontkovskaya",
          "hidden": false
        },
        {
          "_id": "67cbfff12cc05acaab147f0d",
          "name": "Evgeny Burnaev",
          "hidden": false
        },
        {
          "_id": "67cbfff12cc05acaab147f0e",
          "name": "Serguei Barannikov",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-05T15:33:52.000Z",
      "title": "Connaissance sur l'automateur codificateur sparse qui détecte la littérature écrite artificiellement au niveau des caractéristiques",
      "summary": "La détection de texte artificiel (ATD) devient de plus en plus importante avec l'augmentation des modèles de langage de haut niveau (LLMs). Cependant, malgré les efforts conjoints, il n'est pas possible de montrer un bon rendement constant pour différents types de textes initiaux et il n'est pas possible de garantir une extension valide pour de nouveaux LLMs. L'interprétabilité joue un rôle crucial pour atteindre cet objectif. Dans cette étude, des autoencodeurs spars (SAE) sont utilisés pour extraire des caractéristiques de la streaming résiduelle de Gemma-2-2b et améliorer l'interprétabilité de la ATD. Deux caractéristiques interprétables et efficaces sont identifiées et leur signification et leur association sont analysées à l'aide de statistiques propres au domaine et au modèle, d'un approche de stationnement, ou d'un analyse manuelle ou basée sur des LLMs. Notre méthode fournit des informations précieuses sur la manière dont le texte écrit par des humains diffère dans différents modèles. De plus, les LLMs actuels ont une représentation spéciale dans des domaines à haute densité d'information et peuvent générer des sorties similaires à celles des humains en utilisant des techniques de programmation professionnelle.",
      "upvotes": 85,
      "discussionId": "67cbfff22cc05acaab147f4d",
      "ai_keywords": [
        "Sparse Autoencoders",
        "Gemma-2-2b",
        "residual stream",
        "interpretability",
        "domain-specific statistics",
        "model-specific statistics",
        "steering approach",
        "LLM-based interpretation",
        "writing style",
        "information-dense domains",
        "human-like outputs"
      ]
    },
    "publishedAt": "2025-03-05T10:33:52.000Z",
    "title": "Feature-Level Insights into Artificial Text Detection with Sparse\n  Autoencoders",
    "summary": "Artificial Text Detection (ATD) is becoming increasingly important with the\nrise of advanced Large Language Models (LLMs). Despite numerous efforts, no\nsingle algorithm performs consistently well across different types of unseen\ntext or guarantees effective generalization to new LLMs. Interpretability plays\na crucial role in achieving this goal. In this study, we enhance ATD\ninterpretability by using Sparse Autoencoders (SAE) to extract features from\nGemma-2-2b residual stream. We identify both interpretable and efficient\nfeatures, analyzing their semantics and relevance through domain- and\nmodel-specific statistics, a steering approach, and manual or LLM-based\ninterpretation. Our methods offer valuable insights into how texts from various\nmodels differ from human-written content. We show that modern LLMs have a\ndistinct writing style, especially in information-dense domains, even though\nthey can produce human-like outputs with personalized prompts.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.03601.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.07365",
      "authors": [
        {
          "_id": "67cf9cd037bc7273882147a3",
          "name": "Fanqing Meng",
          "hidden": false
        },
        {
          "_id": "67cf9cd037bc7273882147a4",
          "name": "Lingxiao Du",
          "hidden": false
        },
        {
          "_id": "67cf9cd037bc7273882147a5",
          "name": "Zongkai Liu",
          "hidden": false
        },
        {
          "_id": "67cf9cd037bc7273882147a6",
          "name": "Zhixiang Zhou",
          "hidden": false
        },
        {
          "_id": "67cf9cd037bc7273882147a7",
          "name": "Quanfeng Lu",
          "hidden": false
        },
        {
          "_id": "67cf9cd037bc7273882147a8",
          "name": "Daocheng Fu",
          "hidden": false
        },
        {
          "_id": "67cf9cd037bc7273882147a9",
          "name": "Botian Shi",
          "hidden": false
        },
        {
          "_id": "67cf9cd037bc7273882147aa",
          "name": "Wenhai Wang",
          "hidden": false
        },
        {
          "_id": "67cf9cd037bc7273882147ab",
          "name": "Junjun He",
          "hidden": false
        },
        {
          "_id": "67cf9cd037bc7273882147ac",
          "name": "Kaipeng Zhang",
          "hidden": false
        },
        {
          "_id": "67cf9cd037bc7273882147ad",
          "name": "Ping Luo",
          "hidden": false
        },
        {
          "_id": "67cf9cd037bc7273882147ae",
          "name": "Yu Qiao",
          "hidden": false
        },
        {
          "_id": "67cf9cd037bc7273882147af",
          "name": "Qiaosheng Zhang",
          "hidden": false
        },
        {
          "_id": "67cf9cd037bc7273882147b0",
          "name": "Wenqi Shao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T14:23:12.000Z",
      "title": "MM-Eureka : Apprentissage par renforcement basé sur les règles pour explorer visuellement les \"moments de douleur\" à grande échelle",
      "summary": "MM-Eureka est un modèle de logique multi-modèle et a réussi à étendre l'apprentissage par renforcement basé sur les règles (RL) à un modèle de logique multi-modèle. L'apprentissage par renforcement basé sur les règles a démontré un succès surprenant en améliorant les capacités logiques des modèles de langage grands (LLM) dans des contextes, mais sa mise en œuvre dans un environnement multi-modèle a été difficile. Notre recherche a récréé les principales caractéristiques des systèmes d'apprentissage par renforcement basé sur le contexte, comme DeepSeek-R1, dans un espace multi-modèle, et a inclus des améliorations en termes de précision, une augmentation stable de la longueur des réponses et l'apparition d'actions de rétroaction. Nous montrons que, en utilisant l'apprentissage par renforcement basé sur les règles, des modèles d'apprentissage par exemple et des modèles entraînés précédemment peuvent développer une forte capacité de logique multi-modèle sans la limitation d'un professeur, et qu'ils présentent une plus grande efficacité en données par rapport à d'autres approches. Nous avons ouvert le code complet de notre pipeline sur GitHub à l'adresse https://github.com/ModalMinds/MM-EUREKA et promouvons le développement de cette zone. Tout notre code, nos modèles et nos données sont disponibles dans un nouveau lancement.",
      "upvotes": 38,
      "discussionId": "67cf9cd137bc7273882147e2",
      "ai_keywords": [
        "multimodal reasoning",
        "rule-based reinforcement learning (RL)",
        "large-scale rule-based reinforcement learning (RL)",
        "DeepSeek-R1",
        "multimodal space",
        "accuracy reward",
        "response length",
        "reflection behaviors",
        "instruction-tuned",
        "pre-trained models",
        "multimodal reasoning capabilities",
        "rule-based RL",
        "supervised fine-tuning",
        "data efficiency"
      ]
    },
    "publishedAt": "2025-03-10T10:23:12.000Z",
    "title": "MM-Eureka: Exploring Visual Aha Moment with Rule-based Large-scale\n  Reinforcement Learning",
    "summary": "We present MM-Eureka, a multimodal reasoning model that successfully extends\nlarge-scale rule-based reinforcement learning (RL) to multimodal reasoning.\nWhile rule-based RL has shown remarkable success in improving LLMs' reasoning\nabilities in text domains, its application to multimodal settings has remained\nchallenging. Our work reproduces key characteristics of text-based RL systems\nlike DeepSeek-R1 in the multimodal space, including steady increases in\naccuracy reward and response length, and the emergence of reflection behaviors.\nWe demonstrate that both instruction-tuned and pre-trained models can develop\nstrong multimodal reasoning capabilities through rule-based RL without\nsupervised fine-tuning, showing superior data efficiency compared to\nalternative approaches. We open-source our complete pipeline to foster further\nresearch in this area. We release all our codes, models, data, etc. at\nhttps://github.com/ModalMinds/MM-EUREKA",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07365.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.07605",
      "authors": [
        {
          "_id": "67cfa0c1edb742caa3572982",
          "name": "Xun Liang",
          "hidden": false
        },
        {
          "_id": "67cfa0c1edb742caa3572983",
          "user": {
            "_id": "669e0b93c7cb0568dac6e92e",
            "avatarUrl": "/avatars/a39ea77d7391f164af8a80f94f85f2ca.svg",
            "isPro": false,
            "fullname": "hanyu Wang",
            "user": "UglyToilet",
            "type": "user"
          },
          "name": "Hanyu Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-11T08:22:46.104Z",
          "hidden": false
        },
        {
          "_id": "67cfa0c1edb742caa3572984",
          "name": "Huayi Lai",
          "hidden": false
        },
        {
          "_id": "67cfa0c1edb742caa3572985",
          "name": "Simin Niu",
          "hidden": false
        },
        {
          "_id": "67cfa0c1edb742caa3572986",
          "name": "Shichao Song",
          "hidden": false
        },
        {
          "_id": "67cfa0c1edb742caa3572987",
          "name": "Jiawei Yang",
          "hidden": false
        },
        {
          "_id": "67cfa0c1edb742caa3572988",
          "name": "Jihao Zhao",
          "hidden": false
        },
        {
          "_id": "67cfa0c1edb742caa3572989",
          "name": "Feiyu Xiong",
          "hidden": false
        },
        {
          "_id": "67cfa0c1edb742caa357298a",
          "name": "Bo Tang",
          "hidden": false
        },
        {
          "_id": "67cfa0c1edb742caa357298b",
          "name": "Zhiyu Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T17:59:03.000Z",
      "title": "SEAP : Entraînement sans nécessité de pas de pas pour préparer l'apprentissage actif d'un modèle de langage à grande échelle.",
      "summary": "Le modèle de langage nature est capable de réaliser des succès impressionnants dans divers tâches de traitement du langage nature, mais son coût de calcul élevé pendant l'inférence reste un obstacle principal. Dans cet article, nous présentons une technique de réduction de paramètres sans entraînement appelée Pruning d'Activation de Professeurs Sparse (SEAP), qui permet de réduire l'overhead de l'inférence en maintenant les paramètres sélectionnés liés à la tâche. Inspirée de l'état caché et des motifs d'activation de grands modèles de langage, SEAP identifie des motifs d'activation spécifiques pour chaque tâche, réduisant le modèle tout en maintenant l'efficacité de calcul et la précision. Les résultats des expériences montrent que SEAP peut réduire significativement l'overhead de calcul et maintenir une précision relative. En particulier, il a montré un effet supérieur à celui de WandA et FLAP lorsque le modèle est réduit de 50%, avec une perte de performance de seulement 2,2% lorsque le modèle est réduit de 20%. Ces résultats démontrent l'échelle et l'efficacité de SEAP, et suggèrent la possibilité d'une approche appropriée pour l'optimisation de grands modèles de langage nature.",
      "upvotes": 36,
      "discussionId": "67cfa0c2edb742caa35729dc",
      "githubRepo": "https://github.com/IAAR-Shanghai/SEAP",
      "ai_keywords": [
        "Sparse Expert Activation Pruning (SEAP)",
        "hidden states",
        "activations",
        "task-specific expert activation patterns",
        "computational efficiency"
      ]
    },
    "publishedAt": "2025-03-10T13:59:03.000Z",
    "title": "SEAP: Training-free Sparse Expert Activation Pruning Unlock the\n  Brainpower of Large Language Models",
    "summary": "Large Language Models have achieved remarkable success across various natural\nlanguage processing tasks, yet their high computational cost during inference\nremains a major bottleneck. This paper introduces Sparse Expert Activation\nPruning (SEAP), a training-free pruning method that selectively retains\ntask-relevant parameters to reduce inference overhead. Inspired by the\nclustering patterns of hidden states and activations in LLMs, SEAP identifies\ntask-specific expert activation patterns and prunes the model while preserving\ntask performance and enhancing computational efficiency. Experimental results\ndemonstrate that SEAP significantly reduces computational overhead while\nmaintaining competitive accuracy. Notably, at 50% pruning, SEAP surpasses both\nWandA and FLAP by over 20%, and at 20% pruning, it incurs only a 2.2%\nperformance drop compared to the dense model. These findings highlight SEAP's\nscalability and effectiveness, making it a promising approach for optimizing\nlarge-scale LLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07605.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.07002",
      "authors": [
        {
          "_id": "67cfa814d212c9c5048845a0",
          "name": "Jiazheng Liu",
          "hidden": false
        },
        {
          "_id": "67cfa814d212c9c5048845a1",
          "name": "Sipeng Zheng",
          "hidden": false
        },
        {
          "_id": "67cfa814d212c9c5048845a2",
          "user": {
            "_id": "61e52be53d6dbb1da842316a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61e52be53d6dbb1da842316a/gx0WGPcOCClXPymoKglc4.jpeg",
            "isPro": false,
            "fullname": "Börje Karlsson",
            "user": "tellarin",
            "type": "user"
          },
          "name": "Börje F. Karlsson",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-11T08:22:32.095Z",
          "hidden": false
        },
        {
          "_id": "67cfa814d212c9c5048845a3",
          "name": "Zongqing Lu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T07:32:53.000Z",
      "title": "Nota et concentration - Une approche de dialogue multimodale\n\nApprentissage",
      "summary": "Introduis le jeu de données MMDiag de modèles multiples. Ce jeu de données a été conçu spécialement avec des règles spécifiques et avec l'aide de GPT, généré conjointement, en maintenant une forte corrélation entre les questions, entre les questions et les images, et entre différentes zones d'images, ce qui a permis de créer une expérience plus proche de la réalité. MMDiag joue un rôle de référence fort dans l'apprentissage de jeux de données multiples, présentant des défis plus importants pour la capacité de raisonnement logique des MLLM. De plus, présente un MLLM appelé DiagNote, qui est connecté au traitement visuel humain. DiagNote fonctionne avec deux modules qui interagissent mutuellement : la planification et la visualisation. DiagNote expérimente en montrant une combinaison de meilleures bases de connaissances, informations visuelles et langage, ce qui permet une meilleure capacité de raisonnement logique.",
      "upvotes": 29,
      "discussionId": "67cfa818d212c9c504884689",
      "ai_keywords": [
        "multimodal large language models (MLLMs)",
        "vision towers",
        "multi-turn vision question-answering tasks",
        "multi-turn multimodal dialogue dataset (MMDiag)",
        "GPT assistant",
        "multimodal dialogue learning",
        "grounding",
        "reasoning capabilities",
        "Deliberate module",
        "Gaze module",
        "Chain-of-Thought"
      ]
    },
    "publishedAt": "2025-03-10T03:32:53.000Z",
    "title": "Taking Notes Brings Focus? Towards Multi-Turn Multimodal Dialogue\n  Learning",
    "summary": "Multimodal large language models (MLLMs), built on large-scale pre-trained\nvision towers and language models, have shown great capabilities in multimodal\nunderstanding. However, most existing MLLMs are trained on single-turn vision\nquestion-answering tasks, which do not accurately reflect real-world human\nconversations. In this paper, we introduce MMDiag, a multi-turn multimodal\ndialogue dataset. This dataset is collaboratively generated through\ndeliberately designed rules and GPT assistance, featuring strong correlations\nbetween questions, between questions and images, and among different image\nregions; thus aligning more closely with real-world scenarios. MMDiag serves as\na strong benchmark for multi-turn multimodal dialogue learning and brings more\nchallenges to the grounding and reasoning capabilities of MLLMs. Further,\ninspired by human vision processing, we present DiagNote, an MLLM equipped with\nmultimodal grounding and reasoning capabilities. DiagNote consists of two\nmodules (Deliberate and Gaze) interacting with each other to perform\nChain-of-Thought and annotations respectively, throughout multi-turn dialogues.\nWe empirically demonstrate the advantages of DiagNote in both grounding and\njointly processing and reasoning with vision and language information over\nexisting MLLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07002.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.07314",
      "authors": [
        {
          "_id": "67cfa750c8f2a661dc9798fe",
          "name": "Weijia Wu",
          "hidden": false
        },
        {
          "_id": "67cfa750c8f2a661dc9798ff",
          "name": "Zeyu Zhu",
          "hidden": false
        },
        {
          "_id": "67cfa750c8f2a661dc979900",
          "name": "Mike Zheng Shou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T13:33:27.000Z",
      "title": "Automatisation de la génération de films pour le planification de contexte multi-agente",
      "summary": "Actuellement, les outils de travail pour la création de longs vidéos sont limités par une manque de planification automatique, ce qui nécessite des entrées manuelles pour l'interaction entre scènes, histoires, conception de vidéos et personnages, ce qui génère de hauts coûts et des efficacités insatisfaisantes. Pour résoudre ces problèmes, nous présentons MovieAgent. MovieAgent est un outil de travail automatique de génération de films utilisant une planification efficace de Chaîne de Pensée (CoT). MovieAgent offre deux principales avantages : 1) nous explorons et définissons un modèle automatique de génération de films/vidéos longues au début. En fournissant un scénario et un vecteur de personnages, notre MovieAgent génère des vidéos longues de plusieurs scènes et angles, maintenant la cohérence des personnages, les sous-titres coordonnés et la cohérence des sous-narrations, assurant une cohérence et une structure cohérente. 2) MovieAgent introduit un processus de raisonnement heuristique basé sur CoT pour construire automatiquement la composition des scènes, la configuration des caméras et la conception des vidéos, réduisant significativement l'effort humain. En imitant le rôle des réalisateurs de films, des concepteurs de scènes, des artistes de storyboard et des gestionnaires de visages, MovieAgent utilise plusieurs agents d'IA pour streamline le processus productif. Les expériences ont atteint des résultats optimaux en fonction du scénario, de la cohérence des personnages et de la connectivité narrative en utilisant la nouvelle stratégie de distance minimale. Notre modèle heuristique offre de nouvelles perspectives dans la génération automatique complète de films. Les codes et le site web du projet sont disponibles sur les URLs suivants : https://github.com/showlab/MovieAgent et https://weijiawu.github.io/MovieAgent.",
      "upvotes": 24,
      "discussionId": "67cfa752c8f2a661dc9799b8",
      "ai_keywords": [
        "MovieAgent",
        "Chain of Thought (CoT)",
        "automated movie/long-video generation",
        "multi-scene, multi-shot long-form videos",
        "coherent narrative",
        "character consistency",
        "synchronized subtitles",
        "stable audio",
        "hierarchical CoT-based reasoning",
        "multiple LLM agents",
        "director",
        "screenwriter",
        "storyboard artist",
        "location manager",
        "script faithfulness",
        "narrative coherence",
        "fully automated movie generation"
      ]
    },
    "publishedAt": "2025-03-10T09:33:27.000Z",
    "title": "Automated Movie Generation via Multi-Agent CoT Planning",
    "summary": "Existing long-form video generation frameworks lack automated planning,\nrequiring manual input for storylines, scenes, cinematography, and character\ninteractions, resulting in high costs and inefficiencies. To address these\nchallenges, we present MovieAgent, an automated movie generation via\nmulti-agent Chain of Thought (CoT) planning. MovieAgent offers two key\nadvantages: 1) We firstly explore and define the paradigm of automated\nmovie/long-video generation. Given a script and character bank, our MovieAgent\ncan generates multi-scene, multi-shot long-form videos with a coherent\nnarrative, while ensuring character consistency, synchronized subtitles, and\nstable audio throughout the film. 2) MovieAgent introduces a hierarchical\nCoT-based reasoning process to automatically structure scenes, camera settings,\nand cinematography, significantly reducing human effort. By employing multiple\nLLM agents to simulate the roles of a director, screenwriter, storyboard\nartist, and location manager, MovieAgent streamlines the production pipeline.\nExperiments demonstrate that MovieAgent achieves new state-of-the-art results\nin script faithfulness, character consistency, and narrative coherence. Our\nhierarchical framework takes a step forward and provides new insights into\nfully automated movie generation. The code and project website are available\nat: https://github.com/showlab/MovieAgent and\nhttps://weijiawu.github.io/MovieAgent.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07314.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.07216",
      "authors": [
        {
          "_id": "67cfa6fcd77496ce0c154bdc",
          "name": "Sangwoo Park",
          "hidden": false
        },
        {
          "_id": "67cfa6fcd77496ce0c154bdd",
          "name": "Seanie Lee",
          "hidden": false
        },
        {
          "_id": "67cfa6fcd77496ce0c154bde",
          "name": "Byungjoo Kim",
          "hidden": false
        },
        {
          "_id": "67cfa6fcd77496ce0c154bdf",
          "name": "Sung Ju Hwang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T11:55:50.000Z",
      "title": "FedRand : Amélioration de la protection de l'information personnelle dans l'apprentissage collaboratif par la randomisation de LoRA dans les unités d'actualisation",
      "summary": "Federado Learning (FL) est un cadre de travail largement utilisé pour entraîner des modèles de manière distribuée. Ce méthode est conçue de manière que le serveur central n'ait pas accès direct aux données locales. Cependant, cette approche présente des problèmes en raison du fait que lorsque le modèle se concentre sur le serveur central, la confidentialité des données ne peut pas être protégée complètement. Ce problème est particulièrement pertinent lorsque l'on entraîne un Modèle de Vision et de Langue (VLM) par FL. Les VLM peuvent facilement mémoriser les instances des données d'entraînement et peuvent être vulnérables aux attaques d'inférence des membres (MIAs). Pour faire face à ces défis, on propose le cadre de travail FedRand. Ce cadre est conçu de manière que aucun paramètre de toutes les ordinateurs ne soit publié. Dans FedRand, chaque ordinateur sélectionne des sous-paramètres d'adaptation de bas niveau (LoRA) de manière aléatoire par le serveur et maintient les autres poids de LoRA comme paramètres non publiques. Après avoir entraîné les deux paramètres dans le jeu de données non publique de l'ordinateur, seuls les paramètres non publiques sont envoyés au serveur, ce qui concentre le modèle VLM. Cette approche réduit le risque d'exposition des paramètres de VLM de l'ordinateur et améliore la confidentialité des données. Expérimentalement, FedRand améliore la robustesse face aux MIAs par rapport à des références pertinentes et montre que sa précision est équivalente à celle des méthodes qui envoient complètement les paramètres LoRA sur différents jeux de données de test.",
      "upvotes": 22,
      "discussionId": "67cfa6fdd77496ce0c154c18",
      "ai_keywords": [
        "Federated Learning (FL)",
        "vision-language models (VLMs)",
        "membership inference attacks (MIAs)",
        "FedRand framework",
        "Low-Rank Adaptation (LoRA)",
        "subparameters",
        "non-private client parameters",
        "client parameters",
        "aggregation",
        "robustness"
      ]
    },
    "publishedAt": "2025-03-10T07:55:50.000Z",
    "title": "FedRand: Enhancing Privacy in Federated Learning with Randomized LoRA\n  Subparameter Updates",
    "summary": "Federated Learning (FL) is a widely used framework for training models in a\ndecentralized manner, ensuring that the central server does not have direct\naccess to data from local clients. However, this approach may still fail to\nfully preserve data privacy, as models from local clients are exposed to the\ncentral server during the aggregation process. This issue becomes even more\ncritical when training vision-language models (VLMs) with FL, as VLMs can\neasily memorize training data instances, making them vulnerable to membership\ninference attacks (MIAs). To address this challenge, we propose the FedRand\nframework, which avoids disclosing the full set of client parameters. In this\nframework, each client randomly selects subparameters of Low-Rank Adaptation\n(LoRA) from the server and keeps the remaining counterparts of the LoRA weights\nas private parameters. After training both parameters on the client's private\ndataset, only the non-private client parameters are sent back to the server for\naggregation. This approach mitigates the risk of exposing client-side VLM\nparameters, thereby enhancing data privacy. We empirically validate that\nFedRand improves robustness against MIAs compared to relevant baselines while\nachieving accuracy comparable to methods that communicate full LoRA parameters\nacross several benchmark datasets.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07216.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.07067",
      "authors": [
        {
          "_id": "67cfa99b7c95194db8d75468",
          "user": {
            "_id": "64b7628af902508f0d7ae112",
            "avatarUrl": "/avatars/83c155254486e80c1dfd14676fdf9215.svg",
            "isPro": false,
            "fullname": "Jongwoo Ko",
            "user": "jongwooko",
            "type": "user"
          },
          "name": "Jongwoo Ko",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-11T08:22:29.622Z",
          "hidden": false
        },
        {
          "_id": "67cfa99b7c95194db8d75469",
          "user": {
            "_id": "64ad94f05a4a60156925ec96",
            "avatarUrl": "/avatars/643bdb076e703bfcc89cec6fccb756c6.svg",
            "isPro": false,
            "fullname": "Tianyi Chen",
            "user": "tianyic",
            "type": "user"
          },
          "name": "Tianyi Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-11T08:22:27.139Z",
          "hidden": false
        },
        {
          "_id": "67cfa99b7c95194db8d7546a",
          "name": "Sungnyun Kim",
          "hidden": false
        },
        {
          "_id": "67cfa99b7c95194db8d7546b",
          "name": "Tianyu Ding",
          "hidden": false
        },
        {
          "_id": "67cfa99b7c95194db8d7546c",
          "name": "Luming Liang",
          "hidden": false
        },
        {
          "_id": "67cfa99b7c95194db8d7546d",
          "name": "Ilya Zharkov",
          "hidden": false
        },
        {
          "_id": "67cfa99b7c95194db8d7546e",
          "name": "Se-Young Yun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T08:51:32.000Z",
      "title": "DistiLLM-2 : Une approche contrastive pour améliorer le niveau de stylisation des LLM",
      "summary": "Bien sûr, voici la traduction en français :\n\nMalgré le succès réalisé dans la dédistillation de grands modèles de langue (LLMs), presque tous les études précédentes appliquent la même fonction de perte aux données de génération des modèles de maître et d'étudiant. Ces stratégies ignorent l'association entre la représentation de la perte et le type de données, et également ignorent que des résultats optimaux ne peuvent pas être obtenus pour améliorer le rendement du modèle étudiant. Dans ce sens, nous proposons un approche relative qui utilise l'association de manière que le modèle étudiant diminue la probabilité de sa réponse lorsque la probabilité de la réponse du modèle maître augmente. Nos amples expériences montrent que DistiLLM-2 peut construire un modèle étudiant très efficace qui résout divers problèmes, comme la génération d'instructions et de code, et soutient des applications telles que la configuration des préférences et l'extension du langage visuel. Ces résultats révèlent que cette approche relative peut effectuer efficacement l'association entre modèles maître et étudiant, et améliorer l'effet de la dédistillation de LLMs pour différents types de données.",
      "upvotes": 19,
      "discussionId": "67cfa99c7c95194db8d754bf",
      "githubRepo": "https://github.com/jongwooko/distillm-2",
      "ai_keywords": [
        "contrastive approach",
        "likelihood",
        "DistiLLM-2",
        "instruction-following",
        "code generation",
        "preference alignment",
        "vision-language extensions"
      ]
    },
    "publishedAt": "2025-03-10T04:51:32.000Z",
    "title": "DistiLLM-2: A Contrastive Approach Boosts the Distillation of LLMs",
    "summary": "Despite the success of distillation in large language models (LLMs), most\nprior work applies identical loss functions to both teacher- and\nstudent-generated data. These strategies overlook the synergy between loss\nformulations and data types, leading to a suboptimal performance boost in\nstudent models. To address this, we propose DistiLLM-2, a contrastive approach\nthat simultaneously increases the likelihood of teacher responses and decreases\nthat of student responses by harnessing this synergy. Our extensive experiments\nshow that DistiLLM-2 not only builds high-performing student models across a\nwide range of tasks, including instruction-following and code generation, but\nalso supports diverse applications, such as preference alignment and\nvision-language extensions. These findings highlight the potential of a\ncontrastive approach to enhance the efficacy of LLM distillation by effectively\naligning teacher and student models across varied data types.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07067.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.06680",
      "authors": [
        {
          "_id": "67cf94d9f2b1fe815db6db40",
          "name": "Wei Li",
          "hidden": false
        },
        {
          "_id": "67cf94d9f2b1fe815db6db41",
          "user": {
            "_id": "641a9a4b05290a135041a3ed",
            "avatarUrl": "/avatars/95d66ac607973abe95bd3558c6c93739.svg",
            "isPro": false,
            "fullname": "Pluto",
            "user": "CharonBony",
            "type": "user"
          },
          "name": "Xin Zhang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-11T01:41:47.194Z",
          "hidden": false
        },
        {
          "_id": "67cf94d9f2b1fe815db6db42",
          "name": "Zhongxin Guo",
          "hidden": false
        },
        {
          "_id": "67cf94d9f2b1fe815db6db43",
          "name": "Shaoguang Mao",
          "hidden": false
        },
        {
          "_id": "67cf94d9f2b1fe815db6db44",
          "name": "Wen Luo",
          "hidden": false
        },
        {
          "_id": "67cf94d9f2b1fe815db6db45",
          "name": "Guangyue Peng",
          "hidden": false
        },
        {
          "_id": "67cf94d9f2b1fe815db6db46",
          "name": "Yangyu Huang",
          "hidden": false
        },
        {
          "_id": "67cf94d9f2b1fe815db6db47",
          "name": "Houfeng Wang",
          "hidden": false
        },
        {
          "_id": "67cf94d9f2b1fe815db6db48",
          "name": "Scarlett Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-09T16:11:57.000Z",
      "title": "FEA-Bench : FEA-Bench est un cadre de référence utilisé pour évaluer la génération de codes au niveau de la référence de dépôt pour l'implémentation de caractéristiques d'analyse des éléments finis (FEA).",
      "summary": "L'implémentation de nouvelles fonctions à un niveau de code est un domaine important pour les modèles de génération de code. Cependant, actuellement, les cadres d'évaluation ne couvrent pas sa capacité de manière professionnelle. Pour corriger cela, nous présentons FEA-Bench. FEA-Bench est conçu pour évaluer la capacité de grands modèles de langage (LLMs) à effectuer des développements dans un dépôt. Nous avons récolté des demandes de pull request dans un dépôt GitHub de 83 pages et nous avons construit des instances de tâche avec un enfoque sur le développement de nouvelles fonctions, en utilisant des filtres basés sur des règles et des filtres basés sur l'intention. Les modifications de code dans chaque instance de tâche peuvent être validées avec les fichiers de tests unitaires liés. Pour l'implémentation de nouvelles fonctions, les LLMs doivent compléter le code de nouveaux composants et éditer d'autres parties liées dans le dépôt. FEA-Bench évalue plus en détail la capacité des LLMs à la génération automatique de logiciel. Les résultats des tests montrent que les LLMs ont présenté un rendement considérablement faible dans FEA-Bench et des problèmes importants ont été découverts dans le développement au niveau du dépôt.",
      "upvotes": 16,
      "discussionId": "67cf94dbf2b1fe815db6db9e"
    },
    "publishedAt": "2025-03-09T12:11:57.000Z",
    "title": "FEA-Bench: A Benchmark for Evaluating Repository-Level Code Generation\n  for Feature Implementation",
    "summary": "Implementing new features in repository-level codebases is a crucial\napplication of code generation models. However, current benchmarks lack a\ndedicated evaluation framework for this capability. To fill this gap, we\nintroduce FEA-Bench, a benchmark designed to assess the ability of large\nlanguage models (LLMs) to perform incremental development within code\nrepositories. We collect pull requests from 83 GitHub repositories and use\nrule-based and intent-based filtering to construct task instances focused on\nnew feature development. Each task instance containing code changes is paired\nwith relevant unit test files to ensure that the solution can be verified. The\nfeature implementation requires LLMs to simultaneously possess code completion\ncapabilities for new components and code editing abilities for other relevant\nparts in the code repository, providing a more comprehensive evaluation method\nof LLMs' automated software engineering capabilities. Experimental results show\nthat LLMs perform significantly worse in the FEA-Bench, highlighting\nconsiderable challenges in such repository-level incremental code development.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.06680.png",
    "numComments": 5,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.07027",
      "authors": [
        {
          "_id": "67cf98fd59dbba733d8c531e",
          "user": {
            "_id": "636b3f9ce3ad78bc68b67541",
            "avatarUrl": "/avatars/2b7e745953ae39e01222e99fb63b279e.svg",
            "isPro": false,
            "fullname": "yuxuan",
            "user": "zzyx",
            "type": "user"
          },
          "name": "Yuxuan Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-11T08:22:57.021Z",
          "hidden": false
        },
        {
          "_id": "67cf98fd59dbba733d8c531f",
          "name": "Yirui Yuan",
          "hidden": false
        },
        {
          "_id": "67cf98fd59dbba733d8c5320",
          "name": "Yiren Song",
          "hidden": false
        },
        {
          "_id": "67cf98fd59dbba733d8c5321",
          "user": {
            "_id": "637745113a63a2983ffbde13",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669187672174-637745113a63a2983ffbde13.jpeg",
            "isPro": false,
            "fullname": "Haofan Wang",
            "user": "wanghaofan",
            "type": "user"
          },
          "name": "Haofan Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-11T08:22:54.821Z",
          "hidden": false
        },
        {
          "_id": "67cf98fd59dbba733d8c5322",
          "name": "Jiaming Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T08:07:17.000Z",
      "title": "EasyControl : Système qui ajoute un contrôle efficace et flexible aux modèles de diffusion Transformer",
      "summary": "Récemment, le développement de modèles de diffusion basés sur l'Unet a introduit des structures efficaces de contrôle spatial et thématique, comme ControlNet et IP-Adapter. Cependant, l'architecture DiT (Diffusion Transformer) rencontre des défis en termes d'efficience et de flexibilité de contrôle. Pour résoudre ces problèmes, on propose le nouveau cadre de travail EasyControl. Ce cadre intègre un transformeur de diffusion induit par conditions, offrant une haute efficacité et une grande flexibilité. Il s'appuie sur trois innovations clés : premièrement, le module d'injection de conditions avec Round Weights LoRA, qui traite les signaux de conditions séparément et fonctionne comme une solution plug-in et play-in, garantissant la compatibilité avec les modèles utilisateurs et permettant une injection flexible de diverses conditions. De plus, il supporte l'expansion de 0-shot multi-condition avec harmonie et force, même avec seulement un donnée de condition. Deuxièmement, on propose le Paradigme d'Entraînement de Positions Associées, qui normalise les conditions d'entrée dans une région standard, permettant la génération d'images avec n'importe quelle proportion et région flexible. Ce point de vue optimise l'efficience du calcul et offre une application pratique dans des applications réelles. Finalement, on applique la mécanique d'attention causale et la technologie de cache KV dans des tâches de génération conditionnelle, réduisant significativement la latence de synthèse d'images et améliorant l'efficience du cadre complet. Dans des expériences étendues, EasyControl montre des résultats exceptionnels dans divers scénarios d'application. Ces innovations partagent une haute efficacité, flexibilité et caractéristiques appropriées pour diverses tâches du cadre.",
      "upvotes": 15,
      "discussionId": "67cf990359dbba733d8c545d",
      "ai_keywords": [
        "Unet-based diffusion models",
        "ControlNet",
        "IP-Adapter",
        "DiT (Diffusion Transformer)",
        "Condition Injection LoRA Module",
        "Condition Injection",
        "zero-shot multi-condition generalization",
        "Position-Aware Training Paradigm",
        "Position-Aware",
        "Causal Attention Mechanism",
        "KV Cache",
        "conditional generation tasks",
        "image synthesis"
      ]
    },
    "publishedAt": "2025-03-10T04:07:17.000Z",
    "title": "EasyControl: Adding Efficient and Flexible Control for Diffusion\n  Transformer",
    "summary": "Recent advancements in Unet-based diffusion models, such as ControlNet and\nIP-Adapter, have introduced effective spatial and subject control mechanisms.\nHowever, the DiT (Diffusion Transformer) architecture still struggles with\nefficient and flexible control. To tackle this issue, we propose EasyControl, a\nnovel framework designed to unify condition-guided diffusion transformers with\nhigh efficiency and flexibility. Our framework is built on three key\ninnovations. First, we introduce a lightweight Condition Injection LoRA Module.\nThis module processes conditional signals in isolation, acting as a\nplug-and-play solution. It avoids modifying the base model weights, ensuring\ncompatibility with customized models and enabling the flexible injection of\ndiverse conditions. Notably, this module also supports harmonious and robust\nzero-shot multi-condition generalization, even when trained only on\nsingle-condition data. Second, we propose a Position-Aware Training Paradigm.\nThis approach standardizes input conditions to fixed resolutions, allowing the\ngeneration of images with arbitrary aspect ratios and flexible resolutions. At\nthe same time, it optimizes computational efficiency, making the framework more\npractical for real-world applications. Third, we develop a Causal Attention\nMechanism combined with the KV Cache technique, adapted for conditional\ngeneration tasks. This innovation significantly reduces the latency of image\nsynthesis, improving the overall efficiency of the framework. Through extensive\nexperiments, we demonstrate that EasyControl achieves exceptional performance\nacross various application scenarios. These innovations collectively make our\nframework highly efficient, flexible, and suitable for a wide range of tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07027.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.07602",
      "authors": [
        {
          "_id": "67cfb2efb77bc8e7d415f904",
          "name": "Yujie Wei",
          "hidden": false
        },
        {
          "_id": "67cfb2efb77bc8e7d415f905",
          "name": "Shiwei Zhang",
          "hidden": false
        },
        {
          "_id": "67cfb2efb77bc8e7d415f906",
          "user": {
            "_id": "649d54b314afbb10ce2a9eeb",
            "avatarUrl": "/avatars/15c325d8c2273ff63569f23015e98486.svg",
            "isPro": false,
            "fullname": "Hangjie Yuan",
            "user": "JacobYuan",
            "type": "user"
          },
          "name": "Hangjie Yuan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-11T08:21:32.780Z",
          "hidden": false
        },
        {
          "_id": "67cfb2efb77bc8e7d415f907",
          "name": "Biao Gong",
          "hidden": false
        },
        {
          "_id": "67cfb2efb77bc8e7d415f908",
          "user": {
            "_id": "6492a0d8d4ae24c933ace44d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/DXIky2sdPwmiCOR9p-JBQ.png",
            "isPro": false,
            "fullname": "Longxiang Tang",
            "user": "lloong",
            "type": "user"
          },
          "name": "Longxiang Tang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-11T08:21:30.700Z",
          "hidden": false
        },
        {
          "_id": "67cfb2efb77bc8e7d415f909",
          "name": "Xiang Wang",
          "hidden": false
        },
        {
          "_id": "67cfb2efb77bc8e7d415f90a",
          "name": "Haonan Qiu",
          "hidden": false
        },
        {
          "_id": "67cfb2efb77bc8e7d415f90b",
          "name": "Hengjia Li",
          "hidden": false
        },
        {
          "_id": "67cfb2efb77bc8e7d415f90c",
          "name": "Shuai Tan",
          "hidden": false
        },
        {
          "_id": "67cfb2efb77bc8e7d415f90d",
          "name": "Yingya Zhang",
          "hidden": false
        },
        {
          "_id": "67cfb2efb77bc8e7d415f90e",
          "name": "Hongming Shan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T17:58:03.000Z",
      "title": "DreamRelation: Centre de Relations: Service Vidéo Personnalisé",
      "summary": "La création de vidéos de relations Cascade est un travail important pour comprendre les relations entre deux thèmes spécifiques dans des contenus visuels. Les méthodes existantes peuvent spécialiser l'apparence et le comportement des thèmes, mais ne s'adaptent pas aux vidéos complexes de relations de Cascade. Il est crucial d'avoir un modèle de relations précis et une forte généralisation des catégories de thèmes. L'une des principales difficultés est la configuration complexe des espaces, les changements d'ordre et les mouvements temporels dans les relations. Actuellement, les modèles ne détectent pas les interactions significatives et mettent trop d'accent sur les détails visuels irrélevants des relations. Pour résoudre ces problèmes, nous proposons DreamRelation. DreamRelation est une nouvelle approche pour spécialiser les relations en utilisant de petits vidéos d'exemple. Elle utilise deux composants principaux : l'apprentissage de décodification de relations et l'architecture dynamique de relations. Dans l'apprentissage de décodification de relations, nous utilisons une stratégie d'entraînement avec des tuples de rotateurs de relations et des masques combinés pour séparer les relations de l'apparence des thèmes et garantir une meilleure généralisation dans différentes relations. De plus, nous analysons les rôles différents de la structure d'actions de MM-DiT pour optimiser le design des tuples de rotateurs de relations, ce qui rend DreamRelation un premier cadre explicable pour la génération de vidéos de première relation. Dans l'architecture dynamique de relations, nous introduisons une perte de comparaison spatiale de relations pour prioriser la dynamique des relations et éviter de dépendre de l'apparence détaillée des thèmes. Les résultats de validation d'extension montrent que DreamRelation dépasse les méthodes de l'état de l'art dans la représentation de vidéos de relations de Cascade. Le code et le modèle sont disponibles publiquement.",
      "upvotes": 10,
      "discussionId": "67cfb2f1b77bc8e7d415f96b",
      "ai_keywords": [
        "Relational Decoupling Learning",
        "Relational Dynamics Enhancement",
        "relation LoRA triplet",
        "hybrid mask training strategy",
        "attention mechanism",
        "space-time relational contrastive loss",
        "MM-DiT"
      ]
    },
    "publishedAt": "2025-03-10T13:58:03.000Z",
    "title": "DreamRelation: Relation-Centric Video Customization",
    "summary": "Relational video customization refers to the creation of personalized videos\nthat depict user-specified relations between two subjects, a crucial task for\ncomprehending real-world visual content. While existing methods can personalize\nsubject appearances and motions, they still struggle with complex relational\nvideo customization, where precise relational modeling and high generalization\nacross subject categories are essential. The primary challenge arises from the\nintricate spatial arrangements, layout variations, and nuanced temporal\ndynamics inherent in relations; consequently, current models tend to\noveremphasize irrelevant visual details rather than capturing meaningful\ninteractions. To address these challenges, we propose DreamRelation, a novel\napproach that personalizes relations through a small set of exemplar videos,\nleveraging two key components: Relational Decoupling Learning and Relational\nDynamics Enhancement. First, in Relational Decoupling Learning, we disentangle\nrelations from subject appearances using relation LoRA triplet and hybrid mask\ntraining strategy, ensuring better generalization across diverse relationships.\nFurthermore, we determine the optimal design of relation LoRA triplet by\nanalyzing the distinct roles of the query, key, and value features within\nMM-DiT's attention mechanism, making DreamRelation the first relational video\ngeneration framework with explainable components. Second, in Relational\nDynamics Enhancement, we introduce space-time relational contrastive loss,\nwhich prioritizes relational dynamics while minimizing the reliance on detailed\nsubject appearances. Extensive experiments demonstrate that DreamRelation\noutperforms state-of-the-art methods in relational video customization. Code\nand models will be made publicly available.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07602.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.06580",
      "authors": [
        {
          "_id": "67cfa71827c7f0b2db19f7c2",
          "user": {
            "_id": "645b4a2978730bcc103dfe4d",
            "avatarUrl": "/avatars/de544de899897fd0a83506ff287123bc.svg",
            "isPro": false,
            "fullname": "Yuxiang Zhang",
            "user": "TokerZ",
            "type": "user"
          },
          "name": "Yuxiang Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-11T08:22:40.336Z",
          "hidden": false
        },
        {
          "_id": "67cfa71827c7f0b2db19f7c3",
          "name": "Yuqi Yang",
          "hidden": false
        },
        {
          "_id": "67cfa71827c7f0b2db19f7c4",
          "name": "Jiangming Shu",
          "hidden": false
        },
        {
          "_id": "67cfa71827c7f0b2db19f7c5",
          "name": "Xinyan Wen",
          "hidden": false
        },
        {
          "_id": "67cfa71827c7f0b2db19f7c6",
          "name": "Jitao Sang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-09T12:19:47.000Z",
      "title": "Agent Model : Modèle qui enveloppe la génération d'actions sous forme de raisons logiques continuelles",
      "summary": "Le flux de travail traditionnel des agents utilise des prompts externes pour gérer l'interaction entre les outils et les environnements, ce qui limite l'autonomie des modèles d'inférence. Nous proposons la génération interne de la Chaîne d'Actions (CoA) pour décider de manière autonome l'utilisation de outils externes au moment et des méthodes choisies, en utilisant des LAMs (Modèles de Railz Generatifs). Le cadre proposé, AutoCoA, combine l'entraînement normal (SFT) et l'entraînement par renforcement (RL) pour permettre aux modèles de continuer à inférer et à agir de manière efficace, en gérant l'interaction avec l'environnement. Les principaux composants incluent le déclencheur d'actions par étape, l'optimisation de la CoA au niveau de la trajectoire, et le modèle de monde interne. Dans les évaluations pour des tâches de réponse à des questions ouvertes, les modèles d'agents entraînés avec AutoCoA dépassent significativement les flux de travail basés sur ReAct, surtout pour les tâches nécessitant un raisonnement à long terme et des actions multi-niveaux. Le code et les ensembles de données sont disponibles sur https://github.com/ADaM-BJTU/AutoCoA.",
      "upvotes": 10,
      "discussionId": "67cfa71927c7f0b2db19f817",
      "githubRepo": "https://github.com/ADaM-BJTU/AutoCoA",
      "ai_keywords": [
        "Large Agent Models (LAMs)",
        "Chain-of-Action (CoA)",
        "AutoCoA framework",
        "supervised fine-tuning (SFT)",
        "reinforcement learning (RL)",
        "step-level action triggering",
        "trajectory-level CoA optimization",
        "internal world model"
      ]
    },
    "publishedAt": "2025-03-09T08:19:47.000Z",
    "title": "Agent models: Internalizing Chain-of-Action Generation into Reasoning\n  models",
    "summary": "Traditional agentic workflows rely on external prompts to manage interactions\nwith tools and the environment, which limits the autonomy of reasoning models.\nWe position Large Agent Models (LAMs) that internalize the generation of\nChain-of-Action (CoA), enabling the model to autonomously decide when\nand how to use external tools. Our proposed AutoCoA framework combines\nsupervised fine-tuning (SFT) and reinforcement learning (RL), allowing the\nmodel to seamlessly switch between reasoning and action while efficiently\nmanaging environment interactions. Main components include step-level action\ntriggering, trajectory-level CoA optimization, and an internal world model to\nreduce real-environment interaction costs. Evaluations on open-domain QA tasks\ndemonstrate that AutoCoA-trained agent models significantly outperform\nReAct-based workflows in task completion, especially in tasks that require\nlong-term reasoning and multi-step actions. Code and dataset are available at\nhttps://github.com/ADaM-BJTU/AutoCoA",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.06580.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.07608",
      "authors": [
        {
          "_id": "67cfa5bcb17ca92d24da9033",
          "user": {
            "_id": "65a4a180c8a09bd5e8e900b8",
            "avatarUrl": "/avatars/c135db68f6ff2c40119acd2e9ddce968.svg",
            "isPro": false,
            "fullname": "Bo Jiang",
            "user": "rb93dett",
            "type": "user"
          },
          "name": "Bo Jiang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-11T08:22:43.665Z",
          "hidden": false
        },
        {
          "_id": "67cfa5bcb17ca92d24da9034",
          "name": "Shaoyu Chen",
          "hidden": false
        },
        {
          "_id": "67cfa5bcb17ca92d24da9035",
          "name": "Qian Zhang",
          "hidden": false
        },
        {
          "_id": "67cfa5bcb17ca92d24da9036",
          "name": "Wenyu Liu",
          "hidden": false
        },
        {
          "_id": "67cfa5bcb17ca92d24da9037",
          "name": "Xinggang Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T17:59:42.000Z",
      "title": "AlphaDrive : Apprentissage par répétition et logique pour libérer les capacités des VLMs en conduite autonome",
      "summary": "OpenAI o1 et DeepSeek R1 jouent un rôle crucial dans les domaines complexes de la mathématiques et de la science, en atteignant ou dépassant le niveau de performance d'un professionnel humain grâce à l'apprentissage par renforcement (RL) et la logique. Dans le domaine de la conduite automatique, les modèles ont significativement amélioré leur capacité à la planification depuis peu, mais continuent de rencontrer des problèmes en raison de limitations de connaissance et de logique, surtout dans la résolution de problèmes de file d'attente longues. Certaines recherches intègrent des modèles de langage visuo-linguistique (VLMs) pour l'étude de la conduite automatique, mais généralement dépendent d'un apprentissage supervisé simple (SFT) et ne font pas l'objet d'optimisations stratégiques appropriées pour la planification. Dans ce travail, nous proposons un cadre RL et logique pour les VLMs en conduite automatique. AlphaDrive introduit quatre récompenses RL basées sur GRPO adaptées à la planification et utilise une stratégie d'apprentissage logique-mathématique à deux étapes combinant SFT et RL. Cela a permis que AlphaDrive améliore significativement son rendement de planification et l'efficacité de l'apprentissage par rapport aux cas où seul SFT est utilisé ou la logique est exclue. De plus, après l'apprentissage par RL, AlphaDrive a démontré la capacité à découvrir diverses habiletés de planification cruciales pour la sécurité et l'efficacité du manœuvre. Nous reconnaissons que nos limites de connaissance empêchent AlphaDrive d'être un modèle complet RL et logique de planification pour la conduite automatique. Le code sera publié pour encourager futures recherches.",
      "upvotes": 9,
      "discussionId": "67cfa5bdb17ca92d24da9064",
      "ai_keywords": [
        "reinforcement learning (RL)",
        "reasoning",
        "end-to-end models",
        "vision-language models (VLMs)",
        "supervised fine-tuning (SFT)",
        "GRPO-based RL rewards",
        "two-stage planning reasoning training strategy",
        "emergent multimodal planning capabilities"
      ]
    },
    "publishedAt": "2025-03-10T13:59:42.000Z",
    "title": "AlphaDrive: Unleashing the Power of VLMs in Autonomous Driving via\n  Reinforcement Learning and Reasoning",
    "summary": "OpenAI o1 and DeepSeek R1 achieve or even surpass human expert-level\nperformance in complex domains like mathematics and science, with reinforcement\nlearning (RL) and reasoning playing a crucial role. In autonomous driving,\nrecent end-to-end models have greatly improved planning performance but still\nstruggle with long-tailed problems due to limited common sense and reasoning\nabilities. Some studies integrate vision-language models (VLMs) into autonomous\ndriving, but they typically rely on pre-trained models with simple supervised\nfine-tuning (SFT) on driving data, without further exploration of training\nstrategies or optimizations specifically tailored for planning. In this paper,\nwe propose AlphaDrive, a RL and reasoning framework for VLMs in autonomous\ndriving. AlphaDrive introduces four GRPO-based RL rewards tailored for planning\nand employs a two-stage planning reasoning training strategy that combines SFT\nwith RL. As a result, AlphaDrive significantly improves both planning\nperformance and training efficiency compared to using only SFT or without\nreasoning. Moreover, we are also excited to discover that, following RL\ntraining, AlphaDrive exhibits some emergent multimodal planning capabilities,\nwhich is critical for improving driving safety and efficiency. To the best of\nour knowledge, AlphaDrive is the first to integrate GRPO-based RL with planning\nreasoning into autonomous driving. Code will be released to facilitate future\nresearch.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07608.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.05244",
      "authors": [
        {
          "_id": "67cfebe18a4265f3656a50aa",
          "user": {
            "_id": "642d430a7f9efee76b8713c0",
            "avatarUrl": "/avatars/4981f166a6df8e2ea60cd4c41c2f44d4.svg",
            "isPro": false,
            "fullname": "YuningWu",
            "user": "AQuarterMile",
            "type": "user"
          },
          "name": "Yuning Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-11T08:24:29.900Z",
          "hidden": false
        },
        {
          "_id": "67cfebe18a4265f3656a50ab",
          "name": "Jiahao Mei",
          "hidden": false
        },
        {
          "_id": "67cfebe18a4265f3656a50ac",
          "name": "Ming Yan",
          "hidden": false
        },
        {
          "_id": "67cfebe18a4265f3656a50ad",
          "name": "Chenliang Li",
          "hidden": false
        },
        {
          "_id": "67cfebe18a4265f3656a50ae",
          "name": "SHaopeng Lai",
          "hidden": false
        },
        {
          "_id": "67cfebe18a4265f3656a50af",
          "name": "Yuran Ren",
          "hidden": false
        },
        {
          "_id": "67cfebe18a4265f3656a50b0",
          "name": "Zijia Wang",
          "hidden": false
        },
        {
          "_id": "67cfebe18a4265f3656a50b1",
          "name": "Ji Zhang",
          "hidden": false
        },
        {
          "_id": "67cfebe18a4265f3656a50b2",
          "name": "Mengyue Wu",
          "hidden": false
        },
        {
          "_id": "67cfebe18a4265f3656a50b3",
          "name": "Qin Jin",
          "hidden": false
        },
        {
          "_id": "67cfebe18a4265f3656a50b4",
          "name": "Fei Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-07T08:56:20.000Z",
      "title": "WritingBench : Marca de Test pour Images Complexes de Générateurs",
      "summary": "Le développement récent des grands modèles de langue (LLMs) a considérablement augmenté la capacité de génération de texte, mais l'évaluation du rendement des phrases générées est un problème complexe. Les référentiels actuels se concentrent principalement sur la génération de texte général ou sur des tâches spécifiques de phrases, mais ne comprennent pas les exigences en matière de contenu de haute qualité dans diverses formes de phrases. Pour résoudre ce problème, nous présentons WritingBench. Ce référentiel comprend 6 directions clés et 100 sous-directions détaillées, y compris des phrases créatives et explicatives, informatives et techniques. De plus, nous proposons un référentiel d'évaluation qui dépend de questions que les LLMs peuvent générer dynamiquement pour évaluer des critères d'évaluation propres. Ce référentiel d'évaluation est complété par des modèles d'évaluation qui permettent l'évaluation du style, du format et de la longueur. L'efficacité de ce référentiel d'évaluation est démontrée en montrant que un modèle de 7B paramètres approche les performances les plus récentes (SOTA). Nous publions ce référentiel, les outils d'évaluation et les composants modulaires du référentiel d'évaluation pour contribuer au développement de phrases dans les LLMs.",
      "upvotes": 9,
      "discussionId": "67cfebe38a4265f3656a5136",
      "githubRepo": "https://github.com/X-PLUG/WritingBench",
      "ai_keywords": [
        "large language models (LLMs)",
        "text generation",
        "generative writing",
        "benchmarks",
        "writing domains",
        "subdomains",
        "creative writing",
        "persuasive writing",
        "informative writing",
        "technical writing",
        "query-dependent evaluation framework",
        "instance-specific assessment criteria",
        "critic model",
        "criteria-aware scoring",
        "data curation"
      ]
    },
    "publishedAt": "2025-03-07T03:56:20.000Z",
    "title": "WritingBench: A Comprehensive Benchmark for Generative Writing",
    "summary": "Recent advancements in large language models (LLMs) have significantly\nenhanced text generation capabilities, yet evaluating their performance in\ngenerative writing remains a challenge. Existing benchmarks primarily focus on\ngeneric text generation or limited in writing tasks, failing to capture the\ndiverse requirements of high-quality written contents across various domains.\nTo bridge this gap, we present WritingBench, a comprehensive benchmark designed\nto evaluate LLMs across 6 core writing domains and 100 subdomains, encompassing\ncreative, persuasive, informative, and technical writing. We further propose a\nquery-dependent evaluation framework that empowers LLMs to dynamically generate\ninstance-specific assessment criteria. This framework is complemented by a\nfine-tuned critic model for criteria-aware scoring, enabling evaluations in\nstyle, format and length. The framework's validity is further demonstrated by\nits data curation capability, which enables 7B-parameter models to approach\nstate-of-the-art (SOTA) performance. We open-source the benchmark, along with\nevaluation tools and modular framework components, to advance the development\nof LLMs in writing.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05244.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.04629",
      "authors": [
        {
          "_id": "67cfbab6607797f40c6d4164",
          "name": "Xiangchao Yan",
          "hidden": false
        },
        {
          "_id": "67cfbab6607797f40c6d4165",
          "name": "Shiyang Feng",
          "hidden": false
        },
        {
          "_id": "67cfbab6607797f40c6d4166",
          "name": "Jiakang Yuan",
          "hidden": false
        },
        {
          "_id": "67cfbab6607797f40c6d4167",
          "name": "Renqiu Xia",
          "hidden": false
        },
        {
          "_id": "67cfbab6607797f40c6d4168",
          "name": "Bin Wang",
          "hidden": false
        },
        {
          "_id": "67cfbab6607797f40c6d4169",
          "name": "Bo Zhang",
          "hidden": false
        },
        {
          "_id": "67cfbab6607797f40c6d416a",
          "name": "Lei Bai",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-06T17:15:48.000Z",
      "title": "Slide Poering : Génération Heuristique En Ligne, Génération Guidée par la Mémoire et Évaluation Multidimensionnelle de la Génération Automatique de Rapports",
      "summary": "Les articles de revue jouent un rôle crucial dans la recherche scientifique, surtout dans le contexte du fort accroissement des articles de recherche. Récemment, les chercheurs s'efforcent de générer automatiquement les articles de revue et d'améliorer l'efficacité de la recherche en utilisant des Modèles de Langue de Haute Niveau (LLM). Cependant, il existe des différences claires entre la qualité des articles de revue générés par LLM et ceux écrits par des humains, notamment en ce qui concerne la qualité des approches et la précision des citations. Pour atténuer ces différences, nous présentons SurveyForge. SurveyForge analyse la structure logique des approches humaines et génère des articles de revue en se basant sur des articles de la même domaine de recherche. Ensuite, nous utilisons notre navigateur académique pour générer et améliorer le contenu des articles générés à travers des articles de haute qualité trouvés. De plus, pour effectuer une évaluation globale, nous avons construit SurveyBench, qui comprend 100 articles de revue écrits par des humains et compare l'efficacité des articles générés par l'IA en termes de qualité des approches et de contenu, en les évaluant sur trois dimensions. Les expériences montrent que SurveyForge produit des résultats supérieurs à ceux précédents.",
      "upvotes": 9,
      "discussionId": "67cfbab9607797f40c6d4206",
      "ai_keywords": [
        "LLMs (Large Language Models)",
        "SurveyForge",
        "SurveyBench",
        "AutoSurvey"
      ]
    },
    "publishedAt": "2025-03-06T12:15:48.000Z",
    "title": "SurveyForge: On the Outline Heuristics, Memory-Driven Generation, and\n  Multi-dimensional Evaluation for Automated Survey Writing",
    "summary": "Survey paper plays a crucial role in scientific research, especially given\nthe rapid growth of research publications. Recently, researchers have begun\nusing LLMs to automate survey generation for better efficiency. However, the\nquality gap between LLM-generated surveys and those written by human remains\nsignificant, particularly in terms of outline quality and citation accuracy. To\nclose these gaps, we introduce SurveyForge, which first generates the outline\nby analyzing the logical structure of human-written outlines and referring to\nthe retrieved domain-related articles. Subsequently, leveraging high-quality\npapers retrieved from memory by our scholar navigation agent, SurveyForge can\nautomatically generate and refine the content of the generated article.\nMoreover, to achieve a comprehensive evaluation, we construct SurveyBench,\nwhich includes 100 human-written survey papers for win-rate comparison and\nassesses AI-generated survey papers across three dimensions: reference,\noutline, and content quality. Experiments demonstrate that SurveyForge can\noutperform previous works such as AutoSurvey.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04629.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.04812",
      "authors": [
        {
          "_id": "67ce5542818e1825dea7440b",
          "user": {
            "_id": "6626449503e1f561573d30e9",
            "avatarUrl": "/avatars/e7f9720ccd01bae32d0a03a1b0dacab5.svg",
            "isPro": false,
            "fullname": "Zhibin Lan",
            "user": "zhibinlan",
            "type": "user"
          },
          "name": "Zhibin Lan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-10T08:00:54.535Z",
          "hidden": false
        },
        {
          "_id": "67ce5542818e1825dea7440c",
          "user": {
            "_id": "635239137d071f23d083b056",
            "avatarUrl": "/avatars/1f1a0ed38d8de499d4b78922801c6d95.svg",
            "isPro": false,
            "fullname": "liqiang niu",
            "user": "lqniu",
            "type": "user"
          },
          "name": "Liqiang Niu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-10T08:00:51.713Z",
          "hidden": false
        },
        {
          "_id": "67ce5542818e1825dea7440d",
          "name": "Fandong Meng",
          "hidden": false
        },
        {
          "_id": "67ce5542818e1825dea7440e",
          "name": "Jie Zhou",
          "hidden": false
        },
        {
          "_id": "67ce5542818e1825dea7440f",
          "name": "Jinsong Su",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-04T10:21:57.000Z",
      "title": "LLAVE : Comparaison de l'Entraînement Pondéré avec des Modèles Intrinsèques de Langage et de Vision",
      "summary": "Les modèles d'embedding multimodal universels jouent un rôle critique dans des tâches telles que la récupération alternée d'images et de texte, la RAG multimodal et l'agrégation multimodal. Cependant, nos résultats empiriques indiquent que les modèles d'embedding basés sur LMM entraînés avec la perte standard InfoNCE présentent une forte superposition dans la distribution de similarité entre paires positives et négatives, ce qui rend difficile la distinction efficace de paires négatives difficiles. Pour aborder ce problème, nous proposons un cadre simple mais efficace qui améliore dynamiquement l'apprentissage de représentation du modèle d'embedding pour les paires négatives en fonction de leur difficulté discriminative. Dans ce cadre, nous entraînons une série de modèles appelés LLaVE et évaluons-les sur le benchmark MMEB, qui couvre 4 tâches méta et 36 ensembles de données. Les résultats expérimentaux montrent que LLaVE établit des bases plus solides qui atteignent le niveau de performance de l'état de l'art (SOTA) tout en démontrant une grande échelonnabilité et efficacité. Spécifiquement, LLaVE-2B dépasse les modèles SOTA de 7B antérieurs, tandis que LLaVE-7B atteint une amélioration de performance supplémentaire de 6,2 points. Malgré que LLaVE soit entraîné sur des données d'images et de texte, elle peut être généralisée à des tâches de récupération de texte-vidéo de manière zero-shot et atteindre un rendement fort, démontrant son potentiel notable pour la transfert vers d'autres tâches d'embedding.",
      "upvotes": 9,
      "discussionId": "67ce5543818e1825dea74480",
      "githubRepo": "https://github.com/DeepLearnXMU/LLaVE",
      "ai_keywords": [
        "multimodal embedding models",
        "interleaved image-text retrieval",
        "multimodal RAG",
        "multimodal clustering",
        "LMM-based embedding models",
        "InfoNCE loss",
        "similarity distribution",
        "hard negative pairs",
        "representation learning",
        "LLaVE",
        "MMEB benchmark",
        "meta-tasks",
        "datasets",
        "state-of-the-art (SOTA)",
        "scalability",
        "efficiency",
        "text-video retrieval tasks",
        "zero-shot manner",
        "transfer"
      ]
    },
    "publishedAt": "2025-03-04T05:21:57.000Z",
    "title": "LLaVE: Large Language and Vision Embedding Models with Hardness-Weighted\n  Contrastive Learning",
    "summary": "Universal multimodal embedding models play a critical role in tasks such as\ninterleaved image-text retrieval, multimodal RAG, and multimodal clustering.\nHowever, our empirical results indicate that existing LMM-based embedding\nmodels trained with the standard InfoNCE loss exhibit a high degree of overlap\nin similarity distribution between positive and negative pairs, making it\nchallenging to distinguish hard negative pairs effectively. To deal with this\nissue, we propose a simple yet effective framework that dynamically improves\nthe embedding model's representation learning for negative pairs based on their\ndiscriminative difficulty. Within this framework, we train a series of models,\nnamed LLaVE, and evaluate them on the MMEB benchmark, which covers 4 meta-tasks\nand 36 datasets. Experimental results show that LLaVE establishes stronger\nbaselines that achieve state-of-the-art (SOTA) performance while demonstrating\nstrong scalability and efficiency. Specifically, LLaVE-2B surpasses the\nprevious SOTA 7B models, while LLaVE-7B achieves a further performance\nimprovement of 6.2 points. Although LLaVE is trained on image-text data, it can\ngeneralize to text-video retrieval tasks in a zero-shot manner and achieve\nstrong performance, demonstrating its remarkable potential for transfer to\nother embedding tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04812.png",
    "numComments": 2,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.07459",
      "authors": [
        {
          "_id": "67cfd1934fed2b7e3e4cbb34",
          "user": {
            "_id": "63357c608adfa81faf2ac180",
            "avatarUrl": "/avatars/ae0314c644f882251baf59b9134fd36f.svg",
            "isPro": false,
            "fullname": "Xiangru Tang",
            "user": "RTT1",
            "type": "user"
          },
          "name": "Xiangru Tang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-11T06:00:52.457Z",
          "hidden": false
        },
        {
          "_id": "67cfd1934fed2b7e3e4cbb35",
          "name": "Daniel Shao",
          "hidden": false
        },
        {
          "_id": "67cfd1934fed2b7e3e4cbb36",
          "name": "Jiwoong Sohn",
          "hidden": false
        },
        {
          "_id": "67cfd1934fed2b7e3e4cbb37",
          "name": "Jiapeng Chen",
          "hidden": false
        },
        {
          "_id": "67cfd1934fed2b7e3e4cbb38",
          "name": "Jiayi Zhang",
          "hidden": false
        },
        {
          "_id": "67cfd1934fed2b7e3e4cbb39",
          "name": "Jinyu Xiang",
          "hidden": false
        },
        {
          "_id": "67cfd1934fed2b7e3e4cbb3a",
          "name": "Fang Wu",
          "hidden": false
        },
        {
          "_id": "67cfd1934fed2b7e3e4cbb3b",
          "name": "Yilun Zhao",
          "hidden": false
        },
        {
          "_id": "67cfd1934fed2b7e3e4cbb3c",
          "name": "Chenglin Wu",
          "hidden": false
        },
        {
          "_id": "67cfd1934fed2b7e3e4cbb3d",
          "user": {
            "_id": "65cae89119683f9817c049ea",
            "avatarUrl": "/avatars/b08b10d7c72e2cf1108147e659411b32.svg",
            "isPro": false,
            "fullname": "Wenqi Shi",
            "user": "wshi83",
            "type": "user"
          },
          "name": "Wenqi Shi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-11T08:21:16.321Z",
          "hidden": false
        },
        {
          "_id": "67cfd1934fed2b7e3e4cbb3e",
          "name": "Arman Cohan",
          "hidden": false
        },
        {
          "_id": "67cfd1934fed2b7e3e4cbb3f",
          "name": "Mark Gerstein",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T15:38:44.000Z",
      "title": "MedAgentsBench : Modèle de pensée et cadre d'agents pour l'évaluation des éthiques médicales complexes",
      "summary": "Les modèles de langage grand (LLMs) ont démontré un rendement impressionnant dans l'évaluation des questions médicales actuelles. Cette efficacité élevée a rendu difficile la distinction claire entre des évaluations significatives et des développements avancés. Bien que nos modèles montrent un excellent rendement dans des tests standards, ils font face à des défis pour se concentrer sur la constitution de diagnostics, des plans de traitement et des scénarios cliniques multidimensionnels. Actuellement, les modèles montrent un rendement fort dans des tests standards, mais manquent de concentration sur la constitution de diagnostics, des plans de traitement et des scénarios cliniques multidimensionnels.",
      "upvotes": 7,
      "discussionId": "67cfd1944fed2b7e3e4cbb81",
      "ai_keywords": [
        "MedAgentsBench",
        "multi-step clinical reasoning",
        "diagnosis formulation",
        "treatment planning",
        "MedAgentsBench",
        "DeepSeek R1",
        "OpenAI o3",
        "advanced search-based agent methods"
      ]
    },
    "publishedAt": "2025-03-10T11:38:44.000Z",
    "title": "MedAgentsBench: Benchmarking Thinking Models and Agent Frameworks for\n  Complex Medical Reasoning",
    "summary": "Large Language Models (LLMs) have shown impressive performance on existing\nmedical question-answering benchmarks. This high performance makes it\nincreasingly difficult to meaningfully evaluate and differentiate advanced\nmethods. We present MedAgentsBench, a benchmark that focuses on challenging\nmedical questions requiring multi-step clinical reasoning, diagnosis\nformulation, and treatment planning-scenarios where current models still\nstruggle despite their strong performance on standard tests. Drawing from seven\nestablished medical datasets, our benchmark addresses three key limitations in\nexisting evaluations: (1) the prevalence of straightforward questions where\neven base models achieve high performance, (2) inconsistent sampling and\nevaluation protocols across studies, and (3) lack of systematic analysis of the\ninterplay between performance, cost, and inference time. Through experiments\nwith various base models and reasoning methods, we demonstrate that the latest\nthinking models, DeepSeek R1 and OpenAI o3, exhibit exceptional performance in\ncomplex medical reasoning tasks. Additionally, advanced search-based agent\nmethods offer promising performance-to-cost ratios compared to traditional\napproaches. Our analysis reveals substantial performance gaps between model\nfamilies on complex questions and identifies optimal model selections for\ndifferent computational constraints. Our benchmark and evaluation framework are\npublicly available at https://github.com/gersteinlab/medagents-benchmark.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07459.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.06749",
      "authors": [
        {
          "_id": "67cfb6495944a8e54f24cd9a",
          "name": "Wenxuan Huang",
          "hidden": false
        },
        {
          "_id": "67cfb6495944a8e54f24cd9b",
          "name": "Bohan Jia",
          "hidden": false
        },
        {
          "_id": "67cfb6495944a8e54f24cd9c",
          "name": "Zijie Zhai",
          "hidden": false
        },
        {
          "_id": "67cfb6495944a8e54f24cd9d",
          "name": "Shaosheng Cao",
          "hidden": false
        },
        {
          "_id": "67cfb6495944a8e54f24cd9e",
          "name": "Zheyu Ye",
          "hidden": false
        },
        {
          "_id": "67cfb6495944a8e54f24cd9f",
          "name": "Fei Zhao",
          "hidden": false
        },
        {
          "_id": "67cfb6495944a8e54f24cda0",
          "name": "Yao Hu",
          "hidden": false
        },
        {
          "_id": "67cfb6495944a8e54f24cda1",
          "name": "Shaohui Lin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-09T20:06:45.000Z",
      "title": "Vision R1 : Modèle qui favorise la capacité des modèles de langue multilingues à comprendre et à traiter plusieurs langues.",
      "summary": "DeepSeek-R1-Zero a réussi à découvrir les capacités cognitives logiques d'un modèle de langage de machine (LLM) à travers l'apprentissage par renforcement (RL). Cette approche innovante a étudié si l'apprentissage par renforcement peut améliorer les capacités cognitives logiques d'un MLLM. Cependant, il a été reconnu que l'apprentissage direct par renforcement est difficile pour activer des capacités cognitives logiques complexes dans un MLLM (par exemple, les questions et la réflexion). Pour résoudre ce problème, il a été proposé un MLLM logique, Vision-R1. Spécifiquement, un dataset de haut rendement logique diversifié a été construit en utilisant un modèle d'échange et de filtrage de données, ainsi que 200K de données logiques diverses et le dataset Vision-R1-cold. Ce dataset a été utilisé comme données initiales pour Vision-R1. Une stratégie de contrôle de pensée progressive (PTST) a été proposée pour atténuer les problèmes d'optimisation excessive dans les premiers pas, et la capacité d'apprentissage de processus logiques complexes a été entraînée progressivement à l'aide d'un dataset de 10K de données mathématiques diverses, avec le groupe de politiques d'optimisation (GRPO) et une fonction de récompense formalisée stricte. Les expériences détaillées ont montré un augmentation moyenne de 6% sur les benchmarks de mathématiques logiques. Vision-R1-7B a atteint une précision de 73,5% sur le benchmark MathVista, montrant le niveau le plus élevé parmi les modèles logiques, avec un rendement 0,4% inférieur à OpenAI O1. Les datasets et codes sont disponibles sur l'URL suivante : https://github.com/Osilly/Vision-R1",
      "upvotes": 7,
      "discussionId": "67cfb64f5944a8e54f24cf33",
      "ai_keywords": [
        "Reinforcement Learning (RL)",
        "MLLMs",
        "multimodal reasoning",
        "CoT dataset",
        "modality bridging",
        "data filtering",
        "Vision-R1-cold dataset",
        "Progressive Thinking Suppression Training (PTST)",
        "Group Relative Policy Optimization (GRPO)",
        "hard formatting result reward function",
        "multimodal math dataset",
        "MathVista benchmark",
        "OpenAI O1"
      ]
    },
    "publishedAt": "2025-03-09T16:06:45.000Z",
    "title": "Vision-R1: Incentivizing Reasoning Capability in Multimodal Large\n  Language Models",
    "summary": "DeepSeek-R1-Zero has successfully demonstrated the emergence of reasoning\ncapabilities in LLMs purely through Reinforcement Learning (RL). Inspired by\nthis breakthrough, we explore how RL can be utilized to enhance the reasoning\ncapability of MLLMs. However, direct training with RL struggles to activate\ncomplex reasoning capabilities such as questioning and reflection in MLLMs, due\nto the absence of substantial high-quality multimodal reasoning data. To\naddress this issue, we propose the reasoning MLLM, Vision-R1, to improve\nmultimodal reasoning capability. Specifically, we first construct a\nhigh-quality multimodal CoT dataset without human annotations by leveraging an\nexisting MLLM and DeepSeek-R1 through modality bridging and data filtering to\nobtain a 200K multimodal CoT dataset, Vision-R1-cold dataset. It serves as\ncold-start initialization data for Vision-R1. To mitigate the optimization\nchallenges caused by overthinking after cold start, we propose Progressive\nThinking Suppression Training (PTST) strategy and employ Group Relative Policy\nOptimization (GRPO) with the hard formatting result reward function to\ngradually refine the model's ability to learn correct and complex reasoning\nprocesses on a 10K multimodal math dataset. Comprehensive experiments show our\nmodel achieves an average improvement of sim6% across various multimodal\nmath reasoning benchmarks. Vision-R1-7B achieves a 73.5% accuracy on the widely\nused MathVista benchmark, which is only 0.4% lower than the leading reasoning\nmodel, OpenAI O1. The datasets and code will be released in:\nhttps://github.com/Osilly/Vision-R1 .",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.06749.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.07507",
      "authors": [
        {
          "_id": "67cfa44c3a9d50150f59ffe1",
          "name": "Jie Hu",
          "hidden": false
        },
        {
          "_id": "67cfa44c3a9d50150f59ffe2",
          "name": "Shizun Wang",
          "hidden": false
        },
        {
          "_id": "67cfa44c3a9d50150f59ffe3",
          "name": "Xinchao Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T16:29:10.000Z",
      "title": "PE3R: Efficacité de l'Observation dans la Reconstruction 3D",
      "summary": "Le développement récent du reconnaissance 3D à partir d'images 2D a considérablement amélioré la compréhension de scènes 3D à partir d'images 2D. Cependant, les méthodes actuelles présentent des problèmes importants, tels que la généralisation limitée entre scènes, la diminution de la précision du reconnaissance et la lenteur dans la reconstruction, entre autres. Pour résoudre ces limitations, nous proposons un nouveau cadre de travail \"Perception-Efficient 3D Reconstruction (PE3R)\" qui atteint à la même fois la précision et l'efficacité. PE3R utilise une architecture proactive qui permet la reconstruction rapide de champs de sens 3D. Ce cadre de travail montre une forte généralisation 0-shot pour différentes scènes et objets, et améliore considérablement la vitesse de reconstruction. Des expériences de division de vecteurs ouverts en 2D et de reconstruction 3D ont démontré les effets et la large applicabilité de PE3R. Ce cadre de travail a atteint une vitesse de reconstruction de champs de sens 3D plus de 9 fois plus rapide, montrant également un grand impact sur la précision du reconnaissance et de la reconstruction, établissant de nouveaux standards dans le domaine. Le code est disponible sur https://github.com/hujiecpp/PE3R.",
      "upvotes": 5,
      "discussionId": "67cfa4503a9d50150f5a0137",
      "ai_keywords": [
        "feed-forward architecture",
        "3D semantic field reconstruction",
        "zero-shot generalization",
        "2D-to-3D open-vocabulary segmentation",
        "perception accuracy",
        "reconstruction precision",
        "speedup"
      ]
    },
    "publishedAt": "2025-03-10T12:29:10.000Z",
    "title": "PE3R: Perception-Efficient 3D Reconstruction",
    "summary": "Recent advancements in 2D-to-3D perception have significantly improved the\nunderstanding of 3D scenes from 2D images. However, existing methods face\ncritical challenges, including limited generalization across scenes, suboptimal\nperception accuracy, and slow reconstruction speeds. To address these\nlimitations, we propose Perception-Efficient 3D Reconstruction (PE3R), a novel\nframework designed to enhance both accuracy and efficiency. PE3R employs a\nfeed-forward architecture to enable rapid 3D semantic field reconstruction. The\nframework demonstrates robust zero-shot generalization across diverse scenes\nand objects while significantly improving reconstruction speed. Extensive\nexperiments on 2D-to-3D open-vocabulary segmentation and 3D reconstruction\nvalidate the effectiveness and versatility of PE3R. The framework achieves a\nminimum 9-fold speedup in 3D semantic field reconstruction, along with\nsubstantial gains in perception accuracy and reconstruction precision, setting\nnew benchmarks in the field. The code is publicly available at:\nhttps://github.com/hujiecpp/PE3R.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07507.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.07197",
      "authors": [
        {
          "_id": "67cfa76cf36e4221c5009654",
          "user": {
            "_id": "624f909eac5dd186b01ac3f5",
            "avatarUrl": "/avatars/71a5c93c491064ef9e1eda80fda90665.svg",
            "isPro": false,
            "fullname": "Zebin You",
            "user": "yyyou",
            "type": "user"
          },
          "name": "Zebin You",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-11T08:22:38.059Z",
          "hidden": false
        },
        {
          "_id": "67cfa76cf36e4221c5009655",
          "name": "Jingyang Ou",
          "hidden": false
        },
        {
          "_id": "67cfa76cf36e4221c5009656",
          "name": "Xiaolu Zhang",
          "hidden": false
        },
        {
          "_id": "67cfa76cf36e4221c5009657",
          "name": "Jun Hu",
          "hidden": false
        },
        {
          "_id": "67cfa76cf36e4221c5009658",
          "name": "Jun Zhou",
          "hidden": false
        },
        {
          "_id": "67cfa76cf36e4221c5009659",
          "name": "Chongxuan Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T11:27:12.000Z",
      "title": "Modèle efficace et efficace pour la génération d'images de masques",
      "summary": "Les modèles d'encodage caché et les modèles de diffusion cachée, bien que conçus pour des raisons et des objectifs différents, peuvent être intégrés dans un seul cadre. À cette perspective, nous explorons l'espace de conception de l'entraînement et de l'échantillonnage, identifiant les facteurs qui contribuent à l'efficacité et au rendement. Sur la base de ces améliorations, nous développons notre modèle sous le nom d'eMIGM (Enhanced Mixture of Implicit Generative Models). Expérimentalement, eMIGM a montré un impact fort sur la génération d'ImageNet. En particulier, pour ImageNet 256x256, en utilisant un nombre similaire de fonctions d'évaluation (NFEs) et un nombre de paramètres du modèle, eMIGM a dépassé les modèles les plus récents de VAR. Tant que le nombre de NFEs et de paramètres du modèle augmente, eMIGM atteint un rendement similaire aux modèles de diffusion continue, mais nécessite moins de 40% de NFEs. De plus, pour ImageNet 512x512, avec une réduction d'environ 60% de NFEs, eMIGM a dépassé les modèles de diffusion continue.",
      "upvotes": 5,
      "discussionId": "67cfa76df36e4221c5009686",
      "ai_keywords": [
        "masked image generation models",
        "masked diffusion models",
        "training and sampling",
        "Fr\\'echet Inception Distance (FID)",
        "function evaluations (NFEs)",
        "VAR",
        "continuous diffusion models"
      ]
    },
    "publishedAt": "2025-03-10T07:27:12.000Z",
    "title": "Effective and Efficient Masked Image Generation Models",
    "summary": "Although masked image generation models and masked diffusion models are\ndesigned with different motivations and objectives, we observe that they can be\nunified within a single framework. Building upon this insight, we carefully\nexplore the design space of training and sampling, identifying key factors that\ncontribute to both performance and efficiency. Based on the improvements\nobserved during this exploration, we develop our model, referred to as eMIGM.\nEmpirically, eMIGM demonstrates strong performance on ImageNet generation, as\nmeasured by Fr\\'echet Inception Distance (FID). In particular, on ImageNet\n256x256, with similar number of function evaluations (NFEs) and model\nparameters, eMIGM outperforms the seminal VAR. Moreover, as NFE and model\nparameters increase, eMIGM achieves performance comparable to the\nstate-of-the-art continuous diffusion models while requiring less than 40% of\nthe NFE. Additionally, on ImageNet 512x512, with only about 60% of the NFE,\neMIGM outperforms the state-of-the-art continuous diffusion models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07197.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.06520",
      "authors": [
        {
          "_id": "67cf990ca80a73999cc816c3",
          "name": "Yuqi Liu",
          "hidden": false
        },
        {
          "_id": "67cf990ca80a73999cc816c4",
          "name": "Bohao Peng",
          "hidden": false
        },
        {
          "_id": "67cf990ca80a73999cc816c5",
          "name": "Zhisheng Zhong",
          "hidden": false
        },
        {
          "_id": "67cf990ca80a73999cc816c6",
          "name": "Zihao Yue",
          "hidden": false
        },
        {
          "_id": "67cf990ca80a73999cc816c7",
          "name": "Fanbin Lu",
          "hidden": false
        },
        {
          "_id": "67cf990ca80a73999cc816c8",
          "name": "Bei Yu",
          "hidden": false
        },
        {
          "_id": "67cf990ca80a73999cc816c9",
          "name": "Jiaya Jia",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-09T08:48:51.000Z",
      "title": "Seg-Zero : Connexion de raisons par renforcement cérébral : Guide de division",
      "summary": "Les méthodes traditionnelles de division d'inférence utilisent des étiquettes de classification et des explications brèves pour dépendre de réglages micro-régulés et standardisés, limitant ainsi la capacité de généralisation en dehors des domaines et présentant une absence de processus d'inférence explicites. Pour résoudre ces limites, nous proposons un nouveau cadre de travail appelé Seg-Zero. Ce cadre de travail montre une capacité de généralisation notable et permet d'obtenir des chaînes d'inférence explicites grâce à un renforcement cognitif. Seg-Zero introduit une architecture décodeur-encodeur composée de modèles de raisonnement et de modèles de division. Le modèle de raisonnement comprend le but du utilisateur et génère des chaînes d'inférence explicites, tandis que le modèle de division génère des masques au niveau de pixel en utilisant des frontières de position. Nous avons conçu une structure de récompense complexe qui inclut la précision et la formalité, et nous avons entraîné de manière spéciale en utilisant GRPO, ce qui permet à Seg-Zero de démontrer une puissante capacité de généralisation 0-shot sans besoin de données d'inférence explicites. Les tests montrent que Seg-Zero-7B a un rendement de 57,5 en Zero-shot, surpassant LISA-7B d'au-delà de 18%, démontrant un accroissement impressionnant qui met en évidence la capacité de Seg-Zero à généraliser en dehors des domaines et à fournir des processus d'inférence explicites. Le code est disponible sur https://github.com/dvlab-research/Seg-Zero.",
      "upvotes": 5,
      "discussionId": "67cf990da80a73999cc81723",
      "ai_keywords": [
        "Seg-Zero",
        "decoupled architecture",
        "reasoning model",
        "segmentation model",
        "positional prompts",
        "pixel-level masks",
        "cognitive reinforcement",
        "reward mechanism",
        "reinforcement learning",
        "GRPO",
        "zero-shot generalization",
        "ReasonSeg benchmark",
        "emergent test-time reasoning"
      ]
    },
    "publishedAt": "2025-03-09T04:48:51.000Z",
    "title": "Seg-Zero: Reasoning-Chain Guided Segmentation via Cognitive\n  Reinforcement",
    "summary": "Traditional methods for reasoning segmentation rely on supervised fine-tuning\nwith categorical labels and simple descriptions, limiting its out-of-domain\ngeneralization and lacking explicit reasoning processes. To address these\nlimitations, we propose Seg-Zero, a novel framework that demonstrates\nremarkable generalizability and derives explicit chain-of-thought reasoning\nthrough cognitive reinforcement. Seg-Zero introduces a decoupled architecture\nconsisting of a reasoning model and a segmentation model. The reasoning model\ninterprets user intentions, generates explicit reasoning chains, and produces\npositional prompts, which are subsequently used by the segmentation model to\ngenerate precious pixel-level masks. We design a sophisticated reward mechanism\nthat integrates both format and accuracy rewards to effectively guide\noptimization directions. Trained exclusively via reinforcement learning with\nGRPO and without explicit reasoning data, Seg-Zero achieves robust zero-shot\ngeneralization and exhibits emergent test-time reasoning capabilities.\nExperiments show that Seg-Zero-7B achieves a zero-shot performance of 57.5 on\nthe ReasonSeg benchmark, surpassing the prior LISA-7B by 18\\%. This significant\nimprovement highlights Seg-Zero's ability to generalize across domains while\npresenting an explicit reasoning process. Code is available at\nhttps://github.com/dvlab-research/Seg-Zero.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.06520.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.06121",
      "authors": [
        {
          "_id": "67cfa436d37b8309603da1ee",
          "user": {
            "_id": "66de61d7174e9c6971dbb253",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/sM0xfS7HAkf_6GmkEGjDk.png",
            "isPro": false,
            "fullname": "Alic Li",
            "user": "Alic-Li",
            "type": "user"
          },
          "name": "Li weile",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-03-11T04:29:57.207Z",
          "hidden": false
        },
        {
          "_id": "67cfa436d37b8309603da1ef",
          "user": {
            "_id": "6176b32847ee6431f632981e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6176b32847ee6431f632981e/02rZ_oLAI0Ll6Y6be7Q9F.jpeg",
            "isPro": false,
            "fullname": "IvanD",
            "user": "xiaol",
            "type": "user"
          },
          "name": "Liu Xiao",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-03-11T03:33:06.087Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-08T08:31:18.000Z",
      "title": "Black Ghost Rimmer : Utilise RWKV-7 comme un remplaçable simple et excellent de Transformers, et il est un méthode appropriée pour des modèles de séquences temporelles à grande échelle.",
      "summary": "Les modèles de séries temporelles posent des problèmes cruciaux pour traiter de grands ensembles de données complexes, ce qui est nécessaire pour atteindre l'échelle des modèles de langage grands comme les LLMs. En raison des caractéristiques des données de séries temporelles et des exigences computationnelles du taille du modèle, des nouvelles approximations sont nécessaires. Les chercheurs ont exploré des architectures comme les Transformers, les LSTMs et les GRUs pour aborder ces défis, mais proposent maintenant une nouvelle solution en introduisant le méta-apprentissage avec RWKV-7. En intégrant les composants de mélange temporel et de mélange de canaux de RWKV-7 dans un modèle de séries temporelle basé sur les Transformers appelé Timer, on a réussi à améliorer le rendement d'un facteur approximativement 1,13 à 43,3 avec un nombre de paramètres de 1/23, tout en réduisant le temps d'entraînement d'un facteur de 4,5. Notre code et les poids du modèle sont disponibles sur https://github.com/Alic-Li/BlackGoose_Rimer et peuvent contribuer à l'évolution de la recherche et au développement.",
      "upvotes": 5,
      "discussionId": "67cfa437d37b8309603da253",
      "ai_keywords": [
        "Transformers",
        "LSTMs",
        "GRUs",
        "RWKV-7",
        "meta-learning",
        "state update mechanism",
        "time mix",
        "channel mix",
        "Timer",
        "performance improvement",
        "training time",
        "parameters"
      ]
    },
    "publishedAt": "2025-03-08T03:31:18.000Z",
    "title": "BlackGoose Rimer: Harnessing RWKV-7 as a Simple yet Superior Replacement\n  for Transformers in Large-Scale Time Series Modeling",
    "summary": "Time series models face significant challenges in scaling to handle large and\ncomplex datasets, akin to the scaling achieved by large language models (LLMs).\nThe unique characteristics of time series data and the computational demands of\nmodel scaling necessitate innovative approaches. While researchers have\nexplored various architectures such as Transformers, LSTMs, and GRUs to address\nthese challenges, we propose a novel solution using RWKV-7, which incorporates\nmeta-learning into its state update mechanism. By integrating RWKV-7's time mix\nand channel mix components into the transformer-based time series model Timer,\nwe achieve a substantial performance improvement of approximately 1.13 to 43.3x\nand a 4.5x reduction in training time with 1/23 parameters, all while utilizing\nfewer parameters. Our code and model weights are publicly available for further\nresearch and development at https://github.com/Alic-Li/BlackGoose_Rimer.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.06121.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.03499",
      "authors": [
        {
          "_id": "67cb02680a2a716f25805cb4",
          "name": "Wonjun Kang",
          "hidden": false
        },
        {
          "_id": "67cb02680a2a716f25805cb5",
          "name": "Kevin Galim",
          "hidden": false
        },
        {
          "_id": "67cb02680a2a716f25805cb6",
          "name": "Yuchen Zeng",
          "hidden": false
        },
        {
          "_id": "67cb02680a2a716f25805cb7",
          "name": "Minjae Lee",
          "hidden": false
        },
        {
          "_id": "67cb02680a2a716f25805cb8",
          "name": "Hyung Il Koo",
          "hidden": false
        },
        {
          "_id": "67cb02680a2a716f25805cb9",
          "name": "Nam Ik Cho",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-05T13:44:42.000Z",
      "title": "État Offset Tuning : Modèle d'État pour l'Ajustement Efficace des Paramètres Basé sur l'État",
      "summary": "Les modèles d'état d'espace (SSMs) se sont transformés en méthodes plus efficaces pour réduire les coûts de calcul du Transformer. Cependant, l'application de méthodes de fine-tuning efficace en paramètres (PEFT) pour les SSMs a été peu explorée. En particulier, la Tuning de Prompt et la Tuning de Prefix, qui sont largement utilisées dans les Transformers, ne montrent pas de résultats exceptionnels pour les SSMs. En réponse à cette situation, nous proposons un méthode basée sur les états qui peut constituer une bonne alternative à la Tuning de Prompt. Cette nouvelle famille de méthodes est naturellement issu de caractéristiques structurales des SSMs. La méthode basée sur les états ne dépend pas de prompts externes et ajuste directement les caractéristiques liées à l'état. De plus, nous présentons un nouveau méthode de PEFT basée sur les états appelée Tuning de Décalage d'État. Ce méthode affecte directement l'état actuel à chaque étape temporelle. Nous avons démontré l'efficacité de notre méthode grâce à des expérimentations avec différents ensembles de données. Le code est disponible sur https://github.com/furiosa-ai/ssm-state-tuning.",
      "upvotes": 3,
      "discussionId": "67cb02690a2a716f25805cfd",
      "githubRepo": "https://github.com/furiosa-ai/ssm-state-tuning",
      "ai_keywords": [
        "State Space Models (SSMs)",
        "Parameter-Efficient Fine-Tuning (PEFT)",
        "Prompt Tuning",
        "Prefix-Tuning",
        "State-based methods",
        "State-offset Tuning",
        "timesteps",
        "state-related features",
        "state-at-the-current-step"
      ]
    },
    "publishedAt": "2025-03-05T08:44:42.000Z",
    "title": "State-offset Tuning: State-based Parameter-Efficient Fine-Tuning for\n  State Space Models",
    "summary": "State Space Models (SSMs) have emerged as efficient alternatives to\nTransformers, mitigating their quadratic computational cost. However, the\napplication of Parameter-Efficient Fine-Tuning (PEFT) methods to SSMs remains\nlargely unexplored. In particular, prompt-based methods like Prompt Tuning and\nPrefix-Tuning, which are widely used in Transformers, do not perform well on\nSSMs. To address this, we propose state-based methods as a superior alternative\nto prompt-based methods. This new family of methods naturally stems from the\narchitectural characteristics of SSMs. State-based methods adjust state-related\nfeatures directly instead of depending on external prompts. Furthermore, we\nintroduce a novel state-based PEFT method: State-offset Tuning. At every\ntimestep, our method directly affects the state at the current step, leading to\nmore effective adaptation. Through extensive experiments across diverse\ndatasets, we demonstrate the effectiveness of our method. Code is available at\nhttps://github.com/furiosa-ai/ssm-state-tuning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.03499.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.07603",
      "authors": [
        {
          "_id": "67cfc310f2b1fe815dc24ebf",
          "name": "Sedrick Keh",
          "hidden": false
        },
        {
          "_id": "67cfc310f2b1fe815dc24ec0",
          "name": "Jean Mercat",
          "hidden": false
        },
        {
          "_id": "67cfc310f2b1fe815dc24ec1",
          "name": "Samir Yitzhak Gadre",
          "hidden": false
        },
        {
          "_id": "67cfc310f2b1fe815dc24ec2",
          "name": "Kushal Arora",
          "hidden": false
        },
        {
          "_id": "67cfc310f2b1fe815dc24ec3",
          "name": "Igor Vasiljevic",
          "hidden": false
        },
        {
          "_id": "67cfc310f2b1fe815dc24ec4",
          "name": "Benjamin Burchfiel",
          "hidden": false
        },
        {
          "_id": "67cfc310f2b1fe815dc24ec5",
          "name": "Shuran Song",
          "hidden": false
        },
        {
          "_id": "67cfc310f2b1fe815dc24ec6",
          "name": "Russ Tedrake",
          "hidden": false
        },
        {
          "_id": "67cfc310f2b1fe815dc24ec7",
          "name": "Thomas Kollar",
          "hidden": false
        },
        {
          "_id": "67cfc310f2b1fe815dc24ec8",
          "name": "Ludwig Schmidt",
          "hidden": false
        },
        {
          "_id": "67cfc310f2b1fe815dc24ec9",
          "name": "Achal Dave",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T17:58:19.000Z",
      "title": "VLMs doivent être entraînés préalablement avec des données d'image.",
      "summary": "Les modèles pré-traités montrent un excellent rendement dans des tâches de langage visuel après une formation supplémentaire avec des données d'image. En ajoutant des images au deuxième pas de formation, ces capacités peuvent être développées efficacement, mais comparés aux VLMs qui intègrent rapidement les images, il n'est pas clair ces bénéfices ou ces pertes qui se produisent. Pour étudier cela, les ensembles de données, l'échelle, la proportion d'images à texte et le niveau de pré-traitement ont été limités, et des modèles ont été entraînés avec ces paramètres. Ensuite, ces modèles ont été évalués dans des tâches de langage visuel et dans des tâches qui ne comprenaient que du texte, et un ajustement fin a été réalisé pour comparer leur rendement. Les modèles qui ont été pré-traités avec une mélange d'images et de texte ont maintenu un bon rendement dans des évaluations avec uniquement du texte, mais ont montré un meilleur rendement dans des tâches de langage visuel. En moyenne sur six tâches différentes, en ajoutant les tokens d'image au 80% du traitement dans un modèle de 1B, le rendement moyen a amélioré de 2%.",
      "upvotes": 2,
      "discussionId": "67cfc314f2b1fe815dc24fe3",
      "ai_keywords": [
        "pre-trained LLMs",
        "vision-language tasks",
        "fine-tune",
        "vision tokens"
      ]
    },
    "publishedAt": "2025-03-10T13:58:19.000Z",
    "title": "Should VLMs be Pre-trained with Image Data?",
    "summary": "Pre-trained LLMs that are further trained with image data perform well on\nvision-language tasks. While adding images during a second training phase\neffectively unlocks this capability, it is unclear how much of a gain or loss\nthis two-step pipeline gives over VLMs which integrate images earlier into the\ntraining process. To investigate this, we train models spanning various\ndatasets, scales, image-text ratios, and amount of pre-training done before\nintroducing vision tokens. We then fine-tune these models and evaluate their\ndownstream performance on a suite of vision-language and text-only tasks. We\nfind that pre-training with a mixture of image and text data allows models to\nperform better on vision-language tasks while maintaining strong performance on\ntext-only evaluations. On an average of 6 diverse tasks, we find that for a 1B\nmodel, introducing visual tokens 80% of the way through pre-training results in\na 2% average improvement over introducing visual tokens to a fully pre-trained\nmodel.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07603.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.06885",
      "authors": [
        {
          "_id": "67cfc1c5182d970d40896a5e",
          "user": {
            "_id": "655b813476e4fad5529f3256",
            "avatarUrl": "/avatars/73d83e45d921531f9830a0ea80f76491.svg",
            "isPro": false,
            "fullname": "Yan Yang",
            "user": "HelloKKMe",
            "type": "user"
          },
          "name": "Yan Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-11T08:21:28.574Z",
          "hidden": false
        },
        {
          "_id": "67cfc1c5182d970d40896a5f",
          "user": {
            "_id": "6357362f811ee2fa05070f64",
            "avatarUrl": "/avatars/2cf37efb80f5cfb3e4e9d08674de6dd1.svg",
            "isPro": false,
            "fullname": "Dongxu Li",
            "user": "dxli1",
            "type": "user"
          },
          "name": "Dongxu Li",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-11T04:53:27.837Z",
          "hidden": false
        },
        {
          "_id": "67cfc1c5182d970d40896a60",
          "name": "Haoning Wu",
          "hidden": false
        },
        {
          "_id": "67cfc1c5182d970d40896a61",
          "name": "Bei Chen",
          "hidden": false
        },
        {
          "_id": "67cfc1c5182d970d40896a62",
          "name": "Liu Liu",
          "hidden": false
        },
        {
          "_id": "67cfc1c5182d970d40896a63",
          "name": "Liyuan Pan",
          "hidden": false
        },
        {
          "_id": "67cfc1c5182d970d40896a64",
          "name": "Junnan Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T03:29:18.000Z",
      "title": "ProBench : Évaluation d'un modèle multimodal pour évaluer des experts de diverses disciplines dans l'évaluation d'actions ouvertes.",
      "summary": "Les tâches de haut niveau de multimodalité sont l'essence de l'intelligence générale maximale. Alors que la capacité des modèles de langage multimodal (MLLMs) continue d'améliorer, il est nécessaire et difficile d'évaluer et de comprendre le développement de leur intelligence multimodal. Dans cet article, nous présentons ProBench, un cadre de référence de questions ouvertes qui nécessitent des connaissances spécialisées et leur développement. ProBench couvre une large gamme de 10 domaines et 56 sous-domaines, allant des sciences aux arts, aux lettres, à la programmation, à la mathématiques et à la littérature créative. Expérimentalement, nous avons évalué et comparé 24 modèles les plus récents en utilisant MLLM-as-a-Judge. Enfin, le meilleur modèle open-source a été opposé au modèle propriétaire, mais ProBench pose des questions cruciales sur le savoir visuel, la compréhension contextuelle, le savoir par domaine et le raisonnement du développement, offrant des orientations pour les futures études en IA multimodal.",
      "upvotes": 2,
      "discussionId": "67cfc1c7182d970d40896b1d",
      "projectPage": "https://yan98.github.io/ProBench/",
      "githubRepo": "https://github.com/Yan98/ProBench_eval",
      "ai_keywords": [
        "multimodal large language models (MLLMs)",
        "benchmark",
        "user queries",
        "professional expertise",
        "advanced reasoning",
        "high-quality samples",
        "daily productivity demands",
        "fields",
        "sub-fields",
        "visual perception",
        "textual understanding",
        "domain knowledge"
      ]
    },
    "publishedAt": "2025-03-09T23:29:18.000Z",
    "title": "ProBench: Judging Multimodal Foundation Models on Open-ended\n  Multi-domain Expert Tasks",
    "summary": "Solving expert-level multimodal tasks is a key milestone towards general\nintelligence. As the capabilities of multimodal large language models (MLLMs)\ncontinue to improve, evaluation of such advanced multimodal intelligence\nbecomes necessary yet challenging. In this work, we introduce ProBench, a\nbenchmark of open-ended user queries that require professional expertise and\nadvanced reasoning. ProBench consists of 4,000 high-quality samples\nindependently submitted by professionals based on their daily productivity\ndemands. It spans across 10 fields and 56 sub-fields, including science, arts,\nhumanities, coding, mathematics, and creative writing. Experimentally, we\nevaluate and compare 24 latest models using MLLM-as-a-Judge. Our results reveal\nthat although the best open-source models rival the proprietary ones, ProBench\npresents significant challenges in visual perception, textual understanding,\ndomain knowledge and advanced reasoning, thus providing valuable directions for\nfuture multimodal AI research efforts.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.06885.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.02199",
      "authors": [
        {
          "_id": "67c90dad6f3ef3c2c77689b0",
          "name": "Ailin Deng",
          "hidden": false
        },
        {
          "_id": "67c90dad6f3ef3c2c77689b1",
          "name": "Tri Cao",
          "hidden": false
        },
        {
          "_id": "67c90dad6f3ef3c2c77689b2",
          "user": {
            "_id": "67cbb6ea2cc05acaab023f75",
            "avatarUrl": "/avatars/79272c8889a8c472cf75172ead72daea.svg",
            "isPro": false,
            "fullname": "Zhirui Chen",
            "user": "ryanchen42",
            "type": "user"
          },
          "name": "Zhirui Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-10T08:03:34.042Z",
          "hidden": false
        },
        {
          "_id": "67c90dad6f3ef3c2c77689b3",
          "name": "Bryan Hooi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-04T02:21:07.000Z",
      "title": "Words or Vision: Lenguaje y Visión: ¿Creer en el Lenguaje o en la Visión?",
      "summary": "Les modèles de Vision-Langue (VLMs) conçus pour des données d'images similaires et diversifiées d'entrées de contexte montrent un excellent rendement lorsque l'information des images et le contexte sont intégrées. Cependant, il est connu que l'étude de l'équilibre des préférences entre modèles n'est pas suffisante. Pour explorer la préférence entre modèles de VLMs pour des données d'images et des contextes diversifiés, 10 VLMs ont été évalués en appliquant des modifications au contexte dans 4 tâches similaires d'images. On a découvert le phénomène de \"ignorer le contexte\" : lorsqu'il existe une asymétrie, les VLMs ont tendance à confiance excessive dans l'information du contexte par rapport à celle des images, ce qui peut entraîner une perte de rendement et des doutes sur la sécurité. Les facteurs provoquant cette inclination vers le contexte ont été analysés : planification des commandes, taille du modèle de langue, pertinence du contexte, ordre des tokens, et l'interaction entre la confiance dans l'image et le contexte. L'inclination vers le contexte peut être exacerbée par la position du modèle de langue, car des facteurs comme l'ordre des tokens peuvent aggraver cette inclination. Pour résoudre ce problème, des contextes ont été élargis et des entraînements supervisés ont été effectués, montrant leurs effets. De plus, des analyses théoriques ont été fournies indiquant que le phénomène d'ignorer le contexte est causé par une simple inégalité entre contextes lors de l'entraînement et l'asymétrie entre différents données des modèles. Ces résultats soulignent l'importance de l'ajustement de l'interaction entre modèles et l'équilibre de l'entraînement dans les VLMs, ainsi que la nécessité de gérer l'asymétrie entre les données des modèles pour améliorer la robustesse et la confiance.",
      "upvotes": 2,
      "discussionId": "67c90dae6f3ef3c2c77689ec",
      "ai_keywords": [
        "Vision-Language Models (VLMs)",
        "blind faith in text",
        "modality preferences",
        "textual variations",
        "vision-centric tasks",
        "text bias",
        "instruction prompts",
        "language model size",
        "token order",
        "positional biases",
        "multi-modal data",
        "supervised fine-tuning",
        "text augmentation",
        "balanced training",
        "modality interactions"
      ]
    },
    "publishedAt": "2025-03-03T21:21:07.000Z",
    "title": "Words or Vision: Do Vision-Language Models Have Blind Faith in Text?",
    "summary": "Vision-Language Models (VLMs) excel in integrating visual and textual\ninformation for vision-centric tasks, but their handling of inconsistencies\nbetween modalities is underexplored. We investigate VLMs' modality preferences\nwhen faced with visual data and varied textual inputs in vision-centered\nsettings. By introducing textual variations to four vision-centric tasks and\nevaluating ten Vision-Language Models (VLMs), we discover a ``blind faith\nin text'' phenomenon: VLMs disproportionately trust textual data over visual\ndata when inconsistencies arise, leading to significant performance drops under\ncorrupted text and raising safety concerns. We analyze factors influencing this\ntext bias, including instruction prompts, language model size, text relevance,\ntoken order, and the interplay between visual and textual certainty. While\ncertain factors, such as scaling up the language model size, slightly mitigate\ntext bias, others like token order can exacerbate it due to positional biases\ninherited from language models. To address this issue, we explore supervised\nfine-tuning with text augmentation and demonstrate its effectiveness in\nreducing text bias. Additionally, we provide a theoretical analysis suggesting\nthat the blind faith in text phenomenon may stem from an imbalance of pure text\nand multi-modal data during training. Our findings highlight the need for\nbalanced training and careful consideration of modality interactions in VLMs to\nenhance their robustness and reliability in handling multi-modal data\ninconsistencies.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.02199.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.07595",
      "authors": [
        {
          "_id": "67cfa440aff9c98bb3f45a56",
          "user": {
            "_id": "64b3fc1fa24816979609dcb3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b3fc1fa24816979609dcb3/cHRMs4YegRcgbZO8_bBaZ.jpeg",
            "isPro": false,
            "fullname": "Sinclair Schneider",
            "user": "SinclairSchneider",
            "type": "user"
          },
          "name": "Sinclair Schneider",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-03-11T02:48:03.261Z",
          "hidden": false
        },
        {
          "_id": "67cfa440aff9c98bb3f45a57",
          "name": "Florian Steuber",
          "hidden": false
        },
        {
          "_id": "67cfa440aff9c98bb3f45a58",
          "name": "Joao A. G. Schneider",
          "hidden": false
        },
        {
          "_id": "67cfa440aff9c98bb3f45a59",
          "name": "Gabi Dreo Rodosek",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T17:56:25.000Z",
      "title": "Technologie d'évasion de la détection dans les modèles de langage grands",
      "summary": "La croissance des modèles de langage grands a conduit à une utilisation large, mais a également augmenté les risques et a causé des problèmes comme la propagation de la vérité. Par conséquent, le développement de systèmes de classification comme DetectGPT a été important. Ces détecteurs ont montré être vulnérables aux techniques d'évasion de bas niveau, ce qui a été démontré dans une série d'expériences. En modifiant systématiquement la température de génération, les détecteurs superficiels ont montré la moindre confiance. En ajustant les modèles de génération par l'apprentissage supplémentaire, il a été possible d'éviter les détecteurs basés sur BERT. Enfin, la reconfiguration a réussi à atteindre plus de 90% d'évasion dans les détecteurs comme DetectGPT, bien que les phrases résultantes soient similaires aux originales en termes de similarité. Comparés aux études précédentes, les méthodes présentées ont démontré un meilleur rendement. Les impacts sur la société et les possibilités d'investigation sont également en discussion.",
      "upvotes": 1,
      "discussionId": "67cfa441aff9c98bb3f45a95",
      "ai_keywords": [
        "large language models",
        "fake news",
        "classification systems",
        "DetectGPT",
        "evasion techniques",
        "generative models",
        "temperature",
        "shallow learning-detectors",
        "fine-tuning",
        "reinforcement learning",
        "BERT-based-detectors",
        "zero-shot-detectors",
        "rephrasing"
      ]
    },
    "publishedAt": "2025-03-10T13:56:25.000Z",
    "title": "Detection Avoidance Techniques for Large Language Models",
    "summary": "The increasing popularity of large language models has not only led to\nwidespread use but has also brought various risks, including the potential for\nsystematically spreading fake news. Consequently, the development of\nclassification systems such as DetectGPT has become vital. These detectors are\nvulnerable to evasion techniques, as demonstrated in an experimental series:\nSystematic changes of the generative models' temperature proofed shallow\nlearning-detectors to be the least reliable. Fine-tuning the generative model\nvia reinforcement learning circumvented BERT-based-detectors. Finally,\nrephrasing led to a >90\\% evasion of zero-shot-detectors like DetectGPT,\nalthough texts stayed highly similar to the original. A comparison with\nexisting work highlights the better performance of the presented methods.\nPossible implications for society and further research are discussed.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07595.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.07465",
      "authors": [
        {
          "_id": "67cfaaed7f229132171f596b",
          "name": "Ao Wang",
          "hidden": false
        },
        {
          "_id": "67cfaaed7f229132171f596c",
          "name": "Lihao Liu",
          "hidden": false
        },
        {
          "_id": "67cfaaed7f229132171f596d",
          "name": "Hui Chen",
          "hidden": false
        },
        {
          "_id": "67cfaaed7f229132171f596e",
          "name": "Zijia Lin",
          "hidden": false
        },
        {
          "_id": "67cfaaed7f229132171f596f",
          "name": "Jungong Han",
          "hidden": false
        },
        {
          "_id": "67cfaaed7f229132171f5970",
          "name": "Guiguang Ding",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T15:42:59.000Z",
      "title": "YOLOE : efficace en temps, cela se voit facilement",
      "summary": "La détection et le segmentation d'objets sont largement utilisés dans les applications de vision par ordinateur, mais les modèles traditionnels comme la série YOLO sont limités par des catégories décisives et sont peu adaptés aux scénarios ouverts. Les méthodes récentes de jeux ouverts utilisent des prompts textuels, visuels ou sans prompts pour surmonter ces problèmes, mais leur forte consommation de calcul et la complexité du jeu peuvent entraîner des pertes de performance et d'efficacité. Dans cet article, nous proposons un approche qui intègre la détection et le segmentation d'objets en temps réel dans un modèle très efficace appelé YOLOE, en utilisant différentes structures de prompts ouverts. Nous proposons Re-parameterizable Region-Text Alignment (RepRTA) pour les prompts textuels, qui utilise une réseau plus léger et ré-paramétrisable pour affiner les codages textuels préalablement entraînés et renforcer l'alignement visuel-textuel. Pour les prompts visuels, nous présentons Semantic-Activated Visual Prompt Encoder (SAVPE), qui utilise des sémantiques séparées et des activations de vecteurs pour améliorer la codification visuelle avec précision et faible complexité. Pour les scénarios sans prompts, nous proposons Lazy Region-Prompt Contrast (LRPC), qui utilise un grand vocabulaire et des codages spécialisés pour reconnaître tous les objets. Des expériences extensives montrent la excellente capacité de 0-shot de YOLOE, sa haute efficacité d'inférence et le faible coût d'entraînement. Spécifiquement, sur LVIS, nous atteignons un coût d'entraînement 3 fois plus faible et une augmentation de la vitesse d'inférence de 1,4 fois, et YOLOE-v8-S dépasse YOLO-Worldv2-S en 3,5 AP. Sur COCO, YOLOE-v8-L obtient un temps d'entraînement approximativement 4 fois plus faible que YOLOv8-L dans les jeux fermés, avec des améliorations de 0,6 AP^b et 0,4 AP^m. Le code et les modèles sont disponibles sur https://github.com/THU-MIG/yoloe.",
      "upvotes": 0,
      "discussionId": "67cfaaf27f229132171f5ab4",
      "ai_keywords": [
        "Object detection",
        "Segmentation",
        "YOLO series",
        "Open-set methods",
        "Text prompts",
        "Visual cues",
        "Prompt-free paradigm",
        "YOLOE",
        "Rep-parameterizable Region-Text Alignment (RepRTA)",
        "Pretrained textual embeddings",
        "Re-parameterizable lightweight auxiliary network",
        "Semantic-Activated Visual Prompt Encoder (SAVPE)",
        "Decoupled semantic and activation branches",
        "Visual embedding",
        "Lazy Region-Prompt Contrast (LRPC)",
        "Large vocabulary",
        "Specialized embedding",
        "LVIS",
        "Zero-shot performance",
        "Transferability",
        "Inference efficiency",
        "Training cost",
        "AP",
        "COCO",
        "Closed-set YOLOv8-L",
        "Inference speedup",
        "Training time"
      ]
    },
    "publishedAt": "2025-03-10T11:42:59.000Z",
    "title": "YOLOE: Real-Time Seeing Anything",
    "summary": "Object detection and segmentation are widely employed in computer vision\napplications, yet conventional models like YOLO series, while efficient and\naccurate, are limited by predefined categories, hindering adaptability in open\nscenarios. Recent open-set methods leverage text prompts, visual cues, or\nprompt-free paradigm to overcome this, but often compromise between performance\nand efficiency due to high computational demands or deployment complexity. In\nthis work, we introduce YOLOE, which integrates detection and segmentation\nacross diverse open prompt mechanisms within a single highly efficient model,\nachieving real-time seeing anything. For text prompts, we propose\nRe-parameterizable Region-Text Alignment (RepRTA) strategy. It refines\npretrained textual embeddings via a re-parameterizable lightweight auxiliary\nnetwork and enhances visual-textual alignment with zero inference and\ntransferring overhead. For visual prompts, we present Semantic-Activated Visual\nPrompt Encoder (SAVPE). It employs decoupled semantic and activation branches\nto bring improved visual embedding and accuracy with minimal complexity. For\nprompt-free scenario, we introduce Lazy Region-Prompt Contrast (LRPC) strategy.\nIt utilizes a built-in large vocabulary and specialized embedding to identify\nall objects, avoiding costly language model dependency. Extensive experiments\nshow YOLOE's exceptional zero-shot performance and transferability with high\ninference efficiency and low training cost. Notably, on LVIS, with 3times\nless training cost and 1.4times inference speedup, YOLOE-v8-S surpasses\nYOLO-Worldv2-S by 3.5 AP. When transferring to COCO, YOLOE-v8-L achieves 0.6\nAP^b and 0.4 AP^m gains over closed-set YOLOv8-L with nearly 4times less\ntraining time. Code and models are available at\nhttps://github.com/THU-MIG/yoloe.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07465.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.07426",
      "authors": [
        {
          "_id": "67cff321f2b1fe815dce3722",
          "name": "Junkang Wu",
          "hidden": false
        },
        {
          "_id": "67cff321f2b1fe815dce3723",
          "name": "Kexin Huang",
          "hidden": false
        },
        {
          "_id": "67cff321f2b1fe815dce3724",
          "name": "Xue Wang",
          "hidden": false
        },
        {
          "_id": "67cff321f2b1fe815dce3725",
          "name": "Jinyang Gao",
          "hidden": false
        },
        {
          "_id": "67cff321f2b1fe815dce3726",
          "name": "Bolin Ding",
          "hidden": false
        },
        {
          "_id": "67cff321f2b1fe815dce3727",
          "name": "Jiancan Wu",
          "hidden": false
        },
        {
          "_id": "67cff321f2b1fe815dce3728",
          "name": "Xiangnan He",
          "hidden": false
        },
        {
          "_id": "67cff321f2b1fe815dce3729",
          "user": {
            "_id": "65fca775fa59bdf4737b1a84",
            "avatarUrl": "/avatars/a161b510bde8f57e7686cbb0b4aa6a52.svg",
            "isPro": false,
            "fullname": "Xiang Wang",
            "user": "xiangwang1223",
            "type": "user"
          },
          "name": "Xiang Wang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-11T08:24:02.839Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T15:11:07.000Z",
      "title": "RePO : Optimisation des Préférences Basée sur le ReLU",
      "summary": "Adéquer la nature humaine des LLM est importante dans des fonctions réalistes, mais les méthodes actuelles ont des problèmes de calcul et d'stabilité. DPO construit un modèle en ligne en utilisant un seul paramètre beta, mais des méthodes ultérieures comme SimPO réintroduisent la complexité et utilisent deux paramètres (beta, gamma). Nous proposons un algorithme d'optimisation de caractéristiques basé sur le ReLU (RePO). RePO maintient un marge sans référence de SimPO, élimine beta grâce à un analyse graphique, et introduit une perte max-margin basée sur le ReLU pour filtrer les paires légères de manière naturelle, ce qui élimine beta. Théoriquement, RePO se transforme en le limite de SimPO, où les poids de la fonction logistique convergent vers un valeur binaire et forment l'enveloppe convexe de la perte 0-1. Les résultats des expériences sur AlpacaEval 2 et Arena-Hard indiquent que RePO dépasse DPO et SimPO sur plusieurs modèles de base, et nécessite seulement l'ajustement d'un paramètre.",
      "upvotes": 0,
      "discussionId": "67cff322f2b1fe815dce3787",
      "ai_keywords": [
        "LLMS",
        "RLHF",
        "DPO",
        "SimPO",
        "ReLU-based Preference Optimization (RePO)",
        "reference-free margins",
        "gradient analysis",
        "ReLU-based max-margin loss",
        "convex envelope",
        "0-1 loss"
      ]
    },
    "publishedAt": "2025-03-10T11:11:07.000Z",
    "title": "RePO: ReLU-based Preference Optimization",
    "summary": "Aligning large language models (LLMs) with human preferences is critical for\nreal-world deployment, yet existing methods like RLHF face computational and\nstability challenges. While DPO establishes an offline paradigm with single\nhyperparameter beta, subsequent methods like SimPO reintroduce complexity\nthrough dual parameters (beta, gamma). We propose {ReLU-based Preference\nOptimization (RePO)}, a streamlined algorithm that eliminates beta via two\nadvances: (1) retaining SimPO's reference-free margins but removing beta\nthrough gradient analysis, and (2) adopting a ReLU-based max-margin loss that\nnaturally filters trivial pairs. Theoretically, RePO is characterized as\nSimPO's limiting case (beta to infty), where the logistic weighting\ncollapses to binary thresholding, forming a convex envelope of the 0-1 loss.\nEmpirical results on AlpacaEval 2 and Arena-Hard show that RePO outperforms DPO\nand SimPO across multiple base models, requiring only one hyperparameter to\ntune.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07426.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.06362",
      "authors": [
        {
          "_id": "67cffd119f703990a8e25925",
          "name": "Umberto Cappellazzo",
          "hidden": false
        },
        {
          "_id": "67cffd119f703990a8e25926",
          "name": "Minsu Kim",
          "hidden": false
        },
        {
          "_id": "67cffd119f703990a8e25927",
          "name": "Stavros Petridis",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-09T00:02:10.000Z",
      "title": "Adaptation Audio-Visuelle de la Reconnaissance Vocale avec des Modèles de Grande Taille Monomodales basés sur les Matrices de Matrizon",
      "summary": "La Reconnaissance Visuelle de la Voix (RVV) utilise deux modèles, un de la voix et un d'image, pour améliorer la robustesse du reconnaissance de la voix dans des environnements bruyants. Les récents avancées des modèles de langage à grande échelle (LLMs) ont également montré une efficacité tant dans le reconnaissance de la voix que dans la RVV. Cependant, la longueur des représentations de la voix est longue, ce qui rend l'intégration directe avec les LLMs très coûteuse en termes de calculs. Les méthodes existantes abordent ce problème en compressant les représentations de la voix avant qu'elles soient utilisées comme entrée pour les LLMs. Cependant, un taux élevé de compression peut causer un dégradement du rendement et un équilibre entre les coûts de calcul et la précision du reconnaissance. Pour résoudre ces défis, nous proposons le premier LLM basé sur MATRIO-SK pour la RVV, Llama-MTSK. Ce modèle peut ajuster l'attribution des tokens de la voix et de l'image de manière flexible en fonction des contraintes de calcul, maintenant ainsi des rendements élevés. Notre approche utilise l'apprentissage de la représentation MATRIO-SK comme modèle, codifie les représentations de la voix et de l'image multigranulairement à l'intérieur du modèle et élimine la nécessité d'entraîner des modèles avec des niveaux de compression différents. De plus, nous introduisons trois stratégies basées sur LoRA pour une fine-tuning efficace du LLM, en utilisant des modules LoRA globaux et propres à l'échelle. Les tests sur les deux ensembles de données maximaux de RVV montrent que Llama-MTSK obtient les meilleurs résultats et dépasse les résultats des modèles entraînés indépendamment à un niveau de compression fixé.",
      "upvotes": 0,
      "discussionId": "67cffd129f703990a8e25990",
      "ai_keywords": [
        "Audio-Visual Speech Recognition (AVSR)",
        "Large Language Models (LLMs)",
        "speech representations",
        "computational costs",
        "audio-visual token allocation",
        "Matryoshka-based Multimodal LLM",
        "Matryoshka Representation Learning",
        "global LoRA modules",
        "scale-specific LoRA modules",
        "LoRA-based Matryoshka strategies"
      ]
    },
    "publishedAt": "2025-03-08T19:02:10.000Z",
    "title": "Adaptive Audio-Visual Speech Recognition via Matryoshka-Based Multimodal\n  LLMs",
    "summary": "Audio-Visual Speech Recognition (AVSR) leverages both audio and visual\nmodalities to enhance speech recognition robustness, particularly in noisy\nenvironments. Recent advancements in Large Language Models (LLMs) have\ndemonstrated their effectiveness in speech recognition, including AVSR.\nHowever, due to the significant length of speech representations, direct\nintegration with LLMs imposes substantial computational costs. Prior approaches\naddress this by compressing speech representations before feeding them into\nLLMs. However, higher compression ratios often lead to performance degradation,\nnecessitating a trade-off between computational efficiency and recognition\naccuracy. To address this challenge, we propose Llama-MTSK, the first\nMatryoshka-based Multimodal LLM for AVSR, which enables flexible adaptation of\nthe audio-visual token allocation based on specific computational constraints\nwhile preserving high performance. Our approach, inspired by Matryoshka\nRepresentation Learning, encodes audio-visual representations at multiple\ngranularities within a single model, eliminating the need to train separate\nmodels for different compression levels. Moreover, to efficiently fine-tune the\nLLM, we introduce three LoRA-based Matryoshka strategies using global and\nscale-specific LoRA modules. Extensive evaluations on the two largest AVSR\ndatasets demonstrate that Llama-MTSK achieves state-of-the-art results,\nmatching or surpassing models trained independently at fixed compression\nlevels.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.06362.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.05283",
      "authors": [
        {
          "_id": "67cef721e5ab8ec0550b7a66",
          "name": "Souhail Hadgi",
          "hidden": false
        },
        {
          "_id": "67cef721e5ab8ec0550b7a67",
          "name": "Luca Moschella",
          "hidden": false
        },
        {
          "_id": "67cef721e5ab8ec0550b7a68",
          "user": {
            "_id": "5e8ef1f14957053f606489e6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1635502086699-5e8ef1f14957053f606489e6.jpeg",
            "isPro": false,
            "fullname": "Andrea Santilli",
            "user": "teelinsan",
            "type": "user"
          },
          "name": "Andrea Santilli",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-10T14:35:44.397Z",
          "hidden": false
        },
        {
          "_id": "67cef721e5ab8ec0550b7a69",
          "name": "Diego Gomez",
          "hidden": false
        },
        {
          "_id": "67cef721e5ab8ec0550b7a6a",
          "name": "Qixing Huang",
          "hidden": false
        },
        {
          "_id": "67cef721e5ab8ec0550b7a6b",
          "name": "Emanuele Rodolà",
          "hidden": false
        },
        {
          "_id": "67cef721e5ab8ec0550b7a6c",
          "name": "Simone Melzi",
          "hidden": false
        },
        {
          "_id": "67cef721e5ab8ec0550b7a6d",
          "name": "Maks Ovsjanikov",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-07T09:51:56.000Z",
      "title": "L'évasion 3D et les espaces de puissance de texte par alignement d'arrangements du vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux vieux viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo",
      "summary": "Recentes études ont montré que les caractéristiques apprises par un unique codateur visuel 2D et un codateur de texte lors de l'entraînement à grande échelle ne sont pas affectées de manière significative par les représentations d'autres modalités, démontrant une structure surprenante et intrinsèque. Cependant, le rôle que peut jouer un codateur 3D en relation avec d'autres modalités n'a pas été étudié. De plus, les modèles 3D fondamentaux actuels ont été entraînés avec de grands ensembles de données, mais généralement en se basant sur des codateurs libres de représentations et des objectifs d'alignement explicites. Dans cette étude, on examine la possibilité d'aligner des espaces de caractéristiques obtenus d'un unique codateur 3D avec des espaces de caractéristiques basés sur le texte et de traitement postérieur. Le rendement d'un alignement de caractéristiques basé sur le traitement postérieur dans un seul modèle de texte et un codateur 3D est limité. Ensuite, on extrait des espaces partiels des espaces de caractéristiques correspondants et on projette les représentations entraînées dans un espace partiel de basse dimension sélectionné, ce qui améliore significativement la qualité de l'alignement et augmente la précision dans des tâches de matching et de recherche. Notre analyse révèle que ces espaces partiels partagés ont des caractéristiques principalement sémantiques et géométriques, différenciant clairement d'autres représentations. En général, notre étude vise à établir un norme pour l'alignement des espaces de caractéristiques entre un unique codateur 3D et un espace de caractéristiques basé sur le texte, et révèle des caractéristiques générales partagées et des caractéristiques distinctives comparées aux données 3D et d'autres représentations.",
      "upvotes": 0,
      "discussionId": "67cef723e5ab8ec0550b7ac8",
      "ai_keywords": [
        "uni-modal 2D vision",
        "text encoders",
        "learned features",
        "3D encoders",
        "3D foundation models",
        "alignment objectives",
        "feature alignment",
        "subspaces",
        "lower-dimensional subspaces",
        "semantic data representations",
        "geometric data representations",
        "matching tasks",
        "retrieval tasks"
      ]
    },
    "publishedAt": "2025-03-07T04:51:56.000Z",
    "title": "Escaping Plato's Cave: Towards the Alignment of 3D and Text Latent\n  Spaces",
    "summary": "Recent works have shown that, when trained at scale, uni-modal 2D vision and\ntext encoders converge to learned features that share remarkable structural\nproperties, despite arising from different representations. However, the role\nof 3D encoders with respect to other modalities remains unexplored.\nFurthermore, existing 3D foundation models that leverage large datasets are\ntypically trained with explicit alignment objectives with respect to frozen\nencoders from other representations. In this work, we investigate the\npossibility of a posteriori alignment of representations obtained from\nuni-modal 3D encoders compared to text-based feature spaces. We show that naive\npost-training feature alignment of uni-modal text and 3D encoders results in\nlimited performance. We then focus on extracting subspaces of the corresponding\nfeature spaces and discover that by projecting learned representations onto\nwell-chosen lower-dimensional subspaces the quality of alignment becomes\nsignificantly higher, leading to improved accuracy on matching and retrieval\ntasks. Our analysis further sheds light on the nature of these shared\nsubspaces, which roughly separate between semantic and geometric data\nrepresentations. Overall, ours is the first work that helps to establish a\nbaseline for post-training alignment of 3D uni-modal and text feature spaces,\nand helps to highlight both the shared and unique properties of 3D data\ncompared to other representations.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05283.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.03511",
      "authors": [
        {
          "_id": "67cd7ace999766d8cd73fb18",
          "user": {
            "_id": "6732f2c24c2f18a60e76b915",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6732f2c24c2f18a60e76b915/W6oozAjM-zu7E3SL9uQ97.jpeg",
            "isPro": false,
            "fullname": "Fan",
            "user": "KianYale",
            "type": "user"
          },
          "name": "Qingyu Fan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-10T08:01:41.233Z",
          "hidden": false
        },
        {
          "_id": "67cd7ace999766d8cd73fb19",
          "name": "Yinghao Cai",
          "hidden": false
        },
        {
          "_id": "67cd7ace999766d8cd73fb1a",
          "name": "Chao Li",
          "hidden": false
        },
        {
          "_id": "67cd7ace999766d8cd73fb1b",
          "name": "Wenzhe He",
          "hidden": false
        },
        {
          "_id": "67cd7ace999766d8cd73fb1c",
          "name": "Xudong Zheng",
          "hidden": false
        },
        {
          "_id": "67cd7ace999766d8cd73fb1d",
          "name": "Tao Lu",
          "hidden": false
        },
        {
          "_id": "67cd7ace999766d8cd73fb1e",
          "name": "Bin Liang",
          "hidden": false
        },
        {
          "_id": "67cd7ace999766d8cd73fb1f",
          "name": "Shuo Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-05T13:57:37.000Z",
      "title": "NeuGrasp : Détection de la Production d'Objets Matériels Indépendants par la Construction de Surfaces Neurales Généralisables à l'aide de Profils de Caractéristiques",
      "summary": "Quand un robot capture des objets transparents et hautement réfléchissants, les méthodes basées sur l'information de profondeur exacte rencontrent de grands problèmes. Dans cet article, nous présentons NeuGrasp, une technique qui utilise des projections de fond pour effectuer des opérations de bruit irrelevant indépendamment du matériau. NeuGrasp combine des transformers et des projections globales pour codifier spatialement et accumuler des caractéristiques de points de vue, permettant ainsi une reconstruction de surfaces puissante même dans des conditions de points de vue étroits ou rares. Elle renforce les caractéristiques résiduelles d'objets tels que les porocones et améliore la perception spatiale en utilisant la projection de la légende. NeuGrasp montre un excellent rendement, même lorsqu'il capture des objets transparents et hautement réfléchissants. Pour obtenir plus de détails, consultez le site web https://neugrasp.github.io/.",
      "upvotes": 0,
      "discussionId": "67cd7ad0999766d8cd73fb77",
      "ai_keywords": [
        "neural surface reconstruction",
        "background priors",
        "material-agnostic grasp detection",
        "transformers",
        "global prior volumes",
        "multi-view features",
        "spatial encoding",
        "narrow and sparse viewing conditions",
        "residual feature enhancement",
        "occupancy-prior volume",
        "transparent objects",
        "specular surfaces"
      ]
    },
    "publishedAt": "2025-03-05T08:57:37.000Z",
    "title": "NeuGrasp: Generalizable Neural Surface Reconstruction with Background\n  Priors for Material-Agnostic Object Grasp Detection",
    "summary": "Robotic grasping in scenes with transparent and specular objects presents\ngreat challenges for methods relying on accurate depth information. In this\npaper, we introduce NeuGrasp, a neural surface reconstruction method that\nleverages background priors for material-agnostic grasp detection. NeuGrasp\nintegrates transformers and global prior volumes to aggregate multi-view\nfeatures with spatial encoding, enabling robust surface reconstruction in\nnarrow and sparse viewing conditions. By focusing on foreground objects through\nresidual feature enhancement and refining spatial perception with an\noccupancy-prior volume, NeuGrasp excels in handling objects with transparent\nand specular surfaces. Extensive experiments in both simulated and real-world\nscenarios show that NeuGrasp outperforms state-of-the-art methods in grasping\nwhile maintaining comparable reconstruction quality. More details are available\nat https://neugrasp.github.io/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.03511.png",
    "numComments": 1,
    "isAuthorParticipating": true
  }
]