[
  {
    "paper": {
      "id": "2503.15265",
      "authors": [
        {
          "_id": "67db8c4c9e4f93ee46411c1d",
          "name": "Ruowen Zhao",
          "hidden": false
        },
        {
          "_id": "67db8c4c9e4f93ee46411c1e",
          "name": "Junliang Ye",
          "hidden": false
        },
        {
          "_id": "67db8c4c9e4f93ee46411c1f",
          "name": "Zhengyi Wang",
          "hidden": false
        },
        {
          "_id": "67db8c4c9e4f93ee46411c20",
          "name": "Guangce Liu",
          "hidden": false
        },
        {
          "_id": "67db8c4c9e4f93ee46411c21",
          "name": "Yiwen Chen",
          "hidden": false
        },
        {
          "_id": "67db8c4c9e4f93ee46411c22",
          "name": "Yikai Wang",
          "hidden": false
        },
        {
          "_id": "67db8c4c9e4f93ee46411c23",
          "name": "Jun Zhu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-19T14:39:30.000Z",
      "submittedOnDailyAt": "2025-03-20T03:22:40.364Z",
      "title": "DeepMesh : Génération d'Artiste Matching Automatique par Apprentissage par Référence",
      "submittedOnDailyBy": {
        "_id": "6522e4fbd89bc7773ddc4b58",
        "avatarUrl": "/avatars/3e9b158af52c5f738a3eae72dcbb3824.svg",
        "isPro": false,
        "fullname": "Ruowen Zhao",
        "user": "zzzrw",
        "type": "user"
      },
      "summary": "Le triangle de maillage est un élément important dans la manipulation et la rendition efficaces dans les applications 3D. Le méthode de retour automatique est utilisée pour générer un triangle de maillage structuré en prédisant des tokens beta dispersés, mais sa efficacité est limitée par la quantité de faces et les incomplétudes du triangle de maillage. Pour résoudre ces problèmes, nous proposons le cadre de travail DeepMesh. Ce cadre intègre deux innovations clés pour optimiser la génération de triangles de maillage. 1. Il inclut un nouveau algorithme de tokenisation efficace, qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de tokenisation efficace qui inclut un algorithme de",
      "upvotes": 25,
      "discussionId": "67db8c519e4f93ee46411d60",
      "projectPage": "https://zhaorw02.github.io/DeepMesh/",
      "githubRepo": "https://github.com/zhaorw02/DeepMesh",
      "ai_keywords": [
        "triangle meshes",
        "auto-regressive methods",
        "discrete vertex tokens",
        "face counts",
        "mesh incompleteness",
        "DeepMesh",
        "tokenization algorithm",
        "data curation",
        "data processing",
        "Reinforcement Learning (RL)",
        "Direct Preference Optimization (DPO)",
        "human evaluation",
        "3D metrics",
        "point clouds",
        "intricate details",
        "precise topology",
        "state-of-the-art methods",
        "precision",
        "quality"
      ]
    },
    "publishedAt": "2025-03-19T10:39:30.000Z",
    "title": "DeepMesh: Auto-Regressive Artist-mesh Creation with Reinforcement\n  Learning",
    "summary": "Triangle meshes play a crucial role in 3D applications for efficient\nmanipulation and rendering. While auto-regressive methods generate structured\nmeshes by predicting discrete vertex tokens, they are often constrained by\nlimited face counts and mesh incompleteness. To address these challenges, we\npropose DeepMesh, a framework that optimizes mesh generation through two key\ninnovations: (1) an efficient pre-training strategy incorporating a novel\ntokenization algorithm, along with improvements in data curation and\nprocessing, and (2) the introduction of Reinforcement Learning (RL) into 3D\nmesh generation to achieve human preference alignment via Direct Preference\nOptimization (DPO). We design a scoring standard that combines human evaluation\nwith 3D metrics to collect preference pairs for DPO, ensuring both visual\nappeal and geometric accuracy. Conditioned on point clouds and images, DeepMesh\ngenerates meshes with intricate details and precise topology, outperforming\nstate-of-the-art methods in both precision and quality. Project page:\nhttps://zhaorw02.github.io/DeepMesh/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.15265.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6522e4fbd89bc7773ddc4b58",
      "avatarUrl": "/avatars/3e9b158af52c5f738a3eae72dcbb3824.svg",
      "fullname": "Ruowen Zhao",
      "name": "zzzrw",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.13288",
      "authors": [
        {
          "_id": "67dbc49d85eacb364e913c38",
          "name": "Fangzhi Xu",
          "hidden": false
        },
        {
          "_id": "67dbc49d85eacb364e913c39",
          "user": {
            "_id": "67dbe3d969655e406fda64b8",
            "avatarUrl": "/avatars/6053c84e32d0e46dd1e490c493f766ed.svg",
            "isPro": false,
            "fullname": "Mei Tuan",
            "user": "Meituannnnnn",
            "type": "user"
          },
          "name": "Hang Yan",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-20T09:48:32.179Z",
          "hidden": false
        },
        {
          "_id": "67dbc49d85eacb364e913c3a",
          "name": "Chang Ma",
          "hidden": false
        },
        {
          "_id": "67dbc49d85eacb364e913c3b",
          "name": "Haiteng Zhao",
          "hidden": false
        },
        {
          "_id": "67dbc49d85eacb364e913c3c",
          "name": "Jun Liu",
          "hidden": false
        },
        {
          "_id": "67dbc49d85eacb364e913c3d",
          "name": "Qika Lin",
          "hidden": false
        },
        {
          "_id": "67dbc49d85eacb364e913c3e",
          "name": "Zhiyong Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-17T15:38:33.000Z",
      "submittedOnDailyAt": "2025-03-20T06:08:48.330Z",
      "title": "φ-Decoding : Exploration du temps d'inférence équilibré et utilisation d'échantillonnage prédictif adaptatif",
      "submittedOnDailyBy": {
        "_id": "64e6cf78ecce34cb442dc889",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e6cf78ecce34cb442dc889/qVZFiUEpBpSkmH8SQeinm.jpeg",
        "isPro": false,
        "fullname": "Fangzhi Xu",
        "user": "xufangzhi",
        "type": "user"
      },
      "summary": "La optimisation de l'inférence est un processus qui cherche à obtenir une série de raisons justifiantes pour échelonner les calculs de manière efficace, garantissant un rendement optimal. Cette approche permet d'améliorer l'efficacité de l'inférence. Les méthodes d'exploration passées, qui abordaient les déficiences de la génération automatique de retours, ont découvert que l'immense espace de recherche rend difficile le maintien d'un équilibre efficace entre l'exploration et le développement, ce qui rend difficile l'obtention d'étapes optimales. Pour résoudre ce problème, nous introduisons un nouveau méthode appelé \"folorescan sampling\" pour prédire les étapes optimales globalement, en calculant les étapes futures. Ce méthode permet de représenter précisément les valeurs des étapes à travers des évaluations fortement expressives. En utilisant folorescan et clustering, nous pouvons approcher deux distributions et effectuer des échantillonnages pour sélectionner et développer les étapes optimales. Pour soutenir l'efficacité de l'inférence, nous proposons des méthodes d'exploration \"plausizing\" en largeur et en profondeur qui permettent l'adaptation de l'architecture de calcul. Les expériences sur un large éventail de 7 benchmarks ont montré que phi-Decoding dépasse significativement les baselines en termes d'efficacité et de rendement. L'analyse de développement montre que cette méthodologie s'étend à différents modèles de langage grand et montre une grande capacité d'échellabilité dans différents buckets de calcul. Le code est disponible sur GitHub (https://github.com/xufangzhi/phi-Decoding) et sera également fourni sous forme de paquet de code open sur PyPI.",
      "upvotes": 24,
      "discussionId": "67dbc49f85eacb364e913d20",
      "githubRepo": "https://github.com/xufangzhi/phi-Decoding",
      "ai_keywords": [
        "inference-time optimization",
        "auto-regressive generation",
        "foresight sampling",
        "$\\phi$-Decoding",
        "joint distribution",
        "in-width and in-depth pruning",
        "LLMs (Large Language Models)"
      ]
    },
    "publishedAt": "2025-03-17T11:38:33.000Z",
    "title": "φ-Decoding: Adaptive Foresight Sampling for Balanced Inference-Time\n  Exploration and Exploitation",
    "summary": "Inference-time optimization scales computation to derive deliberate reasoning\nsteps for effective performance. While previous search-based strategies address\nthe short-sightedness of auto-regressive generation, the vast search space\nleads to excessive exploration and insufficient exploitation. To strike an\nefficient balance to derive the optimal step, we frame the decoding strategy as\nforesight sampling, leveraging simulated future steps to obtain globally\noptimal step estimation. Built on it, we propose a novel decoding strategy,\nnamed phi-Decoding. To provide a precise and expressive estimation of step\nvalue, phi-Decoding approximates two distributions via foresight and\nclustering. Sampling from the joint distribution, the optimal steps can be\nselected for exploitation. To support adaptive computation allocation, we\npropose in-width and in-depth pruning strategies, featuring a light-weight\nsolution to achieve inference efficiency. Extensive experiments across seven\nbenchmarks show phi-Decoding outperforms strong baselines in both\nperformance and efficiency. Additional analysis demonstrates its generalization\nacross various LLMs and scalability across a wide range of computing budgets.\nThe code will be released at https://github.com/xufangzhi/phi-Decoding, and the\nopen-source PyPI package is coming soon.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13288.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64e6cf78ecce34cb442dc889",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e6cf78ecce34cb442dc889/qVZFiUEpBpSkmH8SQeinm.jpeg",
      "fullname": "Fangzhi Xu",
      "name": "xufangzhi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.15485",
      "authors": [
        {
          "_id": "67db7dd224fe67fe45b21e63",
          "name": "Zineng Tang",
          "hidden": false
        },
        {
          "_id": "67db7dd224fe67fe45b21e64",
          "name": "Long Lian",
          "hidden": false
        },
        {
          "_id": "67db7dd224fe67fe45b21e65",
          "name": "Seun Eisape",
          "hidden": false
        },
        {
          "_id": "67db7dd224fe67fe45b21e66",
          "name": "XuDong Wang",
          "hidden": false
        },
        {
          "_id": "67db7dd224fe67fe45b21e67",
          "name": "Roei Herzig",
          "hidden": false
        },
        {
          "_id": "67db7dd224fe67fe45b21e68",
          "name": "Adam Yala",
          "hidden": false
        },
        {
          "_id": "67db7dd224fe67fe45b21e69",
          "name": "Alane Suhr",
          "hidden": false
        },
        {
          "_id": "67db7dd224fe67fe45b21e6a",
          "name": "Trevor Darrell",
          "hidden": false
        },
        {
          "_id": "67db7dd224fe67fe45b21e6b",
          "name": "David M. Chan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-19T17:58:57.000Z",
      "submittedOnDailyAt": "2025-03-20T01:01:18.127Z",
      "title": "TULIP : Exercice de prédiction intégrée de langage et d'image",
      "submittedOnDailyBy": {
        "_id": "6388f68c43d8b0797a09ff84",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669920369989-noauth.jpeg",
        "isPro": false,
        "fullname": "David Chan",
        "user": "davidchan",
        "type": "user"
      },
      "summary": "Bien sûr, voici la traduction en français :\n\nBien que les succès de CLIP et SigLIP aient démontré leur efficacité, ces modèles ont été améliorés pour s'adapter de manière adéquate à des tâches visuelles qui exigent une compréhension des images avec une précision élevée (par exemple, segmentation, estimation de profondeur, reconnaissance d'objets détaillés), et ont mis l'accent sur la disposition du langage, ce qui a affecté leur compréhension des images. D'autre part, les modèles centrés sur la vision, bien que spécialisés dans le traitement de l'information visuelle, rencontrent des difficultés dans la compréhension du langage et limitent la flexibilité des tâches dirigées par le langage. Dans cette étude, nous présentons TULIP, un modèle ouvert qui peut remplacer directement CLIP et d'autres modèles. Notre approche utilise l'expansion de données génératives, l'apprentissage comparatif renforcé entre images et entre langues, et la normalisation de la reconstruction d'images et de langage pour entraîner des caractéristiques visuelles détaillées tout en maintenant la disposition du sens global. Notre approche comporte plus de 10 milliards de paramètres, dépassant les modèles SOTA sur plusieurs benchmarks, atteignant un nouveau SOTA sur ImageNet-1K avec 0 shot, améliorant d'au moins un facteur de 2 sur RxRx1 par rapport à SigLIP et d'au moins 3 sur MMVP. Les codes et les checkpoints sont disponibles sur https://tulip-berkeley.github.io.",
      "upvotes": 18,
      "discussionId": "67db7dd424fe67fe45b21ee1",
      "projectPage": "https://tulip-berkeley.github.io/",
      "ai_keywords": [
        "generative data augmentation",
        "enhanced image-image and text-text contrastive learning",
        "image/text reconstruction regularization",
        "fine-grained visual features",
        "global semantic alignment",
        "zero-shot performance",
        "few-shot classification",
        "vision-language models"
      ]
    },
    "publishedAt": "2025-03-19T13:58:57.000Z",
    "title": "TULIP: Towards Unified Language-Image Pretraining",
    "summary": "Despite the recent success of image-text contrastive models like CLIP and\nSigLIP, these models often struggle with vision-centric tasks that demand\nhigh-fidelity image understanding, such as counting, depth estimation, and\nfine-grained object recognition. These models, by performing language\nalignment, tend to prioritize high-level semantics over visual understanding,\nweakening their image understanding. On the other hand, vision-focused models\nare great at processing visual information but struggle to understand language,\nlimiting their flexibility for language-driven tasks. In this work, we\nintroduce TULIP, an open-source, drop-in replacement for existing CLIP-like\nmodels. Our method leverages generative data augmentation, enhanced image-image\nand text-text contrastive learning, and image/text reconstruction\nregularization to learn fine-grained visual features while preserving global\nsemantic alignment. Our approach, scaling to over 1B parameters, outperforms\nexisting state-of-the-art (SOTA) models across multiple benchmarks,\nestablishing a new SOTA zero-shot performance on ImageNet-1K, delivering up to\na 2times enhancement over SigLIP on RxRx1 in linear probing for few-shot\nclassification, and improving vision-language models, achieving over 3times\nhigher scores than SigLIP on MMVP. Our code/checkpoints are available at\nhttps://tulip-berkeley.github.io",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.15485.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6388f68c43d8b0797a09ff84",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669920369989-noauth.jpeg",
      "fullname": "David Chan",
      "name": "davidchan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.15475",
      "authors": [
        {
          "_id": "67db729fa720e711cff4d205",
          "name": "Foundation AI Team",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d206",
          "name": "Kiran Bhat",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d207",
          "name": "Nishchaie Khanna",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d208",
          "name": "Karun Channa",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d209",
          "name": "Tinghui Zhou",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d20a",
          "name": "Yiheng Zhu",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d20b",
          "name": "Xiaoxia Sun",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d20c",
          "name": "Charles Shang",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d20d",
          "name": "Anirudh Sudarshan",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d20e",
          "name": "Maurice Chu",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d20f",
          "name": "Daiqing Li",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d210",
          "name": "Kangle Deng",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d211",
          "name": "Jean-Philippe Fauconnier",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d212",
          "name": "Tijmen Verhulsdonck",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d213",
          "name": "Maneesh Agrawala",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d214",
          "name": "Kayvon Fatahalian",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d215",
          "name": "Alexander Weiss",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d216",
          "name": "Christian Reiser",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d217",
          "name": "Ravi Kiran Chirravuri",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d218",
          "name": "Ravali Kandur",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d219",
          "name": "Alejandro Pelaez",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d21a",
          "name": "Akash Garg",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d21b",
          "name": "Michael Palleschi",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d21c",
          "name": "Jessica Wang",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d21d",
          "name": "Skylar Litz",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d21e",
          "name": "Leon Liu",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d21f",
          "name": "Anying Li",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d220",
          "name": "David Harmon",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d221",
          "name": "Derek Liu",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d222",
          "name": "Liangjun Feng",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d223",
          "name": "Denis Goupil",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d224",
          "name": "Lukas Kuczynski",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d225",
          "name": "Jihyun Yoon",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d226",
          "name": "Naveen Marri",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d227",
          "name": "Peiye Zhuang",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d228",
          "name": "Yinan Zhang",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d229",
          "name": "Brian Yin",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d22a",
          "name": "Haomiao Jiang",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d22b",
          "name": "Marcel van Workum",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d22c",
          "name": "Thomas Lane",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d22d",
          "name": "Bryce Erickson",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d22e",
          "name": "Salil Pathare",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d22f",
          "name": "Kyle Price",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d230",
          "name": "Anupam Singh",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d231",
          "name": "David Baszucki",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-19T17:52:17.000Z",
      "submittedOnDailyAt": "2025-03-20T00:57:52.833Z",
      "title": "Cube: Intelligence 3D en Roboloco",
      "submittedOnDailyBy": {
        "_id": "62cd5c43299c0c2e0e437842",
        "avatarUrl": "/avatars/8fa8511baf2d9bd95d3ba4535a5b3d69.svg",
        "isPro": false,
        "fullname": "Jean-Philippe Fauconnier",
        "user": "j4kn",
        "type": "user"
      },
      "summary": "Les modèles de base entraînés avec une grande quantité de données montrent une excellence et une capacité de génération qui surprennent dans le domaine des phrases, des images, de la voix et des vidéos. Notre objectif à Robolox est de construire des modèles de base basés sur de l'information 3D. Ce modèle se concentre sur aider les développeurs à créer et animer des objets 3D ou des espaces, ainsi que à écrire des scripts de programmation qui décrivent le réseau des cordes des personnages ou les actions des objets. On discute les trois principales exigences de conception de ces modèles de base 3D et on présente le premier pas dans leur construction. On espère que la géométrie 3D soit un type de données clé et on explique la solution du tokenisateur de formes 3D. Notre scénario de tokenisateur montre comment il peut être utilisé dans des applications telles que la génération de texte en forme, de forme en texte, et la génération d'espace à partir de texte, entre autres. Ces applications montrent comment les modèles de langage à grande échelle (LLMs) se intègrent avec l'interprétation spatiale et la raison. Enfin, on discute le chemin pour construire un modèle de base complet basé sur de l'information 3D.",
      "upvotes": 16,
      "discussionId": "67db72a1a720e711cff4d292",
      "githubRepo": "https://github.com/Roblox/cube",
      "ai_keywords": [
        "3D foundation model",
        "3D geometric shapes",
        "3D shape tokenizer",
        "text-to-shape generation",
        "shape-to-text generation",
        "text-to-scene generation",
        "large language models (LLMs)",
        "scene analysis",
        "reasoning",
        "unified foundation model"
      ]
    },
    "publishedAt": "2025-03-19T13:52:17.000Z",
    "title": "Cube: A Roblox View of 3D Intelligence",
    "summary": "Foundation models trained on vast amounts of data have demonstrated\nremarkable reasoning and generation capabilities in the domains of text,\nimages, audio and video. Our goal at Roblox is to build such a foundation model\nfor 3D intelligence, a model that can support developers in producing all\naspects of a Roblox experience, from generating 3D objects and scenes to\nrigging characters for animation to producing programmatic scripts describing\nobject behaviors. We discuss three key design requirements for such a 3D\nfoundation model and then present our first step towards building such a model.\nWe expect that 3D geometric shapes will be a core data type and describe our\nsolution for 3D shape tokenizer. We show how our tokenization scheme can be\nused in applications for text-to-shape generation, shape-to-text generation and\ntext-to-scene generation. We demonstrate how these applications can collaborate\nwith existing large language models (LLMs) to perform scene analysis and\nreasoning. We conclude with a discussion outlining our path to building a fully\nunified foundation model for 3D intelligence.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.15475.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62cd5c43299c0c2e0e437842",
      "avatarUrl": "/avatars/8fa8511baf2d9bd95d3ba4535a5b3d69.svg",
      "fullname": "Jean-Philippe Fauconnier",
      "name": "j4kn",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.14868",
      "authors": [
        {
          "_id": "67db9f06842d8b6642a5eeaf",
          "name": "Hoigi Seo",
          "hidden": false
        },
        {
          "_id": "67db9f06842d8b6642a5eeb0",
          "name": "Wongi Jeong",
          "hidden": false
        },
        {
          "_id": "67db9f06842d8b6642a5eeb1",
          "name": "Kyungryeol Lee",
          "hidden": false
        },
        {
          "_id": "67db9f06842d8b6642a5eeb2",
          "name": "Se Young Chun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-19T03:45:37.000Z",
      "submittedOnDailyAt": "2025-03-20T03:25:51.779Z",
      "title": "Modèle de diffuseur réduit sans application efficace de diffusion de faible proportion personnalisée",
      "submittedOnDailyBy": {
        "_id": "633e6f07309a99325095dd42",
        "avatarUrl": "/avatars/57b91a488ac1745b3c0509c04eb6ad93.svg",
        "isPro": false,
        "fullname": "Hoigi Seo",
        "user": "Agorium",
        "type": "user"
      },
      "summary": "Le module de dépyrivation a montré un rendement impressionnant dans la synthèse d'images, mais nécessite une grande quantité de calculs et de ressources de mémoire pour son entraînement, ajustement et inférence. La technologie de cuvantization développée a pu minimiser l'utilisation de mémoire pendant l'inférence, mais l'entraînement et l'ajustement de ces modules de cuvantization a requis un grand consommateur de mémoire en raison de calculs précis et de la propagation du gradient basée sur les gradients. Cependant, un ajustement efficace en mémoire est particulièrement adapté aux applications qui traitent des données personnelles sur des appareils mobiles ou d'autres dispositifs de bord. Dans cet article, la technique de dépyrivation est appliquée à un module personnalisé grâce à l'insertion de texte, et le token de zéro est optimisé pour personnaliser le module de dépyrivation de manière à ce qu'il n'ait pas besoin de mémoire pour le gradient et l'activation basée sur la propagation du gradient. L'estimation du gradient par l'optimisation des zéros élimine le bruit dans la plupart des cas, où le gradient est projeté dans un espace de dimensions basé sur l'historique des tokens passés et est appelé \"Subspace Gradient\". Ce méthode est utilisée pour l'échantillonnage de pas de temps efficaces de dépyrivation appelés \"Partial Uniform Timestep Sampling\", ce qui permet de comparer la concordance entre des images et du texte uniquement en passant vers l'avenir, améliorant l'efficacité de la mémoire pendant l'entraînement d'un facteur de 8.2.",
      "upvotes": 15,
      "discussionId": "67db9f11842d8b6642a5f165",
      "projectPage": "https://ignoww.github.io/ZOODiP_project/",
      "githubRepo": "https://github.com/ignoww/ZOODiP",
      "ai_keywords": [
        "diffusion models",
        "image synthesis",
        "quantization techniques",
        "dequantization",
        "gradient-based algorithms",
        "memory-efficient fine-tuning",
        "Textual Inversion",
        "zeroth-order optimization",
        "personalization tokens",
        "gradient estimation",
        "Subspace Gradient",
        "subspace projection",
        "text embedding",
        "Partial Uniform Timestep Sampling",
        "diffusion timesteps",
        "Stable Diffusion",
        "image and text alignment scores"
      ]
    },
    "publishedAt": "2025-03-18T23:45:37.000Z",
    "title": "Efficient Personalization of Quantized Diffusion Model without\n  Backpropagation",
    "summary": "Diffusion models have shown remarkable performance in image synthesis, but\nthey demand extensive computational and memory resources for training,\nfine-tuning and inference. Although advanced quantization techniques have\nsuccessfully minimized memory usage for inference, training and fine-tuning\nthese quantized models still require large memory possibly due to\ndequantization for accurate computation of gradients and/or backpropagation for\ngradient-based algorithms. However, memory-efficient fine-tuning is\nparticularly desirable for applications such as personalization that often must\nbe run on edge devices like mobile phones with private data. In this work, we\naddress this challenge by quantizing a diffusion model with personalization via\nTextual Inversion and by leveraging a zeroth-order optimization on\npersonalization tokens without dequantization so that it does not require\ngradient and activation storage for backpropagation that consumes considerable\nmemory. Since a gradient estimation using zeroth-order optimization is quite\nnoisy for a single or a few images in personalization, we propose to denoise\nthe estimated gradient by projecting it onto a subspace that is constructed\nwith the past history of the tokens, dubbed Subspace Gradient. In addition, we\ninvestigated the influence of text embedding in image generation, leading to\nour proposed time steps sampling, dubbed Partial Uniform Timestep Sampling for\nsampling with effective diffusion timesteps. Our method achieves comparable\nperformance to prior methods in image and text alignment scores for\npersonalizing Stable Diffusion with only forward passes while reducing training\nmemory demand up to 8.2times.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14868.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "633e6f07309a99325095dd42",
      "avatarUrl": "/avatars/57b91a488ac1745b3c0509c04eb6ad93.svg",
      "fullname": "Hoigi Seo",
      "name": "Agorium",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.15417",
      "authors": [
        {
          "_id": "67db8e05842d8b6642a135d0",
          "name": "Harold Haodong Chen",
          "hidden": false
        },
        {
          "_id": "67db8e05842d8b6642a135d1",
          "name": "Haojian Huang",
          "hidden": false
        },
        {
          "_id": "67db8e05842d8b6642a135d2",
          "name": "Xianfeng Wu",
          "hidden": false
        },
        {
          "_id": "67db8e05842d8b6642a135d3",
          "name": "Yexin Liu",
          "hidden": false
        },
        {
          "_id": "67db8e05842d8b6642a135d4",
          "name": "Yajing Bai",
          "hidden": false
        },
        {
          "_id": "67db8e05842d8b6642a135d5",
          "name": "Wen-Jie Shu",
          "hidden": false
        },
        {
          "_id": "67db8e05842d8b6642a135d6",
          "name": "Harry Yang",
          "hidden": false
        },
        {
          "_id": "67db8e05842d8b6642a135d7",
          "name": "Ser-Nam Lim",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-19T16:59:32.000Z",
      "submittedOnDailyAt": "2025-03-20T02:10:52.068Z",
      "title": "La normalisation du temps renforce significativement le générateur de vidéos.",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Le temps est un aspect important dans la génération de vidéos, garantissant des mouvements cohérents entre les frames et des actions réalistes. Cependant, atteindre une haute cohérence temporelle et une diversité est difficile. Dans cet article, nous explorons pour la première fois le rôle du temps dans la génération de vidéos et présentons FluxFlow. FluxFlow est une stratégie pour améliorer le temps, conçue pour manipuler les données au niveau des données et éviter des changements dans l'architecture, ce qui n'est pas nécessaire. Les expériences prolongées sur UCF-101 et VBench montrent que FluxFlow améliore significativement la cohérence temporelle et la diversité, tout en maintenant la dépendance spatiale, tant pour U-Net, DiT, que pour des structures basées sur AR. Ces résultats démontrent la possibilité que le temps puisse être un méthode simple et efficace pour améliorer la qualité de la génération de vidéos.",
      "upvotes": 12,
      "discussionId": "67db8e07842d8b6642a1365f",
      "ai_keywords": [
        "temporal augmentation",
        "FluxFlow",
        "temporal perturbations",
        "temporal quality",
        "temporal coherence",
        "UCF-101",
        "VBench",
        "U-Net",
        "DiT",
        "AR-based architectures",
        "spatial fidelity"
      ]
    },
    "publishedAt": "2025-03-19T12:59:32.000Z",
    "title": "Temporal Regularization Makes Your Video Generator Stronger",
    "summary": "Temporal quality is a critical aspect of video generation, as it ensures\nconsistent motion and realistic dynamics across frames. However, achieving high\ntemporal coherence and diversity remains challenging. In this work, we explore\ntemporal augmentation in video generation for the first time, and introduce\nFluxFlow for initial investigation, a strategy designed to enhance temporal\nquality. Operating at the data level, FluxFlow applies controlled temporal\nperturbations without requiring architectural modifications. Extensive\nexperiments on UCF-101 and VBench benchmarks demonstrate that FluxFlow\nsignificantly improves temporal coherence and diversity across various video\ngeneration models, including U-Net, DiT, and AR-based architectures, while\npreserving spatial fidelity. These findings highlight the potential of temporal\naugmentation as a simple yet effective approach to advancing video generation\nquality.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.15417.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6408
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.12532",
      "authors": [
        {
          "_id": "67da1df040371958e1732c83",
          "name": "Fanbin Lu",
          "hidden": false
        },
        {
          "_id": "67da1df040371958e1732c84",
          "name": "Zhisheng Zhong",
          "hidden": false
        },
        {
          "_id": "67da1df040371958e1732c85",
          "name": "Ziqin Wei",
          "hidden": false
        },
        {
          "_id": "67da1df040371958e1732c86",
          "name": "Shu Liu",
          "hidden": false
        },
        {
          "_id": "67da1df040371958e1732c87",
          "name": "Chi-Wing Fu",
          "hidden": false
        },
        {
          "_id": "67da1df040371958e1732c88",
          "name": "Jiaya Jia",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-16T14:53:43.000Z",
      "submittedOnDailyAt": "2025-03-20T01:38:29.350Z",
      "title": "STEVE: Processus de certification en phases pour l'entraînement des agents informatiques",
      "submittedOnDailyBy": {
        "_id": "6418554a0956be7233a1023e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6418554a0956be7233a1023e/9EKN0GoOpcDbvBDmAQEJf.png",
        "isPro": false,
        "fullname": "zhang yuechen",
        "user": "julianjuaner",
        "type": "user"
      },
      "summary": "L'automatique de gestion des interfaces graphiques par des agents intelligents est une tâche complexe qui nécessite un temps considérable. L'avancement récent dans l'échelle des données a favorisé l'entraînement d'agents de type utilisateur en utilisant des ensembles d'instructions élargies, mais pour les entraîner par clonage d'actions, une grande quantité de trafic de haute qualité est nécessaire. Pour répondre à ces besoins, nous avons conçu STEVE, un méthode d'entraînement d'agents de type utilisateur. Tout d'abord, nous établissons un ensemble d'instructions élargies pour un agent de type utilisateur et nous collectons des données de trafic en utilisant des agents sous-optimaux. GPT-4o vérifie la précision de chaque étape du trafic en se basant sur des écrans d'exécution et assigne des étiquettes binaires à chaque étape. Enfin, nous utilisons l'optimisation de Kahneman et Tversky pour optimiser les agents en fonction d'étiquettes d'étapes binaires. Les expériences prolongées ont montré que nos agents sont supérieurs à ceux entraînés de manière normale, en utilisant autant des actions positives que négatives dans le trafic. De plus, STEVE offre une grande efficacité et une réduction des coûts, ce qui permet d'entraîner un modèle de langage visuel de 7B en agents de type utilisateur et d'atteindre un rendement de pointe dans des environnements difficiles comme WinAgentArena. Le code et les données sont disponibles sur https://github.com/FanbinLu/STEVE.",
      "upvotes": 8,
      "discussionId": "67da1df240371958e1732d2f",
      "githubRepo": "https://github.com/FanbinLu/STEVE",
      "ai_keywords": [
        "behavior cloning",
        "trajectory data",
        "suboptimal agents",
        "GPT-4o",
        "step verification pipeline",
        "correctness verification",
        "Kahneman and Tversky Optimization",
        "positive actions",
        "negative actions",
        "vision-language model",
        "computer-use agent",
        "live desktop environment",
        "WinAgentArena"
      ]
    },
    "publishedAt": "2025-03-16T10:53:43.000Z",
    "title": "STEVE: AStep Verification Pipeline for Computer-use Agent Training",
    "summary": "Developing AI agents to autonomously manipulate graphical user interfaces is\na long challenging task. Recent advances in data scaling law inspire us to\ntrain computer-use agents with a scaled instruction set, yet using behavior\ncloning to train agents still requires immense high-quality trajectories. To\nmeet the scalability need, we designed STEVE, a step verification pipeline for\ncomputer-use agent training. First, we establish a large instruction set for\ncomputer-use agents and collect trajectory data with some suboptimal agents.\nGPT-4o is used to verify the correctness of each step in the trajectories based\non the screens before and after the action execution, assigning each step with\na binary label. Last, we adopt the Kahneman and Tversky Optimization to\noptimize the agent from the binary stepwise labels. Extensive experiments\nmanifest that our agent outperforms supervised finetuning by leveraging both\npositive and negative actions within a trajectory. Also, STEVE enables us to\ntrain a 7B vision-language model as a computer-use agent, achieving leading\nperformance in the challenging live desktop environment WinAgentArena with\ngreat efficiency at a reduced cost. Code and data:\nhttps://github.com/FanbinLu/STEVE.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12532.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6418554a0956be7233a1023e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6418554a0956be7233a1023e/9EKN0GoOpcDbvBDmAQEJf.png",
      "fullname": "zhang yuechen",
      "name": "julianjuaner",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.15264",
      "authors": [
        {
          "_id": "67dbbe8fafd5251fc6b55730",
          "name": "Hengrui Kang",
          "hidden": false
        },
        {
          "_id": "67dbbe8fafd5251fc6b55731",
          "name": "Siwei Wen",
          "hidden": false
        },
        {
          "_id": "67dbbe8fafd5251fc6b55732",
          "name": "Zichen Wen",
          "hidden": false
        },
        {
          "_id": "67dbbe8fafd5251fc6b55733",
          "name": "Junyan Ye",
          "hidden": false
        },
        {
          "_id": "67dbbe8fafd5251fc6b55734",
          "name": "Weijia Li",
          "hidden": false
        },
        {
          "_id": "67dbbe8fafd5251fc6b55735",
          "name": "Peilin Feng",
          "hidden": false
        },
        {
          "_id": "67dbbe8fafd5251fc6b55736",
          "name": "Baichuan Zhou",
          "hidden": false
        },
        {
          "_id": "67dbbe8fafd5251fc6b55737",
          "name": "Bin Wang",
          "hidden": false
        },
        {
          "_id": "67dbbe8fafd5251fc6b55738",
          "name": "Dahua Lin",
          "hidden": false
        },
        {
          "_id": "67dbbe8fafd5251fc6b55739",
          "name": "Linfeng Zhang",
          "hidden": false
        },
        {
          "_id": "67dbbe8fafd5251fc6b5573a",
          "name": "Conghui He",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-19T14:37:21.000Z",
      "submittedOnDailyAt": "2025-03-20T05:54:43.430Z",
      "title": "Région : Apprentissage, fondements et explication pour la détection d'images synthétiques",
      "submittedOnDailyBy": {
        "_id": "653b8c3e97a4d71d950e2f20",
        "avatarUrl": "/avatars/b68880022e14556d0be58c69615db3be.svg",
        "isPro": false,
        "fullname": "Zichen Wen",
        "user": "zichenwen",
        "type": "user"
      },
      "summary": "Le développement rapide des technologies génératives montre une dualité. Elle offre des outils conviviaux et puissants, mais également porte des préoccupations sociales significatives. Les méthodes actuelles de détection d'images synthétiques souvent manquent de possibilité d'interprétation textuelle. De plus, elles se concentrent excessivement sur la détection de manipulations d'images, et rencontrent des difficultés en raison de générateurs non mis à jour et d'une manque d'annotations détaillées. Dans cet article, nous présentons un jeu de données de haute qualité et divers, comprenant 12 236 images synthétiques complètes. Cela caractérise les 4 types de contenu d'images, les 3 catégories d'effets, les annotations au niveau de pixel, une interprétation textuelle détaillée et des annotations spécifiques des catégories d'effets. De plus, nous proposons un cadre d'analyse d'images fausses basé sur un grand modèle de langage multimodal (MLLM) appelé LEGION (Apprendre à fonder et expliquer pour la détection d'images synthétiques), qui intègre la détection, la segmentation et l'interprétation des effets. Avec cette capacité, nous étendons LEGION en tant que contrôleur pour le faire intégrer dans la chaîne de production d'images pour guider la génération d'images de haute qualité et plus réalistes. Les expériences étendues montrent que LEGION dépasse les méthodes actuelles dans plusieurs cadres de référence, et dépasse les meilleurs experts en SynthScars avec un mIoU de 3,31% et un score F1 de 7,75%. De plus, les images sélectionnées générées sous la direction de LEGION montrent un accord plus élevé avec la préférence humaine. Le code, le modèle et le jeu de données sont libérés.",
      "upvotes": 6,
      "discussionId": "67dbbe92afd5251fc6b55825",
      "projectPage": "https://opendatalab.github.io/LEGION",
      "githubRepo": "https://github.com/opendatalab/LEGION",
      "ai_keywords": [
        "SynthScars",
        "LEGION",
        "multimodal large language model",
        "image forgery analysis framework",
        "artifact detection",
        "segmentation",
        "explanation",
        "mIoU",
        "F1 score"
      ]
    },
    "publishedAt": "2025-03-19T10:37:21.000Z",
    "title": "LEGION: Learning to Ground and Explain for Synthetic Image Detection",
    "summary": "The rapid advancements in generative technology have emerged as a\ndouble-edged sword. While offering powerful tools that enhance convenience,\nthey also pose significant social concerns. As defenders, current synthetic\nimage detection methods often lack artifact-level textual interpretability and\nare overly focused on image manipulation detection, and current datasets\nusually suffer from outdated generators and a lack of fine-grained annotations.\nIn this paper, we introduce SynthScars, a high-quality and diverse dataset\nconsisting of 12,236 fully synthetic images with human-expert annotations. It\nfeatures 4 distinct image content types, 3 categories of artifacts, and\nfine-grained annotations covering pixel-level segmentation, detailed textual\nexplanations, and artifact category labels. Furthermore, we propose LEGION\n(LEarning to Ground and explain for Synthetic Image detectiON), a multimodal\nlarge language model (MLLM)-based image forgery analysis framework that\nintegrates artifact detection, segmentation, and explanation. Building upon\nthis capability, we further explore LEGION as a controller, integrating it into\nimage refinement pipelines to guide the generation of higher-quality and more\nrealistic images. Extensive experiments show that LEGION outperforms existing\nmethods across multiple benchmarks, particularly surpassing the second-best\ntraditional expert on SynthScars by 3.31% in mIoU and 7.75% in F1 score.\nMoreover, the refined images generated under its guidance exhibit stronger\nalignment with human preferences. The code, model, and dataset will be\nreleased.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.15264.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "653b8c3e97a4d71d950e2f20",
      "avatarUrl": "/avatars/b68880022e14556d0be58c69615db3be.svg",
      "fullname": "Zichen Wen",
      "name": "zichenwen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.14505",
      "authors": [
        {
          "_id": "67db13f71956dcedf0b4d357",
          "name": "Susung Hong",
          "hidden": false
        },
        {
          "_id": "67db13f71956dcedf0b4d358",
          "name": "Ira Kemelmacher-Shlizerman",
          "hidden": false
        },
        {
          "_id": "67db13f71956dcedf0b4d359",
          "name": "Brian Curless",
          "hidden": false
        },
        {
          "_id": "67db13f71956dcedf0b4d35a",
          "name": "Steven M. Seitz",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-18T17:59:58.000Z",
      "submittedOnDailyAt": "2025-03-20T02:39:46.392Z",
      "title": "MusicInfuser : Méthode pour élargir l'audience et danser dans les films qui incluent de la musique",
      "submittedOnDailyBy": {
        "_id": "635a6dd21668c4ead3ed19fa",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1679159537671-635a6dd21668c4ead3ed19fa.jpeg",
        "isPro": false,
        "fullname": "Susung Hong",
        "user": "susunghong",
        "type": "user"
      },
      "summary": "MusicInfuser vise à générer des vidéos de danse de haute qualité synchronisées avec des œuvres musicales spécifiques. Alors que l'on essaye de concevoir et d'entraîner un nouveau modèle de vidéo multimodal, il propose un approche pour que les modèles de diffusion de vidéo s'adaptent aux entrées musicales. En introduisant critiquement l'attention croisée et un adaptateur de faible performance, MusicInfuser réussit à générer des vidéos de danse de haute qualité sans nécessiter de données de capture de danse, ce qui est différent de ce qui a été précédemment étudié. Grâce à la finaison des vidéos de danse, MusicInfuser effectue la génération de vidéos de haute qualité dirigée par la musique, tout en maintenant la flexibilité et la capacité de génération du modèle de base. Un cadre d'évaluation est introduit en utilisant les Video-LLMs pour évaluer la qualité de la génération de danse depuis plusieurs perspectives. La page du projet et le code sont disponibles sur https://susunghong.github.io/MusicInfuser.",
      "upvotes": 5,
      "discussionId": "67db13fc1956dcedf0b4d470",
      "ai_keywords": [
        "video diffusion models",
        "multimodal audio-video model",
        "music-video cross-attention",
        "low-rank adapter",
        "dance videos",
        "motion capture data",
        "music-driven video generation",
        "Video-LLMs"
      ]
    },
    "publishedAt": "2025-03-18T13:59:58.000Z",
    "title": "MusicInfuser: Making Video Diffusion Listen and Dance",
    "summary": "We introduce MusicInfuser, an approach for generating high-quality dance\nvideos that are synchronized to a specified music track. Rather than attempting\nto design and train a new multimodal audio-video model, we show how existing\nvideo diffusion models can be adapted to align with musical inputs by\nintroducing lightweight music-video cross-attention and a low-rank adapter.\nUnlike prior work requiring motion capture data, our approach fine-tunes only\non dance videos. MusicInfuser achieves high-quality music-driven video\ngeneration while preserving the flexibility and generative capabilities of the\nunderlying models. We introduce an evaluation framework using Video-LLMs to\nassess multiple dimensions of dance generation quality. The project page and\ncode are available at https://susunghong.github.io/MusicInfuser.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14505.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "635a6dd21668c4ead3ed19fa",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1679159537671-635a6dd21668c4ead3ed19fa.jpeg",
      "fullname": "Susung Hong",
      "name": "susunghong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.12769",
      "authors": [
        {
          "_id": "67d8ded81a1b6ae91f79eb18",
          "name": "Shenghao Fu",
          "hidden": false
        },
        {
          "_id": "67d8ded81a1b6ae91f79eb19",
          "name": "Qize Yang",
          "hidden": false
        },
        {
          "_id": "67d8ded81a1b6ae91f79eb1a",
          "name": "Yuan-Ming Li",
          "hidden": false
        },
        {
          "_id": "67d8ded81a1b6ae91f79eb1b",
          "name": "Yi-Xing Peng",
          "hidden": false
        },
        {
          "_id": "67d8ded81a1b6ae91f79eb1c",
          "name": "Kun-Yu Lin",
          "hidden": false
        },
        {
          "_id": "67d8ded81a1b6ae91f79eb1d",
          "name": "Xihan Wei",
          "hidden": false
        },
        {
          "_id": "67d8ded81a1b6ae91f79eb1e",
          "name": "Jian-Fang Hu",
          "hidden": false
        },
        {
          "_id": "67d8ded81a1b6ae91f79eb1f",
          "name": "Xiaohua Xie",
          "hidden": false
        },
        {
          "_id": "67d8ded81a1b6ae91f79eb20",
          "name": "Wei-Shi Zheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-17T03:05:31.000Z",
      "submittedOnDailyAt": "2025-03-20T00:48:47.112Z",
      "title": "ViSpeak : Visualisation de Réactions Instinctives dans les Films",
      "submittedOnDailyBy": {
        "_id": "6686044047f2a33570e59e31",
        "avatarUrl": "/avatars/2656bf2cecd6d7cbffd0a912a54d25de.svg",
        "isPro": false,
        "fullname": "Jiaxing Zhao",
        "user": "StarJiaxing",
        "type": "user"
      },
      "summary": "Le développement récent des grands modèles de vidéo multimodal (LMMs) se concentre principalement sur la compréhension de vidéos en ligne. D'autre part, la compréhension de vidéos en streaming présente de grands défis en raison des caractéristiques temporelles, multimodalité et interactivité. Dans cette étude, nous proposons d'étendre la vision de la compréhension de vidéos en streaming et proposons un nouveau défi appelé \"Visual Instruction Feedback\", qui se concentre sur l'entraînement des modèles à reconnaître du contenu visuel et à extraire des instructions. Par exemple, lorsqu'un utilisateur bouge la main dans un environnement extérieur, l'environnement extérieur doit identifier le mouvement de la main pour initier une conversation. De cette manière, l'extraction d'instructions dans la modalité visuelle peut significativement améliorer l'interaction entre l'environnement extérieur et l'utilisateur. Pour encourager cette recherche, nous définissons sept sous-tâches principaux liés à la modalité visuelle et nous collectons un jeu de données d'entraînement ViSpeak-Instruct et un benchmark d'évaluation ViSpeak-Bench. De plus, nous proposons le modèle ViSpeak, un LMM de compréhension de vidéos en streaming à niveau GPT-4o, qui peut être utilisé pour entraîner les capacités de base de feedback visuel de l'instruction et jouera un rôle crucial comme base pour futures recherches.",
      "upvotes": 4,
      "discussionId": "67d8ded91a1b6ae91f79eb5c",
      "ai_keywords": [
        "Large Multi-modal Models (LMMs)",
        "streaming video understanding",
        "Visual Instruction Feedback",
        "visual contents",
        "instructions",
        "gesture recognition",
        "user-agent interactions",
        "subtasks",
        "ViSpeak-Instruct dataset",
        "ViSpeak-Bench",
        "ViSpeak model",
        "GPT-4o-level performance",
        "streaming video understanding benchmarks",
        "finetuning"
      ]
    },
    "publishedAt": "2025-03-16T23:05:31.000Z",
    "title": "ViSpeak: Visual Instruction Feedback in Streaming Videos",
    "summary": "Recent advances in Large Multi-modal Models (LMMs) are primarily focused on\noffline video understanding. Instead, streaming video understanding poses great\nchallenges to recent models due to its time-sensitive, omni-modal and\ninteractive characteristics. In this work, we aim to extend the streaming video\nunderstanding from a new perspective and propose a novel task named Visual\nInstruction Feedback in which models should be aware of visual contents and\nlearn to extract instructions from them. For example, when users wave their\nhands to agents, agents should recognize the gesture and start conversations\nwith welcome information. Thus, following instructions in visual modality\ngreatly enhances user-agent interactions. To facilitate research, we define\nseven key subtasks highly relevant to visual modality and collect the\nViSpeak-Instruct dataset for training and the ViSpeak-Bench for evaluation.\nFurther, we propose the ViSpeak model, which is a SOTA streaming video\nunderstanding LMM with GPT-4o-level performance on various streaming video\nunderstanding benchmarks. After finetuning on our ViSpeak-Instruct dataset,\nViSpeak is equipped with basic visual instruction feedback ability, serving as\na solid baseline for future research.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12769.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6686044047f2a33570e59e31",
      "avatarUrl": "/avatars/2656bf2cecd6d7cbffd0a912a54d25de.svg",
      "fullname": "Jiaxing Zhao",
      "name": "StarJiaxing",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 13
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.11227",
      "authors": [
        {
          "_id": "67da533bb443470b7908a048",
          "user": {
            "_id": "658be7fe135580745c510323",
            "avatarUrl": "/avatars/830e5cec4565efdc23226a86a0fcef0e.svg",
            "isPro": false,
            "fullname": "Jian Zhang",
            "user": "VentureZJ",
            "type": "user"
          },
          "name": "Jian Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-19T09:44:13.546Z",
          "hidden": false
        },
        {
          "_id": "67da533bb443470b7908a049",
          "name": "Bifan Wei",
          "hidden": false
        },
        {
          "_id": "67da533bb443470b7908a04a",
          "name": "Shihao Qi",
          "hidden": false
        },
        {
          "_id": "67da533bb443470b7908a04b",
          "name": "haiping Zhu",
          "hidden": false
        },
        {
          "_id": "67da533bb443470b7908a04c",
          "name": "Jun Liu",
          "hidden": false
        },
        {
          "_id": "67da533bb443470b7908a04d",
          "name": "Qika Lin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-14T09:23:22.000Z",
      "submittedOnDailyAt": "2025-03-20T06:15:24.085Z",
      "title": "GKG-LLM: Marco de Frame de Construcción de Grafos de Conocimiento Generalizados",
      "submittedOnDailyBy": {
        "_id": "658be7fe135580745c510323",
        "avatarUrl": "/avatars/830e5cec4565efdc23226a86a0fcef0e.svg",
        "isPro": false,
        "fullname": "Jian Zhang",
        "user": "VentureZJ",
        "type": "user"
      },
      "summary": "La construction de graphes de connaissance généralisées (GKG) comprend trois types : graphes de connaissance, graphes de connaissance d'événements et graphes de connaissance de savoir, et joue un rôle fondamental dans diverses tâches de traitement du langage naturel. Actuellement, la recherche construit chacun de ces graphes de manière indépendante, en ignorant la possibilité d'une intégration globale. Cependant, les défis dans le développement d'un cadre unifié pour le GKG sont entravés par les différences propres à chaque tâche. Dans cette étude, nous proposons un cadre unifié approprié pour la construction du GKG pour résoudre ces défis. Tout d'abord, nous récoltons des données de 15 sous-tâches à partir de 29 ensembles de données correspondant aux trois types de graphes, classifiant-les comme données d'échantillon, tâches contraires et données hors distribution (OOD). Ensuite, nous proposons un cadre d'ajustement micro pour un apprentissage de clustering en trois étapes, en injectant répétitivement la capture des trois types de graphes dans des modèles grands de langage. À travers de nombreux expérimentations, notre modèle proposé a montré des améliorations dans la construction de tous les trois types de graphes, tant pour les données de domaine, OOD que pour les tâches contraires.",
      "upvotes": 3,
      "discussionId": "67da533db443470b7908a0e6",
      "ai_keywords": [
        "knowledge graph",
        "event knowledge graph",
        "commonsense knowledge graph",
        "natural language processing",
        "unified framework",
        "in-sample data",
        "counter-task data",
        "out-of-distribution data",
        "three-stage curriculum learning fine-tuning framework",
        "Large Language Models"
      ]
    },
    "publishedAt": "2025-03-14T05:23:22.000Z",
    "title": "GKG-LLM: A Unified Framework for Generalized Knowledge Graph\n  Construction",
    "summary": "The construction of Generalized Knowledge Graph (GKG), including knowledge\ngraph, event knowledge graph and commonsense knowledge graph, is fundamental\nfor various natural language processing tasks. Current studies typically\nconstruct these types of graph separately, overlooking holistic insights and\npotential unification that could be beneficial in computing resources and usage\nperspectives. However, a key challenge in developing a unified framework for\nGKG is obstacles arising from task-specific differences. In this study, we\npropose a unified framework for constructing generalized knowledge graphs to\naddress this challenge. First, we collect data from 15 sub-tasks in 29 datasets\nacross the three types of graphs, categorizing them into in-sample,\ncounter-task, and out-of-distribution (OOD) data. Then, we propose a\nthree-stage curriculum learning fine-tuning framework, by iteratively injecting\nknowledge from the three types of graphs into the Large Language Models.\nExtensive experiments show that our proposed model improves the construction of\nall three graph types across in-domain, OOD and counter-task data.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.11227.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "658be7fe135580745c510323",
      "avatarUrl": "/avatars/830e5cec4565efdc23226a86a0fcef0e.svg",
      "fullname": "Jian Zhang",
      "name": "VentureZJ",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.13360",
      "authors": [
        {
          "_id": "67d8e21dea26d6d743f2adde",
          "name": "Hai-Long Sun",
          "hidden": false
        },
        {
          "_id": "67d8e21dea26d6d743f2addf",
          "name": "Zhun Sun",
          "hidden": false
        },
        {
          "_id": "67d8e21dea26d6d743f2ade0",
          "name": "Houwen Peng",
          "hidden": false
        },
        {
          "_id": "67d8e21dea26d6d743f2ade1",
          "name": "Han-Jia Ye",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-17T16:45:12.000Z",
      "submittedOnDailyAt": "2025-03-20T04:52:23.426Z",
      "title": "Utilisant le Conditionnement Visuel pour le Logique de Contexte à Long Terme Multi-Type",
      "submittedOnDailyBy": {
        "_id": "6623975c728f756224d4b768",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6623975c728f756224d4b768/US9n0k45Y1TF6WqOhgBhZ.jpeg",
        "isPro": false,
        "fullname": "Allen Sun",
        "user": "Allen8",
        "type": "user"
      },
      "summary": "Récemment, le développement de modèles de langage grands (LLMs) a évolué depuis l'apprentissage par exemple de la chaîne de pensée (CoT) jusqu'à des solutions avancées telles que OpenAI ou 1. Lors du processus de ré-implémentation de ces modèles, il a été identifié que les modèles multimodales de langage grands (MLLMs) tendent à se concentrer moins sur l'information visuelle dans les tâches multimodales nécessitant des entrées visuelles (par exemple, des problèmes géométriques). Cela signifie que, au fur et à mesure que le processus logique progresse, les MLLMs diminuent leur attention à l'information visuelle et deviennent trop dépendants du texte, ce qui peut affecter la précision de leurs réponses.\n\nPour étudier ce phénomène, des tests ont été réalisés où le processus logique a été interrompu et les entrées visuelles ont été supprimées pour vérifier leur impact. Spécifiquement, le processus logique a été interrompu et les entrées visuelles ont été supprimées, puis les processus logiques ont été réinitialisés avec seulement du texte. Dans la collection de tests durs de MathVista, cette action a entraîné une réduction d'environ 2% en précision, mais clairement a montré que le texte domine la production logique et influence la précision des réponses.\n\nPar conséquent, une stratégie de conditionnement visuel accompagné (TVC) est proposée. Dans cette stratégie, l'entrée visuelle est déplacée dans des étapes logiques importantes et les tokens visuels excessifs sont réduits par production dynamique. Cette stratégie aide à maintenir l'attention sur les éléments visuels pendant le processus logique. Notre approche a enregistré le meilleur rendement moyen sur 5 marqueurs de logique mathématique, démontrant que la TVC est efficace pour renforcer les systèmes multimodales de logique.",
      "upvotes": 1,
      "discussionId": "67d8e21eea26d6d743f2ae50",
      "ai_keywords": [
        "Chain-of-Thought (CoT)",
        "OpenAI o1",
        "Multimodal LLMs (MLLMs)",
        "attention to visual information",
        "text-over-relied outputs",
        "ablate image inputs",
        "long-chain reasoning",
        "MathVista's test-hard subset",
        "Take-along Visual Conditioning (TVC)",
        "critical reasoning stages",
        "dynamic pruning",
        "multimodal reasoning systems"
      ]
    },
    "publishedAt": "2025-03-17T12:45:12.000Z",
    "title": "Mitigating Visual Forgetting via Take-along Visual Conditioning for\n  Multi-modal Long CoT Reasoning",
    "summary": "Recent advancements in Large Language Models (LLMs) have demonstrated\nenhanced reasoning capabilities, evolving from Chain-of-Thought (CoT) prompting\nto advanced, product-oriented solutions like OpenAI o1. During our\nre-implementation of this model, we noticed that in multimodal tasks requiring\nvisual input (e.g., geometry problems), Multimodal LLMs (MLLMs) struggle to\nmaintain focus on the visual information, in other words, MLLMs suffer from a\ngradual decline in attention to visual information as reasoning progresses,\ncausing text-over-relied outputs. To investigate this, we ablate image inputs\nduring long-chain reasoning. Concretely, we truncate the reasoning process\nmidway, then re-complete the reasoning process with the input image removed. We\nobserve only a ~2% accuracy drop on MathVista's test-hard subset, revealing the\nmodel's textual outputs dominate the following reasoning process. Motivated by\nthis, we propose Take-along Visual Conditioning (TVC), a strategy that shifts\nimage input to critical reasoning stages and compresses redundant visual tokens\nvia dynamic pruning. This methodology helps the model retain attention to the\nvisual components throughout the reasoning. Our approach achieves\nstate-of-the-art performance on average across five mathematical reasoning\nbenchmarks (+3.4% vs previous sota), demonstrating the effectiveness of TVC in\nenhancing multimodal reasoning systems.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13360.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6623975c728f756224d4b768",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6623975c728f756224d4b768/US9n0k45Y1TF6WqOhgBhZ.jpeg",
      "fullname": "Allen Sun",
      "name": "Allen8",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.15055",
      "authors": [
        {
          "_id": "67db9586a2f164ac51f84c72",
          "user": {
            "_id": "641ee9fe632a1ec42caf1fa6",
            "avatarUrl": "/avatars/4efb005ce798ce82d4053e00e0ba9f23.svg",
            "isPro": false,
            "fullname": "Arina Razmyslovich",
            "user": "lavriz",
            "type": "user"
          },
          "name": "Arina Razmyslovich",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-03-20T04:13:32.101Z",
          "hidden": false
        },
        {
          "_id": "67db9586a2f164ac51f84c73",
          "name": "Kseniia Murasheva",
          "hidden": false
        },
        {
          "_id": "67db9586a2f164ac51f84c74",
          "name": "Sofia Sedlova",
          "hidden": false
        },
        {
          "_id": "67db9586a2f164ac51f84c75",
          "name": "Julien Capitaine",
          "hidden": false
        },
        {
          "_id": "67db9586a2f164ac51f84c76",
          "name": "Eugene Dmitriev",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-19T09:46:54.000Z",
      "submittedOnDailyAt": "2025-03-20T02:46:26.189Z",
      "title": "ELTEX : Frèmewrk de Génération de Données Síntiques Dominé par le Domaine",
      "submittedOnDailyBy": {
        "_id": "641ee9fe632a1ec42caf1fa6",
        "avatarUrl": "/avatars/4efb005ce798ce82d4053e00e0ba9f23.svg",
        "isPro": false,
        "fullname": "Arina Razmyslovich",
        "user": "lavriz",
        "type": "user"
      },
      "summary": "El Étecks (Efficient LLM Token Extraction) est une approche domaine-spécifique destinée à la génération de données d'entraînement de haute qualité et synthétiques dans les domaines professionnels. Les modèles de langage à grande échelle (LLMs) montrent des capacités générales, mais la manque de données spécifiques dans des domaines comme la sécurité informatique entrave les améliorations du rendement. L'Étecks intègre systématiquement l'extraction d'indicateurs spécifiques du domaine et la programmation dynamique pour atteindre un processus de génération efficace, en maintenant le savoir professionnel. Son efficacité est démontrée dans le contexte de la détection d'attaques cybernétiques liées au blockchain. Des combinaisons de données réelles avec des données générées par l'Étecks sont testées en utilisant le modèle Jema-2B. Enfin, le modèle étendu par l'Étecks atteint le rendement des modèles tels que GPT-4 en classification de classes et correction d'incertitude, réduisant significativement l'utilisation de ressources informatiques. Un ensemble de données générées pour la détection d'attaques cybernétiques dans le blockchain est publié, démontrant que la génération de données synthétiques dirigée par domaine peut surpasser significativement les différences de rendement entre modèles efficaces et grandes architectures.",
      "upvotes": 0,
      "discussionId": "67db958fa2f164ac51f84f51",
      "githubRepo": "https://github.com/1712n/eltex",
      "ai_keywords": [
        "ELTEX",
        "domain-driven framework",
        "high-quality synthetic training data",
        "Large Language Models (LLMs)",
        "cohort indicator extraction",
        "dynamic prompting",
        "critical domain knowledge",
        "blockchain-related cyberattack detection",
        "Gemma-2B",
        "performance competitive",
        "GPT-4",
        "standard classification metrics",
        "uncertainty calibration",
        "computational resources",
        "synthetic dataset",
        "social media texts",
        "domain-driven synthetic data generation",
        "performance gap",
        "resource-efficient models",
        "larger architectures"
      ]
    },
    "publishedAt": "2025-03-19T05:46:54.000Z",
    "title": "ELTEX: A Framework for Domain-Driven Synthetic Data Generation",
    "summary": "We present ELTEX (Efficient LLM Token Extraction), a domain-driven framework\nfor generating high-quality synthetic training data in specialized domains.\nWhile Large Language Models (LLMs) have shown impressive general capabilities,\ntheir performance in specialized domains like cybersecurity remains limited by\nthe scarcity of domain-specific training data. ELTEX addresses this challenge\nby systematically integrating explicit domain indicator extraction with dynamic\nprompting to preserve critical domain knowledge throughout the generation\nprocess. We demonstrate ELTEX's effectiveness in the context of\nblockchain-related cyberattack detection, where we fine-tune Gemma-2B using\nvarious combinations of real and ELTEX-generated data. Our results show that\nthe ELTEX-enhanced model achieves performance competitive with GPT-4 across\nboth standard classification metrics and uncertainty calibration, while\nrequiring significantly fewer computational resources. We release a curated\nsynthetic dataset of social media texts for cyberattack detection in\nblockchain. Our work demonstrates that domain-driven synthetic data generation\ncan effectively bridge the performance gap between resource-efficient models\nand larger architectures in specialized domains.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.15055.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "641ee9fe632a1ec42caf1fa6",
      "avatarUrl": "/avatars/4efb005ce798ce82d4053e00e0ba9f23.svg",
      "fullname": "Arina Razmyslovich",
      "name": "lavriz",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]