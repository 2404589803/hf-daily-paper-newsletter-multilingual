[
  {
    "paper": {
      "id": "2502.01362",
      "authors": [
        {
          "_id": "67a2ad6ac7caec9bf5a45e61",
          "name": "Nikita Gushchin",
          "hidden": false
        },
        {
          "_id": "67a2ad6ac7caec9bf5a45e62",
          "name": "David Li",
          "hidden": false
        },
        {
          "_id": "67a2ad6ac7caec9bf5a45e63",
          "name": "Daniil Selikhanovych",
          "hidden": false
        },
        {
          "_id": "67a2ad6ac7caec9bf5a45e64",
          "name": "Evgeny Burnaev",
          "hidden": false
        },
        {
          "_id": "67a2ad6ac7caec9bf5a45e65",
          "name": "Dmitry Baranchuk",
          "hidden": false
        },
        {
          "_id": "67a2ad6ac7caec9bf5a45e66",
          "name": "Alexander Korotin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-03T13:56:03.000Z",
      "title": "Reverso de Bridging Matching Distillation",
      "summary": "Le modèle de transition de DiPi génétique Bridge est simple, mais transformer cela en quelque chose de pratique rapide est artistique. Les modèles de DiPi génétique Bridge (DBMs) sont évalués comme une prometteuse extension des modèles de DiPi génétique pour le domaine de la transformation d'images. Cependant, comme beaucoup d'autres modèles de DiPi génétique et de flux modernes, les DBMs souffrent d'une lente vitesse d'inférence. En réponse à cela, nous proposons une nouvelle technique de transition basée sur la formulation de la correspondance inverse de Bridge et calculons des fonctions objectifs calculables pour trouver des solutions pratiques. Une différence avec les technologies de transition de DBMs développées précédemment est que la proposition peut transitionner aussi bien des modèles conditionnels que non conditionnels, générant une transition dans un générateur en un seul pas et, en outre, en entraînant avec seulement des images détruites. Les techniques de transition conditionnelle et non conditionnelle ont été évaluées dans une large gamme de configurations, y compris la conversion d'images en super-résolution, le JPEG lifting, la conversion de flux en images et d'autres tâches. Notre technologie de transition a accéléré la vitesse d'inférence des DBMs dans un intervalle de 4 à 100 fois, et dans certaines configurations, a montré une qualité de génération qui dépasse les modèles d'entraîneur.",
      "upvotes": 16,
      "discussionId": "67a2ad70c7caec9bf5a45fb0"
    },
    "publishedAt": "2025-02-05T03:01:40.464Z",
    "title": "Inverse Bridge Matching Distillation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.01362.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "672503c59f68afdd63cc81a2",
      "avatarUrl": "/avatars/91207207b56a1fc2b4a8197b1ab3a7f9.svg",
      "fullname": "Nikita Gushchin",
      "name": "ngushchin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.01718",
      "authors": [
        {
          "_id": "67a2d995c97974764a8c294c",
          "name": "Huaye Zeng",
          "hidden": false
        },
        {
          "_id": "67a2d995c97974764a8c294d",
          "name": "Dongfu Jiang",
          "hidden": false
        },
        {
          "_id": "67a2d995c97974764a8c294e",
          "name": "Haozhe Wang",
          "hidden": false
        },
        {
          "_id": "67a2d995c97974764a8c294f",
          "name": "Ping Nie",
          "hidden": false
        },
        {
          "_id": "67a2d995c97974764a8c2950",
          "name": "Xiaotong Chen",
          "hidden": false
        },
        {
          "_id": "67a2d995c97974764a8c2951",
          "name": "Wenhu Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-03T18:46:04.000Z",
      "title": "ACECODER : Code de test automatique pour améliorer la RL de bons codes",
      "summary": "Le développement récent des modèles de code est motivé par plusieurs types d'apprentissage supervisé (SFT), mais la possibilité d'apprentissage par réponse (RL) dans le domaine du code est principalement explorée en raison de la manque de données de récompense fiables et de la rareté des modèles. Dans cet article, ces défis sont abordés en utilisant la synthèse de cas de test à grande échelle pour améliorer l'entraînement des modèles de code. Spécifiquement, une pipeline est conçue pour générer de grandes paires (problème, cas de test) à partir des données de code actuelles. Avec ces cas de test, des paires de préférence basées sur le rendement du programme sont construites et un modèle de récompense est entraîné en utilisant la perte d'indépendance. Cela a entraîné une augmentation moyenne de 10 points sur Llama-3.1-8B-Ins et de 5 points sur Qwen2.5-Coder-7B-Ins (en sélectionnant les meilleurs 3 sur 32 échantillons). De plus, l'apprentissage par réponse est effectué en utilisant la récompense par étape des cas de test et des modèles de récompense, observant des améliorations constantes dans les tests HumanEval, MBPP, BigCodeBench et LiveCodeBench (V4). En particulier, en commençant par l'entraînement de R1 et en continuant par l'entraînement direct sur Qwen2.5-Coder-base, une augmentation de plus de 25% sur HumanEval-plus et de 6% sur MBPP-plus (avec 80 étapes d'optimisation) a été observée. Nous pensons que l'apprentissage par réponse a un grand potentiel dans les modèles de code.",
      "upvotes": 12,
      "discussionId": "67a2d996c97974764a8c29a1"
    },
    "publishedAt": "2025-02-04T22:23:07.858Z",
    "title": "ACECODER: Acing Coder RL via Automated Test-Case Synthesis",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.01718.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5946
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.02492",
      "authors": [
        {
          "_id": "67a2ec904ea0e3138ac966f2",
          "user": {
            "_id": "6181c72cdcc1df2c9de8a4d8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1655248010394-6181c72cdcc1df2c9de8a4d8.jpeg",
            "isPro": false,
            "fullname": "Hila Chefer",
            "user": "Hila",
            "type": "user"
          },
          "name": "Hila Chefer",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-02-05T04:44:03.218Z",
          "hidden": false
        },
        {
          "_id": "67a2ec904ea0e3138ac966f3",
          "name": "Uriel Singer",
          "hidden": false
        },
        {
          "_id": "67a2ec904ea0e3138ac966f4",
          "name": "Amit Zohar",
          "hidden": false
        },
        {
          "_id": "67a2ec904ea0e3138ac966f5",
          "name": "Yuval Kirstain",
          "hidden": false
        },
        {
          "_id": "67a2ec904ea0e3138ac966f6",
          "name": "Adam Polyak",
          "hidden": false
        },
        {
          "_id": "67a2ec904ea0e3138ac966f7",
          "name": "Yaniv Taigman",
          "hidden": false
        },
        {
          "_id": "67a2ec904ea0e3138ac966f8",
          "name": "Lior Wolf",
          "hidden": false
        },
        {
          "_id": "67a2ec904ea0e3138ac966f9",
          "name": "Shelly Sheynin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-04T17:07:10.000Z",
      "title": "VideoJAM : Méthode commune d'expression pour l'émergence et le fonctionnement des améliorations fonctionnelles dans les modèles vidéo",
      "summary": "Bien que l'on ait récemment observé un développement impressionnant, les modèles de génération vidéo manquent de capacité à capturer facilement le mouvement, la mécanique et la physique du monde réel. Nous avons montré que ces limitations sont prioritisées dans les objectifs traditionnels de reconstruction de pixels, ce qui affecte la fidélité extérieure et la cohérence du mouvement du modèle. Pour y remédier, nous présentons le nouveau cadre de travail VideoJAM, qui fournit une orientation efficace du mouvement aux modèles de vidéo génération, permettant ainsi qu'ils apprennent une représentation commune de ces deux aspects. VideoJAM est composé de deux unités d'interpolation et, pendant l'entraînement, nous avons étendu l'objectif fonctionnel pour prédire les pixels générés et le mouvement relatif, établissant des objectifs qui ne sont exécutés que sur les représentations entraînées. Pendant l'inférence, nous utilisons les changements dans la prédiction du mouvement comme signaux de guidance dynamique pour introduire une structure d'Inner-Guidance qui guide la génération du mouvement. En particulier, notre cadre est conçu de manière à ce qu'il ne nécessite pas de changements dans les données d'entraînement ou le taille du modèle. VideoJAM démontre une performance leader en termes de cohérence du mouvement, dépassant les modèles compétitifs de haut niveau et améliorant la qualité visuelle des contenus générés. Ces résultats soulignent que la combinaison efficace de l'extérieur et du mouvement dans un vidéo généré peut améliorer à la fois sa qualité visuelle et sa cohérence. Site web du projet : https://hila-chefer.github.io/videojam-paper.github.io/",
      "upvotes": 11,
      "discussionId": "67a2ec934ea0e3138ac9678e"
    },
    "publishedAt": "2025-02-04T23:46:17.626Z",
    "title": "VideoJAM: Joint Appearance-Motion Representations for Enhanced Motion Generation in Video Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.02492.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6181c72cdcc1df2c9de8a4d8",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1655248010394-6181c72cdcc1df2c9de8a4d8.jpeg",
      "fullname": "Hila Chefer",
      "name": "Hila",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.02584",
      "authors": [
        {
          "_id": "67a2d59fd5ad3369a66ff394",
          "name": "Zongyu Lin",
          "hidden": false
        },
        {
          "_id": "67a2d59fd5ad3369a66ff395",
          "name": "Yao Tang",
          "hidden": false
        },
        {
          "_id": "67a2d59fd5ad3369a66ff396",
          "name": "Xingcheng Yao",
          "hidden": false
        },
        {
          "_id": "67a2d59fd5ad3369a66ff397",
          "name": "Da Yin",
          "hidden": false
        },
        {
          "_id": "67a2d59fd5ad3369a66ff398",
          "name": "Ziniu Hu",
          "hidden": false
        },
        {
          "_id": "67a2d59fd5ad3369a66ff399",
          "name": "Yizhou Sun",
          "hidden": false
        },
        {
          "_id": "67a2d59fd5ad3369a66ff39a",
          "name": "Kai-Wei Chang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-04T18:58:31.000Z",
      "title": "QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QL",
      "summary": "Le langage des agents intelligents a évolué comme une solution prometteuse pour les tâches d'interaction complexes. L'un des éléments les plus importants pour le succès des agents est le modèle de récompense dans le flux des agents. Ce modèle fournit une guidance utile pendant le processus d'apprentissage et d'inférence. Cependant, en raison du manque d'explications suffisantes des interactions intermédiaires, de nombreux études optimisent les politiques qui construisent toute la route en utilisant le modèle de récompense final du résultat. Cela peut empêcher d'atteindre une politique optimale et réduire le rendement général. Pour résoudre ces problèmes, on propose QLASS (Guide Q-Guided Langage Agent STEP-WISE STEERING). QLASS calcule les valeurs Q de manière pas à pas pour créer automatiquement des explications dans les agents de langage ouverts. En présentant des arbres de causes et en modélisant les récompenses à chaque pas, QLASS fournit des explications intermédiaires efficaces à chaque étape. En recevant des explications pas à pas, QLASS acquiert une meilleure adaptation à long terme, améliorant significativement le rendement dans les tâches d'agents d'interaction complexes. En particulier, QLASS maintient un rendement fort même lorsque des données d'explications proches sont utilisées. De plus, QLASS a démontré son efficacité par analyse qualitative, permettant de prendre des décisions plus efficaces. Les code et les données sont publiés.",
      "upvotes": 7,
      "discussionId": "67a2d5a0d5ad3369a66ff3d4"
    },
    "publishedAt": "2025-02-04T22:08:25.652Z",
    "title": "QLASS: Boosting Language Agent Inference via Q-Guided Stepwise Search",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.02584.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "634e4670a51d5df8c2d92fce",
      "avatarUrl": "/avatars/c52d7150b4de6a2eb2d83b345d35cbc2.svg",
      "fullname": "Da Yin",
      "name": "DaYin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.01941",
      "authors": [
        {
          "_id": "67a2e2a02dd2adbc88755a47",
          "name": "Xiang Liu",
          "hidden": false
        },
        {
          "_id": "67a2e2a02dd2adbc88755a48",
          "name": "Zhenheng Tang",
          "hidden": false
        },
        {
          "_id": "67a2e2a02dd2adbc88755a49",
          "name": "Hong Chen",
          "hidden": false
        },
        {
          "_id": "67a2e2a02dd2adbc88755a4a",
          "name": "Peijie Dong",
          "hidden": false
        },
        {
          "_id": "67a2e2a02dd2adbc88755a4b",
          "name": "Zeyu Li",
          "hidden": false
        },
        {
          "_id": "67a2e2a02dd2adbc88755a4c",
          "name": "Xiuze Zhou",
          "hidden": false
        },
        {
          "_id": "67a2e2a02dd2adbc88755a4d",
          "name": "Bo Li",
          "hidden": false
        },
        {
          "_id": "67a2e2a02dd2adbc88755a4e",
          "name": "Xuming Hu",
          "hidden": false
        },
        {
          "_id": "67a2e2a02dd2adbc88755a4f",
          "name": "Xiaowen Chu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-04T02:23:06.000Z",
      "title": "Les modèles de langage grands (LLM) peuvent-ils maintenir leurs capacités de base sous l'effet de la compression de la cache KV ?",
      "summary": "Cet article effectue une recherche sur les problèmes non explorés dans les modèles de langage grands (LLMs) et examine comment la compression du cache KV affecte les capacités de base des LLMs. Les méthodes existantes atteignent une compression impressionnante dans les cadres d'évaluation de contextes longs, mais leur impact sur les capacités clés du modèle a été peu étudié. Nous présentons des résultats d'expériences détaillées qui évaluent les méthodes de compression du cache KV de pointe dans diverses tâches, comme le savoir mondial, les insights, le raisonnement arithmétique, la génération de code, la sécurité, la compréhension et la génération de contextes longs. L'analyse montre que la compression du cache KV peut diminuer le rendement dans certaines tâches. En particulier, les tâches de raisonnement arithmétique sont souvent particulièrement sensibles à la compression, avec un baisse de rendement de 17,4% à 43,3%. Nous constatons que le modèle DeepSeek R1 Distill présente une plus grande résistance à la compression par rapport aux modèles entraînés de manière directe, avec un baisse de rendement de 9,67% à 25,53%. Nous proposons une nouvelle approche de compression qui maintient une cohérence significative tout en traitant spécialement la phase de pré-traitement et de décodage. Les résultats des expériences montrent que, même à des niveaux de compression sévères, on atteint un amélioration du rendement de 9% à 18% dans les tâches de génération de contextes longs.",
      "upvotes": 6,
      "discussionId": "67a2e2a22dd2adbc88755ab4"
    },
    "publishedAt": "2025-02-04T23:04:25.888Z",
    "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63024676056ec3a2a8714b24/XcgjmhpXd3dH6LnFZGupJ.png",
      "https://cdn-uploads.huggingface.co/production/uploads/63024676056ec3a2a8714b24/hxWz1iVOUcE76E_K5z-B0.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.01941.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63024676056ec3a2a8714b24",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1661093436322-noauth.jpeg",
      "fullname": "Xiang Liu",
      "name": "Dominic789654",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.02508",
      "authors": [
        {
          "_id": "67a2d1f9bc9d072d9459e857",
          "user": {
            "_id": "6553c985a7aded0380b5f928",
            "avatarUrl": "/avatars/36109d6f536d2b34d98822b88eac9608.svg",
            "isPro": false,
            "fullname": "Maohao Shen",
            "user": "maohaos2",
            "type": "user"
          },
          "name": "Maohao Shen",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-02-05T03:00:33.470Z",
          "hidden": false
        },
        {
          "_id": "67a2d1f9bc9d072d9459e858",
          "name": "Guangtao Zeng",
          "hidden": false
        },
        {
          "_id": "67a2d1f9bc9d072d9459e859",
          "name": "Zhenting Qi",
          "hidden": false
        },
        {
          "_id": "67a2d1f9bc9d072d9459e85a",
          "name": "Zhang-Wei Hong",
          "hidden": false
        },
        {
          "_id": "67a2d1f9bc9d072d9459e85b",
          "name": "Zhenfang Chen",
          "hidden": false
        },
        {
          "_id": "67a2d1f9bc9d072d9459e85c",
          "name": "Wei Lu",
          "hidden": false
        },
        {
          "_id": "67a2d1f9bc9d072d9459e85d",
          "name": "Gregory Wornell",
          "hidden": false
        },
        {
          "_id": "67a2d1f9bc9d072d9459e85e",
          "name": "Subhro Das",
          "hidden": false
        },
        {
          "_id": "67a2d1f9bc9d072d9459e85f",
          "name": "David Cox",
          "hidden": false
        },
        {
          "_id": "67a2d1f9bc9d072d9459e860",
          "name": "Chuang Gan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-04T17:26:58.000Z",
      "title": "Satori : Utilisant le pensée en chaînes d'actions pour améliorer l'apprentissage par récompense des LLM. Théorie logique basée sur les recherches automatiques.",
      "summary": "Les grands modèles de langue (LLMs) montrent des capacités logiques impressionnantes dans diverses domaines. Selon des études récentes, l'augmentation de la quantité de calculs améliore la capacité logique des LLMs. Cela inclut généralement une large échantillon de données de validation externe pour l'inférence, ce qui forme un système de deux joueurs. Peu importe si il y a un guide externe, ce système démontre l'efficacité d'un seul LLM pour réaliser des tâches complexes. Nous proposons donc un nouveau problème de recherche : déterminer si il est possible d'internaliser la capacité d'exploration d'un seul LLM pour améliorer ses habiletés logiques de manière fondamentale. Cette recherche se concentre sur les LLMs ultérieurs et se focalise sur un approche orthogonale qui vise à automatiser la collaboration d'exploration (y compris l'auto-reflexion et la recherche de nouvelles stratégies dans un processus logique amplifié). Pour y parvenir, on utilise la logique d'action et de pensée (COAT) et un paradigme d'apprentissage à deux étapes : (1) un état d'entraînement à petite échelle pour internaliser le format de la COAT et (2) un état d'entraînement à grande échelle en utilisant l'apprentissage par récompense. Avec notre approche, le 7B LLM Satori a été entraîné avec des modèles et des données ouvertes. Selon les évaluations de nombreuses expériences, Satori a atteint les meilleurs résultats sur les indicateurs logiques mathématiques et montre une forte capacité de généralisation dans des tâches en dehors du domaine. Tout le code, les données et les modèles sont complètement ouverts.",
      "upvotes": 5,
      "discussionId": "67a2d1fcbc9d072d9459e91b"
    },
    "publishedAt": "2025-02-04T21:55:09.693Z",
    "title": "Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.02508.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60ad0de755f970745d4ec28d",
      "avatarUrl": "/avatars/b0de0222b8ed5fdac8dc7cb0336d2ec7.svg",
      "fullname": "GtZeng",
      "name": "chaoscodes",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isMod": false,
      "followerCount": 11
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.01720",
      "authors": [
        {
          "_id": "67a2fddb4044bf1c86f765a3",
          "name": "Nupur Kumari",
          "hidden": false
        },
        {
          "_id": "67a2fddb4044bf1c86f765a4",
          "name": "Xi Yin",
          "hidden": false
        },
        {
          "_id": "67a2fddb4044bf1c86f765a5",
          "name": "Jun-Yan Zhu",
          "hidden": false
        },
        {
          "_id": "67a2fddb4044bf1c86f765a6",
          "name": "Ishan Misra",
          "hidden": false
        },
        {
          "_id": "67a2fddb4044bf1c86f765a7",
          "name": "Samaneh Azadi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-03T18:59:41.000Z",
      "title": "Création adaptée d'images - Technologie pour la génération de données de synthèse multi-images",
      "summary": "Les utilisateurs de modèles de texte peuvent personnaliser ces modèles pour introduire des concepts personnalisés et générer ces concepts avec des configurations jamais vues. Actuellement, les méthodes dépendent de l'optimisation de tests coûteux ou de l'entraînement de l'encodeur avec seulement une image, ce qui peut dégrader la qualité des images. Nous proposons un approche simple pour résoudre les deux problèmes. Tout d'abord, nous utilisons des modèles de texte et des ensembles de données 3D pour créer un ensemble de données de personnalisation haut de gamme (SynCD) qui inclut plusieurs images de la même objet avec différentes illuminations, fonds et postures. Ensuite, nous proposons une nouvelle architecture d'encodeur basée sur l'attribution partagée pour incorporer des détails visuels plus précis de l'image d'entrée. Enfin, nous proposons un nouveau méthode d'inférence pour normaliser les vecteurs de texte et d'image pour atténuer le problème d'overexposition. Dans des expériences étendues, il a été démontré que les modèles entraînés sur l'ensemble de données de personnalisation avec l'encodeur et l'algorithme d'inférence proposés dépassent les méthodes actuelles sans tonnage dans la personnalisation standard.",
      "upvotes": 2,
      "discussionId": "67a2fde34044bf1c86f767ba"
    },
    "publishedAt": "2025-02-05T00:59:11.275Z",
    "title": "Generating Multi-Image Synthetic Data for Text-to-Image Customization",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.01720.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62f6a894c3372328414c7021",
      "avatarUrl": "/avatars/e8b10912355712f38f10805c31bea962.svg",
      "fullname": "Nupur Kumari",
      "name": "nupurkmr9",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  }
]