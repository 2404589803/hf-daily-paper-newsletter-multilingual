[
  {
    "paper": {
      "id": "2502.17157",
      "authors": [
        {
          "_id": "67bd3285ac4a596a43b53205",
          "user": {
            "_id": "646efd223dd912a539e0bd46",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/EOFAv5xvOgJOzuDgh4nSb.png",
            "isPro": true,
            "fullname": "Canyu Zhao",
            "user": "Canyu",
            "type": "user"
          },
          "name": "Canyu Zhao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-25T09:40:20.829Z",
          "hidden": false
        },
        {
          "_id": "67bd3285ac4a596a43b53206",
          "name": "Mingyu Liu",
          "hidden": false
        },
        {
          "_id": "67bd3285ac4a596a43b53207",
          "user": {
            "_id": "64d60375d7e30889c65e8cf4",
            "avatarUrl": "/avatars/640f7c570fc45194557ce7931bdfe87f.svg",
            "isPro": false,
            "fullname": "Huanyi Zheng",
            "user": "zhyya",
            "type": "user"
          },
          "name": "Huanyi Zheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-25T09:40:18.731Z",
          "hidden": false
        },
        {
          "_id": "67bd3285ac4a596a43b53208",
          "user": {
            "_id": "632179745fc60c44fd91fc33",
            "avatarUrl": "/avatars/37d4fefbcc19f091dccffefec9706de2.svg",
            "isPro": false,
            "fullname": "zhumuzhi",
            "user": "Z-MU-Z",
            "type": "user"
          },
          "name": "Muzhi Zhu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-25T09:40:11.968Z",
          "hidden": false
        },
        {
          "_id": "67bd3285ac4a596a43b53209",
          "name": "Zhiyue Zhao",
          "hidden": false
        },
        {
          "_id": "67bd3285ac4a596a43b5320a",
          "name": "Hao Chen",
          "hidden": false
        },
        {
          "_id": "67bd3285ac4a596a43b5320b",
          "name": "Tong He",
          "hidden": false
        },
        {
          "_id": "67bd3285ac4a596a43b5320c",
          "name": "Chunhua Shen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-24T13:51:06.000Z",
      "title": "Design : Modèle général pour le reconnaissance visuelle des tâches",
      "summary": "Le principal objectif de cet article est de créer un bon modèle général de reconnaissance capable de gérer plusieurs tâches, en tenant compte des limitations en termes de ressources informatiques et de données d'entraînement. Pour y parvenir, on utilise un modèle étendu d'images à partir du texte fourni dans des millions d'images. Dans nos critères d'évaluation, DICEPTION a démontré être efficace pour plusieurs tâches de reconnaissance et d'atteindre les performances des modèles les plus récents. En utilisant seulement environ 0.06% des données de SAM-vit-h (par exemple, 600K images avec des étiquettes de niveau de pixel sur 1B images), on obtient les mêmes résultats. Inspiré de Wang et al., DICEPTION configure les résultats de diverses tâches de reconnaissance avec une codification de couleurs et attribue des couleurs aléatoires à différentes instances, ce qui est très efficace pour la segmentation d'objets et de sens. En unifiant les tâches de reconnaissance par la génération d'images conditionnelles, on peut maximiser l'utilisation de modèles pré-entraînés depuis le texte à l'image. De cette manière, DICEPTION peut être entraîné efficacement avec moins de données ou de paramètres par rapport aux modèles existants, surtout lorsqu'il est appliqué à diverses tâches. En appliquant le modèle à d'autres tâches, il suffit de 50 images. DICEPTION offre une contribution précieuse aux modèles généraux visuels et une solution plus durable.",
      "upvotes": 35,
      "discussionId": "67bd328aac4a596a43b532ae"
    },
    "publishedAt": "2025-02-24T22:39:29.837Z",
    "title": "DICEPTION: A Generalist Diffusion Model for Visual Perceptual Tasks",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.17157.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "646efd223dd912a539e0bd46",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/EOFAv5xvOgJOzuDgh4nSb.png",
      "fullname": "Canyu Zhao",
      "name": "Canyu",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.17129",
      "authors": [
        {
          "_id": "67bd37cb0d41e01cca99aa8b",
          "user": {
            "_id": "64f033ef82c6eea604c4da8b",
            "avatarUrl": "/avatars/51b93fea7fd68b4274ee03701245dcca.svg",
            "isPro": false,
            "fullname": "Liu Xiaoran",
            "user": "LiuXR",
            "type": "user"
          },
          "name": "Xiaoran Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-25T09:40:07.298Z",
          "hidden": false
        },
        {
          "_id": "67bd37cb0d41e01cca99aa8c",
          "name": "Ruixiao Li",
          "hidden": false
        },
        {
          "_id": "67bd37cb0d41e01cca99aa8d",
          "name": "Mianqiu Huang",
          "hidden": false
        },
        {
          "_id": "67bd37cb0d41e01cca99aa8e",
          "name": "Zhigeng Liu",
          "hidden": false
        },
        {
          "_id": "67bd37cb0d41e01cca99aa8f",
          "name": "Yuerong Song",
          "hidden": false
        },
        {
          "_id": "67bd37cb0d41e01cca99aa90",
          "name": "Qipeng Guo",
          "hidden": false
        },
        {
          "_id": "67bd37cb0d41e01cca99aa91",
          "name": "Siyang He",
          "hidden": false
        },
        {
          "_id": "67bd37cb0d41e01cca99aa92",
          "name": "Qiqi Wang",
          "hidden": false
        },
        {
          "_id": "67bd37cb0d41e01cca99aa93",
          "name": "Linlin Li",
          "hidden": false
        },
        {
          "_id": "67bd37cb0d41e01cca99aa94",
          "name": "Qun Liu",
          "hidden": false
        },
        {
          "_id": "67bd37cb0d41e01cca99aa95",
          "name": "Yaqian Zhou",
          "hidden": false
        },
        {
          "_id": "67bd37cb0d41e01cca99aa96",
          "name": "Xuanjing Huang",
          "hidden": false
        },
        {
          "_id": "67bd37cb0d41e01cca99aa97",
          "name": "Xipeng Qiu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-24T13:19:33.000Z",
      "title": "**Titre :** Phrases Longues et Mots de Grands Modèles de Langue\n\n**Texte :**\n\nPhrases longues et grands modèles de langue sont conçus pour lire et comprendre des phrases longues. Ce modèle peut fournir des réponses plus précises en considérant le contexte long. Cette capacité est importante dans de nombreuses applications et est utilisée dans diverses domaines tels que l'analyse de texte, la traduction et le design de systèmes.",
      "summary": "La longueur du contexte est un problème important dans le traitement du langage naturel (NLP), développé par des architectures de NLP, et offre une grande opportunité pour l'apprentissage continu humain dans les modèles de langage à grande échelle (LLMs). Cependant, l'approche de la longueur du contexte fait face à plusieurs obstacles. Cependant, la longueur du contexte maintient la compétence centrale des LLMs. Au cours des deux derniers ans, la longueur du contexte des LLMs a augmenté jusqu'à des millions de tokens, démontrant un grand progrès. De plus, la recherche sur la longueur du contexte des LLMs ne doit pas se concentrer uniquement sur la longueur, mais aussi sur l'architecture, l'infrastructure, l'entraînement et l'évaluation.\n\nEn se basant sur la citation de l'orateur de Smart Field \"C'est ainsi qu'il a dit\", nous avons trouvé une similitude entre le désir d'étendre le contexte des LLMs et l'effort de l'humanité pour surmonter le douleur de sa mort. Dans cette étude, nous expliquons la nécessité des LLMs pour une longueur du contexte importante et la nécessité d'accepter que cela finalement soit limitée. Pour y parvenir, nous présentons la vie cycle des LLMs de longueur du contexte à partir de quatre perspectives : architecture, infrastructure, entraînement et évaluation, montrant tout le panorama de la technologie. Enfin, cette étude présente 10 questions sans réponse dans les LLMs de longueur du contexte actuels. Nous espérons que cette étude servira de base systématique pour la recherche sur la longueur du contexte des LLMs.",
      "upvotes": 25,
      "discussionId": "67bd37cc0d41e01cca99ab1e"
    },
    "publishedAt": "2025-02-24T22:27:11.566Z",
    "title": "Thus Spake Long-Context Large Language Model",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.17129.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64f033ef82c6eea604c4da8b",
      "avatarUrl": "/avatars/51b93fea7fd68b4274ee03701245dcca.svg",
      "fullname": "Liu Xiaoran",
      "name": "LiuXR",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.15814",
      "authors": [
        {
          "_id": "67bd3972f077ddf1f98bacda",
          "user": {
            "_id": "66b9bc2dacdbc1d0b39c3b50",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/hwR0pVfP_E8XjimXIxDOU.jpeg",
            "isPro": false,
            "fullname": "Gallil Maimon",
            "user": "gallilmaimon",
            "type": "user"
          },
          "name": "Gallil Maimon",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-25T09:39:36.258Z",
          "hidden": false
        },
        {
          "_id": "67bd3972f077ddf1f98bacdb",
          "user": {
            "_id": "644662145004f2cb3af08b27",
            "avatarUrl": "/avatars/5f2af24c7410a5db46374d0b84fb479d.svg",
            "isPro": false,
            "fullname": "Avishai Elmakies",
            "user": "avishai-elmakies",
            "type": "user"
          },
          "name": "Avishai Elmakies",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-25T09:39:33.712Z",
          "hidden": false
        },
        {
          "_id": "67bd3972f077ddf1f98bacdc",
          "name": "Yossi Adi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-19T17:21:15.000Z",
      "title": "Slam : Un modèle de langage est entraîné via un coup de pied d'une journée.",
      "summary": "Slam est une recette pour entraîner des modèles de langue de haute qualité (SLMs) sur un seul GPU académique en 24 heures. Pour cela, des expériences sont réalisées qui comprennent l'initialisation du modèle, l'architecture, la génération de données synthétiques, l'optimisation des préférences basée sur des données synthétiques, et l'ajustement de tous les autres composants. Bien que cette recette d'entraînement ait été démontrée expérimentalement, elle peut être échelonnée pour utiliser plus de calcul, et nécessite moins de coût de calcul pour obtenir des résultats avancés comme les SLMs. Ces observations sont fournies pour faciliter l'entraînement et l'investigation des SLMs. À partir de la scalabilité des SLMs, nos résultats dépassent significativement la performance informatique prédite et offrent une vision plus équilibrée des possibilités des SLMs. Les codes, les données, les modèles et les exemples peuvent être trouvés sur la suivante URL : https://pages.cs.huji.ac.il/adiyoss-lab/slamming.",
      "upvotes": 19,
      "discussionId": "67bd3973f077ddf1f98bacf9"
    },
    "publishedAt": "2025-02-24T23:14:12.363Z",
    "title": "Slamming: Training a Speech Language Model on One GPU in a Day",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/66b9bc2dacdbc1d0b39c3b50/t93GkoiYRplnXH1Go0MmY.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15814.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66b9bc2dacdbc1d0b39c3b50",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/hwR0pVfP_E8XjimXIxDOU.jpeg",
      "fullname": "Gallil Maimon",
      "name": "gallilmaimon",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.16584",
      "authors": [
        {
          "_id": "67bd42386959e61abd265a9b",
          "name": "Liumeng Xue",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265a9c",
          "name": "Ziya Zhou",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265a9d",
          "name": "Jiahao Pan",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265a9e",
          "name": "Zixuan Li",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265a9f",
          "name": "Shuai Fan",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265aa0",
          "name": "Yinghao Ma",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265aa1",
          "name": "Sitong Cheng",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265aa2",
          "name": "Dongchao Yang",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265aa3",
          "name": "Haohan Guo",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265aa4",
          "name": "Yujia Xiao",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265aa5",
          "name": "Xinsheng Wang",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265aa6",
          "name": "Zixuan Shen",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265aa7",
          "name": "Chuanbo Zhu",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265aa8",
          "name": "Xinshen Zhang",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265aa9",
          "name": "Tianchi Liu",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265aaa",
          "name": "Ruibin Yuan",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265aab",
          "name": "Zeyue Tian",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265aac",
          "name": "Haohe Liu",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265aad",
          "name": "Emmanouil Benetos",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265aae",
          "name": "Ge Zhang",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265aaf",
          "name": "Yike Guo",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265ab0",
          "name": "Wei Xue",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-23T14:24:15.000Z",
      "title": "Audio-FLAN : Préliminaires Liris",
      "summary": "Le développement récent des modèles de tokenisation de voix dans les grandes réseaux de langage (LLMs) a considérablement amélioré l'intégration des fonctions de voix. Cependant, la compréhension et la génération de voix sont généralement considérées comme des tâches différentes, ce qui rend le développement de modèles de langage de voix intégrés plus difficile. La fine-tuning d'instances a démontré un succès notable en matière de généralisation et d'apprentissage sans exemples dans le domaine de l'image et du texte, mais son application dans le domaine du voix est encore peu explorée. L'une des principales obstacles est la rareté de jeux de données spécifiques pour intégrer la compréhension et la génération de voix. Pour aborder ce problème, nous présentons Audio-FLAN, un grand jeu de données de fine-tuning d'instances. Audio-FLAN comprend 80 tâches différentes et couvre le domaine du voix, de la musique et du son, avec plus de 100 000 instances. Audio-FLAN serve de base pour construire des modèles de langage de voix intégrés qui peuvent traiter tant la compréhension (par exemple, traduction, compréhension) que la génération (par exemple, voix, musique, son). Le jeu de données Audio-FLAN est disponible sur HuggingFace et GitHub et est régulièrement mis à jour.",
      "upvotes": 18,
      "discussionId": "67bd423b6959e61abd265b88"
    },
    "publishedAt": "2025-02-24T23:14:20.487Z",
    "title": "Audio-FLAN: A Preliminary Release",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.16584.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5fd6f670053c8345eddc1b68",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5fd6f670053c8345eddc1b68/cuTsu2krRYHC6zYGD2dpQ.jpeg",
      "fullname": "Ruibin Yuan",
      "name": "a43992899",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 13
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.17435",
      "authors": [
        {
          "_id": "67bd6b4b8edd1ce8ad5603a0",
          "name": "Chen-Wei Chang",
          "hidden": false
        },
        {
          "_id": "67bd6b4b8edd1ce8ad5603a1",
          "name": "Cheng-De Fan",
          "hidden": false
        },
        {
          "_id": "67bd6b4b8edd1ce8ad5603a2",
          "name": "Chia-Che Chang",
          "hidden": false
        },
        {
          "_id": "67bd6b4b8edd1ce8ad5603a3",
          "name": "Yi-Chen Lo",
          "hidden": false
        },
        {
          "_id": "67bd6b4b8edd1ce8ad5603a4",
          "name": "Yu-Chee Tseng",
          "hidden": false
        },
        {
          "_id": "67bd6b4b8edd1ce8ad5603a5",
          "name": "Jiun-Long Huang",
          "hidden": false
        },
        {
          "_id": "67bd6b4b8edd1ce8ad5603a6",
          "name": "Yu-Lun Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-24T18:59:54.000Z",
      "title": "GCC : L'écran de vérification de couleur est élargi et le couleur généré est normalisé.",
      "summary": "La loi de la constante de tono de couleur reconnaît souvent que c'est difficile de généraliser entre différents capteurs de caméra. Nous présentons le GCC (Constante de Couleur Global) en utilisant des modèles de diffusion. Le GCC effectue la constante de tono de couleur en interpolant des marqueurs de couleur dans les images. Notre innovation principale consiste à : (1) un approche d'inférence sécurisée qui reflète la lumière du panorame en un seul pas, (2) un méthode de dérivation de Laplace qui permet une adaptation du ton de couleur en fonction de la lumière tout en maintenant la structure des marqueurs, et (3) une stratégie d'expansion des données basée sur des masques pour gérer des annotations incorrectes des marqueurs de couleur. Le GCC montre la plus grande robustesse dans des scénarios de caméras multiples, avec un pourcentage d'erreur maximum de 25% de 5,15° et 4,32°. Ces résultats montrent que le modèle ne nécessite pas de considérer les caractéristiques de la caméra ni de recevoir un entraînement spécifique pour les capteurs, ce qui le transforme en une solution large d'applications en réalité.",
      "upvotes": 16,
      "discussionId": "67bd6b4d8edd1ce8ad560401"
    },
    "publishedAt": "2025-02-25T02:06:00.809Z",
    "title": "GCC: Generative Color Constancy via Diffusing a Color Checker",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/gDAYQUcbNE2Ps2pQFxg_m.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.17435.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6459d5da3b6fafd9664807ab",
      "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg",
      "fullname": "Yu-Lun Liu",
      "name": "yulunliu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.16614",
      "authors": [
        {
          "_id": "67bd36334a9a04b9ca9bbb68",
          "name": "Alexander Zhang",
          "hidden": false
        },
        {
          "_id": "67bd36334a9a04b9ca9bbb69",
          "name": "Marcus Dong",
          "hidden": false
        },
        {
          "_id": "67bd36334a9a04b9ca9bbb6a",
          "name": "Jiaheng Liu",
          "hidden": false
        },
        {
          "_id": "67bd36334a9a04b9ca9bbb6b",
          "name": "Wei Zhang",
          "hidden": false
        },
        {
          "_id": "67bd36334a9a04b9ca9bbb6c",
          "name": "Yejie Wang",
          "hidden": false
        },
        {
          "_id": "67bd36334a9a04b9ca9bbb6d",
          "name": "Jian Yang",
          "hidden": false
        },
        {
          "_id": "67bd36334a9a04b9ca9bbb6e",
          "name": "Ge Zhang",
          "hidden": false
        },
        {
          "_id": "67bd36334a9a04b9ca9bbb6f",
          "name": "Tianyu Liu",
          "hidden": false
        },
        {
          "_id": "67bd36334a9a04b9ca9bbb70",
          "name": "Zhongyuan Peng",
          "hidden": false
        },
        {
          "_id": "67bd36334a9a04b9ca9bbb71",
          "name": "Yingshui Tan",
          "hidden": false
        },
        {
          "_id": "67bd36334a9a04b9ca9bbb72",
          "name": "Yuanxing Zhang",
          "hidden": false
        },
        {
          "_id": "67bd36334a9a04b9ca9bbb73",
          "name": "Zhexu Wang",
          "hidden": false
        },
        {
          "_id": "67bd36334a9a04b9ca9bbb74",
          "name": "Weixun Wang",
          "hidden": false
        },
        {
          "_id": "67bd36334a9a04b9ca9bbb75",
          "name": "Yancheng He",
          "hidden": false
        },
        {
          "_id": "67bd36334a9a04b9ca9bbb76",
          "name": "Ken Deng",
          "hidden": false
        },
        {
          "_id": "67bd36334a9a04b9ca9bbb77",
          "name": "Wangchunshu Zhou",
          "hidden": false
        },
        {
          "_id": "67bd36334a9a04b9ca9bbb78",
          "name": "Wenhao Huang",
          "hidden": false
        },
        {
          "_id": "67bd36334a9a04b9ca9bbb79",
          "name": "Zhaoxiang Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-23T15:36:43.000Z",
      "title": "CodeCriticBench : Modèle de langage de programmation qui fournit un cadre de référence pour évaluer le code de manière générale.",
      "summary": "La capacité critique des LLMs est essentielle pour fournir des compétences logiques et des suggestions nécessaires (par exemple, une analyse détaillée et une rétroaction pratique). Par conséquent, les méthodes d'évaluation de la capacité critique des LLMs reçoivent beaucoup d'attention et plusieurs marqueurs critiques ont été proposés. Cependant, les marqueurs critiques actuels ont des limitations : (1) ils se concentrent sur des tâches logiques générales dans différentes domaines, mais l'évaluation dans les tâches de code est insuffisante (par exemple, seulement des tâches de génération de code sont évaluées). Ces tâches ont un niveau de difficulté relativement simple (par exemple, les requêtes de code dans CriticBench sont tirées de Humaneval et MBPP). (2) il manque une évaluation détaillée sur de nombreux aspects. Pour faire face à ces limitations, nous présentons un nouveau marqueur critique basé sur des cartes appelé CodeCriticBench. En particulier, notre CodeCriticBench comprend deux tâches de code principales (génération de code et questionnement de code) et son protocole d'évaluation inclut des évaluations critiques de base et avancées, ainsi que des listes de contrôle détaillées pour les évaluations avancées, qui sont adaptées à ces configurations. Enfin, nous avons effectué des expériences avec différents LLMs et démontré l'efficacité de CodeCriticBench.",
      "upvotes": 13,
      "discussionId": "67bd36354a9a04b9ca9bbc16"
    },
    "publishedAt": "2025-02-24T22:17:28.937Z",
    "title": "CodeCriticBench: A Holistic Code Critique Benchmark for Large Language Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.16614.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65377c30e48353201e6fdda0",
      "avatarUrl": "/avatars/a8f803b6f2e598eaee9c52c0d2ddfc16.svg",
      "fullname": "Jiaheng Liu",
      "name": "CheeryLJH",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.16033",
      "authors": [
        {
          "_id": "67bd31d0d055a27740b16a30",
          "name": "Qianqi Yan",
          "hidden": false
        },
        {
          "_id": "67bd31d0d055a27740b16a31",
          "name": "Yue Fan",
          "hidden": false
        },
        {
          "_id": "67bd31d0d055a27740b16a32",
          "name": "Hongquan Li",
          "hidden": false
        },
        {
          "_id": "67bd31d0d055a27740b16a33",
          "name": "Shan Jiang",
          "hidden": false
        },
        {
          "_id": "67bd31d0d055a27740b16a34",
          "name": "Yang Zhao",
          "hidden": false
        },
        {
          "_id": "67bd31d0d055a27740b16a35",
          "name": "Xinze Guan",
          "hidden": false
        },
        {
          "_id": "67bd31d0d055a27740b16a36",
          "name": "Ching-Chen Kuo",
          "hidden": false
        },
        {
          "_id": "67bd31d0d055a27740b16a37",
          "name": "Xin Eric Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-22T01:52:37.000Z",
      "title": "Polymorphism Inappropriateness Inference (MMIR): Nouveau Benchmark pour les Modèles d'Inférence de Polymorphisme",
      "summary": "Actuellement, les modèles de langage multimodal (MLLM) sont entraînés et validés principalement avec des entrées cohérentes d'image-texte, mais la question est posée de savoir si ils peuvent gérer adéquatement l'inadéquation dans des contenus très riches et variés comme ceux du Ruffled Room. Pour corriger cette inadéquation, nous proposons le Multimodal Inconsistency Reasoning (MMIR) comme un cadre d'évaluation pour la capacité des MLLM à détecter et justifier la sémantique inadéquate. MMIR comprend 534 échantillons difficiles et intègre des erreurs synthétiques dans cinq domaines causants : contraires factuels, identité non définie, incohérence contextuelle, désaccord quantitatif et désaccord temporel/spatial. Nous évaluons six modèles de MLLM optimaux et observons que les modèles avec des capacités de raisonnement sémantique (comme o1) dépassent significativement leurs modèles de conteneur, mais les modèles open-source sont particulièrement vulnérables aux erreurs d'inadéquation. L'analyse détaillée montre que la capacité de détecter l'inadéquation sémantique est particulièrement défiante dans le texte, tandis que les modèles expérimentent des conflits entre modèles et des complications dans les contenus du Ruffled Room. Des expériences exploratoires ont montré des effets micro sur la prompting des modèles uniques qui incluent des méthodes comme Chain-of-Thought (CoT) ou Set-of-Mark (SoM), et ont montré un registre de raisonnement croisé de modèles. Nos résultats démontrent la nécessité d'avances dans le raisonnement multimodal et orientent futures recherches vers l'étude de l'inadéquation multimodal.",
      "upvotes": 11,
      "discussionId": "67bd31d2d055a27740b16ad9"
    },
    "publishedAt": "2025-02-24T21:59:50.456Z",
    "title": "Multimodal Inconsistency Reasoning (MMIR): A New Benchmark for Multimodal Reasoning Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.16033.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64679a226192d39142245e5e",
      "avatarUrl": "/avatars/05abee0b6317f100923936ca2099e9eb.svg",
      "fullname": "Xin Eric Wang",
      "name": "xw-eric",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.16894",
      "authors": [
        {
          "_id": "67bd396ea06bae99f3866911",
          "user": {
            "_id": "641aa5e391e3376a057bbd4c",
            "avatarUrl": "/avatars/5818797f27444fde078b503774ee081c.svg",
            "isPro": false,
            "fullname": "Chenghao Fan",
            "user": "Facico",
            "type": "user"
          },
          "name": "Chenghao Fan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-25T09:39:38.942Z",
          "hidden": false
        },
        {
          "_id": "67bd396ea06bae99f3866912",
          "name": "Zhenyi Lu",
          "hidden": false
        },
        {
          "_id": "67bd396ea06bae99f3866913",
          "name": "Sichen Liu",
          "hidden": false
        },
        {
          "_id": "67bd396ea06bae99f3866914",
          "name": "Xiaoye Qu",
          "hidden": false
        },
        {
          "_id": "67bd396ea06bae99f3866915",
          "name": "Wei Wei",
          "hidden": false
        },
        {
          "_id": "67bd396ea06bae99f3866916",
          "name": "Chengfeng Gu",
          "hidden": false
        },
        {
          "_id": "67bd396ea06bae99f3866917",
          "name": "Yu Cheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-24T06:48:13.000Z",
      "title": "L'amélioration de LoRA en utilisant l'optimisation des valeurs propres adaptatives et l'alignement d'experts dans l'optimisation de LoRA",
      "summary": "La Low-Rank Adaptation (LoRA) permet une ajuste micro-paramétrique efficace dans les Grands Modèles de Langue (LLMs). Cependant, ce rendement est généralement inférieur à celui du Fine-Tuning Complète (Full FT). Les méthodes actuelles initialisent et optimisent LoRA à travers des sous-ensembles de SVD (Décomposition des Valeurs Singulières), mais cette forme n'est pas connectée à l'exploitation optimale des connaissances préalablement entraînées. Une autre façon d'améliorer LoRA est d'introduire l'architecture Mixture-of-Experts (MoE). Cependant, l'introduction de SVD dans une architecture MoE présente des problèmes tels que la mauvaise distribution des poids et la complexité de la dynamique des gradients. Pour résoudre ces problèmes, nous proposons le cadre de travail Great LoRA Mixture-of-Expert (GOAT). Ce cadre fournit : (1) l'intégration adaptative de leaders relatifs en utilisant une architecture MoE structurée par SVD et (2) le calcul d'un facteur d'échelle théorique pour offrir des fonctionnalités pour le MoE Fine-Tuned Complète et l'optimisation. Cette approche peut améliorer l'efficacité et le rendement de LoRA MoE sans nécessiter de modifier l'architecture ou l'algorithme d'entraînement. Les expériences réalisées sur 25 jeux de données de traitement du langage naturel, de prédiction de connaissances générales, de classification d'images et de génération de langage nature montrent le rendement récent du GOAT et réduisent la différence avec le Full FT.",
      "upvotes": 10,
      "discussionId": "67bd396fa06bae99f3866964"
    },
    "publishedAt": "2025-02-24T22:35:41.042Z",
    "title": "Make LoRA Great Again: Boosting LoRA with Adaptive Singular Values and Mixture-of-Experts Optimization Alignment",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.16894.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "641aa5e391e3376a057bbd4c",
      "avatarUrl": "/avatars/5818797f27444fde078b503774ee081c.svg",
      "fullname": "Chenghao Fan",
      "name": "Facico",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.17407",
      "authors": [
        {
          "_id": "67bd48d4becb766415a5d19d",
          "name": "Guijin Son",
          "hidden": false
        },
        {
          "_id": "67bd48d4becb766415a5d19e",
          "name": "Jiwoo Hong",
          "hidden": false
        },
        {
          "_id": "67bd48d4becb766415a5d19f",
          "user": {
            "_id": "63e087b6a98d931aa90c1b9c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e087b6a98d931aa90c1b9c/96c6IT3f1pWGLbRdRDB2U.png",
            "isPro": false,
            "fullname": "Hyunwoo Ko",
            "user": "Cartinoe5930",
            "type": "user"
          },
          "name": "Hyunwoo Ko",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-25T09:39:12.933Z",
          "hidden": false
        },
        {
          "_id": "67bd48d4becb766415a5d1a0",
          "name": "James Thorne",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-24T18:36:15.000Z",
      "title": "Le contenu sur la généralisation linguistique de l'échelle au moment de la mesure",
      "summary": "L'augmentation des tests a démontré être efficace pour l'atteinte du multilinguisme, mais son efficacité à augmenter avec le même impact pendant la test est incertaine. Dans cet article, nous présentons MCLM (Benchmark Multilingual de Mathématiques), une version multilingue qui aborde des problèmes de niveau compétitif dans 55 langues. Trois méthodes d'augmentation ont été vérifiées lors des tests : Outcome Reward Modeling (ORM), Process Reward Modeling (ORM) et Budget Forcing (BF) pour Qwen2.5-1.5B Math et MR1-1.5B. En appliquant l'ORM à Qwen2.5-1.5B Math, un score de 35,8 points a été obtenu, tandis que l'application de BF à MR1-1.5B a conduit à un score de 35,2 points. \"Thinking LLMs\" a récemment attiré l'attention, mais son rendement est relativement bon lorsque la quantité de FLOPs d'inférence est limitée, comparé aux méthodes traditionnelles d'augmentation. De plus, BF a montré un gain de 20 points sur AIME en anglais, mais seulement un gain moyen de 1,94 points sur d'autres langues. Ce pattern est maintenu pour d'autres méthodes d'augmentation lors des tests. L'augmentation pendant les tests n'est pas efficacement étendue aux tâches multilingues. Pour avancer, nous proposons MCLM, MR1-1.5B et nous publions les résultats d'évaluation.",
      "upvotes": 9,
      "discussionId": "67bd48d5becb766415a5d1e9"
    },
    "publishedAt": "2025-02-24T23:37:53.138Z",
    "title": "Linguistic Generalizability of Test-Time Scaling in Mathematical Reasoning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.17407.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60d3e619b8448e1785bbda2a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60d3e619b8448e1785bbda2a/q2re5u1HNwsCCyIMtid_I.jpeg",
      "fullname": "GUIJIN SON",
      "name": "amphora",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 43
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.17110",
      "authors": [
        {
          "_id": "67bd3936daef22cbce6d7ef2",
          "name": "Junyang Wang",
          "hidden": false
        },
        {
          "_id": "67bd3936daef22cbce6d7ef3",
          "user": {
            "_id": "645b10e80c73ea27d13f7aca",
            "avatarUrl": "/avatars/95e565306472a15067440b5b43e07a6f.svg",
            "isPro": false,
            "fullname": "xuhaiyang",
            "user": "xhyandwyy",
            "type": "user"
          },
          "name": "Haiyang Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-25T09:39:41.528Z",
          "hidden": false
        },
        {
          "_id": "67bd3936daef22cbce6d7ef4",
          "name": "Xi Zhang",
          "hidden": false
        },
        {
          "_id": "67bd3936daef22cbce6d7ef5",
          "name": "Ming Yan",
          "hidden": false
        },
        {
          "_id": "67bd3936daef22cbce6d7ef6",
          "name": "Ji Zhang",
          "hidden": false
        },
        {
          "_id": "67bd3936daef22cbce6d7ef7",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "67bd3936daef22cbce6d7ef8",
          "name": "Jitao Sang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-24T12:51:23.000Z",
      "title": "Mobile-Agent-V : Apprentissage de l'action de dispositifs mobiles par collaboration de plusieurs agents dans un scénario vidéo",
      "summary": "Avec l'augmentation de l'utilisation de dispositifs mobiles, il est nécessaire d'améliorer continuellement la gestion des tâches. Cependant, de nombreux cadres de travail dirigés par l'IA rencontrent des défis en raison de la manque de connaissances opérationnelles. Le savoir manuel peut être utile, mais coûteux et inefficace. Pour résoudre ces problèmes, on présente le cadre de travail Mobile-Agent-V. Ce cadre de travail fournit des connaissances opérationnelles pour l'automatisation mobile riches et coûte-efficaces en utilisant des guides vidéo. Mobile-Agent-V ne nécessite pas de formation ou de pré-traitement et améliore la capacité d'exécution de tâches en utilisant des entrées vidéo. Il adopte une stratégie de fenêtres glissantes et combine des agents vidéo avec des agents de réflexion profonde, garantissant que les actions correspondent aux instructions du utilisateur. Avec cette méthodologie innovante, les utilisateurs peuvent recevoir des guides pour enregistrer des processus de travail et le système peut apprendre automatiquement et exécuter des tâches de manière efficace. Les résultats des tests montrent que Mobile-Agent-V atteint un augmentation de 30% de rendement par rapport aux autres cadres de travail actuels.",
      "upvotes": 8,
      "discussionId": "67bd3938daef22cbce6d7f9d"
    },
    "publishedAt": "2025-02-24T22:31:17.771Z",
    "title": "Mobile-Agent-V: Learning Mobile Device Operation Through Video-Guided Multi-Agent Collaboration",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/645b10e80c73ea27d13f7aca/mshxtP77rrnN07f6ux6_0.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.17110.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645b10e80c73ea27d13f7aca",
      "avatarUrl": "/avatars/95e565306472a15067440b5b43e07a6f.svg",
      "fullname": "xuhaiyang",
      "name": "xhyandwyy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.16922",
      "authors": [
        {
          "_id": "67bd3d6b60186d7478467208",
          "user": {
            "_id": "6643261b8876db14227eeb19",
            "avatarUrl": "/avatars/67428c9e37a2273697c0547e1783ec6b.svg",
            "isPro": false,
            "fullname": "Zhenglin Wang",
            "user": "wzl0228",
            "type": "user"
          },
          "name": "Zhenglin Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-25T09:39:15.633Z",
          "hidden": false
        },
        {
          "_id": "67bd3d6b60186d7478467209",
          "name": "Jialong Wu",
          "hidden": false
        },
        {
          "_id": "67bd3d6b60186d747846720a",
          "name": "Pengfei LI",
          "hidden": false
        },
        {
          "_id": "67bd3d6b60186d747846720b",
          "name": "Yong Jiang",
          "hidden": false
        },
        {
          "_id": "67bd3d6b60186d747846720c",
          "name": "Deyu Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-24T07:27:54.000Z",
      "title": "Benchmark de Temps-Élément et de Réseau de Cellules de l'Époque Chinoise",
      "summary": "La raison temporelle est la base cognitive humaine et est importante dans divers applications de la réalité. Le développement récent des grands modèles de langue a démontré des capacités attendues en raisonnement temporel, mais les benchmarks actuels sont principalement construits en se basant sur des règles, avec une profondeur de contexte insuffisante et un éventail limité d'entités temporelles. Pour résoudre ces limitations, nous présentons le Chinese Temporal Inference (CTM). Le CTM est un benchmark conçu pour évaluer la raisonnement temporel dans une large gamme de séquences temporelles en Chine. Le CTM met en avant les relations entre entités croisées, les énoncés de dates en temps réversé, les raisonnements contextuels et fondés culturellement, et offre des évaluations détaillées. A travers des expériences élargies, le CTM clarifie les problèmes et présente des possibilités d'amélioration.",
      "upvotes": 7,
      "discussionId": "67bd3d6c60186d7478467249"
    },
    "publishedAt": "2025-02-24T22:48:30.357Z",
    "title": "Benchmarking Temporal Reasoning and Alignment Across Chinese Dynasties",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.16922.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "644a4fbc2166258fccc664bc",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
      "fullname": "Jialong Wu",
      "name": "callanwu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.15894",
      "authors": [
        {
          "_id": "67bd3bd26faf9f04b2170f61",
          "name": "Min Zhao",
          "hidden": false
        },
        {
          "_id": "67bd3bd26faf9f04b2170f62",
          "name": "Guande He",
          "hidden": false
        },
        {
          "_id": "67bd3bd26faf9f04b2170f63",
          "name": "Yixiao Chen",
          "hidden": false
        },
        {
          "_id": "67bd3bd26faf9f04b2170f64",
          "user": {
            "_id": "64c269a52d73768f07ac266c",
            "avatarUrl": "/avatars/d497a960f8aef6a974907b68ed750c1c.svg",
            "isPro": false,
            "fullname": "Zhu Hongzhou",
            "user": "zhuhz22",
            "type": "user"
          },
          "name": "Hongzhou Zhu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-25T09:39:23.502Z",
          "hidden": false
        },
        {
          "_id": "67bd3bd26faf9f04b2170f65",
          "name": "Chongxuan Li",
          "hidden": false
        },
        {
          "_id": "67bd3bd26faf9f04b2170f66",
          "name": "Jun Zhu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-21T19:28:05.000Z",
      "title": "RIFLEx : Démarrage Gratuit pour l'Estimation de Longueur dans la Division d'Images par Transformeurs",
      "summary": "L'avancement récent dans la génération d'images a permis à des modèles de synthétiser des images de haute qualité en un minute. Cependant, la génération d'images longues et le maintien de la continuité temporelle constituent un grand défi, et les méthodes actuelles d'estimation de longueur rencontrent des problèmes en raison de la récréation temporelle ou de la lenteur du mouvement. Dans cet article, on analyse systématiquement le rôle des composantes de fréquence de la localisation et on identifie une fréquence unique qui principalement contrôle l'action d'estimation de longueur. En se basant sur cette observation, on propose une approximation minimale et efficace appelée RIFLEx. RIFLEx réduit la fréquence unique pour maintenir la cohérence du mouvement tout en inhibant la récréation, sans nécessité de changements supplémentaires. RIFLEx effectue une estimation de longueur de qualité maximale en double grâce à un entraînement complet avec les transformateurs de diffusion d'images les plus récents. De plus, avec un ajustement minimal, il est possible d'atteindre une estimation de longueur trois fois plus longue, sans nécessité d'images longues. Le site web du projet et le code sont disponibles sur l'URL fournie.",
      "upvotes": 6,
      "discussionId": "67bd3bd66faf9f04b21710d1"
    },
    "publishedAt": "2025-02-25T00:09:04.483Z",
    "title": "RIFLEx: A Free Lunch for Length Extrapolation in Video Diffusion Transformers",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15894.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6205
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.16707",
      "authors": [
        {
          "_id": "67bd3bcc797e4d53ce0bc70d",
          "user": {
            "_id": "64f8fbd95515d7dcceb906b1",
            "avatarUrl": "/avatars/1c7d034de408930b166592465e65fc31.svg",
            "isPro": false,
            "fullname": "Yunhai Feng",
            "user": "yunhaif",
            "type": "user"
          },
          "name": "Yunhai Feng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-25T09:39:31.085Z",
          "hidden": false
        },
        {
          "_id": "67bd3bcc797e4d53ce0bc70e",
          "user": {
            "_id": "62318c0386753f5f41d0e261",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62318c0386753f5f41d0e261/xO_5PvOf7lXhQPnQLcmnq.jpeg",
            "isPro": false,
            "fullname": "Jiaming Han",
            "user": "csuhan",
            "type": "user"
          },
          "name": "Jiaming Han",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-25T09:39:28.772Z",
          "hidden": false
        },
        {
          "_id": "67bd3bcc797e4d53ce0bc70f",
          "name": "Zhuoran Yang",
          "hidden": false
        },
        {
          "_id": "67bd3bcc797e4d53ce0bc710",
          "name": "Xiangyu Yue",
          "hidden": false
        },
        {
          "_id": "67bd3bcc797e4d53ce0bc711",
          "name": "Sergey Levine",
          "hidden": false
        },
        {
          "_id": "67bd3bcc797e4d53ce0bc712",
          "name": "Jianlan Luo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-23T20:42:15.000Z",
      "title": "Replicable Planning : Modèles Vision-Langage pour les Opérations Robotiques à Long Terme à Plusieurs Étapes",
      "summary": "Pour résoudre les problèmes complexes et à long terme de manipulation de robots, des compétences de planification élevées, une compréhension du monde physique et des capacités de prédiction à long terme sont nécessaires pour aborder l'accumulation d'erreurs prédictives à long terme. Les modèles de langue visuelle (VLMs) ont été entraînés précédemment avec des données de l'internet, ce qui les permet de fournir un cadre pour résoudre ces problèmes. Cependant, dans leur forme actuelle, les VLMs ne comprennent pas les détails physiques complexes nécessaires pour la manipulation automatique ni ne disposent de capacités de prédiction à long terme pour aborder l'accumulation d'erreurs prédictives à long terme. Dans cet article, nous présentons un nouveau cadre de calcul pour renforcer la compréhension des modèles de VLMs, permettant de résoudre des tâches de manipulation multi-étapes. L'approche centrale est une approche qui utilise la structure \"reflexive\" pour améliorer récurrentement les VLMs entraînés. Cette approche utilise des modèles génératifs qui imaginent l'avenir, guidant les décisions d'action basées sur ces prédictions et améliorant critiquement toute inadéquation potentielle. Les résultats des expérimentations montrent que notre approche dépasse considérablement les VLMs commerciales avancés et les approches de post-entraînement comme la recherche d'arbres de Monte Carlo (MCTS). Vous pouvez voir le vidéo sur https://reflect-vlm.github.io.",
      "upvotes": 5,
      "discussionId": "67bd3bcf797e4d53ce0bc7ff"
    },
    "publishedAt": "2025-02-25T01:02:05.395Z",
    "title": "Reflective Planning: Vision-Language Models for Multi-Stage Long-Horizon Robotic Manipulation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.16707.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64f8cb8ed04a890f5380d9a4",
      "avatarUrl": "/avatars/d6fdfdbb0c10141aa3b4c832d928121b.svg",
      "fullname": "Jianlan Luo",
      "name": "jianlanluo",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.15987",
      "authors": [
        {
          "_id": "67bd46ea3e090b402d70f1f4",
          "user": {
            "_id": "64dfbcb18e2084e1d7b51b46",
            "avatarUrl": "/avatars/fafe30beea2d7e8eec3f3ba985c582f7.svg",
            "isPro": false,
            "fullname": "Kushal Raj Bhandari",
            "user": "KBhandari11",
            "type": "user"
          },
          "name": "Kushal Raj Bhandari",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-02-25T04:30:32.676Z",
          "hidden": false
        },
        {
          "_id": "67bd46ea3e090b402d70f1f5",
          "name": "Pin-Yu Chen",
          "hidden": false
        },
        {
          "_id": "67bd46ea3e090b402d70f1f6",
          "name": "Jianxi Gao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-21T22:52:19.000Z",
      "title": "Prévision du développement des modèles d'IA ouverts de Hugging Face",
      "summary": "Le développement de la capacité de poids ouverts dans le tableau d'échelle de l'IA, conjointement avec le développement de la recherche sur les modèles, l'investissement massive et l'augmentation de l'intérêt des utilisateurs, a conduit à ce que l'innovation et l'écosystème de l'IA soient essentiellement dirigées par les modèles. Un cadre est proposé pour quantifier comment évolue l'impact des modèles de poids ouverts, en se basant sur la tendance des citations et la similitude de la littérature scientifique. En particulier, trois facteurs sont utilisés : l'immediacité, la longévité et la fitness relative, appliqués à un modèle de citation scientifique introduit par Wang et al. pour suivre l'accumulation des modèles de micro-ajustement des modèles de poids ouverts. Nos résultats montrent que l'approche des citations est efficace pour comprendre les différentes voies d'adoption des modèles de poids ouverts, que la plupart des modèles sont adaptés à ces voies et que les auteurs présentent des patrons caractéristiques ou un rapide accroissement d'utilisation.",
      "upvotes": 4,
      "discussionId": "67bd46ee3e090b402d70f317"
    },
    "publishedAt": "2025-02-24T23:30:36.556Z",
    "title": "Forecasting Open-Weight AI Model Growth on Hugging Face",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/5e67bdd61009063689407479/kQHArNjaT0CM1KCujtDc1.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15987.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "5e67bdd61009063689407479",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1583857146757-5e67bdd61009063689407479.jpeg",
      "fullname": "Clem 🤗",
      "name": "clem",
      "type": "user",
      "isPro": true,
      "isHf": true,
      "isMod": false,
      "followerCount": 2052
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.16701",
      "authors": [
        {
          "_id": "67bd31d6bf6d46017e515a58",
          "user": {
            "_id": "62543749b777cd32720675c2",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1658760912583-62543749b777cd32720675c2.jpeg",
            "isPro": false,
            "fullname": "Irene Solaiman",
            "user": "irenesolaiman",
            "type": "user"
          },
          "name": "Irene Solaiman",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-02-25T03:43:21.348Z",
          "hidden": false
        },
        {
          "_id": "67bd31d6bf6d46017e515a59",
          "name": "Rishi Bommasani",
          "hidden": false
        },
        {
          "_id": "67bd31d6bf6d46017e515a5a",
          "name": "Dan Hendrycks",
          "hidden": false
        },
        {
          "_id": "67bd31d6bf6d46017e515a5b",
          "name": "Ariel Herbert-Voss",
          "hidden": false
        },
        {
          "_id": "67bd31d6bf6d46017e515a5c",
          "name": "Yacine Jernite",
          "hidden": false
        },
        {
          "_id": "67bd31d6bf6d46017e515a5d",
          "name": "Aviya Skowron",
          "hidden": false
        },
        {
          "_id": "67bd31d6bf6d46017e515a5e",
          "name": "Andrew Trask",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-23T20:06:12.000Z",
      "title": "Au-delà de la Libération : Révision de l'Accès aux Systèmes d'IA de Génération",
      "summary": "La décision de lancement d'un IA générative se fonde sur la détermination des disponibilités des composants du système pour son utilisation, mais le lancement ne peut pas traiter les variations de multiples éléments du système qui modifient la façon dont l'utilisateur collabore avec le système. Au lieu de cela, l'accessibilité aux composants du système fournit des informations sur les risques et les avantages potentiels. L'accessibilité comprend les exigences pratiques, l'infrastructure, les techniques et les conditions sociales nécessaires pour l'utilisation des composants. L'accessibilité est divisée en trois axes : les ressources, la disponibilité technique et l'utilité. Dans chaque catégorie, des échelles de configuration sont utilisées pour expliquer la perte. Par exemple, les ressources nécessitent l'accès à l'infrastructure de calcul pour stocker les poids du modèle sur un serveur. De plus, les accessibilités de quatre modèles de langage de haute efficacité, incluant deux modèles ouverts et deux modèles fermés, sont comparées, et les mêmes critères sont appliqués à tous les modèles. Les variables d'accessibilité sont la base pour créer des structures qui peuvent étendre ou améliorer l'accessibilité pour les utilisateurs, et sont étudiées comment l'échelle d'accessibilité affecte la gestion des risques et la mitigation des risques. Ce cadre vise à améliorer la compréhension des risques et des avantages du lancement du système, et fournit des informations pour la décision de lancement du système, la recherche et les politiques.",
      "upvotes": 4,
      "discussionId": "67bd31d7bf6d46017e515a7e"
    },
    "publishedAt": "2025-02-24T21:59:15.571Z",
    "title": "Beyond Release: Access Considerations for Generative AI Systems",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62543749b777cd32720675c2/LwZmJUoXiJriC_c1DZ7qM.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.16701.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62543749b777cd32720675c2",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1658760912583-62543749b777cd32720675c2.jpeg",
      "fullname": "Irene Solaiman",
      "name": "irenesolaiman",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 79
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.17258",
      "authors": [
        {
          "_id": "67bd515c0417e7f92283d3b8",
          "name": "Xiangpeng Yang",
          "hidden": false
        },
        {
          "_id": "67bd515c0417e7f92283d3b9",
          "name": "Linchao Zhu",
          "hidden": false
        },
        {
          "_id": "67bd515c0417e7f92283d3ba",
          "name": "Hehe Fan",
          "hidden": false
        },
        {
          "_id": "67bd515c0417e7f92283d3bb",
          "name": "Yi Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-24T15:39:14.000Z",
      "title": "VideoGrain : Régulation de l'attention en temps et en espace pour l'édition vidéo multigranulaire",
      "summary": "Le développement récent des modèles Diffusion a considérablement amélioré la capacité de génération et d'édition de films. Cependant, l'édition de films à plusieurs niveaux (classe, instance, partie) représente un défi complexe. Les principaux défis de l'édition multi-granularité résident dans l'ajustement significatif du contrôle des régions et la combinaison de caractéristiques au sein du modèle Diffusion. Pour aborder ces défis, nous proposons VideoGrain. VideoGrain est une approche 0-shot qui permet un contrôle précis sur le contenu des films par la régulation des institutions d'attention spatio-temporelles (croisée et auto). Nous nous efforçons de renforcer le contrôle des régions et de minimiser l'interaction entre régions non pertinentes dans l'attention croisée. De plus, nous réduisons l'interférence entre régions dans l'attention auto et nous renforçons le reconnaissance au sein des régions. Des expériences extensives montrent que notre méthode atteint les meilleurs résultats dans des scénarios réels. Notre code, données et démo sont disponibles sur https://knightyxp.github.io/VideoGrain_project_page/.",
      "upvotes": 3,
      "discussionId": "67bd51620417e7f92283d4e9"
    },
    "publishedAt": "2025-02-25T00:13:12.214Z",
    "title": "VideoGrain: Modulating Space-Time Attention for Multi-grained Video Editing",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.17258.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6205
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.14132",
      "authors": [
        {
          "_id": "67b86819d00e69f10c1f31b9",
          "user": {
            "_id": "6231d3ce86753f5f41d39c6f",
            "avatarUrl": "/avatars/9b18f368e5f80cfc935b2e339d42a85f.svg",
            "isPro": false,
            "fullname": "Nadav Borenstein",
            "user": "Nadav",
            "type": "user"
          },
          "name": "Nadav Borenstein",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-25T09:40:52.278Z",
          "hidden": false
        },
        {
          "_id": "67b86819d00e69f10c1f31ba",
          "user": {
            "_id": "6698cffdb2ebada9f4a7e7d7",
            "avatarUrl": "/avatars/e66d946c14595d3b008185f2be8d2f57.svg",
            "isPro": false,
            "fullname": "Greta Warren",
            "user": "gretawarren",
            "type": "user"
          },
          "name": "Greta Warren",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-21T14:42:45.791Z",
          "hidden": false
        },
        {
          "_id": "67b86819d00e69f10c1f31bb",
          "name": "Desmond Elliott",
          "hidden": false
        },
        {
          "_id": "67b86819d00e69f10c1f31bc",
          "name": "Isabelle Augenstein",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-19T22:26:39.000Z",
      "title": "La communauté peut-elle remplacer les vérificateurs professionnels de faits ?",
      "summary": "Dans les réseaux sociaux, deux stratégies couramment utilisées pour prévenir la propagation des rumeurs sont : (i) la vérification des faits effectuée par des organisations professionnelles et (ii) la moderation communautaire effectuée par les utilisateurs du plateforme. Les changements dans les politiques de Twitter/X et Meta ont évolué vers une dépendance plus grande de la collaboration avec des groupes de vérification des faits, grâce à des associations avec eux. Cependant, la nature et le degré de cette dépendance entre la vérification des faits et la moderation communautaire ne sont pas clairs. Pour aborder ce problème, un grand corpus de Twitter/X a été enregistré en utilisant des modèles de langage pour classifier des propriétés telles que la négation d'affirmations liées à des explications larges des rumeurs, basées sur des thèmes, des sources citées et des notes qui référent à des explications larges des rumeurs. Les résultats de l'analyse montrent que les notes communautaires citent des sources de vérification des faits cinq fois plus souvent que dans les rapports précédents, et que les notes de vérification des faits sont particulièrement importantes dans les posts qui contiennent des explications larges, référençant des sources de vérification des faits avec deux fois plus de probabilité que d'autres sources. En conclusion, nos résultats montrent que une bonne moderation communautaire dépend fortement de la vérification des faits professionnelles.",
      "upvotes": 2,
      "discussionId": "67b8681bd00e69f10c1f3267"
    },
    "publishedAt": "2025-02-25T04:11:18.915Z",
    "title": "Can Community Notes Replace Professional Fact-Checkers?",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6231d3ce86753f5f41d39c6f/CwWaf1c9-jOzJ-gD5lvCH.jpeg",
      "https://cdn-uploads.huggingface.co/production/uploads/6231d3ce86753f5f41d39c6f/WrrBClUkuDsXHcfxP_N8B.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14132.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6231d3ce86753f5f41d39c6f",
      "avatarUrl": "/avatars/9b18f368e5f80cfc935b2e339d42a85f.svg",
      "fullname": "Nadav Borenstein",
      "name": "Nadav",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.15122",
      "authors": [
        {
          "_id": "67bbd6d5ba0bb31293e11210",
          "user": {
            "_id": "675f68e3074ff89c5c078bf3",
            "avatarUrl": "/avatars/e3b78d90f032659d411761f47c3cf43e.svg",
            "isPro": false,
            "fullname": "Angus",
            "user": "angus924",
            "type": "user"
          },
          "name": "Angus Dempster",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-02-24T02:18:57.914Z",
          "hidden": false
        },
        {
          "_id": "67bbd6d5ba0bb31293e11211",
          "name": "Navid Mohammadi Foumani",
          "hidden": false
        },
        {
          "_id": "67bbd6d5ba0bb31293e11212",
          "name": "Chang Wei Tan",
          "hidden": false
        },
        {
          "_id": "67bbd6d5ba0bb31293e11213",
          "name": "Lynn Miller",
          "hidden": false
        },
        {
          "_id": "67bbd6d5ba0bb31293e11214",
          "name": "Amish Mishra",
          "hidden": false
        },
        {
          "_id": "67bbd6d5ba0bb31293e11215",
          "name": "Mahsa Salehi",
          "hidden": false
        },
        {
          "_id": "67bbd6d5ba0bb31293e11216",
          "name": "Charlotte Pelletier",
          "hidden": false
        },
        {
          "_id": "67bbd6d5ba0bb31293e11217",
          "name": "Daniel F. Schmidt",
          "hidden": false
        },
        {
          "_id": "67bbd6d5ba0bb31293e11218",
          "name": "Geoffrey I. Webb",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-21T00:54:40.000Z",
      "title": "Monstruo : Monstre évoluant dans le temps série évaluation dépôt",
      "summary": "Monster (MONSTER) - Repositoire d'évaluation de la série temporelle de l'école de Monash Scalable Time Series Series - Présente la récolte d'un grand ensemble de données dans le domaine de la classification de séries temporelles. Ce domaine reçoit une rétroalimentation selon les cadres de référence établis par UCR et UEA. Cependant, les ensembles de données inclus dans ces cadres de référence sont petits, avec des valeurs moyennes de 217 et 255. Cela tend à réduire l'espace des modèles qui peuvent atteindre des erreurs de classification faibles sur une large variété de petits ensembles de données. Cette tendance est attribuée à la priorité accordée aux modèles qui minimisent la variation et qui ne se concentrent pas suffisamment sur des aspects tels que l'échellabilité et les problèmes de calcul. Notre espérance est de construire ces cadres de référence avec de grands ensembles de données pour diversifier le domaine. Nous participons aux défis théoriques et pratiques de l'entraînement de modèles avec de grands ensembles de données, en croyant dans le grand potentiel de nouveaux développements pour le domaine.",
      "upvotes": 2,
      "discussionId": "67bbd6d6ba0bb31293e11258"
    },
    "publishedAt": "2025-02-25T00:37:53.138Z",
    "title": "MONSTER: Monash Scalable Time Series Evaluation Repository",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15122.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "675f68e3074ff89c5c078bf3",
      "avatarUrl": "/avatars/e3b78d90f032659d411761f47c3cf43e.svg",
      "fullname": "Angus",
      "name": "angus924",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.17414",
      "authors": [
        {
          "_id": "67bd526001d5bfa0abfcc5ba",
          "name": "Zeyuan Chen",
          "hidden": false
        },
        {
          "_id": "67bd526001d5bfa0abfcc5bb",
          "name": "Hongyi Xu",
          "hidden": false
        },
        {
          "_id": "67bd526001d5bfa0abfcc5bc",
          "name": "Guoxian Song",
          "hidden": false
        },
        {
          "_id": "67bd526001d5bfa0abfcc5bd",
          "name": "You Xie",
          "hidden": false
        },
        {
          "_id": "67bd526001d5bfa0abfcc5be",
          "name": "Chenxu Zhang",
          "hidden": false
        },
        {
          "_id": "67bd526001d5bfa0abfcc5bf",
          "name": "Xin Chen",
          "hidden": false
        },
        {
          "_id": "67bd526001d5bfa0abfcc5c0",
          "name": "Chao Wang",
          "hidden": false
        },
        {
          "_id": "67bd526001d5bfa0abfcc5c1",
          "name": "Di Chang",
          "hidden": false
        },
        {
          "_id": "67bd526001d5bfa0abfcc5c2",
          "name": "Linjie Luo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-24T18:47:54.000Z",
      "title": "X-Dancer : Génération de vidéos de musique d'expression et de danse humaine",
      "summary": "X-Dancer est une nouvelle technologie de pipeline d'animation d'images de danse musicales qui génère des vidéos de danse de personnes vivantes de longue durée à partir d'une seule image fixe. Son essence réside dans un seul canal de dispersion intégré, où un modèle de rétroaction automatique est appliqué pour synthétiser une séquence de termes de positions du corps, de la tête et des mains, étendues et motivées par la musique. Ces séquences sont utilisées via un modèle de dispersion pour générer des images de danse réalistes. Au contraire des méthodes existantes, X-Dancer ne se concentre pas sur la génération de mouvements 3D du corps humain, mais sur l'aborder les limites des données, modéliser un large éventail de mouvements 2D de danse et ajuster avec précision le rythme de la musique dans des vidéos monochromes, améliorant la qualité de l'échelle. Pour y parvenir, une représentation spatiale structurée des termes est construite en combinant des étiquettes de positions 2D du corps humain et la confiance des points clés. Un modèle de canaux est conçu pour transformer la musique en actions, une séquence de termes de positions de danse motivée par la musique est générée automatiquement, et une attention globale est appliquée pour inclure le style de la musique et le contexte des actions précédentes. Enfin, ce terme de position synthétisé est ajouté à l'Image de Référence pour compléter un cadre final entièrement différenciable. Les résultats des tests montrent que X-Dancer peut générer des vidéos de danse avec diverses caractéristiques, surpassant significativement les méthodes les plus avancées en termes de diversité, expression et réalisme. Le code et les modèles sont disponibles pour la recherche.",
      "upvotes": 2,
      "discussionId": "67bd526101d5bfa0abfcc62c"
    },
    "publishedAt": "2025-02-25T00:17:51.431Z",
    "title": "X-Dancer: Expressive Music to Human Dance Video Generation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.17414.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6205
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.13074",
      "authors": [
        {
          "_id": "67bd8759fdecc637bd621e6b",
          "name": "Omer Angel",
          "hidden": false
        },
        {
          "_id": "67bd8759fdecc637bd621e6c",
          "name": "Emmanuel Jacob",
          "hidden": false
        },
        {
          "_id": "67bd8759fdecc637bd621e6d",
          "name": "Brett Kolesnik",
          "hidden": false
        },
        {
          "_id": "67bd8759fdecc637bd621e6e",
          "name": "Grégory Miermont",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-18T17:21:44.000Z",
      "title": "La serpente dans le cerveau de Brown",
      "summary": "La Broxñar espefia est un phénomène qui apparaît comme le limite d'échelle de plusieurs types de cartes plans aléatoires dans des espaces bidimensionnels et homogènes. La structure directe de la Broxñar espefia est réalisée par une correspondance directe ANALOGIQUE avec la correspondance Cori-Vauquelin-Schaeffer (CVS). Cette correspondance CVS permet d'associer des arbres standardisés à des cartes planes et est utilisée pour associer les arbres aléatoires continus dans le temps (Broxñar espefia) à la Broxñar espefia. Dans cet article, on explique comment construire la Broxñar espefia comme une fonction mesurable qui représente la Broxñar espefia, pour expliquer l'inverse de la correspondance CVS continue. Il est important de prendre en compte le traitement de la direction dans la Broxñar espefia.",
      "upvotes": 0,
      "discussionId": "67bd875afdecc637bd621e95"
    },
    "publishedAt": "2025-02-25T04:03:39.758Z",
    "title": "The snake in the Brownian sphere",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13074.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "636d12455aaed143cd665607",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1679399015950-636d12455aaed143cd665607.png",
      "fullname": "ZLW",
      "name": "ZarkLngeW",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.15167",
      "authors": [
        {
          "_id": "67bc7ea06f88ef9a2b8283d3",
          "name": "Chuan Cui",
          "hidden": false
        },
        {
          "_id": "67bc7ea06f88ef9a2b8283d4",
          "name": "Kejiang Chen",
          "hidden": false
        },
        {
          "_id": "67bc7ea06f88ef9a2b8283d5",
          "name": "Zhihua Wei",
          "hidden": false
        },
        {
          "_id": "67bc7ea06f88ef9a2b8283d6",
          "name": "Wen Shen",
          "hidden": false
        },
        {
          "_id": "67bc7ea06f88ef9a2b8283d7",
          "name": "Weiming Zhang",
          "hidden": false
        },
        {
          "_id": "67bc7ea06f88ef9a2b8283d8",
          "name": "Nenghai Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-21T03:05:45.000Z",
      "title": "M3-AGIQA : Évaluation de la qualité des images générées par l'IA avec vision multimodale, multiroutinière et multiaspectueuse",
      "summary": "Le rapide développement de modèles d'images générés par l'IA (AGI) génère des problèmes importants dans l'évaluation de leur qualité. Pour résoudre ces problèmes, nous proposons M3-AGIQA, un cadre d'évaluation qui effectue des évaluations multimodales, multitournées et multiaspects. Notre approche utilise les capacités de modèles grands de langage multimodal (MLLMs), fonctionnant avec un encodeur de texte et d'image, et intègre la haute capacité de capture des MLLMs dans des modèles locaux grâce à l'Adaptation de Bas Rang (LoRA). Ce cadre inclut une structure d'évaluation structurée, générant des descriptions d'images intermédiaires et offrant des perspectives profondes sur la qualité, la correspondance et l'accessibilité. Pour prédire les jugements visuels humains, nous utilisons un prédicteur composé d'un xLSTM et d'une tête de régression, qui traitent la logique de séquence et prédisent des scores de Notre Opinion Médiane (MOS). Ce cadre a été étendu sur plusieurs ensembles de données de test et a démontré un excellent rendement de généralisation lors des tests de validation croisée. Le code est disponible sur https://github.com/strawhatboy/M3-AGIQA.",
      "upvotes": 0,
      "discussionId": "67bc7ea26f88ef9a2b828473"
    },
    "publishedAt": "2025-02-25T03:36:50.480Z",
    "title": "M3-AGIQA: Multimodal, Multi-Round, Multi-Aspect AI-Generated Image Quality Assessment",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15167.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5f1158120c833276f61f1a84",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
      "fullname": "Niels Rogge",
      "name": "nielsr",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 771
    },
    "isAuthorParticipating": false
  }
]