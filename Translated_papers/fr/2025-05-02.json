[
  {
    "paper": {
      "id": "2504.21853",
      "authors": [
        {
          "_id": "681441e64d6a681c7c840b1f",
          "name": "Jiwen Yu",
          "hidden": false
        },
        {
          "_id": "681441e64d6a681c7c840b20",
          "name": "Yiran Qin",
          "hidden": false
        },
        {
          "_id": "681441e64d6a681c7c840b21",
          "name": "Haoxuan Che",
          "hidden": false
        },
        {
          "_id": "681441e64d6a681c7c840b22",
          "name": "Quande Liu",
          "hidden": false
        },
        {
          "_id": "681441e64d6a681c7c840b23",
          "name": "Xintao Wang",
          "hidden": false
        },
        {
          "_id": "681441e64d6a681c7c840b24",
          "name": "Pengfei Wan",
          "hidden": false
        },
        {
          "_id": "681441e64d6a681c7c840b25",
          "name": "Di Zhang",
          "hidden": false
        },
        {
          "_id": "681441e64d6a681c7c840b26",
          "name": "Kun Gai",
          "hidden": false
        },
        {
          "_id": "681441e64d6a681c7c840b27",
          "name": "Hao Chen",
          "hidden": false
        },
        {
          "_id": "681441e64d6a681c7c840b28",
          "name": "Xihui Liu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64105a6d14215c0775dfdd14/Y9dvfNIIrOJ4rCzwIB6XI.jpeg",
        "https://cdn-uploads.huggingface.co/production/uploads/64105a6d14215c0775dfdd14/5kPNXDj9LIsgShhFz5WSg.jpeg"
      ],
      "publishedAt": "2025-04-30T17:59:02.000Z",
      "submittedOnDailyAt": "2025-05-02T02:29:37.187Z",
      "title": "Interface Generation Video Investigation",
      "submittedOnDailyBy": {
        "_id": "64105a6d14215c0775dfdd14",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64105a6d14215c0775dfdd14/-VX-cUYOLjHIg7QnWhRGG.jpeg",
        "isPro": false,
        "fullname": "Jiwen Yu",
        "user": "VictorYuki",
        "type": "user"
      },
      "summary": "L'IGV (Vidéo Générative Interactive) a émergé comme une technologie importante en raison de l'augmentation de la demande en contenu vidéo interactif de haute qualité. Dans cet article, l'IGV est définie comme une technologie qui, en plus de sa capacité à générer, offre des fonctions interactives pour créer divers contenus vidéo de haute qualité et encourager la participation de l'utilisateur. L'état actuel des applications de l'IGV est examiné, avec une attention particulière sur trois domaines principaux : les jeux, l'IA de spécification et la conduite autonome. Dans les jeux, l'IGV permet d'explorer infinitément le monde virtuel. Dans l'IA de spécification, on réalise la synthèse d'environnements dynamiques qui facilitent diverses interactions. Dans la conduite autonome, on offre une fonction de simulation de traitement postérieur fermée pour des tests et validations cruciales pour la sécurité. Pour guider le développement futur, on propose un cadre détaillé qui divise un système idéal d'IGV en cinq modules basiques : génération, contrôle, mémoire, dynamique et intelligence. De plus, on analyse systématiquement les problèmes techniques et les directions futures pour chaque composant d'un système idéal d'IGV. Cette analyse systématique est attendue encourager la recherche et le développement dans le domaine de l'IGV et aider à développer des applications plus complexes et pratiques.",
      "upvotes": 24,
      "discussionId": "681441e84d6a681c7c840bae",
      "ai_keywords": [
        "generative capabilities",
        "interactive features",
        "control signals",
        "responsive feedback",
        "virtual worlds",
        "physics-aware environment synthesizer",
        "multimodal interaction",
        "dynamically evolving scenes",
        "closed-loop simulation",
        "safety-critical testing",
        "validation",
        "real-time generation",
        "open-domain control",
        "long-term coherence",
        "accurate physics",
        "causal reasoning"
      ]
    },
    "publishedAt": "2025-04-30T13:59:02.000Z",
    "title": "A Survey of Interactive Generative Video",
    "summary": "Interactive Generative Video (IGV) has emerged as a crucial technology in\nresponse to the growing demand for high-quality, interactive video content\nacross various domains. In this paper, we define IGV as a technology that\ncombines generative capabilities to produce diverse high-quality video content\nwith interactive features that enable user engagement through control signals\nand responsive feedback. We survey the current landscape of IGV applications,\nfocusing on three major domains: 1) gaming, where IGV enables infinite\nexploration in virtual worlds; 2) embodied AI, where IGV serves as a\nphysics-aware environment synthesizer for training agents in multimodal\ninteraction with dynamically evolving scenes; and 3) autonomous driving, where\nIGV provides closed-loop simulation capabilities for safety-critical testing\nand validation. To guide future development, we propose a comprehensive\nframework that decomposes an ideal IGV system into five essential modules:\nGeneration, Control, Memory, Dynamics, and Intelligence. Furthermore, we\nsystematically analyze the technical challenges and future directions in\nrealizing each component for an ideal IGV system, such as achieving real-time\ngeneration, enabling open-domain control, maintaining long-term coherence,\nsimulating accurate physics, and integrating causal reasoning. We believe that\nthis systematic analysis will facilitate future research and development in the\nfield of IGV, ultimately advancing the technology toward more sophisticated and\npractical applications.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64105a6d14215c0775dfdd14/Y9dvfNIIrOJ4rCzwIB6XI.jpeg",
      "https://cdn-uploads.huggingface.co/production/uploads/64105a6d14215c0775dfdd14/5kPNXDj9LIsgShhFz5WSg.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.21853.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64105a6d14215c0775dfdd14",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64105a6d14215c0775dfdd14/-VX-cUYOLjHIg7QnWhRGG.jpeg",
      "fullname": "Jiwen Yu",
      "name": "VictorYuki",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.00662",
      "authors": [
        {
          "_id": "68142e4a551709da9244e8d1",
          "user": {
            "_id": "64b7df742f5a966b973e25f7",
            "avatarUrl": "/avatars/e24e7769188d441317b3b7d10ef8fd60.svg",
            "isPro": false,
            "fullname": "Wenkai Yang",
            "user": "Keven16",
            "type": "user"
          },
          "name": "Wenkai Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-02T06:34:18.531Z",
          "hidden": false
        },
        {
          "_id": "68142e4a551709da9244e8d2",
          "name": "Jingwen Chen",
          "hidden": false
        },
        {
          "_id": "68142e4a551709da9244e8d3",
          "name": "Yankai Lin",
          "hidden": false
        },
        {
          "_id": "68142e4a551709da9244e8d4",
          "name": "Ji-Rong Wen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-01T17:03:17.000Z",
      "submittedOnDailyAt": "2025-05-02T01:12:33.949Z",
      "title": "DeepCritic : Critique avec prudence contre le modèle de langage.",
      "submittedOnDailyBy": {
        "_id": "64b7df742f5a966b973e25f7",
        "avatarUrl": "/avatars/e24e7769188d441317b3b7d10ef8fd60.svg",
        "isPro": false,
        "fullname": "Wenkai Yang",
        "user": "Keven16",
        "type": "user"
      },
      "summary": "Le rapide développement des LLM a conduit à ce que la fourniture de rétroaction précise et de surveillance échelonnable deviennent une question urgente et importante. La réalisation d'un modèle d'évaluation pour superviser automatiquement les LLM est une solution souhaitable. Dans cette étude, nous nous concentrons sur la recherche et l'amélioration des capacités d'évaluation mathématique des LLM. Les évaluateurs actuels des LLM offrent des évaluations superficielles et superficielles à chaque étape, avec une précision faible et ne fournissent pas de rétroactions suffisantes pour le générateur des LLM. Pour aborder ces problèmes, nous proposons un nouveau cadre efficace de 2 étapes pour développer un évaluateur qui évalue intentionnellement chaque étape logique d'une réponse mathématique. Le premier pas utilise Qwen2.5-72B-Instruct pour générer des évaluations de 4.5K de longueur et de format texte extensif, fournissant des données d'ajustement micro avec rétroaction manuelle. Chaque évaluation comprend une évaluation profonde initiale dans différentes vérifications et étapes logiques. Ensuite, on utilise des données d'étiquetage humain depuis PRM800K et des données de notes automatiques basées sur le sampling Monte Carlo pour estimer la précision, et on effectue l'apprentissage par renforcement pour améliorer le modèle d'ajustement, favorisant la capacité d'évaluation. Le modèle d'évaluation basé sur Qwen2.5-7B-Instruct est notablement supérieur dans le cadre de tests de reconnaissance d'erreurs par rapport aux évaluateurs actuels des LLM (y compris des modèles du même taille que DeepSeek-R1-distill et GPT-4o), et peut aider efficacement à corriger les erreurs du générateur des LLM avec des rétroactions plus détaillées.",
      "upvotes": 21,
      "discussionId": "68142e4b551709da9244e8f8",
      "ai_keywords": [
        "LLMs (Large Language Models)",
        "critique models",
        "automated supervision",
        "math critique ability",
        "supervised fine-tuning",
        "Qwen2.5-72B-Instruct",
        "seed data",
        "deliberate step-wise critiques",
        "multi-perspective verifications",
        "reinforcement learning",
        "PRM800K",
        "Monte Carlo sampling-based correctness estimation",
        "Qwen2.5-7B-Instruct",
        "DeepSeek-R1-distill models",
        "GPT-4o",
        "error identification benchmarks"
      ]
    },
    "publishedAt": "2025-05-01T13:03:17.000Z",
    "title": "DeepCritic: Deliberate Critique with Large Language Models",
    "summary": "As Large Language Models (LLMs) are rapidly evolving, providing accurate\nfeedback and scalable oversight on their outputs becomes an urgent and critical\nproblem. Leveraging LLMs as critique models to achieve automated supervision is\na promising solution. In this work, we focus on studying and enhancing the math\ncritique ability of LLMs. Current LLM critics provide critiques that are too\nshallow and superficial on each step, leading to low judgment accuracy and\nstruggling to offer sufficient feedback for the LLM generator to correct\nmistakes. To tackle this issue, we propose a novel and effective two-stage\nframework to develop LLM critics that are capable of deliberately critiquing on\neach reasoning step of math solutions. In the first stage, we utilize\nQwen2.5-72B-Instruct to generate 4.5K long-form critiques as seed data for\nsupervised fine-tuning. Each seed critique consists of deliberate step-wise\ncritiques that includes multi-perspective verifications as well as in-depth\ncritiques of initial critiques for each reasoning step. Then, we perform\nreinforcement learning on the fine-tuned model with either existing\nhuman-labeled data from PRM800K or our automatically annotated data obtained\nvia Monte Carlo sampling-based correctness estimation, to further incentivize\nits critique ability. Our developed critique model built on Qwen2.5-7B-Instruct\nnot only significantly outperforms existing LLM critics (including the\nsame-sized DeepSeek-R1-distill models and GPT-4o) on various error\nidentification benchmarks, but also more effectively helps the LLM generator\nrefine erroneous steps through more detailed feedback.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.00662.png",
    "numComments": 6,
    "submittedBy": {
      "_id": "64b7df742f5a966b973e25f7",
      "avatarUrl": "/avatars/e24e7769188d441317b3b7d10ef8fd60.svg",
      "fullname": "Wenkai Yang",
      "name": "Keven16",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.00703",
      "authors": [
        {
          "_id": "681428debcdf962d03da2797",
          "name": "Dongzhi Jiang",
          "hidden": false
        },
        {
          "_id": "681428debcdf962d03da2798",
          "name": "Ziyu Guo",
          "hidden": false
        },
        {
          "_id": "681428debcdf962d03da2799",
          "name": "Renrui Zhang",
          "hidden": false
        },
        {
          "_id": "681428debcdf962d03da279a",
          "name": "Zhuofan Zong",
          "hidden": false
        },
        {
          "_id": "681428debcdf962d03da279b",
          "name": "Hao Li",
          "hidden": false
        },
        {
          "_id": "681428debcdf962d03da279c",
          "name": "Le Zhuo",
          "hidden": false
        },
        {
          "_id": "681428debcdf962d03da279d",
          "name": "Shilin Yan",
          "hidden": false
        },
        {
          "_id": "681428debcdf962d03da279e",
          "name": "Pheng-Ann Heng",
          "hidden": false
        },
        {
          "_id": "681428debcdf962d03da279f",
          "name": "Hongsheng Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-01T17:59:46.000Z",
      "submittedOnDailyAt": "2025-05-02T00:38:40.412Z",
      "title": "T2I-R1 : Amélioration de la génération d'images en utilisant des niveaux de signification complexes et des tokens du CoT.",
      "submittedOnDailyBy": {
        "_id": "6349214f8146350b3a4c5cdf",
        "avatarUrl": "/avatars/cfd24caac9a87efb528d0f4c375932bc.svg",
        "isPro": false,
        "fullname": "Dongzhi Jiang",
        "user": "CaraJ",
        "type": "user"
      },
      "summary": "Le développement récent de grands modèles de langue a montré que les stratégies de chain-of-thought (CoT) et l'apprentissage par renforcement (RL) peuvent significativement améliorer le rendement. Cependant, l'application de ces approches logiques à la génération visuelle est principalement exploratoire. Dans cet article, nous présentons un nouveau modèle de texte-image (T2I-R1) qui inclut RL et adopte un processus bi-level CoT logique. Spécifiquement, deux niveaux de CoT sont identifiés pour améliorer la génération à différentes étapes : (1) un CoT au niveau de sens pour planifier de manière élevée le prompt et (2) un CoT au niveau de token pour planifier de manière basse le traitement de chaque patch de l'image. Pour améliorer la coordination de ces deux niveaux de CoT, nous introduisons BiCoT-GRPO et utilisons un ensemble de compensations de génération pour optimiser de manière continue les deux CoT dans le même processus d'entraînement. En appliquant ces stratégies logiques à la base de modèle Janus-Pro, nous avons réussi à améliorer de 13% sur T2I-CompBench et de 19% sur le WISE benchmark, dépassant le modèle leader FLUX. Le code est disponible sur la URL suivante : https://github.com/CaraJ7/T2I-R1",
      "upvotes": 13,
      "discussionId": "681428dfbcdf962d03da281c",
      "githubRepo": "https://github.com/CaraJ7/T2I-R1",
      "ai_keywords": [
        "chain-of-thought (CoT)",
        "reinforcement learning (RL)",
        "text-to-image generation model",
        "bi-level CoT reasoning process",
        "semantic-level CoT",
        "token-level CoT",
        "BiCoT-GRPO",
        "generation rewards",
        "Janus-Pro",
        "T2I-CompBench",
        "WISE benchmark",
        "FLUX"
      ]
    },
    "publishedAt": "2025-05-01T13:59:46.000Z",
    "title": "T2I-R1: Reinforcing Image Generation with Collaborative Semantic-level\n  and Token-level CoT",
    "summary": "Recent advancements in large language models have demonstrated how\nchain-of-thought (CoT) and reinforcement learning (RL) can improve performance.\nHowever, applying such reasoning strategies to the visual generation domain\nremains largely unexplored. In this paper, we present T2I-R1, a novel\nreasoning-enhanced text-to-image generation model, powered by RL with a\nbi-level CoT reasoning process. Specifically, we identify two levels of CoT\nthat can be utilized to enhance different stages of generation: (1) the\nsemantic-level CoT for high-level planning of the prompt and (2) the\ntoken-level CoT for low-level pixel processing during patch-by-patch\ngeneration. To better coordinate these two levels of CoT, we introduce\nBiCoT-GRPO with an ensemble of generation rewards, which seamlessly optimizes\nboth generation CoTs within the same training step. By applying our reasoning\nstrategies to the baseline model, Janus-Pro, we achieve superior performance\nwith 13% improvement on T2I-CompBench and 19% improvement on the WISE\nbenchmark, even surpassing the state-of-the-art model FLUX.1. Code is available\nat: https://github.com/CaraJ7/T2I-R1",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.00703.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6349214f8146350b3a4c5cdf",
      "avatarUrl": "/avatars/cfd24caac9a87efb528d0f4c375932bc.svg",
      "fullname": "Dongzhi Jiang",
      "name": "CaraJ",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.21659",
      "authors": [
        {
          "_id": "68142de6111ccf18a993c890",
          "name": "Haotian Luo",
          "hidden": false
        },
        {
          "_id": "68142de6111ccf18a993c891",
          "name": "Haiying He",
          "hidden": false
        },
        {
          "_id": "68142de6111ccf18a993c892",
          "name": "Yibo Wang",
          "hidden": false
        },
        {
          "_id": "68142de6111ccf18a993c893",
          "name": "Jinluan Yang",
          "hidden": false
        },
        {
          "_id": "68142de6111ccf18a993c894",
          "name": "Rui Liu",
          "hidden": false
        },
        {
          "_id": "68142de6111ccf18a993c895",
          "name": "Naiqiang Tan",
          "hidden": false
        },
        {
          "_id": "68142de6111ccf18a993c896",
          "name": "Xiaochun Cao",
          "hidden": false
        },
        {
          "_id": "68142de6111ccf18a993c897",
          "name": "Dacheng Tao",
          "hidden": false
        },
        {
          "_id": "68142de6111ccf18a993c898",
          "name": "Li Shen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-30T14:01:45.000Z",
      "submittedOnDailyAt": "2025-05-02T01:01:49.479Z",
      "title": "Optimisation logique des niveaux adaptatifs dans des contextes hybrides à long terme",
      "submittedOnDailyBy": {
        "_id": "632ab8f5a968c34257da5c52",
        "avatarUrl": "/avatars/59df09e6c9e1e633170514d950ad7981.svg",
        "isPro": false,
        "fullname": "Haotian Luo",
        "user": "LordNoah",
        "type": "user"
      },
      "summary": "Récemment, les modèles de théorie de raisonnement basés sur des considérations à long terme ont montré un performance puissante dans des tâches complexes de théorie de raisonnement, mais leur efficacité est affectée par un augmentation significative de l'overhead d'inférence. Notre analyse expérimentale a démontré que les bénéfices de la considération à long terme peuvent varier selon la situation : dans certains cas, la nécessité de donner des raisons détaillées est cruciale, tandis que dans d'autres, la considération à long terme ne améliore pas ou même réduit la précision. Cela a conduit à la nécessité d'une stratégie adaptative de théorie de raisonnement qui ajuste la structure des raisonnements aux caractéristiques de l'entrée. Cependant, la plupart des études précédentes ont été limitées à réduire la redondance dans les longues chaînes de théorie de raisonnement et à explorer des stratégies plus efficaces qui dépassent le modèle de théorie de raisonnement à long terme. En réponse à cela, nous proposons un nouveau cadre de deux étapes pour une théorie de raisonnement adaptative et efficace. Premièrement, nous intéguons des modèles de théorie de raisonnement à long terme et à court terme pour faciliter différents styles de raisonnement. Ensuite, nous appliquons un entraînement bidirectionnel pour guider le modèle à choisir un style de raisonnement approprié et à prioriser des raisonnements clairs et précis au sein de chaque groupe de styles. Nos expérimentations ont montré que cette approche peut réduire significativement les coûts d'inférence tout en maintenant ou améliorant le rendement, surtout dans cinq ensembles de données mathématiques, où la longueur moyenne des raisonnements a été réduite d'au-delà de 50%. Cette étude a également révélé la possibilité d'améliorer l'efficacité de la théorie de raisonnement dans des modèles de langage grands à grande échelle. Notre code sera publié bientôt sur : https://github.com/StarDewXXX/AdaR1.",
      "upvotes": 3,
      "discussionId": "68142de7111ccf18a993c8ba",
      "ai_keywords": [
        "CoT models",
        "Long-CoT",
        "hybrid reasoning model",
        "bi-level preference training",
        "adaptive reasoning strategies"
      ]
    },
    "publishedAt": "2025-04-30T10:01:45.000Z",
    "title": "AdaR1: From Long-CoT to Hybrid-CoT via Bi-Level Adaptive Reasoning\n  Optimization",
    "summary": "Recently, long-thought reasoning models achieve strong performance on complex\nreasoning tasks, but often incur substantial inference overhead, making\nefficiency a critical concern. Our empirical analysis reveals that the benefit\nof using Long-CoT varies across problems: while some problems require elaborate\nreasoning, others show no improvement, or even degraded accuracy. This\nmotivates adaptive reasoning strategies that tailor reasoning depth to the\ninput. However, prior work primarily reduces redundancy within long reasoning\npaths, limiting exploration of more efficient strategies beyond the Long-CoT\nparadigm. To address this, we propose a novel two-stage framework for adaptive\nand efficient reasoning. First, we construct a hybrid reasoning model by\nmerging long and short CoT models to enable diverse reasoning styles. Second,\nwe apply bi-level preference training to guide the model to select suitable\nreasoning styles (group-level), and prefer concise and correct reasoning within\neach style group (instance-level). Experiments demonstrate that our method\nsignificantly reduces inference costs compared to other baseline approaches,\nwhile maintaining performance. Notably, on five mathematical datasets, the\naverage length of reasoning is reduced by more than 50%, highlighting the\npotential of adaptive strategies to optimize reasoning efficiency in large\nlanguage models. Our code is coming soon at https://github.com/StarDewXXX/AdaR1",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.21659.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "632ab8f5a968c34257da5c52",
      "avatarUrl": "/avatars/59df09e6c9e1e633170514d950ad7981.svg",
      "fullname": "Haotian Luo",
      "name": "LordNoah",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.00497",
      "authors": [
        {
          "_id": "68147d4d687b82a9b6308cfd",
          "name": "Antoni Bigata",
          "hidden": false
        },
        {
          "_id": "68147d4d687b82a9b6308cfe",
          "name": "Rodrigo Mira",
          "hidden": false
        },
        {
          "_id": "68147d4d687b82a9b6308cff",
          "name": "Stella Bounareli",
          "hidden": false
        },
        {
          "_id": "68147d4d687b82a9b6308d00",
          "name": "Michał Stypułkowski",
          "hidden": false
        },
        {
          "_id": "68147d4d687b82a9b6308d01",
          "name": "Konstantinos Vougioukas",
          "hidden": false
        },
        {
          "_id": "68147d4d687b82a9b6308d02",
          "name": "Stavros Petridis",
          "hidden": false
        },
        {
          "_id": "68147d4d687b82a9b6308d03",
          "name": "Maja Pantic",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/640777812e309e65452491dd/4ao3SssNM8wCiTo5amAxn.mp4"
      ],
      "publishedAt": "2025-05-01T12:56:17.000Z",
      "submittedOnDailyAt": "2025-05-02T06:38:20.471Z",
      "title": "KeySync : méthode puissante pour réaliser une synchronisation des mouvements de la bouche à haute résolution et fluide.",
      "submittedOnDailyBy": {
        "_id": "640777812e309e65452491dd",
        "avatarUrl": "/avatars/01b927675e78f212408168522f65fe36.svg",
        "isPro": true,
        "fullname": "Antoni Bigata",
        "user": "toninio19",
        "type": "user"
      },
      "summary": "La synchronisation des lèvres est un processus qui ajuste les mouvements des lèvres dans un vidéo existant avec un nouvel entrée de voix, généralement traité comme une simple déformation du mouvement facial de la voix. Cependant, la synchronisation des lèvres présente des problèmes supplémentaires tels que la perte d'expressions et l'occultation de la face dans le vidéo d'entrée, ainsi que les problèmes communs de la génération de tokens. Pour résoudre ces problèmes, nous proposons un cadre de travail à deux étapes appelé KeySync. Ce cadre aborde la consistence temporelle et utilise des techniques de masques ajustés pour faire face aux pertes et à l'occultation de la face. Nous montrons que KeySync obtient les meilleurs résultats en reconstruction des lèvres et en synchronisation croisée en utilisant notre nouveau métrique de perte \"LipLeak\", améliorant la qualité visuelle et réduisant la perte. De plus, nous présentons un nouvel approche de masques pour le traitement de l'occultation de la face, justifiant des décisions structurelles à travers plusieurs études de réduction. Les codes et les poids du modèle peuvent être obtenus sur https://antonibigata.github.io/KeySync.",
      "upvotes": 2,
      "discussionId": "68147d53687b82a9b6308e59",
      "projectPage": "https://antonibigata.github.io/KeySync/",
      "githubRepo": "https://github.com/antonibigata/keysync",
      "ai_keywords": [
        "KeySync",
        "lip synchronization",
        "audio-driven facial animation",
        "talking head generation",
        "temporal consistency",
        "expression leakage",
        "facial occlusions",
        "automated dubbing",
        "lip reconstruction",
        "cross-synchronization",
        "visual quality",
        "LipLeak",
        "masking strategy",
        "ablation studies"
      ]
    },
    "publishedAt": "2025-05-01T08:56:17.000Z",
    "title": "KeySync: A Robust Approach for Leakage-free Lip Synchronization in High\n  Resolution",
    "summary": "Lip synchronization, known as the task of aligning lip movements in an\nexisting video with new input audio, is typically framed as a simpler variant\nof audio-driven facial animation. However, as well as suffering from the usual\nissues in talking head generation (e.g., temporal consistency), lip\nsynchronization presents significant new challenges such as expression leakage\nfrom the input video and facial occlusions, which can severely impact\nreal-world applications like automated dubbing, but are often neglected in\nexisting works. To address these shortcomings, we present KeySync, a two-stage\nframework that succeeds in solving the issue of temporal consistency, while\nalso incorporating solutions for leakage and occlusions using a carefully\ndesigned masking strategy. We show that KeySync achieves state-of-the-art\nresults in lip reconstruction and cross-synchronization, improving visual\nquality and reducing expression leakage according to LipLeak, our novel leakage\nmetric. Furthermore, we demonstrate the effectiveness of our new masking\napproach in handling occlusions and validate our architectural choices through\nseveral ablation studies. Code and model weights can be found at\nhttps://antonibigata.github.io/KeySync.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/640777812e309e65452491dd/4ao3SssNM8wCiTo5amAxn.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.00497.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "640777812e309e65452491dd",
      "avatarUrl": "/avatars/01b927675e78f212408168522f65fe36.svg",
      "fullname": "Antoni Bigata",
      "name": "toninio19",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.20605",
      "authors": [
        {
          "_id": "68131e73f0f2a4d8b2d4b06a",
          "user": {
            "_id": "642bcb8ae5b6823cde9301bd",
            "avatarUrl": "/avatars/cddd29afbdbc2fbea90612567090147b.svg",
            "isPro": false,
            "fullname": "Mihai Dan Nadăș",
            "user": "mihainadas",
            "type": "user"
          },
          "name": "Mihai Nadas",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-02T06:34:48.889Z",
          "hidden": false
        },
        {
          "_id": "68131e73f0f2a4d8b2d4b06b",
          "name": "Laura Diosan",
          "hidden": false
        },
        {
          "_id": "68131e73f0f2a4d8b2d4b06c",
          "user": {
            "_id": "67b2344d0ce2aaa57c8c9997",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67b2344d0ce2aaa57c8c9997/LSMjuQNjRsUllUQyt9vNo.jpeg",
            "isPro": false,
            "fullname": "Andrei Piscoran",
            "user": "andreiPiscoran",
            "type": "user"
          },
          "name": "Andrei Piscoran",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-01T07:11:31.916Z",
          "hidden": false
        },
        {
          "_id": "68131e73f0f2a4d8b2d4b06d",
          "user": {
            "_id": "677e4393ef848c5a5352d082",
            "avatarUrl": "/avatars/bcb60ead58969601e2911053550fec62.svg",
            "isPro": false,
            "fullname": "Andreea Tomescu",
            "user": "andreeatomescu",
            "type": "user"
          },
          "name": "Andreea Tomescu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-01T07:10:43.780Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-29T10:15:28.000Z",
      "submittedOnDailyAt": "2025-05-02T06:22:34.343Z",
      "title": "TF1-EN-3M : Ensemble de données pour un petit modèle de langage ouvert entraîné avec 3 millions d'histoires morales synthétiques",
      "submittedOnDailyBy": {
        "_id": "642bcb8ae5b6823cde9301bd",
        "avatarUrl": "/avatars/cddd29afbdbc2fbea90612567090147b.svg",
        "isPro": false,
        "fullname": "Mihai Dan Nadăș",
        "user": "mihainadas",
        "type": "user"
      },
      "summary": "La histoire de Mori est un outil historique utilisé pour transmettre du valeur et de la signification, tandis que le NLP moderne manque de grandes structures de textes cohérentes et d'enseignements éthiques explicites. Pour corriger cette lacune, un ensemble de données TF1-EN-3M a été préparé. C'est le premier ensemble de données publique qui génère 3 millions de films en anglais avec un modèle de 800 millions de paramètres ajusté aux instances. Chaque histoire inclut un design de 6 potentiels (personnage -> caractéristique -> environnement -> conflit -> résolution -> enseignement éthique) et un moteur d'apprentissage qui maintient la cohérence du genre tout en couvrant un large espace thématique.\n\nLe pipeline d'évaluation hybride combine (i) évaluateurs basés sur GPT qui attribuent des scores en fonction de la grammaire, de la créativité, de la clarté éthique et du respect du template, et (ii) métriques de diversité et de lisibilité sans référence. Parmi les 10 candidatures ouvertes, la version de Llama-3 avec 800 millions de paramètres offre la meilleure combinaison de qualité et de vitesse, générant des films avec des scores élevés sur 1 000 films. Cette tâche peut être réalisée en environ 13,5 secondes avec un seul GPU de consolle (<24GB de VRAM).\n\nL'ensemble de données est publié gratuitement, ainsi que le code de génération, les scripts d'évaluation et les métadonnées complètes, permettant la reproductibilité et le benchmark des coûts. TF1-EN-3M ouvre des voies pour la recherche en intelligence artificielle éducative pour les enfants, démontrant que la transmission de grandes histoires éthiques n'a pas besoin de grands modèles.",
      "upvotes": 2,
      "discussionId": "68131e73f0f2a4d8b2d4b087",
      "githubRepo": "https://github.com/klusai/tinyfabulist",
      "ai_keywords": [
        "instruction-tuned models",
        "combinatorial prompt engine",
        "GPT-based critic",
        "template adherence",
        "reference-free diversity",
        "Llama-3 variant",
        "computational efficiency",
        "permissive license",
        "child-friendly educational AI",
        "moral storytelling"
      ]
    },
    "publishedAt": "2025-04-29T06:15:28.000Z",
    "title": "TF1-EN-3M: Three Million Synthetic Moral Fables for Training Small, Open\n  Language Models",
    "summary": "Moral stories are a time-tested vehicle for transmitting values, yet modern\nNLP lacks a large, structured corpus that couples coherent narratives with\nexplicit ethical lessons. We close this gap with TF1-EN-3M, the first open\ndataset of three million English-language fables generated exclusively by\ninstruction-tuned models no larger than 8B parameters. Each story follows a\nsix-slot scaffold (character -> trait -> setting -> conflict -> resolution ->\nmoral), produced through a combinatorial prompt engine that guarantees genre\nfidelity while covering a broad thematic space.\n  A hybrid evaluation pipeline blends (i) a GPT-based critic that scores\ngrammar, creativity, moral clarity, and template adherence with (ii)\nreference-free diversity and readability metrics. Among ten open-weight\ncandidates, an 8B-parameter Llama-3 variant delivers the best quality-speed\ntrade-off, producing high-scoring fables on a single consumer GPU (<24 GB VRAM)\nat approximately 13.5 cents per 1,000 fables.\n  We release the dataset, generation code, evaluation scripts, and full\nmetadata under a permissive license, enabling exact reproducibility and cost\nbenchmarking. TF1-EN-3M opens avenues for research in instruction following,\nnarrative intelligence, value alignment, and child-friendly educational AI,\ndemonstrating that large-scale moral storytelling no longer requires\nproprietary giant models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.20605.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642bcb8ae5b6823cde9301bd",
      "avatarUrl": "/avatars/cddd29afbdbc2fbea90612567090147b.svg",
      "fullname": "Mihai Dan Nadăș",
      "name": "mihainadas",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.18983",
      "authors": [
        {
          "_id": "681470d72175e5e7ca0ea002",
          "name": "Xuyin Qi",
          "hidden": false
        },
        {
          "_id": "681470d72175e5e7ca0ea003",
          "name": "Zeyu Zhang",
          "hidden": false
        },
        {
          "_id": "681470d72175e5e7ca0ea004",
          "name": "Canxuan Gang",
          "hidden": false
        },
        {
          "_id": "681470d72175e5e7ca0ea005",
          "name": "Hao Zhang",
          "hidden": false
        },
        {
          "_id": "681470d72175e5e7ca0ea006",
          "name": "Lei Zhang",
          "hidden": false
        },
        {
          "_id": "681470d72175e5e7ca0ea007",
          "name": "Zhiwei Zhang",
          "hidden": false
        },
        {
          "_id": "681470d72175e5e7ca0ea008",
          "name": "Yang Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-26T17:56:56.000Z",
      "submittedOnDailyAt": "2025-05-02T05:44:53.105Z",
      "title": "MediAug : Recherche en Visualisation et Augmentation des Images Médicales",
      "submittedOnDailyBy": {
        "_id": "64ec877bb93654d4ca5c92e9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
        "isPro": false,
        "fullname": "Zeyu Zhang",
        "user": "SteveZeyuZhang",
        "type": "user"
      },
      "summary": "La augmentation des données est essentielle pour améliorer la précision de la classification, la détection des dommages et la segmentation des organes dans les images médicales, où la qualité des données est limitée. Cependant, deux problèmes significatifs persistent. Le premier est l'importante divergence entre les images naturelles et les images médicales, qui peut altérer des caractéristiques importantes des maladies. Le second est que la recherche sur l'augmentation des données d'images médicales est restreinte à des tâches ou des architectures spécifiques, et les avantages de stratégies mixtes ne sont pas clairs. Pour résoudre ces problèmes, nous proposons un cadre d'évaluation unique utilisant six méthodes d'augmentation des données basées sur des mix, combinant deux modèles de Convolution et Transformer avec des images d'IRM de tumeurs cérébrales et des images d'œil avec des affections. Notre contribution comprend trois aspects : 1. Nous présentons MediAug, une évaluation avancée pour l'augmentation des données d'images médicales. 2. Nous évaluons systématiquement MixUp, YOCO, CropMix, CutMix, AugMix et SnapMix en utilisant ResNet-50 et ViT-B. 3. À travers une large gamme d'expériences, nous observons que MixUp a amélioré la précision dans la tâche de classification de tumeurs cérébrales avec ResNet-50 à 79.19%, SnapMix à 99.44% avec ViT-B, YOCO à 91.60% avec ResNet-50 pour la tâche de classification des affections oculaires et CutMix à 97.94% avec ViT-B. Le code est disponible sur https://github.com/AIGeeksGroup/MediAug.",
      "upvotes": 1,
      "discussionId": "681470d92175e5e7ca0ea065",
      "ai_keywords": [
        "MediAug",
        "MixUp",
        "YOCO",
        "CropMix",
        "CutMix",
        "AugMix",
        "SnapMix",
        "ResNet-50",
        "ViT-B",
        "brain tumour MRI",
        "eye disease fundus datasets",
        "domain gap",
        "lesion detection",
        "organ segmentation",
        "classification accuracy"
      ]
    },
    "publishedAt": "2025-04-26T13:56:56.000Z",
    "title": "MediAug: Exploring Visual Augmentation in Medical Imaging",
    "summary": "Data augmentation is essential in medical imaging for improving\nclassification accuracy, lesion detection, and organ segmentation under limited\ndata conditions. However, two significant challenges remain. First, a\npronounced domain gap between natural photographs and medical images can\ndistort critical disease features. Second, augmentation studies in medical\nimaging are fragmented and limited to single tasks or architectures, leaving\nthe benefits of advanced mix-based strategies unclear. To address these\nchallenges, we propose a unified evaluation framework with six mix-based\naugmentation methods integrated with both convolutional and transformer\nbackbones on brain tumour MRI and eye disease fundus datasets. Our\ncontributions are threefold. (1) We introduce MediAug, a comprehensive and\nreproducible benchmark for advanced data augmentation in medical imaging. (2)\nWe systematically evaluate MixUp, YOCO, CropMix, CutMix, AugMix, and SnapMix\nwith ResNet-50 and ViT-B backbones. (3) We demonstrate through extensive\nexperiments that MixUp yields the greatest improvement on the brain tumor\nclassification task for ResNet-50 with 79.19% accuracy and SnapMix yields the\ngreatest improvement for ViT-B with 99.44% accuracy, and that YOCO yields the\ngreatest improvement on the eye disease classification task for ResNet-50 with\n91.60% accuracy and CutMix yields the greatest improvement for ViT-B with\n97.94% accuracy. Code will be available at\nhttps://github.com/AIGeeksGroup/MediAug.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.18983.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ec877bb93654d4ca5c92e9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
      "fullname": "Zeyu Zhang",
      "name": "SteveZeyuZhang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  }
]