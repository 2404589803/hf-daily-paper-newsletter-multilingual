[
  {
    "paper": {
      "id": "2504.21853",
      "authors": [
        {
          "_id": "681441e64d6a681c7c840b1f",
          "name": "Jiwen Yu",
          "hidden": false
        },
        {
          "_id": "681441e64d6a681c7c840b20",
          "name": "Yiran Qin",
          "hidden": false
        },
        {
          "_id": "681441e64d6a681c7c840b21",
          "name": "Haoxuan Che",
          "hidden": false
        },
        {
          "_id": "681441e64d6a681c7c840b22",
          "name": "Quande Liu",
          "hidden": false
        },
        {
          "_id": "681441e64d6a681c7c840b23",
          "name": "Xintao Wang",
          "hidden": false
        },
        {
          "_id": "681441e64d6a681c7c840b24",
          "name": "Pengfei Wan",
          "hidden": false
        },
        {
          "_id": "681441e64d6a681c7c840b25",
          "name": "Di Zhang",
          "hidden": false
        },
        {
          "_id": "681441e64d6a681c7c840b26",
          "name": "Kun Gai",
          "hidden": false
        },
        {
          "_id": "681441e64d6a681c7c840b27",
          "name": "Hao Chen",
          "hidden": false
        },
        {
          "_id": "681441e64d6a681c7c840b28",
          "name": "Xihui Liu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64105a6d14215c0775dfdd14/Y9dvfNIIrOJ4rCzwIB6XI.jpeg",
        "https://cdn-uploads.huggingface.co/production/uploads/64105a6d14215c0775dfdd14/5kPNXDj9LIsgShhFz5WSg.jpeg"
      ],
      "publishedAt": "2025-04-30T17:59:02.000Z",
      "submittedOnDailyAt": "2025-05-02T02:29:37.187Z",
      "title": "Interactivo Génération de Vidéos Recherche",
      "submittedOnDailyBy": {
        "_id": "64105a6d14215c0775dfdd14",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64105a6d14215c0775dfdd14/-VX-cUYOLjHIg7QnWhRGG.jpeg",
        "isPro": false,
        "fullname": "Jiwen Yu",
        "user": "VictorYuki",
        "type": "user"
      },
      "summary": "L'IGV (Interactive Generative Video) a émergé comme une technologie cruciale en réponse à la demande croissante en contenu vidéo interactif de haute qualité. Dans cet article, l'IGV est définie comme une technologie qui génère non seulement du contenu vidéo de haute qualité, mais qui fournit également des fonctions interactives qui contrôlent les actions de l'utilisateur et offrent une rétroaction. L'état actuel des applications de l'IGV est examiné, avec un accent sur trois domaines principaux : les jeux, l'intelligence artificielle de visualisation et la conduite autonome. Dans les jeux, l'IGV permet d'explorer sans fin le monde de base. En intelligence artificielle de visualisation, l'IGV utilise des environnements synthétiques avec des connaissances physiques pour apprendre dans des environnements interactifs. En conduite autonome, l'IGV fournit des fonctions de simulation et de validation en boucle fermée pour garantir la sécurité. Pour guider le développement futur, un cadre complexe de travail est proposé, qui divise un système idéal d'IGV en cinq modules de base : génération, contrôle, mémoire, dynamique et intelligence. De plus, les problèmes techniques et les perspectives futures de chaque composant du système idéal d'IGV sont analysés systématiquement. Cette analyse systématique estime qu'elle encouragera la recherche et le développement dans le domaine de l'IGV et favorisera le développement d'applications plus complexes et pratiques.",
      "upvotes": 24,
      "discussionId": "681441e84d6a681c7c840bae",
      "ai_keywords": [
        "generative capabilities",
        "interactive features",
        "control signals",
        "responsive feedback",
        "virtual worlds",
        "physics-aware environment synthesizer",
        "multimodal interaction",
        "dynamically evolving scenes",
        "closed-loop simulation",
        "safety-critical testing",
        "validation",
        "real-time generation",
        "open-domain control",
        "long-term coherence",
        "accurate physics",
        "causal reasoning"
      ]
    },
    "publishedAt": "2025-04-30T13:59:02.000Z",
    "title": "A Survey of Interactive Generative Video",
    "summary": "Interactive Generative Video (IGV) has emerged as a crucial technology in\nresponse to the growing demand for high-quality, interactive video content\nacross various domains. In this paper, we define IGV as a technology that\ncombines generative capabilities to produce diverse high-quality video content\nwith interactive features that enable user engagement through control signals\nand responsive feedback. We survey the current landscape of IGV applications,\nfocusing on three major domains: 1) gaming, where IGV enables infinite\nexploration in virtual worlds; 2) embodied AI, where IGV serves as a\nphysics-aware environment synthesizer for training agents in multimodal\ninteraction with dynamically evolving scenes; and 3) autonomous driving, where\nIGV provides closed-loop simulation capabilities for safety-critical testing\nand validation. To guide future development, we propose a comprehensive\nframework that decomposes an ideal IGV system into five essential modules:\nGeneration, Control, Memory, Dynamics, and Intelligence. Furthermore, we\nsystematically analyze the technical challenges and future directions in\nrealizing each component for an ideal IGV system, such as achieving real-time\ngeneration, enabling open-domain control, maintaining long-term coherence,\nsimulating accurate physics, and integrating causal reasoning. We believe that\nthis systematic analysis will facilitate future research and development in the\nfield of IGV, ultimately advancing the technology toward more sophisticated and\npractical applications.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64105a6d14215c0775dfdd14/Y9dvfNIIrOJ4rCzwIB6XI.jpeg",
      "https://cdn-uploads.huggingface.co/production/uploads/64105a6d14215c0775dfdd14/5kPNXDj9LIsgShhFz5WSg.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.21853.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64105a6d14215c0775dfdd14",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64105a6d14215c0775dfdd14/-VX-cUYOLjHIg7QnWhRGG.jpeg",
      "fullname": "Jiwen Yu",
      "name": "VictorYuki",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.00662",
      "authors": [
        {
          "_id": "68142e4a551709da9244e8d1",
          "user": {
            "_id": "64b7df742f5a966b973e25f7",
            "avatarUrl": "/avatars/e24e7769188d441317b3b7d10ef8fd60.svg",
            "isPro": false,
            "fullname": "Wenkai Yang",
            "user": "Keven16",
            "type": "user"
          },
          "name": "Wenkai Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-02T06:34:18.531Z",
          "hidden": false
        },
        {
          "_id": "68142e4a551709da9244e8d2",
          "name": "Jingwen Chen",
          "hidden": false
        },
        {
          "_id": "68142e4a551709da9244e8d3",
          "name": "Yankai Lin",
          "hidden": false
        },
        {
          "_id": "68142e4a551709da9244e8d4",
          "name": "Ji-Rong Wen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-01T17:03:17.000Z",
      "submittedOnDailyAt": "2025-05-02T01:12:33.949Z",
      "title": "DeepCritic : Évaluation Prédictive à Travers des Modèles de Langue",
      "submittedOnDailyBy": {
        "_id": "64b7df742f5a966b973e25f7",
        "avatarUrl": "/avatars/e24e7769188d441317b3b7d10ef8fd60.svg",
        "isPro": false,
        "fullname": "Wenkai Yang",
        "user": "Keven16",
        "type": "user"
      },
      "summary": "Le développement rapide des LLM rend urgent et importante la fourniture de rétroaction précise et un suivi échelonnable. Utiliser des modèles d'évaluation de LLM pour atteindre une supervision automatisée est une solution souhaitable. Dans cette étude, on cherche et améliore la capacité des LLM à évaluer mathématiquement. Les modèles actuels d'évaluation de LLM ne fournissent que des évaluations superficielles à chaque étape, avec une faible précision et ne proposent pas de rétroaction suffisante pour les générateurs de LLM. Pour résoudre ces problèmes, on propose un nouveau cadre à deux étapes efficace pour évaluer chaque étape de la résolution de problèmes mathématiques de manière intentionnelle. Dans la première étape, on utilise Qwen2.5-72B-Instruct pour générer des évaluations de 4.5K longues phrases, qui sont utilisées comme données d'entraînement pour des ajustements normaux. Chaque évaluation comprend une vérification à partir de différentes perspectives et des évaluations détaillées de l'évaluation initiale à chaque étape. Ensuite, on utilise des données d'étiquetage humain provenant de PRM800K et des données de notes automatiques basées sur le méthode de Monte Carlo et l'évaluation de précision pour effectuer l'entraînement par renforcement du modèle ajusté, ce qui améliore sa capacité d'évaluation. Le modèle d'évaluation basé sur Qwen2.5-7B-Instruct surpasse significativement les modèles actuels d'évaluation de LLM (y compris DeepSeek-R1-distill de la même taille et GPT-4o) dans le cadre de référence de reconnaissance d'erreurs. De plus, il offre une rétroaction plus détaillée qui peut effectivement pousser la correction des erreurs dans les générateurs de LLM.",
      "upvotes": 21,
      "discussionId": "68142e4b551709da9244e8f8",
      "ai_keywords": [
        "LLMs (Large Language Models)",
        "critique models",
        "automated supervision",
        "math critique ability",
        "supervised fine-tuning",
        "Qwen2.5-72B-Instruct",
        "seed data",
        "deliberate step-wise critiques",
        "multi-perspective verifications",
        "reinforcement learning",
        "PRM800K",
        "Monte Carlo sampling-based correctness estimation",
        "Qwen2.5-7B-Instruct",
        "DeepSeek-R1-distill models",
        "GPT-4o",
        "error identification benchmarks"
      ]
    },
    "publishedAt": "2025-05-01T13:03:17.000Z",
    "title": "DeepCritic: Deliberate Critique with Large Language Models",
    "summary": "As Large Language Models (LLMs) are rapidly evolving, providing accurate\nfeedback and scalable oversight on their outputs becomes an urgent and critical\nproblem. Leveraging LLMs as critique models to achieve automated supervision is\na promising solution. In this work, we focus on studying and enhancing the math\ncritique ability of LLMs. Current LLM critics provide critiques that are too\nshallow and superficial on each step, leading to low judgment accuracy and\nstruggling to offer sufficient feedback for the LLM generator to correct\nmistakes. To tackle this issue, we propose a novel and effective two-stage\nframework to develop LLM critics that are capable of deliberately critiquing on\neach reasoning step of math solutions. In the first stage, we utilize\nQwen2.5-72B-Instruct to generate 4.5K long-form critiques as seed data for\nsupervised fine-tuning. Each seed critique consists of deliberate step-wise\ncritiques that includes multi-perspective verifications as well as in-depth\ncritiques of initial critiques for each reasoning step. Then, we perform\nreinforcement learning on the fine-tuned model with either existing\nhuman-labeled data from PRM800K or our automatically annotated data obtained\nvia Monte Carlo sampling-based correctness estimation, to further incentivize\nits critique ability. Our developed critique model built on Qwen2.5-7B-Instruct\nnot only significantly outperforms existing LLM critics (including the\nsame-sized DeepSeek-R1-distill models and GPT-4o) on various error\nidentification benchmarks, but also more effectively helps the LLM generator\nrefine erroneous steps through more detailed feedback.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.00662.png",
    "numComments": 6,
    "submittedBy": {
      "_id": "64b7df742f5a966b973e25f7",
      "avatarUrl": "/avatars/e24e7769188d441317b3b7d10ef8fd60.svg",
      "fullname": "Wenkai Yang",
      "name": "Keven16",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.00703",
      "authors": [
        {
          "_id": "681428debcdf962d03da2797",
          "name": "Dongzhi Jiang",
          "hidden": false
        },
        {
          "_id": "681428debcdf962d03da2798",
          "name": "Ziyu Guo",
          "hidden": false
        },
        {
          "_id": "681428debcdf962d03da2799",
          "name": "Renrui Zhang",
          "hidden": false
        },
        {
          "_id": "681428debcdf962d03da279a",
          "name": "Zhuofan Zong",
          "hidden": false
        },
        {
          "_id": "681428debcdf962d03da279b",
          "name": "Hao Li",
          "hidden": false
        },
        {
          "_id": "681428debcdf962d03da279c",
          "name": "Le Zhuo",
          "hidden": false
        },
        {
          "_id": "681428debcdf962d03da279d",
          "name": "Shilin Yan",
          "hidden": false
        },
        {
          "_id": "681428debcdf962d03da279e",
          "name": "Pheng-Ann Heng",
          "hidden": false
        },
        {
          "_id": "681428debcdf962d03da279f",
          "name": "Hongsheng Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-01T17:59:46.000Z",
      "submittedOnDailyAt": "2025-05-02T00:38:40.412Z",
      "title": "T2I-R1 : Méthode pour renforcer la génération d'images en coopérant sémantiquement et au niveau des tokens",
      "submittedOnDailyBy": {
        "_id": "6349214f8146350b3a4c5cdf",
        "avatarUrl": "/avatars/cfd24caac9a87efb528d0f4c375932bc.svg",
        "isPro": false,
        "fullname": "Dongzhi Jiang",
        "user": "CaraJ",
        "type": "user"
      },
      "summary": "Le développement récent de modèles de langage à grande échelle a montré que le chain-of-thought (CoT) et l'apprentissage par renforcement (RL) améliorent les résultats. Cependant, l'application de ces méthodes dans le domaine de la génération d'images a été peu étudiée. Dans cet article, nous proposons une nouvelle théorie d'inférence renforcée avec CoT bi-niveau pour un modèle de génération d'images à partir de texte, nommé T2I-R1. Spécifiquement, pour améliorer les différences lors du processus de génération, deux niveaux de CoT sont distingués : le niveau de signification et le niveau de tokens. De plus, pour améliorer la coordination de ces deux niveaux, BiCoT-GRPO est introduit et une compensation de génération divise les deux CoT dans le même état d'entraînement. En appliquant ces innovations à la base de modèle Janus-Pro, un améliorament de 13% sur T2I-CompBench et de 19% sur le WISE benchmark est atteint, dépassant les modèles les plus récents comme FLUX. Le code est disponible sur la URL suivante : https://github.com/CaraJ7/T2I-R1",
      "upvotes": 13,
      "discussionId": "681428dfbcdf962d03da281c",
      "githubRepo": "https://github.com/CaraJ7/T2I-R1",
      "ai_keywords": [
        "chain-of-thought (CoT)",
        "reinforcement learning (RL)",
        "text-to-image generation model",
        "bi-level CoT reasoning process",
        "semantic-level CoT",
        "token-level CoT",
        "BiCoT-GRPO",
        "generation rewards",
        "Janus-Pro",
        "T2I-CompBench",
        "WISE benchmark",
        "FLUX"
      ]
    },
    "publishedAt": "2025-05-01T13:59:46.000Z",
    "title": "T2I-R1: Reinforcing Image Generation with Collaborative Semantic-level\n  and Token-level CoT",
    "summary": "Recent advancements in large language models have demonstrated how\nchain-of-thought (CoT) and reinforcement learning (RL) can improve performance.\nHowever, applying such reasoning strategies to the visual generation domain\nremains largely unexplored. In this paper, we present T2I-R1, a novel\nreasoning-enhanced text-to-image generation model, powered by RL with a\nbi-level CoT reasoning process. Specifically, we identify two levels of CoT\nthat can be utilized to enhance different stages of generation: (1) the\nsemantic-level CoT for high-level planning of the prompt and (2) the\ntoken-level CoT for low-level pixel processing during patch-by-patch\ngeneration. To better coordinate these two levels of CoT, we introduce\nBiCoT-GRPO with an ensemble of generation rewards, which seamlessly optimizes\nboth generation CoTs within the same training step. By applying our reasoning\nstrategies to the baseline model, Janus-Pro, we achieve superior performance\nwith 13% improvement on T2I-CompBench and 19% improvement on the WISE\nbenchmark, even surpassing the state-of-the-art model FLUX.1. Code is available\nat: https://github.com/CaraJ7/T2I-R1",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.00703.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6349214f8146350b3a4c5cdf",
      "avatarUrl": "/avatars/cfd24caac9a87efb528d0f4c375932bc.svg",
      "fullname": "Dongzhi Jiang",
      "name": "CaraJ",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.21659",
      "authors": [
        {
          "_id": "68142de6111ccf18a993c890",
          "name": "Haotian Luo",
          "hidden": false
        },
        {
          "_id": "68142de6111ccf18a993c891",
          "name": "Haiying He",
          "hidden": false
        },
        {
          "_id": "68142de6111ccf18a993c892",
          "name": "Yibo Wang",
          "hidden": false
        },
        {
          "_id": "68142de6111ccf18a993c893",
          "name": "Jinluan Yang",
          "hidden": false
        },
        {
          "_id": "68142de6111ccf18a993c894",
          "name": "Rui Liu",
          "hidden": false
        },
        {
          "_id": "68142de6111ccf18a993c895",
          "name": "Naiqiang Tan",
          "hidden": false
        },
        {
          "_id": "68142de6111ccf18a993c896",
          "name": "Xiaochun Cao",
          "hidden": false
        },
        {
          "_id": "68142de6111ccf18a993c897",
          "name": "Dacheng Tao",
          "hidden": false
        },
        {
          "_id": "68142de6111ccf18a993c898",
          "name": "Li Shen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-30T14:01:45.000Z",
      "submittedOnDailyAt": "2025-05-02T01:01:49.479Z",
      "title": "Processus Logistique Adaptatif Bi-Niveau du Long Terme vers un Contexte Hybrid",
      "submittedOnDailyBy": {
        "_id": "632ab8f5a968c34257da5c52",
        "avatarUrl": "/avatars/59df09e6c9e1e633170514d950ad7981.svg",
        "isPro": false,
        "fullname": "Haotian Luo",
        "user": "LordNoah",
        "type": "user"
      },
      "summary": "Récemment, les modèles de pensée à long terme ont démontré un excellent rendement dans des tâches complexes de pensée, mais leur inférence est coûteuse et leur efficacité est un problème général. Notre analyse expérimentale montre que les avantages de l'utilisation de la pensée à long terme dépendent du problème : dans certains cas, la pensée complexe est nécessaire, tandis que dans d'autres, aucun amélioration ou une détérioration de la précision n'est observée. Par conséquent, il est nécessaire d'adopter une stratégie adaptative pour ajuster la profondeur de la pensée à l'entrée, mais les études précédentes sont limitées à réduire la redondance au sein du pas de pensée à long terme et à explorer des stratégies plus efficaces qui dépassent le paradigme de Long-CoT. En réponse à cette situation, nous proposons un nouveau cadre à deux étapes pour la pensée adaptative et efficace. Premièrement, nous intéguons des modèles de pensée à long terme et de court terme pour construire un modèle de pensée confus qui permet plusieurs styles de pensée. Ensuite, nous appliquons un apprentissage par niveaux pour induire le modèle à choisir le style de pensée approprié et à guider les modèles à l'intérieur de chaque groupe de styles vers des pensées plus claires et précises. Les expériences montrent que cette approche peut réduire significativement les coûts d'inférence, tout en maintenant ou améliorant la précision. En particulier, la longueur moyenne des pensées dans 5 ensembles de données de mathématiques a été réduite d'au-delà de 50%, démontrant clairement la possibilité d'optimiser l'efficacité de la pensée dans les modèles de langage grands. Notre code est disponible sur https://github.com/StarDewXXX/AdaR1.",
      "upvotes": 3,
      "discussionId": "68142de7111ccf18a993c8ba",
      "ai_keywords": [
        "CoT models",
        "Long-CoT",
        "hybrid reasoning model",
        "bi-level preference training",
        "adaptive reasoning strategies"
      ]
    },
    "publishedAt": "2025-04-30T10:01:45.000Z",
    "title": "AdaR1: From Long-CoT to Hybrid-CoT via Bi-Level Adaptive Reasoning\n  Optimization",
    "summary": "Recently, long-thought reasoning models achieve strong performance on complex\nreasoning tasks, but often incur substantial inference overhead, making\nefficiency a critical concern. Our empirical analysis reveals that the benefit\nof using Long-CoT varies across problems: while some problems require elaborate\nreasoning, others show no improvement, or even degraded accuracy. This\nmotivates adaptive reasoning strategies that tailor reasoning depth to the\ninput. However, prior work primarily reduces redundancy within long reasoning\npaths, limiting exploration of more efficient strategies beyond the Long-CoT\nparadigm. To address this, we propose a novel two-stage framework for adaptive\nand efficient reasoning. First, we construct a hybrid reasoning model by\nmerging long and short CoT models to enable diverse reasoning styles. Second,\nwe apply bi-level preference training to guide the model to select suitable\nreasoning styles (group-level), and prefer concise and correct reasoning within\neach style group (instance-level). Experiments demonstrate that our method\nsignificantly reduces inference costs compared to other baseline approaches,\nwhile maintaining performance. Notably, on five mathematical datasets, the\naverage length of reasoning is reduced by more than 50%, highlighting the\npotential of adaptive strategies to optimize reasoning efficiency in large\nlanguage models. Our code is coming soon at https://github.com/StarDewXXX/AdaR1",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.21659.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "632ab8f5a968c34257da5c52",
      "avatarUrl": "/avatars/59df09e6c9e1e633170514d950ad7981.svg",
      "fullname": "Haotian Luo",
      "name": "LordNoah",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.00497",
      "authors": [
        {
          "_id": "68147d4d687b82a9b6308cfd",
          "name": "Antoni Bigata",
          "hidden": false
        },
        {
          "_id": "68147d4d687b82a9b6308cfe",
          "name": "Rodrigo Mira",
          "hidden": false
        },
        {
          "_id": "68147d4d687b82a9b6308cff",
          "name": "Stella Bounareli",
          "hidden": false
        },
        {
          "_id": "68147d4d687b82a9b6308d00",
          "name": "Michał Stypułkowski",
          "hidden": false
        },
        {
          "_id": "68147d4d687b82a9b6308d01",
          "name": "Konstantinos Vougioukas",
          "hidden": false
        },
        {
          "_id": "68147d4d687b82a9b6308d02",
          "name": "Stavros Petridis",
          "hidden": false
        },
        {
          "_id": "68147d4d687b82a9b6308d03",
          "name": "Maja Pantic",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/640777812e309e65452491dd/4ao3SssNM8wCiTo5amAxn.mp4"
      ],
      "publishedAt": "2025-05-01T12:56:17.000Z",
      "submittedOnDailyAt": "2025-05-02T06:38:20.471Z",
      "title": "KeySync : Synchronisation efficace et robuste de la bouche, sans nécessité de synchronisation haute résolution ou de bouche fine.",
      "submittedOnDailyBy": {
        "_id": "640777812e309e65452491dd",
        "avatarUrl": "/avatars/01b927675e78f212408168522f65fe36.svg",
        "isPro": true,
        "fullname": "Antoni Bigata",
        "user": "toninio19",
        "type": "user"
      },
      "summary": "Le problème de synchronisation des lèvres est un travail qui consiste à ajuster les mouvements des lèvres d'une vidéo avec un nouvel entraînement de voix, généralement traité comme une version simple d'animation faciale vocale. Cependant, la synchronisation des lèvres ajoute des problèmes tels que la synchronisation temporelle, tandis que les vidéos d'entrée présentent de nouveaux problèmes tels que la manque d'expression faciale et l'occultation de la face, ce qui peut affecter significativement des applications réelles telles que la traduction automatique. Cependant, actuellement, ces problèmes ne sont pas analysés de manière adéquate dans la recherche. Pour résoudre ces inconvénients, nous proposons un cadre de travail à deux étapes appelé KeySync. Ce cadre résout les problèmes de synchronisation temporelle et utilise une technologie de masque conçue avec précision pour aborder la manque d'expression et l'occultation de la face. KeySync réalise les meilleurs résultats en reconstruction des lèvres et synchronisation croisée par le nouveau métrique de manque d'expression LipLeak, améliorant la qualité visuelle et réduisant la manque d'expression. De plus, il montre l'effet de nouvelles techniques de masque et évalue les décisions structurelles par des études de réduction. Les codes et poids du modèle sont disponibles sur https://antonibigata.github.io/KeySync.",
      "upvotes": 2,
      "discussionId": "68147d53687b82a9b6308e59",
      "projectPage": "https://antonibigata.github.io/KeySync/",
      "githubRepo": "https://github.com/antonibigata/keysync",
      "ai_keywords": [
        "KeySync",
        "lip synchronization",
        "audio-driven facial animation",
        "talking head generation",
        "temporal consistency",
        "expression leakage",
        "facial occlusions",
        "automated dubbing",
        "lip reconstruction",
        "cross-synchronization",
        "visual quality",
        "LipLeak",
        "masking strategy",
        "ablation studies"
      ]
    },
    "publishedAt": "2025-05-01T08:56:17.000Z",
    "title": "KeySync: A Robust Approach for Leakage-free Lip Synchronization in High\n  Resolution",
    "summary": "Lip synchronization, known as the task of aligning lip movements in an\nexisting video with new input audio, is typically framed as a simpler variant\nof audio-driven facial animation. However, as well as suffering from the usual\nissues in talking head generation (e.g., temporal consistency), lip\nsynchronization presents significant new challenges such as expression leakage\nfrom the input video and facial occlusions, which can severely impact\nreal-world applications like automated dubbing, but are often neglected in\nexisting works. To address these shortcomings, we present KeySync, a two-stage\nframework that succeeds in solving the issue of temporal consistency, while\nalso incorporating solutions for leakage and occlusions using a carefully\ndesigned masking strategy. We show that KeySync achieves state-of-the-art\nresults in lip reconstruction and cross-synchronization, improving visual\nquality and reducing expression leakage according to LipLeak, our novel leakage\nmetric. Furthermore, we demonstrate the effectiveness of our new masking\napproach in handling occlusions and validate our architectural choices through\nseveral ablation studies. Code and model weights can be found at\nhttps://antonibigata.github.io/KeySync.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/640777812e309e65452491dd/4ao3SssNM8wCiTo5amAxn.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.00497.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "640777812e309e65452491dd",
      "avatarUrl": "/avatars/01b927675e78f212408168522f65fe36.svg",
      "fullname": "Antoni Bigata",
      "name": "toninio19",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.20605",
      "authors": [
        {
          "_id": "68131e73f0f2a4d8b2d4b06a",
          "user": {
            "_id": "642bcb8ae5b6823cde9301bd",
            "avatarUrl": "/avatars/cddd29afbdbc2fbea90612567090147b.svg",
            "isPro": false,
            "fullname": "Mihai Dan Nadăș",
            "user": "mihainadas",
            "type": "user"
          },
          "name": "Mihai Nadas",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-02T06:34:48.889Z",
          "hidden": false
        },
        {
          "_id": "68131e73f0f2a4d8b2d4b06b",
          "name": "Laura Diosan",
          "hidden": false
        },
        {
          "_id": "68131e73f0f2a4d8b2d4b06c",
          "user": {
            "_id": "67b2344d0ce2aaa57c8c9997",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67b2344d0ce2aaa57c8c9997/LSMjuQNjRsUllUQyt9vNo.jpeg",
            "isPro": false,
            "fullname": "Andrei Piscoran",
            "user": "andreiPiscoran",
            "type": "user"
          },
          "name": "Andrei Piscoran",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-01T07:11:31.916Z",
          "hidden": false
        },
        {
          "_id": "68131e73f0f2a4d8b2d4b06d",
          "user": {
            "_id": "677e4393ef848c5a5352d082",
            "avatarUrl": "/avatars/bcb60ead58969601e2911053550fec62.svg",
            "isPro": false,
            "fullname": "Andreea Tomescu",
            "user": "andreeatomescu",
            "type": "user"
          },
          "name": "Andreea Tomescu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-01T07:10:43.780Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-29T10:15:28.000Z",
      "submittedOnDailyAt": "2025-05-02T06:22:34.343Z",
      "title": "TF1-EN-3M : Ensemble de données d'entraînement pour des petits modèles de langage ouverts utilisant des histoires de moule de trois millions de mots",
      "submittedOnDailyBy": {
        "_id": "642bcb8ae5b6823cde9301bd",
        "avatarUrl": "/avatars/cddd29afbdbc2fbea90612567090147b.svg",
        "isPro": false,
        "fullname": "Mihai Dan Nadăș",
        "user": "mihainadas",
        "type": "user"
      },
      "summary": "La histoire de Morna a été utilisée comme outil historique pour transmettre des valeurs, mais la NLP moderne manque de grandes structures de corpus qui combinent une logique cohérente et des enseignements éthiques explicites. Pour combler cette lacune, nous proposons TF1-EN-3M, le premier jeu de données ouvert qui inclut 3 millions de textes en anglais de Morna, générés avec un modèle de 8B paramètres. Chaque texte est combiné en utilisant un moteur de génération basé sur les séquences de 6 fosfores (personnage -> caractéristique -> environnement -> conflit -> résultat -> éthique), maintenant la véracité des genres et couvrant une large gamme de thèmes.\n\nLe processus d'évaluation hybride évalue (i) la grammaire, créativité, clarté éthique et respect du modèle avec des évaluateurs basés sur GPT, et (ii) combine des métriques de diversité et de lisibilité sans référence. Entre les 10 candidats de poids ouverts, la version de Llama-3 avec 8B paramètres offre le meilleur équilibre entre qualité et vitesse. Avec un seul GPU de la machine (<24GB de VRAM), il est possible de générer 1 000 textes de Morna avec des scores élevés (environ 13,5 centimètres).\n\nLe jeu de données, code de génération, scripts d'évaluation et images complètes de média sont disponibles sous une licence d'utilisation libre, et un benchmark complet de reproductibilité et de coût est également proposé. TF1-EN-3M ouvre des voies pour des recherches en engineering narratif, ajustement de valeurs et éducation par l'IA pour les enfants, démontrant que des modèles de narration à grande échelle ne sont pas nécessaires pour transmettre des valeurs éthiques.",
      "upvotes": 2,
      "discussionId": "68131e73f0f2a4d8b2d4b087",
      "githubRepo": "https://github.com/klusai/tinyfabulist",
      "ai_keywords": [
        "instruction-tuned models",
        "combinatorial prompt engine",
        "GPT-based critic",
        "template adherence",
        "reference-free diversity",
        "Llama-3 variant",
        "computational efficiency",
        "permissive license",
        "child-friendly educational AI",
        "moral storytelling"
      ]
    },
    "publishedAt": "2025-04-29T06:15:28.000Z",
    "title": "TF1-EN-3M: Three Million Synthetic Moral Fables for Training Small, Open\n  Language Models",
    "summary": "Moral stories are a time-tested vehicle for transmitting values, yet modern\nNLP lacks a large, structured corpus that couples coherent narratives with\nexplicit ethical lessons. We close this gap with TF1-EN-3M, the first open\ndataset of three million English-language fables generated exclusively by\ninstruction-tuned models no larger than 8B parameters. Each story follows a\nsix-slot scaffold (character -> trait -> setting -> conflict -> resolution ->\nmoral), produced through a combinatorial prompt engine that guarantees genre\nfidelity while covering a broad thematic space.\n  A hybrid evaluation pipeline blends (i) a GPT-based critic that scores\ngrammar, creativity, moral clarity, and template adherence with (ii)\nreference-free diversity and readability metrics. Among ten open-weight\ncandidates, an 8B-parameter Llama-3 variant delivers the best quality-speed\ntrade-off, producing high-scoring fables on a single consumer GPU (<24 GB VRAM)\nat approximately 13.5 cents per 1,000 fables.\n  We release the dataset, generation code, evaluation scripts, and full\nmetadata under a permissive license, enabling exact reproducibility and cost\nbenchmarking. TF1-EN-3M opens avenues for research in instruction following,\nnarrative intelligence, value alignment, and child-friendly educational AI,\ndemonstrating that large-scale moral storytelling no longer requires\nproprietary giant models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.20605.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642bcb8ae5b6823cde9301bd",
      "avatarUrl": "/avatars/cddd29afbdbc2fbea90612567090147b.svg",
      "fullname": "Mihai Dan Nadăș",
      "name": "mihainadas",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.18983",
      "authors": [
        {
          "_id": "681470d72175e5e7ca0ea002",
          "name": "Xuyin Qi",
          "hidden": false
        },
        {
          "_id": "681470d72175e5e7ca0ea003",
          "name": "Zeyu Zhang",
          "hidden": false
        },
        {
          "_id": "681470d72175e5e7ca0ea004",
          "name": "Canxuan Gang",
          "hidden": false
        },
        {
          "_id": "681470d72175e5e7ca0ea005",
          "name": "Hao Zhang",
          "hidden": false
        },
        {
          "_id": "681470d72175e5e7ca0ea006",
          "name": "Lei Zhang",
          "hidden": false
        },
        {
          "_id": "681470d72175e5e7ca0ea007",
          "name": "Zhiwei Zhang",
          "hidden": false
        },
        {
          "_id": "681470d72175e5e7ca0ea008",
          "name": "Yang Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-26T17:56:56.000Z",
      "submittedOnDailyAt": "2025-05-02T05:44:53.105Z",
      "title": "MediAug : Révision de la visualisation d'images médicales dans l'environnement de travail d'attention au utilisateur",
      "submittedOnDailyBy": {
        "_id": "64ec877bb93654d4ca5c92e9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
        "isPro": false,
        "fullname": "Zeyu Zhang",
        "user": "SteveZeyuZhang",
        "type": "user"
      },
      "summary": "L'extension des données est cruciale pour améliorer la précision de la classification d'images médicales, ainsi que pour améliorer la détection de lesions et la séparation d'organes dans des conditions limitées de données. Cependant, elle présente deux problèmes significatifs. Le premier est le grand écart entre le domaine des images naturelles et médicales, qui peut altérer des caractéristiques importantes des maladies. Le second est que la recherche sur l'extension des données médicales est limitée à une seule architecture ou stratégie, ce qui ne reflète pas clairement les avantages des stratégies hybrides avancées. Pour aborder ces problèmes, nous proposons un cadre d'évaluation qui intègre six méthodes d'extension basées sur des hybrides et des transformers dans deux stratégies : hybride et transformer, appliquées à un ensemble de données d'IRM de tumeurs cérébrales et à des images de fondus d'affections oculaires. Notre contribution comprend trois aspects : premièrement, nous introduisons MediAug, un cadre de référence détaillé et reproductible pour l'extension des données médicales ; deuxièmement, nous effectuons une évaluation systématique des méthodes comme MixUp, YOCO, CropMix, CutMix, AugMix et SnapMix, basées sur ResNet-50 et ViT-B ; et troisièmement, nous présentons des résultats d'expériences qui montrent que MixUp est le plus efficace dans la tâche de classification de tumeurs cérébrales sur ResNet-50 (79,19% de précision), SnapMix dans la tâche de classification de tumeurs cérébrales sur ViT-B (99,44% de précision), YOCO dans la tâche de classification d'affections oculaires sur ResNet-50 (91,60% de précision) et CutMix dans la tâche de classification d'affections oculaires sur ViT-B (97,94% de précision). Le code est disponible sur https://github.com/AIGeeksGroup/MediAug.",
      "upvotes": 1,
      "discussionId": "681470d92175e5e7ca0ea065",
      "ai_keywords": [
        "MediAug",
        "MixUp",
        "YOCO",
        "CropMix",
        "CutMix",
        "AugMix",
        "SnapMix",
        "ResNet-50",
        "ViT-B",
        "brain tumour MRI",
        "eye disease fundus datasets",
        "domain gap",
        "lesion detection",
        "organ segmentation",
        "classification accuracy"
      ]
    },
    "publishedAt": "2025-04-26T13:56:56.000Z",
    "title": "MediAug: Exploring Visual Augmentation in Medical Imaging",
    "summary": "Data augmentation is essential in medical imaging for improving\nclassification accuracy, lesion detection, and organ segmentation under limited\ndata conditions. However, two significant challenges remain. First, a\npronounced domain gap between natural photographs and medical images can\ndistort critical disease features. Second, augmentation studies in medical\nimaging are fragmented and limited to single tasks or architectures, leaving\nthe benefits of advanced mix-based strategies unclear. To address these\nchallenges, we propose a unified evaluation framework with six mix-based\naugmentation methods integrated with both convolutional and transformer\nbackbones on brain tumour MRI and eye disease fundus datasets. Our\ncontributions are threefold. (1) We introduce MediAug, a comprehensive and\nreproducible benchmark for advanced data augmentation in medical imaging. (2)\nWe systematically evaluate MixUp, YOCO, CropMix, CutMix, AugMix, and SnapMix\nwith ResNet-50 and ViT-B backbones. (3) We demonstrate through extensive\nexperiments that MixUp yields the greatest improvement on the brain tumor\nclassification task for ResNet-50 with 79.19% accuracy and SnapMix yields the\ngreatest improvement for ViT-B with 99.44% accuracy, and that YOCO yields the\ngreatest improvement on the eye disease classification task for ResNet-50 with\n91.60% accuracy and CutMix yields the greatest improvement for ViT-B with\n97.94% accuracy. Code will be available at\nhttps://github.com/AIGeeksGroup/MediAug.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.18983.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ec877bb93654d4ca5c92e9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
      "fullname": "Zeyu Zhang",
      "name": "SteveZeyuZhang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  }
]