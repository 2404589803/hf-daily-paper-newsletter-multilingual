[
  {
    "paper": {
      "id": "2503.09566",
      "authors": [
        {
          "_id": "67d274c467e782a7eeb4ab70",
          "name": "Lingmin Ran",
          "hidden": false
        },
        {
          "_id": "67d274c467e782a7eeb4ab71",
          "name": "Mike Zheng Shou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-12T17:33:22.000Z",
      "title": "TPDiff : Pyramide de Temps de Diffusion de Vidéo",
      "summary": "Dans le développement de modèles de diffusion vidéo, des problèmes importants ont été identifiés, notamment la demande significative de calculs. Pour atténuer cette question, nous avons constaté que le processus inverse de diffusion possède la caractéristique interne de réduction de l'entropie. En considérant l'inutilité entre les frames d'un modèle vidéo, il n'est pas nécessaire de maintenir la vitesse des frames à un haut niveau d'entropie. Sur la base de cette perspective, nous proposons un cadre unifié appelé TPDiff. Dans ce cadre, nous divisons la diffusion en étapes et seule la dernière étape opère sur la vitesse des frames totales, optimisant l'efficacité des calculs. Dans le contexte de l'entraînement de modèles de diffusion multi-étapes, nous introduisons un cadre d'entraînement spécial appelé \"diffusion étape par étape\". Dans cette stratégie d'entraînement, nous résolvons l'équation différentielle du flux généralisé de diffusion et de bruit (ODE) pour des données et du bruit, ce qui permet d'appliquer différentes formes de diffusion et améliore l'efficacité de l'entraînement. Selon les résultats expérimentaux spécifiques d'évaluation, nous avons démontré la généralité de notre méthode, avec un baisse du coût d'entraînement de 50% et un accroissement de l'efficacité d'inférence de 1,5 fois.",
      "upvotes": 27,
      "discussionId": "67d274c567e782a7eeb4abb0",
      "ai_keywords": [
        "video diffusion models",
        "entropy-reducing nature",
        "inter-frame redundancy",
        "TPDiff",
        "unified framework",
        "frame rate",
        "diffusion stages",
        "stage-wise diffusion",
        "partitioned probability flow ordinary differential equations (ODE)",
        "diffusion forms"
      ]
    },
    "publishedAt": "2025-03-12T13:33:22.000Z",
    "title": "TPDiff: Temporal Pyramid Video Diffusion Model",
    "summary": "The development of video diffusion models unveils a significant challenge:\nthe substantial computational demands. To mitigate this challenge, we note that\nthe reverse process of diffusion exhibits an inherent entropy-reducing nature.\nGiven the inter-frame redundancy in video modality, maintaining full frame\nrates in high-entropy stages is unnecessary. Based on this insight, we propose\nTPDiff, a unified framework to enhance training and inference efficiency. By\ndividing diffusion into several stages, our framework progressively increases\nframe rate along the diffusion process with only the last stage operating on\nfull frame rate, thereby optimizing computational efficiency. To train the\nmulti-stage diffusion model, we introduce a dedicated training framework:\nstage-wise diffusion. By solving the partitioned probability flow ordinary\ndifferential equations (ODE) of diffusion under aligned data and noise, our\ntraining strategy is applicable to various diffusion forms and further enhances\ntraining efficiency. Comprehensive experimental evaluations validate the\ngenerality of our method, demonstrating 50% reduction in training cost and 1.5x\nimprovement in inference efficiency.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.09566.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.09151",
      "authors": [
        {
          "_id": "67d2784fbe3b4e06086d8eec",
          "user": {
            "_id": "656ee8008bb9f4f8d95bd8f7",
            "avatarUrl": "/avatars/4069d70f1279d928da521211c495d638.svg",
            "isPro": true,
            "fullname": "Hyeonho Jeong",
            "user": "hyeonho-jeong-video",
            "type": "user"
          },
          "name": "Hyeonho Jeong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-13T08:23:54.861Z",
          "hidden": false
        },
        {
          "_id": "67d2784fbe3b4e06086d8eed",
          "name": "Suhyeon Lee",
          "hidden": false
        },
        {
          "_id": "67d2784fbe3b4e06086d8eee",
          "name": "Jong Chul Ye",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-12T08:26:15.000Z",
      "title": "Reangle-A-Video : Génération de Vidéo 4D comme Traduction Vidéo-à-Vidéo",
      "summary": "Reangle-A-Video est un cadre intégré pour générer des vidéos de polyèdres synchronisés à partir d'un seul vidéo d'entrée. Contrairement à la méthode dominante, il ne fait pas usage d'un modèle d'expansion de polyèdres entraîné avec de grands jeux de données 4D. Notre approche utilise des projections d'expansion d'images et de vidéos disponibles publiquement pour reconstruire le problème de traduction vidéo en vidéo. En essence, Reangle-A-Video fonctionne en deux étapes : 1) Entraînement de l'expansion de polyèdres : une transformation d'expansion d'images en vidéo est entraînée automatiquement et ajustée finement pour synchroniser, en extrayant progressivement des mouvements non variables des vidéos glissantes. 2) Traduction des images de coincidence de polyèdres en images : la première frame du vidéo d'entrée est utilisée avec DUSt3R pour générer une image d'initialisation de coincidence de polyèdres sous les guides de coincidence de vues croisées, en soumettant des mouvements et une inflation. Des expériences d'expansion de mouvements statiques et de contrôle dynamique de la caméra montrent que Reangle-A-Video dépasse les méthodes existantes et fournit une nouvelle solution pour la génération de vidéos de polyèdres. Notre code et nos données sont disponibles. Page du projet : https://hyeonho99.github.io/reangle-a-video/",
      "upvotes": 21,
      "discussionId": "67d27857be3b4e06086d9160",
      "projectPage": "https://hyeonho99.github.io/reangle-a-video/",
      "githubRepo": "https://github.com/HyeonHo99/Reangle-Video",
      "ai_keywords": [
        "image-to-video diffusion transformer",
        "self-supervised manner",
        "view-invariant motion",
        "warped videos",
        "Multi-View Consistent Image-to-Images Translation",
        "cross-view consistency",
        "DUSt3R",
        "multi-view video generation",
        "static view transport",
        "dynamic camera control"
      ]
    },
    "publishedAt": "2025-03-12T04:26:15.000Z",
    "title": "Reangle-A-Video: 4D Video Generation as Video-to-Video Translation",
    "summary": "We introduce Reangle-A-Video, a unified framework for generating synchronized\nmulti-view videos from a single input video. Unlike mainstream approaches that\ntrain multi-view video diffusion models on large-scale 4D datasets, our method\nreframes the multi-view video generation task as video-to-videos translation,\nleveraging publicly available image and video diffusion priors. In essence,\nReangle-A-Video operates in two stages. (1) Multi-View Motion Learning: An\nimage-to-video diffusion transformer is synchronously fine-tuned in a\nself-supervised manner to distill view-invariant motion from a set of warped\nvideos. (2) Multi-View Consistent Image-to-Images Translation: The first frame\nof the input video is warped and inpainted into various camera perspectives\nunder an inference-time cross-view consistency guidance using DUSt3R,\ngenerating multi-view consistent starting images. Extensive experiments on\nstatic view transport and dynamic camera control show that Reangle-A-Video\nsurpasses existing methods, establishing a new solution for multi-view video\ngeneration. We will publicly release our code and data. Project page:\nhttps://hyeonho99.github.io/reangle-a-video/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.09151.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.09573",
      "authors": [
        {
          "_id": "67d2511e7d0fc37e67269f85",
          "name": "Marianne Arriola",
          "hidden": false
        },
        {
          "_id": "67d2511e7d0fc37e67269f86",
          "name": "Aaron Gokaslan",
          "hidden": false
        },
        {
          "_id": "67d2511e7d0fc37e67269f87",
          "name": "Justin T Chiu",
          "hidden": false
        },
        {
          "_id": "67d2511e7d0fc37e67269f88",
          "name": "Zhihan Yang",
          "hidden": false
        },
        {
          "_id": "67d2511e7d0fc37e67269f89",
          "name": "Zhixuan Qi",
          "hidden": false
        },
        {
          "_id": "67d2511e7d0fc37e67269f8a",
          "name": "Jiaqi Han",
          "hidden": false
        },
        {
          "_id": "67d2511e7d0fc37e67269f8b",
          "name": "Subham Sekhar Sahoo",
          "hidden": false
        },
        {
          "_id": "67d2511e7d0fc37e67269f8c",
          "name": "Volodymyr Kuleshov",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-12T17:43:40.000Z",
      "title": "Bloque Difusor : Interplâsma entre Auto-Régression et Diffuseur Modèle de Langue",
      "summary": "Les modèles de langue diffusion possèdent des caractéristiques uniques par rapport aux modèles de régression automatique, caractérisés par la possibilité de génération parallèle et la contrôlabilité, tandis que leur généralisation probabiliste est lente et elles se limitent à la génération de longueurs fixes. Dans cette étude, en introduisant la classe de modèles de langue diffusion blocaux qui couvrent l'espace intermédiaire entre les modèles de diffusion et les modèles de régression automatique, on réalise de surmonter les principales limitations de ces deux modèles, offrant une génération de longueurs flexibles et améliorant l'efficacité de l'inférence grâce à l'utilisation de la capture de KV et de la génération de tokens parallèles. On propose un algorithme qui inclut l'estimation du gradient, l'échelle du bruit de données et la minimisation de la variance, pour la construction de modèles de diffusion blocaux efficaces. Ce type de modèles enregistre les meilleurs résultats sur les benchmarks de modélisation de langue, permettant la génération de séquences de longueurs arbitraires. Les codes, les poids du modèle et le blog du projet peuvent être trouvés sur : https://m-arriola.com/bd3lms/",
      "upvotes": 15,
      "discussionId": "67d2511e7d0fc37e67269fbf",
      "projectPage": "https://m-arriola.com/bd3lms/",
      "ai_keywords": [
        "diffusion language models",
        "autoregressive models",
        "parallelized generation",
        "controllability",
        "likelihood modeling",
        "fixed-length generation",
        "block diffusion language models",
        "discrete denoising diffusion",
        "flexible-length generation",
        "inference efficiency",
        "KV caching",
        "parallel token sampling",
        "efficient training algorithm",
        "gradient variance estimators",
        "data-driven noise schedules",
        "arbitrary-length sequences"
      ]
    },
    "publishedAt": "2025-03-12T13:43:40.000Z",
    "title": "Block Diffusion: Interpolating Between Autoregressive and Diffusion\n  Language Models",
    "summary": "Diffusion language models offer unique benefits over autoregressive models\ndue to their potential for parallelized generation and controllability, yet\nthey lag in likelihood modeling and are limited to fixed-length generation. In\nthis work, we introduce a class of block diffusion language models that\ninterpolate between discrete denoising diffusion and autoregressive models.\nBlock diffusion overcomes key limitations of both approaches by supporting\nflexible-length generation and improving inference efficiency with KV caching\nand parallel token sampling. We propose a recipe for building effective block\ndiffusion models that includes an efficient training algorithm, estimators of\ngradient variance, and data-driven noise schedules to minimize the variance.\nBlock diffusion sets a new state-of-the-art performance among diffusion models\non language modeling benchmarks and enables generation of arbitrary-length\nsequences. We provide the code, along with the model weights and blog post on\nthe project page: https://m-arriola.com/bd3lms/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.09573.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.08525",
      "authors": [
        {
          "_id": "67d280f20a6a6dd4a0ffe9e8",
          "name": "Tong Wei",
          "hidden": false
        },
        {
          "_id": "67d280f20a6a6dd4a0ffe9e9",
          "name": "Yijun Yang",
          "hidden": false
        },
        {
          "_id": "67d280f20a6a6dd4a0ffe9ea",
          "name": "Junliang Xing",
          "hidden": false
        },
        {
          "_id": "67d280f20a6a6dd4a0ffe9eb",
          "name": "Yuanchun Shi",
          "hidden": false
        },
        {
          "_id": "67d280f20a6a6dd4a0ffe9ec",
          "name": "Zongqing Lu",
          "hidden": false
        },
        {
          "_id": "67d280f20a6a6dd4a0ffe9ed",
          "name": "Deheng Ye",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-11T15:17:02.000Z",
      "title": "GTR : Guide d'Stimulation de Force Fermée pour Prévenir la Rupture de la Mémoire dans les Agents de RL basés sur des VLM",
      "summary": "RLVR (Apprentissage par renforcement avec récompenses vérifiables) est effectivement appliquée pour accroître le pensée continue (CoT) dans les modèles de langage grands (LLMs). Cependant, cette efficacité n'est pas visible lors de l'entraînement d'agents VLM (Modèles Vision-Language) qui orientent leurs actions vers des objectifs dans des environnements visuels. Dans cette étude, grâce à une large gamme d'expériences, nous avons investigué ce problème en utilisant des tâches concrètes comme des jeux de cartes (par exemple, le jeu de 24 points) ou des tâches dans ALFWorld. Nous avons constaté que lorsque la récompense est basée uniquement sur les résultats des actions, le RL ne peut pas encourager la logique de CoT des VLMs, et nous avons observé un phénomène que nous appelons \"Destruction de la Mente\" (Désintégration de la pensée). Ce phénomène se caractérise par une perte rapide de la diversité des pensées des agents et une logique incomplète et indépendante du contexte qui continue de générer des actions et des récompenses négatives. Pour lutter contre ce phénomène, nous proposons un cadre GTR (Apprentissage par renforcement de la pensée guidée) qui évalue et améliore de manière automatique la logique des agents en étapes. Ce cadre ne nécessite pas d'entraînement étroitement lié à des étiquettes humaines à chaque étape, permettant ainsi un apprentissage simultané de logique et d'actions. Nos expériences montrent que le GTR améliore significativement la capacité de rendement et de généralisation du modèle LLaVA-7b, atteignant un rendement entre 3 et 5 fois supérieur aux modèles de pointe en termes de succès dans les tâches.",
      "upvotes": 8,
      "discussionId": "67d280f30a6a6dd4a0ffea45",
      "ai_keywords": [
        "reinforcement learning",
        "verifiable outcome rewards",
        "chain-of-thought (CoT) reasoning",
        "large language models (LLMs)",
        "vision-language model (VLM)",
        "goal-directed action reasoning",
        "visual environments",
        "complex card games",
        "24 points",
        "embodied tasks",
        "ALFWorld",
        "action outcomes",
        "thought collapse",
        "diversity",
        "state-irrelevant",
        "incomplete reasoning",
        "invalid actions",
        "negative rewards",
        "process guidance",
        "automated corrector",
        "GTR (Guided Thought Reinforcement)",
        "simultaneous training",
        "human labeling",
        "performance",
        "generalization",
        "task success rates",
        "state-of-the-art (SoTA) models",
        "model sizes"
      ]
    },
    "publishedAt": "2025-03-11T11:17:02.000Z",
    "title": "GTR: Guided Thought Reinforcement Prevents Thought Collapse in RL-based\n  VLM Agent Training",
    "summary": "Reinforcement learning with verifiable outcome rewards (RLVR) has effectively\nscaled up chain-of-thought (CoT) reasoning in large language models (LLMs).\nYet, its efficacy in training vision-language model (VLM) agents for\ngoal-directed action reasoning in visual environments is less established. This\nwork investigates this problem through extensive experiments on complex card\ngames, such as 24 points, and embodied tasks from ALFWorld. We find that when\nrewards are based solely on action outcomes, RL fails to incentivize CoT\nreasoning in VLMs, instead leading to a phenomenon we termed thought collapse,\ncharacterized by a rapid loss of diversity in the agent's thoughts,\nstate-irrelevant and incomplete reasoning, and subsequent invalid actions,\nresulting in negative rewards. To counteract thought collapse, we highlight the\nnecessity of process guidance and propose an automated corrector that evaluates\nand refines the agent's reasoning at each RL step. This simple and scalable GTR\n(Guided Thought Reinforcement) framework trains reasoning and action\nsimultaneously without the need for dense, per-step human labeling. Our\nexperiments demonstrate that GTR significantly enhances the performance and\ngeneralization of the LLaVA-7b model across various visual environments,\nachieving 3-5 times higher task success rates compared to SoTA models with\nnotably smaller model sizes.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.08525.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.04388",
      "authors": [
        {
          "_id": "67d05aa2348bae81a8ae572e",
          "name": "Shahar Levy",
          "hidden": false
        },
        {
          "_id": "67d05aa2348bae81a8ae572f",
          "name": "Nir Mazor",
          "hidden": false
        },
        {
          "_id": "67d05aa2348bae81a8ae5730",
          "user": {
            "_id": "63b433ee7af2e415f25b1a7b",
            "avatarUrl": "/avatars/0b03f66d263bffd22ed864d1241fe28b.svg",
            "isPro": false,
            "fullname": "Lihi Shalmon",
            "user": "LihiShalmon",
            "type": "user"
          },
          "name": "Lihi Shalmon",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-11T16:09:23.471Z",
          "hidden": false
        },
        {
          "_id": "67d05aa2348bae81a8ae5731",
          "name": "Michael Hassid",
          "hidden": false
        },
        {
          "_id": "67d05aa2348bae81a8ae5732",
          "name": "Gabriel Stanovsky",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-06T12:38:17.000Z",
      "title": "Plus de registres, même longueur : séparer plusieurs registres pour identifier des problèmes dans le RAG",
      "summary": "RAG fournit des documents liés aux LLMs. Dans des études passées, on a noté que la recherche de plusieurs documents peut affecter le rendement, mais on n'a pas déterminé comment l'augmentation de la quantité de documents affecte le contexte à long terme. Nous avons évalué diverses modèles de langue en utilisant des ensembles de données de tâches de question et de réponse personnalisées. Nous avons modifié la quantité de documents tout en maintenant fixe la longueur du contexte et la position de l'information pertinente. Nous avons constaté que l'augmentation de la quantité de documents dans le RAG peut être un grand problème pour les LLMs. De plus, nos résultats indiquent que le traitement de plusieurs documents est un problème indépendant du traitement de contextes longs. Les données et le code sont disponibles sur : https://github.com/shaharl6000/MoreDocsSameLen",
      "upvotes": 8,
      "discussionId": "67d05aa3348bae81a8ae5780",
      "githubRepo": "https://github.com/shaharl6000/MoreDocsSameLen",
      "ai_keywords": [
        "Retrieval-augmented generation (RAG)",
        "LLMs",
        "relevant documents",
        "multi-hop QA task",
        "document count",
        "long contexts"
      ]
    },
    "publishedAt": "2025-03-06T07:38:17.000Z",
    "title": "More Documents, Same Length: Isolating the Challenge of Multiple\n  Documents in RAG",
    "summary": "Retrieval-augmented generation (RAG) provides LLMs with relevant documents.\nAlthough previous studies noted that retrieving many documents can degrade\nperformance, they did not isolate how the quantity of documents affects\nperformance while controlling for context length. We evaluate various language\nmodels on custom datasets derived from a multi-hop QA task. We keep the context\nlength and position of relevant information constant while varying the number\nof documents, and find that increasing the document count in RAG settings poses\nsignificant challenges for LLMs. Additionally, our results indicate that\nprocessing multiple documents is a separate challenge from handling long\ncontexts. We also make the datasets and code available:\nhttps://github.com/shaharl6000/MoreDocsSameLen .",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04388.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.09601",
      "authors": [
        {
          "_id": "67d29b617d0fc37e673c7e65",
          "name": "Itay Chachy",
          "hidden": false
        },
        {
          "_id": "67d29b617d0fc37e673c7e66",
          "name": "Guy Yariv",
          "hidden": false
        },
        {
          "_id": "67d29b617d0fc37e673c7e67",
          "name": "Sagie Benaim",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-12T17:59:47.000Z",
      "title": "L'intégration des scores par moyenne pondérée des récompenses",
      "summary": "Le Score Distillation Style Sampling (SDS) a apparu comme une métrique efficace pour des tâches telles que la génération de 3D à partir du texte, en utilisant le principe de diffusion 2D. Cependant, bien que puissant, SDS rencontre des difficultés dans l'ajustement précis des objectifs des utilisateurs. Pour surmonter ce problème, nous présentons une nouvelle approche, RewardSDS, qui utilise un poids basé sur les scores d'alignement du modèle de récompenses pour le sampling du bruit. Cette fonction de perte priorise le gradient du sampling du bruit qui produit de hautes récompenses. Notre approche est flexible et permet d'étendre les méthodes basées sur SDS. En particulier, nous présentons RewardVSD pour la Variation de Score Distillation (VSD), évaluant à la fois RewardSDS et RewardVSD sur des tâches telles que la génération d'images à partir du texte, l'édition 2D et la génération de 3D à partir du texte. Dans une variété de métriques, y compris la qualité de la génération et l'ajustement par rapport au modèle de récompenses, RewardSDS et RewardVSD montrent des améliorations significatives par rapport à SDS et VSD, atteignant des performances de pointe dans le domaine. Le site web du projet est disponible sur https://itaychachy.github.io/reward-sds/.",
      "upvotes": 7,
      "discussionId": "67d29b637d0fc37e673c7efc",
      "projectPage": "https://itaychachy.github.io/reward-sds/",
      "githubRepo": "https://github.com/itaychachy/RewardSDS",
      "ai_keywords": [
        "score distillation sampling (SDS)",
        "2D diffusion priors",
        "text-to-3D generation",
        "reward model",
        "weighted SDS loss",
        "gradients",
        "high-reward output",
        "variational score distillation (VSD)",
        "RewardVSD",
        "text-to-image",
        "2D editing",
        "generation quality",
        "alignment to desired reward models"
      ]
    },
    "publishedAt": "2025-03-12T13:59:47.000Z",
    "title": "RewardSDS: Aligning Score Distillation via Reward-Weighted Sampling",
    "summary": "Score Distillation Sampling (SDS) has emerged as an effective technique for\nleveraging 2D diffusion priors for tasks such as text-to-3D generation. While\npowerful, SDS struggles with achieving fine-grained alignment to user intent.\nTo overcome this, we introduce RewardSDS, a novel approach that weights noise\nsamples based on alignment scores from a reward model, producing a weighted SDS\nloss. This loss prioritizes gradients from noise samples that yield aligned\nhigh-reward output. Our approach is broadly applicable and can extend SDS-based\nmethods. In particular, we demonstrate its applicability to Variational Score\nDistillation (VSD) by introducing RewardVSD. We evaluate RewardSDS and\nRewardVSD on text-to-image, 2D editing, and text-to-3D generation tasks,\nshowing significant improvements over SDS and VSD on a diverse set of metrics\nmeasuring generation quality and alignment to desired reward models, enabling\nstate-of-the-art performance. Project page is available at https://itaychachy.\ngithub.io/reward-sds/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.09601.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.06955",
      "authors": [
        {
          "_id": "67d2748117d5cbff7c23621e",
          "user": {
            "_id": "64ec877bb93654d4ca5c92e9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
            "isPro": false,
            "fullname": "Zeyu Zhang",
            "user": "SteveZeyuZhang",
            "type": "user"
          },
          "name": "Zeyu Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-13T08:23:56.553Z",
          "hidden": false
        },
        {
          "_id": "67d2748117d5cbff7c23621f",
          "name": "Yiran Wang",
          "hidden": false
        },
        {
          "_id": "67d2748117d5cbff7c236220",
          "name": "Wei Mao",
          "hidden": false
        },
        {
          "_id": "67d2748117d5cbff7c236221",
          "name": "Danning Li",
          "hidden": false
        },
        {
          "_id": "67d2748117d5cbff7c236222",
          "name": "Rui Zhao",
          "hidden": false
        },
        {
          "_id": "67d2748117d5cbff7c236223",
          "name": "Biao Wu",
          "hidden": false
        },
        {
          "_id": "67d2748117d5cbff7c236224",
          "name": "Zirui Song",
          "hidden": false
        },
        {
          "_id": "67d2748117d5cbff7c236225",
          "name": "Bohan Zhuang",
          "hidden": false
        },
        {
          "_id": "67d2748117d5cbff7c236226",
          "name": "Ian Reid",
          "hidden": false
        },
        {
          "_id": "67d2748117d5cbff7c236227",
          "name": "Richard Hartley",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T06:04:31.000Z",
      "title": "Mouvement Total : C'est la génération de mouvement",
      "summary": "La génération de mouvements conditionnés est un domaine d'étude en vision par ordinateur qui a développé diverses recherches, mais deux problèmes importants restent encore à résoudre. Premier, les méthodes automatiques d'amplification conditionnée récentes ont dépassé les méthodes basées sur les arbres, mais les modèles de masque actuels ne disposent pas de la structure nécessaire pour prioriser en fonction des conditions dynamiques des cadres ou des parties du corps. Deuxième, les méthodes actuelles ne peuvent pas intégrer efficacement plusieurs modèles conditionnés, ce qui limite le contrôle et la cohérence des mouvements générés. Pour aborder ces problèmes, nous introduisons un approche de modélisation de masque basée sur l'attention et proposons le cadre de travail de génération de mouvements pour plusieurs modèles appelé Motion Anything, qui permet un contrôle spatial et temporel détaillé des cadres clés et des actions. Notre modèle améliore la capacité de contrôle par l'encodage adaptatif de conditions multiples, comme texte et musique. De plus, nous avons introduit un nouveau jeu de données de mouvements Text-Music-Dance (TMD), qui comprend 2,153 paires de texte, musique et danse, et est deux fois plus grand que AIST++. Ce jeu de données complète des lacunes importantes dans la communauté. Les expériences prolongées montrent que Motion Anything dépasse les méthodes les plus avancées sur plusieurs benchmarks, améliorant le FID sur HumanML3D d'un 15% et montrant le même gain sur AIST++ et TMD. Vous pouvez trouver plus d'informations sur le projet sur le site web : https://steve-zeyu-zhang.github.io/MotionAnything.",
      "upvotes": 5,
      "discussionId": "67d2748317d5cbff7c2362f2",
      "projectPage": "https://steve-zeyu-zhang.github.io/MotionAnything",
      "githubRepo": "https://github.com/steve-zeyu-zhang/MotionAnything",
      "ai_keywords": [
        "masked autoregressive methods",
        "diffusion-based approaches",
        "masking models",
        "dynamic frames",
        "body parts",
        "conditional motion generation",
        "multimodal motion generation framework",
        "Attention-based Mask Modeling",
        "key frames",
        "actions",
        "multimodal conditions",
        "Text-Music-Dance (TMD)",
        "FID",
        "HumanML3D",
        "AIST++"
      ]
    },
    "publishedAt": "2025-03-10T02:04:31.000Z",
    "title": "Motion Anything: Any to Motion Generation",
    "summary": "Conditional motion generation has been extensively studied in computer\nvision, yet two critical challenges remain. First, while masked autoregressive\nmethods have recently outperformed diffusion-based approaches, existing masking\nmodels lack a mechanism to prioritize dynamic frames and body parts based on\ngiven conditions. Second, existing methods for different conditioning\nmodalities often fail to integrate multiple modalities effectively, limiting\ncontrol and coherence in generated motion. To address these challenges, we\npropose Motion Anything, a multimodal motion generation framework that\nintroduces an Attention-based Mask Modeling approach, enabling fine-grained\nspatial and temporal control over key frames and actions. Our model adaptively\nencodes multimodal conditions, including text and music, improving\ncontrollability. Additionally, we introduce Text-Music-Dance (TMD), a new\nmotion dataset consisting of 2,153 pairs of text, music, and dance, making it\ntwice the size of AIST++, thereby filling a critical gap in the community.\nExtensive experiments demonstrate that Motion Anything surpasses\nstate-of-the-art methods across multiple benchmarks, achieving a 15%\nimprovement in FID on HumanML3D and showing consistent performance gains on\nAIST++ and TMD. See our project website\nhttps://steve-zeyu-zhang.github.io/MotionAnything",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.06955.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.07103",
      "authors": [
        {
          "_id": "67d27d4c6fbfb8ee21886b60",
          "user": {
            "_id": "663486a1f64712540644cb68",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/663486a1f64712540644cb68/YZFR41ERY6UrC6rCC6Nan.jpeg",
            "isPro": true,
            "fullname": "Alessandro",
            "user": "Devy1",
            "type": "user"
          },
          "name": "Alessandro Giagnorio",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-13T08:23:50.840Z",
          "hidden": false
        },
        {
          "_id": "67d27d4c6fbfb8ee21886b61",
          "name": "Antonio Mastropaolo",
          "hidden": false
        },
        {
          "_id": "67d27d4c6fbfb8ee21886b62",
          "name": "Saima Afrin",
          "hidden": false
        },
        {
          "_id": "67d27d4c6fbfb8ee21886b63",
          "user": {
            "_id": "63f378004745321de351a554",
            "avatarUrl": "/avatars/3be79f0f6eb0bcfdc9f31b46d2bafc14.svg",
            "isPro": false,
            "fullname": "Max Di Penta",
            "user": "mdiipenta",
            "type": "user"
          },
          "name": "Massimiliano Di Penta",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-13T06:38:05.744Z",
          "hidden": false
        },
        {
          "_id": "67d27d4c6fbfb8ee21886b64",
          "name": "Gabriele Bavota",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T09:26:08.000Z",
      "title": "La digitalisation appliquée au code de Raytheon Longbow Mowtow pour sa reproduction indépendante",
      "summary": "Les modèles de langage grands (LLMs) montrent une capacité impressionnante pour la génération de code, notamment dans l'implémentation automatique de requêtes écrites en nature. L'efficacité des LLMs augmente généralement lorsqu'ils sont plus grands : un nombre plus élevé de paramètres entrainables du modèle implique un meilleur rendement dans l'implémentation de code. Cependant, lorsqu'on introduit des générateurs de code basés sur des LLMs, on rencontre de grands défis dans leur adaptation au mémoire (et en conséquence, aux processeurs sur lesquels fonctionnent) des dispositifs. Dans des recherches précédentes, Wei et al. ont proposé l'utilisation de générateurs de code basés sur des LLMs pour améliorer l'adaptation au mémoire, et ont travaillé pour éviter une réduction effective. En résumé, en prenant comme exemple un LLM avec 16B paramètres, il a été démontré que l'on peut réduire la précision de type de point flottant 32 bits à 8 bits d'entier, et que cet effet est limité. Alors que la capacité des LLMs et les technologies de réduction continuent d'évoluer rapidement, cet article reproduit les résultats de Wei et al. et explore des orientations pour le processus de réduction de modèles de code liés (avec 34B paramètres), des technologies de réduction modèle (niveau de réduction maximale de paramètres du modèle de 2 bits) et des ensembles de données. Selon l'évaluation expérimentale, la réduction des LLMs atteint un nouveau limite de précision de 4 bits, avec une réduction moyenne de 70% en adaptation au mémoire, sans observation d'une perte importante de rendement. De plus, dans des cas de réduction plus extrêmes (3 bits et 2 bits), les ensembles de données de code peuvent limiter la perte de rendement.",
      "upvotes": 4,
      "discussionId": "67d27d4d6fbfb8ee21886bab",
      "githubRepo": "https://github.com/Devy99/lowbit-quantization",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "code generation",
        "trainable parameters",
        "memory footprint",
        "quantization techniques",
        "precision",
        "floating point 32 bits",
        "int 8 bits",
        "code generation performance",
        "calibration datasets",
        "code-specific calibration datasets"
      ]
    },
    "publishedAt": "2025-03-10T05:26:08.000Z",
    "title": "Quantizing Large Language Models for Code Generation: A Differentiated\n  Replication",
    "summary": "Large Language Models (LLMs) have shown an impressive capability in code\ngeneration and, specifically, to automatically implement requirements described\nin natural language. The LLM effectiveness generally increases with its size:\nThe higher the number of LLM's trainable parameters the better its ability to\nimplement code. However, when it comes to deploying LLM-based code generators,\nlarger LLMs pose significant challenges related to their memory (and,\nconsequently, carbon) footprint. A previous work by Wei et al. proposed to\nleverage quantization techniques to reduce the memory footprint of LLM-based\ncode generators without substantially degrading their effectiveness. In short,\nthey studied LLMs featuring up to 16B parameters, quantizing their precision\nfrom floating point 32 bits down to int 8 bits and showing their limited impact\non code generation performance. Given the fast pace at which LLM capabilities\nand quantization techniques are evolving, in this work we present a\ndifferentiated replication of the work by Wei et al. in which we consider (i)\non the one side, more recent and larger code-related LLMs, of up to 34B\nparameters; (ii) the latest advancements in model quantization techniques,\nwhich allow pushing the compression to the extreme quantization level of 2 bits\nper model parameter and; (iii) different types of calibration datasets to guide\nthe quantization process, including code-specific ones. Our empirical\nevaluation reveals that the new frontier for LLM quantization is 4-bit\nprecision, resulting in an average memory footprint reduction of 70% compared\nto the original model without observing any significant decrease in\nperformance. Additionally, when the quantization becomes even more extreme (3\nand 2 bits), a code-specific calibration dataset helps to limit the loss of\nperformance.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07103.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.09402",
      "authors": [
        {
          "_id": "67d27a9117d5cbff7c2519f6",
          "user": {
            "_id": "64440be5af034cdfd69ca3a7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64440be5af034cdfd69ca3a7/qmx24QiDFT29vleCxL9TX.jpeg",
            "isPro": true,
            "fullname": "Qinghong (Kevin) Lin",
            "user": "KevinQHLin",
            "type": "user"
          },
          "name": "Kevin Qinghong Lin",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-13T08:23:53.060Z",
          "hidden": false
        },
        {
          "_id": "67d27a9117d5cbff7c2519f7",
          "name": "Mike Zheng Shou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-12T13:53:30.000Z",
      "title": "VLog : Recherche d'explications génératives sur les modèles de langage vidéo\nVocabulaire",
      "summary": "Les activités quotidiennes humaines peuvent être exprimées comme une séquence d'événements réguliers, tel que l'allumer l'alarme, et forment l'extérieur de ces activités. En se basant sur cette observation, nous introduisons dans le nouveau cadre de compréhension des images VLog, un méthode pour définir l'extérieur de la description des images, en dépassant les extérieurs généraux des modèles de langage d'images génératifs existants. VLog est construit sur un modèle GPT-2 léger et comporte trois innovations significatives : (i) un modèle de recherche génératif qui combine le pouvoir logique complexe du modèle de langage avec une recherche relativement efficace ; (ii) un extérieur heuristique obtenu à partir des descriptions d'images larges d'un modèle de radiation, qui permet d'indexer de manière efficace des événements spécifiques (par exemple, couper un tomate) en utilisant l'algorithme de codification de paires de description ; (iii) une stratégie pour mettre à jour l'extérieur lorsque l'on détecte un nouvel événement, en utilisant le modèle génératif. Pour valider notre approche, nous présentons VidCap-Eval, un ensemble d'évaluation qui nécessite une évaluation conjointe des images et des descriptions. Les expérimentations sur EgoSchema, COIN et HiREST mettent en évidence l'efficacité de VLog, soulignant sa capacité à générer des descriptions claires, contextuelles et efficaces, et offrant une nouvelle perspective sur la compréhension des images. Le code est disponible sur GitHub à l'adresse https://github.com/showlab/VLog.",
      "upvotes": 3,
      "discussionId": "67d27a9517d5cbff7c251b41",
      "githubRepo": "https://github.com/showlab/VLog",
      "ai_keywords": [
        "generative retrieval model",
        "contrastive retrieval",
        "hierarchical vocabulary",
        "narration pair encoding algorithm",
        "expressive postfixes",
        "vocabulary update strategy",
        "generative models",
        "VidCap-Eval",
        "EgoSchema",
        "COIN",
        "HiREST",
        "concise narrations",
        "reasoning relationships"
      ]
    },
    "publishedAt": "2025-03-12T09:53:30.000Z",
    "title": "VLog: Video-Language Models by Generative Retrieval of Narration\n  Vocabulary",
    "summary": "Human daily activities can be concisely narrated as sequences of routine\nevents (e.g., turning off an alarm) in video streams, forming an event\nvocabulary. Motivated by this, we introduce VLog, a novel video understanding\nframework that define video narrations as vocabulary, going beyond the typical\nsubword vocabularies in existing generative video-language models. Built on the\nlightweight language model GPT-2, VLog feature three key innovations: (i) A\ngenerative retrieval model, marrying language model's complex reasoning\ncapabilities with contrastive retrieval's efficient similarity search. (ii) A\nhierarchical vocabulary derived from large-scale video narrations using our\nnarration pair encoding algorithm, enabling efficient indexing of specific\nevents (e.g., cutting a tomato) by identifying broader scenarios (e.g.,\nkitchen) with expressive postfixes (e.g., by the left hand). (iii) A vocabulary\nupdate strategy leveraging generative models to extend the vocabulary for novel\nevents encountered during inference. To validate our approach, we introduce\nVidCap-Eval, a development set requiring concise narrations with reasoning\nrelationships (e.g., before and after). Experiments on EgoSchema, COIN, and\nHiREST further demonstrate the effectiveness of VLog, highlighting its ability\nto generate concise, contextually accurate, and efficient narrations, offering\na novel perspective on video understanding. Codes are released at\nhttps://github.com/showlab/VLog.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.09402.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.06573",
      "authors": [
        {
          "_id": "67d02a147d82f613a31ed396",
          "name": "Gili Lior",
          "hidden": false
        },
        {
          "_id": "67d02a147d82f613a31ed397",
          "name": "Asaf Yehudai",
          "hidden": false
        },
        {
          "_id": "67d02a147d82f613a31ed398",
          "name": "Ariel Gera",
          "hidden": false
        },
        {
          "_id": "67d02a147d82f613a31ed399",
          "name": "Liat Ein-Dor",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-09T12:06:29.000Z",
      "title": "WildIFEval : Suivez les directives naturelles.",
      "summary": "Récemment, les LMs ont démontré un succès en satisfaisant les instructions des utilisateurs, mais la traitement d'instructions contenant plusieurs restrictions est un grand défi. Dans cet article, nous présentons un grand ensemble de données appelé \"WildIFEval\", qui comprend 12 000 instructions utilisateurs réelles. Différent des ensembles de données précédents, cet ensemble récolte des instructions naturelles des utilisateurs qui abordent diverses grammaires et sujets. Ces restrictions sont classifiées en 8 catégories d'haut niveau pour comprendre leur distribution et les tendances dans des scénarios réels. En utilisant \"WildIFEval\", des expériences larges ont été réalisées pour évaluer la capacité des LMs à suivre les instructions. Il a été découvert que tous les modèles évalués souvent perdent de l'efficacité lorsque les restrictions augmentent. Ces résultats montrent que tous les modèles ont beaucoup de potentiel pour s'améliorer. De plus, il a été constaté que la nature spécifique des restrictions a un grand impact sur le rendement des modèles. L'objectif de publier cet ensemble de données est de favoriser la recherche sur la suivi d'instructions sous conditions complexes et pratiques.",
      "upvotes": 3,
      "discussionId": "67d02a167d82f613a31ed44b"
    },
    "publishedAt": "2025-03-09T08:06:29.000Z",
    "title": "WildIFEval: Instruction Following in the Wild",
    "summary": "Recent LLMs have shown remarkable success in following user instructions, yet\nhandling instructions with multiple constraints remains a significant\nchallenge. In this work, we introduce WildIFEval - a large-scale dataset of 12K\nreal user instructions with diverse, multi-constraint conditions. Unlike prior\ndatasets, our collection spans a broad lexical and topical spectrum of\nconstraints, in natural user prompts. We categorize these constraints into\neight high-level classes to capture their distribution and dynamics in\nreal-world scenarios. Leveraging WildIFEval, we conduct extensive experiments\nto benchmark the instruction-following capabilities of leading LLMs. Our\nfindings reveal that all evaluated models experience performance degradation\nwith an increasing number of constraints. Thus, we show that all models have a\nlarge room for improvement on such tasks. Moreover, we observe that the\nspecific type of constraint plays a critical role in model performance. We\nrelease our dataset to promote further research on instruction-following under\ncomplex, realistic conditions.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.06573.png",
    "numComments": 2,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.09579",
      "authors": [
        {
          "_id": "67d26ae40a955727b9687d8f",
          "user": {
            "_id": "6144e4667f2544bb450787b2",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6144e4667f2544bb450787b2/5wSDDqJbI4TGtBLP9IvjY.png",
            "isPro": false,
            "fullname": "Yingfa Chen",
            "user": "chen-yingfa",
            "type": "user"
          },
          "name": "Yingfa Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-13T08:24:03.831Z",
          "hidden": false
        },
        {
          "_id": "67d26ae40a955727b9687d90",
          "name": "Yutong Wu",
          "hidden": false
        },
        {
          "_id": "67d26ae40a955727b9687d91",
          "name": "Xu Han",
          "hidden": false
        },
        {
          "_id": "67d26ae40a955727b9687d92",
          "name": "Zhiyuan Liu",
          "hidden": false
        },
        {
          "_id": "67d26ae40a955727b9687d93",
          "name": "Maosong Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-12T17:50:42.000Z",
      "title": "Application de l'Attention des Questions Groupées de Coût Optimal (Attention des Questions Groupées de Coût Optimal) dans les LMMs avec un Contexte Long",
      "summary": "Les modèles de grande taille et efficaces basés sur le Transformer (LLMs) ont été l'objet de recherches récentes, nécessitant d'améliorer la capacité linguistique du modèle et de minimiser les coûts d'entraînement et de batch. Les tentatives précédentes ont principalement expliqué les complexes relations entre le rendement du modèle, le nombre de paramètres et la taille des données, et ont cherché l'optimum de distribution des calculs pour les LLMs. Cependant, ces tentatives n'ont pas considéré l'impact de la longueur du contexte et du design des feuilles d'attention (nombre de feuilles de requête et de clé-valeur groupées). Dans cet article, nous comparons de manière systématique différents modèles en considérant le nombre de paramètres, la longueur du contexte et le design des feuilles d'attention. Notre objectif est d'étendre les méthodes actuelles de scalabilité (basées sur le nombre de paramètres et sur le calcul d'entraînement) pour guider la construction de LLMs optimisés en termes de coûts tant d'entraînement que d'inférence. Selon notre étude de scalabilité quantitative, lorsque des séquences suffisamment longues sont traitées, des modèles grands avec peu de feuilles d'attention peuvent réduire la perte, le coût du calcul et le coût de la mémoire. Nos résultats sont particulièrement précieux pour le développement de LLMs à grande échelle, montrant que des modèles grands avec peu de feuilles d'attention sont efficaces en termes de perte, coût du calcul et mémoire.",
      "upvotes": 2,
      "discussionId": "67d26ae90a955727b9687eff",
      "githubRepo": "https://github.com/thunlp/cost-optimal-gqa",
      "ai_keywords": [
        "Transformer-based",
        "large language models (LLMs)",
        "parameter size",
        "context length",
        "attention head configuration",
        "grouped-query attention",
        "query and key-value heads",
        "model performance",
        "computational cost",
        "memory cost",
        "cost-optimal LLMs"
      ]
    },
    "publishedAt": "2025-03-12T13:50:42.000Z",
    "title": "Cost-Optimal Grouped-Query Attention for Long-Context LLMs",
    "summary": "Building effective and efficient Transformer-based large language models\n(LLMs) has recently become a research focus, requiring maximizing model\nlanguage capabilities and minimizing training and deployment costs. Existing\nefforts have primarily described complex relationships among model performance,\nparameter size, and data size, as well as searched for the optimal compute\nallocation to train LLMs. However, they overlook the impacts of context length\nand attention head configuration (the number of query and key-value heads in\ngrouped-query attention) on training and inference. In this paper, we\nsystematically compare models with different parameter sizes, context lengths,\nand attention head configurations in terms of model performance, computational\ncost, and memory cost. Then, we extend the existing scaling methods, which are\nbased solely on parameter size and training compute, to guide the construction\nof cost-optimal LLMs during both training and inference. Our quantitative\nscaling studies show that, when processing sufficiently long sequences, a\nlarger model with fewer attention heads can achieve a lower loss while\nincurring lower computational and memory costs. Our findings provide valuable\ninsights for developing practical LLMs, especially in long-context processing\nscenarios. We will publicly release our code and data.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.09579.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.09419",
      "authors": [
        {
          "_id": "67d27032825fbe674d2e4109",
          "user": {
            "_id": "64a63f9449b08110f761cd73",
            "avatarUrl": "/avatars/61860202fc818b105ef24e74dd4f7d3c.svg",
            "isPro": false,
            "fullname": "Yifan Zhou",
            "user": "SingleZombie",
            "type": "user"
          },
          "name": "Yifan Zhou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-13T08:24:00.485Z",
          "hidden": false
        },
        {
          "_id": "67d27032825fbe674d2e410a",
          "name": "Zeqi Xiao",
          "hidden": false
        },
        {
          "_id": "67d27032825fbe674d2e410b",
          "name": "Shuai Yang",
          "hidden": false
        },
        {
          "_id": "67d27032825fbe674d2e410c",
          "name": "Xingang Pan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-12T14:16:30.000Z",
      "title": "Le modèle de diffusion de la laténique de l'or : amélioration de la symétrie et de la qualité de la diffusion dans l'espace des points de coupure de la diffusion",
      "summary": "Les modèles de diffusion potentiels (LDMs) sont connus pour leur processus de génération instable, ce qui peut entraîner de grandes différences dans les sorties graphiques à partir de petits changements ou de perturbations dans le bruit d'entrée. Cela empêche leur application dans des domaines qui exigent une cohérence dans les résultats. Dans cet article, nous proposons de redéfinir les LDMs pour renforcer la symétrie autour de la corner (ES) et améliorer la cohérence des résultats. L'introduction d'opérations de caricaturisation peut améliorer partiellement la symétrie autour de la corner, mais des problèmes inhérents aux LDMs, tels que l'expansion de la caricaturisation, l'entraînement de la VAE et l'inférence de multiples U-Nets, qui génèrent des résultats instables, persistent. Pour résoudre ces problèmes, nous proposons de redéfinir un architecture de U-Net avec symétrie autour de la corner (ES) et une perte de symétrie autour de la corner (ES) qui supprime effectivement les caractéristiques dans le domaine de la fréquence. Ainsi obtenu, le LDM sans caricaturisation (AF-LDM) présente une forte symétrie autour de la corner et est robuste aux réductions et aux augmentations non régulières. Les expériences étendues montrent que l'AF-LDM génère des résultats plus uniformes dans des applications variées, comme l'édition de vidéos et la traduction d'images en images, comparés aux modèles de base de LDMs. Le code est disponible sur https://github.com/SingleZombie/AFLDM.",
      "upvotes": 2,
      "discussionId": "67d27034825fbe674d2e4185",
      "projectPage": "https://zhouyifan.net/AF-LDM-Page/",
      "githubRepo": "https://github.com/SingleZombie/AFLDM",
      "ai_keywords": [
        "Latent Diffusion Models (LDMs)",
        "shift-equivariant",
        "anti-aliasing operations",
        "aliasing amplification",
        "VAE training",
        "U-Net",
        "self-attention modules",
        "attention modules",
        "equivariance loss",
        "alias-free LDM (AF-LDM)",
        "video editing",
        "image-to-image translation"
      ]
    },
    "publishedAt": "2025-03-12T10:16:30.000Z",
    "title": "Alias-Free Latent Diffusion Models:Improving Fractional Shift\n  Equivariance of Diffusion Latent Space",
    "summary": "Latent Diffusion Models (LDMs) are known to have an unstable generation\nprocess, where even small perturbations or shifts in the input noise can lead\nto significantly different outputs. This hinders their applicability in\napplications requiring consistent results. In this work, we redesign LDMs to\nenhance consistency by making them shift-equivariant. While introducing\nanti-aliasing operations can partially improve shift-equivariance, significant\naliasing and inconsistency persist due to the unique challenges in LDMs,\nincluding 1) aliasing amplification during VAE training and multiple U-Net\ninferences, and 2) self-attention modules that inherently lack\nshift-equivariance. To address these issues, we redesign the attention modules\nto be shift-equivariant and propose an equivariance loss that effectively\nsuppresses the frequency bandwidth of the features in the continuous domain.\nThe resulting alias-free LDM (AF-LDM) achieves strong shift-equivariance and is\nalso robust to irregular warping. Extensive experiments demonstrate that AF-LDM\nproduces significantly more consistent results than vanilla LDM across various\napplications, including video editing and image-to-image translation. Code is\navailable at: https://github.com/SingleZombie/AFLDM",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.09419.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.09600",
      "authors": [
        {
          "_id": "67d270d88f3def6bcbb87b6b",
          "user": {
            "_id": "658e85bb5b7553ca5c29ba89",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658e85bb5b7553ca5c29ba89/KK6UpS9agtrxevvBoup5N.jpeg",
            "isPro": false,
            "fullname": "Jihao Zhao",
            "user": "Robot2050",
            "type": "user"
          },
          "name": "Jihao Zhao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-13T08:23:58.864Z",
          "hidden": false
        },
        {
          "_id": "67d270d88f3def6bcbb87b6c",
          "name": "Zhiyuan Ji",
          "hidden": false
        },
        {
          "_id": "67d270d88f3def6bcbb87b6d",
          "name": "Zhaoxin Fan",
          "hidden": false
        },
        {
          "_id": "67d270d88f3def6bcbb87b6e",
          "name": "Hanyu Wang",
          "hidden": false
        },
        {
          "_id": "67d270d88f3def6bcbb87b6f",
          "name": "Simin Niu",
          "hidden": false
        },
        {
          "_id": "67d270d88f3def6bcbb87b70",
          "name": "Bo Tang",
          "hidden": false
        },
        {
          "_id": "67d270d88f3def6bcbb87b71",
          "name": "Feiyu Xiong",
          "hidden": false
        },
        {
          "_id": "67d270d88f3def6bcbb87b72",
          "name": "Zhiyu Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-12T17:59:42.000Z",
      "title": "Mock : Système de génération de recherches par confusion d'un chatbot de texte",
      "summary": "Le REVIEW AGUAJE (RAG) joue un rôle pratique d'assistance pour les modèles de langage grands (LLMs), mais il mal comprend certains aspects importants du texte segmenté (chunking). Dans cet article, nous présentons une méthodologie d'évaluation double qui intègre l'équilibre et la cohérence du segment, permettant la quantification directe de la qualité du traitement du segment. En utilisant cette évaluation, nous montrons que le traitement du segment individuel et sémantique ont des limites pour aborder des problèmes subtils et complexes dans des contextes étendus, ce qui démontre la nécessité d'intégrer les LLMs dans le processus de traitement du segment pour améliorer le rendement des systèmes RAG. Pour résoudre le compromis intrinsèque entre l'efficacité du calcul et la précision du segment dans les approches basées sur les LLMs, nous proposons un cadre de travail Mixture-of-Chunkers (MoC) pour le degré du segment. Ce cadre de travail consiste en une structure de traitement en trois étapes, avec l'objectif de générer des listes structurées de traitement du segment et d'extraire les expressions régulières de traitement du segment du texte original. Les expériences étendues montrent que notre méthode et le cadre de travail MoC résolvent les problèmes du traitement du segment et améliorent le rendement des systèmes RAG.",
      "upvotes": 1,
      "discussionId": "67d270d98f3def6bcbb87bfb",
      "ai_keywords": [
        "Retrieval-Augmented Generation (RAG)",
        "Large language models (LLMs)",
        "text chunking",
        "dual-metric evaluation method",
        "Boundary Clarity",
        "Chunk Stickiness",
        "chunking quality",
        "traditional chunking",
        "semantic chunking",
        "contextual nuances",
        "granularity-aware Mixture-of-Chunkers (MoC)",
        "three-stage processing mechanism",
        "chunking regular expressions",
        "chunk extraction",
        "chunking task",
        "chunking kernel"
      ]
    },
    "publishedAt": "2025-03-12T13:59:42.000Z",
    "title": "MoC: Mixtures of Text Chunking Learners for Retrieval-Augmented\n  Generation System",
    "summary": "Retrieval-Augmented Generation (RAG), while serving as a viable complement to\nlarge language models (LLMs), often overlooks the crucial aspect of text\nchunking within its pipeline. This paper initially introduces a dual-metric\nevaluation method, comprising Boundary Clarity and Chunk Stickiness, to enable\nthe direct quantification of chunking quality. Leveraging this assessment\nmethod, we highlight the inherent limitations of traditional and semantic\nchunking in handling complex contextual nuances, thereby substantiating the\nnecessity of integrating LLMs into chunking process. To address the inherent\ntrade-off between computational efficiency and chunking precision in LLM-based\napproaches, we devise the granularity-aware Mixture-of-Chunkers (MoC)\nframework, which consists of a three-stage processing mechanism. Notably, our\nobjective is to guide the chunker towards generating a structured list of\nchunking regular expressions, which are subsequently employed to extract chunks\nfrom the original text. Extensive experiments demonstrate that both our\nproposed metrics and the MoC framework effectively settle challenges of the\nchunking task, revealing the chunking kernel while enhancing the performance of\nthe RAG system.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.09600.png",
    "numComments": 2,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.09427",
      "authors": [
        {
          "_id": "67d247c2a48964532e40e78e",
          "name": "Yaorui Shi",
          "hidden": false
        },
        {
          "_id": "67d247c2a48964532e40e78f",
          "name": "Jiaqi Yang",
          "hidden": false
        },
        {
          "_id": "67d247c2a48964532e40e790",
          "name": "Sihang Li",
          "hidden": false
        },
        {
          "_id": "67d247c2a48964532e40e791",
          "name": "Junfeng Fang",
          "hidden": false
        },
        {
          "_id": "67d247c2a48964532e40e792",
          "name": "Xiang Wang",
          "hidden": false
        },
        {
          "_id": "67d247c2a48964532e40e793",
          "name": "Zhiyuan Liu",
          "hidden": false
        },
        {
          "_id": "67d247c2a48964532e40e794",
          "name": "Yang Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-12T14:26:16.000Z",
      "title": "Modélisation des Modalités de Transcriptivité de Cellules Uniques à l'Aide de Modèles de Langue de Haute Précision dans le Domaine-Dal",
      "summary": "Les modèles de langue préalablement entraînés (PLMs) stimulent les innovations dans la recherche scientifique, mais leur application dans l'analyse de cellules uniques est limitée. Les PLMs de contexte ne peuvent pas traiter les données d'ARN de cellules uniques, tandis que les PLMs de cellules ne disposent pas de capacité à traiter des phrases libres, ce qui limite leur utilisation dans des tâches multimodales. L'effet de la croisette entre les modèles actuels peut entraîner des pertes d'information ou un rendement suboptimal en raison de la faiblesse de l'apprentissage antérieur dans les modèles uniques. Pour résoudre ces problèmes, nous proposons le Single-Cell MultiModal Generative Pre-trained Transformer (scMMGPT), un PLM intégré pour les modèles de cellules uniques. Le scMMGPT intègre efficacement les meilleurs PLMs de cellules et de contexte, promouvant la partage de connaissances croisées pour améliorer le rendement. Pour combler la breche entre les modèles de contexte et cellules, le scMMGPT utilise un produiteur croisé de modèles, et a été entraîné avec le plus grand ensemble de données jusqu'à présent, composé de 27 millions de données de 27 millions de cellules uniques.",
      "upvotes": 1,
      "discussionId": "67d247c4a48964532e40e802",
      "ai_keywords": [
        "single-cell RNA sequencing data",
        "pre-trained language models (PLMs)",
        "cell PLMs",
        "cross-modal tasks",
        "information loss",
        "single-modal pre-training",
        "Single-Cell MultiModal Generative Pre-trained Transformer (scMMGPT)",
        "cross-modal projectors",
        "multimodal cell-text PLMs",
        "cell description generation",
        "cell type annotation",
        "$k$-NN accuracy",
        "text-conditioned pseudo-cell generation"
      ]
    },
    "publishedAt": "2025-03-12T10:26:16.000Z",
    "title": "Multimodal Language Modeling for High-Accuracy Single Cell\n  Transcriptomics Analysis and Generation",
    "summary": "Pre-trained language models (PLMs) have revolutionized scientific research,\nyet their application to single-cell analysis remains limited. Text PLMs cannot\nprocess single-cell RNA sequencing data, while cell PLMs lack the ability to\nhandle free text, restricting their use in multimodal tasks. Existing efforts\nto bridge these modalities often suffer from information loss or inadequate\nsingle-modal pre-training, leading to suboptimal performances. To address these\nchallenges, we propose Single-Cell MultiModal Generative Pre-trained\nTransformer (scMMGPT), a unified PLM for joint cell and text modeling. scMMGPT\neffectively integrates the state-of-the-art cell and text PLMs, facilitating\ncross-modal knowledge sharing for improved performance. To bridge the text-cell\nmodality gap, scMMGPT leverages dedicated cross-modal projectors, and undergoes\nextensive pre-training on 27 million cells -- the largest dataset for\nmultimodal cell-text PLMs to date. This large-scale pre-training enables\nscMMGPT to excel in joint cell-text tasks, achieving an 84\\% relative\nimprovement of textual discrepancy for cell description generation, 20.5\\%\nhigher accuracy for cell type annotation, and 4\\% improvement in k-NN\naccuracy for text-conditioned pseudo-cell generation, outperforming baselines.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.09427.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.07588",
      "authors": [
        {
          "_id": "67d1808e13d7b3f8c6ea9147",
          "name": "Junwei Luo",
          "hidden": false
        },
        {
          "_id": "67d1808e13d7b3f8c6ea9148",
          "name": "Yingying Zhang",
          "hidden": false
        },
        {
          "_id": "67d1808e13d7b3f8c6ea9149",
          "name": "Xue Yang",
          "hidden": false
        },
        {
          "_id": "67d1808e13d7b3f8c6ea914a",
          "name": "Kang Wu",
          "hidden": false
        },
        {
          "_id": "67d1808e13d7b3f8c6ea914b",
          "name": "Qi Zhu",
          "hidden": false
        },
        {
          "_id": "67d1808e13d7b3f8c6ea914c",
          "name": "Lei Liang",
          "hidden": false
        },
        {
          "_id": "67d1808e13d7b3f8c6ea914d",
          "name": "Jingdong Chen",
          "hidden": false
        },
        {
          "_id": "67d1808e13d7b3f8c6ea914e",
          "name": "Yansheng Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T17:51:16.000Z",
      "title": "L'encontre du langage de modèle de vision des scènes et des images de surveillance à grande échelle :\nprogrammation des tokens depuis la base jusqu'aux thèmes de peinture",
      "summary": "La compréhension efficace du langage visuel et linguistique dans les images de télédétection à grande échelle (RSIs) a un grand intérêt, mais elle est difficile. Les modèles de vision grande échelle langage (LVLMs) actuels traitent généralement les images en utilisant une grille limitée, ce qui entraîne une perte d'information lorsque l'on traite des RSIs de gigapixel. D'autre part, l'utilisation d'une grille illimitée augmente considérablement les coûts de calcul. Pour préserver les détails des images tout en réduisant la complexité du calcul, nous proposons un méthode de réduction de tokens dans les modèles de texte guidés par des images qui intègre la pyramide graphique cinématique (DIP). Notre approche comprend : (i) un module de focalisation des zones (RFM) qui détecte des régions liées au texte et spécifie des tokens visuels importants. (ii) Une stratégie de sélection de contraste d'images basée sur DIP et réduction de tokens visuels, qui s'appuient sur le résultat du RFM. De plus, les cadres de référence existants pour évaluer la capacité d'observation des LVLMs sont accompagnés d'une diversité limitée de problèmes et de tailles d'images. Nous avons construit un nouveau cadre de référence et établi LRS-VQA, qui comprend 7 333 pairs de questions et réponses dans 8 catégories, et la longueur des images peut s'étendre jusqu'à 27 328 pixels. Notre méthode dépasse les stratégies de haute résolution sur quatre ensembles de données en utilisant la même information, et montre une plus grande efficacité dans des configurations de haute résolution par rapport aux méthodes de réduction de tokens existantes. Les ensembles de données et le code sont disponibles sur https://github.com/VisionXLab/LRS-VQA.",
      "upvotes": 1,
      "discussionId": "67d1809013d7b3f8c6ea9271",
      "ai_keywords": [
        "Large Vision-Language Models (LVLMs)",
        "gigapixel RSIs",
        "Dynamic Image Pyramid (DIP)",
        "text-guided token pruning",
        "Region Focus Module (RFM)",
        "text-aware region localization",
        "vision tokens",
        "coarse-to-fine image tile selection",
        "vision token pruning strategy",
        "large imagery",
        "LRS-VQA",
        "QA pairs",
        "high-resolution strategies",
        "token reduction methods"
      ]
    },
    "publishedAt": "2025-03-10T13:51:16.000Z",
    "title": "When Large Vision-Language Model Meets Large Remote Sensing Imagery:\n  Coarse-to-Fine Text-Guided Token Pruning",
    "summary": "Efficient vision-language understanding of large Remote Sensing Images (RSIs)\nis meaningful but challenging. Current Large Vision-Language Models (LVLMs)\ntypically employ limited pre-defined grids to process images, leading to\ninformation loss when handling gigapixel RSIs. Conversely, using unlimited\ngrids significantly increases computational costs. To preserve image details\nwhile reducing computational complexity, we propose a text-guided token pruning\nmethod with Dynamic Image Pyramid (DIP) integration. Our method introduces: (i)\na Region Focus Module (RFM) that leverages text-aware region localization\ncapability to identify critical vision tokens, and (ii) a coarse-to-fine image\ntile selection and vision token pruning strategy based on DIP, which is guided\nby RFM outputs and avoids directly processing the entire large imagery.\nAdditionally, existing benchmarks for evaluating LVLMs' perception ability on\nlarge RSI suffer from limited question diversity and constrained image sizes.\nWe construct a new benchmark named LRS-VQA, which contains 7,333 QA pairs\nacross 8 categories, with image length up to 27,328 pixels. Our method\noutperforms existing high-resolution strategies on four datasets using the same\ndata. Moreover, compared to existing token reduction methods, our approach\ndemonstrates higher efficiency under high-resolution settings. Dataset and code\nare in https://github.com/VisionXLab/LRS-VQA.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07588.png",
    "numComments": 1,
    "isAuthorParticipating": false
  }
]