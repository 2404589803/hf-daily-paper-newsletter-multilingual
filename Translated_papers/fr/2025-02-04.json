[
  {
    "paper": {
      "id": "2502.01237",
      "authors": [
        {
          "_id": "67a1c1428747511e7b9a1965",
          "user": {
            "_id": "62897fce5d9e25c10e4f319d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62897fce5d9e25c10e4f319d/bMlfAyzkNNZlkQ5mCW6Vc.jpeg",
            "isPro": false,
            "fullname": "Alexey Gorbatovski",
            "user": "Myashka",
            "type": "user"
          },
          "name": "Alexey Gorbatovski",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-04T09:39:00.767Z",
          "hidden": false
        },
        {
          "_id": "67a1c1428747511e7b9a1966",
          "name": "Boris Shaposhnikov",
          "hidden": false
        },
        {
          "_id": "67a1c1428747511e7b9a1967",
          "user": {
            "_id": "6416272d986557e8cac64ece",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6416272d986557e8cac64ece/s3CLjNN_pGj-vJDcENFD2.jpeg",
            "isPro": false,
            "fullname": "Viacheslav",
            "user": "ummagumm-a",
            "type": "user"
          },
          "name": "Viacheslav Sinii",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-04T09:38:52.039Z",
          "hidden": false
        },
        {
          "_id": "67a1c1428747511e7b9a1968",
          "user": {
            "_id": "636e71b2b0ebc04888157b71",
            "avatarUrl": "/avatars/957ba705d470e3a01792741d7f0ff038.svg",
            "isPro": false,
            "fullname": "Alexey Malakhov",
            "user": "ZeL1k7",
            "type": "user"
          },
          "name": "Alexey Malakhov",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-04T09:38:54.121Z",
          "hidden": false
        },
        {
          "_id": "67a1c1428747511e7b9a1969",
          "user": {
            "_id": "62a9c8edc19f92ae443ab37f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669110208492-62a9c8edc19f92ae443ab37f.png",
            "isPro": false,
            "fullname": "Daniil Gavrilov",
            "user": "kefirski",
            "type": "user"
          },
          "name": "Daniil Gavrilov",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-04T09:38:57.087Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-03T10:54:14.000Z",
      "title": "Les différences dans l'algorithme de disposition en ligne sont similaires à celles de l'algorithme de recherche.",
      "summary": "Les algorithmes d'alignement direct (DAAs) remplacent l'apprentissage par rétroalimentation humaine (RLHF) et la modélisation de récompenses (RM) dans l'apprentissage par renforcement (RL) en optimisant directement les politiques pour simplifier la calibration des modèles de langue. Les DAAs sont classifiés en fonction de la perte de classification (classement, tant de type paire contre point) et de la récompense utilisée pour cette perte (par exemple, le raisonnement des probabilités entre la politique et la politique de référence) ou si un pas d'apprentissage supervisé (SFT, tant en deux étapes vs. une étape) est nécessaire. Tout d'abord, il est montré que les méthodes en une étape ont un rendement inférieur aux méthodes en deux étapes. Ces méthodes sont appliquées à ORPO et ASFT en une étape, incluant un pas explicite d'SFT et l'introduction du paramètre beta pour contrôler l'intensité de l'optimisation de la préférence de la politique, ce qui améliore les résultats sur Alpaca Eval 2 avec un +3.46 (ORPO) et un +8.27 (ASFT), démontrant un rendement comparable aux méthodes en deux étapes comme DPO. De plus, l'analyse montre que l'utilisation de cibles de classement ou de classification de paire peut ne pas être également efficace pour toutes les fonctions de récompense ou de perte, soulignant l'importance d'une évaluation précise pour éviter des augmentations excessives du rendement ou l'affirmation de l'excellence générale d'un algorithme de calibration.",
      "upvotes": 33,
      "discussionId": "67a1c1438747511e7b9a19ae"
    },
    "publishedAt": "2025-02-04T03:10:49.348Z",
    "title": "The Differences Between Direct Alignment Algorithms are a Blur",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62897fce5d9e25c10e4f319d/ndKErkZSfT5LvqKfIrC7f.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.01237.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "62897fce5d9e25c10e4f319d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62897fce5d9e25c10e4f319d/bMlfAyzkNNZlkQ5mCW6Vc.jpeg",
      "fullname": "Alexey Gorbatovski",
      "name": "Myashka",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.01456",
      "authors": [
        {
          "_id": "67a19d705efa4fab15497775",
          "user": {
            "_id": "650eba9555dc1e841746f132",
            "avatarUrl": "/avatars/af6f5ee78f161d25ec0afc45d2def8eb.svg",
            "isPro": false,
            "fullname": "Ganqu Cui",
            "user": "ganqu",
            "type": "user"
          },
          "name": "Ganqu Cui",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-04T09:39:23.889Z",
          "hidden": false
        },
        {
          "_id": "67a19d705efa4fab15497776",
          "name": "Lifan Yuan",
          "hidden": false
        },
        {
          "_id": "67a19d705efa4fab15497777",
          "name": "Zefan Wang",
          "hidden": false
        },
        {
          "_id": "67a19d705efa4fab15497778",
          "user": {
            "_id": "6321152b8c0da827c72c7c16",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678783813705-6321152b8c0da827c72c7c16.jpeg",
            "isPro": false,
            "fullname": "Hanbin Wang",
            "user": "hanbin",
            "type": "user"
          },
          "name": "Hanbin Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-04T09:39:25.869Z",
          "hidden": false
        },
        {
          "_id": "67a19d705efa4fab15497779",
          "name": "Wendi Li",
          "hidden": false
        },
        {
          "_id": "67a19d705efa4fab1549777a",
          "name": "Bingxiang He",
          "hidden": false
        },
        {
          "_id": "67a19d705efa4fab1549777b",
          "name": "Yuchen Fan",
          "hidden": false
        },
        {
          "_id": "67a19d705efa4fab1549777c",
          "name": "Tianyu Yu",
          "hidden": false
        },
        {
          "_id": "67a19d705efa4fab1549777d",
          "name": "Qixin Xu",
          "hidden": false
        },
        {
          "_id": "67a19d705efa4fab1549777e",
          "name": "Weize Chen",
          "hidden": false
        },
        {
          "_id": "67a19d705efa4fab1549777f",
          "name": "Jiarui Yuan",
          "hidden": false
        },
        {
          "_id": "67a19d705efa4fab15497780",
          "name": "Huayu Chen",
          "hidden": false
        },
        {
          "_id": "67a19d705efa4fab15497781",
          "name": "Kaiyan Zhang",
          "hidden": false
        },
        {
          "_id": "67a19d705efa4fab15497782",
          "name": "Xingtai Lv",
          "hidden": false
        },
        {
          "_id": "67a19d705efa4fab15497783",
          "name": "Shuo Wang",
          "hidden": false
        },
        {
          "_id": "67a19d705efa4fab15497784",
          "name": "Yuan Yao",
          "hidden": false
        },
        {
          "_id": "67a19d705efa4fab15497785",
          "name": "Xu Han",
          "hidden": false
        },
        {
          "_id": "67a19d705efa4fab15497786",
          "name": "Hao Peng",
          "hidden": false
        },
        {
          "_id": "67a19d705efa4fab15497787",
          "name": "Yu Cheng",
          "hidden": false
        },
        {
          "_id": "67a19d705efa4fab15497788",
          "name": "Zhiyuan Liu",
          "hidden": false
        },
        {
          "_id": "67a19d705efa4fab15497789",
          "name": "Maosong Sun",
          "hidden": false
        },
        {
          "_id": "67a19d705efa4fab1549778a",
          "name": "Bowen Zhou",
          "hidden": false
        },
        {
          "_id": "67a19d705efa4fab1549778b",
          "name": "Ning Ding",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-03T15:43:48.000Z",
      "title": "L'entraînement par récompenses cachées",
      "summary": "Les récompenses denses ont démontré être efficaces comme substituts des récompenses rares dans l'escalage des modèles de langage grands (LLMs) pour des tâches nécessitant une logique multiniveau complexe, comparativement aux niveaux de résultats des récompenses rares. Cependant, elles sont également attrayantes dans l'apprentissage par renforcement (RL) des LLMs, car elle résolvent des problèmes fondamentaux des récompenses de résultat. Cependant, cette possibilité n'a pas encore été atteinte de manière significative. Cela est dû à la facteur de coût et de vulnérabilité à la \"piling on\" de récompenses liée à la collecte de marqueurs de processus de haute qualité lors de l'entraînement en ligne des modèles de récompenses de processus (PRMs). Pour résoudre ces problèmes, on propose PRIME (Apprentissage par Renforcement avec Récompenses Implicites). PRIME permet d'actualiser en ligne les PRMs en utilisant des politiques de rollout et des marqueurs de résultat. PRIME omet le pas d'entraînement des modèles de récompenses spécialisés nécessaires dans les méthodes actuelles, réduisant significativement la charge du dispositif. Les effets de PRIME sont montrés en mathématiques et en programmation. À partir de Qwen2.5-Math-7B-Base, PRIME peut dépasser les modèles de SFT. En particulier, notre modèle Eurus-2-7B-PRIME dépasse Qwen2.5-Math-7B-Instruct en utilisant seulement 10% des données d'entraînement.",
      "upvotes": 26,
      "discussionId": "67a19d705efa4fab154977d0"
    },
    "publishedAt": "2025-02-04T00:02:39.922Z",
    "title": "Process Reinforcement through Implicit Rewards",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.01456.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6321152b8c0da827c72c7c16",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678783813705-6321152b8c0da827c72c7c16.jpeg",
      "fullname": "Hanbin Wang",
      "name": "hanbin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.01534",
      "authors": [
        {
          "_id": "67a1ad77d797fac51fa80770",
          "name": "Dawei Li",
          "hidden": false
        },
        {
          "_id": "67a1ad77d797fac51fa80771",
          "user": {
            "_id": "653a195b0da86d726c9c580c",
            "avatarUrl": "/avatars/61649e1d600fdc1edc50ead0dfa99fdd.svg",
            "isPro": false,
            "fullname": "Renliang Sun",
            "user": "RLSNLP",
            "type": "user"
          },
          "name": "Renliang Sun",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-04T09:39:11.035Z",
          "hidden": false
        },
        {
          "_id": "67a1ad77d797fac51fa80772",
          "name": "Yue Huang",
          "hidden": false
        },
        {
          "_id": "67a1ad77d797fac51fa80773",
          "name": "Ming Zhong",
          "hidden": false
        },
        {
          "_id": "67a1ad77d797fac51fa80774",
          "name": "Bohan Jiang",
          "hidden": false
        },
        {
          "_id": "67a1ad77d797fac51fa80775",
          "name": "Jiawei Han",
          "hidden": false
        },
        {
          "_id": "67a1ad77d797fac51fa80776",
          "name": "Xiangliang Zhang",
          "hidden": false
        },
        {
          "_id": "67a1ad77d797fac51fa80777",
          "name": "Wei Wang",
          "hidden": false
        },
        {
          "_id": "67a1ad77d797fac51fa80778",
          "name": "Huan Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-03T17:13:03.000Z",
      "title": "Bias Leakage : Le Problème de Contamination dans LLM-as-a-Judge",
      "summary": "Les langages grands (LLMs) ont acquis un rôle fondamental dans le développement de modèles, et la synthèse de données basée sur les LLMs a été l'un des deux principaux méthodes de l'explication des données dirigés par les LLMs. Cette combinaison a contribué de manière significative à améliorer l'efficacité dans l'entraînement et l'évaluation des modèles, mais a reçu peu d'attention en relation avec les problèmes potentiels de contamination qui surgissent de ce nouveau paradigme de développement de modèles. Dans cet article, on analyse les problèmes de contamination causés par le biais des jurys en tant qu'évaluateurs et la relation entre les générateurs de données basés sur les LLMs et les évaluateurs. Pour étudier ces problèmes, on définit trois types de relations communes entre les modèles de génération de données et les modèles d'évaluation : le même modèle, une relation d'héritage ou appartenir au même ensemble de modèles. À travers des expériences étendues, on a confirmé expérimentalement que le biais de l'évaluation peut être reconnu lorsque le modèle d'évaluation est biaisé par la préférence de l'évaluateur. L'analyse réalisée montre que le biais de la préférence est un problème large qui est difficile à détecter par rapport au biais des modèles évalués par jurys précédents. Tous ces résultats indiquent que le biais de la préférence est un problème difficile à résoudre dans le domaine des modèles évalués par jurys. Tout le code et les données sont disponibles sur la suivante URL : https://github.com/David-Li0406/Preference-Leakage.",
      "upvotes": 11,
      "discussionId": "67a1ad78d797fac51fa807c1"
    },
    "publishedAt": "2025-02-04T01:04:33.630Z",
    "title": "Preference Leakage: A Contamination Problem in LLM-as-a-judge",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.01534.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6474e1afb68461d5cf7c41cc",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6474e1afb68461d5cf7c41cc/bcoiD_qPrjHUBlB259djg.png",
      "fullname": "Dawei Li",
      "name": "wjldw",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.01061",
      "authors": [
        {
          "_id": "67a1a7a166a8a88726963ef4",
          "name": "Gaojie Lin",
          "hidden": false
        },
        {
          "_id": "67a1a7a166a8a88726963ef5",
          "name": "Jianwen Jiang",
          "hidden": false
        },
        {
          "_id": "67a1a7a166a8a88726963ef6",
          "name": "Jiaqi Yang",
          "hidden": false
        },
        {
          "_id": "67a1a7a166a8a88726963ef7",
          "name": "Zerong Zheng",
          "hidden": false
        },
        {
          "_id": "67a1a7a166a8a88726963ef8",
          "name": "Chao Liang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-03T05:17:32.000Z",
      "title": "OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1:",
      "summary": "Le développement notable dans l'animation humaine, comme le cas des personnes sous l'effet de l'ébranlement sur la table, a montré des progrès significatifs ces dernières années. Cependant, les méthodes actuelles ne peuvent pas être étendues pour des modèles vidéo à grande échelle, ce qui limite leur applicabilité dans des domaines réels. Dans cet article, nous proposons un cadre scalable pour étendre efficacement la base de données en mélangeant des conditions liées au mouvement lors de la phase d'apprentissage. Cela permet d'introduire deux principes d'apprentissage liés au mouvement et de proposer une structure de modèle et des stratégies d'inférence correspondantes. Ce design permet à OmniHuman de maximiser l'utilisation de la génération de mouvements de données pour créer des vidéos de haute qualité de personnes. L'important est que OmniHuman supporte des contenus visuels de différents types, tels que des visages, des photos, des demi-corps et des corps complets, ainsi que des dialogues et des chansons, et aussi traite des interactions entre les personnes et les objets et des états corporels complexes, en plus de gérer différents styles d'images. En comparaison aux méthodes actuelles d'ébranlement, OmniHuman génère des vidéos réalistes et a une flexibilité accrue pour les entrées. De plus, elle supporte plusieurs modes d'ébranlement (ébranlement, vidéo, signaux combinés). Les exemples de vidéos sont disponibles sur la page du projet du ttfamily (https://omnihuman-lab.github.io).",
      "upvotes": 11,
      "discussionId": "67a1a7a466a8a88726963f90"
    },
    "publishedAt": "2025-02-04T00:37:57.949Z",
    "title": "OmniHuman-1: Rethinking the Scaling-Up of One-Stage Conditioned Human Animation Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.01061.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5927
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.18636",
      "authors": [
        {
          "_id": "67a1bfc314cba2eba6da4b2b",
          "name": "Xun Liang",
          "hidden": false
        },
        {
          "_id": "67a1bfc314cba2eba6da4b2c",
          "name": "Simin Niu",
          "hidden": false
        },
        {
          "_id": "67a1bfc314cba2eba6da4b2d",
          "name": "Zhiyu Li",
          "hidden": false
        },
        {
          "_id": "67a1bfc314cba2eba6da4b2e",
          "name": "Sensen Zhang",
          "hidden": false
        },
        {
          "_id": "67a1bfc314cba2eba6da4b2f",
          "user": {
            "_id": "669e0b93c7cb0568dac6e92e",
            "avatarUrl": "/avatars/a39ea77d7391f164af8a80f94f85f2ca.svg",
            "isPro": false,
            "fullname": "hanyu Wang",
            "user": "UglyToilet",
            "type": "user"
          },
          "name": "Hanyu Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-04T09:39:04.452Z",
          "hidden": false
        },
        {
          "_id": "67a1bfc314cba2eba6da4b30",
          "name": "Feiyu Xiong",
          "hidden": false
        },
        {
          "_id": "67a1bfc314cba2eba6da4b31",
          "name": "Jason Zhaoxin Fan",
          "hidden": false
        },
        {
          "_id": "67a1bfc314cba2eba6da4b32",
          "name": "Bo Tang",
          "hidden": false
        },
        {
          "_id": "67a1bfc314cba2eba6da4b33",
          "name": "Shichao Song",
          "hidden": false
        },
        {
          "_id": "67a1bfc314cba2eba6da4b34",
          "name": "Mengwei Wang",
          "hidden": false
        },
        {
          "_id": "67a1bfc314cba2eba6da4b35",
          "name": "Jiawei Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-28T17:01:31.000Z",
      "title": "SafeRAG : Sécurité RAG dans le cadre d'un cadre de référence de sécurité",
      "summary": "Le paradigme de génération avec enregistrements (RAG) dans le domaine de la recherche et de la génération d'enregistrements a connu un grand succès en intégrant un grand modèle de langage (LLMs) avec un banc de données externe pour résoudre des tâches de haut contenu de connaissance. Cependant, l'intégration avec un banc de données externe et un banc de données non confirmé augmente la fragilité des LLMs, permettant aux attaquants de manipuler le savoir pour réaliser des tâches d'attaque. Dans ce travail, on utilise le cadre de référence \"SafeRAG\" pour évaluer la sécurité de RAG. Tout d'abord, les tâches d'attaque sont classifiées en or au son, conflits entre contextes, adware doux, et services blancs de déni. Ensuite, un ensemble de données d'évaluation de sécurité de RAG (ou SafeRAG) est construit manuellement pour chaque tâche d'attaque. Finalement, divers scénarios d'attaque que RAG peut rencontrer sont simulés à l'aide de l'ensemble de données SafeRAG. Des expériences sont réalisées avec 14 composants représentatifs de RAG, montrant que RAG est vulnérable à toutes les tâches d'attaque et que la tâche d'attaque la plus claire est de passer facilement par un module de recherche d'enregistrement existant, un filtre ou un LLM avancé, ce qui réduit la qualité des services de RAG. Le code est disponible sur la URL suivante : https://github.com/IAAR-Shanghai/SafeRAG.",
      "upvotes": 8,
      "discussionId": "67a1bfc414cba2eba6da4b63"
    },
    "publishedAt": "2025-02-04T03:22:06.520Z",
    "title": "SafeRAG: Benchmarking Security in Retrieval-Augmented Generation of Large Language Model",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.18636.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "62a155e615eeab266b2f2243",
      "avatarUrl": "/avatars/e89ef156e73af028e3ce3664e6cb4e62.svg",
      "fullname": "Zhiyu Li",
      "name": "jimi888",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.01068",
      "authors": [
        {
          "_id": "67a1a75f6aa8429da4945eeb",
          "user": {
            "_id": "639ffbc6beb95d698de9640d",
            "avatarUrl": "/avatars/7ef1aaadd5b378d00e17dc548e42cb7e.svg",
            "isPro": false,
            "fullname": "Dongwon Jo",
            "user": "dongwonjo",
            "type": "user"
          },
          "name": "Dongwon Jo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-04T09:39:16.125Z",
          "hidden": false
        },
        {
          "_id": "67a1a75f6aa8429da4945eec",
          "user": {
            "_id": "662672eaebdfec5cfdf1d034",
            "avatarUrl": "/avatars/61bc7add693c555e29ad3c1112215684.svg",
            "isPro": false,
            "fullname": "Jiwon Song",
            "user": "jiwonsong",
            "type": "user"
          },
          "name": "Jiwon Song",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-04T09:39:14.253Z",
          "hidden": false
        },
        {
          "_id": "67a1a75f6aa8429da4945eed",
          "name": "Yulhwa Kim",
          "hidden": false
        },
        {
          "_id": "67a1a75f6aa8429da4945eee",
          "name": "Jae-Joon Kim",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-03T05:25:09.000Z",
      "title": "FastKV : Accélération du traitement de contextes longs par compression de cache KV et propagation sélective de tokens",
      "summary": "Les Modèles de Langage Grands (LLMs) ont un excellent rendement pour traiter des séquences de contexte longues, mais nécessitent une grande mémoire de valeurs clés (KV) pour stocker l'information contextuelle, ce qui représente une charge importante en termes d'efficience informatique et de mémoire. Les tentatives précédentes de compression de la mémoire se sont concentrées uniquement sur la réduction de la demande en mémoire, mais étaient limitées à améliorer le temps de réponse. Pour résoudre ces problèmes, nous présentons FastKV, un méthode de compression de la mémoire conçue pour améliorer le temps de réponse dans des séquences de contexte longues. Pour accroître la vitesse de traitement tout en maintenant la précision, FastKV conserve toute l'information contextuelle dans les couches initiales des LLMs et propage certaines informations de manière sélective dans les couches plus profondes, adoptant une nouvelle approche appelée Propagation de Tokens Selective (TSP). De plus, FastKV adopte la compression de la mémoire de valeurs clés liée au Processus de Consultation de Groupe (GQA) et exploite les avantages de la mémoire et de l'efficience informatique de GQA. À travers les résultats expérimentaux, FastKV a réalisé des améliorations significatives par rapport aux méthodes de compression de la mémoire les plus avancées comme HeadKV, avec un accroissement de 2,00 en TTFT et de 1,40 en Transtorp. De plus, FastKV a réussi à maintenir la précision tout en améliorant le rendement dans les tests de benchmark de contexte long. Le code est disponible sur https://github.com/dongwonjo/FastKV.",
      "upvotes": 7,
      "discussionId": "67a1a7616aa8429da4945f95"
    },
    "publishedAt": "2025-02-04T00:45:45.545Z",
    "title": "FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.01068.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "639ffbc6beb95d698de9640d",
      "avatarUrl": "/avatars/7ef1aaadd5b378d00e17dc548e42cb7e.svg",
      "fullname": "Dongwon Jo",
      "name": "dongwonjo",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.00094",
      "authors": [
        {
          "_id": "67a185ab908f4534beb94b8c",
          "user": {
            "_id": "656864e12d73834278a8dea7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg",
            "isPro": true,
            "fullname": "Ahmed Heakl",
            "user": "ahmedheakl",
            "type": "user"
          },
          "name": "Ahmed Heakl",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-04T09:39:32.712Z",
          "hidden": false
        },
        {
          "_id": "67a185ab908f4534beb94b8d",
          "name": "Sara Ghaboura",
          "hidden": false
        },
        {
          "_id": "67a185ab908f4534beb94b8e",
          "name": "Omkar Thawkar",
          "hidden": false
        },
        {
          "_id": "67a185ab908f4534beb94b8f",
          "name": "Fahad Shahbaz Khan",
          "hidden": false
        },
        {
          "_id": "67a185ab908f4534beb94b90",
          "name": "Hisham Cholakkal",
          "hidden": false
        },
        {
          "_id": "67a185ab908f4534beb94b91",
          "name": "Rao Muhammad Anwer",
          "hidden": false
        },
        {
          "_id": "67a185ab908f4534beb94b92",
          "name": "Salman Khan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-31T18:58:20.000Z",
      "title": "AIN : Modèle d'Inclusion Arabe pour la Diversification Massive",
      "summary": "Le développement rapide des LLMs et l'évolution des grands modèles multimodal (LMMs) à l'autre extrémité, dans les langues comme l'anglais ou le chinois, ont connu un développement notable. En contraste, les modèles de LLMs en arabe ont progressé notablement, mais les LMMs n'ont pas encore été explorés dans de nombreux aspects, surtout dans des aspects spécifiques d'un langage ou dans la compréhension visuelle. Pour combler ce vide, les modèles monomodales en arabe n'ont pas encore été explorés. Le modèle monomodal en arabe (AIN) n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en arabe qui n'a pas encore été exploré. AIN est un modèle monomodal en ara",
      "upvotes": 7,
      "discussionId": "67a185b0908f4534beb94c49"
    },
    "publishedAt": "2025-02-03T22:22:44.375Z",
    "title": "AIN: The Arabic INclusive Large Multimodal Model",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/mmf9V_8rdsi9hN-QdFZV8.png",
      "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/uLq0E1qq75-P4P1KV4xWF.png",
      "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/1eixiKjHGNVm6RaJpdWeq.png",
      "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/XVJSPAgIQcQn8Zi4gUVwi.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.00094.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "656864e12d73834278a8dea7",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg",
      "fullname": "Ahmed Heakl",
      "name": "ahmedheakl",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isMod": false,
      "followerCount": 17
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.01081",
      "authors": [
        {
          "_id": "67a1a56d83c3565727d22f0c",
          "name": "Vernon Y. H. Toh",
          "hidden": false
        },
        {
          "_id": "67a1a56d83c3565727d22f0d",
          "name": "Yew Ken Chia",
          "hidden": false
        },
        {
          "_id": "67a1a56d83c3565727d22f0e",
          "name": "Deepanway Ghosal",
          "hidden": false
        },
        {
          "_id": "67a1a56d83c3565727d22f0f",
          "name": "Soujanya Poria",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-03T05:47:04.000Z",
      "title": "Je ne peux pas traduire le texte car il semble être un mélange de caractères spéciaux et de mots non cohérents, ce qui rend la traduction inopérante.",
      "summary": "La publication de o1 et o3 de OpenAI marque un changement de paradigme important en l'amélioration des capacités logiques avancées dans les modèles de langage grands. En particulier, o3 a démontré un rendement exceptionnel dans la résolution de nouveaux problèmes et dans l'acquisition de nouvelles technologies dans le contexte de l'intelligence artificielle générale (IAG) et l'apprentissage logique à partir d'un corpus de texte. Cependant, ces modèles sont limités aux patrons symboliques, tandis que l'humanité est capable de reconnaître et d'appliquer la logique dans une variété d'escenarios qui incluent des données visuelles et linguistiques. Par conséquent, il est nécessaire d'explorer le développement de différentes capacités logiques dans des modèles comme GPT-[n] et o-[n]. Cela se fait en suivant l'évolution de ces modèles et en défier la résolution de puzzles complexes qui requièrent un reconnaissance visuelle et une logique abstraite ou algorithmique. L'amélioration de la capacité logique de o1 est approximativement 750 fois plus efficace que GPT-4o, ce qui génère des doutes sur son efficacité. Nos résultats montrent un amélioration claire de la capacité logique au cours du processus d'entraînement des modèles, avec un saut clair dans le rendement de GPT-4o à o1. Cependant, o1 présente des difficultés dans des situations qui requièrent une logique abstraite dans des puzzles avec des patrons divers. De plus, son rendement dans les puzzles algorithmiques est toujours insatisfaisant. Nous planifions de suivre continuellement les nouveaux modèles de cette série et d'actualiser les résultats selon cet article. Tous les ressources utilisées dans cette évaluation sont disponibles pour un usage public (https://github.com/declare-lab/LLM-PuzzleTest).",
      "upvotes": 5,
      "discussionId": "67a1a57083c3565727d22fc6"
    },
    "publishedAt": "2025-02-04T00:28:35.436Z",
    "title": "The Jumping Reasoning Curve? Tracking the Evolution of Reasoning Performance in GPT-[n] and o-[n] Models on Multimodal Puzzles",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.01081.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5927
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.01142",
      "authors": [
        {
          "_id": "67a1b4630e9634919de9bc52",
          "user": {
            "_id": "643407dd4b34368fdb0149e8",
            "avatarUrl": "/avatars/9477b9267d5692a4fe59e30590e9639d.svg",
            "isPro": false,
            "fullname": "Xinyan Guan",
            "user": "xinyan233333",
            "type": "user"
          },
          "name": "Xinyan Guan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-04T09:39:08.849Z",
          "hidden": false
        },
        {
          "_id": "67a1b4630e9634919de9bc53",
          "name": "Jiali Zeng",
          "hidden": false
        },
        {
          "_id": "67a1b4630e9634919de9bc54",
          "name": "Fandong Meng",
          "hidden": false
        },
        {
          "_id": "67a1b4630e9634919de9bc55",
          "name": "Chunlei Xin",
          "hidden": false
        },
        {
          "_id": "67a1b4630e9634919de9bc56",
          "name": "Yaojie Lu",
          "hidden": false
        },
        {
          "_id": "67a1b4630e9634919de9bc57",
          "name": "Hongyu Lin",
          "hidden": false
        },
        {
          "_id": "67a1b4630e9634919de9bc58",
          "name": "Xianpei Han",
          "hidden": false
        },
        {
          "_id": "67a1b4630e9634919de9bc59",
          "name": "Le Sun",
          "hidden": false
        },
        {
          "_id": "67a1b4630e9634919de9bc5a",
          "name": "Jie Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-03T08:22:45.000Z",
      "title": "DeepRAG : Modèle de langage à grande échelle pour considérer des étapes de recherche de manière itérative",
      "summary": "Les modèles de langage grands (LLMs) rencontrent des défis dans les conversations factuelles selon la temporalité, la précision et l'efficacité de couverture, démontrant la possibilité de l'argumentation logique. D'un autre côté, l'intégration de l'argumentation logique avec la génération de recherche associée (RAG) fait face à des défis liés à la division efficace des tâches et à l'introduction de bruit et à la perte de qualité dans les réponses en raison de recherches longues et denses. Dans cet article, la logique de la génération de recherche est modélisée comme un processus de décision de Markov (MDP) et est proposé le cadre de travail DeepRAG pour permettre des recherches stratégiques et adaptatives. DeepRAG divise les questions de manière continue et décide dynamiquement si une recherche externe de connaissances sera effectuée ou selon une logique paramétrique. Les résultats des expérimentations montrent que DeepRAG améliore l'efficacité de la recherche et peut augmenter la précision des réponses de 21,99 %, démontrant son efficacité dans l'optimisation de la logique de la génération de recherche associée.",
      "upvotes": 3,
      "discussionId": "67a1b4640e9634919de9bc8b"
    },
    "publishedAt": "2025-02-04T04:35:57.149Z",
    "title": "DeepRAG: Thinking to Retrieval Step by Step for Large Language Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.01142.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "643407dd4b34368fdb0149e8",
      "avatarUrl": "/avatars/9477b9267d5692a4fe59e30590e9639d.svg",
      "fullname": "Xinyan Guan",
      "name": "xinyan233333",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.01100",
      "authors": [
        {
          "_id": "67a1a649f4aecd0dfc96ebf4",
          "user": {
            "_id": "607f666a4ad99100d63ce35c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/607f666a4ad99100d63ce35c/QxhxnvfeV6efkxwUFHwjI.png",
            "isPro": false,
            "fullname": "Bill Yuchen Lin",
            "user": "yuchenlin",
            "type": "user"
          },
          "name": "Bill Yuchen Lin",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-04T09:39:17.972Z",
          "hidden": false
        },
        {
          "_id": "67a1a649f4aecd0dfc96ebf5",
          "user": {
            "_id": "635049104e753c9940fefd71",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/635049104e753c9940fefd71/HgR43XIFw3dneY5ufrAE8.jpeg",
            "isPro": false,
            "fullname": "Ronan Le Bras",
            "user": "ronanlb",
            "type": "user"
          },
          "name": "Ronan Le Bras",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-02-04T05:31:56.722Z",
          "hidden": false
        },
        {
          "_id": "67a1a649f4aecd0dfc96ebf6",
          "name": "Kyle Richardson",
          "hidden": false
        },
        {
          "_id": "67a1a649f4aecd0dfc96ebf7",
          "name": "Ashish Sabharwal",
          "hidden": false
        },
        {
          "_id": "67a1a649f4aecd0dfc96ebf8",
          "name": "Radha Poovendran",
          "hidden": false
        },
        {
          "_id": "67a1a649f4aecd0dfc96ebf9",
          "name": "Peter Clark",
          "hidden": false
        },
        {
          "_id": "67a1a649f4aecd0dfc96ebfa",
          "name": "Yejin Choi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-03T06:44:49.000Z",
      "title": "Jerrabo-Jia: Limites de l'scalabilité de la raisonnabilité logique dans les modèles de langage grands (LLMs)",
      "summary": "Introducing ZebraLogic, un cadre de évaluation détaillé, pour évaluer les performances logiques des LLMs dans les puzzles de grille de logique, qui traduisent les Problèmes de Satisfaction de Contraintes (CSPs) en puzzles plus complexes et quantitatifs. ZebraLogic permet la création de puzzles structurément ajustables et de complexité quantitative, favorisant ainsi une recherche systématique sur les limites d'échelle des modèles tels que Llama, o1 et DeepSeek-R1. En construisant un environnement d'évaluation qui considère la complexité de l'espace de recherche et diverses contraintes logiques, et en fournissant un environnement structuré pour évaluer la logique en fonction de l'augmentation de la difficulté.\n\nNos résultats montrent clairement que lorsque la complexité du problème augmente, l'exactitude diminue significativement, phénomène que nous appelons \"la malédiction de la complexité\", soulignant les limitations inhérentes dans la performance logique des LLMs actuels. De plus, nous examinons des stratégies pour améliorer la syntaxe logique et présentons des méthodes telles que l'échantillonnage Best-of-N, les structures de rétro-traitement et les prompts de preuve automatique. Nos trouvailles fournissent des insights importants sur l'échelle de la performance logique des LLMs, révèlent des limites fondamentales et offrent des possibilités d'amélioration.",
      "upvotes": 3,
      "discussionId": "67a1a64cf4aecd0dfc96ecb8"
    },
    "publishedAt": "2025-02-04T00:32:03.929Z",
    "title": "ZebraLogic: On the Scaling Limits of LLMs for Logical Reasoning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.01100.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5927
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.01441",
      "authors": [
        {
          "_id": "67a189e8fbbab3ce03462fb3",
          "user": {
            "_id": "63e083e6f351dc0745745d17",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e083e6f351dc0745745d17/N0GE4uLrkm14blAQMnm2E.jpeg",
            "isPro": false,
            "fullname": "Quan Dao",
            "user": "quandao10",
            "type": "user"
          },
          "name": "Quan Dao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-04T09:39:30.529Z",
          "hidden": false
        },
        {
          "_id": "67a189e8fbbab3ce03462fb4",
          "name": "Khanh Doan",
          "hidden": false
        },
        {
          "_id": "67a189e8fbbab3ce03462fb5",
          "name": "Di Liu",
          "hidden": false
        },
        {
          "_id": "67a189e8fbbab3ce03462fb6",
          "user": {
            "_id": "66db7db231e772c5ec4c5576",
            "avatarUrl": "/avatars/aa0eb054bd6c881054431a22daf1aea1.svg",
            "isPro": false,
            "fullname": "Trung Le",
            "user": "trungleuc",
            "type": "user"
          },
          "name": "Trung Le",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-02-04T03:30:50.175Z",
          "hidden": false
        },
        {
          "_id": "67a189e8fbbab3ce03462fb7",
          "name": "Dimitris Metaxas",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-03T15:25:58.000Z",
      "title": "Méthode d'entraînement de modèles potentiels de l'incertitude élargie",
      "summary": "Le modèle de consistance est un nouveau membre de la famille des modèles génératifs, capable de générer des échantillons de haute qualité en un seul pas ou plusieurs pas. Récemment, il a démontré un comportement impressionnant, atteignant des résultats dans l'espace de pixels et atteignant des résultats équivalents à ceux des modèles de variation. Cependant, le succès dans l'expansion de l'entraînement de consistance à de grands ensembles de données a été particulièrement décisif dans les tâches de génération d'images et de vidéos à partir du texte, en fonction du rendement dans l'espace potentiel. Dans cet article, on analyse les différences statistiques entre l'espace de pixels et l'espace potentiel, et on a constaté que les données potentielles incluaient des excès d'entrée élevée des outliers, ce qui affectait significativement le rendement du modèle de consistance dans l'espace potentiel. Pour résoudre ce problème, on a remplacé la perte Pseudo-Huber par la perte Cauchy pour réduire efficacement l'influence des outliers. De plus, on a introduit une perte de variation dans les étapes initiales et on a amélioré le rendement en utilisant des copies de transfert optimal (OT). Enfin, on a introduit un scheduler adaptatif et on a intégré une LayerNorm non scalable pour améliorer la compréhension de la statistique des caractéristiques et réduire l'influence des outliers. Ces stratégies ont permis de générer des échantillons de haute qualité en un seul ou plusieurs pas, réduisant significativement la différence entre le rendement des modèles de consistance potentiel et les modèles de variation. La mise en œuvre peut être trouvée sur le lien suivant : https://github.com/quandao10/sLCT/",
      "upvotes": 2,
      "discussionId": "67a189eafbbab3ce0346300b"
    },
    "publishedAt": "2025-02-03T22:32:23.956Z",
    "title": "Improved Training Technique for Latent Consistency Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.01441.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63e083e6f351dc0745745d17",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e083e6f351dc0745745d17/N0GE4uLrkm14blAQMnm2E.jpeg",
      "fullname": "Quan Dao",
      "name": "quandao10",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.01636",
      "authors": [
        {
          "_id": "67a1aa5dc7fa0ccf0a32ceb1",
          "user": {
            "_id": "64e8f4a24f3f7b0b84834315",
            "avatarUrl": "/avatars/242bb68c7ccffe5061c2d1c229ea3b0b.svg",
            "isPro": false,
            "fullname": "Akshat Gupta",
            "user": "akshat57",
            "type": "user"
          },
          "name": "Akshat Gupta",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-02-04T05:53:11.213Z",
          "hidden": false
        },
        {
          "_id": "67a1aa5dc7fa0ccf0a32ceb2",
          "name": "Phudish Prateepamornkul",
          "hidden": false
        },
        {
          "_id": "67a1aa5dc7fa0ccf0a32ceb3",
          "name": "Maochuan Lu",
          "hidden": false
        },
        {
          "_id": "67a1aa5dc7fa0ccf0a32ceb4",
          "name": "Ahmed Alaa",
          "hidden": false
        },
        {
          "_id": "67a1aa5dc7fa0ccf0a32ceb5",
          "name": "Thomas Hartvigsen",
          "hidden": false
        },
        {
          "_id": "67a1aa5dc7fa0ccf0a32ceb6",
          "name": "Gopala Anumanchipalli",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-03T18:59:14.000Z",
      "title": "Lifelong Sequential Knowledge Editing without Model Degradation\n\n한국어로 번역하면:\n\n수명주기 순차적 지식 편집에 모델 저하 없이\n\n(수명주기 순차적 지식 편집에 모델 성능 저하 없이)",
      "summary": "Dans les études précédentes, il a été clairement démontré que les modifications de paramètres dans l'édition des connaissances peuvent causer un impact grave sur le modèle. Dans cet article, on étudie la cause de ce phénomène et on essaie de réaliser 10 000 éditions séquentielles de connaissances tout en maintenant le rendement du modèle initial. Tout d'abord, on montre que l'approche d'édition des connaissances \"éditer en lieu spécifique\" peut causer un surajustement des faits édités. De plus, on démontre clairement que d'utiliser cette approche de manière continue peut conduire à un déséquilibre du norme des noeuds de la matrice éditée. Ensuite, on offre une perspective importante et complexe sur la fonction interne de cette approche. Le développement de la norme est représenté comme une technologie cachée qui attribue une grande importance aux sorties actives générées dans les couches éditées, contribuant de manière significative au rendement du modèle. Cette \"importance de la qualité\" permet que les couches éditées contribuent de manière significative au rendement du modèle. Pour atténuer ces problèmes, on propose ENCORE (Édition de Connaissances Robuste avec Interruption Initiale et Limites de Norma), qui inhibe le surajustement et le déséquilibre du développement de la norme, permettant des éditions séquentielles à long terme et évitant la perte de rendement ultérieure, ce qui permet de réaliser 10 000 éditions séquentielles. De plus, dans Llama3-8B, elle fonctionne 61% plus rapidement que MEMIT et 64% plus rapidement que AlphaEdit, montrant une efficacité notable.",
      "upvotes": 1,
      "discussionId": "67a1aa5fc7fa0ccf0a32cf90"
    },
    "publishedAt": "2025-02-04T00:50:46.370Z",
    "title": "Lifelong Sequential Knowledge Editing without Model Degradation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.01636.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64e8f4a24f3f7b0b84834315",
      "avatarUrl": "/avatars/242bb68c7ccffe5061c2d1c229ea3b0b.svg",
      "fullname": "Akshat Gupta",
      "name": "akshat57",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.01637",
      "authors": [
        {
          "_id": "67a1a51e6aa8429da493d0b5",
          "name": "Da Yu",
          "hidden": false
        },
        {
          "_id": "67a1a51e6aa8429da493d0b6",
          "name": "Edith Cohen",
          "hidden": false
        },
        {
          "_id": "67a1a51e6aa8429da493d0b7",
          "name": "Badih Ghazi",
          "hidden": false
        },
        {
          "_id": "67a1a51e6aa8429da493d0b8",
          "name": "Yangsibo Huang",
          "hidden": false
        },
        {
          "_id": "67a1a51e6aa8429da493d0b9",
          "name": "Pritish Kamath",
          "hidden": false
        },
        {
          "_id": "67a1a51e6aa8429da493d0ba",
          "name": "Ravi Kumar",
          "hidden": false
        },
        {
          "_id": "67a1a51e6aa8429da493d0bb",
          "name": "Daogao Liu",
          "hidden": false
        },
        {
          "_id": "67a1a51e6aa8429da493d0bc",
          "name": "Chiyuan Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-03T18:59:32.000Z",
      "title": "Escalage des couches internes dans les modèles de réseaux",
      "summary": "SCONEN (escalable, context-aware, offline, N-gram embedding) propose une méthode pour améliorer le rendement des modèles de langage lorsque l'étendue des couches est augmentée. Des embeddings de N-grames généraux sont ajoutés tout en maintenant le vecteur de mots original. Ces embeddings fournissent une représentation contextuelle pour chaque token d'entrée et sont entraînés dans un autre modèle. Pendant l'inférence, ils sont calculés et stockés en mémoire off-line de manière anticipée pour minimiser l'impact sur la vitesse d'inférence. SCONEN permet deux nouvelles stratégies d'escalabilité : augmenter le nombre d'embeddings de N-grames stockés et échelonner le modèle qui les entraîne, tout cela sous un budget fixe de FLOPS en temps d'inférence. En appliquant les deux escalabilités, SCONEN dépasse le limite de 1,9 milliards de paramètres et montre un meilleur rendement en temps d'inférence réduit, même sur des corpus divers.",
      "upvotes": 1,
      "discussionId": "67a1a51e6aa8429da493d0d5"
    },
    "publishedAt": "2025-02-04T00:27:13.960Z",
    "title": "Scaling Embedding Layers in Language Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.01637.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5927
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.01591",
      "authors": [
        {
          "_id": "67a1a4b72bf092a7612b36eb",
          "name": "Antoine Dedieu",
          "hidden": false
        },
        {
          "_id": "67a1a4b72bf092a7612b36ec",
          "name": "Joseph Ortiz",
          "hidden": false
        },
        {
          "_id": "67a1a4b72bf092a7612b36ed",
          "name": "Xinghua Lou",
          "hidden": false
        },
        {
          "_id": "67a1a4b72bf092a7612b36ee",
          "name": "Carter Wendelken",
          "hidden": false
        },
        {
          "_id": "67a1a4b72bf092a7612b36ef",
          "name": "Wolfgang Lehrach",
          "hidden": false
        },
        {
          "_id": "67a1a4b72bf092a7612b36f0",
          "name": "J Swaroop Guntupalli",
          "hidden": false
        },
        {
          "_id": "67a1a4b72bf092a7612b36f1",
          "name": "Miguel Lazaro-Gredilla",
          "hidden": false
        },
        {
          "_id": "67a1a4b72bf092a7612b36f2",
          "name": "Kevin Patrick Murphy",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-03T18:25:17.000Z",
      "title": "Amélioration des Modèles Transformer Monde pour Optimiser la RL par rapport aux Données",
      "summary": "Nous proposons un approche appropriée pour l'apprentissage par renforcement basé sur des modèles, permettant d'atteindre de nouveaux rendements sur le défiant benchmark Craftax-classic. Dans ce jeu de survie 2D dans un monde ouvert, il est nécessaire de démontrer des habiletés robustes. Nous avons ajusté les épisodes d'exemples pour améliorer leur qualité et, en appliquant l'algorithme MBRL de 1M pas d'environnement, nous avons atteint un 67,4% de récompense, dépassant significativement le 53,2% de DreamerV3 et atteignant pour la première fois un rendement supérieur à celui des humains (65,0%). Notre méthode construit une ligne de points de l'état de l'art sans modèles et utilise une nouvelle architecture de politiques. Ensuite, nous avons ajouté trois améliorations aux environnements standards de MBRL : (a) \"Dyna avec chauffage\" - apprentissage de la politique avec des données réelles et imaginaires, (b) \"tokenisateur de voisins proches\" - amélioration de la structure d'entrée pour le Modèle Monde Transformer (TWM) pour les images, et (c) \"forçage de blocs\" - explication des tokens futurs du prochain pas de temps par le TWM.",
      "upvotes": 1,
      "discussionId": "67a1a4b82bf092a7612b371b"
    },
    "publishedAt": "2025-02-04T00:25:52.071Z",
    "title": "Improving Transformer World Models for Data-Efficient RL",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.01591.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5927
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.01584",
      "authors": [
        {
          "_id": "67a1e658a68ad21bcdffead6",
          "name": "Carolyn Jane Anderson",
          "hidden": false
        },
        {
          "_id": "67a1e658a68ad21bcdffead7",
          "name": "Joydeep Biswas",
          "hidden": false
        },
        {
          "_id": "67a1e658a68ad21bcdffead8",
          "name": "Aleksander Boruch-Gruszecki",
          "hidden": false
        },
        {
          "_id": "67a1e658a68ad21bcdffead9",
          "name": "Federico Cassano",
          "hidden": false
        },
        {
          "_id": "67a1e658a68ad21bcdffeada",
          "name": "Molly Q Feldman",
          "hidden": false
        },
        {
          "_id": "67a1e658a68ad21bcdffeadb",
          "name": "Arjun Guha",
          "hidden": false
        },
        {
          "_id": "67a1e658a68ad21bcdffeadc",
          "name": "Francesca Lucchetti",
          "hidden": false
        },
        {
          "_id": "67a1e658a68ad21bcdffeadd",
          "name": "Zixuan Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-03T18:10:38.000Z",
      "title": "PhD Knowledge Not Required : Un Défi de Raisonnement pour les Modèles de Langue Large",
      "summary": "Les modèles leaders actuels de benchmark mesurent la connue \"niveau de pile de outils\" de connaissance professionnelle, qui est difficile à comprendre pour les experts. En contraste, nous proposons un benchmark basé sur le défi des pertes du dimanche de NPR, qui requiert des connaissances générales. Notre benchmark est difficile pour les humains et les modèles, mais permet de facilement voir des solutions précises et de détecter des erreurs dans les modèles.\n\nNotre étude révèle des déficiences dans des capacités non détectées par les benchmarks actuels : o1 de OpenAI a une avantage significatif sur les autres modèles de connaissance professionnelle. De plus, notre analyse des sorties de raisonnement a détecté des erreurs dans de nouveaux domaines. Par exemple, DeepSeek R1, qui fournit des réponses incorrectes avant de dire \"aucun autre plan\", montre une \"insatisfaction\" exceptionnelle. D'autre part, la capacité de conclure des raisonnements de Cosa, qui montre que le modèle considère la nécessité de \"mettre fin\" à la forme de raisonnement. De plus, R1 et Gemini Thinking ont été quantifiés dans l'effet de la raisonnement à long terme, et des limites ont été établies dans le benchmark pour améliorer la précision, en particulier lorsqu'un raisonnement à long terme est nécessaire.",
      "upvotes": 0,
      "discussionId": "67a1e659a68ad21bcdffeb04"
    },
    "publishedAt": "2025-02-04T05:06:50.415Z",
    "title": "PhD Knowledge Not Required: A Reasoning Challenge for Large Language Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.01584.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62d8315bad693a1a962864b3",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1664332914111-62d8315bad693a1a962864b3.png",
      "fullname": "Arjun Guha",
      "name": "arjunguha",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.18055",
      "authors": [
        {
          "_id": "67a197099b2f48315e74dcde",
          "user": {
            "_id": "67225dd94201755d88e104c4",
            "avatarUrl": "/avatars/6da69788ce0cd41c86f9dd0bf8d092aa.svg",
            "isPro": false,
            "fullname": "Edwin D. de Jong",
            "user": "EdwinDdeJong",
            "type": "user"
          },
          "name": "Edwin D. de Jong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-04T09:39:28.120Z",
          "hidden": false
        },
        {
          "_id": "67a197099b2f48315e74dcdf",
          "name": "Eric Marcus",
          "hidden": false
        },
        {
          "_id": "67a197099b2f48315e74dce0",
          "name": "Jonas Teuwen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-29T23:38:14.000Z",
      "title": "Les modèles basés sur la pathologie actuellement sont vulnérables aux différences entre institutions médicales.",
      "summary": "La modélisation basée sur la pathologie (FM) a un grand potentiel en la médecine. Il est essentiel de maintenir des différences fortes entre institutions médicales avant de les utiliser dans la pratique clinique. On évalue si les FM se concentrent sur les caractéristiques biologiques des cellules ou du cancer, ou si elles se concentrent sur les différences de signaux causées par la technique de cytologie ou d'autres techniques. On introduit l'Indice de Robustesse. Ce nouveau métrique de robustesse reflète dans quelle mesure les caractéristiques biologiques ont une priorité sur les caractéristiques des signaux. On évalue actuellement 10 FM de pathologie publiques. Les FM actuelles mettent fortement en évidence les caractéristiques des institutions médicales. On observe une différence significative dans l'Indice de Robustesse. Jusqu'à présent, ces FM n'étaient pas aussi robustes, mais il n'y avait pas de modèles qui reflétaient une certaine priorité des caractéristiques biologiques sur celles des signaux. On décrit un méthode pour évaluer l'influence des différences entre institutions médicales sur le rendement prédictif des FM. On analyse comment l'invariance du Modèle de Robustesse Fix (MRF) affecte le rendement de classification des modèles inférieurs, et on confirme que les erreurs de classification par type de cancer ne sont pas aléatoires, mais correspondent spécialement aux signaux de la même institution médicale. On visualise les espaces de cartographie des FM, ce qui montre que les structures de la théorie biologique sont plus fortes que celles imposées par l'institution médicale. En conséquence, la source d'origine de l'institution médicale est plus précise pour prédire le type de tissu et du cancer. L'Indice de Rendement de Robustesse Fix introduit promeut le développement de FM de pathologie pour des applications cliniques fiables et puissantes.",
      "upvotes": 0,
      "discussionId": "67a1970b9b2f48315e74dd5d"
    },
    "publishedAt": "2025-02-04T04:59:22.696Z",
    "title": "Current Pathology Foundation Models are unrobust to Medical Center Differences",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/67225dd94201755d88e104c4/oD8gcxl4D9G3FPXWGVGiz.png",
      "https://cdn-uploads.huggingface.co/production/uploads/67225dd94201755d88e104c4/_jrPyZDKwbr3K9-Q4_sCH.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.18055.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67225dd94201755d88e104c4",
      "avatarUrl": "/avatars/6da69788ce0cd41c86f9dd0bf8d092aa.svg",
      "fullname": "Edwin D. de Jong",
      "name": "EdwinDdeJong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.00314",
      "authors": [
        {
          "_id": "67a1d1ca167bea74d520eb59",
          "name": "Moein Heidari",
          "hidden": false
        },
        {
          "_id": "67a1d1ca167bea74d520eb5a",
          "name": "Ehsan Khodapanah Aghdam",
          "hidden": false
        },
        {
          "_id": "67a1d1ca167bea74d520eb5b",
          "name": "Alexander Manzella",
          "hidden": false
        },
        {
          "_id": "67a1d1ca167bea74d520eb5c",
          "name": "Daniel Hsu",
          "hidden": false
        },
        {
          "_id": "67a1d1ca167bea74d520eb5d",
          "name": "Rebecca Scalabrino",
          "hidden": false
        },
        {
          "_id": "67a1d1ca167bea74d520eb5e",
          "name": "Wenjin Chen",
          "hidden": false
        },
        {
          "_id": "67a1d1ca167bea74d520eb5f",
          "name": "David J. Foran",
          "hidden": false
        },
        {
          "_id": "67a1d1ca167bea74d520eb60",
          "name": "Ilker Hacihaliloglu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-01T04:25:28.000Z",
      "title": "Recherche sur l'amélioration de la performance de la version U-Net - Étude sur la segmentation du cancer du pancréas postérieur",
      "summary": "Derrière le péritoine existent diverses neoplasies, y compris des rares tumeurs bénignes et malignes, ce qui complique leur diagnostic et leur traitement en raison de leur rarité et de leur proximité à des structures importantes. L'estimation du volume de ces tumeurs est complexe en raison de l'asymétrie morphologique. La séparation manuelle nécessite du temps. Bien que l'utilisation de modèles tels que U-Net puisse fournir des résultats attendus, son application implique un consommation informatique élevée. Pour résoudre ces problèmes, des modèles tels que le Modèle de l'État de l'Espace Mamba (SSM) et la Long-Short Term Memory Extended (xLSTM) proposent des solutions efficaces qui réduisent le consommation de ressources tout en gérant les dépendances à longue distance. Dans cette étude, l'extension de U-Net (CNN, ViT, Mamba, xLSTM) est évaluée sur un nouveau jeu de données de scanner et sur un jeu de données de séparation des institutions publiques. Le modèle proposé, ViLU-Net, améliore la séparation en utilisant des blocs Vi. Enfin, xLSTM montre clairement sa efficacité dans le cadre de U-Net. Le code est disponible sur GitHub.",
      "upvotes": 0,
      "discussionId": "67a1d1cd167bea74d520ebf6"
    },
    "publishedAt": "2025-02-04T03:38:34.899Z",
    "title": "A Study on the Performance of U-Net Modifications in Retroperitoneal Tumor Segmentation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.00314.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "61ba19bf6122a4fd29049371",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1639586194527-noauth.jpeg",
      "fullname": "Moein Heidari",
      "name": "moein99",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  }
]