[
  {
    "paper": {
      "id": "2504.15376",
      "authors": [
        {
          "_id": "680bda9c34c8d0bd08e01a25",
          "user": {
            "_id": "64c170190bfb901b04399295",
            "avatarUrl": "/avatars/c30ce7566ae3497ddc989ec8918d37cc.svg",
            "isPro": false,
            "fullname": "Zhiqiu Lin",
            "user": "zhiqiulin",
            "type": "user"
          },
          "name": "Zhiqiu Lin",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-26T08:53:01.030Z",
          "hidden": false
        },
        {
          "_id": "680bda9c34c8d0bd08e01a26",
          "user": {
            "_id": "65f82fb0de5e636ca20184fa",
            "avatarUrl": "/avatars/82974f2e66fa30ecb6d101b19e023910.svg",
            "isPro": false,
            "fullname": "Alan",
            "user": "syCen",
            "type": "user"
          },
          "name": "Siyuan Cen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-26T08:53:05.915Z",
          "hidden": false
        },
        {
          "_id": "680bda9c34c8d0bd08e01a27",
          "name": "Daniel Jiang",
          "hidden": false
        },
        {
          "_id": "680bda9c34c8d0bd08e01a28",
          "name": "Jay Karhade",
          "hidden": false
        },
        {
          "_id": "680bda9c34c8d0bd08e01a29",
          "user": {
            "_id": "67b2db158904ba09ca8feb79",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67b2db158904ba09ca8feb79/faCKKdyroDNCcylEAQZKu.png",
            "isPro": false,
            "fullname": "Hewei Wang",
            "user": "Stephen624",
            "type": "user"
          },
          "name": "Hewei Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-26T08:53:03.301Z",
          "hidden": false
        },
        {
          "_id": "680bda9c34c8d0bd08e01a2a",
          "name": "Chancharik Mitra",
          "hidden": false
        },
        {
          "_id": "680bda9c34c8d0bd08e01a2b",
          "name": "Tiffany Ling",
          "hidden": false
        },
        {
          "_id": "680bda9c34c8d0bd08e01a2c",
          "name": "Yuhan Huang",
          "hidden": false
        },
        {
          "_id": "680bda9c34c8d0bd08e01a2d",
          "name": "Sifan Liu",
          "hidden": false
        },
        {
          "_id": "680bda9c34c8d0bd08e01a2e",
          "name": "Mingyu Chen",
          "hidden": false
        },
        {
          "_id": "680bda9c34c8d0bd08e01a2f",
          "name": "Rushikesh Zawar",
          "hidden": false
        },
        {
          "_id": "680bda9c34c8d0bd08e01a30",
          "name": "Xue Bai",
          "hidden": false
        },
        {
          "_id": "680bda9c34c8d0bd08e01a31",
          "name": "Yilun Du",
          "hidden": false
        },
        {
          "_id": "680bda9c34c8d0bd08e01a32",
          "name": "Chuang Gan",
          "hidden": false
        },
        {
          "_id": "680bda9c34c8d0bd08e01a33",
          "name": "Deva Ramanan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-21T18:34:57.000Z",
      "submittedOnDailyAt": "2025-04-28T00:10:15.204Z",
      "title": "Vers la compréhension des mouvements de la caméra dans n'importe quel vidéo",
      "submittedOnDailyBy": {
        "_id": "65f82fb0de5e636ca20184fa",
        "avatarUrl": "/avatars/82974f2e66fa30ecb6d101b19e023910.svg",
        "isPro": false,
        "fullname": "Alan",
        "user": "syCen",
        "type": "user"
      },
      "summary": "CameraBench est un vaste ensemble de données et de référentiels conçus pour comprendre et améliorer le comportement des caméras. Il comprend environ 3 000 vidéos différentes de l'internet, expliquées par des experts dans le cadre d'un processus de gestion de qualité multiniveau rigoureux. L'un de nos contributeurs est le développement de la base de connaissances fondamentale de l'action de la caméra, l'apprentissage profond. Par exemple, il a été découvert que des actions comme \"tracking\" (ou disparition) nécessitent comprendre le contenu d'un sujet en mouvement. Nous avons effectué des recherches à grande échelle pour mesurer la capacité d'explication humaine, démontrant que le savoir professionnel et la formation basée sur les tutoriels améliorent significativement la précision. Par exemple, les débutants peuvent confondre \"zun\" (changements intrinsèques) avec \"jin\" (changements extrinsèques), mais reçoivent un entraînement pour les distinguer. En utilisant CameraBench, nous évaluons l'action à partir de la structure (SfM) et des modèles de langage vidéo (VLM). Les modèles de SfM facilitent la compréhension des éléments significatifs de base selon le contenu de la scène, tandis que les VLM facilitent la compréhension des éléments de base géométriques nécessitant une estimation de trajectoire précise. Ensuite, nous utilisons CameraBench pour ajuster et optimiser les deux modèles, démontrant des applications variées comme l'ajout de sous-séquences, la réponse à des questions vidéo et la recherche de texte vidéo. Nous espérons que l'avenir de nos efforts pour comprendre le comportement des caméras dans les vidéos se dirige vers l'objectif final, en utilisant l'apprentissage profond, les référentiels et les tutoriels.",
      "upvotes": 110,
      "discussionId": "680bda9e34c8d0bd08e01ae9",
      "projectPage": "https://linzhiqiu.github.io/papers/camerabench/",
      "githubRepo": "https://github.com/sy77777en/CameraBench",
      "ai_keywords": [
        "Structure-from-Motion (SfM)",
        "Video-Language Models (VLMs)",
        "semantic primitives",
        "geometric primitives",
        "generative VLM",
        "motion-augmented captioning",
        "video question answering",
        "video-text retrieval"
      ]
    },
    "publishedAt": "2025-04-21T14:34:57.000Z",
    "title": "Towards Understanding Camera Motions in Any Video",
    "summary": "We introduce CameraBench, a large-scale dataset and benchmark designed to\nassess and improve camera motion understanding. CameraBench consists of ~3,000\ndiverse internet videos, annotated by experts through a rigorous multi-stage\nquality control process. One of our contributions is a taxonomy of camera\nmotion primitives, designed in collaboration with cinematographers. We find,\nfor example, that some motions like \"follow\" (or tracking) require\nunderstanding scene content like moving subjects. We conduct a large-scale\nhuman study to quantify human annotation performance, revealing that domain\nexpertise and tutorial-based training can significantly enhance accuracy. For\nexample, a novice may confuse zoom-in (a change of intrinsics) with translating\nforward (a change of extrinsics), but can be trained to differentiate the two.\nUsing CameraBench, we evaluate Structure-from-Motion (SfM) and Video-Language\nModels (VLMs), finding that SfM models struggle to capture semantic primitives\nthat depend on scene content, while VLMs struggle to capture geometric\nprimitives that require precise estimation of trajectories. We then fine-tune a\ngenerative VLM on CameraBench to achieve the best of both worlds and showcase\nits applications, including motion-augmented captioning, video question\nanswering, and video-text retrieval. We hope our taxonomy, benchmark, and\ntutorials will drive future efforts towards the ultimate goal of understanding\ncamera motions in any video.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15376.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65f82fb0de5e636ca20184fa",
      "avatarUrl": "/avatars/82974f2e66fa30ecb6d101b19e023910.svg",
      "fullname": "Alan",
      "name": "syCen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.16656",
      "authors": [
        {
          "_id": "6809a4ac81a95c83f0c81c83",
          "name": "Chris",
          "hidden": false
        },
        {
          "_id": "6809a4ac81a95c83f0c81c84",
          "name": "Yichen Wei",
          "hidden": false
        },
        {
          "_id": "6809a4ac81a95c83f0c81c85",
          "name": "Yi Peng",
          "hidden": false
        },
        {
          "_id": "6809a4ac81a95c83f0c81c86",
          "name": "Xiaokun Wang",
          "hidden": false
        },
        {
          "_id": "6809a4ac81a95c83f0c81c87",
          "name": "Weijie Qiu",
          "hidden": false
        },
        {
          "_id": "6809a4ac81a95c83f0c81c88",
          "name": "Wei Shen",
          "hidden": false
        },
        {
          "_id": "6809a4ac81a95c83f0c81c89",
          "name": "Tianyidan Xie",
          "hidden": false
        },
        {
          "_id": "6809a4ac81a95c83f0c81c8a",
          "name": "Jiangbo Pei",
          "hidden": false
        },
        {
          "_id": "6809a4ac81a95c83f0c81c8b",
          "name": "Jianhao Zhang",
          "hidden": false
        },
        {
          "_id": "6809a4ac81a95c83f0c81c8c",
          "name": "Yunzhuo Hao",
          "hidden": false
        },
        {
          "_id": "6809a4ac81a95c83f0c81c8d",
          "user": {
            "_id": "6462b241b438438da3c25a5d",
            "avatarUrl": "/avatars/606a67f1be639c9a5e36f293abd5f27a.svg",
            "isPro": false,
            "fullname": "Xuchen Song",
            "user": "xuchensong",
            "type": "user"
          },
          "name": "Xuchen Song",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-25T08:35:17.241Z",
          "hidden": false
        },
        {
          "_id": "6809a4ac81a95c83f0c81c8e",
          "name": "Yang Liu",
          "hidden": false
        },
        {
          "_id": "6809a4ac81a95c83f0c81c8f",
          "name": "Yahui Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-23T12:24:10.000Z",
      "submittedOnDailyAt": "2025-04-28T05:19:19.230Z",
      "title": "Skywork R1V2 : Étude d'un hybride d'apprentissage par renforcement pour la logique en utilisant la domination hybride de la logique",
      "submittedOnDailyBy": {
        "_id": "6462b241b438438da3c25a5d",
        "avatarUrl": "/avatars/606a67f1be639c9a5e36f293abd5f27a.svg",
        "isPro": false,
        "fullname": "Xuchen Song",
        "user": "xuchensong",
        "type": "user"
      },
      "summary": "Introduis Skywork R1V2. Ceci est un modèle cognitif multimodal de ces générations suivantes, qui a évolué de manière significative par rapport à l'anterieur, le Skywork R1V. Le cœur de R1V2 est un paradigme d'apprentissage par renforcement hybride qui combine un modèle de récompense et des stratégies basées sur des règles. Cela permet de résoudre des problèmes anciens de capacité cognitif complexe et de généralisation large. De plus, nous proposons une structure de buffer d'échantillons sélectionnés (SSB) pour améliorer l'efficacité de l'entraînement. Cette structure priorise les échantillons de haute valeur pendant le processus d'optimisation, ce qui résout le problème des \"gains perdus\" dans l'optimiseur de politiques de groupe (GRPO). Spécifiquement, nous observons que des faux positifs se produisent en raison de signaux de forte force excessive, et par un ajustement de la récompense, ce phénomène est observé et atténué systématiquement. Les résultats des expériences démontrent les excellentes capacités de R1V2, montrant un rendement de 62,6 sur OlympiadBench, 79,0 sur AIME2024, 63,6 sur LiveCodeBench et 74,0 sur MMMU. Ces résultats dépassent les modèles open-source actuels et réduisent la différence de rendement avec les systèmes avancés comme Gemini 2,5 ou o4-mini de OpenAI. Les poids du modèle Skywork R1V2 sont disponibles et publiés pour encourager la transparence et la reproductibilité. Le site web est https://huggingface.co/Skywork/Skywork-R1V2-38B.",
      "upvotes": 38,
      "discussionId": "6809a4ae81a95c83f0c81cda",
      "githubRepo": "https://github.com/SkyworkAI/Skywork-R1V",
      "ai_keywords": [
        "reinforcement learning",
        "reward-model guidance",
        "rule-based strategies",
        "Selective Sample Buffer (SSB)",
        "Vanishing Advantages",
        "Group Relative Policy Optimization (GRPO)",
        "visual hallucinations",
        "calibrated reward thresholds",
        "benchmark-leading performances",
        "OlympiadBench",
        "AIME2024",
        "LiveCodeBench",
        "MMMU",
        "Skywork R1V2-38B"
      ]
    },
    "publishedAt": "2025-04-23T08:24:10.000Z",
    "title": "Skywork R1V2: Multimodal Hybrid Reinforcement Learning for Reasoning",
    "summary": "We present Skywork R1V2, a next-generation multimodal reasoning model and a\nmajor leap forward from its predecessor, Skywork R1V. At its core, R1V2\nintroduces a hybrid reinforcement learning paradigm that harmonizes\nreward-model guidance with rule-based strategies, thereby addressing the\nlong-standing challenge of balancing sophisticated reasoning capabilities with\nbroad generalization. To further enhance training efficiency, we propose the\nSelective Sample Buffer (SSB) mechanism, which effectively counters the\n``Vanishing Advantages'' dilemma inherent in Group Relative Policy Optimization\n(GRPO) by prioritizing high-value samples throughout the optimization process.\nNotably, we observe that excessive reinforcement signals can induce visual\nhallucinations--a phenomenon we systematically monitor and mitigate through\ncalibrated reward thresholds throughout the training process. Empirical results\naffirm the exceptional capability of R1V2, with benchmark-leading performances\nsuch as 62.6 on OlympiadBench, 79.0 on AIME2024, 63.6 on LiveCodeBench, and\n74.0 on MMMU. These results underscore R1V2's superiority over existing\nopen-source models and demonstrate significant progress in closing the\nperformance gap with premier proprietary systems, including Gemini 2.5 and\nOpenAI o4-mini. The Skywork R1V2 model weights have been publicly released to\npromote openness and reproducibility\nhttps://huggingface.co/Skywork/Skywork-R1V2-38B.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16656.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6462b241b438438da3c25a5d",
      "avatarUrl": "/avatars/606a67f1be639c9a5e36f293abd5f27a.svg",
      "fullname": "Xuchen Song",
      "name": "xuchensong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.18415",
      "authors": [
        {
          "_id": "680ef1549cc294f617fb14b4",
          "name": "Hongyu Wang",
          "hidden": false
        },
        {
          "_id": "680ef1549cc294f617fb14b5",
          "name": "Shuming Ma",
          "hidden": false
        },
        {
          "_id": "680ef1549cc294f617fb14b6",
          "name": "Furu Wei",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-25T15:17:52.000Z",
      "submittedOnDailyAt": "2025-04-28T01:39:22.422Z",
      "title": "BitNet v2 : Application de la Transformation Hedimard dans un LLM à 1 Bit avec Action Active en 4 Bits Locaux",
      "submittedOnDailyBy": {
        "_id": "63f71771d36951307fcb4dcd",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f71771d36951307fcb4dcd/1c-GSQmSSuDXyVWjxZFy_.jpeg",
        "isPro": false,
        "fullname": "Hongyu Wang",
        "user": "hongyuw",
        "type": "user"
      },
      "summary": "1-bit Large Language Models (LLMs) subissent une efficacité compromise en raison de la présence d'outliers d'activation (outliers d'activation). Ces outliers compliquent la quantification à faibles bits, rendant ainsi l'emplacement efficace plus difficile. Nous présentons un nouveau cadre de travail, BitNet v2, qui permet de quantifier l'activation native de 4 bits des 1-bit LLMs. En particulier, nous proposons le module H-BitLinear, qui applique la transformation de Hadamard en ligne pour résoudre les problèmes d'outliers d'activation dans les réseaux neuronaux d'attention et de propagation avant. Cette transformation régularise la distribution d'activation vers une distribution gaussienne, la transformant en un format approprié pour des représentations en bas de bits. Les résultats expérimentaux montrent que l'entraînement de BitNet v2 avec une activation de 8 bits atteint des rendements comparables à BitNet b1.58. Il est important de souligner que BitNet v2 est entraîné en utilisant l'activation native de 4 bits, ce qui permet des réductions significatives du consommation de mémoire et du coût de calcul dans l'inférence de l'emplacement, sans une diminution significative du rendement.",
      "upvotes": 17,
      "discussionId": "680ef1559cc294f617fb1536",
      "ai_keywords": [
        "BitNet v2",
        "1-bit Large Language Models (LLMs)",
        "activation outliers",
        "quantization",
        "4-bit activation quantization",
        "H-BitLinear",
        "Hadamard transformation",
        "activation distributions",
        "Gaussian-like forms",
        "low-bit representation",
        "8-bit activations",
        "BitNet b1.58",
        "batched inference"
      ]
    },
    "publishedAt": "2025-04-25T11:17:52.000Z",
    "title": "BitNet v2: Native 4-bit Activations with Hadamard Transformation for\n  1-bit LLMs",
    "summary": "Efficient deployment of 1-bit Large Language Models (LLMs) is hindered by\nactivation outliers, which complicate quantization to low bit-widths. We\nintroduce BitNet v2, a novel framework enabling native 4-bit activation\nquantization for 1-bit LLMs. To tackle outliers in attention and feed-forward\nnetwork activations, we propose H-BitLinear, a module applying an online\nHadamard transformation prior to activation quantization. This transformation\nsmooths sharp activation distributions into more Gaussian-like forms, suitable\nfor low-bit representation. Experiments show BitNet v2 trained from scratch\nwith 8-bit activations matches BitNet b1.58 performance. Crucially, BitNet v2\nachieves minimal performance degradation when trained with native 4-bit\nactivations, significantly reducing memory footprint and computational cost for\nbatched inference.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.18415.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63f71771d36951307fcb4dcd",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f71771d36951307fcb4dcd/1c-GSQmSSuDXyVWjxZFy_.jpeg",
      "fullname": "Hongyu Wang",
      "name": "hongyuw",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 17
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.17821",
      "authors": [
        {
          "_id": "680f56b8da9639d22c64443f",
          "name": "Xinyu Chen",
          "hidden": false
        },
        {
          "_id": "680f56b8da9639d22c644440",
          "name": "Yunxin Li",
          "hidden": false
        },
        {
          "_id": "680f56b8da9639d22c644441",
          "name": "Haoyuan Shi",
          "hidden": false
        },
        {
          "_id": "680f56b8da9639d22c644442",
          "name": "Baotian Hu",
          "hidden": false
        },
        {
          "_id": "680f56b8da9639d22c644443",
          "name": "Wenhan Luo",
          "hidden": false
        },
        {
          "_id": "680f56b8da9639d22c644444",
          "name": "Yaowei Wang",
          "hidden": false
        },
        {
          "_id": "680f56b8da9639d22c644445",
          "name": "Min Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-23T13:47:30.000Z",
      "submittedOnDailyAt": "2025-04-28T09:15:13.533Z",
      "title": "VideoVista-CulturalLingo : Limites de 360° - Connexion de la culture, du langage et des zones dans la compréhension des films",
      "submittedOnDailyBy": {
        "_id": "62fdb01bc1588e1d4c6c1a7c",
        "avatarUrl": "/avatars/bd03085995b1c34e0ac8a845cf2c4e83.svg",
        "isPro": false,
        "fullname": "Yunxin Li",
        "user": "YunxinLi",
        "type": "user"
      },
      "summary": "Évaluation d'un système d'IA multimodal pour comprendre des vidéos mesure sa capacité d'entendre et d'expliquer. Les marqueurs d'évaluation de vidéos se concentrent généralement sur l'anglais et utilisent des vidéos typiquement de cultures occidentales. Dans cet article, nous présentons VideoVista-CulturalLingo, le premier marqueur d'évaluation de vidéos. Ce marqueur relie les échanges culturels, linguistiques et régionaux dans la compréhension de vidéos. Notre recherche présente des différences par rapport aux marqueurs existants :\n\n1. Diversité Culturelle : On utilise des vidéos de Chine, États-Unis et Europe.\n2. Multilinguisme : On inclut des questions écrites en chinois et anglais, les deux langues les plus communes.\n3. Larges Domaines : On utilise des vidéos de milliers d'aires créées par des humains.\n\nVideoVista-CulturalLingo inclut 1 389 vidéos et 3 134 paires de questions et réponses, évaluant 24 modèles de vidéos ouvert source ou propriétaire. Les résultats des expérimentations révèlent :\n\n1. Les modèles actuels ne montrent pas de meilleurs résultats pour les questions axées sur les cultures orientales, en particulier pour les questions sur l'histoire de la Chine.\n2. Les modèles ouvert source présentent des limites spécifiques à la compréhension temporelle, atteignant un maximum de 45,2 % de note dans des tâches comme la détection d'événements.\n3. Les modèles principaux montrent une excellente compétence pour les questions scientifiques générales, mais les modèles ouvert source présentent des déficiences en mathématiques.",
      "upvotes": 14,
      "discussionId": "680f56bdda9639d22c64456b",
      "projectPage": "https://videovista-culturallingo.github.io/",
      "githubRepo": "https://github.com/HITsz-TMG/VideoVista"
    },
    "publishedAt": "2025-04-23T09:47:30.000Z",
    "title": "VideoVista-CulturalLingo: 360^circ Horizons-Bridging Cultures,\n  Languages, and Domains in Video Comprehension",
    "summary": "Assessing the video comprehension capabilities of multimodal AI systems can\neffectively measure their understanding and reasoning abilities. Most video\nevaluation benchmarks are limited to a single language, typically English, and\npredominantly feature videos rooted in Western cultural contexts. In this\npaper, we present VideoVista-CulturalLingo, the first video evaluation\nbenchmark designed to bridge cultural, linguistic, and domain divide in video\ncomprehension. Our work differs from existing benchmarks in the following ways:\n1) Cultural diversity, incorporating cultures from China, North America, and\nEurope; 2) Multi-linguistics, with questions presented in Chinese and\nEnglish-two of the most widely spoken languages; and 3) Broad domain, featuring\nvideos sourced from hundreds of human-created domains. VideoVista-CulturalLingo\ncontains 1,389 videos and 3,134 QA pairs, and we have evaluated 24 recent\nopen-source or proprietary video large models. From the experiment results, we\nobserve that: 1) Existing models perform worse on Chinese-centric questions\nthan Western-centric ones, particularly those related to Chinese history; 2)\nCurrent open-source models still exhibit limitations in temporal understanding,\nespecially in the Event Localization task, achieving a maximum score of only\n45.2%; 3) Mainstream models demonstrate strong performance in general\nscientific questions, while open-source models demonstrate weak performance in\nmathematics.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17821.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62fdb01bc1588e1d4c6c1a7c",
      "avatarUrl": "/avatars/bd03085995b1c34e0ac8a845cf2c4e83.svg",
      "fullname": "Yunxin Li",
      "name": "YunxinLi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.16427",
      "authors": [
        {
          "_id": "680c48805ec65044c2861a6a",
          "user": {
            "_id": "669090c01e3f5b16ce22b535",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/669090c01e3f5b16ce22b535/AT-k66Mt5FtbImnhNQJ_J.jpeg",
            "isPro": false,
            "fullname": "Hanlei Zhang",
            "user": "HanleiZhang",
            "type": "user"
          },
          "name": "Hanlei Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-26T08:52:58.965Z",
          "hidden": false
        },
        {
          "_id": "680c48805ec65044c2861a6b",
          "name": "Zhuohang Li",
          "hidden": false
        },
        {
          "_id": "680c48805ec65044c2861a6c",
          "name": "Yeshuang Zhu",
          "hidden": false
        },
        {
          "_id": "680c48805ec65044c2861a6d",
          "name": "Hua Xu",
          "hidden": false
        },
        {
          "_id": "680c48805ec65044c2861a6e",
          "name": "Peiwu Wang",
          "hidden": false
        },
        {
          "_id": "680c48805ec65044c2861a6f",
          "name": "Haige Zhu",
          "hidden": false
        },
        {
          "_id": "680c48805ec65044c2861a70",
          "name": "Jie Zhou",
          "hidden": false
        },
        {
          "_id": "680c48805ec65044c2861a71",
          "name": "Jinchao Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-23T05:25:13.000Z",
      "submittedOnDailyAt": "2025-04-28T00:53:49.838Z",
      "title": "Le module modèle Rare Rare est utile pour l'analyse du langage multimodal ? MMLA : Benchmark complexe",
      "submittedOnDailyBy": {
        "_id": "669090c01e3f5b16ce22b535",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/669090c01e3f5b16ce22b535/AT-k66Mt5FtbImnhNQJ_J.jpeg",
        "isPro": false,
        "fullname": "Hanlei Zhang",
        "user": "HanleiZhang",
        "type": "user"
      },
      "summary": "L'analyse du langage multimodal est un domaine qui a connu un développement rapide pour comprendre la profonde sémantique des conversations humaines en utilisant divers modules. Bien que ce domaine soit crucial, la recherche sur la capacité des modèles multimodal de langage (MLLMs) à comprendre les sémantiques cognitives est insuffisante. Dans cet article, nous présentons une solution pour ce vide, présentant un benchmark spécifique appelé MMLA. MMLA comprend plus de 61K dialogues multimodales et est constitué de scénarios itératifs et de scénarios réels, couvrant six dimensions clés de la sémantique multimodale : intention, émotion, actions de dialogue, sentiment, style de conversation et actions de communication. Il est évalué sur trois méthodes parmi les 8 principales zones des modèles de langage grand (LLMs) et MLLMs : inférence 0-shot, ajustement de sous-projets et ajuste manuel. Les résultats des expériences extensives montrent que même les modèles modérément ajustés peuvent atteindre une précision de 60 à 70 %, ce qui clairement démontre les limitations actuelles des MLLMs dans la compréhension du langage humain complexe. MMLA fournit une base solide pour l'analyse du langage multimodal dans les modèles de langage grand et offre des ressources précieuses pour le progrès de ce domaine. Le dataset et le code sont disponibles dans la version open sur GitHub : https://github.com/thuiar/MMLA.",
      "upvotes": 9,
      "discussionId": "680c48825ec65044c2861ac4",
      "githubRepo": "https://github.com/thuiar/MMLA",
      "ai_keywords": [
        "multimodal language models (MLLMs)",
        "MMLA (Multimodal Language Analysis)",
        "multimodal utterances",
        "intent",
        "emotion",
        "dialogue act",
        "sentiment",
        "speaking style",
        "communication behavior",
        "zero-shot inference",
        "supervised fine-tuning",
        "instruction tuning",
        "large language models (LLMs)"
      ]
    },
    "publishedAt": "2025-04-23T01:25:13.000Z",
    "title": "Can Large Language Models Help Multimodal Language Analysis? MMLA: A\n  Comprehensive Benchmark",
    "summary": "Multimodal language analysis is a rapidly evolving field that leverages\nmultiple modalities to enhance the understanding of high-level semantics\nunderlying human conversational utterances. Despite its significance, little\nresearch has investigated the capability of multimodal large language models\n(MLLMs) to comprehend cognitive-level semantics. In this paper, we introduce\nMMLA, a comprehensive benchmark specifically designed to address this gap. MMLA\ncomprises over 61K multimodal utterances drawn from both staged and real-world\nscenarios, covering six core dimensions of multimodal semantics: intent,\nemotion, dialogue act, sentiment, speaking style, and communication behavior.\nWe evaluate eight mainstream branches of LLMs and MLLMs using three methods:\nzero-shot inference, supervised fine-tuning, and instruction tuning. Extensive\nexperiments reveal that even fine-tuned models achieve only about 60%~70%\naccuracy, underscoring the limitations of current MLLMs in understanding\ncomplex human language. We believe that MMLA will serve as a solid foundation\nfor exploring the potential of large language models in multimodal language\nanalysis and provide valuable resources to advance this field. The datasets and\ncode are open-sourced at https://github.com/thuiar/MMLA.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16427.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "669090c01e3f5b16ce22b535",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/669090c01e3f5b16ce22b535/AT-k66Mt5FtbImnhNQJ_J.jpeg",
      "fullname": "Hanlei Zhang",
      "name": "HanleiZhang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.17768",
      "authors": [
        {
          "_id": "680f2668db85fd31cd5080ff",
          "name": "Piotr Nawrot",
          "hidden": false
        },
        {
          "_id": "680f2668db85fd31cd508100",
          "name": "Robert Li",
          "hidden": false
        },
        {
          "_id": "680f2668db85fd31cd508101",
          "name": "Renjie Huang",
          "hidden": false
        },
        {
          "_id": "680f2668db85fd31cd508102",
          "name": "Sebastian Ruder",
          "hidden": false
        },
        {
          "_id": "680f2668db85fd31cd508103",
          "name": "Kelly Marchisio",
          "hidden": false
        },
        {
          "_id": "680f2668db85fd31cd508104",
          "name": "Edoardo M. Ponti",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-24T17:39:25.000Z",
      "submittedOnDailyAt": "2025-04-28T05:26:26.185Z",
      "title": "Sparse Frontier : Pointe d'Équilibre de la Foco Éclatante dans les Transformer LLMs",
      "submittedOnDailyBy": {
        "_id": "640deb5d3c82bd463ee44735",
        "avatarUrl": "/avatars/0e748d7c91d97526b280e40ccb25c9e0.svg",
        "isPro": false,
        "fullname": "Piotr Nawrot",
        "user": "pnawrot",
        "type": "user"
      },
      "summary": "Sparse attention est une stratégie potentiellement utile pour élargir la capacité de traitement de contexte long de LLMs Transformer, mais la possibilité, l'efficacité et la précision entre elles, ainsi que l'investigation sur l'échelle systématique, n'ont pas encore été explorées. Pour combler cette lacune, on compare avec plus de détails différents méthodes d'attention sparse non entraînées sur un ensemble de tâches de contexte long (qui inclut des tâches basées sur le langage naturel, mais maintiennent un contrôle et une évaluation simple). En se basant sur les résultats des expériences, on rapporte les principaux résultats suivants : 1) dans l'analyse isoFLOPS, dans des cas de contextes très longs, des modèles grands et hautement sparses sont supérieurs aux modèles petits et denses. 2) Le niveau de sparseness nécessaire pour maintenir la précision statistique est plus élevé pendant le traitement précédent que pendant le décodage, ce qui indique que le décodage, ce niveau peut dépendre du taille du modèle. 3) Il n'y a pas un point optimal de sparseness pour des tâches ou des étapes complètes, mais il dépend des scénarios avec des unités de sparseness et d'adaptation visuelles différentes. Cela implique que, à des niveaux modérément sparses, on observe une perte notable de performance dans au moins une tâche, ce qui souligne que l'attention sparse n'est pas une solution générale. 4) On propose une règle spécifique d'échelle pour l'attention sparse et on démontre que cette règle existe et s'applique en dehors du domaine des expériences. En se basant sur ces observations, l'attention sparse est une outil important pour améliorer la capacité de traitement de contexte long de LLMs Transformer, et il est nécessaire de soigneusement équilibrer le rendement et le coût dans les applications de spin-off.",
      "upvotes": 7,
      "discussionId": "680f2669db85fd31cd50815e",
      "ai_keywords": [
        "Sparse attention",
        "Transformer LLMs",
        "Training-free",
        "IsoFLOPS analysis",
        "Sequence lengths",
        "Sparsity levels",
        "Long-sequence tasks",
        "Natural language",
        "Accuracy preservation",
        "Decoding",
        "Prefilling",
        "Budget adaptivity",
        "Performance degradation",
        "Scaling laws"
      ]
    },
    "publishedAt": "2025-04-24T13:39:25.000Z",
    "title": "The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs",
    "summary": "Sparse attention offers a promising strategy to extend long-context\ncapabilities in Transformer LLMs, yet its viability, its efficiency-accuracy\ntrade-offs, and systematic scaling studies remain unexplored. To address this\ngap, we perform a careful comparison of training-free sparse attention methods\nat varying model scales, sequence lengths, and sparsity levels on a diverse\ncollection of long-sequence tasks-including novel ones that rely on natural\nlanguage while remaining controllable and easy to evaluate. Based on our\nexperiments, we report a series of key findings: 1) an isoFLOPS analysis\nreveals that for very long sequences, larger and highly sparse models are\npreferable to smaller and dense ones. 2) The level of sparsity attainable while\nstatistically guaranteeing accuracy preservation is higher during decoding than\nprefilling, and correlates with model size in the former. 3) There is no clear\nstrategy that performs best across tasks and phases, with different units of\nsparsification or budget adaptivity needed for different scenarios. Even\nmoderate sparsity levels often result in significant performance degradation on\nat least one task, highlighting that sparse attention is not a universal\nsolution. 4) We introduce and validate novel scaling laws specifically tailored\nfor sparse attention, providing evidence that our findings are likely to hold\ntrue beyond our range of experiments. Through these insights, we demonstrate\nthat sparse attention is a key tool to enhance the capabilities of Transformer\nLLMs for processing longer sequences, but requires careful evaluation of\ntrade-offs for performance-sensitive applications.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17768.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "640deb5d3c82bd463ee44735",
      "avatarUrl": "/avatars/0e748d7c91d97526b280e40ccb25c9e0.svg",
      "fullname": "Piotr Nawrot",
      "name": "pnawrot",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.17816",
      "authors": [
        {
          "_id": "680ed2679e529f7799a0689f",
          "user": {
            "_id": "636b20591340f879a2eb98d0",
            "avatarUrl": "/avatars/4fc5cb13f916bcbc842ccf387bd5f6c0.svg",
            "isPro": false,
            "fullname": "Daneul Kim",
            "user": "carpedkm",
            "type": "user"
          },
          "name": "Daneul Kim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-28T07:38:37.502Z",
          "hidden": false
        },
        {
          "_id": "680ed2679e529f7799a068a0",
          "name": "Jingxu Zhang",
          "hidden": false
        },
        {
          "_id": "680ed2679e529f7799a068a1",
          "name": "Wonjoon Jin",
          "hidden": false
        },
        {
          "_id": "680ed2679e529f7799a068a2",
          "name": "Sunghyun Cho",
          "hidden": false
        },
        {
          "_id": "680ed2679e529f7799a068a3",
          "user": {
            "_id": "65115c00a588fdb36558b673",
            "avatarUrl": "/avatars/1f36263dc4bfaf696a4aa959a6aab1e1.svg",
            "isPro": false,
            "fullname": "Qi Dai",
            "user": "daiqi",
            "type": "user"
          },
          "name": "Qi Dai",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-28T00:57:12.914Z",
          "hidden": false
        },
        {
          "_id": "680ed2679e529f7799a068a4",
          "name": "Jaesik Park",
          "hidden": false
        },
        {
          "_id": "680ed2679e529f7799a068a5",
          "user": {
            "_id": "676a328148d749b7086782d0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Tt7u8l8f_1oVBWmBp7tkm.png",
            "isPro": false,
            "fullname": "Chong Luo",
            "user": "cluo-ms",
            "type": "user"
          },
          "name": "Chong Luo",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-28T00:57:12.914Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/636b20591340f879a2eb98d0/ZUDDcDv2cTWIFx65RIEbG.mp4"
      ],
      "publishedAt": "2025-04-23T06:48:31.000Z",
      "submittedOnDailyAt": "2025-04-28T07:02:51.113Z",
      "title": "**\"Création de vidéos axées sur le thème : Identités différenciées et mouvements\"**",
      "submittedOnDailyBy": {
        "_id": "636b20591340f879a2eb98d0",
        "avatarUrl": "/avatars/4fc5cb13f916bcbc842ccf387bd5f6c0.svg",
        "isPro": false,
        "fullname": "Daneul Kim",
        "user": "carpedkm",
        "type": "user"
      },
      "summary": "Nous proposons un méthode pour entraîner un modèle de génération de vidéos personnalisées sans entraînement supplémentaire dans un environnement 0-shot, en séparant l'apprentissage par objets et la dynamique temporelle. Les méthodes traditionnelles de régression sans entraînement pour des vidéos personnalisées se basent sur de grands et étiquetés ensembles de vidéos, ce qui est coûteux en termes de calcul et nécessite une étiquetage étendu. Au contraire de ces approches, nous présentons un méthode qui utilise directement un ensemble d'images personnalisées pour entraîner le modèle de vidéo personnalisé. Le modèle de vidéo personnalisé est divisé en deux parties : (1) l'injection de reconnaissance avec des données d'images personnalisées et (2) le méthode d'entraînement image-à-vidéo en utilisant un petit ensemble de vidéos non étiquetés pour préserver la modélisation temporelle. De plus, pour atténuer les problèmes de copie et collage lors de l'ajustement micro, nous utilisons l'élimination de tokens d'images aléatoires et l'initialisation d'images aléatoires lors de l'entraînement image-à-vidéo. Pour améliorer l'apprentissage, nous introduisons un changement probabiliste dans l'optimisation conjointe de caractéristiques par objets et temporelles pour atténuer les oublis critiques. Notre méthode présente une forte consistence par objets et une extensibilité, démontrant un excellent rendement dans un environnement 0-shot, contribuant à démontrer l'efficacité de notre cadre de travail.",
      "upvotes": 5,
      "discussionId": "680ed2689e529f7799a06907",
      "projectPage": "https://carpedkm.github.io/projects/disentangled_sub/",
      "githubRepo": "https://github.com/carpedkm/disentangled-subject-to-vid",
      "ai_keywords": [
        "subject-specific learning",
        "temporal dynamics",
        "image customization dataset",
        "identity injection",
        "temporal modeling",
        "image-to-video training method",
        "random image token dropping",
        "randomized image initialization",
        "image-to-video fine-tuning",
        "stochastic switching",
        "joint optimization",
        "catastrophic forgetting",
        "subject consistency",
        "zero-shot settings"
      ]
    },
    "publishedAt": "2025-04-23T02:48:31.000Z",
    "title": "Subject-driven Video Generation via Disentangled Identity and Motion",
    "summary": "We propose to train a subject-driven customized video generation model\nthrough decoupling the subject-specific learning from temporal dynamics in\nzero-shot without additional tuning. A traditional method for video\ncustomization that is tuning-free often relies on large, annotated video\ndatasets, which are computationally expensive and require extensive annotation.\nIn contrast to the previous approach, we introduce the use of an image\ncustomization dataset directly on training video customization models,\nfactorizing the video customization into two folds: (1) identity injection\nthrough image customization dataset and (2) temporal modeling preservation with\na small set of unannotated videos through the image-to-video training method.\nAdditionally, we employ random image token dropping with randomized image\ninitialization during image-to-video fine-tuning to mitigate the copy-and-paste\nissue. To further enhance learning, we introduce stochastic switching during\njoint optimization of subject-specific and temporal features, mitigating\ncatastrophic forgetting. Our method achieves strong subject consistency and\nscalability, outperforming existing video customization models in zero-shot\nsettings, demonstrating the effectiveness of our framework.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/636b20591340f879a2eb98d0/ZUDDcDv2cTWIFx65RIEbG.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17816.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "636b20591340f879a2eb98d0",
      "avatarUrl": "/avatars/4fc5cb13f916bcbc842ccf387bd5f6c0.svg",
      "fullname": "Daneul Kim",
      "name": "carpedkm",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.15716",
      "authors": [
        {
          "_id": "680dcc5d3478de07603a8036",
          "user": {
            "_id": "642656cbad1e3b0e6e91b752",
            "avatarUrl": "/avatars/3bf0ee15fd528e09b2b889f5cce3cbd0.svg",
            "isPro": false,
            "fullname": "Jie Zhu",
            "user": "amazingj",
            "type": "user"
          },
          "name": "Jie Zhu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-28T07:39:02.713Z",
          "hidden": false
        },
        {
          "_id": "680dcc5d3478de07603a8037",
          "name": "Qian Chen",
          "hidden": false
        },
        {
          "_id": "680dcc5d3478de07603a8038",
          "name": "Huaixia Dou",
          "hidden": false
        },
        {
          "_id": "680dcc5d3478de07603a8039",
          "name": "Junhui Li",
          "hidden": false
        },
        {
          "_id": "680dcc5d3478de07603a803a",
          "name": "Lifan Guo",
          "hidden": false
        },
        {
          "_id": "680dcc5d3478de07603a803b",
          "name": "Feng Chen",
          "hidden": false
        },
        {
          "_id": "680dcc5d3478de07603a803c",
          "name": "Chi Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-22T09:01:04.000Z",
      "submittedOnDailyAt": "2025-04-28T06:16:26.234Z",
      "title": "Évaluation et amélioration financières de modèles de langage à grande échelle",
      "submittedOnDailyBy": {
        "_id": "642656cbad1e3b0e6e91b752",
        "avatarUrl": "/avatars/3bf0ee15fd528e09b2b889f5cce3cbd0.svg",
        "isPro": false,
        "fullname": "Jie Zhu",
        "user": "amazingj",
        "type": "user"
      },
      "summary": "La logique basée sur les raisons est un problème crucial dans le domaine financier des grands modèles de langage (LLMs), ce qui a été reconnu. Pour résoudre ces problèmes, des connaissances spécifiques du secteur, des calculs numériques précis et la respect des règles strictes sont nécessaires. Nous proposons un cadre de travail pour renforcer la logique basée sur les raisons appelé DianJin-R1, avec l'objectif de résoudre ces problèmes. L'approche clé de notre travail est DianJin-R1-Data. Ce jeu de données a été construit à partir de CFLUE, FinQA et un corpus personnalisé de violations (Vérification de Violations en Chine, CCC). Ce jeu de données combine des échelles de logique basée sur les raisons dans différentes zones financières et des notes validées. Nos modèles, DianJin-R1-7B et DianJin-R1-32B, ont été construits à partir de Qwen2.5-7B-Instruct et Qwen2.5-32B-Instruct, et ont été ajustés en utilisant un format structuré pour générer des étapes basées sur les raisons et les réponses finales. Pour améliorer la qualité de la logique basée sur les raisons, nous appliquons la Group Relative Policy Optimization (GRPO), un méthode d'apprentissage par récompense qui inclut un signal de récompense pour promouvoir des sorties structurées et un autre pour la précision de la réponse. Nos modèles ont été évalués sur 5 benchmarks : 3 datasets financiers (CFLUE, FinQA, CCC) et 2 benchmarks généraux de logique basée sur les raisons (MATH-500, GPQA-Diamond). Les résultats des expériences montrent que les modèles DianJin-R1 dépassent les modèles sans logique basée sur les raisons, surtout dans des tâches financières complexes. De plus, dans le jeu de données CCC, notre modèle basé sur les raisons a démontré sa capacité à dépasser le rendement des systèmes multi-agent avec un coût informatique élevé. Ces résultats démontrent l'efficacité de DianJin-R1 dans renforcer la logique basée sur les raisons dans le financement grâce à l'apprentissage structuré et l'optimisation de récompenses, offrant des solutions scalables et des pratiques pour des applications financières réelles.",
      "upvotes": 5,
      "discussionId": "680dcc5e3478de07603a807e",
      "ai_keywords": [
        "reasoning-enhanced framework",
        "reasoning-augmented supervision",
        "reinforcement learning",
        "DianJin-R1-Data",
        "CFLUE",
        "FinQA",
        "Chinese Compliance Check (CCC)",
        "high-quality dataset",
        "DianJin-R1-7B",
        "DianJin-R1-32B",
        "Qwen2.5-7B-Instruct",
        "Qwen2.5-32B-Instruct",
        "structured format",
        "reasoning steps",
        "Group Relative Policy Optimization (GRPO)",
        "dual reward signals",
        "structured outputs",
        "answer correctness",
        "MATH-500",
        "GPQA-Diamond",
        "financial datasets",
        "single-call reasoning models",
        "multi-agent systems",
        "real-world applications"
      ]
    },
    "publishedAt": "2025-04-22T05:01:04.000Z",
    "title": "DianJin-R1: Evaluating and Enhancing Financial Reasoning in Large\n  Language Models",
    "summary": "Effective reasoning remains a core challenge for large language models (LLMs)\nin the financial domain, where tasks often require domain-specific knowledge,\nprecise numerical calculations, and strict adherence to compliance rules. We\npropose DianJin-R1, a reasoning-enhanced framework designed to address these\nchallenges through reasoning-augmented supervision and reinforcement learning.\nCentral to our approach is DianJin-R1-Data, a high-quality dataset constructed\nfrom CFLUE, FinQA, and a proprietary compliance corpus (Chinese Compliance\nCheck, CCC), combining diverse financial reasoning scenarios with verified\nannotations. Our models, DianJin-R1-7B and DianJin-R1-32B, are fine-tuned from\nQwen2.5-7B-Instruct and Qwen2.5-32B-Instruct using a structured format that\ngenerates both reasoning steps and final answers. To further refine reasoning\nquality, we apply Group Relative Policy Optimization (GRPO), a reinforcement\nlearning method that incorporates dual reward signals: one encouraging\nstructured outputs and another rewarding answer correctness. We evaluate our\nmodels on five benchmarks: three financial datasets (CFLUE, FinQA, and CCC) and\ntwo general reasoning benchmarks (MATH-500 and GPQA-Diamond). Experimental\nresults show that DianJin-R1 models consistently outperform their non-reasoning\ncounterparts, especially on complex financial tasks. Moreover, on the\nreal-world CCC dataset, our single-call reasoning models match or even surpass\nthe performance of multi-agent systems that require significantly more\ncomputational cost. These findings demonstrate the effectiveness of DianJin-R1\nin enhancing financial reasoning through structured supervision and\nreward-aligned learning, offering a scalable and practical solution for\nreal-world applications.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15716.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642656cbad1e3b0e6e91b752",
      "avatarUrl": "/avatars/3bf0ee15fd528e09b2b889f5cce3cbd0.svg",
      "fullname": "Jie Zhu",
      "name": "amazingj",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.12080",
      "authors": [
        {
          "_id": "680afc5f2c4b584e1d786eee",
          "name": "Mengshi Qi",
          "hidden": false
        },
        {
          "_id": "680afc5f2c4b584e1d786eef",
          "user": {
            "_id": "66a8c8e4f5cda7b8690205ef",
            "avatarUrl": "/avatars/5c43b7b50aeb3d1459307334ddcd1d1b.svg",
            "isPro": false,
            "fullname": "Pengfei Zhu",
            "user": "zaplm",
            "type": "user"
          },
          "name": "Pengfei Zhu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-25T08:34:30.195Z",
          "hidden": false
        },
        {
          "_id": "680afc5f2c4b584e1d786ef0",
          "name": "Xiangtai Li",
          "hidden": false
        },
        {
          "_id": "680afc5f2c4b584e1d786ef1",
          "name": "Xiaoyang Bi",
          "hidden": false
        },
        {
          "_id": "680afc5f2c4b584e1d786ef2",
          "name": "Lu Qi",
          "hidden": false
        },
        {
          "_id": "680afc5f2c4b584e1d786ef3",
          "name": "Huadong Ma",
          "hidden": false
        },
        {
          "_id": "680afc5f2c4b584e1d786ef4",
          "name": "Ming-Hsuan Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-16T13:41:59.000Z",
      "submittedOnDailyAt": "2025-04-28T02:11:37.182Z",
      "title": "DC-SAM : Méthode par double coincidence pour la segmentation dans le contexte dans les images et les vidéos",
      "submittedOnDailyBy": {
        "_id": "66a8c8e4f5cda7b8690205ef",
        "avatarUrl": "/avatars/5c43b7b50aeb3d1459307334ddcd1d1b.svg",
        "isPro": false,
        "fullname": "Pengfei Zhu",
        "user": "zaplm",
        "type": "user"
      },
      "summary": "Quand un exemple d'étiquetage est fourni pour des données graphiques, la segmentation en contexte vise à segmenter les objets correspondants. Cette configuration est connue sous le nom de segmentation à peu de modèles (few-shot segmentation) et est appliquée à diverses tâches visuelles, comme la compréhension d'environnements et l'édition d'images ou de vidéos. Les derniers modèles de Segment Anything (SAM) ont réalisé des résultats avancés grâce à une segmentation interactive, mais cette méthodologie ne s'applique pas directement à la segmentation en contexte. Dans cet article, nous proposons un méthode basée sur l'ajustement de prompt appelée Dual Consistency SAM (DC-SAM), qui est applicable à des images et des vidéos dans la segmentation en contexte. L'idée principale consiste à renforcer l'encodeur de prompts de SAM pour fournir des prompts visuels de haute qualité. Nous fusionnons les caractéristiques de SAM pour créer une masque, ce qui permet que l'encodeur de prompts se ajuste mieux. Ensuite, nous concevons une attention croisée itérative qui se aligne avec les prompts initiaux et les caractéristiques fusionnées. Ensuite, nous proposons un design binaire en utilisant des prompts positifs et négatifs différents dans l'encodeur de prompts. De plus, nous concevons une stratégie d'apprentissage simple pour appliquer la méthode de double consistance à la segmentation de masques. DC-SAM est principalement appliqué à des images, mais avec l'appui de SAM2, il peut être étendu à l'domaine des vidéos. Étant donné qu'il n'existe pas de segmentation en contexte pour les vidéos, nous corrigeons le premier benchmark IC-VOS (In-Context Video Object Segmentation) et nous le construisons en utilisant des ensembles de données de segmentation de vidéos existantes. Avec ce benchmark, nous réalisons des expériences étendues qui atteignent un mIoU de 55,5 (+1,4) sur COCO-20i, un mIoU de 73,0 (+1,1) sur PASCAL-5i et un score J&F de 71,52 sur le benchmark IC-VOS proposé. La source du code de DC-SAM et le benchmark sont disponibles sur https://github.com/zaplm/DC-SAM.",
      "upvotes": 5,
      "discussionId": "680afc622c4b584e1d786f9e",
      "ai_keywords": [
        "prompt-tuning",
        "prompt encoder",
        "mask prior",
        "cycle-consistent cross-attention",
        "dual-branch design",
        "discriminative positive prompts",
        "negative prompts",
        "mask-tube",
        "In-Context Video Object Segmentation (IC-VOS)",
        "mIoU"
      ]
    },
    "publishedAt": "2025-04-16T09:41:59.000Z",
    "title": "DC-SAM: In-Context Segment Anything in Images and Videos via Dual\n  Consistency",
    "summary": "Given a single labeled example, in-context segmentation aims to segment\ncorresponding objects. This setting, known as one-shot segmentation in few-shot\nlearning, explores the segmentation model's generalization ability and has been\napplied to various vision tasks, including scene understanding and image/video\nediting. While recent Segment Anything Models have achieved state-of-the-art\nresults in interactive segmentation, these approaches are not directly\napplicable to in-context segmentation. In this work, we propose the Dual\nConsistency SAM (DC-SAM) method based on prompt-tuning to adapt SAM and SAM2\nfor in-context segmentation of both images and videos. Our key insights are to\nenhance the features of the SAM's prompt encoder in segmentation by providing\nhigh-quality visual prompts. When generating a mask prior, we fuse the SAM\nfeatures to better align the prompt encoder. Then, we design a cycle-consistent\ncross-attention on fused features and initial visual prompts. Next, a\ndual-branch design is provided by using the discriminative positive and\nnegative prompts in the prompt encoder. Furthermore, we design a simple\nmask-tube training strategy to adopt our proposed dual consistency method into\nthe mask tube. Although the proposed DC-SAM is primarily designed for images,\nit can be seamlessly extended to the video domain with the support of SAM2.\nGiven the absence of in-context segmentation in the video domain, we manually\ncurate and construct the first benchmark from existing video segmentation\ndatasets, named In-Context Video Object Segmentation (IC-VOS), to better assess\nthe in-context capability of the model. Extensive experiments demonstrate that\nour method achieves 55.5 (+1.4) mIoU on COCO-20i, 73.0 (+1.1) mIoU on\nPASCAL-5i, and a J&F score of 71.52 on the proposed IC-VOS benchmark. Our\nsource code and benchmark are available at https://github.com/zaplm/DC-SAM.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.12080.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66a8c8e4f5cda7b8690205ef",
      "avatarUrl": "/avatars/5c43b7b50aeb3d1459307334ddcd1d1b.svg",
      "fullname": "Pengfei Zhu",
      "name": "zaplm",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]