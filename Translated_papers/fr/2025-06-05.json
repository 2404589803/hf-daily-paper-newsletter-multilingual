[
  {
    "paper": {
      "id": "2506.03569",
      "authors": [
        {
          "_id": "6841003e45e7d8a890731765",
          "name": "Xiaomi LLM-Core Team",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731767",
          "name": "Zihao Yue",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731768",
          "name": "Zhenru Lin",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731769",
          "name": "Yifan Song",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073176a",
          "name": "Weikun Wang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073176b",
          "user": {
            "_id": "60d2e681b8448e1785bbda06",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1624434302056-noauth.jpeg",
            "isPro": false,
            "fullname": "Shuhuai Ren",
            "user": "ShuhuaiRen",
            "type": "user"
          },
          "name": "Shuhuai Ren",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:27:00.497Z",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073176c",
          "user": {
            "_id": "642e72cec1b0f8e4e76af16d",
            "avatarUrl": "/avatars/f900811d3c22a114c67283b646949f86.svg",
            "isPro": false,
            "fullname": "shuhao gu",
            "user": "gsh33",
            "type": "user"
          },
          "name": "Shuhao Gu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:27:04.948Z",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073176d",
          "name": "Shicheng Li",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073176e",
          "name": "Peidian Li",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073176f",
          "name": "Liang Zhao",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731770",
          "user": {
            "_id": "6038d6d0612f5eef3cc05ea9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6038d6d0612f5eef3cc05ea9/ryhvAX5djQpD5OrIlZQ1f.jpeg",
            "isPro": false,
            "fullname": "Lei Li",
            "user": "tobiaslee",
            "type": "user"
          },
          "name": "Lei Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:27:07.044Z",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731771",
          "name": "Kainan Bao",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731772",
          "name": "Hao Tian",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731773",
          "name": "Hailin Zhang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731774",
          "name": "Gang Wang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731775",
          "user": {
            "_id": "64d2fce8129a210e569e0c76",
            "avatarUrl": "/avatars/a79a832dc3a46ece1b9e542369fc4888.svg",
            "isPro": false,
            "fullname": "Dawei Zhu",
            "user": "dwzhu",
            "type": "user"
          },
          "name": "Dawei Zhu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:27:02.720Z",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731776",
          "name": "Cici",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731777",
          "name": "Chenhong He",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731778",
          "name": "Bowen Ye",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731779",
          "name": "Bowen Shen",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073177a",
          "name": "Zihan Zhang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073177b",
          "name": "Zihan Jiang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073177c",
          "name": "Zhixian Zheng",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073177d",
          "name": "Zhichao Song",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073177e",
          "name": "Zhenbo Luo",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073177f",
          "name": "Yue Yu",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731780",
          "name": "Yudong Wang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731781",
          "name": "Yuanyuan Tian",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731782",
          "name": "Yu Tu",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731783",
          "name": "Yihan Yan",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731784",
          "name": "Yi Huang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731785",
          "name": "Xu Wang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731786",
          "name": "Xinzhe Xu",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731787",
          "name": "Xingchen Song",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731788",
          "name": "Xing Zhang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731789",
          "name": "Xing Yong",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073178a",
          "name": "Xin Zhang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073178b",
          "name": "Xiangwei Deng",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073178c",
          "name": "Wenyu Yang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073178d",
          "name": "Wenhan Ma",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073178e",
          "name": "Weiwei Lv",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073178f",
          "name": "Weiji Zhuang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731790",
          "name": "Wei Liu",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731791",
          "name": "Sirui Deng",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731792",
          "name": "Shuo Liu",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731793",
          "name": "Shimao Chen",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731794",
          "name": "Shihua Yu",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731795",
          "name": "Shaohui Liu",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731796",
          "name": "Shande Wang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731797",
          "name": "Rui Ma",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731798",
          "name": "Qiantong Wang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731799",
          "name": "Peng Wang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073179a",
          "name": "Nuo Chen",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073179b",
          "name": "Menghang Zhu",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073179c",
          "name": "Kangyang Zhou",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073179d",
          "name": "Kang Zhou",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073179e",
          "name": "Kai Fang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073179f",
          "name": "Jun Shi",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a8907317a0",
          "name": "Jinhao Dong",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a8907317a1",
          "name": "Jiebao Xiao",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a8907317a2",
          "name": "Jiaming Xu",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a8907317a3",
          "user": {
            "_id": "680e9a219e529f779991be0c",
            "avatarUrl": "/avatars/327b945649192b0881fe290298d10e23.svg",
            "isPro": false,
            "fullname": "Huaqiu Liu",
            "user": "Prestonprom",
            "type": "user"
          },
          "name": "Huaqiu Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:26:58.279Z",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a8907317a4",
          "name": "Hongshen Xu",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a8907317a5",
          "name": "Heng Qu",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a8907317a6",
          "name": "Haochen Zhao",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a8907317a7",
          "name": "Hanglong Lv",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a8907317a8",
          "name": "Guoan Wang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a8907317a9",
          "name": "Duo Zhang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a8907317aa",
          "name": "Dong Zhang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a8907317ab",
          "name": "Di Zhang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a8907317ac",
          "name": "Chong Ma",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a8907317ad",
          "name": "Chang Liu",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a8907317ae",
          "name": "Can Cai",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a8907317af",
          "name": "Bingquan Xia",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-04T04:32:54.000Z",
      "submittedOnDailyAt": "2025-06-05T00:57:27.734Z",
      "title": "Le rapport technique MiMo-VL (Multiple Instance Multimodal Vision-Language) présente les dernières technologies et futurs développements dans le domaine de la Vision-Langue (V&L), qui constituent une technologie essentielle pour traiter de l'information de manière plus efficace et efficace grâce à l'interaction entre l'information visuelle et linguistique. Ce rapport détaille rigoureusement les principes fondamentaux, le contexte de développement et les tendances de recherche actuelles, ainsi que les perspectives futures du MiMo-VL. Ce document fournit des informations cruciales pour les chercheurs et développeurs dans le domaine de l'intelligence artificielle, soulignant la possibilité d'applications dans diverses zones.",
      "submittedOnDailyBy": {
        "_id": "6038d6d0612f5eef3cc05ea9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6038d6d0612f5eef3cc05ea9/ryhvAX5djQpD5OrIlZQ1f.jpeg",
        "isPro": false,
        "fullname": "Lei Li",
        "user": "tobiaslee",
        "type": "user"
      },
      "summary": "Nous publions deux modèles visuels-langage puissants : MiMo-VL-7B-SFT et MiMo-VL-7B-RL, tous deux sous licence open source. Ce modèle offre un rendement au niveau de la récente compréhension visuelle générale et du traitement logique multimodal. MiMo-VL-7B-RL dépasse 35 sur 40 évaluations, atteignant un score de 59,4 sur OlympiadBench, dépassant les modèles avec 78B paramètres. Dans l'application de GUI fixée, il enregistre un score de 56,1 sur OSWorld-G, dépassant les modèles comme UI-TARS. Notre entraînement combine quatre étapes d'entraînement préalable (2,4 milliards de tokens) et un apprentissage par renforcement avec politiques mixtes (MORL), intégrant divers signaux de récompense. Nous soulignons l'importance des données de raisonnement de chaîne de pensée de haute qualité dans l'entraînement préalable et les défis d'optimisation dans plusieurs domaines, soulignant les avantages de l'apprentissage par renforcement mixte. De plus, nous fournissons un ensemble d'évaluation intégral qui comprend plus de 50 tâches, favorisant la reproductibilité et le développement de la discipline. Les points de contrôle du modèle et l'ensemble complet des évaluations sont disponibles sur https://github.com/XiaomiMiMo/MiMo-VL.",
      "upvotes": 44,
      "discussionId": "6841004145e7d8a890731853",
      "githubRepo": "https://github.com/XiaomiMiMo/MiMo-VL",
      "ai_summary": "MiMo-VL-7B-SFT and MiMo-VL-7B-RL provide state-of-the-art general visual understanding and multimodal reasoning through four-stage pre-training and Mixed On-policy Reinforcement Learning, outperforming models with up to 78B parameters.",
      "ai_keywords": [
        "vision-language models",
        "multimodal reasoning",
        "four-stage pre-training",
        "Mixed On-policy Reinforcement Learning",
        "MORL",
        "Chain-of-Thought",
        "reproducibility"
      ]
    },
    "publishedAt": "2025-06-04T00:32:54.000Z",
    "title": "MiMo-VL Technical Report",
    "summary": "We open-source MiMo-VL-7B-SFT and MiMo-VL-7B-RL, two powerful vision-language\nmodels delivering state-of-the-art performance in both general visual\nunderstanding and multimodal reasoning. MiMo-VL-7B-RL outperforms Qwen2.5-VL-7B\non 35 out of 40 evaluated tasks, and scores 59.4 on OlympiadBench, surpassing\nmodels with up to 78B parameters. For GUI grounding applications, it sets a new\nstandard with 56.1 on OSWorld-G, even outperforming specialized models such as\nUI-TARS. Our training combines four-stage pre-training (2.4 trillion tokens)\nwith Mixed On-policy Reinforcement Learning (MORL) integrating diverse reward\nsignals. We identify the importance of incorporating high-quality reasoning\ndata with long Chain-of-Thought into pre-training stages, and the benefits of\nmixed RL despite challenges in simultaneous multi-domain optimization. We also\ncontribute a comprehensive evaluation suite covering 50+ tasks to promote\nreproducibility and advance the field. The model checkpoints and full\nevaluation suite are available at https://github.com/XiaomiMiMo/MiMo-VL.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03569.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6038d6d0612f5eef3cc05ea9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6038d6d0612f5eef3cc05ea9/ryhvAX5djQpD5OrIlZQ1f.jpeg",
      "fullname": "Lei Li",
      "name": "tobiaslee",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.04207",
      "authors": [
        {
          "_id": "684117e22db29aa7b403af8d",
          "name": "Shuang Chen",
          "hidden": false
        },
        {
          "_id": "684117e22db29aa7b403af8e",
          "name": "Yue Guo",
          "hidden": false
        },
        {
          "_id": "684117e22db29aa7b403af8f",
          "user": {
            "_id": "64264095ba51f8a2136946a0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64264095ba51f8a2136946a0/FR33boVpkDXcrvGMBmprF.jpeg",
            "isPro": false,
            "fullname": "Zhaochen Su",
            "user": "Warrieryes",
            "type": "user"
          },
          "name": "Zhaochen Su",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:26:45.759Z",
          "hidden": false
        },
        {
          "_id": "684117e22db29aa7b403af90",
          "name": "Yafu Li",
          "hidden": false
        },
        {
          "_id": "684117e22db29aa7b403af91",
          "name": "Yulun Wu",
          "hidden": false
        },
        {
          "_id": "684117e22db29aa7b403af92",
          "user": {
            "_id": "65352acb7139c5dd8d9a8590",
            "avatarUrl": "/avatars/e2ff22b596aee45cdfb8f68dc15572f9.svg",
            "isPro": false,
            "fullname": "JiachengChen",
            "user": "JC-Chen",
            "type": "user"
          },
          "name": "Jiacheng Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:48:38.463Z",
          "hidden": false
        },
        {
          "_id": "684117e22db29aa7b403af93",
          "name": "Jiayu Chen",
          "hidden": false
        },
        {
          "_id": "684117e22db29aa7b403af94",
          "name": "Weijie Wang",
          "hidden": false
        },
        {
          "_id": "684117e22db29aa7b403af95",
          "name": "Xiaoye Qu",
          "hidden": false
        },
        {
          "_id": "684117e22db29aa7b403af96",
          "name": "Yu Cheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-04T17:51:08.000Z",
      "submittedOnDailyAt": "2025-06-05T02:38:24.366Z",
      "title": "Avancant avec plusieurs modèles logiques : début optimisé froid et froid, avec apprentissage par renforcement en étapes progressives",
      "submittedOnDailyBy": {
        "_id": "65352acb7139c5dd8d9a8590",
        "avatarUrl": "/avatars/e2ff22b596aee45cdfb8f68dc15572f9.svg",
        "isPro": false,
        "fullname": "JiachengChen",
        "user": "JC-Chen",
        "type": "user"
      },
      "summary": "DEEP SEEKI-R1 se caractérise par une excellente capacité logique dans des tâches de contexte complexe, ce qui est réalisé grâce à l'application de nombreux travaux sur l'apprentissage par renforcement (RL). Cependant, l'activation de logiques complexes est un défi. Dans cet article, l'importance d'une initialisation efficace pour améliorer la capacité logique est soulignée. Intéressamment, il est observé que l'utilisation de données sélectionnées meilleures avec seulement des données de contexte peut conduire à un rendement supérieur par rapport aux modèles logiques de plusieurs modèles. De plus, lorsque le GRPO standard est appliqué à l'apprentissage par renforcement de plusieurs modèles, un décrochage de gradient affecte la stabilité de l'apprentissage et son rendement. Enfin, pour améliorer encore plus la capacité logique de plusieurs modèles, il est nécessaire de réaliser un apprentissage par renforcement uniquement sur le texte après la phase d'apprentissage par renforcement de plusieurs modèles. Cette approche d'apprentissage en phases peut maintenir un équilibre entre les bases visuelles et le développement cognitif logique. En intégrant ces lignes directrices, les problèmes d'apprentissage par renforcement de plusieurs modèles sont résolus et ReVisual-R1 est présenté. Dans MathVerse, MathVision, WeMath, LogicVista, DynaMath, et dans les défis marques de référence AIME2024 et AIME2025, de nouveaux rendements optimaux sont atteints pour les MLLMs de 7B de code ouvert.",
      "upvotes": 35,
      "discussionId": "684117e32db29aa7b403afc2",
      "githubRepo": "https://github.com/CSfufu/Revisual-R1"
    },
    "publishedAt": "2025-06-04T13:51:08.000Z",
    "title": "Advancing Multimodal Reasoning: From Optimized Cold Start to Staged\n  Reinforcement Learning",
    "summary": "Inspired by the remarkable reasoning capabilities of Deepseek-R1 in complex\ntextual tasks, many works attempt to incentivize similar capabilities in\nMultimodal Large Language Models (MLLMs) by directly applying reinforcement\nlearning (RL). However, they still struggle to activate complex reasoning. In\nthis paper, rather than examining multimodal RL in isolation, we delve into\ncurrent training pipelines and identify three crucial phenomena: 1) Effective\ncold start initialization is critical for enhancing MLLM reasoning.\nIntriguingly, we find that initializing with carefully selected text data alone\ncan lead to performance surpassing many recent multimodal reasoning models,\neven before multimodal RL. 2) Standard GRPO applied to multimodal RL suffers\nfrom gradient stagnation, which degrades training stability and performance. 3)\nSubsequent text-only RL training, following the multimodal RL phase, further\nenhances multimodal reasoning. This staged training approach effectively\nbalances perceptual grounding and cognitive reasoning development. By\nincorporating the above insights and addressing multimodal RL issues, we\nintroduce ReVisual-R1, achieving a new state-of-the-art among open-source 7B\nMLLMs on challenging benchmarks including MathVerse, MathVision, WeMath,\nLogicVista, DynaMath, and challenging AIME2024 and AIME2025.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04207.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "65352acb7139c5dd8d9a8590",
      "avatarUrl": "/avatars/e2ff22b596aee45cdfb8f68dc15572f9.svg",
      "fullname": "JiachengChen",
      "name": "JC-Chen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.04089",
      "authors": [
        {
          "_id": "684153cf911d1b3135fa5dfe",
          "name": "Anastasiia Ivanova",
          "hidden": false
        },
        {
          "_id": "684153cf911d1b3135fa5dff",
          "user": {
            "_id": "661af24d8328f43c6abc2d11",
            "avatarUrl": "/avatars/afe7eaf1f7a378dbcdba5cd3e86adf9c.svg",
            "isPro": false,
            "fullname": "Eva",
            "user": "tenebrissilvam",
            "type": "user"
          },
          "name": "Eva Bakaeva",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T10:00:02.435Z",
          "hidden": false
        },
        {
          "_id": "684153cf911d1b3135fa5e00",
          "user": {
            "_id": "64198f70ed725fef6442b37e",
            "avatarUrl": "/avatars/580ab07a3067a9deb2977b0894226fe3.svg",
            "isPro": false,
            "fullname": "Alexey Kovalev",
            "user": "AlexeyKov",
            "type": "user"
          },
          "name": "Zoya Volovikova",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-05T08:22:39.926Z",
          "hidden": false
        },
        {
          "_id": "684153cf911d1b3135fa5e01",
          "name": "Alexey K. Kovalev",
          "hidden": false
        },
        {
          "_id": "684153cf911d1b3135fa5e02",
          "name": "Aleksandr I. Panov",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-04T15:47:07.000Z",
      "submittedOnDailyAt": "2025-06-05T07:08:16.935Z",
      "title": "AmbiK : Ensemble de données de travail incertain dans l'environnement de cuisine",
      "submittedOnDailyBy": {
        "_id": "64198f70ed725fef6442b37e",
        "avatarUrl": "/avatars/580ab07a3067a9deb2977b0894226fe3.svg",
        "isPro": false,
        "fullname": "Alexey Kovalev",
        "user": "AlexeyKov",
        "type": "user"
      },
      "summary": "Les modèles de langage de grande taille (LLMs) sont des agents automatiques, généralement utilisés pour décider des plans d'action en fonction de commandes de nature langage utilisateur. Cependant, le traitement de commandes grammaticalement ambiguës dans des environnements réels est une tâche très difficile pour les LLMs. Des méthodes diverses ont été proposées pour détecter les tâches grammaticalement ambiguës, mais leur comparaison est difficile en raison de la manque de jeux de données ou de référentiels communs. Dans ce contexte, nous proposons AmbiK (Jeu de Données de Tâches Grammaticalement Ambiguës dans des Environnements de Restaurant). AmbiK est un jeu de données complète de contexte pour traiter des instructions grammaticalement ambiguës vers les dispositifs, collecté avec l'aide des LLMs et vérifié par des humains. AmbiK inclut 1000 paires de tâches grammaticalement ambiguës et leurs réponses adaptées, classées par types d'ambiguïté (préférences humaines, connaissance générale, sécurité). De plus, il contient 2000 jeux de données complets qui incluent des descriptions de l'environnement, des questions et des réponses, l'intention de l'utilisateur et des plans de tâches. AmbiK permet une comparaison unifiée des méthodes de détection d'ambiguïté et est disponible sur https://github.com/cog-model/AmbiK-dataset.",
      "upvotes": 31,
      "discussionId": "684153cf911d1b3135fa5e2e",
      "ai_summary": "AmbiK, a textual dataset of ambiguous instructions for kitchen robots, enables unified comparison of ambiguity detection methods.",
      "ai_keywords": [
        "Large Language Models",
        "LLMs",
        "behavior planning",
        "ambiguous instructions",
        "task ambiguity detection",
        "AmbiK",
        "dataset",
        "human-validated",
        "ambiguity types",
        "Human Preferences",
        "Common Sense Knowledge",
        "Safety",
        "environment descriptions",
        "clarifying questions",
        "user intents",
        "task plans"
      ]
    },
    "publishedAt": "2025-06-04T11:47:07.000Z",
    "title": "AmbiK: Dataset of Ambiguous Tasks in Kitchen Environment",
    "summary": "As a part of an embodied agent, Large Language Models (LLMs) are typically\nused for behavior planning given natural language instructions from the user.\nHowever, dealing with ambiguous instructions in real-world environments remains\na challenge for LLMs. Various methods for task ambiguity detection have been\nproposed. However, it is difficult to compare them because they are tested on\ndifferent datasets and there is no universal benchmark. For this reason, we\npropose AmbiK (Ambiguous Tasks in Kitchen Environment), the fully textual\ndataset of ambiguous instructions addressed to a robot in a kitchen\nenvironment. AmbiK was collected with the assistance of LLMs and is\nhuman-validated. It comprises 1000 pairs of ambiguous tasks and their\nunambiguous counterparts, categorized by ambiguity type (Human Preferences,\nCommon Sense Knowledge, Safety), with environment descriptions, clarifying\nquestions and answers, user intents, and task plans, for a total of 2000 tasks.\nWe hope that AmbiK will enable researchers to perform a unified comparison of\nambiguity detection methods. AmbiK is available at\nhttps://github.com/cog-model/AmbiK-dataset.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04089.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64198f70ed725fef6442b37e",
      "avatarUrl": "/avatars/580ab07a3067a9deb2977b0894226fe3.svg",
      "fullname": "Alexey Kovalev",
      "name": "AlexeyKov",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.02921",
      "authors": [
        {
          "_id": "683ff4dcfbc9041ef7274c51",
          "user": {
            "_id": "657eea68f4f72f2c4c44640d",
            "avatarUrl": "/avatars/033bc4f063cd36a79a0b4761f6ebe32c.svg",
            "isPro": false,
            "fullname": "Yijun YANG",
            "user": "thomasyyj",
            "type": "user"
          },
          "name": "Yijun Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:53:47.455Z",
          "hidden": false
        },
        {
          "_id": "683ff4dcfbc9041ef7274c52",
          "name": "Zeyu Huang",
          "hidden": false
        },
        {
          "_id": "683ff4dcfbc9041ef7274c53",
          "name": "Wenhao Zhu",
          "hidden": false
        },
        {
          "_id": "683ff4dcfbc9041ef7274c54",
          "name": "Zihan Qiu",
          "hidden": false
        },
        {
          "_id": "683ff4dcfbc9041ef7274c55",
          "name": "Fei Yuan",
          "hidden": false
        },
        {
          "_id": "683ff4dcfbc9041ef7274c56",
          "name": "Jeff Z. Pan",
          "hidden": false
        },
        {
          "_id": "683ff4dcfbc9041ef7274c57",
          "name": "Ivan Titov",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T14:23:06.000Z",
      "submittedOnDailyAt": "2025-06-05T02:04:55.586Z",
      "title": "Controlable Inspection Semblable à la Traitement du Langage Naturel",
      "submittedOnDailyBy": {
        "_id": "657eea68f4f72f2c4c44640d",
        "avatarUrl": "/avatars/033bc4f063cd36a79a0b4761f6ebe32c.svg",
        "isPro": false,
        "fullname": "Yijun YANG",
        "user": "thomasyyj",
        "type": "user"
      },
      "summary": "Actuellement, le cadre d'évaluation des modèles de langage de long contexte (LCLM) est classifié en tâches de monde réel et en tâches de synthèse. Les deux approches ont des limitations propres. Les tâches de monde réel deviennent complexes, difficiles à interpréter et caractériser, et sont vulnérables à la contamination des données. D'autre part, les tâches de synthèse adoptent le format « Naive in the High Stack » (NIAH), et la discontinuité entre « Naive » et « High Stack » détruit la justification des modèles comme représentants pratiques d'applications. En réponse à ces défis, il est proposé que un cadre d'évaluation idéal de long contexte doit avoir trois caractéristiques fondamentales : contexte continu, configuration contrôlable et évaluation de sécurité. Dans cette étude, un nouveau cadre de référence « LongBioBench » est présenté en utilisant des résultats de business générés artificiellement, et des évaluations de compréhension, d'inférence et de confiance dans les LCLM sont effectuées.\n\nDans l'évaluation expérimentale, 18 modèles de LCLM sont inclus, et plusieurs modèles présentent des erreurs significatives dans la compréhension des résultats de recherche et dans l'inférence de base, avec une diminution de confiance lorsque la longueur du contexte augmente. Une analyse approfondie montre que les décisions de conception, comme la discontinuité du contexte, la « Naive » des nombres et l'absence de détection d'erreurs, sont des vulnérabilités dans la vérification de la capacité de long contexte des modèles. De plus, l'entraînement de prédictions continues en long contexte s'adapte principalement à l'adaptation des embeddings RoPE à de longues longueurs de contexte. En résumé, comparé aux cadres de référence de synthèse existants, LongBioBench reflète des tâches de langage réelles, maintient la possibilité de contrôle et atteint un équilibre amélioré, étant hautement interprétable et configurable.",
      "upvotes": 25,
      "discussionId": "683ff4ddfbc9041ef7274c73",
      "githubRepo": "https://github.com/Thomasyyj/LongBio-Benchmark",
      "ai_summary": "LongBioBench is a new benchmark using artificially generated biographies to evaluate long-context language models across understanding, reasoning, and trustworthiness dimensions, addressing limitations in existing frameworks.",
      "ai_keywords": [
        "long-context language models (LCLM)",
        "real-world tasks",
        "synthetic tasks",
        "needle-in-the-haystack (NIAH)",
        "seamless context",
        "controllable setting",
        "sound evaluation",
        "LongBioBench",
        "semantic understanding",
        "elementary reasoning",
        "trustworthiness",
        "long-context continual pretraining",
        "RoPE embedding"
      ]
    },
    "publishedAt": "2025-06-03T10:23:06.000Z",
    "title": "A Controllable Examination for Long-Context Language Models",
    "summary": "Existing frameworks for evaluating long-context language models (LCLM) can be\nbroadly categorized into real-world and synthetic tasks. Despite their utility,\nboth approaches are accompanied by certain intrinsic limitations. Real-world\ntasks are too complex to interpret or characterize and are susceptible to data\ncontamination. In contrast, synthetic tasks often adopt the\nneedle-in-the-haystack (NIAH) format, wherein a lack of coherence between the\n\"needle\" and the \"haystack\" compromises their validity as proxies for realistic\napplications. In response to these challenges, we posit that an ideal\nlong-context evaluation framework should be characterized by three essential\nfeatures: seamless context, controllable setting, and\nsound evaluation. This study introduces LongBioBench, a\nnovel benchmark that utilizes artificially generated biographies as a\ncontrolled environment for assessing LCLMs across dimensions of\nunderstanding, reasoning, and trustworthiness.\nOur experimental evaluation, which includes 18 LCLMs in total,\ndemonstrates that most models still exhibit deficiencies in semantic\nunderstanding and elementary reasoning over retrieved results and are less\ntrustworthy as context length increases. Our further analysis indicates some\ndesign choices employed by existing synthetic benchmarks, such as contextual\nnon-coherence, numerical needles, and the absence of distractors, rendering\nthem vulnerable to test the model long-context capabilities. Moreover, we also\nreveal that long-context continual pretraining primarily adjusts RoPE embedding\nto accommodate extended context lengths. To sum up, compared to previous\nsynthetic benchmarks, LongBioBench achieves a better trade-off between\nmirroring authentic language tasks and maintaining controllability, and is\nhighly interpretable and configurable.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.02921.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "657eea68f4f72f2c4c44640d",
      "avatarUrl": "/avatars/033bc4f063cd36a79a0b4761f6ebe32c.svg",
      "fullname": "Yijun YANG",
      "name": "thomasyyj",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.16968",
      "authors": [
        {
          "_id": "683656aefd55e753bf26ed3e",
          "user": {
            "_id": "656864e12d73834278a8dea7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg",
            "isPro": true,
            "fullname": "Ahmed Heakl",
            "user": "ahmedheakl",
            "type": "user"
          },
          "name": "Ahmed Heakl",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-28T08:58:30.760Z",
          "hidden": false
        },
        {
          "_id": "683656aefd55e753bf26ed3f",
          "user": {
            "_id": "62676a94dacab364889bb36c",
            "avatarUrl": "/avatars/0ead41b44957eb30564ea685ed22781a.svg",
            "isPro": false,
            "fullname": "SARIM HASHMI",
            "user": "Sarim-Hash",
            "type": "user"
          },
          "name": "Sarim Hashmi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-02T07:49:01.879Z",
          "hidden": false
        },
        {
          "_id": "683656aefd55e753bf26ed40",
          "user": {
            "_id": "62eaadf4086bd1debb30a122",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62eaadf4086bd1debb30a122/wgxsPVnkOuEfq1oqlUhiB.jpeg",
            "isPro": false,
            "fullname": "Gustavo Stahl",
            "user": "GustavoStahl",
            "type": "user"
          },
          "name": "Gustavo Bertolo Stahl",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T08:31:48.782Z",
          "hidden": false
        },
        {
          "_id": "683656aefd55e753bf26ed41",
          "name": "Seung Hun Eddie Han",
          "hidden": false
        },
        {
          "_id": "683656aefd55e753bf26ed42",
          "name": "Salman Khan",
          "hidden": false
        },
        {
          "_id": "683656aefd55e753bf26ed43",
          "name": "Abdulrahman Mahmoud",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/T4ESSrZsC7163P3I8p17C.png",
        "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/WF-SJEyKKtpa3Zq0JvBXA.png",
        "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/Hl8Dkgmc4QL_l9YKPhRvD.png",
        "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/p-io7OU8TtxwvBp4_M1Hd.png",
        "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/bu6bpeVfonZgrXopd9f79.png"
      ],
      "publishedAt": "2025-05-22T17:48:53.000Z",
      "submittedOnDailyAt": "2025-06-05T06:33:02.615Z",
      "title": "CASS : Transpléando de Nvidia à AMD en utilisant des données, des modèles et des benchmarks de Nvidia",
      "submittedOnDailyBy": {
        "_id": "656864e12d73834278a8dea7",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg",
        "isPro": true,
        "fullname": "Ahmed Heakl",
        "user": "ahmedheakl",
        "type": "user"
      },
      "summary": "Introduis CASS. Ceci est le premier grand ensemble de données et système de modèles qui se concentre sur la transaction du code de GPU entre architectures de calcul. Il se concentre sur la traduction au niveau de code source (CUDA ⇔ HIP) et d'assembleur (Nvidia SASS ⇔ AMD RDNA3). Ce ensemble de données comprend 70k paires de codes validées et résout des déficiences importantes en puissance de code de GPU de bas niveau tant sur le hôte que sur le dispositif. En utilisant cette ressource, on entraîne des modèles de langage spécialisés de la famille CASS, atteignant une précision de traduction du code source de 95% et une précision de traduction d'assembleur de 37,5%, dépassant considérablement des lignes commerciales telles que GPT-4o, Claude et Hipify. Les codes générés maintiennent un rendement d'un nouveau paradigme dans plus de 85% des cas de test, maintenant un équilibre entre temps d'exécution et mémoire. Pour évaluer strictement, je présente CASS-Bench, un cadre de référence personnalisé qui s'étend à 16 types de jeux de GPU et inclut des résultats d'exécution réels. Tous les données, modèles et outils d'évaluation ont été lancés sous forme de code open pour encourager le développement de compilateurs de GPU, biais binaires et traductions de matériel guidées par modèles de langage. L'ensemble de données et le cadre de référence sont disponibles à https://huggingface.co/datasets/MBZUAI/cass{blue{HuggingFace}}, tandis que le code est à https://github.com/GustavoStahl/CASS{blue{GitHub}}.",
      "upvotes": 23,
      "discussionId": "683656b0fd55e753bf26edf7",
      "githubRepo": "https://github.com/GustavoStahl/CASS",
      "ai_summary": "CASS is a dataset and model suite for GPU code transpilation at both source and assembly levels, achieving high accuracy and performance matching with native code.",
      "ai_keywords": [
        "cross-architecture GPU code transpilation",
        "CASS",
        "CUDA",
        "HIP",
        "Nvidia SASS",
        "AMD RDNA3",
        "domain-specific language models",
        "source translation accuracy",
        "assembly translation accuracy",
        "native performance",
        "CASS-Bench",
        "GPU compiler tooling",
        "binary compatibility",
        "LLM-guided hardware translation"
      ]
    },
    "publishedAt": "2025-05-22T13:48:53.000Z",
    "title": "CASS: Nvidia to AMD Transpilation with Data, Models, and Benchmark",
    "summary": "We introduce CASS, the first large-scale dataset and model suite for\ncross-architecture GPU code transpilation, targeting both source-level (CUDA\nleftrightarrow HIP) and assembly-level (Nvidia SASS leftrightarrow AMD\nRDNA3) translation. The dataset comprises 70k verified code pairs across host\nand device, addressing a critical gap in low-level GPU code portability.\nLeveraging this resource, we train the CASS family of domain-specific language\nmodels, achieving 95% source translation accuracy and 37.5% assembly\ntranslation accuracy, substantially outperforming commercial baselines such as\nGPT-4o, Claude, and Hipify. Our generated code matches native performance in\nover 85% of test cases, preserving runtime and memory behavior. To support\nrigorous evaluation, we introduce CASS-Bench, a curated benchmark spanning 16\nGPU domains with ground-truth execution. All data, models, and evaluation tools\nare released as open source to foster progress in GPU compiler tooling, binary\ncompatibility, and LLM-guided hardware translation. Dataset and benchmark are\non\nhttps://huggingface.co/datasets/MBZUAI/cass{blue{HuggingFace}},\nwith code at\nhttps://github.com/GustavoStahl/CASS{blue{GitHub}}.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/T4ESSrZsC7163P3I8p17C.png",
      "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/WF-SJEyKKtpa3Zq0JvBXA.png",
      "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/Hl8Dkgmc4QL_l9YKPhRvD.png",
      "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/p-io7OU8TtxwvBp4_M1Hd.png",
      "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/bu6bpeVfonZgrXopd9f79.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16968.png",
    "numComments": 5,
    "submittedBy": {
      "_id": "656864e12d73834278a8dea7",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg",
      "fullname": "Ahmed Heakl",
      "name": "ahmedheakl",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 39
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.04180",
      "authors": [
        {
          "_id": "6840fefb3098ab525906d852",
          "name": "Yuhao Wu",
          "hidden": false
        },
        {
          "_id": "6840fefb3098ab525906d853",
          "name": "Yushi Bai",
          "hidden": false
        },
        {
          "_id": "6840fefb3098ab525906d854",
          "user": {
            "_id": "637f228152229c63921119c3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637f228152229c63921119c3/acwXorra1r9_7i3KlBFjS.jpeg",
            "isPro": false,
            "fullname": "Zhiqiang Hu",
            "user": "Zhiqiang007",
            "type": "user"
          },
          "name": "Zhiqiang Hu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:27:09.305Z",
          "hidden": false
        },
        {
          "_id": "6840fefb3098ab525906d855",
          "name": "Juanzi Li",
          "hidden": false
        },
        {
          "_id": "6840fefb3098ab525906d856",
          "name": "Roy Ka-Wei Lee",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-04T17:27:42.000Z",
      "submittedOnDailyAt": "2025-06-05T00:58:27.883Z",
      "title": "Super Lítera : Modèle de génération de longues phrases synthétiques",
      "submittedOnDailyBy": {
        "_id": "64ed568ccf6118a9379a61b8",
        "avatarUrl": "/avatars/6d040cbcb4a9b624cbe64c9d01cd5c88.svg",
        "isPro": false,
        "fullname": "Yushi Bai",
        "user": "bys0318",
        "type": "user"
      },
      "summary": "La génération de longs textes est un défi à long terme pour les modèles de langage de grande taille (LLMs), notamment en raison de la nécessité de maintenir la continuité, garantir la cohérence logique et maintenir la qualité de la texture à mesure que la longueur de la séquence augmente. Pour résoudre ces limitations, nous proposons le SuperWriter-Agent. Le SuperWriter-Agent est un cadre de travail basé sur des agents conçus pour améliorer la qualité et la cohérence de la génération de longs textes. Il introduit des étapes explicites et structurées de pensée et d'amélioration dans la chaîne de génération, ce qui conduit le modèle à suivre un processus plus rigoureux et cognitivement orienté. En se basant sur ce cadre, nous avons construit un ensemble de données de fine-tuning standard pour entraîner le SuperWriter-LM de 7B et développé un processus d'Optimisation de Préférences Directes (DPO) hiérarchique. Ce processus utilise la recherche d'arbre de Monte Carlo (MCTS) pour évaluer la qualité finale et optimiser chaque étape de génération de manière appropriée. Les résultats des expériences sur différents benchmarks montrent que le SuperWriter-LM a atteint un rendement plus récent et a dépassé les modèles plus grands dans les évaluations automatiques et humaines. De plus, des tests détaillés montrent l'effet du DPO hiérarchique et mettent en avant la fonction des étapes de pensée structurées dans l'amélioration de la qualité de la génération de longs textes.",
      "upvotes": 20,
      "discussionId": "6840fefc3098ab525906d89c",
      "ai_summary": "SuperWriter-Agent enhances long-form text generation by integrating structured planning and refinement, achieving top performance with a 7B model and hierarchical Direct Preference Optimization.",
      "ai_keywords": [
        "agent-based framework",
        "structured thinking-through planning",
        "refinement stages",
        "SuperWriter-Agent",
        "SuperWriter-LM",
        "hierarchical Direct Preference Optimization",
        "Monte Carlo Tree Search",
        "DPO",
        "automatic evaluation",
        "human evaluation",
        "ablation studies"
      ]
    },
    "publishedAt": "2025-06-04T13:27:42.000Z",
    "title": "SuperWriter: Reflection-Driven Long-Form Generation with Large Language\n  Models",
    "summary": "Long-form text generation remains a significant challenge for large language\nmodels (LLMs), particularly in maintaining coherence, ensuring logical\nconsistency, and preserving text quality as sequence length increases. To\naddress these limitations, we propose SuperWriter-Agent, an agent-based\nframework designed to enhance the quality and consistency of long-form text\ngeneration. SuperWriter-Agent introduces explicit structured thinking-through\nplanning and refinement stages into the generation pipeline, guiding the model\nto follow a more deliberate and cognitively grounded process akin to that of a\nprofessional writer. Based on this framework, we construct a supervised\nfine-tuning dataset to train a 7B SuperWriter-LM. We further develop a\nhierarchical Direct Preference Optimization (DPO) procedure that uses Monte\nCarlo Tree Search (MCTS) to propagate final quality assessments and optimize\neach generation step accordingly. Empirical results across diverse benchmarks\ndemonstrate that SuperWriter-LM achieves state-of-the-art performance,\nsurpassing even larger-scale baseline models in both automatic evaluation and\nhuman evaluation. Furthermore, comprehensive ablation studies demonstrate the\neffectiveness of hierarchical DPO and underscore the value of incorporating\nstructured thinking steps to improve the quality of long-form text generation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04180.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ed568ccf6118a9379a61b8",
      "avatarUrl": "/avatars/6d040cbcb4a9b624cbe64c9d01cd5c88.svg",
      "fullname": "Yushi Bai",
      "name": "bys0318",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.01320",
      "authors": [
        {
          "_id": "684124368cb0edba3ab8f738",
          "name": "Taehoon Yoon",
          "hidden": false
        },
        {
          "_id": "684124368cb0edba3ab8f739",
          "name": "Yunhong Min",
          "hidden": false
        },
        {
          "_id": "684124368cb0edba3ab8f73a",
          "name": "Kyeongmin Yeo",
          "hidden": false
        },
        {
          "_id": "684124368cb0edba3ab8f73b",
          "name": "Minhyuk Sung",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-02T05:02:33.000Z",
      "submittedOnDailyAt": "2025-06-05T03:38:33.152Z",
      "title": "Ψ-Sampler : Ajuste de la récompense lors de l'échantillonnage initial des particules pour l'inférence basée sur le SMC à l'aide d'un modèle de pondération",
      "submittedOnDailyBy": {
        "_id": "66ee81b676a8038cb42c8caa",
        "avatarUrl": "/avatars/9b4c5ded9c94788c35ce7ffbc2f8d24b.svg",
        "isPro": false,
        "fullname": "Yunhong Min",
        "user": "myhong",
        "type": "user"
      },
      "summary": "Psi-Sampler est présenté. Ce cadre de travail soutient efficacement l'ajustement de récompenses dans l'inférence de modèles génératifs basés sur des scores, en utilisant un processus d'échantillonnage initial basé sur pCNL. L'ajustement de récompenses dans l'inférence de modèles génératifs basés sur des scores a été un sujet plus largement abordé, grâce au changement de paradigme qui a émergé depuis l'optimisation après l'entraînement jusqu'à la modification de plugins. Le cœur de ce processus se réalise par l'application du méthode de Monte Carlo Sequential (SMC). Actuellement, les méthodes utilisent généralement initialiser les particules par une distribution gaussienne, ce qui ne permet pas une compréhension suffisante des régions liées aux récompenses, ce qui diminue l'efficacité de l'échantillonnage. Nous montrons que l'initialisation des particules de manière intéressante pour les récompenses améliore significativement le rendement de l'ajustement. On présente l'algorithme preconditioned Crank-Nicolson Langevin (pCNL) pour faciliter l'échantillonnage de particules dans des espaces potentiels de haute dimension, en combinant la proposition par dimension et la dérivée avec la dynamique. Cette approximation permet un échantillonnage efficace et scalable de particules, montrant un améliorament positif du rendement dans des tâches de génération de modèles de couches, de génération sensorielle et de génération de préférences artistiques.",
      "upvotes": 15,
      "discussionId": "6841243c8cb0edba3ab8f8bf",
      "ai_summary": "The framework $\\Psi$-Sampler uses SMC with pCNL for efficient posterior sampling and reward alignment in score-based generative models, enhancing performance across various tasks.",
      "ai_keywords": [
        "SMC-based framework",
        "pCNL-based initial particle sampling",
        "inference-time reward alignment",
        "score-based generative model",
        "Sequential Monte Carlo",
        "denoising process",
        "Gaussian prior",
        "reward-aware posterior",
        "preconditioned Crank-Nicolson Langevin",
        "layout-to-image generation",
        "quantity-aware generation",
        "aesthetic-preference generation"
      ]
    },
    "publishedAt": "2025-06-02T01:02:33.000Z",
    "title": "Ψ-Sampler: Initial Particle Sampling for SMC-Based Inference-Time\n  Reward Alignment in Score Models",
    "summary": "We introduce Psi-Sampler, an SMC-based framework incorporating pCNL-based\ninitial particle sampling for effective inference-time reward alignment with a\nscore-based generative model. Inference-time reward alignment with score-based\ngenerative models has recently gained significant traction, following a broader\nparadigm shift from pre-training to post-training optimization. At the core of\nthis trend is the application of Sequential Monte Carlo (SMC) to the denoising\nprocess. However, existing methods typically initialize particles from the\nGaussian prior, which inadequately captures reward-relevant regions and results\nin reduced sampling efficiency. We demonstrate that initializing from the\nreward-aware posterior significantly improves alignment performance. To enable\nposterior sampling in high-dimensional latent spaces, we introduce the\npreconditioned Crank-Nicolson Langevin (pCNL) algorithm, which combines\ndimension-robust proposals with gradient-informed dynamics. This approach\nenables efficient and scalable posterior sampling and consistently improves\nperformance across various reward alignment tasks, including layout-to-image\ngeneration, quantity-aware generation, and aesthetic-preference generation, as\ndemonstrated in our experiments.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01320.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66ee81b676a8038cb42c8caa",
      "avatarUrl": "/avatars/9b4c5ded9c94788c35ce7ffbc2f8d24b.svg",
      "fullname": "Yunhong Min",
      "name": "myhong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.04225",
      "authors": [
        {
          "_id": "68413366adeec0116d071af2",
          "user": {
            "_id": "63425d394c9a81858b36aeb5",
            "avatarUrl": "/avatars/511ad6a75bd1c10fc510ef527e7f8e5b.svg",
            "isPro": false,
            "fullname": "Tianyu Huang",
            "user": "tyhuang",
            "type": "user"
          },
          "name": "Tianyu Huang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:26:41.198Z",
          "hidden": false
        },
        {
          "_id": "68413366adeec0116d071af3",
          "name": "Wangguandong Zheng",
          "hidden": false
        },
        {
          "_id": "68413366adeec0116d071af4",
          "name": "Tengfei Wang",
          "hidden": false
        },
        {
          "_id": "68413366adeec0116d071af5",
          "name": "Yuhao Liu",
          "hidden": false
        },
        {
          "_id": "68413366adeec0116d071af6",
          "name": "Zhenwei Wang",
          "hidden": false
        },
        {
          "_id": "68413366adeec0116d071af7",
          "name": "Junta Wu",
          "hidden": false
        },
        {
          "_id": "68413366adeec0116d071af8",
          "name": "Jie Jiang",
          "hidden": false
        },
        {
          "_id": "68413366adeec0116d071af9",
          "name": "Hui Li",
          "hidden": false
        },
        {
          "_id": "68413366adeec0116d071afa",
          "name": "Rynson W. H. Lau",
          "hidden": false
        },
        {
          "_id": "68413366adeec0116d071afb",
          "name": "Wangmeng Zuo",
          "hidden": false
        },
        {
          "_id": "68413366adeec0116d071afc",
          "name": "Chunchao Guo",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63425d394c9a81858b36aeb5/cjZH2kR6B3y9IAmmRHNJS.mp4"
      ],
      "publishedAt": "2025-06-04T17:59:04.000Z",
      "submittedOnDailyAt": "2025-06-05T06:00:26.815Z",
      "title": "Voyager : Grande distance et conception de vidéos mondiales pour la création de panneaux 3D explorables",
      "submittedOnDailyBy": {
        "_id": "63425d394c9a81858b36aeb5",
        "avatarUrl": "/avatars/511ad6a75bd1c10fc510ef527e7f8e5b.svg",
        "isPro": false,
        "fullname": "Tianyu Huang",
        "user": "tyhuang",
        "type": "user"
      },
      "summary": "Dans le développement d'applications en réalité, la technologie de modélisation 3D est essentielle dans diverses domaines comme les jeux 3D et la réalité virtuelle (VR). Récemment, les avancées dans la génération de modèles 3D ont été développées à partir de textes et d'images, mais la configuration de scènes 3D reste un problème complexe et difficile. Dans cet article, nous présentons un nouveau cadre de travail appelé \"Voyager\" qui permet de générer des séquences de clusters de points 3D globalement cohérents à partir d'une seule image, en se basant sur un chemin de caméra spécifié par l'utilisateur. À différence des méthodes existantes, Voyager maintient la cohérence de la correspondance dans chaque cadre tout en réalisant la génération et la reconstruction de scènes 3D cohérentes, réduisant ainsi la nécessité de processus de reconstruction 3D (par exemple, actions basées sur la structure, stéréoscopie multiple points). Notre approche comprend trois éléments clés : 1) Diffusion de vidéos globalement cohérentes : une architecture unifiée qui permet de générer des séquences de vidéo RGB et de profondeur cohérentes en se basant sur des observations mondiales, assurant la cohérence globale. 2) Exploration mondiale à longue distance : en utilisant des cachés mondiaux efficaces, l'élimination de points, l'inférence automatique et le sampling de vidéo doux, on réalise l'expansion répétitive de scènes et la cohérence dans le contexte. 3) Moteur de données interchangeables : automatise l'estimation de l'orientation de la caméra et la prédiction de la profondeur pour n'importe quel vidéo, facilitant la collecte de grands ensembles de données et divers, sans nécessité de marquer 3D. Cette architecture améliore clairement la qualité visuelle et la précision de généralisation par rapport aux méthodes actuelles, et permet d'être appliquée dans diverses applications.",
      "upvotes": 14,
      "discussionId": "6841336badeec0116d071c2b",
      "projectPage": "https://voyager-world.github.io",
      "githubRepo": "https://github.com/Voyager-World/Voyager",
      "ai_summary": "Voyager is a video diffusion framework that generates world-consistent 3D point-cloud sequences from a single image, enabling long-range, consistent 3D scene exploration with user-defined camera paths.",
      "ai_keywords": [
        "video diffusion",
        "world-consistent video diffusion",
        "3D point-cloud sequences",
        "camera path",
        "end-to-end scene generation",
        "consistent frames",
        "unified architecture",
        "RGB and depth video sequences",
        "world observation",
        "global coherence",
        "long-range world exploration",
        "world cache",
        "point culling",
        "auto-regressive inference",
        "smooth video sampling",
        "scene extension",
        "context-aware consistency",
        "scalable data engine",
        "camera pose estimation",
        "metric depth prediction",
        "large-scale",
        "diverse training data"
      ]
    },
    "publishedAt": "2025-06-04T13:59:04.000Z",
    "title": "Voyager: Long-Range and World-Consistent Video Diffusion for Explorable\n  3D Scene Generation",
    "summary": "Real-world applications like video gaming and virtual reality often demand\nthe ability to model 3D scenes that users can explore along custom camera\ntrajectories. While significant progress has been made in generating 3D objects\nfrom text or images, creating long-range, 3D-consistent, explorable 3D scenes\nremains a complex and challenging problem. In this work, we present Voyager, a\nnovel video diffusion framework that generates world-consistent 3D point-cloud\nsequences from a single image with user-defined camera path. Unlike existing\napproaches, Voyager achieves end-to-end scene generation and reconstruction\nwith inherent consistency across frames, eliminating the need for 3D\nreconstruction pipelines (e.g., structure-from-motion or multi-view stereo).\nOur method integrates three key components: 1) World-Consistent Video\nDiffusion: A unified architecture that jointly generates aligned RGB and depth\nvideo sequences, conditioned on existing world observation to ensure global\ncoherence 2) Long-Range World Exploration: An efficient world cache with point\nculling and an auto-regressive inference with smooth video sampling for\niterative scene extension with context-aware consistency, and 3) Scalable Data\nEngine: A video reconstruction pipeline that automates camera pose estimation\nand metric depth prediction for arbitrary videos, enabling large-scale, diverse\ntraining data curation without manual 3D annotations. Collectively, these\ndesigns result in a clear improvement over existing methods in visual quality\nand geometric accuracy, with versatile applications.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63425d394c9a81858b36aeb5/cjZH2kR6B3y9IAmmRHNJS.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04225.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63425d394c9a81858b36aeb5",
      "avatarUrl": "/avatars/511ad6a75bd1c10fc510ef527e7f8e5b.svg",
      "fullname": "Tianyu Huang",
      "name": "tyhuang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.04228",
      "authors": [
        {
          "_id": "684103aed45a1fc5540ddc10",
          "name": "Sihui Ji",
          "hidden": false
        },
        {
          "_id": "684103aed45a1fc5540ddc11",
          "name": "Hao Luo",
          "hidden": false
        },
        {
          "_id": "684103aed45a1fc5540ddc12",
          "user": {
            "_id": "644a1b6401e18bf93a6f45c1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644a1b6401e18bf93a6f45c1/P0i_CgCrIzOS2tYRlxoE9.png",
            "isPro": false,
            "fullname": "xichen",
            "user": "xichenhku",
            "type": "user"
          },
          "name": "Xi Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:26:50.027Z",
          "hidden": false
        },
        {
          "_id": "684103aed45a1fc5540ddc13",
          "name": "Yuanpeng Tu",
          "hidden": false
        },
        {
          "_id": "684103aed45a1fc5540ddc14",
          "name": "Yiyang Wang",
          "hidden": false
        },
        {
          "_id": "684103aed45a1fc5540ddc15",
          "name": "Hengshuang Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-04T17:59:58.000Z",
      "submittedOnDailyAt": "2025-06-05T01:11:16.967Z",
      "title": "LayerFlow : Modèle d'unité de génération de vidéos à la couche d'intérêt",
      "submittedOnDailyBy": {
        "_id": "644a1b6401e18bf93a6f45c1",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644a1b6401e18bf93a6f45c1/P0i_CgCrIzOS2tYRlxoE9.png",
        "isPro": false,
        "fullname": "xichen",
        "user": "xichenhku",
        "type": "user"
      },
      "summary": "LayerFlow est une solution intégrale pour la génération d'images qui reconnaît l'information des couches. En offrant un traitement par couche, LayerFlow génère des images qui comprennent un fond transparent, un fond clair et des scènes brandées. De plus, il soutient également la décomposition d'images brandées, la génération de fonds pour les marques de font, et plusieurs versions différentes. Il commence le processus de diffusion vidéo en contexte, organise les images de chaque couche comme sous-clips et utilise des techniques d'enveloppement de couches pour différencier les sous-clips par couche. De cette manière, les versions précédemment mentionnées peuvent être facilement soutenues dans un seul cadre intégré. Étant donné l'absence de haute qualité de formation de vidéos par couche, il est conçu une stratégie d'entraînement pas à pas pour répondre aux images statiques avec des annotations de couches de haute qualité. Spécifiquement, il commence par l'entraînement de modèles avec des données de vidéo de basse qualité, ajuste la LoRA des actions pour s'adapter aux frames statiques, et ensuite entraîne la LoRA du contenu avec une mix de données d'images de haute qualité par couche et de vidéos de copie et collage. Pendant l'inférence, il supprime la LoRA des actions et génère des vidéos lisses de la couche désirée.",
      "upvotes": 12,
      "discussionId": "684103b0d45a1fc5540ddca8",
      "ai_summary": "LayerFlow is a unified framework for generating layer-aware videos using a text-to-video diffusion transformer and layer embeddings, supporting various video generation tasks with a multi-stage training strategy.",
      "ai_keywords": [
        "LayerFlow",
        "text-to-video diffusion transformer",
        "layer embeddings",
        "sub-clips",
        "multi-stage training strategy",
        "motion LoRA",
        "content LoRA",
        "layered images",
        "copy-pasted video data",
        "smooth videos"
      ]
    },
    "publishedAt": "2025-06-04T13:59:58.000Z",
    "title": "LayerFlow: A Unified Model for Layer-aware Video Generation",
    "summary": "We present LayerFlow, a unified solution for layer-aware video generation.\nGiven per-layer prompts, LayerFlow generates videos for the transparent\nforeground, clean background, and blended scene. It also supports versatile\nvariants like decomposing a blended video or generating the background for the\ngiven foreground and vice versa. Starting from a text-to-video diffusion\ntransformer, we organize the videos for different layers as sub-clips, and\nleverage layer embeddings to distinguish each clip and the corresponding\nlayer-wise prompts. In this way, we seamlessly support the aforementioned\nvariants in one unified framework. For the lack of high-quality layer-wise\ntraining videos, we design a multi-stage training strategy to accommodate\nstatic images with high-quality layer annotations. Specifically, we first train\nthe model with low-quality video data. Then, we tune a motion LoRA to make the\nmodel compatible with static frames. Afterward, we train the content LoRA on\nthe mixture of image data with high-quality layered images along with\ncopy-pasted video data. During inference, we remove the motion LoRA thus\ngenerating smooth videos with desired layers.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04228.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "644a1b6401e18bf93a6f45c1",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644a1b6401e18bf93a6f45c1/P0i_CgCrIzOS2tYRlxoE9.png",
      "fullname": "xichen",
      "name": "xichenhku",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 41
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.03139",
      "authors": [
        {
          "_id": "683fb0a7be8421eda3152283",
          "user": {
            "_id": "676b86e79ff0244316f7202f",
            "avatarUrl": "/avatars/3e1d26312a96752356895ab88eeb3ce0.svg",
            "isPro": false,
            "fullname": "chensiqi",
            "user": "xiaoooobai",
            "type": "user"
          },
          "name": "Siqi Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:28:25.792Z",
          "hidden": false
        },
        {
          "_id": "683fb0a7be8421eda3152284",
          "name": "Xinyu Dong",
          "hidden": false
        },
        {
          "_id": "683fb0a7be8421eda3152285",
          "user": {
            "_id": "6692aff88db712bad780f02a",
            "avatarUrl": "/avatars/5dc4b1c27c70f6a64864711dbff4910f.svg",
            "isPro": false,
            "fullname": "xhl",
            "user": "zjuxhl",
            "type": "user"
          },
          "name": "Haolei Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:56:42.705Z",
          "hidden": false
        },
        {
          "_id": "683fb0a7be8421eda3152286",
          "name": "Xingyu Wu",
          "hidden": false
        },
        {
          "_id": "683fb0a7be8421eda3152287",
          "name": "Fei Tang",
          "hidden": false
        },
        {
          "_id": "683fb0a7be8421eda3152288",
          "name": "Hang Zhang",
          "hidden": false
        },
        {
          "_id": "683fb0a7be8421eda3152289",
          "user": {
            "_id": "64098738342c26884c792c93",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64098738342c26884c792c93/SxBUd-wLrl-PjQsrVYJte.jpeg",
            "isPro": false,
            "fullname": "Yuchen Yan",
            "user": "yanyc",
            "type": "user"
          },
          "name": "Yuchen Yan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:56:47.244Z",
          "hidden": false
        },
        {
          "_id": "683fb0a7be8421eda315228a",
          "name": "Linjuan Wu",
          "hidden": false
        },
        {
          "_id": "683fb0a7be8421eda315228b",
          "name": "Wenqi Zhang",
          "hidden": false
        },
        {
          "_id": "683fb0a7be8421eda315228c",
          "name": "Guiyang Hou",
          "hidden": false
        },
        {
          "_id": "683fb0a7be8421eda315228d",
          "name": "Yongliang Shen",
          "hidden": false
        },
        {
          "_id": "683fb0a7be8421eda315228e",
          "name": "Weiming Lu",
          "hidden": false
        },
        {
          "_id": "683fb0a7be8421eda315228f",
          "name": "Yueting Zhuang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T17:58:57.000Z",
      "submittedOnDailyAt": "2025-06-05T03:44:24.728Z",
      "title": "SVGenius : Benchmark de compréhension, édition et génération de SVG avec LLM",
      "submittedOnDailyBy": {
        "_id": "5e1058e9fcf41d740b69966d",
        "avatarUrl": "/avatars/ce74839ba871f2b54313a670a233ba82.svg",
        "isPro": false,
        "fullname": "Yongliang Shen",
        "user": "tricktreat",
        "type": "user"
      },
      "summary": "Les modèles de langage grand (LLMs) et les modèles multimodal de LLMs ont démontré les capacités attendues dans le traitement de SVG, mais les benchmarks actuels sont limités dans leur couverture réelle et ne disposent pas d'une classification de complexité ni d'un paradigme d'évaluation séparé. Nous présentons SVGenius, un benchmark strict qui inclut 2,377 questions dans trois étapes : compréhension, édition et génération, construit à partir de données réelles et avec une classification systématique de complexité. Ce benchmark évalue les modèles dans 8 catégories de tâches et 18 métriques. 22 modèles principaux (à différentes échelles, architectures, paradigmes d'apprentissage et niveaux d'accessibilité) sont évalués. L'analyse montre que les modèles propres ont un défi significatif par rapport aux modèles ouverts, mais tous les modèles montrent une diminution systématique de leur performance lorsque la complexité augmente, démontrant les limitations fondamentales des méthodes actuelles. Cependant, l'apprentissage par renforcement avec logique est plus efficace pour surmonter ces limitations, bien que la capacité la plus difficile chez tous les modèles soit la transformation d'un style. SVGenius est le premier cadre d'évaluation strict pour le traitement de SVG, fournissant des insights importants pour le développement de modèles de graphiques vectoriels et l'avancement des applications de conception graphique automatique. Les données supplémentaires et complémentaires (y compris les données et le code) sont disponibles sur https://zju-real.github.io/SVGenius.",
      "upvotes": 12,
      "discussionId": "683fb0a7be8421eda31522ca",
      "projectPage": "https://zju-real.github.io/SVGenius/",
      "githubRepo": "https://github.com/ZJU-REAL/SVGenius",
      "ai_summary": "SVGenius evaluates Large Language Models and Multimodal LLMs for SVG processing using a comprehensive benchmark across three dimensions: understanding, editing, and generation, revealing insights into model capabilities and limitations.",
      "ai_keywords": [
        "Large Language Models",
        "Multimodal LLMs",
        "SVG processing",
        "SVGenius",
        "complexity stratification",
        "reasoning-enhanced training",
        "style transfer"
      ]
    },
    "publishedAt": "2025-06-03T13:58:57.000Z",
    "title": "SVGenius: Benchmarking LLMs in SVG Understanding, Editing and Generation",
    "summary": "Large Language Models (LLMs) and Multimodal LLMs have shown promising\ncapabilities for SVG processing, yet existing benchmarks suffer from limited\nreal-world coverage, lack of complexity stratification, and fragmented\nevaluation paradigms. We introduce SVGenius, a comprehensive benchmark\ncomprising 2,377 queries across three progressive dimensions: understanding,\nediting, and generation. Built on real-world data from 24 application domains\nwith systematic complexity stratification, SVGenius evaluates models through 8\ntask categories and 18 metrics. We assess 22 mainstream models spanning\ndifferent scales, architectures, training paradigms, and accessibility levels.\nOur analysis reveals that while proprietary models significantly outperform\nopen-source counterparts, all models exhibit systematic performance degradation\nwith increasing complexity, indicating fundamental limitations in current\napproaches; however, reasoning-enhanced training proves more effective than\npure scaling for overcoming these limitations, though style transfer remains\nthe most challenging capability across all model types. SVGenius establishes\nthe first systematic evaluation framework for SVG processing, providing crucial\ninsights for developing more capable vector graphics models and advancing\nautomated graphic design applications. Appendix and supplementary materials\n(including all data and code) are available at\nhttps://zju-real.github.io/SVGenius.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03139.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5e1058e9fcf41d740b69966d",
      "avatarUrl": "/avatars/ce74839ba871f2b54313a670a233ba82.svg",
      "fullname": "Yongliang Shen",
      "name": "tricktreat",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 22
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.03295",
      "authors": [
        {
          "_id": "6840e7d81fadbc85ae3bdc0f",
          "name": "Yubo Wang",
          "hidden": false
        },
        {
          "_id": "6840e7d81fadbc85ae3bdc10",
          "name": "Ping Nie",
          "hidden": false
        },
        {
          "_id": "6840e7d81fadbc85ae3bdc11",
          "name": "Kai Zou",
          "hidden": false
        },
        {
          "_id": "6840e7d81fadbc85ae3bdc12",
          "name": "Lijun Wu",
          "hidden": false
        },
        {
          "_id": "6840e7d81fadbc85ae3bdc13",
          "name": "Wenhu Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T18:35:52.000Z",
      "submittedOnDailyAt": "2025-06-05T02:48:14.083Z",
      "title": "La libération du potentiel logique des modèles de langue pré-entraînés par apprentissage automatique à travers l'intuition\nAjustements pour le problème spécifique",
      "submittedOnDailyBy": {
        "_id": "636a35eff8d9af4aea181608",
        "avatarUrl": "/avatars/d9c5cf3491243d1f2b1c5df1873ee8e7.svg",
        "isPro": false,
        "fullname": "yubo",
        "user": "ubowang",
        "type": "user"
      },
      "summary": "Nous avons observé que des LLMs forts (par exemple : Qwen-Math, MiMo, Phi-4) peuvent démontrer de manière impressionnante la grande possibilité logique dès le début de leur entraînement. L'apprentissage par renforcement (RL) peut améliorer considérablement ces capacités logiques dans des tâches. Selon des études récentes, l'apprentissage par renforcement pour un problème unique peut libérer la capacité logique de ces modèles. Cependant, l'RL est très coûteux et instable. Même un apprentissage par renforcement simple nécessite des centaines d'heures de hardware, ce qui soulève une question importante : existent-ils des méthodes plus efficaces pour libérer la capacité logique de ces puissants modèles de base ? Ce travail présente une solution à ce problème, montrant que l'apprentissage par critique et fine-tuning (CFT) pour des problèmes uniques peut libérer efficacement la capacité logique des LLMs. Notre méthode rassemble des solutions générées par différents modèles pour un problème unique et construit des données de critique à l'aide d'un modèle d'apprentissage détaillé. Nous avons effectué un fine-tuning avec des données de CFT pour des modèles de Qwen et Llama (nombre de paramètres : 1,5B à 14B), observant une amélioration claire dans différentes tâches logiques. Par exemple, Qwen-Math-7B-CFT a montré un accroissement moyen de 15% sur 6 marqueurs mathématiques et de 16% sur 3 marqueurs de théorie de la logique, même avec quelques 5 heures d'entraînement. Ces résultats sont comparables à ceux de RL, mais avec un consommation de calcul réduite d'au moins 20 fois, démontrant l'efficacité et la généralité du CFT pour des problèmes uniques. Selon la recherche, la robustesse du CFT pour des problèmes uniques se manifeste également dans d'autres problèmes de prompt. Ces résultats montrent qu'il existe des méthodes simples et avec un marge de calcul pour libérer la capacité logique des LLMs modernes.",
      "upvotes": 10,
      "discussionId": "6840e7d81fadbc85ae3bdc45",
      "ai_summary": "Critique Fine-Tuning on a single problem can efficiently enhance the reasoning capabilities of large language models with significant performance gains and reduced computational cost compared to reinforcement learning.",
      "ai_keywords": [
        "Critique Fine-Tuning",
        "teacher LLMs",
        "Qwen-Math",
        "Llama family models",
        "reasoning tasks",
        "one-shot CFT",
        "performance gains",
        "logic reasoning benchmarks",
        "math benchmarks",
        "prompt problems"
      ]
    },
    "publishedAt": "2025-06-03T14:35:52.000Z",
    "title": "Unleashing the Reasoning Potential of Pre-trained LLMs by Critique\n  Fine-Tuning on One Problem",
    "summary": "We have witnessed that strong LLMs like Qwen-Math, MiMo, and Phi-4 possess\nimmense reasoning potential inherited from the pre-training stage. With\nreinforcement learning (RL), these models can improve dramatically on reasoning\ntasks. Recent studies have shown that even RL on a single problem can unleash\nthese models' reasoning capabilities. However, RL is not only expensive but\nalso unstable. Even one-shot RL requires hundreds of GPU hours. This raises a\ncritical question: Is there a more efficient way to unleash the reasoning\npotential of these powerful base LLMs? In this work, we demonstrate that\nCritique Fine-Tuning (CFT) on only one problem can effectively unleash the\nreasoning potential of LLMs. Our method constructs critique data by collecting\ndiverse model-generated solutions to a single problem and using teacher LLMs to\nprovide detailed critiques. We fine-tune Qwen and Llama family models, ranging\nfrom 1.5B to 14B parameters, on the CFT data and observe significant\nperformance gains across diverse reasoning tasks. For example, with just 5 GPU\nhours of training, Qwen-Math-7B-CFT show an average improvement of 15% on six\nmath benchmarks and 16% on three logic reasoning benchmarks. These results are\ncomparable to or even surpass the results from RL with 20x less compute.\nAblation studies reveal the robustness of one-shot CFT across different prompt\nproblems. These results highlight one-shot CFT as a simple, general, and\ncompute-efficient approach to unleashing the reasoning capabilities of modern\nLLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03295.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "636a35eff8d9af4aea181608",
      "avatarUrl": "/avatars/d9c5cf3491243d1f2b1c5df1873ee8e7.svg",
      "fullname": "yubo",
      "name": "ubowang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.24500",
      "authors": [
        {
          "_id": "683fb063ef97de05eb2a44cc",
          "name": "Guiyang Hou",
          "hidden": false
        },
        {
          "_id": "683fb063ef97de05eb2a44cd",
          "name": "Xing Gao",
          "hidden": false
        },
        {
          "_id": "683fb063ef97de05eb2a44ce",
          "name": "Yuchuan Wu",
          "hidden": false
        },
        {
          "_id": "683fb063ef97de05eb2a44cf",
          "name": "Xiang Huang",
          "hidden": false
        },
        {
          "_id": "683fb063ef97de05eb2a44d0",
          "name": "Wenqi Zhang",
          "hidden": false
        },
        {
          "_id": "683fb063ef97de05eb2a44d1",
          "name": "Zhe Zheng",
          "hidden": false
        },
        {
          "_id": "683fb063ef97de05eb2a44d2",
          "name": "Yongliang Shen",
          "hidden": false
        },
        {
          "_id": "683fb063ef97de05eb2a44d3",
          "name": "Jialu Du",
          "hidden": false
        },
        {
          "_id": "683fb063ef97de05eb2a44d4",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "683fb063ef97de05eb2a44d5",
          "name": "Yongbin Li",
          "hidden": false
        },
        {
          "_id": "683fb063ef97de05eb2a44d6",
          "name": "Weiming Lu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-30T12:01:06.000Z",
      "submittedOnDailyAt": "2025-06-05T00:45:50.911Z",
      "title": "Reconnaissance d'information temporelle avec apprentissage par renforcement hyper-corona : application de l'apprentissage par renforcement hyper-corona pour la reconnaissance de séquences temporelles",
      "submittedOnDailyBy": {
        "_id": "67c03110e8c7d56a8e135ac8",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/eP3y_8_tyB8tcrT7py4L7.png",
        "isPro": false,
        "fullname": "Hou",
        "user": "Guiyang1001",
        "type": "user"
      },
      "summary": "Récemment, les modèles de langage grands (LLMs) ont réalisé des progrès notables dans des domaines comme la mathématique ou le code, où un pensée soigneuse est requise. Cependant, le développement cognitif des LLMs à partir d'un point de vue social, surtout après un entraînement, n'a pas encore été suffisamment étudié.\n\nLe monde social comporte d'autres dimensions temporelles et nécessite une combinaison riche de réactions intuitionnelles (Système 1) et de pensées superficielles avec des pensées profondes (Système 2), ce qui est plus clairement nécessaire que la mathématique. La mathématique dépend principalement du cognitif du Système 2 (pensée soigneuse, explicative en étapes), ce qui nous permet de comprendre l'importance de cette combinaison cognitive. En réponse à cette problématique, nous introduisons un entraînement cognitif lié au temps pour améliorer l'intelligence sociale (TimeHC-RL).\n\nDans les expériences, nous évaluons le rendement du TimeHC-RL pour améliorer l'intelligence social des LLMs en utilisant 8 ensembles de données, 5 paradigmes d'entraînement et 2 paradigmes d'interaction de test. Les résultats des expériences montrent clairement que le méthode TimeHC-RL fournit un rendement supérieur par rapport à l'apprentissage par répétition de Système 2. Ce méthode permet de comparer le rendement de modèles avancés tels que DeepSeek-R1 et OpenAI-O3, et offre également une exploration systématique qui fournit de nombreux feedbacks précieux pour améliorer l'intelligence sociale.",
      "upvotes": 10,
      "discussionId": "683fb064ef97de05eb2a452b",
      "ai_summary": "Temporal-aware Hierarchical Cognitive Reinforcement Learning enhances LLMs' social intelligence by addressing the distinct cognitive demands of social domains.",
      "ai_keywords": [
        "Large Language Models",
        "Temporal-aware Hierarchical Cognitive Reinforcement Learning",
        "TimeHC-RL",
        "System 1",
        "System 2",
        "RL",
        "DeepSeek-R1",
        "OpenAI-O3"
      ]
    },
    "publishedAt": "2025-05-30T08:01:06.000Z",
    "title": "TimeHC-RL: Temporal-aware Hierarchical Cognitive Reinforcement Learning\n  for Enhancing LLMs' Social Intelligence",
    "summary": "Recently, Large Language Models (LLMs) have made significant progress in\nIQ-related domains that require careful thinking, such as mathematics and\ncoding. However, enhancing LLMs' cognitive development in social domains,\nparticularly from a post-training perspective, remains underexplored.\nRecognizing that the social world follows a distinct timeline and requires a\nricher blend of cognitive modes (from intuitive reactions (System 1) and\nsurface-level thinking to deliberate thinking (System 2)) than mathematics,\nwhich primarily relies on System 2 cognition (careful, step-by-step reasoning),\nwe introduce Temporal-aware Hierarchical Cognitive Reinforcement Learning\n(TimeHC-RL) for enhancing LLMs' social intelligence. In our experiments, we\nsystematically explore improving LLMs' social intelligence and validate the\neffectiveness of the TimeHC-RL method, through five other post-training\nparadigms and two test-time intervention paradigms on eight datasets with\ndiverse data patterns. Experimental results reveal the superiority of our\nproposed TimeHC-RL method compared to the widely adopted System 2 RL method. It\ngives the 7B backbone model wings, enabling it to rival the performance of\nadvanced models like DeepSeek-R1 and OpenAI-O3. Additionally, the systematic\nexploration from post-training and test-time interventions perspectives to\nimprove LLMs' social intelligence has uncovered several valuable insights.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24500.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67c03110e8c7d56a8e135ac8",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/eP3y_8_tyB8tcrT7py4L7.png",
      "fullname": "Hou",
      "name": "Guiyang1001",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.04158",
      "authors": [
        {
          "_id": "6840fb71d4e16ff5f95108aa",
          "name": "Yujia Hu",
          "hidden": false
        },
        {
          "_id": "6840fb71d4e16ff5f95108ab",
          "name": "Songhua Liu",
          "hidden": false
        },
        {
          "_id": "6840fb71d4e16ff5f95108ac",
          "name": "Zhenxiong Tan",
          "hidden": false
        },
        {
          "_id": "6840fb71d4e16ff5f95108ad",
          "user": {
            "_id": "634cfebc350bcee9bed20a4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/634cfebc350bcee9bed20a4d/fN47nN5rhw-HJaFLBZWQy.png",
            "isPro": false,
            "fullname": "Xingyi Yang",
            "user": "adamdad",
            "type": "user"
          },
          "name": "Xingyi Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:27:11.256Z",
          "hidden": false
        },
        {
          "_id": "6840fb71d4e16ff5f95108ae",
          "name": "Xinchao Wang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/634cfebc350bcee9bed20a4d/tp0cLz8OZhdI3vs50cxhF.jpeg"
      ],
      "publishedAt": "2025-06-04T16:57:24.000Z",
      "submittedOnDailyAt": "2025-06-05T00:38:20.484Z",
      "title": "L'Imagen editée est un programme qui utilise des modèles de diffusion.",
      "submittedOnDailyBy": {
        "_id": "634cfebc350bcee9bed20a4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/634cfebc350bcee9bed20a4d/fN47nN5rhw-HJaFLBZWQy.png",
        "isPro": false,
        "fullname": "Xingyi Yang",
        "user": "adamdad",
        "type": "user"
      },
      "summary": "Les modèles de diffusion ont connu un succès impressionnant dans la génération d'images à partir de texte, mais ils ont rencontré des problèmes considérables dans l'édition d'images à partir de commandes. Dans cette étude, des problèmes importants ont été découverts : ces modèles souffrent particulièrement dans les éditions structurellement désaccordées. Pour résoudre ce problème, un cadre intégré pour l'édition d'images basé sur l'architecture de Transformer de Diffusion (DiT), appelé Image Editing As Programs (IEAP), a été utilisé. Le cœur de l'IEAP est de traiter l'édition à partir de commandes d'une manière simplifiée et de décomposer des instructions d'édition complexes en une séquence d'opérations de base. Chaque opération partage le même backend de DiT et se spécialise dans une forme d'édition. Les agents basés sur des modèles de vision et de langage (VLM) sont programmés pour soutenir des transformations arbitraires et structurellement désaccordées. En modularisant et en traitant les éditions de manière séquentielle, l'IEAP s'adapte largement aux tâches d'édition, allant de petits ajustements à des changements structurels importants. Les expériences prolongées montrent que l'IEAP dépasse significativement les méthodes les plus avancées sur les benchmarks standards. Dans ces évaluations, notre cadre de travail offre une haute précision et fidélité sémantique, surtout pour des instructions complexes et par étapes. Le code est disponible sur https://github.com/YujiaHu1109/IEAP.",
      "upvotes": 8,
      "discussionId": "6840fb73d4e16ff5f9510950",
      "projectPage": "https://yujiahu1109.github.io/IEAP/",
      "githubRepo": "https://github.com/YujiaHu1109/IEAP",
      "ai_summary": "A unified image editing framework, IEAP, built on Diffusion Transformer (DiT) decomposes complex editing instructions into operations performed by vision-language models for robust editing across various tasks.",
      "ai_keywords": [
        "diffusion models",
        "text-to-image generation",
        "instruction-driven image editing",
        "structurally inconsistent edits",
        "Image Editing As Programs (IEAP)",
        "Diffusion Transformer (DiT)",
        "atomic operations",
        "lightweight adapter",
        "vision-language model (VLM)",
        "modularizing edits"
      ]
    },
    "publishedAt": "2025-06-04T12:57:24.000Z",
    "title": "Image Editing As Programs with Diffusion Models",
    "summary": "While diffusion models have achieved remarkable success in text-to-image\ngeneration, they encounter significant challenges with instruction-driven image\nediting. Our research highlights a key challenge: these models particularly\nstruggle with structurally inconsistent edits that involve substantial layout\nchanges. To mitigate this gap, we introduce Image Editing As Programs (IEAP), a\nunified image editing framework built upon the Diffusion Transformer (DiT)\narchitecture. At its core, IEAP approaches instructional editing through a\nreductionist lens, decomposing complex editing instructions into sequences of\natomic operations. Each operation is implemented via a lightweight adapter\nsharing the same DiT backbone and is specialized for a specific type of edit.\nProgrammed by a vision-language model (VLM)-based agent, these operations\ncollaboratively support arbitrary and structurally inconsistent\ntransformations. By modularizing and sequencing edits in this way, IEAP\ngeneralizes robustly across a wide range of editing tasks, from simple\nadjustments to substantial structural changes. Extensive experiments\ndemonstrate that IEAP significantly outperforms state-of-the-art methods on\nstandard benchmarks across various editing scenarios. In these evaluations, our\nframework delivers superior accuracy and semantic fidelity, particularly for\ncomplex, multi-step instructions. Codes are available at\nhttps://github.com/YujiaHu1109/IEAP.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/634cfebc350bcee9bed20a4d/tp0cLz8OZhdI3vs50cxhF.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04158.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "634cfebc350bcee9bed20a4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/634cfebc350bcee9bed20a4d/fN47nN5rhw-HJaFLBZWQy.png",
      "fullname": "Xingyi Yang",
      "name": "adamdad",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 17
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.04141",
      "authors": [
        {
          "_id": "684106fc8cb0edba3ab212bb",
          "name": "Kejian Zhu",
          "hidden": false
        },
        {
          "_id": "684106fc8cb0edba3ab212bc",
          "name": "Zhuoran Jin",
          "hidden": false
        },
        {
          "_id": "684106fc8cb0edba3ab212bd",
          "name": "Hongbang Yuan",
          "hidden": false
        },
        {
          "_id": "684106fc8cb0edba3ab212be",
          "name": "Jiachun Li",
          "hidden": false
        },
        {
          "_id": "684106fc8cb0edba3ab212bf",
          "name": "Shangqing Tu",
          "hidden": false
        },
        {
          "_id": "684106fc8cb0edba3ab212c0",
          "name": "Pengfei Cao",
          "hidden": false
        },
        {
          "_id": "684106fc8cb0edba3ab212c1",
          "name": "Yubo Chen",
          "hidden": false
        },
        {
          "_id": "684106fc8cb0edba3ab212c2",
          "name": "Kang Liu",
          "hidden": false
        },
        {
          "_id": "684106fc8cb0edba3ab212c3",
          "name": "Jun Zhao",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/648c48d8c0ddeee6df5b6d22/_-WD0IU9jQMInqhXgGfUP.jpeg"
      ],
      "publishedAt": "2025-06-04T16:33:41.000Z",
      "submittedOnDailyAt": "2025-06-05T04:38:26.132Z",
      "title": "MMR-V : Qu'est-ce qui n'est pas dit et ce qui reste ? Point de référence logique profond de la structure multidimensionnelle dans le vidéo",
      "submittedOnDailyBy": {
        "_id": "648c48d8c0ddeee6df5b6d22",
        "avatarUrl": "/avatars/8706b0b16dfc332b96c91d3ced31bd0b.svg",
        "isPro": false,
        "fullname": "Shangqing Tu",
        "user": "tsq2000",
        "type": "user"
      },
      "summary": "La structure séquentielle du vidéo défie les modèles de langage multimodal (MLLM) dans leur capacité à identifier et à exécuter des logiques de multiples exemples dans différents cadres de référence. Cependant, les évaluations actuelles se concentrent principalement sur des tâches de compréhension, exigeant que les modèles reconnaissent seulement les cadres proches du cadre de référence mentionné dans la tâche (par la suite, \"cadre de référence de la tâche\"). Pour aborder ces limitations, nous proposons le MMR-V (Benchmark de logique profonde de multiples exemples dans les vidéos), qui présente les caractéristiques suivantes : (1) logique de multiples exemples à longue distance : les modèles doivent inférer et analyser des cadres d'évidence qui soient éloignés du cadre de référence de la tâche. (2) au-delà de la reconnaissance : la tâche ne peut pas être résolue directement par reconnaissance, mais nécessite de traiter des informations logiques cachées. (3) fiabilité : toutes les tâches sont effectuées via des annotations directes, et sont basées sur la compréhension d'un large public de utilisateurs réels pour s'adapter à la compréhension générale. (4) confusion : des stratégies d'annotation de détecteurs soigneusement conçues ont été utilisées pour réduire les faiblesses du modèle. Le MMR-V comprend 317 vidéos et 1,257 tâches. Nos expériences montrent que les modèles actuels font face à des difficultés avec la logique de multiples exemples, et le meilleur modèle, o4-mini, atteint une précision de 52.5%. De plus, les stratégies de développement logique actuelles (CoT et amplification de calcul en temps de exécution) ont montré des effets limités. À travers l'évolution de l'analyse, il a été découvert que le CoT diffère de la logique de la littérature, ce qui explique une partie des limitations de performance. Nous cherchons que le MMR-V encourage le développement des habiletés de logique de multiples exemples.",
      "upvotes": 7,
      "discussionId": "684106ff8cb0edba3ab21374",
      "ai_summary": "A new benchmark, MMR-V, is proposed to challenge multimodal large language models with long-range, multi-frame reasoning and hidden information processing in videos, revealing their limitations and inspiring further research.",
      "ai_keywords": [
        "multimodal large language models",
        "MMR-V",
        "long-range",
        "multi-frame reasoning",
        "multimodal reasoning",
        "manual annotation",
        "distractor annotation strategy",
        "Chain-of-Thought"
      ]
    },
    "publishedAt": "2025-06-04T12:33:41.000Z",
    "title": "MMR-V: What's Left Unsaid? A Benchmark for Multimodal Deep Reasoning in\n  Videos",
    "summary": "The sequential structure of videos poses a challenge to the ability of\nmultimodal large language models (MLLMs) to locate multi-frame evidence and\nconduct multimodal reasoning. However, existing video benchmarks mainly focus\non understanding tasks, which only require models to match frames mentioned in\nthe question (hereafter referred to as \"question frame\") and perceive a few\nadjacent frames. To address this gap, we propose MMR-V: A Benchmark for\nMultimodal Deep Reasoning in Videos. The benchmark is characterized by the\nfollowing features. (1) Long-range, multi-frame reasoning: Models are required\nto infer and analyze evidence frames that may be far from the question frame.\n(2) Beyond perception: Questions cannot be answered through direct perception\nalone but require reasoning over hidden information. (3) Reliability: All tasks\nare manually annotated, referencing extensive real-world user understanding to\nalign with common perceptions. (4) Confusability: Carefully designed distractor\nannotation strategies to reduce model shortcuts. MMR-V consists of 317 videos\nand 1,257 tasks. Our experiments reveal that current models still struggle with\nmulti-modal reasoning; even the best-performing model, o4-mini, achieves only\n52.5% accuracy. Additionally, current reasoning enhancement strategies\n(Chain-of-Thought and scaling test-time compute) bring limited gains. Further\nanalysis indicates that the CoT demanded for multi-modal reasoning differs from\nit in textual reasoning, which partly explains the limited performance gains.\nWe hope that MMR-V can inspire further research into enhancing multi-modal\nreasoning capabilities.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/648c48d8c0ddeee6df5b6d22/_-WD0IU9jQMInqhXgGfUP.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04141.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "648c48d8c0ddeee6df5b6d22",
      "avatarUrl": "/avatars/8706b0b16dfc332b96c91d3ced31bd0b.svg",
      "fullname": "Shangqing Tu",
      "name": "tsq2000",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.03930",
      "authors": [
        {
          "_id": "6841090145662bb7d322ecc6",
          "user": {
            "_id": "64de37ee5e192985054be575",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64de37ee5e192985054be575/fVV7JQMtp_J3uFqszJJHH.jpeg",
            "isPro": false,
            "fullname": "Yuansheng Ni",
            "user": "yuanshengni",
            "type": "user"
          },
          "name": "Yuansheng Ni",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T10:00:04.196Z",
          "hidden": false
        },
        {
          "_id": "6841090145662bb7d322ecc7",
          "name": "Ping Nie",
          "hidden": false
        },
        {
          "_id": "6841090145662bb7d322ecc8",
          "name": "Kai Zou",
          "hidden": false
        },
        {
          "_id": "6841090145662bb7d322ecc9",
          "name": "Xiang Yue",
          "hidden": false
        },
        {
          "_id": "6841090145662bb7d322ecca",
          "name": "Wenhu Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-04T13:24:44.000Z",
      "submittedOnDailyAt": "2025-06-05T05:54:39.464Z",
      "title": "VisCoder : Génération de code de visualisation Python exécutable à travers un LLM",
      "submittedOnDailyBy": {
        "_id": "64de37ee5e192985054be575",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64de37ee5e192985054be575/fVV7JQMtp_J3uFqszJJHH.jpeg",
        "isPro": false,
        "fullname": "Yuansheng Ni",
        "user": "yuanshengni",
        "type": "user"
      },
      "summary": "Les modèles de langage grand (LLMs) nécessitent de considérer à la fois la précision du code et le sens visuel dans les tâches de visualisation. Les ensembles de données de fine-tuning actuels basés sur l'exécution manquent de support et sont limités aux corrections de code répétitives, ce qui peut conduire à la génération de graphiques insatisfaisants et incertains. Nous présentons \"VisCode-200K\", un ensemble de données de fine-tuning d'instructions basé sur Python pour la visualisation et la correction automatique. Ce ensemble comprend plus de 200K exemples et comporte les aspects suivants : 1) code de visualisation vérifié dans des dépôts de code ouverts, combiné avec des instructions de langage naturel et des graphiques visualisés, et 2) 45K dialogues de correction de code à plusieurs étapes, filtrés par rétroaction du modèle lors de l'exécution du code. Nous avons fine-tuné Qwen2.5-Coder-Instruct avec VisCode-200K pour créer \"VisCoder\". VisCoder dépasse significativement les normes du code ouvert et ressemble au rendement des modèles comme GPT-4o-mini. De plus, nous introduisons un protocole d'évaluation de corrections itératives pour montrer la bêta de l'apprentissage dirigé par rétroaction dans la génération de code visuellement précis et exécutable.",
      "upvotes": 7,
      "discussionId": "6841090245662bb7d322ed1f",
      "projectPage": "https://tiger-ai-lab.github.io/VisCoder/",
      "githubRepo": "https://github.com/TIGER-AI-Lab/VisCoder",
      "ai_summary": "VisCode-200K, a large-scale dataset for visualization, improves plot generation performance by integrating execution-grounded supervision and iterative code correction, outperforming open-source models and rivaling proprietary ones.",
      "ai_keywords": [
        "large language models",
        "LLMs",
        "visualization tasks",
        "plot generation",
        "execution-grounded supervision",
        "iterative code correction",
        "VisCode-200K",
        "Python-based visualization",
        "validated plotting code",
        "natural language instructions",
        "rendered plots",
        "correction dialogues",
        "Qwen2.5-Coder-Instruct",
        "VisCoder",
        "PandasPlotBench",
        "self-debug evaluation",
        "feedback-driven learning"
      ]
    },
    "publishedAt": "2025-06-04T09:24:44.000Z",
    "title": "VisCoder: Fine-Tuning LLMs for Executable Python Visualization Code\n  Generation",
    "summary": "Large language models (LLMs) often struggle with visualization tasks like\nplotting diagrams, charts, where success depends on both code correctness and\nvisual semantics. Existing instruction-tuning datasets lack execution-grounded\nsupervision and offer limited support for iterative code correction, resulting\nin fragile and unreliable plot generation. We present VisCode-200K, a\nlarge-scale instruction tuning dataset for Python-based visualization and\nself-correction. It contains over 200K examples from two sources: (1) validated\nplotting code from open-source repositories, paired with natural language\ninstructions and rendered plots; and (2) 45K multi-turn correction dialogues\nfrom Code-Feedback, enabling models to revise faulty code using runtime\nfeedback. We fine-tune Qwen2.5-Coder-Instruct on VisCode-200K to create\nVisCoder, and evaluate it on PandasPlotBench. VisCoder significantly\noutperforms strong open-source baselines and approaches the performance of\nproprietary models like GPT-4o-mini. We further adopt a self-debug evaluation\nprotocol to assess iterative repair, demonstrating the benefits of\nfeedback-driven learning for executable, visually accurate code generation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03930.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64de37ee5e192985054be575",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64de37ee5e192985054be575/fVV7JQMtp_J3uFqszJJHH.jpeg",
      "fullname": "Yuansheng Ni",
      "name": "yuanshengni",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 13
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.03517",
      "authors": [
        {
          "_id": "68412f853c22997c7329f3a0",
          "user": {
            "_id": "62980664ff0acd7e027d6686",
            "avatarUrl": "/avatars/364d4c8432c24775a099641fc576dbdc.svg",
            "isPro": false,
            "fullname": "Ziyi Wu",
            "user": "Dazitu616",
            "type": "user"
          },
          "name": "Ziyi Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:26:43.314Z",
          "hidden": false
        },
        {
          "_id": "68412f853c22997c7329f3a1",
          "name": "Anil Kag",
          "hidden": false
        },
        {
          "_id": "68412f853c22997c7329f3a2",
          "name": "Ivan Skorokhodov",
          "hidden": false
        },
        {
          "_id": "68412f853c22997c7329f3a3",
          "name": "Willi Menapace",
          "hidden": false
        },
        {
          "_id": "68412f853c22997c7329f3a4",
          "name": "Ashkan Mirzaei",
          "hidden": false
        },
        {
          "_id": "68412f853c22997c7329f3a5",
          "name": "Igor Gilitschenski",
          "hidden": false
        },
        {
          "_id": "68412f853c22997c7329f3a6",
          "name": "Sergey Tulyakov",
          "hidden": false
        },
        {
          "_id": "68412f853c22997c7329f3a7",
          "name": "Aliaksandr Siarohin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-04T03:06:08.000Z",
      "submittedOnDailyAt": "2025-06-05T04:18:57.242Z",
      "title": "DenseDPO : Optimisation des préférences temporelles minimales dans les vidéos",
      "submittedOnDailyBy": {
        "_id": "62980664ff0acd7e027d6686",
        "avatarUrl": "/avatars/364d4c8432c24775a099641fc576dbdc.svg",
        "isPro": false,
        "fullname": "Ziyi Wu",
        "user": "Dazitu616",
        "type": "user"
      },
      "summary": "La Optimisation des Préférences Directes (Direct Preference Optimization, DPO) est une technique récente pour l'entraînement de modèles de diffusion de films à partir de texte. Pour obtenir des données d'entraînement, on demande de fournir des préférences de films générés par deux bruits indépendants. Cependant, cette approche interdit des comparaisons détaillées et introduit un biais pour les petits clips. Dans cette étude, on introduit le méthode DenseDPO, qui offre trois contributions pour résoudre ces problèmes. Premièrement, dans le DPO, on crée des paires de vidéos en utilisant des bruits réels de films, ce qui permet de générer des paires alignées avec des structures similaires mais des détails locaux différents, neutralisant le biais du mouvement. Ensuite, on utilise ces alignements temporels pour étiquetter des préférences dans des segments courts, obtenant des signaux d'apprentissage plus denses et précis. Avec seulement un tiers des données étiquetées, DenseDPO améliore significativement la génération de mouvements, tout en maintenant le rendement en alignement de texte, qualité visuelle et cohérence temporelle similaire au DPO. Enfin, DenseDPO montre la génération automatique d'étiquettes de préférence en utilisant des modèles de langage visuel et de langage (VLMs) : GPT prédit des niveaux de préférence similaires à ceux des modèles de récompense de films entraînés, ce qui permet à DenseDPO d'atteindre un rendement similaire lorsque des étiquettes humaines sont utilisées.",
      "upvotes": 7,
      "discussionId": "68412f8a3c22997c7329f4ff",
      "projectPage": "https://snap-research.github.io/DenseDPO/"
    },
    "publishedAt": "2025-06-03T23:06:08.000Z",
    "title": "DenseDPO: Fine-Grained Temporal Preference Optimization for Video\n  Diffusion Models",
    "summary": "Direct Preference Optimization (DPO) has recently been applied as a\npost-training technique for text-to-video diffusion models. To obtain training\ndata, annotators are asked to provide preferences between two videos generated\nfrom independent noise. However, this approach prohibits fine-grained\ncomparisons, and we point out that it biases the annotators towards low-motion\nclips as they often contain fewer visual artifacts. In this work, we introduce\nDenseDPO, a method that addresses these shortcomings by making three\ncontributions. First, we create each video pair for DPO by denoising corrupted\ncopies of a ground truth video. This results in aligned pairs with similar\nmotion structures while differing in local details, effectively neutralizing\nthe motion bias. Second, we leverage the resulting temporal alignment to label\npreferences on short segments rather than entire clips, yielding a denser and\nmore precise learning signal. With only one-third of the labeled data, DenseDPO\ngreatly improves motion generation over vanilla DPO, while matching it in text\nalignment, visual quality, and temporal consistency. Finally, we show that\nDenseDPO unlocks automatic preference annotation using off-the-shelf Vision\nLanguage Models (VLMs): GPT accurately predicts segment-level preferences\nsimilar to task-specifically fine-tuned video reward models, and DenseDPO\ntrained on these labels achieves performance close to using human labels.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03517.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62980664ff0acd7e027d6686",
      "avatarUrl": "/avatars/364d4c8432c24775a099641fc576dbdc.svg",
      "fullname": "Ziyi Wu",
      "name": "Dazitu616",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.04142",
      "authors": [
        {
          "_id": "684132cb725b7fb67f68ffb8",
          "name": "Kejian Zhu",
          "hidden": false
        },
        {
          "_id": "684132cb725b7fb67f68ffb9",
          "name": "Shangqing Tu",
          "hidden": false
        },
        {
          "_id": "684132cb725b7fb67f68ffba",
          "name": "Zhuoran Jin",
          "hidden": false
        },
        {
          "_id": "684132cb725b7fb67f68ffbb",
          "name": "Lei Hou",
          "hidden": false
        },
        {
          "_id": "684132cb725b7fb67f68ffbc",
          "name": "Juanzi Li",
          "hidden": false
        },
        {
          "_id": "684132cb725b7fb67f68ffbd",
          "name": "Jun Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-04T16:33:44.000Z",
      "submittedOnDailyAt": "2025-06-05T04:32:19.273Z",
      "title": "Analyse des Neurones Coupés pour Évaluer avec Confiance un LLM",
      "submittedOnDailyBy": {
        "_id": "648c48d8c0ddeee6df5b6d22",
        "avatarUrl": "/avatars/8706b0b16dfc332b96c91d3ced31bd0b.svg",
        "isPro": false,
        "fullname": "Shangqing Tu",
        "user": "tsq2000",
        "type": "user"
      },
      "summary": "L'évaluation de la confiance dans le développement de modèles de langage grands (LLMs) est cruciale, mais actuellement, cette évaluation dépend principalement de cadres d'évaluation publiés, ce qui affecte significativement l'équité en raison des problèmes de contemporanité des données. Dans les études précédentes, l'accent a été mis sur la construction de cadres d'évaluation dynamiques pour résoudre ce problème, mais la construction continue de nouveaux cadres est coûteuse et cyclique. Dans cet article, nous analysons la structure des modèles contemporains pour aborder le problème de contemporanité. Dans les expérimentations, nous avons démontré que la surévaluation des modèles contemporains est due à ce que les paramètres ont obtenu des solutions de court chemin lors de l'entraînement. De plus, nous avons fourni un analyse relative et causal pour identifier et restreindre les neurones de court chemin en utilisant un nouveau méthode. Cela a permis d'introduire une évaluation qui supprime les neurones de court chemin, connue comme \"patch de neurones de court chemin\". Les expérimentations ont montré que cette approche mitigue le problème de contemporanité. De plus, l'évaluation conjointement avec les cadres d'évaluation publiés récemment comme MixEval a montré une forte corrélation linéaire, avec un coefficient de Shurman (rho) supérieur à 0,95. Cette forte corrélation indique que cette approche montre la véritable capacité des modèles et des évaluations fiables. De plus, nous avons démontré la généralisation de cette approche par des expérimentations supplémentaires dans différents cadres d'évaluation et des configurations de paramètres. Code : https://github.com/GaryStack/Trustworthy-Evaluation",
      "upvotes": 6,
      "discussionId": "684132cc725b7fb67f68fff5",
      "githubRepo": "https://github.com/GaryStack/Trustworthy-Evaluation",
      "ai_summary": "A method called shortcut neuron patching identifies and suppresses shortcut neurons in language models to mitigate data contamination issues in trustworthy evaluations.",
      "ai_keywords": [
        "large language models",
        "LLMs",
        "trustworthy evaluation",
        "data contamination",
        "benchmarks",
        "dynamic benchmarks",
        "shortcut solutions",
        "shortcut neurons",
        "comparative analysis",
        "causal analysis",
        "shortcut neuron patching",
        "MixEval",
        "Spearman coefficient"
      ]
    },
    "publishedAt": "2025-06-04T12:33:44.000Z",
    "title": "Establishing Trustworthy LLM Evaluation via Shortcut Neuron Analysis",
    "summary": "The development of large language models (LLMs) depends on trustworthy\nevaluation. However, most current evaluations rely on public benchmarks, which\nare prone to data contamination issues that significantly compromise fairness.\nPrevious researches have focused on constructing dynamic benchmarks to address\ncontamination. However, continuously building new benchmarks is costly and\ncyclical. In this work, we aim to tackle contamination by analyzing the\nmechanisms of contaminated models themselves. Through our experiments, we\ndiscover that the overestimation of contaminated models is likely due to\nparameters acquiring shortcut solutions in training. We further propose a novel\nmethod for identifying shortcut neurons through comparative and causal\nanalysis. Building on this, we introduce an evaluation method called shortcut\nneuron patching to suppress shortcut neurons. Experiments validate the\neffectiveness of our approach in mitigating contamination. Additionally, our\nevaluation results exhibit a strong linear correlation with MixEval, a recently\nreleased trustworthy benchmark, achieving a Spearman coefficient (rho)\nexceeding 0.95. This high correlation indicates that our method closely reveals\ntrue capabilities of the models and is trustworthy. We conduct further\nexperiments to demonstrate the generalizability of our method across various\nbenchmarks and hyperparameter settings. Code:\nhttps://github.com/GaryStack/Trustworthy-Evaluation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04142.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "648c48d8c0ddeee6df5b6d22",
      "avatarUrl": "/avatars/8706b0b16dfc332b96c91d3ced31bd0b.svg",
      "fullname": "Shangqing Tu",
      "name": "tsq2000",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.04108",
      "authors": [
        {
          "_id": "684104a16b106ae42f5acc1a",
          "name": "Yutao Sun",
          "hidden": false
        },
        {
          "_id": "684104a16b106ae42f5acc1b",
          "name": "Tianzhu Ye",
          "hidden": false
        },
        {
          "_id": "684104a16b106ae42f5acc1c",
          "name": "Li Dong",
          "hidden": false
        },
        {
          "_id": "684104a16b106ae42f5acc1d",
          "name": "Yuqing Xia",
          "hidden": false
        },
        {
          "_id": "684104a16b106ae42f5acc1e",
          "name": "Jian Chen",
          "hidden": false
        },
        {
          "_id": "684104a16b106ae42f5acc1f",
          "name": "Yizhao Gao",
          "hidden": false
        },
        {
          "_id": "684104a16b106ae42f5acc20",
          "name": "Shijie Cao",
          "hidden": false
        },
        {
          "_id": "684104a16b106ae42f5acc21",
          "name": "Jianyong Wang",
          "hidden": false
        },
        {
          "_id": "684104a16b106ae42f5acc22",
          "name": "Furu Wei",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-04T16:01:48.000Z",
      "submittedOnDailyAt": "2025-06-05T01:16:28.444Z",
      "title": "Rectified Sparse Attention",
      "submittedOnDailyBy": {
        "_id": "6300ef4779c5ddbc6cf83e1a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1661005591657-noauth.jpeg",
        "isPro": false,
        "fullname": "Yutao Sun",
        "user": "sunyt32",
        "type": "user"
      },
      "summary": "La génération efficace de têtes de longues phrases est un problème important dans les modèles de langage de grande taille. Les méthodes de décodage rares sont capables de augmenter l'efficacité, mais leur asymétrie dans le cache de KV et l'accumulation d'erreurs approximatives peuvent diminuer la qualité de la génération. Dans ce travail, nous proposons une méthodologie simple et efficace appelée Attention avec Attention Récupérée Sparse (ReSA), qui combine l'attention sparse de blocs avec un ajustement dense périodique. ReSA utilise des étapes denses à intervalles fixes pour mettre à jour le cache de KV, ce qui limite l'accumulation d'erreurs et maintient la correspondance avec la distribution d'entraînement précédente. A travers des expériences mathématiques, de modélisation de langage et de tâches de recherche, ReSA a réussi à générer de manière approximative sans perte et a amélioré significativement l'efficacité. En particulier, une vitesse de 2,42 fois plus rapide a été atteinte dans la décodage de phrases de 256K mots, ce qui peut être une solution pratique pour l'inférence de contextes longs dans des tâches de recherche. Le code est disponible sur https://aka.ms/ReSA-LM.",
      "upvotes": 6,
      "discussionId": "684104a26b106ae42f5acc50",
      "ai_summary": "Rectified Sparse Attention (ReSA) improves the efficiency of long-sequence generation in Large Language Models by combining block-sparse attention with periodic dense rectification, maintaining high-quality generation.",
      "ai_keywords": [
        "sparse decoding",
        "KV cache misalignment",
        "Rectified Sparse Attention",
        "ReSA",
        "block-sparse attention",
        "dense rectification",
        "pretraining distribution",
        "long-context inference"
      ]
    },
    "publishedAt": "2025-06-04T12:01:48.000Z",
    "title": "Rectified Sparse Attention",
    "summary": "Efficient long-sequence generation is a critical challenge for Large Language\nModels. While recent sparse decoding methods improve efficiency, they suffer\nfrom KV cache misalignment, where approximation errors accumulate and degrade\ngeneration quality. In this work, we propose Rectified Sparse Attention (ReSA),\na simple yet effective method that combines block-sparse attention with\nperiodic dense rectification. By refreshing the KV cache at fixed intervals\nusing a dense forward pass, ReSA bounds error accumulation and preserves\nalignment with the pretraining distribution. Experiments across math reasoning,\nlanguage modeling, and retrieval tasks demonstrate that ReSA achieves\nnear-lossless generation quality with significantly improved efficiency.\nNotably, ReSA delivers up to 2.42times end-to-end speedup under decoding at\n256K sequence length, making it a practical solution for scalable long-context\ninference. Code is available at https://aka.ms/ReSA-LM.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04108.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6300ef4779c5ddbc6cf83e1a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1661005591657-noauth.jpeg",
      "fullname": "Yutao Sun",
      "name": "sunyt32",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.03099",
      "authors": [
        {
          "_id": "684107c6142b5c0b4226025f",
          "name": "Chetwin Low",
          "hidden": false
        },
        {
          "_id": "684107c6142b5c0b42260260",
          "name": "Weimin Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T17:29:28.000Z",
      "submittedOnDailyAt": "2025-06-05T01:30:00.782Z",
      "title": "TalkingMachines: Implémenter un modèle de classification de vidéos de style Pitch-style voix-guidé par heure",
      "submittedOnDailyBy": {
        "_id": "62b43ffec624a43b1a1ada46",
        "avatarUrl": "/avatars/77298e99d2797cf917fdddc6d6de46eb.svg",
        "isPro": false,
        "fullname": "weimin wang ",
        "user": "weiminwang",
        "type": "user"
      },
      "summary": "Dans cet article, nous proposons le cadre efficace appelé TalkingMachines. Ce cadre transforme les modèles de génération de vidéo pré-entraînés en modèles d'animation de personnages animés par voix. TalkingMachines intègre le modèle de langage vocal-langage (LLM) et notre modèle de génération de vidéo pour permettre des expériences de conversation naturelle. Nos principales contributions sont les suivantes :\n\n(1) Nous appliquons DiT à la génération d'avatars animés par voix à partir d'images pré-entraînées de niveau optimal, créant un modèle contenant 1,8 milliards de paramètres.\n(2) Nous observons des expériences de connaissance non symétrique à partir de modèles symétriques de correction, évitant ainsi l'accumulation d'erreurs et permettant un flux vidéo infini.\n(3) Nous concevons une architecture haute exécution et basse latence pour l'exécution, utilisant les techniques de optimisation clés suivantes :\n(a) Nous séparons DiT et le décodificateur VAE sur des dispositifs séparés.\n(b) Nous utilisons des flux CUDA pour communiquer et calculer de manière efficace et parallèle entre dispositifs.\n(c) Nous réduisons les calculs répétés et maximisons le flux de transformation de la génération de frames.\n\nVous pouvez voir la démonstration vidéo ici - https://aaxwaz.github.io/TalkingMachines/",
      "upvotes": 6,
      "discussionId": "684107c8142b5c0b42260293",
      "projectPage": "https://aaxwaz.github.io/TalkingMachines/",
      "githubRepo": "https://github.com/aaxwaz/TalkingMachines",
      "ai_summary": "TalkingMachines transforms a pretrained image-to-video model into an audio-driven avatar generator, supports infinite video streaming, and uses engineering optimizations for real-time performance.",
      "ai_keywords": [
        "DiT",
        "audio large language model",
        "asymmetric knowledge distillation",
        "bidirectional teacher model",
        "sparse causal",
        "autoregressive student model",
        "inference pipeline",
        "CUDA streams",
        "frame-generation throughput"
      ]
    },
    "publishedAt": "2025-06-03T13:29:28.000Z",
    "title": "TalkingMachines: Real-Time Audio-Driven FaceTime-Style Video via\n  Autoregressive Diffusion Models",
    "summary": "In this paper, we present TalkingMachines -- an efficient framework that\ntransforms pretrained video generation models into real-time, audio-driven\ncharacter animators. TalkingMachines enables natural conversational experiences\nby integrating an audio large language model (LLM) with our video generation\nfoundation model. Our primary contributions include: (1) We adapt a pretrained\nSOTA image-to-video DiT into an audio-driven avatar generation model of 18\nbillion parameters; (2) We enable infinite video streaming without error\naccumulation through asymmetric knowledge distillation from a bidirectional\nteacher model into a sparse causal, autoregressive student model; (3) We design\na high-throughput, low-latency inference pipeline incorporating several key\nengineering optimizations such as: (a) disaggregation of the DiT and VAE\ndecoder across separate devices, (b) efficient overlap of inter-device\ncommunication and computation using CUDA streams, (c) elimination of redundant\nrecomputations to maximize frame-generation throughput. Please see demo videos\nhere - https://aaxwaz.github.io/TalkingMachines/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03099.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62b43ffec624a43b1a1ada46",
      "avatarUrl": "/avatars/77298e99d2797cf917fdddc6d6de46eb.svg",
      "fullname": "weimin wang ",
      "name": "weiminwang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.02592",
      "authors": [
        {
          "_id": "684104c89ec96d9991484c24",
          "user": {
            "_id": "65309a1d657ae56cdb65e0e7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/lHQI9RNjfz8E5v1uyCGeV.png",
            "isPro": false,
            "fullname": "Zhi-Yuan Chen",
            "user": "JaxChen",
            "type": "user"
          },
          "name": "Zhi-Yuan Chen",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-05T02:45:29.221Z",
          "hidden": false
        },
        {
          "_id": "684104c89ec96d9991484c25",
          "name": "Hao Wang",
          "hidden": false
        },
        {
          "_id": "684104c89ec96d9991484c26",
          "name": "Xinyu Zhang",
          "hidden": false
        },
        {
          "_id": "684104c89ec96d9991484c27",
          "name": "Enrui Hu",
          "hidden": false
        },
        {
          "_id": "684104c89ec96d9991484c28",
          "name": "Yankai Lin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T08:12:47.000Z",
      "submittedOnDailyAt": "2025-06-05T01:30:17.419Z",
      "title": "Plus sérieuse que la surface : La mesure de la préférence propre dans la décision d'un LLM",
      "submittedOnDailyBy": {
        "_id": "65309a1d657ae56cdb65e0e7",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/lHQI9RNjfz8E5v1uyCGeV.png",
        "isPro": false,
        "fullname": "Zhi-Yuan Chen",
        "user": "JaxChen",
        "type": "user"
      },
      "summary": "Selon des études récentes, les modèles de langage à grande échelle (LLMs) ont tendance à préférer leurs propres réponses lorsqu'elles sont évaluées par jury. Cela signifie que les modèles préfèrent leurs propres réponses par rapport à celles générées par d'autres modèles. Actuellement, les méthodes calculent généralement la différence entre les scores attribués par le modèle évaluateur aux réponses générées par le modèle et aux réponses générées par d'autres modèles pour mesurer cette préférence. Cependant, cette approche peut confondre la qualité de la réponse avec la préférence, car elle peut reconnaître des différences positives de scores même lorsque le modèle évaluateur n'a pas de préférence. Pour résoudre ce problème, nous utilisons la qualité de la réponse comme proxy avec le \"gold-standard\" et proposons le score DBG, qui mesure la différence entre le score attribué par le modèle évaluateur à la réponse du modèle et le score du gold-standard correspondant. Le gold-standard reflète la qualité réelle de la réponse, par conséquent, le score DBG peut réduire l'influence de la confusion entre la qualité de la réponse et la préférence. En utilisant le score DBG, nous évaluons la préférence des modèles pour leurs propres réponses dans différentes versions, tailles et capacités logiques. De plus, nous explorons deux facteurs qui peuvent affecter la préférence des réponses : le style de texte des réponses et les données d'entraînement supplémentaires du modèle évaluateur. Enfin, nous examinons la structure potentielle de la préférence des réponses à partir d'une perspective basée sur l'attention. Notre code et nos données sont disponibles sur https://github.com/zhiyuanc2001/self-preference.",
      "upvotes": 6,
      "discussionId": "684104c99ec96d9991484c5e",
      "githubRepo": "https://github.com/zhiyuanc2001/self-preference",
      "ai_summary": "The DBG score is introduced to measure self-preference bias in large language models by using gold judgments as proxies for response quality, addressing the confounding effect of response quality.",
      "ai_keywords": [
        "large language models",
        "self-preference bias",
        "judge model",
        "gold judgments",
        "DBG score",
        "response quality",
        "attention-based perspective"
      ]
    },
    "publishedAt": "2025-06-03T04:12:47.000Z",
    "title": "Beyond the Surface: Measuring Self-Preference in LLM Judgments",
    "summary": "Recent studies show that large language models (LLMs) exhibit self-preference\nbias when serving as judges, meaning they tend to favor their own responses\nover those generated by other models. Existing methods typically measure this\nbias by calculating the difference between the scores a judge model assigns to\nits own responses and those it assigns to responses from other models. However,\nthis approach conflates self-preference bias with response quality, as\nhigher-quality responses from the judge model may also lead to positive score\ndifferences, even in the absence of bias. To address this issue, we introduce\ngold judgments as proxies for the actual quality of responses and propose the\nDBG score, which measures self-preference bias as the difference between the\nscores assigned by the judge model to its own responses and the corresponding\ngold judgments. Since gold judgments reflect true response quality, the DBG\nscore mitigates the confounding effect of response quality on bias measurement.\nUsing the DBG score, we conduct comprehensive experiments to assess\nself-preference bias across LLMs of varying versions, sizes, and reasoning\nabilities. Additionally, we investigate two factors that influence and help\nalleviate self-preference bias: response text style and the post-training data\nof judge models. Finally, we explore potential underlying mechanisms of\nself-preference bias from an attention-based perspective. Our code and data are\navailable at https://github.com/zhiyuanc2001/self-preference.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.02592.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65309a1d657ae56cdb65e0e7",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/lHQI9RNjfz8E5v1uyCGeV.png",
      "fullname": "Zhi-Yuan Chen",
      "name": "JaxChen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.03106",
      "authors": [
        {
          "_id": "684104d9ee7646c073776b2e",
          "name": "Xiaoying Zhang",
          "hidden": false
        },
        {
          "_id": "684104d9ee7646c073776b2f",
          "name": "Hao Sun",
          "hidden": false
        },
        {
          "_id": "684104d9ee7646c073776b30",
          "user": {
            "_id": "666e91b1623133f1ce35acc5",
            "avatarUrl": "/avatars/cc78520e6cfb83817c1d0c1ac867ebdd.svg",
            "isPro": false,
            "fullname": "YipengZhang",
            "user": "YipengZhang",
            "type": "user"
          },
          "name": "Yipeng Zhang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-05T02:45:46.851Z",
          "hidden": false
        },
        {
          "_id": "684104d9ee7646c073776b31",
          "name": "Kaituo Feng",
          "hidden": false
        },
        {
          "_id": "684104d9ee7646c073776b32",
          "name": "Chaochao Lu",
          "hidden": false
        },
        {
          "_id": "684104d9ee7646c073776b33",
          "name": "Chao Yang",
          "hidden": false
        },
        {
          "_id": "684104d9ee7646c073776b34",
          "name": "Helen Meng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T17:39:02.000Z",
      "submittedOnDailyAt": "2025-06-05T01:16:53.836Z",
      "title": "Critique-GRPO : Développement de l'inférence logique dans les LM utilisant la rétroaction de langage et de nombres",
      "submittedOnDailyBy": {
        "_id": "67079840a9bcb7459b8d2a46",
        "avatarUrl": "/avatars/32466863c5554f20cb2775b138832ac3.svg",
        "isPro": false,
        "fullname": "Kaituo Feng",
        "user": "KaituoFeng",
        "type": "user"
      },
      "summary": "Le développement récent de l'apprentissage par renforcement (RL) en utilisant une rétroaction numérique (par exemple, scalaire) a considérablement amélioré les capacités logiques complexes des modèles de langage grands (LLMs). Malgré ce succès, nous avons identifié trois problèmes importants liés à l'utilisation exclusive de rétroaction numérique dans l'RL : la plateification du rendement, la limitation de la réflexion autonome et le manque à long terme. De plus, nous avons démontré que les modèles fine-tunés en RL peuvent générer un fine-tuning correct pour résoudre le problème de manque à long terme en utilisant une rétroaction de langage naturel (forme d'évaluation), même après que le rendement soit plateifié. Sur la base de cette perspective, nous proposons un cadre RL en ligne efficace pour l'optimisation de politiques qui intègre la rétroaction de langage naturel et numérique, appelé Critique-GRPO. Critique-GRPO permet aux LLMs d'apprendre en même temps à améliorer les réponses initiales et à être évaluées, tout en maintenant l'exploration. Avec Qwen2.5-7B-Base et Qwen3-8B-Base, et validés sur 8 tâches difficiles en mathématiques, STEM et logique générale, Critique-GRPO améliore en moyenne de 4,5% à 5% en termes de taux de passage @1, dépassant la ligne de base. En particulier, Critique-GRPO dépasse une ligne de base forte qui inclut des guides d'experts. Dans l'analyse supplémentaire, deux perspectives importantes sur l'exploration de politiques ont été clairement identifiées : (1) une haute entropie ne garantit pas toujours un apprentissage efficace en exploration, et (2) des réponses longues ne garantissent pas toujours une efficacité en exploration.",
      "upvotes": 5,
      "discussionId": "684104daee7646c073776b88",
      "githubRepo": "https://github.com/zhangxy-2019/critique-GRPO",
      "ai_summary": "Critique-GRPO, an RL framework combining numerical and natural language feedback, enhances LLM reasoning across tasks and outperforms existing methods.",
      "ai_keywords": [
        "reinforcement learning",
        "RL",
        "large language models",
        "LLMs",
        "scalar rewards",
        "performance plateaus",
        "self-reflection",
        "persistent failures",
        "natural language feedback",
        "critiques",
        "policy optimization",
        "Qwen2.5-7B-Base",
        "Qwen3-8B-Base",
        "pass@1",
        "policy exploration",
        "entropy",
        "response length"
      ]
    },
    "publishedAt": "2025-06-03T13:39:02.000Z",
    "title": "Critique-GRPO: Advancing LLM Reasoning with Natural Language and\n  Numerical Feedback",
    "summary": "Recent advances in reinforcement learning (RL) with numerical feedback, such\nas scalar rewards, have significantly enhanced the complex reasoning\ncapabilities of large language models (LLMs). Despite this success, we identify\nthree key challenges encountered by RL with solely numerical feedback:\nperformance plateaus, limited effectiveness of self-reflection, and persistent\nfailures. We then demonstrate that RL-finetuned models, even after exhibiting\nperformance plateaus, can generate correct refinements on persistently failed\nproblems by leveraging natural language feedback in the form of critiques.\nBuilding on this insight, we propose Critique-GRPO, an online RL framework that\nintegrates both natural language and numerical feedback for effective policy\noptimization. Critique-GRPO enables LLMs to learn from initial responses and\ncritique-guided refinements simultaneously while maintaining exploration.\nExtensive experiments using Qwen2.5-7B-Base and Qwen3-8B-Base show that\nCritique-GRPO consistently outperforms supervised learning-based and RL-based\nfine-tuning approaches across eight challenging mathematical, STEM, and general\nreasoning tasks, improving average pass@1 scores by approximately 4.5% and 5%,\nrespectively. Notably, Critique-GRPO surpasses a strong baseline that\nincorporates expert demonstrations within online RL. Further analysis reveals\ntwo critical insights about policy exploration: (1) higher entropy does not\nalways guarantee efficient learning from exploration, and (2) longer responses\ndo not necessarily lead to more effective exploration.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03106.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67079840a9bcb7459b8d2a46",
      "avatarUrl": "/avatars/32466863c5554f20cb2775b138832ac3.svg",
      "fullname": "Kaituo Feng",
      "name": "KaituoFeng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.21541",
      "authors": [
        {
          "_id": "6840f79ceb249b555b244efc",
          "name": "Zitong Wang",
          "hidden": false
        },
        {
          "_id": "6840f79ceb249b555b244efd",
          "name": "Hang Zhao",
          "hidden": false
        },
        {
          "_id": "6840f79ceb249b555b244efe",
          "name": "Qianyu Zhou",
          "hidden": false
        },
        {
          "_id": "6840f79ceb249b555b244eff",
          "name": "Xuequan Lu",
          "hidden": false
        },
        {
          "_id": "6840f79ceb249b555b244f00",
          "name": "Xiangtai Li",
          "hidden": false
        },
        {
          "_id": "6840f79ceb249b555b244f01",
          "name": "Yiren Song",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-24T16:08:04.000Z",
      "submittedOnDailyAt": "2025-06-05T00:21:34.798Z",
      "title": "DiffDecompose : Décomposition par couches d'un entraîneur de rames pour des images synthétiques d'alpha",
      "submittedOnDailyBy": {
        "_id": "64311a95034ecbefddd141ef",
        "avatarUrl": "/avatars/b6dc5ca373bedbaa368208517954c375.svg",
        "isPro": true,
        "fullname": "Yiren Song",
        "user": "yiren98",
        "type": "user"
      },
      "summary": "Le modèle de diffusion a été très réussi dans les tâches d'élimination d'objets et de plusieurs générations récentes. Cependant, la technologie actuelle de décomposition d'images est difficile à utiliser pour séparer les occlusions de couches transparentes ou semitransparentes en raison des dépendances de la masque de projection, l'hypothèse d'objets statiques et la rareté des ensembles de données. Dans cet article, nous abordons un nouveau défi : la décomposition d'images alpha par couches, où les couches des composants sont reconstruites à partir d'images qui s'empilent, dans des situations d'occultation non linéaire de couches alpha transparentes ou semitransparentes. Pour faire face à l'incertitude des couches, à la généralisation et à la rareté des données, nous présentons d'abord AlphaBlend. AlphaBlend utilise un ensemble de données de haute qualité initiale à grande échelle pour aborder la décomposition de couches transparentes et semitransparentes et supporte six tâches réelles sous-jacentes (par exemple : l'élimination de feux semitransparents, la décomposition de cellules semitransparentes, la décomposition d'articles de verre). Sur la base de cet ensemble de données, nous proposons DiffDecompose. DiffDecompose apprend le processus de décomposition de couches en fonction de l'image d'entrée, du prompt sémantique et du type de branding. Au lieu de prédire directement la matrice alpha, DiffDecompose effectue la décomposition en contexte et permet au modèle de prédire une ou plusieurs couches. Pour maintenir la correspondance des pixels entre les couches, nous introduisons la clonage de codification de position des couches (Clonage de Codification de Position des Couches). Nous évaluons l'effet de DiffDecompose sur l'ensemble de données alpha proposé et sur l'ensemble de données de logos publiés. Les codes et ensembles de données seront fournis après la révision de l'article. Les codes peuvent être utilisés via la URL suivante : https://github.com/Wangzt1121/DiffDecompose.",
      "upvotes": 5,
      "discussionId": "6840f7a1eb249b555b244ffe",
      "ai_summary": "DiffDecompose, a diffusion Transformer-based framework, effectively decomposes images into constituent layers with semantic prompts, addressing challenges in transparent layer decomposition.",
      "ai_keywords": [
        "diffusion models",
        "diffusion Transformer",
        "posterior",
        "semantic prompts",
        "blending type",
        "In-Context Decomposition",
        "Layer Position Encoding Cloning",
        "AlphaBlend dataset",
        "translucent flare removal",
        "semi-transparent cell decomposition",
        "glassware decomposition",
        "LOGO dataset"
      ]
    },
    "publishedAt": "2025-05-24T12:08:04.000Z",
    "title": "DiffDecompose: Layer-Wise Decomposition of Alpha-Composited Images via\n  Diffusion Transformers",
    "summary": "Diffusion models have recently motivated great success in many generation\ntasks like object removal. Nevertheless, existing image decomposition methods\nstruggle to disentangle semi-transparent or transparent layer occlusions due to\nmask prior dependencies, static object assumptions, and the lack of datasets.\nIn this paper, we delve into a novel task: Layer-Wise Decomposition of\nAlpha-Composited Images, aiming to recover constituent layers from single\noverlapped images under the condition of semi-transparent/transparent alpha\nlayer non-linear occlusion. To address challenges in layer ambiguity,\ngeneralization, and data scarcity, we first introduce AlphaBlend, the first\nlarge-scale and high-quality dataset for transparent and semi-transparent layer\ndecomposition, supporting six real-world subtasks (e.g., translucent flare\nremoval, semi-transparent cell decomposition, glassware decomposition).\nBuilding on this dataset, we present DiffDecompose, a diffusion\nTransformer-based framework that learns the posterior over possible layer\ndecompositions conditioned on the input image, semantic prompts, and blending\ntype. Rather than regressing alpha mattes directly, DiffDecompose performs\nIn-Context Decomposition, enabling the model to predict one or multiple layers\nwithout per-layer supervision, and introduces Layer Position Encoding Cloning\nto maintain pixel-level correspondence across layers. Extensive experiments on\nthe proposed AlphaBlend dataset and public LOGO dataset verify the\neffectiveness of DiffDecompose. The code and dataset will be available upon\npaper acceptance. Our code will be available at:\nhttps://github.com/Wangzt1121/DiffDecompose.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21541.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64311a95034ecbefddd141ef",
      "avatarUrl": "/avatars/b6dc5ca373bedbaa368208517954c375.svg",
      "fullname": "Yiren Song",
      "name": "yiren98",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 21
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.03956",
      "authors": [
        {
          "_id": "6841396eee27975702b57e87",
          "user": {
            "_id": "6759546743971eff5a12a087",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/esJm_83zW1R6NqWltof8P.png",
            "isPro": false,
            "fullname": "Aojun Lu",
            "user": "Kurt1024",
            "type": "user"
          },
          "name": "Aojun Lu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:26:29.156Z",
          "hidden": false
        },
        {
          "_id": "6841396eee27975702b57e88",
          "name": "Tao Feng",
          "hidden": false
        },
        {
          "_id": "6841396eee27975702b57e89",
          "user": {
            "_id": "649d54b314afbb10ce2a9eeb",
            "avatarUrl": "/avatars/15c325d8c2273ff63569f23015e98486.svg",
            "isPro": false,
            "fullname": "Hangjie Yuan",
            "user": "JacobYuan",
            "type": "user"
          },
          "name": "Hangjie Yuan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:26:31.411Z",
          "hidden": false
        },
        {
          "_id": "6841396eee27975702b57e8a",
          "name": "Chunhui Ding",
          "hidden": false
        },
        {
          "_id": "6841396eee27975702b57e8b",
          "name": "Yanan Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-04T13:46:33.000Z",
      "submittedOnDailyAt": "2025-06-05T05:00:41.771Z",
      "title": "Adapter avant de Continual Learning",
      "submittedOnDailyBy": {
        "_id": "649d54b314afbb10ce2a9eeb",
        "avatarUrl": "/avatars/15c325d8c2273ff63569f23015e98486.svg",
        "isPro": false,
        "fullname": "Hangjie Yuan",
        "user": "JacobYuan",
        "type": "user"
      },
      "summary": "La continua apprentissage (CA) vise à faire que les réseaux neuronaux ajoutent de nouveaux savoirs tout en maintenant les connaissances existantes (sur-ajustement). Les modèles de prédiction (MPPs) jouent un rôle important dans la CA, mais dans les méthodes actuelles d'accès, le corps des MPPs est fixe pour maintenir l'stabilité et limiter la pertinence, ce qui présente des problèmes particuliers lorsque des grandes différences de domaines sont observées lors de tâches en progression. En revanche, ajuster de manière séquentielle le MPP complet est exposé au oubli des connaissances dispersées et à un important compromis entre stabilité et pertinence. Pour relever ces défis, nous proposons un nouveau cadre de travail \"Adaptant MPPs avant le processus de CA central (ACL)\" qui améliore le corps des MPPs avant les processus clés de la CA. ACL améliore le corps des MPPs avant d'apprendre chaque nouvelle tâche, en utilisant une phase d'adaptation plug-and-play avec l'utilisation de techniques d'apprentissage continu existantes (par exemple, fine-tuning). ACL a été testée théoriquement et expérimentalement, démontrant comment équilibre la stabilité et la pertinence en ajustant les embeddings pour qu'ils coincinent avec les prototypes de classes originales et se séparent de d'autres classes, améliorant ainsi la pertinence. Les expériences étendues montrent que ACL améliore significativement le rendement de la CA lorsqu'elle est intégrée dans des cadres de référence et des méthodes d'expansion. Elle offre des solutions larges pour la CA basée sur les MPPs.",
      "upvotes": 4,
      "discussionId": "6841396eee27975702b57eb7",
      "projectPage": "https://github.com/byyx666/ACL_code",
      "githubRepo": "https://github.com/byyx666/ACL_code",
      "ai_summary": "Adapting Pre-trained Models before the core CL process (ACL) improves Continual Learning by enhancing plasticity while maintaining stability.",
      "ai_keywords": [
        "Continual Learning",
        "CL",
        "Pre-trained models",
        "PTMs",
        "plasticity",
        "stability",
        "domain gaps",
        "catastrophic forgetting",
        "prompt tuning",
        "embeddings",
        "class prototypes"
      ]
    },
    "publishedAt": "2025-06-04T09:46:33.000Z",
    "title": "Adapt before Continual Learning",
    "summary": "Continual Learning (CL) seeks to enable neural networks to incrementally\nacquire new knowledge (plasticity) while retaining existing knowledge\n(stability). While pre-trained models (PTMs) have become pivotal in CL,\nprevailing approaches freeze the PTM backbone to preserve stability, limiting\ntheir plasticity, particularly when encountering significant domain gaps in\nincremental tasks. Conversely, sequentially finetuning the entire PTM risks\ncatastrophic forgetting of generalizable knowledge, exposing a critical\nstability-plasticity trade-off. To address this challenge, we propose Adapting\nPTMs before the core CL process (ACL), a novel framework that refines the PTM\nbackbone through a plug-and-play adaptation phase before learning each new task\nwith existing CL approaches (e.g., prompt tuning). ACL enhances plasticity by\naligning embeddings with their original class prototypes while distancing them\nfrom others, theoretically and empirically shown to balance stability and\nplasticity. Extensive experiments demonstrate that ACL significantly improves\nCL performance across benchmarks and integrated methods, offering a versatile\nsolution for PTM-based CL.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03956.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "649d54b314afbb10ce2a9eeb",
      "avatarUrl": "/avatars/15c325d8c2273ff63569f23015e98486.svg",
      "fullname": "Hangjie Yuan",
      "name": "JacobYuan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.03448",
      "authors": [
        {
          "_id": "684105479060432bf302b432",
          "name": "Bimsara Pathiraja",
          "hidden": false
        },
        {
          "_id": "684105479060432bf302b433",
          "user": {
            "_id": "622d2ff38d04fd29a9ccf1a7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/622d2ff38d04fd29a9ccf1a7/ORpGrlU8Lm_oSEVftcZEK.jpeg",
            "isPro": false,
            "fullname": "Maitreya Patel",
            "user": "mpatel57",
            "type": "user"
          },
          "name": "Maitreya Patel",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:26:48.135Z",
          "hidden": false
        },
        {
          "_id": "684105479060432bf302b434",
          "name": "Shivam Singh",
          "hidden": false
        },
        {
          "_id": "684105479060432bf302b435",
          "name": "Yezhou Yang",
          "hidden": false
        },
        {
          "_id": "684105479060432bf302b436",
          "name": "Chitta Baral",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T23:20:24.000Z",
      "submittedOnDailyAt": "2025-06-05T01:19:11.023Z",
      "title": "RefEdit : Marco et Méthodologie pour Améliorer les Modèles d'Édition d'Images Basés sur des Expressions de Référence",
      "submittedOnDailyBy": {
        "_id": "622d2ff38d04fd29a9ccf1a7",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/622d2ff38d04fd29a9ccf1a7/ORpGrlU8Lm_oSEVftcZEK.jpeg",
        "isPro": false,
        "fullname": "Maitreya Patel",
        "user": "mpatel57",
        "type": "user"
      },
      "summary": "Récemment, le développement des technologies d'édition d'images basées sur la réversibilité et les directions a conduit à ce que les méthodes actuelles sont principalement adaptées pour éditer uniquement l'objet principal, mais dans des cas où il s'agit de schémas complexes qui incluent plusieurs existences, ils rencontrent de grands défis. Pour quantifier ces limitations, nous présentons RefEdit-Bench. C'est un strict cadre de test de la réalité, basé sur RefCOCO, avec des millions d'échantillons d'entraînement qui ne dépassent presque pas la ligne de base. Pour surmonter ces limites, nous présentons RefEdit, un modèle d'édition basé sur des instructions entraîné avec notre pipeline scalable de génération de données synthétiques. RefEdit est entraîné avec 20 000 triplets d'édition et surpasse la ligne de base basée sur des modèles Flux/SD3 entraînés avec des millions de données, améliorant les évaluations étendues dans différents cadres de référence. Notre modèle surpasse également les tâches de représentation de référence, améliore le rendement des cadres de référence traditionnels et met en œuvre des résultats récents comme des méthodes de copie de son clos. Pour garantir la reproductibilité, nous publions les données et les checkpoints.",
      "upvotes": 4,
      "discussionId": "6841054b9060432bf302b559",
      "projectPage": "https://refedit.vercel.app",
      "githubRepo": "https://github.com/bimsarapathiraja/refedit",
      "ai_summary": "RefEdit, an instruction-based editing model trained on synthetic data, outperforms baselines in complex scene editing and referring expression tasks.",
      "ai_keywords": [
        "RefEdit-Bench",
        "RefCOCO",
        "instruction-based editing model",
        "scalable synthetic data generation pipeline",
        "Flux/SD3",
        "state-of-the-art results"
      ]
    },
    "publishedAt": "2025-06-03T19:20:24.000Z",
    "title": "RefEdit: A Benchmark and Method for Improving Instruction-based Image\n  Editing Model on Referring Expressions",
    "summary": "Despite recent advances in inversion and instruction-based image editing,\nexisting approaches primarily excel at editing single, prominent objects but\nsignificantly struggle when applied to complex scenes containing multiple\nentities. To quantify this gap, we first introduce RefEdit-Bench, a rigorous\nreal-world benchmark rooted in RefCOCO, where even baselines trained on\nmillions of samples perform poorly. To overcome this limitation, we introduce\nRefEdit -- an instruction-based editing model trained on our scalable synthetic\ndata generation pipeline. Our RefEdit, trained on only 20,000 editing triplets,\noutperforms the Flux/SD3 model-based baselines trained on millions of data.\nExtensive evaluations across various benchmarks demonstrate that our model not\nonly excels in referring expression tasks but also enhances performance on\ntraditional benchmarks, achieving state-of-the-art results comparable to\nclosed-source methods. We release data \\& checkpoint for reproducibility.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03448.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "622d2ff38d04fd29a9ccf1a7",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/622d2ff38d04fd29a9ccf1a7/ORpGrlU8Lm_oSEVftcZEK.jpeg",
      "fullname": "Maitreya Patel",
      "name": "mpatel57",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.03355",
      "authors": [
        {
          "_id": "68415c5cce09e3eca94e9839",
          "name": "Elias Abad Rocamora",
          "hidden": false
        },
        {
          "_id": "68415c5cce09e3eca94e983a",
          "user": {
            "_id": "6310a6bb0a43f97f6c5567d3",
            "avatarUrl": "/avatars/04b07c3a6c811337939c951567cb2bf2.svg",
            "isPro": false,
            "fullname": "Christian Schlarmann",
            "user": "chs20",
            "type": "user"
          },
          "name": "Christian Schlarmann",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T09:59:48.051Z",
          "hidden": false
        },
        {
          "_id": "68415c5cce09e3eca94e983b",
          "name": "Naman Deep Singh",
          "hidden": false
        },
        {
          "_id": "68415c5cce09e3eca94e983c",
          "name": "Yongtao Wu",
          "hidden": false
        },
        {
          "_id": "68415c5cce09e3eca94e983d",
          "name": "Matthias Hein",
          "hidden": false
        },
        {
          "_id": "68415c5cce09e3eca94e983e",
          "name": "Volkan Cevher",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T19:57:09.000Z",
      "submittedOnDailyAt": "2025-06-05T07:29:30.561Z",
      "title": "Robustez dans les deux domaines : CLIP nécessite un puissant encodeur de texte.",
      "submittedOnDailyBy": {
        "_id": "6310a6bb0a43f97f6c5567d3",
        "avatarUrl": "/avatars/04b07c3a6c811337939c951567cb2bf2.svg",
        "isPro": false,
        "fullname": "Christian Schlarmann",
        "user": "chs20",
        "type": "user"
      },
      "summary": "Les attaques d'entrée adversaires peuvent modifier de manière significative les embeddings de CLIP. Cela peut affecter la robustesse des modèles qui intègrent CLIP dans leur architecture. Par exemple, des modèles de génération d'images à partir de texte ou de grands modèles de langage visuo-linguistique. Bien que l'on ait travaillé sur la robustesse de l'encodeur de CLIP pour les images, la robustesse de l'encodeur de texte reste incertaine. Cet article comble cette lacune dans la littérature. Nous proposons LEAF (Fine-tuning Adversarialement Amplié Localement). LEAF est un méthode efficace de fine-tuning adversaire pour le domaine du texte et peut être appliquée à des modèles de CLIP à grande échelle. Notre modèle améliore significativement la précision 0-shot dans le domaine du texte et maintient le rendement visuel fourni par un encodeur d'images robuste. En association avec des modèles d'expansion d'images à partir de texte, LEAF améliore la qualité de génération sous ruisselle adverse. Dans des évaluations de diversité, notre encodeur robuste de CLIP peut augmenter la probabilité de réponse face aux bruits adversaires par rapport aux modèles standards de CLIP. Enfin, un encodeur de texte robuste optimise directement la reconstruction des requêtes d'entrée pour les améliorer.",
      "upvotes": 4,
      "discussionId": "68415c5ece09e3eca94e98e4",
      "ai_summary": "LEAF, an adversarial finetuning method, enhances the robustness of CLIP text encoders, improving zero-shot accuracy and multimodal retrieval performance under adversarial noise.",
      "ai_keywords": [
        "adversarial input attacks",
        "CLIP embeddings",
        "text-to-image generative models",
        "large vision language models",
        "adversarial finetuning",
        "zero-shot adversarial accuracy",
        "text-to-image diffusion models",
        "multimodal retrieval tasks",
        "robust text encoders"
      ]
    },
    "publishedAt": "2025-06-03T15:57:09.000Z",
    "title": "Robustness in Both Domains: CLIP Needs a Robust Text Encoder",
    "summary": "Adversarial input attacks can cause a significant shift of CLIP embeddings.\nThis can affect the downstream robustness of models incorporating CLIP in the\npipeline, such as text-to-image generative models or large vision language\nmodels. While some efforts have been done towards making the CLIP image\nencoders robust, the robustness of text encoders remains unexplored. In this\nwork, we cover this gap in the literature. We propose LEAF: an efficient\nadversarial finetuning method for the text domain, with the ability to scale to\nlarge CLIP models. Our models significantly improve the zero-shot adversarial\naccuracy in the text domain, while maintaining the vision performance provided\nby robust image encoders. When combined with text-to-image diffusion models, we\ncan improve the generation quality under adversarial noise. When employing our\nrobust CLIP encoders in multimodal retrieval tasks, we improve the recall under\nadversarial noise over standard CLIP models. Finally, we show that robust text\nencoders facilitate better reconstruction of input text from its embedding via\ndirect optimization.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03355.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6310a6bb0a43f97f6c5567d3",
      "avatarUrl": "/avatars/04b07c3a6c811337939c951567cb2bf2.svg",
      "fullname": "Christian Schlarmann",
      "name": "chs20",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.02945",
      "authors": [
        {
          "_id": "6841008f2f66f731bf010feb",
          "name": "Aishwarya Sahoo",
          "hidden": false
        },
        {
          "_id": "6841008f2f66f731bf010fec",
          "name": "Jeevana Kruthi Karnuthala",
          "hidden": false
        },
        {
          "_id": "6841008f2f66f731bf010fed",
          "name": "Tushar Parmanand Budhwani",
          "hidden": false
        },
        {
          "_id": "6841008f2f66f731bf010fee",
          "name": "Pranchal Agarwal",
          "hidden": false
        },
        {
          "_id": "6841008f2f66f731bf010fef",
          "name": "Sankaran Vaidyanathan",
          "hidden": false
        },
        {
          "_id": "6841008f2f66f731bf010ff0",
          "name": "Alexa Siu",
          "hidden": false
        },
        {
          "_id": "6841008f2f66f731bf010ff1",
          "user": {
            "_id": "62c5947524171688a9feb992",
            "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
            "isPro": false,
            "fullname": "Franck Dernoncourt",
            "user": "Franck-Dernoncourt",
            "type": "user"
          },
          "name": "Franck Dernoncourt",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:26:55.170Z",
          "hidden": false
        },
        {
          "_id": "6841008f2f66f731bf010ff2",
          "name": "Jennifer Healey",
          "hidden": false
        },
        {
          "_id": "6841008f2f66f731bf010ff3",
          "name": "Nedim Lipka",
          "hidden": false
        },
        {
          "_id": "6841008f2f66f731bf010ff4",
          "name": "Ryan Rossi",
          "hidden": false
        },
        {
          "_id": "6841008f2f66f731bf010ff5",
          "name": "Uttaran Bhattacharya",
          "hidden": false
        },
        {
          "_id": "6841008f2f66f731bf010ff6",
          "name": "Branislav Kveton",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T14:44:23.000Z",
      "submittedOnDailyAt": "2025-06-05T00:57:33.123Z",
      "title": "Juges Quantitatifs de LLM\n\nChampionnat LLM de Scandale",
      "submittedOnDailyBy": {
        "_id": "62c5947524171688a9feb992",
        "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
        "isPro": false,
        "fullname": "Franck Dernoncourt",
        "user": "Franck-Dernoncourt",
        "type": "user"
      },
      "summary": "LLM-as-a-judge est un cadre de travail qui permet aux modèles de langage grands (LLM) d'évaluer automatiquement les sorties d'autres modèles de langage grands. Nous proposons un juge quantitatif de LLM. Ce modèle ajuste les scores évalués par le juge actuel de LLM de manière à les faire correspondre aux scores évalués par un humain dans certains domaines, en utilisant un modèle de régression. Ce modèle a été entraîné en utilisant les contextes et les scores d'évaluation des évaluateurs pour améliorer les scores originaux des évaluateurs. Nous présentons quatre juges quantitatifs. Cela démontre la généralité et la diversité de notre cadre de travail. Nous espérons que notre cadre de travail soit efficace en termes de calcul, ne basant pas sur l'apprentissage par observation, mais sur une rétroaction entraînée, et statistiquement efficace lorsque la rétroaction humaine est limitée. Nous avons vérifié l'effet de notre cadre de travail par des expériences avec quatre ensembles de données et deux juges de base. Nos expériences montrent que les juges quantitatifs améliorent efficacement la capacité de prédiction des juges existants.",
      "upvotes": 4,
      "discussionId": "684100902f66f731bf01101e",
      "ai_summary": "A framework uses quantitative LLM judges to align existing LLM evaluation scores with human scores, improving predictive power and efficiency through regression models.",
      "ai_keywords": [
        "LLM-as-a-judge",
        "large language model",
        "quantitative LLM judges",
        "regression models",
        "score alignment",
        "predictive power",
        "post-hoc modeling"
      ]
    },
    "publishedAt": "2025-06-03T10:44:23.000Z",
    "title": "Quantitative LLM Judges",
    "summary": "LLM-as-a-judge is a framework in which a large language model (LLM)\nautomatically evaluates the output of another LLM. We propose quantitative LLM\njudges, which align evaluation scores of existing LLM judges to human scores in\na given domain using regression models. The models are trained to improve the\nscore of the original judge by using the judge's textual evaluation and score.\nWe present four quantitative judges for different types of absolute and\nrelative feedback, which showcases the generality and versatility of our\nframework. Our framework is more computationally efficient than supervised\nfine-tuning and can be more statistically efficient when human feedback is\nlimited, which is expected in most applications of our work. We validate these\nclaims empirically on four datasets using two base judges. Our experiments show\nthat quantitative judges can effectively improve the predictive power of\nexisting judges through post-hoc modeling.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.02945.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62c5947524171688a9feb992",
      "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
      "fullname": "Franck Dernoncourt",
      "name": "Franck-Dernoncourt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.00482",
      "authors": [
        {
          "_id": "68402986a50b67f983749710",
          "user": {
            "_id": "6576ace7769f3ee9bd7b1b88",
            "avatarUrl": "/avatars/5b5921e54413a37afde6ce017809c86e.svg",
            "isPro": false,
            "fullname": "Eunsu Kim",
            "user": "EunsuKim",
            "type": "user"
          },
          "name": "Eunsu Kim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:27:49.056Z",
          "hidden": false
        },
        {
          "_id": "68402986a50b67f983749711",
          "name": "Haneul Yoo",
          "hidden": false
        },
        {
          "_id": "68402986a50b67f983749712",
          "name": "Guijin Son",
          "hidden": false
        },
        {
          "_id": "68402986a50b67f983749713",
          "name": "Hitesh Patel",
          "hidden": false
        },
        {
          "_id": "68402986a50b67f983749714",
          "name": "Amit Agarwal",
          "hidden": false
        },
        {
          "_id": "68402986a50b67f983749715",
          "user": {
            "_id": "60e0251ea9b5d8282481f2b7",
            "avatarUrl": "/avatars/43441373af054a6184c22097bfeb97e4.svg",
            "isPro": false,
            "fullname": "Alice Oh",
            "user": "aliceoh",
            "type": "user"
          },
          "name": "Alice Oh",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-06-05T06:37:21.889Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-31T09:24:32.000Z",
      "submittedOnDailyAt": "2025-06-05T04:40:48.000Z",
      "title": "BenchHub : Feuille de calcul de benchmark pour unités. Évaluation générale et définible par l'utilisateur d'un LLM.",
      "submittedOnDailyBy": {
        "_id": "6576ace7769f3ee9bd7b1b88",
        "avatarUrl": "/avatars/5b5921e54413a37afde6ce017809c86e.svg",
        "isPro": false,
        "fullname": "Eunsu Kim",
        "user": "EunsuKim",
        "type": "user"
      },
      "summary": "À mesure que les modèles de langage de haut niveau (LLMs) évoluent, la nécessité de jeux de données plus récents et de cadres d'évaluation corrigés est devenue de plus en plus importante. Cependant, les jeux de données actuels sont dispersés et leur gestion est difficile, ce qui rend l'exécution d'évaluations pour des exigences ou des domaines spécifiques plus complexe. En particulier, l'importance accrue des modèles dans des domaines spécifiques comme la mathématique ou le code a été notable. Dans cet article, nous présentons la récolte de jeux de données d'évaluation et l'introduction d'un dépôt d'évaluation, BenchHub, qui offre des classes dynamiques pour que les chercheurs et développeurs puissent évaluer efficacement les LLMs. BenchHub intègre 303K questions de 38 cadres d'évaluation, offrant des mises à jour continuelles et une gestion de données échelonnable. De cette manière, des évaluations flexibles peuvent être réalisées dans différentes zones et cas d'utilisation. À travers des expériences avec des familles de modèles de LLMs, nous montrons que le rendement des modèles peut varier considérablement dans des sous-ensembles spécifiques de domaines, soulignant l'importance des cadres d'évaluation par domaine. BenchHub encourage la réutilisation de jeux de données, rend la comparaison des modèles plus transparente, facilite l'identification des zones peu abordées dans les cadres d'évaluation existants et fournit une infrastructure cruciale pour le développement de la recherche en évaluation des LLMs.",
      "upvotes": 3,
      "discussionId": "68402987a50b67f983749746",
      "projectPage": "https://huggingface.co/BenchHub",
      "githubRepo": "https://github.com/rladmstn1714/BenchHub",
      "ai_summary": "BenchHub is a dynamic benchmark repository that aggregates and classifies datasets for large language models, facilitating domain-specific evaluations and improving model comparisons.",
      "ai_keywords": [
        "large language models",
        "LLMs",
        "BenchHub",
        "benchmark repository",
        "domain-specific models",
        "benchmark datasets",
        "continuous updates",
        "scalable data management",
        "model performance",
        "domain-aware benchmarking"
      ]
    },
    "publishedAt": "2025-05-31T05:24:32.000Z",
    "title": "BenchHub: A Unified Benchmark Suite for Holistic and Customizable LLM\n  Evaluation",
    "summary": "As large language models (LLMs) continue to advance, the need for up-to-date\nand well-organized benchmarks becomes increasingly critical. However, many\nexisting datasets are scattered, difficult to manage, and make it challenging\nto perform evaluations tailored to specific needs or domains, despite the\ngrowing importance of domain-specific models in areas such as math or code. In\nthis paper, we introduce BenchHub, a dynamic benchmark repository that empowers\nresearchers and developers to evaluate LLMs more effectively. BenchHub\naggregates and automatically classifies benchmark datasets from diverse\ndomains, integrating 303K questions across 38 benchmarks. It is designed to\nsupport continuous updates and scalable data management, enabling flexible and\ncustomizable evaluation tailored to various domains or use cases. Through\nextensive experiments with various LLM families, we demonstrate that model\nperformance varies significantly across domain-specific subsets, emphasizing\nthe importance of domain-aware benchmarking. We believe BenchHub can encourage\nbetter dataset reuse, more transparent model comparisons, and easier\nidentification of underrepresented areas in existing benchmarks, offering a\ncritical infrastructure for advancing LLM evaluation research.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.00482.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6576ace7769f3ee9bd7b1b88",
      "avatarUrl": "/avatars/5b5921e54413a37afde6ce017809c86e.svg",
      "fullname": "Eunsu Kim",
      "name": "EunsuKim",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.23807",
      "authors": [
        {
          "_id": "683fec0a9f37285365be6142",
          "user": {
            "_id": "656201912d309fa7e27ddf40",
            "avatarUrl": "/avatars/d1bb9b263a758a0b0e7f803f4f888e95.svg",
            "isPro": false,
            "fullname": "Yuli chen",
            "user": "yulichen",
            "type": "user"
          },
          "name": "Yuli Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T15:03:32.826Z",
          "hidden": false
        },
        {
          "_id": "683fec0a9f37285365be6143",
          "name": "Bo Cheng",
          "hidden": false
        },
        {
          "_id": "683fec0a9f37285365be6144",
          "name": "Jiale Han",
          "hidden": false
        },
        {
          "_id": "683fec0a9f37285365be6145",
          "name": "Yingying Zhang",
          "hidden": false
        },
        {
          "_id": "683fec0a9f37285365be6146",
          "name": "Yingting Li",
          "hidden": false
        },
        {
          "_id": "683fec0a9f37285365be6147",
          "name": "Shuhao Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-27T07:35:00.000Z",
      "submittedOnDailyAt": "2025-06-05T00:42:31.891Z",
      "title": "DLP: Prueba dynamique de chaque couche (Pruning Dynamique de Couche)",
      "submittedOnDailyBy": {
        "_id": "656201912d309fa7e27ddf40",
        "avatarUrl": "/avatars/d1bb9b263a758a0b0e7f803f4f888e95.svg",
        "isPro": false,
        "fullname": "Yuli chen",
        "user": "yulichen",
        "type": "user"
      },
      "summary": "La pruning a été largement introduit récemment pour réduire l'ampleur des paramètres de grands modèles de langage (LLMs) et améliorer l'efficacité de l'inférence. Les méthodes principales de pruning se basent généralement sur des stratégies uniformes par couche et souffrent souvent d'une perte significative de performance à des taux de sparsité élevés. Des recherches récentes ont mis l'accent sur le pruning non uniforme par couche, reconnaissant la contribution différente de chaque couche, mais ces approximations souvent se basent sur des valeurs prédéfinies et ne réussissent pas à atteindre le meilleur rendement. Pour surmonter ces limitations, nous proposons un nouveau méthode appelée « Dynamic Layerwise Pruning (DLP) ». Cette approche détermine l'importance relative de chaque couche en intégrant l'information des poids et des activations d'entrée, attribuant appropriément les niveaux de pruning. Les résultats des expériences montrent que DLP maintient le rendement du modèle à des taux de sparsité élevés et s'applique efficacement à plusieurs LLMs. En particulier, à un taux de sparsité de 70%, DLP réduit la perplexité de LLaMA2-7B de 7,79% et améliore la précision moyenne de 2,7% par rapport aux méthodes les plus avancées. De plus, DLP s'intègre facilement avec les technologies de compression actuelles de LLMs et avec le fine-tuning efficace des paramètres (PEFT). Nous publions le code sur GitHub à l'adresse https://github.com/ironartisan/DLP et invitons à poursuivre la recherche.",
      "upvotes": 3,
      "discussionId": "683fec0a9f37285365be617f",
      "ai_summary": "A dynamic layerwise pruning method adaptively determines layer importance by combining model weights and activation information to maintain performance in large language models at high sparsity.",
      "ai_keywords": [
        "pruning",
        "Large Language Models (LLMs)",
        "uniform layerwise pruning",
        "non-uniform layerwise pruning",
        "Dynamic Layerwise Pruning (DLP)",
        "perplexity",
        "Parameter-Efficient Fine-Tuning (PEFT)"
      ]
    },
    "publishedAt": "2025-05-27T03:35:00.000Z",
    "title": "DLP: Dynamic Layerwise Pruning in Large Language Models",
    "summary": "Pruning has recently been widely adopted to reduce the parameter scale and\nimprove the inference efficiency of Large Language Models (LLMs). Mainstream\npruning techniques often rely on uniform layerwise pruning strategies, which\ncan lead to severe performance degradation at high sparsity levels. Recognizing\nthe varying contributions of different layers in LLMs, recent studies have\nshifted their focus toward non-uniform layerwise pruning. However, these\napproaches often rely on pre-defined values, which can result in suboptimal\nperformance. To overcome these limitations, we propose a novel method called\nDynamic Layerwise Pruning (DLP). This approach adaptively determines the\nrelative importance of each layer by integrating model weights with input\nactivation information, assigning pruning rates accordingly. Experimental\nresults show that DLP effectively preserves model performance at high sparsity\nlevels across multiple LLMs. Specifically, at 70% sparsity, DLP reduces the\nperplexity of LLaMA2-7B by 7.79 and improves the average accuracy by 2.7%\ncompared to state-of-the-art methods. Moreover, DLP is compatible with various\nexisting LLM compression techniques and can be seamlessly integrated into\nParameter-Efficient Fine-Tuning (PEFT). We release the code at\nhttps://github.com/ironartisan/DLP to facilitate future research.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23807.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "656201912d309fa7e27ddf40",
      "avatarUrl": "/avatars/d1bb9b263a758a0b0e7f803f4f888e95.svg",
      "fullname": "Yuli chen",
      "name": "yulichen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.04133",
      "authors": [
        {
          "_id": "6840f32dda736de98e843831",
          "user": {
            "_id": "64d3c16a0553a2522f1aa792",
            "avatarUrl": "/avatars/951e272ffccf2388f138b248e5ef7142.svg",
            "isPro": false,
            "fullname": "Shaina Raza",
            "user": "shainar",
            "type": "user"
          },
          "name": "Shaina Raza",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-06-05T01:39:24.184Z",
          "hidden": false
        },
        {
          "_id": "6840f32dda736de98e843832",
          "name": "Ranjan Sapkota",
          "hidden": false
        },
        {
          "_id": "6840f32dda736de98e843833",
          "name": "Manoj Karkee",
          "hidden": false
        },
        {
          "_id": "6840f32dda736de98e843834",
          "name": "Christos Emmanouilidis",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/67ddd80896ac367438d400a6/7jK4mzUkVjPRUDMAacaCO.jpeg"
      ],
      "publishedAt": "2025-06-04T16:26:11.000Z",
      "submittedOnDailyAt": "2025-06-05T00:02:27.010Z",
      "title": "TRiSM pour l'IA des agents : revue de la confiance, du risque et de la gestion de la sécurité dans les systèmes multi-agents basés sur l'IA de langage de machine",
      "submittedOnDailyBy": {
        "_id": "67ddd80896ac367438d400a6",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/C1NY6Nv5i0erwLnzCrTUM.png",
        "isPro": false,
        "fullname": "Ranjan Sapkota",
        "user": "RanjanSapkota",
        "type": "user"
      },
      "summary": "Le système d'IA agente s'appuie sur des modèles de langage grands (LLM) et est construit à partir de multiples agents, rédefinissant l'autonomie, la coopération et les décisions dans le domaine entreprise et social. Cette revue fournit un analyse structurée sur la gestion de la confiance, des risques et de la sécurité (TRiSM) dans des systèmes à plusieurs agents basés sur des LLM. Tout d'abord, on examine la base conceptuelle de l'IA agente, la différence avec le cadre de travail de l'IA agente et le design des systèmes qui étendront les fonctions des agents. Ensuite, on décrit en détail les quatre piliers de TRiSM dans le cadre de l'IA agente : Justice, Explicabilité, ModelOps et Privacité/Sécurité. On identifie les vecteurs de risques caractéristiques d'applications d'IA agente et présente des études de cas montrant des vulnérabilités réelles qui peuvent affecter les systèmes de risque. De plus, on étudie les institutions de construction de la confiance, les méthodes de transparence et de surveillance et les stratégies d'explicabilité dans des systèmes d'agents distribués de LLM. On évalue également des métriques pour la confiance, l'explicabilité et la humano-science, y compris la participation à des défis de benchmark ouverts. Enfin, on examine la sécurité et la privacité, avec des recherches sur la cryptographie, la défense contre les attaques et l'évolution des lois de l'IA. Cette étude fournit une guide pour l'adoption responsable de l'IA agente et suggère des orientations de recherche pour l'adoption sécurisée, responsable et transparente de principes forts de TRiSM.",
      "upvotes": 2,
      "discussionId": "6840f32eda736de98e843858",
      "ai_summary": "A review of trust, risk, and security management in LLM-based agentic multi-agent systems, examining governance, explainability, ModelOps, and privacy/security.",
      "ai_keywords": [
        "LLMs",
        "agentic AI",
        "multi-agent systems",
        "TRiSM",
        "governance",
        "explainability",
        "ModelOps",
        "privacy",
        "security",
        "encryption",
        "adversarial defense",
        "compliance",
        "AI regulations",
        "trust-building mechanisms",
        "transparency",
        "oversight",
        "interpretability",
        "human-centered performance",
        "benchmarking",
        "responsible AI",
        "research directions"
      ]
    },
    "publishedAt": "2025-06-04T12:26:11.000Z",
    "title": "TRiSM for Agentic AI: A Review of Trust, Risk, and Security Management\n  in LLM-based Agentic Multi-Agent Systems",
    "summary": "Agentic AI systems, built on large language models (LLMs) and deployed in\nmulti-agent configurations, are redefining intelligent autonomy, collaboration\nand decision-making across enterprise and societal domains. This review\npresents a structured analysis of Trust, Risk, and Security Management (TRiSM)\nin the context of LLM-based agentic multi-agent systems (AMAS). We begin by\nexamining the conceptual foundations of agentic AI, its architectural\ndifferences from traditional AI agents, and the emerging system designs that\nenable scalable, tool-using autonomy. The TRiSM in the agentic AI framework is\nthen detailed through four pillars governance, explainability, ModelOps, and\nprivacy/security each contextualized for agentic LLMs. We identify unique\nthreat vectors and introduce a comprehensive risk taxonomy for the agentic AI\napplications, supported by case studies illustrating real-world\nvulnerabilities. Furthermore, the paper also surveys trust-building mechanisms,\ntransparency and oversight techniques, and state-of-the-art explainability\nstrategies in distributed LLM agent systems. Additionally, metrics for\nevaluating trust, interpretability, and human-centered performance are reviewed\nalongside open benchmarking challenges. Security and privacy are addressed\nthrough encryption, adversarial defense, and compliance with evolving AI\nregulations. The paper concludes with a roadmap for responsible agentic AI,\nproposing research directions to align emerging multi-agent systems with robust\nTRiSM principles for safe, accountable, and transparent deployment.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/67ddd80896ac367438d400a6/7jK4mzUkVjPRUDMAacaCO.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04133.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67ddd80896ac367438d400a6",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/C1NY6Nv5i0erwLnzCrTUM.png",
      "fullname": "Ranjan Sapkota",
      "name": "RanjanSapkota",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.04034",
      "authors": [
        {
          "_id": "6840ff0b535bfb4942b31576",
          "name": "Qing Jiang",
          "hidden": false
        },
        {
          "_id": "6840ff0b535bfb4942b31577",
          "name": "Xingyu Chen",
          "hidden": false
        },
        {
          "_id": "6840ff0b535bfb4942b31578",
          "name": "Zhaoyang Zeng",
          "hidden": false
        },
        {
          "_id": "6840ff0b535bfb4942b31579",
          "name": "Junzhi Yu",
          "hidden": false
        },
        {
          "_id": "6840ff0b535bfb4942b3157a",
          "name": "Lei Zhang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/647f46b6838ac3601fc89852/0J-cvgz2dA6bVQJJNb_Yz.jpeg"
      ],
      "publishedAt": "2025-06-04T14:56:57.000Z",
      "submittedOnDailyAt": "2025-06-05T00:56:30.698Z",
      "title": "Référence des objets basée sur la logique de contexte connecté",
      "submittedOnDailyBy": {
        "_id": "647f46b6838ac3601fc89852",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647f46b6838ac3601fc89852/N5cr1MFEtgKLJ4sVAhS04.jpeg",
        "isPro": true,
        "fullname": "Qing Jiang",
        "user": "Mountchicken",
        "type": "user"
      },
      "summary": "L'objectif de la détection d'objets est de détecter tous les objets présents dans une image qui correspondent à une description donnée en langage naturel. Nous argumentons que c'est crucial que un modèle fort de détection d'objets fasse des prédictions précises sur le contenu visuel lorsqu'il reçoit une description. En particulier, il doit répondre à deux caractéristiques principales : 1) Visualité, fournir une explication interprétable avec une association claire avec des preuves visuelles pour justifier les prédictions ; 2) Fiabilité, rejeter les représentations qui ne correspondent pas à des objets présents dans l'image, même si ces représentations sont exprimées. Cependant, de nombreux méthodes traitent la détection d'objets directement comme une tâche de prédiction de boîtes à contours, ce qui limite son interprétabilité et rend impossible la rejet de représentations sans objet. Dans cette étude, nous proposons le modèle Rex-Thinker pour configurer la détection d'objets comme une tâche explicite d'inférence basée sur le contexte (CoT). Lorsqu'une représentation d'un objet est fournie, Rex-Thinker identifie toutes les instances d'objets candidats qui correspondent à la catégorie de l'objet. Pour chaque candidat, Rex-Thinker effectue une évaluation pas à pas sur si elle correspond à la représentation donnée et termine par une prédiction. Pour soutenir cela, nous avons construit un grand ensemble de données de détection d'objets au style de CoT, HumanRef-CoT, en utilisant GPT-4o sur l'ensemble de données HumanRef. Chaque tracé de raison est représenté dans un format structuré de construction, action et résumé, permettant au modèle d'apprendre des raisons interprétables sur les instances d'objets candidats. De plus, Rex-Thinker est entraîné en deux étapes : premièrement avec un entraînement de base guidé pas à pas, pour enseigner au modèle à construire des raisons, puis avec un apprentissage par renforcement basé sur GRPO pour améliorer la précision et la généralisation. Les expériences montrent que notre approche dépasse les limites de précision et d'interprétabilité dans les évaluations dans le domaine, et montre également sa capacité à rejeter des représentations fausses et sa forte généralisation dans des environnements hors domaine.",
      "upvotes": 2,
      "discussionId": "6840ff0e535bfb4942b3165f",
      "projectPage": "https://rexthinker.github.io/",
      "githubRepo": "https://github.com/IDEA-Research/Rex-Thinker",
      "ai_summary": "Rex-Thinker is a CoT-based model that enhances object referring by performing step-by-step reasoning over candidate objects, leading to improved interpretability and rejection of mismatched queries.",
      "ai_keywords": [
        "CoT reasoning",
        "HumanRef-CoT",
        "GPT-4o",
        "structured reasoning",
        "cold-start supervised fine-tuning",
        "GRPO-based RL learning"
      ]
    },
    "publishedAt": "2025-06-04T10:56:57.000Z",
    "title": "Rex-Thinker: Grounded Object Referring via Chain-of-Thought Reasoning",
    "summary": "Object referring aims to detect all objects in an image that match a given\nnatural language description. We argue that a robust object referring model\nshould be grounded, meaning its predictions should be both explainable and\nfaithful to the visual content. Specifically, it should satisfy two key\nproperties: 1) Verifiable, by producing interpretable reasoning that justifies\nits predictions and clearly links them to visual evidence; and 2) Trustworthy,\nby learning to abstain when no object in the image satisfies the given\nexpression. However, most methods treat referring as a direct bounding box\nprediction task, offering limited interpretability and struggling to reject\nexpressions with no matching object. In this work, we propose Rex-Thinker, a\nmodel that formulates object referring as an explicit CoT reasoning task. Given\na referring expression, we first identify all candidate object instances\ncorresponding to the referred object category. Rex-Thinker then performs\nstep-by-step reasoning over each candidate to assess whether it matches the\ngiven expression, before making a final prediction. To support this paradigm,\nwe construct a large-scale CoT-style referring dataset named HumanRef-CoT by\nprompting GPT-4o on the HumanRef dataset. Each reasoning trace follows a\nstructured planning, action, and summarization format, enabling the model to\nlearn decomposed, interpretable reasoning over object candidates. We then train\nRex-Thinker in two stages: a cold-start supervised fine-tuning phase to teach\nthe model how to perform structured reasoning, followed by GRPO-based RL\nlearning to improve accuracy and generalization. Experiments show that our\napproach outperforms standard baselines in both precision and interpretability\non in-domain evaluation, while also demonstrating improved ability to reject\nhallucinated outputs and strong generalization in out-of-domain settings.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/647f46b6838ac3601fc89852/0J-cvgz2dA6bVQJJNb_Yz.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04034.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "647f46b6838ac3601fc89852",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647f46b6838ac3601fc89852/N5cr1MFEtgKLJ4sVAhS04.jpeg",
      "fullname": "Qing Jiang",
      "name": "Mountchicken",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.03951",
      "authors": [
        {
          "_id": "68415a1cce09e3eca94e02ef",
          "user": {
            "_id": "6759546743971eff5a12a087",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/esJm_83zW1R6NqWltof8P.png",
            "isPro": false,
            "fullname": "Aojun Lu",
            "user": "Kurt1024",
            "type": "user"
          },
          "name": "Aojun Lu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T09:59:50.242Z",
          "hidden": false
        },
        {
          "_id": "68415a1cce09e3eca94e02f0",
          "user": {
            "_id": "649d54b314afbb10ce2a9eeb",
            "avatarUrl": "/avatars/15c325d8c2273ff63569f23015e98486.svg",
            "isPro": false,
            "fullname": "Hangjie Yuan",
            "user": "JacobYuan",
            "type": "user"
          },
          "name": "Hangjie Yuan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T09:59:52.907Z",
          "hidden": false
        },
        {
          "_id": "68415a1cce09e3eca94e02f1",
          "name": "Tao Feng",
          "hidden": false
        },
        {
          "_id": "68415a1cce09e3eca94e02f2",
          "name": "Yanan Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-04T13:40:41.000Z",
      "submittedOnDailyAt": "2025-06-05T07:19:58.382Z",
      "title": "Reevaluons l'équilibre entre la stabilité et la flexibilité à partir de la perspective structurale du apprentissage continu.",
      "submittedOnDailyBy": {
        "_id": "649d54b314afbb10ce2a9eeb",
        "avatarUrl": "/avatars/15c325d8c2273ff63569f23015e98486.svg",
        "isPro": false,
        "fullname": "Hangjie Yuan",
        "user": "JacobYuan",
        "type": "user"
      },
      "summary": "Le problème de l'apprentissage continu (CL) est qu'il propose de donner aux réseaux neuronaux un apprentissage et une capacité d'adaptation au fil du temps. L'approche centrale de la CL est de résoudre le \"dilemme de l'instabilité et de la variabilité\". Cela implique permettre à la même fois la conservation des connaissances acquises et l'acquisition de nouvelles connaissances. Les méthodes de CL travaillent pour équilibrer ces aspects, mais souvent, les influences que la structure de la réseau a sur l'instabilité et la variabilité sont perdues. Dans cet article, nous nous concentrons sur le conflit d'instabilité et de variabilité au niveau structural. Dans ce contexte, il est observé que les réseaux profonds montrent une meilleure variabilité sous contraintes de paramètres, tandis que les réseaux plus larges montrent une plus grande stabilité. Pour résoudre ce dilemme, nous proposons un nouveau cadre appelé Dual-Arch. Ce cadre fonctionne comme un composant pluggable de CL et utilise les avantages complémentaires de deux réseaux neuronaux indépendants. Ces réseaux ont des structures différentes adaptées à leurs objectifs respectifs, et les expériences élargies montrent que Dual-Arch améliore le rendement des méthodes de CL existantes et augmente la compression de paramètres de plus de 87%.",
      "upvotes": 2,
      "discussionId": "68415a1dce09e3eca94e0314",
      "projectPage": "https://github.com/byyx666/Dual-Arch",
      "githubRepo": "https://github.com/byyx666/Dual-Arch",
      "ai_summary": "A novel framework, Dual-Arch, enhances Continual Learning by addressing the stability-plasticity dilemma at the architectural level using two specialized networks.",
      "ai_keywords": [
        "Continual Learning",
        "stability-plasticity dilemma",
        "deep networks",
        "wide networks",
        "Dual-Arch"
      ]
    },
    "publishedAt": "2025-06-04T09:40:41.000Z",
    "title": "Rethinking the Stability-Plasticity Trade-off in Continual Learning from\n  an Architectural Perspective",
    "summary": "The quest for Continual Learning (CL) seeks to empower neural networks with\nthe ability to learn and adapt incrementally. Central to this pursuit is\naddressing the stability-plasticity dilemma, which involves striking a balance\nbetween two conflicting objectives: preserving previously learned knowledge and\nacquiring new knowledge. While numerous CL methods aim to achieve this\ntrade-off, they often overlook the impact of network architecture on stability\nand plasticity, restricting the trade-off to the parameter level. In this\npaper, we delve into the conflict between stability and plasticity at the\narchitectural level. We reveal that under an equal parameter constraint, deeper\nnetworks exhibit better plasticity, while wider networks are characterized by\nsuperior stability. To address this architectural-level dilemma, we introduce a\nnovel framework denoted Dual-Arch, which serves as a plug-in component for CL.\nThis framework leverages the complementary strengths of two distinct and\nindependent networks: one dedicated to plasticity and the other to stability.\nEach network is designed with a specialized and lightweight architecture,\ntailored to its respective objective. Extensive experiments demonstrate that\nDual-Arch enhances the performance of existing CL methods while being up to 87%\nmore compact in terms of parameters.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03951.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "649d54b314afbb10ce2a9eeb",
      "avatarUrl": "/avatars/15c325d8c2273ff63569f23015e98486.svg",
      "fullname": "Hangjie Yuan",
      "name": "JacobYuan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.03614",
      "authors": [
        {
          "_id": "684134ca20ff8abcccb11302",
          "name": "Zhanhui Zhou",
          "hidden": false
        },
        {
          "_id": "684134ca20ff8abcccb11303",
          "name": "Lingjie Chen",
          "hidden": false
        },
        {
          "_id": "684134ca20ff8abcccb11304",
          "name": "Chao Yang",
          "hidden": false
        },
        {
          "_id": "684134ca20ff8abcccb11305",
          "name": "Chaochao Lu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-04T06:46:06.000Z",
      "submittedOnDailyAt": "2025-06-05T04:41:50.905Z",
      "title": "VLMs peuvent effectuer le repérage de données d'entraînement distribuées pour les concentrer.",
      "submittedOnDailyBy": {
        "_id": "642e5a7ba0b65dce1f87a7a2",
        "avatarUrl": "/avatars/3ae01c9330a47e98fac9f1eb0ba94073.svg",
        "isPro": false,
        "fullname": "Zhanhui Zhou",
        "user": "ZHZisZZ",
        "type": "user"
      },
      "summary": "Une des méthodes pour réduire le risque des modèles de vision et langage (VLMs) est d'éliminer les échantillons dangereux du jeu d'entraînement. Cependant, des images dangereuses peuvent être disséminées dans de petits, \"sauvages\" motifs qui se répandent dans plusieurs exemples d'entraînement, ce qui facilite que ces données ne soient pas détectées et ignorées pendant le modélisation. Par conséquent, lors de l'entraînement, les VLMs peuvent combiner ces motifs pour générer des réponses dangereuses lors de l'inférence. Par exemple, si un motif d'image représentant de la sang est combiné avec une description de \"sécurité\", un VLM entraîné peut expliquer l'image ou le contexte comme \"sécurité\".\n\nLa capacité clé des VLMs qui permet ces menaces est l'« stylisation visuelle ». Cela est défini comme la capacité à intégrer des informations visuelles dispersées dans plusieurs exemples d'entraînement qui partagent la même description textuelle. Dans notre étude, nous avons démontré la capacité de stylisation visuelle des VLMs sur trois ensembles de données en attribuant un identifiant unique de synthèse à chaque image. Tout d'abord, les paires (image, ID) sont transformées en paires (motif, ID) à différents niveaux et les modèles sont entraînés. Ensuite, nous montrons que les modèles entraînés peuvent identifier correctement l'ID à travers l'image complète ou le contexte. En nous appuyant sur cela, nous simulons une stratégie similaire pour éviter le modélisation d'images dangereuses, en remplaçant les motifs d'images dangereux par des descriptions de \"sécurité\" ou \"insécurité\", ce qui permet d'éviter le contenu dangereux lors du modélisation et de le reconstruire ultérieurement par la stylisation visuelle, ce qui met en jeu la sécurité des VLMs. Le code est disponible sur : https://github.com/ZHZisZZ/visual-stitching.",
      "upvotes": 2,
      "discussionId": "684134cb20ff8abcccb11334",
      "githubRepo": "https://github.com/ZHZisZZ/visual-stitching",
      "ai_summary": "VLMs exhibit visual stitching, an ability to integrate fragmented visual information, which enables harmful content to evade data moderation and be reconstructed during inference.",
      "ai_keywords": [
        "vision-language models",
        "VLMs",
        "visual stitching",
        "data moderation",
        "adversarial data poisoning",
        "image patches",
        "textual descriptions",
        "inference"
      ]
    },
    "publishedAt": "2025-06-04T02:46:06.000Z",
    "title": "VLMs Can Aggregate Scattered Training Patches",
    "summary": "One way to mitigate risks in vision-language models (VLMs) is to remove\ndangerous samples in their training data. However, such data moderation can be\neasily bypassed when harmful images are split into small, benign-looking\npatches, scattered across many training samples. VLMs may then learn to piece\nthese fragments together during training and generate harmful responses at\ninference, either from full images or text references. For instance, if trained\non image patches from a bloody scene paired with the descriptions \"safe,\" VLMs\nmay later describe, the full image or a text reference to the scene, as \"safe.\"\nWe define the core ability of VLMs enabling this attack as visual\nstitching -- the ability to integrate visual information spread across\nmultiple training samples that share the same textual descriptions. In our\nwork, we first demonstrate visual stitching abilities in common open-source\nVLMs on three datasets where each image is labeled with a unique synthetic ID:\nwe split each (image, ID) pair into {(patch,\nID)} pairs at different granularity for finetuning, and we find that\ntuned models can verbalize the correct IDs from full images or text reference.\nBuilding on this, we simulate the adversarial data poisoning scenario mentioned\nabove by using patches from dangerous images and replacing IDs with text\ndescriptions like ``safe'' or ``unsafe'', demonstrating how harmful content can\nevade moderation in patches and later be reconstructed through visual\nstitching, posing serious VLM safety risks. Code is available at\nhttps://github.com/ZHZisZZ/visual-stitching.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03614.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642e5a7ba0b65dce1f87a7a2",
      "avatarUrl": "/avatars/3ae01c9330a47e98fac9f1eb0ba94073.svg",
      "fullname": "Zhanhui Zhou",
      "name": "ZHZisZZ",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.01344",
      "authors": [
        {
          "_id": "6841009bdf863485e04879c8",
          "name": "Manan Suri",
          "hidden": false
        },
        {
          "_id": "6841009bdf863485e04879c9",
          "user": {
            "_id": "65c16444d4c3b8dff2f0d78d",
            "avatarUrl": "/avatars/4ed764c1657bd260d2a12ba61c111062.svg",
            "isPro": false,
            "fullname": "Puneet Mathur",
            "user": "puneetm",
            "type": "user"
          },
          "name": "Puneet Mathur",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-05T02:27:41.018Z",
          "hidden": false
        },
        {
          "_id": "6841009bdf863485e04879ca",
          "name": "Nedim Lipka",
          "hidden": false
        },
        {
          "_id": "6841009bdf863485e04879cb",
          "user": {
            "_id": "62c5947524171688a9feb992",
            "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
            "isPro": false,
            "fullname": "Franck Dernoncourt",
            "user": "Franck-Dernoncourt",
            "type": "user"
          },
          "name": "Franck Dernoncourt",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:26:52.205Z",
          "hidden": false
        },
        {
          "_id": "6841009bdf863485e04879cc",
          "name": "Ryan A. Rossi",
          "hidden": false
        },
        {
          "_id": "6841009bdf863485e04879cd",
          "name": "Vivek Gupta",
          "hidden": false
        },
        {
          "_id": "6841009bdf863485e04879ce",
          "name": "Dinesh Manocha",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-02T06:02:41.000Z",
      "submittedOnDailyAt": "2025-06-05T00:57:44.204Z",
      "title": "Flujo superado : Caractérisation de la propriété de flux micro par des agents neurosinboriques",
      "submittedOnDailyBy": {
        "_id": "62c5947524171688a9feb992",
        "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
        "isPro": false,
        "fullname": "Franck Dernoncourt",
        "user": "Franck-Dernoncourt",
        "type": "user"
      },
      "summary": "Les diagrammes de flux sont une outil important pour visualiser le processus de liquidation. Cependant, leur structure non linéaire et les relations complexes visuelles et grammaticales génèrent des défis pour l'interprétation des diagrammes de flux à l'aide de modèles de langage de haut niveau (LLMs). Cela a conduit à une réduction de la confiance dans l'automatisation du traitement des diagrammes de flux dans des domaines importants comme la logistique, la santé et l'ingénierie. Nous proposons le travail d'attribution d'éléments spécifiques de diagrammes de flux basé sur les réponses d'un LLM, appelé Fine-grained Flowchart Attribution. L'attribution des diagrammes de flux garantit la probabilité des prédictions de l'LLM et améliore l'interprétabilité en reliant la réponse générée à la structure du diagramme de flux. Nous proposons FlowPathAgent, un agent neurosymbolique qui utilise un raisonnement basé sur les graphes pour réaliser l'attribution post hoc en grand détail. Cet agent divise les diagrammes de flux et les transforme en graphes structurés de symboles, interagissant dynamiquement avec le graphe et générant des chemins d'attribution. De plus, nous présentons FlowExplainBench, un nouveau benchmark pour évaluer l'attribution des diagrammes de flux de différents styles, domaines et types de questions. Les résultats des expériences montrent que FlowPathAgent réduit l'explication visuelle des réponses de l'LLM lors de la consultation de diagrammes de flux et dépasse les baselines de 10 à 14% sur le jeu de données FlowExplainBench.",
      "upvotes": 2,
      "discussionId": "6841009ddf863485e0487a38",
      "ai_summary": "FlowPathAgent, a neurosymbolic agent, enhances the reliability of LLM predictions for flowchart interpretation by tracing specific components and generating accurate attribution paths.",
      "ai_keywords": [
        "Flowcharts",
        "Fine-grained Flowchart Attribution",
        "FlowPathAgent",
        "graph-based reasoning",
        "symbolic graph",
        "neurosymbolic agent",
        "flowExplainBench",
        "flowchart QA"
      ]
    },
    "publishedAt": "2025-06-02T02:02:41.000Z",
    "title": "Follow the Flow: Fine-grained Flowchart Attribution with Neurosymbolic\n  Agents",
    "summary": "Flowcharts are a critical tool for visualizing decision-making processes.\nHowever, their non-linear structure and complex visual-textual relationships\nmake it challenging to interpret them using LLMs, as vision-language models\nfrequently hallucinate nonexistent connections and decision paths when\nanalyzing these diagrams. This leads to compromised reliability for automated\nflowchart processing in critical domains such as logistics, health, and\nengineering. We introduce the task of Fine-grained Flowchart Attribution, which\ntraces specific components grounding a flowchart referring LLM response.\nFlowchart Attribution ensures the verifiability of LLM predictions and improves\nexplainability by linking generated responses to the flowchart's structure. We\npropose FlowPathAgent, a neurosymbolic agent that performs fine-grained post\nhoc attribution through graph-based reasoning. It first segments the flowchart,\nthen converts it into a structured symbolic graph, and then employs an agentic\napproach to dynamically interact with the graph, to generate attribution paths.\nAdditionally, we present FlowExplainBench, a novel benchmark for evaluating\nflowchart attributions across diverse styles, domains, and question types.\nExperimental results show that FlowPathAgent mitigates visual hallucinations in\nLLM answers over flowchart QA, outperforming strong baselines by 10-14% on our\nproposed FlowExplainBench dataset.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01344.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62c5947524171688a9feb992",
      "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
      "fullname": "Franck Dernoncourt",
      "name": "Franck-Dernoncourt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.03817",
      "authors": [
        {
          "_id": "68415ee454d7c6b3f9786deb",
          "name": "Julius Gonsior",
          "hidden": false
        },
        {
          "_id": "68415ee454d7c6b3f9786dec",
          "name": "Tim Rieß",
          "hidden": false
        },
        {
          "_id": "68415ee454d7c6b3f9786ded",
          "name": "Anja Reusch",
          "hidden": false
        },
        {
          "_id": "68415ee454d7c6b3f9786dee",
          "name": "Claudio Hartmann",
          "hidden": false
        },
        {
          "_id": "68415ee454d7c6b3f9786def",
          "name": "Maik Thiele",
          "hidden": false
        },
        {
          "_id": "68415ee454d7c6b3f9786df0",
          "name": "Wolfgang Lehner",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-04T10:41:37.000Z",
      "submittedOnDailyAt": "2025-06-05T07:40:58.805Z",
      "title": "Investigation des paramètres en apprentissage actif : le défi de l'échelle dans l'analyse de la voix",
      "submittedOnDailyBy": {
        "_id": "637638fa1f0421002b42facb",
        "avatarUrl": "/avatars/ed1e3024cd2ed7284b437db4bbeb2668.svg",
        "isPro": false,
        "fullname": "Julius Gonsior",
        "user": "jgonsior",
        "type": "user"
      },
      "summary": "L'explication des données nécessite du temps et du coût, mais il y a une nécessité intrinsèque pour l'apprentissage automatique à la maison (Homeline Machine Learning). L'apprentissage actif (AL) est un méthode établie pour sélectionner de manière continue des échantillons non étiquetés avec la plus grande quantité d'information et pour demander des explications d'un expert, ce qui minimise le travail d'étiquetage humain et améliore le rendement de classification. Bien que l'AL soit connu depuis des décennies, son utilisation dans des applications réelles est encore rare. Comme le montre une étude web du comité de NLP, deux raisons principales qui empêchent son utilisation sont : 1. La complexité de la configuration de l'AL et 2. La manque de confiance en son efficacité. Ces deux raisons sont attribuées à deux problèmes : un grand espace de paramètres de configuration, qui peut conduire à des résultats incorrects de l'AL ou à des expériences non reproductibles. Dans cette étude, des actions ont été réalisées : 1. Plus de 4,6 millions de combinaisons de paramètres de configuration ont été générées dans une grande grille de paramètres, 2. Les rendements de chaque combinaison ont été enregistrés et 3. Les effets de chaque paramètre de configuration sur les résultats ont été analysés. En fin de compte, des recommandations sur l'impact de chaque paramètre de configuration ont été fournies, montrant que l'AL peut être influencant dans la mise en œuvre de stratégies concrètes, et qu'avec un effort informatique minimal, des expériences reproductibles d'AL peuvent être conçues pour contribuer aux futurs études de l'AL fiables.",
      "upvotes": 1,
      "discussionId": "68415ee554d7c6b3f9786e15",
      "githubRepo": "https://github.com/jgonsior/olympic-games-of-active-learning",
      "ai_summary": "The study investigates the impact of hyperparameters on Active Learning performance, providing insights to improve its practical application and reproducibility.",
      "ai_keywords": [
        "Active Learning",
        "hyperparameter space",
        "hyperparameter grid",
        "experimental study design",
        "reproducibility",
        "trustworthiness"
      ]
    },
    "publishedAt": "2025-06-04T06:41:37.000Z",
    "title": "Survey of Active Learning Hyperparameters: Insights from a Large-Scale\n  Experimental Grid",
    "summary": "Annotating data is a time-consuming and costly task, but it is inherently\nrequired for supervised machine learning. Active Learning (AL) is an\nestablished method that minimizes human labeling effort by iteratively\nselecting the most informative unlabeled samples for expert annotation, thereby\nimproving the overall classification performance. Even though AL has been known\nfor decades, AL is still rarely used in real-world applications. As indicated\nin the two community web surveys among the NLP community about AL, two main\nreasons continue to hold practitioners back from using AL: first, the\ncomplexity of setting AL up, and second, a lack of trust in its effectiveness.\nWe hypothesize that both reasons share the same culprit: the large\nhyperparameter space of AL. This mostly unexplored hyperparameter space often\nleads to misleading and irreproducible AL experiment results. In this study, we\nfirst compiled a large hyperparameter grid of over 4.6 million hyperparameter\ncombinations, second, recorded the performance of all combinations in the\nso-far biggest conducted AL study, and third, analyzed the impact of each\nhyperparameter in the experiment results. In the end, we give recommendations\nabout the influence of each hyperparameter, demonstrate the surprising\ninfluence of the concrete AL strategy implementation, and outline an\nexperimental study design for reproducible AL experiments with minimal\ncomputational effort, thus contributing to more reproducible and trustworthy AL\nresearch in the future.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03817.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "637638fa1f0421002b42facb",
      "avatarUrl": "/avatars/ed1e3024cd2ed7284b437db4bbeb2668.svg",
      "fullname": "Julius Gonsior",
      "name": "jgonsior",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.03538",
      "authors": [
        {
          "_id": "6841585dd777f13c59460b47",
          "name": "Chengqi Li",
          "hidden": false
        },
        {
          "_id": "6841585dd777f13c59460b48",
          "name": "Zhihao Shi",
          "hidden": false
        },
        {
          "_id": "6841585dd777f13c59460b49",
          "name": "Yangdi Lu",
          "hidden": false
        },
        {
          "_id": "6841585dd777f13c59460b4a",
          "name": "Wenbo He",
          "hidden": false
        },
        {
          "_id": "6841585dd777f13c59460b4b",
          "user": {
            "_id": "634e60454677a5891c0902f4",
            "avatarUrl": "/avatars/4dc143719afe7686e05b7f2c2c5c1871.svg",
            "isPro": false,
            "fullname": "Xiangyu Xu",
            "user": "xjcvcvxj",
            "type": "user"
          },
          "name": "Xiangyu Xu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-05T08:42:10.367Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-04T03:40:33.000Z",
      "submittedOnDailyAt": "2025-06-05T07:13:24.079Z",
      "title": "Nous utilisons l'Asmium Dual 3rd Gauss Spreading pour le rendu de la réseau neuronal en état naturel stable.",
      "submittedOnDailyBy": {
        "_id": "634e60454677a5891c0902f4",
        "avatarUrl": "/avatars/4dc143719afe7686e05b7f2c2c5c1871.svg",
        "isPro": false,
        "fullname": "Xiangyu Xu",
        "user": "xjcvcvxj",
        "type": "user"
      },
      "summary": "La reconstruction 3D enfrente desafíos debido às conditions d'illumination instables e facteurs de dispersion instantanés. Les méthodes actuelles utilisent généralement stratégies héuristiques pour traiter les données d'entraînement de faible qualité, mais ces facteurs nuisent à la génération de reconstructions stables et cohérentes, ce qui entraîne des artefacts visuels. Dans cet article, nous proposons un nouveau cadre de travail appelé Asymmetric Dual 3DGS, qui exploite la randomité de ces artefacts. Spécifiquement, notre approche entraîne deux modèles 3D Gaussian Splatting (3DGS) en parallèle, impose des restrictions de cohérence pour favoriser l'entraînement fiable et limite les artefacts incertains. Pour éviter que les deux modèles convergent vers le même mode de défaillance, nous appliquons des masques différents pour prévenir la rupture par surcharge de checkpoints avec biais. Concrètement, nous utilisons des masques multiples applicables et une masque légère automatique pour mettre en œuvre des processus d'entraînement déséquilibrés entre les deux modèles et réduire les modes d'erreur partagés. De plus, pour optimiser l'entraînement du modèle, nous introduisons une version légère appelée Dynamic EMA Proxy, qui remplace l'average mobile exponentiel (EMA) pour mettre à jour dynamiquement un modèle et maintient la diversité grâce à des processus de masque interchangeable. À travers de nombreux expériments sur des ensembles de données réels difficiles, notre méthode dépasse les approximations actuelles et atteint de hautes efficacités. Les codes et modèles entraînés sont disponibles.",
      "upvotes": 1,
      "discussionId": "68415862d777f13c59460c85",
      "ai_summary": "A novel Asymmetric Dual 3DGS framework improves 3D reconstruction by training dual models with consistency constraints and divergent masking, outperforming existing methods with high efficiency.",
      "ai_keywords": [
        "3D reconstruction",
        "3D Gaussian Splatting (3DGS)",
        "stochastic artifacts",
        "consistency constraint",
        "confirmation bias",
        "divergent masking",
        "multi-cue adaptive mask",
        "self-supervised soft mask",
        "Dynamic EMA Proxy",
        "lightweight variant",
        "Exponential Moving Average (EMA)",
        "alternating masking strategy"
      ]
    },
    "publishedAt": "2025-06-03T23:40:33.000Z",
    "title": "Robust Neural Rendering in the Wild with Asymmetric Dual 3D Gaussian\n  Splatting",
    "summary": "3D reconstruction from in-the-wild images remains a challenging task due to\ninconsistent lighting conditions and transient distractors. Existing methods\ntypically rely on heuristic strategies to handle the low-quality training data,\nwhich often struggle to produce stable and consistent reconstructions,\nfrequently resulting in visual artifacts. In this work, we propose Asymmetric\nDual 3DGS, a novel framework that leverages the stochastic nature of these\nartifacts: they tend to vary across different training runs due to minor\nrandomness. Specifically, our method trains two 3D Gaussian Splatting (3DGS)\nmodels in parallel, enforcing a consistency constraint that encourages\nconvergence on reliable scene geometry while suppressing inconsistent\nartifacts. To prevent the two models from collapsing into similar failure modes\ndue to confirmation bias, we introduce a divergent masking strategy that\napplies two complementary masks: a multi-cue adaptive mask and a\nself-supervised soft mask, which leads to an asymmetric training process of the\ntwo models, reducing shared error modes. In addition, to improve the efficiency\nof model training, we introduce a lightweight variant called Dynamic EMA Proxy,\nwhich replaces one of the two models with a dynamically updated Exponential\nMoving Average (EMA) proxy, and employs an alternating masking strategy to\npreserve divergence. Extensive experiments on challenging real-world datasets\ndemonstrate that our method consistently outperforms existing approaches while\nachieving high efficiency. Codes and trained models will be released.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03538.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "634e60454677a5891c0902f4",
      "avatarUrl": "/avatars/4dc143719afe7686e05b7f2c2c5c1871.svg",
      "fullname": "Xiangyu Xu",
      "name": "xjcvcvxj",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.02294",
      "authors": [
        {
          "_id": "6840cd169241913d43af9d28",
          "name": "Niclas Popp",
          "hidden": false
        },
        {
          "_id": "6840cd169241913d43af9d29",
          "name": "Kevin Alexander Laube",
          "hidden": false
        },
        {
          "_id": "6840cd169241913d43af9d2a",
          "name": "Matthias Hein",
          "hidden": false
        },
        {
          "_id": "6840cd169241913d43af9d2b",
          "name": "Lukas Schott",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-02T22:15:59.000Z",
      "submittedOnDailyAt": "2025-06-05T00:15:23.870Z",
      "title": "L'amélioration du connaissance réduite par les mouvements de voisins inconnus a conduit à la confiance dans le traitement des données",
      "submittedOnDailyBy": {
        "_id": "655646baf8a2d3c020546ec8",
        "avatarUrl": "/avatars/4ca8de82745bb5a4fda511569bb6bd94.svg",
        "isPro": false,
        "fullname": "Niclas P",
        "user": "NPBP26",
        "type": "user"
      },
      "summary": "Les modèles de base entraînés avec de grands ensembles de données montrent une forte capacité de 0-shot dans diverses domaines. Lorsque le taille des données et le taille du modèle sont limités, leur succès peut être reproduit en incorporant des connaissances provenant des modèles de base dans de petites réseaux neuronaux apprenants. Cependant, l'effet d'absorption de connaissances est strictement limité par le taille des données disponibles. Cet article aborde les problèmes communs et utiles dans l'absorption de connaissances, comme la transformation variacionale, et met en avant les caractéristiques spirales qui apparaissent lors de l'entraînement mais pas lors de la validation. Bien que ces caractéristiques soient mal compris, on cherche à déterminer si les apprenants peuvent apprendre efficacement lorsqu'ils ont un professeur fort. Pour résoudre ce problème, une nouvelle stratégie d'expansion de données basée sur la diffusion est introduite, qui maximise la séparation des opinions entre le professeur et l'apprenant pour générer des images. Les expériences montrent que cette approche n'est pas affectée par le changement de coût, augmente significativement la précision dans les groupes les moins performants et les moyennes de mAUC sur CelebA, SpuCo Birds et ImageNet Spirales, et dépasse les lignes les plus avancées basées sur la diffusion de données.",
      "upvotes": 1,
      "discussionId": "6840cd199241913d43af9dac",
      "ai_summary": "A diffusion-based data augmentation strategy improves robustness in knowledge distillation by generating challenging samples, enhancing accuracy and spurious feature resilience.",
      "ai_keywords": [
        "knowledge distillation",
        "diffusion-based data augmentation",
        "covariate shift",
        "teacher-student model",
        "CelebA",
        "SpuCo Birds",
        "spurious ImageNet",
        "mean group accuracy",
        "worst group accuracy",
        "spurious mAUC"
      ]
    },
    "publishedAt": "2025-06-02T18:15:59.000Z",
    "title": "Improving Knowledge Distillation Under Unknown Covariate Shift Through\n  Confidence-Guided Data Augmentation",
    "summary": "Large foundation models trained on extensive datasets demonstrate strong\nzero-shot capabilities in various domains. To replicate their success when data\nand model size are constrained, knowledge distillation has become an\nestablished tool for transferring knowledge from foundation models to small\nstudent networks. However, the effectiveness of distillation is critically\nlimited by the available training data. This work addresses the common\npractical issue of covariate shift in knowledge distillation, where spurious\nfeatures appear during training but not at test time. We ask the question: when\nthese spurious features are unknown, yet a robust teacher is available, is it\npossible for a student to also become robust to them? We address this problem\nby introducing a novel diffusion-based data augmentation strategy that\ngenerates images by maximizing the disagreement between the teacher and the\nstudent, effectively creating challenging samples that the student struggles\nwith. Experiments demonstrate that our approach significantly improves worst\ngroup and mean group accuracy on CelebA and SpuCo Birds as well as the spurious\nmAUC on spurious ImageNet under covariate shift, outperforming state-of-the-art\ndiffusion-based data augmentation baselines",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.02294.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "655646baf8a2d3c020546ec8",
      "avatarUrl": "/avatars/4ca8de82745bb5a4fda511569bb6bd94.svg",
      "fullname": "Niclas P",
      "name": "NPBP26",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  }
]