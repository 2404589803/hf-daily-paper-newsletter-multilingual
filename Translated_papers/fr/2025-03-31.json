[
  {
    "paper": {
      "id": "2503.19693",
      "authors": [
        {
          "_id": "67ea363dd13d75fc156ec498",
          "user": {
            "_id": "671f8106d677d3a764a6f9a5",
            "avatarUrl": "/avatars/90b4b00058aac30c060c5eac8debb1c7.svg",
            "isPro": false,
            "fullname": "itay nakash",
            "user": "itaynakash",
            "type": "user"
          },
          "name": "Itay Nakash",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-31T08:24:09.634Z",
          "hidden": false
        },
        {
          "_id": "67ea363dd13d75fc156ec499",
          "user": {
            "_id": "62d6a0c18faee0ac953c51fa",
            "avatarUrl": "/avatars/ca818cebdb089a8d853c5bc4d5e0987b.svg",
            "isPro": false,
            "fullname": "Nitay Calderon",
            "user": "nitay",
            "type": "user"
          },
          "name": "Nitay Calderon",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-31T08:24:15.760Z",
          "hidden": false
        },
        {
          "_id": "67ea363dd13d75fc156ec49a",
          "user": {
            "_id": "6645fc650e6706053171ce51",
            "avatarUrl": "/avatars/54b03ac6939d4b8943606b12b979ce52.svg",
            "isPro": false,
            "fullname": "Eyal Ben-David",
            "user": "eyalbd",
            "type": "user"
          },
          "name": "Eyal Ben David",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-31T08:24:21.914Z",
          "hidden": false
        },
        {
          "_id": "67ea363dd13d75fc156ec49b",
          "user": {
            "_id": "630480fa6dbbb80f16352ee3",
            "avatarUrl": "/avatars/f39ce2fe96a578f42a57e3bfe3a2d137.svg",
            "isPro": false,
            "fullname": "Elad Hoffer",
            "user": "ehoffer",
            "type": "user"
          },
          "name": "Elad Hoffer",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-31T08:24:27.895Z",
          "hidden": false
        },
        {
          "_id": "67ea363dd13d75fc156ec49c",
          "name": "Roi Reichart",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/671f8106d677d3a764a6f9a5/Rq3iCgXutkz1jOx5krnk-.png"
      ],
      "publishedAt": "2025-03-25T14:18:21.000Z",
      "submittedOnDailyAt": "2025-03-31T05:02:48.696Z",
      "title": "AdaptiVocab : Adaptateur léger de vocabulaire pour optimiser l'efficacité des LLM dans certains domaines spécifiques",
      "submittedOnDailyBy": {
        "_id": "671f8106d677d3a764a6f9a5",
        "avatarUrl": "/avatars/90b4b00058aac30c060c5eac8debb1c7.svg",
        "isPro": false,
        "fullname": "itay nakash",
        "user": "itaynakash",
        "type": "user"
      },
      "summary": "Les modèles de langage général (LLMs) présentent une grande diversité. Cependant, leur large gamme d'applications est accompagnée d'un chargement informatique élevé. En particulier, dans le décodage automatique, un pas d'avance est nécessaire à chaque étape. Dans des environnements spécialisés, des fonctions communes ne sont pas nécessaires, mais des efficiences peuvent être atteintes. Dans cet article, une nouvelle perspective d'adaptation aux domaines est adoptée, en appliquant un vocabulaire approprié pour la région, avec l'objectif de réduire l'utilisation du latin et les coûts informatiques. À travers AdaptiVocab, une approche initiale et finale, on atteint l'efficience des LLMs dans des environnements à faibles ressources. AdaptiVocab s'applique à n'importe quel type de tokeniser ou architecture, réduisant le nombre de tokens nécessaires pour le traitement d'entrée et la génération de sortie, en utilisant des tokens basés sur des noms de chemin. Avec l'utilisation d'un ajustement micro parallèle efficace sur une seule GPU, AdaptiVocab initialise de nouveaux embeddings de n-tokens en utilisant une combinaison de poids exponentiels des embeddings existants. Deux modèles de 7B ont été évalués dans trois domaines, en évaluant l'efficience, la qualité de la génération et le rendement dans des tâches finales. Nos résultats montrent que AdaptiVocab peut réduire l'utilisation de tokens d'au moins 25% sans perdre d'efficacité.",
      "upvotes": 22,
      "discussionId": "67ea363ed13d75fc156ec4e8",
      "ai_keywords": [
        "AdaptiVocab",
        "vocabulary adaptation",
        "n-gram-based tokens",
        "token embeddings",
        "lightweight fine-tuning"
      ]
    },
    "publishedAt": "2025-03-25T10:18:21.000Z",
    "title": "AdaptiVocab: Enhancing LLM Efficiency in Focused Domains through\n  Lightweight Vocabulary Adaptation",
    "summary": "Large Language Models (LLMs) have shown impressive versatility as general\npurpose models. However, their broad applicability comes at a high-cost\ncomputational overhead, particularly in auto-regressive decoding where each\nstep requires a forward pass. In domain-specific settings, general-purpose\ncapabilities are unnecessary and can be exchanged for efficiency. In this work,\nwe take a novel perspective on domain adaptation, reducing latency and\ncomputational costs by adapting the vocabulary to focused domains of interest.\nWe introduce AdaptiVocab, an end-to-end approach for vocabulary adaptation,\ndesigned to enhance LLM efficiency in low-resource domains. AdaptiVocab can be\napplied to any tokenizer and architecture, modifying the vocabulary by\nreplacing tokens with domain-specific n-gram-based tokens, thereby reducing the\nnumber of tokens required for both input processing and output generation.\nAdaptiVocab initializes new n-token embeddings using an exponentially weighted\ncombination of existing embeddings and employs a lightweight fine-tuning phase\nthat can be efficiently performed on a single GPU. We evaluate two 7B LLMs\nacross three niche domains, assessing efficiency, generation quality, and\nend-task performance. Our results show that AdaptiVocab reduces token usage by\nover 25% without compromising performance",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/671f8106d677d3a764a6f9a5/Rq3iCgXutkz1jOx5krnk-.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19693.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "671f8106d677d3a764a6f9a5",
      "avatarUrl": "/avatars/90b4b00058aac30c060c5eac8debb1c7.svg",
      "fullname": "itay nakash",
      "name": "itaynakash",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.22230",
      "authors": [
        {
          "_id": "67e9fdd446d9dd867e9728d3",
          "user": {
            "_id": "6468823272d9180d4ac90bdf",
            "avatarUrl": "/avatars/70cb7d65d30ecb944595000ceeeedb1b.svg",
            "isPro": false,
            "fullname": "Wei Shen",
            "user": "Swtheking",
            "type": "user"
          },
          "name": "Wei Shen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-31T08:24:54.522Z",
          "hidden": false
        },
        {
          "_id": "67e9fdd446d9dd867e9728d4",
          "user": {
            "_id": "67805c4a43a58ab7b52a05ea",
            "avatarUrl": "/avatars/759d0466020b6f7c0207aaf62ad89eca.svg",
            "isPro": false,
            "fullname": "Guanlin Liu",
            "user": "glnbyte",
            "type": "user"
          },
          "name": "Guanlin Liu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-31T02:28:37.898Z",
          "hidden": false
        },
        {
          "_id": "67e9fdd446d9dd867e9728d5",
          "user": {
            "_id": "648223754de983d03190f4af",
            "avatarUrl": "/avatars/36c70a6a3a1aa8a7cc0de106d5902a81.svg",
            "isPro": false,
            "fullname": "Zheng Wu",
            "user": "zhengwu07",
            "type": "user"
          },
          "name": "Zheng Wu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-31T08:25:03.076Z",
          "hidden": false
        },
        {
          "_id": "67e9fdd446d9dd867e9728d6",
          "name": "Ruofei Zhu",
          "hidden": false
        },
        {
          "_id": "67e9fdd446d9dd867e9728d7",
          "user": {
            "_id": "64d20e1821aed29b2ffd2d99",
            "avatarUrl": "/avatars/b0719319a74e8f51fc8a1404aca367e6.svg",
            "isPro": false,
            "fullname": "Qingping Yang",
            "user": "qingping95",
            "type": "user"
          },
          "name": "Qingping Yang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-31T08:25:15.803Z",
          "hidden": false
        },
        {
          "_id": "67e9fdd446d9dd867e9728d8",
          "user": {
            "_id": "661bca6576ac250a1106bfa6",
            "avatarUrl": "/avatars/200327d87103f13f7cbbb40d11f2f188.svg",
            "isPro": false,
            "fullname": "Chao Xin",
            "user": "amusingchao",
            "type": "user"
          },
          "name": "Chao Xin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-31T08:25:23.410Z",
          "hidden": false
        },
        {
          "_id": "67e9fdd446d9dd867e9728d9",
          "name": "Yu Yue",
          "hidden": false
        },
        {
          "_id": "67e9fdd446d9dd867e9728da",
          "name": "Lin Yan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-28T08:26:41.000Z",
      "submittedOnDailyAt": "2025-03-31T00:59:21.502Z",
      "title": "Étude sur le tendance et l'impact de l'échelle des données dans l'apprentissage par renforcement en réponse aux réactions humaines",
      "submittedOnDailyBy": {
        "_id": "6468823272d9180d4ac90bdf",
        "avatarUrl": "/avatars/70cb7d65d30ecb944595000ceeeedb1b.svg",
        "isPro": false,
        "fullname": "Wei Shen",
        "user": "Swtheking",
        "type": "user"
      },
      "summary": "En l'apprentissage par renforcement à partir de l'humain (RLHF), il est crucial d'ajuster les modèles de langage aux préférences humaines. Bien que les derniers études aient porté sur l'amélioration des algorithmes, l'importance de la construction de données de prompt n'a pas été sous-estimée. Cet article investigate les limites d'un approche de données pour l'amélioration du rendement de RLHF et cherche à combler ce vide. En particulier, il se concentre sur la mitigation de la piraterie des récompenses et la réduction de la diversité des réponses. Un système de récompenses hybride, combinant le RTV (Vérification de Texte avec Tables) de la théorie de la logique et le modèle de récompenses génératives (GenRM), est proposé pour atténuer la piraterie des récompenses. De plus, un nouveau méthode de sélection de prompts appelé Pre-PPO est proposée pour maintenir la diversité des réponses et améliorer l'effet de l'entraînement. Il a été trouvé que dans les phases initiales de l'entraînement de RLHF, la prioritisation de l'entraînement de tâches mathématiques et de programmation a un grand impact sur le rendement. Les expérimentations sur deux échelles de taille de modèle démontrent l'efficacité et l'extensibilité des méthodes proposées. Enfin, RTV montre la plus grande résistance face à la piraterie des récompenses, suivi de GenRM (valeur réelle) et ensuite de GenRM (réponses des N meilleures de la SFT). Les stratégies proposées dans cet article permettent de rapidement identifier les petites différences caractéristiques de tâches simples et d'améliorer significativement le rendement de RLHF. Cette étude souligne l'importance de la construction de données avec soin et fournit des méthodes pratiques pour surmonter les limites de RLHF.",
      "upvotes": 20,
      "discussionId": "67e9fdd546d9dd867e97292c",
      "ai_keywords": [
        "Reinforcement Learning from Human Feedback (RLHF)",
        "reward hacking",
        "response diversity",
        "reasoning task verifiers (RTV)",
        "generative reward model (GenRM)",
        "Pre-PPO",
        "prompt-selection method",
        "mathematical tasks",
        "coding tasks",
        "GenRM with ground truth",
        "GenRM with SFT Best-of-N responses"
      ]
    },
    "publishedAt": "2025-03-28T04:26:41.000Z",
    "title": "Exploring Data Scaling Trends and Effects in Reinforcement Learning from\n  Human Feedback",
    "summary": "Reinforcement Learning from Human Feedback (RLHF) is crucial for aligning\nlarge language models with human preferences. While recent research has focused\non algorithmic improvements, the importance of prompt-data construction has\nbeen overlooked. This paper addresses this gap by exploring data-driven\nbottlenecks in RLHF performance scaling, particularly reward hacking and\ndecreasing response diversity. We introduce a hybrid reward system combining\nreasoning task verifiers (RTV) and a generative reward model (GenRM) to\nmitigate reward hacking. We also propose a novel prompt-selection method,\nPre-PPO, to maintain response diversity and enhance learning effectiveness.\nAdditionally, we find that prioritizing mathematical and coding tasks early in\nRLHF training significantly improves performance. Experiments across two model\nsizes validate our methods' effectiveness and scalability. Results show that\nRTV is most resistant to reward hacking, followed by GenRM with ground truth,\nand then GenRM with SFT Best-of-N responses. Our strategies enable rapid\ncapture of subtle task-specific distinctions, leading to substantial\nimprovements in overall RLHF performance. This work highlights the importance\nof careful data construction and provides practical methods to overcome\nperformance barriers in RLHF.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.22230.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6468823272d9180d4ac90bdf",
      "avatarUrl": "/avatars/70cb7d65d30ecb944595000ceeeedb1b.svg",
      "fullname": "Wei Shen",
      "name": "Swtheking",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.22675",
      "authors": [
        {
          "_id": "67e9f6b7d13d75fc155c7f2e",
          "user": {
            "_id": "65acfb3a14e6582c30b4ce76",
            "avatarUrl": "/avatars/3402ba72fe2436a9c2c2f92e56b15deb.svg",
            "isPro": false,
            "fullname": "TangJiakai",
            "user": "TangJiakai5704",
            "type": "user"
          },
          "name": "Jiakai Tang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-31T08:11:47.198Z",
          "hidden": false
        },
        {
          "_id": "67e9f6b7d13d75fc155c7f2f",
          "user": {
            "_id": "64db88993725f8d9a908c077",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64db88993725f8d9a908c077/JZEGk0kius6mrANlwOWw9.jpeg",
            "isPro": false,
            "fullname": "Sunhao Dai",
            "user": "KID-22",
            "type": "user"
          },
          "name": "Sunhao Dai",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-31T08:23:02.371Z",
          "hidden": false
        },
        {
          "_id": "67e9f6b7d13d75fc155c7f30",
          "user": {
            "_id": "66152fbe1bcd61054402449b",
            "avatarUrl": "/avatars/17cb2f997e7983d706d87cf7c8c5c3dd.svg",
            "isPro": false,
            "fullname": "Shi",
            "user": "TengShi",
            "type": "user"
          },
          "name": "Teng Shi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-31T08:23:10.725Z",
          "hidden": false
        },
        {
          "_id": "67e9f6b7d13d75fc155c7f31",
          "name": "Jun Xu",
          "hidden": false
        },
        {
          "_id": "67e9f6b7d13d75fc155c7f32",
          "name": "Xu Chen",
          "hidden": false
        },
        {
          "_id": "67e9f6b7d13d75fc155c7f33",
          "name": "Wen Chen",
          "hidden": false
        },
        {
          "_id": "67e9f6b7d13d75fc155c7f34",
          "name": "Wu Jian",
          "hidden": false
        },
        {
          "_id": "67e9f6b7d13d75fc155c7f35",
          "name": "Yuning Jiang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-28T17:59:03.000Z",
      "submittedOnDailyAt": "2025-03-31T00:29:30.669Z",
      "title": "Pensez d'abord : Libération de la puissance de l'inférence cachée, nécessité de penser d'abord dans les systèmes de recommandation en séquences.",
      "submittedOnDailyBy": {
        "_id": "64db88993725f8d9a908c077",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64db88993725f8d9a908c077/JZEGk0kius6mrANlwOWw9.jpeg",
        "isPro": false,
        "fullname": "Sunhao Dai",
        "user": "KID-22",
        "type": "user"
      },
      "summary": "Sequential Recommendation (SeqRec) vise à identifier des motifs séquentiels à partir des interactions historiques du utilisateur pour prédire l'item suivant, et joue un rôle important dans de nombreux systèmes de recommandation réels. Cependant, la plupart des approches actuelles se basent sur un paradigme de calcul direct vers l'avenir. Dans ce paradigme, l'état final de l'encodeur de séquences est utilisé comme représentation de l'utilisateur. Nous argumentons que ce paradigme limite la profondeur computationnelle et présente des difficultés pour modéliser des structures complexes et changeantes de préférences de l'utilisateur, ainsi qu'une faible compréhension spécifique pour les items de la fin de la séquence. Pour résoudre ces problèmes, nous proposons ReaRec. ReaRec est un cadre de calcul initial, et la représentation de l'utilisateur est renforcée par un traitement de pas cachés. En particulier, ReaRec intègre automatiquement l'état final dans le système de recommandation séquentielle, en séparant l'espace d'encodage des items de l'espace d'inférence des pas, y compris des positions de remplissage spéciales. De plus, nous présentons deux méthodes d'apprentissage basées sur l'inférence légère : l'apprentissage d'inférence en temps réel (ERL) et l'apprentissage d'inférence de développement (PRL), avec l'objectif de tirer parti de la possibilité d'inférence de ReaRec de manière appropriée. Les expériences étendues avec 5 ensembles de données publiques et d'autres architectures de SeqRec montrent la généralité et l'efficacité de notre proposition, ReaRec. En particulier, l'analyse postérieure montre clairement que ReaRec améliore de 30% à 50% le rendement de plusieurs codes basés sur la recommandation séquentielle. Par conséquent, nous espérons que cette recherche ouvre de nouvelles perspectives pour l'avenir de la recommandation séquentielle en utilisant des calculs d'inférence.",
      "upvotes": 19,
      "discussionId": "67e9f6bdd13d75fc155c805e",
      "githubRepo": "https://github.com/TangJiakai/ReaRec",
      "ai_keywords": [
        "ReaRec",
        "reasoning position embeddings",
        "Ensemble Reasoning Learning (ERL)",
        "Progressive Reasoning Learning (PRL)",
        "sequential recommendation backbones",
        "autoregressive feeding"
      ]
    },
    "publishedAt": "2025-03-28T13:59:03.000Z",
    "title": "Think Before Recommend: Unleashing the Latent Reasoning Power for\n  Sequential Recommendation",
    "summary": "Sequential Recommendation (SeqRec) aims to predict the next item by capturing\nsequential patterns from users' historical interactions, playing a crucial role\nin many real-world recommender systems. However, existing approaches\npredominantly adopt a direct forward computation paradigm, where the final\nhidden state of the sequence encoder serves as the user representation. We\nargue that this inference paradigm, due to its limited computational depth,\nstruggles to model the complex evolving nature of user preferences and lacks a\nnuanced understanding of long-tail items, leading to suboptimal performance. To\naddress this issue, we propose ReaRec, the first inference-time\ncomputing framework for recommender systems, which enhances user\nrepresentations through implicit multi-step reasoning. Specifically, ReaRec\nautoregressively feeds the sequence's last hidden state into the sequential\nrecommender while incorporating special reasoning position embeddings to\ndecouple the original item encoding space from the multi-step reasoning space.\nMoreover, we introduce two lightweight reasoning-based learning methods,\nEnsemble Reasoning Learning (ERL) and Progressive Reasoning Learning (PRL), to\nfurther effectively exploit ReaRec's reasoning potential. Extensive experiments\non five public real-world datasets and different SeqRec architectures\ndemonstrate the generality and effectiveness of our proposed ReaRec.\nRemarkably, post-hoc analyses reveal that ReaRec significantly elevates the\nperformance ceiling of multiple sequential recommendation backbones by\napproximately 30\\%-50\\%. Thus, we believe this work can open a new and\npromising avenue for future research in inference-time computing for sequential\nrecommendation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.22675.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64db88993725f8d9a908c077",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64db88993725f8d9a908c077/JZEGk0kius6mrANlwOWw9.jpeg",
      "fullname": "Sunhao Dai",
      "name": "KID-22",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.21614",
      "authors": [
        {
          "_id": "67ea331c1238e1aa16fc18b3",
          "user": {
            "_id": "64cb54da1af278541d663708",
            "avatarUrl": "/avatars/c44507cc92bb2e83154bad31b90ce6dd.svg",
            "isPro": false,
            "fullname": "Xiaoye Qu",
            "user": "Xiaoye08",
            "type": "user"
          },
          "name": "Xiaoye Qu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-31T08:27:07.849Z",
          "hidden": false
        },
        {
          "_id": "67ea331c1238e1aa16fc18b4",
          "user": {
            "_id": "63f3502a520c14618925825a",
            "avatarUrl": "/avatars/e986a2a6625e7be6890616a417f908d2.svg",
            "isPro": false,
            "fullname": "Yafu Li",
            "user": "yaful",
            "type": "user"
          },
          "name": "Yafu Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-31T08:27:17.977Z",
          "hidden": false
        },
        {
          "_id": "67ea331c1238e1aa16fc18b5",
          "user": {
            "_id": "64264095ba51f8a2136946a0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64264095ba51f8a2136946a0/FR33boVpkDXcrvGMBmprF.jpeg",
            "isPro": false,
            "fullname": "Zhaochen Su",
            "user": "Warrieryes",
            "type": "user"
          },
          "name": "Zhaochen Su",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-31T08:11:31.516Z",
          "hidden": false
        },
        {
          "_id": "67ea331c1238e1aa16fc18b6",
          "user": {
            "_id": "6246bb33da617c00b48e4d92",
            "avatarUrl": "/avatars/0304a9f6eb7f5dee4d933d03222f94e9.svg",
            "isPro": false,
            "fullname": "Weigao Sun",
            "user": "weigao266",
            "type": "user"
          },
          "name": "Weigao Sun",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-31T08:28:08.547Z",
          "hidden": false
        },
        {
          "_id": "67ea331c1238e1aa16fc18b7",
          "user": {
            "_id": "6086838b19137b3a6ba760e7",
            "avatarUrl": "/avatars/d63eea3e39b22c6e65b82c28192696f1.svg",
            "isPro": false,
            "fullname": "Jianhao Yan",
            "user": "Elliott",
            "type": "user"
          },
          "name": "Jianhao Yan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-31T08:28:21.561Z",
          "hidden": false
        },
        {
          "_id": "67ea331c1238e1aa16fc18b8",
          "user": {
            "_id": "657fe7a8504da7f6f30a2832",
            "avatarUrl": "/avatars/65987e3cba449b5d250616510ee11f33.svg",
            "isPro": false,
            "fullname": "Dongrui Liu",
            "user": "Max9803",
            "type": "user"
          },
          "name": "Dongrui Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-31T08:28:40.653Z",
          "hidden": false
        },
        {
          "_id": "67ea331c1238e1aa16fc18b9",
          "user": {
            "_id": "650eba9555dc1e841746f132",
            "avatarUrl": "/avatars/af6f5ee78f161d25ec0afc45d2def8eb.svg",
            "isPro": false,
            "fullname": "Ganqu Cui",
            "user": "ganqu",
            "type": "user"
          },
          "name": "Ganqu Cui",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-31T08:28:47.399Z",
          "hidden": false
        },
        {
          "_id": "67ea331c1238e1aa16fc18ba",
          "name": "Daizong Liu",
          "hidden": false
        },
        {
          "_id": "67ea331c1238e1aa16fc18bb",
          "user": {
            "_id": "640052d5330a45b0360483aa",
            "avatarUrl": "/avatars/0836247e9e0ecbf68b069eaa3c6edd47.svg",
            "isPro": false,
            "fullname": "Shuxian Liang",
            "user": "liang4sx",
            "type": "user"
          },
          "name": "Shuxian Liang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-31T08:29:01.763Z",
          "hidden": false
        },
        {
          "_id": "67ea331c1238e1aa16fc18bc",
          "user": {
            "_id": "615f34ec3f6d24d67c1b5c78",
            "avatarUrl": "/avatars/6dcff6477993d9e57c5cb92b6f95eb66.svg",
            "isPro": false,
            "fullname": "Junxian He",
            "user": "jxhe",
            "type": "user"
          },
          "name": "Junxian He",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-31T08:29:08.537Z",
          "hidden": false
        },
        {
          "_id": "67ea331c1238e1aa16fc18bd",
          "name": "Peng Li",
          "hidden": false
        },
        {
          "_id": "67ea331c1238e1aa16fc18be",
          "name": "Wei Wei",
          "hidden": false
        },
        {
          "_id": "67ea331c1238e1aa16fc18bf",
          "name": "Jing Shao",
          "hidden": false
        },
        {
          "_id": "67ea331c1238e1aa16fc18c0",
          "name": "Chaochao Lu",
          "hidden": false
        },
        {
          "_id": "67ea331c1238e1aa16fc18c1",
          "name": "Yue Zhang",
          "hidden": false
        },
        {
          "_id": "67ea331c1238e1aa16fc18c2",
          "name": "Xian-Sheng Hua",
          "hidden": false
        },
        {
          "_id": "67ea331c1238e1aa16fc18c3",
          "user": {
            "_id": "669f614b59adf5b56e05bce3",
            "avatarUrl": "/avatars/ffd4189efbceb0e63a03db273065a44b.svg",
            "isPro": false,
            "fullname": "BowenZhou",
            "user": "bowenZhou",
            "type": "user"
          },
          "name": "Bowen Zhou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-31T08:29:49.460Z",
          "hidden": false
        },
        {
          "_id": "67ea331c1238e1aa16fc18c4",
          "name": "Yu Cheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-27T15:36:30.000Z",
      "submittedOnDailyAt": "2025-03-31T04:49:37.564Z",
      "title": "Étude sur l'efficacité de l'inférence dans les modèles d'inférence à grande échelle : langage, multimodalité et plus.",
      "submittedOnDailyBy": {
        "_id": "64cb54da1af278541d663708",
        "avatarUrl": "/avatars/c44507cc92bb2e83154bad31b90ce6dd.svg",
        "isPro": false,
        "fullname": "Xiaoye Qu",
        "user": "Xiaoye08",
        "type": "user"
      },
      "summary": "Récemment, les grands modèles logiques (LRMs), comme DeepSeek-R1 et OpenAI ou1, ont montré une amélioration significative de leur performance en étendant la longueur de la logique de type \"Chain-of-Thought\" (CoT) lors de l'inférence. Cependant, la tendance à générer des registres logiques longs augmente, ce qui peut conduire à des contenus répétitifs (par exemple, la définition de mots répétées) et à un analyse excessive de problèmes simples ou à une révision superficielle de multiples chemins logiques dans des tâches difficiles. Ces inefficiences peuvent causer des problèmes significatifs dans des environnements où l'économie de tokens est cruciale, tant lors de l'entraînement, l'inférence que du traitement dans le monde réel (par exemple, dans des systèmes basés sur des agents). Dans cette recherche, un résumé complet des efforts récents en matière d'efficience logique des LRMs est fourni, l'accent est mis sur les problèmes propres au nouveau paradigme, des patrons communs d'inefficience sont identifiés, la méthodologie proposée est examinée tout au long du cycle des LRMs (depuis l'entraînement jusqu'à l'inférence) et des directions futures prometteuses de la recherche sont discutées. De plus, pour soutenir le développement actuel, un dépôt GitHub est maintenu où les progrès récents sont mis à jour en temps réel. Cette recherche est basée sur une exploration plus approfondie et attend d'encourager l'innovation dans un domaine qui évolue rapidement.",
      "upvotes": 15,
      "discussionId": "67ea331d1238e1aa16fc190f",
      "githubRepo": "https://github.com/XiaoYee/Awesome_Efficient_LRM_Reasoning",
      "ai_keywords": [
        "Large Reasoning Models (LRMs)",
        "DeepSeek-R1",
        "OpenAI o1",
        "Chain-of-Thought (CoT) reasoning",
        "reasoning traces",
        "redundant content",
        "over-analysis",
        "superficial exploration",
        "reasoning efficiency",
        "token economy",
        "agent-based systems",
        "pretraining",
        "inference",
        "GitHub repository"
      ]
    },
    "publishedAt": "2025-03-27T11:36:30.000Z",
    "title": "A Survey of Efficient Reasoning for Large Reasoning Models: Language,\n  Multimodality, and Beyond",
    "summary": "Recent Large Reasoning Models (LRMs), such as DeepSeek-R1 and OpenAI o1, have\ndemonstrated strong performance gains by scaling up the length of\nChain-of-Thought (CoT) reasoning during inference. However, a growing concern\nlies in their tendency to produce excessively long reasoning traces, which are\noften filled with redundant content (e.g., repeated definitions), over-analysis\nof simple problems, and superficial exploration of multiple reasoning paths for\nharder tasks. This inefficiency introduces significant challenges for training,\ninference, and real-world deployment (e.g., in agent-based systems), where\ntoken economy is critical. In this survey, we provide a comprehensive overview\nof recent efforts aimed at improving reasoning efficiency in LRMs, with a\nparticular focus on the unique challenges that arise in this new paradigm. We\nidentify common patterns of inefficiency, examine methods proposed across the\nLRM lifecycle, i.e., from pretraining to inference, and discuss promising\nfuture directions for research. To support ongoing development, we also\nmaintain a real-time GitHub repository tracking recent progress in the field.\nWe hope this survey serves as a foundation for further exploration and inspires\ninnovation in this rapidly evolving area.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.21614.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64cb54da1af278541d663708",
      "avatarUrl": "/avatars/c44507cc92bb2e83154bad31b90ce6dd.svg",
      "fullname": "Xiaoye Qu",
      "name": "Xiaoye08",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.22194",
      "authors": [
        {
          "_id": "67e9eebe1f495035ca228ded",
          "user": {
            "_id": "66ee81b676a8038cb42c8caa",
            "avatarUrl": "/avatars/9b4c5ded9c94788c35ce7ffbc2f8d24b.svg",
            "isPro": false,
            "fullname": "Yunhong Min",
            "user": "myhong",
            "type": "user"
          },
          "name": "Yunhong Min",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-31T08:11:49.474Z",
          "hidden": false
        },
        {
          "_id": "67e9eebe1f495035ca228dee",
          "user": {
            "_id": "6616702547ea6347974667e5",
            "avatarUrl": "/avatars/6bf95e50ba19df163ef89867ed63fecc.svg",
            "isPro": false,
            "fullname": "Daehyeon Choi",
            "user": "daehyeonchoi",
            "type": "user"
          },
          "name": "Daehyeon Choi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-31T08:11:51.359Z",
          "hidden": false
        },
        {
          "_id": "67e9eebe1f495035ca228def",
          "user": {
            "_id": "659e42cfb65ee9ee1fd11e61",
            "avatarUrl": "/avatars/a9220d099f32800fc43ae79bb519c1e9.svg",
            "isPro": false,
            "fullname": "Kyeongmin Yeo",
            "user": "32V",
            "type": "user"
          },
          "name": "Kyeongmin Yeo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-31T08:26:31.020Z",
          "hidden": false
        },
        {
          "_id": "67e9eebe1f495035ca228df0",
          "name": "Jihyun Lee",
          "hidden": false
        },
        {
          "_id": "67e9eebe1f495035ca228df1",
          "user": {
            "_id": "631f432b5ba8c026340a7890",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/631f432b5ba8c026340a7890/9PK7A_TRMpugwYjCsNBf1.jpeg",
            "isPro": false,
            "fullname": "Minhyuk Sung",
            "user": "Minhyuk",
            "type": "user"
          },
          "name": "Minhyuk Sung",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-31T08:26:40.432Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-28T07:23:12.000Z",
      "submittedOnDailyAt": "2025-03-31T00:39:49.117Z",
      "title": "3D Zero-Shot Endpoint Generation",
      "submittedOnDailyBy": {
        "_id": "6616702547ea6347974667e5",
        "avatarUrl": "/avatars/6bf95e50ba19df163ef89867ed63fecc.svg",
        "isPro": false,
        "fullname": "Daehyeon Choi",
        "user": "daehyeonchoi",
        "type": "user"
      },
      "summary": "Introducing ORIGEN. C'est la première méthode de zéro-shot pour effectuer la graphitification 3D sur diverses objets et catégories d'images. Les recherches précédentes ont principalement été centrées sur la manipulation des positions 2D pour la graphitification spatiale dans la génération d'images, mais il y avait des inconvénients à la commande des directions 3D. Pour y remédier, nous proposons une approche d'échantillonnage en utilisant un modèle de discriminateur pré-entraîné pour l'évaluation des directions 3D et un modèle de flux de génération d'images en un seul pas avec une orientation par récompense. La méthode d'ascension de gradient pour la récompense dans la graphitification est une choix naturel, mais la maintenance de la réalité de l'image a été difficile. Au lieu de cela, nous adoptons une approche d'échantillonnage en utilisant un dynamique de débruitage, et par rapport à l'ascension de gradient, cela nécessite seulement ajouter une ligne de code pour injecter du bruit aléatoire. De plus, nous introduisons une recalculation temporelle adaptative basée sur la fonction de récompense pour accélérer la convergence. Selon nos résultats expérimentaux, ORIGEN dépasse les méthodes de base d'entraînement et de guidage en temps de test, tant en termes de mesures quantitatives que dans les étapes utilisateurs.",
      "upvotes": 13,
      "discussionId": "67e9eebf1f495035ca228e34",
      "projectPage": "https://origen2025.github.io/",
      "ai_keywords": [
        "zero-shot method",
        "3D orientation grounding",
        "text-to-image generation",
        "spatial grounding",
        "reward-guided sampling approach",
        "pretrained discriminative model",
        "3D orientation estimation",
        "one-step text-to-image generative flow model",
        "gradient-ascent-based optimization",
        "Langevin dynamics",
        "adaptive time rescaling"
      ]
    },
    "publishedAt": "2025-03-28T03:23:12.000Z",
    "title": "ORIGEN: Zero-Shot 3D Orientation Grounding in Text-to-Image Generation",
    "summary": "We introduce ORIGEN, the first zero-shot method for 3D orientation grounding\nin text-to-image generation across multiple objects and diverse categories.\nWhile previous work on spatial grounding in image generation has mainly focused\non 2D positioning, it lacks control over 3D orientation. To address this, we\npropose a reward-guided sampling approach using a pretrained discriminative\nmodel for 3D orientation estimation and a one-step text-to-image generative\nflow model. While gradient-ascent-based optimization is a natural choice for\nreward-based guidance, it struggles to maintain image realism. Instead, we\nadopt a sampling-based approach using Langevin dynamics, which extends gradient\nascent by simply injecting random noise--requiring just a single additional\nline of code. Additionally, we introduce adaptive time rescaling based on the\nreward function to accelerate convergence. Our experiments show that ORIGEN\noutperforms both training-based and test-time guidance methods across\nquantitative metrics and user studies.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.22194.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6616702547ea6347974667e5",
      "avatarUrl": "/avatars/6bf95e50ba19df163ef89867ed63fecc.svg",
      "fullname": "Daehyeon Choi",
      "name": "daehyeonchoi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.21821",
      "authors": [
        {
          "_id": "67e9ffab6887b70da56d0de5",
          "user": {
            "_id": "668b6668cc2c0b4ae303bdb8",
            "avatarUrl": "/avatars/2a4e30c0a5ee76b66232f425d5e62747.svg",
            "isPro": false,
            "fullname": "Kaiyue Feng",
            "user": "Carrie777",
            "type": "user"
          },
          "name": "Kaiyue Feng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-31T08:30:14.677Z",
          "hidden": false
        },
        {
          "_id": "67e9ffab6887b70da56d0de6",
          "user": {
            "_id": "62f662bcc58915315c4eccea",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
            "isPro": true,
            "fullname": "Yilun",
            "user": "yilunzhao",
            "type": "user"
          },
          "name": "Yilun Zhao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-31T08:11:42.629Z",
          "hidden": false
        },
        {
          "_id": "67e9ffab6887b70da56d0de7",
          "user": {
            "_id": "6244de1c1c560fb11edfca44",
            "avatarUrl": "/avatars/36558928bd04be7f49837d4c603681d7.svg",
            "isPro": false,
            "fullname": "Yixin Liu",
            "user": "henryL7",
            "type": "user"
          },
          "name": "Yixin Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-31T08:30:33.598Z",
          "hidden": false
        },
        {
          "_id": "67e9ffab6887b70da56d0de8",
          "name": "Tianyu Yang",
          "hidden": false
        },
        {
          "_id": "67e9ffab6887b70da56d0de9",
          "name": "Chen Zhao",
          "hidden": false
        },
        {
          "_id": "67e9ffab6887b70da56d0dea",
          "user": {
            "_id": "6626e136cee7ea5738a8442b",
            "avatarUrl": "/avatars/8c1537773e2c70f9c10b51a004380824.svg",
            "isPro": false,
            "fullname": "John Sous",
            "user": "jsous",
            "type": "user"
          },
          "name": "John Sous",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-31T08:30:58.653Z",
          "hidden": false
        },
        {
          "_id": "67e9ffab6887b70da56d0deb",
          "user": {
            "_id": "5f5ba21188f57f65f951f255",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1599840760465-noauth.png",
            "isPro": false,
            "fullname": "Arman Cohan",
            "user": "armanc",
            "type": "user"
          },
          "name": "Arman Cohan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-31T08:31:04.921Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-26T06:21:56.000Z",
      "submittedOnDailyAt": "2025-03-31T01:07:24.176Z",
      "title": "Physique : Test de référence pour modèles de physique universitaire de base\n  Résolution du problème",
      "submittedOnDailyBy": {
        "_id": "62f662bcc58915315c4eccea",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
        "isPro": true,
        "fullname": "Yilun",
        "user": "yilunzhao",
        "type": "user"
      },
      "summary": "Fijix présente un benchmark intégral pour la résolution de problèmes physiques universitaires. Ce benchmark comprend 1297 problèmes spécialisés dans 6 domaines clés : mécanique classique, physique quantique, thermodynamique et statistique, électromagnétisme, physique atomique et optique. Chaque problème requiert un haut niveau de connaissances physiques et mathématiques. Un système de évaluation automatique puissant a été développé pour assurer une validation stricte et fiable. Les limites pratiques des modèles de base ont été révélées par leur évaluation. Le modèle le plus avancé, o3-mini, atteint une précision de 59,9 %, révélant un problème fondamental dans la résolution de problèmes scientifiques de haut niveau. Grâce à l'analyse des erreurs, l'exploration de stratégies de test diverses et l'expansion des connaissances basée sur Rag, les points clés pour amélioration sont identifiés et la base est établie pour les avancées futures.",
      "upvotes": 10,
      "discussionId": "67e9ffac6887b70da56d0e15",
      "githubRepo": "https://github.com/yale-nlp/Physics"
    },
    "publishedAt": "2025-03-26T02:21:56.000Z",
    "title": "PHYSICS: Benchmarking Foundation Models on University-Level Physics\n  Problem Solving",
    "summary": "We introduce PHYSICS, a comprehensive benchmark for university-level physics\nproblem solving. It contains 1297 expert-annotated problems covering six core\nareas: classical mechanics, quantum mechanics, thermodynamics and statistical\nmechanics, electromagnetism, atomic physics, and optics. Each problem requires\nadvanced physics knowledge and mathematical reasoning. We develop a robust\nautomated evaluation system for precise and reliable validation. Our evaluation\nof leading foundation models reveals substantial limitations. Even the most\nadvanced model, o3-mini, achieves only 59.9% accuracy, highlighting significant\nchallenges in solving high-level scientific problems. Through comprehensive\nerror analysis, exploration of diverse prompting strategies, and\nRetrieval-Augmented Generation (RAG)-based knowledge augmentation, we identify\nkey areas for improvement, laying the foundation for future advancements.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.21821.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62f662bcc58915315c4eccea",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
      "fullname": "Yilun",
      "name": "yilunzhao",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.20785",
      "authors": [
        {
          "_id": "67e7b4ba05d7355e476f4a10",
          "user": {
            "_id": "66d347eebb76fb26eedb256e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66d347eebb76fb26eedb256e/iCPF7GkmZu--XCsWzoucl.jpeg",
            "isPro": false,
            "fullname": "tianqi liu",
            "user": "tqliu",
            "type": "user"
          },
          "name": "Tianqi Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-31T08:12:50.975Z",
          "hidden": false
        },
        {
          "_id": "67e7b4ba05d7355e476f4a11",
          "user": {
            "_id": "631b24f2f6bc4be4a64c4d43",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/631b24f2f6bc4be4a64c4d43/P9_tVF7SESmVxxGKVCgCk.jpeg",
            "isPro": false,
            "fullname": "Zihao Huang",
            "user": "Inso",
            "type": "user"
          },
          "name": "Zihao Huang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-31T08:33:22.318Z",
          "hidden": false
        },
        {
          "_id": "67e7b4ba05d7355e476f4a12",
          "user": {
            "_id": "62fc8cf7ee999004b5a8b982",
            "avatarUrl": "/avatars/6c5dda9e58747054a989f077a078f3dc.svg",
            "isPro": false,
            "fullname": "Zhaoxi Chen",
            "user": "FrozenBurning",
            "type": "user"
          },
          "name": "Zhaoxi Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-31T08:33:43.896Z",
          "hidden": false
        },
        {
          "_id": "67e7b4ba05d7355e476f4a13",
          "user": {
            "_id": "62e893da40bd989bb71b8f89",
            "avatarUrl": "/avatars/34e755d1124303a498429a3c4d01367b.svg",
            "isPro": false,
            "fullname": "Guangcong Wang",
            "user": "GuangcongWang",
            "type": "user"
          },
          "name": "Guangcong Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-31T08:33:54.167Z",
          "hidden": false
        },
        {
          "_id": "67e7b4ba05d7355e476f4a14",
          "user": {
            "_id": "6503be91a450492f84314af8",
            "avatarUrl": "/avatars/ef94efdad0bc8a423262d25a8cf77e41.svg",
            "isPro": false,
            "fullname": "Shoukang Hu",
            "user": "skhu101",
            "type": "user"
          },
          "name": "Shoukang Hu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-31T08:34:21.036Z",
          "hidden": false
        },
        {
          "_id": "67e7b4ba05d7355e476f4a15",
          "name": "Liao Shen",
          "hidden": false
        },
        {
          "_id": "67e7b4ba05d7355e476f4a16",
          "name": "Huiqiang Sun",
          "hidden": false
        },
        {
          "_id": "67e7b4ba05d7355e476f4a17",
          "name": "Zhiguo Cao",
          "hidden": false
        },
        {
          "_id": "67e7b4ba05d7355e476f4a18",
          "name": "Wei Li",
          "hidden": false
        },
        {
          "_id": "67e7b4ba05d7355e476f4a19",
          "user": {
            "_id": "62ab1ac1d48b4d8b048a3473",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656826685333-62ab1ac1d48b4d8b048a3473.png",
            "isPro": false,
            "fullname": "Ziwei Liu",
            "user": "liuziwei7",
            "type": "user"
          },
          "name": "Ziwei Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-31T08:34:47.540Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-26T17:59:44.000Z",
      "submittedOnDailyAt": "2025-03-31T03:00:39.077Z",
      "title": "Free4D : Génération de scénarios 4D avec cohérence espace-temporelle",
      "submittedOnDailyBy": {
        "_id": "62fc8cf7ee999004b5a8b982",
        "avatarUrl": "/avatars/6c5dda9e58747054a989f077a078f3dc.svg",
        "isPro": false,
        "fullname": "Zhaoxi Chen",
        "user": "FrozenBurning",
        "type": "user"
      },
      "summary": "Free4D est un nouveau cadre de travail pour la génération de scènes 4D à partir d'une seule image. Les méthodes existantes se concentrent sur la génération au niveau d'objet, ce qui peut rendre la génération au niveau de scène plus difficile. De plus, elles nécessitent un entraînement à haute fréquence avec des ensembles de vidéos de grand nombre d'angles, ce qui limite la capacité de généralisation en raison de la rareté des données de scènes 4D. D'autre part, notre principal objectif est de styliser des modèles pré-entraînés pour fournir une représentation cohérente de scènes 4D. De cette manière, il est possible d'améliorer l'efficacité et la capacité de généralisation. 1) Pour y parvenir, nous commençons par déplacer les images à l'aide d'un modèle de diffusion d'animation et nous initialisons la structure géométrique 4D. 2) Pour convertir cette structure de base en un animé d'angles cohérent en espace et en temps, nous concevons une stratégie de bruit de points guidés qui garantit la cohérence spatiale et une nouvelle stratégie d'échange de données potentielles qui garantit la cohérence temporelle. 3) Pour améliorer les observations en une représentation 4D cohérente, nous proposons une correction basée sur l'ajustement qui maximise l'information générée et réduit les divergences. Ces représentations 4D permettent de rendre en temps réel et de manière contrôlée les transformations temporelles, démontrant un avancé important dans la génération de scènes 4D à partir d'une seule image.",
      "upvotes": 9,
      "discussionId": "67e7b4bb05d7355e476f4a74",
      "ai_keywords": [
        "diffusion models",
        "4D scene representation",
        "image-to-video",
        "adaptive guidance mechanism",
        "point-guided denoising",
        "latent replacement strategy",
        "temporal coherence",
        "modulation-based refinement"
      ]
    },
    "publishedAt": "2025-03-26T13:59:44.000Z",
    "title": "Free4D: Tuning-free 4D Scene Generation with Spatial-Temporal\n  Consistency",
    "summary": "We present Free4D, a novel tuning-free framework for 4D scene generation from\na single image. Existing methods either focus on object-level generation,\nmaking scene-level generation infeasible, or rely on large-scale multi-view\nvideo datasets for expensive training, with limited generalization ability due\nto the scarcity of 4D scene data. In contrast, our key insight is to distill\npre-trained foundation models for consistent 4D scene representation, which\noffers promising advantages such as efficiency and generalizability. 1) To\nachieve this, we first animate the input image using image-to-video diffusion\nmodels followed by 4D geometric structure initialization. 2) To turn this\ncoarse structure into spatial-temporal consistent multiview videos, we design\nan adaptive guidance mechanism with a point-guided denoising strategy for\nspatial consistency and a novel latent replacement strategy for temporal\ncoherence. 3) To lift these generated observations into consistent 4D\nrepresentation, we propose a modulation-based refinement to mitigate\ninconsistencies while fully leveraging the generated information. The resulting\n4D representation enables real-time, controllable rendering, marking a\nsignificant advancement in single-image-based 4D scene generation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20785.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62fc8cf7ee999004b5a8b982",
      "avatarUrl": "/avatars/6c5dda9e58747054a989f077a078f3dc.svg",
      "fullname": "Zhaoxi Chen",
      "name": "FrozenBurning",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 22
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.22236",
      "authors": [
        {
          "_id": "67e9fe132a2d5e305e4e6b80",
          "name": "Chongjie Ye",
          "hidden": false
        },
        {
          "_id": "67e9fe132a2d5e305e4e6b81",
          "name": "Yushuang Wu",
          "hidden": false
        },
        {
          "_id": "67e9fe132a2d5e305e4e6b82",
          "user": {
            "_id": "6735f447eb4b9c3f36dea354",
            "avatarUrl": "/avatars/396d007ba4d49ca62e604f3b5c227b42.svg",
            "isPro": false,
            "fullname": "鲁子腾",
            "user": "LUZITENG",
            "type": "user"
          },
          "name": "Ziteng Lu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-31T08:31:42.733Z",
          "hidden": false
        },
        {
          "_id": "67e9fe132a2d5e305e4e6b83",
          "name": "Jiahao Chang",
          "hidden": false
        },
        {
          "_id": "67e9fe132a2d5e305e4e6b84",
          "name": "Xiaoyang Guo",
          "hidden": false
        },
        {
          "_id": "67e9fe132a2d5e305e4e6b85",
          "name": "Jiaqing Zhou",
          "hidden": false
        },
        {
          "_id": "67e9fe132a2d5e305e4e6b86",
          "name": "Hao Zhao",
          "hidden": false
        },
        {
          "_id": "67e9fe132a2d5e305e4e6b87",
          "name": "Xiaoguang Han",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-28T08:39:20.000Z",
      "submittedOnDailyAt": "2025-03-31T01:00:02.027Z",
      "title": "Hi3DGen : Connexion de génération d'images de haute densité pour la génération de géométrie 3D.",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "L'augmentation de la demande de modèles 3D de haute qualité en images 2D, à un moment où les méthodes actuelles rencontrent de grands défis en raison de la différence de couleurs RGB et de l'incertitude inhérente, a conduit à la proposition d'un nouveau cadre de travail appelé \"Hi3DGen\" pour la génération de qualité généralisée 3D à partir d'images. Hi3DGen comprend trois composants principaux : un estimateur de normales en images, une approche d'apprentissage généralisé à partir de normales, et un proxy pour la synthèse de données 3D. Des expériences extensives ont démontré que notre cadre de travail produit des résultats exceptionnels en termes de profondeur généralisée 3D, dépassant les méthodes les plus récentes, et offre une nouvelle orientation pour la génération de qualité généralisée 3D à partir d'images.",
      "upvotes": 8,
      "discussionId": "67e9fe172a2d5e305e4e6ce6",
      "ai_keywords": [
        "image-to-normal estimator",
        "dual-stream training",
        "normal-regularized latent diffusion learning",
        "3D data synthesis pipeline",
        "normal maps"
      ]
    },
    "publishedAt": "2025-03-28T04:39:20.000Z",
    "title": "Hi3DGen: High-fidelity 3D Geometry Generation from Images via Normal\n  Bridging",
    "summary": "With the growing demand for high-fidelity 3D models from 2D images, existing\nmethods still face significant challenges in accurately reproducing\nfine-grained geometric details due to limitations in domain gaps and inherent\nambiguities in RGB images. To address these issues, we propose Hi3DGen, a novel\nframework for generating high-fidelity 3D geometry from images via normal\nbridging. Hi3DGen consists of three key components: (1) an image-to-normal\nestimator that decouples the low-high frequency image pattern with noise\ninjection and dual-stream training to achieve generalizable, stable, and sharp\nestimation; (2) a normal-to-geometry learning approach that uses\nnormal-regularized latent diffusion learning to enhance 3D geometry generation\nfidelity; and (3) a 3D data synthesis pipeline that constructs a high-quality\ndataset to support training. Extensive experiments demonstrate the\neffectiveness and superiority of our framework in generating rich geometric\ndetails, outperforming state-of-the-art methods in terms of fidelity. Our work\nprovides a new direction for high-fidelity 3D geometry generation from images\nby leveraging normal maps as an intermediate representation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.22236.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6522
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.22268",
      "authors": [
        {
          "_id": "67e9fec1564b123aa5d70388",
          "name": "Nan Huang",
          "hidden": false
        },
        {
          "_id": "67e9fec1564b123aa5d70389",
          "name": "Wenzhao Zheng",
          "hidden": false
        },
        {
          "_id": "67e9fec1564b123aa5d7038a",
          "user": {
            "_id": "654ca71d5255ee86711b52c5",
            "avatarUrl": "/avatars/52bf00fd74c8db5643c4daa185c678e6.svg",
            "isPro": false,
            "fullname": "Chenfeng Xu",
            "user": "chenfengx",
            "type": "user"
          },
          "name": "Chenfeng Xu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-31T09:56:25.025Z",
          "hidden": false
        },
        {
          "_id": "67e9fec1564b123aa5d7038b",
          "user": {
            "_id": "6251bf4b183aa4266924ad91",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678041834400-6251bf4b183aa4266924ad91.jpeg",
            "isPro": true,
            "fullname": "Kurt Keutzer",
            "user": "kurtkeutzer",
            "type": "user"
          },
          "name": "Kurt Keutzer",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-31T09:56:18.832Z",
          "hidden": false
        },
        {
          "_id": "67e9fec1564b123aa5d7038c",
          "name": "Shanghang Zhang",
          "hidden": false
        },
        {
          "_id": "67e9fec1564b123aa5d7038d",
          "user": {
            "_id": "6478cf7150ff7001631679c3",
            "avatarUrl": "/avatars/65ec385a9cc44c972e6caf952e759ff1.svg",
            "isPro": false,
            "fullname": "Angjoo Kanazawa",
            "user": "akanazawa",
            "type": "user"
          },
          "name": "Angjoo Kanazawa",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-31T09:56:03.262Z",
          "hidden": false
        },
        {
          "_id": "67e9fec1564b123aa5d7038e",
          "user": {
            "_id": "6616f0c4c2e30710e607c2bf",
            "avatarUrl": "/avatars/a5941e0ed940439f4c7c67747318cbfc.svg",
            "isPro": false,
            "fullname": "Qianqian Wang",
            "user": "qianqian68",
            "type": "user"
          },
          "name": "Qianqian Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-31T09:55:54.563Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-28T09:34:11.000Z",
      "submittedOnDailyAt": "2025-03-31T01:03:54.297Z",
      "title": "Segmenter tout mouvement dans des vidéos",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "La segmentation des objets dynamiques est un travail important pour atteindre une compréhension visuelle élevée et a de nombreuses applications. L'humain peut facilement segmenter des objets en mouvement dans des vidéos. Les études précédentes ont dépendu du flux optique pour fournir des codes de mouvement, mais cette approche a conduit à des prédictions insuffisantes en raison de problèmes tels que des mouvements partiels, des déformations complexes, des interférences de mouvement et du fond. Nous proposons un nouvel approche qui combine des codes de trajectoire à longue distance et des caractéristiques significatives basées sur DINO, en utilisant la stratégie de prompting itératif avec SAM2 pour augmenter effectivement la densité des masques de pixels. Notre modèle utilise l'attention spectrale-thématique de trajectoire et l'embedding séparé de mouvement et sémantique pour prioriser le mouvement tout en intégrant un appui significatif. Nous avons effectué des tests larges sur différents ensembles de données et nous avons montré un rendement récent, démontrant des résultats excellents sur des échelles difficiles et dans la segmentation de plusieurs objets en détail. Notre code est disponible sur https://motion-seg.github.io/.",
      "upvotes": 6,
      "discussionId": "67e9fec2564b123aa5d70406",
      "ai_keywords": [
        "DINO-based",
        "SAM2",
        "Spatio-Temporal Trajectory Attention",
        "Motion-Semantic Decoupled Embedding"
      ]
    },
    "publishedAt": "2025-03-28T05:34:11.000Z",
    "title": "Segment Any Motion in Videos",
    "summary": "Moving object segmentation is a crucial task for achieving a high-level\nunderstanding of visual scenes and has numerous downstream applications. Humans\ncan effortlessly segment moving objects in videos. Previous work has largely\nrelied on optical flow to provide motion cues; however, this approach often\nresults in imperfect predictions due to challenges such as partial motion,\ncomplex deformations, motion blur and background distractions. We propose a\nnovel approach for moving object segmentation that combines long-range\ntrajectory motion cues with DINO-based semantic features and leverages SAM2 for\npixel-level mask densification through an iterative prompting strategy. Our\nmodel employs Spatio-Temporal Trajectory Attention and Motion-Semantic\nDecoupled Embedding to prioritize motion while integrating semantic support.\nExtensive testing on diverse datasets demonstrates state-of-the-art\nperformance, excelling in challenging scenarios and fine-grained segmentation\nof multiple objects. Our code is available at https://motion-seg.github.io/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.22268.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6522
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.17827",
      "authors": [
        {
          "_id": "67e2b7beb408962c5815c52d",
          "user": {
            "_id": "658167e4499fbe1b9541adb9",
            "avatarUrl": "/avatars/0cec32b67c31b2d17b86f5a498400a17.svg",
            "isPro": false,
            "fullname": "Wenxuan Zhu",
            "user": "vxuanz",
            "type": "user"
          },
          "name": "Wenxuan Zhu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-28T14:22:33.321Z",
          "hidden": false
        },
        {
          "_id": "67e2b7beb408962c5815c52e",
          "user": {
            "_id": "666ddb45c0f3d5afc27e85ba",
            "avatarUrl": "/avatars/dce98fc77bd8cf9e348f2d91bc3c0225.svg",
            "isPro": false,
            "fullname": "Bing Li",
            "user": "bing-li-ai",
            "type": "user"
          },
          "name": "Bing Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-31T08:15:06.874Z",
          "hidden": false
        },
        {
          "_id": "67e2b7beb408962c5815c52f",
          "name": "Cheng Zheng",
          "hidden": false
        },
        {
          "_id": "67e2b7beb408962c5815c530",
          "name": "Jinjie Mai",
          "hidden": false
        },
        {
          "_id": "67e2b7beb408962c5815c531",
          "name": "Jun Chen",
          "hidden": false
        },
        {
          "_id": "67e2b7beb408962c5815c532",
          "user": {
            "_id": "67ea63f26da1353351989746",
            "avatarUrl": "/avatars/c7766b43f082bc71623a8fc1a23768ff.svg",
            "isPro": false,
            "fullname": "Letian Jiang",
            "user": "TonNew",
            "type": "user"
          },
          "name": "Letian Jiang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-31T09:57:38.643Z",
          "hidden": false
        },
        {
          "_id": "67e2b7beb408962c5815c533",
          "user": {
            "_id": "62fe3442e9061c0170d06e0b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1660827186084-62fe3442e9061c0170d06e0b.png",
            "isPro": false,
            "fullname": "Abdullah Hamdi",
            "user": "ajhamdi",
            "type": "user"
          },
          "name": "Abdullah Hamdi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-31T09:57:44.416Z",
          "hidden": false
        },
        {
          "_id": "67e2b7beb408962c5815c534",
          "name": "Sara Rojas Martinez",
          "hidden": false
        },
        {
          "_id": "67e2b7beb408962c5815c535",
          "name": "Chia-Wen Lin",
          "hidden": false
        },
        {
          "_id": "67e2b7beb408962c5815c536",
          "user": {
            "_id": "64a27d39649b0d08ca0a4ca6",
            "avatarUrl": "/avatars/e4446a875506c10de9ae28411dc6416d.svg",
            "isPro": false,
            "fullname": "Mohamed Elhoseiny",
            "user": "mhelhoseiny",
            "type": "user"
          },
          "name": "Mohamed Elhoseiny",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-31T09:58:09.462Z",
          "hidden": false
        },
        {
          "_id": "67e2b7beb408962c5815c537",
          "name": "Bernard Ghanem",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-22T17:55:53.000Z",
      "submittedOnDailyAt": "2025-03-31T07:19:16.539Z",
      "title": "4D-Bench : Bâtiment de référence pour l'interprétation des objets 4D avec un modèle de langage grand et multimodal",
      "submittedOnDailyBy": {
        "_id": "666ddb45c0f3d5afc27e85ba",
        "avatarUrl": "/avatars/dce98fc77bd8cf9e348f2d91bc3c0225.svg",
        "isPro": false,
        "fullname": "Bing Li",
        "user": "bing-li-ai",
        "type": "user"
      },
      "summary": "Les modèles de langage multimodal de DeepMood (MLLMs) montrent un imprègnenant pouvoir de compréhension d'images/vidéos 2D. Cependant, il n'existe pas de référentiels ouverts et standardisés pour évaluer la capacité de compréhension d'objets 4D (le développement temporel d'objets 3D). Dans cet article, nous présentons le premier référentiel pour évaluer la compréhension d'objets 4D, appelé 4D-Bench. Ce référentiel offre deux tâches : Répondre à des questions sur des objets 4D (4D Objet QA) et capturer des objets 4D (4D Objet Captura). 4D-Bench fournit des objets 4D de diverses catégories, des annotations de haute qualité, et des tâches nécessitant la compréhension de différentes perspectives et temps dans l'espace. En utilisant 4D-Bench, nous avons évalué les capacités de différents MLLMs ouverts-source et propriétaires-source. Les résultats de l'expérience de capture d'objets 4D montrent clairement que les MLLMs ont une compréhension temporelle plus faible par rapport à leur compréhension esthétique. Spécifiquement, les modèles ouverts-source approchent des performances des modèles propriétaires-source en termes de compréhension esthétique, mais présentent une différence plus importante en termes de compréhension temporelle. La tâche de 4D Objet QA a produit des résultats surprenants : même dans des vidéos contenant un seul objet simple, les MLLMs montrent de faibles performances, et le plus avancé, le GPT-4o, atteint une précision de 63% par rapport à un référentiel humain de 91%. Ces résultats clairement démontrent les grands erreurs dans la compréhension d'objets 4D et la nécessité d'avancer dans les MLLMs.",
      "upvotes": 5,
      "discussionId": "67e2b7c1b408962c5815c671",
      "projectPage": "https://wenxuanzhu1103.github.io/4dbench.github.io/",
      "githubRepo": "https://github.com/WenxuanZhu1103/4D-Bench",
      "ai_keywords": [
        "Multimodal Large Language Models (MLLMs)",
        "4D objects",
        "4D-Bench",
        "4D object Question Answering (4D object QA)",
        "4D object captioning",
        "multi-view",
        "spatial-temporal understanding",
        "GPT-4o"
      ]
    },
    "publishedAt": "2025-03-22T13:55:53.000Z",
    "title": "4D-Bench: Benchmarking Multi-modal Large Language Models for 4D Object\n  Understanding",
    "summary": "Multimodal Large Language Models (MLLMs) have demonstrated impressive 2D\nimage/video understanding capabilities. However, there are no publicly\nstandardized benchmarks to assess the abilities of MLLMs in understanding the\n4D objects (3D objects with temporal evolution over time). In this paper, we\nintroduce 4D-Bench, the first benchmark to evaluate the capabilities of MLLMs\nin 4D object understanding, featuring tasks in 4D object Question Answering (4D\nobject QA) and 4D object captioning. 4D-Bench provides 4D objects with diverse\ncategories, high-quality annotations, and tasks necessitating multi-view\nspatial-temporal understanding, different from existing 2D image/video-based\nbenchmarks. With 4D-Bench, we evaluate a wide range of open-source and\nclosed-source MLLMs. The results from the 4D object captioning experiment\nindicate that MLLMs generally exhibit weaker temporal understanding compared to\ntheir appearance understanding, notably, while open-source models approach\nclosed-source performance in appearance understanding, they show larger\nperformance gaps in temporal understanding. 4D object QA yields surprising\nfindings: even with simple single-object videos, MLLMs perform poorly, with\nstate-of-the-art GPT-4o achieving only 63\\% accuracy compared to the human\nbaseline of 91\\%. These findings highlight a substantial gap in 4D object\nunderstanding and the need for further advancements in MLLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17827.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "666ddb45c0f3d5afc27e85ba",
      "avatarUrl": "/avatars/dce98fc77bd8cf9e348f2d91bc3c0225.svg",
      "fullname": "Bing Li",
      "name": "bing-li-ai",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.22329",
      "authors": [
        {
          "_id": "67ea01e3d13d75fc155fa69d",
          "user": {
            "_id": "6071c4b270e11b30cfcfd7a3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6071c4b270e11b30cfcfd7a3/-1ekCBzSTpqxkkul0bgmI.jpeg",
            "isPro": false,
            "fullname": "Louis Owen",
            "user": "louisowen6",
            "type": "user"
          },
          "name": "Louis Owen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-31T08:11:39.976Z",
          "hidden": false
        },
        {
          "_id": "67ea01e3d13d75fc155fa69e",
          "user": {
            "_id": "645a0d3dd6648853107c5fdc",
            "avatarUrl": "/avatars/1e3b6a4f5ce81a707ba7cbdf81631091.svg",
            "isPro": false,
            "fullname": "Nilabhra Roy Chowdhury",
            "user": "nilabhra",
            "type": "user"
          },
          "name": "Nilabhra Roy Chowdhury",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-31T08:11:37.938Z",
          "hidden": false
        },
        {
          "_id": "67ea01e3d13d75fc155fa69f",
          "user": {
            "_id": "62cd4b03c5cc157be82f0b56",
            "avatarUrl": "/avatars/351e963c1c763d507ae78cbcd62966a3.svg",
            "isPro": false,
            "fullname": "Abhay kumar",
            "user": "akanyaani",
            "type": "user"
          },
          "name": "Abhay Kumar",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-31T08:11:35.757Z",
          "hidden": false
        },
        {
          "_id": "67ea01e3d13d75fc155fa6a0",
          "name": "Fabian Güra",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-28T11:08:34.000Z",
      "submittedOnDailyAt": "2025-03-31T01:17:56.852Z",
      "title": "「Analyse des actions à grande échelle dans les LLMs」",
      "submittedOnDailyBy": {
        "_id": "6071c4b270e11b30cfcfd7a3",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6071c4b270e11b30cfcfd7a3/-1ekCBzSTpqxkkul0bgmI.jpeg",
        "isPro": false,
        "fullname": "Louis Owen",
        "user": "louisowen6",
        "type": "user"
      },
      "summary": "Cet article se concentre sur la relation entre l'entraînement à faible précision et l'oxydation dans les modèles de langage grands (LLMs), ce qui a attiré beaucoup d'attention récemment. Cependant, l'analyse actuelle est limitée dans son portée et il n'est pas clair si elle peut être généralisée à différentes structures. Cet article fournit un analyse approfondie de l'activation massive dans les LLMs et cherche à résoudre ces problèmes. Nos résultats contredisent la logique existante. En particulier, on souligne les points suivants : 1) L'activation massive n'a pas toujours un impact négatif. En d'autres termes, sa restriction n'est pas directement liée à l'explosion du chaos ni à la perte du rendement des tâches ultérieures. 2) La plupart des solutions proposées sont spécifiques au modèle et ne sont pas toujours efficaces. Par conséquent, nous avons examiné une nouvelle combinaison hybride de solutions, comme la Réscalage de Variance cible (TVR), le biais de KV dans l'attention et la fonction tanh dynamique (DyT), qui ont réussi à atténuer l'activation massive et préserver le rendement du modèle. Notre code est disponible sur : https://github.com/bluorion-com/refine_massive_activations.",
      "upvotes": 4,
      "discussionId": "67ea01e4d13d75fc155fa6d2",
      "githubRepo": "https://github.com/bluorion-com/refine_massive_activations",
      "ai_keywords": [
        "massive activations",
        "low-precision training",
        "quantization",
        "large language models (LLMs)",
        "GLU-based architectures",
        "Attention KV bias",
        "Target Variance Rescaling (TVR)",
        "Dynamic Tanh (DyT)",
        "perplexity",
        "downstream task performance"
      ]
    },
    "publishedAt": "2025-03-28T07:08:34.000Z",
    "title": "A Refined Analysis of Massive Activations in LLMs",
    "summary": "Motivated in part by their relevance for low-precision training and\nquantization, massive activations in large language models (LLMs) have recently\nemerged as a topic of interest. However, existing analyses are limited in\nscope, and generalizability across architectures is unclear. This paper helps\naddress some of these gaps by conducting an analysis of massive activations\nacross a broad range of LLMs, including both GLU-based and non-GLU-based\narchitectures. Our findings challenge several prior assumptions, most\nimportantly: (1) not all massive activations are detrimental, i.e. suppressing\nthem does not lead to an explosion of perplexity or a collapse in downstream\ntask performance; (2) proposed mitigation strategies such as Attention KV bias\nare model-specific and ineffective in certain cases. We consequently\ninvestigate novel hybrid mitigation strategies; in particular pairing Target\nVariance Rescaling (TVR) with Attention KV bias or Dynamic Tanh (DyT)\nsuccessfully balances the mitigation of massive activations with preserved\ndownstream model performance in the scenarios we investigated. Our code is\navailable at: https://github.com/bluorion-com/refine_massive_activations.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.22329.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6071c4b270e11b30cfcfd7a3",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6071c4b270e11b30cfcfd7a3/-1ekCBzSTpqxkkul0bgmI.jpeg",
      "fullname": "Louis Owen",
      "name": "louisowen6",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.21732",
      "authors": [
        {
          "_id": "67e620f77203bed82eb944e9",
          "user": {
            "_id": "66744b514f3d4b3327cd228d",
            "avatarUrl": "/avatars/9768587af7442fbb140f6b3d58100f91.svg",
            "isPro": false,
            "fullname": "XianglongHe",
            "user": "XianglongHe",
            "type": "user"
          },
          "name": "Xianglong He",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T10:51:50.659Z",
          "hidden": false
        },
        {
          "_id": "67e620f77203bed82eb944ea",
          "user": {
            "_id": "644dbf6453ad80c6593bf748",
            "avatarUrl": "/avatars/0e170cf2aa8d7f0f3f83e36f06f023f8.svg",
            "isPro": false,
            "fullname": "Zixin Zou",
            "user": "zouzx",
            "type": "user"
          },
          "name": "Zi-Xin Zou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T10:52:17.598Z",
          "hidden": false
        },
        {
          "_id": "67e620f77203bed82eb944eb",
          "name": "Chia-Hao Chen",
          "hidden": false
        },
        {
          "_id": "67e620f77203bed82eb944ec",
          "user": {
            "_id": "6346aaa3f06b237ba4e297b0",
            "avatarUrl": "/avatars/5acb986e993eab1461200f3e9d99d022.svg",
            "isPro": false,
            "fullname": "Yuan-Chen Guo",
            "user": "bennyguo",
            "type": "user"
          },
          "name": "Yuan-Chen Guo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T10:52:33.753Z",
          "hidden": false
        },
        {
          "_id": "67e620f77203bed82eb944ed",
          "name": "Ding Liang",
          "hidden": false
        },
        {
          "_id": "67e620f77203bed82eb944ee",
          "name": "Chun Yuan",
          "hidden": false
        },
        {
          "_id": "67e620f77203bed82eb944ef",
          "name": "Wanli Ouyang",
          "hidden": false
        },
        {
          "_id": "67e620f77203bed82eb944f0",
          "user": {
            "_id": "638066faf022c8a5803f7eb8",
            "avatarUrl": "/avatars/4cfd699c3f6c5461b12b7dc5e3fe183d.svg",
            "isPro": false,
            "fullname": "Yanpei Cao",
            "user": "pookiefoof",
            "type": "user"
          },
          "name": "Yan-Pei Cao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T10:52:50.823Z",
          "hidden": false
        },
        {
          "_id": "67e620f77203bed82eb944f1",
          "user": {
            "_id": "64d71083a787c9bc7b9f1238",
            "avatarUrl": "/avatars/d0b0546dec7fc5792921154bec41385a.svg",
            "isPro": false,
            "fullname": "Yangguang Li",
            "user": "Lp256",
            "type": "user"
          },
          "name": "Yangguang Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T10:52:59.331Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-27T17:46:42.000Z",
      "submittedOnDailyAt": "2025-03-31T01:22:51.212Z",
      "title": "SparseFlex : Modélisation de formes 3D à haute résolution et de thèmes arbitraires de données",
      "submittedOnDailyBy": {
        "_id": "64d71083a787c9bc7b9f1238",
        "avatarUrl": "/avatars/d0b0546dec7fc5792921154bec41385a.svg",
        "isPro": false,
        "fullname": "Yangguang Li",
        "user": "Lp256",
        "type": "user"
      },
      "summary": "La génération d'un maillage 3D avec toute topologie incluse est un défi important, car il peut inclure des structures ouvertes et complexes internes. Les méthodes de champs cachés sont coûteuses et nécessitent des transformations denses qui réduisent l'information détaillée, ce qui rend leur utilisation difficile à des résolutions élevées. Dans cet article, nous présentons une nouvelle représentation de surfaces avec des structures rares qui permet de différencier à un niveau de résolution d'environ 1024^3 en utilisant des pertes de rendu. Ce méthode combine la précision des Flexicubes avec la structure de cellules rares pour traiter efficacement des structures ouvertes, en se concentrant sur l'aire de contact superficielle. Il est crucial d'introduire une stratégie d'entraînement des cellules qui se concentre sur l'aire superficielle du modèle 3D, activant les cellules pertinentes uniquement lors du rendu pour réduire significativement l'utilisation de la mémoire et permettre l'entraînement à des résolutions élevées. Cela permet de reconstruire d'abord l'intérieur du maillage uniquement par le rendu. À partir de cela, nous entraînons un codificateur de compression binaire (VAE) et un convertisseur de flux modifié pour générer des qualités 3D de haute précision. Les résultats des expérimentations montrent une réduction de 82% de la distance de Chamfer et un augmentation de 88% de la F-score, ce qui permet la génération de hautes résolutions et de formes 3D détaillées avec toute topologie. La génération différenciable à haute résolution par des pertes de rendu et la structure rare de SparseFlex rendent ce méthode une technologie leader en la représentation et le modélisation de formes 3D.",
      "upvotes": 3,
      "discussionId": "67e620fb7203bed82eb945e8",
      "projectPage": "https://xianglonghe.github.io/TripoSF/index.html",
      "githubRepo": "https://github.com/VAST-AI-Research/TripoSF",
      "ai_keywords": [
        "SparseFlex",
        "isosurface representation",
        "differentiable mesh reconstruction",
        "Flexicubes",
        "sparse voxel structure",
        "frustum-aware",
        "sectional voxel training strategy",
        "memory consumption",
        "variational autoencoder (VAE)",
        "rectified flow transformer",
        "high-quality 3D shape generation",
        "Chamfer Distance",
        "F-score",
        "high-resolution, differentiable mesh reconstruction",
        "3D shape representation",
        "3D shape modeling"
      ]
    },
    "publishedAt": "2025-03-27T13:46:42.000Z",
    "title": "SparseFlex: High-Resolution and Arbitrary-Topology 3D Shape Modeling",
    "summary": "Creating high-fidelity 3D meshes with arbitrary topology, including open\nsurfaces and complex interiors, remains a significant challenge. Existing\nimplicit field methods often require costly and detail-degrading watertight\nconversion, while other approaches struggle with high resolutions. This paper\nintroduces SparseFlex, a novel sparse-structured isosurface representation that\nenables differentiable mesh reconstruction at resolutions up to 1024^3\ndirectly from rendering losses. SparseFlex combines the accuracy of Flexicubes\nwith a sparse voxel structure, focusing computation on surface-adjacent regions\nand efficiently handling open surfaces. Crucially, we introduce a frustum-aware\nsectional voxel training strategy that activates only relevant voxels during\nrendering, dramatically reducing memory consumption and enabling\nhigh-resolution training. This also allows, for the first time, the\nreconstruction of mesh interiors using only rendering supervision. Building\nupon this, we demonstrate a complete shape modeling pipeline by training a\nvariational autoencoder (VAE) and a rectified flow transformer for high-quality\n3D shape generation. Our experiments show state-of-the-art reconstruction\naccuracy, with a ~82% reduction in Chamfer Distance and a ~88% increase in\nF-score compared to previous methods, and demonstrate the generation of\nhigh-resolution, detailed 3D shapes with arbitrary topology. By enabling\nhigh-resolution, differentiable mesh reconstruction and generation with\nrendering losses, SparseFlex significantly advances the state-of-the-art in 3D\nshape representation and modeling.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.21732.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64d71083a787c9bc7b9f1238",
      "avatarUrl": "/avatars/d0b0546dec7fc5792921154bec41385a.svg",
      "fullname": "Yangguang Li",
      "name": "Lp256",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.21332",
      "authors": [
        {
          "_id": "67e623f10aaa5e9f7cf8a179",
          "user": {
            "_id": "65642d7401de72cb63165d22",
            "avatarUrl": "/avatars/1f4417c4ac5e781ce73eae1060e3f7f2.svg",
            "isPro": true,
            "fullname": "ytaewon",
            "user": "hamzzi",
            "type": "user"
          },
          "name": "Taewon Yun",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-28T08:36:56.604Z",
          "hidden": false
        },
        {
          "_id": "67e623f10aaa5e9f7cf8a17a",
          "name": "Jihwan Oh",
          "hidden": false
        },
        {
          "_id": "67e623f10aaa5e9f7cf8a17b",
          "user": {
            "_id": "6510c8ebf26dbb8827ee5e80",
            "avatarUrl": "/avatars/cc49a2f176c951007006e0dae331bc50.svg",
            "isPro": false,
            "fullname": "Hyangsuk Min",
            "user": "hyang0503",
            "type": "user"
          },
          "name": "Hyangsuk Min",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-31T10:01:07.989Z",
          "hidden": false
        },
        {
          "_id": "67e623f10aaa5e9f7cf8a17c",
          "user": {
            "_id": "63f6eec4c96958470d207698",
            "avatarUrl": "/avatars/7fba5e561b809a1623bf2228435f1aad.svg",
            "isPro": false,
            "fullname": "Yuho Lee",
            "user": "Myyhlee",
            "type": "user"
          },
          "name": "Yuho Lee",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-31T10:01:13.949Z",
          "hidden": false
        },
        {
          "_id": "67e623f10aaa5e9f7cf8a17d",
          "user": {
            "_id": "644938d43def32791088b762",
            "avatarUrl": "/avatars/1f17916b92ef13452151175cb8cafdf9.svg",
            "isPro": false,
            "fullname": "Jihwan Bang",
            "user": "hwany-j",
            "type": "user"
          },
          "name": "Jihwan Bang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-31T10:01:20.117Z",
          "hidden": false
        },
        {
          "_id": "67e623f10aaa5e9f7cf8a17e",
          "user": {
            "_id": "6463c26aa5af935cfe70f08d",
            "avatarUrl": "/avatars/33b1210098891db54f57d1344b5110fb.svg",
            "isPro": false,
            "fullname": "Jinglun (Jason) Cai",
            "user": "jasoncai",
            "type": "user"
          },
          "name": "Jason Cai",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-31T10:01:43.066Z",
          "hidden": false
        },
        {
          "_id": "67e623f10aaa5e9f7cf8a17f",
          "name": "Hwanjun Song",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-27T10:11:41.000Z",
      "submittedOnDailyAt": "2025-03-31T05:50:46.001Z",
      "title": "Refeed : Amélioration du résumé multidimensionnel en utilisant la théorie logique réflexive pour améliorer le résumé",
      "submittedOnDailyBy": {
        "_id": "65642d7401de72cb63165d22",
        "avatarUrl": "/avatars/1f4417c4ac5e781ce73eae1060e3f7f2.svg",
        "isPro": true,
        "fullname": "ytaewon",
        "user": "hamzzi",
        "type": "user"
      },
      "summary": "Un résumé de correction de diversité a plusieurs problèmes. Dans cet article, nous présentons un système de correction de résumé puissant appelé ReFeed, qui explique comment améliore plusieurs aspects en utilisant une réflexion basée sur la rétroalimentation. Pour y parvenir, nous lançons un grand ensemble de données de contexte à long terme appelé SumFeed-CoT et nous optimisons l'entraînement de modèles légers pour apprendre des raisons réflexives. Les expériences montrent comment la quantité de dimensions, l'exposition à la rétroalimentation et les stratégies de raisons affectent le rendement de la correction, et soulignent l'importance d'introduire plusieurs rétroalimentations simultanées pour atténuer le compromis entre dimensions. De plus, ReFeed est résistante aux rétroalimentations avec bruit et à la séquence des rétroalimentations. Enfin, il est affirmé que la génération de données avec des objectifs et des guides est une base fondamentale pour des raisons efficaces. Les ensembles de données et les modèles sont lancés.",
      "upvotes": 3,
      "discussionId": "67e623f20aaa5e9f7cf8a1dc",
      "ai_keywords": [
        "Long-CoT-based dataset",
        "reflective reasoning",
        "refinement performance",
        "ReFeed",
        "SumFeed-CoT"
      ]
    },
    "publishedAt": "2025-03-27T06:11:41.000Z",
    "title": "ReFeed: Multi-dimensional Summarization Refinement with Reflective\n  Reasoning on Feedback",
    "summary": "Summarization refinement faces challenges when extending to multi-dimension.\nIn this paper, we introduce ReFeed, a powerful summarization refinement\npipeline that enhances multiple dimensions through reflective reasoning on\nfeedback. To achieve this, we release SumFeed-CoT, a large-scale Long-CoT-based\ndataset optimized for training a lightweight model with reflective reasoning.\nOur experiments reveal how the number of dimensions, feedback exposure, and\nreasoning policy influence refinement performance, highlighting reflective\nreasoning and simultaneously addressing multiple feedback is crucial to\nmitigate trade-off between dimensions. Furthermore, ReFeed is robust to noisy\nfeedback and feedback order. Lastly, our finding emphasizes that creating data\nwith a proper goal and guideline constitutes a fundamental pillar of effective\nreasoning. The dataset and model will be released.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.21332.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "65642d7401de72cb63165d22",
      "avatarUrl": "/avatars/1f4417c4ac5e781ce73eae1060e3f7f2.svg",
      "fullname": "ytaewon",
      "name": "hamzzi",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.18968",
      "authors": [
        {
          "_id": "67ea0ede7b856e8fa8ff50d0",
          "user": {
            "_id": "65be4d7d5e342a230dc19a54",
            "avatarUrl": "/avatars/04ba16980da94954d811032f0091212f.svg",
            "isPro": false,
            "fullname": "Ziyue Wang",
            "user": "ZiyueWang",
            "type": "user"
          },
          "name": "Ziyue Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-31T09:59:13.929Z",
          "hidden": false
        },
        {
          "_id": "67ea0ede7b856e8fa8ff50d1",
          "user": {
            "_id": "6317257fc92fd6fee317ff7c",
            "avatarUrl": "/avatars/2f460a2f28562c987becb2acad8d93e7.svg",
            "isPro": false,
            "fullname": "Junde Wu",
            "user": "morson",
            "type": "user"
          },
          "name": "Junde Wu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-31T03:41:20.293Z",
          "hidden": false
        },
        {
          "_id": "67ea0ede7b856e8fa8ff50d2",
          "name": "Chang Han Low",
          "hidden": false
        },
        {
          "_id": "67ea0ede7b856e8fa8ff50d3",
          "name": "Yueming Jin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-21T14:04:18.000Z",
      "submittedOnDailyAt": "2025-03-31T02:13:07.501Z",
      "title": "MedAgent-Pro : Flux de travail d'agent de raisonnement basé sur des preuves pour le diagnostic médical multimodal",
      "submittedOnDailyBy": {
        "_id": "65be4d7d5e342a230dc19a54",
        "avatarUrl": "/avatars/04ba16980da94954d811032f0091212f.svg",
        "isPro": false,
        "fullname": "Ziyue Wang",
        "user": "ZiyueWang",
        "type": "user"
      },
      "summary": "A long term, the development of reliable artificial intelligence (AI) systems that support medical doctors in multimodal diagnostics has been an important goal for researchers. Recently, multimodal language models of images (MLLMs) have received much attention and have been successful in various areas. It is expected that, with their ability to perform various tasks under the guidance of workforce and users, they can improve medical diagnosis. However, directly applying MLLMs in the medical field remains a challenging problem. The lack of detailed image recognition and the limitation in quantitative image analysis are crucial elements in medical diagnosis. Moreover, MLLMs show a lack of continuity and reasoning, which is necessary in medical diagnosis, where strict adherence to existing standards is required. To address these issues, we propose the development of an evidence-based agent system for reliable, interpretable, and precise medical diagnoses, called \"MedAgent-Pro\". This system generates reliable diagnostic plans based on clinical criteria at the work level. At the case level, various tool agents process different types of inputs, analyze different parameters according to the plan, and provide a final diagnosis based on quantitative and interpretative evidence. Detailed experiments in 2D and 3D medical diagnosis tasks demonstrate the excellent efficacy and performance of MedAgent-Pro, and the subset of cases data improve its reliability and interpretability. The code is available at: https://github.com/jinlab-imvr/MedAgent-Pro.",
      "upvotes": 3,
      "discussionId": "67ea0ee07b856e8fa8ff514f",
      "ai_keywords": [
        "Multi-modal Large Language Models (MLLMs)",
        "knowledge-based reasoning",
        "task level",
        "case level",
        "tool agents",
        "multi-modal inputs",
        "diagnostic plans",
        "quantitative analysis",
        "qualitative evidence",
        "hierarchical workflow",
        "evidence-based reasoning",
        "explainable",
        "precise medical diagnoses",
        "superior",
        "effective",
        "reliability",
        "interpretability",
        "clinical criteria"
      ]
    },
    "publishedAt": "2025-03-21T10:04:18.000Z",
    "title": "MedAgent-Pro: Towards Multi-modal Evidence-based Medical Diagnosis via\n  Reasoning Agentic Workflow",
    "summary": "Developing reliable AI systems to assist human clinicians in multi-modal\nmedical diagnosis has long been a key objective for researchers. Recently,\nMulti-modal Large Language Models (MLLMs) have gained significant attention and\nachieved success across various domains. With strong reasoning capabilities and\nthe ability to perform diverse tasks based on user instructions, they hold\ngreat potential for enhancing medical diagnosis. However, directly applying\nMLLMs to the medical domain still presents challenges. They lack detailed\nperception of visual inputs, limiting their ability to perform quantitative\nimage analysis, which is crucial for medical diagnostics. Additionally, MLLMs\noften exhibit hallucinations and inconsistencies in reasoning, whereas clinical\ndiagnoses must adhere strictly to established criteria. To address these\nchallenges, we propose MedAgent-Pro, an evidence-based reasoning agentic system\ndesigned to achieve reliable, explainable, and precise medical diagnoses. This\nis accomplished through a hierarchical workflow: at the task level,\nknowledge-based reasoning generate reliable diagnostic plans for specific\ndiseases following retrieved clinical criteria. While at the case level,\nmultiple tool agents process multi-modal inputs, analyze different indicators\naccording to the plan, and provide a final diagnosis based on both quantitative\nand qualitative evidence. Comprehensive experiments on both 2D and 3D medical\ndiagnosis tasks demonstrate the superiority and effectiveness of MedAgent-Pro,\nwhile case studies further highlight its reliability and interpretability. The\ncode is available at https://github.com/jinlab-imvr/MedAgent-Pro.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18968.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65be4d7d5e342a230dc19a54",
      "avatarUrl": "/avatars/04ba16980da94954d811032f0091212f.svg",
      "fullname": "Ziyue Wang",
      "name": "ZiyueWang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.21779",
      "authors": [
        {
          "_id": "67e69b75113f7c9e552bea69",
          "user": {
            "_id": "660b9dfc8b022f13fdc8db83",
            "avatarUrl": "/avatars/62c68259c8a4d53f121c41a2831cb89a.svg",
            "isPro": false,
            "fullname": "vortexyu",
            "user": "vortex778",
            "type": "user"
          },
          "name": "Weihao Yu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-28T14:22:23.190Z",
          "hidden": false
        },
        {
          "_id": "67e69b75113f7c9e552bea6a",
          "user": {
            "_id": "673969726c12c4b98b6ab29f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/C2elfn7L68jAt4dtHzDAW.png",
            "isPro": false,
            "fullname": "Yuanhao Cai",
            "user": "CaiYuanhao",
            "type": "user"
          },
          "name": "Yuanhao Cai",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-31T09:59:39.476Z",
          "hidden": false
        },
        {
          "_id": "67e69b75113f7c9e552bea6b",
          "name": "Ruyi Zha",
          "hidden": false
        },
        {
          "_id": "67e69b75113f7c9e552bea6c",
          "user": {
            "_id": "6526386e1c6a09292d8d0a22",
            "avatarUrl": "/avatars/471de830de2d775d35368678c1579f87.svg",
            "isPro": false,
            "fullname": "fan",
            "user": "Fanzhiwen",
            "type": "user"
          },
          "name": "Zhiwen Fan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-31T09:59:58.337Z",
          "hidden": false
        },
        {
          "_id": "67e69b75113f7c9e552bea6d",
          "user": {
            "_id": "6421c1cdeaad1bcb28b0e903",
            "avatarUrl": "/avatars/7c720d0e39536a7e49340052f464a80d.svg",
            "isPro": false,
            "fullname": "Chenxin Li",
            "user": "XGGNet",
            "type": "user"
          },
          "name": "Chenxin Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-31T10:00:05.746Z",
          "hidden": false
        },
        {
          "_id": "67e69b75113f7c9e552bea6e",
          "user": {
            "_id": "640fdfc9f2d7c41a1ea112ef",
            "avatarUrl": "/avatars/780328c388ac4bc9acdf063e7833259d.svg",
            "isPro": false,
            "fullname": "yxyuan",
            "user": "yixuanyuan",
            "type": "user"
          },
          "name": "Yixuan Yuan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-31T10:00:15.694Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/660b9dfc8b022f13fdc8db83/Em05evtcueTrruwgHTLlo.gif"
      ],
      "publishedAt": "2025-03-27T17:59:57.000Z",
      "submittedOnDailyAt": "2025-03-31T05:01:34.606Z",
      "title": "X²-Gaussian : Réconstruction de la Position en Temps Continu par Spreading Gaussien Radial en 4 Dimensions",
      "submittedOnDailyBy": {
        "_id": "660b9dfc8b022f13fdc8db83",
        "avatarUrl": "/avatars/62c68259c8a4d53f121c41a2831cb89a.svg",
        "isPro": false,
        "fullname": "vortexyu",
        "user": "vortex778",
        "type": "user"
      },
      "summary": "4D CT (Reconstruction of Image Sections of Calculator in Four Dimensions) esté crucial pour comprendre les changements dynamiques dans la pathologie, mais il a des limitations intrinsèques aux flux de travail traditionnels de classification. Les méthodes actuelles utilisent des dispositifs de gating respiratoire pour diviser les images en segments temporels fixes et intègrent la normalisation radiatique gaussienne pour réaliser la reconstruction 4D-CT en temps réel. Notre approche prédit la déformation gaussienne du temps et modélise la dynamique médicale dans une architecture codificateur-décodificateur espace-temps pour éliminer la segmentation des images. Nous introduisons une perte en accord avec la périodicité physiologique pour éliminer la dépendance des dispositifs de gating externes et optimisons différentiellement pour entraîner directement la périodicité respiratoire du patient. Des expériences extensives montrent un rendement avancé, atteignant un PSNR de 9,93 dB plus que les méthodes traditionnelles et améliorant de 2,25 dB par rapport aux méthodes de normalisation gaussienne pionnières. En intégrant le modèle du temps et l'apprentissage de la périodicité sans utilisation de matériel, X²-Gaussian a considérablement amélioré la reconstruction 4D CT des images médicales dynamiques. Le site web du projet est disponible sur https://x2-gaussian.github.io/.",
      "upvotes": 2,
      "discussionId": "67e69b76113f7c9e552beaa1",
      "projectPage": "https://x2-gaussian.github.io/",
      "githubRepo": "https://github.com/yuyouxixi/x2-gaussian",
      "ai_keywords": [
        "X$^2$-Gaussian",
        "continuous-time 4D-CT",
        "dynamic radiative Gaussian splatting",
        "self-supervised respiratory motion learning",
        "spatiotemporal encoder-decoder architecture",
        "time-varying Gaussian deformations",
        "physiology-driven periodic consistency loss",
        "differentiable optimization",
        "PSNR gain",
        "hardware-free period learning",
        "high-fidelity 4D CT reconstruction"
      ]
    },
    "publishedAt": "2025-03-27T13:59:57.000Z",
    "title": "X^{2}-Gaussian: 4D Radiative Gaussian Splatting for Continuous-time\n  Tomographic Reconstruction",
    "summary": "Four-dimensional computed tomography (4D CT) reconstruction is crucial for\ncapturing dynamic anatomical changes but faces inherent limitations from\nconventional phase-binning workflows. Current methods discretize temporal\nresolution into fixed phases with respiratory gating devices, introducing\nmotion misalignment and restricting clinical practicality. In this paper, We\npropose X^2-Gaussian, a novel framework that enables continuous-time 4D-CT\nreconstruction by integrating dynamic radiative Gaussian splatting with\nself-supervised respiratory motion learning. Our approach models anatomical\ndynamics through a spatiotemporal encoder-decoder architecture that predicts\ntime-varying Gaussian deformations, eliminating phase discretization. To remove\ndependency on external gating devices, we introduce a physiology-driven\nperiodic consistency loss that learns patient-specific breathing cycles\ndirectly from projections via differentiable optimization. Extensive\nexperiments demonstrate state-of-the-art performance, achieving a 9.93 dB PSNR\ngain over traditional methods and 2.25 dB improvement against prior Gaussian\nsplatting techniques. By unifying continuous motion modeling with hardware-free\nperiod learning, X^2-Gaussian advances high-fidelity 4D CT reconstruction for\ndynamic clinical imaging. Project website at: https://x2-gaussian.github.io/.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/660b9dfc8b022f13fdc8db83/Em05evtcueTrruwgHTLlo.gif"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.21779.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "660b9dfc8b022f13fdc8db83",
      "avatarUrl": "/avatars/62c68259c8a4d53f121c41a2831cb89a.svg",
      "fullname": "vortexyu",
      "name": "vortex778",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.21851",
      "authors": [
        {
          "_id": "67ea45e0cdd38e64b1134ec3",
          "user": {
            "_id": "633f243c13e836a0fc507388",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/633f243c13e836a0fc507388/Td8TWn1q2L78sZN9098AO.jpeg",
            "isPro": false,
            "fullname": "Alessandro Conti",
            "user": "altndrr",
            "type": "user"
          },
          "name": "Alessandro Conti",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-31T08:11:29.043Z",
          "hidden": false
        },
        {
          "_id": "67ea45e0cdd38e64b1134ec4",
          "user": {
            "_id": "62cf293d3200bfd438e81f1f",
            "avatarUrl": "/avatars/608c19ee375ef091ca77d7cfbc40e76e.svg",
            "isPro": false,
            "fullname": "Massimiliano Mancini",
            "user": "massimilianom",
            "type": "user"
          },
          "name": "Massimiliano Mancini",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-31T10:01:57.391Z",
          "hidden": false
        },
        {
          "_id": "67ea45e0cdd38e64b1134ec5",
          "name": "Enrico Fini",
          "hidden": false
        },
        {
          "_id": "67ea45e0cdd38e64b1134ec6",
          "name": "Yiming Wang",
          "hidden": false
        },
        {
          "_id": "67ea45e0cdd38e64b1134ec7",
          "user": {
            "_id": "62f3b1ea81861bd9bc5c5538",
            "avatarUrl": "/avatars/0aef9ac5bfa91b9894166fe3c29925da.svg",
            "isPro": false,
            "fullname": "Paolo Rota",
            "user": "paolorota",
            "type": "user"
          },
          "name": "Paolo Rota",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-31T10:02:22.544Z",
          "hidden": false
        },
        {
          "_id": "67ea45e0cdd38e64b1134ec8",
          "name": "Elisa Ricci",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-27T17:03:18.000Z",
      "submittedOnDailyAt": "2025-03-31T06:13:51.445Z",
      "title": "Grande multi-modèle pour la classification d'images dans le monde entier",
      "submittedOnDailyBy": {
        "_id": "633f243c13e836a0fc507388",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/633f243c13e836a0fc507388/Td8TWn1q2L78sZN9098AO.jpeg",
        "isPro": false,
        "fullname": "Alessandro Conti",
        "user": "altndrr",
        "type": "user"
      },
      "summary": "La classification des images traditionnelle nécessite une liste de catégories sémantiques prédéfinies. En contraste, les grands modèles (LMMs) peuvent classifier directement les images en utilisant le langage naturel, évitant ainsi ces exigences (par exemple, répondant à des questions comme \"Qu'est-ce la principale figure dans l'image ?\"). L'étude existante sur le rendement de classification des LMMs a été concentrée sur un intervalle limité, généralement en supposant un ensemble de catégories prédéfinies. Cet article aborde ces limitations en évaluant en détail le rendement de classification des LMMs dans un environnement ouvert. Tout d'abord, les tâches sont formalisées et différentes métriques sont définies pour évaluer la concordance entre les prédictions et les classes réelles en utilisant des protocoles d'évaluation. Ensuite, 13 modèles sont évalués dans 10 référentiels, montrant les problèmes auxquels sont confrontés les LMMs, y compris des classes circulaires, non circulaires, spécifiques et très spécifiques. Sur la base des métriques proposées, une amélioration est démontrée et les types d'erreurs produits par les LMMs sont identifiés, soulignant particulièrement les problèmes liés à la spécification et aux classes très spécifiques, et montrant comment les prompts typés et la logique peuvent résoudre ces problèmes.",
      "upvotes": 1,
      "discussionId": "67ea45e1cdd38e64b1134f35",
      "githubRepo": "https://github.com/altndrr/lmms-owc",
      "ai_keywords": [
        "Large Multimodal Models (LMMs)",
        "open-world setting",
        "evaluation protocol",
        "metrics",
        "alignment between predicted and ground truth classes",
        "prototypical",
        "non-prototypical",
        "fine-grained",
        "very fine-grained classes",
        "granularity",
        "fine-grained capabilities",
        "tailored prompting",
        "reasoning"
      ]
    },
    "publishedAt": "2025-03-27T13:03:18.000Z",
    "title": "On Large Multimodal Models as Open-World Image Classifiers",
    "summary": "Traditional image classification requires a predefined list of semantic\ncategories. In contrast, Large Multimodal Models (LMMs) can sidestep this\nrequirement by classifying images directly using natural language (e.g.,\nanswering the prompt \"What is the main object in the image?\"). Despite this\nremarkable capability, most existing studies on LMM classification performance\nare surprisingly limited in scope, often assuming a closed-world setting with a\npredefined set of categories. In this work, we address this gap by thoroughly\nevaluating LMM classification performance in a truly open-world setting. We\nfirst formalize the task and introduce an evaluation protocol, defining various\nmetrics to assess the alignment between predicted and ground truth classes. We\nthen evaluate 13 models across 10 benchmarks, encompassing prototypical,\nnon-prototypical, fine-grained, and very fine-grained classes, demonstrating\nthe challenges LMMs face in this task. Further analyses based on the proposed\nmetrics reveal the types of errors LMMs make, highlighting challenges related\nto granularity and fine-grained capabilities, showing how tailored prompting\nand reasoning can alleviate them.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.21851.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "633f243c13e836a0fc507388",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/633f243c13e836a0fc507388/Td8TWn1q2L78sZN9098AO.jpeg",
      "fullname": "Alessandro Conti",
      "name": "altndrr",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.20308",
      "authors": [
        {
          "_id": "67ea2b9a676ae1ad3402eede",
          "user": {
            "_id": "67ea28b89f3eff13b78260ca",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/9Azj8WDU1XAG2Ihrbc4Mb.jpeg",
            "isPro": false,
            "fullname": "Lee Chae-Yeon",
            "user": "Chae-Yeon",
            "type": "user"
          },
          "name": "Lee Chae-Yeon",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-31T08:11:33.799Z",
          "hidden": false
        },
        {
          "_id": "67ea2b9a676ae1ad3402eedf",
          "name": "Oh Hyun-Bin",
          "hidden": false
        },
        {
          "_id": "67ea2b9a676ae1ad3402eee0",
          "user": {
            "_id": "65067ef0d8d96e913b3213ee",
            "avatarUrl": "/avatars/91732e9cead404fc18e11aa339641f6d.svg",
            "isPro": false,
            "fullname": "Han EunGi",
            "user": "Han-EunGi",
            "type": "user"
          },
          "name": "Han EunGi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-31T10:02:59.233Z",
          "hidden": false
        },
        {
          "_id": "67ea2b9a676ae1ad3402eee1",
          "user": {
            "_id": "66d0986b9678056278ce86f2",
            "avatarUrl": "/avatars/816cd3fad6c11c7232ea10c9899fa016.svg",
            "isPro": false,
            "fullname": "KIM SUNGBIN",
            "user": "backryun",
            "type": "user"
          },
          "name": "Kim Sung-Bin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-31T10:03:07.568Z",
          "hidden": false
        },
        {
          "_id": "67ea2b9a676ae1ad3402eee2",
          "user": {
            "_id": "6760e12288be0baf4b1196f2",
            "avatarUrl": "/avatars/487631d07b0ab439778836dfcd12dfe4.svg",
            "isPro": false,
            "fullname": "suekyeong nam",
            "user": "akasha9890",
            "type": "user"
          },
          "name": "Suekyeong Nam",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-31T10:03:13.804Z",
          "hidden": false
        },
        {
          "_id": "67ea2b9a676ae1ad3402eee3",
          "user": {
            "_id": "674622d01310ed05c6c5a5aa",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/0ga3pvGwd8oTEEWxIuPr6.png",
            "isPro": false,
            "fullname": "Tae-Hyun Oh",
            "user": "taehyunoh",
            "type": "user"
          },
          "name": "Tae-Hyun Oh",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-31T10:03:19.362Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-26T08:18:57.000Z",
      "submittedOnDailyAt": "2025-03-31T06:59:13.941Z",
      "title": "L'expérience précise de la génération de têtes de conversation 3D : nouvelle définition, représentation de grille d'audio et indicateurs d'évaluation",
      "submittedOnDailyBy": {
        "_id": "67ea28b89f3eff13b78260ca",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/9Azj8WDU1XAG2Ihrbc4Mb.jpeg",
        "isPro": false,
        "fullname": "Lee Chae-Yeon",
        "user": "Chae-Yeon",
        "type": "user"
      },
      "summary": "Récemment, la technologie de génération de visages 3D par voix a montré un progrès dans le mouvement des lèvres. Cependant, les modèles actuels ne peuvent pas capturer la motivation visuelle du mouvement des lèvres en réponse aux caractéristiques variées du son. Dans cet article, nous présentons trois critères importants pour atteindre des mouvements visuellement précis des lèvres : la motivation temporelle, la lisibilité des lèvres et l'expression. Nous supposons l'existence d'un espace de représentation adéquat qui satisfait ces critères, et introduisons une représentation de synchronisation du son pour comprendre la relation entre le son complexe et l'ajustement 3D de la face. La représentation apprise montre des caractéristiques désirées et permet de synchroniser plus précisément le mouvement des lèvres avec le son, en ajoutant une perte visuelle aux modèles actuels. De plus, nous utilisons cette représentation comme critère d'évaluation visuelle pour évaluer la concordance avec ces trois critères, introduisant deux indicateurs physiques d'évaluation basés sur la synchronisation des lèvres. Les expériences utilisent la perte visuelle pour entraîner un modèle de génération de visages 3D du son, montrant des avancées dans trois aspects visuels de la synchronisation des lèvres. Les codes et ensembles de données sont disponibles sur https://perceptual-3d-talking-head.github.io/.",
      "upvotes": 1,
      "discussionId": "67ea2b9b676ae1ad3402ef63",
      "ai_keywords": [
        "speech-mesh synchronized representation",
        "perceptual loss",
        "perceptual metric",
        "lip synchronization metrics",
        "Temporal Synchronization",
        "Lip Readability",
        "Expressiveness",
        "3D talking head generation",
        "lip movements",
        "speech signals",
        "3D face meshes"
      ]
    },
    "publishedAt": "2025-03-26T04:18:57.000Z",
    "title": "Perceptually Accurate 3D Talking Head Generation: New Definitions,\n  Speech-Mesh Representation, and Evaluation Metrics",
    "summary": "Recent advancements in speech-driven 3D talking head generation have made\nsignificant progress in lip synchronization. However, existing models still\nstruggle to capture the perceptual alignment between varying speech\ncharacteristics and corresponding lip movements. In this work, we claim that\nthree criteria -- Temporal Synchronization, Lip Readability, and Expressiveness\n-- are crucial for achieving perceptually accurate lip movements. Motivated by\nour hypothesis that a desirable representation space exists to meet these three\ncriteria, we introduce a speech-mesh synchronized representation that captures\nintricate correspondences between speech signals and 3D face meshes. We found\nthat our learned representation exhibits desirable characteristics, and we plug\nit into existing models as a perceptual loss to better align lip movements to\nthe given speech. In addition, we utilize this representation as a perceptual\nmetric and introduce two other physically grounded lip synchronization metrics\nto assess how well the generated 3D talking heads align with these three\ncriteria. Experiments show that training 3D talking head generation models with\nour perceptual loss significantly improve all three aspects of perceptually\naccurate lip synchronization. Codes and datasets are available at\nhttps://perceptual-3d-talking-head.github.io/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20308.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67ea28b89f3eff13b78260ca",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/9Azj8WDU1XAG2Ihrbc4Mb.jpeg",
      "fullname": "Lee Chae-Yeon",
      "name": "Chae-Yeon",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.21751",
      "authors": [
        {
          "_id": "67e6d76a3394f1ed9c9d804b",
          "user": {
            "_id": "66e1103cde9aca0f831f05d8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66e1103cde9aca0f831f05d8/tkdrZgBH9Kyxv_VrGK05R.png",
            "isPro": false,
            "fullname": "Yan XIA",
            "user": "IsshikiHugh",
            "type": "user"
          },
          "name": "Yan Xia",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-31T08:14:23.016Z",
          "hidden": false
        },
        {
          "_id": "67e6d76a3394f1ed9c9d804c",
          "name": "Xiaowei Zhou",
          "hidden": false
        },
        {
          "_id": "67e6d76a3394f1ed9c9d804d",
          "name": "Etienne Vouga",
          "hidden": false
        },
        {
          "_id": "67e6d76a3394f1ed9c9d804e",
          "name": "Qixing Huang",
          "hidden": false
        },
        {
          "_id": "67e6d76a3394f1ed9c9d804f",
          "user": {
            "_id": "6478d3433b7f8b1f6249b469",
            "avatarUrl": "/avatars/11e7d7a94ae26500c1c2ad62e760726f.svg",
            "isPro": false,
            "fullname": "Georgios Pavlakos",
            "user": "geopavlakos",
            "type": "user"
          },
          "name": "Georgios Pavlakos",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-31T10:03:56.262Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-27T17:56:24.000Z",
      "submittedOnDailyAt": "2025-03-31T08:28:45.743Z",
      "title": "Reconstruise l'humanité selon son squelette osseux avec précision, en maintenant l'exactitude de la biomécanique.",
      "submittedOnDailyBy": {
        "_id": "66e1103cde9aca0f831f05d8",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66e1103cde9aca0f831f05d8/tkdrZgBH9Kyxv_VrGK05R.png",
        "isPro": false,
        "fullname": "Yan XIA",
        "user": "IsshikiHugh",
        "type": "user"
      },
      "summary": "Dans cet article, nous présentons un méthode pour reconstruire des corps 3D à partir d'une seule image, en utilisant des modèles de squelette mécaniquement précis. Pour y parvenir, nous utilisons l'image comme entrée et nous entraînons un modèle pour estimer les paramètres du modèle. Pour résoudre ce problème, nous construisons une chaîne d'opérations qui génère les paramètres du modèle à partir de données réelles d'images, et nous implémentons un processus d'entraînement qui est répété pour affiner ces étiquettes. Comparé aux méthodes les plus récentes de reconstruction 3D de corps, notre modèle atteint un niveau de performance compétitif dans les référentiels standards et dépasse considérablement les positions 3D extrêmes et les configurations de perspective. De plus, notre modèle montre que les méthodes de reconstruction existantes violen les contraintes d'angles articulaires et induisent des rotations non naturelles. En contraste, notre approche utilise une liberté mécanique appropriée pour estimer la rotation des articulations de manière plus naturelle. Notre approche est validée sur plusieurs référentiels de positionnement corporel. Le code, le modèle et les données sont disponibles sur l'URL suivant : https://isshikihugh.github.io/HSMR/",
      "upvotes": 0,
      "discussionId": "67e6d76c3394f1ed9c9d80c4",
      "projectPage": "https://isshikihugh.github.io/HSMR/",
      "githubRepo": "https://github.com/IsshikiHugh/HSMR",
      "ai_keywords": [
        "transformer",
        "biomechanically accurate skeleton model",
        "pseudo ground truth",
        "iterative refinement",
        "state-of-the-art methods",
        "3D human mesh recovery",
        "Standard benchmarks",
        "extreme 3D poses",
        "viewpoints",
        "joint angle limits",
        "biomechanically plausible degrees of freedom",
        "human pose estimation benchmarks"
      ]
    },
    "publishedAt": "2025-03-27T13:56:24.000Z",
    "title": "Reconstructing Humans with a Biomechanically Accurate Skeleton",
    "summary": "In this paper, we introduce a method for reconstructing 3D humans from a\nsingle image using a biomechanically accurate skeleton model. To achieve this,\nwe train a transformer that takes an image as input and estimates the\nparameters of the model. Due to the lack of training data for this task, we\nbuild a pipeline to produce pseudo ground truth model parameters for single\nimages and implement a training procedure that iteratively refines these pseudo\nlabels. Compared to state-of-the-art methods for 3D human mesh recovery, our\nmodel achieves competitive performance on standard benchmarks, while it\nsignificantly outperforms them in settings with extreme 3D poses and\nviewpoints. Additionally, we show that previous reconstruction methods\nfrequently violate joint angle limits, leading to unnatural rotations. In\ncontrast, our approach leverages the biomechanically plausible degrees of\nfreedom making more realistic joint rotation estimates. We validate our\napproach across multiple human pose estimation benchmarks. We make the code,\nmodels and data available at: https://isshikihugh.github.io/HSMR/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.21751.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66e1103cde9aca0f831f05d8",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66e1103cde9aca0f831f05d8/tkdrZgBH9Kyxv_VrGK05R.png",
      "fullname": "Yan XIA",
      "name": "IsshikiHugh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]