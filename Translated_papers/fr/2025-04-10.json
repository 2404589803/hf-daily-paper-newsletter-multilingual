[
  {
    "paper": {
      "id": "2504.05741",
      "authors": [
        {
          "_id": "67f726dc0b5aa5777fd3a431",
          "user": {
            "_id": "66615c855fd9d736e670e0a9",
            "avatarUrl": "/avatars/0ff3127b513552432a7c651e21d7f283.svg",
            "isPro": false,
            "fullname": "wangshuai",
            "user": "wangsssssss",
            "type": "user"
          },
          "name": "Shuai Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-10T06:44:49.192Z",
          "hidden": false
        },
        {
          "_id": "67f726dc0b5aa5777fd3a432",
          "name": "Zhi Tian",
          "hidden": false
        },
        {
          "_id": "67f726dc0b5aa5777fd3a433",
          "name": "Weilin Huang",
          "hidden": false
        },
        {
          "_id": "67f726dc0b5aa5777fd3a434",
          "user": {
            "_id": "62c77f4352d8ae531f5511f9",
            "avatarUrl": "/avatars/50198ccb02ccd286975a4613fbabee28.svg",
            "isPro": false,
            "fullname": "Limin Wang",
            "user": "lmwang",
            "type": "user"
          },
          "name": "Limin Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T07:58:42.903Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-08T07:17:45.000Z",
      "submittedOnDailyAt": "2025-04-10T00:40:02.945Z",
      "title": "DDT: Séparation DIFUJENTRANSFORMER",
      "submittedOnDailyBy": {
        "_id": "66615c855fd9d736e670e0a9",
        "avatarUrl": "/avatars/0ff3127b513552432a7c651e21d7f283.svg",
        "isPro": false,
        "fullname": "wangshuai",
        "user": "wangsssssss",
        "type": "user"
      },
      "summary": "Les transformateurs de diffusion montrent une excellente qualité de génération, mais nécessitent de longues itérations d'entraînement et plusieurs étapes d'inférence. À chaque étape de bruit, les transformateurs de diffusion codifient l'entrée avec du bruit pour extraire des composantes significatives de basse fréquence et décodifient les composantes de haute fréquence avec le même module. Cette méthodologie pose des difficultés propres à l'optimisation : pour codifier des composantes significatives de basse fréquence, il est nécessaire de réduire les composantes de haute fréquence, ce qui génère une tension entre le codifié significatif et le décodifié de haute fréquence. Pour résoudre ce problème, nous proposons un nouveau transformateur de diffusion, DDT-XL/2, qui a une architecture séparée pour l'extraction de composantes significatives et la décodification de vitesse. Nos expériences montrent que lorsque le modèle est plus grand, un codifieur plus puissant améliore le rendement. Sur ImageNet 256x256, notre DDT-XL/2 atteint une convergence d'entraînement approchéement 4 fois plus rapide que les transformateurs de diffusion précédents et atteint un nouveau rendement optimal. Sur ImageNet 512x512, notre DDT-XL/2 atteint un nouveau FID optimal de 1,28. De plus, comme le développement lent, l'architecture séparée améliore la vitesse d'inférence en partageant des conditions entre étapes de bruit adjacentes. Pour minimiser le détérioration de la qualité, nous proposons une nouvelle stratégie statistique et dynamique pour déterminer la meilleure manière de partager ces conditions.",
      "upvotes": 34,
      "discussionId": "67f726dd0b5aa5777fd3a463",
      "githubRepo": "https://github.com/MCG-NJU/DDT"
    },
    "publishedAt": "2025-04-08T03:17:45.000Z",
    "title": "DDT: Decoupled Diffusion Transformer",
    "summary": "Diffusion transformers have demonstrated remarkable generation quality,\nalbeit requiring longer training iterations and numerous inference steps. In\neach denoising step, diffusion transformers encode the noisy inputs to extract\nthe lower-frequency semantic component and then decode the higher frequency\nwith identical modules. This scheme creates an inherent optimization dilemma:\nencoding low-frequency semantics necessitates reducing high-frequency\ncomponents, creating tension between semantic encoding and high-frequency\ndecoding. To resolve this challenge, we propose a new\n\\color{ddtD}ecoupled \\color{ddtD}iffusion\n\\color{ddtT}ransformer~(\\color{ddtDDT}), with a decoupled\ndesign of a dedicated condition encoder for semantic extraction alongside a\nspecialized velocity decoder. Our experiments reveal that a more substantial\nencoder yields performance improvements as model size increases. For ImageNet\n256times256, Our DDT-XL/2 achieves a new state-of-the-art performance of\n{1.31 FID}~(nearly 4times faster training convergence compared to previous\ndiffusion transformers). For ImageNet 512times512, Our DDT-XL/2 achieves a\nnew state-of-the-art FID of 1.28. Additionally, as a beneficial by-product, our\ndecoupled architecture enhances inference speed by enabling the sharing\nself-condition between adjacent denoising steps. To minimize performance\ndegradation, we propose a novel statistical dynamic programming approach to\nidentify optimal sharing strategies.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.05741.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66615c855fd9d736e670e0a9",
      "avatarUrl": "/avatars/0ff3127b513552432a7c651e21d7f283.svg",
      "fullname": "wangshuai",
      "name": "wangsssssss",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.07083",
      "authors": [
        {
          "_id": "67f72c452eec6ce5c8b9e8e6",
          "user": {
            "_id": "64de20c5808492ba6e65d124",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64de20c5808492ba6e65d124/58IX_TI5vJw73qS1knw56.jpeg",
            "isPro": false,
            "fullname": "Zhang Mengchen",
            "user": "Dubhe-zmc",
            "type": "user"
          },
          "name": "Mengchen Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-10T06:39:42.813Z",
          "hidden": false
        },
        {
          "_id": "67f72c452eec6ce5c8b9e8e7",
          "name": "Tong Wu",
          "hidden": false
        },
        {
          "_id": "67f72c452eec6ce5c8b9e8e8",
          "user": {
            "_id": "65367c40061949598892dbdc",
            "avatarUrl": "/avatars/4baf27263841471cbd5f629a8b99424d.svg",
            "isPro": false,
            "fullname": "Jing Tan",
            "user": "jingtan",
            "type": "user"
          },
          "name": "Jing Tan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:01:54.122Z",
          "hidden": false
        },
        {
          "_id": "67f72c452eec6ce5c8b9e8e9",
          "user": {
            "_id": "62ab1ac1d48b4d8b048a3473",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656826685333-62ab1ac1d48b4d8b048a3473.png",
            "isPro": false,
            "fullname": "Ziwei Liu",
            "user": "liuziwei7",
            "type": "user"
          },
          "name": "Ziwei Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:01:11.166Z",
          "hidden": false
        },
        {
          "_id": "67f72c452eec6ce5c8b9e8ea",
          "user": {
            "_id": "6694e583ac96ca2c17131505",
            "avatarUrl": "/avatars/6e7a31f257e36cf301da6f879dc0a122.svg",
            "isPro": false,
            "fullname": "Gordon Wetzstein",
            "user": "wetzste1",
            "type": "user"
          },
          "name": "Gordon Wetzstein",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:01:03.935Z",
          "hidden": false
        },
        {
          "_id": "67f72c452eec6ce5c8b9e8eb",
          "user": {
            "_id": "636317ed80c1a705a6eff396",
            "avatarUrl": "/avatars/3db090e101b916d9256d0d3e043db71d.svg",
            "isPro": false,
            "fullname": "Dahua Lin",
            "user": "lindahua",
            "type": "user"
          },
          "name": "Dahua Lin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:00:57.092Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64de20c5808492ba6e65d124/b1N08r8EbruYc8Yapg4J9.qt"
      ],
      "publishedAt": "2025-04-09T17:56:01.000Z",
      "submittedOnDailyAt": "2025-04-10T01:13:43.884Z",
      "title": "GenDoP : Générateur automatique d'entraîneur de caméras de portrait pour directeurs de photographie",
      "submittedOnDailyBy": {
        "_id": "64de20c5808492ba6e65d124",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64de20c5808492ba6e65d124/58IX_TI5vJw73qS1knw56.jpeg",
        "isPro": false,
        "fullname": "Zhang Mengchen",
        "user": "Dubhe-zmc",
        "type": "user"
      },
      "summary": "Le design de la trajectoire de la caméra joue un rôle important dans la production de films et est une outil de base pour transmettre l'intention du réalisateur, améliorant ainsi la transmission des histoires visuelles. Lors de la prise de vue, les photographes-réalisateurs planifient avec précision le mouvement de la caméra pour atteindre des phrases de photographie expressives et intentionnelles. Cependant, la génération de la trajectoire de la caméra est limitée par les méthodes actuelles : les approches traditionnelles basées sur l'optimisation géométrique ou les systèmes de processus conçus directement, et les méthodes basées sur l'apprentissage récent qui tendent vers le biais structurel et limitent la créativité en ne coincidant pas suffisamment avec le contexte. Dans cet article, nous présentons une méthodologie pour générer des trajectoires de caméra artistiques et expressives basée sur un modèle de régression automatique qui s'appuie sur le savoir des photographes-réalisateurs. Tout d'abord, nous présentons une grande base de données multi-type qui inclut 29K images réelles, DataDoP. Cette base de données intègre des trajectoires de mouvement libre, des cartes de profondeur, des actions spécifiques, des interactions avec l'espace et des captures détaillées de l'intention du réalisateur. Grâce à cette information détaillée, nous développons un décodeur automatique et rétroactionnel, Transformer, pour générer des mouvements de caméra ordonnés et contextuels basés sur des guides de texte et des entrées RGBD, qu'on appelle GenDoP. Les expériences étendues montrent que GenDoP offre une plus grande possibilité de contrôle, un meilleur ajustement de la trajectoire et une plus grande stabilité du mouvement par rapport aux méthodes existantes. Nous pensons que cette méthodologie établira un nouveau standard pour la prise de vue basée sur l'apprentissage et estimons qu'elle encouragera le développement futur du contrôle de la caméra et de la production cinématographique. Le site web du projet est disponible sur https://kszpxxzmc.github.io/GenDoP/.",
      "upvotes": 16,
      "discussionId": "67f72c472eec6ce5c8b9e97b",
      "projectPage": "https://kszpxxzmc.github.io/GenDoP/",
      "githubRepo": "https://github.com/3DTopia/GenDoP"
    },
    "publishedAt": "2025-04-09T13:56:01.000Z",
    "title": "GenDoP: Auto-regressive Camera Trajectory Generation as a Director of\n  Photography",
    "summary": "Camera trajectory design plays a crucial role in video production, serving as\na fundamental tool for conveying directorial intent and enhancing visual\nstorytelling. In cinematography, Directors of Photography meticulously craft\ncamera movements to achieve expressive and intentional framing. However,\nexisting methods for camera trajectory generation remain limited: Traditional\napproaches rely on geometric optimization or handcrafted procedural systems,\nwhile recent learning-based methods often inherit structural biases or lack\ntextual alignment, constraining creative synthesis. In this work, we introduce\nan auto-regressive model inspired by the expertise of Directors of Photography\nto generate artistic and expressive camera trajectories. We first introduce\nDataDoP, a large-scale multi-modal dataset containing 29K real-world shots with\nfree-moving camera trajectories, depth maps, and detailed captions in specific\nmovements, interaction with the scene, and directorial intent. Thanks to the\ncomprehensive and diverse database, we further train an auto-regressive,\ndecoder-only Transformer for high-quality, context-aware camera movement\ngeneration based on text guidance and RGBD inputs, named GenDoP. Extensive\nexperiments demonstrate that compared to existing methods, GenDoP offers better\ncontrollability, finer-grained trajectory adjustments, and higher motion\nstability. We believe our approach establishes a new standard for\nlearning-based cinematography, paving the way for future advancements in camera\ncontrol and filmmaking. Our project website:\nhttps://kszpxxzmc.github.io/GenDoP/.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64de20c5808492ba6e65d124/b1N08r8EbruYc8Yapg4J9.qt"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07083.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64de20c5808492ba6e65d124",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64de20c5808492ba6e65d124/58IX_TI5vJw73qS1knw56.jpeg",
      "fullname": "Zhang Mengchen",
      "name": "Dubhe-zmc",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.07096",
      "authors": [
        {
          "_id": "67f72bb1f9d51b79dca06d0a",
          "user": {
            "_id": "635f46d1928a42bc95cfcf7c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/635f46d1928a42bc95cfcf7c/5KF8aLiDCJdl7B1SdJ-7V.png",
            "isPro": false,
            "fullname": "Jiacheng Liu",
            "user": "liujch1998",
            "type": "user"
          },
          "name": "Jiacheng Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-10T06:39:44.913Z",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d0b",
          "user": {
            "_id": "6675a65557208377a15f745b",
            "avatarUrl": "/avatars/361dc6d0919f4d4545ff4fdd005332b5.svg",
            "isPro": false,
            "fullname": "Taylor Blanton",
            "user": "taylorb",
            "type": "user"
          },
          "name": "Taylor Blanton",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:02:05.681Z",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d0c",
          "user": {
            "_id": "623ca115a795593324c4353f",
            "avatarUrl": "/avatars/bf11fe728df2786d52ed4d2de12b48d3.svg",
            "isPro": false,
            "fullname": "Yanai Elazar",
            "user": "yanaiela",
            "type": "user"
          },
          "name": "Yanai Elazar",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:02:12.016Z",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d0d",
          "user": {
            "_id": "63a76d0de27a6dbd485fe863",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a76d0de27a6dbd485fe863/qJJwHOuvyQGq1o0KscOF_.jpeg",
            "isPro": false,
            "fullname": "Sewon Min",
            "user": "sewon",
            "type": "user"
          },
          "name": "Sewon Min",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:02:17.909Z",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d0e",
          "user": {
            "_id": "6697093a37d2483826562c24",
            "avatarUrl": "/avatars/0e526b4be6db07e2485f7ef862080339.svg",
            "isPro": false,
            "fullname": "Chen",
            "user": "Yensung",
            "type": "user"
          },
          "name": "YenSung Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:02:26.950Z",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d0f",
          "name": "Arnavi Chheda-Kothary",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d10",
          "name": "Huy Tran",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d11",
          "name": "Byron Bischoff",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d12",
          "name": "Eric Marsh",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d13",
          "name": "Michael Schmitz",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d14",
          "name": "Cassidy Trier",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d15",
          "user": {
            "_id": "65b1520bf7638a13a641a620",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65b1520bf7638a13a641a620/KTauZL0kXlmYnbkI2lFBG.png",
            "isPro": false,
            "fullname": "Aaron Sarnat",
            "user": "aaronsarnat",
            "type": "user"
          },
          "name": "Aaron Sarnat",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:03:34.212Z",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d16",
          "name": "Jenna James",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d17",
          "name": "Jon Borchardt",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d18",
          "user": {
            "_id": "65316953791d5a2611426c20",
            "avatarUrl": "/avatars/e632a9a30a57f62d59f9fe42eba8fd7d.svg",
            "isPro": false,
            "fullname": "bailey kuehl",
            "user": "baileyk",
            "type": "user"
          },
          "name": "Bailey Kuehl",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:03:47.506Z",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d19",
          "name": "Evie Cheng",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d1a",
          "user": {
            "_id": "66213c05e288b64070184cac",
            "avatarUrl": "/avatars/ded6a173e60722200b372b8b046fc359.svg",
            "isPro": false,
            "fullname": "Karen Farley",
            "user": "AI2Karen",
            "type": "user"
          },
          "name": "Karen Farley",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:03:58.149Z",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d1b",
          "name": "Sruthi Sreeram",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d1c",
          "user": {
            "_id": "65de20ad4e73a7dea7fb4f08",
            "avatarUrl": "/avatars/f3b0ad6cc9417e8ea3f0607fa62824d1.svg",
            "isPro": false,
            "fullname": "Taira Anderson",
            "user": "tairaa",
            "type": "user"
          },
          "name": "Taira Anderson",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:04:09.193Z",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d1d",
          "name": "David Albright",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d1e",
          "user": {
            "_id": "6024546dc1f3c79f98e4b384",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1612993792778-6024546dc1f3c79f98e4b384.jpeg",
            "isPro": false,
            "fullname": "Carissa Schoenick",
            "user": "CarissaS",
            "type": "user"
          },
          "name": "Carissa Schoenick",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:04:25.492Z",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d1f",
          "user": {
            "_id": "5f04d8c45d08220171a0ad32",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5f04d8c45d08220171a0ad32/uXEta6nqBabrUlAOXnS5g.jpeg",
            "isPro": false,
            "fullname": "Luca Soldaini",
            "user": "soldni",
            "type": "user"
          },
          "name": "Luca Soldaini",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:04:31.958Z",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d20",
          "user": {
            "_id": "60369745413a78f892e7339c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1636671879171-60369745413a78f892e7339c.png",
            "isPro": false,
            "fullname": "Dirk Groeneveld",
            "user": "dirkgr",
            "type": "user"
          },
          "name": "Dirk Groeneveld",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:04:40.029Z",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d21",
          "name": "Rock Yuren Pang",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d22",
          "user": {
            "_id": "641b4263abfce26bcf7b27de",
            "avatarUrl": "/avatars/e91b4205e4f74b0dd8c333c23203a924.svg",
            "isPro": false,
            "fullname": "Pang Wei Koh",
            "user": "pangwei",
            "type": "user"
          },
          "name": "Pang Wei Koh",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:04:53.298Z",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d23",
          "name": "Noah A. Smith",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d24",
          "user": {
            "_id": "65b301f04c9e50e74a893954",
            "avatarUrl": "/avatars/f52366959f9e7613576603c0272ff2c5.svg",
            "isPro": false,
            "fullname": "Sophie Lebrecht",
            "user": "Lebrechts",
            "type": "user"
          },
          "name": "Sophie Lebrecht",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:05:06.279Z",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d25",
          "user": {
            "_id": "64d42729f63b01b7f676b176",
            "avatarUrl": "/avatars/52e54bdd6a1fb6c774a40cd70f3d7925.svg",
            "isPro": false,
            "fullname": "Yejin Choi",
            "user": "yejinchoinka",
            "type": "user"
          },
          "name": "Yejin Choi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:05:13.983Z",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d26",
          "name": "Hannaneh Hajishirzi",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d27",
          "user": {
            "_id": "6660d4c1818c5c5ca0f31266",
            "avatarUrl": "/avatars/1d2972894cb3b9df1900fdb162d9c364.svg",
            "isPro": false,
            "fullname": "alifarhadi ",
            "user": "alifarhadi051",
            "type": "user"
          },
          "name": "Ali Farhadi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:05:24.948Z",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d28",
          "user": {
            "_id": "6283f38567d336d3e5d5280e",
            "avatarUrl": "/avatars/d0a54aaec74a90b050e671c191b87a80.svg",
            "isPro": false,
            "fullname": "Jesse Dodge",
            "user": "JesseDodge",
            "type": "user"
          },
          "name": "Jesse Dodge",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:05:31.942Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-09T17:59:35.000Z",
      "submittedOnDailyAt": "2025-04-10T00:54:36.448Z",
      "title": "OLMoTrace : 100 millions de tokens d'entraînement sont suivis à travers la génération de modèles de langue subséquents.",
      "submittedOnDailyBy": {
        "_id": "635f46d1928a42bc95cfcf7c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/635f46d1928a42bc95cfcf7c/5KF8aLiDCJdl7B1SdJ-7V.png",
        "isPro": false,
        "fullname": "Jiacheng Liu",
        "user": "liujch1998",
        "type": "user"
      },
      "summary": "OLMoTrace est le premier système qui rétrospecte en temps réel les sorties d'un modèle de langue pour tous les tokens de toute la donnée d'entraînement de multillion. OLMoTrace détecte et montre la précise correspondance entre les sorties du modèle de langue et les fragments de documents des textes d'entraînement. Ceci est une extension de infani-gram, renvoyant des résultats de recherche en quelques secondes. OLMoTrace aide à comprendre l'action du modèle de langue à travers une vision de la donnée d'entraînement. Ce système présente des méthodes de vérification de faits, de comparaison de chaînes et d'étude de la créativité du modèle de langue. OLMoTrace est disponible pour l'utilisation publique et est complètement open-source.",
      "upvotes": 14,
      "discussionId": "67f72bb3f9d51b79dca06d8c"
    },
    "publishedAt": "2025-04-09T13:59:35.000Z",
    "title": "OLMoTrace: Tracing Language Model Outputs Back to Trillions of Training\n  Tokens",
    "summary": "We present OLMoTrace, the first system that traces the outputs of language\nmodels back to their full, multi-trillion-token training data in real time.\nOLMoTrace finds and shows verbatim matches between segments of language model\noutput and documents in the training text corpora. Powered by an extended\nversion of infini-gram (Liu et al., 2024), our system returns tracing results\nwithin a few seconds. OLMoTrace can help users understand the behavior of\nlanguage models through the lens of their training data. We showcase how it can\nbe used to explore fact checking, hallucination, and the creativity of language\nmodels. OLMoTrace is publicly available and fully open-source.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07096.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "635f46d1928a42bc95cfcf7c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/635f46d1928a42bc95cfcf7c/5KF8aLiDCJdl7B1SdJ-7V.png",
      "fullname": "Jiacheng Liu",
      "name": "liujch1998",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 18
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.06514",
      "authors": [
        {
          "_id": "67f72e933eacf8888816f3b0",
          "user": {
            "_id": "64a8121e35fab7cd04c30ed0",
            "avatarUrl": "/avatars/48849b84703158772f1022932331b143.svg",
            "isPro": false,
            "fullname": "Chenrui Fan",
            "user": "Fcr09",
            "type": "user"
          },
          "name": "Chenrui Fan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:05:59.999Z",
          "hidden": false
        },
        {
          "_id": "67f72e933eacf8888816f3b1",
          "name": "Ming Li",
          "hidden": false
        },
        {
          "_id": "67f72e933eacf8888816f3b2",
          "user": {
            "_id": "65a52766215aabac489e3468",
            "avatarUrl": "/avatars/fe05e22cd7e12e961296426434e17c76.svg",
            "isPro": false,
            "fullname": "Lichao Sun",
            "user": "sunlichao137",
            "type": "user"
          },
          "name": "Lichao Sun",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:06:12.092Z",
          "hidden": false
        },
        {
          "_id": "67f72e933eacf8888816f3b3",
          "user": {
            "_id": "647f5af5b0e96764589f3b2a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
            "isPro": false,
            "fullname": "Tianyi Zhou",
            "user": "zhoutianyi",
            "type": "user"
          },
          "name": "Tianyi Zhou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-10T06:39:37.906Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/647f5af5b0e96764589f3b2a/FrHRMBKuB2v57LZWVJPxi.png"
      ],
      "publishedAt": "2025-04-09T01:25:27.000Z",
      "submittedOnDailyAt": "2025-04-10T01:07:13.718Z",
      "title": "La manque de bases peut conduire à un pensée excessive : est-ce qu'on perd la capacité de pensée critique pour le modèle de raisonnement ?",
      "submittedOnDailyBy": {
        "_id": "647f5af5b0e96764589f3b2a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
        "isPro": false,
        "fullname": "Tianyi Zhou",
        "user": "zhoutianyi",
        "type": "user"
      },
      "summary": "Nous avons introduit un nouveau scénario dans lequel la longueur des réponses des modèles de LLMs augmente significativement lorsqu'ils sont entraînés par apprentissage par renforcement ou supervision, surtout pour des questions fausses avec un présupposé microscopique (MiP). Cela entraîne une répétition et une inefficacité dans le pensée, et exacerbe de nombreuses parties des problèmes de pensée excessive. Cette faille contrarie la \"loi d'échelle de test\" mais, nous avons observé cette faille dans plusieurs ensembles de données, y compris MiP, ce qui indique une absence de pensée critique et un excès de pensée générale. Très étonnamment, les modèles de LLMs qui n'ont pas été entraînés spécifiquement pour faire de l'inférence montrent un comportement beaucoup plus performant dans le scénario de MiP, générant des réponses plus courtes et identifiant rapidement les questions fausses. Cela reflète une importante limite dans la recette actuelle d'entraînement des modèles d'inférence, qui ne parvient pas à encourager suffisamment un pensée efficace. Pour explorer plus profondément la cause de ces failles, nous avons analysé la longueur de l'inférence, les patrons de pensée excessive et la position importante du pensée dans différents modèles de LLMs. De plus, nous avons démontré que le pensée excessive peut propager les réponses des modèles d'inférence vers l'oubli. Ces résultats améliorent la compréhension du pensée excessive et fournissent de nouvelles perspectives pour atténuer les problèmes.",
      "upvotes": 10,
      "discussionId": "67f72e943eacf8888816f3fa",
      "githubRepo": "https://github.com/tianyi-lab/MiP-Overthinking"
    },
    "publishedAt": "2025-04-08T21:25:27.000Z",
    "title": "Missing Premise exacerbates Overthinking: Are Reasoning Models losing\n  Critical Thinking Skill?",
    "summary": "We find that the response length of reasoning LLMs, whether trained by\nreinforcement learning or supervised learning, drastically increases for\nill-posed questions with missing premises (MiP), ending up with redundant and\nineffective thinking. This newly introduced scenario exacerbates the general\noverthinking issue to a large extent, which we name as the MiP-Overthinking.\nSuch failures are against the ``test-time scaling law'' but have been widely\nobserved on multiple datasets we curated with MiP, indicating the harm of cheap\noverthinking and a lack of critical thinking. Surprisingly, LLMs not\nspecifically trained for reasoning exhibit much better performance on the MiP\nscenario, producing much shorter responses that quickly identify ill-posed\nqueries. This implies a critical flaw of the current training recipe for\nreasoning LLMs, which does not encourage efficient thinking adequately, leading\nto the abuse of thinking patterns. To further investigate the reasons behind\nsuch failures, we conduct fine-grained analyses of the reasoning length,\noverthinking patterns, and location of critical thinking on different types of\nLLMs. Moreover, our extended ablation study reveals that the overthinking is\ncontagious through the distillation of reasoning models' responses. These\nresults improve the understanding of overthinking and shed novel insights into\nmitigating the problem.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/647f5af5b0e96764589f3b2a/FrHRMBKuB2v57LZWVJPxi.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.06514.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "647f5af5b0e96764589f3b2a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
      "fullname": "Tianyi Zhou",
      "name": "zhoutianyi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.04842",
      "authors": [
        {
          "_id": "67f72ca8353d129fc7bdd504",
          "name": "Mengchao Wang",
          "hidden": false
        },
        {
          "_id": "67f72ca8353d129fc7bdd505",
          "user": {
            "_id": "653b195c5f1703225b2fd571",
            "avatarUrl": "/avatars/b7f376225cef6c13952c9c5540dd43be.svg",
            "isPro": false,
            "fullname": "wangqiang",
            "user": "wangqiang9",
            "type": "user"
          },
          "name": "Qiang Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-10T06:39:40.647Z",
          "hidden": false
        },
        {
          "_id": "67f72ca8353d129fc7bdd506",
          "user": {
            "_id": "63048ea19aef62c4013c77aa",
            "avatarUrl": "/avatars/b2b2243ccc63cfb5a3289bc2eb1d6293.svg",
            "isPro": false,
            "fullname": "fanjiang",
            "user": "fanjiang",
            "type": "user"
          },
          "name": "Fan Jiang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:50:07.747Z",
          "hidden": false
        },
        {
          "_id": "67f72ca8353d129fc7bdd507",
          "name": "Yaqi Fan",
          "hidden": false
        },
        {
          "_id": "67f72ca8353d129fc7bdd508",
          "name": "Yunpeng Zhang",
          "hidden": false
        },
        {
          "_id": "67f72ca8353d129fc7bdd509",
          "name": "Yonggang Qi",
          "hidden": false
        },
        {
          "_id": "67f72ca8353d129fc7bdd50a",
          "name": "Kun Zhao",
          "hidden": false
        },
        {
          "_id": "67f72ca8353d129fc7bdd50b",
          "name": "Mu Xu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-07T08:56:01.000Z",
      "submittedOnDailyAt": "2025-04-10T00:58:44.876Z",
      "title": "FantasyTokking: Génération de Clips Réalistes par Synthèse de Mouvement Cohérent à Partir de Photographies",
      "submittedOnDailyBy": {
        "_id": "653b195c5f1703225b2fd571",
        "avatarUrl": "/avatars/b7f376225cef6c13952c9c5540dd43be.svg",
        "isPro": false,
        "fullname": "wangqiang",
        "user": "wangqiang9",
        "type": "user"
      },
      "summary": "Créer un avatar animable réellement possible à partir d'une simple photo est difficile. Les méthodes actuelles d'accès ont des difficultés à comprendre facilement les expressions faciales superficielles, les mouvements corporels liés et les fonds dynamiques. Pour résoudre ces limitations, nous proposons un nouveau cadre de travail et utilisons un modèle de transformeur pré-entraîné pour la diffusion de vidéos pour générer des images de haute qualité à partir de dialogues coopératifs. Le cœur de notre étude est une stratégie d'alignement visio-voqueux en deux étapes. Dans le premier pas, nous utilisons un schéma d'entraînement au niveau de clip pour ajuster les mouvements menés à bien par l'espace complet du voix et pour s'assurer qu'ils correspondent aux images de référence, aux objets de contexte et aux fonds. Dans le deuxième pas, nous ajustons les mouvements de la tête avec précision au niveau de frame et nous assurons la synchronisation en utilisant des masques de suivi de la tête et des signaux de voix. En maintenant la flexibilité de l'animation, nous remplaçons la réseau de référence commun et nous concentrons notre approche sur le visage, en utilisant un module d'attention d'échange pour maintenir la cohérence du visage tout au long de la vidéo. De plus, pour contrôler avec précision l'intensité de l'expression et le mouvement corporel, nous incluons un module de régulation de l'intensité du mouvement et nous contrôlons non seulement le mouvement de la tête, mais aussi le mouvement des images animées. Nos résultats expérimentaux étendus montrent que notre approche fournit une qualité plus élevée et réaliste, ainsi qu'une plus grande coopération, intensité du mouvement et flexibilité de l'animation. Notre page de projet est disponible sur https://fantasy-amap.github.io/fantasy-talking/.",
      "upvotes": 7,
      "discussionId": "67f72cac353d129fc7bdd60f",
      "projectPage": "https://fantasy-amap.github.io/fantasy-talking/",
      "githubRepo": "https://github.com/Fantasy-AMAP/fantasy-talking"
    },
    "publishedAt": "2025-04-07T04:56:01.000Z",
    "title": "FantasyTalking: Realistic Talking Portrait Generation via Coherent\n  Motion Synthesis",
    "summary": "Creating a realistic animatable avatar from a single static portrait remains\nchallenging. Existing approaches often struggle to capture subtle facial\nexpressions, the associated global body movements, and the dynamic background.\nTo address these limitations, we propose a novel framework that leverages a\npretrained video diffusion transformer model to generate high-fidelity,\ncoherent talking portraits with controllable motion dynamics. At the core of\nour work is a dual-stage audio-visual alignment strategy. In the first stage,\nwe employ a clip-level training scheme to establish coherent global motion by\naligning audio-driven dynamics across the entire scene, including the reference\nportrait, contextual objects, and background. In the second stage, we refine\nlip movements at the frame level using a lip-tracing mask, ensuring precise\nsynchronization with audio signals. To preserve identity without compromising\nmotion flexibility, we replace the commonly used reference network with a\nfacial-focused cross-attention module that effectively maintains facial\nconsistency throughout the video. Furthermore, we integrate a motion intensity\nmodulation module that explicitly controls expression and body motion\nintensity, enabling controllable manipulation of portrait movements beyond mere\nlip motion. Extensive experimental results show that our proposed approach\nachieves higher quality with better realism, coherence, motion intensity, and\nidentity preservation. Ours project page:\nhttps://fantasy-amap.github.io/fantasy-talking/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.04842.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "653b195c5f1703225b2fd571",
      "avatarUrl": "/avatars/b7f376225cef6c13952c9c5540dd43be.svg",
      "fullname": "wangqiang",
      "name": "wangqiang9",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.07089",
      "authors": [
        {
          "_id": "67f7676d0ab78ef7b16a820f",
          "user": {
            "_id": "6614fb3d5aed02b298a4b469",
            "avatarUrl": "/avatars/d0ddb4f989ad1a3f24128cc843347bde.svg",
            "isPro": false,
            "fullname": "yiting lu",
            "user": "yeeeeeyy",
            "type": "user"
          },
          "name": "Yiting Lu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T07:58:03.992Z",
          "hidden": false
        },
        {
          "_id": "67f7676d0ab78ef7b16a8210",
          "user": {
            "_id": "64a3d1ddb3239f3e3892b24b",
            "avatarUrl": "/avatars/7ce585f5fc1d077fb1d70cc18c4da2c1.svg",
            "isPro": false,
            "fullname": "Jiakang Yuan",
            "user": "JiakangYuan",
            "type": "user"
          },
          "name": "Jiakang Yuan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:08:31.119Z",
          "hidden": false
        },
        {
          "_id": "67f7676d0ab78ef7b16a8211",
          "name": "Zhen Li",
          "hidden": false
        },
        {
          "_id": "67f7676d0ab78ef7b16a8212",
          "name": "Shitian Zhao",
          "hidden": false
        },
        {
          "_id": "67f7676d0ab78ef7b16a8213",
          "user": {
            "_id": "66bb136002fd8eb58bc84ffb",
            "avatarUrl": "/avatars/122cb8f59c502392768099b3c2afe043.svg",
            "isPro": false,
            "fullname": "qinqi",
            "user": "Dakerqi",
            "type": "user"
          },
          "name": "Qi Qin",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-10T08:06:06.570Z",
          "hidden": false
        },
        {
          "_id": "67f7676d0ab78ef7b16a8214",
          "name": "Xinyue Li",
          "hidden": false
        },
        {
          "_id": "67f7676d0ab78ef7b16a8215",
          "name": "Le Zhuo",
          "hidden": false
        },
        {
          "_id": "67f7676d0ab78ef7b16a8216",
          "user": {
            "_id": "64a7c43ae940d769194055df",
            "avatarUrl": "/avatars/441ccadd62e039fb8cb112f138ed917d.svg",
            "isPro": false,
            "fullname": "Licheng Wen",
            "user": "Wayne-lc",
            "type": "user"
          },
          "name": "Licheng Wen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:09:07.684Z",
          "hidden": false
        },
        {
          "_id": "67f7676d0ab78ef7b16a8217",
          "user": {
            "_id": "646f1bef075e11ca78da3bb7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646f1bef075e11ca78da3bb7/gNS-ikyZXYeMrf4a7HTQE.jpeg",
            "isPro": false,
            "fullname": "Dongyang Liu (Chris Liu)",
            "user": "Cxxs",
            "type": "user"
          },
          "name": "Dongyang Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:09:21.398Z",
          "hidden": false
        },
        {
          "_id": "67f7676d0ab78ef7b16a8218",
          "name": "Yuewen Cao",
          "hidden": false
        },
        {
          "_id": "67f7676d0ab78ef7b16a8219",
          "user": {
            "_id": "65b88b92e0bde92c176a888a",
            "avatarUrl": "/avatars/fc1cb54328ca93860e97fc73a3c1eb2f.svg",
            "isPro": false,
            "fullname": "Xiangchao Yan",
            "user": "yxc97",
            "type": "user"
          },
          "name": "Xiangchao Yan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:09:35.388Z",
          "hidden": false
        },
        {
          "_id": "67f7676d0ab78ef7b16a821a",
          "name": "Xin Li",
          "hidden": false
        },
        {
          "_id": "67f7676d0ab78ef7b16a821b",
          "user": {
            "_id": "643df87f7cd64d872cb9fabd",
            "avatarUrl": "/avatars/c53bfabcee08de448dde973915e8b31d.svg",
            "isPro": false,
            "fullname": "Botian Shi",
            "user": "friskit",
            "type": "user"
          },
          "name": "Botian Shi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:09:41.754Z",
          "hidden": false
        },
        {
          "_id": "67f7676d0ab78ef7b16a821c",
          "name": "Tao Chen",
          "hidden": false
        },
        {
          "_id": "67f7676d0ab78ef7b16a821d",
          "user": {
            "_id": "66d963e52e82d53d3b81031b",
            "avatarUrl": "/avatars/302dbffc033ff47813a2435a2cec02f1.svg",
            "isPro": false,
            "fullname": "Zhibo Chen",
            "user": "winhelp",
            "type": "user"
          },
          "name": "Zhibo Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:10:04.682Z",
          "hidden": false
        },
        {
          "_id": "67f7676d0ab78ef7b16a821e",
          "name": "Lei Bai",
          "hidden": false
        },
        {
          "_id": "67f7676d0ab78ef7b16a821f",
          "user": {
            "_id": "643dfd235aafbdca3a5792c0",
            "avatarUrl": "/avatars/ce8553cf5936012c692e08054ee27937.svg",
            "isPro": false,
            "fullname": "Bo Zhang",
            "user": "BoZhang",
            "type": "user"
          },
          "name": "Bo Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-10T09:57:56.032Z",
          "hidden": false
        },
        {
          "_id": "67f7676d0ab78ef7b16a8220",
          "name": "Peng Gao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-09T17:58:58.000Z",
      "submittedOnDailyAt": "2025-04-10T05:22:13.319Z",
      "title": "OmniCaptioner : En un clic, contrôle tout.",
      "submittedOnDailyBy": {
        "_id": "6614fb3d5aed02b298a4b469",
        "avatarUrl": "/avatars/d0ddb4f989ad1a3f24128cc843347bde.svg",
        "isPro": false,
        "fullname": "yiting lu",
        "user": "yeeeeeyy",
        "type": "user"
      },
      "summary": "OmniCaptioner est un cadre de captation visuelle fonctionnel qui génère des captions textuelles détaillées dans différentes zones visuelles. Il n'est pas limité à certains types d'images spécifiques (par exemple, des images naturelles ou visuogéométriques), ce qui lui permet de fournir une solution intégrée pour capter des images naturelles, du texte visuel (par exemple, posters, interface utilisateur, livres) et de la vision structurée (par exemple, documents, tableaux, graphiques). Il transforme l'information de pixels de haut niveau en représentations textuelles riches, garantissant une cohérence entre le modèle visuel et le modèle textuel. Voici ses trois principales avantages : (i) amélioration de l'inférence visuelle par des modèles de grand modèle de langage, car les captions de grand contexte des modèles de la série DeepSeek-R1 permettent une inférence efficace dans de nombreux scénarios entre modèles ; (ii) amélioration de la génération d'images, car les captions détaillées améliorent la création d'images ou la transformation d'images ; (iii) apprentissage de captions visuelles valides, ce qui permet de converger plus rapidement avec moins de données. J'espère que les capacités et l'applicabilité d'OmniCaptioner offriront une nouvelle perspective dans la réconciliation entre modèles de langage et vision.",
      "upvotes": 5,
      "discussionId": "67f767700ab78ef7b16a82d6"
    },
    "publishedAt": "2025-04-09T13:58:58.000Z",
    "title": "OmniCaptioner: One Captioner to Rule Them All",
    "summary": "We propose OmniCaptioner, a versatile visual captioning framework for\ngenerating fine-grained textual descriptions across a wide variety of visual\ndomains. Unlike prior methods limited to specific image types (e.g., natural\nimages or geometric visuals), our framework provides a unified solution for\ncaptioning natural images, visual text (e.g., posters, UIs, textbooks), and\nstructured visuals (e.g., documents, tables, charts). By converting low-level\npixel information into semantically rich textual representations, our framework\nbridges the gap between visual and textual modalities. Our results highlight\nthree key advantages: (i) Enhanced Visual Reasoning with LLMs, where\nlong-context captions of visual modalities empower LLMs, particularly the\nDeepSeek-R1 series, to reason effectively in multimodal scenarios; (ii)\nImproved Image Generation, where detailed captions improve tasks like\ntext-to-image generation and image transformation; and (iii) Efficient\nSupervised Fine-Tuning (SFT), which enables faster convergence with less data.\nWe believe the versatility and adaptability of OmniCaptioner can offer a new\nperspective for bridging the gap between language and visual modalities.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07089.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6614fb3d5aed02b298a4b469",
      "avatarUrl": "/avatars/d0ddb4f989ad1a3f24128cc843347bde.svg",
      "fullname": "yiting lu",
      "name": "yeeeeeyy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.07086",
      "authors": [
        {
          "_id": "67f75609b2d783993db63aba",
          "user": {
            "_id": "64ff3944f0d65cca9b867ed2",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ff3944f0d65cca9b867ed2/jWnHkF4AUzh51MkC0UT6b.png",
            "isPro": false,
            "fullname": "Andreas Hochlehnert",
            "user": "libeanim",
            "type": "user"
          },
          "name": "Andreas Hochlehnert",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:07:58.732Z",
          "hidden": false
        },
        {
          "_id": "67f75609b2d783993db63abb",
          "user": {
            "_id": "6556760b35f26c82c09a010f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6556760b35f26c82c09a010f/hNbwvXRBsKo6pqrHbXNzz.jpeg",
            "isPro": false,
            "fullname": "Hardik Bhatnagar",
            "user": "hrdkbhatnagar",
            "type": "user"
          },
          "name": "Hardik Bhatnagar",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:08:04.922Z",
          "hidden": false
        },
        {
          "_id": "67f75609b2d783993db63abc",
          "user": {
            "_id": "6304da46ce6b12280b1bd575",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6304da46ce6b12280b1bd575/V96ocKW4HOoysAGxuAH1X.jpeg",
            "isPro": false,
            "fullname": "Vishaal Udandarao",
            "user": "vishaal27",
            "type": "user"
          },
          "name": "Vishaal Udandarao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:08:10.717Z",
          "hidden": false
        },
        {
          "_id": "67f75609b2d783993db63abd",
          "user": {
            "_id": "62f3efefd6ba2ee26651f44a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1660153837083-noauth.png",
            "isPro": false,
            "fullname": "Samuel Albanie",
            "user": "albanie",
            "type": "user"
          },
          "name": "Samuel Albanie",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:08:16.809Z",
          "hidden": false
        },
        {
          "_id": "67f75609b2d783993db63abe",
          "user": {
            "_id": "6464a0d41683d3c81f51924a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6464a0d41683d3c81f51924a/s7yYVwfUB4WOhVFJS6A6T.jpeg",
            "isPro": false,
            "fullname": "Ameya Prabhu",
            "user": "AmeyaPrabhu",
            "type": "user"
          },
          "name": "Ameya Prabhu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-10T06:39:24.192Z",
          "hidden": false
        },
        {
          "_id": "67f75609b2d783993db63abf",
          "name": "Matthias Bethge",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-09T17:58:17.000Z",
      "submittedOnDailyAt": "2025-04-10T03:54:51.677Z",
      "title": "「Examiner le progrès de la théorie des modèles de langage avec calme : erreurs de mise en œuvre et longs」",
      "submittedOnDailyBy": {
        "_id": "6464a0d41683d3c81f51924a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6464a0d41683d3c81f51924a/s7yYVwfUB4WOhVFJS6A6T.jpeg",
        "isPro": false,
        "fullname": "Ameya Prabhu",
        "user": "AmeyaPrabhu",
        "type": "user"
      },
      "summary": "Dans le domaine de l'étude de l'intelligence artificielle (IA), on attend que le prochain grand avancé des modèles de langue (LM) se développe rapidement dans les laboratoires académiques et industriels. Cependant, cet avancé va plus loin que la méthodologie et de nombreuses évaluations dépendent de pratiques de marqueurs qui ne sont pas transparents, robustes ou basées sur une base statistique. Dans cette étude, grâce à des expériences précises, on a découvert que les marqueurs basés sur le raisonnement mathématique sont extrêmement sensibles aux décisions micro de mise en œuvre, comme la décodification de paramètres, les seeds aléatoires, le format des prompts, les configurations de matériel et de logiciel. Les améliorations récentes rapportées dans la littérature sont présentées de manière incompréhensible ou avec des variations non rapportées, ce qui complique leur compréhension. Pour résoudre ces problèmes, on propose un cadre d'évaluation standardisé qui inclut des pratiques optimales et des normes de rapport clairement définies. Ce cadre permet de réévaluer des méthodes récentes et de montrer que l'approche d'apprentissage par renforcement (RL) a démontré des améliorations plus importantes que les affirmations précédentes, en particulier dans les petits marqueurs où le risque d'overfitting est élevé. D'autre part, l'approche d'apprentissage supervisé (SFT) a montré une capacité de croissance cohérente. Pour faciliter la reproductibilité, le code, les prompts et les résultats de tous les marqueurs ont été publiés, et une base plus rigoureuse est en cours de construction pour des futures recherches.",
      "upvotes": 5,
      "discussionId": "67f7560cb2d783993db63b6b"
    },
    "publishedAt": "2025-04-09T13:58:17.000Z",
    "title": "A Sober Look at Progress in Language Model Reasoning: Pitfalls and Paths\n  to Reproducibility",
    "summary": "Reasoning has emerged as the next major frontier for language models (LMs),\nwith rapid advances from both academic and industrial labs. However, this\nprogress often outpaces methodological rigor, with many evaluations relying on\nbenchmarking practices that lack transparency, robustness, or statistical\ngrounding. In this work, we conduct a comprehensive empirical study and find\nthat current mathematical reasoning benchmarks are highly sensitive to subtle\nimplementation choices - including decoding parameters, random seeds, prompt\nformatting, and even hardware and software-framework configurations.\nPerformance gains reported in recent studies frequently hinge on unclear\ncomparisons or unreported sources of variance. To address these issues, we\npropose a standardized evaluation framework with clearly defined best practices\nand reporting standards. Using this framework, we reassess recent methods and\nfind that reinforcement learning (RL) approaches yield only modest improvements\n- far below prior claims - and are prone to overfitting, especially on\nsmall-scale benchmarks like AIME24. In contrast, supervised finetuning (SFT)\nmethods show consistently stronger generalization. To foster reproducibility,\nwe release all code, prompts, and model outputs, for reasoning benchmarks,\nestablishing more rigorous foundations for future work.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07086.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6464a0d41683d3c81f51924a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6464a0d41683d3c81f51924a/s7yYVwfUB4WOhVFJS6A6T.jpeg",
      "fullname": "Ameya Prabhu",
      "name": "AmeyaPrabhu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.07046",
      "authors": [
        {
          "_id": "67f74727353d129fc7c4be7a",
          "name": "Jifang Wang",
          "hidden": false
        },
        {
          "_id": "67f74727353d129fc7c4be7b",
          "name": "Xue Yang",
          "hidden": false
        },
        {
          "_id": "67f74727353d129fc7c4be7c",
          "name": "Longyue Wang",
          "hidden": false
        },
        {
          "_id": "67f74727353d129fc7c4be7d",
          "user": {
            "_id": "639c379cdb7c5f35004066cb",
            "avatarUrl": "/avatars/3e435506ee85aa7d2d0ec2174a07462f.svg",
            "isPro": false,
            "fullname": "Zhenran Xu",
            "user": "imryanxu",
            "type": "user"
          },
          "name": "Zhenran Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-10T09:58:11.729Z",
          "hidden": false
        },
        {
          "_id": "67f74727353d129fc7c4be7e",
          "name": "Yiyu Wang",
          "hidden": false
        },
        {
          "_id": "67f74727353d129fc7c4be7f",
          "name": "Yaowei Wang",
          "hidden": false
        },
        {
          "_id": "67f74727353d129fc7c4be80",
          "name": "Weihua Luo",
          "hidden": false
        },
        {
          "_id": "67f74727353d129fc7c4be81",
          "name": "Kaifu Zhang",
          "hidden": false
        },
        {
          "_id": "67f74727353d129fc7c4be82",
          "name": "Baotian Hu",
          "hidden": false
        },
        {
          "_id": "67f74727353d129fc7c4be83",
          "name": "Min Zhang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/639c379cdb7c5f35004066cb/xXlL1RROzluluflNDOIRv.png"
      ],
      "publishedAt": "2025-04-09T17:04:14.000Z",
      "submittedOnDailyAt": "2025-04-10T07:56:53.441Z",
      "title": "Une évaluation de génération d'images conditionnelles par un seul cadre de sortie d'intégration",
      "submittedOnDailyBy": {
        "_id": "639c379cdb7c5f35004066cb",
        "avatarUrl": "/avatars/3e435506ee85aa7d2d0ec2174a07462f.svg",
        "isPro": false,
        "fullname": "Zhenran Xu",
        "user": "imryanxu",
        "type": "user"
      },
      "summary": "La génération d'images conditionnelles a attiré l'attention pour sa capacité à protéger la vie privée des contenus. Cependant, ce domaine rencontre des défis dans la création de critères d'évaluation indépendants de tâches et fiables et expliquables. Dans cet article, nous présentons un cadre efficace appelé CIGEval, qui permet d'effectuer des évaluations détaillées de la tâche de génération d'images conditionnelles. CIGEval est basé sur une grande variété de modèles multimodal (LMMs) et combine diverses outils fonctionnels pour construire un cadre d'évaluation détaillé. De plus, il intègre le processus d'évaluation finale de l'entraînement, permettant aux modèles de petite taille LMMs de sélectionner automatiquement les outils appropriés et de réaliser des analyses détaillées basées sur les sorties des outils. Dans des expériences représentatives de 7 tâches de génération d'images conditionnelles, CIGEval (version GPT-4o) a atteint une corrélation élevée avec l'évaluation humaine de 0,4625 et a ressemblé étroitement à la corrélation entre évaluateurs de 0,47. De plus, en utilisant un LMM de 7B et en mettant en œuvre 2,3K processus d'entraînement, CIGEval a dépassé les méthodes les plus avancées basées sur GPT-4o. Dans un cas d'étude de la génération d'images avec GPT-4o, CIGEval a démontré sa capacité à identifier des problèmes subtils en matière de cohérence du thème et d'adhérence aux directives de contrôle, montrant de grands potentiels pour une évaluation automatique à l'échelle humaine de confiance dans la tâche de génération d'images.",
      "upvotes": 5,
      "discussionId": "67f7472b353d129fc7c4bf4b",
      "githubRepo": "https://github.com/HITsz-TMG/Agentic-CIGEval"
    },
    "publishedAt": "2025-04-09T13:04:14.000Z",
    "title": "A Unified Agentic Framework for Evaluating Conditional Image Generation",
    "summary": "Conditional image generation has gained significant attention for its ability\nto personalize content. However, the field faces challenges in developing\ntask-agnostic, reliable, and explainable evaluation metrics. This paper\nintroduces CIGEval, a unified agentic framework for comprehensive evaluation of\nconditional image generation tasks. CIGEval utilizes large multimodal models\n(LMMs) as its core, integrating a multi-functional toolbox and establishing a\nfine-grained evaluation framework. Additionally, we synthesize evaluation\ntrajectories for fine-tuning, empowering smaller LMMs to autonomously select\nappropriate tools and conduct nuanced analyses based on tool outputs.\nExperiments across seven prominent conditional image generation tasks\ndemonstrate that CIGEval (GPT-4o version) achieves a high correlation of 0.4625\nwith human assessments, closely matching the inter-annotator correlation of\n0.47. Moreover, when implemented with 7B open-source LMMs using only 2.3K\ntraining trajectories, CIGEval surpasses the previous GPT-4o-based\nstate-of-the-art method. Case studies on GPT-4o image generation highlight\nCIGEval's capability in identifying subtle issues related to subject\nconsistency and adherence to control guidance, indicating its great potential\nfor automating evaluation of image generation tasks with human-level\nreliability.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/639c379cdb7c5f35004066cb/xXlL1RROzluluflNDOIRv.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07046.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "639c379cdb7c5f35004066cb",
      "avatarUrl": "/avatars/3e435506ee85aa7d2d0ec2174a07462f.svg",
      "fullname": "Zhenran Xu",
      "name": "imryanxu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.07092",
      "authors": [
        {
          "_id": "67f7826a8b50772851ccb603",
          "user": {
            "_id": "64198d7efdfc2970b350f48f",
            "avatarUrl": "/avatars/c0a0f30e1cbc22f1eb6bbc4549a5709c.svg",
            "isPro": false,
            "fullname": "Alexander Rubinstein",
            "user": "arubique",
            "type": "user"
          },
          "name": "Alexander Rubinstein",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:52:04.485Z",
          "hidden": false
        },
        {
          "_id": "67f7826a8b50772851ccb604",
          "user": {
            "_id": "6464a0d41683d3c81f51924a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6464a0d41683d3c81f51924a/s7yYVwfUB4WOhVFJS6A6T.jpeg",
            "isPro": false,
            "fullname": "Ameya Prabhu",
            "user": "AmeyaPrabhu",
            "type": "user"
          },
          "name": "Ameya Prabhu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:52:11.053Z",
          "hidden": false
        },
        {
          "_id": "67f7826a8b50772851ccb605",
          "name": "Matthias Bethge",
          "hidden": false
        },
        {
          "_id": "67f7826a8b50772851ccb606",
          "user": {
            "_id": "638a50450f10aa3064f03f23",
            "avatarUrl": "/avatars/0c068458e42950c851758a238225c3a6.svg",
            "isPro": false,
            "fullname": "Seong Joon Oh",
            "user": "coallaoh",
            "type": "user"
          },
          "name": "Seong Joon Oh",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:52:30.797Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-09T17:59:05.000Z",
      "submittedOnDailyAt": "2025-04-10T07:04:07.299Z",
      "title": "C'est une question sur l'apprentissage axé sur l'objectif.",
      "submittedOnDailyBy": {
        "_id": "6464a0d41683d3c81f51924a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6464a0d41683d3c81f51924a/s7yYVwfUB4WOhVFJS6A6T.jpeg",
        "isPro": false,
        "fullname": "Ameya Prabhu",
        "user": "AmeyaPrabhu",
        "type": "user"
      },
      "summary": "L'apprentissage centré sur les objets (OCL) est un approche qui vise à obtenir des représentations qui séparent les objets des autres objets ou des fonds dans un espace de représentation. Cette approche soutient plusieurs objectifs, tels que la généralisation en dehors de l'échantillonnage (OOD), l'efficacité dans l'utilisation d'échantillons et la modélisation d'environnements structurés. De nombreux études ont développé des méthodes pour diviser les objets sans supervision dans l'espace de représentation et ont utilisé la détection d'objets sans supervision pour évaluer ces méthodes. Cependant, récemment, il a été possible de séparer et de représenter les objets de manière indépendante dans l'espace des pixels en utilisant des modèles efficaces de division d'échantillons. Cela a permis d'atteindre un rendement considérablement élevé sur le benchmark de découverte d'objets en dehors de l'échantillonnage (OOD) avec un modèle scalable et adaptable à la quantité de slots. De cette manière, on a réussi significativement à atteindre l'objectif de l'OCL de obtenir des représentations centrées sur les objets.\n\nEn plus de ce progrès, l'une des questions les plus importantes est la contribution que la capacité de diviser les objets a pour la généralisation en dehors de l'échantillonnage et autres objectifs de l'OCL. Pour aborder cette question, on a étudié le défi de la généralisation en dehors de l'échantillonnage avec des objets séparés de fonds incorrectement d'une perspective d'OCL et on a proposé un nouveau ensemble d'entraînement non réalisé appelé \"Classification des classes centrales d'objets et application de masque (OCCAM)\". Cela a démontré que les représentations basées sur la division individuelle des objets sont supérieures aux méthodes de l'OCL basées sur les slots. Cependant, il reste des problèmes dans les applications réelles. La communauté de l'OCL fournit une outil box pour la représentation centrée sur les objets scalable et se concentre sur des applications pratiques et des problèmes fondamentaux, tels que la compréhension de la reconnaissance cognitive d'objets humains. Le code est disponible sur https://github.com/AlexanderRubinstein/OCCAM.",
      "upvotes": 3,
      "discussionId": "67f7826b8b50772851ccb64c"
    },
    "publishedAt": "2025-04-09T13:59:05.000Z",
    "title": "Are We Done with Object-Centric Learning?",
    "summary": "Object-centric learning (OCL) seeks to learn representations that only encode\nan object, isolated from other objects or background cues in a scene. This\napproach underpins various aims, including out-of-distribution (OOD)\ngeneralization, sample-efficient composition, and modeling of structured\nenvironments. Most research has focused on developing unsupervised mechanisms\nthat separate objects into discrete slots in the representation space,\nevaluated using unsupervised object discovery. However, with recent\nsample-efficient segmentation models, we can separate objects in the pixel\nspace and encode them independently. This achieves remarkable zero-shot\nperformance on OOD object discovery benchmarks, is scalable to foundation\nmodels, and can handle a variable number of slots out-of-the-box. Hence, the\ngoal of OCL methods to obtain object-centric representations has been largely\nachieved. Despite this progress, a key question remains: How does the ability\nto separate objects within a scene contribute to broader OCL objectives, such\nas OOD generalization? We address this by investigating the OOD generalization\nchallenge caused by spurious background cues through the lens of OCL. We\npropose a novel, training-free probe called Object-Centric\nClassification with Applied Masks (OCCAM), demonstrating that\nsegmentation-based encoding of individual objects significantly outperforms\nslot-based OCL methods. However, challenges in real-world applications remain.\nWe provide the toolbox for the OCL community to use scalable object-centric\nrepresentations, and focus on practical applications and fundamental questions,\nsuch as understanding object perception in human cognition. Our code is\navailable https://github.com/AlexanderRubinstein/OCCAM{here}.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07092.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6464a0d41683d3c81f51924a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6464a0d41683d3c81f51924a/s7yYVwfUB4WOhVFJS6A6T.jpeg",
      "fullname": "Ameya Prabhu",
      "name": "AmeyaPrabhu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.06947",
      "authors": [
        {
          "_id": "67f78485cfcd3569910c99ab",
          "name": "Natalia Loukachevitch",
          "hidden": false
        },
        {
          "_id": "67f78485cfcd3569910c99ac",
          "name": "Natalia Tkachenko",
          "hidden": false
        },
        {
          "_id": "67f78485cfcd3569910c99ad",
          "name": "Anna Lapanitsyna",
          "hidden": false
        },
        {
          "_id": "67f78485cfcd3569910c99ae",
          "user": {
            "_id": "652cedbdf120598322ae358a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652cedbdf120598322ae358a/RrxrP0gtQus4SfNwfyAg_.jpeg",
            "isPro": false,
            "fullname": "Mikhail",
            "user": "RefalMachine",
            "type": "user"
          },
          "name": "Mikhail Tikhomirov",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-04-10T09:18:43.707Z",
          "hidden": false
        },
        {
          "_id": "67f78485cfcd3569910c99af",
          "user": {
            "_id": "64e62d11d27a8292c3637f86",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e62d11d27a8292c3637f86/aptDeBHpCJxcREj6KPLN1.jpeg",
            "isPro": false,
            "fullname": "Nicolay Rusnachenko",
            "user": "nicolay-r",
            "type": "user"
          },
          "name": "Nicolay Rusnachenko",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-10T09:57:54.353Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64e62d11d27a8292c3637f86/pVL1YkBNlHeaQUud0VmSt.png",
        "https://cdn-uploads.huggingface.co/production/uploads/64e62d11d27a8292c3637f86/1ph_RyOzjsRcMs_04naOR.png",
        "https://cdn-uploads.huggingface.co/production/uploads/64e62d11d27a8292c3637f86/ewSfbBvFUKclWyZTGCOiE.png",
        "https://cdn-uploads.huggingface.co/production/uploads/64e62d11d27a8292c3637f86/gbTZ0dZq7O6zWKcstfwWy.png"
      ],
      "publishedAt": "2025-04-09T14:54:00.000Z",
      "submittedOnDailyAt": "2025-04-10T07:29:49.621Z",
      "title": "Opinion NE-2024 : Tuples d'Opinions Extraites d'Articles de Presse Russes de 2024",
      "submittedOnDailyBy": {
        "_id": "64e62d11d27a8292c3637f86",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e62d11d27a8292c3637f86/aptDeBHpCJxcREj6KPLN1.jpeg",
        "isPro": false,
        "fullname": "Nicolay Rusnachenko",
        "user": "nicolay-r",
        "type": "user"
      },
      "summary": "Dans cet article, nous présentons la tâche commune d'évaluation de dialogue pour l'extraction d'opinions structurées dans les textes de journaux en russe. L'objectif de la compétition est d'extraire des tuples d'opinions à partir de phrases données. Ces tuples sont composés du sujet de l'émotion, de son objectif, de l'expression envers le sujet de l'émotion et de l'émotion. En total, plus de 100 entrées ont été reçues pour ce défi. Les participants ont principalement expérimenté avec des modèles de langage grands dans des formats de 0-shot, few-shot et fine-tuning. Le meilleur résultat sur l'échantillon de test a été obtenu par fine-tuning d'un modèle de langage grand. De plus, 30 modèles de profenet et 11 modèles de langage ouvert (entre 3 et 32 milliards de paramètres) ont été comparés dans des configurations de 1-shot et 10-shot pour trouver le meilleur modèle et le meilleur profenet.",
      "upvotes": 3,
      "discussionId": "67f78486cfcd3569910c9a13",
      "projectPage": "https://codalab.lisn.upsaclay.fr/competitions/20244",
      "githubRepo": "https://github.com/dialogue-evaluation/RuOpinionNE-2024"
    },
    "publishedAt": "2025-04-09T10:54:00.000Z",
    "title": "RuOpinionNE-2024: Extraction of Opinion Tuples from Russian News Texts",
    "summary": "In this paper, we introduce the Dialogue Evaluation shared task on extraction\nof structured opinions from Russian news texts. The task of the contest is to\nextract opinion tuples for a given sentence; the tuples are composed of a\nsentiment holder, its target, an expression and sentiment from the holder to\nthe target. In total, the task received more than 100 submissions. The\nparticipants experimented mainly with large language models in zero-shot,\nfew-shot and fine-tuning formats. The best result on the test set was obtained\nwith fine-tuning of a large language model. We also compared 30 prompts and 11\nopen source language models with 3-32 billion parameters in the 1-shot and\n10-shot settings and found the best models and prompts.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64e62d11d27a8292c3637f86/pVL1YkBNlHeaQUud0VmSt.png",
      "https://cdn-uploads.huggingface.co/production/uploads/64e62d11d27a8292c3637f86/1ph_RyOzjsRcMs_04naOR.png",
      "https://cdn-uploads.huggingface.co/production/uploads/64e62d11d27a8292c3637f86/ewSfbBvFUKclWyZTGCOiE.png",
      "https://cdn-uploads.huggingface.co/production/uploads/64e62d11d27a8292c3637f86/gbTZ0dZq7O6zWKcstfwWy.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.06947.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64e62d11d27a8292c3637f86",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e62d11d27a8292c3637f86/aptDeBHpCJxcREj6KPLN1.jpeg",
      "fullname": "Nicolay Rusnachenko",
      "name": "nicolay-r",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 123
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.04010",
      "authors": [
        {
          "_id": "67f766cb1879ad2f13bee3d1",
          "user": {
            "_id": "63c9f93cdfac8071d01ed56f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674180895772-noauth.jpeg",
            "isPro": false,
            "fullname": "Maksim Siniukov",
            "user": "havent-invented",
            "type": "user"
          },
          "name": "Maksim Siniukov",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:10:42.914Z",
          "hidden": false
        },
        {
          "_id": "67f766cb1879ad2f13bee3d2",
          "user": {
            "_id": "64a5d8219f3b568c202b3137",
            "avatarUrl": "/avatars/eef6fb7c70d272555a53183c0e50dbaf.svg",
            "isPro": false,
            "fullname": "Di Chang",
            "user": "Boese0601",
            "type": "user"
          },
          "name": "Di Chang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:10:49.652Z",
          "hidden": false
        },
        {
          "_id": "67f766cb1879ad2f13bee3d3",
          "user": {
            "_id": "632b6c08ca316c73cd3e4d8c",
            "avatarUrl": "/avatars/a02aa14823dd729df0267a9b55779edd.svg",
            "isPro": false,
            "fullname": "Minh Tran",
            "user": "minhtran",
            "type": "user"
          },
          "name": "Minh Tran",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:10:55.537Z",
          "hidden": false
        },
        {
          "_id": "67f766cb1879ad2f13bee3d4",
          "user": {
            "_id": "6605cfc7b85b7b4ea506a33d",
            "avatarUrl": "/avatars/92abda656abf2298c9d28b9b2e3643a3.svg",
            "isPro": false,
            "fullname": "Hongkun Gong",
            "user": "hongkung",
            "type": "user"
          },
          "name": "Hongkun Gong",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:11:01.857Z",
          "hidden": false
        },
        {
          "_id": "67f766cb1879ad2f13bee3d5",
          "user": {
            "_id": "6541185dbd60d2bd193f7999",
            "avatarUrl": "/avatars/4ce4a0feff9bfbb87e9f40431718ba00.svg",
            "isPro": false,
            "fullname": "Ashutosh Chaubey",
            "user": "chaubeyG",
            "type": "user"
          },
          "name": "Ashutosh Chaubey",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:11:08.391Z",
          "hidden": false
        },
        {
          "_id": "67f766cb1879ad2f13bee3d6",
          "user": {
            "_id": "65fcb99d383d3f256c3a92d2",
            "avatarUrl": "/avatars/b85d32f4d7a19816b8d499e05b173ad1.svg",
            "isPro": false,
            "fullname": "Mohammad Soleymani",
            "user": "msoleymani",
            "type": "user"
          },
          "name": "Mohammad Soleymani",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:11:15.030Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-05T01:19:46.000Z",
      "submittedOnDailyAt": "2025-04-10T05:06:42.197Z",
      "title": "Détails du liste : Génération de vidéos audio de haute qualité avec distribution contrôlable",
      "submittedOnDailyBy": {
        "_id": "64a5d8219f3b568c202b3137",
        "avatarUrl": "/avatars/eef6fb7c70d272555a53183c0e50dbaf.svg",
        "isPro": false,
        "fullname": "Di Chang",
        "user": "Boese0601",
        "type": "user"
      },
      "summary": "Le mouvement des écouteurs naturels et minimaux est un problème ouvert qui persiste dans l'interaction au fil du temps. Les méthodes existantes se fondent principalement sur des codes de mouvement de basse dimension pour générer des mouvements faciaux, dépendant de ceci pour effectuer un rendu réaliste, ce qui limite la précision visuelle et l'expressionnalité. Pour résoudre ces défis, nous présentons DiTaiListener, un modèle de diffusion vidéo à plusieurs modalités. Notre méthode commence avec DiTaiListener-Gen, qui génère des réponses des écouteurs à des petites sections de vidéo à partir de la voix et du mouvement facial de l'interlocuteur. Ensuite, DiTaiListener-Edit ajuste ces fragments de vidéo au niveau des pixels pour atteindre une transition sans interruptions. Spécifiquement, DiTaiListener-Gen introduit un adaptateur multimodal de séries temporelles causales (CTM-Adapter) pour processer les signaux auditifs et visuels de l'interlocuteur et générer des images d'écouteurs en utilisant un transformeur de diffusion (DiT). Le CTM-Adapter intègre les données de l'interlocuteur dans un mode causal lors du processus de génération de vidéo, assurant la continuité temporelle de la réponse de l'écouteur. Pour générer des vidéos longues, nous introduisons DiTaiListener-Edit pour combiner des sections de vidéo de manière harmonieuse et continue, en sûrant que les expressions faciales et la qualité de l'image soient cohérentes dans le temps lors de la combinaison de fragments de vidéo générés par DiTaiListener-Gen. De manière duale, DiTaiListener a atteint les meilleurs résultats dans le jeu de données de référence, améliorant significativement la réalisme (73,8% d'amélioration sur FID dans RealTalk) et l'expression du mouvement (6,1% d'amélioration sur l'indice FD dans VICO). Les recherches de utilisateurs ont confirmé l'excellence de DiTaiListener, démontrant un défi clair en termes de qualité, diversité et lisibilité par rapport aux concurrents.",
      "upvotes": 3,
      "discussionId": "67f766ce1879ad2f13bee47a",
      "projectPage": "https://cv.maxi.su/DiTaiListener/"
    },
    "publishedAt": "2025-04-04T21:19:46.000Z",
    "title": "DiTaiListener: Controllable High Fidelity Listener Video Generation with\n  Diffusion",
    "summary": "Generating naturalistic and nuanced listener motions for extended\ninteractions remains an open problem. Existing methods often rely on\nlow-dimensional motion codes for facial behavior generation followed by\nphotorealistic rendering, limiting both visual fidelity and expressive\nrichness. To address these challenges, we introduce DiTaiListener, powered by a\nvideo diffusion model with multimodal conditions. Our approach first generates\nshort segments of listener responses conditioned on the speaker's speech and\nfacial motions with DiTaiListener-Gen. It then refines the transitional frames\nvia DiTaiListener-Edit for a seamless transition. Specifically,\nDiTaiListener-Gen adapts a Diffusion Transformer (DiT) for the task of listener\nhead portrait generation by introducing a Causal Temporal Multimodal Adapter\n(CTM-Adapter) to process speakers' auditory and visual cues. CTM-Adapter\nintegrates speakers' input in a causal manner into the video generation process\nto ensure temporally coherent listener responses. For long-form video\ngeneration, we introduce DiTaiListener-Edit, a transition refinement\nvideo-to-video diffusion model. The model fuses video segments into smooth and\ncontinuous videos, ensuring temporal consistency in facial expressions and\nimage quality when merging short video segments produced by DiTaiListener-Gen.\nQuantitatively, DiTaiListener achieves the state-of-the-art performance on\nbenchmark datasets in both photorealism (+73.8% in FID on RealTalk) and motion\nrepresentation (+6.1% in FD metric on VICO) spaces. User studies confirm the\nsuperior performance of DiTaiListener, with the model being the clear\npreference in terms of feedback, diversity, and smoothness, outperforming\ncompetitors by a significant margin.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.04010.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64a5d8219f3b568c202b3137",
      "avatarUrl": "/avatars/eef6fb7c70d272555a53183c0e50dbaf.svg",
      "fullname": "Di Chang",
      "name": "Boese0601",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.07081",
      "authors": [
        {
          "_id": "67f7823da630bcdabbd8b3eb",
          "name": "Gabriel Grand",
          "hidden": false
        },
        {
          "_id": "67f7823da630bcdabbd8b3ec",
          "name": "Joshua B. Tenenbaum",
          "hidden": false
        },
        {
          "_id": "67f7823da630bcdabbd8b3ed",
          "name": "Vikash K. Mansinghka",
          "hidden": false
        },
        {
          "_id": "67f7823da630bcdabbd8b3ee",
          "user": {
            "_id": "673cc08370644bb836283fec",
            "avatarUrl": "/avatars/b8c7f1a10ddf76dcd06398c59f553b61.svg",
            "isPro": false,
            "fullname": "Alexander Lew",
            "user": "alexanderlew",
            "type": "user"
          },
          "name": "Alexander K. Lew",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-10T08:33:02.433Z",
          "hidden": false
        },
        {
          "_id": "67f7823da630bcdabbd8b3ef",
          "name": "Jacob Andreas",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-09T17:54:22.000Z",
      "submittedOnDailyAt": "2025-04-10T07:03:34.220Z",
      "title": "InternLM (书生·浦语)",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Selon la théorie logique lors du test, les modèles de langage peuvent réaliser des tâches complexes mais sont lents et coûteux en recherche et planification, en plus d'être les causes d'erreurs. Cependant, il existe la possibilité que un modèle de langage (LM) soit aligné avec les étapes précises d'une théorie logique nécessaire à sa résolution, bien qu'il soit excellant dans la structure abstraite. Dans cet article, on présente le méthode de « autocontrol ». Cette méthode consiste à ce que un modèle de planification génère un programme d'inférence pour répondre à une tâche, qui est ensuite exécuté par un groupe de modèles de suivi. Notre approche permet de rédiger les modèles de langage de manière récursive dans un ordre de recherche et de guider l'inférence des modèles de langage avec un nouveau format logique. Dans des cas où des petits modèles de suivi (par exemple, Llama-3.2-1B) sont utilisés, DisCIPL est un modèle grand comme GPT-4o ou o1. En séparant la planification de l'exécution, notre étude explore l'espace de conception pour une stratégie de Monte Carlo de haute dimension et parallèle qui dépasse la meilleure échantillonnage standard de N, montrant ce qui est automatiquement implémentable dans les modèles de langage actuels.",
      "upvotes": 1,
      "discussionId": "67f7823ea630bcdabbd8b42e"
    },
    "publishedAt": "2025-04-09T13:54:22.000Z",
    "title": "Self-Steering Language Models",
    "summary": "While test-time reasoning enables language models to tackle complex tasks,\nsearching or planning in natural language can be slow, costly, and error-prone.\nBut even when LMs struggle to emulate the precise reasoning steps needed to\nsolve a problem, they often excel at describing its abstract structure--both\nhow to verify solutions and how to search for them. This paper introduces\nDisCIPL, a method for \"self-steering\" LMs where a Planner model generates a\ntask-specific inference program that is executed by a population of Follower\nmodels. Our approach equips LMs with the ability to write recursive search\nprocedures that guide LM inference, enabling new forms of verifiable and\nefficient reasoning. When instantiated with a small Follower (e.g.,\nLlama-3.2-1B), DisCIPL matches (and sometimes outperforms) much larger models,\nincluding GPT-4o and o1, on challenging constrained generation tasks. In\ndecoupling planning from execution, our work opens up a design space of\nhighly-parallelized Monte Carlo inference strategies that outperform standard\nbest-of-N sampling, require no finetuning, and can be implemented automatically\nby existing LMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07081.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6621
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.03886",
      "authors": [
        {
          "_id": "67f78d2850c25afaf8a1210f",
          "name": "Jianhao Zheng",
          "hidden": false
        },
        {
          "_id": "67f78d2850c25afaf8a12110",
          "name": "Zihan Zhu",
          "hidden": false
        },
        {
          "_id": "67f78d2850c25afaf8a12111",
          "name": "Valentin Bieri",
          "hidden": false
        },
        {
          "_id": "67f78d2850c25afaf8a12112",
          "name": "Marc Pollefeys",
          "hidden": false
        },
        {
          "_id": "67f78d2850c25afaf8a12113",
          "name": "Songyou Peng",
          "hidden": false
        },
        {
          "_id": "67f78d2850c25afaf8a12114",
          "name": "Iro Armeni",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-04T19:19:40.000Z",
      "submittedOnDailyAt": "2025-04-10T07:53:22.145Z",
      "title": "WildGS-SLAM : SLAM basé sur une caméra unique dans des environnements dynamiques",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "WildGS-SLAM présente un système de SLAM monocaméra RGB efficace et puissant. Ce système fonctionne dans des environnements dynamiques en exploitant une cartographie géométrique qui prend en compte l'incertitude. Les systèmes traditionnels de SLAM supposent des scènes statiques, mais notre approche détecte les objets dynamiques et améliore la performance de la cartographie et de la rendition en intégrant des informations de profondeur et d'incertitude. Nous introduisons la carte d'incertitude prédite. Cette carte guide la suppression des objets dynamiques et est utilisée à la fois pour la cartographie et pour le chargement. La carte d'incertitude améliore l'exactitude de la reconstruction grâce à l'ajustement de faisceau dense et à l'optimisation de la carte gaussienne. WildGS-SLAM démontre une synthèse visuelle sans artefacts sur plusieurs jeux de données, prouvant une performance élevée par rapport aux méthodes de pointe actuelles dans des environnements dynamiques.",
      "upvotes": 1,
      "discussionId": "67f78d2e50c25afaf8a122c9"
    },
    "publishedAt": "2025-04-04T15:19:40.000Z",
    "title": "WildGS-SLAM: Monocular Gaussian Splatting SLAM in Dynamic Environments",
    "summary": "We present WildGS-SLAM, a robust and efficient monocular RGB SLAM system\ndesigned to handle dynamic environments by leveraging uncertainty-aware\ngeometric mapping. Unlike traditional SLAM systems, which assume static scenes,\nour approach integrates depth and uncertainty information to enhance tracking,\nmapping, and rendering performance in the presence of moving objects. We\nintroduce an uncertainty map, predicted by a shallow multi-layer perceptron and\nDINOv2 features, to guide dynamic object removal during both tracking and\nmapping. This uncertainty map enhances dense bundle adjustment and Gaussian map\noptimization, improving reconstruction accuracy. Our system is evaluated on\nmultiple datasets and demonstrates artifact-free view synthesis. Results\nshowcase WildGS-SLAM's superior performance in dynamic environments compared to\nstate-of-the-art methods.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.03886.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6621
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.06958",
      "authors": [
        {
          "_id": "67f783fc2eec6ce5c8d18be3",
          "user": {
            "_id": "672f8a28c53c174f39b08ac1",
            "avatarUrl": "/avatars/9d865f757667de14381d7c4d7ba7e4c4.svg",
            "isPro": false,
            "fullname": "XINHAO LI",
            "user": "xinhaoli",
            "type": "user"
          },
          "name": "Xinhao Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:52:46.553Z",
          "hidden": false
        },
        {
          "_id": "67f783fc2eec6ce5c8d18be4",
          "user": {
            "_id": "65499e5f2a292b3e2e5715a3",
            "avatarUrl": "/avatars/087b3e36dfb66e044265b856bab31657.svg",
            "isPro": false,
            "fullname": "ziang yan",
            "user": "Aurorana",
            "type": "user"
          },
          "name": "Ziang Yan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:52:52.960Z",
          "hidden": false
        },
        {
          "_id": "67f783fc2eec6ce5c8d18be5",
          "user": {
            "_id": "63217b7231205fe84a9626ca",
            "avatarUrl": "/avatars/ed1e96c713c0b884adc87b8c12faa32c.svg",
            "isPro": false,
            "fullname": "Desen Meng",
            "user": "desenmeng",
            "type": "user"
          },
          "name": "Desen Meng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:53:00.046Z",
          "hidden": false
        },
        {
          "_id": "67f783fc2eec6ce5c8d18be6",
          "user": {
            "_id": "666946fdec88f15e04db6022",
            "avatarUrl": "/avatars/84511d4cd2a6bdc229dd2b1057d4b2ab.svg",
            "isPro": false,
            "fullname": "Lu Dong",
            "user": "donglu",
            "type": "user"
          },
          "name": "Lu Dong",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:53:11.431Z",
          "hidden": false
        },
        {
          "_id": "67f783fc2eec6ce5c8d18be7",
          "user": {
            "_id": "660a7e1c3fbd33a1d0b0e233",
            "avatarUrl": "/avatars/ceff1231078115cae8f3f4f87d026963.svg",
            "isPro": false,
            "fullname": "Xiangyu Zeng",
            "user": "Lanxingxuan",
            "type": "user"
          },
          "name": "Xiangyu Zeng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:53:25.534Z",
          "hidden": false
        },
        {
          "_id": "67f783fc2eec6ce5c8d18be8",
          "user": {
            "_id": "65b9d9961fe588f824fde191",
            "avatarUrl": "/avatars/a9245958cc998a4b4b870bf2490fdaee.svg",
            "isPro": false,
            "fullname": "Yinan He",
            "user": "yinanhe",
            "type": "user"
          },
          "name": "Yinan He",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:53:32.159Z",
          "hidden": false
        },
        {
          "_id": "67f783fc2eec6ce5c8d18be9",
          "name": "Yali Wang",
          "hidden": false
        },
        {
          "_id": "67f783fc2eec6ce5c8d18bea",
          "name": "Yu Qiao",
          "hidden": false
        },
        {
          "_id": "67f783fc2eec6ce5c8d18beb",
          "name": "Yi Wang",
          "hidden": false
        },
        {
          "_id": "67f783fc2eec6ce5c8d18bec",
          "user": {
            "_id": "643d4996482011f5f2be271f",
            "avatarUrl": "/avatars/134b8f5d44b85d55eaaa2bbe6c409917.svg",
            "isPro": false,
            "fullname": "limin wang",
            "user": "flyacht",
            "type": "user"
          },
          "name": "Limin Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:53:52.538Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-09T15:09:27.000Z",
      "submittedOnDailyAt": "2025-04-10T07:11:25.167Z",
      "title": "VideoChat-R1 : Améliore le reconnaissance de l'espace-temps par apprentissage par renforcement.",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Le développement récent de l'apprentissage par renforcement a considérablement amélioré les capacités des modèles multimodales de langue et d'images (MLLMs). Des méthodes telles que l'optimisation de politiques en groupe (GRPO) et des structures de récompense basées sur des règles ont montré des résultats satisfaisants dans le domaine des phrases et d'images, mais leur application dans la compréhension des vidéos est limitée. Dans cet article, nous explorons systématiquement l'apprentissage par renforcement avec ajustements micro (RFT) en utilisant GRPO, avec l'objectif d'améliorer la perception temporelle-spatiale tout en maintenant les capacités générales. Nos expériences montrent que le RFT démontre un haut rendement de données en améliorant les tâches spécifiques. Nous avons développé VideoChat-R1, un puissant MLLM de vidéo qui améliore la perception temporelle-spatiale dans des tâches de ce type, tout en maintenant sa capacité de chat sans perdre. Comparé à Qwen2.5-VL-7B, VideoChat-R1 obtient des améliorations significatives dans des tâches telles que la détection de temps (+31.8) et le suivi d'objets (+31.2). De plus, il montre également des améliorations notables dans des benchmarks généraux comme Video MME (+0.9), MVBench (+1.0) et Test de Perception (+0.9). Nos résultats soulignent la possibilité que le RFT puisse améliorer les tâches spécifiques des MLLMs de vidéo. Notre recherche espère que futurs études en apprentissage par renforcement fournissent plus de données pour les MLLMs de vidéo, ce qui pourrait améliorer encore plus leurs capacités.",
      "upvotes": 0,
      "discussionId": "67f783fd2eec6ce5c8d18c2e"
    },
    "publishedAt": "2025-04-09T11:09:27.000Z",
    "title": "VideoChat-R1: Enhancing Spatio-Temporal Perception via Reinforcement\n  Fine-Tuning",
    "summary": "Recent advancements in reinforcement learning have significantly advanced the\nreasoning capabilities of multimodal large language models (MLLMs). While\napproaches such as Group Relative Policy Optimization (GRPO) and rule-based\nreward mechanisms demonstrate promise in text and image domains, their\napplication to video understanding remains limited. This paper presents a\nsystematic exploration of Reinforcement Fine-Tuning (RFT) with GRPO for video\nMLLMs, aiming to enhance spatio-temporal perception while maintaining general\ncapabilities. Our experiments reveal that RFT is highly data-efficient for\ntask-specific improvements. Through multi-task RFT on spatio-temporal\nperception objectives with limited samples, we develop VideoChat-R1, a powerful\nvideo MLLM that achieves state-of-the-art performance on spatio-temporal\nperception tasks without sacrificing chat ability, while exhibiting emerging\nspatio-temporal reasoning abilities. Compared to Qwen2.5-VL-7B, VideoChat-R1\nboosts performance several-fold in tasks like temporal grounding (+31.8) and\nobject tracking (+31.2). Additionally, it significantly improves on general QA\nbenchmarks such as VideoMME (+0.9), MVBench (+1.0), and Perception Test (+0.9).\nOur findings underscore the potential of RFT for specialized task enhancement\nof Video MLLMs. We hope our work offers valuable insights for future RL\nresearch in video MLLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.06958.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6621
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.05287",
      "authors": [
        {
          "_id": "67f6b394b67801f1ab494709",
          "user": {
            "_id": "67f6b0fd2142abc30f1a193e",
            "avatarUrl": "/avatars/abc81a1bef1055da378c780d435dcc0a.svg",
            "isPro": false,
            "fullname": "Hui Zhang",
            "user": "ethHuiZhang",
            "type": "user"
          },
          "name": "Hui Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-10T06:45:15.453Z",
          "hidden": false
        },
        {
          "_id": "67f6b394b67801f1ab49470a",
          "name": "Zijian Wu",
          "hidden": false
        },
        {
          "_id": "67f6b394b67801f1ab49470b",
          "name": "Linyi Huang",
          "hidden": false
        },
        {
          "_id": "67f6b394b67801f1ab49470c",
          "name": "Sammy Christen",
          "hidden": false
        },
        {
          "_id": "67f6b394b67801f1ab49470d",
          "name": "Jie Song",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-07T17:38:19.000Z",
      "submittedOnDailyAt": "2025-04-10T06:35:15.115Z",
      "title": "RobustDexGrasp : Observation de mains robustes et sûres pour une vision générale des objets communs",
      "submittedOnDailyBy": {
        "_id": "67f6b0fd2142abc30f1a193e",
        "avatarUrl": "/avatars/abc81a1bef1055da378c780d435dcc0a.svg",
        "isPro": false,
        "fullname": "Hui Zhang",
        "user": "ethHuiZhang",
        "type": "user"
      },
      "summary": "Capturer des différences fortes dans des observations multiples de différents objets est la base des robots doubles. Dans des études précédentes, des objets complètement observables, des guides d'experts ou des postures statiques ont été utilisés pour capturer des différences, mais ces méthodes limitaient l'extensibilité et l'adaptation à la confusion externe. Dans cet article, nous proposons un cadre d'apprentissage par renforcement qui permet de capturer des différences dynamiques d'objets non vus en 0-shot. De plus, il peut effectuer des actions adaptatives face à la confusion externe. En utilisant une représentation concentrée des objets, nous extrayons des caractéristiques de forme, nous mettons en avant des caractéristiques locales liées à l'interaction et nous améliorons la robustesse face aux changements de forme et aux incertitudes. Pour répondre à la confusion avec des observations limitées, nous proposons un état d'apprentissage mixte de Kalman. Tout d'abord, nous utilisons une rétroaction visuelle tactile avec une distribution privilégiée du temps pour induire l'apprentissage de la politique par apprentissage simulé, puis nous passons à l'apprentissage par renforcement pour apprendre des actions adaptatives face au bruit d'observation et à la randomisation dynamique. Les expérimentations montrent que l'extensibilité pour capturer des différences dans des postures sans confusion d'objets non vus a été renforcée, atteignant un succès de 97.0% avec 247,786 objets de simulation et un succès de 94.6% avec 512 objets réels. De plus, des évaluations quantitatives et qualitatives peuvent démontrer sa robustesse face à diverses confusions. Page du projet : https://zdchan.github.io/Robust_DexGrasp/",
      "upvotes": 0,
      "discussionId": "67f6b399b67801f1ab49487f",
      "projectPage": "https://zdchan.github.io/Robust_DexGrasp/"
    },
    "publishedAt": "2025-04-07T13:38:19.000Z",
    "title": "RobustDexGrasp: Robust Dexterous Grasping of General Objects from\n  Single-view Perception",
    "summary": "Robust grasping of various objects from single-view perception is fundamental\nfor dexterous robots. Previous works often rely on fully observable objects,\nexpert demonstrations, or static grasping poses, which restrict their\ngeneralization ability and adaptability to external disturbances. In this\npaper, we present a reinforcement-learning-based framework that enables\nzero-shot dynamic dexterous grasping of a wide range of unseen objects from\nsingle-view perception, while performing adaptive motions to external\ndisturbances. We utilize a hand-centric object representation for shape feature\nextraction that emphasizes interaction-relevant local shapes, enhancing\nrobustness to shape variance and uncertainty. To enable effective hand\nadaptation to disturbances with limited observations, we propose a mixed\ncurriculum learning strategy, which first utilizes imitation learning to\ndistill a policy trained with privileged real-time visual-tactile feedback, and\ngradually transfers to reinforcement learning to learn adaptive motions under\ndisturbances caused by observation noises and dynamic randomization. Our\nexperiments demonstrate strong generalization in grasping unseen objects with\nrandom poses, achieving success rates of 97.0% across 247,786 simulated objects\nand 94.6% across 512 real objects. We also demonstrate the robustness of our\nmethod to various disturbances, including unobserved object movement and\nexternal forces, through both quantitative and qualitative evaluations. Project\nPage: https://zdchan.github.io/Robust_DexGrasp/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.05287.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67f6b0fd2142abc30f1a193e",
      "avatarUrl": "/avatars/abc81a1bef1055da378c780d435dcc0a.svg",
      "fullname": "Hui Zhang",
      "name": "ethHuiZhang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]