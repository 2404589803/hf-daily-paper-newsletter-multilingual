[
  {
    "paper": {
      "id": "2502.05173",
      "authors": [
        {
          "_id": "67a97a47174028234b74f687",
          "name": "Xilin Wei",
          "hidden": false
        },
        {
          "_id": "67a97a47174028234b74f688",
          "user": {
            "_id": "64f033ef82c6eea604c4da8b",
            "avatarUrl": "/avatars/51b93fea7fd68b4274ee03701245dcca.svg",
            "isPro": false,
            "fullname": "Liu Xiaoran",
            "user": "LiuXR",
            "type": "user"
          },
          "name": "Xiaoran Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-10T09:49:59.999Z",
          "hidden": false
        },
        {
          "_id": "67a97a47174028234b74f689",
          "user": {
            "_id": "63859cf3b2906edaf83af9f0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63859cf3b2906edaf83af9f0/iUQm5FAomzqYi6fkqIn9F.jpeg",
            "isPro": false,
            "fullname": "Yuhang Zang",
            "user": "yuhangzang",
            "type": "user"
          },
          "name": "Yuhang Zang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-10T09:50:02.011Z",
          "hidden": false
        },
        {
          "_id": "67a97a47174028234b74f68a",
          "name": "Xiaoyi Dong",
          "hidden": false
        },
        {
          "_id": "67a97a47174028234b74f68b",
          "name": "Pan Zhang",
          "hidden": false
        },
        {
          "_id": "67a97a47174028234b74f68c",
          "name": "Yuhang Cao",
          "hidden": false
        },
        {
          "_id": "67a97a47174028234b74f68d",
          "name": "Jian Tong",
          "hidden": false
        },
        {
          "_id": "67a97a47174028234b74f68e",
          "name": "Haodong Duan",
          "hidden": false
        },
        {
          "_id": "67a97a47174028234b74f68f",
          "name": "Qipeng Guo",
          "hidden": false
        },
        {
          "_id": "67a97a47174028234b74f690",
          "name": "Jiaqi Wang",
          "hidden": false
        },
        {
          "_id": "67a97a47174028234b74f691",
          "name": "Xipeng Qiu",
          "hidden": false
        },
        {
          "_id": "67a97a47174028234b74f692",
          "name": "Dahua Lin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-07T18:56:04.000Z",
      "title": "VideoRoPE : Quelles sont les conditions appropriées pour générer un vidéo de rotation de position injection ?",
      "summary": "Les positions de rotation (RoPE) et leurs variantes sont largement utilisées en raison de leur capacité à comprendre des contextes longs, mais l'extension de ces méthodes à des images pour comprendre des structures complexes d'ordre spatio-temporel reste un défi ouvert. Dans cette étude, nous identifions quatre caractéristiques essentielles nécessaires pour une application efficace de RoPE et examinons des aspects qui n'ont pas été complètement considérés dans les recherches précédentes. Dans une partie de notre analyse, nous proposons le défi V-NIAH-D (Visuo-NIAH-D), qui ajoute un distorseur périodique à V-NIAH. L'objectif de V-NIAH-D est de démontrer que les variantes précédentes de RoPE échouent à maintenir l'ordre temporel correct, ce qui peut entraîner des erreurs dans le distorseur. En nous basant sur cette analyse, nous proposons VideoRoPE. VideoRoPE concevoit une structure 3D pour maintenir les relations d'ordre spatio-temporel. VideoRoPE utilise une distribution de temps à faible fréquence pour supprimer les mouvements périodiques, une disposition de gradients pour maintenir la symétrie spatiale, et sépare l'espace temporel et l'indice spatial en utilisant un espace temporel variable. VideoRoPE concorde dans des tâches ultérieures comme la recherche de vidéos de grand contexte, la compréhension de vidéos et la génération de vidéos par rapport aux variantes précédentes de RoPE. Le code est disponible sur https://github.com/Wiselnn570/VideoRoPE.",
      "upvotes": 32,
      "discussionId": "67a97a4a174028234b74f707"
    },
    "publishedAt": "2025-02-09T23:03:21.947Z",
    "title": "VideoRoPE: What Makes for Good Video Rotary Position Embedding?",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05173.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b4eec4faa3181a5eab9c46",
      "avatarUrl": "/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg",
      "fullname": "Jiaqi Wang",
      "name": "myownskyW7",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isMod": false,
      "followerCount": 15
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.04507",
      "authors": [
        {
          "_id": "67a98cd1b8b21202c9004628",
          "name": "Peiyuan Zhang",
          "hidden": false
        },
        {
          "_id": "67a98cd1b8b21202c9004629",
          "user": {
            "_id": "65416817271d3bc4d70f6745",
            "avatarUrl": "/avatars/55cc24918c62ab39540c4df813b026ef.svg",
            "isPro": false,
            "fullname": "Yongqi Chen",
            "user": "BrianChen1129",
            "type": "user"
          },
          "name": "Yongqi Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-10T09:49:48.410Z",
          "hidden": false
        },
        {
          "_id": "67a98cd1b8b21202c900462a",
          "name": "Runlong Su",
          "hidden": false
        },
        {
          "_id": "67a98cd1b8b21202c900462b",
          "name": "Hangliang Ding",
          "hidden": false
        },
        {
          "_id": "67a98cd1b8b21202c900462c",
          "name": "Ion Stoica",
          "hidden": false
        },
        {
          "_id": "67a98cd1b8b21202c900462d",
          "name": "Zhenghong Liu",
          "hidden": false
        },
        {
          "_id": "67a98cd1b8b21202c900462e",
          "name": "Hao Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-06T21:17:09.000Z",
      "title": "Attention avec style de Sliding pour la génération de vidéos rapides",
      "summary": "Les Transformers de Diffusion (DiTs) utilisent la théorie de diffusion 3D pour atteindre une génération de vidéos plus avancées, bien que le coût en calcul soit élevé. Pendant la génération d'une vidéo de 5 secondes à 720p, le temps de diffusion représente seulement 945 secondes, mais occupe 800 secondes. Dans cet article, nous présentons l'Attention Sliding Style (STA) comme solution à ce problème. L'STA se base sur le fait que les scores d'attention dans des modèles de diffusion entraînés précédemment se concentrent principalement dans des fenêtres 3D locales. En appliquant l'attention dans des domaines spectral et temporel locaux, l'STA réduit l'inefficacité de la diffusion. Au contraire de l'Attention Sliding Window (SWA) basée sur des tokens, l'STA utilise un design de fenêtres de décalage pour la reconnaissance de nouvelles outils de hardware, permettant une meilleure efficacité tout en maintenant la représentation. Grâce à des ajustements au niveau de la fenêtre de reconnaissance, l'STA fournit des implémentations efficaces d'attention 2D/3D, atteignant un utilisation maximale de 58,79% de la capacité de traitement (MFU). De plus, l'STA accélère FlashAttention-2 (FA2) dans un intervalle de 2,8 à 17 fois et FlashAttention-3 (FA3) dans un intervalle de 1,6 à 10 fois. Dans les DiTs de vidéo avancés, l'STA réduit finalement le temps de diffusion de 945 secondes à 685 secondes, en fonctionnant sans changement de qualité et sans nécessité d'entraînement. Après une fin de mise à jour, le temps de diffusion est réduit à 268 secondes, avec un taux d'erreur de diffusion du modèle VBench autorisé de 0,09%.",
      "upvotes": 30,
      "discussionId": "67a98cd7b8b21202c90047c5"
    },
    "publishedAt": "2025-02-10T00:22:26.568Z",
    "title": "Fast Video Generation with Sliding Tile Attention",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04507.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63565cc56d7fcf1bedb7d347",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63565cc56d7fcf1bedb7d347/XGcHP4VkO_oieA1gZ4IAX.jpeg",
      "fullname": "Zhang Peiyuan",
      "name": "PY007",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 80
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.05176",
      "authors": [
        {
          "_id": "67a9889dc1fbde5146aba8b1",
          "name": "Chung-Ho Wu",
          "hidden": false
        },
        {
          "_id": "67a9889dc1fbde5146aba8b2",
          "name": "Yang-Jung Chen",
          "hidden": false
        },
        {
          "_id": "67a9889dc1fbde5146aba8b3",
          "name": "Ying-Huan Chen",
          "hidden": false
        },
        {
          "_id": "67a9889dc1fbde5146aba8b4",
          "name": "Jie-Ying Lee",
          "hidden": false
        },
        {
          "_id": "67a9889dc1fbde5146aba8b5",
          "name": "Bo-Hsu Ke",
          "hidden": false
        },
        {
          "_id": "67a9889dc1fbde5146aba8b6",
          "name": "Chun-Wei Tuan Mu",
          "hidden": false
        },
        {
          "_id": "67a9889dc1fbde5146aba8b7",
          "name": "Yi-Chuan Huang",
          "hidden": false
        },
        {
          "_id": "67a9889dc1fbde5146aba8b8",
          "name": "Chin-Yang Lin",
          "hidden": false
        },
        {
          "_id": "67a9889dc1fbde5146aba8b9",
          "user": {
            "_id": "64ae22dd1aee69ece065cdcd",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae22dd1aee69ece065cdcd/JG7QaHIrr4i2k4uwR4pZK.png",
            "isPro": false,
            "fullname": "Min-Hung Chen",
            "user": "cmhungsteve",
            "type": "user"
          },
          "name": "Min-Hung Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-10T09:49:50.370Z",
          "hidden": false
        },
        {
          "_id": "67a9889dc1fbde5146aba8ba",
          "name": "Yen-Yu Lin",
          "hidden": false
        },
        {
          "_id": "67a9889dc1fbde5146aba8bb",
          "name": "Yu-Lun Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-07T18:59:55.000Z",
      "title": "Aurora Fusion 360 : Implémentation de 360 degrés sans limites basée sur des références, avec ornements dans des zones non visibles",
      "summary": "La 3D scène ingénio est crucial dans de nombreuses applications, allant des réalités virtuelles à la visualisation architecturale, mais les méthodes actuelles rencontrent des défis dans la consistance de la perspective et la précision géométrique dans des scènes sans limites de 360 degrés. Nous présentons un nouveau méthode basé sur des références qui permet l'élimination de objets de haute qualité et le remplissage de défauts dans des scènes 3D, appelé AuraFusion360. Notre approche consiste en : (1) la génération de parties cachées mesurées, pour un bon reconnaissance de la masque, (2) un diffuseur de guides adaptatifs, qui permet un point d'initialisation précis sans nécessité d'entraînement supplémentaire, (3) l'expansion basée sur SDEdit, qui garantit la consistance des points de retour. De plus, nous présentons le premier dataset détaillé qui inclut des données réelles de scènes sans limites de 360 degrés, 360-USID. Grâce à de larges expériences, AuraFusion360 dépasse significativement les méthodes actuelles, maintenant la précision de localisation à travers des changements de perspective et atteignant une qualité visuelle élevée. Vous pouvez voir les résultats sur un video et le dataset sur la page du projet.",
      "upvotes": 18,
      "discussionId": "67a988a4c1fbde5146abaa3b"
    },
    "publishedAt": "2025-02-10T00:05:28.205Z",
    "title": "AuraFusion360: Augmented Unseen Region Alignment for Reference-based 360° Unbounded Scene Inpainting",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/KMKt5j_3UB0zDhxjSiyxI.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05176.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6459d5da3b6fafd9664807ab",
      "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg",
      "fullname": "Yu-Lun Liu",
      "name": "yulunliu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.04896",
      "authors": [
        {
          "_id": "67a983ea9b72585dd12587fb",
          "user": {
            "_id": "6412a33900634c4fe9873652",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6412a33900634c4fe9873652/Nmn_yRA1gGD2VO1YbSOYF.jpeg",
            "isPro": false,
            "fullname": "Shoufa Chen",
            "user": "ShoufaChen",
            "type": "user"
          },
          "name": "Shoufa Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-10T09:49:52.136Z",
          "hidden": false
        },
        {
          "_id": "67a983ea9b72585dd12587fc",
          "name": "Chongjian Ge",
          "hidden": false
        },
        {
          "_id": "67a983ea9b72585dd12587fd",
          "name": "Yuqi Zhang",
          "hidden": false
        },
        {
          "_id": "67a983ea9b72585dd12587fe",
          "name": "Yida Zhang",
          "hidden": false
        },
        {
          "_id": "67a983ea9b72585dd12587ff",
          "name": "Fengda Zhu",
          "hidden": false
        },
        {
          "_id": "67a983ea9b72585dd1258800",
          "name": "Hao Yang",
          "hidden": false
        },
        {
          "_id": "67a983ea9b72585dd1258801",
          "name": "Hongxiang Hao",
          "hidden": false
        },
        {
          "_id": "67a983ea9b72585dd1258802",
          "name": "Hui Wu",
          "hidden": false
        },
        {
          "_id": "67a983ea9b72585dd1258803",
          "name": "Zhichao Lai",
          "hidden": false
        },
        {
          "_id": "67a983ea9b72585dd1258804",
          "name": "Yifei Hu",
          "hidden": false
        },
        {
          "_id": "67a983ea9b72585dd1258805",
          "name": "Ting-Che Lin",
          "hidden": false
        },
        {
          "_id": "67a983ea9b72585dd1258806",
          "name": "Shilong Zhang",
          "hidden": false
        },
        {
          "_id": "67a983ea9b72585dd1258807",
          "name": "Fu Li",
          "hidden": false
        },
        {
          "_id": "67a983ea9b72585dd1258808",
          "name": "Chuan Li",
          "hidden": false
        },
        {
          "_id": "67a983ea9b72585dd1258809",
          "name": "Xing Wang",
          "hidden": false
        },
        {
          "_id": "67a983ea9b72585dd125880a",
          "name": "Yanghua Peng",
          "hidden": false
        },
        {
          "_id": "67a983ea9b72585dd125880b",
          "name": "Peize Sun",
          "hidden": false
        },
        {
          "_id": "67a983ea9b72585dd125880c",
          "name": "Ping Luo",
          "hidden": false
        },
        {
          "_id": "67a983ea9b72585dd125880d",
          "name": "Yi Jiang",
          "hidden": false
        },
        {
          "_id": "67a983ea9b72585dd125880e",
          "name": "Zehuan Yuan",
          "hidden": false
        },
        {
          "_id": "67a983ea9b72585dd125880f",
          "name": "Bingyue Peng",
          "hidden": false
        },
        {
          "_id": "67a983ea9b72585dd1258810",
          "name": "Xiaobing Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-07T13:03:55.000Z",
      "title": "Sûr, voici la traduction en français :\n\n\"Sun Kū : Modèle de Base de Pièce pour la Génération de Vidéos Basée sur la Fondation\"",
      "summary": "Dans cet article, on présente l'introduction à la dernière architecture de traitement d'images et vidéos, Goku. Goku est une série de modèles qui utilisent des transformeurs normalisés pour générer des images et des vidéos de manière simultanée, et est le leader de l'industrie par son meilleur rendement. On détaille les éléments fondamentaux permettant la génération de visualisations de haute qualité, y compris le système de chargement de données, le design de l'architecture du modèle, la formalisation des formes et l'infrastructure efficace et puissante pour l'entraînement à grande échelle. Les modèles de Goku montrent le meilleur rendement tant qualitativement que quantitatif, établissant de nouveaux standards de test dans toutes les principales tâches. En particulier, on a atteint un 0,76 sur GenEval, 83,65 sur DPG-Bench et 84,85 sur VBench. Nous sommes convaincus que cette recherche fournira des conseils précieux et des développements pratiques pour le développement de modèles de génération d'images et de vidéos dans le domaine des points de vente.",
      "upvotes": 14,
      "discussionId": "67a983ee9b72585dd125890f"
    },
    "publishedAt": "2025-02-09T23:43:39.239Z",
    "title": "Goku: Flow Based Video Generative Foundation Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04896.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5997
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.05003",
      "authors": [
        {
          "_id": "67a9b1a69a99341e859c488d",
          "user": {
            "_id": "623753b5eddd7763adc9346a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623753b5eddd7763adc9346a/rcpQAKZNrkn1-tMtraQBX.jpeg",
            "isPro": false,
            "fullname": "Andrei Panferov",
            "user": "BlackSamorez",
            "type": "user"
          },
          "name": "Andrei Panferov",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-02-10T08:09:18.686Z",
          "hidden": false
        },
        {
          "_id": "67a9b1a69a99341e859c488e",
          "name": "Jiale Chen",
          "hidden": false
        },
        {
          "_id": "67a9b1a69a99341e859c488f",
          "user": {
            "_id": "632a2e325f2ff1958c0103be",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/632a2e325f2ff1958c0103be/Tb0ql9e4LcaFktTK1hzqe.jpeg",
            "isPro": false,
            "fullname": "Soroush Tabesh",
            "user": "soroushtabesh",
            "type": "user"
          },
          "name": "Soroush Tabesh",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-10T09:49:37.573Z",
          "hidden": false
        },
        {
          "_id": "67a9b1a69a99341e859c4890",
          "name": "Roberto L. Castro",
          "hidden": false
        },
        {
          "_id": "67a9b1a69a99341e859c4891",
          "user": {
            "_id": "6526b8ebba9a8279c139616b",
            "avatarUrl": "/avatars/09f6b677603a03be128996a0765233e6.svg",
            "isPro": false,
            "fullname": "Mahdi Nikdan",
            "user": "mnikdan97",
            "type": "user"
          },
          "name": "Mahdi Nikdan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-10T09:50:25.944Z",
          "hidden": false
        },
        {
          "_id": "67a9b1a69a99341e859c4892",
          "user": {
            "_id": "64ef52c2718f94ae8e78a5e7",
            "avatarUrl": "/avatars/d169f4ee62786a3eb4a3fa9d1fec52e9.svg",
            "isPro": false,
            "fullname": "Alistarh",
            "user": "d-alistarh",
            "type": "user"
          },
          "name": "Dan Alistarh",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-10T09:49:35.449Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-07T15:23:34.000Z",
      "title": "Quelle est la poids d'un bit et comment il est utilisé pour régler l'entraînement d'un modèle de langue de réseau de neurones (LLM) pour améliorer sa stabilité ?",
      "summary": "Une des grandes réductions de coût dans un grand modèle de langue (LLMs) est l'utilisation de la quantification ou de la représentation épars pendant l'entraînement ou l'implémentation. Les méthodes de compression après l'entraînement sont très populaires. Cependant, le problème d'entraîner directement ces représentations pour obtenir des modèles de compression plus précis reste ouvert. Par exemple, des recherches récentes (arXiv:2411.04330v2) montrent que l'« optimum » largeur en bits pour entraîner un modèle avec QAT (Entraînement Connaissant de la Quantification) est l'utilisation de poids et d'activations de 8 bits, comparé à la précision standard de FP16/BF16.\n\nPour atteindre cette amélioration, nous utilisons un nouveau méthode appelé QuEST. Ce méthode offre une précision améliorée par rapport à FP16, permet de réduire la taille du modèle et peut utiliser des poids et des activations de 4 bits ou moins. De plus, QuEST permet d'entraîner un modèle stable en utilisant des poids et des activations de 1 bit. QuEST améliore deux aspects essentiels des méthodes de QAT : (1) la quantification de distributions précises et rapides par la normalisation de Hadamard et l'optimisation de MSE, et (2) l'évaluation des gradients sans bruit lors de la quantification et la minimisation explicite de l'erreur de la « précision totale » (même si elle est inconnue). Les expérimentations avec l'architecture du type Llama montrent que QuEST peut exécuter des modèles de manière efficace, ce qui peut également être étendu aux représentations épars. Le support de claviers de GPU montre que les modèles générés avec QuEST peuvent être exécutés de manière efficace, et le code est disponible sur https://github.com/IST-DASLab/QuEST.",
      "upvotes": 11,
      "discussionId": "67a9b1a79a99341e859c48c7"
    },
    "publishedAt": "2025-02-10T03:00:12.065Z",
    "title": "QuEST: Stable Training of LLMs with 1-Bit Weights and Activations",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05003.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ef52c2718f94ae8e78a5e7",
      "avatarUrl": "/avatars/d169f4ee62786a3eb4a3fa9d1fec52e9.svg",
      "fullname": "Alistarh",
      "name": "d-alistarh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.05163",
      "authors": [
        {
          "_id": "67a9604851169a582d14c113",
          "user": {
            "_id": "642f4c789b2484d7d8551a93",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642f4c789b2484d7d8551a93/0lH4YXcbZa-Xlzj6ESo7F.jpeg",
            "isPro": true,
            "fullname": "Yihe Deng",
            "user": "ydeng9",
            "type": "user"
          },
          "name": "Yihe Deng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-10T09:50:06.136Z",
          "hidden": false
        },
        {
          "_id": "67a9604851169a582d14c114",
          "name": "Yu Yang",
          "hidden": false
        },
        {
          "_id": "67a9604851169a582d14c115",
          "name": "Junkai Zhang",
          "hidden": false
        },
        {
          "_id": "67a9604851169a582d14c116",
          "name": "Wei Wang",
          "hidden": false
        },
        {
          "_id": "67a9604851169a582d14c117",
          "name": "Bo Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-07T18:45:03.000Z",
      "title": "Dualogue : Cadre de Conduite de Véhicules RL pour la Protection de l'IA Multilingue de Deux Utilisateurs",
      "summary": "Le rapide développement des modèles de langue grande (LLMs) a augmenté la nécessité de modèles de responsabilité dans leur utilisation. En particulier, l'importance de cette responsabilité a augmenté dans le domaine de la détection de contenus insecuritaires et illégaux. Il est connu qu'il existe une abondance de données sécuritaires en anglais. Cependant, l'absence de données de sécurité ouvertes dans d'autres langues a empêché que la recherche sur les modèles de responsabilité multilingues ne soit pas menée. Pour combler cette lacune, nous proposons un nouveau cadre d'apprentissage par refonte (RL) avec deux joueurs. Dans ce cadre, le générateur et le modèle de responsabilité évoluent de manière compétitive, avec l'objectif de générer des données synthétiques de haute qualité pour l'entraînement de responsabilité multilingue. Cette interaction est régularisée théoriquement et a été démontrée qu'elle atteint un équilibre Nash. Dans l'évaluation expérimentale, notre modèle \\ours dépasse les modèles les plus avancés et atteint un amélioration de 10% par rapport à LlamaGuard3 (8B), en plus d'être 4,5 fois plus rapide et de taille 0,5B. Dans des tâches de sécurité multilingue, notamment sur des données réelles recueillies, notre modèle résout l'inégalité entre les langues riches en ressources. Les études d'impact soulignent l'importance de la génération de données synthétiques pour spécifier l'inégalité dans les données ouvertes. Ces résultats suggèrent une approche échellable et efficace pour la génération de données synthétiques, associée à l'amélioration de la sécurité des LLMs à travers des modèles de responsabilité multilingues. Le code, le modèle et les données sont disponibles en open source sur https://github.com/yihedeng9/DuoGuard.",
      "upvotes": 11,
      "discussionId": "67a9604951169a582d14c14d"
    },
    "publishedAt": "2025-02-10T00:43:32.191Z",
    "title": "DuoGuard: A Two-Player RL-Driven Framework for Multilingual LLM Guardrails",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05163.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642f4c789b2484d7d8551a93",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642f4c789b2484d7d8551a93/0lH4YXcbZa-Xlzj6ESo7F.jpeg",
      "fullname": "Yihe Deng",
      "name": "ydeng9",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.05171",
      "authors": [
        {
          "_id": "67a97e27495b23306cd5ea56",
          "name": "Jonas Geiping",
          "hidden": false
        },
        {
          "_id": "67a97e27495b23306cd5ea57",
          "name": "Sean McLeish",
          "hidden": false
        },
        {
          "_id": "67a97e27495b23306cd5ea58",
          "name": "Neel Jain",
          "hidden": false
        },
        {
          "_id": "67a97e27495b23306cd5ea59",
          "name": "John Kirchenbauer",
          "hidden": false
        },
        {
          "_id": "67a97e27495b23306cd5ea5a",
          "name": "Siddharth Singh",
          "hidden": false
        },
        {
          "_id": "67a97e27495b23306cd5ea5b",
          "name": "Brian R. Bartoldson",
          "hidden": false
        },
        {
          "_id": "67a97e27495b23306cd5ea5c",
          "name": "Bhavya Kailkhura",
          "hidden": false
        },
        {
          "_id": "67a97e27495b23306cd5ea5d",
          "name": "Abhinav Bhatele",
          "hidden": false
        },
        {
          "_id": "67a97e27495b23306cd5ea5e",
          "name": "Tom Goldstein",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-07T18:55:02.000Z",
      "title": "La méthodologie d'échelle en temps de test de calcul, réalisée à travers la théorie de la réseau de réseaux, utilise l'approche d'accès aux réseaux convolutifs.",
      "summary": "Nous étudions une nouvelle architecture de modèles de langage qui permet d'occulter la computation pendant le processus d'évaluation. Notre modèle fonctionne en itérant des blocs récursifs et peut s'étendre à une profondeur arbitraire lors du processus d'évaluation. Cela oppose au méthode d'expansion de la computation par génération de tokens, utilisée par les modèles de raisonnement dominants. Notre méthodologie est différente de celle basée sur les chaînes de Scope, car elle ne nécessite pas de données d'entraînement spécialisées, fonctionne avec de petites fenêtres de contexte et permet d'identifier différents types de raisons difficiles à exprimer en langue. Nous avons étendu nos modèles de test à 350 millions de paramètres et 800 milliards de tokens. Nous avons vérifié que ces modèles peuvent améliorer le rendement sur les benchmarks de raisonnement, parfois de manière surprenante, démontrant une charge de calcul équivalente à 500 millions de paramètres.",
      "upvotes": 7,
      "discussionId": "67a97e29495b23306cd5eae5"
    },
    "publishedAt": "2025-02-09T23:19:16.714Z",
    "title": "Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05171.png",
    "numComments": 4,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5997
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.04403",
      "authors": [
        {
          "_id": "67a97c7542d4d2f92ee57d20",
          "name": "David Abel",
          "hidden": false
        },
        {
          "_id": "67a97c7542d4d2f92ee57d21",
          "name": "André Barreto",
          "hidden": false
        },
        {
          "_id": "67a97c7542d4d2f92ee57d22",
          "name": "Michael Bowling",
          "hidden": false
        },
        {
          "_id": "67a97c7542d4d2f92ee57d23",
          "name": "Will Dabney",
          "hidden": false
        },
        {
          "_id": "67a97c7542d4d2f92ee57d24",
          "name": "Shi Dong",
          "hidden": false
        },
        {
          "_id": "67a97c7542d4d2f92ee57d25",
          "name": "Steven Hansen",
          "hidden": false
        },
        {
          "_id": "67a97c7542d4d2f92ee57d26",
          "name": "Anna Harutyunyan",
          "hidden": false
        },
        {
          "_id": "67a97c7542d4d2f92ee57d27",
          "name": "Khimya Khetarpal",
          "hidden": false
        },
        {
          "_id": "67a97c7542d4d2f92ee57d28",
          "name": "Clare Lyle",
          "hidden": false
        },
        {
          "_id": "67a97c7542d4d2f92ee57d29",
          "name": "Razvan Pascanu",
          "hidden": false
        },
        {
          "_id": "67a97c7542d4d2f92ee57d2a",
          "name": "Georgios Piliouras",
          "hidden": false
        },
        {
          "_id": "67a97c7542d4d2f92ee57d2b",
          "name": "Doina Precup",
          "hidden": false
        },
        {
          "_id": "67a97c7542d4d2f92ee57d2c",
          "name": "Jonathan Richens",
          "hidden": false
        },
        {
          "_id": "67a97c7542d4d2f92ee57d2d",
          "name": "Mark Rowland",
          "hidden": false
        },
        {
          "_id": "67a97c7542d4d2f92ee57d2e",
          "name": "Tom Schaul",
          "hidden": false
        },
        {
          "_id": "67a97c7542d4d2f92ee57d2f",
          "name": "Satinder Singh",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-06T08:34:57.000Z",
      "title": "Agence dépend du cadre.",
      "summary": "Bonjour !\n\nL'agence (Agency) est un système capable de contrôler les résultats liés à l'objectif, ce qui la rend l'un des thèmes centraux de la recherche dans les domaines de la biologie, la philosophie, les sciences cognitives et l'intelligence artificielle. Déterminer si une entité a une agence est une question très complexe, et Dennett (1989) soulève le problème de découvrir si des pierres, des cybernétiques et des robots ont une agence. Pour aborder ce problème, nous nous concentrons depuis la perspective de l'apprentissage par renforcement (Reinforcement Learning). L'agence est essentiellement dépendante d'un cadre, par conséquent, pour mesurer l'agence d'un système, il est nécessaire de mesurer sa dépendance du cadre. Pour soutenir cette position, Banderara (2009) et Moreno (2018) fournissent une démonstration logique que les caractéristiques basiques de l'agence ont une dépendance du cadre. La théorie de base de l'agence conclut qu'elle dépend d'un cadre, et on discute les effets de cet approche dans l'apprentissage par renforcement.",
      "upvotes": 7,
      "discussionId": "67a97c7642d4d2f92ee57d77"
    },
    "publishedAt": "2025-02-09T23:11:57.959Z",
    "title": "Agency Is Frame-Dependent",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04403.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5997
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.04520",
      "authors": [
        {
          "_id": "67a97eea96d822bc6e13a1bb",
          "name": "Letian Peng",
          "hidden": false
        },
        {
          "_id": "67a97eea96d822bc6e13a1bc",
          "name": "Chenyang An",
          "hidden": false
        },
        {
          "_id": "67a97eea96d822bc6e13a1bd",
          "name": "Shibo Hao",
          "hidden": false
        },
        {
          "_id": "67a97eea96d822bc6e13a1be",
          "name": "Chengyu Dong",
          "hidden": false
        },
        {
          "_id": "67a97eea96d822bc6e13a1bf",
          "user": {
            "_id": "660655119e3555d648f6c6b5",
            "avatarUrl": "/avatars/ae1e2c97a08be39b77a9f1a5c2a718ef.svg",
            "isPro": false,
            "fullname": "Jingbo Shang",
            "user": "shangjingbo",
            "type": "user"
          },
          "name": "Jingbo Shang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-10T09:49:54.200Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-06T21:44:30.000Z",
      "title": "L'amplitude de la composition de LM et la relation rectiligne avec la fantaisie",
      "summary": "Le modèle de langue (LM) de généralisation est actuellement l'objet d'une discussion active et compare la possibilité d'une intelligence généralisée avec le dégât de la structure de base de connaissance (par exemple, l'énigme de l'inversion/conversion). Cet article révèle le phénomène de la corrélation linéaire dans les constituants de la structure de connaissance d'un LM. En particulier, la transformation linéaire entre connaissances associées cartographie la logique de prédiction du token suivant dans les prompts. Par exemple, \"X vive dans la ville de\" → \"X vive dans le pays de\". Cela imite la linéarité de la constitution du savoir humain. Ce que nous avons découvert est que la relation positive et réaliste avec l'ajustement micrographique dans le cas d'un LM généralisé se aligne avec l'actualisation du savoir, tandis que le cas qui déviant attire l'attention vers les erreurs. Les résultats expérimentaux montrent que la corrélation linéaire peut jouer un rôle potentiel dans l'identification de la généralisation d'un LM. Enfin, cette corrélation linéaire peut être apprise dans une réseau rétropropagé unidirectionnel et avec des représentations de mots pré-entraînées, ce qui démontre sa grande dépendance sur la généralisation d'un LM.",
      "upvotes": 6,
      "discussionId": "67a97eea96d822bc6e13a1e7"
    },
    "publishedAt": "2025-02-09T23:22:06.784Z",
    "title": "Linear Correlation in LM's Compositional Generalization and Hallucination",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04520.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5997
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.05179",
      "authors": [
        {
          "_id": "67a9901cc0310368e2488929",
          "name": "Shilong Zhang",
          "hidden": false
        },
        {
          "_id": "67a9901cc0310368e248892a",
          "name": "Wenbo Li",
          "hidden": false
        },
        {
          "_id": "67a9901cc0310368e248892b",
          "user": {
            "_id": "6412a33900634c4fe9873652",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6412a33900634c4fe9873652/Nmn_yRA1gGD2VO1YbSOYF.jpeg",
            "isPro": false,
            "fullname": "Shoufa Chen",
            "user": "ShoufaChen",
            "type": "user"
          },
          "name": "Shoufa Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-10T09:49:46.264Z",
          "hidden": false
        },
        {
          "_id": "67a9901cc0310368e248892c",
          "name": "Chongjian Ge",
          "hidden": false
        },
        {
          "_id": "67a9901cc0310368e248892d",
          "name": "Peize Sun",
          "hidden": false
        },
        {
          "_id": "67a9901cc0310368e248892e",
          "name": "Yida Zhang",
          "hidden": false
        },
        {
          "_id": "67a9901cc0310368e248892f",
          "name": "Yi Jiang",
          "hidden": false
        },
        {
          "_id": "67a9901cc0310368e2488930",
          "name": "Zehuan Yuan",
          "hidden": false
        },
        {
          "_id": "67a9901cc0310368e2488931",
          "name": "Binyue Peng",
          "hidden": false
        },
        {
          "_id": "67a9901cc0310368e2488932",
          "name": "Ping Luo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-07T18:59:59.000Z",
      "title": "FlashVideo : Fusion de la Fidélité des Pixels et de la Fidélité des Détails pour la Génération de Vidéos à Haute Résolution Efficace",
      "summary": "Les modèles de diffusion DiT ont réussi à obtenir un grand succès dans la génération d'animations à partir du texte, en exploitant la scalabilité de la capacité du modèle et du taille des données. Cependant, pour mettre en œuvre des contenus de haute qualité et de précision des actions correspondant aux textes, des grands paramètres du modèle et une grande quantité d'évaluations fonctionnelles (NFEs) sont nécessaires. Les détails réalistes et visuellement appréciables se reflètent généralement dans un haut rendement de sortie, ce qui augmente particulièrement la charge de calcul d'un modèle DiT en un seul pas. Pour résoudre ces problèmes, nous proposons un nouveau cadre de travail à deux étapes appelé FlashVideo. FlashVideo se concentre sur la distribution stratégique de la capacité du modèle et des NFEs à l'intérieur de chaque frame pour équilibrer la précision et la qualité de la génération. Dans la première étape, nous utilisons un processus de génération à faible résolution pour améliorer l'efficacité de calcul en utilisant de grands paramètres et un nombre suffisant d'NFEs, en prioritisant la précision du prévisionnement. Dans la deuxième étape, nous établissons un ajustement de flux entre la résolution faible et la haute pour générer des détails avec le minimum de NFEs. Avec ces résultats positifs, FlashVideo montre qu'il est possible de générer des animations à haute résolution plus avancées et d'afficher une forte efficacité de calcul. De plus, le design à deux étapes permet aux utilisateurs de voir une sortie initiale avant de confirmer la génération à toute la résolution, réduisant significativement les coûts de calcul et le temps d'attente, ainsi que d'augmenter la possibilité commerciale.",
      "upvotes": 5,
      "discussionId": "67a9901ec0310368e24889c2"
    },
    "publishedAt": "2025-02-10T00:35:37.019Z",
    "title": "FlashVideo:Flowing Fidelity to Detail for Efficient High-Resolution Video Generation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05179.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5997
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.04363",
      "authors": [
        {
          "_id": "67a98180d0dc1ed664297368",
          "name": "Bosung Kim",
          "hidden": false
        },
        {
          "_id": "67a98180d0dc1ed664297369",
          "name": "Kyuhwan Lee",
          "hidden": false
        },
        {
          "_id": "67a98180d0dc1ed66429736a",
          "name": "Isu Jeong",
          "hidden": false
        },
        {
          "_id": "67a98180d0dc1ed66429736b",
          "name": "Jungmin Cheon",
          "hidden": false
        },
        {
          "_id": "67a98180d0dc1ed66429736c",
          "name": "Yeojin Lee",
          "hidden": false
        },
        {
          "_id": "67a98180d0dc1ed66429736d",
          "name": "Seulki Lee",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-05T05:42:29.000Z",
      "title": "Sora est une technologie qui permet de générer des vidéos à partir de texte basée sur la distribution sur des dispositifs mobiles.",
      "summary": "Sora est apparu dans le dispositif. Elle est une solution leader pour la génération de vidéos à partir de textes basée sur la diffusion, qui fonctionne efficacement sur des dispositifs de niveau de téléphone intelligent. Basée sur Open-Sora, Sora dans le dispositif résout les défis de la génération de vidéos à partir de textes basée sur la diffusion sur des dispositifs mobiles avec des limitations en calcul et mémoire par trois nouvelles technologies. Tout d'abord, le Salto Lineal Proporcional (SLP) utilise un approche efficace de sauts pour réduire les étapes de desnoise excessifs nécessaires pour la diffusion de la vidéo. Ensuite, le Token Merging en Dimensions de Temps (TDTM) minimise les calculs de traitement de tokens continus dans les couches d'attention, fusionnant les tokens continus selon la dimension du temps. De plus, l'Inférence Parallèle et Chargement Dynamique (CI-DL) divise grands modèles en petits blocs qui sont chargés dynamiquement dans la mémoire et inférés en parallèle, résolvant effectivement les limitations de la mémoire du dispositif. Sora a été implémentée dans le dispositif sur l'iPhone 15 Pro, et selon l'évaluation expérimentale, elle peut générer des vidéos de haute qualité et produire des résultats similaires à ceux de Open-Sora exécuté sur un GPU de haut rendement. Ces résultats montrent que Sora dans le dispositif permet la génération de vidéos de haute qualité et de faible qualité de ressources sur des dispositifs mobiles limités, étendant l'accès, protégeant les droits des utilisateurs, réduisant la dépendance à l'infrastructure de nuage et diminuant les coûts associés. La proposition de Sora dans le dispositif est considérée comme une importante première étape pour la démocratisation des technologies de création leader, permettant aux dispositifs mobiles et intégrés la fonctionnalité de génération de vidéos. L'implémentation du code est publiée dans le répositoire GitHub : https://github.com/eai-lab/On-device-Sora.",
      "upvotes": 3,
      "discussionId": "67a98185d0dc1ed664297491"
    },
    "publishedAt": "2025-02-09T23:33:13.185Z",
    "title": "On-device Sora: Enabling Diffusion-Based Text-to-Video Generation for Mobile Devices",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04363.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5997
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.04728",
      "authors": [
        {
          "_id": "67a97d1c02da0cdf059cb0d8",
          "name": "Zhouliang Yu",
          "hidden": false
        },
        {
          "_id": "67a97d1c02da0cdf059cb0d9",
          "name": "Yuhuan Yuan",
          "hidden": false
        },
        {
          "_id": "67a97d1c02da0cdf059cb0da",
          "name": "Tim Z. Xiao",
          "hidden": false
        },
        {
          "_id": "67a97d1c02da0cdf059cb0db",
          "name": "Fuxiang Frank Xia",
          "hidden": false
        },
        {
          "_id": "67a97d1c02da0cdf059cb0dc",
          "name": "Jie Fu",
          "hidden": false
        },
        {
          "_id": "67a97d1c02da0cdf059cb0dd",
          "user": {
            "_id": "638efcf4c67af472d316d424",
            "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
            "isPro": false,
            "fullname": "Ge Zhang",
            "user": "zhangysk",
            "type": "user"
          },
          "name": "Ge Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-10T09:49:56.250Z",
          "hidden": false
        },
        {
          "_id": "67a97d1c02da0cdf059cb0de",
          "name": "Ge Lin",
          "hidden": false
        },
        {
          "_id": "67a97d1c02da0cdf059cb0df",
          "name": "Weiyang Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-07T07:52:25.000Z",
      "title": "Programmation du temps de tests pour la génération d'un modèle mondial de symboles à travers de grands modèles de langue",
      "summary": "Pour résoudre les problèmes complexes de planification, les modèles de langage grand (LLMs) doivent clairement modéliser les transformations d'état, éviter les violations de règles, suivre les contraintes et garantir l'optimalité. Ces défis sont complexifiés par la nature intrinsèquement incertaine du langage naturel. Pour surmonter cette incertitude, le langage de domaine de planification (PDDL) est utilisé pour abstrair la planification et fournir une description formelle précise de l'état. Avec le PDDL, nous pouvons créer des modèles symboliques du monde et appliquer des algorithmes de recherche classiques comme A* pour trouver des plans optimaux. Cependant, les LLMs actuels ne peuvent générer directement des zones de PDDL en raison de la rareté des données d'entraînement de PDDL pour les problèmes ouverts. Pour résoudre ce problème, nous étendons les calculs des LLMs lors du test pour améliorer leur capacité de raisonnement en PDDL et faciliter la génération de zones de PDDL de haute qualité. En particulier, nous introduisons des algorithmes simples et efficaces qui utilisent un approche de sélection de la meilleure de N pour améliorer la qualité des solutions initiales et raffiner les solutions avec apprentissage automatique. Notre méthode dépasse considérablement o1-mini dans les deux tâches : génération de zones de PDDL à partir de descriptions naturelles et génération de zones de PDDL à partir de problèmes de PDDL, atteignant un rendement supérieur de plus de 50% sans nécessité d'entraînement supplémentaire. Le PDDL est utilisé dans notre méthode pour l'abstraction de l'état. Notre méthode peut dépasser la meilleure technique dans presque tous les niveaux de compétence des tâches de planification actuelles.",
      "upvotes": 3,
      "discussionId": "67a97d1d02da0cdf059cb11a"
    },
    "publishedAt": "2025-02-09T23:17:42.258Z",
    "title": "Generating Symbolic World Models via Test-time Scaling of Large Language Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04728.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5997
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.04404",
      "authors": [
        {
          "_id": "67a97bc5500b3bcf5babc5e8",
          "user": {
            "_id": "64bb3d1eb1a618880956da76",
            "avatarUrl": "/avatars/ec393b5eee8a3ccec61107b4aa63c4d9.svg",
            "isPro": false,
            "fullname": "Xiao-Wen Yang",
            "user": "yangxw",
            "type": "user"
          },
          "name": "Xiao-Wen Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-10T09:49:57.842Z",
          "hidden": false
        },
        {
          "_id": "67a97bc5500b3bcf5babc5e9",
          "name": "Xuan-Yi Zhu",
          "hidden": false
        },
        {
          "_id": "67a97bc5500b3bcf5babc5ea",
          "name": "Wen-Da Wei",
          "hidden": false
        },
        {
          "_id": "67a97bc5500b3bcf5babc5eb",
          "name": "Ding-Chu Zhang",
          "hidden": false
        },
        {
          "_id": "67a97bc5500b3bcf5babc5ec",
          "name": "Jie-Jing Shao",
          "hidden": false
        },
        {
          "_id": "67a97bc5500b3bcf5babc5ed",
          "name": "Zhi Zhou",
          "hidden": false
        },
        {
          "_id": "67a97bc5500b3bcf5babc5ee",
          "name": "Lan-Zhe Guo",
          "hidden": false
        },
        {
          "_id": "67a97bc5500b3bcf5babc5ef",
          "name": "Yu-Feng Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-06T08:52:43.000Z",
      "title": "StepBack Forward : Amélioration de l'Inférence des Modèles de Langue par Suivi Automatique Inverse",
      "summary": "La mise en œuvre de la structure de Swapping dans les LLMs peut sembler un système similaire à celui d'OpenAI's o1, offrant une voie souhaitable pour atteindre les Reasoners de niveau 2 AGI. Cependant, des problèmes importants restent encore. L'un d'entre eux est l'over-oversampling et la dépendance excessive avec des modèles de contrôle assisté. Nous sommes convaincus que ces limitations sont causées par le fait que les LLMs ne peuvent pas intégrer le processus d'exploration. Une étape cruciale pour résoudre ces problèmes est de permettre aux LLMs de décider automatiquement le moment et le lieu pour effectuer des retours. Dans ce sens, nous proposons une structure de retours automatiques qui permet aux LLMs de retourner tant dans l'apprentissage que dans l'inférence. Cette structure transforme le processus de Swapping en un Swapping rapide grâce à des améliorations automatiques, améliorant ainsi la capacité logique et l'efficacité. Selon les évaluations expérimentales, notre proposition atteint un amélioration de 40% plus par rapport aux méthodes d'ajustement de sous-ensembles de chemins optimaux. Nous croyons que cette recherche introduit de nouvelles méthodologies pour le développement de puissants Reasoners.",
      "upvotes": 2,
      "discussionId": "67a97bc7500b3bcf5babc64e"
    },
    "publishedAt": "2025-02-09T23:09:01.160Z",
    "title": "Step Back to Leap Forward: Self-Backtracking for Boosting Reasoning of Language Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04404.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5997
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.04350",
      "authors": [
        {
          "_id": "67a97a77d163c9e6ea2bdb85",
          "name": "Yongchao Chen",
          "hidden": false
        },
        {
          "_id": "67a97a77d163c9e6ea2bdb86",
          "name": "Yilun Hao",
          "hidden": false
        },
        {
          "_id": "67a97a77d163c9e6ea2bdb87",
          "name": "Yueying Liu",
          "hidden": false
        },
        {
          "_id": "67a97a77d163c9e6ea2bdb88",
          "name": "Yang Zhang",
          "hidden": false
        },
        {
          "_id": "67a97a77d163c9e6ea2bdb89",
          "name": "Chuchu Fan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-04T15:53:59.000Z",
      "title": "CodeSteer : Modèle de langage symbolique via des orientations de code/texte",
      "summary": "Les méthodes actuelles ne peuvent pas contrôler efficacement la logique de langage et la génération de code dans les modèles de langage grands (LLMs), ce qui a conduit à un usage excessif de la capacité de calcul de symboles. Nous présentons CodeSteer, une méthodologie efficace pour guider la génération de code et de texte par un LLM. Nous avons construit un cadre de référence détaillé appelé SymBench, qui comprend 37 tâches symboliques, et nous avons synthétisé 12k traces de guidage/génération multiniveaux et 5.5k étapes de comparaison de guides. Nous utilisons un entraînement de supervision de sous-projets multiniveaux (SFT) et une optimisation directe d'intérêt (DPO) pour affiner le modèle Llama-3-8B. Le modèle résultant, CodeSteerLLM, peut guider efficacement la génération de code/texte de grands modèles en ajoutant de nouveaux vérificateurs symboliques et réponses automatiques. L'ajout de CodeSteer à GPT-4o a augmenté sa note moyenne de 53.3 à 86.4, dépassant les meilleurs LLMs actuels tels que OpenAI ou1 (82.7), o1-preview (74.8) et DeepSeek R1 (76.8) sur toutes les 37 tâches (28 confirmées, 9 non confirmées). L'entraînement de CodeSteer a démontré un améliorament moyen de 41.8 dans le rendement de GPT-4o par rapport à Claude, Mistral et GPT-3.5. Les modèles guidés par CodeSteer maintiennent une haute efficacité dans des tâches complexes et utilisent complètement la capacité de calcul de symboles. Les modèles, données et code sont disponibles sur la URL suivante : https://github.com/yongchao98/CodeSteer-v1.0.",
      "upvotes": 2,
      "discussionId": "67a97a79d163c9e6ea2bdc0c"
    },
    "publishedAt": "2025-02-09T23:03:14.294Z",
    "title": "CodeSteer: Symbolic-Augmented Language Models via Code/Text Guidance",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04350.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5997
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.04689",
      "authors": [
        {
          "_id": "67a9b911b1f5eece682d7961",
          "user": {
            "_id": "64510a21f800611f94f0d9f8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/lOeHK9Bvt3IXcB7Urx6jZ.jpeg",
            "isPro": false,
            "fullname": "Yuwei Yin",
            "user": "yuweiyin",
            "type": "user"
          },
          "name": "Yuwei Yin",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-10T09:49:32.672Z",
          "hidden": false
        },
        {
          "_id": "67a9b911b1f5eece682d7962",
          "name": "Giuseppe Carenini",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-07T06:30:33.000Z",
      "title": "ARR : Analyse des Réponses aux Questions avec des Modèles de Langue de Grande Taille, Recherche et Logique des Arguments",
      "summary": "Les modèles de langage grands (LLMs) atteignent des résultats impressionnants sur des indicateurs structurés difficiles tels que les tâches de réponse à des questions (QA). Le Zero-shot Chain-of-Thought (CoT) pruning renforce la logique des LLMs, mais est limité à \"penser un à un\" et à des directives générales. Dans cet article, nous présentons une méthodologie zero-shot efficace et intuitive appelée ARR pour résoudre des tâches de QA. Cette méthodologie se base sur trois étapes clés : l'analyse du but de la question, la recherche d'informations pertinentes et la présentation de raisons en étapes. Des expériences extensives sur des tâches de QA difficiles montrent que l'ARR améliore constamment un Baseline (sans inclure l'ARR) et dépasse le CoT. Des expériences contrôlées et des études de cas démontrent une fois de plus la contribution positive de chaque composante : l'analyse, la recherche et les raisons. En particulier, l'analyse du but est cruciale dans l'ARR. De plus, des évaluations sur une large gamme de tailles de modèles, de séries de LLMs et de configurations de génération démontrent l'efficacité, la robustesse et la capacité de généralisation de l'ARR.",
      "upvotes": 1,
      "discussionId": "67a9b911b1f5eece682d798c"
    },
    "publishedAt": "2025-02-10T03:30:51.974Z",
    "title": "ARR: Question Answering with Large Language Models via Analyzing, Retrieving, and Reasoning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04689.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64510a21f800611f94f0d9f8",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/lOeHK9Bvt3IXcB7Urx6jZ.jpeg",
      "fullname": "Yuwei Yin",
      "name": "yuweiyin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.03512",
      "authors": [
        {
          "_id": "67a9a7cb6be3ca4a7ede471e",
          "name": "Amitava Das",
          "hidden": false
        },
        {
          "_id": "67a9a7cb6be3ca4a7ede471f",
          "name": "Yaswanth Narsupalli",
          "hidden": false
        },
        {
          "_id": "67a9a7cb6be3ca4a7ede4720",
          "name": "Gurpreet Singh",
          "hidden": false
        },
        {
          "_id": "67a9a7cb6be3ca4a7ede4721",
          "name": "Vinija Jain",
          "hidden": false
        },
        {
          "_id": "67a9a7cb6be3ca4a7ede4722",
          "name": "Vasu Sharma",
          "hidden": false
        },
        {
          "_id": "67a9a7cb6be3ca4a7ede4723",
          "name": "Suranjana Trivedy",
          "hidden": false
        },
        {
          "_id": "67a9a7cb6be3ca4a7ede4724",
          "user": {
            "_id": "63a4754927f1f64ed7238dac",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
            "isPro": false,
            "fullname": "Aman Chadha",
            "user": "amanchadha",
            "type": "user"
          },
          "name": "Aman Chadha",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-10T09:49:39.550Z",
          "hidden": false
        },
        {
          "_id": "67a9a7cb6be3ca4a7ede4725",
          "name": "Amit Sheth",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-05T18:46:20.000Z",
      "title": "InversoAlign : Défi et Optimisation Multiobjectif Basée sur DPO",
      "summary": "En un système T2I, il est essentiel que les images générées comprennent exactement le but du utilisateur et se conforment à des normes éthiques et esthétiques strictes. Par exemple, comme c'a eu lieu avec Google's DeepMind, lorsque des résultats inappropriés ont provoqué une réaction publique forte, il a été souligné la nécessité d'une structure de réponse robuste. En contraste, les modèles de langage à grande échelle (LLMs) ont réussi significativement dans ces responsabilités. En se basant sur ces avancées, les chercheurs cherchent à appliquer des techniques comme l'Optimisation Directe de la Préférence (DPO) dans les systèmes T2I pour améliorer la précision et la fiabilité de la génération d'images.\n\nNous proposons YinYangAlign, un avancé cadre de référence d'évaluation. Ce cadre vise à concevoir un système qui quantifie de manière systématique la précision de la réponse d'un T2I, abordant six problèmes fondamentaux et uniques. Chaque paire représente une base fondamentale de la génération d'images et, par exemple, équilibrer le comportement en fonction du prompt utilisateur avec la créativité, ou maintenir la diversité et la cohérence visuelle. YinYangAlign inclut un ensemble de données détaillées qui contient le prompt humain, la sortie de la réponse choisie (élection), la sortie de la génération inappropriée de l'IA (rejet), et une explication postérieure des contradictions.",
      "upvotes": 1,
      "discussionId": "67a9a7cf6be3ca4a7ede47d5"
    },
    "publishedAt": "2025-02-10T02:21:52.370Z",
    "title": "YINYANG-ALIGN: Benchmarking Contradictory Objectives and Proposing Multi-Objective Optimization based DPO for Text-to-Image Alignment",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.03512.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a4754927f1f64ed7238dac",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
      "fullname": "Aman Chadha",
      "name": "amanchadha",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.05178",
      "authors": [
        {
          "_id": "67a99dfe98423dca45d8f659",
          "user": {
            "_id": "638fe91639f7e2a7f9d2a8c6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638fe91639f7e2a7f9d2a8c6/hB7DMVODcdAEUdQnXxWA8.jpeg",
            "isPro": false,
            "fullname": "Yue Zhao",
            "user": "zhaoyue-zephyrus",
            "type": "user"
          },
          "name": "Yue Zhao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-10T09:49:43.493Z",
          "hidden": false
        },
        {
          "_id": "67a99dfe98423dca45d8f65a",
          "name": "Fuzhao Xue",
          "hidden": false
        },
        {
          "_id": "67a99dfe98423dca45d8f65b",
          "name": "Scott Reed",
          "hidden": false
        },
        {
          "_id": "67a99dfe98423dca45d8f65c",
          "name": "Linxi Fan",
          "hidden": false
        },
        {
          "_id": "67a99dfe98423dca45d8f65d",
          "name": "Yuke Zhu",
          "hidden": false
        },
        {
          "_id": "67a99dfe98423dca45d8f65e",
          "name": "Jan Kautz",
          "hidden": false
        },
        {
          "_id": "67a99dfe98423dca45d8f65f",
          "name": "Zhiding Yu",
          "hidden": false
        },
        {
          "_id": "67a99dfe98423dca45d8f660",
          "name": "Philipp Krähenbühl",
          "hidden": false
        },
        {
          "_id": "67a99dfe98423dca45d8f661",
          "name": "De-An Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-07T18:59:57.000Z",
      "title": "QLIP : La tokenisation visuelle des correspondances textuelles intègre la compréhension multimodale déductive et la génération automatique.",
      "summary": "QLIP (Pré-entraînement de Langue et Images Quantisée) est présenté. QLIP est un méthode de tokenisation visuelle qui intègre la meilleure qualité de reconstruction et la meilleure compréhension d'images en 0-shot. QLIP entraîne un autoencodeur basé sur la quantification binaire de deux couches supervisées pour l'objectif de reconstruction et de réarrangement d'images et de langage. Tout d'abord, QLIP a démontré que ces deux objectifs ne se conflit pas. QLIP a mis en place un flux d'entraînement en deux étapes pour équilibrer dynamiquement ces deux termes de perte, permettant une fusion plus efficace des grands lots d'entraînement d'un dictionnaire d'images et de langage avec les points de coupure de mémoire liés aux objectifs de reconstruction, ainsi que pour améliorer l'efficacité. L'efficacité de QLIP a été vérifiée dans la compréhension multimodale et la génération de documents avec des images. En particulier, QLIP peut remplacer l'encodeur visuel de LLaVA et le tokenisateur d'images de LlamaGen, montrant un rendement relativement optimal ou amélioré. Enfin, QLIP démontre l'implémentation d'un modèle hybride intégré pour la compréhension et la génération.",
      "upvotes": 1,
      "discussionId": "67a99dfe98423dca45d8f691"
    },
    "publishedAt": "2025-02-10T01:35:35.818Z",
    "title": "QLIP: Text-Aligned Visual Tokenization Unifies Auto-Regressive Multimodal Understanding and Generation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05178.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "638fe91639f7e2a7f9d2a8c6",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638fe91639f7e2a7f9d2a8c6/hB7DMVODcdAEUdQnXxWA8.jpeg",
      "fullname": "Yue Zhao",
      "name": "zhaoyue-zephyrus",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.04376",
      "authors": [
        {
          "_id": "67a998fe495b23306cdbf51d",
          "name": "Lingxiang Hu",
          "hidden": false
        },
        {
          "_id": "67a998fe495b23306cdbf51e",
          "name": "Shurun Yuan",
          "hidden": false
        },
        {
          "_id": "67a998fe495b23306cdbf51f",
          "name": "Xiaoting Qin",
          "hidden": false
        },
        {
          "_id": "67a998fe495b23306cdbf520",
          "name": "Jue Zhang",
          "hidden": false
        },
        {
          "_id": "67a998fe495b23306cdbf521",
          "name": "Qingwei Lin",
          "hidden": false
        },
        {
          "_id": "67a998fe495b23306cdbf522",
          "name": "Dongmei Zhang",
          "hidden": false
        },
        {
          "_id": "67a998fe495b23306cdbf523",
          "name": "Saravan Rajmohan",
          "hidden": false
        },
        {
          "_id": "67a998fe495b23306cdbf524",
          "name": "Qi Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-05T16:25:43.000Z",
      "title": "Représentant de la réunion : Je présente la normalisation des normes de l'LLM pour la réunion.",
      "summary": "Dans l'actualité du monde du travail, les réunions sont essentielles pour l'échange d'idées et la coordination de l'équipe, mais présentent des problèmes tels que le consommation de temps, les conflits dans la programmation et la diminution de l'efficacité due à une participation inadéquate. Le développement récent des modèles de langage grands (LLMs) a démontré des capacités fortes en génération et inférence de langage, ce qui a conduit à la question : \"Peuvent-ils les LLMs déléguer efficacement aux participants des réunions ?\" Pour étudier cette question, nous avons développé un système de délégation de réunions en utilisant des modèles de LLM et avons créé un cadre de référence détaillé en utilisant des transcriptions de réunions réelles. Les résultats de l'évaluation montrent que GPT-4/4o maintient un équilibre entre une participation active et soignée, tandis que Gemini 1.5 Pro montre un comportement plus soigné, et Gemini 1.5 Flash et Llama3-8B/70B montrent un comportement plus actif. En général, environ 60% des réponses traitent d'au moins un point clé de la vérité. Cependant, il est nécessaire de réduire les contenus irrélevants et les répétitions, ainsi que d'améliorer la robustesse face aux erreurs de copie dans des environnements réels. De plus, nous avons mis en œuvre le système dans des environnements pratiques et collecté des feedback réalistes de démonstrations. Nos résultats révèlent la possibilité et les défis de l'utilisation des LLMs pour la délégation de réunions, avec l'objectif de fournir des contenus précieux pour des applications pratiques qui résolvent les défis des réunions.",
      "upvotes": 1,
      "discussionId": "67a99900495b23306cdbf57e"
    },
    "publishedAt": "2025-02-10T01:15:52.070Z",
    "title": "MEETING DELEGATE: Benchmarking LLMs on Attending Meetings on Our Behalf",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04376.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "662b0bc9c709a61df8291c0f",
      "avatarUrl": "/avatars/16dd4d945e9fbef5ac889a8087101ded.svg",
      "fullname": "Xiaoting Qin",
      "name": "XiaotingQin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.03738",
      "authors": [
        {
          "_id": "67a8d049406cb5a65f847eb1",
          "name": "Feng Wang",
          "hidden": false
        },
        {
          "_id": "67a8d049406cb5a65f847eb2",
          "name": "Yaodong Yu",
          "hidden": false
        },
        {
          "_id": "67a8d049406cb5a65f847eb3",
          "name": "Guoyizhe Wei",
          "hidden": false
        },
        {
          "_id": "67a8d049406cb5a65f847eb4",
          "name": "Wei Shao",
          "hidden": false
        },
        {
          "_id": "67a8d049406cb5a65f847eb5",
          "name": "Yuyin Zhou",
          "hidden": false
        },
        {
          "_id": "67a8d049406cb5a65f847eb6",
          "name": "Alan Yuille",
          "hidden": false
        },
        {
          "_id": "67a8d049406cb5a65f847eb7",
          "name": "Cihang Xie",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-06T03:01:38.000Z",
      "title": "En l'échelle de Raz, le facteur des patches : les images ont plus de valeur que 50,176 tokens.",
      "summary": "Après l'introduction du Vision Transformer (ViT), le Patch Factory a été reconnu comme une pratique efficace pour la tokenisation d'images dans des architectures visuelles simples à long terme. Ce méthode permet de réduire le taille spatial des images, ce qui réduit efficacement les coûts de calcul dans des architectures simples comme le ViT. Dans cet article, on étudie en détail les patterns de compression qui génèrent une perte d'information dans le Patch Factory et on analyse l'impact sur la compréhension visuelle. Grâce à une large gamme d'expériences avec des échelles de taille de patch, on a découvert les lois d'échelle des points d'intérêt dans le Patch Factory. Le modèle obtient des avantages constants en réduisant le taille du patch, augmentant la précision de la prédiction, mais lorsque le taille minimal de 1x1 est atteint, se produit la tokenisation des pixels. Cette conclusion peut être largement appliquée à diverses tâches visuelles, échelles d'entrée, architectures comme le ViT et Mamba. De plus, lorsque le taille du patch diminue, l'importance des têtes de décodage pour certaines tâches diminue en relation avec la prédiction dense. Dans les expériences, on a étendu l'extension des séquences d'images jusqu'à 50,176 tokens, et le modèle de taille standard a atteint une précision de test compétitive de 84,6% sur le benchmark ImageNet-1k. Ce travail offre des feedbacks et une base théorique pour la construction de modèles visuels non compressés à l'avenir. Le code est disponible sur https://github.com/wangf3014/Patch_Scaling.",
      "upvotes": 0,
      "discussionId": "67a8d04a406cb5a65f847ed3"
    },
    "publishedAt": "2025-02-10T02:34:31.480Z",
    "title": "Scaling Laws in Patchification: An Image Is Worth 50,176 Tokens And More",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.03738.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5f1158120c833276f61f1a84",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
      "fullname": "Niels Rogge",
      "name": "nielsr",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 754
    },
    "isAuthorParticipating": false
  }
]