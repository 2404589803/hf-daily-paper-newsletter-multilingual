[
  {
    "paper": {
      "id": "2503.18878",
      "authors": [
        {
          "_id": "67e25fe88e6c927eb7794abd",
          "name": "Andrey Galichin",
          "hidden": false
        },
        {
          "_id": "67e25fe88e6c927eb7794abe",
          "user": {
            "_id": "60cd95ee15ecba5f2200304a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60cd95ee15ecba5f2200304a/3gMYeWm8wQO5KfqE5RmEe.jpeg",
            "isPro": false,
            "fullname": "Alexey Dontsov",
            "user": "therem",
            "type": "user"
          },
          "name": "Alexey Dontsov",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-25T08:18:43.467Z",
          "hidden": false
        },
        {
          "_id": "67e25fe88e6c927eb7794abf",
          "name": "Polina Druzhinina",
          "hidden": false
        },
        {
          "_id": "67e25fe88e6c927eb7794ac0",
          "user": {
            "_id": "6172aaeec8e66e2aa84c06b9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6172aaeec8e66e2aa84c06b9/ZdRZSp3P1SU6CIDbvQwkv.jpeg",
            "isPro": false,
            "fullname": "Anton Razzhigaev",
            "user": "razzant",
            "type": "user"
          },
          "name": "Anton Razzhigaev",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-25T09:05:58.409Z",
          "hidden": false
        },
        {
          "_id": "67e25fe88e6c927eb7794ac1",
          "name": "Oleg Y. Rogov",
          "hidden": false
        },
        {
          "_id": "67e25fe88e6c927eb7794ac2",
          "user": {
            "_id": "662f8d645c4db70c77a203b0",
            "avatarUrl": "/avatars/72f9a3c39b3ba5114388d16a35524835.svg",
            "isPro": false,
            "fullname": "Elena Tutubalina",
            "user": "tlenusik",
            "type": "user"
          },
          "name": "Elena Tutubalina",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-25T09:05:56.401Z",
          "hidden": false
        },
        {
          "_id": "67e25fe88e6c927eb7794ac3",
          "name": "Ivan Oseledets",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-24T16:54:26.000Z",
      "submittedOnDailyAt": "2025-03-25T06:45:16.781Z",
      "title": "Tout les fondements ont été couverts : On a utilisé des codificateurs autonomes et sparses pour interpréter les caractéristiques théoriques des grands modèles de langage.",
      "submittedOnDailyBy": {
        "_id": "60cd95ee15ecba5f2200304a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60cd95ee15ecba5f2200304a/3gMYeWm8wQO5KfqE5RmEe.jpeg",
        "isPro": false,
        "fullname": "Alexey Dontsov",
        "user": "therem",
        "type": "user"
      },
      "summary": "Les modèles de langage grand (LLMs) ont connu un succès impressionnant dans le traitement du langage naturel. Les avancées récentes sont liées au développement de nouvelles classes de LLMs logiques. Par exemple, DeepSeek-R1, une source ouverte, a réussi à obtenir les meilleurs résultats grâce à sa combinaison de pensée profonde et de logique complexe. Cependant, la structure logique interne de ces modèles n'a pas encore été étudiée. Dans cet article, nous utilisons le méthode de décomposition des caractéristiques de la représentation potentielle des réseaux neuronaux par des autoencodeurs à faible codage (SAEs) pour identifier les caractéristiques logiques de la série de modèles DeepSeek-R1. Tout d'abord, nous proposons un approche pour extraire \"caractéristiques logiques\" candidates à partir des représentations des SAEs. Ces caractéristiques sont validées par un analyse expérimentale et des méthodes interprétables, montrant une association directe avec le rendement logique du modèle. Il est important de souligner que contrôler de manière systématique ces caractéristiques peut améliorer le rendement logique et fournir une explication structurelle initiale de la logique dans les LLMs. Le code est disponible sur : https://github.com/AIRI-Institute/SAE-Reasoning.",
      "upvotes": 63,
      "discussionId": "67e25fea8e6c927eb7794b25",
      "ai_keywords": [
        "Sparse Autoencoders (SAEs)",
        "latent representations",
        "interpretable features",
        "reasoning features",
        "empirical analysis",
        "interpretability methods",
        "systematic enhancement"
      ]
    },
    "publishedAt": "2025-03-24T12:54:26.000Z",
    "title": "I Have Covered All the Bases Here: Interpreting Reasoning Features in\n  Large Language Models via Sparse Autoencoders",
    "summary": "Large Language Models (LLMs) have achieved remarkable success in natural\nlanguage processing. Recent advances have led to the developing of a new class\nof reasoning LLMs; for example, open-source DeepSeek-R1 has achieved\nstate-of-the-art performance by integrating deep thinking and complex\nreasoning. Despite these impressive capabilities, the internal reasoning\nmechanisms of such models remain unexplored. In this work, we employ Sparse\nAutoencoders (SAEs), a method to learn a sparse decomposition of latent\nrepresentations of a neural network into interpretable features, to identify\nfeatures that drive reasoning in the DeepSeek-R1 series of models. First, we\npropose an approach to extract candidate ''reasoning features'' from SAE\nrepresentations. We validate these features through empirical analysis and\ninterpretability methods, demonstrating their direct correlation with the\nmodel's reasoning abilities. Crucially, we demonstrate that steering these\nfeatures systematically enhances reasoning performance, offering the first\nmechanistic account of reasoning in LLMs. Code available at\nhttps://github.com/AIRI-Institute/SAE-Reasoning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18878.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60cd95ee15ecba5f2200304a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60cd95ee15ecba5f2200304a/3gMYeWm8wQO5KfqE5RmEe.jpeg",
      "fullname": "Alexey Dontsov",
      "name": "therem",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.17359",
      "authors": [
        {
          "_id": "67e16a266280a70b45b8a16c",
          "user": {
            "_id": "64105a6d14215c0775dfdd14",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64105a6d14215c0775dfdd14/-VX-cUYOLjHIg7QnWhRGG.jpeg",
            "isPro": false,
            "fullname": "Jiwen Yu",
            "user": "VictorYuki",
            "type": "user"
          },
          "name": "Jiwen Yu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:05:33.251Z",
          "hidden": false
        },
        {
          "_id": "67e16a266280a70b45b8a16d",
          "name": "Yiran Qin",
          "hidden": false
        },
        {
          "_id": "67e16a266280a70b45b8a16e",
          "user": {
            "_id": "652404d0050781c16f1c51b0",
            "avatarUrl": "/avatars/4ad62f2c65406dd0af36c6d0697ae599.svg",
            "isPro": false,
            "fullname": "Haoxuan Che",
            "user": "chehx",
            "type": "user"
          },
          "name": "Haoxuan Che",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:06:13.592Z",
          "hidden": false
        },
        {
          "_id": "67e16a266280a70b45b8a16f",
          "name": "Quande Liu",
          "hidden": false
        },
        {
          "_id": "67e16a266280a70b45b8a170",
          "user": {
            "_id": "60e272ca6c78a8c122b12127",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60e272ca6c78a8c122b12127/xldEGBzGrU-bX6IwAw0Ie.jpeg",
            "isPro": false,
            "fullname": "Xintao Wang",
            "user": "Xintao",
            "type": "user"
          },
          "name": "Xintao Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:06:29.181Z",
          "hidden": false
        },
        {
          "_id": "67e16a266280a70b45b8a171",
          "name": "Pengfei Wan",
          "hidden": false
        },
        {
          "_id": "67e16a266280a70b45b8a172",
          "user": {
            "_id": "644c8324f02250233d0d67d9",
            "avatarUrl": "/avatars/feb39d281457c1750f3eada3c060a23e.svg",
            "isPro": false,
            "fullname": "Di Zhang",
            "user": "dizhang",
            "type": "user"
          },
          "name": "Di Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:06:50.971Z",
          "hidden": false
        },
        {
          "_id": "67e16a266280a70b45b8a173",
          "user": {
            "_id": "65d5ec74cd05bc1eaa125040",
            "avatarUrl": "/avatars/2de1b1539a86452c2c89570eeb02f5ab.svg",
            "isPro": false,
            "fullname": "Xihui Liu",
            "user": "XihuiLiu",
            "type": "user"
          },
          "name": "Xihui Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:06:56.864Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-21T17:59:22.000Z",
      "submittedOnDailyAt": "2025-03-25T01:42:15.880Z",
      "title": "Interactivo Génératif de Vidéo comme Moteur de Jeu de la Prochaine Génération",
      "submittedOnDailyBy": {
        "_id": "64105a6d14215c0775dfdd14",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64105a6d14215c0775dfdd14/-VX-cUYOLjHIg7QnWhRGG.jpeg",
        "isPro": false,
        "fullname": "Jiwen Yu",
        "user": "VictorYuki",
        "type": "user"
      },
      "summary": "Le développement actuel des jeux vidéo fait face à de grands problèmes liés à la créativité et au coût, en raison du contenu déterminant contenu des moteurs de jeux vidéo existants. Récemment, le développement de modèles de génération d'images a démontré la possibilité de synthétiser des environnements virtuels interactifs réalistes, offrant des opportunités pour innover dans la création de jeux vidéo. Dans cet article, nous proposons que l'Interactive Generative Video (IGV) soit la base pour les Generative Game Engines (GGE), avec l'objectif de générer des contenus infiniment nouveaux dans les prochains jeux vidéo. Les GGE utilisent les caractéristiques de l'IGV pour la synthèse de contenu de haute qualité, la modélisation de mondes avec des connaissances physiques, l'interaction contrôlable, la capacité à la mémoire à long terme et la capacité à l'inférence de causes. Les modules clés des GGE sont décrits en détail et une carte de maturité standard (L0-L4) est présentée pour guider leur développement. Notre travail ouvre de nouvelles perspectives dans le développement des jeux vidéo à l'ère de l'IA, imaginant un futur où les systèmes de génération AI changeront fondamentalement la création et l'expérience des jeux vidéo.",
      "upvotes": 47,
      "discussionId": "67e16a276280a70b45b8a214",
      "ai_keywords": [
        "Interactive Generative Video (IGV)",
        "Generative Game Engines (GGE)",
        "video generation models",
        "high-quality content synthesis",
        "physics-aware world modeling",
        "user-controlled interactivity",
        "long-term memory capabilities",
        "causal reasoning",
        "hierarchical maturity roadmap (L0-L4)"
      ]
    },
    "publishedAt": "2025-03-21T13:59:22.000Z",
    "title": "Position: Interactive Generative Video as Next-Generation Game Engine",
    "summary": "Modern game development faces significant challenges in creativity and cost\ndue to predetermined content in traditional game engines. Recent breakthroughs\nin video generation models, capable of synthesizing realistic and interactive\nvirtual environments, present an opportunity to revolutionize game creation. In\nthis position paper, we propose Interactive Generative Video (IGV) as the\nfoundation for Generative Game Engines (GGE), enabling unlimited novel content\ngeneration in next-generation gaming. GGE leverages IGV's unique strengths in\nunlimited high-quality content synthesis, physics-aware world modeling,\nuser-controlled interactivity, long-term memory capabilities, and causal\nreasoning. We present a comprehensive framework detailing GGE's core modules\nand a hierarchical maturity roadmap (L0-L4) to guide its evolution. Our work\ncharts a new course for game development in the AI era, envisioning a future\nwhere AI-powered generative systems fundamentally reshape how games are created\nand experienced.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17359.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64105a6d14215c0775dfdd14",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64105a6d14215c0775dfdd14/-VX-cUYOLjHIg7QnWhRGG.jpeg",
      "fullname": "Jiwen Yu",
      "name": "VictorYuki",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.18942",
      "authors": [
        {
          "_id": "67e226039cd910bee045e38f",
          "user": {
            "_id": "6505a02f9310ce8c400edc63",
            "avatarUrl": "/avatars/bbf781594fc8c812316711aa8e2797aa.svg",
            "isPro": false,
            "fullname": "Fangfu Liu",
            "user": "Liuff23",
            "type": "user"
          },
          "name": "Fangfu Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:07:42.279Z",
          "hidden": false
        },
        {
          "_id": "67e226039cd910bee045e390",
          "name": "Hanyang Wang",
          "hidden": false
        },
        {
          "_id": "67e226039cd910bee045e391",
          "name": "Yimo Cai",
          "hidden": false
        },
        {
          "_id": "67e226039cd910bee045e392",
          "user": {
            "_id": "60bc94cd85a3ab33829b6211",
            "avatarUrl": "/avatars/b57d36c7577fbbb42ea5b963eef4144a.svg",
            "isPro": false,
            "fullname": "Kaiyan Zhang",
            "user": "iseesaw",
            "type": "user"
          },
          "name": "Kaiyan Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-25T08:20:20.614Z",
          "hidden": false
        },
        {
          "_id": "67e226039cd910bee045e393",
          "user": {
            "_id": "6528fc319474946b8541b36f",
            "avatarUrl": "/avatars/08ea388cbcd7c0f1361980127a8d33c3.svg",
            "isPro": false,
            "fullname": "Xiaohang Zhan",
            "user": "xhangzhan",
            "type": "user"
          },
          "name": "Xiaohang Zhan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:08:05.983Z",
          "hidden": false
        },
        {
          "_id": "67e226039cd910bee045e394",
          "user": {
            "_id": "66c8131afafc0fc87ca99650",
            "avatarUrl": "/avatars/a6eeba2ccf011d5c9964fd38f85bd671.svg",
            "isPro": false,
            "fullname": "Yueqi Duan",
            "user": "duanyueqi",
            "type": "user"
          },
          "name": "Yueqi Duan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:08:11.759Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-24T17:59:04.000Z",
      "submittedOnDailyAt": "2025-03-25T02:12:44.893Z",
      "title": "Video-T1 : Montage de vidéo dans l'essai de génération de vidéo",
      "submittedOnDailyBy": {
        "_id": "6505a02f9310ce8c400edc63",
        "avatarUrl": "/avatars/bbf781594fc8c812316711aa8e2797aa.svg",
        "isPro": false,
        "fullname": "Fangfu Liu",
        "user": "Liuff23",
        "type": "user"
      },
      "summary": "Dans le domaine de la génération de vidéos, les progrès dans la quantité de données d'entraînement, le taille du modèle et la capacité d'échelle des coûts de calcul ont réalisé des résultats impressionnants dans la génération numérique, permettant aux utilisateurs de développer leur créativité dans divers domaines. Récemment, les chercheurs de modèles de langage grands (LLMs) ont montré que lorsqu'on élargit l'échelle pendant le test et utilise des calculs pendant l'inférence, on peut significativement améliorer le rendement des LLMs. Au lieu de réaliser l'échelle des modèles de vidéo avec des coûts d'entraînement élevés, l'objectif a été d'élargir la force de l'échelle pendant le test (TTS) à la génération de vidéos, en trouvant des méthodes pour utiliser la quantité de calculs pendant l'inférence dans des cas où le traitement du texte est difficile. Dans cette étude, l'échelle pendant le test de la génération de vidéos a été définie comme le problème de sampling des meilleures trajectoires dans l'espace de bruit Gaussien vers la distribution de vidéos cibles, avec un accent particulier sur la construction de l'espace de recherche en utilisant des données de test et des algorithmes heuristiques. Lorsque l'on fournit un texte, on examine la stratégie intuitive de recherche linéaire pour augmenter les candidats de bruit pendant l'inférence, mais en raison des limites significatives dans les coûts de calcul pendant le test, on a conçu un méthode efficace de TTS adaptée à la génération de vidéos, appelée \"Time of Flight\" (ToF) pour les frames. On a adapté et élargi le branche de régression automatique pour améliorer la qualité de la génération de vidéos. Les expériences dans le cadre de référence de la génération de vidéos conditionnées par texte et d'autres détails montrent que l'augmentation de la quantité de calculs pendant le test peut significativement améliorer la qualité des vidéos. Page du projet : https://liuff19.github.io/Video-T1",
      "upvotes": 41,
      "discussionId": "67e226059cd910bee045e42b",
      "projectPage": "https://liuff19.github.io/Video-T1/",
      "githubRepo": "https://github.com/liuff19/Video-T1",
      "ai_keywords": [
        "Test-Time Scaling (TTS)",
        "video foundation models",
        "inference-time computation",
        "Gaussian noise space",
        "target video distribution",
        "test-time verifiers",
        "heuristic algorithms",
        "linear search strategy",
        "noise candidates",
        "full-step denoising",
        "inference time",
        "Tree-of-Frames (ToF)",
        "autoregressive manner",
        "text-conditioned video generation benchmarks"
      ]
    },
    "publishedAt": "2025-03-24T13:59:04.000Z",
    "title": "Video-T1: Test-Time Scaling for Video Generation",
    "summary": "With the scale capability of increasing training data, model size, and\ncomputational cost, video generation has achieved impressive results in digital\ncreation, enabling users to express creativity across various domains.\nRecently, researchers in Large Language Models (LLMs) have expanded the scaling\nto test-time, which can significantly improve LLM performance by using more\ninference-time computation. Instead of scaling up video foundation models\nthrough expensive training costs, we explore the power of Test-Time Scaling\n(TTS) in video generation, aiming to answer the question: if a video generation\nmodel is allowed to use non-trivial amount of inference-time compute, how much\ncan it improve generation quality given a challenging text prompt. In this\nwork, we reinterpret the test-time scaling of video generation as a searching\nproblem to sample better trajectories from Gaussian noise space to the target\nvideo distribution. Specifically, we build the search space with test-time\nverifiers to provide feedback and heuristic algorithms to guide searching\nprocess. Given a text prompt, we first explore an intuitive linear search\nstrategy by increasing noise candidates at inference time. As full-step\ndenoising all frames simultaneously requires heavy test-time computation costs,\nwe further design a more efficient TTS method for video generation called\nTree-of-Frames (ToF) that adaptively expands and prunes video branches in an\nautoregressive manner. Extensive experiments on text-conditioned video\ngeneration benchmarks demonstrate that increasing test-time compute\nconsistently leads to significant improvements in the quality of videos.\nProject page: https://liuff19.github.io/Video-T1",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18942.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6505a02f9310ce8c400edc63",
      "avatarUrl": "/avatars/bbf781594fc8c812316711aa8e2797aa.svg",
      "fullname": "Fangfu Liu",
      "name": "Liuff23",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.18945",
      "authors": [
        {
          "_id": "67e22eca9455abdd1d257263",
          "name": "Aether Team",
          "hidden": false
        },
        {
          "_id": "67e22eca9455abdd1d257264",
          "user": {
            "_id": "6283546209aa80237c6c482c",
            "avatarUrl": "/avatars/0d6fc5846c0456d5282d82d5bf4d7056.svg",
            "isPro": false,
            "fullname": "Haoyi Zhu",
            "user": "HaoyiZhu",
            "type": "user"
          },
          "name": "Haoyi Zhu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:15:13.586Z",
          "hidden": false
        },
        {
          "_id": "67e22eca9455abdd1d257265",
          "name": "Yifan Wang",
          "hidden": false
        },
        {
          "_id": "67e22eca9455abdd1d257266",
          "user": {
            "_id": "667e81565934c9fae29207ef",
            "avatarUrl": "/avatars/431e777c71fccf7cf48ce013e5f6f1cb.svg",
            "isPro": false,
            "fullname": "Zhou",
            "user": "ZhouTimeMachine",
            "type": "user"
          },
          "name": "Jianjun Zhou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-25T09:05:54.719Z",
          "hidden": false
        },
        {
          "_id": "67e22eca9455abdd1d257267",
          "user": {
            "_id": "67a5b0fe5a8652514e67c38c",
            "avatarUrl": "/avatars/28da8e93ee00fd77c7e62d16f9b94045.svg",
            "isPro": false,
            "fullname": "Wenzheng Chang",
            "user": "AmberHeart",
            "type": "user"
          },
          "name": "Wenzheng Chang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-25T08:20:10.947Z",
          "hidden": false
        },
        {
          "_id": "67e22eca9455abdd1d257268",
          "name": "Yang Zhou",
          "hidden": false
        },
        {
          "_id": "67e22eca9455abdd1d257269",
          "user": {
            "_id": "65e7eb86c7a0617cc71d3df4",
            "avatarUrl": "/avatars/01020b6b5ccb08bf8aa10fd5f8b2701d.svg",
            "isPro": false,
            "fullname": "lizizun",
            "user": "lizizun",
            "type": "user"
          },
          "name": "Zizun Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:16:06.912Z",
          "hidden": false
        },
        {
          "_id": "67e22eca9455abdd1d25726a",
          "user": {
            "_id": "6679bb85972a0f224cde335c",
            "avatarUrl": "/avatars/bf0649645458e206ba5224b001723641.svg",
            "isPro": false,
            "fullname": "Junyi Chen",
            "user": "Junyichen",
            "type": "user"
          },
          "name": "Junyi Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:16:13.751Z",
          "hidden": false
        },
        {
          "_id": "67e22eca9455abdd1d25726b",
          "name": "Chunhua Shen",
          "hidden": false
        },
        {
          "_id": "67e22eca9455abdd1d25726c",
          "user": {
            "_id": "65783ee6ee33d547aecc3ffc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65783ee6ee33d547aecc3ffc/lWZX88c-0dCsN-yB9Jhlf.jpeg",
            "isPro": false,
            "fullname": "Jiangmiao Pang",
            "user": "Jiangmiao",
            "type": "user"
          },
          "name": "Jiangmiao Pang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:16:26.855Z",
          "hidden": false
        },
        {
          "_id": "67e22eca9455abdd1d25726d",
          "user": {
            "_id": "64478c64e2148488340229db",
            "avatarUrl": "/avatars/f5c23489a068e896381cdc25836ce3dd.svg",
            "isPro": false,
            "fullname": "he",
            "user": "tonghe",
            "type": "user"
          },
          "name": "Tong He",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:16:35.824Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-24T17:59:51.000Z",
      "submittedOnDailyAt": "2025-03-25T02:50:34.610Z",
      "title": "Aether : Modélisation de mondes intégrés en géométrie",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Nous allons maintenant fournir la traduction en français du document.\n\nLe développement de systèmes d'intelligence artificielle capable de comprendre la compréhension spatiale est considéré comme un problème important. Dans ce contexte, la reconstruction géométrique et la modélisation générative sont intégrées. Dans cet article, nous proposons un cadre de travail universel appelé \"Aether\", qui optimise trois capacités essentielles : la reconstruction dynamique en 4D, la prédiction de vidéos conditionnées par des actions et la planification visuelle conditionnée par des objectifs. Aether se concentre sur la partage de connaissances simplifiées pour la reconstruction, la prédiction et la planification. En utilisant l'apprentissage de caractéristiques, Aether permet l'échange de travail. Comme base pour des modèles de génération de vidéos, ce cadre de travail montre un nouveau rendement en étendant les données de la réalité avec des données synthétiques jamais vues. De plus, cette méthodologie atteint l'expansion 0-shot en la tracé d'actions et en la reconstruction, et permet de modéliser des géométries uniques. En particulier, il n'est pas nécessaire de voir des données de la réalité, et le rendement de la reconstruction peut être significativement meilleur que ceux des modèles spécifiques. De plus, Aether utilise des espaces d'actions avec des connaissances géométriques pour transformer les prédictions en actions de manière continue et planifier des trajets automatiques. Notre travail vise à encourager la communauté dans l'exploration de nouvelles frontières dans la modélisation physiquement plausible.",
      "upvotes": 18,
      "discussionId": "67e22ecb9455abdd1d2572af",
      "projectPage": "https://aether-world.github.io/",
      "githubRepo": "https://github.com/OpenRobotLab/Aether",
      "ai_keywords": [
        "Aether",
        "4D dynamic reconstruction",
        "action-conditioned video prediction",
        "goal-conditioned visual planning",
        "task-interleaved feature learning",
        "video generation models",
        "synthetic-to-real generalization",
        "zero-shot generalization",
        "geometric modeling",
        "geometry-informed action space",
        "autonomous trajectory planning",
        "physically-reasonable world modeling"
      ]
    },
    "publishedAt": "2025-03-24T13:59:51.000Z",
    "title": "Aether: Geometric-Aware Unified World Modeling",
    "summary": "The integration of geometric reconstruction and generative modeling remains a\ncritical challenge in developing AI systems capable of human-like spatial\nreasoning. This paper proposes Aether, a unified framework that enables\ngeometry-aware reasoning in world models by jointly optimizing three core\ncapabilities: (1) 4D dynamic reconstruction, (2) action-conditioned video\nprediction, and (3) goal-conditioned visual planning. Through task-interleaved\nfeature learning, Aether achieves synergistic knowledge sharing across\nreconstruction, prediction, and planning objectives. Building upon video\ngeneration models, our framework demonstrates unprecedented synthetic-to-real\ngeneralization despite never observing real-world data during training.\nFurthermore, our approach achieves zero-shot generalization in both action\nfollowing and reconstruction tasks, thanks to its intrinsic geometric modeling.\nRemarkably, even without real-world data, its reconstruction performance far\nexceeds that of domain-specific models. Additionally, Aether leverages a\ngeometry-informed action space to seamlessly translate predictions into\nactions, enabling effective autonomous trajectory planning. We hope our work\ninspires the community to explore new frontiers in physically-reasonable world\nmodeling and its applications.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18945.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6456
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.18892",
      "authors": [
        {
          "_id": "67e22ce1155ea10f2fdbe5d2",
          "user": {
            "_id": "62751082b43ccfeef483424f",
            "avatarUrl": "/avatars/fec83e4478e7d1731ba6033328131852.svg",
            "isPro": false,
            "fullname": "WeihaoZeng",
            "user": "AndrewZeng",
            "type": "user"
          },
          "name": "Weihao Zeng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-25T08:20:18.367Z",
          "hidden": false
        },
        {
          "_id": "67e22ce1155ea10f2fdbe5d3",
          "user": {
            "_id": "6462def82a83863b97c0611e",
            "avatarUrl": "/avatars/c03e9cc7d75b0266fcc56ecb6ee62148.svg",
            "isPro": false,
            "fullname": "Yuzhen Huang",
            "user": "yuzhen17",
            "type": "user"
          },
          "name": "Yuzhen Huang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-25T08:20:13.781Z",
          "hidden": false
        },
        {
          "_id": "67e22ce1155ea10f2fdbe5d4",
          "user": {
            "_id": "612ee6a7b960e78c6d2319d4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/612ee6a7b960e78c6d2319d4/2Hu9BaAyXbyh1vt0v1Qui.jpeg",
            "isPro": false,
            "fullname": "Qian Liu",
            "user": "SivilTaram",
            "type": "user"
          },
          "name": "Qian Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-25T08:20:15.927Z",
          "hidden": false
        },
        {
          "_id": "67e22ce1155ea10f2fdbe5d5",
          "name": "Wei Liu",
          "hidden": false
        },
        {
          "_id": "67e22ce1155ea10f2fdbe5d6",
          "user": {
            "_id": "64bf71792915a87970c07446",
            "avatarUrl": "/avatars/b24403f9fa699e0143e441b56528e6af.svg",
            "isPro": false,
            "fullname": "Keqing He",
            "user": "HelicHe",
            "type": "user"
          },
          "name": "Keqing He",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:17:09.770Z",
          "hidden": false
        },
        {
          "_id": "67e22ce1155ea10f2fdbe5d7",
          "name": "Zejun Ma",
          "hidden": false
        },
        {
          "_id": "67e22ce1155ea10f2fdbe5d8",
          "user": {
            "_id": "615f34ec3f6d24d67c1b5c78",
            "avatarUrl": "/avatars/6dcff6477993d9e57c5cb92b6f95eb66.svg",
            "isPro": false,
            "fullname": "Junxian He",
            "user": "jxhe",
            "type": "user"
          },
          "name": "Junxian He",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:16:49.208Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-24T17:06:10.000Z",
      "submittedOnDailyAt": "2025-03-25T02:41:40.812Z",
      "title": "SimpleRL-Zoo : Recherche et Contrôle de L'apprentissage par renforcement Zero-Shot dans des Environnements Naturels basés sur des Bases Ouvertes",
      "submittedOnDailyBy": {
        "_id": "62751082b43ccfeef483424f",
        "avatarUrl": "/avatars/fec83e4478e7d1731ba6033328131852.svg",
        "isPro": false,
        "fullname": "WeihaoZeng",
        "user": "AndrewZeng",
        "type": "user"
      },
      "summary": "DeepSeek-R1 a montré qu'il est possible d'obtenir une logique de pensée continu et naturelle (CoT) dans un cadre d'apprentissage par renforcement (RL) basé sur des règles simples. Cet apprentissage commence directement dans les modèles de base, ce qui est connu sous le terme de \"zero RL training\". Les expériences récentes axées sur la série de modèles Qwen2.5 ont montré des résultats significatifs, car ces modèles déjà montrent une forte capacité à suivre des instructions et à se réfléchir, ce qui les rend non représentatifs. Dans cette étude, plusieurs modèles de base ont été examinés, y compris LLama3-8B, Mistral-7B/24B, DeepSeek-Math-7B, Qwen2.5-math-7B et la série de modèles Qwen2.5 (de 0.5B à 32B). Grâce à des stratégies clés telles que l'ajustement des récompenses et le contrôle de la difficulté des questions, un grand progrès a été réalisé en matière de précision logique et de longueur des réponses. Cependant, en observant la dynamique de l'apprentissage, il a été noté que chaque modèle de base présente des patrons différents pendant l'apprentissage. Par exemple, l'augmentation de la longueur de la réponse n'est pas directement liée à l'apparition de certains comportements cognitifs (par exemple, \"grands concepts\"). En particulier, l'apparition de \"grands concepts\" a été observée dans les petits modèles de la famille Qwen. Cette étude partage les conceptions clés, les découvertes et les pratiques permettant le \"zero RL training\", et offre les code, les modèles et les outils analytiques sous licence open-source pour encourager le développement futur.",
      "upvotes": 16,
      "discussionId": "67e22ce3155ea10f2fdbe6c0",
      "githubRepo": "https://github.com/hkust-nlp/simpleRL-reason",
      "ai_keywords": [
        "reinforcement learning",
        "rule-based rewards",
        "zero RL training",
        "long chain-of-thought (CoT) reasoning",
        "instruction-following",
        "self-reflection",
        "base models",
        "Qwen2.5 model series",
        "LLama3-8B",
        "Mistral-7B/24B",
        "DeepSeek-Math-7B",
        "Qwen2.5-math-7B",
        "response length",
        "reasoning accuracy",
        "cognitive behaviors",
        "verification",
        "training dynamics"
      ]
    },
    "publishedAt": "2025-03-24T13:06:10.000Z",
    "title": "SimpleRL-Zoo: Investigating and Taming Zero Reinforcement Learning for\n  Open Base Models in the Wild",
    "summary": "DeepSeek-R1 has shown that long chain-of-thought (CoT) reasoning can\nnaturally emerge through a simple reinforcement learning (RL) framework with\nrule-based rewards, where the training may directly start from the base\nmodels-a paradigm referred to as zero RL training. Most recent efforts to\nreproduce zero RL training have primarily focused on the Qwen2.5 model series,\nwhich may not be representative as we find the base models already exhibit\nstrong instruction-following and self-reflection abilities. In this work, we\ninvestigate zero RL training across 10 diverse base models, spanning different\nfamilies and sizes including LLama3-8B, Mistral-7B/24B, DeepSeek-Math-7B,\nQwen2.5-math-7B, and all Qwen2.5 models from 0.5B to 32B. Leveraging several\nkey design strategies-such as adjusting format reward and controlling query\ndifficulty-we achieve substantial improvements in both reasoning accuracy and\nresponse length across most settings. However, by carefully monitoring the\ntraining dynamics, we observe that different base models exhibit distinct\npatterns during training. For instance, the increased response length does not\nalways correlate with the emergence of certain cognitive behaviors such as\nverification (i.e., the \"aha moment\"). Notably, we observe the \"aha moment\" for\nthe first time in small models not from the Qwen family. We share the key\ndesigns that enable successful zero RL training, along with our findings and\npractices. To facilitate further research, we open-source the code, models, and\nanalysis tools.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18892.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62751082b43ccfeef483424f",
      "avatarUrl": "/avatars/fec83e4478e7d1731ba6033328131852.svg",
      "fullname": "WeihaoZeng",
      "name": "AndrewZeng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.17439",
      "authors": [
        {
          "_id": "67e21f63fb4213c53714be08",
          "user": {
            "_id": "6565e24fe5aac326bfd15a9d",
            "avatarUrl": "/avatars/28ad90df0e0dbc10ef25ee6499a50dec.svg",
            "isPro": false,
            "fullname": "Zhuoshi Pan",
            "user": "panzs",
            "type": "user"
          },
          "name": "Zhuoshi Pan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:17:23.682Z",
          "hidden": false
        },
        {
          "_id": "67e21f63fb4213c53714be09",
          "name": "Yu Li",
          "hidden": false
        },
        {
          "_id": "67e21f63fb4213c53714be0a",
          "user": {
            "_id": "640d99628512ec51d7ef71c7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640d99628512ec51d7ef71c7/fcBkqnxfxuuuZTqfN_BGy.jpeg",
            "isPro": false,
            "fullname": "Honglin Lin",
            "user": "LHL3341",
            "type": "user"
          },
          "name": "Honglin Lin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:17:30.635Z",
          "hidden": false
        },
        {
          "_id": "67e21f63fb4213c53714be0b",
          "user": {
            "_id": "6397f6081323f19c578f142e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6397f6081323f19c578f142e/it7FYYKjlLX8wSsMLm8EO.jpeg",
            "isPro": false,
            "fullname": "QizhiPei",
            "user": "QizhiPei",
            "type": "user"
          },
          "name": "Qizhi Pei",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:17:42.836Z",
          "hidden": false
        },
        {
          "_id": "67e21f63fb4213c53714be0c",
          "user": {
            "_id": "66580d3d80ee5b1e11a94e57",
            "avatarUrl": "/avatars/1a88e7337f9095c40c6d402fab797d83.svg",
            "isPro": false,
            "fullname": "Zinan Tang",
            "user": "Word2Li",
            "type": "user"
          },
          "name": "Zinan Tang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:17:50.791Z",
          "hidden": false
        },
        {
          "_id": "67e21f63fb4213c53714be0d",
          "name": "Wei Wu",
          "hidden": false
        },
        {
          "_id": "67e21f63fb4213c53714be0e",
          "user": {
            "_id": "677e133ee86d0754dc7ce296",
            "avatarUrl": "/avatars/c16511c1876b50c2d049925c5f320d15.svg",
            "isPro": false,
            "fullname": "mingchenlin",
            "user": "mingchenlin2025",
            "type": "user"
          },
          "name": "Chenlin Ming",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:18:12.485Z",
          "hidden": false
        },
        {
          "_id": "67e21f63fb4213c53714be0f",
          "name": "H. Vicky Zhao",
          "hidden": false
        },
        {
          "_id": "67e21f63fb4213c53714be10",
          "user": {
            "_id": "63f9fca8d4349b157a109eec",
            "avatarUrl": "/avatars/fa1f2ae7972d7cde99dab178136ccbb0.svg",
            "isPro": false,
            "fullname": "Conghui He",
            "user": "conghui",
            "type": "user"
          },
          "name": "Conghui He",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:18:25.368Z",
          "hidden": false
        },
        {
          "_id": "67e21f63fb4213c53714be11",
          "name": "Lijun Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-21T17:59:10.000Z",
      "submittedOnDailyAt": "2025-03-25T01:47:52.213Z",
      "title": "Le méthode d'apprentissage dans la bibliothèque de machines qui vise le développement mathématique est un méthode d'apprentissage basée sur les erreurs.",
      "submittedOnDailyBy": {
        "_id": "6397f6081323f19c578f142e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6397f6081323f19c578f142e/it7FYYKjlLX8wSsMLm8EO.jpeg",
        "isPro": false,
        "fullname": "QizhiPei",
        "user": "QizhiPei",
        "type": "user"
      },
      "summary": "Les modèles de langue grands (LLMs) ont démontré une habileté impressionnante pour la résolution de problèmes mathématiques. Cependant, l'approche actuelle se concentre principalement sur l'amélioration de la qualité des données d'entraînement correctes. Par exemple, des solutions de haute qualité sont extraites de modèles avancés, en ignorant les données avec des erreurs et potentiellement limitant la capacité de rétroaction du modèle. Cependant, certaines recherches tentent d'utiliser des données avec des erreurs, ces essais incluant des structures complexes. Par exemple, l'exploration de nœuds avec des erreurs par MCTS (Exploration d'Arbres de Monte-Carlo) est utilisée. Dans cet article, nous proposons LEMMA (Apprentissage des erreurs pour améliorer les aptitudes mathématiques) pour encourager le développement de la mathématique. LEMMA construit et ajuste des données qui comprennent des solutions incorrectes et des étapes d'erreur, ainsi que la rétroaction vers des solutions correctes. Spécifiquement, nous analysons de manière systématique les types d'erreurs générées par le modèle et introduisons un méthode d'expansion des erreurs basée sur les types d'erreurs pour collecter divers et représentatifs erreurs. Les solutions correctes peuvent émerger à partir de la modification des erreurs ou d'un nouveau départ. Les solutions aux erreurs se transforment en solutions correctes grâce à la rétroaction douce fournie par le modèle. L'ensemble de données construit permet que le modèle se ajuste automatiquement pendant le processus de génération, sans dépendre de modèles externes d'évaluation. Les résultats des expérimentations montrent que LEMMA atteint une amélioration significative du rendement par rapport à d'autres fortes bases de référence.",
      "upvotes": 12,
      "discussionId": "67e21f64fb4213c53714be6b",
      "githubRepo": "https://github.com/pzs19/LEMMA",
      "ai_keywords": [
        "Learning from Errors for Mathematical Advancement (LEMMA)",
        "mistake augmentation",
        "model-aware smooth reflection connection",
        "autonomous error correction"
      ]
    },
    "publishedAt": "2025-03-21T13:59:10.000Z",
    "title": "LEMMA: Learning from Errors for MatheMatical Advancement in LLMs",
    "summary": "Large language models (LLMs) have demonstrated remarkable reasoning\ncapability in solving mathematical problems. However, existing approaches\nprimarily focus on improving the quality of correct training data, e.g.,\ndistilling high-quality correct solutions from advanced models, neglecting the\nvalue contained in error data, potentially hindering the model's reflective\nability. Though some studies attempt to leverage error data, they often involve\ncomplex mechanisms, such as Monte Carlo Tree Search (MCTS) to explore error\nnodes. In this work, we propose to enhance LLMs' reasoning ability by Learning\nfrom Errors for Mathematical Advancement (LEMMA). LEMMA constructs data\nconsisting of an incorrect solution with an erroneous step and a reflection\nconnection to a correct solution for fine-tuning. Specifically, we\nsystematically analyze the model-generated error types and introduce an\nerror-type grounded mistake augmentation method to collect diverse and\nrepresentative errors. Correct solutions are either from fixing the errors or\ngenerating a fresh start. Through a model-aware smooth reflection connection,\nthe erroneous solution is transferred to the correct one. By fine-tuning on the\nconstructed dataset, the model is able to self-correct errors autonomously\nwithin the generation process without relying on external critique models.\nExperimental results demonstrate that LEMMA achieves significant performance\nimprovements over other strong baselines.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17439.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6397f6081323f19c578f142e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6397f6081323f19c578f142e/it7FYYKjlLX8wSsMLm8EO.jpeg",
      "fullname": "QizhiPei",
      "name": "QizhiPei",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 17
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.18940",
      "authors": [
        {
          "_id": "67e21b305d20ec3277dac34a",
          "user": {
            "_id": "64e357dd825f4133e7427bf8",
            "avatarUrl": "/avatars/aeb6869d075f65a581797df2aabfb02f.svg",
            "isPro": false,
            "fullname": "tyfeld",
            "user": "tyfeld",
            "type": "user"
          },
          "name": "Ye Tian",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-25T08:20:24.659Z",
          "hidden": false
        },
        {
          "_id": "67e21b305d20ec3277dac34b",
          "name": "Xin Xia",
          "hidden": false
        },
        {
          "_id": "67e21b305d20ec3277dac34c",
          "user": {
            "_id": "6618d5e83b412cdc85334ca8",
            "avatarUrl": "/avatars/5fe356d58c4c822a60370dbee8d78a69.svg",
            "isPro": false,
            "fullname": "renyuxi",
            "user": "renyuxi",
            "type": "user"
          },
          "name": "Yuxi Ren",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:19:25.116Z",
          "hidden": false
        },
        {
          "_id": "67e21b305d20ec3277dac34d",
          "name": "Shanchuan Lin",
          "hidden": false
        },
        {
          "_id": "67e21b305d20ec3277dac34e",
          "name": "Xing Wang",
          "hidden": false
        },
        {
          "_id": "67e21b305d20ec3277dac34f",
          "user": {
            "_id": "646b7f71df2609a541c1ab9f",
            "avatarUrl": "/avatars/48b82e5fd9b06f41ff825507c36816cd.svg",
            "isPro": false,
            "fullname": "Xuefeng Xiao",
            "user": "xiaoxuefeng",
            "type": "user"
          },
          "name": "Xuefeng Xiao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:19:05.020Z",
          "hidden": false
        },
        {
          "_id": "67e21b305d20ec3277dac350",
          "name": "Yunhai Tong",
          "hidden": false
        },
        {
          "_id": "67e21b305d20ec3277dac351",
          "user": {
            "_id": "64fde4e252e82dd432b74ce9",
            "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
            "isPro": false,
            "fullname": "Ling Yang",
            "user": "Lingaaaaaaa",
            "type": "user"
          },
          "name": "Ling Yang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:19:44.463Z",
          "hidden": false
        },
        {
          "_id": "67e21b305d20ec3277dac352",
          "user": {
            "_id": "67b2795f0bd4ddcd84426bb4",
            "avatarUrl": "/avatars/d4346ac5a0ebbaeb828d832cc6ca9f0b.svg",
            "isPro": false,
            "fullname": "Bin Cui",
            "user": "lazybone128",
            "type": "user"
          },
          "name": "Bin Cui",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:18:48.809Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-24T17:59:02.000Z",
      "submittedOnDailyAt": "2025-03-25T01:26:48.606Z",
      "title": "Notation de formation floue pour l'accélérateur de noeuds de réseaux neuronaux",
      "submittedOnDailyBy": {
        "_id": "64fde4e252e82dd432b74ce9",
        "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
        "isPro": false,
        "fullname": "Ling Yang",
        "user": "Lingaaaaaaa",
        "type": "user"
      },
      "summary": "Les modèles de diffusion montrent une capacité impressionnante pour la génération de contenu visuel, mais présentent des problèmes d'implémentation en raison des coûts de calcul élevés dans le processus d'inférence. Ce surcharge de calcul principalement provient de la complexité bidimensionnelle de l'attention auto-associée par rapport à la résolution des images ou des vidéos. Les méthodes d'accélération existantes réduisent souvent la qualité du résultat ou nécessitent un retraining à coût élevé. Cependant, nous avons découvert que de nombreux modèles de diffusion peuvent être traités à des résolutions basses, ce qui offre une opportunité pour effectuer une inférence plus efficace sans perdre la qualité. Dans cet article, nous présentons un cadre d'apprentissage sans contraintes appelé « Sampling Bottleneck », qui utilise des résolutions basses pour réduire la surcharge de calcul et maintenir la précision du résultat. Sampling Bottleneck effectue le traitement du bruit à haute résolution dans les étapes initiales et finales, et utilise un flux de travail de bruit « High-Low-High » pour traiter les étapes intermédiaires à basse résolution. Pour cela, nous améliorons progressivement les points de transition de résolution et ajustons de manière adaptative le niveau de bruit à chaque étape pour réduire la quantité d'opérations et la complexité. Sampling Bottleneck a été validé dans des tâches de génération d'images et de vidéos, et comparé à l'inférence sur des échantillons de résolution complète, maintient une qualité de sortie similaire, augmentant la vitesse d'inférence de 3 fois pour la génération d'images et de 2,5 fois pour la génération de vidéos. Le code est disponible sur la URL suivante : https://github.com/tyfeld/Bottleneck-Sampling",
      "upvotes": 10,
      "discussionId": "67e21b365d20ec3277dac500",
      "projectPage": "https://tyfeld.github.io/BottleneckSampling.github.io",
      "githubRepo": "https://github.com/tyfeld/Bottleneck-Sampling",
      "ai_keywords": [
        "diffusion models",
        "self-attention",
        "computational overhead",
        "low-resolution priors",
        "Bottleneck Sampling",
        "denoising workflow",
        "high-resolution denoising",
        "aliasing",
        "blurring artifacts",
        "resolution transition points",
        "adaptive timesteps"
      ]
    },
    "publishedAt": "2025-03-24T13:59:02.000Z",
    "title": "Training-free Diffusion Acceleration with Bottleneck Sampling",
    "summary": "Diffusion models have demonstrated remarkable capabilities in visual content\ngeneration but remain challenging to deploy due to their high computational\ncost during inference. This computational burden primarily arises from the\nquadratic complexity of self-attention with respect to image or video\nresolution. While existing acceleration methods often compromise output quality\nor necessitate costly retraining, we observe that most diffusion models are\npre-trained at lower resolutions, presenting an opportunity to exploit these\nlow-resolution priors for more efficient inference without degrading\nperformance. In this work, we introduce Bottleneck Sampling, a training-free\nframework that leverages low-resolution priors to reduce computational overhead\nwhile preserving output fidelity. Bottleneck Sampling follows a high-low-high\ndenoising workflow: it performs high-resolution denoising in the initial and\nfinal stages while operating at lower resolutions in intermediate steps. To\nmitigate aliasing and blurring artifacts, we further refine the resolution\ntransition points and adaptively shift the denoising timesteps at each stage.\nWe evaluate Bottleneck Sampling on both image and video generation tasks, where\nextensive experiments demonstrate that it accelerates inference by up to\n3times for image generation and 2.5times for video generation, all while\nmaintaining output quality comparable to the standard full-resolution sampling\nprocess across multiple evaluation metrics. Code is available at:\nhttps://github.com/tyfeld/Bottleneck-Sampling",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18940.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64fde4e252e82dd432b74ce9",
      "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
      "fullname": "Ling Yang",
      "name": "Lingaaaaaaa",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.17489",
      "authors": [
        {
          "_id": "67e21f300e6b6fcc3eb38ae1",
          "name": "Shu Pu",
          "hidden": false
        },
        {
          "_id": "67e21f300e6b6fcc3eb38ae2",
          "name": "Yaochen Wang",
          "hidden": false
        },
        {
          "_id": "67e21f300e6b6fcc3eb38ae3",
          "user": {
            "_id": "65e2be1e630e2db23829ee8d",
            "avatarUrl": "/avatars/294f9ba909037f03669dc0bb80cabfe3.svg",
            "isPro": false,
            "fullname": "Dongping Chen",
            "user": "fjchendp",
            "type": "user"
          },
          "name": "Dongping Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:20:55.986Z",
          "hidden": false
        },
        {
          "_id": "67e21f300e6b6fcc3eb38ae4",
          "user": {
            "_id": "64964aae457f60023c6a6f9d",
            "avatarUrl": "/avatars/342603e0028204f33fe7f5e3f3da1aa3.svg",
            "isPro": false,
            "fullname": "Yuhang Chen",
            "user": "yuhangchen",
            "type": "user"
          },
          "name": "Yuhang Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:21:02.911Z",
          "hidden": false
        },
        {
          "_id": "67e21f300e6b6fcc3eb38ae5",
          "user": {
            "_id": "67c94fd48670a35a7c05f36c",
            "avatarUrl": "/avatars/a59a7872bcc58fec7747225f2d3da3f9.svg",
            "isPro": false,
            "fullname": "Guohao Wang",
            "user": "NiuniuWang",
            "type": "user"
          },
          "name": "Guohao Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:21:09.345Z",
          "hidden": false
        },
        {
          "_id": "67e21f300e6b6fcc3eb38ae6",
          "name": "Qi Qin",
          "hidden": false
        },
        {
          "_id": "67e21f300e6b6fcc3eb38ae7",
          "name": "Zhongyi Zhang",
          "hidden": false
        },
        {
          "_id": "67e21f300e6b6fcc3eb38ae8",
          "name": "Zhiyuan Zhang",
          "hidden": false
        },
        {
          "_id": "67e21f300e6b6fcc3eb38ae9",
          "user": {
            "_id": "6697e7e55ef2828a1ff371c3",
            "avatarUrl": "/avatars/b361ea817760f7cb5c5d39028ee6b507.svg",
            "isPro": false,
            "fullname": "Zetong Zhou",
            "user": "Frywind",
            "type": "user"
          },
          "name": "Zetong Zhou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:21:29.118Z",
          "hidden": false
        },
        {
          "_id": "67e21f300e6b6fcc3eb38aea",
          "user": {
            "_id": "67575cac2f7acf9a8b4626fa",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/1OkoZh8A4jPKHpTg5iSXP.png",
            "isPro": false,
            "fullname": "Shuang Gong",
            "user": "shuang72",
            "type": "user"
          },
          "name": "Shuang Gong",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:21:35.465Z",
          "hidden": false
        },
        {
          "_id": "67e21f300e6b6fcc3eb38aeb",
          "name": "Yi Gui",
          "hidden": false
        },
        {
          "_id": "67e21f300e6b6fcc3eb38aec",
          "name": "Yao Wan",
          "hidden": false
        },
        {
          "_id": "67e21f300e6b6fcc3eb38aed",
          "name": "Philip S. Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-21T18:59:20.000Z",
      "submittedOnDailyAt": "2025-03-25T01:46:07.985Z",
      "title": "Juge Tout : MLLM a la capacité de rendre jugements sur n'importe quel modèle.",
      "submittedOnDailyBy": {
        "_id": "643be8879f5d314db2d9ed23",
        "avatarUrl": "/avatars/64e9bb2c4e10fbe03e2b81afedf40865.svg",
        "isPro": false,
        "fullname": "Chen Dongping",
        "user": "shuaishuaicdp",
        "type": "user"
      },
      "summary": "L'évaluation de modèles basés sur la génération (MMG) dans des tâches ouvertes comme la compréhension multimodale (MMU) fait face à de grands défis en raison de la complexité des interactions croisées. En réponse à cette situation, l'idée de utiliser des modèles de compréhension multimodale (MLLMs) comme évaluateurs automatiques a été développée, montrant des résultats exceptionnels dans l'évaluation de tâches de compréhension multimodale. De plus, cet article élargit les modèles d'évaluation cohérents pour les MLLMs grâce à deux nouveaux benchmarks : TaskAnything et JudgeAnything. TaskAnything évalue les capacités de 15 catégories de MLLMs en MMU et MMG en utilisant 1 500 questions. JudgeAnything, par contre, évalue la capacité de jugement de cinq modèles avancés (comme GPT-4o et Gemini-2.0-Flash) à partir de la perspective de comparaison de pairs et d'évaluation de scores, offrant une base de test standardisée qui inclut des jugements humains et des guides d'évaluation détaillés. Nos expériences d'expansion montrent que les MLLMs atteignent les résultats souhaités dans l'évaluation de MMU (66,55% en comparaison de pairs et 42,79% en évaluation de scores), mais présentent de grands défis dans l'évaluation de MMG (53,37% en comparaison de pairs et 30,05% en évaluation de scores), révélant des biais croisés et des problèmes de configuration. Pour aborder ces défis, on présente la plateforme automatisée OmniArena, dont l'objectif est d'évaluer ces modèles et de proposer des récompenses pour la multimodalité. Notre recherche souligne la nécessité de protocoles d'évaluation équitables et une forte concordance avec les préférences humaines. Les sources de code et les ensembles de données sont disponibles sur la suivante URL : https://urrealhero.github.io/judgeanythingweb/",
      "upvotes": 10,
      "discussionId": "67e21f350e6b6fcc3eb38c35",
      "ai_keywords": [
        "Multimodal LLMs (MLLMs)",
        "TaskAnything",
        "JudgeAnything",
        "open-ended multimodal understanding (MMU)",
        "open-ended multimodal generation (MMG)",
        "cross-modal interactions",
        "vision-language understanding tasks",
        "any-to-any modality tasks",
        "Pair Comparison",
        "Score Evaluation",
        "omni-models",
        "multimodal reward models",
        "cross-modality biases",
        "hallucination issues"
      ]
    },
    "publishedAt": "2025-03-21T14:59:20.000Z",
    "title": "Judge Anything: MLLM as a Judge Across Any Modality",
    "summary": "Evaluating generative foundation models on open-ended multimodal\nunderstanding (MMU) and generation (MMG) tasks across diverse modalities (e.g.,\nimages, audio, video) poses significant challenges due to the complexity of\ncross-modal interactions. To this end, the idea of utilizing Multimodal LLMs\n(MLLMs) as automated judges has emerged, with encouraging results in assessing\nvision-language understanding tasks. Moving further, this paper extends\nMLLM-as-a-Judge across modalities to a unified manner by introducing two\nbenchmarks, TaskAnything and JudgeAnything, to respectively evaluate the\noverall performance and judging capabilities of MLLMs across any-to-any\nmodality tasks. Specifically, TaskAnything evaluates the MMU and MMG\ncapabilities across 15 any-to-any modality categories, employing 1,500 queries\ncurated from well-established benchmarks. Furthermore, JudgeAnything evaluates\nthe judging capabilities of 5 advanced (e.g., GPT-4o and Gemini-2.0-Flash) from\nthe perspectives of Pair Comparison and Score Evaluation, providing a\nstandardized testbed that incorporates human judgments and detailed rubrics.\nOur extensive experiments reveal that while these MLLMs show promise in\nassessing MMU (i.e., achieving an average of 66.55% in Pair Comparison setting\nand 42.79% in Score Evaluation setting), they encounter significant challenges\nwith MMG tasks (i.e., averaging only 53.37% in Pair Comparison setting and\n30.05% in Score Evaluation setting), exposing cross-modality biases and\nhallucination issues. To address this, we present OmniArena, an automated\nplatform for evaluating omni-models and multimodal reward models. Our work\nhighlights the need for fairer evaluation protocols and stronger alignment with\nhuman preferences. The source code and dataset are publicly available at:\nhttps://urrealhero.github.io/judgeanythingweb/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17489.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "643be8879f5d314db2d9ed23",
      "avatarUrl": "/avatars/64e9bb2c4e10fbe03e2b81afedf40865.svg",
      "fullname": "Chen Dongping",
      "name": "shuaishuaicdp",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.18948",
      "authors": [
        {
          "_id": "67e24217db11e1d382285cd4",
          "user": {
            "_id": "6447a5806ffed6ece1fcf723",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/NjOA7G_QCa3bCluA69hSs.jpeg",
            "isPro": false,
            "fullname": "Ruixiao Dong",
            "user": "dongruixiao",
            "type": "user"
          },
          "name": "Ruixiao Dong",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:20:32.045Z",
          "hidden": false
        },
        {
          "_id": "67e24217db11e1d382285cd5",
          "user": {
            "_id": "63f5993afcf95ecac2b419b5",
            "avatarUrl": "/avatars/a8c020080a84d9a663789c4fb19270e9.svg",
            "isPro": false,
            "fullname": "Mengde Xu",
            "user": "Mendel192",
            "type": "user"
          },
          "name": "Mengde Xu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:20:25.156Z",
          "hidden": false
        },
        {
          "_id": "67e24217db11e1d382285cd6",
          "name": "Zigang Geng",
          "hidden": false
        },
        {
          "_id": "67e24217db11e1d382285cd7",
          "name": "Li Li",
          "hidden": false
        },
        {
          "_id": "67e24217db11e1d382285cd8",
          "user": {
            "_id": "665d88640e92f92b0e7eb17f",
            "avatarUrl": "/avatars/ff3a410e1e7bfb00ff0ec8ce4d5b1463.svg",
            "isPro": false,
            "fullname": "han hu",
            "user": "hanhu2",
            "type": "user"
          },
          "name": "Han Hu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:20:10.271Z",
          "hidden": false
        },
        {
          "_id": "67e24217db11e1d382285cd9",
          "name": "Shuyang Gu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-24T17:59:57.000Z",
      "submittedOnDailyAt": "2025-03-25T05:57:15.975Z",
      "title": "La modélisation des images planes est le processus de projeter un modèle 3D sur une surface 2D, avec l'objectif de représenter chacune des faces du modèle 3D sur la surface 2D. Ce processus implique de projeter chacune des faces du modèle 3D sur la surface 2D pour qu'elles soient représentées sur l'écran 2D. La modélisation des images planes est le processus de projeter un modèle 3D sur une surface 2D, avec l'objectif de représenter chacune des faces du modèle 3D sur la surface 2D. Ce processus implique de projeter chacune des faces du modèle 3D sur la surface 2D pour qu'elles soient représentées sur l'écran 2D. La modélisation des images planes est le processus de projeter un modèle 3D sur une surface 2D, avec l'objectif de représenter chacune des faces du modèle 3D sur la surface 2D. Ce processus implique de projeter chacune des faces du modèle 3D sur la surface 2D pour qu'elles soient représentées sur l'écran 2D. La modélisation des images planes est le processus de projeter un modèle 3D sur une surface 2D, avec l'objectif de représenter chacune des faces du modèle 3D sur la surface 2D. Ce processus implique de projeter chacune des faces du modèle 3D sur la surface 2D pour qu'elles soient représentées sur l'écran 2D. La modélisation des images planes est le processus de projeter un modèle 3D sur une surface 2D, avec l'objectif de représenter chacune des faces du modèle 3D sur la surface 2D. Ce processus implique de projeter chacune des faces du modèle 3D sur la surface 2D pour qu'elles soient représentées sur l'écran 2D. La modélisation des images planes est le processus de projeter un modèle 3D sur une surface 2D, avec l'objectif de représenter chacune des faces du modèle 3D sur la surface 2D. Ce processus implique de projeter chacune des faces du modèle 3D sur la surface 2D pour qu'elles soient représentées sur l'écran 2D. La modélisation des images planes est le processus de projeter un modèle 3D sur une surface 2D, avec l'objectif de représenter chacune des faces du modèle 3D sur la surface 2D. Ce processus implique de projeter chacune des faces du modèle 3D sur la surface 2D pour qu'elles soient représentées sur l'écran 2D. La modélisation des images planes est le processus de projeter un modèle 3D sur une surface 2D, avec l'objectif de représenter chacune des faces du modèle 3D sur la surface 2D. Ce processus implique de projeter chacune des faces du modèle 3D sur la surface 2D pour qu'elles soient représentées sur l'écran 2D. La modélisation des images planes est le processus de projeter un modèle 3D sur une surface 2D, avec l'objectif de représenter chacune des faces du modèle 3D sur la surface 2D. Ce processus implique de projeter chacune des faces du modèle 3D sur la surface 2D pour qu'elles soient représentées sur l'écran 2D. La modélisation des images planes est le processus de projeter un modèle 3D sur une surface 2D, avec l'objectif de représenter chacune des faces du modèle 3D sur la surface 2D. Ce processus implique de projeter chacune des faces du modèle 3D sur la surface 2D pour qu'elles soient représentées sur l'écran 2D. La modélisation des images planes est le processus de projeter un modèle 3D sur une surface 2D, avec l'objectif de représenter chacune des faces du modèle 3D sur la surface 2D. Ce processus implique de projeter chacune des faces du modèle 3D sur la surface 2D pour qu'elles soient représentées sur l'écran 2D. La modélisation des images planes est le processus de projeter un modèle 3D sur une surface 2D, avec l'objectif de représenter chacune des faces du modèle 3D sur la surface 2D. Ce processus implique de projeter chacune des faces du modèle 3D sur la surface 2D pour qu'elles soient représentées sur l'écran 2D. La modélisation des images planes est le processus de projeter un modèle 3D sur une surface 2D, avec l'objectif de représenter chacune des faces du modèle 3D sur la surface 2D. Ce processus implique de projeter chacune des faces du modèle 3D sur la surface 2D pour qu'elles soient représentées sur l'écran 2D. La modélisation des images planes est le processus de projeter un modèle 3D sur une surface 2D, avec l'objectif de représenter chacune des faces du modèle 3D sur la surface 2D. Ce processus implique de projeter chacune des faces du modèle 3D sur la surface 2D pour qu'elles soient représentées sur l'écran 2D. La modélisation des images planes est le processus de projeter un modèle 3D sur une surface 2D, avec l'objectif de représenter chacune des faces du modèle 3D sur la surface 2D. Ce processus implique de projeter chacune des faces du modèle 3D sur la surface 2D pour qu'elles soient représentées sur l'écran 2D. La modélisation des images planes est le processus de projeter un modèle 3D sur une surface 2D, avec l'objectif de représenter chacune des faces du modèle 3D sur la surface 2D. Ce processus implique de projeter chacune des faces du modèle 3D sur la surface 2D pour qu'elles soient représentées sur l'écran 2D. La modélisation des images planes est le processus de projeter un modèle 3D sur une surface 2D, avec l'objectif de représenter chacune des faces du modèle 3D sur la surface 2D. Ce processus implique de projeter chacune des faces du modèle 3D sur la surface 2D pour qu'elles soient représentées sur l'écran 2D. La modélisation des images planes est le processus de projeter un modèle 3D sur une surface 2D, avec l'objectif de représenter chacune des faces du modèle 3D sur la surface 2D. Ce processus implique de projeter chacune des faces du modèle 3D sur la surface 2D pour qu'elles soient représentées sur l'écran 2D. La modélisation des images planes est le processus de projeter un modèle 3D sur une surface 2D, avec l'objectif de représenter chacune des faces du modèle 3D sur la surface 2D. Ce processus implique de projeter chacune des faces du modèle 3D sur la surface 2D pour qu'elles soient représentées sur l'écran 2D. La modélisation des images planes est le processus de projeter un modèle 3D sur une surface 2D, avec l'objectif de représenter chacune des faces du modèle 3D sur la surface 2D. Ce processus implique de projeter chacune des faces du modèle 3D sur la surface 2D pour qu'elles soient représentées sur l'écran 2D. La modélisation des images planes est le processus de projeter un modèle 3D sur une surface 2D, avec l'objectif de représenter chacune des faces du modèle 3D sur la surface 2D. Ce processus implique de projeter chacune des faces du modèle 3D sur la surface 2D pour qu'elles soient représentées sur l'écran 2D. La modélisation des images planes est le processus de projeter un modèle 3D sur une surface 2D, avec l'objectif de représenter chacune des faces du modèle 3D sur la surface 2D. Ce processus implique de projeter chacune des faces du modèle 3D sur la surface 2D pour qu'elles soient représentées sur l'écran 2D. La modélisation des images planes est le processus de projeter un modèle 3D sur une surface 2D, avec l'objectif de représenter chacune des faces du modèle 3D sur la surface 2D. Ce processus implique de projeter chacune des faces du modèle 3D sur la surface 2D pour qu'elles soient représentées sur l'écran 2D. La modélisation des images planes est le processus de projeter un modèle 3D sur une surface 2D, avec l'objectif de représenter chacune des faces du modèle 3D sur la surface 2D. Ce processus implique de projeter chacune des faces du modèle 3D sur la surface 2D pour qu'elles soient représentées sur l'écran 2D. La modélisation des images planes est le processus de projeter un modèle 3D sur une surface 2D, avec l'objectif de représenter chacune des faces du modèle 3D sur la surface 2D. Ce processus implique de projeter chacune des faces du modèle 3D sur la surface 2D pour qu'elles soient représentées sur l'écran 2D. La modélisation des images planes est le processus de projeter un modèle 3D sur une surface 2D, avec l'objectif de représenter chacune des faces du modèle 3D sur la surface 2D. Ce processus implique de projeter chacune des faces du modèle 3D sur la surface 2D pour qu'elles soient représentées sur l'écran 2D. La modélisation des images planes est le processus de projeter un modèle 3D sur une surface 2D, avec l'objectif de représenter chacune des faces du modèle 3D sur la surface 2D. Ce processus implique de projeter chacune des faces du modèle 3D sur la surface 2D pour qu'elles soient représentées sur l'écran 2D. La modélisation des images planes est le processus de projeter un modèle 3D sur une surface 2D, avec l'objectif de représenter chacune des faces du modèle 3D sur la surface 2D. Ce processus implique de projeter chacune des faces du modèle 3D sur la surface 2D pour qu'elles soient représentées sur l'écran 2D. La modélisation des images planes est le processus de projeter un modèle 3D sur une surface 2D, avec l'objectif de représenter chacune des faces du modèle 3D sur la surface 2D. Ce processus implique de projeter chacune des faces du modèle 3D sur la surface 2D pour qu'elles soient représentées sur l'écran 2D. La modélisation des images planes est le processus de projeter un modèle 3D sur une surface 2D, avec l'objectif de représenter chacune des faces du modèle 3D sur la surface 2D. Ce processus implique de projeter chacune des faces du modèle 3D sur la surface 2D pour qu'elles soient représentées sur l'écran 2D. La modélisation des images planes est le processus de projeter un modèle 3D sur une surface 2D, avec l'objectif de représenter chacune des faces du modèle 3D sur la surface 2D. Ce processus implique de projeter chacune des faces du modèle 3D sur la surface 2D pour qu'elles soient représentées sur l'écran 2D. La modélisation des images planes est le processus de projeter un modèle 3D sur une surface 2D, avec l'objectif de représenter chacune des faces du modèle 3D sur la surface 2D. Ce processus implique de projeter chacune des faces du modèle 3D sur la surface 2D pour qu'elles soient représentées sur l'écran 2D. La modélisation des images planes est le processus de projeter un modèle 3D sur une surface 2D, avec l'objectif de représenter chacune des faces du modèle 3D sur la surface 2D. Ce processus implique de projeter chacune des faces du modèle 3D sur la surface 2D pour qu'elles soient représentées sur l'écran 2D. La modélisation des images planes est le processus de projeter un modèle 3D sur une surface 2D, avec l'objectif de représenter chacune des faces du modèle 3D sur la surface 2D. Ce processus implique de projeter chacune des faces du modèle 3D sur la surface 2D pour qu'elles soient représentées sur l'écran 2D. La modélisation des images planes est le processus de projeter un modèle 3D sur une surface 2D, avec l'objectif de représenter chacune des faces du modèle 3D sur la surface 2D. Ce processus implique de projeter chacune des faces du modèle 3D sur la surface 2D pour qu'elles soient représentées sur l'écran 2D. La modélisation des images planes est le processus de projeter un modèle 3D sur une surface 2D, avec l'objectif de représenter chacune des faces du modèle 3D sur la surface 2D. Ce processus implique de projeter chacune des faces du modèle 3D sur la surface 2D pour qu'elles soient représentées sur l'écran 2D. La modélisation des images planes est le processus de projeter un modèle 3D sur une surface 2D, avec l'objectif de représenter chacune des faces du modèle 3D sur la surface 2D. Ce processus implique de projeter chacune des faces du modèle 3D sur la surface 2D pour qu'elles soient représentées sur l'écran 2D. La modélisation des images planes est le processus de projeter un modèle 3D sur une surface 2D, avec l'objectif de représenter chacune des faces du modèle 3D sur la surface 2D. Ce processus implique de projeter chacune des faces du modèle 3D sur la surface 2D pour qu'elles soient représentées sur l'écran 2D. La modélisation des images planes est le processus de projeter un modèle 3D sur une surface 2D, avec l'objectif de représenter chacune des faces du modèle 3D sur la surface 2D. Ce processus implique de projeter chacune des faces du modèle 3D sur la surface 2D pour qu'elles soient représentées sur l'écran 2D. La modélisation des images planes est le processus de projeter un modèle 3D sur une surface 2D, avec l'objectif de représenter chacune des faces du modèle 3D sur la surface 2D. Ce processus implique de projeter chacune des faces du modèle 3D sur la surface 2D pour qu'elles soient représentées sur l'écran 2D. La modélisation des images planes est le processus de projeter un modèle 3D sur une surface 2D, avec l'objectif de représenter chacune des faces du modèle 3D sur la surface 2D. Ce processus implique de projeter chacune des faces du modèle 3D sur la surface 2D pour qu'elles soient représentées sur l'écran 2D. La modélisation des images planes est le processus de projeter un modèle 3D sur une surface 2D, avec l'objectif de représenter chacune des faces du modèle 3D sur la surface 2D. Ce processus implique de projeter chacune des faces du modèle 3D sur la surface 2D pour qu'elles soient représentées sur l'écran 2D. La modélisation des images planes est le processus de projeter un modèle 3D sur une surface 2D, avec l'objectif de représenter chacune des faces du modèle 3D sur la surface 2D. Ce processus implique de projeter chacune des faces du modèle 3D sur la surface 2D pour qu'elles soient représentées sur l'écran 2D. La modélisation des images planes est le processus de projeter un modèle 3D sur une surface 2D, avec l'objectif de représenter chacune des faces du modèle 3D sur la surface 2D. Ce processus implique de projeter chacune des faces du modèle 3D sur la surface 2D pour qu'elles soient représentées sur l'écran 2D. La modélisation des images planes est le processus de projeter un modèle 3D sur une surface 2D, avec l'objectif de représenter chacune des faces du modèle 3D sur la surface 2D. Ce processus implique de projeter chacune des faces du modèle 3D sur la surface 2D pour qu'elles soient représentées sur l'écran 2D. La modélisation des images planes est le processus de projeter un modèle 3D sur une surface 2D, avec l'objectif de représenter chacune des faces du modèle 3D sur la surface 2D. Ce processus implique de projeter chacune des faces du modèle 3D sur la surface 2D pour qu'elles soient représentées sur l'écran 2D. La modélisation des images planes est le processus de projeter un modèle 3D sur une surface 2D, avec l'objectif de représenter chacune des faces du modèle 3D sur la surface 2D. Ce processus implique de projeter chacune des faces du modèle 3D sur la surface 2D pour qu'elles soient représentées sur l'écran 2D. La modélisation des images planes est le processus de projeter un modèle 3D sur une surface 2D, avec l'objectif de représenter chacune des faces du modèle 3D sur la surface 2D. Ce processus implique de projeter chacune des faces du modèle 3D sur la surface 2D pour qu'elles soient représentées sur l'écran 2D. La modélisation des images planes est le processus de projeter un modèle 3D sur une surface 2D, avec l'objectif de représenter chacune des faces du modèle 3D sur la surface 2D. Ce processus implique de projeter chacune des faces du modèle 3D sur la surface 2D pour qu'elles soient représentées sur l'écran 2D. La modélisation des images planes est le processus de projeter un modèle 3D sur une surface 2D, avec l'objectif de représenter chacune des faces du modèle 3D sur la surface 2D. Ce processus implique de projeter chacune des faces du modèle 3D sur la surface 2D pour qu'elles soient représentées sur l'écran 2D. La modélisation des images planes est le processus de projeter un modèle 3D sur une surface 2D, avec l'objectif de représenter chacune des faces du modèle 3D sur la surface 2D. Ce processus implique de projeter chacune des faces du modèle 3D sur la surface 2D pour qu'elles soient représentées sur l'écran 2D. La modélisation des images planes est le processus de projeter un modèle 3D sur une surface 2D, avec l'objectif de représenter chacune des faces du modèle 3D sur la surface 2D. Ce processus implique de projeter chacune des faces du modèle 3D sur la surface 2D pour qu'elles soient représentées sur l'écran 2D. La modélisation des images planes est le processus de projeter un modèle 3D sur une surface 2D, avec l'objectif de représenter chacune des faces du modèle 3D sur la surface 2D. Ce processus implique de projeter chacune des faces du modèle 3D sur la surface 2D pour qu'elles soient représentées sur l'écran 2D. La modélisation des images planes est le processus de projeter un modèle 3D sur une surface 2D, avec l'objectif de représenter chacune des faces du modèle 3D sur la surface 2D. Ce processus implique de projeter chacune des faces du modèle 3D sur la surface 2D pour qu'elles soient représentées sur l'écran 2D. La modélisation des images planes est le processus de projeter un modèle 3D sur une surface 2D, avec l'objectif de représenter chacune des faces du modèle 3D sur la surface 2D. Ce processus implique de projeter chacune des faces du modèle 3D sur la surface 2D pour qu'elles soient représentées sur l'écran 2D. La modélisation des images planes est le processus de projeter un modèle 3D sur une surface 2D, avec l'objectif de représenter chacune des faces du modèle 3D sur la surface 2D. Ce processus implique de projeter chacune des faces du modèle 3D sur la surface 2D pour qu'elles soient représentées sur l'écran 2D. La modélisation des images planes est le processus de projeter un modèle 3D sur une surface 2D, avec l'objectif de représenter chacune des faces du modèle 3D sur la surface 2D. Ce processus implique de projeter chacune des faces du modèle 3D sur la surface 2D pour qu'elles soient représentées sur l'écran 2D. La modélisation des images planes est le processus de projeter un modèle 3D sur une surface 2D, avec l'objectif de représenter chacune des faces du modèle 3D sur la surface 2D. Ce processus implique de projeter chacune des faces du modèle 3D sur la surface 2D pour qu'elles soient représentées sur l'écran 2D. La modélisation des images planes est le processus de projeter un modèle 3D sur une surface 2D, avec l'objectif de représenter chacune des faces du modèle 3D sur la surface 2D. Ce processus implique de projeter chacune des faces du modèle 3D sur la surface 2D pour qu'elles soient représentées sur l'écran 2D. La modélisation des images planes est le processus de projeter un modèle 3D sur une surface 2D, avec l'objectif de représenter chacune des faces du modèle 3D sur la surface 2D. Ce processus implique de projeter chacune des faces du modèle 3D sur la surface 2D pour qu'elles soient représentées sur l'écran 2D. La modélisation des images planes est le processus de projeter un modèle 3D sur une surface 2D, avec l'objectif de représenter chacune des faces du modèle 3D sur la surface 2D. Ce processus implique de projeter chacune des faces du modèle 3D sur la surface 2D pour qu'elles soient représentées sur l'écran 2D. La modélisation des images planes est le processus de projeter un modèle 3D sur une surface 2D, avec l'objectif de représenter chacune des faces du modèle 3D sur la surface 2D. Ce processus implique de projeter chacune des faces du modèle 3D sur la surface 2D pour qu'elles soient représentées sur l'écran 2D. La modélisation des images planes est le processus de projeter un modèle 3D sur une surface 2D, avec l'objectif de représenter chacune des faces du modèle 3D sur la surface 2D. Ce processus implique de projeter chacune des faces du modèle 3D sur la surface 2D pour qu'elles soient représentées sur l'écran 2D. La modélisation des images planes est le processus de projeter un modèle 3D sur une surface 2D, avec l'objectif de représenter chacune des faces du modèle 3D sur la surface 2D. Ce processus implique de projeter chacune des faces du modèle 3D sur la surface 2D pour qu'elles soient représentées sur l'écran 2D. La modélisation des images planes est le processus de projeter un modèle 3D sur une surface 2D, avec l'objectif de représenter chacune des faces du modèle 3D sur la surface 2D. Ce processus implique de projeter chacune des faces du modèle 3D sur la surface 2D pour qu'elles soient représentées sur l'écran 2D. La modélisation des images planes est le processus de projeter un modèle 3D sur une surface 2D, avec l'objectif de représenter chacune des faces du modèle 3D sur la surface 2D. Ce processus implique de projeter chacune des faces du modèle 3D sur la surface 2D pour qu'elles soient représentées sur l'écran 2D. La modélisation des images planes est le processus de projeter un modèle 3D sur une surface 2D, avec l'objectif de représenter chacune des faces du modèle 3D sur la surface 2D. Ce processus implique de projeter chacune des faces du modèle 3D sur la surface 2D pour qu'elles soient représentées sur l'écran 2D. La modélisation des images planes est le processus de projeter un modèle 3D sur une surface 2D, avec l'objectif de représenter chacune des faces du modèle 3D sur la surface 2D. Ce processus implique de projeter chacune des faces du modèle 3D sur la surface 2D pour qu'elles soient représentées sur l'écran 2D. La modélisation des images planes est le processus de projeter un modèle 3D sur une surface 2D, avec l'objectif de représenter chacune des faces du modèle 3D sur la surface 2D. Ce processus implique de projeter chacune des faces du modèle 3D sur la surface 2D pour qu'elles soient représentées sur l'écran 2D. La modélisation des images planes est le processus de projeter un modèle 3D sur une surface 2D, avec l'objectif de représenter chacune des faces du modèle 3D sur la surface 2D. Ce processus implique de projeter chacune des faces du modèle 3D sur la surface 2D pour qu'elles soient représentées sur l'écran 2D. La modélisation des images planes est le processus de projeter un modèle 3D sur une surface 2D, avec l'objectif de représenter chacune des faces du modèle 3D sur la surface 2D. Ce processus implique de projeter chacune des faces du modèle 3D sur la surface 2D pour qu'elles soient représentées sur l'écran 2D. La modélisation des images planes est le processus de projeter un modèle 3D sur une surface 2D, avec l'objectif de représenter chacune des faces du modèle 3D sur la surface 2D. Ce processus implique de projeter chacune des faces du modèle 3D sur la surface 2D pour qu'elles soient représentées sur l'écran 2D. La modélisation des images planes est le processus de projeter un modèle 3D sur une surface 2D, avec l'objectif de représenter chacune des faces du modèle 3D sur la surface 2D. Ce processus implique de projeter chacune des faces du modèle 3D sur la surface 2D pour qu'elles soient représentées sur l'écran 2D. La modélisation des images planes est le processus de projeter un modèle 3D sur une surface 2D, avec l'objectif de représenter chacune des faces du modèle 3D sur la surface 2D. Ce processus implique de projeter chacune des faces du modèle 3D sur la surface 2D pour qu'elles soient représentées sur l'écran 2D. La modélisation des images planes est le processus de projeter un modèle 3D sur une surface 2D, avec l'objectif de représenter chacune des faces du modèle 3D sur la surface 2D. Ce processus implique de projeter chacune des faces du modèle 3D sur la surface 2D pour qu'elles soient représentées sur l'écran 2D. La modélisation des images planes est le processus de projeter un modèle 3D sur une surface 2D, avec l'objectif de représenter chacune des faces du modèle 3D sur la surface 2D. Ce processus implique de projeter chacune des faces du modèle 3D sur la surface 2D pour qu'elles soient représentées sur l'écran 2D. La modélisation des images planes est le processus de projeter un modèle 3D sur une surface 2D, avec l'objectif de représenter chacune des faces du modèle 3D sur la surface 2D. Ce processus implique de projeter chacune des faces du modèle 3D sur la surface 2D pour qu'elles soient représentées sur l'écran 2D. La modélisation des images planes est le processus de projeter un modèle 3D sur une surface 2D, avec l'objectif de représenter chacune des faces du modèle 3D sur la surface 2D. Ce processus implique de projeter chacune des faces du modèle 3D sur la surface 2D pour qu'elles soient représentées sur l'écran 2D. La modélisation des images planes est le processus de projeter un modèle 3D sur une surface 2D, avec l'objectif de représenter chacune des faces du modèle 3D sur la surface 2D. Ce processus implique de projeter chacune des faces du modèle 3D sur la surface 2D pour qu'elles soient représentées sur l'écran 2D. La modélisation des images planes est le processus de projeter un modèle 3D sur une surface 2D, avec l'objectif de représenter chacune des faces du modèle 3D sur la surface 2D. Ce processus implique de projeter chacune des faces du modèle 3D sur la surface 2D pour qu'elles soient représentées sur l'écran 2D. La modélisation des images planes est le processus de projeter un modèle 3D sur une surface 2D, avec l'objectif de représenter chacune des faces du modèle 3D sur la surface 2D. Ce processus implique de projeter chacune des faces du modèle 3D sur la surface 2D pour qu'elles soient représentées sur l'écran 2D. La modélisation des images planes est le processus de projeter un modèle 3D sur une surface 2D, avec l'objectif de représenter chacune des faces du modèle 3D sur la surface 2D. Ce processus implique de projeter chacune des faces du modèle 3D sur la surface 2D pour qu'elles soient représentées sur l'écran 2D. La modélisation des images planes est le processus de projeter un modèle 3D sur une surface 2D, avec l'objectif de représenter chacune des faces du modèle 3D sur la surface 2D. Ce processus implique de projeter chacune des faces du modèle 3D sur la surface 2D pour qu'elles soient représentées sur l'écran 2D. La modélisation des images planes est le processus de projeter un modèle 3D sur une surface 2D, avec l'objectif de représenter chacune des faces du modèle 3D sur la surface 2D. Ce processus implique de projeter chacune des faces du modèle 3D sur la surface 2D pour qu'elles soient représentées sur l'écran 2D. La modélisation des images planes est le processus de projeter un modèle 3D sur une surface 2D, avec l'objectif de représenter chacune des faces du modèle 3D sur la surface",
      "submittedOnDailyBy": {
        "_id": "64c38fcf573c5a427e12cd37",
        "avatarUrl": "/avatars/2b9de06f29147ed2c212e920afba0eaf.svg",
        "isPro": false,
        "fullname": "cientgu",
        "user": "cientgu",
        "type": "user"
      },
      "summary": "Les modèles génératifs actuels (par exemple, les modèles d'auto-complétion et les méthodes d'accès distribué) se basent sur la décomposition de l'apprentissage de la distribution de données de haute dimension dans des tâches sous-ordonnées dans un ordre simple. Cependant, lors de l'optimisation commune de ces tâches sous-ordonnées, des collisions uniques apparaissent que les approches actuelles ne peuvent résoudre sans perdre d'efficacité et d'échelle. Nous utilisons l'invariance du mouvement du signal visuel naturel pour aligner les objectifs d'optimisation entre les tâches sous-ordonnées de manière interne. Nous proposons un nouveau cadre de modélisation d'images avec des symétries équivariantes. Notre méthode consiste à : (1) introduire la bipolarité en colonne pour renforcer la symétrie du mouvement dans la direction horizontale et (2) introduire l'attention causale sur la taille de la fenêtre pour imposer des relations de contexte cohérentes entre les positions. L'évaluation de la génération d'ImageNet conditionnée par classe (résolution 256x256) montre que notre approche atteint un rendement similaire au meilleur modèle AR de l'état de l'art, tout en utilisant moins de coût de calcul. Une analyse systématique montre que l'extension de la symétrie équivariante peut réduire les collisions entre tâches, améliorer significativement la capacité d'expansion 0 shot et permettre la synthèse d'images à long terme. Cette recherche établit le premier cadre de travail pour la décomposition de la coïncidence de tâches dans les modèles génératifs, et fournit une vision sur la partage efficace de paramètres et l'optimisation sans collisions. Le code et le modèle sont disponibles sur https://github.com/drx-code/EquivariantModeling.",
      "upvotes": 9,
      "discussionId": "67e2421edb11e1d382285f9b",
      "ai_keywords": [
        "autoregressive",
        "diffusion approaches",
        "high-dimensional data distribution learning",
        "subtasks",
        "joint optimization",
        "equivariant image modeling framework",
        "translation invariance",
        "column-wise tokenization",
        "translational symmetry",
        "windowed causal attention",
        "contextual relationships",
        "class-conditioned ImageNet generation",
        "state-of-the-art AR models",
        "computational resources",
        "enhanced equivariance",
        "zero-shot generalization",
        "ultra-long image synthesis",
        "task-aligned decomposition",
        "efficient parameter sharing",
        "conflict-free optimization"
      ]
    },
    "publishedAt": "2025-03-24T13:59:57.000Z",
    "title": "Equivariant Image Modeling",
    "summary": "Current generative models, such as autoregressive and diffusion approaches,\ndecompose high-dimensional data distribution learning into a series of simpler\nsubtasks. However, inherent conflicts arise during the joint optimization of\nthese subtasks, and existing solutions fail to resolve such conflicts without\nsacrificing efficiency or scalability. We propose a novel equivariant image\nmodeling framework that inherently aligns optimization targets across subtasks\nby leveraging the translation invariance of natural visual signals. Our method\nintroduces (1) column-wise tokenization which enhances translational symmetry\nalong the horizontal axis, and (2) windowed causal attention which enforces\nconsistent contextual relationships across positions. Evaluated on\nclass-conditioned ImageNet generation at 256x256 resolution, our approach\nachieves performance comparable to state-of-the-art AR models while using fewer\ncomputational resources. Systematic analysis demonstrates that enhanced\nequivariance reduces inter-task conflicts, significantly improving zero-shot\ngeneralization and enabling ultra-long image synthesis. This work establishes\nthe first framework for task-aligned decomposition in generative modeling,\noffering insights into efficient parameter sharing and conflict-free\noptimization. The code and models are publicly available at\nhttps://github.com/drx-code/EquivariantModeling.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18948.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64c38fcf573c5a427e12cd37",
      "avatarUrl": "/avatars/2b9de06f29147ed2c212e920afba0eaf.svg",
      "fullname": "cientgu",
      "name": "cientgu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.18886",
      "authors": [
        {
          "_id": "67e21d3484513315a9169aae",
          "user": {
            "_id": "6481764e8af4675862efb22e",
            "avatarUrl": "/avatars/fc2e076bc861693f598a528a068a696e.svg",
            "isPro": true,
            "fullname": "weichenfan",
            "user": "weepiess2383",
            "type": "user"
          },
          "name": "Weichen Fan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-25T08:20:22.659Z",
          "hidden": false
        },
        {
          "_id": "67e21d3484513315a9169aaf",
          "name": "Amber Yijia Zheng",
          "hidden": false
        },
        {
          "_id": "67e21d3484513315a9169ab0",
          "name": "Raymond A. Yeh",
          "hidden": false
        },
        {
          "_id": "67e21d3484513315a9169ab1",
          "name": "Ziwei Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-24T16:59:57.000Z",
      "submittedOnDailyAt": "2025-03-25T06:34:48.098Z",
      "title": "CFG-Zero* : Amélioration de la classe de guidage sans paires dans le modèle de flux d'ajustement",
      "submittedOnDailyBy": {
        "_id": "6481764e8af4675862efb22e",
        "avatarUrl": "/avatars/fc2e076bc861693f598a528a068a696e.svg",
        "isPro": true,
        "fullname": "weichenfan",
        "user": "weepiess2383",
        "type": "user"
      },
      "summary": "Classifier-Free Guidance (CFG) est une technologie largement utilisée dans les modèles de réseau de neurones pour améliorer la qualité et la contrôlabilité des images. Dans cette étude, une analyse qualitative est effectuée sur l'impact de CFG dans un modèle de Flow Matching entraîné avec des mélanges de Gaussiennes. Au début des étapes d'entraînement, lorsque l'estimation du Flow est inaccurate, CFG oriente les échantillons vers des trajectoires erronées. Sur la base de cette observation, on propose CFG-Zero*. CFG-Zero* comporte deux contributions : premièrement, ajuste la échelle pour optimiser un scalaire qui corrige la précision de la vitesse estimée ; secondement, initialise à zéro les premiers pas d'un Solveur d'Équation Différentielle (ODE Solver). Lors de la génération d'images à partir du texte (Lumina-Next, Stable Diffusion 3, Flux) et de vidéos à partir du texte (Wan-2.1), CFG-Zero* montre des résultats supérieurs à CFG, démontrant effectivement son utilité en tant que guide dans les modèles de Flow Matching. Le code est disponible sur github.com/WeichenFan/CFG-Zero-star.",
      "upvotes": 7,
      "discussionId": "67e21d3884513315a9169bba",
      "projectPage": "https://weichenfan.github.io/webpage-cfg-zero-star/",
      "githubRepo": "https://github.com/WeichenFan/CFG-Zero-star",
      "ai_keywords": [
        "Classifier-Free Guidance (CFG)",
        "diffusion/flow models",
        "image fidelity",
        "controllability",
        "flow matching models",
        "Gaussian mixtures",
        "ground-truth flow",
        "flow estimation",
        "estimated velocity",
        "scalar optimization",
        "ODE solver",
        "text-to-image",
        "Lumina-Next",
        "Stable Diffusion 3",
        "Flux",
        "text-to-video",
        "Wan-2.1",
        "CFG-Zero*"
      ]
    },
    "publishedAt": "2025-03-24T12:59:57.000Z",
    "title": "CFG-Zero*: Improved Classifier-Free Guidance for Flow Matching Models",
    "summary": "Classifier-Free Guidance (CFG) is a widely adopted technique in\ndiffusion/flow models to improve image fidelity and controllability. In this\nwork, we first analytically study the effect of CFG on flow matching models\ntrained on Gaussian mixtures where the ground-truth flow can be derived. We\nobserve that in the early stages of training, when the flow estimation is\ninaccurate, CFG directs samples toward incorrect trajectories. Building on this\nobservation, we propose CFG-Zero*, an improved CFG with two contributions: (a)\noptimized scale, where a scalar is optimized to correct for the inaccuracies in\nthe estimated velocity, hence the * in the name; and (b) zero-init, which\ninvolves zeroing out the first few steps of the ODE solver. Experiments on both\ntext-to-image (Lumina-Next, Stable Diffusion 3, and Flux) and text-to-video\n(Wan-2.1) generation demonstrate that CFG-Zero* consistently outperforms CFG,\nhighlighting its effectiveness in guiding Flow Matching models. (Code is\navailable at github.com/WeichenFan/CFG-Zero-star)",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18886.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6481764e8af4675862efb22e",
      "avatarUrl": "/avatars/fc2e076bc861693f598a528a068a696e.svg",
      "fullname": "weichenfan",
      "name": "weepiess2383",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.18923",
      "authors": [
        {
          "_id": "67e226f401cdb8cf3a1c7cd8",
          "user": {
            "_id": "640222f83e3d0f2745b097b2",
            "avatarUrl": "/avatars/c5dbac84734855369a7f57b051f16caa.svg",
            "isPro": false,
            "fullname": "Meng Cao",
            "user": "mengcao",
            "type": "user"
          },
          "name": "Meng Cao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:22:38.738Z",
          "hidden": false
        },
        {
          "_id": "67e226f401cdb8cf3a1c7cd9",
          "name": "Pengfei Hu",
          "hidden": false
        },
        {
          "_id": "67e226f401cdb8cf3a1c7cda",
          "name": "Yingyao Wang",
          "hidden": false
        },
        {
          "_id": "67e226f401cdb8cf3a1c7cdb",
          "user": {
            "_id": "65733c1b244aefdfc45cc771",
            "avatarUrl": "/avatars/7223cedbeed065c28a400e130cea30ae.svg",
            "isPro": false,
            "fullname": "Jihao Guo",
            "user": "grejioh",
            "type": "user"
          },
          "name": "Jihao Gu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:22:23.718Z",
          "hidden": false
        },
        {
          "_id": "67e226f401cdb8cf3a1c7cdc",
          "name": "Haoran Tang",
          "hidden": false
        },
        {
          "_id": "67e226f401cdb8cf3a1c7cdd",
          "name": "Haoze Zhao",
          "hidden": false
        },
        {
          "_id": "67e226f401cdb8cf3a1c7cde",
          "name": "Jiahua Dong",
          "hidden": false
        },
        {
          "_id": "67e226f401cdb8cf3a1c7cdf",
          "user": {
            "_id": "63f095be6309c84d5f48848a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f095be6309c84d5f48848a/pL2CKi-r-0mMfIGhYSAsm.jpeg",
            "isPro": false,
            "fullname": "Wangbo Yu",
            "user": "Drexubery",
            "type": "user"
          },
          "name": "Wangbo Yu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:23:02.455Z",
          "hidden": false
        },
        {
          "_id": "67e226f401cdb8cf3a1c7ce0",
          "user": {
            "_id": "638efcf4c67af472d316d424",
            "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
            "isPro": false,
            "fullname": "Ge Zhang",
            "user": "zhangysk",
            "type": "user"
          },
          "name": "Ge Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:22:08.334Z",
          "hidden": false
        },
        {
          "_id": "67e226f401cdb8cf3a1c7ce1",
          "name": "Ian Reid",
          "hidden": false
        },
        {
          "_id": "67e226f401cdb8cf3a1c7ce2",
          "name": "Xiaodan Liang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-24T17:46:09.000Z",
      "submittedOnDailyAt": "2025-03-25T02:17:41.453Z",
      "title": "Video SimpleQA : La question de l'évaluation de la vérité dans les modèles de langage vidéo à grande échelle",
      "submittedOnDailyBy": {
        "_id": "638efcf4c67af472d316d424",
        "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
        "isPro": false,
        "fullname": "Ge Zhang",
        "user": "zhangysk",
        "type": "user"
      },
      "summary": "Récemment, le développement des grands modèles de langue vidéo (LVLMs) a démontré la possibilité de comprendre divers modes, mais l'évaluation de la base factuelle dans les contenus vidéo est un problème important qui n'a pas encore été résolu. Pour aborder ce problème, nous présentons Video SimpleQA, le premier cadre d'évaluation détaillé pour l'évaluation de la factualité dans les LVLMs. Notre étude présente les caractéristiques principales suivantes :\n\n1. **Necessité de connaissance** : Intègre l'ajout de connaissance externe et dépasse la narration explicite.\n2. **Questions d'exploration de la vérité** : Évite l'interprétation subjective et se concentre sur des événements ou des relations objectifs et sans controverse.\n3. **Réponses sûres et brèves** : Les réponses sont écrites dans un format court, facile à comprendre et avec une haute certitude, et peuvent être évaluées automatiquement via le cadre de jugement LLM-as-a-judge, avec un minimum de variation ponctuelle.\n4. **Vérification de sources externes** : Tous les commentaires sont vérifiés avec des ressources externes de haut niveau et un processus de vérification rigoureux, assurant leur crédibilité.\n5. **Necessité d'inférence temporelle cyclique** : La nature des questions commentées inclut tant la compréhension d'un seul cadre statique que l'inférence temporelle cyclique dynamique, et elle est évaluée en fonction du contexte à long terme pour évaluer clairement la factualité des LVLMs.\n\nNous avons évalué largement 41 des LVLMs les plus avancés. Les principales conclusions sont :\n\n1. Les LVLMs actuels, en particulier les modèles open-source, présentent une claire manque de confiance en leur base factuelle. Le modèle avec les meilleurs résultats, Gemini-1.5-Pro, atteint un F-score d'environ 54.4%.\n2. Le paradigme de calcul pendant les tests ne fournit pas une amélioration significative du rendement. Un limite fondamental a été démontré pour l'amélioration de la factualité en effectuant des calculs ultérieurs.\n3. Les recherches supplémentaires consomment du temps supplémentaire pour l'inférence, mais montrent une amélioration constante, indiquant un important équilibre entre efficacité et rendement.",
      "upvotes": 6,
      "discussionId": "67e226f601cdb8cf3a1c7d73",
      "projectPage": "https://videosimpleqa.github.io",
      "ai_keywords": [
        "Large Video Language Models (LVLMs)",
        "multi-modal understanding",
        "factuality evaluation",
        "Video SimpleQA",
        "external knowledge",
        "objective events",
        "relationships",
        "short-form answer",
        "LLM-as-a-judge",
        "automated evaluation",
        "scQUIre",
        "authoritative external references",
        "temporal reasoning",
        "long-context dependencies",
        "F-score",
        "test-time compute",
        "Retrieval-Augmented Generation",
        "inference time overhead",
        "efficiency-performance trade-off"
      ]
    },
    "publishedAt": "2025-03-24T13:46:09.000Z",
    "title": "Video SimpleQA: Towards Factuality Evaluation in Large Video Language\n  Models",
    "summary": "Recent advancements in Large Video Language Models (LVLMs) have highlighted\ntheir potential for multi-modal understanding, yet evaluating their factual\ngrounding in video contexts remains a critical unsolved challenge. To address\nthis gap, we introduce Video SimpleQA, the first comprehensive benchmark\ntailored for factuality evaluation of LVLMs. Our work distinguishes from\nexisting video benchmarks through the following key features: 1) Knowledge\nrequired: demanding integration of external knowledge beyond the explicit\nnarrative; 2) Fact-seeking question: targeting objective, undisputed events or\nrelationships, avoiding subjective interpretation; 3) Definitive & short-form\nanswer: Answers are crafted as unambiguous and definitively correct in a short\nformat, enabling automated evaluation through LLM-as-a-judge frameworks with\nminimal scoring variance; 4) External-source verified: All annotations undergo\nrigorous validation against authoritative external references to ensure the\nreliability; 5) Temporal reasoning required: The annotated question types\nencompass both static single-frame understanding and dynamic temporal\nreasoning, explicitly evaluating LVLMs factuality under the long-context\ndependencies. We extensively evaluate 41 state-of-the-art LVLMs and summarize\nkey findings as follows: 1) Current LVLMs exhibit notable deficiencies in\nfactual adherence, particularly for open-source models. The best-performing\nmodel Gemini-1.5-Pro achieves merely an F-score of 54.4%; 2) Test-time compute\nparadigms show insignificant performance gains, revealing fundamental\nconstraints for enhancing factuality through post-hoc computation; 3)\nRetrieval-Augmented Generation demonstrates consistent improvements at the cost\nof additional inference time overhead, presenting a critical\nefficiency-performance trade-off.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18923.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "638efcf4c67af472d316d424",
      "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
      "fullname": "Ge Zhang",
      "name": "zhangysk",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 43
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.14428",
      "authors": [
        {
          "_id": "67e217941cb9bded659267f0",
          "name": "Hongyu Zhang",
          "hidden": false
        },
        {
          "_id": "67e217941cb9bded659267f1",
          "user": {
            "_id": "64210d1fd039a891a914986d",
            "avatarUrl": "/avatars/b178a768657eb223bdbfbd9e0a2000ff.svg",
            "isPro": false,
            "fullname": "Yufan Deng",
            "user": "dyf",
            "type": "user"
          },
          "name": "Yufan Deng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:28:50.390Z",
          "hidden": false
        },
        {
          "_id": "67e217941cb9bded659267f2",
          "user": {
            "_id": "63468720dd6d90d82ccf3450",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
            "isPro": false,
            "fullname": "YSH",
            "user": "BestWishYsh",
            "type": "user"
          },
          "name": "Shenghai Yuan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-25T08:20:33.315Z",
          "hidden": false
        },
        {
          "_id": "67e217941cb9bded659267f3",
          "user": {
            "_id": "63ad0b04e3b217fb36d36c13",
            "avatarUrl": "/avatars/5a3715ba20859052ba04c048db9e03c2.svg",
            "isPro": false,
            "fullname": "Peng Jin",
            "user": "Pengjin",
            "type": "user"
          },
          "name": "Peng Jin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:29:15.403Z",
          "hidden": false
        },
        {
          "_id": "67e217941cb9bded659267f4",
          "user": {
            "_id": "65b2529285b6c21448a10d65",
            "avatarUrl": "/avatars/1b09e2742aecce1bbdc57f0c4504cf38.svg",
            "isPro": false,
            "fullname": "Zesen Cheng",
            "user": "ClownRat",
            "type": "user"
          },
          "name": "Zesen Cheng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:28:42.575Z",
          "hidden": false
        },
        {
          "_id": "67e217941cb9bded659267f5",
          "name": "Yian Zhao",
          "hidden": false
        },
        {
          "_id": "67e217941cb9bded659267f6",
          "name": "Chang Liu",
          "hidden": false
        },
        {
          "_id": "67e217941cb9bded659267f7",
          "name": "Jie Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-18T17:02:14.000Z",
      "submittedOnDailyAt": "2025-03-25T01:10:50.245Z",
      "title": "Dernier composant : Configuration de la génération vidéo à l'aide d'ajustements micro à deux étapes sans entraînement",
      "submittedOnDailyBy": {
        "_id": "63468720dd6d90d82ccf3450",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
        "isPro": false,
        "fullname": "YSH",
        "user": "BestWishYsh",
        "type": "user"
      },
      "summary": "La génération de vidéo à partir du texte (T2V) se concentre sur le développement de modèles de diffusion. Cependant, les méthodes actuelles rencontrent des défis dans la combinaison précise de caractéristiques, la détermination des relations spatiales et la compréhension des interactions complexes d'actions. Pour résoudre ces limitations, nous proposons un méthode appelée MagicComp, qui renforce la génération structurée de T2V à travers deux étapes, sans limitations d'apprentissage. Spécifiquement :\n\n1. Dans l'étape conditionnelle, nous introduisons la Sémantique Anchor Disambiguation pour renforcer le sens propre du thème et résoudre progressivement les incertitudes entre thèmes, en injectant des vecteurs de direction sémantiques dans les embeddings de texte.\n\n2. Dans l'étape de désenumération, nous proposons l'Attention Fusion Dynamique pour intégrer des connaissances locales et une reconnaissance spatiale adaptative au modèle, et ajuster l'attention avec des masques pour combiner les thèmes de manière flexible dans le domaine temporel-spectral.\n\nMagicComp est indépendant du modèle et fonctionnel, ce qui permet une intégration facile dans les architectures actuelles de T2V. Les expériences étendues sur T2V-CompBench et VBench montrent que MagicComp dépasse les meilleurs méthodes existantes, soulignant la possibilité de générer des vidéos complexes basées sur des prompts ou contrôlables par des chemins. Page du projet : https://hong-yu-zhang.github.io/MagicComp-Page/",
      "upvotes": 6,
      "discussionId": "67e217981cb9bded65926978",
      "projectPage": "https://hong-yu-zhang.github.io/MagicComp-Page/",
      "githubRepo": "https://github.com/Hong-yu-Zhang/MagicComp",
      "ai_keywords": [
        "Semantic Anchor Disambiguation",
        "Dynamic Layout Fusion Attention",
        "grounding priors",
        "model-adaptive spatial perception",
        "masked attention modulation"
      ]
    },
    "publishedAt": "2025-03-18T13:02:14.000Z",
    "title": "MagicComp: Training-free Dual-Phase Refinement for Compositional Video\n  Generation",
    "summary": "Text-to-video (T2V) generation has made significant strides with diffusion\nmodels. However, existing methods still struggle with accurately binding\nattributes, determining spatial relationships, and capturing complex action\ninteractions between multiple subjects. To address these limitations, we\npropose MagicComp, a training-free method that enhances compositional T2V\ngeneration through dual-phase refinement. Specifically, (1) During the\nConditioning Stage: We introduce the Semantic Anchor Disambiguation to\nreinforces subject-specific semantics and resolve inter-subject ambiguity by\nprogressively injecting the directional vectors of semantic anchors into\noriginal text embedding; (2) During the Denoising Stage: We propose Dynamic\nLayout Fusion Attention, which integrates grounding priors and model-adaptive\nspatial perception to flexibly bind subjects to their spatiotemporal regions\nthrough masked attention modulation. Furthermore, MagicComp is a model-agnostic\nand versatile approach, which can be seamlessly integrated into existing T2V\narchitectures. Extensive experiments on T2V-CompBench and VBench demonstrate\nthat MagicComp outperforms state-of-the-art methods, highlighting its potential\nfor applications such as complex prompt-based and trajectory-controllable video\ngeneration. Project page: https://hong-yu-zhang.github.io/MagicComp-Page/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14428.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63468720dd6d90d82ccf3450",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
      "fullname": "YSH",
      "name": "BestWishYsh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 35
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.18908",
      "authors": [
        {
          "_id": "67e230fd4b9f234b60d06389",
          "user": {
            "_id": "66857bd849a4ed9de4c31936",
            "avatarUrl": "/avatars/f6f016bf36fad5b29f30fbec6cde3e4d.svg",
            "isPro": false,
            "fullname": "Akhiad Bercovich",
            "user": "abercovich",
            "type": "user"
          },
          "name": "Akhiad Bercovich",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:30:49.482Z",
          "hidden": false
        },
        {
          "_id": "67e230fd4b9f234b60d0638a",
          "user": {
            "_id": "6756aa3741b39ab0d327de52",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/VjjYWljIgPn9HEMDyKtft.png",
            "isPro": false,
            "fullname": "Mohammad Dabbah",
            "user": "mdabbah-nvidia",
            "type": "user"
          },
          "name": "Mohammad Dabbah",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:31:05.696Z",
          "hidden": false
        },
        {
          "_id": "67e230fd4b9f234b60d0638b",
          "user": {
            "_id": "6509a96c61c4bb4636fd0fd2",
            "avatarUrl": "/avatars/8ffa9b4dd698469f7d70d4d9144aac82.svg",
            "isPro": false,
            "fullname": "Omri Puny",
            "user": "omripuny",
            "type": "user"
          },
          "name": "Omri Puny",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:31:13.161Z",
          "hidden": false
        },
        {
          "_id": "67e230fd4b9f234b60d0638c",
          "name": "Ido Galil",
          "hidden": false
        },
        {
          "_id": "67e230fd4b9f234b60d0638d",
          "user": {
            "_id": "65006cac12c1442d993d6d51",
            "avatarUrl": "/avatars/6700109303b902d453f3d8e2b45a103f.svg",
            "isPro": false,
            "fullname": "Geifman",
            "user": "AmnonGeifman",
            "type": "user"
          },
          "name": "Amnon Geifman",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:31:22.938Z",
          "hidden": false
        },
        {
          "_id": "67e230fd4b9f234b60d0638e",
          "user": {
            "_id": "604bc69e0fe8ff3ec13d71cd",
            "avatarUrl": "/avatars/fe4b14b24befdbed02eecb43a25c67f4.svg",
            "isPro": false,
            "fullname": "Yonatan Geifman",
            "user": "geifmany",
            "type": "user"
          },
          "name": "Yonatan Geifman",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:31:30.392Z",
          "hidden": false
        },
        {
          "_id": "67e230fd4b9f234b60d0638f",
          "name": "Izhak Golan",
          "hidden": false
        },
        {
          "_id": "67e230fd4b9f234b60d06390",
          "name": "Ehud Karpas",
          "hidden": false
        },
        {
          "_id": "67e230fd4b9f234b60d06391",
          "user": {
            "_id": "668578fdd24e614fec97eac8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/668578fdd24e614fec97eac8/n5xYnqo5nQbVX2tgaRfEi.jpeg",
            "isPro": false,
            "fullname": "Itay Levy",
            "user": "itlevy",
            "type": "user"
          },
          "name": "Itay Levy",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:31:50.928Z",
          "hidden": false
        },
        {
          "_id": "67e230fd4b9f234b60d06392",
          "user": {
            "_id": "61ee58f1af500c0acfc4d8eb",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1643010228464-noauth.png",
            "isPro": false,
            "fullname": "Zach Moshe",
            "user": "zachmoshe",
            "type": "user"
          },
          "name": "Zach Moshe",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:31:56.910Z",
          "hidden": false
        },
        {
          "_id": "67e230fd4b9f234b60d06393",
          "user": {
            "_id": "63a16d5d5d09b819fee9a350",
            "avatarUrl": "/avatars/d1a3fef0131688e92e272cbd80856fc3.svg",
            "isPro": false,
            "fullname": "Najeeb Nabwani",
            "user": "NajeebDeci",
            "type": "user"
          },
          "name": "Najeeb Nabwani",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:32:02.613Z",
          "hidden": false
        },
        {
          "_id": "67e230fd4b9f234b60d06394",
          "user": {
            "_id": "6671634f1820f293a9995b12",
            "avatarUrl": "/avatars/50c8f7b4bfb00f2169b808f3c72c7686.svg",
            "isPro": false,
            "fullname": "Tomer Ronen",
            "user": "tomer-nv",
            "type": "user"
          },
          "name": "Tomer Ronen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:32:10.072Z",
          "hidden": false
        },
        {
          "_id": "67e230fd4b9f234b60d06395",
          "user": {
            "_id": "665f0a46f065b1d42806000d",
            "avatarUrl": "/avatars/927f042a3c95c5846621e2a381c66bbf.svg",
            "isPro": false,
            "fullname": "Itamar Schen",
            "user": "ischen-nvidia",
            "type": "user"
          },
          "name": "Itamar Schen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:32:17.605Z",
          "hidden": false
        },
        {
          "_id": "67e230fd4b9f234b60d06396",
          "user": {
            "_id": "5f5b0efe10b2753d9000c888",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1628140531144-5f5b0efe10b2753d9000c888.jpeg",
            "isPro": false,
            "fullname": "Elad Segal",
            "user": "eladsegal",
            "type": "user"
          },
          "name": "Elad Segal",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:32:24.511Z",
          "hidden": false
        },
        {
          "_id": "67e230fd4b9f234b60d06397",
          "user": {
            "_id": "666ef13c14f1c262feeb706c",
            "avatarUrl": "/avatars/7dca59acf5e069d96bdbb98dace9199b.svg",
            "isPro": false,
            "fullname": "Ido Shahaf",
            "user": "ishahaf",
            "type": "user"
          },
          "name": "Ido Shahaf",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:32:30.927Z",
          "hidden": false
        },
        {
          "_id": "67e230fd4b9f234b60d06398",
          "user": {
            "_id": "66b089f14ae4ae811218cdb6",
            "avatarUrl": "/avatars/a50fe725922dfdbe0e731fade381b22e.svg",
            "isPro": false,
            "fullname": "Oren Tropp",
            "user": "otropp",
            "type": "user"
          },
          "name": "Oren Tropp",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:32:38.161Z",
          "hidden": false
        },
        {
          "_id": "67e230fd4b9f234b60d06399",
          "user": {
            "_id": "666027917c3f9c72113cc75c",
            "avatarUrl": "/avatars/a276ebe8e2731b6a05e3c61c2ae0ddae.svg",
            "isPro": false,
            "fullname": "Ran Zilberstein",
            "user": "RanZilberstein-Nvidia",
            "type": "user"
          },
          "name": "Ran Zilberstein",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:32:44.768Z",
          "hidden": false
        },
        {
          "_id": "67e230fd4b9f234b60d0639a",
          "user": {
            "_id": "65758349983403462a54ac06",
            "avatarUrl": "/avatars/4f337c732f31bd748738c2717b50a99c.svg",
            "isPro": false,
            "fullname": "Ran El-Yaniv",
            "user": "ranielyaniv",
            "type": "user"
          },
          "name": "Ran El-Yaniv",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:32:56.726Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-24T17:20:35.000Z",
      "submittedOnDailyAt": "2025-03-25T02:59:12.174Z",
      "title": "FFN Fusion : Récupération des calculs séquentiels dans l'entraînement de grands modèles de langage",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "FFN Fusion est un méthode d'optimisation pour réduire les calculs séquentiels de grands modèles de langage. Cette méthode bénéficie naturellement du traitement parallèle. Notre idée principale est que la permutation de la couche de Feed-Forward Network (FFN) peut être supprimée sans affecter significativement la précision, et que cette couche peut être traitée de manière parallèle. Nous avons développé la théorie de base de cette permutation et de sa combinaison, et nous avons transformé ces opérations en calculs parallèles pour réduire considérablement le temps d'inférence et maintenir le comportement du modèle. Nous avons appliqué ces techniques sur Llama-3.1-405B-Instruct pour créer Llama-Nemotron-Ultra-253B-Base (Ultra-253B-Base). Ce modèle est efficace, sera publié dans le futur, et a démontré un temps d'inférence 1,71 fois plus rapide, un coût par carte 35 fois moins élevé et une excellente performance sur les benchmarks. Nous avons effectué des expériences de dispersion pour des modèles avec entre 49B et 253B de paramètres, et nous avons démontré que FFN Fusion fonctionne efficacement à grande échelle et peut compléter d'autres méthodes d'optimisation (par exemple, la quantification et la réduction). Une observation intéressante est que des blocs complets de transformer qui incluent des couches d'attention et d'FFN peuvent souvent être traités de manière parallèle, offrant une nouvelle direction dans le design d'architectures neuronales.",
      "upvotes": 5,
      "discussionId": "67e230fe4b9f234b60d063ec",
      "ai_keywords": [
        "FFN Fusion",
        "Feed-Forward Network (FFN)",
        "parallelization",
        "inference latency",
        "Llama-3.1-405B-Instruct",
        "Ultra-253B-Base",
        "model behavior",
        "per-token cost",
        "benchmarks",
        "transformer blocks",
        "quantization",
        "pruning"
      ]
    },
    "publishedAt": "2025-03-24T13:20:35.000Z",
    "title": "FFN Fusion: Rethinking Sequential Computation in Large Language Models",
    "summary": "We introduce FFN Fusion, an architectural optimization technique that reduces\nsequential computation in large language models by identifying and exploiting\nnatural opportunities for parallelization. Our key insight is that sequences of\nFeed-Forward Network (FFN) layers, particularly those remaining after the\nremoval of specific attention layers, can often be parallelized with minimal\naccuracy impact. We develop a principled methodology for identifying and fusing\nsuch sequences, transforming them into parallel operations that significantly\nreduce inference latency while preserving model behavior. Applying these\ntechniques to Llama-3.1-405B-Instruct, we create Llama-Nemotron-Ultra-253B-Base\n(Ultra-253B-Base), an efficient and soon-to-be publicly available model that\nachieves a 1.71X speedup in inference latency and 35X lower per-token cost\nwhile maintaining strong performance across benchmarks. Through extensive\nexperiments on models from 49B to 253B parameters, we demonstrate that FFN\nFusion becomes increasingly effective at larger scales and can complement\nexisting optimization techniques like quantization and pruning. Most\nintriguingly, we find that even full transformer blocks containing both\nattention and FFN layers can sometimes be parallelized, suggesting new\ndirections for neural architecture design.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18908.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6456
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.18102",
      "authors": [
        {
          "_id": "67e22206ddc9b120cbde6fbe",
          "name": "Samuel Schmidgall",
          "hidden": false
        },
        {
          "_id": "67e22206ddc9b120cbde6fbf",
          "user": {
            "_id": "6438d1d843d932c462404500",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6438d1d843d932c462404500/6UrtJbed9N5ETbImgIh-C.png",
            "isPro": false,
            "fullname": "Michael Moor",
            "user": "mdmoor",
            "type": "user"
          },
          "name": "Michael Moor",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:34:00.877Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-23T15:16:42.000Z",
      "submittedOnDailyAt": "2025-03-25T05:09:36.523Z",
      "title": "AgentRxiv: Anomalia pour l'Automatisation de l'Étude du Modèle de Collaboration",
      "submittedOnDailyBy": {
        "_id": "6438d1d843d932c462404500",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6438d1d843d932c462404500/6UrtJbed9N5ETbImgIh-C.png",
        "isPro": false,
        "fullname": "Michael Moor",
        "user": "mdmoor",
        "type": "user"
      },
      "summary": "L'évolution des découvertes scientifiques ne se réduit pas à un moment instantané de \"Eureka\" mais à un résultat de la collaboration structurée de centaines de scientifiques travaillant vers un objectif commun. Le flux actuel de travail des agents peut effectuer des recherches automatiquement, mais il est isolé et n'a pas la capacité de continuer à améliorer les résultats de recherche antérieurs. Pour aborder ces problèmes, nous présentons le cadre AgentRxiv, qui permet aux laboratoires d'agents de télécharger et télécharger des rapports d'un serveur commun, facilitant la collaboration, le partage de connaissances et la construction de recherches mutuellement. Les laboratoires d'agents ont développé de nouvelles techniques de raisonnement et de prompting, atteignant un accroissement de 11,4% en rendement par rapport aux agents isolés. La stratégie qui a le mieux fonctionné peut également être généralisée à d'autres domaines, avec un accroissement moyen de 3,3%. Grâce à AgentRxiv, les laboratoires d'agents qui partagent des recherches peuvent collaborer pour atteindre un objectif commun, avancer plus rapidement que ceux isolés, et atteindre une précision générale de 13,7% plus élevée. Ces résultats montrent que les agents autonomes peuvent contribuer au design de systèmes d'IA de manière humaine. Nous espérons que AgentRxiv permette aux agents de collaborer vers l'objectif de la recherche et d'accélérer les découvertes des chercheurs.",
      "upvotes": 5,
      "discussionId": "67e22207ddc9b120cbde702c",
      "ai_keywords": [
        "LLM (Large Language Model)",
        "agent laboratories",
        "preprint server",
        "reasoning techniques",
        "prompting techniques",
        "performance improvements",
        "benchmarks",
        "accuracy"
      ]
    },
    "publishedAt": "2025-03-23T11:16:42.000Z",
    "title": "AgentRxiv: Towards Collaborative Autonomous Research",
    "summary": "Progress in scientific discovery is rarely the result of a single \"Eureka\"\nmoment, but is rather the product of hundreds of scientists incrementally\nworking together toward a common goal. While existing agent workflows are\ncapable of producing research autonomously, they do so in isolation, without\nthe ability to continuously improve upon prior research results. To address\nthese challenges, we introduce AgentRxiv-a framework that lets LLM agent\nlaboratories upload and retrieve reports from a shared preprint server in order\nto collaborate, share insights, and iteratively build on each other's research.\nWe task agent laboratories to develop new reasoning and prompting techniques\nand find that agents with access to their prior research achieve higher\nperformance improvements compared to agents operating in isolation (11.4%\nrelative improvement over baseline on MATH-500). We find that the best\nperforming strategy generalizes to benchmarks in other domains (improving on\naverage by 3.3%). Multiple agent laboratories sharing research through\nAgentRxiv are able to work together towards a common goal, progressing more\nrapidly than isolated laboratories, achieving higher overall accuracy (13.7%\nrelative improvement over baseline on MATH-500). These findings suggest that\nautonomous agents may play a role in designing future AI systems alongside\nhumans. We hope that AgentRxiv allows agents to collaborate toward research\ngoals and enables researchers to accelerate discovery.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18102.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6438d1d843d932c462404500",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6438d1d843d932c462404500/6UrtJbed9N5ETbImgIh-C.png",
      "fullname": "Michael Moor",
      "name": "mdmoor",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.15879",
      "authors": [
        {
          "_id": "67dea7cc5b44ace7a30e237e",
          "user": {
            "_id": "6540fbf9cb7fffd683942b43",
            "avatarUrl": "/avatars/d4a64fbde511d0949e1c339179586850.svg",
            "isPro": false,
            "fullname": "DongGeon Lee",
            "user": "oneonlee",
            "type": "user"
          },
          "name": "DongGeon Lee",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-24T08:52:18.850Z",
          "hidden": false
        },
        {
          "_id": "67dea7cc5b44ace7a30e237f",
          "name": "Ahjeong Park",
          "hidden": false
        },
        {
          "_id": "67dea7cc5b44ace7a30e2380",
          "user": {
            "_id": "666a8be869a08ea4aac5e73e",
            "avatarUrl": "/avatars/be42632414bafb0af74b5f4d4f03d223.svg",
            "isPro": false,
            "fullname": "keira lee",
            "user": "keirahrlee",
            "type": "user"
          },
          "name": "Hyeri Lee",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-22T12:06:36.938Z",
          "hidden": false
        },
        {
          "_id": "67dea7cc5b44ace7a30e2381",
          "name": "Hyeonseo Nam",
          "hidden": false
        },
        {
          "_id": "67dea7cc5b44ace7a30e2382",
          "name": "Yunho Maeng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-20T06:04:12.000Z",
      "submittedOnDailyAt": "2025-03-25T03:48:51.972Z",
      "title": "Typed-RAG : Méthode pour obtenir des réponses à des questions non factorielles par la décomposition multidimensionnelle des types",
      "submittedOnDailyBy": {
        "_id": "6540fbf9cb7fffd683942b43",
        "avatarUrl": "/avatars/d4a64fbde511d0949e1c339179586850.svg",
        "isPro": false,
        "fullname": "DongGeon Lee",
        "user": "oneonlee",
        "type": "user"
      },
      "summary": "La réponse non-factoid (NFQA) a des problèmes graves en raison de sa nature ouverte, de la diversité des intentions et de la nécessité multidimensionnelle. Les méthodes basées sur des approches simples de réponse non-factoid (en particulier, la génération d'arguments de révision (RAG)) ne sont pas suffisantes pour aborder ces défis. Comparées aux problèmes factuels, les problèmes non-factoid (NFQ) n'ont pas de réponses décisives, ce qui nécessite l'intégration d'informations de multiples sources et la recherche de raisons à différents niveaux. Pour résoudre ces limitations, nous présentons un cadre de travail de multidimensionnalité \"Typed-RAG\" dans le paradigme de RAG. Typed-RAG classe les NFQ en types spécifiques comme le débat, l'expérience, la comparaison, et applique un approche de résolution basée sur des aspects pour raffiner la stratégie de révision et de génération. En décomposant les NFQ multidimensionnelles en sous-consultations d'un seul aspect et en sommant les résultats, Typed-RAG génère des réponses informatives et contextuelles. Dans l'évaluation de Typed-RAG, nous présentons un ensemble de données de référence \"Wiki-NFQA\" qui couvre divers types de NFQ. Les résultats des expérimentations montrent que Typed-RAG dépasse les normes et démontre l'importance de la décomposition par types dans la révision et la génération de NFQ. Notre code et ensemble de données sont disponibles sur la suivante URL : https://github.com/TeamNLP/Typed-RAG.",
      "upvotes": 5,
      "discussionId": "67dea7cc5b44ace7a30e23b8",
      "githubRepo": "https://github.com/TeamNLP/Typed-RAG",
      "ai_keywords": [
        "retrieval-augmented generation (RAG)",
        "non-factoid question-answering (NFQA)",
        "multi-aspect reasoning",
        "type-aware multi-aspect decomposition framework",
        "single-aspect sub-queries",
        "Wiki-NFQA",
        "type-aware decomposition"
      ]
    },
    "publishedAt": "2025-03-20T02:04:12.000Z",
    "title": "Typed-RAG: Type-aware Multi-Aspect Decomposition for Non-Factoid\n  Question Answering",
    "summary": "Non-factoid question-answering (NFQA) poses a significant challenge due to\nits open-ended nature, diverse intents, and the need for multi-aspect\nreasoning, which renders conventional factoid QA approaches, including\nretrieval-augmented generation (RAG), inadequate. Unlike factoid questions,\nnon-factoid questions (NFQs) lack definitive answers and require synthesizing\ninformation from multiple sources across various reasoning dimensions. To\naddress these limitations, we introduce Typed-RAG, a type-aware multi-aspect\ndecomposition framework within the RAG paradigm for NFQA. Typed-RAG classifies\nNFQs into distinct types -- such as debate, experience, and comparison -- and\napplies aspect-based decomposition to refine retrieval and generation\nstrategies. By decomposing multi-aspect NFQs into single-aspect sub-queries and\naggregating the results, Typed-RAG generates more informative and contextually\nrelevant responses. To evaluate Typed-RAG, we introduce Wiki-NFQA, a benchmark\ndataset covering diverse NFQ types. Experimental results demonstrate that\nTyped-RAG outperforms baselines, thereby highlighting the importance of\ntype-aware decomposition for effective retrieval and generation in NFQA. Our\ncode and dataset are available at\nhttps://github.com/TeamNLP/Typed-RAG{https://github.com/TeamNLP/Typed-RAG}.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.15879.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6540fbf9cb7fffd683942b43",
      "avatarUrl": "/avatars/d4a64fbde511d0949e1c339179586850.svg",
      "fullname": "DongGeon Lee",
      "name": "oneonlee",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.18866",
      "authors": [
        {
          "_id": "67e2290da4525cbb1d718ae2",
          "user": {
            "_id": "65619949d2e4352d64365606",
            "avatarUrl": "/avatars/4241b6e901e146a9ff8b5bc7de3f960a.svg",
            "isPro": true,
            "fullname": "Yangjun Ruan",
            "user": "ryoungj",
            "type": "user"
          },
          "name": "Yangjun Ruan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:33:10.423Z",
          "hidden": false
        },
        {
          "_id": "67e2290da4525cbb1d718ae3",
          "user": {
            "_id": "630bc38809eceb8fafe5ed7f",
            "avatarUrl": "/avatars/5f2a1268f8a7b51cca8446ef0be6445f.svg",
            "isPro": true,
            "fullname": "Neil Band",
            "user": "nband",
            "type": "user"
          },
          "name": "Neil Band",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:33:16.895Z",
          "hidden": false
        },
        {
          "_id": "67e2290da4525cbb1d718ae4",
          "user": {
            "_id": "66a7f54fbb22d7e78a2aeaf4",
            "avatarUrl": "/avatars/3ab8899935f8f7b14e89c623cc6c0fd2.svg",
            "isPro": false,
            "fullname": "Chris J. Maddison",
            "user": "cmaddis",
            "type": "user"
          },
          "name": "Chris J. Maddison",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:33:22.633Z",
          "hidden": false
        },
        {
          "_id": "67e2290da4525cbb1d718ae5",
          "name": "Tatsunori Hashimoto",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-24T16:41:23.000Z",
      "submittedOnDailyAt": "2025-03-25T02:25:31.127Z",
      "title": "Les motifs pour apprendre à Rotunda Takeda",
      "submittedOnDailyBy": {
        "_id": "65619949d2e4352d64365606",
        "avatarUrl": "/avatars/4241b6e901e146a9ff8b5bc7de3f960a.svg",
        "isPro": true,
        "fullname": "Yangjun Ruan",
        "user": "ryoungj",
        "type": "user"
      },
      "summary": "L'accroissement rapide de l'entraînement préalable d'un Modèle de Langue (ML) est accompagné par la quantité de phrases écrites par des humains, et il est préoccupé que les données soient le limite de cet accroissement. Dans ces conditions de limitation des données, nous proposons que clarifier et estimer les pensées potentielles lors du processus de génération de phrases puisse avoir un effet considérable sur l'efficacité de l'entraînement préalable. Intuitivement, notre approche ne voit pas les textes web comme des résultats finaux de pensées humaines abrégées, mais que les pensées potentielles incluent des connaissances contextuelles et des étapes de rationalisation cruciales pour l'apprentissage efficace des données. Nous démontrons l'efficacité de notre approche dans un entraînement préalable avec des restrictions de données. Premièrement, l'approche des données synthétiques pour estimer les pensées potentielles améliore significativement l'efficacité de l'entraînement préalable et permet d'apprendre avec la même quantité de données (de 5,7% à 25,4% en MATH). De plus, il est montré que les pensées potentielles peuvent être estimées même en l'absence de textures fortes. Le ML est entraîné de manière continue avec l'algorithme EM pour améliorer la qualité de l'entraînement préalable, y compris la combinaison de la capacité du ML et des pensées potentielles. Un ML de 1B peut augmenter son rendement d'au moins 3 itérations, dépassant considérablement les normes avec seulement des données, et le calcul supplémentaire des estimations lors de la phase E peut augmenter son effet. Les effets de l'accroissement des estimations et des itérations de EM offrent de nouvelles opportunités pour l'accroissement de l'entraînement préalable sous des restrictions de données.",
      "upvotes": 4,
      "discussionId": "67e2290ea4525cbb1d718b18",
      "ai_keywords": [
        "latent thoughts",
        "data-efficient learning",
        "web text",
        "verbose human thought process",
        "synthetic data",
        "data-constrained regime",
        "EM algorithm",
        "thought-augmented pretraining data",
        "inference compute",
        "data-constrained pretraining"
      ]
    },
    "publishedAt": "2025-03-24T12:41:23.000Z",
    "title": "Reasoning to Learn from Latent Thoughts",
    "summary": "Compute scaling for language model (LM) pretraining has outpaced the growth\nof human-written texts, leading to concerns that data will become the\nbottleneck to LM scaling. To continue scaling pretraining in this\ndata-constrained regime, we propose that explicitly modeling and inferring the\nlatent thoughts that underlie the text generation process can significantly\nimprove pretraining data efficiency. Intuitively, our approach views web text\nas the compressed final outcome of a verbose human thought process and that the\nlatent thoughts contain important contextual knowledge and reasoning steps that\nare critical to data-efficient learning. We empirically demonstrate the\neffectiveness of our approach through data-constrained continued pretraining\nfor math. We first show that synthetic data approaches to inferring latent\nthoughts significantly improve data efficiency, outperforming training on the\nsame amount of raw data (5.7\\% rightarrow 25.4\\% on MATH). Furthermore, we\ndemonstrate latent thought inference without a strong teacher, where an LM\nbootstraps its own performance by using an EM algorithm to iteratively improve\nthe capability of the trained LM and the quality of thought-augmented\npretraining data. We show that a 1B LM can bootstrap its performance across at\nleast three iterations and significantly outperform baselines trained on raw\ndata, with increasing gains from additional inference compute when performing\nthe E-step. The gains from inference scaling and EM iterations suggest new\nopportunities for scaling data-constrained pretraining.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18866.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65619949d2e4352d64365606",
      "avatarUrl": "/avatars/4241b6e901e146a9ff8b5bc7de3f960a.svg",
      "fullname": "Yangjun Ruan",
      "name": "ryoungj",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.18013",
      "authors": [
        {
          "_id": "67e22902af6628c90b525a2b",
          "name": "Yufei Zhan",
          "hidden": false
        },
        {
          "_id": "67e22902af6628c90b525a2c",
          "name": "Yousong Zhu",
          "hidden": false
        },
        {
          "_id": "67e22902af6628c90b525a2d",
          "name": "Shurong Zheng",
          "hidden": false
        },
        {
          "_id": "67e22902af6628c90b525a2e",
          "name": "Hongyin Zhao",
          "hidden": false
        },
        {
          "_id": "67e22902af6628c90b525a2f",
          "name": "Fan Yang",
          "hidden": false
        },
        {
          "_id": "67e22902af6628c90b525a30",
          "name": "Ming Tang",
          "hidden": false
        },
        {
          "_id": "67e22902af6628c90b525a31",
          "name": "Jinqiao Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-23T10:21:14.000Z",
      "submittedOnDailyAt": "2025-03-25T02:25:32.811Z",
      "title": "Vision-R1 : Le développement d'un langage de vision modèle sans humains se base sur une directive de vision pour le renforcement d'entraînement.",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Les modèles de langue visuo-linguistique (LVLMs) utilisent généralement un schéma d'apprentissage à deux étapes : l'apprentissage préalable et l'ajustement de normalisation de contrôle. Récemment, l'optimisation des préférences inspirée du domaine du langage a émergé comme une stratégie efficace d'apprentissage supplémentaire pour améliorer les capacités des LVLMs. Cependant, la construction de données de préférences annotées de haute qualité et le développement de modèles de récompense puissants qui modélisent ces préférences impliquent à la fois des coûts et des difficultés. Sur la base de cette observation, nous proposons Vision-R1. Vision-R1 est un algorithme d'apprentissage par renforcement similaire à R1, qui fournit une récompense aux modèles qui reçoivent une rétroaction visuelle claire. Cela élimine la nécessité de modèles de récompense professionnels et de jeux de données de préférences directement annotés. De plus, nous utilisons une fonction de récompense guidée par des règles qui intègrent une rétroaction multidimensionnelle pour évaluer des évaluations basées sur la logique de tâches visuelles. Nous introduisons également une stratégie d'apprentissage évolutif et ajustons dynamiquement les critères d'évaluation de récompense pendant l'apprentissage pour atteindre un amélioration continue du modèle et la prévention du hacking de la récompense. Des expériences larges dans des cadres de référence à l'intérieur et à l'extérieur de la distribution montrent que l'ajustement de Vision-R1 dans un LVLM de 7B a atteint un améliorament uniforme, avec un accroissement de 50% et une supériorité sur les modèles de la taille avancée de 10 fois.",
      "upvotes": 4,
      "discussionId": "67e22903af6628c90b525a71",
      "ai_keywords": [
        "Large Vision-Language Models (LVLMs)",
        "pretraining",
        "supervised fine-tuning",
        "preference optimization",
        "reinforcement learning",
        "Vision-R1",
        "criterion-driven reward function",
        "progressive rule refinement strategy",
        "reward criteria",
        "model completions",
        "vision task logic",
        "reward hacking",
        "state-of-the-art"
      ]
    },
    "publishedAt": "2025-03-23T06:21:14.000Z",
    "title": "Vision-R1: Evolving Human-Free Alignment in Large Vision-Language Models\n  via Vision-Guided Reinforcement Learning",
    "summary": "Large Vision-Language Models (LVLMs) typically follow a two-stage training\nparadigm-pretraining and supervised fine-tuning. Recently, preference\noptimization, derived from the language domain, has emerged as an effective\npost-training reinforcement strategy to enhance capabilities of LVLMs. However,\nconstructing high-quality human-annotated preference data and developing robust\nreward models to mimic these preferences are both costly and challenging.\nMotivated by this observation, we propose Vision-R1, a novel vision-guided\nR1-like reinforcement learning algorithm for LVLMs that rewards models with\ndefinitive vision feedback. It only leverages curated instruction data,\neliminating the need for specialized reward models and handcrafted preference\ndatasets. We incorporate a criterion-driven reward function that further\nintegrates multi-dimensional feedback to evaluate model completions\ncomprehensively based on the vision task logic. Furthermore, we introduce a\nprogressive rule refinement strategy that dynamically adjusts the reward\ncriteria during training, enabling continuous model improvement and mitigating\nreward hacking. Extensive experiments on both in-distribution and\nout-of-distribution benchmarks demonstrate that fine-tuning the 7B LVLMs with\nVision-R1 achieves consistent performance gains, with even up to 50%\nimprovement and surpassing the state-of-the-art 10x size model.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18013.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6456
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.17422",
      "authors": [
        {
          "_id": "67e226b3b1acaf8a7680e926",
          "name": "Javier J. Poveda Rodrigo",
          "hidden": false
        },
        {
          "_id": "67e226b3b1acaf8a7680e927",
          "name": "Mohamed Amine Ahmdi",
          "hidden": false
        },
        {
          "_id": "67e226b3b1acaf8a7680e928",
          "name": "Alessio Burrello",
          "hidden": false
        },
        {
          "_id": "67e226b3b1acaf8a7680e929",
          "name": "Daniele Jahier Pagliari",
          "hidden": false
        },
        {
          "_id": "67e226b3b1acaf8a7680e92a",
          "name": "Luca Benini",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-21T09:00:19.000Z",
      "submittedOnDailyAt": "2025-03-25T02:15:09.508Z",
      "title": "V-Seek : Plateforme pour accélérer la logique des LLM sur une plateforme RISC-V d'ordinateurs de serveurs ouverts",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Récemment, le croissance exponentielle des Modèles de Langue Grande (LLMs) a dépendé des systèmes basés sur des GPU, mais les CPU sont en train d'être largement étendues en tant qu'option flexible et à faible coût pour des tâches d'inférence et de logique. RISC-V est en expansion rapide dans ce domaine. RISC-V dispose d'une architecture d'instructions ouverte et non dépendante de versions commerciales, ce qui signifie que l'écosystème de hardware et de logiciel RISC-V adapté pour des tâches de LLMs n'est pas complètement mature en raison de l'exigence de corrections spécifiques. Cet article vise à combler cette lacune. En particulier, il se concentre sur l'optimisation de l'inférence de LLMs sur la première CPU multi-coûté RISC-V vendue, Sophon SG2042.\n\nDans les plus avancés des LLMs, on a atteint une vitesse de 4,32/2,29 tokens/seconde pour la génération de tokens et de 6,54/3,68 tokens/seconde pour le traitement des prompts, ce qui représente un accroissement de 2,9/3,0 fois par rapport aux normes standards.",
      "upvotes": 3,
      "discussionId": "67e226b3b1acaf8a7680e96b",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "GPU-based systems",
        "CPUs",
        "RISC-V",
        "ISA",
        "RISC-V hardware",
        "software ecosystem",
        "domain-specific tuning",
        "Sophon SG2042",
        "many-core RISC-V CPU",
        "vector processing capabilities",
        "DeepSeek R1 Distill Llama 8B",
        "DeepSeek R1 Distill QWEN 14B",
        "token generation",
        "prompt processing"
      ]
    },
    "publishedAt": "2025-03-21T05:00:19.000Z",
    "title": "V-Seek: Accelerating LLM Reasoning on Open-hardware Server-class RISC-V\n  Platforms",
    "summary": "The recent exponential growth of Large Language Models (LLMs) has relied on\nGPU-based systems. However, CPUs are emerging as a flexible and lower-cost\nalternative, especially when targeting inference and reasoning workloads.\nRISC-V is rapidly gaining traction in this area, given its open and\nvendor-neutral ISA. However, the RISC-V hardware for LLM workloads and the\ncorresponding software ecosystem are not fully mature and streamlined, given\nthe requirement of domain-specific tuning. This paper aims at filling this gap,\nfocusing on optimizing LLM inference on the Sophon SG2042, the first\ncommercially available many-core RISC-V CPU with vector processing\ncapabilities.\n  On two recent state-of-the-art LLMs optimized for reasoning, DeepSeek R1\nDistill Llama 8B and DeepSeek R1 Distill QWEN 14B, we achieve 4.32/2.29 token/s\nfor token generation and 6.54/3.68 token/s for prompt processing, with a speed\nup of up 2.9x/3.0x compared to our baseline.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17422.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6456
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.18813",
      "authors": [
        {
          "_id": "67e24a997210beea5ecae330",
          "user": {
            "_id": "631dd96f6d6a5870f3d42528",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/631dd96f6d6a5870f3d42528/YMsxboRfIBBGE-z-_DeNx.jpeg",
            "isPro": false,
            "fullname": "Edoardo Debenedetti",
            "user": "dedeswim",
            "type": "user"
          },
          "name": "Edoardo Debenedetti",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:26:38.111Z",
          "hidden": false
        },
        {
          "_id": "67e24a997210beea5ecae331",
          "user": {
            "_id": "6475c2794766357252e69e9f",
            "avatarUrl": "/avatars/db428715dfd2239df2aeaaff1282323f.svg",
            "isPro": false,
            "fullname": "i",
            "user": "iliashum",
            "type": "user"
          },
          "name": "Ilia Shumailov",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:27:10.362Z",
          "hidden": false
        },
        {
          "_id": "67e24a997210beea5ecae332",
          "name": "Tianqi Fan",
          "hidden": false
        },
        {
          "_id": "67e24a997210beea5ecae333",
          "name": "Jamie Hayes",
          "hidden": false
        },
        {
          "_id": "67e24a997210beea5ecae334",
          "user": {
            "_id": "6303fa9ba362e7e8b51d8f2a",
            "avatarUrl": "/avatars/53e53c84f987989deb351dd2ae6ee558.svg",
            "isPro": false,
            "fullname": "Nicholas Carlini",
            "user": "carlini",
            "type": "user"
          },
          "name": "Nicholas Carlini",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:27:26.752Z",
          "hidden": false
        },
        {
          "_id": "67e24a997210beea5ecae335",
          "name": "Daniel Fabian",
          "hidden": false
        },
        {
          "_id": "67e24a997210beea5ecae336",
          "name": "Christoph Kern",
          "hidden": false
        },
        {
          "_id": "67e24a997210beea5ecae337",
          "name": "Chongyang Shi",
          "hidden": false
        },
        {
          "_id": "67e24a997210beea5ecae338",
          "name": "Andreas Terzis",
          "hidden": false
        },
        {
          "_id": "67e24a997210beea5ecae339",
          "user": {
            "_id": "63568f18ba90b4ea9fe91cb5",
            "avatarUrl": "/avatars/3e8b3c573e20cf80d329a312bfc34728.svg",
            "isPro": false,
            "fullname": "Florian Tramer",
            "user": "ftramer",
            "type": "user"
          },
          "name": "Florian Tramèr",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:27:51.364Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-24T15:54:10.000Z",
      "submittedOnDailyAt": "2025-03-25T04:48:52.478Z",
      "title": "Design pour la récupération de l'Injection de Prompts",
      "submittedOnDailyBy": {
        "_id": "6475c2794766357252e69e9f",
        "avatarUrl": "/avatars/db428715dfd2239df2aeaaff1282323f.svg",
        "isPro": false,
        "fullname": "i",
        "user": "iliashum",
        "type": "user"
      },
      "summary": "Les modèles de langage grands (LLMs) sont en augmentation de leur utilisation dans les systèmes d'agents qui interagissent avec l'environnement extérieur. Cependant, les agents basés sur des LLMs sont vulnérables aux attaques de mise en œuvre de prompt lorsqu'ils traitent des données non sécurisées. Dans cet article, nous proposons une stratégie de défense robuste appelée CaMeL. Cette méthodologie crée des couches de protection autour du modèle LLM, assurant sa sécurité même si le modèle est vulnérable aux attaques. CaMeL clairement extrait le flux de contrôle et le flux de données traitées, garantissant que les données non sécurisées ne touchent pas le flux d'exécution du modèle. De plus, pour prévenir la sortie de données confidentielles, CaMeL utilise le concept de couche de protection. Nous avons démontré l'efficacité de CaMeL dans le cadre d'un récent benchmark de sécurité d'agents, AgentDojo [NeurIPS 2024], où nous avons réussi à résoudre avec sécurité et efficacité 67% des tâches.",
      "upvotes": 2,
      "discussionId": "67e24a9b7210beea5ecae3a0",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "prompt injection attacks",
        "protective system layer",
        "trusted query",
        "untrusted data",
        "program flow",
        "capability",
        "private data exfiltration",
        "unauthorized data flows",
        "AgentDojo"
      ]
    },
    "publishedAt": "2025-03-24T11:54:10.000Z",
    "title": "Defeating Prompt Injections by Design",
    "summary": "Large Language Models (LLMs) are increasingly deployed in agentic systems\nthat interact with an external environment. However, LLM agents are vulnerable\nto prompt injection attacks when handling untrusted data. In this paper we\npropose CaMeL, a robust defense that creates a protective system layer around\nthe LLM, securing it even when underlying models may be susceptible to attacks.\nTo operate, CaMeL explicitly extracts the control and data flows from the\n(trusted) query; therefore, the untrusted data retrieved by the LLM can never\nimpact the program flow. To further improve security, CaMeL relies on a notion\nof a capability to prevent the exfiltration of private data over unauthorized\ndata flows. We demonstrate effectiveness of CaMeL by solving 67% of tasks\nwith provable security in AgentDojo [NeurIPS 2024], a recent agentic security\nbenchmark.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18813.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6475c2794766357252e69e9f",
      "avatarUrl": "/avatars/db428715dfd2239df2aeaaff1282323f.svg",
      "fullname": "i",
      "name": "iliashum",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.18769",
      "authors": [
        {
          "_id": "67e2177c77d32fd1ed8a496b",
          "user": {
            "_id": "62d7b2339b629105a5d6888a",
            "avatarUrl": "/avatars/c3f164fde6b8f9a671890e08ce8a3e75.svg",
            "isPro": false,
            "fullname": "Alan Dao",
            "user": "alandao",
            "type": "user"
          },
          "name": "Alan Dao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-25T08:20:35.578Z",
          "hidden": false
        },
        {
          "_id": "67e2177c77d32fd1ed8a496c",
          "name": "Dinh Bach Vu",
          "hidden": false
        },
        {
          "_id": "67e2177c77d32fd1ed8a496d",
          "name": "Bui Quang Huy",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/62d7b2339b629105a5d6888a/505sM0N1UIUHUOunaHtAG.mp4"
      ],
      "publishedAt": "2025-03-24T15:16:51.000Z",
      "submittedOnDailyAt": "2025-03-25T01:11:03.544Z",
      "title": "AlphaSpace : Sémantique de Tokenisation et Inférence Significative pour l'Optimisation des Actions de Robotique",
      "submittedOnDailyBy": {
        "_id": "62d7b2339b629105a5d6888a",
        "avatarUrl": "/avatars/c3f164fde6b8f9a671890e08ce8a3e75.svg",
        "isPro": false,
        "fullname": "Alan Dao",
        "user": "alandao",
        "type": "user"
      },
      "summary": "Dans cet article, on présente un nouveau méthode appelé AlphaSpace. AlphaSpace a été conçu pour améliorer la capacité cognitive spatiale des grands modèles de langue (LLMs) dans l'espace cartésien tridimensionnel. Il utilise une stratégie de tokenisation basée sur la sémantique, codant des informations de haute valeur pour des tokens sémantiques spécifiques et intégrant principalement des données cognitives complexes et iconiques. De cette manière, les LLMs peuvent localiser et manipuler des objets avec précision en coordonnées [x, y, z]. Les résultats des expériences montrent que AlphaSpace dépasse considérablement les modèles existants. La précision dans les tâches partielles atteint 66,67% plus que le 37,5% de GPT-4o et 29,17% plus que celle de Claude 3.5 Sonnet.",
      "upvotes": 2,
      "discussionId": "67e2177d77d32fd1ed8a49ac",
      "ai_keywords": [
        "semantics-based tokenization",
        "semantic tokens",
        "Cartesian space",
        "3D Cartesian space",
        "positioning",
        "manipulation subtasks"
      ]
    },
    "publishedAt": "2025-03-24T11:16:51.000Z",
    "title": "AlphaSpace: Enabling Robotic Actions through Semantic Tokenization and\n  Symbolic Reasoning",
    "summary": "This paper presents AlphaSpace, a novel methodology designed to enhance the\nspatial reasoning capabilities of large language models (LLMs) for 3D Cartesian\nspace navigation. AlphaSpace employs a semantics-based tokenization strategy,\nencoding height information through specialized semantic tokens, and integrates\nprimarily symbolic synthetic reasoning data. This approach enables LLMs to\naccurately manipulate objects by positioning them at specific [x, y, z]\ncoordinates. Experimental results demonstrate that AlphaSpace significantly\noutperforms existing models on manipulation subtasks, achieving a total\naccuracy of 66.67%, compared to 37.5% for GPT-4o and 29.17% for Claude 3.5\nSonnet.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62d7b2339b629105a5d6888a/505sM0N1UIUHUOunaHtAG.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18769.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62d7b2339b629105a5d6888a",
      "avatarUrl": "/avatars/c3f164fde6b8f9a671890e08ce8a3e75.svg",
      "fullname": "Alan Dao",
      "name": "alandao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.18559",
      "authors": [
        {
          "_id": "67e22d5236076dc847989434",
          "name": "Takashi Isobe",
          "hidden": false
        },
        {
          "_id": "67e22d5236076dc847989435",
          "name": "He Cui",
          "hidden": false
        },
        {
          "_id": "67e22d5236076dc847989436",
          "name": "Dong Zhou",
          "hidden": false
        },
        {
          "_id": "67e22d5236076dc847989437",
          "user": {
            "_id": "6463685fd2044cd1d7c74b81",
            "avatarUrl": "/avatars/334637e2d63efb7cc2129fec6ea54725.svg",
            "isPro": false,
            "fullname": "gemengmeng",
            "user": "gemengmeng",
            "type": "user"
          },
          "name": "Mengmeng Ge",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:30:24.973Z",
          "hidden": false
        },
        {
          "_id": "67e22d5236076dc847989438",
          "name": "Dong Li",
          "hidden": false
        },
        {
          "_id": "67e22d5236076dc847989439",
          "user": {
            "_id": "65adc9d086f88a686be41215",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65adc9d086f88a686be41215/xizVHuZPkE0Gu8_ulx0Fm.jpeg",
            "isPro": false,
            "fullname": "Emad Barsoum",
            "user": "ebarsoum",
            "type": "user"
          },
          "name": "Emad Barsoum",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:30:09.267Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-24T11:13:33.000Z",
      "submittedOnDailyAt": "2025-03-25T02:43:21.795Z",
      "title": "AMD-Hummingbird : Orientation vers un modèle efficace de texte en vidéo",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "La génération de vidéo à partir de texte (T2V) se concentre sur la capacité de synthétiser des vidéos réalistes à partir de contextes. Cependant, les modèles actuels font face à des défis dans l'équilibre entre l'efficacité computationnelle et la qualité visuelle, surtout sur des dispositifs à faibles ressources tels que les iGPUs et les téléphones intelligents. Les études précédentes privilégient la réalisme visuel à la detriment de la nécessité de modèles d'apprentissage automatique réalistes, petits et efficaces. En réponse à ces défis, nous proposons un nouveau cadre de travail léger T2V appelé \"Hummingbird\" à travers un apprentissage par réaction pour améliorer la qualité visuelle. Notre approche réduit la quantité de paramètres de U-Net de 1,4 milliards à 700 millions, améliorant considérablement l'efficacité tout en maintenant la génération de vidéo de haute qualité. De plus, nous introduisons un nouveau traitement de données en utilisant des modèles de grands modèles de langage (LLMs) et l'évaluation de la qualité de vidéo (VQA), améliorant à la fois la production contextuelle et la qualité du vidéo. Pour l'entraînement dirigé par utilisateur et la personnalisation stylistique, nous publions le code complet qui inclut le traitement de données et l'entraînement du modèle. Grâce à une large gamme d'expériences, notre méthode a atteint un augmentation de 31 fois par rapport aux modèles les plus avancés comme VideoCrafter2, atteignant les meilleurs scores sur VBench. De plus, notre méthode dépasse les limitations de la génération de vidéos longues des U-Net actuels, permettant la création de vidéos de 26 frames. En particulier, le processus d'entraînement complet est possible avec seulement quatre GPU, offrant un rendement comparable aux méthodes les plus avancées. Hummingbird est une solution pratique qui combine une haute efficacité, une scalabilité et une flexibilité, proposant des applications réalistes de la génération de vidéo à partir de texte.",
      "upvotes": 2,
      "discussionId": "67e22d5836076dc84798964e",
      "ai_keywords": [
        "U-Net",
        "Visual feedback learning",
        "Large Language Models (LLMs)",
        "Video Quality Assessment (VQA)",
        "VBench"
      ]
    },
    "publishedAt": "2025-03-24T07:13:33.000Z",
    "title": "AMD-Hummingbird: Towards an Efficient Text-to-Video Model",
    "summary": "Text-to-Video (T2V) generation has attracted significant attention for its\nability to synthesize realistic videos from textual descriptions. However,\nexisting models struggle to balance computational efficiency and high visual\nquality, particularly on resource-limited devices, e.g.,iGPUs and mobile\nphones. Most prior work prioritizes visual fidelity while overlooking the need\nfor smaller, more efficient models suitable for real-world deployment. To\naddress this challenge, we propose a lightweight T2V framework, termed\nHummingbird, which prunes existing models and enhances visual quality through\nvisual feedback learning. Our approach reduces the size of the U-Net from 1.4\nbillion to 0.7 billion parameters, significantly improving efficiency while\npreserving high-quality video generation. Additionally, we introduce a novel\ndata processing pipeline that leverages Large Language Models (LLMs) and Video\nQuality Assessment (VQA) models to enhance the quality of both text prompts and\nvideo data. To support user-driven training and style customization, we\npublicly release the full training code, including data processing and model\ntraining. Extensive experiments show that our method achieves a 31X speedup\ncompared to state-of-the-art models such as VideoCrafter2, while also attaining\nthe highest overall score on VBench. Moreover, our method supports the\ngeneration of videos with up to 26 frames, addressing the limitations of\nexisting U-Net-based methods in long video generation. Notably, the entire\ntraining process requires only four GPUs, yet delivers performance competitive\nwith existing leading methods. Hummingbird presents a practical and efficient\nsolution for T2V generation, combining high performance, scalability, and\nflexibility for real-world applications.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18559.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6456
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.18033",
      "authors": [
        {
          "_id": "67e2706af6cf2764a534d4a5",
          "user": {
            "_id": "630f0d48982455e61cc4cc08",
            "avatarUrl": "/avatars/eea6ed2e112e830effa98a4661c5474f.svg",
            "isPro": false,
            "fullname": "Samuel",
            "user": "Dvir",
            "type": "user"
          },
          "name": "Dvir Samuel",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-03-25T09:00:01.542Z",
          "hidden": false
        },
        {
          "_id": "67e2706af6cf2764a534d4a6",
          "user": {
            "_id": "66633be10875aaaa9153c963",
            "avatarUrl": "/avatars/f47aaaf7b029ad3e99f49676a8f9a479.svg",
            "isPro": false,
            "fullname": "Matan Levy",
            "user": "m98levy",
            "type": "user"
          },
          "name": "Matan Levy",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:25:56.024Z",
          "hidden": false
        },
        {
          "_id": "67e2706af6cf2764a534d4a7",
          "name": "Nir Darshan",
          "hidden": false
        },
        {
          "_id": "67e2706af6cf2764a534d4a8",
          "user": {
            "_id": "6493393f357b252af72196c5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6493393f357b252af72196c5/EWSy18XRcMRa_4XMM3Fu-.jpeg",
            "isPro": false,
            "fullname": "Gal Chechik",
            "user": "galchechik",
            "type": "user"
          },
          "name": "Gal Chechik",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:26:05.156Z",
          "hidden": false
        },
        {
          "_id": "67e2706af6cf2764a534d4a9",
          "user": {
            "_id": "64c5f22c2581696666ebed88",
            "avatarUrl": "/avatars/e85cd2d82f16ec10cad2b63929b2f05a.svg",
            "isPro": false,
            "fullname": "Rami Ben-Ari",
            "user": "ramiben",
            "type": "user"
          },
          "name": "Rami Ben-Ari",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:26:11.853Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-23T11:26:48.000Z",
      "submittedOnDailyAt": "2025-03-25T07:31:06.785Z",
      "title": "OmnimatteZero : Modèle de diffusion de vidéo entraîné précédemment en utilisant Omnimatte en temps réel (sans entraînement)",
      "submittedOnDailyBy": {
        "_id": "630f0d48982455e61cc4cc08",
        "avatarUrl": "/avatars/eea6ed2e112e830effa98a4661c5474f.svg",
        "isPro": false,
        "fullname": "Samuel",
        "user": "Dvir",
        "type": "user"
      },
      "summary": "Omnimatte a pour but la séparation d'un vidéo en couches très profondes. Cela inclut le fond, des objets individuels et des effets liés à eux (par exemple, l'illumination et les réflexions). Les méthodes actuelles nécessitent un apprentissage très complexe et une optimisation automatique. Dans cet article, nous proposons OmnimatteZero, une approche sans nécessité d'apprentissage. Ce méthode utilise un modèle simple de décalage vidéo pour réaliser la tâche d'Omnimatte. Son objectif est d'éliminer des objets d'un vidéo et d'extraire leurs effets, ce qui permet d'inclure ces objets dans une nouvelle séquence vidéo. Pour y parvenir, une méthode de complétion d'images 0-shot est appliquée pour éliminer les objets de vidéo, et des améliorations sont ajoutées pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. De plus, une méthode de complétion d'images 0-shot est appliquée et des améliorations sont ajoutées pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo. Cette méthode est appliquée et complétée avec des améliorations pour corriger tout défi que l'on puisse rencontrer lorsque cette méthode est appliquée à un vidéo",
      "upvotes": 2,
      "discussionId": "67e2706df6cf2764a534d570",
      "ai_keywords": [
        "diffusion models",
        "zero-shot image inpainting",
        "self-attention maps",
        "latent arithmetic",
        "real-time performance",
        "frame runtime"
      ]
    },
    "publishedAt": "2025-03-23T07:26:48.000Z",
    "title": "OmnimatteZero: Training-free Real-time Omnimatte with Pre-trained Video\n  Diffusion Models",
    "summary": "Omnimatte aims to decompose a given video into semantically meaningful\nlayers, including the background and individual objects along with their\nassociated effects, such as shadows and reflections. Existing methods often\nrequire extensive training or costly self-supervised optimization. In this\npaper, we present OmnimatteZero, a training-free approach that leverages\noff-the-shelf pre-trained video diffusion models for omnimatte. It can remove\nobjects from videos, extract individual object layers along with their effects,\nand composite those objects onto new videos. We accomplish this by adapting\nzero-shot image inpainting techniques for video object removal, a task they\nfail to handle effectively out-of-the-box. We then show that self-attention\nmaps capture information about the object and its footprints and use them to\ninpaint the object's effects, leaving a clean background. Additionally, through\nsimple latent arithmetic, object layers can be isolated and recombined\nseamlessly with new video layers to produce new videos. Evaluations show that\nOmnimatteZero not only achieves superior performance in terms of background\nreconstruction but also sets a new record for the fastest Omnimatte approach,\nachieving real-time performance with minimal frame runtime.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18033.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "630f0d48982455e61cc4cc08",
      "avatarUrl": "/avatars/eea6ed2e112e830effa98a4661c5474f.svg",
      "fullname": "Samuel",
      "name": "Dvir",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.17500",
      "authors": [
        {
          "_id": "67e23d503ef5318b1550f1bc",
          "user": {
            "_id": "6071c4b270e11b30cfcfd7a3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6071c4b270e11b30cfcfd7a3/-1ekCBzSTpqxkkul0bgmI.jpeg",
            "isPro": false,
            "fullname": "Louis Owen",
            "user": "louisowen6",
            "type": "user"
          },
          "name": "Louis Owen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-25T08:19:59.211Z",
          "hidden": false
        },
        {
          "_id": "67e23d503ef5318b1550f1bd",
          "user": {
            "_id": "62cd4b03c5cc157be82f0b56",
            "avatarUrl": "/avatars/351e963c1c763d507ae78cbcd62966a3.svg",
            "isPro": false,
            "fullname": "Abhay kumar",
            "user": "akanyaani",
            "type": "user"
          },
          "name": "Abhay Kumar",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-25T09:23:34.033Z",
          "hidden": false
        },
        {
          "_id": "67e23d503ef5318b1550f1be",
          "user": {
            "_id": "645a0d3dd6648853107c5fdc",
            "avatarUrl": "/avatars/1e3b6a4f5ce81a707ba7cbdf81631091.svg",
            "isPro": false,
            "fullname": "Nilabhra Roy Chowdhury",
            "user": "nilabhra",
            "type": "user"
          },
          "name": "Nilabhra Roy Chowdhury",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-25T09:15:43.961Z",
          "hidden": false
        },
        {
          "_id": "67e23d503ef5318b1550f1bf",
          "user": {
            "_id": "65e4be59e8b017ee1310a1b6",
            "avatarUrl": "/avatars/c3f7cdf5d0859cb80bfb2b970a675dfa.svg",
            "isPro": false,
            "fullname": "Fabian",
            "user": "gueraf",
            "type": "user"
          },
          "name": "Fabian Güra",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-25T08:19:56.909Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6071c4b270e11b30cfcfd7a3/AaY617TTwvPiB-ub2B9bA.png"
      ],
      "publishedAt": "2025-03-21T19:23:08.000Z",
      "submittedOnDailyAt": "2025-03-25T03:52:18.158Z",
      "title": "Controle de la Variance par Réajustement des Poids dans l'Apprentissage de Réservation de LLM",
      "submittedOnDailyBy": {
        "_id": "6071c4b270e11b30cfcfd7a3",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6071c4b270e11b30cfcfd7a3/-1ekCBzSTpqxkkul0bgmI.jpeg",
        "isPro": false,
        "fullname": "Louis Owen",
        "user": "louisowen6",
        "type": "user"
      },
      "summary": "Les résultats d'entraînement précédent d'un modèle de Langage Large (LLM) sont très affectés par l'initialisation des poids et les stratégies de contrôle de la variance. L'importance de contrôler la variance initiale dans les réseaux neuronaux généraux est bien documentée, mais la littérature sur l'initialisation et le contrôle du croissance lors de l'entraînement précédent d'un LLM est limitée. Dans cet article, nous présentons les techniques d'initialisation des poids Layer Index Rescaling (LIR) et la stratégie de contrôle de la variance Target Variance Rescaling (TVR). Les expériences avec un modèle LLaMA de 1B paramètres montrent que ces méthodologies améliorent significativement la gestion de la variance, ce qui conduit à un amélioration considérable du rendement dans les tâches ultérieures, réduit les valeurs extrêmes d'activation et atténue les problèmes liés à l'entraînement de l'activation et à la précision basse. Le code est disponible sur la suivante URL : https://github.com/bluorion-com/weight_rescaling.",
      "upvotes": 2,
      "discussionId": "67e23d513ef5318b1550f22c",
      "githubRepo": "https://github.com/bluorion-com/weight_rescaling",
      "ai_keywords": [
        "Layer Index Rescaling (LIR)",
        "Target Variance Rescaling (TVR)",
        "weight initialization",
        "variance control"
      ]
    },
    "publishedAt": "2025-03-21T15:23:08.000Z",
    "title": "Variance Control via Weight Rescaling in LLM Pre-training",
    "summary": "The outcome of Large Language Model (LLM) pre-training strongly depends on\nweight initialization and variance control strategies. Although the importance\nof initial variance control has been well documented in neural networks in\ngeneral, the literature on initialization and management of its growth during\nLLM pre-training, specifically, is somewhat sparse. In this paper, we introduce\nthe Layer Index Rescaling (LIR) weight initialization scheme, and the Target\nVariance Rescaling (TVR) variance control strategy. Experiments on a 1B\nparameter LLaMA model demonstrate that better variance management using these\ntechniques yields substantial improvements in downstream task performance (up\nto 4.6% on common pre-training benchmarks) and reduces extreme activation\nvalues, thus mitigating challenges associated with quantization and\nlow-precision training. Our code is available at:\nhttps://github.com/bluorion-com/weight_rescaling.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6071c4b270e11b30cfcfd7a3/AaY617TTwvPiB-ub2B9bA.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17500.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6071c4b270e11b30cfcfd7a3",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6071c4b270e11b30cfcfd7a3/-1ekCBzSTpqxkkul0bgmI.jpeg",
      "fullname": "Louis Owen",
      "name": "louisowen6",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.18470",
      "authors": [
        {
          "_id": "67e23d7ddb11e1d38226cafd",
          "user": {
            "_id": "669794c5813d96b4eb0b3fd6",
            "avatarUrl": "/avatars/b4e2bb7b07cc281932d783dcbb64f211.svg",
            "isPro": true,
            "fullname": "Zhenyu Pan",
            "user": "zhenyupan",
            "type": "user"
          },
          "name": "Zhenyu Pan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-25T08:19:54.569Z",
          "hidden": false
        },
        {
          "_id": "67e23d7ddb11e1d38226cafe",
          "name": "Han Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-24T09:18:01.000Z",
      "submittedOnDailyAt": "2025-03-25T06:54:21.536Z",
      "title": "MetaSpatial : Développement de VLMs avec amélioration du reconnaissance des espaces 3D basée sur des métas",
      "submittedOnDailyBy": {
        "_id": "669794c5813d96b4eb0b3fd6",
        "avatarUrl": "/avatars/b4e2bb7b07cc281932d783dcbb64f211.svg",
        "isPro": true,
        "fullname": "Zhenyu Pan",
        "user": "zhenyupan",
        "type": "user"
      },
      "summary": "MetaSpatial est l'un des premiers cadres de travail basés sur l'apprentissage par renforcement (RL). Ce cadre de travail renforce la perception spatiale 3D dans les modèles de langage visuel-linguistique (VLMs) et évite la nécessité d'optimisation codée pour la génération en temps réel de scènes 3D. MetaSpatial résout deux problèmes fondamentaux : (i) la manque de perception spatiale 3D dans les VLMs, ce qui limite la capacité de génération de dispositions réalistes ; et (ii) l'inadéquation de la technique de fine-tuning de haute précision (SFT) pour les tâches de création d'objets, en raison de la manque de labellisation complète des données réelles. L'innovation principale est la structure d'optimisation basée sur l'apprentissage par renforcement en étapes, qui intègre l'évaluation de contraintes physiques et d'images rendues. Cela conduit à des dispositions 3D qui sont cohérentes, physiquement possibles et artistiquement convaincantes. Méthodologiquement, MetaSpatial introduit un processus itératif adaptatif, où les VLMs analysent les sorties rendues pour améliorer progressivement la disposition spatiale. Dans des évaluations expérimentales, MetaSpatial améliore significativement la cohérence spatiale et la stabilité de forme dans des modèles de différents tailles. Après l'entraînement, les dispositions d'objets sont réalistes et fonctionnellement cohérentes. Cela démontre l'effet de l'RL sur la perception spatiale 3D dans le métaverse, la AR/VR, les humanoides numériques et le développement de jeux. Le code, les données et le pipeline d'entraînement sont disponibles sur https://github.com/PzySeere/MetaSpatial.",
      "upvotes": 1,
      "discussionId": "67e23d7fdb11e1d38226cb7b",
      "ai_keywords": [
        "reinforcement learning (RL)",
        "vision-language models (VLMs)",
        "3D spatial reasoning",
        "real-time 3D scene generation",
        "internalized 3D spatial reasoning",
        "supervised fine-tuning (SFT)",
        "multi-turn RL-based optimization",
        "physics-aware constraints",
        "rendered image evaluations",
        "adaptive, iterative reasoning process",
        "scene coherence",
        "spatial consistency",
        "formatting stability",
        "object placements",
        "metaverse",
        "AR/VR",
        "digital twins",
        "game development",
        "empirical evaluations"
      ]
    },
    "publishedAt": "2025-03-24T05:18:01.000Z",
    "title": "MetaSpatial: Reinforcing 3D Spatial Reasoning in VLMs for the Metaverse",
    "summary": "We present MetaSpatial, the first reinforcement learning (RL)-based framework\ndesigned to enhance 3D spatial reasoning in vision-language models (VLMs),\nenabling real-time 3D scene generation without the need for hard-coded\noptimizations. MetaSpatial addresses two core challenges: (i) the lack of\ninternalized 3D spatial reasoning in VLMs, which limits their ability to\ngenerate realistic layouts, and (ii) the inefficiency of traditional supervised\nfine-tuning (SFT) for layout generation tasks, as perfect ground truth\nannotations are unavailable. Our key innovation is a multi-turn RL-based\noptimization mechanism that integrates physics-aware constraints and rendered\nimage evaluations, ensuring generated 3D layouts are coherent, physically\nplausible, and aesthetically consistent. Methodologically, MetaSpatial\nintroduces an adaptive, iterative reasoning process, where the VLM refines\nspatial arrangements over multiple turns by analyzing rendered outputs,\nimproving scene coherence progressively. Empirical evaluations demonstrate that\nMetaSpatial significantly enhances the spatial consistency and formatting\nstability of various scale models. Post-training, object placements are more\nrealistic, aligned, and functionally coherent, validating the effectiveness of\nRL for 3D spatial reasoning in metaverse, AR/VR, digital twins, and game\ndevelopment applications. Our code, data, and training pipeline are publicly\navailable at https://github.com/PzySeere/MetaSpatial.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18470.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "669794c5813d96b4eb0b3fd6",
      "avatarUrl": "/avatars/b4e2bb7b07cc281932d783dcbb64f211.svg",
      "fullname": "Zhenyu Pan",
      "name": "zhenyupan",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.18352",
      "authors": [
        {
          "_id": "67e217a272e17348c5b3f0a2",
          "name": "Jinjin Zhang",
          "hidden": false
        },
        {
          "_id": "67e217a272e17348c5b3f0a3",
          "user": {
            "_id": "6708e399672d9dcd31575fbc",
            "avatarUrl": "/avatars/0f947f17b5426186aadaa4224571f47b.svg",
            "isPro": false,
            "fullname": "qiuyuhuang",
            "user": "qiuyuhuang",
            "type": "user"
          },
          "name": "Qiuyu Huang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:24:20.054Z",
          "hidden": false
        },
        {
          "_id": "67e217a272e17348c5b3f0a4",
          "name": "Junjie Liu",
          "hidden": false
        },
        {
          "_id": "67e217a272e17348c5b3f0a5",
          "user": {
            "_id": "64905cd589f22918ecaca080",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/2S7I7uZL49CXbUN2T7p63.jpeg",
            "isPro": false,
            "fullname": "Xiefan Guo",
            "user": "xiefan-guo",
            "type": "user"
          },
          "name": "Xiefan Guo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:24:02.736Z",
          "hidden": false
        },
        {
          "_id": "67e217a272e17348c5b3f0a6",
          "user": {
            "_id": "62c581177b48ba0bb8cdb737",
            "avatarUrl": "/avatars/d1a85c28f13bb86481b6be80824eb1fa.svg",
            "isPro": false,
            "fullname": "di huang",
            "user": "dihuang",
            "type": "user"
          },
          "name": "Di Huang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:23:56.412Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-24T05:25:07.000Z",
      "submittedOnDailyAt": "2025-03-25T07:48:32.671Z",
      "title": "Diffusion-4K : Utilisation d'un modèle de diffusion potentiel pour la synthèse d'images à haute résolution",
      "submittedOnDailyBy": {
        "_id": "5f1158120c833276f61f1a84",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
        "isPro": false,
        "fullname": "Niels Rogge",
        "user": "nielsr",
        "type": "user"
      },
      "summary": "Dans cet article, nous proposons le nouveau cadre appelé Diffusion-4K, et nous présentons l'utilisation de modèles de diffusion pour la synthèse directe d'images à haute résolution. Les points clés du développement sont les suivants :\n\n(1) Benchmark Aesthetic-4K : Nous abordons la problématique selon laquelle aucun ensemble de données de synthèse d'images à 4K n'a encore été publié. Nous avons construit le benchmark Aesthetic-4K, un ensemble de données complète pour l'évaluation. Ce benchmark a été construit en utilisant des images de qualité élevée et des captures provenant de GPT-4o. De plus, nous effectuons des évaluations détaillées en utilisant le score GLCM et le rapport de calcul, et nous les combinons avec des indicateurs généraux tels que FID, artistique et CLIPScore pour évaluer les images à haute résolution.\n\n(2) Ajuste micro basé sur la Wavelet : Nous proposons un approche d'ajuste micro basée sur la Wavelet pour entraîner des images réalistes à 4K directement. Cette approche est applicable à différents modèles de diffusion et est efficace pour la synthèse d'images de grande qualité. Par conséquent, Diffusion-4K atteint un bon rendement en combinant la synchronisation de la synthèse d'images de grande qualité avec la planification de texte, en particulier avec des modèles grands de diffusion modernes (comme SD3-2B et Flux-12B). Les résultats des expériences étendues obtenus dans ce benchmark démontrent la excellente capacité de synthèse d'images à haute résolution de Diffusion-4K.",
      "upvotes": 1,
      "discussionId": "67e217a772e17348c5b3f20a",
      "ai_keywords": [
        "diffusion models",
        "text-to-image diffusion models",
        "Aesthetic-4K Benchmark",
        "wavelet-based fine-tuning",
        "latent diffusion models",
        "SD3-2B",
        "Flux-12B",
        "GLCM Score",
        "Compression Ratio",
        "FID",
        "Aesthetics",
        "CLIPScore",
        "ultra-high-resolution image synthesis",
        "photorealistic 4K images",
        "high-quality image synthesis",
        "text prompt adherence"
      ]
    },
    "publishedAt": "2025-03-24T01:25:07.000Z",
    "title": "Diffusion-4K: Ultra-High-Resolution Image Synthesis with Latent\n  Diffusion Models",
    "summary": "In this paper, we present Diffusion-4K, a novel framework for direct\nultra-high-resolution image synthesis using text-to-image diffusion models. The\ncore advancements include: (1) Aesthetic-4K Benchmark: addressing the absence\nof a publicly available 4K image synthesis dataset, we construct Aesthetic-4K,\na comprehensive benchmark for ultra-high-resolution image generation. We\ncurated a high-quality 4K dataset with carefully selected images and captions\ngenerated by GPT-4o. Additionally, we introduce GLCM Score and Compression\nRatio metrics to evaluate fine details, combined with holistic measures such as\nFID, Aesthetics and CLIPScore for a comprehensive assessment of\nultra-high-resolution images. (2) Wavelet-based Fine-tuning: we propose a\nwavelet-based fine-tuning approach for direct training with photorealistic 4K\nimages, applicable to various latent diffusion models, demonstrating its\neffectiveness in synthesizing highly detailed 4K images. Consequently,\nDiffusion-4K achieves impressive performance in high-quality image synthesis\nand text prompt adherence, especially when powered by modern large-scale\ndiffusion models (e.g., SD3-2B and Flux-12B). Extensive experimental results\nfrom our benchmark demonstrate the superiority of Diffusion-4K in\nultra-high-resolution image synthesis.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18352.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5f1158120c833276f61f1a84",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
      "fullname": "Niels Rogge",
      "name": "nielsr",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 799
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.17735",
      "authors": [
        {
          "_id": "67e22bc349edf14060e5747a",
          "name": "Zhiqiang Yuan",
          "hidden": false
        },
        {
          "_id": "67e22bc349edf14060e5747b",
          "name": "Ting Zhang",
          "hidden": false
        },
        {
          "_id": "67e22bc349edf14060e5747c",
          "name": "Ying Deng",
          "hidden": false
        },
        {
          "_id": "67e22bc349edf14060e5747d",
          "name": "Jiapei Zhang",
          "hidden": false
        },
        {
          "_id": "67e22bc349edf14060e5747e",
          "name": "Yeshuang Zhu",
          "hidden": false
        },
        {
          "_id": "67e22bc349edf14060e5747f",
          "name": "Zexi Jia",
          "hidden": false
        },
        {
          "_id": "67e22bc349edf14060e57480",
          "name": "Jie Zhou",
          "hidden": false
        },
        {
          "_id": "67e22bc349edf14060e57481",
          "name": "Jinchao Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-22T11:28:25.000Z",
      "submittedOnDailyAt": "2025-03-25T02:36:41.359Z",
      "title": "RDTF : Cadre d'entraînement efficace de double masque pour la génération de bandes d'animation multiples",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Récemment, les progrès significatifs dans la technologie de génération d'images ont attiré une grande attention des académiques. Pour appliquer cette technologie dans des applications à faibles ressources, les chercheurs utilisent généralement des méthodes d'ajustement de paramètres efficaces et d'autres modèles de base d'apprentissage pour le fine-tuning. Cette approche permet de transmettre le savoir de la région des données sources à la région des données cibles, mais peut également entraîner une perte de capacité de généralisation et la possibilité de sécher le savoir de la région des données sources dans la région des données cibles lors du processus d'inférence. Dans cet article, il est argumenté que, lorsque des limitations de ressources existent, entraîner un petit modèle de génération d'images depuis le départ avec un niveau de données de quelques millions, dépasse l'ajustement de paramètres efficaces de grands modèles dans des applications à faibles ressources : cela repose sur l'efficacité de l'utilisation de données et sur l'efficacité de la mise en œuvre de stratégies correctives. Un cas expérimental de la Génération d'Animations de Poupées (GAS) est utilisé pour démontrer ce point. Une réseau de génération de cadences discrètes est construit pour la création de ces poupées à une vitesse de cadence basse, et la demande en paramètres du modèle d'entraînement est satisfaite sous les contraintes de ressources. Pour fournir un soutien de données au modèle d'entraînement depuis le départ, une réseau de génération de cadences discrètes est proposé, et une stratégie d'utilisation de données basée sur deux masques est présentée pour améliorer l'efficacité de l'utilisation de données et étendre la diversité des données limitées. Un méthode d'apprentissage adaptatif à la difficulté est proposée pour accélérer la convergence dans ces deux masques, et l'entropie des échantillons est décomposée en composantes statiques et adaptatives pour faciliter le processus d'entraînement depuis le départ jusqu'à la fin. Les expériences comparent quantitativement et qualitativement avec des méthodes d'ajustement de paramètres efficaces comme I2V-Adapter et SimDA, montrant que cette méthode dépasse les autres et démontre la possibilité de cette approche dans des tâches à faibles ressources. Le code est disponible.",
      "upvotes": 1,
      "discussionId": "67e22bc449edf14060e574e3",
      "ai_keywords": [
        "parameter-efficient tuning",
        "Adapter",
        "Lora",
        "discrete frame generation network",
        "dual-mask based data utilization strategy",
        "curriculum learning method",
        "difficulty-adaptive curriculum learning",
        "sample entropy",
        "I2V-Adapter",
        "SimDA"
      ]
    },
    "publishedAt": "2025-03-22T07:28:25.000Z",
    "title": "RDTF: Resource-efficient Dual-mask Training Framework for Multi-frame\n  Animated Sticker Generation",
    "summary": "Recently, great progress has been made in video generation technology,\nattracting the widespread attention of scholars. To apply this technology to\ndownstream applications under resource-constrained conditions, researchers\nusually fine-tune the pre-trained models based on parameter-efficient tuning\nmethods such as Adapter or Lora. Although these methods can transfer the\nknowledge from the source domain to the target domain, fewer training\nparameters lead to poor fitting ability, and the knowledge from the source\ndomain may lead to the inference process deviating from the target domain. In\nthis paper, we argue that under constrained resources, training a smaller video\ngeneration model from scratch using only million-level samples can outperform\nparameter-efficient tuning on larger models in downstream applications: the\ncore lies in the effective utilization of data and curriculum strategy. Take\nanimated sticker generation (ASG) as a case study, we first construct a\ndiscrete frame generation network for stickers with low frame rates, ensuring\nthat its parameters meet the requirements of model training under constrained\nresources. In order to provide data support for models trained from scratch, we\ncome up with a dual-mask based data utilization strategy, which manages to\nimprove the availability and expand the diversity of limited data. To\nfacilitate convergence under dual-mask situation, we propose a\ndifficulty-adaptive curriculum learning method, which decomposes the sample\nentropy into static and adaptive components so as to obtain samples from easy\nto difficult. The experiment demonstrates that our resource-efficient dual-mask\ntraining framework is quantitatively and qualitatively superior to\nefficient-parameter tuning methods such as I2V-Adapter and SimDA, verifying the\nfeasibility of our method on downstream tasks under constrained resources. Code\nwill be available.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17735.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6456
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.16924",
      "authors": [
        {
          "_id": "67e230f384513315a91c5602",
          "user": {
            "_id": "664207e5af62c6c26653b369",
            "avatarUrl": "/avatars/031453340e114efb10b7d650e1d1c384.svg",
            "isPro": false,
            "fullname": "Joo Chan Lee",
            "user": "maincold2",
            "type": "user"
          },
          "name": "Joo Chan Lee",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-25T08:20:07.628Z",
          "hidden": false
        },
        {
          "_id": "67e230f384513315a91c5603",
          "name": "Jong Hwan Ko",
          "hidden": false
        },
        {
          "_id": "67e230f384513315a91c5604",
          "user": {
            "_id": "655e0141d36a195f663ee4b0",
            "avatarUrl": "/avatars/97bb695ccefdcb2139b94bcae808cf99.svg",
            "isPro": false,
            "fullname": "Eunbyung Park",
            "user": "epark",
            "type": "user"
          },
          "name": "Eunbyung Park",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:25:09.995Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-21T07:41:45.000Z",
      "submittedOnDailyAt": "2025-03-25T02:59:24.076Z",
      "title": "Optimisation du minimum 3D gaussien de la mousse",
      "submittedOnDailyBy": {
        "_id": "664207e5af62c6c26653b369",
        "avatarUrl": "/avatars/031453340e114efb10b7d650e1d1c384.svg",
        "isPro": false,
        "fullname": "Joo Chan Lee",
        "user": "maincold2",
        "type": "user"
      },
      "summary": "3D Gaussian Splatting (3DGS) a émergé comme une technique révolutionnaire permettant de rendre des images avec une efficacité en temps élevée, offrant une représentation forte applicable dans une large gamme d'applications. Cependant, la représentation explicite de la vision 3D à travers des primitives gaussiennes nécessite un grand espace de stockage et une surcharge mémoire. Selon les derniers études, il a été démontré que le nombre de gaussiennes peut être significativement réduit pour atteindre un rendu de haute qualité, mais les méthodes de compression actuelles de 3DGS se concentrent principalement sur la compression des propriétés, ce qui implique l'utilisation d'un nombre relativement élevé de gaussiennes. Cela peut rendre un petit ensemble de gaussiennes vulnérable à une compression inefficace des propriétés et peut conduire à une perte notable de qualité. Le nombre de gaussiennes est directement lié au coût du calcul, et il est crucial de le réduire efficacement pour améliorer la efficacité de stockage. Dans cet article, nous proposons la représentation Optimized Minimal Gaussians (OMG). OMG réduit significativement l'espace de stockage tout en utilisant un nombre minimum de primitives. Tout d'abord, nous identifions les gaussiennes qui diffèrent de manière significative de celles qui sont proches, minimisant les parties inutiles sans sacrifier la qualité. Ensuite, nous proposons une représentation efficace de la continuité et de la discontinuité des primitives, ainsi qu'une représentation précise des propriétés. De plus, nous améliorons la représentation de la discontinuité par la technique de sous-vecteur quantisation, maintenant un entraînement rapide indépendant du taille du codebook. Selon les expériences étendues, OMG réduit approximativement le 50% de l'espace de stockage nécessaire par rapport aux limites actuelles, permettant un rendu de haute qualité et un rendu de plus de 600 FPS. Le code source est disponible sur https://maincold2.github.io/omg/.",
      "upvotes": 1,
      "discussionId": "67e230f484513315a91c5678",
      "projectPage": "https://maincold2.github.io/omg/",
      "githubRepo": "https://github.com/maincold2/OMG",
      "ai_keywords": [
        "Gaussian Splatting (3DGS)",
        "real-time",
        "high-performance rendering",
        "3D scenes",
        "explicit Gaussian primitives",
        "storage",
        "memory overhead",
        "high-quality rendering",
        "attribute compression",
        "quality degradation",
        "computational costs",
        "Optimized Minimal Gaussians representation (OMG)",
        "distinct Gaussian",
        "redundancy",
        "attribute representation",
        "continuity",
        "irregularity",
        "sub-vector quantization",
        "codebook size",
        "FPS rendering",
        "rendering quality"
      ]
    },
    "publishedAt": "2025-03-21T03:41:45.000Z",
    "title": "Optimized Minimal 3D Gaussian Splatting",
    "summary": "3D Gaussian Splatting (3DGS) has emerged as a powerful representation for\nreal-time, high-performance rendering, enabling a wide range of applications.\nHowever, representing 3D scenes with numerous explicit Gaussian primitives\nimposes significant storage and memory overhead. Recent studies have shown that\nhigh-quality rendering can be achieved with a substantially reduced number of\nGaussians when represented with high-precision attributes. Nevertheless,\nexisting 3DGS compression methods still rely on a relatively large number of\nGaussians, focusing primarily on attribute compression. This is because a\nsmaller set of Gaussians becomes increasingly sensitive to lossy attribute\ncompression, leading to severe quality degradation. Since the number of\nGaussians is directly tied to computational costs, it is essential to reduce\nthe number of Gaussians effectively rather than only optimizing storage. In\nthis paper, we propose Optimized Minimal Gaussians representation (OMG), which\nsignificantly reduces storage while using a minimal number of primitives.\nFirst, we determine the distinct Gaussian from the near ones, minimizing\nredundancy without sacrificing quality. Second, we propose a compact and\nprecise attribute representation that efficiently captures both continuity and\nirregularity among primitives. Additionally, we propose a sub-vector\nquantization technique for improved irregularity representation, maintaining\nfast training with a negligible codebook size. Extensive experiments\ndemonstrate that OMG reduces storage requirements by nearly 50% compared to the\nprevious state-of-the-art and enables 600+ FPS rendering while maintaining high\nrendering quality. Our source code is available at\nhttps://maincold2.github.io/omg/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16924.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "664207e5af62c6c26653b369",
      "avatarUrl": "/avatars/031453340e114efb10b7d650e1d1c384.svg",
      "fullname": "Joo Chan Lee",
      "name": "maincold2",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.18494",
      "authors": [
        {
          "_id": "67e21a81e2e69ea26eee4f67",
          "user": {
            "_id": "65bef46337491e7adc5ee7c9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/bQ2WY0bU65IQ3IBn6SrnA.png",
            "isPro": false,
            "fullname": "Hao-Yuan Chen",
            "user": "MarkChenX",
            "type": "user"
          },
          "name": "Hao-Yuan Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-25T08:20:30.277Z",
          "hidden": false
        },
        {
          "_id": "67e21a81e2e69ea26eee4f68",
          "name": "Cheng-Pong Huang",
          "hidden": false
        },
        {
          "_id": "67e21a81e2e69ea26eee4f69",
          "name": "Jui-Ming Yao",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/65bef46337491e7adc5ee7c9/kVdnqHbvP4VXfEWb2MAs_.jpeg",
        "https://cdn-uploads.huggingface.co/production/uploads/65bef46337491e7adc5ee7c9/amlMqiikDojWAwsnS3d5e.png",
        "https://cdn-uploads.huggingface.co/production/uploads/65bef46337491e7adc5ee7c9/cndDykrtJViO6gRavkF9D.png"
      ],
      "publishedAt": "2025-03-24T09:48:59.000Z",
      "submittedOnDailyAt": "2025-03-25T06:52:38.289Z",
      "title": "Amélioration des agents de codification par le moyen du traitement du langage",
      "submittedOnDailyBy": {
        "_id": "65bef46337491e7adc5ee7c9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/bQ2WY0bU65IQ3IBn6SrnA.png",
        "isPro": false,
        "fullname": "Hao-Yuan Chen",
        "user": "MarkChenX",
        "type": "user"
      },
      "summary": "Le début de langages de code de haut niveau et l'application d'agents d'IA ont entraîné un grand progrès dans les plus avancés marqueurs de génération de code, modifiant ainsi les tâches de développement de logiciels. Cependant, même en utilisant des modèles de raisonnement calculés en temps de test, ces systèmes font face à des défis liés à la complexité du développement de logiciels. Dans cet article, nous présentons un système d'intelligence du code et de raisonnement d'agent, appelé CURA, qui a été renforcé par la fonction de Visualisation de Processus Linguistiques (VPS). Ce système a réussi à améliorer les modèles de référence dans des marqueurs difficiles comme BigCodeBench d'un 3,65%. De plus, la combinaison de CURA, le modèle o3-mini et le méthode de VPS a permis d'atteindre les meilleurs résultats. Cette recherche avance dans l'intégration de l'architecture de génération de code basée sur des modèles de langage de haut niveau et du raisonnement d'agent, permettant de résoudre des tâches complexes de développement de logiciels en utilisant le raisonnement d'agent de modèles de langage.",
      "upvotes": 0,
      "discussionId": "67e21a82e2e69ea26eee4fba",
      "ai_keywords": [
        "large language models (LLMs)",
        "code generation",
        "software engineering tasks",
        "CURA",
        "code understanding and reasoning agent system",
        "verbal process supervision (VPS)",
        "BigCodeBench",
        "o3-mini model",
        "reasoning-driven architectures",
        "agentic reasoning"
      ]
    },
    "publishedAt": "2025-03-24T05:48:59.000Z",
    "title": "Verbal Process Supervision Elicits Better Coding Agents",
    "summary": "The emergence of large language models and their applications as AI agents\nhave significantly advanced state-of-the-art code generation benchmarks,\ntransforming modern software engineering tasks. However, even with test-time\ncomputed reasoning models, these systems still struggle with complex software\nengineering challenges. This work introduces CURA, a code understanding and\nreasoning agent system enhanced with verbal process supervision (VPS),\nachieving a 3.65\\% improvement over baseline models on challenging benchmarks\nlike BigCodeBench. Furthermore, CURA, when paired with the o3-mini model and\nVPS techniques, attains state-of-the-art performance. This work represents a\nstep forward in integrating reasoning-driven architectures with LLM-based code\ngeneration, enabling agentic reasoning for language models to solve complex\nsoftware engineering tasks.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/65bef46337491e7adc5ee7c9/kVdnqHbvP4VXfEWb2MAs_.jpeg",
      "https://cdn-uploads.huggingface.co/production/uploads/65bef46337491e7adc5ee7c9/amlMqiikDojWAwsnS3d5e.png",
      "https://cdn-uploads.huggingface.co/production/uploads/65bef46337491e7adc5ee7c9/cndDykrtJViO6gRavkF9D.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18494.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65bef46337491e7adc5ee7c9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/bQ2WY0bU65IQ3IBn6SrnA.png",
      "fullname": "Hao-Yuan Chen",
      "name": "MarkChenX",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]