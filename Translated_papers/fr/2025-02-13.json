[
  {
    "paper": {
      "id": "2502.08590",
      "authors": [
        {
          "_id": "67ad79552fdac6537b43f120",
          "name": "Yujie Zhou",
          "hidden": false
        },
        {
          "_id": "67ad79552fdac6537b43f121",
          "name": "Jiazi Bu",
          "hidden": false
        },
        {
          "_id": "67ad79552fdac6537b43f122",
          "name": "Pengyang Ling",
          "hidden": false
        },
        {
          "_id": "67ad79552fdac6537b43f123",
          "name": "Pan Zhang",
          "hidden": false
        },
        {
          "_id": "67ad79552fdac6537b43f124",
          "name": "Tong Wu",
          "hidden": false
        },
        {
          "_id": "67ad79552fdac6537b43f125",
          "name": "Qidong Huang",
          "hidden": false
        },
        {
          "_id": "67ad79552fdac6537b43f126",
          "name": "Jinsong Li",
          "hidden": false
        },
        {
          "_id": "67ad79552fdac6537b43f127",
          "name": "Xiaoyi Dong",
          "hidden": false
        },
        {
          "_id": "67ad79552fdac6537b43f128",
          "user": {
            "_id": "63859cf3b2906edaf83af9f0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63859cf3b2906edaf83af9f0/iUQm5FAomzqYi6fkqIn9F.jpeg",
            "isPro": false,
            "fullname": "Yuhang Zang",
            "user": "yuhangzang",
            "type": "user"
          },
          "name": "Yuhang Zang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-13T08:21:31.817Z",
          "hidden": false
        },
        {
          "_id": "67ad79552fdac6537b43f129",
          "name": "Yuhang Cao",
          "hidden": false
        },
        {
          "_id": "67ad79552fdac6537b43f12a",
          "name": "Anyi Rao",
          "hidden": false
        },
        {
          "_id": "67ad79552fdac6537b43f12b",
          "name": "Jiaqi Wang",
          "hidden": false
        },
        {
          "_id": "67ad79552fdac6537b43f12c",
          "name": "Li Niu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-12T17:24:19.000Z",
      "title": "Light-A-Video : Fusion progressive de la lumière pour le redimensionnement de vidéos sans apprentissage",
      "summary": "L'évolution des modèles de réestructuration de l'illumination dans les images a été motivée par l'utilisation de grands ensembles de données et de modèles pré-entraînés de diffusion. Cet avancé a permis d'établir des illuminations cohérentes, mais la réestructuration de l'illumination dans les images a été retardée en raison des coûts d'entraînement exagérés et de la rareté d'ensembles de données d'illumination réestructurée de haute qualité et de diversité. En appliquant des modèles de réestructuration de l'illumination de manière frame-by-frame, des discontinuités apparaissent dans l'illumination et dans les bords réestructurés, ce qui génère des artefacts dans les images résultantes.\n\nDans cet article, nous proposons un approche sans nécessité d'entraînement appelée Light-A-Video, qui réestructure l'illumination des images de manière douce dans le temps. L'adaptation des modèles de réestructuration de l'illumination à Light-A-Video introduit deux technologies cruciales pour améliorer la cohérence de l'illumination : d'abord, nous concevons le module d'Attention de Lumière Consistente (CLA) pour renforcer l'interaction entre frames et stabiliser la génération d'illumination dans le fond. Ensuite, nous utilisons la physique de la propagation de la lumière pour combiner linéairement l'illumination originale et la réestructurée par la stratégie de Fusion de Lumière Progressive (PLF), assurant une illumination douce dans le temps. Les expériences montrent que Light-A-Video améliore la cohérence temporelle des images réestructurées, maintient la qualité des images et assure la mobilité de l'illumination entre frames. Page du projet : https://bujiazi.github.io/light-a-video.github.io/",
      "upvotes": 25,
      "discussionId": "67ad79572fdac6537b43f189"
    },
    "publishedAt": "2025-02-12T23:47:56.223Z",
    "title": "Light-A-Video: Training-free Video Relighting via Progressive Light Fusion",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.08590.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b4eec4faa3181a5eab9c46",
      "avatarUrl": "/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg",
      "fullname": "Jiaqi Wang",
      "name": "myownskyW7",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isMod": false,
      "followerCount": 16
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.07870",
      "authors": [
        {
          "_id": "67ad79cb60ec3f444b21cbcb",
          "name": "Alex Jinpeng Wang",
          "hidden": false
        },
        {
          "_id": "67ad79cb60ec3f444b21cbcc",
          "name": "Dongxing Mao",
          "hidden": false
        },
        {
          "_id": "67ad79cb60ec3f444b21cbcd",
          "name": "Jiawei Zhang",
          "hidden": false
        },
        {
          "_id": "67ad79cb60ec3f444b21cbce",
          "name": "Weiming Han",
          "hidden": false
        },
        {
          "_id": "67ad79cb60ec3f444b21cbcf",
          "name": "Zhuobai Dong",
          "hidden": false
        },
        {
          "_id": "67ad79cb60ec3f444b21cbd0",
          "name": "Linjie Li",
          "hidden": false
        },
        {
          "_id": "67ad79cb60ec3f444b21cbd1",
          "name": "Yiqi Lin",
          "hidden": false
        },
        {
          "_id": "67ad79cb60ec3f444b21cbd2",
          "name": "Zhengyuan Yang",
          "hidden": false
        },
        {
          "_id": "67ad79cb60ec3f444b21cbd3",
          "name": "Libo Qin",
          "hidden": false
        },
        {
          "_id": "67ad79cb60ec3f444b21cbd4",
          "name": "Fuwei Zhang",
          "hidden": false
        },
        {
          "_id": "67ad79cb60ec3f444b21cbd5",
          "name": "Lijuan Wang",
          "hidden": false
        },
        {
          "_id": "67ad79cb60ec3f444b21cbd6",
          "name": "Min Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-11T18:59:19.000Z",
      "title": "TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : TextAtlas5M : Text",
      "summary": "Récemment, la génération d'images à partir de texte conditionné a attiré l'attention, car il est possible de traiter des textes longs et complexes. Dans la vie quotidienne, des ornements, des graphiques de design et des signatures, entre autres, apparaissent avec des textes complexes. L'intégration de texte et de vision est cruciale pour transmettre des informations complexes. Cependant, la génération d'images contenant de longs textes reste un problème à long terme, et la limitation actuelle des ensembles de données est la principale cause de ce problème. Pour résoudre cette question, nous présentons le nouveau ensemble de données TextAtlas5M. Cet ensemble de données a été conçu spécifiquement pour évaluer la visualisation de longs textes. Il comprend 5 millions d'images générées avec des textes longs et est combiné avec différents types de données, permettant une évaluation détaillée de la génération d'images avec de longs textes dans des modèles de génération à grande échelle. De plus, un ensemble de tests humain-amélioration TextAtlasEval a été construit, qui parcourt 3 domaines de données et construit le cadre de référence le plus large de la génération conditionnée par texte. Selon les résultats de l'évaluation, le cadre de référence TextAtlasEval pose de grands problèmes même pour les modèles de pointe (par exemple, GPT4o et DallE-3), mais montre un excellent comportement pour l'ensemble de données ouvert. Cette preuve montre que TextAtlas5M occupera un poste efficace comme ensemble de données pour l'entraînement et l'évaluation de modèles de génération d'images conditionnées par texte dans le futur.",
      "upvotes": 24,
      "discussionId": "67ad79d260ec3f444b21cd1f"
    },
    "publishedAt": "2025-02-12T23:50:07.130Z",
    "title": "TextAtlas5M: A Large-scale Dataset for Dense Text Image Generation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07870.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62333a88fd7bb4a39b92d387",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62333a88fd7bb4a39b92d387/e21AhpcXq37Ak_7rZ-Ca9.png",
      "fullname": "Alex Jinpeng Wang",
      "name": "Awiny",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.08639",
      "authors": [
        {
          "_id": "67ad5f25cad644864b436186",
          "name": "Qinghe Wang",
          "hidden": false
        },
        {
          "_id": "67ad5f25cad644864b436187",
          "name": "Yawen Luo",
          "hidden": false
        },
        {
          "_id": "67ad5f25cad644864b436188",
          "name": "Xiaoyu Shi",
          "hidden": false
        },
        {
          "_id": "67ad5f25cad644864b436189",
          "name": "Xu Jia",
          "hidden": false
        },
        {
          "_id": "67ad5f25cad644864b43618a",
          "name": "Huchuan Lu",
          "hidden": false
        },
        {
          "_id": "67ad5f25cad644864b43618b",
          "name": "Tianfan Xue",
          "hidden": false
        },
        {
          "_id": "67ad5f25cad644864b43618c",
          "name": "Xintao Wang",
          "hidden": false
        },
        {
          "_id": "67ad5f25cad644864b43618d",
          "name": "Pengfei Wan",
          "hidden": false
        },
        {
          "_id": "67ad5f25cad644864b43618e",
          "name": "Di Zhang",
          "hidden": false
        },
        {
          "_id": "67ad5f25cad644864b43618f",
          "name": "Kun Gai",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-12T18:55:36.000Z",
      "title": "Reconnaissance 3D d'images et cadre de contrôle de cadres pour la génération de vidéos à partir du texte.",
      "summary": "Dans cette étude, nous proposons un nouveau cadre de travail appelé CineMaster. Ce cadre de travail peut effectuer un reconnaissance 3D et générer des animations à partir de texte. Notre objectif est de fournir aux utilisateurs la possibilité de contrôler de manière similaire à un réalisateur professionnel. Cela permet une position précise d'objets dans les scènes, un gestionnement flexible d'objets et de caméras dans l'espace 3D, et un contrôle intuitif de la séquence de frames rendues. Pour y parvenir, CineMaster fonctionne en deux étapes. Dans la première étape, un flux de travail interactif est conçu pour permettre aux utilisateurs de construire de manière intuitive des signaux conditionnels de reconnaissance 3D. On définit les positions des boîtes de contournement des objets dans l'espace 3D et le mouvement de la caméra. Dans la deuxième étape, les signaux de contrôle (cartes de profondeur rendues, trajectoires de mouvement de la caméra, étiquettes de classe des objets) sont transformés en guides pour un modèle de diffusion d'animation à partir du texte, et les contenus d'animation que les utilisateurs souhaitent sont générés. De plus, pour surmonter la pénurie de données expliquées concernant le mouvement des objets 3D et la posture de la caméra, un plug-in d'explication automatique des données est soigneusement construit pour extraire les boîtes de contournement des objets 3D et les trajectoires de mouvement de la caméra à partir de grandes quantités de données d'animation. Les expériences détaillées et quantitatives montrent que CineMaster dépasse significativement les méthodes actuelles et réussit à générer des animations à partir de texte avec une reconnaissance 3D claire. Page du projet : https://cinemaster-dev.github.io/",
      "upvotes": 22,
      "discussionId": "67ad5f26cad644864b4361cf"
    },
    "publishedAt": "2025-02-12T21:55:44.479Z",
    "title": "CineMaster: A 3D-Aware and Controllable Framework for Cinematic Text-to-Video Generation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.08639.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6063
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.08047",
      "authors": [
        {
          "_id": "67ad92bfbbf3810ab20595c2",
          "name": "Henry Hengyuan Zhao",
          "hidden": false
        },
        {
          "_id": "67ad92bfbbf3810ab20595c3",
          "name": "Difei Gao",
          "hidden": false
        },
        {
          "_id": "67ad92bfbbf3810ab20595c4",
          "name": "Mike Zheng Shou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-12T01:06:10.000Z",
      "title": "WorldGUI : Outil de Surface Graphique Bureautique pour l'Automatisation des Tests de Flottement",
      "summary": "Actuellement, les agents de GUI basés sur la gestion des éléments de GUI atteignent un rendement exceptionnel. Cependant, ils laissent des problèmes très complexes sans résolution, en particulier une sensibilité importante à l'état initial de l'environnement, ce qui les affecte considérablement. Concrètement, les petites différences dans l'état initial, comme si le logiciel cible n'était pas ouvert ou si l'interface n'était pas dans son état par défaut, souvent provoquent des erreurs dans la planification. Ce problème est largement présent dans les scénarios réels d'utilisation, mais les benchmarks actuels ne peuvent pas évaluer cela. Dans cet article, on présente WorldGUI, un nouveau benchmark de GUI. Ce benchmark définit des tâches de GUI avec des états initiaux différents pour mimétiser l'interaction réelle entre utilisateurs et ordinateurs. Il inclut une large gamme de tâches de logiciels populaires tels que PowerPoint, VSCode et Adobe Acrobat. De plus, pour répondre aux tâches d'automatisation de GUI dynamiques, on propose un cadre généralisé appelé GUI-Thinker. Ce cadre de travail utilise des structures d'évaluation pour gérer efficacement l'incertitude et la complexité de l'interaction de GUI. À travers les résultats des expérimentations, GUI-Thinker enregistre un accroissement de 14,9% de la taux de réussite dans les tâches de WorldGUI par rapport à Claude-3.5 (Utilisation de l'ordinateur), démontrant l'efficacité de notre cadre de travail basé sur le pensée critique.",
      "upvotes": 19,
      "discussionId": "67ad92c1bbf3810ab205961c"
    },
    "publishedAt": "2025-02-13T01:39:08.775Z",
    "title": "WorldGUI: Dynamic Testing for Comprehensive Desktop GUI Automation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.08047.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "647d7eb9770c299e56f5b39b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647d7eb9770c299e56f5b39b/CC5JJgkyLkXOxw-BeT4G5.jpeg",
      "fullname": "Hengyuan Zhao",
      "name": "hhenryz",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.07563",
      "authors": [
        {
          "_id": "67ad7929dc2968691c241147",
          "user": {
            "_id": "6246bb33da617c00b48e4d92",
            "avatarUrl": "/avatars/0304a9f6eb7f5dee4d933d03222f94e9.svg",
            "isPro": false,
            "fullname": "Weigao Sun",
            "user": "weigao266",
            "type": "user"
          },
          "name": "Weigao Sun",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-13T08:21:37.445Z",
          "hidden": false
        },
        {
          "_id": "67ad7929dc2968691c241148",
          "user": {
            "_id": "66ea643899af9ac3463639b1",
            "avatarUrl": "/avatars/252d470e761a57834dee3dbc60dfefed.svg",
            "isPro": false,
            "fullname": "Disen Lan",
            "user": "landisen",
            "type": "user"
          },
          "name": "Disen Lan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-13T08:21:33.746Z",
          "hidden": false
        },
        {
          "_id": "67ad7929dc2968691c241149",
          "name": "Yiran Zhong",
          "hidden": false
        },
        {
          "_id": "67ad7929dc2968691c24114a",
          "name": "Xiaoye Qu",
          "hidden": false
        },
        {
          "_id": "67ad7929dc2968691c24114b",
          "name": "Yu Cheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-11T14:01:39.000Z",
      "title": "LASP-2 : Révision et hybride de la parallélisation séquentielle de l'attention linéaire",
      "summary": "Une approche de modélisation de séquences linéaires, comme l'attention linéaire, offre un entraînement en temps linéaire et une inférence en mémoire constante, indépendamment de la longueur de la séquence, offrant des avantages exceptionnels. Cependant, les méthodes actuelles de calcul parallèle de séquences (SP) ne sont pas optimisées pour les caractéristiques de multiplication à droite de l'attention linéaire et ne sont pas capables de gérer l'échelle dans les systèmes de dispersion de grandes séquences en raison de la faible efficacité du parallélisme et des stratégies de communication telle que le \"Style Ring\". Dans cet article, nous présentons un nouveau méthode de SP appelé LASP-2, avec l'objectif de améliorer à la fois le parallélisme de la communication et celui du calcul lors de l'entraînement de modèles de transformer avec attention linéaire et grandes séquences d'entrée. En comparaison avec LASP, LASP-2 reconnaît les minima requis de communication dans la couche d'attention linéaire et réorganise tout le flux de travail de communication et de calcul de LASP. Cela permet qu'il soit nécessaire seulement une unique communication AllGather dans l'état de mémoire indirecte, de taille indépendante de la longueur, ce qui implique un grand gain en parallélisme et en fusion de communication et calcul. De plus, nous proposons une extension de LASP-2 appelée LASP-2H, qui applique un ré-design de communication similaire aux modules d'attention standard pour fournir une solution efficace de SP pour des modèles mixtes qui combinent attention linéaire et standard. Dans l'évaluation du modèle Linear-Llama3, nous avons démontré l'efficacité de LASP-2 et LASP-2H, en particulier, LASP-2 a réussi à augmenter de 15,2% la vitesse d'entraînement par rapport à LASP et de 36,6% par rapport à l'attention de Ring, lors du traitement de séquences de longueur de 2048K sur 64 GPUs. Le code est disponible sur https://github.com/OpenSparseLLMs/Linear-MoE.",
      "upvotes": 17,
      "discussionId": "67ad792adc2968691c241173"
    },
    "publishedAt": "2025-02-12T23:47:31.651Z",
    "title": "LASP-2: Rethinking Sequence Parallelism for Linear Attention and Its Hybrid",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07563.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6246bb33da617c00b48e4d92",
      "avatarUrl": "/avatars/0304a9f6eb7f5dee4d933d03222f94e9.svg",
      "fullname": "Weigao Sun",
      "name": "weigao266",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.07346",
      "authors": [
        {
          "_id": "67ac4e046b8c86e0cc7988f0",
          "user": {
            "_id": "649d1d4c379eada9a580cf59",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649d1d4c379eada9a580cf59/ucXv7KoJDEB3Phgn-Dn5E.png",
            "isPro": false,
            "fullname": "xuhuang",
            "user": "ggdcr",
            "type": "user"
          },
          "name": "Xu Huang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-13T08:25:17.555Z",
          "hidden": false
        },
        {
          "_id": "67ac4e046b8c86e0cc7988f1",
          "name": "Wenhao Zhu",
          "hidden": false
        },
        {
          "_id": "67ac4e046b8c86e0cc7988f2",
          "name": "Hanxu Hu",
          "hidden": false
        },
        {
          "_id": "67ac4e046b8c86e0cc7988f3",
          "name": "Conghui He",
          "hidden": false
        },
        {
          "_id": "67ac4e046b8c86e0cc7988f4",
          "name": "Lei Li",
          "hidden": false
        },
        {
          "_id": "67ac4e046b8c86e0cc7988f5",
          "name": "Shujian Huang",
          "hidden": false
        },
        {
          "_id": "67ac4e046b8c86e0cc7988f6",
          "name": "Fei Yuan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-11T08:17:19.000Z",
      "title": "BenchMAX : Système d'Évaluation Détaillé Multilingue pour Modèles de Langue de Grande Échelle",
      "summary": "Antérieurement, les cadres d'évaluation multilingues se concentraient principalement sur des tâches simples de compréhension, mais les modèles de langage à grande échelle (LLMs) mettent en avant des capacités de haut niveau, comme suivre les mots, raisonner, comprendre les contextes longs et générer du code. Cependant, la mesure de ces capacités dans plusieurs langues a été peu explorée. Pour aborder ce problème, nous présentons BenchMAX, un cadre d'évaluation multilingue. Ce cadre permet une comparaison équitable de ces capacités dans plusieurs langues. Pour maintenir la qualité, les données sont traduites de l'anglais en 16 langues différentes et sont enregistrées indépendamment par 3 locuteurs natifs. De plus, nous présentons de nouveaux défis de traduction dans la construction du jeu de données. Les expériences étendues dans BenchMAX révèlent les différences effectives des capacités clés dans chaque langue et montrent que bien que le taille du modèle augmente, la différence de performance persiste. BenchMAX offre une plateforme uniforme pour l'évaluation dans plusieurs langues et fournit un benchmark de test pour encourager le développement de modèles de langage dans plusieurs langues. Le jeu de données et le code sont accessibles publiquement.",
      "upvotes": 14,
      "discussionId": "67ac4e056b8c86e0cc798952"
    },
    "publishedAt": "2025-02-13T03:34:47.873Z",
    "title": "BenchMAX: A Comprehensive Multilingual Evaluation Suite for Large Language Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07346.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "649d1d4c379eada9a580cf59",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649d1d4c379eada9a580cf59/ucXv7KoJDEB3Phgn-Dn5E.png",
      "fullname": "xuhuang",
      "name": "ggdcr",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.08127",
      "authors": [
        {
          "_id": "67ad5ca29109885ce9b859e4",
          "name": "Lingfei Qian",
          "hidden": false
        },
        {
          "_id": "67ad5ca29109885ce9b859e5",
          "name": "Weipeng Zhou",
          "hidden": false
        },
        {
          "_id": "67ad5ca29109885ce9b859e6",
          "name": "Yan Wang",
          "hidden": false
        },
        {
          "_id": "67ad5ca29109885ce9b859e7",
          "name": "Xueqing Peng",
          "hidden": false
        },
        {
          "_id": "67ad5ca29109885ce9b859e8",
          "user": {
            "_id": "63b58ed5889aa6707f0bb0f4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b58ed5889aa6707f0bb0f4/9-6SJBOLdqUoc2LrKsI6y.jpeg",
            "isPro": true,
            "fullname": "Jimin Huang",
            "user": "jiminHuang",
            "type": "user"
          },
          "name": "Jimin Huang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-02-13T02:44:52.979Z",
          "hidden": false
        },
        {
          "_id": "67ad5ca29109885ce9b859e9",
          "user": {
            "_id": "6479f4317c18dca75e9a9324",
            "avatarUrl": "/avatars/9aa709230b057f57ee4415c04a622c63.svg",
            "isPro": false,
            "fullname": "Xie",
            "user": "QianqianXie1994",
            "type": "user"
          },
          "name": "Qianqian Xie",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-13T08:22:01.539Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-12T05:13:04.000Z",
      "title": "Théorie de l'Application des Modèles de Langue Augmentée par Inférence en Finance",
      "summary": "Le développement récent de grands modèles de langue (LLMs) a démontré une capacité générale de logique puissante, mais son efficacité dans le domaine financier a été peu étudiée. Dans cette étude, trois tâches financières complexes, comprenant des textes financiers, des données de tableaux et des équations, ont été évaluées en utilisant une logique numérique, un analyse des détails de tableaux, une compréhension de termes financiers, un traitement de contextes longs et une résolution de problèmes basées sur des équations. On a utilisé 16 modèles de logique forte et générale de LLMs. Les résultats ont confirmé que l'amélioration des ensembles de données et l'amélioration de l'entraînement précédent peuvent augmenter l'efficacité dans le domaine financier. Cependant, l'ajustement de la logique de raisonnement basé sur la Théorie de la Déduction (CoT) ne produit pas toujours des effets cohérents. De plus, toutes les stratégies de logique ont des difficultés à améliorer le rendement dans des tâches de contexte long ou de plusieurs tableaux. Pour résoudre ces limitations, un modèle d'amélioration de logique financière basé sur Llama-3.1-8B-Instruct a été développé, et un entraînement par renforcement avec des ajustements de CoT et de trajectoires logiques spécifiques de raisonnement a été réalisé. Avec un seul ajustement simple dans un ensemble de données financière, le modèle a atteint un accroissement de rendement positif de 10% dans toutes les tâches, dépassant tous les modèles de 8B et, encore plus, dépassant les moyennes de Llama3-70B-Instruct et Llama3.1-70B-Instruct. Ces résultats soulignent la nécessité de changements spécifiques de raisonnement dans des tâches financières et proposent des directions futures telles que la logique multitable, le traitement de contexte long et la compréhension de termes financiers. Tous les ensembles de données, modèles et codes sont disponibles publiquement. De plus, un rincon de base de données et de modèles de tests futurs a été introduit.",
      "upvotes": 13,
      "discussionId": "67ad5ca59109885ce9b85a5b"
    },
    "publishedAt": "2025-02-12T21:45:28.944Z",
    "title": "Fino1: On the Transferability of Reasoning Enhanced LLMs to Finance",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.08127.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "63b58ed5889aa6707f0bb0f4",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b58ed5889aa6707f0bb0f4/9-6SJBOLdqUoc2LrKsI6y.jpeg",
      "fullname": "Jimin Huang",
      "name": "jiminHuang",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isMod": false,
      "followerCount": 13
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.07864",
      "authors": [
        {
          "_id": "67ad5b3a007d78b391946a57",
          "user": {
            "_id": "643f55d4ec817b766686438a",
            "avatarUrl": "/avatars/0feb460432c92ab9ada0d417a7a38f6a.svg",
            "isPro": false,
            "fullname": "mengfanxu",
            "user": "fxmeng",
            "type": "user"
          },
          "name": "Fanxu Meng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-13T08:22:03.808Z",
          "hidden": false
        },
        {
          "_id": "67ad5b3a007d78b391946a58",
          "name": "Zengwei Yao",
          "hidden": false
        },
        {
          "_id": "67ad5b3a007d78b391946a59",
          "name": "Muhan Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-11T18:20:18.000Z",
      "title": "TransMLA : Le potentiel d'attention multi-headed pourra être utile pour vous.",
      "summary": "Les grands modèles de langue modernes (LLMs) se heurtent actuellement à des problèmes de congestion de communication et sont souvent limités par des restrictions de calcul sur les matériels disponibles. La Multi-Layer Entity (MLA) est une solution qui aborde ces défis en utilisant des matrices de blocage dans les couches clé-valeur (KV), permettant ainsi que ces états potentiels compressés soient stockés dans un cache. Cette approche réduit significativement la taille du cache KV par rapport aux modèles traditionnels, améliorant à la fois la vitesse d'inférence et l'efficacité. De plus, MLA améliore l'expressivité en utilisant des matrices de projection, contribuant ainsi à réduire la congestion de communication. Cependant, MLA a démontré son efficacité et son efficacité dans des modèles comme Deepseek V2/V3/R1, mais beaucoup des principaux fournisseurs de modèles dépendent de GQA et n'ont pas de plans pour l'intégrer, ce qui limite son utilisation plus large. Dans cet article, nous montrons que GQA peut maintenir l'expressivité similaire à MLA, tandis que MLA ne serait pas possible dans le cas contraire. Pour encourager l'utilisation plus large de MLA, nous présentons **TransMLA**, un méthode de formation postérieure qui permet de transformer des modèles pré-entraînés basés sur GQA (comme LLaMA, Qwen, Mixtral) en modèles basés sur MLA. Après la transformation, les modèles peuvent améliorer leur expressivité grâce à un entraînement supplémentaire sans augmenter la taille du cache KV. De plus, nous avons développé des techniques d'optimisation de la vitesse d'inférence pour MLA qui maintiennent des temps de réponse faibles et permettent un design plus efficace du Deepseek R1.",
      "upvotes": 13,
      "discussionId": "67ad5b3b007d78b391946a79"
    },
    "publishedAt": "2025-02-12T21:41:19.791Z",
    "title": "TransMLA: Multi-head Latent Attention Is All You Need",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07864.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "643f55d4ec817b766686438a",
      "avatarUrl": "/avatars/0feb460432c92ab9ada0d417a7a38f6a.svg",
      "fullname": "mengfanxu",
      "name": "fxmeng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.08606",
      "authors": [
        {
          "_id": "67ad77f9cd8de299e5049c05",
          "name": "Dan Busbridge",
          "hidden": false
        },
        {
          "_id": "67ad77f9cd8de299e5049c06",
          "name": "Amitis Shidani",
          "hidden": false
        },
        {
          "_id": "67ad77f9cd8de299e5049c07",
          "name": "Floris Weers",
          "hidden": false
        },
        {
          "_id": "67ad77f9cd8de299e5049c08",
          "name": "Jason Ramapuram",
          "hidden": false
        },
        {
          "_id": "67ad77f9cd8de299e5049c09",
          "name": "Etai Littwin",
          "hidden": false
        },
        {
          "_id": "67ad77f9cd8de299e5049c0a",
          "name": "Russ Webb",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-12T17:52:47.000Z",
      "title": "Distitriusion Scaling Las",
      "summary": "Nous proposons des outils d'auto-réglage de résumé basés sur l'attribution de calcul entre modèles de professeur et étudiant pour prédire le rendement des modèles de résumé basés sur l'attribution de calcul. Ce que nous avons découvert nous aide à réduire les risques associés à l'utilisation du résumé en fonction de l'auto-réglage. L'attribution de calcul entre modèles de professeur et étudiant vise à maximiser le rendement du modèle étudiant. Nous offrons des recettes optimales de calcul pour le résumé lorsque le modèle de professeur existe ou nécessite d'être entraîné. Lorsque l'étudiant est multiple ou que le modèle de professeur existe déjà, le résumé améliore en termes de calcul avant que le niveau de calcul d'entraînement prédictif ne s'accroisse en relation avec la taille du modèle étudiant. En revanche, lorsque l'étudiant est unique ou que le modèle de professeur nécessite d'être entraîné, il est prioritaire d'entraîner le modèle. De plus, nous fournissons une rétroaction basée sur les résultats de nos grandes recherches sur le résumé pour approfondir la compréhension du résumé et fournir des informations pour la conception d'expériences.",
      "upvotes": 8,
      "discussionId": "67ad77fccd8de299e5049d06"
    },
    "publishedAt": "2025-02-12T23:41:41.281Z",
    "title": "Distillation Scaling Laws",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.08606.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6063
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.08168",
      "authors": [
        {
          "_id": "67ad5f32d1a5243cc4fa38ad",
          "user": {
            "_id": "64a0ed5ed5374ca472cfb0ac",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a0ed5ed5374ca472cfb0ac/n_wXamXfR_PPn0hRbnR1X.jpeg",
            "isPro": false,
            "fullname": "ZhimingMa",
            "user": "JimmyMa99",
            "type": "user"
          },
          "name": "Zhiming Ma",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-13T08:21:57.239Z",
          "hidden": false
        },
        {
          "_id": "67ad5f32d1a5243cc4fa38ae",
          "name": "Xiayang Xiao",
          "hidden": false
        },
        {
          "_id": "67ad5f32d1a5243cc4fa38af",
          "name": "Sihao Dong",
          "hidden": false
        },
        {
          "_id": "67ad5f32d1a5243cc4fa38b0",
          "name": "Peidong Wang",
          "hidden": false
        },
        {
          "_id": "67ad5f32d1a5243cc4fa38b1",
          "name": "HaiPeng Wang",
          "hidden": false
        },
        {
          "_id": "67ad5f32d1a5243cc4fa38b2",
          "name": "Qingyun Pan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-12T07:19:36.000Z",
      "title": "SARChat-Bench-2M : Interprétation d'un benchmark de langage vision multi-tâche pour des images SAR",
      "summary": "Dans le domaine de l'interprétation des images radar de type aperture synthétique (SAR), les modèles de langage de vision (VLMs) ont réalisé une révolution impressionnante dans le traitement du langage naturel et la compréhension des images, mais leur application dans le cadre des projets est limitée en raison du manque de connaissances locales. Dans cet article, nous proposons un premier ensemble de données de dialogues de types multiples à grande échelle. Cet ensemble de données comprend environ 2 millions de paires d'images de qualité élevée et de texte, avec des descriptions détaillées des objectifs et une variété d'échantillons. Cet ensemble de données soutient des tâches importantes telles que la compréhension visuelle et la détection d'objets, et présente une innovation unique : il est développé dans cet article un ensemble de données de langage de vision pour le domaine de SAR et un benchmark, ce qui permet d'évaluer la capacité d'interprétation des images SAR des VLMs, et fournit un cadre pour la construction d'ensembles de données de types multiples dans divers domaines d'observation à distance. À travers des expériences avec 16 VLMs principaux, nous démontrons complètement l'effet de cet ensemble de données, et nous établissons avec succès le premier benchmark de dialogue de tâches de types multiples dans le domaine de SAR. Ce projet est lancé sur https://github.com/JimmyMa99/SARChat, avec l'objectif de promouvoir le développement profond et l'application large des modèles de langage de vision pour le domaine de SAR.",
      "upvotes": 8,
      "discussionId": "67ad5f37d1a5243cc4fa399c"
    },
    "publishedAt": "2025-02-12T21:57:30.420Z",
    "title": "SARChat-Bench-2M: A Multi-Task Vision-Language Benchmark for SAR Image Interpretation",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64a0ed5ed5374ca472cfb0ac/LvHzRQCttMAvKS-LM0ZDH.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.08168.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "64a0ed5ed5374ca472cfb0ac",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a0ed5ed5374ca472cfb0ac/n_wXamXfR_PPn0hRbnR1X.jpeg",
      "fullname": "ZhimingMa",
      "name": "JimmyMa99",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.08524",
      "authors": [
        {
          "_id": "67ad783da2808b57a3cd3316",
          "name": "Jihoon Tack",
          "hidden": false
        },
        {
          "_id": "67ad783da2808b57a3cd3317",
          "name": "Jack Lanchantin",
          "hidden": false
        },
        {
          "_id": "67ad783da2808b57a3cd3318",
          "name": "Jane Yu",
          "hidden": false
        },
        {
          "_id": "67ad783da2808b57a3cd3319",
          "name": "Andrew Cohen",
          "hidden": false
        },
        {
          "_id": "67ad783da2808b57a3cd331a",
          "name": "Ilia Kulikov",
          "hidden": false
        },
        {
          "_id": "67ad783da2808b57a3cd331b",
          "name": "Janice Lan",
          "hidden": false
        },
        {
          "_id": "67ad783da2808b57a3cd331c",
          "name": "Shibo Hao",
          "hidden": false
        },
        {
          "_id": "67ad783da2808b57a3cd331d",
          "name": "Yuandong Tian",
          "hidden": false
        },
        {
          "_id": "67ad783da2808b57a3cd331e",
          "name": "Jason Weston",
          "hidden": false
        },
        {
          "_id": "67ad783da2808b57a3cd331f",
          "user": {
            "_id": "659a395421a7431643caedda",
            "avatarUrl": "/avatars/c1e0bbcedce68fe3b4fe39e0cf01c65c.svg",
            "isPro": false,
            "fullname": "Xian Li",
            "user": "xlxxl",
            "type": "user"
          },
          "name": "Xian Li",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-02-13T04:42:38.302Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-12T16:00:11.000Z",
      "title": "**Traduction en Français :**\n\"Exercices de formation des concepts continus pour les LM\"\n\n**Note :**\n- \"LM\" est conservé comme \"LM\" pour préserver l'abréviation courante dans le domaine de l'intelligence artificielle.\n- \"Continu\" est traduit par \"continu\" pour maintenir la précision technique.\n- \"Concepts\" est traduit par \"concepts\" pour refléter le sens de \"concepts\" en français.",
      "summary": "L'ensemble suivant de prédictions de tokens a été utilisé de manière constante comme un objectif de formation standard dans l'apprentissage pré-entraîné des modèles de langage grands. Les erreurs structurelles sont optimisées au niveau de token pour apprendre des représentations. Nous proposons un nouveau cadre d'apprentissage pré-entraîné \"CoCoMix\". Ce cadre prédit des concepts continus dans un autoencodeur épars préalablement entraîné et mélange les représentations cachées des tokens avec l'état caché du modèle. Selon plusieurs benchmarks (modélisation de langage et tâches de théorie de la raison), CoCoMix montre une efficacité de l'échantillon élevée et dépasse toujours la prédiction de token suivant, la propagation de connaissances et l'insertion de tokens postérieurs. Nous avons constaté que la combinaison d'apprentissage de concepts et de croisements dans les cadres d'apprentissage est cruciale pour améliorer le rendement. De plus, CoCoMix permet une révision et un ajustement directs des concepts prédits, et fournit une transparence aux processus de raisonnement interne du modèle, améliorant son interprétabilité et sa manipulabilité.",
      "upvotes": 6,
      "discussionId": "67ad783ea2808b57a3cd3361"
    },
    "publishedAt": "2025-02-12T23:42:44.287Z",
    "title": "LLM Pretraining with Continuous Concepts",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.08524.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6063
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.06533",
      "authors": [
        {
          "_id": "67accc647e1fcf03e14b1033",
          "user": {
            "_id": "6637cd3e691043ccb248d0fd",
            "avatarUrl": "/avatars/94cf09cf817327be50ecba75f7f60fa1.svg",
            "isPro": false,
            "fullname": "Jean Vassoyan",
            "user": "supertardigrade",
            "type": "user"
          },
          "name": "Jean Vassoyan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-13T08:24:24.993Z",
          "hidden": false
        },
        {
          "_id": "67accc647e1fcf03e14b1034",
          "user": {
            "_id": "63da60458658cbc1cc489bd7",
            "avatarUrl": "/avatars/620ce7ea229de7abe4dc9ea93021f0e4.svg",
            "isPro": false,
            "fullname": "Nathanaël Beau",
            "user": "Nbeau",
            "type": "user"
          },
          "name": "Nathanaël Beau",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-02-12T16:29:25.829Z",
          "hidden": false
        },
        {
          "_id": "67accc647e1fcf03e14b1035",
          "user": {
            "_id": "66470e227d73a39a342866e4",
            "avatarUrl": "/avatars/cb0746295492044c483a470692b9637c.svg",
            "isPro": false,
            "fullname": "Roman Plaud",
            "user": "lecraquito",
            "type": "user"
          },
          "name": "Roman Plaud",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-13T08:24:27.330Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-10T14:56:25.000Z",
      "title": "Ignorer la pénalité KL et renforcer la recherche de tokens importants pour améliorer l'ajustement micro du RL.",
      "summary": "La capacité d'atteindre des objectifs à long terme est un problème important dans le développement des grands modèles de langue (LLMs). Pour le résoudre, on peut effectuer un apprentissage par renforcement (RL) pour ajuster les modèles. Cependant, l'exploration des LLMs est très difficile, ce qui fait que, malgré la recherche de nouvelles solutions, on ne peut pas réduire significativement la distance entre les modèles ajustés et les objectifs. En général, on contrôle cela par une pénalisation de Kullback-Leibler (KL). Dans cet article, on étudie la dynamique de l'exploration d'un petit modèle de langue qui effectue des tâches arithmétiques simples, et on montre comment l'apprentissage par renforcement affecte l'exploration et l'importance des \"tokens KL\" qui ont un impact considérable sur les résultats finaux. En conséquence, on propose une amélioration simple de la pénalisation de KL pour optimiser le processus d'apprentissage par renforcement pendant la phase d'ajustement.",
      "upvotes": 5,
      "discussionId": "67accc657e1fcf03e14b109e"
    },
    "publishedAt": "2025-02-13T03:47:28.654Z",
    "title": "Ignore the KL Penalty! Boosting Exploration on Critical Tokens to Enhance RL Fine-Tuning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06533.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66470e227d73a39a342866e4",
      "avatarUrl": "/avatars/cb0746295492044c483a470692b9637c.svg",
      "fullname": "Roman Plaud",
      "name": "lecraquito",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.06145",
      "authors": [
        {
          "_id": "67ad9fb9731ff0d7da9f40e9",
          "user": {
            "_id": "67ad9f06040354c9105b00bc",
            "avatarUrl": "/avatars/39e9f4c48c93bb33f155390653936fc1.svg",
            "isPro": false,
            "fullname": "LiHu",
            "user": "Hookszdp",
            "type": "user"
          },
          "name": "Li Hu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-13T08:21:24.286Z",
          "hidden": false
        },
        {
          "_id": "67ad9fb9731ff0d7da9f40ea",
          "name": "Guangyuan Wang",
          "hidden": false
        },
        {
          "_id": "67ad9fb9731ff0d7da9f40eb",
          "name": "Zhen Shen",
          "hidden": false
        },
        {
          "_id": "67ad9fb9731ff0d7da9f40ec",
          "name": "Xin Gao",
          "hidden": false
        },
        {
          "_id": "67ad9fb9731ff0d7da9f40ed",
          "name": "Dechao Meng",
          "hidden": false
        },
        {
          "_id": "67ad9fb9731ff0d7da9f40ee",
          "name": "Lian Zhuo",
          "hidden": false
        },
        {
          "_id": "67ad9fb9731ff0d7da9f40ef",
          "name": "Peng Zhang",
          "hidden": false
        },
        {
          "_id": "67ad9fb9731ff0d7da9f40f0",
          "name": "Bang Zhang",
          "hidden": false
        },
        {
          "_id": "67ad9fb9731ff0d7da9f40f1",
          "name": "Liefeng Bo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-10T04:20:11.000Z",
      "title": "Animat・Nyanny 2 : Animation d'images de personnages de haute qualité en utilisant le complément d'environnement",
      "summary": "Récemment, on a observé le développement de techniques telles que Animate Anyone dans le domaine de l'animation de cartes de caractéristiques basées sur des modèles de diffusion. Cependant, ces technologies parfois rencontrent des difficultés à créer des connexions logiques entre les cartes de caractéristiques et l'environnement. Pour aborder ce problème, nous présentons Animate Anyone 2, un méthode qui se concentre sur l'animation des cartes de caractéristiques en fonction des fonctions de l'environnement. Pour y parvenir, nous extrayon des signaux de mouvements à partir de la source vidéo, mais nous incluons également la représentation de l'environnement comme entrée conditionnelle. L'environnement est configuré comme une zone qui ne comprend pas les cartes de caractéristiques, permettant ainsi la création de cartes de caractéristiques qui conservent la cohérence avec le contexte de l'environnement. Pour exprimer efficacement la relation entre les cartes de caractéristiques et l'environnement, nous proposons une stratégie de masques sans restrictions de forme. De plus, pour améliorer la précision de l'interaction entre objets, nous utilisons des guides d'objets pour extraire des caractéristiques d'objets qui interagissent et, par spectral branding, nous injectons ces caractéristiques. Nous proposons également une stratégie pour ajuster la posture des objets qui peut gérer différents modèles de mouvement. Nos résultats expérimentaux démontrent la performance exceptionnelle de notre méthode.",
      "upvotes": 3,
      "discussionId": "67ad9fbb731ff0d7da9f4145"
    },
    "publishedAt": "2025-02-13T03:45:43.646Z",
    "title": "Animate Anyone 2: High-Fidelity Character Image Animation with Environment Affordance",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06145.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67ad9f06040354c9105b00bc",
      "avatarUrl": "/avatars/39e9f4c48c93bb33f155390653936fc1.svg",
      "fullname": "LiHu",
      "name": "Hookszdp",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.06872",
      "authors": [
        {
          "_id": "67ad7da995ff670869168209",
          "name": "Bo Ni",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff67086916820a",
          "name": "Zheyuan Liu",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff67086916820b",
          "name": "Leyao Wang",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff67086916820c",
          "name": "Yongjia Lei",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff67086916820d",
          "name": "Yuying Zhao",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff67086916820e",
          "name": "Xueqi Cheng",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff67086916820f",
          "name": "Qingkai Zeng",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff670869168210",
          "name": "Luna Dong",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff670869168211",
          "name": "Yinglong Xia",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff670869168212",
          "name": "Krishnaram Kenthapadi",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff670869168213",
          "name": "Ryan Rossi",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff670869168214",
          "user": {
            "_id": "62c5947524171688a9feb992",
            "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
            "isPro": false,
            "fullname": "Franck Dernoncourt",
            "user": "Franck-Dernoncourt",
            "type": "user"
          },
          "name": "Franck Dernoncourt",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-13T08:21:27.740Z",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff670869168215",
          "name": "Md Mehrab Tanjim",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff670869168216",
          "name": "Nesreen Ahmed",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff670869168217",
          "name": "Xiaorui Liu",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff670869168218",
          "name": "Wenqi Fan",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff670869168219",
          "name": "Erik Blasch",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff67086916821a",
          "name": "Yu Wang",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff67086916821b",
          "name": "Meng Jiang",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff67086916821c",
          "name": "Tyler Derr",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-08T06:50:47.000Z",
      "title": "Trastorrería Stretchiauwa Garshion Pollarr Jowng Ya Iant Rng Jue Esion Modulo: Asid",
      "summary": "Le Rétorique de la Révolution Agricole (RAG) est une technologie avancée conçue pour résoudre les problèmes de contenu généré par l'intelligence artificielle (AIGC). En intégrant la recherche par catégorie dans la génération de contenu, le RAG fournit des connaissances externes et fiables, réduit le bruit irrélevant et garantit la pertinence dans une large gamme de tâches. Cependant, le paradigme du RAG a rencontré de nouveaux risques, notamment des problèmes de robustesse, des préoccupations de confidentialité, des attaques, et des questions de responsabilité, indépendamment de son succès et de ses possibilités. La résolution de ces risques est cruciale pour l'application future du RAG et sa confiance. Des méthodes pour augmenter la confiance dans le RAG ont été développées, mais manque une vision et un cadre structuré uniforme dans ce domaine de recherche. Par conséquent, cet article vise à fournir une guidance détaillée pour le développement de systèmes RAG fiables. Le discours se concentre sur cinq aspects principaux : confiance, confidentialité, sécurité, équité, explicabilité et problèmes de responsabilité. Dans chaque aspect, un cadre général et des technologies sont fournis pour comprendre les problèmes actuels, évaluer les solutions existantes et déterminer les directions futures de recherche. De plus, des applications ayant un grand impact sur l'introduction et l'innovation de systèmes RAG fiables sont mentionnées spécifiquement.",
      "upvotes": 3,
      "discussionId": "67ad7daa95ff670869168251"
    },
    "publishedAt": "2025-02-13T00:06:04.056Z",
    "title": "Towards Trustworthy Retrieval Augmented Generation for Large Language Models: A Survey",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06872.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62c5947524171688a9feb992",
      "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
      "fullname": "Franck Dernoncourt",
      "name": "Franck-Dernoncourt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.05167",
      "authors": [
        {
          "_id": "67aa583c3a878652daeae02e",
          "user": {
            "_id": "60e4738a8c0ddd18fc27ff88",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60e4738a8c0ddd18fc27ff88/lpLeeIW8r85RTY4fGZTva.jpeg",
            "isPro": false,
            "fullname": "Ali Modarressi",
            "user": "amodaresi",
            "type": "user"
          },
          "name": "Ali Modarressi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T07:55:42.560Z",
          "hidden": false
        },
        {
          "_id": "67aa583c3a878652daeae02f",
          "name": "Hanieh Deilamsalehy",
          "hidden": false
        },
        {
          "_id": "67aa583c3a878652daeae030",
          "user": {
            "_id": "62c5947524171688a9feb992",
            "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
            "isPro": false,
            "fullname": "Franck Dernoncourt",
            "user": "Franck-Dernoncourt",
            "type": "user"
          },
          "name": "Franck Dernoncourt",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-13T08:26:01.327Z",
          "hidden": false
        },
        {
          "_id": "67aa583c3a878652daeae031",
          "name": "Trung Bui",
          "hidden": false
        },
        {
          "_id": "67aa583c3a878652daeae032",
          "name": "Ryan A. Rossi",
          "hidden": false
        },
        {
          "_id": "67aa583c3a878652daeae033",
          "name": "Seunghyun Yoon",
          "hidden": false
        },
        {
          "_id": "67aa583c3a878652daeae034",
          "name": "Hinrich Schütze",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-07T18:49:46.000Z",
      "title": "NoLiMa : Évaluation du contexte plutôt qu'une réponse directe textuelle",
      "summary": "Récemment, les grands modèles de langage (LLMs) soutiennent des contextes longs de 128K à 1M tokens. Pour évaluer ces fonctionnalités, un méthode populaire est le \"Nail-In-Up-Lion\" (NIAH). Cette méthode consiste à chercher des informations liées (「nail」) dans un contexte long irrélevant (「sack」). Cette méthode a été étendue pour inclure des interventions, des cycles de vérité et de logique dans le texte. Cependant, les modèles peuvent simplifier le problème en utilisant les correspondances contextuelles entre le \"nail\" et le \"sack\". Pour résoudre ce problème, nous avons introduit le benchmark \"NoLiMa\", qui est une extension du NIAH. Ce benchmark a été conçu de manière à ce que les problèmes et le \"nail\" aient un minimum de répétition de mots, et que le modèle doit inférer la position du \"nail\" dans le \"sack\" en utilisant des cycles potentiels. Nous évaluons actuellement 12 modèles LLMs populaires. Ces modèles fonctionnent bien avec des contextes courts (<1K), mais leur performance diminue considérablement lorsque le contexte est plus long. Par exemple, seuls 5 sur 10 modèles dépassent le 50% d'une ligne de base forte basée sur des contextes courts dans un contexte de 32K. De plus, GPT-4o est une exception notable, et sa performance baisse de presque 100% (99,3%) à 69,7%. Notre analyse suggère que la structure d'action utilisant le contexte devient plus difficile et que la recherche d'informations pertinentes devient plus complexe lorsque le contexte est plus long.",
      "upvotes": 3,
      "discussionId": "67aa583d3a878652daeae06c"
    },
    "publishedAt": "2025-02-13T00:04:29.194Z",
    "title": "NoLiMa: Long-Context Evaluation Beyond Literal Matching",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05167.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62c5947524171688a9feb992",
      "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
      "fullname": "Franck Dernoncourt",
      "name": "Franck-Dernoncourt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.07737",
      "authors": [
        {
          "_id": "67ad5d2f8436e8ea7abb7a15",
          "name": "Shuhuai Ren",
          "hidden": false
        },
        {
          "_id": "67ad5d2f8436e8ea7abb7a16",
          "name": "Shuming Ma",
          "hidden": false
        },
        {
          "_id": "67ad5d2f8436e8ea7abb7a17",
          "name": "Xu Sun",
          "hidden": false
        },
        {
          "_id": "67ad5d2f8436e8ea7abb7a18",
          "name": "Furu Wei",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-11T17:57:53.000Z",
      "title": "Prédiction de Blocs : Génération de Vidéo par Modélisation de Régression Automatique Semi-Manuelle",
      "summary": "Le méthode de Prédiction de Tokens Futurs (PTF) a été utilisée dans la génération automatique de vidéos, mais présente des problèmes tels qu'une dépendance unidirectionnelle et un lent taux d'inférence. Dans cet article, nous proposons un cadre semi-automatique de récupération (semi-AR) pour la génération de vidéos, appelé Next-Block Prediction (NBP). Le contenu de la vidéo est divisé en blocs de même dimension et la unité de génération est modifiée pour que chaque token dans un bloc puisse prédire le token correspondant dans le bloc suivant en même temps. Au contraire des modèles traditionnels d'AR, une attention bidirectionnelle est utilisée à l'intérieur de chaque bloc, permettant de capturer une forte dépendance spatiale. En prédisant plusieurs tokens en parallèle, le modèle NBP réduit considérablement le processus de génération et atteint une inférence rapide et efficace. Sur UCF101, le modèle atteint un FVD de 103,3 et sur K600 un FVD de 25,5, dépassant les modèles PTF d'un moyen de 4,4 points. De plus, en réduisant le temps d'inférence, le modèle NBP génère 8,89 cadences par seconde à une résolution de 128x128, atteignant une vitesse de travail 11 fois plus rapide. De plus, la scalabilité du modèle est démontrée en examinant un intervalle de paramètres de 700M à 3B, montrant un grand améliorament de la qualité de génération, avec un FVD de 55,3 sur UCF101 et de 19,5 sur K600, démontrant sa capacité à s'étendre.",
      "upvotes": 3,
      "discussionId": "67ad5d308436e8ea7abb7a3d"
    },
    "publishedAt": "2025-02-12T21:48:00.325Z",
    "title": "Next Block Prediction: Video Generation via Semi-Autoregressive Modeling",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07737.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60d2e681b8448e1785bbda06",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1624434302056-noauth.jpeg",
      "fullname": "Shuhuai Ren",
      "name": "ShuhuaiRen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.07599",
      "authors": [
        {
          "_id": "67ad5bd2ac32a8e230fc8996",
          "name": "Xiliang Yang",
          "hidden": false
        },
        {
          "_id": "67ad5bd2ac32a8e230fc8997",
          "name": "Feng Jiang",
          "hidden": false
        },
        {
          "_id": "67ad5bd2ac32a8e230fc8998",
          "name": "Qianen Zhang",
          "hidden": false
        },
        {
          "_id": "67ad5bd2ac32a8e230fc8999",
          "name": "Lei Zhao",
          "hidden": false
        },
        {
          "_id": "67ad5bd2ac32a8e230fc899a",
          "name": "Xiao Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-11T14:49:44.000Z",
      "title": "DPO-Shift : Optimisation de la Distribution de la Préférence Directe",
      "summary": "L'optimisation directe des préférences (Direct Preference Optimization, DPO) et ses variantes sont en train de gagner de l'appréciation progressive pour ajuster les modèles de langage aux préférences humaines. Ces méthodes visent à améliorer la différenciation entre ce qui est choisi (ou préféré) et ce qui est rejeté (ou non préféré) par le modèle. Cependant, des études précédentes ont observé que la probabilité de sélection diminue pendant l'entraînement, un phénomène connu sous le nom de \"changement de fréquence\". Pour résoudre ce problème, cette étude utilise une méthodologie pour déformer la distribution de la probabilité de sélection. Cette approche, analysée tant théoriquement qu'expérimentalement, a démontré être une transformation fondamentale qui améliore la probabilité de sélection sans perdre de la récompense, révélant ainsi son importance. De plus, ce méthode dépasse la DPO dans des tâches comme MT-Bench, qui évaluent les probabilités conçues. Nous croyons que cette étude montre que le problème de changement de fréquence dans la DPO peut être efficacement mitigé par une solution simple basée sur la théorie. Le code est disponible sur https://github.com/Meaquadddd/DPO-Shift.",
      "upvotes": 3,
      "discussionId": "67ad5bd3ac32a8e230fc89a7"
    },
    "publishedAt": "2025-02-12T21:43:42.404Z",
    "title": "DPO-Shift: Shifting the Distribution of Direct Preference Optimization",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07599.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66270fcef7cf69d4223a8a3f",
      "avatarUrl": "/avatars/115db0326737e65318c92a7b8dc5ed6a.svg",
      "fullname": "Xiao Li",
      "name": "xli0982",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.04411",
      "authors": [
        {
          "_id": "67adad972883187d78409a7a",
          "name": "Kunfeng Lai",
          "hidden": false
        },
        {
          "_id": "67adad972883187d78409a7b",
          "name": "Zhenheng Tang",
          "hidden": false
        },
        {
          "_id": "67adad972883187d78409a7c",
          "name": "Xinglin Pan",
          "hidden": false
        },
        {
          "_id": "67adad972883187d78409a7d",
          "name": "Peijie Dong",
          "hidden": false
        },
        {
          "_id": "67adad972883187d78409a7e",
          "user": {
            "_id": "63024676056ec3a2a8714b24",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1661093436322-noauth.jpeg",
            "isPro": false,
            "fullname": "Xiang Liu",
            "user": "Dominic789654",
            "type": "user"
          },
          "name": "Xiang Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-13T08:45:17.030Z",
          "hidden": false
        },
        {
          "_id": "67adad972883187d78409a7f",
          "name": "Haolan Chen",
          "hidden": false
        },
        {
          "_id": "67adad972883187d78409a80",
          "name": "Li Shen",
          "hidden": false
        },
        {
          "_id": "67adad972883187d78409a81",
          "name": "Bo Li",
          "hidden": false
        },
        {
          "_id": "67adad972883187d78409a82",
          "name": "Xiaowen Chu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-06T11:26:30.000Z",
      "title": "Media Tar: Conflits de Paramètres Réduits et Mémoire Efficace Basée sur la Confiance des LLM Intégrés",
      "summary": "L'intégration de modèles se fait en renforçant des modèles grands de langage (LLMs) fine-tunés pour diverses tâches. Cependant, le collisionnement de paramètres est lié à une perte de performance moyenne. La routine de modèles sélectionne individuellement des modèles pendant l'inférence pour résoudre ce problème, mais cela génère des coûts de stockage et de calculs excessifs et ne profite pas des connaissances partagées d'autres modèles. Dans cette étude, on observe que le degré de collisionnement de paramètres varie selon les couches, et on se base sur cela pour faire une moyenne des couches avec moins de collisions et utiliser des routines d'experts au niveau de tâche pour les couches avec plus de collisions. Pour réduire encore plus les coûts de stockage, on utilise l'esparsité mathématique des tâches pour séparer plusieurs experts fine-tunés en un expert parfait et en plusieurs experts épars. On considère des échantillons hors de la distribution et on sélectionne un expert approprié en fonction de l'incertitude des tâches d'entrée pour leur intégration. On teste dans des tâches d'inférence réelle de LLaMA et Qwen avec des tailles de paramètres différentes et on évalue dans des tâches d'inférence réelle. Les résultats montrent que l'on peut obtenir une amélioration du rendement avec un coût du système plus faible par rapport aux méthodes actuelles.",
      "upvotes": 2,
      "discussionId": "67adad992883187d78409aa8"
    },
    "publishedAt": "2025-02-13T03:30:35.137Z",
    "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04411.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63024676056ec3a2a8714b24",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1661093436322-noauth.jpeg",
      "fullname": "Xiang Liu",
      "name": "Dominic789654",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.07985",
      "authors": [
        {
          "_id": "67ad9577b469222e0df18134",
          "user": {
            "_id": "5fad8602b8423e1d80b8a965",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5fad8602b8423e1d80b8a965/tRqTwcZmrGka8c1vFq2wX.jpeg",
            "isPro": false,
            "fullname": "Victor Gallego",
            "user": "vicgalle",
            "type": "user"
          },
          "name": "Víctor Gallego",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-02-13T06:47:20.731Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-11T22:06:25.000Z",
      "title": "MetaSC : Modèle de langage pour optimiser le spectre de sécurité dans le test",
      "summary": "Nous proposons un nouveau cadre dynamique de sécurité pour optimiser la logique de sécurité des modèles de langage (LM) sans modifier les poids du modèle. Basé sur le développement récent des méthodes d'auto-évaluation, notre approche utilise une structure de méta-évaluation qui met à jour répétitivement les Prompts de sécurité (un peu comme des 'regles') de manière adaptative, permettant ainsi que le processus d'auto-évaluation et de modification se développe de manière dynamique. Ce processus optimisé améliore le rendement face aux demandes hostiles comme les 'brakes de zorro', évite les dommages moraux dans les tâches générales de sécurité, et exige des réponses véritables pour améliorer la sécurité. Les expériences sur divers modèles de langage montrent que les Prompts de sécurité optimisés dynamiquement montrent un niveau de score de sécurité significativement plus élevé que les Prompts fixes du système et les défenses d'auto-évaluation statiques. Le code est disponible sur https://github.com/vicgalle/meta-self-critique.git.",
      "upvotes": 1,
      "discussionId": "67ad9578b469222e0df18162"
    },
    "publishedAt": "2025-02-13T01:47:30.377Z",
    "title": "MetaSC: Test-Time Safety Specification Optimization for Language Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07985.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5fad8602b8423e1d80b8a965",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5fad8602b8423e1d80b8a965/tRqTwcZmrGka8c1vFq2wX.jpeg",
      "fullname": "Victor Gallego",
      "name": "vicgalle",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 117
    },
    "isAuthorParticipating": true
  }
]