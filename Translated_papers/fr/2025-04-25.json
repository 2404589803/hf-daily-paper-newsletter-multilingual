[
  {
    "paper": {
      "id": "2504.17761",
      "authors": [
        {
          "_id": "680af2df3b93130c9b2b90a7",
          "name": "Shiyu Liu",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90a8",
          "name": "Yucheng Han",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90a9",
          "name": "Peng Xing",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90aa",
          "name": "Fukun Yin",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90ab",
          "name": "Rui Wang",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90ac",
          "user": {
            "_id": "64b914c8ace99c0723ad83a9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b914c8ace99c0723ad83a9/udUHjj6fby82zh8LDjXhL.jpeg",
            "isPro": false,
            "fullname": "Wei Cheng",
            "user": "wchengad",
            "type": "user"
          },
          "name": "Wei Cheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-25T08:34:36.757Z",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90ad",
          "name": "Jiaqi Liao",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90ae",
          "name": "Yingming Wang",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90af",
          "name": "Honghao Fu",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90b0",
          "name": "Chunrui Han",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90b1",
          "name": "Guopeng Li",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90b2",
          "name": "Yuang Peng",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90b3",
          "name": "Quan Sun",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90b4",
          "name": "Jingwei Wu",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90b5",
          "name": "Yan Cai",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90b6",
          "name": "Zheng Ge",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90b7",
          "name": "Ranchen Ming",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90b8",
          "name": "Lei Xia",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90b9",
          "name": "Xianfang Zeng",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90ba",
          "name": "Yibo Zhu",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90bb",
          "name": "Binxing Jiao",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90bc",
          "name": "Xiangyu Zhang",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90bd",
          "user": {
            "_id": "63417332c5565a4b8d43a0d8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63417332c5565a4b8d43a0d8/MmnYG7Wu2Z_lqCBvTfJmy.png",
            "isPro": false,
            "fullname": "Gang Yu",
            "user": "skicy",
            "type": "user"
          },
          "name": "Gang Yu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-25T08:34:34.650Z",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90be",
          "name": "Daxin Jiang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64b914c8ace99c0723ad83a9/lRHqqMDr1SxDfhelcO26J.mp4"
      ],
      "publishedAt": "2025-04-24T17:25:12.000Z",
      "submittedOnDailyAt": "2025-04-25T01:12:44.269Z",
      "title": "Step 1X-Edit : Procédure pratique d'édition d'images générales",
      "submittedOnDailyBy": {
        "_id": "64b914c8ace99c0723ad83a9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b914c8ace99c0723ad83a9/udUHjj6fby82zh8LDjXhL.jpeg",
        "isPro": false,
        "fullname": "Wei Cheng",
        "user": "wchengad",
        "type": "user"
      },
      "summary": "Récemment, les modèles d'édition d'images montrent un développement impressionnant et rapide. La publication de modèles avancés comme GPT-4o et Gemini2 Flash a introduit des capacités élevées d'édition d'images. Ces modèles montrent une excellente capacité à satisfaire les demandes d'édition que les utilisateurs requièrent et ont démontré un grand avancement dans le domaine de la manipulation d'images. Cependant, il existe des différences notables par rapport aux modèles fermés. Par conséquent, dans cet article, nous présentons Step1X-Edit, le modèle d'édition d'images le plus avancé, avec l'objectif de fournir un rendement comparable aux modèles fermés. Concrètement, les images de référence et les instructions d'édition du utilisateur sont traitées par un Multimodal LLM, des embbedings potentiels sont extraits et intégrés avec un décodeur d'images de différence pour obtenir l'image cible. Pour l'entraînement du modèle, une pipeline de génération de données de haute qualité a été construite, et pour l'évaluation, GEdit-Bench, un nouveau critère d'évaluation basé sur des instructions utilisateurs réels, a été développé. Les résultats des tests sur GEdit-Bench montrent que Step1X-Edit se distingue significativement par rapport aux limites de base des modèles fermés et approche les performances des modèles avancés fermés, contribuant de manière significative au domaine de l'édition d'images.",
      "upvotes": 38,
      "discussionId": "680af2e13b93130c9b2b9132",
      "githubRepo": "https://github.com/stepfun-ai/Step1X-Edit",
      "ai_keywords": [
        "Multimodal LLM",
        "latent embedding",
        "diffusion image decoder",
        "data generation pipeline",
        "GEdit-Bench",
        "real-world user instructions"
      ]
    },
    "publishedAt": "2025-04-24T13:25:12.000Z",
    "title": "Step1X-Edit: A Practical Framework for General Image Editing",
    "summary": "In recent years, image editing models have witnessed remarkable and rapid\ndevelopment. The recent unveiling of cutting-edge multimodal models such as\nGPT-4o and Gemini2 Flash has introduced highly promising image editing\ncapabilities. These models demonstrate an impressive aptitude for fulfilling a\nvast majority of user-driven editing requirements, marking a significant\nadvancement in the field of image manipulation. However, there is still a large\ngap between the open-source algorithm with these closed-source models. Thus, in\nthis paper, we aim to release a state-of-the-art image editing model, called\nStep1X-Edit, which can provide comparable performance against the closed-source\nmodels like GPT-4o and Gemini2 Flash. More specifically, we adopt the\nMultimodal LLM to process the reference image and the user's editing\ninstruction. A latent embedding has been extracted and integrated with a\ndiffusion image decoder to obtain the target image. To train the model, we\nbuild a data generation pipeline to produce a high-quality dataset. For\nevaluation, we develop the GEdit-Bench, a novel benchmark rooted in real-world\nuser instructions. Experimental results on GEdit-Bench demonstrate that\nStep1X-Edit outperforms existing open-source baselines by a substantial margin\nand approaches the performance of leading proprietary models, thereby making\nsignificant contributions to the field of image editing.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64b914c8ace99c0723ad83a9/lRHqqMDr1SxDfhelcO26J.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17761.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b914c8ace99c0723ad83a9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b914c8ace99c0723ad83a9/udUHjj6fby82zh8LDjXhL.jpeg",
      "fullname": "Wei Cheng",
      "name": "wchengad",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.17502",
      "authors": [
        {
          "_id": "680b44fb426b7d5bc2018c75",
          "user": {
            "_id": "631da07f6d6a5870f3d2c375",
            "avatarUrl": "/avatars/242e344dca08057bdf1eef09f69b41b2.svg",
            "isPro": false,
            "fullname": "Aviv Slobodkin",
            "user": "lovodkin93",
            "type": "user"
          },
          "name": "Aviv Slobodkin",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-25T08:17:03.471Z",
          "hidden": false
        },
        {
          "_id": "680b44fb426b7d5bc2018c76",
          "name": "Hagai Taitelbaum",
          "hidden": false
        },
        {
          "_id": "680b44fb426b7d5bc2018c77",
          "name": "Yonatan Bitton",
          "hidden": false
        },
        {
          "_id": "680b44fb426b7d5bc2018c78",
          "name": "Brian Gordon",
          "hidden": false
        },
        {
          "_id": "680b44fb426b7d5bc2018c79",
          "name": "Michal Sokolik",
          "hidden": false
        },
        {
          "_id": "680b44fb426b7d5bc2018c7a",
          "name": "Nitzan Bitton Guetta",
          "hidden": false
        },
        {
          "_id": "680b44fb426b7d5bc2018c7b",
          "name": "Almog Gueta",
          "hidden": false
        },
        {
          "_id": "680b44fb426b7d5bc2018c7c",
          "name": "Royi Rassin",
          "hidden": false
        },
        {
          "_id": "680b44fb426b7d5bc2018c7d",
          "name": "Itay Laish",
          "hidden": false
        },
        {
          "_id": "680b44fb426b7d5bc2018c7e",
          "name": "Dani Lischinski",
          "hidden": false
        },
        {
          "_id": "680b44fb426b7d5bc2018c7f",
          "name": "Idan Szpektor",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-24T12:44:51.000Z",
      "submittedOnDailyAt": "2025-04-25T06:50:21.552Z",
      "title": "RefVNLI : Recherche sur la conversion de phrases de thème en images pour évaluations échangées",
      "submittedOnDailyBy": {
        "_id": "631da07f6d6a5870f3d2c375",
        "avatarUrl": "/avatars/242e344dca08057bdf1eef09f69b41b2.svg",
        "isPro": false,
        "fullname": "Aviv Slobodkin",
        "user": "lovodkin93",
        "type": "user"
      },
      "summary": "La génération d'images basée sur le texte (T2I) dans un texte principal vise à créer des images qui correspondent à la description textuelle donnée tout en maintenant l'identité visuelle des images de référence. Son application large est allée de la personnalisation de la génération d'images à la représentation cohérente de personnages dans les réalisations de vidéos. Cependant, son développement est limité par la manque de méthodes d'évaluation automatiques fiables. Les méthodes actuelles ne sont limitées aux parties de la tâche et ne peuvent pas évaluer des aspects tels que la correspondance au texte ou la préservation du thème, dépendant parfois des jugements humains ou d'APIs coûteuses. En réponse à cette situation, nous présentons RefVNLI. RefVNLI est un métrique de faible coût qui évalue simultanément la correspondance du texte et la préservation du thème dans une seule prédiction. Elle a été entraînée dans un cadre de référence de logique vidéo et sur de grands ensembles de données et d'images variées. RefVNLI dépasse ou atteint des niveaux similaires aux actuels, obtenant des gains maximaux de 6,4 points en termes de correspondance du texte et de 8,5 points en termes de préservation du thème. De plus, elle montre une excellente performance avec des concepts inconnus, atteignant une précision de 87% ou plus, ce qui est en accord avec les préférences humaines.",
      "upvotes": 35,
      "discussionId": "680b44ff426b7d5bc2018d85",
      "ai_keywords": [
        "RefVNLI",
        "video-reasoning benchmarks",
        "image perturbations"
      ]
    },
    "publishedAt": "2025-04-24T08:44:51.000Z",
    "title": "RefVNLI: Towards Scalable Evaluation of Subject-driven Text-to-image\n  Generation",
    "summary": "Subject-driven text-to-image (T2I) generation aims to produce images that\nalign with a given textual description, while preserving the visual identity\nfrom a referenced subject image. Despite its broad downstream applicability --\nranging from enhanced personalization in image generation to consistent\ncharacter representation in video rendering -- progress in this field is\nlimited by the lack of reliable automatic evaluation. Existing methods either\nassess only one aspect of the task (i.e., textual alignment or subject\npreservation), misalign with human judgments, or rely on costly API-based\nevaluation. To address this, we introduce RefVNLI, a cost-effective metric that\nevaluates both textual alignment and subject preservation in a single\nprediction. Trained on a large-scale dataset derived from video-reasoning\nbenchmarks and image perturbations, RefVNLI outperforms or matches existing\nbaselines across multiple benchmarks and subject categories (e.g.,\nAnimal, Object), achieving up to 6.4-point gains in textual\nalignment and 8.5-point gains in subject consistency. It also excels with\nlesser-known concepts, aligning with human preferences at over 87\\% accuracy.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17502.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "631da07f6d6a5870f3d2c375",
      "avatarUrl": "/avatars/242e344dca08057bdf1eef09f69b41b2.svg",
      "fullname": "Aviv Slobodkin",
      "name": "lovodkin93",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.17192",
      "authors": [
        {
          "_id": "680aee7bcf67477f2c00ca53",
          "user": {
            "_id": "64f7bf0c7565a69eb693ad1f",
            "avatarUrl": "/avatars/aba6910aa39a3437a7f0df3f5cd49e6d.svg",
            "isPro": false,
            "fullname": "minju",
            "user": "iaminju",
            "type": "user"
          },
          "name": "Minju Seo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-25T08:34:41.304Z",
          "hidden": false
        },
        {
          "_id": "680aee7bcf67477f2c00ca54",
          "user": {
            "_id": "63036b6c5c70c21d0ea79d48",
            "avatarUrl": "/avatars/a7eb03f5cbd4eaa09fe807bbed8bc0f7.svg",
            "isPro": false,
            "fullname": "Jinheon Baek",
            "user": "jinheon",
            "type": "user"
          },
          "name": "Jinheon Baek",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-25T08:34:38.982Z",
          "hidden": false
        },
        {
          "_id": "680aee7bcf67477f2c00ca55",
          "name": "Seongyun Lee",
          "hidden": false
        },
        {
          "_id": "680aee7bcf67477f2c00ca56",
          "name": "Sung Ju Hwang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-24T01:57:01.000Z",
      "submittedOnDailyAt": "2025-04-25T04:17:48.790Z",
      "title": "Paper2Code : Génération automatique de code à partir d'articles scientifiques (apprentissage profond)",
      "submittedOnDailyBy": {
        "_id": "6550c4f27bbfce1878f5f280",
        "avatarUrl": "/avatars/0ecedbcd8a55b2c4abd1da9e741a6652.svg",
        "isPro": false,
        "fullname": "seongyun_lee",
        "user": "Seongyun",
        "type": "user"
      },
      "summary": "Le développement rapide des recherches en apprentissage automatique (Machine Learning) a conduit à ce que l'implémentation de code pour répondre à ces avancées soit généralement impraticable. Par conséquent, les chercheurs doivent faire face à des délais longs et à des efforts significatifs pour reproduire des résultats et étendre des études antérieures. En revanche, les modèles de langue grands (LLMs) récents ont la capacité de comprendre des articles scientifiques et de générer du code de haute qualité. Dans ce contexte, il est proposé PaperCoder, un cadre de travail d'agents LLM qui transforme des articles en apprentissage automatique en dépôts de code fonctionnel. PaperCoder fonctionne en trois étapes : planification, analyse et génération. Dans l'étape de planification, un mappemonde est construit, une architecture du système est conçue dans des diagrammes et les dépendances entre fichiers sont spécifiées, ainsi que des fichiers de configuration sont créés. Dans l'étape d'analyse, des détails spécifiques adaptés à l'implémentation sont interprétés, et dans l'étape de génération, des codes modulaires qui connaissent les dépendances sont produits. De plus, chaque étape est instanciée avec un ensemble d'agents professionnels qui collaborent efficacement dans la chaîne d'approvisionnement. De plus, PaperCoder évalue la génération de code à partir d'articles en apprentissage automatique en utilisant des évaluations basées sur des modèles et des évaluations humaines, et montre son efficacité en présentant des résultats qui peuvent être utilisés par les auteurs originaux des articles. De plus, il montre une capacité très forte en surmontant significativement les limites de PaperBench, un cadre de référence récent.",
      "upvotes": 31,
      "discussionId": "680aee7dcf67477f2c00ca96",
      "githubRepo": "https://github.com/going-doer/Paper2Code"
    },
    "publishedAt": "2025-04-23T21:57:01.000Z",
    "title": "Paper2Code: Automating Code Generation from Scientific Papers in Machine\n  Learning",
    "summary": "Despite the rapid growth of machine learning research, corresponding code\nimplementations are often unavailable, making it slow and labor-intensive for\nresearchers to reproduce results and build upon prior work. In the meantime,\nrecent Large Language Models (LLMs) excel at understanding scientific documents\nand generating high-quality code. Inspired by this, we introduce PaperCoder, a\nmulti-agent LLM framework that transforms machine learning papers into\nfunctional code repositories. PaperCoder operates in three stages: planning,\nwhere it constructs a high-level roadmap, designs the system architecture with\ndiagrams, identifies file dependencies, and generates configuration files;\nanalysis, which focuses on interpreting implementation-specific details; and\ngeneration, where modular, dependency-aware code is produced. Moreover, each\nphase is instantiated through a set of specialized agents designed to\ncollaborate effectively across the pipeline. We then evaluate PaperCoder on\ngenerating code implementations from machine learning papers based on both\nmodel-based and human evaluations, specifically from the original paper\nauthors, with author-released repositories as ground truth if available. Our\nresults demonstrate the effectiveness of PaperCoder in creating high-quality,\nfaithful implementations. Furthermore, it consistently shows strengths in the\nrecently released PaperBench benchmark, surpassing strong baselines by\nsubstantial margins.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17192.png",
    "numComments": 4,
    "submittedBy": {
      "_id": "6550c4f27bbfce1878f5f280",
      "avatarUrl": "/avatars/0ecedbcd8a55b2c4abd1da9e741a6652.svg",
      "fullname": "seongyun_lee",
      "name": "Seongyun",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.17432",
      "authors": [
        {
          "_id": "680adfbe464a44cea0b843c1",
          "name": "Tiancheng Gu",
          "hidden": false
        },
        {
          "_id": "680adfbe464a44cea0b843c2",
          "user": {
            "_id": "63e202f352b7578dba448ab5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e202f352b7578dba448ab5/8itVBLcv14m7OVsoF8h1o.jpeg",
            "isPro": false,
            "fullname": "Yang",
            "user": "Kaichengalex",
            "type": "user"
          },
          "name": "Kaicheng Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-25T08:34:46.935Z",
          "hidden": false
        },
        {
          "_id": "680adfbe464a44cea0b843c3",
          "name": "Ziyong Feng",
          "hidden": false
        },
        {
          "_id": "680adfbe464a44cea0b843c4",
          "name": "Xingjun Wang",
          "hidden": false
        },
        {
          "_id": "680adfbe464a44cea0b843c5",
          "name": "Yanzhao Zhang",
          "hidden": false
        },
        {
          "_id": "680adfbe464a44cea0b843c6",
          "name": "Dingkun Long",
          "hidden": false
        },
        {
          "_id": "680adfbe464a44cea0b843c7",
          "name": "Yingda Chen",
          "hidden": false
        },
        {
          "_id": "680adfbe464a44cea0b843c8",
          "name": "Weidong Cai",
          "hidden": false
        },
        {
          "_id": "680adfbe464a44cea0b843c9",
          "name": "Jiankang Deng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-24T10:51:52.000Z",
      "submittedOnDailyAt": "2025-04-25T01:11:53.967Z",
      "title": "Le mur de modules cassé : apprentissage interne généralisé avec modules de différenciation de niveaux de LLM",
      "submittedOnDailyBy": {
        "_id": "63e202f352b7578dba448ab5",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e202f352b7578dba448ab5/8itVBLcv14m7OVsoF8h1o.jpeg",
        "isPro": false,
        "fullname": "Yang",
        "user": "Kaichengalex",
        "type": "user"
      },
      "summary": "Le cadre de travail CLIP (Pré-entraînement Contrastif Langue-Image) est principalement utilisé pour la recherche et l'agrégation de documents d'image, mais sa efficacité est limitée par trois facteurs : la transformation des tokens de documents, la codification des documents d'image séparés et les erreurs structurelles causées par l'action mot à mot. Les modèles de langage multimodal généralisés (MLLM) ont démontré un grand progrès dans la compréhension générale du langage visuel, mais la possibilité d'apprentissage de représentations dynamiques de multiples modèles nécessite encore plus d'investigation. Dans cet article, nous proposons un nouveau cadre de travail à deux étapes, Universal Multimodal Embedding (UniME), pour entraîner des expressions judiciaires dans différentes tâches ultérieures en utilisant MLLM. Dans la première étape, nous apprenons la capacité d'incorporer des connaissances judiciaires dans la compréhension du langage à travers un modèle professionnel basé sur une forte LLM. Dans la deuxième étape, nous introduisons une régulation d'instructions avec des exemples négatifs renforcés pour encourager l'apprentissage des expressions judiciaires. Spécifiquement, nous essayons de réduire la contamination d'exemples négatifs erronés au début et nous présentons plusieurs exemples négatifs difficiles pour chaque instance au sein de chaque batch pour centrer le modèle sur des échantillons difficiles. Cet approche améliore la capacité de jugement et augmente la capacité de suivi d'instructions dans les tâches ultérieures. Des expériences étendues sont réalisées sur le benchmark MMEB et diverses tâches de recherche, montrant un amélioration uniforme dans toutes les tâches et démontrant des capacités de jugement et de structure excellentes.",
      "upvotes": 21,
      "discussionId": "680adfbf464a44cea0b8440f",
      "projectPage": "https://garygutc.github.io/UniME/",
      "githubRepo": "https://github.com/deepglint/UniME",
      "ai_keywords": [
        "Contrastive Language-Image Pre-training (CLIP)",
        "Multimodal Large Language Models (MLLMs)",
        "Generalized vision-language understanding",
        "UniME (Universal Multimodal Embedding)",
        "Discriminative representations",
        "Textual discriminative knowledge distillation",
        "LLM-based teacher model",
        "Hard negative enhanced instruction tuning",
        "False negative contamination",
        "Challenging samples",
        "Discriminative power",
        "Instruction-following ability",
        "MMEB benchmark",
        "Short caption retrieval",
        "Long caption retrieval",
        "Compositional retrieval"
      ]
    },
    "publishedAt": "2025-04-24T06:51:52.000Z",
    "title": "Breaking the Modality Barrier: Universal Embedding Learning with\n  Multimodal LLMs",
    "summary": "The Contrastive Language-Image Pre-training (CLIP) framework has become a\nwidely used approach for multimodal representation learning, particularly in\nimage-text retrieval and clustering. However, its efficacy is constrained by\nthree key limitations: (1) text token truncation, (2) isolated image-text\nencoding, and (3) deficient compositionality due to bag-of-words behavior.\nWhile recent Multimodal Large Language Models (MLLMs) have demonstrated\nsignificant advances in generalized vision-language understanding, their\npotential for learning transferable multimodal representations remains\nunderexplored.In this work, we present UniME (Universal Multimodal Embedding),\na novel two-stage framework that leverages MLLMs to learn discriminative\nrepresentations for diverse downstream tasks. In the first stage, we perform\ntextual discriminative knowledge distillation from a powerful LLM-based teacher\nmodel to enhance the embedding capability of the MLLM\\'s language component. In\nthe second stage, we introduce hard negative enhanced instruction tuning to\nfurther advance discriminative representation learning. Specifically, we\ninitially mitigate false negative contamination and then sample multiple hard\nnegatives per instance within each batch, forcing the model to focus on\nchallenging samples. This approach not only improves discriminative power but\nalso enhances instruction-following ability in downstream tasks. We conduct\nextensive experiments on the MMEB benchmark and multiple retrieval tasks,\nincluding short and long caption retrieval and compositional retrieval. Results\ndemonstrate that UniME achieves consistent performance improvement across all\ntasks, exhibiting superior discriminative and compositional capabilities.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17432.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63e202f352b7578dba448ab5",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e202f352b7578dba448ab5/8itVBLcv14m7OVsoF8h1o.jpeg",
      "fullname": "Yang",
      "name": "Kaichengalex",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.17207",
      "authors": [
        {
          "_id": "680af2bf2fa10fbf21684bde",
          "name": "Phillip Y. Lee",
          "hidden": false
        },
        {
          "_id": "680af2bf2fa10fbf21684bdf",
          "name": "Jihyeon Je",
          "hidden": false
        },
        {
          "_id": "680af2bf2fa10fbf21684be0",
          "name": "Chanho Park",
          "hidden": false
        },
        {
          "_id": "680af2bf2fa10fbf21684be1",
          "name": "Mikaela Angelina Uy",
          "hidden": false
        },
        {
          "_id": "680af2bf2fa10fbf21684be2",
          "name": "Leonidas Guibas",
          "hidden": false
        },
        {
          "_id": "680af2bf2fa10fbf21684be3",
          "name": "Minhyuk Sung",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-24T02:41:34.000Z",
      "submittedOnDailyAt": "2025-04-25T00:59:29.327Z",
      "title": "Simulation d'images psychologiques à travers la théorie des points de vue (Simulation d'images psychologiques à travers la théorie des points de vue)",
      "submittedOnDailyBy": {
        "_id": "6342796a0875f2c99cfd313b",
        "avatarUrl": "/avatars/98575092404c4197b20c929a6499a015.svg",
        "isPro": false,
        "fullname": "Yuseung \"Phillip\" Lee",
        "user": "phillipinseoul",
        "type": "user"
      },
      "summary": "Nous proposons un cadre de travail utilisant un modèle de langue visuelle (VLM) pour le reconnaissance de relations visuelles en utilisant la mémoire imaginaire. Le reconnaissance de relations visuelles est un critère important pour une compréhension visuelle humaine, et il est nécessaire pour l'interaction avec l'environnement et la collaboration des agents de véhicules autonomes. Les études récentes ont montré que, au fur et à mesure que les VLMs améliorent leur reconnaissance spatiale, ils présentent une notable absence de capacités dans le reconnaissance de relations visuelles et une tendance vers une interprétation centralisée. Nous soulignons le rôle de la mémoire imaginaire pour combler la lacune entre le reconnaissance visuelle des VLMs et le cognitif humain. L'humanité voit le monde à travers des représentations abstraites, ce qui favorise la mobilité visuelle. En se basant sur cela, nous proposons un cadre de travail appelé \"Transformation de Relations Visuelles Abstraites (TRA)\" qui utilise des modèles basés sur la vision (recherche d'objets, segmentation, mesure de direction) pour faciliter l'abstraction spatiale et la transformation de relations visuelles. Par des expériences dans des cadres de référence d'images synthétiques et réelles, nous montrons une amélioration significative du reconnaissance de relations visuelles par rapport aux VLMs, montrant que le modèle d'ajustement microspatiaux et le nouveau point de vue basé sur la synthèse de points visuels dépassent encore davantage.",
      "upvotes": 14,
      "discussionId": "680af2c02fa10fbf21684c1f",
      "ai_keywords": [
        "vision-language models (VLMs)",
        "mental imagery simulation",
        "perspective-taking",
        "visual understanding",
        "environmental interaction",
        "autonomous agents",
        "spatial reasoning",
        "perspective-aware reasoning capabilities",
        "egocentric interpretations",
        "mental imagery",
        "scene abstractions",
        "perspective transformations",
        "object detection",
        "segmentation",
        "orientation estimation",
        "synthetic benchmarks",
        "real-image benchmarks",
        "fine-tuned spatial reasoning models",
        "novel-view-synthesis-based approaches"
      ]
    },
    "publishedAt": "2025-04-23T22:41:34.000Z",
    "title": "Perspective-Aware Reasoning in Vision-Language Models via Mental Imagery\n  Simulation",
    "summary": "We present a framework for perspective-aware reasoning in vision-language\nmodels (VLMs) through mental imagery simulation. Perspective-taking, the\nability to perceive an environment or situation from an alternative viewpoint,\nis a key benchmark for human-level visual understanding, essential for\nenvironmental interaction and collaboration with autonomous agents. Despite\nadvancements in spatial reasoning within VLMs, recent research has shown that\nmodern VLMs significantly lack perspective-aware reasoning capabilities and\nexhibit a strong bias toward egocentric interpretations. To bridge the gap\nbetween VLMs and human perception, we focus on the role of mental imagery,\nwhere humans perceive the world through abstracted representations that\nfacilitate perspective shifts. Motivated by this, we propose a framework for\nperspective-aware reasoning, named Abstract Perspective Change (APC), that\neffectively leverages vision foundation models, such as object detection,\nsegmentation, and orientation estimation, to construct scene abstractions and\nenable perspective transformations. Our experiments on synthetic and real-image\nbenchmarks, compared with various VLMs, demonstrate significant improvements in\nperspective-aware reasoning with our framework, further outperforming\nfine-tuned spatial reasoning models and novel-view-synthesis-based approaches.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17207.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6342796a0875f2c99cfd313b",
      "avatarUrl": "/avatars/98575092404c4197b20c929a6499a015.svg",
      "fullname": "Yuseung \"Phillip\" Lee",
      "name": "phillipinseoul",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.16511",
      "authors": [
        {
          "_id": "680b2a95c94724c1465c20dd",
          "name": "Fengze Liu",
          "hidden": false
        },
        {
          "_id": "680b2a95c94724c1465c20de",
          "name": "Weidong Zhou",
          "hidden": false
        },
        {
          "_id": "680b2a95c94724c1465c20df",
          "name": "Binbin Liu",
          "hidden": false
        },
        {
          "_id": "680b2a95c94724c1465c20e0",
          "name": "Zhimiao Yu",
          "hidden": false
        },
        {
          "_id": "680b2a95c94724c1465c20e1",
          "name": "Yifan Zhang",
          "hidden": false
        },
        {
          "_id": "680b2a95c94724c1465c20e2",
          "name": "Haobin Lin",
          "hidden": false
        },
        {
          "_id": "680b2a95c94724c1465c20e3",
          "name": "Yifeng Yu",
          "hidden": false
        },
        {
          "_id": "680b2a95c94724c1465c20e4",
          "name": "Xiaohuan Zhou",
          "hidden": false
        },
        {
          "_id": "680b2a95c94724c1465c20e5",
          "name": "Taifeng Wang",
          "hidden": false
        },
        {
          "_id": "680b2a95c94724c1465c20e6",
          "name": "Yong Cao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-23T08:36:50.000Z",
      "submittedOnDailyAt": "2025-04-25T04:55:18.773Z",
      "title": "QuaDMix : Sélection de données d'entraînement efficace de LLM basée sur l'équilibre de masse et la diversité",
      "submittedOnDailyBy": {
        "_id": "668f5875b5b3081d776e4094",
        "avatarUrl": "/avatars/8c763393f25afbe5fb8b132f775e746a.svg",
        "isPro": false,
        "fullname": "Xiaohuan Zhou",
        "user": "XiaohuanZhou",
        "type": "user"
      },
      "summary": "La qualité et la diversité sont deux indicateurs importants dans les données d'entraînement de modèles de langage grands (LLMs) et ont un impact positif sur l'amélioration de l'efficacité. Actuellement, les études optimisent ces indicateurs individuellement, mais généralement, effectuent des filtrages de qualité et ajustent ensuite la proportion des données. Cependant, ces approches ignorent le compromis intrinsèque entre qualité et diversité. Il est crucial d'évaluer la qualité de chaque point de données et l'effet complémentaire de la collecte de données en ensemble. Dans cet article, nous proposons un cadre de sélection de données intégré appelé QuaDMix, qui optimise automatiquement la distribution des données pour l'entraînement préalable de LLMs tout en équilibrant qualité et diversité. Spécifiquement, nous proposons plusieurs critères pour mesurer la qualité des données et utilisons la classification de régions pour différencier les points de données, ainsi que mesurer la diversité de la collecte. QuaDMix utilise une fonction de sampling paramétrique pour déterminer la probabilité d'échantillonnage de chaque point de données en fonction d'étiquettes liées à la qualité et à la diversité. Pour accélérer l'exploration de paramètres optimaux du cadre de QuaDMix, nous effectuons des expériences informatiques avec des petits modèles et utilisons RegMix, qui implémente LightGBM. Les résultats des expériences sur un large éventail de modèles et de jeux de données montrent que QuaDMix atteint un améliorament moyen de l'efficacité de 7,2 %. Ces résultats démontrent que QuaDMix est supérieur à des stratégies indépendantes de qualité et de diversité, et soulignent la nécessité et la possibilité d'équilibre entre la qualité et la diversité des données.",
      "upvotes": 13,
      "discussionId": "680b2a97c94724c1465c21a3",
      "ai_keywords": [
        "large language models (LLMs)",
        "QuaDMix",
        "data selection framework",
        "parameterized data sampling function",
        "domain classification",
        "LightGBM",
        "RegMix"
      ]
    },
    "publishedAt": "2025-04-23T04:36:50.000Z",
    "title": "QuaDMix: Quality-Diversity Balanced Data Selection for Efficient LLM\n  Pretraining",
    "summary": "Quality and diversity are two critical metrics for the training data of large\nlanguage models (LLMs), positively impacting performance. Existing studies\noften optimize these metrics separately, typically by first applying quality\nfiltering and then adjusting data proportions. However, these approaches\noverlook the inherent trade-off between quality and diversity, necessitating\ntheir joint consideration. Given a fixed training quota, it is essential to\nevaluate both the quality of each data point and its complementary effect on\nthe overall dataset. In this paper, we introduce a unified data selection\nframework called QuaDMix, which automatically optimizes the data distribution\nfor LLM pretraining while balancing both quality and diversity. Specifically,\nwe first propose multiple criteria to measure data quality and employ domain\nclassification to distinguish data points, thereby measuring overall diversity.\nQuaDMix then employs a unified parameterized data sampling function that\ndetermines the sampling probability of each data point based on these quality\nand diversity related labels. To accelerate the search for the optimal\nparameters involved in the QuaDMix framework, we conduct simulated experiments\non smaller models and use LightGBM for parameters searching, inspired by the\nRegMix method. Our experiments across diverse models and datasets demonstrate\nthat QuaDMix achieves an average performance improvement of 7.2% across\nmultiple benchmarks. These results outperform the independent strategies for\nquality and diversity, highlighting the necessity and ability to balance data\nquality and diversity.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16511.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "668f5875b5b3081d776e4094",
      "avatarUrl": "/avatars/8c763393f25afbe5fb8b132f775e746a.svg",
      "fullname": "Xiaohuan Zhou",
      "name": "XiaohuanZhou",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.17789",
      "authors": [
        {
          "_id": "680b318bbbebf87944bc9595",
          "name": "Xu Ma",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc9596",
          "name": "Peize Sun",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc9597",
          "name": "Haoyu Ma",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc9598",
          "name": "Hao Tang",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc9599",
          "name": "Chih-Yao Ma",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc959a",
          "name": "Jialiang Wang",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc959b",
          "name": "Kunpeng Li",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc959c",
          "name": "Xiaoliang Dai",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc959d",
          "name": "Yujun Shi",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc959e",
          "name": "Xuan Ju",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc959f",
          "name": "Yushi Hu",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc95a0",
          "name": "Artsiom Sanakoyeu",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc95a1",
          "name": "Felix Juefei-Xu",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc95a2",
          "name": "Ji Hou",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc95a3",
          "name": "Junjiao Tian",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc95a4",
          "name": "Tao Xu",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc95a5",
          "name": "Tingbo Hou",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc95a6",
          "name": "Yen-Cheng Liu",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc95a7",
          "name": "Zecheng He",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc95a8",
          "name": "Zijian He",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc95a9",
          "name": "Matt Feiszli",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc95aa",
          "name": "Peizhao Zhang",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc95ab",
          "name": "Peter Vajda",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc95ac",
          "name": "Sam Tsai",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc95ad",
          "name": "Yun Fu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-24T17:59:56.000Z",
      "submittedOnDailyAt": "2025-04-25T05:28:51.493Z",
      "title": "Token Shuffle : Une Étude sur la Génération d'Images à Haute Résolution en Utilisant des Modèles Automatiques de Régression",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Le modèle d'auto-régression (AR) a été dominant dans la génération de langage à long terme et est de plus en plus appliqué à la synthèse d'images, bien qu'en comparaison aux modèles basés sur la diffusion, nous considérons qu'il a de nombreux problèmes. L'un des principaux limites est que le nombre de tokens d'image que nécessite le modèle AR est très élevé, ce qui empêche une efficacité dans l'entraînement et l'inférence et limite la résolution des images. Pour résoudre ce problème, nous proposons le méthode de mélange de tokens. Ceci est un nouveau méthode mais simple et réduit le nombre de tokens d'image dans les modèles transformateurs. Notre idée principale est la redondance de la dimension du vocabulaire visuel dans les modèles multimodales de langage (MLLMs). Dans ce contexte, les codes visuels de faible dimension dans l'encodeur visuel sont directement mappés sur des vocabulaires de langage de haute dimension. En utilisant cela, nous considérons deux tâches principales : mélange de tokens et mélange de tokens. Le mélange de tokens intègre des tokens locaux spatialement dans la dimension spectrale et réduit le nombre de tokens d'entrée. Le mélange de tokens connecte les tokens inférés après les blocs de transformateur et récupère la séquence spatiale de la sortie. L'entraînement avec un cadre de contexte est une partie de notre stratégie, car il ne nécessite pas un encodeur de contexte supplémentaire entraîné. De cette manière, les MLLMs peuvent prédire le prochain token de manière intégrée de langage et image, permettant la synthèse d'images de haute résolution. Pour la première fois, nous avons atteint la frontière de la génération d'images à partir du texte AR à une résolution de 2048x2048, obtenant des résultats satisfaisants. Dans le benchmark GenAI, notre modèle de 2.7B a atteint un score total de 0.77 dans des situations difficiles, dépassant le modèle AR LlamaGen avec un score de 0.18 et le modèle basé sur la diffusion LDM avec un score de 0.15. Dans les évaluations de grande échelle humaine, notre capacité de génération d'images notable en correspondance linguistique, défauts visuels et apparence visuelle a été démontrée. Nous espérons que le mélange de tokens soit la base fondamentale pour la génération efficace d'images de haute résolution dans les MLLMs.",
      "upvotes": 4,
      "discussionId": "680b3191bbebf87944bc9739",
      "ai_keywords": [
        "autoregressive (AR) models",
        "image synthesis",
        "diffusion-based models",
        "image tokens",
        "training and inference efficiency",
        "Transformer",
        "dimensional redundancy",
        "visual vocabularies",
        "Multimodal Large Language Models (MLLMs)",
        "visual encoder",
        "high-dimensional language vocabularies",
        "token-shuffle",
        "spatially local tokens",
        "channel dimension",
        "token-unshuffle",
        "spatial arrangement",
        "unified next-token prediction",
        "text-to-image generation",
        "resolution",
        "generation performance",
        "GenAI-benchmark",
        "textual prompts",
        "pretrained text-encoder",
        "text-alignment",
        "visual flaw",
        "visual appearance"
      ]
    },
    "publishedAt": "2025-04-24T13:59:56.000Z",
    "title": "Token-Shuffle: Towards High-Resolution Image Generation with\n  Autoregressive Models",
    "summary": "Autoregressive (AR) models, long dominant in language generation, are\nincreasingly applied to image synthesis but are often considered less\ncompetitive than Diffusion-based models. A primary limitation is the\nsubstantial number of image tokens required for AR models, which constrains\nboth training and inference efficiency, as well as image resolution. To address\nthis, we present Token-Shuffle, a novel yet simple method that reduces the\nnumber of image tokens in Transformer. Our key insight is the dimensional\nredundancy of visual vocabularies in Multimodal Large Language Models (MLLMs),\nwhere low-dimensional visual codes from visual encoder are directly mapped to\nhigh-dimensional language vocabularies. Leveraging this, we consider two key\noperations: token-shuffle, which merges spatially local tokens along channel\ndimension to decrease the input token number, and token-unshuffle, which\nuntangles the inferred tokens after Transformer blocks to restore the spatial\narrangement for output. Jointly training with textual prompts, our strategy\nrequires no additional pretrained text-encoder and enables MLLMs to support\nextremely high-resolution image synthesis in a unified next-token prediction\nway while maintaining efficient training and inference. For the first time, we\npush the boundary of AR text-to-image generation to a resolution of 2048x2048\nwith gratifying generation performance. In GenAI-benchmark, our 2.7B model\nachieves 0.77 overall score on hard prompts, outperforming AR models LlamaGen\nby 0.18 and diffusion models LDM by 0.15. Exhaustive large-scale human\nevaluations also demonstrate our prominent image generation ability in terms of\ntext-alignment, visual flaw, and visual appearance. We hope that Token-Shuffle\ncan serve as a foundational design for efficient high-resolution image\ngeneration within MLLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17789.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6713
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.17069",
      "authors": [
        {
          "_id": "680b1b33388bb2cfd497ebdb",
          "user": {
            "_id": "62bb84f82ada492aa5775709",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62bb84f82ada492aa5775709/jv4yKL75t8QzHDHLbhBPT.png",
            "isPro": false,
            "fullname": "Rishav Pramanik",
            "user": "rishavpramanik",
            "type": "user"
          },
          "name": "Rishav Pramanik",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-25T08:34:27.724Z",
          "hidden": false
        },
        {
          "_id": "680b1b33388bb2cfd497ebdc",
          "name": "Antoine Poupon",
          "hidden": false
        },
        {
          "_id": "680b1b33388bb2cfd497ebdd",
          "name": "Juan A. Rodriguez",
          "hidden": false
        },
        {
          "_id": "680b1b33388bb2cfd497ebde",
          "name": "Masih Aminbeidokhti",
          "hidden": false
        },
        {
          "_id": "680b1b33388bb2cfd497ebdf",
          "name": "David Vazquez",
          "hidden": false
        },
        {
          "_id": "680b1b33388bb2cfd497ebe0",
          "name": "Christopher Pal",
          "hidden": false
        },
        {
          "_id": "680b1b33388bb2cfd497ebe1",
          "name": "Zhaozheng Yin",
          "hidden": false
        },
        {
          "_id": "680b1b33388bb2cfd497ebe2",
          "name": "Marco Pedersoli",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63a614d264f470027818b066/NUImFkmaqDnfEIBB2l94Q.png",
        "https://cdn-uploads.huggingface.co/production/uploads/63a614d264f470027818b066/OuC3dzu5XCUb8dxTlVf72.png"
      ],
      "publishedAt": "2025-04-23T19:33:58.000Z",
      "submittedOnDailyAt": "2025-04-25T03:50:09.534Z",
      "title": "Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automatique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple. Nous utilisons une fonction automátique simple",
      "submittedOnDailyBy": {
        "_id": "63a614d264f470027818b066",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a614d264f470027818b066/Q5Eih2VqD4NaHB_MkNp32.jpeg",
        "isPro": false,
        "fullname": "Juan A. Rodriguez",
        "user": "joanrodai",
        "type": "user"
      },
      "summary": "La génération automatique d'images basée sur des formes séquentielles a démontré des résultats relativement excellents en termes de qualité d'image et de scalabilité par rapport à d'autres méthodes récentes. Ces modèles peuvent être facilement intégrés dans des modèles de langage visuel et sont également scalables. Cependant, les modèles séquentiels nécessitent une séquence définie pour la génération des formes. Une séquence naturelle se fonde sur des instructions de mots. Cela a un sens dans la génération de texte mais il n'existe pas une séquence inhérente pour la génération d'images. Traditionnellement, l'ordre de raster scan (de la coin supérieur gauche vers la coin inférieur droit) guide les modèles séquentiels de génération d'images. Dans cet article, on soutient que cette séquence n'est pas la plus adéquate et ne respecte pas la causalité des contenus des images : par exemple, un modèle séquentiel automatique pourrait créer des nuages avant le soleil en se basant sur la décomposition visuelle du coucher du soleil. Cependant, le colour de nuages doit être déterminé par le colour du soleil, ce qui signifie que la séquence actuelle est inversée. Dans cet article, on montre que la séquence de génération des formes peut être entraînée de manière arbitraire pour inférer le contenu et la position (ordre) des formes lors du processus de génération. De plus, on démontre que grâce à cette séquence extraite, un modèle de séquence arbitraire peut être ajusté pour générer des images de meilleure qualité. Les expériences montrent que cette nouvelle méthodologie de génération produit des images plus bonnes que la traditionnelle raster scan sur deux ensembles de données, sans besoin de coûts supplémentaires d'entraînement ni d'analyse additionnelles.",
      "upvotes": 4,
      "discussionId": "680b1b35388bb2cfd497ec76",
      "ai_keywords": [
        "autoregressive patch-based image generation",
        "Vision-Language models",
        "raster-scan order",
        "causality",
        "any-given-order",
        "patch content",
        "patch location",
        "fine-tuning"
      ]
    },
    "publishedAt": "2025-04-23T15:33:58.000Z",
    "title": "Distilling semantically aware orders for autoregressive image generation",
    "summary": "Autoregressive patch-based image generation has recently shown competitive\nresults in terms of image quality and scalability. It can also be easily\nintegrated and scaled within Vision-Language models. Nevertheless,\nautoregressive models require a defined order for patch generation. While a\nnatural order based on the dictation of the words makes sense for text\ngeneration, there is no inherent generation order that exists for image\ngeneration. Traditionally, a raster-scan order (from top-left to bottom-right)\nguides autoregressive image generation models. In this paper, we argue that\nthis order is suboptimal, as it fails to respect the causality of the image\ncontent: for instance, when conditioned on a visual description of a sunset, an\nautoregressive model may generate clouds before the sun, even though the color\nof clouds should depend on the color of the sun and not the inverse. In this\nwork, we show that first by training a model to generate patches in\nany-given-order, we can infer both the content and the location (order) of each\npatch during generation. Secondly, we use these extracted orders to finetune\nthe any-given-order model to produce better-quality images. Through our\nexperiments, we show on two datasets that this new generation method produces\nbetter images than the traditional raster-scan approach, with similar training\ncosts and no extra annotations.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63a614d264f470027818b066/NUImFkmaqDnfEIBB2l94Q.png",
      "https://cdn-uploads.huggingface.co/production/uploads/63a614d264f470027818b066/OuC3dzu5XCUb8dxTlVf72.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17069.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a614d264f470027818b066",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a614d264f470027818b066/Q5Eih2VqD4NaHB_MkNp32.jpeg",
      "fullname": "Juan A. Rodriguez",
      "name": "joanrodai",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.17040",
      "authors": [
        {
          "_id": "680af0c4175842e433ae348e",
          "name": "Zhenhailong Wang",
          "hidden": false
        },
        {
          "_id": "680af0c4175842e433ae348f",
          "name": "Senthil Purushwalkam",
          "hidden": false
        },
        {
          "_id": "680af0c4175842e433ae3490",
          "name": "Caiming Xiong",
          "hidden": false
        },
        {
          "_id": "680af0c4175842e433ae3491",
          "name": "Silvio Savarese",
          "hidden": false
        },
        {
          "_id": "680af0c4175842e433ae3492",
          "name": "Heng Ji",
          "hidden": false
        },
        {
          "_id": "680af0c4175842e433ae3493",
          "name": "Ran Xu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-23T18:38:18.000Z",
      "submittedOnDailyAt": "2025-04-25T06:12:13.135Z",
      "title": "Méthodes Efficaces pour l'Implémentation de VLMs de Fusion Dynamique et de Fusion Américaine Basique",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "DyMU est une structure de travail non entraînée pour réduire efficacement la charge de calcul des modèles de langue visuo-linguistique (VLMs) et maintenir de hautes performances pour les tâches. Notre approche comprend deux composants principaux : 1. La Dynamique de Fusion de Token (DToMe) résout l'inadéquation de la sortie de longueur fixe dans un transformateur visuo-linguistique en intégrant des tokens similaires en fonction de la complexité de l'image. 2. L'Unification de Token de Vision (VTU) simule les séquences de tokens attendues par un modèle de grande langue (LLMs) pour réorganiser efficacement les opérations d'attention dans toutes les séquences, permettant ainsi une amélioration du rendement des modèles.",
      "upvotes": 3,
      "discussionId": "680af0c7175842e433ae3544",
      "ai_keywords": [
        "Dynamic Token Merging (DToMe)",
        "Virtual Token Unmerging (VTU)",
        "vision transformers",
        "token compression",
        "attention dynamics",
        "visual encoders",
        "image complexity",
        "computational costs"
      ]
    },
    "publishedAt": "2025-04-23T14:38:18.000Z",
    "title": "DyMU: Dynamic Merging and Virtual Unmerging for Efficient VLMs",
    "summary": "We present DyMU, an efficient, training-free framework that dynamically\nreduces the computational burden of vision-language models (VLMs) while\nmaintaining high task performance. Our approach comprises two key components.\nFirst, Dynamic Token Merging (DToMe) reduces the number of visual token\nembeddings by merging similar tokens based on image complexity, addressing the\ninherent inefficiency of fixed-length outputs in vision transformers. Second,\nVirtual Token Unmerging (VTU) simulates the expected token sequence for large\nlanguage models (LLMs) by efficiently reconstructing the attention dynamics of\na full sequence, thus preserving the downstream performance without additional\nfine-tuning. Unlike previous approaches, our method dynamically adapts token\ncompression to the content of the image and operates completely training-free,\nmaking it readily applicable to most state-of-the-art VLM architectures.\nExtensive experiments on image and video understanding tasks demonstrate that\nDyMU can reduce the average visual token count by 32%-85% while achieving\ncomparable performance to full-length models across diverse VLM architectures,\nincluding the recently popularized AnyRes-based visual encoders. Furthermore,\nthrough qualitative analyses, we demonstrate that DToMe effectively adapts\ntoken reduction based on image complexity and, unlike existing systems,\nprovides users more control over computational costs. Project page:\nhttps://mikewangwzhl.github.io/dymu/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17040.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6713
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.16921",
      "authors": [
        {
          "_id": "680b40774d69b6950c4eabda",
          "name": "José Ángel González",
          "hidden": false
        },
        {
          "_id": "680b40774d69b6950c4eabdb",
          "name": "Ian Borrego Obrador",
          "hidden": false
        },
        {
          "_id": "680b40774d69b6950c4eabdc",
          "name": "Álvaro Romo Herrero",
          "hidden": false
        },
        {
          "_id": "680b40774d69b6950c4eabdd",
          "name": "Areg Mikael Sarvazyan",
          "hidden": false
        },
        {
          "_id": "680b40774d69b6950c4eabde",
          "user": {
            "_id": "60f95c8fda0985b973d59d77",
            "avatarUrl": "/avatars/5606f0191b9f86e6b55f7e5ab6cc8bb6.svg",
            "isPro": false,
            "fullname": "Mara Chinea Rios",
            "user": "mchinea",
            "type": "user"
          },
          "name": "Mara Chinea-Ríos",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-04-25T07:58:32.606Z",
          "hidden": false
        },
        {
          "_id": "680b40774d69b6950c4eabdf",
          "name": "Angelo Basile",
          "hidden": false
        },
        {
          "_id": "680b40774d69b6950c4eabe0",
          "name": "Marc Franco-Salvador",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-23T17:48:25.000Z",
      "submittedOnDailyAt": "2025-04-25T06:28:29.231Z",
      "title": "IberBench : Évaluation de LLM pour le Langage Ibérique",
      "submittedOnDailyBy": {
        "_id": "62308d13e4673fe4985d7fc9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1655555697880-62308d13e4673fe4985d7fc9.jpeg",
        "isPro": false,
        "fullname": "Areg Mikael Sarvazyan",
        "user": "asarvazyan",
        "type": "user"
      },
      "summary": "Les modèles de langage grands (LLMs) sont particulièrement difficiles à évaluer dans les langues qui ne sont pas l'anglais, en raison de la disponibilité limitée de données de haute qualité. Les marques actuelles d'évaluation et les tableurs de classement se concentrent sur l'anglais, ce qui réduit considérablement l'évaluation dans d'autres langues. Ces marques d'évaluation privilégient les compétences de base de traitement du langage naturel (NLP) au lieu de tâches industrielles, et sont peu flexibles et peu variées. Reconnaissant ces limites, nous présentons IberBench. IberBench est un marque d'évaluation détaillée et extensible pour évaluer le rendement des LLMs dans des tâches de base et industrielles de NLP dans les langues utilisées en péninsule ibérique et Amérique ibéro-américaine. IberBench intègre 101 ensembles de données et couvre 22 catégories de tâches, comme l'analyse des émotions, la détection de l'ironie, le résumé, entre autres. Cette marque d'évaluation résout les déficiences du processus actuel d'évaluation, ainsi que la manque de diversité linguistique et la manque de préparation de l'évaluation fixe. IberBench permet des mises à jour continuelles grâce à la collaboration de la communauté et la participation des experts dans le développement de modèles et ensembles de données. 23 modèles de LLM (de 100 millions à 14 milliards de paramètres) ont été évalués, fournissant une compréhension expérimentale de leurs forces et faiblesses. Nos résultats montrent que (i) les LLMs présentent un rendement inférieur dans les tâches industrielles par rapport aux tâches de base, (ii) le rendement moyen en gallois et vasque est faible, (iii) dans certaines tâches, les résultats sont similaires à ceux d'un résultat aléatoire, et (iv) dans d'autres tâches, le rendement est supérieur à un résultat aléatoire mais inférieur aux systèmes de tâches partagées. IberBench offre une implémentation ouverte qui inclut la normalisation des ensembles de données, le hosting, l'évaluation incrémentale des LLMs et un tableur de classement accessible publicement.",
      "upvotes": 3,
      "discussionId": "680b407a4d69b6950c4eac96",
      "projectPage": "https://huggingface.co/spaces/iberbench/leaderboard",
      "githubRepo": "https://github.com/IberBench",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "sentiment and emotion analysis",
        "toxicity detection",
        "summarization",
        "IberBench",
        "dataset normalization",
        "incremental evaluation",
        "leaderboard"
      ]
    },
    "publishedAt": "2025-04-23T13:48:25.000Z",
    "title": "IberBench: LLM Evaluation on Iberian Languages",
    "summary": "Large Language Models (LLMs) remain difficult to evaluate comprehensively,\nparticularly for languages other than English, where high-quality data is often\nlimited. Existing benchmarks and leaderboards are predominantly\nEnglish-centric, with only a few addressing other languages. These benchmarks\nfall short in several key areas: they overlook the diversity of language\nvarieties, prioritize fundamental Natural Language Processing (NLP)\ncapabilities over tasks of industrial relevance, and are static. With these\naspects in mind, we present IberBench, a comprehensive and extensible benchmark\ndesigned to assess LLM performance on both fundamental and industry-relevant\nNLP tasks, in languages spoken across the Iberian Peninsula and Ibero-America.\nIberBench integrates 101 datasets from evaluation campaigns and recent\nbenchmarks, covering 22 task categories such as sentiment and emotion analysis,\ntoxicity detection, and summarization. The benchmark addresses key limitations\nin current evaluation practices, such as the lack of linguistic diversity and\nstatic evaluation setups by enabling continual updates and community-driven\nmodel and dataset submissions moderated by a committee of experts. We evaluate\n23 LLMs ranging from 100 million to 14 billion parameters and provide empirical\ninsights into their strengths and limitations. Our findings indicate that (i)\nLLMs perform worse on industry-relevant tasks than in fundamental ones, (ii)\nperformance is on average lower for Galician and Basque, (iii) some tasks show\nresults close to random, and (iv) in other tasks LLMs perform above random but\nbelow shared task systems. IberBench offers open-source implementations for the\nentire evaluation pipeline, including dataset normalization and hosting,\nincremental evaluation of LLMs, and a publicly accessible leaderboard.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16921.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62308d13e4673fe4985d7fc9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1655555697880-62308d13e4673fe4985d7fc9.jpeg",
      "fullname": "Areg Mikael Sarvazyan",
      "name": "asarvazyan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.15921",
      "authors": [
        {
          "_id": "680ab2f808464b525df64b07",
          "name": "Jian Hu",
          "hidden": false
        },
        {
          "_id": "680ab2f808464b525df64b08",
          "name": "Dimitrios Korkinof",
          "hidden": false
        },
        {
          "_id": "680ab2f808464b525df64b09",
          "name": "Shaogang Gong",
          "hidden": false
        },
        {
          "_id": "680ab2f808464b525df64b0a",
          "name": "Mariano Beguerisse-Diaz",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-22T14:06:01.000Z",
      "submittedOnDailyAt": "2025-04-25T06:39:24.280Z",
      "title": "VISMaP : Réduction manuelle de vidéos en temps réel basée sur l'entraînement expérientiel non sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub",
      "submittedOnDailyBy": {
        "_id": "65e1b6e9501590df0173cbd3",
        "avatarUrl": "/avatars/a73e2139700e23eff455734c99cef5ba.svg",
        "isPro": false,
        "fullname": "Jian Hu",
        "user": "lwpyh",
        "type": "user"
      },
      "summary": "Introducing ViSMap : Une Système de Prompting Méta pour la Séquence de Vidéo Sans Personne. Ce système peut résumer une vidéo d'une heure sans personne de fond. Les modèles actuels de compréhension vidéo sont très efficaces pour des vidéos courtes ou des vidéos d'événements pré-segmentés, mais ils rencontrent des défis avec des vidéos longues où les événements liés sont répartis rarement et non pré-segmentés, rendant la séquence difficile. De plus, la compréhension de longues vidéos nécessite de multiples étapes d'entraînement avec des annotations détaillées, qui sont coûteuses, épuisant les temps et inconsistantes. ViSMap comble le vide entre des vidéos courtes (riches en données d'annotation) et des vidéos longues. En utilisant des LLMs, il génère des résumés de faits optimisés pour des vidéos longues à partir des explications de courts segments obtenus à partir de courtes vidéos. Ces résumés de faits sont utilisés comme données d'entraînement pour la séquence de vidéo longue, évitant la nécessité d'annotations détaillées. Spécifiquement, ViSMap adopte une stratégie de prompting méta pour générer et améliorer itérativement les résumés de faits pour des vidéos longues, guidés par des explications de courts clips. Dans chaque itération, trois LLMs sont utilisés séquentiellement : un pour générer les résumés de faits à partir des explications des clips, un autre pour évaluer eux, et un tiers pour optimiser le prompt du générateur. Cette itération est nécessaire car la qualité des résumés de faits dépend fortement du prompt du générateur et peut varier considérablement entre les vidéos. Évalué sur une large gamme de données de vision, ViSMap atteint un niveau de performance de pointe comparable aux meilleurs modèles dans tous les domaines sans dégradation de performance. Le code sera publié après la publication de l'article.",
      "upvotes": 3,
      "discussionId": "680ab2fb08464b525df64bd2",
      "ai_keywords": [
        "LLMs",
        "ViSMap",
        "Unsupervised Video Summarisation",
        "Meta Prompting",
        "long-form video understanding",
        "supervised hierarchical training",
        "pseudo-summaries",
        "short clip descriptions",
        "meta-prompting strategy",
        "generator prompt"
      ]
    },
    "publishedAt": "2025-04-22T10:06:01.000Z",
    "title": "ViSMaP: Unsupervised Hour-long Video Summarisation by Meta-Prompting",
    "summary": "We introduce ViSMap: Unsupervised Video Summarisation by Meta Prompting, a\nsystem to summarise hour long videos with no-supervision. Most existing video\nunderstanding models work well on short videos of pre-segmented events, yet\nthey struggle to summarise longer videos where relevant events are sparsely\ndistributed and not pre-segmented. Moreover, long-form video understanding\noften relies on supervised hierarchical training that needs extensive\nannotations which are costly, slow and prone to inconsistency. With ViSMaP we\nbridge the gap between short videos (where annotated data is plentiful) and\nlong ones (where it's not). We rely on LLMs to create optimised\npseudo-summaries of long videos using segment descriptions from short ones.\nThese pseudo-summaries are used as training data for a model that generates\nlong-form video summaries, bypassing the need for expensive annotations of long\nvideos. Specifically, we adopt a meta-prompting strategy to iteratively\ngenerate and refine creating pseudo-summaries of long videos. The strategy\nleverages short clip descriptions obtained from a supervised short video model\nto guide the summary. Each iteration uses three LLMs working in sequence: one\nto generate the pseudo-summary from clip descriptions, another to evaluate it,\nand a third to optimise the prompt of the generator. This iteration is\nnecessary because the quality of the pseudo-summaries is highly dependent on\nthe generator prompt, and varies widely among videos. We evaluate our summaries\nextensively on multiple datasets; our results show that ViSMaP achieves\nperformance comparable to fully supervised state-of-the-art models while\ngeneralising across domains without sacrificing performance. Code will be\nreleased upon publication.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15921.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65e1b6e9501590df0173cbd3",
      "avatarUrl": "/avatars/a73e2139700e23eff455734c99cef5ba.svg",
      "fullname": "Jian Hu",
      "name": "lwpyh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.17601",
      "authors": [
        {
          "_id": "680b2b8c6bd146aa35a48222",
          "user": {
            "_id": "64d496b04ab89be0de7fb1a9",
            "avatarUrl": "/avatars/fe60082c26e0d98126e62ee6257a374f.svg",
            "isPro": false,
            "fullname": "Erik Bergh",
            "user": "erikbergh",
            "type": "user"
          },
          "name": "Erik Bergh",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-04-25T06:39:03.785Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-24T14:26:42.000Z",
      "submittedOnDailyAt": "2025-04-25T05:00:56.733Z",
      "title": "Le méthode de réduction de dimension non-linéaire analytique utilisant des transformations linéaires avec des poids gaussiens",
      "submittedOnDailyBy": {
        "_id": "64d496b04ab89be0de7fb1a9",
        "avatarUrl": "/avatars/fe60082c26e0d98126e62ee6257a374f.svg",
        "isPro": false,
        "fullname": "Erik Bergh",
        "user": "erikbergh",
        "type": "user"
      },
      "summary": "Le méthode de réduction de dimensions joue un rôle fondamental dans l'analyse et la visualisation de données de haute dimension. Les techniques existantes telles que t-SNE et PCA présentent un compromis entre représentativité et explicabilité. Cet article présente une nouvelle approche pour résoudre ce compromis, en intégrant l'explicabilité des méthodes linéaires avec la représentativité des transformations non linéaires. L'algorithme proposé utilise des transformations linéaires et des fonctions gaussiennes pour construire un cartographie non linéaire entre des espaces de haute et de basse dimension. Cette architecture permet des transformations non linéaires complexes, tout en maintenant les avantages d'explicabilité des méthodes linéaires, et est conçue pour que chaque transformation puisse être analysée de manière indépendante. Enfin, elle offre une vision transparente de la réduction de dimensions et de l'espace transformé. Une méthode pour expliquer les transformations apprises est également présentée, incluant la détermination de l'importance des dimensions supprimées et les méthodes pour élargir et réduire l'espace. Ces instruments permettent aux utilisateurs de comprendre les relations géométriques qui sont maintenues ou modifiées lors du processus de réduction de dimensions. Pour garantir l'utilité pratique de l'algorithme, l'accent est mis sur le développement d'un package logiciel utilisateur-amiable et son introduction dans le domaine académique et industriel est promue.",
      "upvotes": 1,
      "discussionId": "680b2b8d6bd146aa35a48252",
      "projectPage": "https://github.com/erikbergh/interpretable_dim_reduction/",
      "githubRepo": "https://github.com/erikbergh/interpretable_dim_reduction/",
      "ai_keywords": [
        "t-SNE",
        "PCA",
        "non-linear mapping",
        "Gaussian functions",
        "linear transformations",
        "interpretability",
        "dimensionality reduction",
        "suppressed dimensions",
        "geometric relationships"
      ]
    },
    "publishedAt": "2025-04-24T10:26:42.000Z",
    "title": "Interpretable non-linear dimensionality reduction using gaussian\n  weighted linear transformation",
    "summary": "Dimensionality reduction techniques are fundamental for analyzing and\nvisualizing high-dimensional data. With established methods like t-SNE and PCA\npresenting a trade-off between representational power and interpretability.\nThis paper introduces a novel approach that bridges this gap by combining the\ninterpretability of linear methods with the expressiveness of non-linear\ntransformations. The proposed algorithm constructs a non-linear mapping between\nhigh-dimensional and low-dimensional spaces through a combination of linear\ntransformations, each weighted by Gaussian functions. This architecture enables\ncomplex non-linear transformations while preserving the interpretability\nadvantages of linear methods, as each transformation can be analyzed\nindependently. The resulting model provides both powerful dimensionality\nreduction and transparent insights into the transformed space. Techniques for\ninterpreting the learned transformations are presented, including methods for\nidentifying suppressed dimensions and how space is expanded and contracted.\nThese tools enable practitioners to understand how the algorithm preserves and\nmodifies geometric relationships during dimensionality reduction. To ensure the\npractical utility of this algorithm, the creation of user-friendly software\npackages is emphasized, facilitating its adoption in both academia and\nindustry.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17601.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64d496b04ab89be0de7fb1a9",
      "avatarUrl": "/avatars/fe60082c26e0d98126e62ee6257a374f.svg",
      "fullname": "Erik Bergh",
      "name": "erikbergh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.17414",
      "authors": [
        {
          "_id": "680b3f12c131c3be24f80ce0",
          "name": "Min Wei",
          "hidden": false
        },
        {
          "_id": "680b3f12c131c3be24f80ce1",
          "name": "Chaohui Yu",
          "hidden": false
        },
        {
          "_id": "680b3f12c131c3be24f80ce2",
          "name": "Jingkai Zhou",
          "hidden": false
        },
        {
          "_id": "680b3f12c131c3be24f80ce3",
          "name": "Fan Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-24T10:12:40.000Z",
      "submittedOnDailyAt": "2025-04-25T06:22:09.592Z",
      "title": "3DV-TON : 3D Guide par Modèle Distribué pour la Photographie 3D Coincidente dans le Vidéo Emboîté",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Les vidéochats de vêtement remplacent le vêtement d'une film par un vêtement spécifique. Les méthodes existantes ont des difficultés à maintenir la qualité et la cohérence temporelle dans les résultats de haute qualité lorsqu'il s'agit de motifs complexes de vêtements et de diverses positions corporelles. Nous présentons un nouveau cadre de travail basé sur la diffusion qui maintient la qualité et la cohérence temporelle. Notre approche utilise un guide dynamique généré de manière explicite à chaque frame, permettant au modèle de prioriser la qualité de l'extérieur tout en maintenant la cohérence du comportement, ce qui résout le problème de perdre la cohérence du comportement en prioritisant la qualité de l'extérieur. Cela est réalisé en se référant directement au mouvement du guide de vêtement qui coïncide tout au long de la séquence vidéo. Notre méthode caractérise un flux adaptatif en pile pour générer un guide 3D dynamique : 1) nous sélectionnons un frame clé d'un vidéochat de vêtement 2D, et 2) nous reconstruisons et animons la pose originale de la film et le guide de texture synchronisé. De plus, nous présentons une stratégie de masquage rectangulaire forte pour réduire la propagation d'artefacts causés par la perte directe d'information de vêtement due au mouvement humain et du vêtement. Pour l'étude des vidéochats de vêtement, nous présentons un ensemble de données à haute résolution qui comprend 130 vidéos avec différents types de vêtements et scénarios. Nous démontrons notre excellent rendement grâce à des résultats qualitatifs et quantitatifs. Le site web du projet est disponible sur : https://2y7c3.github.io/3DV-TON/",
      "upvotes": 1,
      "discussionId": "680b3f15c131c3be24f80d65",
      "ai_keywords": [
        "diffusion-based framework",
        "animatable textured 3D meshes",
        "frame-level guidance",
        "motion coherence",
        "garment texture movements",
        "adaptive pipeline",
        "keyframe",
        "2D image try-on",
        "textured 3D mesh",
        "synchronized with original video poses",
        "rectangular masking strategy",
        "artifact propagation",
        "HR-VVT",
        "high-resolution benchmark dataset"
      ]
    },
    "publishedAt": "2025-04-24T06:12:40.000Z",
    "title": "3DV-TON: Textured 3D-Guided Consistent Video Try-on via Diffusion Models",
    "summary": "Video try-on replaces clothing in videos with target garments. Existing\nmethods struggle to generate high-quality and temporally consistent results\nwhen handling complex clothing patterns and diverse body poses. We present\n3DV-TON, a novel diffusion-based framework for generating high-fidelity and\ntemporally consistent video try-on results. Our approach employs generated\nanimatable textured 3D meshes as explicit frame-level guidance, alleviating the\nissue of models over-focusing on appearance fidelity at the expanse of motion\ncoherence. This is achieved by enabling direct reference to consistent garment\ntexture movements throughout video sequences. The proposed method features an\nadaptive pipeline for generating dynamic 3D guidance: (1) selecting a keyframe\nfor initial 2D image try-on, followed by (2) reconstructing and animating a\ntextured 3D mesh synchronized with original video poses. We further introduce a\nrobust rectangular masking strategy that successfully mitigates artifact\npropagation caused by leaking clothing information during dynamic human and\ngarment movements. To advance video try-on research, we introduce HR-VVT, a\nhigh-resolution benchmark dataset containing 130 videos with diverse clothing\ntypes and scenarios. Quantitative and qualitative results demonstrate our\nsuperior performance over existing methods. The project page is at this link\nhttps://2y7c3.github.io/3DV-TON/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17414.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6713
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.17343",
      "authors": [
        {
          "_id": "680b24471c5fbd15909bf1f9",
          "name": "Linli Yao",
          "hidden": false
        },
        {
          "_id": "680b24471c5fbd15909bf1fa",
          "name": "Yicheng Li",
          "hidden": false
        },
        {
          "_id": "680b24471c5fbd15909bf1fb",
          "name": "Yuancheng Wei",
          "hidden": false
        },
        {
          "_id": "680b24471c5fbd15909bf1fc",
          "name": "Lei Li",
          "hidden": false
        },
        {
          "_id": "680b24471c5fbd15909bf1fd",
          "name": "Shuhuai Ren",
          "hidden": false
        },
        {
          "_id": "680b24471c5fbd15909bf1fe",
          "name": "Yuanxin Liu",
          "hidden": false
        },
        {
          "_id": "680b24471c5fbd15909bf1ff",
          "name": "Kun Ouyang",
          "hidden": false
        },
        {
          "_id": "680b24471c5fbd15909bf200",
          "name": "Lean Wang",
          "hidden": false
        },
        {
          "_id": "680b24471c5fbd15909bf201",
          "name": "Shicheng Li",
          "hidden": false
        },
        {
          "_id": "680b24471c5fbd15909bf202",
          "name": "Sida Li",
          "hidden": false
        },
        {
          "_id": "680b24471c5fbd15909bf203",
          "name": "Lingpeng Kong",
          "hidden": false
        },
        {
          "_id": "680b24471c5fbd15909bf204",
          "name": "Qi Liu",
          "hidden": false
        },
        {
          "_id": "680b24471c5fbd15909bf205",
          "name": "Yuanxing Zhang",
          "hidden": false
        },
        {
          "_id": "680b24471c5fbd15909bf206",
          "name": "Xu Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-24T07:59:46.000Z",
      "submittedOnDailyAt": "2025-04-25T06:34:54.517Z",
      "title": "TIMECHAT ONLINE : En les streaming de films, le 80 % des tokens visuels sont naturellement rédundants.",
      "submittedOnDailyBy": {
        "_id": "655ca347f426a304c6b393a1",
        "avatarUrl": "/avatars/67f0310d59c5912d38c2ad8e6448614d.svg",
        "isPro": false,
        "fullname": "Linli Yao",
        "user": "yaolily",
        "type": "user"
      },
      "summary": "Le rapide croissance des plateformes de vidéo en ligne, en particulier grâce aux services de streaming en direct, a généré une demande urgente de systèmes capables de comprendre les vidéos en temps réel. Ces systèmes doivent traiter des vidéos en flux continu et répondre immédiatement aux demandes de l'utilisateur. Actuellement, les Modèles de Langue de Vidéo Grand (VideoLLMs) dépassent la compréhension complète des vidéos, mais présentent des limites pour traiter efficacement les grandes et continus cadres de vidéo en streaming. Nous présentons TimeChat-Online, un nouveau VideoLLM qui innove dans la création de modèles interactifs de vidéo en temps réel. Le cœur de cette innovation est notre Module de Différentiel de Token Drop (DTD). DTD aborde les problèmes fondamentaux de la redondance visuelle dans les vidéos en streaming. En exploitant l'énergie de la change blindness, l'absence de perception visuelle humaine, DTD filtre le contenu statique et redondant entre les cadres, maintenant la variation temporelle. Dans les expérimentations, DTD a réduit de 82,8% les tokens de vidéo sur StreamingBench, tout en maintenant 98% de l'efficacité, et a démontré que plus de 80% du contenu visuel était naturellement redondant. Il est montré qu'il n'est pas nécessaire de guide pour une vidéo en streaming. Cette répétition est effectuée 11 fois : \"Il est montré qu'il n'est pas nécessaire de guide pour une vidéo en streaming.\"",
      "upvotes": 1,
      "discussionId": "680b244a1c5fbd15909bf2ff",
      "ai_keywords": [
        "Video Large Language Models (VideoLLMs)",
        "Differential Token Drop (DTD)",
        "Change Blindness phenomenon",
        "TimeChat-Online",
        "TimeChat-Online-139K",
        "StreamingBench",
        "OvOBench",
        "Video-MME",
        "MLVU",
        "Proactive Response",
        "real-time video interaction",
        "continuous video streams",
        "user queries",
        "visual redundancy",
        "dense, redundant frames",
        "visual content",
        "video tokens",
        "meaningful temporal changes",
        "static, redundant content",
        "backward-tracing",
        "current-perception",
        "future-responding scenarios"
      ]
    },
    "publishedAt": "2025-04-24T03:59:46.000Z",
    "title": "TimeChat-Online: 80% Visual Tokens are Naturally Redundant in Streaming\n  Videos",
    "summary": "The rapid growth of online video platforms, particularly live streaming\nservices, has created an urgent need for real-time video understanding systems.\nThese systems must process continuous video streams and respond to user queries\ninstantaneously, presenting unique challenges for current Video Large Language\nModels (VideoLLMs). While existing VideoLLMs excel at processing complete\nvideos, they face significant limitations in streaming scenarios due to their\ninability to handle dense, redundant frames efficiently. We introduce\nTimeChat-Online, a novel online VideoLLM that revolutionizes real-time video\ninteraction. At its core lies our innovative Differential Token Drop (DTD)\nmodule, which addresses the fundamental challenge of visual redundancy in\nstreaming videos. Drawing inspiration from human visual perception's Change\nBlindness phenomenon, DTD preserves meaningful temporal changes while filtering\nout static, redundant content between frames. Remarkably, our experiments\ndemonstrate that DTD achieves an 82.8% reduction in video tokens while\nmaintaining 98% performance on StreamingBench, revealing that over 80% of\nvisual content in streaming videos is naturally redundant without requiring\nlanguage guidance. To enable seamless real-time interaction, we present\nTimeChat-Online-139K, a comprehensive streaming video dataset featuring diverse\ninteraction patterns including backward-tracing, current-perception, and\nfuture-responding scenarios. TimeChat-Online's unique Proactive Response\ncapability, naturally achieved through continuous monitoring of video scene\ntransitions via DTD, sets it apart from conventional approaches. Our extensive\nevaluation demonstrates TimeChat-Online's superior performance on streaming\nbenchmarks (StreamingBench and OvOBench) and maintaining competitive results on\nlong-form video tasks such as Video-MME and MLVU.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17343.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "655ca347f426a304c6b393a1",
      "avatarUrl": "/avatars/67f0310d59c5912d38c2ad8e6448614d.svg",
      "fullname": "Linli Yao",
      "name": "yaolily",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.16064",
      "authors": [
        {
          "_id": "680b494c6bd146aa35ab2e1c",
          "name": "Theodoros Kouzelis",
          "hidden": false
        },
        {
          "_id": "680b494c6bd146aa35ab2e1d",
          "name": "Efstathios Karypidis",
          "hidden": false
        },
        {
          "_id": "680b494c6bd146aa35ab2e1e",
          "name": "Ioannis Kakogeorgiou",
          "hidden": false
        },
        {
          "_id": "680b494c6bd146aa35ab2e1f",
          "name": "Spyros Gidaris",
          "hidden": false
        },
        {
          "_id": "680b494c6bd146aa35ab2e20",
          "name": "Nikos Komodakis",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/677272184d148b904333e874/QkosGJUTQX94tAZPCF46-.png"
      ],
      "publishedAt": "2025-04-22T17:41:42.000Z",
      "submittedOnDailyAt": "2025-04-25T07:32:33.466Z",
      "title": "Modèle d'images-caractéristiques synthétiques pour améliorer le modèle d'images générées",
      "submittedOnDailyBy": {
        "_id": "677272184d148b904333e874",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/5dUau7gxLk4Wm1TiiJJri.jpeg",
        "isPro": false,
        "fullname": "Efstathios Karypidis",
        "user": "Sta8is",
        "type": "user"
      },
      "summary": "Les modules de dispersion potentielle (LDMs) occupent la majorité des positions dans le domaine de la génération d'images de haute qualité, mais l'intégration de l'apprentissage de représentation et du modélisation de génération est un problème complexe. Nous présentons un nouveau cadre de modélisation d'images générées utilisant des modèles de réseaux neuronaux profonds, qui permettent de modéliser ensemble les potentiels d'images de bas niveau (obtenus d'un autoencodeur variationnel) et les caractéristiques significatives de niveau élevé (obtenues d'un autoencodeur de reconnaissance automatique pré-entraîné comme DINO). Notre approche d'apprentissage profond potentiel permet au modèle d'apprendre à générer des paires de caractéristiques de l'image à partir d'un bruit pur, améliorant significativement la qualité de la génération et l'efficacité de l'apprentissage, avec un minimum de modifications dans l'architecture standard de transformateurs de diffusion. Son avantage est qu'il ne nécessite pas de conceptions complexes, ce qui simplifie l'apprentissage et développe une nouvelle stratégie de l'inférence puissante appelée \"guide de représentation\". Évalués tant conditionnels que non conditionnels, notre méthode améliore significativement la qualité des images et la vitesse de convergence de l'apprentissage, établissant une nouvelle direction dans le modélisation de la génération de représentations.",
      "upvotes": 1,
      "discussionId": "680b494e6bd146aa35ab2e97",
      "githubRepo": "https://github.com/zelaki/ReDi",
      "ai_keywords": [
        "latent diffusion models (LDMs)",
        "generative image modeling",
        "diffusion model",
        "low-level image latents",
        "variational autoencoder",
        "high-level semantic features",
        "pretrained self-supervised encoder",
        "DINO",
        "latent-semantic diffusion",
        "coherent image-feature pairs",
        "generative quality",
        "training efficiency",
        "Diffusion Transformer architectures",
        "complex distillation objectives",
        "Representation Guidance",
        "image quality",
        "training convergence speed",
        "representation-aware generative modeling"
      ]
    },
    "publishedAt": "2025-04-22T13:41:42.000Z",
    "title": "Boosting Generative Image Modeling via Joint Image-Feature Synthesis",
    "summary": "Latent diffusion models (LDMs) dominate high-quality image generation, yet\nintegrating representation learning with generative modeling remains a\nchallenge. We introduce a novel generative image modeling framework that\nseamlessly bridges this gap by leveraging a diffusion model to jointly model\nlow-level image latents (from a variational autoencoder) and high-level\nsemantic features (from a pretrained self-supervised encoder like DINO). Our\nlatent-semantic diffusion approach learns to generate coherent image-feature\npairs from pure noise, significantly enhancing both generative quality and\ntraining efficiency, all while requiring only minimal modifications to standard\nDiffusion Transformer architectures. By eliminating the need for complex\ndistillation objectives, our unified design simplifies training and unlocks a\npowerful new inference strategy: Representation Guidance, which leverages\nlearned semantics to steer and refine image generation. Evaluated in both\nconditional and unconditional settings, our method delivers substantial\nimprovements in image quality and training convergence speed, establishing a\nnew direction for representation-aware generative modeling.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/677272184d148b904333e874/QkosGJUTQX94tAZPCF46-.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16064.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "677272184d148b904333e874",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/5dUau7gxLk4Wm1TiiJJri.jpeg",
      "fullname": "Efstathios Karypidis",
      "name": "Sta8is",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  }
]