[
  {
    "paper": {
      "id": "2504.20734",
      "authors": [
        {
          "_id": "6811966ae20ba7d0683b8adc",
          "user": {
            "_id": "66d30f5fad293ffc4b7672bc",
            "avatarUrl": "/avatars/6f164d813b947940a088820f8fd4dbe8.svg",
            "isPro": false,
            "fullname": "Woongyeong Yeo",
            "user": "wgcyeo",
            "type": "user"
          },
          "name": "Woongyeong Yeo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-30T07:56:03.853Z",
          "hidden": false
        },
        {
          "_id": "6811966ae20ba7d0683b8add",
          "user": {
            "_id": "66ed7737f2f27a5dfd81ef09",
            "avatarUrl": "/avatars/f45eea356e92ac7b3db23c2c92dec9fa.svg",
            "isPro": false,
            "fullname": "Kangsan Kim",
            "user": "KangsanKim71",
            "type": "user"
          },
          "name": "Kangsan Kim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-30T07:56:00.948Z",
          "hidden": false
        },
        {
          "_id": "6811966ae20ba7d0683b8ade",
          "name": "Soyeong Jeong",
          "hidden": false
        },
        {
          "_id": "6811966ae20ba7d0683b8adf",
          "user": {
            "_id": "63036b6c5c70c21d0ea79d48",
            "avatarUrl": "/avatars/a7eb03f5cbd4eaa09fe807bbed8bc0f7.svg",
            "isPro": false,
            "fullname": "Jinheon Baek",
            "user": "jinheon",
            "type": "user"
          },
          "name": "Jinheon Baek",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-30T07:56:07.351Z",
          "hidden": false
        },
        {
          "_id": "6811966ae20ba7d0683b8ae0",
          "name": "Sung Ju Hwang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-29T13:18:58.000Z",
      "submittedOnDailyAt": "2025-04-30T01:50:28.624Z",
      "title": "UniversalRAG : Recherche d'une fusion entre plusieurs documents, modèles divers et niveaux de granularité, génération de raisons de la vision universelle",
      "submittedOnDailyBy": {
        "_id": "66d30f5fad293ffc4b7672bc",
        "avatarUrl": "/avatars/6f164d813b947940a088820f8fd4dbe8.svg",
        "isPro": false,
        "fullname": "Woongyeong Yeo",
        "user": "wgcyeo",
        "type": "user"
      },
      "summary": "La RAG (Retrieval-Augmented Generation) montre la possibilité de significativement améliorer la précision factuelle des réponses du modèle en se basant sur des connaissances externes liées à la question. Cependant, l'approche actuelle de la RAG est principalement limitée aux bases de texte, et les efforts récents ont traité d'étendre la RAG aux images ou aux vidéos, mais généralement fonctionnent dans un corpus propre du modèle. Les questions réelles soulèvent un large éventail de sources de connaissances qui ne peuvent pas être explorées dans un seul point de départ. Dans ce contexte, nous présentons un nouveau cadre de travail de RAG appelé \"UniversalRAG\", qui permet la recherche et l'intégration de connaissances de différents modèles et modalités. En particulier, UniversalRAG oblige tous les modèles à obtenir une représentation unifiée à partir d'un seul espace de connaissances intégrés, et après avoir observé les échanges entre modèles, propose la sélection dynamique d'un corpus optimal pour chaque modèle et l'exécution de recherches spécifiques. De plus, UniversalRAG permet aux modèles dans un plus large éventail d'applications, en assimilant chaque modèle à des niveaux d'entrée différents et en facilitant des recherches appropriées en fonction de la complexité et de l'étendue de la question. UniversalRAG a été validé sur 8 cadres de référence et montre un rendement excellent, dépassant à la fois les limites de base du modèle et la ligne de base unifiée.",
      "upvotes": 37,
      "discussionId": "6811966ae20ba7d0683b8b0e",
      "projectPage": "https://universalrag.github.io",
      "githubRepo": "https://github.com/wgcyeo/UniversalRAG",
      "ai_keywords": [
        "Retrieval-Augmented Generation (RAG)",
        "factual accuracy",
        "external knowledge",
        "text-only corpus",
        "modality-specific corpus",
        "heterogenous sources",
        "diverse modalities",
        "granularities",
        "modality gap",
        "modality-aware routing mechanism",
        "targeted retrieval",
        "granularity levels",
        "fine-tuned retrieval",
        "multi-modal benchmarks",
        "modality-specific baselines",
        "unified baselines"
      ]
    },
    "publishedAt": "2025-04-29T09:18:58.000Z",
    "title": "UniversalRAG: Retrieval-Augmented Generation over Multiple Corpora with\n  Diverse Modalities and Granularities",
    "summary": "Retrieval-Augmented Generation (RAG) has shown substantial promise in\nimproving factual accuracy by grounding model responses with external knowledge\nrelevant to queries. However, most existing RAG approaches are limited to a\ntext-only corpus, and while recent efforts have extended RAG to other\nmodalities such as images and videos, they typically operate over a single\nmodality-specific corpus. In contrast, real-world queries vary widely in the\ntype of knowledge they require, which a single type of knowledge source cannot\naddress. To address this, we introduce UniversalRAG, a novel RAG framework\ndesigned to retrieve and integrate knowledge from heterogeneous sources with\ndiverse modalities and granularities. Specifically, motivated by the\nobservation that forcing all modalities into a unified representation space\nderived from a single combined corpus causes a modality gap, where the\nretrieval tends to favor items from the same modality as the query, we propose\na modality-aware routing mechanism that dynamically identifies the most\nappropriate modality-specific corpus and performs targeted retrieval within it.\nAlso, beyond modality, we organize each modality into multiple granularity\nlevels, enabling fine-tuned retrieval tailored to the complexity and scope of\nthe query. We validate UniversalRAG on 8 benchmarks spanning multiple\nmodalities, showing its superiority over modality-specific and unified\nbaselines.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.20734.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66d30f5fad293ffc4b7672bc",
      "avatarUrl": "/avatars/6f164d813b947940a088820f8fd4dbe8.svg",
      "fullname": "Woongyeong Yeo",
      "name": "wgcyeo",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.20571",
      "authors": [
        {
          "_id": "681187ddda5ce4cbd7556714",
          "user": {
            "_id": "653586fae778506c5b38a3f1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653586fae778506c5b38a3f1/GL_RShZhAkEZmIinA5_8E.jpeg",
            "isPro": false,
            "fullname": "Yiping Wang",
            "user": "ypwang61",
            "type": "user"
          },
          "name": "Yiping Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-30T09:58:59.486Z",
          "hidden": false
        },
        {
          "_id": "681187ddda5ce4cbd7556715",
          "user": {
            "_id": "673a83b99e6f1c0d81a771fc",
            "avatarUrl": "/avatars/f3d8e1bf7d4c36b21adee632ea12ffe0.svg",
            "isPro": false,
            "fullname": "Qing Yang",
            "user": "hushqyang",
            "type": "user"
          },
          "name": "Qing Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-30T07:56:17.953Z",
          "hidden": false
        },
        {
          "_id": "681187ddda5ce4cbd7556716",
          "user": {
            "_id": "64a85e23b6512b8328f9d9e2",
            "avatarUrl": "/avatars/4a6b35752d3f76cb03278f52b3b43426.svg",
            "isPro": false,
            "fullname": "Zhiyuan Zeng",
            "user": "ZhiyuanZeng",
            "type": "user"
          },
          "name": "Zhiyuan Zeng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-30T09:59:12.620Z",
          "hidden": false
        },
        {
          "_id": "681187ddda5ce4cbd7556717",
          "user": {
            "_id": "63815eff4761ddfa00903762",
            "avatarUrl": "/avatars/3419b239d42e091586f1c51b526d88e5.svg",
            "isPro": false,
            "fullname": "Liliang Ren",
            "user": "renll",
            "type": "user"
          },
          "name": "Liliang Ren",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-30T09:59:18.310Z",
          "hidden": false
        },
        {
          "_id": "681187ddda5ce4cbd7556718",
          "name": "Lucas Liu",
          "hidden": false
        },
        {
          "_id": "681187ddda5ce4cbd7556719",
          "user": {
            "_id": "61942296d5c2ba6daa290357",
            "avatarUrl": "/avatars/594021cc183c4922d48b46f43772a062.svg",
            "isPro": false,
            "fullname": "Baolin Peng",
            "user": "Baolin",
            "type": "user"
          },
          "name": "Baolin Peng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-30T09:59:45.735Z",
          "hidden": false
        },
        {
          "_id": "681187ddda5ce4cbd755671a",
          "name": "Hao Cheng",
          "hidden": false
        },
        {
          "_id": "681187ddda5ce4cbd755671b",
          "user": {
            "_id": "6310493158d83e8f64dc8c55",
            "avatarUrl": "/avatars/5f91ac4dfec0d6a5bf7bad6094f0fd0f.svg",
            "isPro": false,
            "fullname": "Xuehai He",
            "user": "Xuehai",
            "type": "user"
          },
          "name": "Xuehai He",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-30T09:59:52.344Z",
          "hidden": false
        },
        {
          "_id": "681187ddda5ce4cbd755671c",
          "user": {
            "_id": "633523b131a2be3938ca1016",
            "avatarUrl": "/avatars/06a18f80927289bb949d9f19ffdc4bda.svg",
            "isPro": false,
            "fullname": "Kuan Wang",
            "user": "Keynes",
            "type": "user"
          },
          "name": "Kuan Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-30T09:59:58.392Z",
          "hidden": false
        },
        {
          "_id": "681187ddda5ce4cbd755671d",
          "user": {
            "_id": "641904caf9d6f1d772ec7af7",
            "avatarUrl": "/avatars/4a63eac71eb30f70b1a0e9d4708f26c1.svg",
            "isPro": false,
            "fullname": "Jianfeng Gao",
            "user": "wyngjf",
            "type": "user"
          },
          "name": "Jianfeng Gao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-30T10:00:04.685Z",
          "hidden": false
        },
        {
          "_id": "681187ddda5ce4cbd755671e",
          "user": {
            "_id": "64da876370446182be5b608d",
            "avatarUrl": "/avatars/e412fdc71404ecdf638e416846e3ebfb.svg",
            "isPro": false,
            "fullname": "Weizhu Chen",
            "user": "chenweizhu",
            "type": "user"
          },
          "name": "Weizhu Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-30T10:00:10.823Z",
          "hidden": false
        },
        {
          "_id": "681187ddda5ce4cbd755671f",
          "user": {
            "_id": "6463b2247572c66a8e625a57",
            "avatarUrl": "/avatars/7722fb5649d42d966ce1e478946d5f8f.svg",
            "isPro": false,
            "fullname": "Wang",
            "user": "Shuohang",
            "type": "user"
          },
          "name": "Shuohang Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-30T10:00:19.855Z",
          "hidden": false
        },
        {
          "_id": "681187ddda5ce4cbd7556720",
          "name": "Simon Shaolei Du",
          "hidden": false
        },
        {
          "_id": "681187ddda5ce4cbd7556721",
          "user": {
            "_id": "6454c337a13edf669cd5d8ea",
            "avatarUrl": "/avatars/a383a0dda7c2ef6a0d6c3c64651f42ff.svg",
            "isPro": false,
            "fullname": "Yelong Shen",
            "user": "uuu6",
            "type": "user"
          },
          "name": "Yelong Shen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-30T10:00:33.179Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-29T09:24:30.000Z",
      "submittedOnDailyAt": "2025-04-30T00:46:23.617Z",
      "title": "1- Point d'apprentissage et exemple d'apprentissage logique dans des modèles de langage à grande échelle",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Nous avons montré que l'apprentissage par renforcement avec récompenses vérifiables (RLVR) fonctionne efficacement comme incitatif pour améliorer la capacité d'inférence mathématique de grands modèles de langue (LLMs) en utilisant un seul exemple d'entraînement (1-shot RLVR). En appliquant RLVR à un modèle basé sur Qwen2.5-Math-1.5B, nous avons augmenté la performance du modèle sur MATH500 de 36.0% à 73.6%, et la moyenne des 6 benchmarks mathématiques généraux a été améliorée de 17.6% à 35.7%. Ces résultats correspondent à ceux obtenus sur le sous-ensemble de 1.2k DeepScaleR (MATH500 : 73.6%, moyenne : 35.9%), y compris également les exemples mentionnés précédemment. Une amélioration significative a été observée sur divers modèles (Qwen2.5-Math-7B, Llama3.2-3B-Instruct, DeepSeek-R1-Distill-Qwen-1.5B), algorithmes d'apprentissage par renforcement (GRPO et PPO) et d'autres exemples mathématiques (beaucoup amélioraient plus de 30% sur MATH500). De plus, lors du processus d'entraînement avec RLVR 1-shot, des phénomènes intéressants ont été observés, tels que la généralisation croisée, l'augmentation de la fréquence d'auto-reflexion et l'amélioration de la performance sur le test après la consolidation de l'entraînement. Ces phénomènes ont été appelés \"généralisation après la consolidation\". De plus, nous avons vérifié que l'efficacité de RLVR est principalement due à la perte de gradient de la politique. Nous avons également démontré l'importance d'ajuster le coefficient approprié de la perte d'entropie pour promouvoir l'exploration lors de l'entraînement avec RLVR 1-shot. De plus, nous avons observé que l'application uniquement de la perte d'entropie et la manque de récompenses a entraîné une augmentation de 27.4% de la performance de Qwen2.5-Math-1.5B sur MATH500. Ces résultats encouragent la recherche future sur l'efficacité des données RLVR et invitent à une révision des avancées récentes et mécanismes fondamentaux de RLVR. Notre code, nos modèles et nos données sont disponibles sous licence open source sur https://github.com/ypwang61/One-Shot-RLVR.",
      "upvotes": 30,
      "discussionId": "681187ddda5ce4cbd7556754",
      "ai_keywords": [
        "reinforcement learning with verifiable reward (RLVR)",
        "1-shot RLVR",
        "large language models (LLMs)",
        "Qwen2.5-Math-1.5B",
        "MATH500",
        "mathematical reasoning benchmarks",
        "Qwen2.5-Math-7B",
        "Llama3.2-3B-Instruct",
        "DeepSeek-R1-Distill-Qwen-1.5B",
        "GRPO",
        "PPO",
        "cross-domain generalization",
        "self-reflection",
        "post-saturation generalization",
        "policy gradient loss",
        "entropic exploration",
        "entropy loss"
      ]
    },
    "publishedAt": "2025-04-29T05:24:30.000Z",
    "title": "Reinforcement Learning for Reasoning in Large Language Models with One\n  Training Example",
    "summary": "We show that reinforcement learning with verifiable reward using one training\nexample (1-shot RLVR) is effective in incentivizing the math reasoning\ncapabilities of large language models (LLMs). Applying RLVR to the base model\nQwen2.5-Math-1.5B, we identify a single example that elevates model performance\non MATH500 from 36.0% to 73.6%, and improves the average performance across six\ncommon mathematical reasoning benchmarks from 17.6% to 35.7%. This result\nmatches the performance obtained using the 1.2k DeepScaleR subset (MATH500:\n73.6%, average: 35.9%), which includes the aforementioned example. Similar\nsubstantial improvements are observed across various models (Qwen2.5-Math-7B,\nLlama3.2-3B-Instruct, DeepSeek-R1-Distill-Qwen-1.5B), RL algorithms (GRPO and\nPPO), and different math examples (many of which yield approximately 30% or\ngreater improvement on MATH500 when employed as a single training example). In\naddition, we identify some interesting phenomena during 1-shot RLVR, including\ncross-domain generalization, increased frequency of self-reflection, and\nsustained test performance improvement even after the training accuracy has\nsaturated, a phenomenon we term post-saturation generalization. Moreover, we\nverify that the effectiveness of 1-shot RLVR primarily arises from the policy\ngradient loss, distinguishing it from the \"grokking\" phenomenon. We also show\nthe critical role of promoting exploration (e.g., by adding entropy loss with\nan appropriate coefficient) in 1-shot RLVR training. As a bonus, we observe\nthat applying entropy loss alone, without any outcome reward, significantly\nenhances Qwen2.5-Math-1.5B's performance on MATH500 by 27.4%. These findings\ncan inspire future work on RLVR data efficiency and encourage a re-examination\nof both recent progress and the underlying mechanisms in RLVR. Our code, model,\nand data are open source at https://github.com/ypwang61/One-Shot-RLVR",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.20571.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6748
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.20595",
      "authors": [
        {
          "_id": "68118a9f4570c2ba44bf4418",
          "user": {
            "_id": "6334a0bd31a2be3938c59537",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6334a0bd31a2be3938c59537/kSetFUWAmJbPQ1KSlNKBr.jpeg",
            "isPro": false,
            "fullname": "Rulin Shao",
            "user": "rulins",
            "type": "user"
          },
          "name": "Rulin Shao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-30T10:02:20.688Z",
          "hidden": false
        },
        {
          "_id": "68118a9f4570c2ba44bf4419",
          "user": {
            "_id": "64ff618c35ec9717626d1431",
            "avatarUrl": "/avatars/941befd75925d6b691133f84cce525f9.svg",
            "isPro": false,
            "fullname": "Rui Qiao",
            "user": "volpato30",
            "type": "user"
          },
          "name": "Rui Qiao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-30T10:02:05.204Z",
          "hidden": false
        },
        {
          "_id": "68118a9f4570c2ba44bf441a",
          "name": "Varsha Kishore",
          "hidden": false
        },
        {
          "_id": "68118a9f4570c2ba44bf441b",
          "user": {
            "_id": "5f1eb362eec0ad2a071ad6e2",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5f1eb362eec0ad2a071ad6e2/IXMYkYKuTwn6kBdWnQeeY.png",
            "isPro": false,
            "fullname": "Niklas Muennighoff",
            "user": "Muennighoff",
            "type": "user"
          },
          "name": "Niklas Muennighoff",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-30T10:02:27.811Z",
          "hidden": false
        },
        {
          "_id": "68118a9f4570c2ba44bf441c",
          "name": "Xi Victoria Lin",
          "hidden": false
        },
        {
          "_id": "68118a9f4570c2ba44bf441d",
          "name": "Daniela Rus",
          "hidden": false
        },
        {
          "_id": "68118a9f4570c2ba44bf441e",
          "name": "Bryan Kian Hsiang Low",
          "hidden": false
        },
        {
          "_id": "68118a9f4570c2ba44bf441f",
          "user": {
            "_id": "63a76d0de27a6dbd485fe863",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a76d0de27a6dbd485fe863/qJJwHOuvyQGq1o0KscOF_.jpeg",
            "isPro": false,
            "fullname": "Sewon Min",
            "user": "sewon",
            "type": "user"
          },
          "name": "Sewon Min",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-30T10:01:46.233Z",
          "hidden": false
        },
        {
          "_id": "68118a9f4570c2ba44bf4420",
          "name": "Wen-tau Yih",
          "hidden": false
        },
        {
          "_id": "68118a9f4570c2ba44bf4421",
          "user": {
            "_id": "641b4263abfce26bcf7b27de",
            "avatarUrl": "/avatars/e91b4205e4f74b0dd8c333c23203a924.svg",
            "isPro": false,
            "fullname": "Pang Wei Koh",
            "user": "pangwei",
            "type": "user"
          },
          "name": "Pang Wei Koh",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-30T10:01:57.373Z",
          "hidden": false
        },
        {
          "_id": "68118a9f4570c2ba44bf4422",
          "name": "Luke Zettlemoyer",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-29T09:49:28.000Z",
      "submittedOnDailyAt": "2025-04-30T00:58:16.950Z",
      "title": "Développement de outils de recherche pour soutenir l'entraînement de l'IR",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "ReasonIR-8B est le premier modèle entraîné spécialement pour des tâches logiques générales. Actuellement, ce modèle est appliqué à une variété de tâches, mais il montre un effet limité dans des tâches logiques. L'ensemble de données d'entraînement actuel se concentre sur des questions de fait simples sur des documents. Nous avons développé un pipeline pour générer des questions complexes négatives qui sont pertinentes mais non utiles, y compris des exemples négatifs. En utilisant ce pipeline, le modèle ReasonIR-8B, entraîné avec des données synthétiques et des données publiques, a atteint de nouveaux niveaux d'excellence sur le benchmark logique BRIGHT, avec un nDCG@10 de 29,9 sans ré-ranking et de 36,9 avec ré-ranking. Dans les tâches de RAG, ReasonIR-8B a augmenté le rendement dans les jeux de données MMLU et GPQA de 6,4% et 22,6% respectivement, et peut concourir avec d'autres modèles et de la recherche. De plus, ReasonIR-8B utilise la computation de test de manière plus efficace : améliore constamment le rendement avec des questions d'information riches et longues sur BRIGHT, et peut être combiné avec un ré-ranking de LLM pour concourir avec d'autres modèles. Notre conception d'entraînement est générale et peut facilement être étendue aux modèles de LLM futurs. Par conséquent, nous ouvrons notre code, nos données et notre modèle sous une licence de code open.",
      "upvotes": 22,
      "discussionId": "68118aa44570c2ba44bf457b",
      "ai_keywords": [
        "retriever",
        "ReasonIR-8B",
        "general reasoning tasks",
        "synthetic data generation pipeline",
        "hard negative",
        "nDCG@10",
        "BRIGHT",
        "information retrieval (IR) benchmark",
        "RAG tasks",
        "MMLU",
        "GPQA",
        "closed-book baseline",
        "LLM reranker",
        "test-time compute",
        "rewritten queries",
        "LLM"
      ]
    },
    "publishedAt": "2025-04-29T05:49:28.000Z",
    "title": "ReasonIR: Training Retrievers for Reasoning Tasks",
    "summary": "We present ReasonIR-8B, the first retriever specifically trained for general\nreasoning tasks. Existing retrievers have shown limited gains on reasoning\ntasks, in part because existing training datasets focus on short factual\nqueries tied to documents that straightforwardly answer them. We develop a\nsynthetic data generation pipeline that, for each document, our pipeline\ncreates a challenging and relevant query, along with a plausibly related but\nultimately unhelpful hard negative. By training on a mixture of our synthetic\ndata and existing public data, ReasonIR-8B achieves a new state-of-the-art of\n29.9 nDCG@10 without reranker and 36.9 nDCG@10 with reranker on BRIGHT, a\nwidely-used reasoning-intensive information retrieval (IR) benchmark. When\napplied to RAG tasks, ReasonIR-8B improves MMLU and GPQA performance by 6.4%\nand 22.6% respectively, relative to the closed-book baseline, outperforming\nother retrievers and search engines. In addition, ReasonIR-8B uses test-time\ncompute more effectively: on BRIGHT, its performance consistently increases\nwith longer and more information-rich rewritten queries; it continues to\noutperform other retrievers when combined with an LLM reranker. Our training\nrecipe is general and can be easily extended to future LLMs; to this end, we\nopen-source our code, data, and model.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.20595.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6748
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.20157",
      "authors": [
        {
          "_id": "68119750ff0764f3840a7f93",
          "user": {
            "_id": "61e0c5053a1781f66b4e9aed",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1642120523097-61e0c5053a1781f66b4e9aed.jpeg",
            "isPro": false,
            "fullname": "Zae Myung Kim",
            "user": "zaemyung",
            "type": "user"
          },
          "name": "Zae Myung Kim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-30T07:55:58.262Z",
          "hidden": false
        },
        {
          "_id": "68119750ff0764f3840a7f94",
          "name": "Chanwoo Park",
          "hidden": false
        },
        {
          "_id": "68119750ff0764f3840a7f95",
          "user": {
            "_id": "60985a0547dc3dbf8a976607",
            "avatarUrl": "/avatars/3c37bf4b7c9db83a46af7c473ee4eb86.svg",
            "isPro": false,
            "fullname": "Vipul Raheja",
            "user": "machineteacher",
            "type": "user"
          },
          "name": "Vipul Raheja",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-30T10:03:14.631Z",
          "hidden": false
        },
        {
          "_id": "68119750ff0764f3840a7f96",
          "user": {
            "_id": "64356b40a4bd75c62cbc5926",
            "avatarUrl": "/avatars/5f4c603464e9c8ad613a3a25fa4cacbf.svg",
            "isPro": false,
            "fullname": "Dongyeop Kang",
            "user": "dykang",
            "type": "user"
          },
          "name": "Dongyeop Kang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-30T10:03:26.612Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6434b6619bd5a84b5dcfa4de/tHS8gWUK0ptmNTs6lZck6.png",
        "https://cdn-uploads.huggingface.co/production/uploads/6434b6619bd5a84b5dcfa4de/uMD9av8pogPwYTW-KNFJ2.png"
      ],
      "publishedAt": "2025-04-28T18:02:35.000Z",
      "submittedOnDailyAt": "2025-04-30T02:04:16.540Z",
      "title": "Direction de pensée évaluative : optimisation des méta-politiques pour l'optimisation des modèles de récompense",
      "submittedOnDailyBy": {
        "_id": "6434b6619bd5a84b5dcfa4de",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6434b6619bd5a84b5dcfa4de/h8Q6kPNjFNc03wmdboHzq.jpeg",
        "isPro": true,
        "fullname": "Young-Jun Lee",
        "user": "passing2961",
        "type": "user"
      },
      "summary": "Les deux principaux limites dans le méthode d'arraymancement de critères de récompense pour les modèles de langage grands (LLMs) sont : la vulnérabilité à la capture de points de mauvaise qualité du signal de récompense pour arraymancer et la dépendance à l'ingénierie de prompts faible et coûteuse lorsque les LLMs sont utilisés comme modèles de récompense. Nous présentons un modèle méta de récompense qui ajuste automatiquement les prompts du modèle de récompense pendant le processus d'entraînement, ainsi qu'un cadre d'optimisation de politiques méta (MPO) pour aborder ces défis. Dans le MPO, le modèle méta de récompense détecte les changements dans le contexte d'entraînement, ajuste continuément les prompts du modèle de récompense, maintient des niveaux élevés d'arraymancement et fournit des récompenses adaptatives pour éviter que la politique adopte des actions non souhaitées. Cette approche d'apprentissage méta favorise l'optimisation de politiques stables et réduit considérablement la nécessité de concevoir des prompts de récompense handcrafted. Comparé aux modèles guidés par prompts handcrafted, cette méthode montre le même ou un meilleur rendement. De plus, le MPO est efficace dans diverses tâches et montre que, dans certains cas, la décomposition de récompenses n'est pas nécessaire. La configuration du MPO pour apprentissage méta peut facilement être étendue à un cadre de haut niveau d'arraymancement, en comparaison avec le RLAIF standard. En résumé, cette méthode résout les défis théoriques et pratiques dans l'arraymancement de critères de récompense pour les LLMs, introduisant des stratégies d'arraymancement plus robustes et adaptatives. Les codes et modèles sont disponibles publiquement.",
      "upvotes": 17,
      "discussionId": "68119751ff0764f3840a7fc5",
      "ai_keywords": [
        "Meta Policy Optimization (MPO)",
        "meta-reward model",
        "reward hacking",
        "prompt engineering",
        "policy optimization",
        "adaptive reward signal",
        "meta-learning approach",
        "prompt design",
        "reward-based RL alignment",
        "question answering",
        "mathematical reasoning"
      ]
    },
    "publishedAt": "2025-04-28T14:02:35.000Z",
    "title": "Toward Evaluative Thinking: Meta Policy Optimization with Evolving\n  Reward Models",
    "summary": "Reward-based alignment methods for large language models (LLMs) face two key\nlimitations: vulnerability to reward hacking, where models exploit flaws in the\nreward signal; and reliance on brittle, labor-intensive prompt engineering when\nLLMs are used as reward models. We introduce Meta Policy Optimization (MPO), a\nframework that addresses these challenges by integrating a meta-reward model\nthat dynamically refines the reward model's prompt throughout training. In MPO,\nthe meta-reward model monitors the evolving training context and continuously\nadjusts the reward model's prompt to maintain high alignment, providing an\nadaptive reward signal that resists exploitation by the policy. This\nmeta-learning approach promotes a more stable policy optimization, and greatly\nreduces the need for manual reward prompt design. It yields performance on par\nwith or better than models guided by extensively hand-crafted reward prompts.\nFurthermore, we show that MPO maintains its effectiveness across diverse tasks,\nsuch as question answering and mathematical reasoning, without requiring\nspecialized reward designs. Beyond standard RLAIF, MPO's meta-learning\nformulation is readily extensible to higher-level alignment frameworks.\nOverall, this method addresses theoretical and practical challenges in\nreward-based RL alignment for LLMs, paving the way for more robust and\nadaptable alignment strategies. The code and models will be publicly shared.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6434b6619bd5a84b5dcfa4de/tHS8gWUK0ptmNTs6lZck6.png",
      "https://cdn-uploads.huggingface.co/production/uploads/6434b6619bd5a84b5dcfa4de/uMD9av8pogPwYTW-KNFJ2.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.20157.png",
    "numComments": 6,
    "submittedBy": {
      "_id": "6434b6619bd5a84b5dcfa4de",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6434b6619bd5a84b5dcfa4de/h8Q6kPNjFNc03wmdboHzq.jpeg",
      "fullname": "Young-Jun Lee",
      "name": "passing2961",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.20995",
      "authors": [
        {
          "_id": "68118c049c2765c9323de70b",
          "user": {
            "_id": "6437c7dae282b4a48eaf065e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6437c7dae282b4a48eaf065e/AxodKQXyrviTFQRyjnL01.jpeg",
            "isPro": false,
            "fullname": "Haoyu Zhen",
            "user": "anyeZHY",
            "type": "user"
          },
          "name": "Haoyu Zhen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-30T10:04:19.238Z",
          "hidden": false
        },
        {
          "_id": "68118c049c2765c9323de70c",
          "name": "Qiao Sun",
          "hidden": false
        },
        {
          "_id": "68118c049c2765c9323de70d",
          "name": "Hongxin Zhang",
          "hidden": false
        },
        {
          "_id": "68118c049c2765c9323de70e",
          "name": "Junyan Li",
          "hidden": false
        },
        {
          "_id": "68118c049c2765c9323de70f",
          "name": "Siyuan Zhou",
          "hidden": false
        },
        {
          "_id": "68118c049c2765c9323de710",
          "user": {
            "_id": "63c9bd445fdc575773c732fe",
            "avatarUrl": "/avatars/def472d1ab3fbf751225357c0932ae7e.svg",
            "isPro": false,
            "fullname": "Yilun Du",
            "user": "yilundu",
            "type": "user"
          },
          "name": "Yilun Du",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-30T10:03:42.041Z",
          "hidden": false
        },
        {
          "_id": "68118c049c2765c9323de711",
          "name": "Chuang Gan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-29T17:59:30.000Z",
      "submittedOnDailyAt": "2025-04-30T01:05:28.658Z",
      "title": "TesserAct : Apprentissage d'un modèle de corps en quatre dimensions",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Dans cet article, nous proposons un nouvel approche efficace pour l'apprentissage d'un modèle mondial de visualisation en 4 dimensions. Cette approche permet de prédire l'évolution dynamique dans un espace tridimensionnel des actions des agents de visualisation, tout en garantissant également la cohérence spatiale et temporelle. Nous proposons l'utilisation de vidéos RGB-DN (RGB, profondeur, normal) pour l'apprentissage d'un modèle mondial en 4 dimensions. Cette approche inclut les détails de forme, de structure et des changements temporels dans les prédictions du modèle, permettant également l'apprentissage précis de son modèle mécanique. En particulier, nous utilisons des modèles de profondeur et de normal pour ajouter des informations aux données de vidéos de manipulation de robots existantes. Ensuite, nous ajustons un modèle de génération de vidéos pour prédire simultanément RGB-DN (RGB, profondeur, normal) en utilisant ces données. Finalement, nous proposons un algorithme pour transformer directement les vidéos RGB, profondeur et normal en un monde 4D de haute qualité. Notre méthode garantit la cohérence temporelle et spatiale dans la prédiction des espaces 4D à partir de scénarios de visualisation, permettant la synthèse de nouvelles images dans l'environnement de visualisation et favorisant l'apprentissage de stratégies qui surpassent significativement les modèles mondiaux basés sur les vidéos existantes.",
      "upvotes": 9,
      "discussionId": "68118c089c2765c9323de81d",
      "ai_keywords": [
        "embodied world models",
        "4D world models",
        "RGB-DN (RGB, Depth, and Normal) videos",
        "video generation model",
        "inverse dynamic models",
        "robotic manipulation video datasets",
        "temporal coherence",
        "spatial coherence",
        "novel view synthesis",
        "policy learning"
      ]
    },
    "publishedAt": "2025-04-29T13:59:30.000Z",
    "title": "TesserAct: Learning 4D Embodied World Models",
    "summary": "This paper presents an effective approach for learning novel 4D embodied\nworld models, which predict the dynamic evolution of 3D scenes over time in\nresponse to an embodied agent's actions, providing both spatial and temporal\nconsistency. We propose to learn a 4D world model by training on RGB-DN (RGB,\nDepth, and Normal) videos. This not only surpasses traditional 2D models by\nincorporating detailed shape, configuration, and temporal changes into their\npredictions, but also allows us to effectively learn accurate inverse dynamic\nmodels for an embodied agent. Specifically, we first extend existing robotic\nmanipulation video datasets with depth and normal information leveraging\noff-the-shelf models. Next, we fine-tune a video generation model on this\nannotated dataset, which jointly predicts RGB-DN (RGB, Depth, and Normal) for\neach frame. We then present an algorithm to directly convert generated RGB,\nDepth, and Normal videos into a high-quality 4D scene of the world. Our method\nensures temporal and spatial coherence in 4D scene predictions from embodied\nscenarios, enables novel view synthesis for embodied environments, and\nfacilitates policy learning that significantly outperforms those derived from\nprior video-based world models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.20995.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6748
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.16046",
      "authors": [
        {
          "_id": "68119c70e8a3493171fadce2",
          "user": {
            "_id": "62fb40b59af1d16bc0ac60f4",
            "avatarUrl": "/avatars/03ff66a419db8f2bc8e89a3b47aaaeac.svg",
            "isPro": false,
            "fullname": "Jack Zhang",
            "user": "jackzhang",
            "type": "user"
          },
          "name": "Jingyu Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-30T07:55:55.481Z",
          "hidden": false
        },
        {
          "_id": "68119c70e8a3493171fadce3",
          "name": "Jiacan Yu",
          "hidden": false
        },
        {
          "_id": "68119c70e8a3493171fadce4",
          "name": "Marc Marone",
          "hidden": false
        },
        {
          "_id": "68119c70e8a3493171fadce5",
          "name": "Benjamin Van Durme",
          "hidden": false
        },
        {
          "_id": "68119c70e8a3493171fadce6",
          "name": "Daniel Khashabi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-22T17:16:53.000Z",
      "submittedOnDailyAt": "2025-04-30T02:15:12.625Z",
      "title": "Certificat de Réduction de Violation de Droits d'Auteur dans les Cas de Piraterie de LLM",
      "submittedOnDailyBy": {
        "_id": "62fb40b59af1d16bc0ac60f4",
        "avatarUrl": "/avatars/03ff66a419db8f2bc8e89a3b47aaaeac.svg",
        "isPro": false,
        "fullname": "Jack Zhang",
        "user": "jackzhang",
        "type": "user"
      },
      "summary": "Les modèles de langage grands (LLMs) en espagnol présentent des préoccupations concernant l'exposition de matériaux de mémoire de droits d'auteur pendant le période d'entraînement réservé, et l'invasion de droits d'auteur inconsciente après l'entraînement. Cela a conduit au développement du méthode \"effacement des droits\" et à la création d'une approche d'entraînement postérieure pour éviter que le modèle génère du contenu similaire à des matériaux de droits d'auteur. L'approche actuelle de compensation est très efficace face aux risques généraux, mais échoue à protéger contre les plus graves risques de droits d'auteur, comme l'inclusion de citations longues. Dans ce contexte, on propose un approche d'inférence efficace et puissante. Ce méthode itère entre des techniques de détection de citations et de substitution pour transformer des sections potentiellement invades. En utilisant des filtres de capture de données efficaces (filtres de Bloom), cette approche permet le scan des grands corpus de la réalité pour les droits d'auteur. Si la longueur de la citation dépasse un seuil, le système peut rejeter la réponse, réduisant ainsi le risque d'invasion. Les résultats des tests montrent que BloomScrub réduit le risque d'invasion tout en maintenant l'utilité du modèle, et peut s'adapter pour rejeter seulement celles des citations qui présentent un risque significatif. Ces résultats montrent que l'approche légère d'inférence est capable de démontrer un impact notable sur la prévention des droits d'auteur.",
      "upvotes": 7,
      "discussionId": "68119c70e8a3493171fadd11",
      "ai_keywords": [
        "BloomScrub",
        "Bloom filters",
        "quotation detection",
        "rewriting techniques",
        "copyright screening",
        "adaptive abstention"
      ]
    },
    "publishedAt": "2025-04-22T13:16:53.000Z",
    "title": "Certified Mitigation of Worst-Case LLM Copyright Infringement",
    "summary": "The exposure of large language models (LLMs) to copyrighted material during\npre-training raises concerns about unintentional copyright infringement post\ndeployment. This has driven the development of \"copyright takedown\" methods,\npost-training approaches aimed at preventing models from generating content\nsubstantially similar to copyrighted ones. While current mitigation approaches\nare somewhat effective for average-case risks, we demonstrate that they\noverlook worst-case copyright risks exhibits by the existence of long, verbatim\nquotes from copyrighted sources. We propose BloomScrub, a remarkably simple yet\nhighly effective inference-time approach that provides certified copyright\ntakedown. Our method repeatedly interleaves quote detection with rewriting\ntechniques to transform potentially infringing segments. By leveraging\nefficient data sketches (Bloom filters), our approach enables scalable\ncopyright screening even for large-scale real-world corpora. When quotes beyond\na length threshold cannot be removed, the system can abstain from responding,\noffering certified risk reduction. Experimental results show that BloomScrub\nreduces infringement risk, preserves utility, and accommodates different levels\nof enforcement stringency with adaptive abstention. Our results suggest that\nlightweight, inference-time methods can be surprisingly effective for copyright\nprevention.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16046.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62fb40b59af1d16bc0ac60f4",
      "avatarUrl": "/avatars/03ff66a419db8f2bc8e89a3b47aaaeac.svg",
      "fullname": "Jack Zhang",
      "name": "jackzhang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.20998",
      "authors": [
        {
          "_id": "6811899ba6198824c5589ed7",
          "name": "Thao Nguyen",
          "hidden": false
        },
        {
          "_id": "6811899ba6198824c5589ed8",
          "name": "Krishna Kumar Singh",
          "hidden": false
        },
        {
          "_id": "6811899ba6198824c5589ed9",
          "name": "Jing Shi",
          "hidden": false
        },
        {
          "_id": "6811899ba6198824c5589eda",
          "name": "Trung Bui",
          "hidden": false
        },
        {
          "_id": "6811899ba6198824c5589edb",
          "name": "Yong Jae Lee",
          "hidden": false
        },
        {
          "_id": "6811899ba6198824c5589edc",
          "name": "Yuheng Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-29T17:59:57.000Z",
      "submittedOnDailyAt": "2025-04-30T00:54:06.806Z",
      "title": "YoChameleon : Vision personnalisée et génération de langage",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Les modèles de grandes réseaux multimodal (par exemple : GPT-4, Gemini, Chameleon) sont en développement comme des outils puissants connus par des centaines de milliers d'utilisateurs. Cependant, ces modèles sont graduels et manquent de connaissances personnalisées sur des concepts spécifiques aux utilisateurs. Les études précédentes ont investigué la personnalisation dans la génération de texte, mais il n'est pas clair comment appliquer ces techniques dans de nouvelles tâches de modélisation (par exemple : génération d'images). Dans cet article, nous présentons une recherche sur la personnalisation des modèles de grandes réseaux multimodal. En fournissant des images de 3 à 5 pages sur un concept spécifique, Yo'Chameleon utilise la technique de fine-tuning de prompts doux pour recueillir des informations liées au thème, répondre à des questions sur lui et générer des images du thème dans de nouveaux contextes. Yo'Chameleon maintient le rendement du modèle équilibré et fonctionne efficacement dans diverses tâches de modélisation, en utilisant une structure automatique d'optimisation de prompts et un approche de génération d'images \"soft positive\", en plus d'être entraîné avec peu d'exemples pour améliorer la qualité des images.",
      "upvotes": 6,
      "discussionId": "6811899ca6198824c5589f45",
      "ai_keywords": [
        "soft-prompt tuning",
        "subject-specific information",
        "self-prompting optimization mechanism",
        "soft-positive image generation approach"
      ]
    },
    "publishedAt": "2025-04-29T13:59:57.000Z",
    "title": "YoChameleon: Personalized Vision and Language Generation",
    "summary": "Large Multimodal Models (e.g., GPT-4, Gemini, Chameleon) have evolved into\npowerful tools with millions of users. However, they remain generic models and\nlack personalized knowledge of specific user concepts. Previous work has\nexplored personalization for text generation, yet it remains unclear how these\nmethods can be adapted to new modalities, such as image generation. In this\npaper, we introduce Yo'Chameleon, the first attempt to study personalization\nfor large multimodal models. Given 3-5 images of a particular concept,\nYo'Chameleon leverages soft-prompt tuning to embed subject-specific information\nto (i) answer questions about the subject and (ii) recreate pixel-level details\nto produce images of the subject in new contexts. Yo'Chameleon is trained with\n(i) a self-prompting optimization mechanism to balance performance across\nmultiple modalities, and (ii) a ``soft-positive\" image generation approach to\nenhance image quality in a few-shot setting.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.20998.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6748
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.20879",
      "authors": [
        {
          "_id": "6811ae6b7f4f553788e905b8",
          "name": "Shivalika Singh",
          "hidden": false
        },
        {
          "_id": "6811ae6b7f4f553788e905b9",
          "name": "Yiyang Nan",
          "hidden": false
        },
        {
          "_id": "6811ae6b7f4f553788e905ba",
          "name": "Alex Wang",
          "hidden": false
        },
        {
          "_id": "6811ae6b7f4f553788e905bb",
          "name": "Daniel D'Souza",
          "hidden": false
        },
        {
          "_id": "6811ae6b7f4f553788e905bc",
          "name": "Sayash Kapoor",
          "hidden": false
        },
        {
          "_id": "6811ae6b7f4f553788e905bd",
          "name": "Ahmet Üstün",
          "hidden": false
        },
        {
          "_id": "6811ae6b7f4f553788e905be",
          "name": "Sanmi Koyejo",
          "hidden": false
        },
        {
          "_id": "6811ae6b7f4f553788e905bf",
          "user": {
            "_id": "63081e15a670ed10f9d44229",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63081e15a670ed10f9d44229/w1b9uq-9774bMMgJbSPsS.jpeg",
            "isPro": true,
            "fullname": "Yuntian Deng",
            "user": "yuntian-deng",
            "type": "user"
          },
          "name": "Yuntian Deng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-30T09:56:42.033Z",
          "hidden": false
        },
        {
          "_id": "6811ae6b7f4f553788e905c0",
          "name": "Shayne Longpre",
          "hidden": false
        },
        {
          "_id": "6811ae6b7f4f553788e905c1",
          "name": "Noah Smith",
          "hidden": false
        },
        {
          "_id": "6811ae6b7f4f553788e905c2",
          "name": "Beyza Ermis",
          "hidden": false
        },
        {
          "_id": "6811ae6b7f4f553788e905c3",
          "name": "Marzieh Fadaee",
          "hidden": false
        },
        {
          "_id": "6811ae6b7f4f553788e905c4",
          "name": "Sara Hooker",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-29T15:48:49.000Z",
      "submittedOnDailyAt": "2025-04-30T03:36:53.331Z",
      "title": "La Ilusion du Classement",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "L'évaluation progressive est la base du développement de tous les domaines scientifiques. Lorsque les benchmarks jouent un rôle central, ils peuvent être déformés et vulnérables. Chatbot Arena a occupé une position principale en tant que principal leaderboard pour classer les systèmes d'IA avec la plus grande capacité. Cependant, dans cette recherche, des problèmes systémiques ont été découverts en raison de la déformation. Nous avons vérifié que les fournisseurs reçoivent des processus de test non publics, ce qui permet de tester plusieurs versions avant la publication et d'ajuster les scores selon la nécessité. Nous avons démontré que les fournisseurs ont la capacité de sélectionner les meilleures scores, ce qui génère un biais dans les scores d'Arena en raison de la publicité. Dans des cas extrêmes, Meta a testé 27 versions non publiques de Llama-4 avant son lancement. De plus, les modèles non publics ont plus d'interactions que les modèles publics ou ouverts, et les modèles ouverts de poids et de code ouvert sont moins probables d'être supprimés d'Arena. Ces politiques génèrent de grandes inégalités dans l'accès à de grandes quantités de données au fur et à mesure du temps. Des fournisseurs comme Google et OpenAI ont obtenu environ 19,2% et 20,4% des données totales d'Arena, tandis que 83 modèles publics ont obtenu environ 29,7%. Nous montrons que l'accès aux données de Chatbot Arena confère de grandes avantages, et en se basant sur notre estimation conservatrice, nous avons observé un accroissement relatif du rendement de 112%. Ces tendances indiquent que Arena a adapté trop à ses propres modèles, ce qui réduit son impact sur la qualité générale des modèles. Chatbot Arena a été conçu comme une plateforme d'évaluation valable maintenue par des organisations et des communautés personnelles. Nous proposons une réforme du cadre d'évaluation de Chatbot Arena et un plan concret et mis en œuvre pour promouvoir un benchmark plus juste et transparent.",
      "upvotes": 6,
      "discussionId": "6811ae6c7f4f553788e905fc"
    },
    "publishedAt": "2025-04-29T11:48:49.000Z",
    "title": "The Leaderboard Illusion",
    "summary": "Measuring progress is fundamental to the advancement of any scientific field.\nAs benchmarks play an increasingly central role, they also grow more\nsusceptible to distortion. Chatbot Arena has emerged as the go-to leaderboard\nfor ranking the most capable AI systems. Yet, in this work we identify\nsystematic issues that have resulted in a distorted playing field. We find that\nundisclosed private testing practices benefit a handful of providers who are\nable to test multiple variants before public release and retract scores if\ndesired. We establish that the ability of these providers to choose the best\nscore leads to biased Arena scores due to selective disclosure of performance\nresults. At an extreme, we identify 27 private LLM variants tested by Meta in\nthe lead-up to the Llama-4 release. We also establish that proprietary closed\nmodels are sampled at higher rates (number of battles) and have fewer models\nremoved from the arena than open-weight and open-source alternatives. Both\nthese policies lead to large data access asymmetries over time. Providers like\nGoogle and OpenAI have received an estimated 19.2% and 20.4% of all data on the\narena, respectively. In contrast, a combined 83 open-weight models have only\nreceived an estimated 29.7% of the total data. We show that access to Chatbot\nArena data yields substantial benefits; even limited additional data can result\nin relative performance gains of up to 112% on the arena distribution, based on\nour conservative estimates. Together, these dynamics result in overfitting to\nArena-specific dynamics rather than general model quality. The Arena builds on\nthe substantial efforts of both the organizers and an open community that\nmaintains this valuable evaluation platform. We offer actionable\nrecommendations to reform the Chatbot Arena's evaluation framework and promote\nfairer, more transparent benchmarking for the field",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.20879.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 77
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.20630",
      "authors": [
        {
          "_id": "6811ea47f8ca0d9acb45374b",
          "user": {
            "_id": "66569729ea21cfae5f5797c4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66569729ea21cfae5f5797c4/IguwJzljFN3QiEd1bn5BP.jpeg",
            "isPro": false,
            "fullname": "Yu Zhang",
            "user": "AaronZ345",
            "type": "user"
          },
          "name": "Yu Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-30T09:56:39.194Z",
          "hidden": false
        },
        {
          "_id": "6811ea47f8ca0d9acb45374c",
          "name": "Wenxiang Guo",
          "hidden": false
        },
        {
          "_id": "6811ea47f8ca0d9acb45374d",
          "name": "Changhao Pan",
          "hidden": false
        },
        {
          "_id": "6811ea47f8ca0d9acb45374e",
          "name": "Zhiyuan Zhu",
          "hidden": false
        },
        {
          "_id": "6811ea47f8ca0d9acb45374f",
          "name": "Tao Jin",
          "hidden": false
        },
        {
          "_id": "6811ea47f8ca0d9acb453750",
          "name": "Zhou Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-29T10:56:44.000Z",
      "submittedOnDailyAt": "2025-04-30T07:54:10.120Z",
      "title": "ISDrama : Génération de drame spectral à travers une approche de programmation multimodale",
      "submittedOnDailyBy": {
        "_id": "66569729ea21cfae5f5797c4",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66569729ea21cfae5f5797c4/IguwJzljFN3QiEd1bn5BP.jpeg",
        "isPro": false,
        "fullname": "Yu Zhang",
        "user": "AaronZ345",
        "type": "user"
      },
      "summary": "Variés sons répétés.",
      "upvotes": 3,
      "discussionId": "6811ea48f8ca0d9acb4537cf",
      "projectPage": "https://aaronz345.github.io/ISDramaDemo/",
      "ai_keywords": [
        "multimodal inputs",
        "binaural speech",
        "dramatic prosody",
        "multimodal prompts",
        "multimodal recorded dataset",
        "contrastive learning",
        "Doppler effect",
        "Multimodal Pose Encoder",
        "flow-based model",
        "mamba-transformer",
        "Drama-MOE",
        "classifier-free guidance",
        "context-consistent guidance"
      ]
    },
    "publishedAt": "2025-04-29T06:56:44.000Z",
    "title": "ISDrama: Immersive Spatial Drama Generation through Multimodal Prompting",
    "summary": "Multimodal immersive spatial drama generation focuses on creating continuous\nmulti-speaker binaural speech with dramatic prosody based on multimodal\nprompts, with potential applications in AR, VR, and others. This task requires\nsimultaneous modeling of spatial information and dramatic prosody based on\nmultimodal inputs, with high data collection costs. To the best of our\nknowledge, our work is the first attempt to address these challenges. We\nconstruct MRSDrama, the first multimodal recorded spatial drama dataset,\ncontaining binaural drama audios, scripts, videos, geometric poses, and textual\nprompts. Then, we propose ISDrama, the first immersive spatial drama generation\nmodel through multimodal prompting. ISDrama comprises these primary components:\n1) Multimodal Pose Encoder, based on contrastive learning, considering the\nDoppler effect caused by moving speakers to extract unified pose information\nfrom multimodal prompts. 2) Immersive Drama Transformer, a flow-based\nmamba-transformer model that generates high-quality drama, incorporating\nDrama-MOE to select proper experts for enhanced prosody and pose control. We\nalso design a context-consistent classifier-free guidance strategy to\ncoherently generate complete drama. Experimental results show that ISDrama\noutperforms baseline models on objective and subjective metrics. The demos and\ndataset are available at https://aaronz345.github.io/ISDramaDemo.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.20630.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66569729ea21cfae5f5797c4",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66569729ea21cfae5f5797c4/IguwJzljFN3QiEd1bn5BP.jpeg",
      "fullname": "Yu Zhang",
      "name": "AaronZ345",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.20996",
      "authors": [
        {
          "_id": "6811c55384adfa26b82abd76",
          "name": "Sicheng Mo",
          "hidden": false
        },
        {
          "_id": "6811c55384adfa26b82abd77",
          "name": "Thao Nguyen",
          "hidden": false
        },
        {
          "_id": "6811c55384adfa26b82abd78",
          "name": "Xun Huang",
          "hidden": false
        },
        {
          "_id": "6811c55384adfa26b82abd79",
          "name": "Siddharth Srinivasan Iyer",
          "hidden": false
        },
        {
          "_id": "6811c55384adfa26b82abd7a",
          "name": "Yijun Li",
          "hidden": false
        },
        {
          "_id": "6811c55384adfa26b82abd7b",
          "name": "Yuchen Liu",
          "hidden": false
        },
        {
          "_id": "6811c55384adfa26b82abd7c",
          "name": "Abhishek Tandon",
          "hidden": false
        },
        {
          "_id": "6811c55384adfa26b82abd7d",
          "name": "Eli Shechtman",
          "hidden": false
        },
        {
          "_id": "6811c55384adfa26b82abd7e",
          "name": "Krishna Kumar Singh",
          "hidden": false
        },
        {
          "_id": "6811c55384adfa26b82abd7f",
          "name": "Yong Jae Lee",
          "hidden": false
        },
        {
          "_id": "6811c55384adfa26b82abd80",
          "name": "Bolei Zhou",
          "hidden": false
        },
        {
          "_id": "6811c55384adfa26b82abd81",
          "name": "Yuheng Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-29T17:59:45.000Z",
      "submittedOnDailyAt": "2025-04-30T05:08:54.031Z",
      "title": "X-Fusion : Nous introduisons un nouveau modèle de théorie dans un grand modèle de langage fixé en gel.",
      "submittedOnDailyBy": {
        "_id": "637c94d3f219c71f93eda9ad",
        "avatarUrl": "/avatars/6dae0c30755196ccc0a5a06b3981c47f.svg",
        "isPro": false,
        "fullname": "Sicheng Mo",
        "user": "Sichengmo",
        "type": "user"
      },
      "summary": "X-Fusion est un cadre de modèle d'apprentissage profond conçu pour les modèles de langage étendu (LLMs) qui peuvent être utilisés dans des tâches impliquant plusieurs modèles, tout en maintenant leur capacité linguistique. Il utilise un design de deux tours liées aux types de modèle, intégrant des informations spécifiques d'images nécessaires pour comprendre et générer, tandis que les paramètres du LLM restent fixes. Selon les expériences, X-Fusion dépasse les architectures équivalentes dans deux tâches : compréhension d'images et génération de texte à partir d'images. L'introduction de données ciblées sur la compréhension améliore la qualité de la génération et la réduction du bruit dans les données d'images améliore le rendement général, tandis que la matrice de caractéristiques accélère la convergence de petits modèles, avec un impact minimal sur les grands modèles. Ces résultats fournissent une rétroaction précieuse pour la construction de modèles multi-modèles intégrés efficaces.",
      "upvotes": 2,
      "discussionId": "6811c55584adfa26b82abdfe",
      "projectPage": "https://sichengmo.github.io/XFusion/",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "multimodal tasks",
        "dual-tower design",
        "modality-specific weights",
        "vision-specific information",
        "image-to-text",
        "text-to-image",
        "understanding-focused data",
        "feature alignment",
        "unified multimodal models"
      ]
    },
    "publishedAt": "2025-04-29T13:59:45.000Z",
    "title": "X-Fusion: Introducing New Modality to Frozen Large Language Models",
    "summary": "We propose X-Fusion, a framework that extends pretrained Large Language\nModels (LLMs) for multimodal tasks while preserving their language\ncapabilities. X-Fusion employs a dual-tower design with modality-specific\nweights, keeping the LLM's parameters frozen while integrating vision-specific\ninformation for both understanding and generation. Our experiments demonstrate\nthat X-Fusion consistently outperforms alternative architectures on both\nimage-to-text and text-to-image tasks. We find that incorporating\nunderstanding-focused data improves generation quality, reducing image data\nnoise enhances overall performance, and feature alignment accelerates\nconvergence for smaller models but has minimal impact on larger ones. Our\nfindings provide valuable insights into building efficient unified multimodal\nmodels.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.20996.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "637c94d3f219c71f93eda9ad",
      "avatarUrl": "/avatars/6dae0c30755196ccc0a5a06b3981c47f.svg",
      "fullname": "Sicheng Mo",
      "name": "Sichengmo",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.18087",
      "authors": [
        {
          "_id": "6810746e4be021d4dcd8d4de",
          "name": "Weipeng Tan",
          "hidden": false
        },
        {
          "_id": "6810746e4be021d4dcd8d4df",
          "name": "Chuming Lin",
          "hidden": false
        },
        {
          "_id": "6810746e4be021d4dcd8d4e0",
          "user": {
            "_id": "652fab9d04a34a9282bf29d6",
            "avatarUrl": "/avatars/cd5967b37ebb1225e9ae1d46f196e2e2.svg",
            "isPro": false,
            "fullname": "Chengming Xu",
            "user": "ChengmingX",
            "type": "user"
          },
          "name": "Chengming Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-30T09:56:46.843Z",
          "hidden": false
        },
        {
          "_id": "6810746e4be021d4dcd8d4e1",
          "name": "FeiFan Xu",
          "hidden": false
        },
        {
          "_id": "6810746e4be021d4dcd8d4e2",
          "name": "Xiaobin Hu",
          "hidden": false
        },
        {
          "_id": "6810746e4be021d4dcd8d4e3",
          "name": "Xiaozhong Ji",
          "hidden": false
        },
        {
          "_id": "6810746e4be021d4dcd8d4e4",
          "name": "Junwei Zhu",
          "hidden": false
        },
        {
          "_id": "6810746e4be021d4dcd8d4e5",
          "name": "Chengjie Wang",
          "hidden": false
        },
        {
          "_id": "6810746e4be021d4dcd8d4e6",
          "name": "Yanwei Fu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-25T05:28:21.000Z",
      "submittedOnDailyAt": "2025-04-30T07:13:30.231Z",
      "title": "Identité Séparation, Coopération Émotionnelle : Génération de Personnages Dialectiques Émotionnels pour la Reconnaissance de Correlations Émotionnelles",
      "submittedOnDailyBy": {
        "_id": "652fab9d04a34a9282bf29d6",
        "avatarUrl": "/avatars/cd5967b37ebb1225e9ae1d46f196e2e2.svg",
        "isPro": false,
        "fullname": "Chengming Xu",
        "user": "ChengmingX",
        "type": "user"
      },
      "summary": "Le développement récent de la Génération de Têtes Parlantes (THG) a permis d'atteindre des mouvements de la bouche impressionnants et une qualité visuelle élevée grâce à des modèles de diffusion, bien que les méthodes existantes n'aient pas réussi à créer des personnages avec des expressions émotionnelles ni à maintenir l'identité de l'orateur. Dans la génération de THG avec émotions actuelles, trois limites importantes ont été identifiées : la manque d'utilisation appropriée des codes émotionnels uniques du son, la perte de l'identité de l'expression émotionnelle et l'apprentissage indépendant des relations émotionnelles. Pour faire face à ces défis, nous proposons un nouveau cadre qui sépare l'identité et l'émotion, puis collabore les émotions avec des caractéristiques similaires. Ce cadre est appelé DICE-Talk. Tout d'abord, nous développons un émotionnel de séparation pour modéliser le code émotionnel du son du vidéo, permettant d'exprimer des émotions sans relation à l'identité. Ensuite, nous introduisons un module de conditionnement émotionnel renforcé qui gère des banques d'émotions apprenables, en utilisant des vecteurs quadratiques et de l'attention pour clarifier les relations entre émotions. Enfin, nous concevons un objectif de discrimination émotionnelle pour imposer la consistence émotionnelle lors du processus de diffusion. Les expériences de diffusion sur les jeux de données MEAD et HDTF dépassent l'approche la plus avancée en termes de précision émotionnelle, tout en maintenant le rendement du mouvement de la bouche, et confirment la performance excellente de notre méthode. Les résultats qualitatifs et l'expérience du utilisateur génèrent des expressions émotionnelles riches et liées à une identité naturellement adaptée, confirmant la capacité de notre méthode.",
      "upvotes": 2,
      "discussionId": "681074704be021d4dcd8d57c",
      "projectPage": "https://toto222.github.io/DICE-Talk/",
      "githubRepo": "https://github.com/toto222/DICE-Talk"
    },
    "publishedAt": "2025-04-25T01:28:21.000Z",
    "title": "Disentangle Identity, Cooperate Emotion: Correlation-Aware Emotional\n  Talking Portrait Generation",
    "summary": "Recent advances in Talking Head Generation (THG) have achieved impressive lip\nsynchronization and visual quality through diffusion models; yet existing\nmethods struggle to generate emotionally expressive portraits while preserving\nspeaker identity. We identify three critical limitations in current emotional\ntalking head generation: insufficient utilization of audio's inherent emotional\ncues, identity leakage in emotion representations, and isolated learning of\nemotion correlations. To address these challenges, we propose a novel framework\ndubbed as DICE-Talk, following the idea of disentangling identity with emotion,\nand then cooperating emotions with similar characteristics. First, we develop a\ndisentangled emotion embedder that jointly models audio-visual emotional cues\nthrough cross-modal attention, representing emotions as identity-agnostic\nGaussian distributions. Second, we introduce a correlation-enhanced emotion\nconditioning module with learnable Emotion Banks that explicitly capture\ninter-emotion relationships through vector quantization and attention-based\nfeature aggregation. Third, we design an emotion discrimination objective that\nenforces affective consistency during the diffusion process through\nlatent-space classification. Extensive experiments on MEAD and HDTF datasets\ndemonstrate our method's superiority, outperforming state-of-the-art approaches\nin emotion accuracy while maintaining competitive lip-sync performance.\nQualitative results and user studies further confirm our method's ability to\ngenerate identity-preserving portraits with rich, correlated emotional\nexpressions that naturally adapt to unseen identities.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.18087.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "652fab9d04a34a9282bf29d6",
      "avatarUrl": "/avatars/cd5967b37ebb1225e9ae1d46f196e2e2.svg",
      "fullname": "Chengming Xu",
      "name": "ChengmingX",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  }
]