[
  {
    "paper": {
      "id": "2503.19325",
      "authors": [
        {
          "_id": "67e35f6fc9d8214b5e1c64c3",
          "name": "Yuchao Gu",
          "hidden": false
        },
        {
          "_id": "67e35f6fc9d8214b5e1c64c4",
          "name": "Weijia Mao",
          "hidden": false
        },
        {
          "_id": "67e35f6fc9d8214b5e1c64c5",
          "name": "Mike Zheng Shou",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63021630a35b21bd8a53305a/SL0MQs7OvQpNlGBhroTW3.png"
      ],
      "publishedAt": "2025-03-25T03:38:06.000Z",
      "submittedOnDailyAt": "2025-03-26T00:37:14.940Z",
      "title": "Modèle de régression automatique dans les vidéos de longue durée et prédiction de la prochaine frame",
      "submittedOnDailyBy": {
        "_id": "63021630a35b21bd8a53305a",
        "avatarUrl": "/avatars/7a7e8b39749eda61e57d8a1908726558.svg",
        "isPro": true,
        "fullname": "Gu Yuchao",
        "user": "guyuchao",
        "type": "user"
      },
      "summary": "Le modèle automatique de contextes longs a progressé de manière significative dans la génération de langage, mais la génération d'images a rencontré des difficultés pour utiliser adéquatement le contexte temporel sur des périodes prolongées. Pour étudier le modèle de contextes longs d'images, nous proposons un standard fort pour le modèle automatique d'images par le biais du Frame AutoRegressive (FAR). Les modèles de langage apprennent les relations causales fondamentales entre tokens (Token), tandis que FAR modélise les relations causales temporelles entre séquences de frames continues, atteignant une convergence meilleure que les modèles AR de tokens ou les transformers de diffusion d'images. En se basant sur FAR, le modèle de contextes visuels longs a montré des problèmes en raison de la redondance visuelle. Actuellement, RoPE n'a pas un approche de dissipation temporelle valide pour des contextes éloignés, ce qui limite sa capacité à faire de bonnes inférences dans des séquences d'images longues. De plus, l'entraînement d'images longues est coûteux en termes de calcul et les tokens visuels croissent rapidement plus que les tokens linguistiques. Pour résoudre ces problèmes, nous proposons FlexRoPE pour équilibrer la dépendance locale et temporelle. FlexRoPE ajoute un approche flexible de dissipation temporelle à RoPE, permettant des inférences dans des contextes visuels 16 fois plus longs. De plus, nous proposons le modèle de contextes visuels courts et longs. Le modèle de contextes visuels courts garantit la cohérence temporelle à grande échelle à travers des fenêtres de contexte visuel de haute résolution, tandis que les modèles de contextes visuels longs codifient des informations à longues distances avec peu de tokens, même dans des fenêtres de contexte visuelles infinies. Cette approche permet d'entraîner des séquences d'images longues. FAR atteint les meilleurs résultats dans la génération d'images courtes et longues, offrant un standard simple et efficace pour le modèle automatique d'images.",
      "upvotes": 49,
      "discussionId": "67e35f72c9d8214b5e1c659b",
      "ai_keywords": [
        "Frame AutoRegressive (FAR)",
        "Token AR",
        "video autoregressive modeling",
        "visual redundancy",
        "RoPE (Rotary Position Embedding)",
        "temporal decay",
        "FlexRoPE",
        "long short-term context modeling",
        "high-resolution short-term context window",
        "long-term context window",
        "state-of-the-art performance",
        "video generation"
      ]
    },
    "publishedAt": "2025-03-24T23:38:06.000Z",
    "title": "Long-Context Autoregressive Video Modeling with Next-Frame Prediction",
    "summary": "Long-context autoregressive modeling has significantly advanced language\ngeneration, but video generation still struggles to fully utilize extended\ntemporal contexts. To investigate long-context video modeling, we introduce\nFrame AutoRegressive (FAR), a strong baseline for video autoregressive\nmodeling. Just as language models learn causal dependencies between tokens\n(i.e., Token AR), FAR models temporal causal dependencies between continuous\nframes, achieving better convergence than Token AR and video diffusion\ntransformers. Building on FAR, we observe that long-context vision modeling\nfaces challenges due to visual redundancy. Existing RoPE lacks effective\ntemporal decay for remote context and fails to extrapolate well to long video\nsequences. Additionally, training on long videos is computationally expensive,\nas vision tokens grow much faster than language tokens. To tackle these issues,\nwe propose balancing locality and long-range dependency. We introduce FlexRoPE,\nan test-time technique that adds flexible temporal decay to RoPE, enabling\nextrapolation to 16x longer vision contexts. Furthermore, we propose long\nshort-term context modeling, where a high-resolution short-term context window\nensures fine-grained temporal consistency, while an unlimited long-term context\nwindow encodes long-range information using fewer tokens. With this approach,\nwe can train on long video sequences with a manageable token context length. We\ndemonstrate that FAR achieves state-of-the-art performance in both short- and\nlong-video generation, providing a simple yet effective baseline for video\nautoregressive modeling.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63021630a35b21bd8a53305a/SL0MQs7OvQpNlGBhroTW3.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19325.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63021630a35b21bd8a53305a",
      "avatarUrl": "/avatars/7a7e8b39749eda61e57d8a1908726558.svg",
      "fullname": "Gu Yuchao",
      "name": "guyuchao",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.18931",
      "authors": [
        {
          "_id": "67e25c4d1908043170bd551d",
          "user": {
            "_id": "64651db3611ae99d14d392ea",
            "avatarUrl": "/avatars/b818dc0dddc999758ab5737d5053e8c3.svg",
            "isPro": false,
            "fullname": "cyt",
            "user": "Row11n",
            "type": "user"
          },
          "name": "Yitong Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-25T08:18:45.692Z",
          "hidden": false
        },
        {
          "_id": "67e25c4d1908043170bd551e",
          "name": "Lingchen Meng",
          "hidden": false
        },
        {
          "_id": "67e25c4d1908043170bd551f",
          "name": "Wujian Peng",
          "hidden": false
        },
        {
          "_id": "67e25c4d1908043170bd5520",
          "name": "Zuxuan Wu",
          "hidden": false
        },
        {
          "_id": "67e25c4d1908043170bd5521",
          "name": "Yu-Gang Jiang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-24T17:52:47.000Z",
      "submittedOnDailyAt": "2025-03-26T01:10:42.553Z",
      "title": "CoMP : Préparation d'entraînement pour des modèles continus basés sur la vision",
      "submittedOnDailyBy": {
        "_id": "64651db3611ae99d14d392ea",
        "avatarUrl": "/avatars/b818dc0dddc999758ab5737d5053e8c3.svg",
        "isPro": false,
        "fullname": "cyt",
        "user": "Row11n",
        "type": "user"
      },
      "summary": "Les modèles de base de vision basés sur la vision (VFMs) fournissent une représentation visuelle forte pour une large gamme d'applications. Dans cet article, les VFMs sont prédits de manière continue et multimodal pour traiter facilement des entrées visuelles de différents tailles et générer des représentations visuelles plus cohérentes avec la représentation linguistique. Pour cela, on introduit le processus de prédiction multimodal continue (CoMP). CoMP soutient le processus de prédiction continue des modèles originaux en utilisant des Embeddings de Position Rotationnelle Continu, et utilise la perte de disposition entre caractéristiques visuelles et textuelles pour aligner les représentations multimodales en utilisant des prototypes linguistiques. Au travers de trois étapes d'entraînement, les VFMs améliorent significativement leur compréhension multimodale, ainsi que les tâches ultérieures comme la classification des classes et la segmentation. En particulier, CoMP-SigLIP atteint des scores de 66,7% sur ChartQA et de 75,9% sur DocVQA, tout en maintenant une précision de 87,4% sur ImageNet-1K et un mIoU de 49,5% sur ADE20K, tout cela sans nécessiter l'utilisation d'un modèle de langage de 0,5B.",
      "upvotes": 19,
      "discussionId": "67e25c4f1908043170bd55a8",
      "projectPage": "https://slimm-x.github.io/comp/",
      "githubRepo": "https://github.com/SliMM-X/CoMP-MM",
      "ai_keywords": [
        "Vision Foundation Models (VFMs)",
        "Continual Rotary Position Embedding",
        "Alignment Loss",
        "language prototypes",
        "multimodal pre-training pipeline",
        "three-stage training",
        "multimodal understanding",
        "classification",
        "segmentation",
        "ChartQA",
        "DocVQA",
        "LLM",
        "ImageNet-1K",
        "ADE20K",
        "frozen chunk evaluation"
      ]
    },
    "publishedAt": "2025-03-24T13:52:47.000Z",
    "title": "CoMP: Continual Multimodal Pre-training for Vision Foundation Models",
    "summary": "Pre-trained Vision Foundation Models (VFMs) provide strong visual\nrepresentations for a wide range of applications. In this paper, we continually\npre-train prevailing VFMs in a multimodal manner such that they can\neffortlessly process visual inputs of varying sizes and produce visual\nrepresentations that are more aligned with language representations, regardless\nof their original pre-training process. To this end, we introduce CoMP, a\ncarefully designed multimodal pre-training pipeline. CoMP uses a Continual\nRotary Position Embedding to support native resolution continual pre-training,\nand an Alignment Loss between visual and textual features through language\nprototypes to align multimodal representations. By three-stage training, our\nVFMs achieve remarkable improvements not only in multimodal understanding but\nalso in other downstream tasks such as classification and segmentation.\nRemarkably, CoMP-SigLIP achieves scores of 66.7 on ChartQA and 75.9 on DocVQA\nwith a 0.5B LLM, while maintaining an 87.4% accuracy on ImageNet-1K and a 49.5\nmIoU on ADE20K under frozen chunk evaluation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18931.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64651db3611ae99d14d392ea",
      "avatarUrl": "/avatars/b818dc0dddc999758ab5737d5053e8c3.svg",
      "fullname": "cyt",
      "name": "Row11n",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.19385",
      "authors": [
        {
          "_id": "67e36241d8da46951f858026",
          "name": "Jaihoon Kim",
          "hidden": false
        },
        {
          "_id": "67e36241d8da46951f858027",
          "name": "Taehoon Yoon",
          "hidden": false
        },
        {
          "_id": "67e36241d8da46951f858028",
          "name": "Jisung Hwang",
          "hidden": false
        },
        {
          "_id": "67e36241d8da46951f858029",
          "name": "Minhyuk Sung",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-25T06:30:45.000Z",
      "submittedOnDailyAt": "2025-03-26T00:49:38.583Z",
      "title": "Flux de modèles qui effectuent l'échelle pendant l'inférence : génération probabiliste et la technique de la file de LoRa force",
      "submittedOnDailyBy": {
        "_id": "6342796a0875f2c99cfd313b",
        "avatarUrl": "/avatars/98575092404c4197b20c929a6499a015.svg",
        "isPro": false,
        "fullname": "Yuseung \"Phillip\" Lee",
        "user": "phillipinseoul",
        "type": "user"
      },
      "summary": "Nous proposons un approche d'échelle pendant l'inférence pour des modèles de flux prédictifs. Récemment, l'échelle pendant l'inférence a suscité un intérêt important dans les modèles de LLMs et de diffusion, étant cruciale pour améliorer la qualité des échantillons et ajuster aux préférences de l'utilisateur par des calculs supplémentaires. Dans les modèles de diffusion, la génération de particules peut être échelonnée efficacement par la randomisation du bruit dans les étapes intermédiaires. D'autre part, les modèles de flux offrent une génération rapide et une qualité élevée d'images et vidéos, ce qui les a convertis en une option populaire par rapport aux modèles de diffusion, bien que les méthodes d'échelle de diffusion ne soient pas directement appliquées dans le processus de génération déterministe. Pour atteindre une échelle efficace pendant l'inférence dans les modèles de flux, nous proposons trois idées principales : 1) la génération basée sur l'EDS, qui permet la génération de particules dans les modèles de flux, 2) la transformation entre protéines, qui élargit l'espace de recherche pour augmenter la diversité des échantillons, et 3) Forçage du Budget de Rollover (RBF), qui assigne une distribution adaptative des canaux de calcul au fil des étapes temporelles pour maximiser son utilisation. Selon nos résultats expérimentaux, la génération basée sur l'EDS, en particulier la génération basée sur les protéines VP (variance preservand), améliore le rendement de la génération de particules lors de l'échelle pendant l'inférence des modèles de flux. De plus, la combinaison de VP-EDS et RBF montre un rendement supérieur à tous les méthodes d'échelle pendant l'inférence existantes.",
      "upvotes": 17,
      "discussionId": "67e36245d8da46951f85802c",
      "ai_keywords": [
        "flow models",
        "inference-time scaling",
        "LLMs",
        "diffusion models",
        "sample quality",
        "user preferences",
        "particle sampling",
        "stochasticity",
        "denoising steps",
        "generative process",
        "SDE-based generation",
        "interpolant conversion",
        "sample diversity",
        "Rollover Budget Forcing (RBF)",
        "adaptive allocation",
        "computational resources",
        "timesteps",
        "budget utilization",
        "variance-preserving (VP)",
        "VP interpolant-based generation"
      ]
    },
    "publishedAt": "2025-03-25T02:30:45.000Z",
    "title": "Inference-Time Scaling for Flow Models via Stochastic Generation and\n  Rollover Budget Forcing",
    "summary": "We propose an inference-time scaling approach for pretrained flow models.\nRecently, inference-time scaling has gained significant attention in LLMs and\ndiffusion models, improving sample quality or better aligning outputs with user\npreferences by leveraging additional computation. For diffusion models,\nparticle sampling has allowed more efficient scaling due to the stochasticity\nat intermediate denoising steps. On the contrary, while flow models have gained\npopularity as an alternative to diffusion models--offering faster generation\nand high-quality outputs in state-of-the-art image and video generative\nmodels--efficient inference-time scaling methods used for diffusion models\ncannot be directly applied due to their deterministic generative process. To\nenable efficient inference-time scaling for flow models, we propose three key\nideas: 1) SDE-based generation, enabling particle sampling in flow models, 2)\nInterpolant conversion, broadening the search space and enhancing sample\ndiversity, and 3) Rollover Budget Forcing (RBF), an adaptive allocation of\ncomputational resources across timesteps to maximize budget utilization. Our\nexperiments show that SDE-based generation, particularly variance-preserving\n(VP) interpolant-based generation, improves the performance of particle\nsampling methods for inference-time scaling in flow models. Additionally, we\ndemonstrate that RBF with VP-SDE achieves the best performance, outperforming\nall previous inference-time scaling approaches.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19385.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "6342796a0875f2c99cfd313b",
      "avatarUrl": "/avatars/98575092404c4197b20c929a6499a015.svg",
      "fullname": "Yuseung \"Phillip\" Lee",
      "name": "phillipinseoul",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.19622",
      "authors": [
        {
          "_id": "67e3706bc9d8214b5e219149",
          "name": "Hongcheng Gao",
          "hidden": false
        },
        {
          "_id": "67e3706bc9d8214b5e21914a",
          "name": "Jiashu Qu",
          "hidden": false
        },
        {
          "_id": "67e3706bc9d8214b5e21914b",
          "name": "Jingyi Tang",
          "hidden": false
        },
        {
          "_id": "67e3706bc9d8214b5e21914c",
          "name": "Baolong Bi",
          "hidden": false
        },
        {
          "_id": "67e3706bc9d8214b5e21914d",
          "name": "Yue Liu",
          "hidden": false
        },
        {
          "_id": "67e3706bc9d8214b5e21914e",
          "name": "Hongyu Chen",
          "hidden": false
        },
        {
          "_id": "67e3706bc9d8214b5e21914f",
          "name": "Li Liang",
          "hidden": false
        },
        {
          "_id": "67e3706bc9d8214b5e219150",
          "name": "Li Su",
          "hidden": false
        },
        {
          "_id": "67e3706bc9d8214b5e219151",
          "name": "Qingming Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-25T13:12:17.000Z",
      "submittedOnDailyAt": "2025-03-26T01:44:03.080Z",
      "title": "Investigation, evaluation, analysis, and measures for the study of slippage in large-scale videos of multimodal models",
      "submittedOnDailyBy": {
        "_id": "62728f4f6253fe2068da1021",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62728f4f6253fe2068da1021/KZ65X0EH98AF3zXemPiap.jpeg",
        "isPro": false,
        "fullname": "Hongcheng Gao",
        "user": "HongchengGao",
        "type": "user"
      },
      "summary": "Les problèmes de l'imagination dans les modèles de langage grands (LMMs) limitent la confiance et la possibilité d'application en fournissant des réponses incorrectes en pratique. Dans cet article, ce problème est étudié et un cadre de référence détaillé appelé \"HAVEN\" est proposé pour évaluer l'imagination dans des tâches de compréhension d'images par LMMs, en les comparant à des modèles dynamiques pour investiguer les problèmes d'imagination dans un modèle plus complexe comme BANK. Ce cadre de référence fournit 6K questions tridimensionnellement organisées selon les causes de l'imagination, ses aspects et la forme des questions, et sont étudiés quantitativement 7 facteurs influants (temps de l'image, taille du modèle, raisonnement du modèle) dans 16 expériences de LMMs. De plus, un modèle de pensée d'images qui inhibe l'imagination en utilisant SRFT (entraînement de raisonnement complémentaire) et TDPO (optimisation directe de préférences) est proposé, basé sur le nouvel algorithme de pensée de modèles comme OpenAI. SRFT améliore la capacité de raisonnement, tandis que TDPO réduit l'imagination lors du processus de pensée. Les larges expériences et analyses les ont démontré que la précision dans l'évaluation de l'imagination a augmenté de 7,65% et les scores de biais ont diminué de 4,5%, démontrant clairement l'effet du modèle. Le code et les données sont disponibles sur https://github.com/Hongcheng-Gao/HAVEN.",
      "upvotes": 16,
      "discussionId": "67e3706dc9d8214b5e2191e0",
      "githubRepo": "https://github.com/Hongcheng-Gao/HAVEN",
      "ai_keywords": [
        "multimodal models (LMMs)",
        "hallucination",
        "video modality",
        "video understanding",
        "HAVEN",
        "hallucination causes",
        "hallucination aspects",
        "question formats",
        "duration time",
        "model sizes",
        "model reasoning",
        "supervised reasoning fine-tuning (SRFT)",
        "direct preference optimization (TDPO)",
        "video-thinking model",
        "accuracy",
        "bias score"
      ]
    },
    "publishedAt": "2025-03-25T09:12:17.000Z",
    "title": "Exploring Hallucination of Large Multimodal Models in Video\n  Understanding: Benchmark, Analysis and Mitigation",
    "summary": "The hallucination of large multimodal models (LMMs), providing responses that\nappear correct but are actually incorrect, limits their reliability and\napplicability. This paper aims to study the hallucination problem of LMMs in\nvideo modality, which is dynamic and more challenging compared to static\nmodalities like images and text. From this motivation, we first present a\ncomprehensive benchmark termed HAVEN for evaluating hallucinations of LMMs in\nvideo understanding tasks. It is built upon three dimensions, i.e.,\nhallucination causes, hallucination aspects, and question formats, resulting in\n6K questions. Then, we quantitatively study 7 influential factors on\nhallucinations, e.g., duration time of videos, model sizes, and model\nreasoning, via experiments of 16 LMMs on the presented benchmark. In addition,\ninspired by recent thinking models like OpenAI o1, we propose a video-thinking\nmodel to mitigate the hallucinations of LMMs via supervised reasoning\nfine-tuning (SRFT) and direct preference optimization (TDPO)-- where SRFT\nenhances reasoning capabilities while TDPO reduces hallucinations in the\nthinking process. Extensive experiments and analyses demonstrate the\neffectiveness. Remarkably, it improves the baseline by 7.65% in accuracy on\nhallucination evaluation and reduces the bias score by 4.5%. The code and data\nare public at https://github.com/Hongcheng-Gao/HAVEN.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19622.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "62728f4f6253fe2068da1021",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62728f4f6253fe2068da1021/KZ65X0EH98AF3zXemPiap.jpeg",
      "fullname": "Hongcheng Gao",
      "name": "HongchengGao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.14905",
      "authors": [
        {
          "_id": "67e250450487eeecfd9a5880",
          "name": "Siwei Wen",
          "hidden": false
        },
        {
          "_id": "67e250450487eeecfd9a5881",
          "name": "Junyan Ye",
          "hidden": false
        },
        {
          "_id": "67e250450487eeecfd9a5882",
          "name": "Peilin Feng",
          "hidden": false
        },
        {
          "_id": "67e250450487eeecfd9a5883",
          "name": "Hengrui Kang",
          "hidden": false
        },
        {
          "_id": "67e250450487eeecfd9a5884",
          "name": "Zichen Wen",
          "hidden": false
        },
        {
          "_id": "67e250450487eeecfd9a5885",
          "name": "Yize Chen",
          "hidden": false
        },
        {
          "_id": "67e250450487eeecfd9a5886",
          "name": "Jiang Wu",
          "hidden": false
        },
        {
          "_id": "67e250450487eeecfd9a5887",
          "name": "Wenjun Wu",
          "hidden": false
        },
        {
          "_id": "67e250450487eeecfd9a5888",
          "name": "Conghui He",
          "hidden": false
        },
        {
          "_id": "67e250450487eeecfd9a5889",
          "name": "Weijia Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-19T05:14:44.000Z",
      "submittedOnDailyAt": "2025-03-26T04:00:13.753Z",
      "title": "\"Détecter le Faux : Détection et Explication des Artifaits d'Images Synthétiques Basées sur des Modèles de Diffusion à Grande Échelle\"",
      "submittedOnDailyBy": {
        "_id": "653b8c3e97a4d71d950e2f20",
        "avatarUrl": "/avatars/b68880022e14556d0be58c69615db3be.svg",
        "isPro": false,
        "fullname": "Zichen Wen",
        "user": "zichenwen",
        "type": "user"
      },
      "summary": "La rapide évolution de la technologie AIGC conduit à des images synthétiques être utilisées de manière plus naturelle dans la vie quotidienne, ce qui pose de nouveaux défis dans l'évaluation de la vérité et la détection. Les méthodes actuelles sont efficaces pour évaluer la vérité des images et identifier les fausses, mais généralement ont un niveau de compréhension humain faible et ne s'adaptent pas suffisamment à la complexité accrue des données synthétiques. Pour résoudre ces problèmes, nous présentons FakeVLM, une outil conçu pour la détection d'images synthétiques et de DeepFake, basé sur un grand modèle de langage. FakeVLM non seulement distingue entre images vraies et fausses, mais fournit également une explication en langage naturel sur les artifices des images, ce qui augmente la possibilité d'interprétation. De plus, nous présentons FakeClue, un ensemble de données détaillé qui comprend plus de 100 000 images dans 7 catégories, pour aider à mieux comprendre ces artifices. FakeVLM montre un rendement comparable aux modèles d'experts, élimine la nécessité de classifieurs supplémentaires et fournit une solution solide pour la détection de données synthétiques. Les évaluations sur divers ensembles de données ont démontré la performance excellente de FakeVLM dans des tâches de classification de la vérité et d'interprétation des artifices, établissant de nouveaux standards de test pour la détection d'images synthétiques. Les ensembles de données et le code sont disponibles sur la suivante URL : https://github.com/opendatalab/FakeVLM.",
      "upvotes": 12,
      "discussionId": "67e250490487eeecfd9a599e",
      "githubRepo": "https://github.com/opendatalab/FakeVLM",
      "ai_keywords": [
        "large multimodal model",
        "FakeVLM",
        "DeepFake detection",
        "image artifacts",
        "natural language explanations",
        "FakeClue",
        "fine-grained artifact clues"
      ]
    },
    "publishedAt": "2025-03-19T01:14:44.000Z",
    "title": "Spot the Fake: Large Multimodal Model-Based Synthetic Image Detection\n  with Artifact Explanation",
    "summary": "With the rapid advancement of Artificial Intelligence Generated Content\n(AIGC) technologies, synthetic images have become increasingly prevalent in\neveryday life, posing new challenges for authenticity assessment and detection.\nDespite the effectiveness of existing methods in evaluating image authenticity\nand locating forgeries, these approaches often lack human interpretability and\ndo not fully address the growing complexity of synthetic data. To tackle these\nchallenges, we introduce FakeVLM, a specialized large multimodal model designed\nfor both general synthetic image and DeepFake detection tasks. FakeVLM not only\nexcels in distinguishing real from fake images but also provides clear, natural\nlanguage explanations for image artifacts, enhancing interpretability.\nAdditionally, we present FakeClue, a comprehensive dataset containing over\n100,000 images across seven categories, annotated with fine-grained artifact\nclues in natural language. FakeVLM demonstrates performance comparable to\nexpert models while eliminating the need for additional classifiers, making it\na robust solution for synthetic data detection. Extensive evaluations across\nmultiple datasets confirm the superiority of FakeVLM in both authenticity\nclassification and artifact explanation tasks, setting a new benchmark for\nsynthetic image detection. The dataset and code will be released in:\nhttps://github.com/opendatalab/FakeVLM.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14905.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "653b8c3e97a4d71d950e2f20",
      "avatarUrl": "/avatars/b68880022e14556d0be58c69615db3be.svg",
      "fullname": "Zichen Wen",
      "name": "zichenwen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.19903",
      "authors": [
        {
          "_id": "67e375d3cc93cc8c42da7699",
          "name": "Baifeng Shi",
          "hidden": false
        },
        {
          "_id": "67e375d3cc93cc8c42da769a",
          "name": "Boyi Li",
          "hidden": false
        },
        {
          "_id": "67e375d3cc93cc8c42da769b",
          "name": "Han Cai",
          "hidden": false
        },
        {
          "_id": "67e375d3cc93cc8c42da769c",
          "name": "Yao Lu",
          "hidden": false
        },
        {
          "_id": "67e375d3cc93cc8c42da769d",
          "name": "Sifei Liu",
          "hidden": false
        },
        {
          "_id": "67e375d3cc93cc8c42da769e",
          "name": "Marco Pavone",
          "hidden": false
        },
        {
          "_id": "67e375d3cc93cc8c42da769f",
          "name": "Jan Kautz",
          "hidden": false
        },
        {
          "_id": "67e375d3cc93cc8c42da76a0",
          "name": "Song Han",
          "hidden": false
        },
        {
          "_id": "67e375d3cc93cc8c42da76a1",
          "name": "Trevor Darrell",
          "hidden": false
        },
        {
          "_id": "67e375d3cc93cc8c42da76a2",
          "name": "Pavlo Molchanov",
          "hidden": false
        },
        {
          "_id": "67e375d3cc93cc8c42da76a3",
          "name": "Hongxu Yin",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/649004218f7cbbc94c782db6/F0o61glyGm9fib9Pl1i-L.mp4"
      ],
      "publishedAt": "2025-03-25T17:58:37.000Z",
      "submittedOnDailyAt": "2025-03-26T02:13:20.800Z",
      "title": "Échelle de résolution 4K similitude des pratiques pratiques",
      "submittedOnDailyBy": {
        "_id": "649004218f7cbbc94c782db6",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/AdgLVfAIpWlug4jXTaEK-.jpeg",
        "isPro": false,
        "fullname": "Baifeng Shi",
        "user": "bfshi",
        "type": "user"
      },
      "summary": "Le reconnaissance des détails visuels à haute résolution est importante dans des tâches quotidiennes. Cependant, l'apprentissage visuel pré-entraîné actuel est limité aux images de basse résolution (par exemple, 378 x 378 pixels) en raison des coûts associés à la traitement d'images grandes, qui sont liés à la dimension bidimensionnelle. Nous introduisons un méthode pour étendre l'apprentissage visuel pré-entraîné de type CLIP à des résolutions de 4K. PS3 remplace la représentation générale de l'image par l'apprentissage relatif, traitant de manière sélective des zones locales et les comparant à des captures détaillées locales pour permettre l'apprentissage de représentations à haute résolution. Cela réduit significativement les coûts de calcul. PS3 peut codifier toute l'image à basse résolution ou traiter de manière sélective des zones à haute résolution liées à un prompt de texte. En appliquant PS3 à un MLLM comme DAMO-Llama, on génère le modèle VILA-HD, qui améliore considérablement la perception visuelle à haute résolution, et comparé à des références comme AnyRes ou S^2, qui n'ont pas d'apprentissage visuel pré-entraîné à haute résolution, augmente la perception visuelle à haute résolution même lorsque 4,3 fois plus de tokens sont utilisés. VILA-HD dépasse les modèles antérieurs comme NVILA et Qwen2-VL en termes de technologies plus récentes, et obtient des résultats excellents sur de nombreux benchmarks. De plus, il est plus efficace que les méthodes d'entraînement de tokens plus récentes. Enfin, actuellement, les benchmarks ne nécessitent pas la perception de résolutions de 4K, par conséquent, nous proposons un nouveau benchmark appelé 4KPro. Dans 4KPro, VILA-HD dépasse tous les MLLM antérieurs, améliore le rendement de GPT-4o de 14,5%, augmente le rendement de Qwen2-VL de 3,2% et fournit un accélération de 2,96 fois.",
      "upvotes": 9,
      "discussionId": "67e375d9cc93cc8c42da785f",
      "projectPage": "https://nvlabs.github.io/PS3/",
      "githubRepo": "https://github.com/NVlabs/PS3",
      "ai_keywords": [
        "PS3",
        "CLIP-style vision pre-training",
        "contrastive learning",
        "local regions",
        "local detailed captions",
        "high-resolution representation learning",
        "computational overhead",
        "saliency",
        "text prompt",
        "VILA-HD",
        "multi-modal LLM",
        "high-resolution visual perception",
        "AnyRes",
        "S^2",
        "scaling properties",
        "test-time compute",
        "NVILA",
        "Qwen2-VL",
        "benchmarks",
        "token pruning approaches",
        "4KPer",
        "image QA",
        "GPT-4o"
      ]
    },
    "publishedAt": "2025-03-25T13:58:37.000Z",
    "title": "Scaling Vision Pre-Training to 4K Resolution",
    "summary": "High-resolution perception of visual details is crucial for daily tasks.\nCurrent vision pre-training, however, is still limited to low resolutions\n(e.g., 378 x 378 pixels) due to the quadratic cost of processing larger images.\nWe introduce PS3 that scales CLIP-style vision pre-training to 4K resolution\nwith a near-constant cost. Instead of contrastive learning on global image\nrepresentation, PS3 is pre-trained by selectively processing local regions and\ncontrasting them with local detailed captions, enabling high-resolution\nrepresentation learning with greatly reduced computational overhead. The\npre-trained PS3 is able to both encode the global image at low resolution and\nselectively process local high-resolution regions based on their saliency or\nrelevance to a text prompt. When applying PS3 to multi-modal LLM (MLLM), the\nresulting model, named VILA-HD, significantly improves high-resolution visual\nperception compared to baselines without high-resolution vision pre-training\nsuch as AnyRes and S^2 while using up to 4.3x fewer tokens. PS3 also unlocks\nappealing scaling properties of VILA-HD, including scaling up resolution for\nfree and scaling up test-time compute for better performance. Compared to state\nof the arts, VILA-HD outperforms previous MLLMs such as NVILA and Qwen2-VL\nacross multiple benchmarks and achieves better efficiency than latest token\npruning approaches. Finally, we find current benchmarks do not require\n4K-resolution perception, which motivates us to propose 4KPro, a new benchmark\nof image QA at 4K resolution, on which VILA-HD outperforms all previous MLLMs,\nincluding a 14.5% improvement over GPT-4o, and a 3.2% improvement and 2.96x\nspeedup over Qwen2-VL.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/649004218f7cbbc94c782db6/F0o61glyGm9fib9Pl1i-L.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19903.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "649004218f7cbbc94c782db6",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/AdgLVfAIpWlug4jXTaEK-.jpeg",
      "fullname": "Baifeng Shi",
      "name": "bfshi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.19855",
      "authors": [
        {
          "_id": "67e36792a281c900d76a93c8",
          "name": "Xiaoyu Tian",
          "hidden": false
        },
        {
          "_id": "67e36792a281c900d76a93c9",
          "name": "Sitong Zhao",
          "hidden": false
        },
        {
          "_id": "67e36792a281c900d76a93ca",
          "name": "Haotian Wang",
          "hidden": false
        },
        {
          "_id": "67e36792a281c900d76a93cb",
          "name": "Shuaiting Chen",
          "hidden": false
        },
        {
          "_id": "67e36792a281c900d76a93cc",
          "name": "Yunjie Ji",
          "hidden": false
        },
        {
          "_id": "67e36792a281c900d76a93cd",
          "name": "Yiping Peng",
          "hidden": false
        },
        {
          "_id": "67e36792a281c900d76a93ce",
          "name": "Han Zhao",
          "hidden": false
        },
        {
          "_id": "67e36792a281c900d76a93cf",
          "name": "Xiangang Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-25T17:19:38.000Z",
      "submittedOnDailyAt": "2025-03-26T01:04:39.479Z",
      "title": "「Alors, réfléchissez à nouveau : l'augmentation des rétroactions de tests répétés dans le processus de validation pour améliorer la logique du LLM」",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Récemment, le développement des modèles de langage grands (LLMs) tels que OpenAI-o1 et DeepSeek-R1 a démontré l'effet de l'échelle sur les modèles. Cela montre que l'amplification d'un processus logique peut considérablement améliorer le rendement du modèle. Cependant, les modèles actuels sont limités par le traitement de longues phrases et l'efficacité de l'apprentissage par renforcement (RL). Pour faire face à ces problèmes, nous proposons un approche simple et efficace d'échelle pendant le test appelée \"Multi-round Thinking\". Cette méthode récolte les réponses précédentes comme prompt pour réconfigurer la logique du modèle, encourager son évolution et améliorer son rendement. Des tests extensives ont été réalisés dans des cadres de référence comme AIME 2024, MATH-500, GPQA-diamond et LiveCodeBench, montrant une augmentation significative du rendement du modèle. Par exemple, la précision de QwQ-32B dans le jeu de données AIME 2024 a augmenté de 80,3% en première ronde à 82,1% en deuxième ronde, et DeepSeek-R1 a également augmenté de 79,7% à 82,0%. Ces résultats montrent que \"Multi-round Thinking\" est largement applicable et peut atteindre un amélioration stable du rendement du modèle avec un approche simple. Cette proposition souligne la possibilité de développement futur des technologies d'échelle pendant le test.",
      "upvotes": 7,
      "discussionId": "67e36793a281c900d76a9459",
      "ai_keywords": [
        "large language models",
        "OpenAI-o1",
        "DeepSeek-R1",
        "test-time scaling",
        "extended reasoning processes",
        "reinforcement learning",
        "Multi-round Thinking",
        "iterative refinement",
        "AIME 2024",
        "MATH-500",
        "GPQA-diamond",
        "LiveCodeBench",
        "accuracy",
        "stable enhancements",
        "test-time scaling techniques"
      ]
    },
    "publishedAt": "2025-03-25T13:19:38.000Z",
    "title": "Think Twice: Enhancing LLM Reasoning by Scaling Multi-round Test-time\n  Thinking",
    "summary": "Recent advances in large language models (LLMs), such as OpenAI-o1 and\nDeepSeek-R1, have demonstrated the effectiveness of test-time scaling, where\nextended reasoning processes substantially enhance model performance. Despite\nthis, current models are constrained by limitations in handling long texts and\nreinforcement learning (RL) training efficiency. To address these issues, we\npropose a simple yet effective test-time scaling approach Multi-round Thinking.\nThis method iteratively refines model reasoning by leveraging previous answers\nas prompts for subsequent rounds. Extensive experiments across multiple models,\nincluding QwQ-32B and DeepSeek-R1, consistently show performance improvements\non various benchmarks such as AIME 2024, MATH-500, GPQA-diamond, and\nLiveCodeBench. For instance, the accuracy of QwQ-32B improved from 80.3% (Round\n1) to 82.1% (Round 2) on the AIME 2024 dataset, while DeepSeek-R1 showed a\nsimilar increase from 79.7% to 82.0%. These results confirm that Multi-round\nThinking is a broadly applicable, straightforward approach to achieving stable\nenhancements in model performance, underscoring its potential for future\ndevelopments in test-time scaling techniques. The key prompt: {Original\nquestion prompt} The assistant's previous answer is: <answer> {last round\nanswer} </answer>, and please re-answer.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19855.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6471
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.19910",
      "authors": [
        {
          "_id": "67e35e4cff080b9ee71e3295",
          "name": "Chuong Huynh",
          "hidden": false
        },
        {
          "_id": "67e35e4cff080b9ee71e3296",
          "name": "Jinyu Yang",
          "hidden": false
        },
        {
          "_id": "67e35e4cff080b9ee71e3297",
          "name": "Ashish Tawari",
          "hidden": false
        },
        {
          "_id": "67e35e4cff080b9ee71e3298",
          "name": "Mubarak Shah",
          "hidden": false
        },
        {
          "_id": "67e35e4cff080b9ee71e3299",
          "name": "Son Tran",
          "hidden": false
        },
        {
          "_id": "67e35e4cff080b9ee71e329a",
          "name": "Raffay Hamid",
          "hidden": false
        },
        {
          "_id": "67e35e4cff080b9ee71e329b",
          "name": "Trishul Chilimbi",
          "hidden": false
        },
        {
          "_id": "67e35e4cff080b9ee71e329c",
          "name": "Abhinav Shrivastava",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-25T17:59:50.000Z",
      "submittedOnDailyAt": "2025-03-26T00:26:00.764Z",
      "title": "CoLLM : Modèle de langage grand pour la recherche d'images synthétiques",
      "submittedOnDailyBy": {
        "_id": "63a4d196cde2b28f82a56bd9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4d196cde2b28f82a56bd9/iqVFOtDteRMUScFGRcx0L.png",
        "isPro": false,
        "fullname": "Chuong Huynh",
        "user": "chuonghm",
        "type": "user"
      },
      "summary": "La Composition of Image Recovery (CIR) est une tâche complexe qui se base sur la demande de plusieurs modèles pour rechercher des images. Les données d'entraînement générales sont constituées de trois éléments : des images de référence, des descriptions textuelles des changements souhaités et des images cibles. L'obtention de ces données nécessite des coûts et du temps. L'absence de jeux de données CIR a été abordée par l'utilisation de méthodes 0-shot qui utilisent trois des éléments mentionnés, ainsi que par l'utilisation de pairs de commentaires d'images web récoltés avec des modèles de langage visuel (VLMs). Cependant, ces méthodes ont des limites, telles que l'absence d'échelle, la diversité et la nature non naturelle des textes de changement, ainsi que l'impédiment au partage d'apprentissage pour plusieurs demandes de modèles en raison de la manque de trois données. De plus, les méthodes actuelles ne s'adaptent pas aux textes de changement complexes. Nous proposons CoLLM pour résoudre ces limites. Notre approche génère automatiquement trois des éléments à partir de pairs de commentaires d'images, permettant un entraînement sous-objectif sans nécessité d'explications manuelles. Nous utilisons des modèles de langage à grande échelle (LLMs) pour générer l'apprentissage partagé des références d'images et des textes de changement, et promouvent la fusion de plusieurs modèles. De plus, nous introduisons le Multi-Text CIR (MTCIR), qui inclut 3,4 millions d'échantillons, améliore les benchmarks actuels de CIR (CIRR et Fashion-IQ) et augmente la confiance dans l'évaluation. Grâce aux résultats des expériences, CoLLM a atteint les meilleurs rendements sur plusieurs benchmarks de CIR et configurations. Le MTCIR a fourni un amélioration de 15% du rendement, et nos améliorations sur les benchmarks ont créé des critères d'évaluation plus fiables pour les modèles de CIR, contribuant au développement de cette zone importante.",
      "upvotes": 6,
      "discussionId": "67e35e4eff080b9ee71e3353",
      "projectPage": "https://collm-cvpr25.github.io/",
      "ai_keywords": [
        "Composed Image Retrieval (CIR)",
        "multimodal query",
        "triplets",
        "reference image",
        "textual description",
        "target image",
        "zero-shot approaches",
        "synthetic triplets",
        "vision-language models (VLMs)",
        "web-crawled image-caption pairs",
        "joint embedding learning",
        "complex and nuanced modification texts",
        "multimodal fusion",
        "CoLLM",
        "Large Language Models (LLMs)",
        "Multi-Text CIR (MTCIR)",
        "CIRR benchmark",
        "Fashion-IQ benchmark",
        "state-of-the-art performance"
      ]
    },
    "publishedAt": "2025-03-25T13:59:50.000Z",
    "title": "CoLLM: A Large Language Model for Composed Image Retrieval",
    "summary": "Composed Image Retrieval (CIR) is a complex task that aims to retrieve images\nbased on a multimodal query. Typical training data consists of triplets\ncontaining a reference image, a textual description of desired modifications,\nand the target image, which are expensive and time-consuming to acquire. The\nscarcity of CIR datasets has led to zero-shot approaches utilizing synthetic\ntriplets or leveraging vision-language models (VLMs) with ubiquitous\nweb-crawled image-caption pairs. However, these methods have significant\nlimitations: synthetic triplets suffer from limited scale, lack of diversity,\nand unnatural modification text, while image-caption pairs hinder joint\nembedding learning of the multimodal query due to the absence of triplet data.\nMoreover, existing approaches struggle with complex and nuanced modification\ntexts that demand sophisticated fusion and understanding of vision and language\nmodalities. We present CoLLM, a one-stop framework that effectively addresses\nthese limitations. Our approach generates triplets on-the-fly from\nimage-caption pairs, enabling supervised training without manual annotation. We\nleverage Large Language Models (LLMs) to generate joint embeddings of reference\nimages and modification texts, facilitating deeper multimodal fusion.\nAdditionally, we introduce Multi-Text CIR (MTCIR), a large-scale dataset\ncomprising 3.4M samples, and refine existing CIR benchmarks (CIRR and\nFashion-IQ) to enhance evaluation reliability. Experimental results demonstrate\nthat CoLLM achieves state-of-the-art performance across multiple CIR benchmarks\nand settings. MTCIR yields competitive results, with up to 15% performance\nimprovement. Our refined benchmarks provide more reliable evaluation metrics\nfor CIR models, contributing to the advancement of this important field.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19910.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a4d196cde2b28f82a56bd9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4d196cde2b28f82a56bd9/iqVFOtDteRMUScFGRcx0L.png",
      "fullname": "Chuong Huynh",
      "name": "chuonghm",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.18446",
      "authors": [
        {
          "_id": "67e367ee4363e3c4bbbaca3a",
          "name": "Jinho Jeong",
          "hidden": false
        },
        {
          "_id": "67e367ee4363e3c4bbbaca3b",
          "name": "Sangmin Han",
          "hidden": false
        },
        {
          "_id": "67e367ee4363e3c4bbbaca3c",
          "name": "Jinwoo Kim",
          "hidden": false
        },
        {
          "_id": "67e367ee4363e3c4bbbaca3d",
          "name": "Seon Joo Kim",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-24T08:50:15.000Z",
      "submittedOnDailyAt": "2025-03-26T01:07:15.007Z",
      "title": "Bien sûr, voici la traduction en français :\n\n\"Veuillez patienter un moment. La traduction est en cours.\"",
      "submittedOnDailyBy": {
        "_id": "66b5f733f0c16f37f307f35e",
        "avatarUrl": "/avatars/29a97e10b4d65aa23d7eae238f809499.svg",
        "isPro": false,
        "fullname": "JinHo Jeong",
        "user": "3587jjh",
        "type": "user"
      },
      "summary": "Dans cet article, nous proposons un nouveau cadre de travail appelé LSRNA pour la génération d'images à haute résolution (supérieure à 1K) dans l'espace potentiel. Les modèles de diffusion rencontrent des difficultés lorsqu'ils dépassent la résolution d'entraînement, présentant des problèmes structurels et une mauvaise représentation du contenu. Les méthodes basées sur des références résolvent ces problèmes en guidant la génération à haute résolution à partir d'images à basse résolution, mais l'upsampling dans l'espace potentiel introduit un biais vers la variabilité réduite, ce qui affecte la qualité du résultat. D'autre part, l'upsampling dans l'espace RGB génère des sorties trop planifiées. Pour surmonter ces limitations, LSRNA combine un upsampling à haute résolution (LSR) dans l'espace potentiel avec un méthode d'ajout de bruit par zones (RNA) pour renforcer les détails de haute fréquence. Comparé aux méthodes les plus récentes basées sur des références dans les modèles de diffusion, LSRNA montre ses avantages et met en avant l'importance de l'upsampling dans l'espace potentiel pour maintenir la variabilité et l'ajout de bruit. Le code est disponible sur https://github.com/3587jjh/LSRNA.",
      "upvotes": 4,
      "discussionId": "67e367f14363e3c4bbbacae1",
      "ai_keywords": [
        "LSRNA",
        "diffusion models",
        "latent space",
        "super-resolution",
        "structural distortions",
        "content repetition",
        "reference-based methods",
        "manifold deviation",
        "RGB space",
        "manifold alignment",
        "Region-wise Noise Addition (RNA)",
        "high-frequency details"
      ]
    },
    "publishedAt": "2025-03-24T04:50:15.000Z",
    "title": "Latent Space Super-Resolution for Higher-Resolution Image Generation\n  with Diffusion Models",
    "summary": "In this paper, we propose LSRNA, a novel framework for higher-resolution\n(exceeding 1K) image generation using diffusion models by leveraging\nsuper-resolution directly in the latent space. Existing diffusion models\nstruggle with scaling beyond their training resolutions, often leading to\nstructural distortions or content repetition. Reference-based methods address\nthe issues by upsampling a low-resolution reference to guide higher-resolution\ngeneration. However, they face significant challenges: upsampling in latent\nspace often causes manifold deviation, which degrades output quality. On the\nother hand, upsampling in RGB space tends to produce overly smoothed outputs.\nTo overcome these limitations, LSRNA combines Latent space Super-Resolution\n(LSR) for manifold alignment and Region-wise Noise Addition (RNA) to enhance\nhigh-frequency details. Our extensive experiments demonstrate that integrating\nLSRNA outperforms state-of-the-art reference-based methods across various\nresolutions and metrics, while showing the critical role of latent space\nupsampling in preserving detail and sharpness. The code is available at\nhttps://github.com/3587jjh/LSRNA.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18446.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "66b5f733f0c16f37f307f35e",
      "avatarUrl": "/avatars/29a97e10b4d65aa23d7eae238f809499.svg",
      "fullname": "JinHo Jeong",
      "name": "3587jjh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.13964",
      "authors": [
        {
          "_id": "67e20852c0c932395394dbb0",
          "name": "Siwei Han",
          "hidden": false
        },
        {
          "_id": "67e20852c0c932395394dbb1",
          "name": "Peng Xia",
          "hidden": false
        },
        {
          "_id": "67e20852c0c932395394dbb2",
          "name": "Ruiyi Zhang",
          "hidden": false
        },
        {
          "_id": "67e20852c0c932395394dbb3",
          "name": "Tong Sun",
          "hidden": false
        },
        {
          "_id": "67e20852c0c932395394dbb4",
          "name": "Yun Li",
          "hidden": false
        },
        {
          "_id": "67e20852c0c932395394dbb5",
          "name": "Hongtu Zhu",
          "hidden": false
        },
        {
          "_id": "67e20852c0c932395394dbb6",
          "name": "Huaxiu Yao",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/643e9ee6f6bb3c31a26e7bc4/smmwVmcnReTUxH6xDnyU1.png"
      ],
      "publishedAt": "2025-03-18T06:57:21.000Z",
      "submittedOnDailyAt": "2025-03-26T03:52:37.520Z",
      "title": "MDocAgent : Marco de Framework Multicuenta Multimodel pour la Compréhension de Documents",
      "submittedOnDailyBy": {
        "_id": "643e9ee6f6bb3c31a26e7bc4",
        "avatarUrl": "/avatars/acfaa7d6a23dada24c86b954c3be116a.svg",
        "isPro": false,
        "fullname": "Peng Xia",
        "user": "richardxp888",
        "type": "user"
      },
      "summary": "DocQA est une tâche très courante. Les méthodes existantes utilisent des grands modèles de langue (LLMs) ou des grands modèles de vision et de langue (LVLMs) et effectuent un traitement de recherche réinforcé génératif (RAG), mais ces méthodes privilégient l'information d'un seul modèle et ne peuvent pas intégrer efficacement le texte et l'image. Cet approche devient complexe et a des limites dans le rendement des documents réels. Nous présentons un nouveau RAG et un cadre de travail multi-agent pour la compréhension des documents, appelé MDocAgent (cadre de travail multi-modèle multi-agent). Ce cadre de travail utilise aussi bien le texte que les images. Le système utilise 5 agents professionnels : agent général, agent évaluateur, agent de texte, agent d'image et agent de résumé. Ces agents se concentrent sur la recherche de contexte multi-modèle et combinent leurs perspectives pour comprendre mieux le contenu du document. Cette approche coopérative permet que le système synthétise l'information des composants textuels et visuels et améliore la précision des réponses aux questions. Les expériences initiales sur 5 benchmarks comme MMLongBench et LongDocURL montrent l'effet de notre MDocAgent, avec un amélioration moyenne de 12,1% par rapport aux méthodes les plus avancées actuelles. Cette étude contribue au développement d'un système plus robuste et détaillé pour la compréhension des documents complexes. Les données et le code sont disponibles sur https://github.com/aiming-lab/MDocAgent.",
      "upvotes": 4,
      "discussionId": "67e20858c0c932395394dde6",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "Large Vision Language Models (LVLMs)",
        "Retrieval Augmented Generation (RAG)",
        "multi-modal reasoning",
        "multi-modal multi-agent framework",
        "general agent",
        "critical agent",
        "text agent",
        "image agent",
        "summarizing agent",
        "multi-modal context retrieval"
      ]
    },
    "publishedAt": "2025-03-18T02:57:21.000Z",
    "title": "MDocAgent: A Multi-Modal Multi-Agent Framework for Document\n  Understanding",
    "summary": "Document Question Answering (DocQA) is a very common task. Existing methods\nusing Large Language Models (LLMs) or Large Vision Language Models (LVLMs) and\nRetrieval Augmented Generation (RAG) often prioritize information from a single\nmodal, failing to effectively integrate textual and visual cues. These\napproaches struggle with complex multi-modal reasoning, limiting their\nperformance on real-world documents. We present MDocAgent (A Multi-Modal\nMulti-Agent Framework for Document Understanding), a novel RAG and multi-agent\nframework that leverages both text and image. Our system employs five\nspecialized agents: a general agent, a critical agent, a text agent, an image\nagent and a summarizing agent. These agents engage in multi-modal context\nretrieval, combining their individual insights to achieve a more comprehensive\nunderstanding of the document's content. This collaborative approach enables\nthe system to synthesize information from both textual and visual components,\nleading to improved accuracy in question answering. Preliminary experiments on\nfive benchmarks like MMLongBench, LongDocURL demonstrate the effectiveness of\nour MDocAgent, achieve an average improvement of 12.1% compared to current\nstate-of-the-art method. This work contributes to the development of more\nrobust and comprehensive DocQA systems capable of handling the complexities of\nreal-world documents containing rich textual and visual information. Our data\nand code are available at https://github.com/aiming-lab/MDocAgent.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/643e9ee6f6bb3c31a26e7bc4/smmwVmcnReTUxH6xDnyU1.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13964.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "643e9ee6f6bb3c31a26e7bc4",
      "avatarUrl": "/avatars/acfaa7d6a23dada24c86b954c3be116a.svg",
      "fullname": "Peng Xia",
      "name": "richardxp888",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.19470",
      "authors": [
        {
          "_id": "67e365b0dcfc2aeae1bf3da2",
          "name": "Mingyang Chen",
          "hidden": false
        },
        {
          "_id": "67e365b0dcfc2aeae1bf3da3",
          "name": "Tianpeng Li",
          "hidden": false
        },
        {
          "_id": "67e365b0dcfc2aeae1bf3da4",
          "name": "Haoze Sun",
          "hidden": false
        },
        {
          "_id": "67e365b0dcfc2aeae1bf3da5",
          "name": "Yijie Zhou",
          "hidden": false
        },
        {
          "_id": "67e365b0dcfc2aeae1bf3da6",
          "name": "Chenzheng Zhu",
          "hidden": false
        },
        {
          "_id": "67e365b0dcfc2aeae1bf3da7",
          "name": "Fan Yang",
          "hidden": false
        },
        {
          "_id": "67e365b0dcfc2aeae1bf3da8",
          "name": "Zenan Zhou",
          "hidden": false
        },
        {
          "_id": "67e365b0dcfc2aeae1bf3da9",
          "name": "Weipeng Chen",
          "hidden": false
        },
        {
          "_id": "67e365b0dcfc2aeae1bf3daa",
          "name": "Haofen Wang",
          "hidden": false
        },
        {
          "_id": "67e365b0dcfc2aeae1bf3dab",
          "name": "Jeff Z. Pan",
          "hidden": false
        },
        {
          "_id": "67e365b0dcfc2aeae1bf3dac",
          "name": "Wen Zhang",
          "hidden": false
        },
        {
          "_id": "67e365b0dcfc2aeae1bf3dad",
          "name": "Huajun Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-25T09:00:58.000Z",
      "submittedOnDailyAt": "2025-03-26T00:56:07.098Z",
      "title": "Étude : Exploiter l'apprentissage par inférence dans les LLMs à l'aide de la recherche et de l'apprentissage par renforcement.",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Les modèles de langue de grande échelle (LLMs) montrent des capacités logiques exceptionnelles, ce qui est observable dans le succès d'OpenAI-o1 et DeepSeek-R1. Cependant, l'intégration de la logique avec des processus de recherche externe est particulièrement complexe lorsqu'il s'agit de questions multi-aspectes nécessitant plusieurs étapes de recherche. Nous proposons un nouveau cadre de travail appelé ReSearch, où nous développons une logique apprise par apprentissage par renforcement, ce qui permet d'éviter l'utilisation de données supervisées pour les étapes logiques. Notre approche considère la recherche comme un composant intégrant de la chaîne logique, et la recherche est guidée par la base de pensée en phrases, avec le temps et la méthode de recherche influençant le pensée ultérieure. ReSearch a été entraîné en utilisant des modèles tels que Qwen2.5-7B(-Instruct) et Qwen2.5-32B(-Instruct), ce qui a permis d'effectuer de nombreux expériments. Malgré l'entraînement sur un seul ensemble de données, notre modèle a démontré une forte capacité de généralisation sur différents benchmarks. L'analyse montre que ReSearch développe naturellement des habiletés logiques progressives lors du processus d'apprentissage par renforcement. De plus, elle fonctionne efficacement sur divers aspects, y compris la nature et la correction automatique.",
      "upvotes": 3,
      "discussionId": "67e365b1dcfc2aeae1bf3df6",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "OpenAI-o1",
        "DeepSeek-R1",
        "complex multi-hop questions",
        "ReSearch",
        "reinforcement learning",
        "text-based thinking",
        "reflection",
        "self-correction",
        "Qwen2.5-7B(-Instruct)",
        "Qwen2.5-32B(-Instruct)"
      ]
    },
    "publishedAt": "2025-03-25T05:00:58.000Z",
    "title": "ReSearch: Learning to Reason with Search for LLMs via Reinforcement\n  Learning",
    "summary": "Large Language Models (LLMs) have shown remarkable capabilities in reasoning,\nexemplified by the success of OpenAI-o1 and DeepSeek-R1. However, integrating\nreasoning with external search processes remains challenging, especially for\ncomplex multi-hop questions requiring multiple retrieval steps. We propose\nReSearch, a novel framework that trains LLMs to Reason with Search via\nreinforcement learning without using any supervised data on reasoning steps.\nOur approach treats search operations as integral components of the reasoning\nchain, where when and how to perform searches is guided by text-based thinking,\nand search results subsequently influence further reasoning. We train ReSearch\non Qwen2.5-7B(-Instruct) and Qwen2.5-32B(-Instruct) models and conduct\nextensive experiments. Despite being trained on only one dataset, our models\ndemonstrate strong generalizability across various benchmarks. Analysis reveals\nthat ReSearch naturally elicits advanced reasoning capabilities such as\nreflection and self-correction during the reinforcement learning process.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19470.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6471
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.19041",
      "authors": [
        {
          "_id": "67e35da0b1b97cc3392024b1",
          "name": "Kangwei Liu",
          "hidden": false
        },
        {
          "_id": "67e35da0b1b97cc3392024b2",
          "name": "Mengru Wang",
          "hidden": false
        },
        {
          "_id": "67e35da0b1b97cc3392024b3",
          "name": "Yujie Luo",
          "hidden": false
        },
        {
          "_id": "67e35da0b1b97cc3392024b4",
          "name": "Lin Yuan",
          "hidden": false
        },
        {
          "_id": "67e35da0b1b97cc3392024b5",
          "name": "Mengshu Sun",
          "hidden": false
        },
        {
          "_id": "67e35da0b1b97cc3392024b6",
          "name": "Ningyu Zhang",
          "hidden": false
        },
        {
          "_id": "67e35da0b1b97cc3392024b7",
          "name": "Lei Liang",
          "hidden": false
        },
        {
          "_id": "67e35da0b1b97cc3392024b8",
          "name": "Zhiqiang Zhang",
          "hidden": false
        },
        {
          "_id": "67e35da0b1b97cc3392024b9",
          "name": "Jun Zhou",
          "hidden": false
        },
        {
          "_id": "67e35da0b1b97cc3392024ba",
          "name": "Huajun Chen",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/620b3bbb0668e435407c8d0a/f0fTHNDhXrS7zWpmuxVU-.png"
      ],
      "publishedAt": "2025-03-24T18:11:42.000Z",
      "submittedOnDailyAt": "2025-03-26T00:22:20.466Z",
      "title": "LookAhead Tuning : Modèles de Langage plus Sécurisés à travers des Prévisualisations Partielles des Réponses",
      "submittedOnDailyBy": {
        "_id": "620b3bbb0668e435407c8d0a",
        "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
        "isPro": false,
        "fullname": "Ningyu Zhang",
        "user": "Ningyu",
        "type": "user"
      },
      "summary": "Dans le cadre du fine-tuning, les modèles de langage grands (LLMs) peuvent s'adapter à un domaine spécifique, mais si ils sont utilisés directement, la régulation de sécurité peut se démanteler. Pour résoudre ce problème, nous avons introduit LookAhead Tuning, qui consiste en deux méthodes simples et à faible consommation de ressources. Ces méthodes visent à maintenir la structure de sécurité propre au modèle et à minimiser les variations dans la distribution des premiers tokens, en modifiant certaines réponses prédites précédemment dans le jeu de données d'entraînement. Les expériences concrètes montrent que LookAhead Tuning peut maintenir la sécurité du modèle sans sacrifier le rendement dans les tâches de décharge, démontrant son efficacité. Nos résultats constituent une confiance dans la capacité de l'adaptation sécurisée et efficace des LLMs, ainsi que dans des solutions efficaces. Le code est disponible sur https://github.com/zjunlp/LookAheadTuning.",
      "upvotes": 3,
      "discussionId": "67e35da1b1b97cc339202525",
      "ai_keywords": [
        "LookAhead Tuning",
        "safety alignment",
        "data-driven methods",
        "partial answer prefixes",
        "token distributions",
        "robust performance",
        "downstream tasks"
      ]
    },
    "publishedAt": "2025-03-24T14:11:42.000Z",
    "title": "LookAhead Tuning: Safer Language Models via Partial Answer Previews",
    "summary": "Fine-tuning enables large language models (LLMs) to adapt to specific\ndomains, but often undermines their previously established safety alignment. To\nmitigate the degradation of model safety during fine-tuning, we introduce\nLookAhead Tuning, which comprises two simple, low-resource, and effective\ndata-driven methods that modify training data by previewing partial answer\nprefixes. Both methods aim to preserve the model's inherent safety mechanisms\nby minimizing perturbations to initial token distributions. Comprehensive\nexperiments demonstrate that LookAhead Tuning effectively maintains model\nsafety without sacrificing robust performance on downstream tasks. Our findings\nposition LookAhead Tuning as a reliable and efficient solution for the safe and\neffective adaptation of LLMs. Code is released at\nhttps://github.com/zjunlp/LookAheadTuning.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/620b3bbb0668e435407c8d0a/f0fTHNDhXrS7zWpmuxVU-.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19041.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "620b3bbb0668e435407c8d0a",
      "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
      "fullname": "Ningyu Zhang",
      "name": "Ningyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 20
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.18783",
      "authors": [
        {
          "_id": "67e2a43d5116df47da357eec",
          "user": {
            "_id": "642438eaa3adbc7142c3ca0f",
            "avatarUrl": "/avatars/8deff70e0c93d259a42ee47f00a31e3e.svg",
            "isPro": false,
            "fullname": "CharlesChen",
            "user": "CharlesChen2023",
            "type": "user"
          },
          "name": "Linwei Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-25T14:36:22.430Z",
          "hidden": false
        },
        {
          "_id": "67e2a43d5116df47da357eed",
          "name": "Lin Gu",
          "hidden": false
        },
        {
          "_id": "67e2a43d5116df47da357eee",
          "name": "Liang Li",
          "hidden": false
        },
        {
          "_id": "67e2a43d5116df47da357eef",
          "name": "Chenggang Yan",
          "hidden": false
        },
        {
          "_id": "67e2a43d5116df47da357ef0",
          "name": "Ying Fu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-24T15:32:06.000Z",
      "submittedOnDailyAt": "2025-03-26T01:08:28.390Z",
      "title": "Frequence Convolution Dynamique pour la Prédiction d'Images Denses",
      "submittedOnDailyBy": {
        "_id": "642438eaa3adbc7142c3ca0f",
        "avatarUrl": "/avatars/8deff70e0c93d259a42ee47f00a31e3e.svg",
        "isPro": false,
        "fullname": "CharlesChen",
        "user": "CharlesChen2023",
        "type": "user"
      },
      "summary": "DY-Conv combine plusieurs poids parallèles et fonctions d'attention pour permettre la sélection adaptative des poids, démontrant un comportement notable, mais avec des réponses fréquentielles très similaires et un coût élevé de paramètres associé à une adaptabilité limitée. Dans cet article, nous présentons une nouvelle approche appelée Frequency Dynamic Convolution (FDConv), qui apprend des vecteurs de paramètres fixes dans le domaine de la fréquence pour atténuer ces limitations. FDConv divise les vecteurs de paramètres en groupes basés sur des indices de fréquence différents, maintenant la diversité fréquentielle tout en évitant l'augmentation du coût de paramètres. Pour améliorer l'adaptabilité, nous proposons le Kernel Spatial Modulation (KSM) et le Frequency Band Modulation (FBM). KSM ajuste la réponse fréquentielle de chaque filtre de manière dynamique au niveau spatial, tandis que FBM ajuste les poids décomposés dans différentes bandes de fréquence en fonction du contenu local, améliorant ainsi l'adaptabilité. Des expérimentations sont réalisées dans diverses domaines comme la détection d'objets, la segmentation et la classification pour démontrer les avantages de FDConv. En appliquant FDConv à ResNet-50, nous atteignons un comportement supérieur avec un léger augmentation du coût de paramètres (+3,6M paramètres), sans nécessité d'un grand augmentation comme dans CondConv (+90M paramètres) ou KW (+76,5M paramètres). De plus, FDConv s'intègre facilement dans différentes architectures comme ConvNeXt et Swin-Transformer, offrant des solutions flexibles et efficaces pour les tâches visuelles modernes. Le code est disponible sur https://github.com/Linwei-Chen/FDConv.",
      "upvotes": 2,
      "discussionId": "67e2a4405116df47da357ff7",
      "ai_keywords": [
        "Dynamic Convolution (DY-Conv)",
        "Frequency Dynamic Convolution (FDConv)",
        "attention mechanism",
        "parameter budget",
        "Fourier domain",
        "frequency-based groups",
        "disjoint Fourier indices",
        "frequency-diverse weights",
        "Kernel Spatial Modulation (KSM)",
        "Frequency Band Modulation (FBM)",
        "frequency response",
        "spatial level",
        "frequency bands",
        "local content",
        "object detection",
        "segmentation",
        "classification",
        "ResNet-50",
        "ConvNeXt",
        "Swin-Transformer",
        "parameter-efficient"
      ]
    },
    "publishedAt": "2025-03-24T11:32:06.000Z",
    "title": "Frequency Dynamic Convolution for Dense Image Prediction",
    "summary": "While Dynamic Convolution (DY-Conv) has shown promising performance by\nenabling adaptive weight selection through multiple parallel weights combined\nwith an attention mechanism, the frequency response of these weights tends to\nexhibit high similarity, resulting in high parameter costs but limited\nadaptability. In this work, we introduce Frequency Dynamic Convolution\n(FDConv), a novel approach that mitigates these limitations by learning a fixed\nparameter budget in the Fourier domain. FDConv divides this budget into\nfrequency-based groups with disjoint Fourier indices, enabling the construction\nof frequency-diverse weights without increasing the parameter cost. To further\nenhance adaptability, we propose Kernel Spatial Modulation (KSM) and Frequency\nBand Modulation (FBM). KSM dynamically adjusts the frequency response of each\nfilter at the spatial level, while FBM decomposes weights into distinct\nfrequency bands in the frequency domain and modulates them dynamically based on\nlocal content. Extensive experiments on object detection, segmentation, and\nclassification validate the effectiveness of FDConv. We demonstrate that when\napplied to ResNet-50, FDConv achieves superior performance with a modest\nincrease of +3.6M parameters, outperforming previous methods that require\nsubstantial increases in parameter budgets (e.g., CondConv +90M, KW +76.5M).\nMoreover, FDConv seamlessly integrates into a variety of architectures,\nincluding ConvNeXt, Swin-Transformer, offering a flexible and efficient\nsolution for modern vision tasks. The code is made publicly available at\nhttps://github.com/Linwei-Chen/FDConv.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18783.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642438eaa3adbc7142c3ca0f",
      "avatarUrl": "/avatars/8deff70e0c93d259a42ee47f00a31e3e.svg",
      "fullname": "CharlesChen",
      "name": "CharlesChen2023",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.17361",
      "authors": [
        {
          "_id": "67e35ca7363374850440d91d",
          "name": "Sophia Tang",
          "hidden": false
        },
        {
          "_id": "67e35ca7363374850440d91e",
          "name": "Yinuo Zhang",
          "hidden": false
        },
        {
          "_id": "67e35ca7363374850440d91f",
          "name": "Alexander Tong",
          "hidden": false
        },
        {
          "_id": "67e35ca7363374850440d920",
          "user": {
            "_id": "64cd5b3f0494187a9e8b7c69",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/eo9HiGbvCxHQZ-QJB1Y_o.jpeg",
            "isPro": false,
            "fullname": "Pranam Chatterjee",
            "user": "pranamanam",
            "type": "user"
          },
          "name": "Pranam Chatterjee",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-03-26T01:57:51.167Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-21T17:59:43.000Z",
      "submittedOnDailyAt": "2025-03-26T00:18:51.908Z",
      "title": "Gelmar Soft Matching Flow avec une interface utilisateur directe pour la génération d'arrangements biologiques contrôlables",
      "submittedOnDailyBy": {
        "_id": "64cd5b3f0494187a9e8b7c69",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/eo9HiGbvCxHQZ-QJB1Y_o.jpeg",
        "isPro": false,
        "fullname": "Pranam Chatterjee",
        "user": "pranamanam",
        "type": "user"
      },
      "summary": "Le flux de formation dans la simplification des nouvelles a apparu comme une stratégie prometteuse pour le design de séquences d'ADN, bien qu'il soit difficile d'écaller ce flux à des simplifications de haut niveau nécessaires pour la génération de peptides et de protéines. Nous avons introduit un nouvel interpréteur de gomerbal-softmax avec température dépendante comme base pour un cadre de génération dans les simplifications, conjointement avec le flux de gomerbal-softmax et le score-matching. En utilisant cet interpréteur, nous avons extrait le flux de gomerbal-softmax de formation et introduit un champ de vitesse paramétrisé qui permet de déplacer d'une distribution catégorielle douce vers une distribution concentrée sur un sommet de la simplification. De plus, nous proposons le score-matching de gomerbal-softmax, apprenant les gradients de densité de probabilité pour réaliser une régression logistique. Notre cadre de travail permet des générations de haute qualité et de diversité, et est scalable à des simplifications de haut niveau. Nous proposons un flux de guidage basé sur des classifieurs (STGFlow) pour faciliter la génération sans entraînement, en utilisant un classifieur prétrainé avec des séquences nettes pour exécuter efficacement le flux de guidage lors de l'inférence. STGFlow peut être combiné avec n'importe quel méthode de flux discret et forme un cadre solide pour la génération de séquences de dénudes contrôlables. Nous avons démontré la capacité de notre cadre pour des designs de promoteurs d'ADN conditionnels, la génération de protéines à partir de séquences, et le design de peptides d'union cible pour le traitement de maladies rares.",
      "upvotes": 1,
      "discussionId": "67e35caa363374850440d9df",
      "ai_keywords": [
        "Gumbel-Softmax Flow",
        "Score Matching",
        "simplex",
        "Gumbel-Softmax interpolant",
        "time-dependent temperature",
        "parameterized velocity field",
        "smooth categorical distributions",
        "Gumbel-Softmax Flow Matching",
        "Straight-Through Guided Flows",
        "STGFlow",
        "straight-through estimators",
        "classifiers",
        "de novo sequence generation",
        "conditional DNA promoter design",
        "sequence-only protein generation",
        "target-binding peptide design"
      ]
    },
    "publishedAt": "2025-03-21T13:59:43.000Z",
    "title": "Gumbel-Softmax Flow Matching with Straight-Through Guidance for\n  Controllable Biological Sequence Generation",
    "summary": "Flow matching in the continuous simplex has emerged as a promising strategy\nfor DNA sequence design, but struggles to scale to higher simplex dimensions\nrequired for peptide and protein generation. We introduce Gumbel-Softmax Flow\nand Score Matching, a generative framework on the simplex based on a novel\nGumbel-Softmax interpolant with a time-dependent temperature. Using this\ninterpolant, we introduce Gumbel-Softmax Flow Matching by deriving a\nparameterized velocity field that transports from smooth categorical\ndistributions to distributions concentrated at a single vertex of the simplex.\nWe alternatively present Gumbel-Softmax Score Matching which learns to regress\nthe gradient of the probability density. Our framework enables high-quality,\ndiverse generation and scales efficiently to higher-dimensional simplices. To\nenable training-free guidance, we propose Straight-Through Guided Flows\n(STGFlow), a classifier-based guidance method that leverages straight-through\nestimators to steer the unconditional velocity field toward optimal vertices of\nthe simplex. STGFlow enables efficient inference-time guidance using\nclassifiers pre-trained on clean sequences, and can be used with any discrete\nflow method. Together, these components form a robust framework for\ncontrollable de novo sequence generation. We demonstrate state-of-the-art\nperformance in conditional DNA promoter design, sequence-only protein\ngeneration, and target-binding peptide design for rare disease treatment.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17361.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64cd5b3f0494187a9e8b7c69",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/eo9HiGbvCxHQZ-QJB1Y_o.jpeg",
      "fullname": "Pranam Chatterjee",
      "name": "pranamanam",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.17237",
      "authors": [
        {
          "_id": "67e2b68e08c6a250edda264a",
          "user": {
            "_id": "67e2063e1ee7f6db889849d6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67e2063e1ee7f6db889849d6/ihiwCCqbXlxQ2V_SSGnng.jpeg",
            "isPro": false,
            "fullname": "Yu-Hsi Chen",
            "user": "wish44165",
            "type": "user"
          },
          "name": "Yu-Hsi Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-25T14:35:46.455Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/67e2063e1ee7f6db889849d6/Dn_qgqu4a6nI8HIAdpDvW.mp4"
      ],
      "publishedAt": "2025-03-21T15:40:18.000Z",
      "submittedOnDailyAt": "2025-03-26T04:35:14.607Z",
      "title": "Baseline fort: YOLOv12 et BoT-SORT-ReID pour le multi-object tracking",
      "submittedOnDailyBy": {
        "_id": "67e2063e1ee7f6db889849d6",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67e2063e1ee7f6db889849d6/ihiwCCqbXlxQ2V_SSGnng.jpeg",
        "isPro": false,
        "fullname": "Yu-Hsi Chen",
        "user": "wish44165",
        "type": "user"
      },
      "summary": "Détecter et suivre une grande quantité d'avions sans pilote (UAV) dans des vidéos infrarouges de température est un problème inhérentment difficile en raison de faibles contrastes, du bruit environnemental et des petites dimensions des objets. Cet article propose une approche simple pour résoudre ce problème dans les vidéos infrarouges de température, en utilisant les avancées récentes en détection et suivi. Un cadre de suivi basé sur YOLOv12 et BoT-SORT est proposé, sans dépendre de YOLOv5 et DeepSORT, et a été amélioré grâce à des stratégies de formation et d'inférence appropriées. Il a été évalué avec des métriques pour le défi des UAV de la quatrième génération, montrant un excellent rendement. En particulier, des résultats caractéristiques ont été obtenus pour suivre plusieurs UAV sans nécessiter de renforcer le contraste ou d'intégrer de l'information de séquence, en se concentrant plutôt sur l'amélioration des caractéristiques propres des UAV. Des détails de l'implémentation, un analyse exhaustive des expériences et une discussion sur des améliorations possibles sont présentés. Le code est disponible sur https://github.com/wish44165/YOLOv12-BoT-SORT-ReID.",
      "upvotes": 1,
      "discussionId": "67e2b69108c6a250edda279f",
      "githubRepo": "https://github.com/wish44165/YOLOv12-BoT-SORT-ReID",
      "ai_keywords": [
        "YOLOv12",
        "BoT-SORT",
        "multi-UAV tracking",
        "thermal infrared video",
        "detection",
        "tracking",
        "tailored training",
        "inference strategies",
        "4th Anti-UAV Challenge",
        "contrast enhancement",
        "temporal information fusion",
        "UAV features",
        "Strong Baseline"
      ]
    },
    "publishedAt": "2025-03-21T11:40:18.000Z",
    "title": "Strong Baseline: Multi-UAV Tracking via YOLOv12 with BoT-SORT-ReID",
    "summary": "Detecting and tracking multiple unmanned aerial vehicles (UAVs) in thermal\ninfrared video is inherently challenging due to low contrast, environmental\nnoise, and small target sizes. This paper provides a straightforward approach\nto address multi-UAV tracking in thermal infrared video, leveraging recent\nadvances in detection and tracking. Instead of relying on the YOLOv5 with the\nDeepSORT pipeline, we present a tracking framework built on YOLOv12 and\nBoT-SORT, enhanced with tailored training and inference strategies. We evaluate\nour approach following the metrics from the 4th Anti-UAV Challenge and\ndemonstrate competitive performance. Notably, we achieve strong results without\nusing contrast enhancement or temporal information fusion to enrich UAV\nfeatures, highlighting our approach as a \"Strong Baseline\" for the multi-UAV\ntracking task. We provide implementation details, in-depth experimental\nanalysis, and a discussion of potential improvements. The code is available at\nhttps://github.com/wish44165/YOLOv12-BoT-SORT-ReID .",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/67e2063e1ee7f6db889849d6/Dn_qgqu4a6nI8HIAdpDvW.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17237.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "67e2063e1ee7f6db889849d6",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67e2063e1ee7f6db889849d6/ihiwCCqbXlxQ2V_SSGnng.jpeg",
      "fullname": "Yu-Hsi Chen",
      "name": "wish44165",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.16965",
      "authors": [
        {
          "_id": "67e35c3bf049c252c672b824",
          "name": "Zhe Hu",
          "hidden": false
        },
        {
          "_id": "67e35c3bf049c252c672b825",
          "name": "Jing Li",
          "hidden": false
        },
        {
          "_id": "67e35c3bf049c252c672b826",
          "name": "Yu Yin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-21T09:25:23.000Z",
      "submittedOnDailyAt": "2025-03-26T00:20:32.465Z",
      "title": "Quand une mot est plus puissante que la vision : Les VLMs peuvent effectuer un entraînement axé sur la perception humaine et améliorer automatiquement à partir du seul texte.",
      "submittedOnDailyBy": {
        "_id": "63999a6fe657365725d0d0a4",
        "avatarUrl": "/avatars/99736de1bc0d5decf4a6eda86e3c7937.svg",
        "isPro": false,
        "fullname": "Derek Zhe Hu",
        "user": "zhehuderek",
        "type": "user"
      },
      "summary": "La décision d'environnement est essentielle pour que les IAs agents fonctionnent dans des environnements réels. Les modèles de langage visuel (VLMs) ont amélioré cette capacité, mais encore dans des situations centrées sur l'humain, elles nécessitent des raisonnements profonds pour prendre des jugements complexes, ce qui les confronte à des difficultés. Dans cette étude, les VLMs ouverts ont été évalués de manière systématique dans des tâches de traitement de jugements centrés sur l'humain. Bien que les modèles de langage grands (LLMs) assumient une échelle similaire aux VLMs, ils ont montré un rendement surprenant seulement dans des contextes textuels, ce qui indique que leur capacité peut être affectée par la gestion d'images. En réponse à ces défis, un nouvel approche d'entraînement utilisant des données de contexte synthétiques est proposée. Cette méthode renforce les composants linguistiques des VLMs, permet de transferer les compétences acquises à diverses inférences et élimine la nécessité de données de paires image-contexte coûteuses. De plus, les VLMs peuvent améliorer leur rendement en utilisant des données d'entraînement générées par le contexte d'un LLM, sans dépendre d'un modèle plus grand comme le GPT-4, démontrant une amélioration significative. Ces résultats recommandent un approche plus efficace et extensible pour renforcer la capacité de jugements centrés sur l'humain des VLMs et ouvrent une nouvelle voie pour optimiser les VLMs par l'intermédiaire de structures d'amélioration automatique.",
      "upvotes": 1,
      "discussionId": "67e35c3cf049c252c672b859",
      "ai_keywords": [
        "Visual Language Models (VLMs)",
        "multimodal human-centered decision-making tasks",
        "Large Language Models (LLMs)",
        "textual descriptions",
        "visual alignment",
        "text-only training approach",
        "synthesized textual data",
        "self-improvement",
        "training data",
        "GPT-4",
        "human-centered decision-making capabilities"
      ]
    },
    "publishedAt": "2025-03-21T05:25:23.000Z",
    "title": "When Words Outperform Vision: VLMs Can Self-Improve Via Text-Only\n  Training For Human-Centered Decision Making",
    "summary": "Embodied decision-making is fundamental for AI agents operating in real-world\nenvironments. While Visual Language Models (VLMs) have advanced this\ncapability, they still struggle with complex decisions, particularly in\nhuman-centered situations that require deep reasoning about human needs and\nvalues. In this study, we systematically evaluate open-sourced VLMs on\nmultimodal human-centered decision-making tasks. We find that LLMs receiving\nonly textual descriptions unexpectedly outperform their VLM counterparts of\nsimilar scale that process actual images, suggesting that visual alignment may\nhinder VLM abilities. To address this challenge, we propose a novel text-only\ntraining approach with synthesized textual data. This method strengthens VLMs'\nlanguage components and transfers the learned abilities to multimodal\ninference, eliminating the need for expensive image-text paired data.\nFurthermore, we show that VLMs can achieve substantial performance gains\nthrough self-improvement, using training data generated by their LLM\ncounterparts rather than relying on larger teacher models like GPT-4. Our\nfindings establish a more efficient and scalable approach to enhancing VLMs'\nhuman-centered decision-making capabilities, opening new avenues for optimizing\nVLMs through self-improvement mechanisms.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16965.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63999a6fe657365725d0d0a4",
      "avatarUrl": "/avatars/99736de1bc0d5decf4a6eda86e3c7937.svg",
      "fullname": "Derek Zhe Hu",
      "name": "zhehuderek",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.11849",
      "authors": [
        {
          "_id": "67e3d0ac304f166b665e4a67",
          "name": "Yi Wang",
          "hidden": false
        },
        {
          "_id": "67e3d0ac304f166b665e4a68",
          "name": "Zhitong Xiong",
          "hidden": false
        },
        {
          "_id": "67e3d0ac304f166b665e4a69",
          "name": "Chenying Liu",
          "hidden": false
        },
        {
          "_id": "67e3d0ac304f166b665e4a6a",
          "name": "Adam J. Stewart",
          "hidden": false
        },
        {
          "_id": "67e3d0ac304f166b665e4a6b",
          "name": "Thomas Dujardin",
          "hidden": false
        },
        {
          "_id": "67e3d0ac304f166b665e4a6c",
          "name": "Nikolaos Ioannis Bountos",
          "hidden": false
        },
        {
          "_id": "67e3d0ac304f166b665e4a6d",
          "name": "Angelos Zavras",
          "hidden": false
        },
        {
          "_id": "67e3d0ac304f166b665e4a6e",
          "name": "Franziska Gerken",
          "hidden": false
        },
        {
          "_id": "67e3d0ac304f166b665e4a6f",
          "name": "Ioannis Papoutsis",
          "hidden": false
        },
        {
          "_id": "67e3d0ac304f166b665e4a70",
          "name": "Laura Leal-Taixé",
          "hidden": false
        },
        {
          "_id": "67e3d0ac304f166b665e4a71",
          "name": "Xiao Xiang Zhu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-14T20:16:48.000Z",
      "submittedOnDailyAt": "2025-03-26T08:34:37.209Z",
      "title": "À propos du modèle Copérnique sur la vision de la Terre",
      "submittedOnDailyBy": {
        "_id": "64cba974a81988d0734c9925",
        "avatarUrl": "/avatars/645c326ca38eb751144f356076cef60f.svg",
        "isPro": false,
        "fullname": "Yi Wang",
        "user": "wangyi111",
        "type": "user"
      },
      "summary": "L'évolution des modèles de base d'observation de la Terre (EO) a permis d'apprendre des représentations générales dans l'espace à partir de grandes quantités de données satellitaires, ce qui bénéficie à de nombreuses applications importantes sur la Terre. Cependant, la plupart des efforts actuels sont limités aux capteurs à spectre fixe, se concentrent sur la surface de la Terre et ignorent des métadonnées plus précieuses que les images. Dans cette étude, nous présentons un pas vers les modèles de base d'EO de la prochaine génération, introduisant trois composants clés : 1) Copernicus-Pretrain, un grand ensemble de données de prédiction basé sur des images d'un réseau de 1.870 missions de capteurs, qui étend le domaine de l'observation de la surface de la Terre jusqu'à l'air. 2) Copernicus-FM, une réseau neuronal dynamique étendu et un codificateur de métadonnées flexible, permettant un modèle de base unifié capable de traiter n'importe quel type de capteur à spectre ou non à spectre. 3) Copernicus-Bench, un cadre de référence qui configure 15 tâches d'application spécifiques pour chaque mission de capteur, de la pré-traitement à la tâche d'application, et évalue systématiquement le système. Notre ensemble de données, de modèles et de cadre de référence améliorent significativement l'échelle, la variation et la diversité des modèles de base d'EO, créant de nouvelles connexions avec la recherche en EO, météorologie et climatologie. Le code, l'ensemble de données et les modèles sont disponibles sur https://github.com/zhu-xlab/Copernicus-FM.",
      "upvotes": 0,
      "discussionId": "67e3d0af304f166b665e4b68",
      "githubRepo": "https://github.com/zhu-xlab/Copernicus-FM"
    },
    "publishedAt": "2025-03-14T16:16:48.000Z",
    "title": "Towards a Unified Copernicus Foundation Model for Earth Vision",
    "summary": "Advances in Earth observation (EO) foundation models have unlocked the\npotential of big satellite data to learn generic representations from space,\nbenefiting a wide range of downstream applications crucial to our planet.\nHowever, most existing efforts remain limited to fixed spectral sensors, focus\nsolely on the Earth's surface, and overlook valuable metadata beyond imagery.\nIn this work, we take a step towards next-generation EO foundation models with\nthree key components: 1) Copernicus-Pretrain, a massive-scale pretraining\ndataset that integrates 18.7M aligned images from all major Copernicus Sentinel\nmissions, spanning from the Earth's surface to its atmosphere; 2)\nCopernicus-FM, a unified foundation model capable of processing any spectral or\nnon-spectral sensor modality using extended dynamic hypernetworks and flexible\nmetadata encoding; and 3) Copernicus-Bench, a systematic evaluation benchmark\nwith 15 hierarchical downstream tasks ranging from preprocessing to specialized\napplications for each Sentinel mission. Our dataset, model, and benchmark\ngreatly improve the scalability, versatility, and multimodal adaptability of EO\nfoundation models, while also creating new opportunities to connect EO,\nweather, and climate research. Codes, datasets and models are available at\nhttps://github.com/zhu-xlab/Copernicus-FM.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.11849.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64cba974a81988d0734c9925",
      "avatarUrl": "/avatars/645c326ca38eb751144f356076cef60f.svg",
      "fullname": "Yi Wang",
      "name": "wangyi111",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  }
]