[
  {
    "paper": {
      "id": "2506.18095",
      "authors": [
        {
          "_id": "685a0ac20e4ad7e2197584ea",
          "name": "Junying Chen",
          "hidden": false
        },
        {
          "_id": "685a0ac20e4ad7e2197584eb",
          "name": "Zhenyang Cai",
          "hidden": false
        },
        {
          "_id": "685a0ac20e4ad7e2197584ec",
          "user": {
            "_id": "675130185d873b8ed24d964a",
            "avatarUrl": "/avatars/30ee8ce21f95423db8ced7db4df3112b.svg",
            "isPro": false,
            "fullname": "Pengcheng Chen",
            "user": "cppppppc",
            "type": "user"
          },
          "name": "Pengcheng Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-26T09:33:07.093Z",
          "hidden": false
        },
        {
          "_id": "685a0ac20e4ad7e2197584ed",
          "name": "Shunian Chen",
          "hidden": false
        },
        {
          "_id": "685a0ac20e4ad7e2197584ee",
          "name": "Ke Ji",
          "hidden": false
        },
        {
          "_id": "685a0ac20e4ad7e2197584ef",
          "name": "Xidong Wang",
          "hidden": false
        },
        {
          "_id": "685a0ac20e4ad7e2197584f0",
          "name": "Yunjin Yang",
          "hidden": false
        },
        {
          "_id": "685a0ac20e4ad7e2197584f1",
          "name": "Benyou Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-22T16:51:09.000Z",
      "submittedOnDailyAt": "2025-06-26T02:58:42.859Z",
      "title": "ShareGPT-4o-Image : Génération d'images au niveau de GPT-4o pour l'alignement de plusieurs modèles",
      "submittedOnDailyBy": {
        "_id": "64097dd1b6a334f53e2b3e4c",
        "avatarUrl": "/avatars/18d036aab5e096054a8706bc78027126.svg",
        "isPro": false,
        "fullname": "Junying Chen",
        "user": "jymcc",
        "type": "user"
      },
      "summary": "L'essor récent des modèles génératifs multimodal a permis la création d'images qui répondent à des commandes, mais les systèmes comme GPT-4o-Image ont des limitations en termes de propriétés et d'accès. Pour démocratiser ces capacités, nous présentons ShareGPT-4o-Image. Ceci est le premier ensemble de données de 45K phrases synthétiques à images et 46K phrases et images à images, en utilisant la capacité de génération d'images de GPT-4o. Ce conjoint de données a été utilisé pour développer Janus-4o, un modèle multimodal qui permet la génération d'images à partir de phrases et d'images à partir de phrases et d'images. Janus-4o améliore significativement la génération d'images à partir de phrases par rapport à Janus-Pro, et introduit également la génération d'images à partir de phrases et d'images. En particulier, il a réussi à obtenir des résultats excellents en génération d'images à partir de phrases et d'images, démontrant son efficacité dans des scénarios d'écriture et de vision. Il a été développé en utilisant 91K échantillons synthétiques et 6 heures d'entraînement sur une machine avec GPU 8A800. La publication de ShareGPT-4o-Image et Janus-4o vise à encourager la recherche ouverte en génération d'images réalistes et répondant à des commandes.",
      "upvotes": 43,
      "discussionId": "685a0ac30e4ad7e2197584f2",
      "githubRepo": "https://github.com/FreedomIntelligence/ShareGPT-4o-Image",
      "ai_summary": "ShareGPT-4o-Image and Janus-4o enable open research in photorealistic, instruction-aligned image generation through a large dataset and multimodal model.",
      "ai_keywords": [
        "multimodal generative models",
        "text-to-image",
        "text-and-image-to-image",
        "photorealistic",
        "instruction-aligned",
        "dataset",
        "large language model",
        "synthetic samples"
      ],
      "githubStars": 63
    },
    "publishedAt": "2025-06-22T12:51:09.000Z",
    "title": "ShareGPT-4o-Image: Aligning Multimodal Models with GPT-4o-Level Image\n  Generation",
    "summary": "Recent advances in multimodal generative models have unlocked photorealistic,\ninstruction-aligned image generation, yet leading systems like GPT-4o-Image\nremain proprietary and inaccessible. To democratize these capabilities, we\npresent ShareGPT-4o-Image, the first dataset comprising 45K text-to-image and\n46K text-and-image-to-image data, all synthesized using GPT-4o's image\ngeneration capabilities for distilling its advanced image generation abilities.\nLeveraging this dataset, we develop Janus-4o, a multimodal large language model\ncapable of both text-to-image and text-and-image-to-image generation. Janus-4o\nnot only significantly improves text-to-image generation over its predecessor,\nJanus-Pro, but also newly supports text-and-image-to-image generation. Notably,\nit achieves impressive performance in text-and-image-to-image generation from\nscratch, using only 91K synthetic samples and 6 hours of training on an 8\nA800-GPU machine. We hope the release of ShareGPT-4o-Image and Janus-4o will\nfoster open research in photorealistic, instruction-aligned image generation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18095.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64097dd1b6a334f53e2b3e4c",
      "avatarUrl": "/avatars/18d036aab5e096054a8706bc78027126.svg",
      "fullname": "Junying Chen",
      "name": "jymcc",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.19697",
      "authors": [
        {
          "_id": "685c1546df8a0d6c70bbf94e",
          "user": {
            "_id": "60f8435644e75317cc02ed51",
            "avatarUrl": "/avatars/68b7fc077fe2bda6607b1c470add8140.svg",
            "isPro": false,
            "fullname": "Jungwoo Park",
            "user": "affjljoo3581",
            "type": "user"
          },
          "name": "Jungwoo Park",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-26T09:32:38.469Z",
          "hidden": false
        },
        {
          "_id": "685c1546df8a0d6c70bbf94f",
          "name": "Taewhoo Lee",
          "hidden": false
        },
        {
          "_id": "685c1546df8a0d6c70bbf950",
          "name": "Chanwoong Yoon",
          "hidden": false
        },
        {
          "_id": "685c1546df8a0d6c70bbf951",
          "name": "Hyeon Hwang",
          "hidden": false
        },
        {
          "_id": "685c1546df8a0d6c70bbf952",
          "name": "Jaewoo Kang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-24T15:03:57.000Z",
      "submittedOnDailyAt": "2025-06-26T02:17:13.263Z",
      "title": "Test d'Entretien Externe pour un Modèle de Langage à Grande Échelle avec une Expression Fixe de 4 Bits pour les Décimales",
      "submittedOnDailyBy": {
        "_id": "60f8435644e75317cc02ed51",
        "avatarUrl": "/avatars/68b7fc077fe2bda6607b1c470add8140.svg",
        "isPro": false,
        "fullname": "Jungwoo Park",
        "user": "affjljoo3581",
        "type": "user"
      },
      "summary": "Les outliers d'activation extrêmes significativement dégradent le rendement de la qualification des grands modèles de langage (LLMs) et obstaculent la fonctionnalité efficace des systèmes. Ils ont été reconnus comme causes de la manipulation des canaux et de l'ajustement des gradients adaptatifs, mais trouver des solutions pratiques est difficile. Nous présentons un guide pratique pour prévenir l'activation extrême par l'entraînement préalable sécurisé des outliers (OSP). L'OSP combine trois innovations clés : 1. L'optimiseur Muon optimizer élimine les bases privilégiées tout en maintenant l'efficacité de l'entraînement. 2. Single-Scale RMSNorm évite l'accès par canaux. 3. La projection des embeddings apprenables réredistribue les magnitudes d'activation dans la matrice d'embeddings. Nous montrons l'effet de l'OSP en entraînant un modèle de 14 milliards de paramètres avec 100 milliards de tokens. Le modèle OSP a obtenu un average de 35,7 points en termes de quantification à 4 bits, dépassant le modèle Adam avec un score de 26,5. De plus, le coût d'entraînement est de seulement 2%. En particulier, le modèle OSP montre des valeurs extrêmes (1818,56) comparées à des valeurs proches de zéro (0,04). Les outliers d'activation extrême ne sont pas exclusifs des LLMs, mais résultats de stratégies d'entraînement. Nous avons ouvert une voie pour la fonctionnalité efficace des LLMs. Les sources de code et les points de contrôle de l'entraînement complets sont disponibles sur https://github.com/dmis-lab/Outlier-Safe-Pre-Training.",
      "upvotes": 29,
      "discussionId": "685c1546df8a0d6c70bbf953",
      "githubRepo": "https://github.com/dmis-lab/Outlier-Safe-Pre-Training",
      "ai_summary": "Outlier-Safe Pre-Training improves large language model quantization performance by preventing extreme activation outliers through innovative training techniques.",
      "ai_keywords": [
        "Muon optimizer",
        "Single-Scale RMSNorm",
        "learnable embedding projection",
        "outlier formation",
        "quantization performance",
        "LLM deployment",
        "excess kurtosis"
      ],
      "githubStars": 5
    },
    "publishedAt": "2025-06-24T11:03:57.000Z",
    "title": "Outlier-Safe Pre-Training for Robust 4-Bit Quantization of Large\n  Language Models",
    "summary": "Extreme activation outliers in Large Language Models (LLMs) critically\ndegrade quantization performance, hindering efficient on-device deployment.\nWhile channel-wise operations and adaptive gradient scaling are recognized\ncauses, practical mitigation remains challenging. We introduce Outlier-Safe\nPre-Training (OSP), a practical guideline that proactively prevents outlier\nformation rather than relying on post-hoc mitigation. OSP combines three key\ninnovations: (1) the Muon optimizer, eliminating privileged bases while\nmaintaining training efficiency; (2) Single-Scale RMSNorm, preventing\nchannel-wise amplification; and (3) a learnable embedding projection,\nredistributing activation magnitudes originating from embedding matrices. We\nvalidate OSP by training a 1.4B-parameter model on 1 trillion tokens, which is\nthe first production-scale LLM trained without such outliers. Under aggressive\n4-bit quantization, our OSP model achieves a 35.7 average score across 10\nbenchmarks (compared to 26.5 for an Adam-trained model), with only a 2%\ntraining overhead. Remarkably, OSP models exhibit near-zero excess kurtosis\n(0.04) compared to extreme values (1818.56) in standard models, fundamentally\naltering LLM quantization behavior. Our work demonstrates that outliers are not\ninherent to LLMs but are consequences of training strategies, paving the way\nfor more efficient LLM deployment. The source code and pretrained checkpoints\nare available at https://github.com/dmis-lab/Outlier-Safe-Pre-Training.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.19697.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f8435644e75317cc02ed51",
      "avatarUrl": "/avatars/68b7fc077fe2bda6607b1c470add8140.svg",
      "fullname": "Jungwoo Park",
      "name": "affjljoo3581",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.16012",
      "authors": [
        {
          "_id": "685cf7c0696820ba1f28f2ea",
          "name": "Boyu Li",
          "hidden": false
        },
        {
          "_id": "685cf7c0696820ba1f28f2eb",
          "name": "Siyuan He",
          "hidden": false
        },
        {
          "_id": "685cf7c0696820ba1f28f2ec",
          "name": "Hang Xu",
          "hidden": false
        },
        {
          "_id": "685cf7c0696820ba1f28f2ed",
          "name": "Haoqi Yuan",
          "hidden": false
        },
        {
          "_id": "685cf7c0696820ba1f28f2ee",
          "name": "Yu Zang",
          "hidden": false
        },
        {
          "_id": "685cf7c0696820ba1f28f2ef",
          "name": "Liwei Hu",
          "hidden": false
        },
        {
          "_id": "685cf7c0696820ba1f28f2f0",
          "name": "Junpeng Yue",
          "hidden": false
        },
        {
          "_id": "685cf7c0696820ba1f28f2f1",
          "name": "Zhenxiong Jiang",
          "hidden": false
        },
        {
          "_id": "685cf7c0696820ba1f28f2f2",
          "name": "Pengbo Hu",
          "hidden": false
        },
        {
          "_id": "685cf7c0696820ba1f28f2f3",
          "user": {
            "_id": "61e52be53d6dbb1da842316a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61e52be53d6dbb1da842316a/gx0WGPcOCClXPymoKglc4.jpeg",
            "isPro": false,
            "fullname": "Börje Karlsson",
            "user": "tellarin",
            "type": "user"
          },
          "name": "Börje F. Karlsson",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-26T09:23:24.327Z",
          "hidden": false
        },
        {
          "_id": "685cf7c0696820ba1f28f2f4",
          "user": {
            "_id": "63e5e3807f9730f523655c5d",
            "avatarUrl": "/avatars/3ded710049790d025e862861039d9df2.svg",
            "isPro": false,
            "fullname": "YehuiTang",
            "user": "WizardTY",
            "type": "user"
          },
          "name": "Yehui Tang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-26T09:23:21.125Z",
          "hidden": false
        },
        {
          "_id": "685cf7c0696820ba1f28f2f5",
          "name": "Zongqing Lu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-19T04:13:36.000Z",
      "submittedOnDailyAt": "2025-06-26T06:05:04.111Z",
      "title": "DualTHOR : Plateforme de Simulation de Déshérités de Deux Bras pour Planifications Intéressées dans le Contexte",
      "submittedOnDailyBy": {
        "_id": "61e52be53d6dbb1da842316a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61e52be53d6dbb1da842316a/gx0WGPcOCClXPymoKglc4.jpeg",
        "isPro": false,
        "fullname": "Börje Karlsson",
        "user": "tellarin",
        "type": "user"
      },
      "summary": "L'évolution de la capacité des modèles d'IA associés pour réaliser des tâches d'interaction complexes dans des scénarios de la réalité est l'un des problèmes fondamentaux de l'intelligence artificielle associée. L'amélioration récente des plateformes de simulation a considérablement augmenté la diversité des tâches lors de l'entraînement des modèles de vision et de langage (VLM), mais la plupart de ces plateformes évitent la simplification des robots et les caractéristiques probabilistes de bas niveau, limitant ainsi la transfert vers les robots de la réalité. Pour aborder ces problèmes, nous présentons DualTHOR, une plateforme de simulation basée sur des robots de bras doubles et physiques. Cette simulation introduit des contenus qui comprennent les actifs de robots de la réalité, des tâches de collaboration des bras, un solvant invernant pour robots associés et des potentialités de défaillances dues à l'exécution physique de bas niveau. Cette simulation permet d'évaluer avec plus de précision la robustesse et la capacité de généralisation des VLM. Selon les évaluations distribuées, actuellement les VLM rencontrent des difficultés dans la collaboration des bras et leur robustesse dans des environnements réalistes avec contenu limité. Grâce à notre simulation, nous mettons en avant l'importance du développement de VLM avec une forte capacité pour les tâches associées. Le code est disponible sur https://github.com/ds199895/DualTHOR.git.",
      "upvotes": 16,
      "discussionId": "685cf7c1696820ba1f28f2f6",
      "ai_summary": "A simulator named DualTHOR for training dual-arm humanoid robots integrates real-world assets and physics to enhance the robustness and generalization of Vision Language Models.",
      "ai_keywords": [
        "embodied AI",
        "Vision Language Models",
        "VLMs",
        "physics-based simulation",
        "DualTHOR",
        "AI2-THOR",
        "dual-arm robots",
        "real-world robot assets",
        "task suite",
        "inverse kinematics solvers",
        "contingency mechanism",
        "physics-based low-level execution"
      ]
    },
    "publishedAt": "2025-06-19T00:13:36.000Z",
    "title": "DualTHOR: A Dual-Arm Humanoid Simulation Platform for Contingency-Aware\n  Planning",
    "summary": "Developing embodied agents capable of performing complex interactive tasks in\nreal-world scenarios remains a fundamental challenge in embodied AI. Although\nrecent advances in simulation platforms have greatly enhanced task diversity to\ntrain embodied Vision Language Models (VLMs), most platforms rely on simplified\nrobot morphologies and bypass the stochastic nature of low-level execution,\nwhich limits their transferability to real-world robots. To address these\nissues, we present a physics-based simulation platform DualTHOR for complex\ndual-arm humanoid robots, built upon an extended version of AI2-THOR. Our\nsimulator includes real-world robot assets, a task suite for dual-arm\ncollaboration, and inverse kinematics solvers for humanoid robots. We also\nintroduce a contingency mechanism that incorporates potential failures through\nphysics-based low-level execution, bridging the gap to real-world scenarios.\nOur simulator enables a more comprehensive evaluation of the robustness and\ngeneralization of VLMs in household environments. Extensive evaluations reveal\nthat current VLMs struggle with dual-arm coordination and exhibit limited\nrobustness in realistic environments with contingencies, highlighting the\nimportance of using our simulator to develop more capable VLMs for embodied\ntasks. The code is available at https://github.com/ds199895/DualTHOR.git.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.16012.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "61e52be53d6dbb1da842316a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61e52be53d6dbb1da842316a/gx0WGPcOCClXPymoKglc4.jpeg",
      "fullname": "Börje Karlsson",
      "name": "tellarin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 26
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.18315",
      "authors": [
        {
          "_id": "685cb786696820ba1f28f286",
          "name": "Lehan He",
          "hidden": false
        },
        {
          "_id": "685cb786696820ba1f28f287",
          "user": {
            "_id": "66c44c6826efa38bc783b07a",
            "avatarUrl": "/avatars/7a09179a2c91adc97f8db851fca37eea.svg",
            "isPro": false,
            "fullname": "Zeren Chen",
            "user": "zx55",
            "type": "user"
          },
          "name": "Zeren Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-26T09:23:31.152Z",
          "hidden": false
        },
        {
          "_id": "685cb786696820ba1f28f288",
          "name": "Zhe Zhang",
          "hidden": false
        },
        {
          "_id": "685cb786696820ba1f28f289",
          "name": "Jing Shao",
          "hidden": false
        },
        {
          "_id": "685cb786696820ba1f28f28a",
          "name": "Xiang Gao",
          "hidden": false
        },
        {
          "_id": "685cb786696820ba1f28f28b",
          "user": {
            "_id": "65b722dbe02a17f0f8d1cc6b",
            "avatarUrl": "/avatars/65f20601ef9b8ebfdddadd737f9153d6.svg",
            "isPro": false,
            "fullname": "Lu Sheng",
            "user": "lsheng2024",
            "type": "user"
          },
          "name": "Lu Sheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-26T09:23:29.105Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-23T06:01:12.000Z",
      "submittedOnDailyAt": "2025-06-26T06:27:20.139Z",
      "title": "Propriétés basées sur des tests pour combiner la génération et la vérification du code de modèles de langage d'intelligence artificielle.",
      "submittedOnDailyBy": {
        "_id": "64a96a375a69e2ca889abdff",
        "avatarUrl": "/avatars/f288c66ace09d907f132a79a740a3701.svg",
        "isPro": false,
        "fullname": "fanhongxing",
        "user": "fanhongxing",
        "type": "user"
      },
      "summary": "Les modèles de langage grand (LLMs) se distinguent dans la génération de code, mais surtout dans les tâches de programmation complexes, vérifier que les résultats fonctionnellement soient corrects est un problème à long terme. Le développement dirigé par tests (TDD) offre un chemin pour l'amélioration du code, mais l'effet de cette méthode diminue en raison de la manque de tests de haute qualité et des problèmes de génération automatique de tests dans les LLMs. La biais des tests et la prédiction imprécise des sorties peuvent échouer dans le processus de modification. Dans cet article, nous présentons un nouveau cadre de travail appelé Solveur Généré à partir de Propriétés, qui utilise la Test de Caractéristiques (PBT). Ce cadre ne dépend pas de tests basés sur des exemples concrets d'entrée et de sortie, confirmant ainsi l'exactitude du code à travers des caractéristiques ou des événements du programme. Ces caractéristiques ne peuvent pas prédire un oracle parfait de tests, mais sont faciles à définir et à vérifier, et rompent le cercle de dépendance entre tests et code, connu sous le nom de \"cycle de la papillon\". Le Solveur Généré à partir de Propriétés utilise deux agents basés sur les LLMs : un générateur spécialisé dans la génération de code et l'itération d'amélioration, et un Testeur qui gère le cycle de vie de la PBT et fournit des feedback significatifs à partir des violations de caractéristiques. Ces agents réalisent des améliorations significatives sur le pas @1, atteignant un effet relatif de 23,1% à 37,3% plus que les méthodes TDD existantes. Ce cadre offre une structure robuste pour que les LLMs se dirigent vers des codes qui peuvent être généralisés correctement, en utilisant la Test de Caractéristiques comme moteur principal de vérification. Les résultats des expériences réparties sur plusieurs cadres de référence de génération de code montrent que le Solveur Généré à partir de Propriétés fournit des feedback efficaces et a un impact positif sur l'amélioration du générateur.",
      "upvotes": 8,
      "discussionId": "685cb786696820ba1f28f28c",
      "githubRepo": "https://github.com/HeLeHanPrivate/PBTwithCodeGen",
      "ai_summary": "A novel framework using Property-Based Testing and collaborative LLM-based agents improves code generation correctness and generalization.",
      "ai_keywords": [
        "Large Language Models",
        "code generation",
        "Test-Driven Development",
        "Property-Based Testing",
        "PBT",
        "iterative refinement",
        "property violations",
        "pass@1 improvements"
      ],
      "githubStars": 2
    },
    "publishedAt": "2025-06-23T02:01:12.000Z",
    "title": "Use Property-Based Testing to Bridge LLM Code Generation and Validation",
    "summary": "Large Language Models (LLMs) excel at code generation, but ensuring their\noutputs to be functionally correct, especially in complex programming tasks, is\na persistent challenge. While traditional Test-Driven Development (TDD) offers\na path for code refinement, its efficacy with LLMs is often undermined by the\nscarcity of high-quality test cases or the pitfalls of automated test\ngeneration, including biased tests or inaccurate output predictions that can\nmisdirect the correction process. This paper introduces Property-Generated\nSolver, a novel framework that leverages Property-Based Testing (PBT) to\nvalidate high-level program properties or invariants, instead of relying on\nspecific input-output examples. These properties are often simpler to define\nand verify than directly predicting exhaustive test oracles, breaking the\n\"cycle of self-deception\" where tests might share flaws with the code they are\nmeant to validate. Property-Generated Solver employs two collaborative\nLLM-based agents: a Generator dedicated to code generation and iterative\nrefinement, and a Tester that manages the PBT life-cycle and formulate\nsemantically rich feedback from property violations. The resulting\ncomprehensive and actionable feedback then guides the Generator in its\nrefinement efforts. By establishing PBT as the core validation engine within\nthis iterative, closed-loop paradigm, Property-Generated Solver provides a\nrobust mechanism for steering LLMs towards more correct and generalizable code.\nExtensive experimental results on multiple code generation benchmarks\ndemonstrate that Property-Generated Solver achieves substantial pass@1\nimprovements, ranging from 23.1% to 37.3% relative gains over established TDD\nmethods.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18315.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64a96a375a69e2ca889abdff",
      "avatarUrl": "/avatars/f288c66ace09d907f132a79a740a3701.svg",
      "fullname": "fanhongxing",
      "name": "fanhongxing",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.18088",
      "authors": [
        {
          "_id": "685ae8e8d2ee4fac76521d03",
          "user": {
            "_id": "65b37a9b06d8b55123ef8921",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65b37a9b06d8b55123ef8921/CT5tLwezjXct1eTszA8sO.jpeg",
            "isPro": false,
            "fullname": "Tianxing Chen",
            "user": "TianxingChen",
            "type": "user"
          },
          "name": "Tianxing Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-24T18:13:30.491Z",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d04",
          "name": "Zanxin Chen",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d05",
          "name": "Baijun Chen",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d06",
          "name": "Zijian Cai",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d07",
          "name": "Yibin Liu",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d08",
          "name": "Qiwei Liang",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d09",
          "name": "Zixuan Li",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d0a",
          "name": "Xianliang Lin",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d0b",
          "name": "Yiheng Ge",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d0c",
          "name": "Zhenyu Gu",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d0d",
          "name": "Weiliang Deng",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d0e",
          "name": "Yubin Guo",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d0f",
          "name": "Tian Nian",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d10",
          "name": "Xuanbing Xie",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d11",
          "name": "Qiangyu Chen",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d12",
          "name": "Kailun Su",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d13",
          "name": "Tianling Xu",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d14",
          "name": "Guodong Liu",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d15",
          "name": "Mengkang Hu",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d16",
          "name": "Huan-ang Gao",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d17",
          "name": "Kaixuan Wang",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d18",
          "name": "Zhixuan Liang",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d19",
          "name": "Yusen Qin",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d1a",
          "name": "Xiaokang Yang",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d1b",
          "name": "Ping Luo",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d1c",
          "name": "Yao Mu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-22T16:26:53.000Z",
      "submittedOnDailyAt": "2025-06-26T06:13:35.593Z",
      "title": "RoboTwin 2.0 : Conception de Manipulation Robuste de Robots Bimanuels avec Générateur de Données Scalable et Fortification de la Randomisation de Domaine, avec Référentiels de Référence",
      "submittedOnDailyBy": {
        "_id": "65b37a9b06d8b55123ef8921",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65b37a9b06d8b55123ef8921/CT5tLwezjXct1eTszA8sO.jpeg",
        "isPro": false,
        "fullname": "Tianxing Chen",
        "user": "TianxingChen",
        "type": "user"
      },
      "summary": "La synthèse de données basée sur la simulation a émergé comme un paradigme puissant pour renforcer la manipulation de robots en réalité. Cependant, les ensembles de données actuels de synthèse ne sont pas suffisants pour gérer robustement les mains gauche et droite, présentant deux défis : (1) la manque de méthodes efficaces pour la génération de données échelonnables pour de nouvelles tâches et (2) l'incapacité à détecter la complexité de la réalité complexe dans des environnements de simulation simplifiés. Nous présentons RoboTwin 2.0, un cadre de travail de simulation échelonnable. Ce cadre de travail permet la génération automatique et à grande échelle de données diverses et réalistes, ainsi qu'un protocole d'évaluation unifié pour les manipulations de mains gauche et droite. Tout d'abord, nous avons construit une bibliothèque d'objets de 147 catégories et 731 instances, chacune avec des étiquettes significatives et liées au comportement. En nous appuyant sur cela, nous avons développé un pipeline de synthèse de données spécialisé qui combine des modèles multimodales de langage et la précision de la simulation en ligne. Pour améliorer la transition de la simulation à la réalité, RoboTwin 2.0 applique une randomisation de domaine structurée sur cinq axes : la tête, l'illumination, le fond, l'altitude de la table et les instructions linguistiques, ce qui augmente la diversité des données et la robustesse des politiques. Ce cadre de travail est capable de mettre en œuvre des tâches de mains gauche et droite dans un intervalle de 50 tâches, dépassant les images corporelles de 5 robots, et a été collecté précédemment un ensemble de 100 000 trajectoires de domaine randomisé. Les résultats des expérimentations montrent que le taux de succès dans la génération de code a augmenté de 10,9%, et la généralisation à de nouveaux scénarios de la réalité a été améliorée. L'ajustement des modèles VLA du jeu de données a entraîné un amélioration relative de 367% sur de nouvelles tâches de la réalité (de 9,0% à 42,0%), et en entraînant uniquement sur des données non supervisées, un bénéfice relatif de 228% (de 9,0% à 42,0%), démontrant une forte généralisation. Les générateurs de données, le benchmark, l'ensemble de données et le code seront libérés, soutenant la recherche échelonnable de manipulations de mains gauche et droite.",
      "upvotes": 7,
      "discussionId": "685ae8e8d2ee4fac76521d1d",
      "projectPage": "https://robotwin-platform.github.io/",
      "githubRepo": "https://github.com/robotwin-Platform/RoboTwin",
      "ai_summary": "RoboTwin 2.0 is a scalable simulation framework for bimanual robotic manipulation that uses expert data synthesis and structured domain randomization to generate diverse and realistic synthetic data, improving sim-to-real transfer and generalization.",
      "ai_keywords": [
        "multimodal large language models",
        "simulation-in-the-loop",
        "structured domain randomization",
        "bimanual manipulation",
        "task-level execution code",
        "sim-to-real transfer",
        "zero-shot learning"
      ],
      "githubStars": 1129
    },
    "publishedAt": "2025-06-22T12:26:53.000Z",
    "title": "RoboTwin 2.0: A Scalable Data Generator and Benchmark with Strong Domain\n  Randomization for Robust Bimanual Robotic Manipulation",
    "summary": "Simulation-based data synthesis has emerged as a powerful paradigm for\nenhancing real-world robotic manipulation. However, existing synthetic datasets\nremain insufficient for robust bimanual manipulation due to two challenges: (1)\nthe lack of an efficient, scalable data generation method for novel tasks, and\n(2) oversimplified simulation environments that fail to capture real-world\ncomplexity. We present RoboTwin 2.0, a scalable simulation framework that\nenables automated, large-scale generation of diverse and realistic data, along\nwith unified evaluation protocols for dual-arm manipulation. We first construct\nRoboTwin-OD, a large-scale object library comprising 731 instances across 147\ncategories, each annotated with semantic and manipulation-relevant labels.\nBuilding on this foundation, we develop an expert data synthesis pipeline that\ncombines multimodal large language models (MLLMs) with simulation-in-the-loop\nrefinement to generate task-level execution code automatically. To improve\nsim-to-real transfer, RoboTwin 2.0 incorporates structured domain randomization\nalong five axes: clutter, lighting, background, tabletop height and language\ninstructions, thereby enhancing data diversity and policy robustness. We\ninstantiate this framework across 50 dual-arm tasks spanning five robot\nembodiments, and pre-collect over 100,000 domain-randomized expert\ntrajectories. Empirical results show a 10.9% gain in code generation success\nand improved generalization to novel real-world scenarios. A VLA model\nfine-tuned on our dataset achieves a 367% relative improvement (42.0% vs. 9.0%)\non unseen scene real-world tasks, while zero-shot models trained solely on our\nsynthetic data achieve a 228% relative gain, highlighting strong generalization\nwithout real-world supervision. We release the data generator, benchmark,\ndataset, and code to support scalable research in robust bimanual manipulation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18088.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65b37a9b06d8b55123ef8921",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65b37a9b06d8b55123ef8921/CT5tLwezjXct1eTszA8sO.jpeg",
      "fullname": "Tianxing Chen",
      "name": "TianxingChen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.18674",
      "authors": [
        {
          "_id": "685cd8fc696820ba1f28f2aa",
          "name": "Raquel Ferrando",
          "hidden": false
        },
        {
          "_id": "685cd8fc696820ba1f28f2ab",
          "name": "Javier Conde",
          "hidden": false
        },
        {
          "_id": "685cd8fc696820ba1f28f2ac",
          "name": "Gonzalo Martínez",
          "hidden": false
        },
        {
          "_id": "685cd8fc696820ba1f28f2ad",
          "name": "Pedro Reviriego",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-23T14:18:46.000Z",
      "submittedOnDailyAt": "2025-06-26T03:52:31.234Z",
      "title": "Le modèle de texte de robots est utilisé pour le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de texte de robots, le modèle de",
      "submittedOnDailyBy": {
        "_id": "64f31365ed48e3bb9c487d5d",
        "avatarUrl": "/avatars/979c1979eadbd4529c95b925bbb58d78.svg",
        "isPro": false,
        "fullname": "Gonzalo",
        "user": "gonzmart",
        "type": "user"
      },
      "summary": "Le coût de calcul et d'énergie des LLM augmente exponentiellement lorsque le taille du modèle et l'introduction de millions d'utilisateurs augmentent. Le coût unitaire des LLM est le calcul de tokens. Par conséquent, la tokenisation joue un rôle important dans l'efficacité du modèle et est ajustée pour minimiser le nombre de tokens du texte du corpus d'entraînement. L'une des applications les plus populaires des LLM est le chatbot, qui interagit avec les utilisateurs. La clé de la performance est que dans un chatbot, le rendement de la tokenisation des textes d'entrée des utilisateurs et des réponses du chatbot est crucial. Cela diffère de la tokenisation du texte du corpus d'entraînement. Par conséquent, l'intérêt immédiat est dans les bénéfices potentiels d'optimiser la tokenisation pour le chatbot. Dans cet article, nous étudions la possibilité d'optimiser la tokenisation pour le chatbot en utilisant un corpus de dialogues de chatbots publics disponibles, en rédesignant la vectorisation et en évaluant son rendement dans ce domaine. Les résultats montrent que la tokenisation optimisée pour le dialogue réduit de manière constante le nombre de tokens dans les dialogues du chatbot, ce qui peut atteindre des réductions significatives d'énergie de 5% à 10%, et a un impact positif significatif ou même plus grand que la tokenisation originale du corpus d'entraînement.",
      "upvotes": 5,
      "discussionId": "685cd8fc696820ba1f28f2ae",
      "ai_summary": "Optimizing tokenizers for chatbot conversations reduces computational costs and energy usage with minimal impact on training corpus performance.",
      "ai_keywords": [
        "Large Language Models",
        "token",
        "tokenizer",
        "chatbots",
        "conversation-optimized tokenizers"
      ]
    },
    "publishedAt": "2025-06-23T10:18:46.000Z",
    "title": "Is There a Case for Conversation Optimized Tokenizers in Large Language\n  Models?",
    "summary": "The computational and energy costs of Large Language Models (LLMs) have\nincreased exponentially driven by the growing model sizes and the massive\nadoption of LLMs by hundreds of millions of users. The unit cost of an LLM is\nthe computation of a token. Therefore, the tokenizer plays an important role in\nthe efficiency of a model, and they are carefully optimized to minimize the\nnumber of tokens for the text in their training corpus. One of the most popular\napplications of LLMs are chatbots that interact with users. A key observation\nis that, for those chatbots, what is important is the performance of the\ntokenizer in the user text input and the chatbot responses. Those are most\nlikely different from the text in the training corpus. So, a question that\nimmediately arises is whether there is a potential benefit in optimizing\ntokenizers for chatbot conversations. In this paper, this idea is explored for\ndifferent tokenizers by using a publicly available corpus of chatbot\nconversations to redesign their vocabularies and evaluate their performance in\nthis domain. The results show that conversation-optimized tokenizers\nconsistently reduce the number of tokens in chatbot dialogues, which can lead\nto meaningful energy savings, in the range of 5% to 10% while having minimal or\neven slightly positive impact on tokenization efficiency for the original\ntraining corpus.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18674.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64f31365ed48e3bb9c487d5d",
      "avatarUrl": "/avatars/979c1979eadbd4529c95b925bbb58d78.svg",
      "fullname": "Gonzalo",
      "name": "gonzmart",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.20544",
      "authors": [
        {
          "_id": "685cdd71696820ba1f28f2b8",
          "user": {
            "_id": "677cfa6cac2db4c2265edb26",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/Kbi96ndfY-CIuJNd2TRZt.jpeg",
            "isPro": false,
            "fullname": "Ammar Khairi",
            "user": "ammar-cohere",
            "type": "user"
          },
          "name": "Ammar Khairi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-26T09:23:26.999Z",
          "hidden": false
        },
        {
          "_id": "685cdd71696820ba1f28f2b9",
          "name": "Daniel D'souza",
          "hidden": false
        },
        {
          "_id": "685cdd71696820ba1f28f2ba",
          "name": "Ye Shen",
          "hidden": false
        },
        {
          "_id": "685cdd71696820ba1f28f2bb",
          "name": "Julia Kreutzer",
          "hidden": false
        },
        {
          "_id": "685cdd71696820ba1f28f2bc",
          "name": "Sara Hooker",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-25T15:37:53.000Z",
      "submittedOnDailyAt": "2025-06-26T04:17:49.221Z",
      "title": "Si tu avais une vie comme un exemple : les avantages pour améliorer l'inférence d'un Multilingual LLM en étendant le calcul.",
      "submittedOnDailyBy": {
        "_id": "6544e43b12da508864c38f96",
        "avatarUrl": "/avatars/76f0cd55b4bf9c03d2686e146c6f795f.svg",
        "isPro": false,
        "fullname": "Julia Kreutzer",
        "user": "JuliaKreutzerCohere",
        "type": "user"
      },
      "summary": "Le développement récent des grands modèles de langue (LLMs) se concentre sur l'augmentation de la quantité de calculs dans l'inférence et l'amélioration du rendement en évitant la réentraînement. Les méthodes courantes comprennent l'échantillonnage en parallèle de plusieurs résultats pour sélectionner le final. Cependant, les recherches actuelles se concentrent sur des domaines comme l'anglais, les mathématiques ou le code. En contraste, nous nous intéressons aux tâches ouvertes, qui peuvent être vérifiées de manière formelle et aux technologies qui peuvent être généralisées entre langues. Dans cette recherche, nous étudions les tâches génératives ouvertes qui considérablement augmentent la quantité de calculs dans l'inférence dans plusieurs langues et configurations de tâches.\n\nNotre résultat est que la stratégie d'échantillonnage basée sur la variance et la stratégie de sélection doivent répondre à des domaines et configurations de langue différents. Nous évaluons les méthodes de sélection existantes et démontrons que les stratégies efficaces en anglais ne peuvent pas être généralisées à d'autres langues. Nous proposons de nouvelles stratégies d'échantillonnage et de sélection adaptées aux scénarios d'inférence dans plusieurs langues et tâches, qui présentent des effets clairs sur les deux. En particulier, notre combinaison de stratégies d'échantillonnage et de sélection a augmenté en moyenne de +6.8 la probabilité de victoire de notre modèle de 8B sur le m-ArenaHard-v2.0, et a également démontré une efficacité face aux modèles propriétaires comme ゲミニ. À grande échelle, en appliquant notre méthode au modèle Command-A (111B), nous avons obtenu une amélioration de la probabilité de victoire de +9.0 sur le même benchmark, avec un grand gain en validation d'un seul échantillon en utilisant seulement 5 échantillons et avec un gain efficace à faible coût. Nos résultats soulignent la nécessité d'intérêts pour la quantité de calculs dans l'inférence liées aux langues et aux tâches, et nous orientons vers la démocratisation comme objectif pour améliorer le rendement dans les langues représentatives.",
      "upvotes": 4,
      "discussionId": "685cdd71696820ba1f28f2bd",
      "ai_summary": "The study examines and proposes new sampling and selection strategies to enhance inference-time compute for multilingual and multi-task large language models, demonstrating significant improvements in win-rates across various languages and tasks.",
      "ai_keywords": [
        "sampling strategy",
        "selection strategy",
        "temperature variation",
        "open-ended generative tasks",
        "multilingual",
        "multi-task",
        "m-ArenaHard-v2.0",
        "win-rates",
        "Command-A",
        "single-sample decoding",
        "language-aware",
        "task-aware"
      ]
    },
    "publishedAt": "2025-06-25T11:37:53.000Z",
    "title": "When Life Gives You Samples: The Benefits of Scaling up Inference\n  Compute for Multilingual LLMs",
    "summary": "Recent advancements in large language models (LLMs) have shifted focus toward\nscaling inference-time compute, improving performance without retraining the\nmodel. A common approach is to sample multiple outputs in parallel, and select\none of these as the final output. However, work to date has focused on English\nand a handful of domains such as math and code. In contrast, we are most\ninterested in techniques that generalize across open-ended tasks, formally\nverifiable tasks, and across languages. In this work, we study how to robustly\nscale inference-time compute for open-ended generative tasks in a multilingual,\nmulti-task setting.\n  Our findings show that both sampling strategy based on temperature variation\nand selection strategy must be adapted to account for diverse domains and\nvaried language settings. We evaluate existing selection methods, revealing\nthat strategies effective in English often fail to generalize across languages.\nWe propose novel sampling and selection strategies specifically adapted for\nmultilingual and multi-task inference scenarios, and show they yield notable\ngains across languages and tasks. In particular, our combined sampling and\nselection methods lead to an average +6.8 jump in win-rates for our 8B models\non m-ArenaHard-v2.0 prompts, against proprietary models such as Gemini. At\nlarger scale, Command-A (111B model) equipped with our methods, shows +9.0\nimprovement in win-rates on the same benchmark with just five samples against\nsingle-sample decoding, a substantial increase at minimal cost. Our results\nunderscore the need for language- and task-aware approaches to inference-time\ncompute, aiming to democratize performance improvements in underrepresented\nlanguages.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.20544.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6544e43b12da508864c38f96",
      "avatarUrl": "/avatars/76f0cd55b4bf9c03d2686e146c6f795f.svg",
      "fullname": "Julia Kreutzer",
      "name": "JuliaKreutzerCohere",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 15
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.20495",
      "authors": [
        {
          "_id": "685d0223696820ba1f28f322",
          "name": "Haoze Wu",
          "hidden": false
        },
        {
          "_id": "685d0223696820ba1f28f323",
          "name": "Yunzhi Yao",
          "hidden": false
        },
        {
          "_id": "685d0223696820ba1f28f324",
          "name": "Wenhao Yu",
          "hidden": false
        },
        {
          "_id": "685d0223696820ba1f28f325",
          "name": "Huajun Chen",
          "hidden": false
        },
        {
          "_id": "685d0223696820ba1f28f326",
          "name": "Ningyu Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-25T14:41:13.000Z",
      "submittedOnDailyAt": "2025-06-26T06:48:06.720Z",
      "title": "ReCode : Apprentissage par Renforcement pour la Mise à Jour des Connaissances d'API de Code",
      "submittedOnDailyBy": {
        "_id": "620b3bbb0668e435407c8d0a",
        "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
        "isPro": false,
        "fullname": "Ningyu Zhang",
        "user": "Ningyu",
        "type": "user"
      },
      "summary": "Les modèles de langue grands (LLMs) montrent une capacité à générer du code, mais échouent à s'adapter aux fréquentes mises à jour des API externes. Cette limitation importante est basée sur des connaissances anciennes des API dans les données d'entraînement et sur la manque de documentation actuelle, ce qui empêche la génération de code fiable dans des environnements dynamiques. Pour résoudre ce problème, nous proposons un nouveau cadre de travail basé sur l'apprentissage par renforcement (ReCode), qui utilise l'apprentissage par renforcement basé sur des règles pour s'adapter aux changements dans les API. En particulier, nous entraînons les LLMs avec des informations d'actualisation basées sur la version des codes en utilisant un ensemble de données qui comprend environ 2 000 entrées de données. De plus, nous utilisons une amélioration de la métrique de similitude de chaînes pour évaluer le code comme récompense dans l'apprentissage par renforcement. Les résultats des tests montrent que ReCode améliore significativement la capacité de génération de code des LLMs dans des environnements dynamiques d'API, avec un effet clair sur la tâche CodeUpdateArena. De plus, nous observons que, comparativement à l'ajustement de l'observation contrôlée, ReCode n'affecte pas la capacité générale de génération de code des LLMs. ReCode a été appliqué à divers LLMs et algorithmes d'apprentissage par renforcement (GRPO et DAPO), obtenant des améliorations constantes dans tous les cas. En particulier, après l'entraînement, Qwen2.5-Coder-7B a dépassé les modèles d'ajustement de code et les modèles de logique de la même architecture. Le code est disponible sur https://github.com/zjunlp/ReCode.",
      "upvotes": 4,
      "discussionId": "685d0224696820ba1f28f327",
      "githubRepo": "https://github.com/zjunlp/ReCode",
      "ai_summary": "ReCode, a rule-based reinforcement learning framework, enhances large language models' adaptation to API updates without compromising their general code generation capabilities.",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "code generation",
        "API updates",
        "dataset",
        "version migration",
        "string similarity metric",
        "reinforcement learning",
        "rule-based",
        "Qwen2.5-Coder-7B",
        "CodeUpdateArena",
        "GRPO",
        "DAPO"
      ],
      "githubStars": 4
    },
    "publishedAt": "2025-06-25T10:41:13.000Z",
    "title": "ReCode: Updating Code API Knowledge with Reinforcement Learning",
    "summary": "Large Language Models (LLMs) exhibit remarkable code generation capabilities\nbut falter when adapting to frequent updates in external library APIs. This\ncritical limitation, stemming from reliance on outdated API knowledge from\ntheir training data, even with access to current documentation, impedes\nreliable code generation in dynamic environments. To tackle this issue, we\npropose ReCode (rule-based Reinforcement learning for Code Update), a novel\nframework that mimics human programmer adaptation to API changes. Specifically,\nwe construct a dataset of approximately 2,000 data entries to train the LLMs to\nperform version migration based on updated information. Then, we introduce a\nmodified string similarity metric for code evaluation as the reward for\nreinforcement learning. Our experiments demonstrate that ReCode substantially\nboosts LLMs' code generation performance in dynamic API scenarios, especially\non the unseen CodeUpdateArena task. Crucially, compared to supervised\nfine-tuning, ReCode has less impact on LLMs' general code generation abilities.\nWe apply ReCode on various LLMs and reinforcement learning algorithms (GRPO and\nDAPO), all achieving consistent improvements. Notably, after training,\nQwen2.5-Coder-7B outperforms that of the 32B parameter code instruction-tuned\nmodel and the reasoning model with the same architecture. Code is available at\nhttps://github.com/zjunlp/ReCode.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.20495.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "620b3bbb0668e435407c8d0a",
      "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
      "fullname": "Ningyu Zhang",
      "name": "Ningyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 25
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.20452",
      "authors": [
        {
          "_id": "685cfd21696820ba1f28f30a",
          "name": "Tobias Vontobel",
          "hidden": false
        },
        {
          "_id": "685cfd21696820ba1f28f30b",
          "user": {
            "_id": "63b4b02a103617b0a5b0ee2e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b4b02a103617b0a5b0ee2e/LasBC-pCnPJ6XuCt6qqcp.jpeg",
            "isPro": false,
            "fullname": "Seyedmorteza Sadat",
            "user": "msadat97",
            "type": "user"
          },
          "name": "Seyedmorteza Sadat",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-26T09:23:18.462Z",
          "hidden": false
        },
        {
          "_id": "685cfd21696820ba1f28f30c",
          "name": "Farnood Salehi",
          "hidden": false
        },
        {
          "_id": "685cfd21696820ba1f28f30d",
          "user": {
            "_id": "630f7646197cd3f24e7f8e9f",
            "avatarUrl": "/avatars/59bbd4ed38277b313051aac78f6808ac.svg",
            "isPro": false,
            "fullname": "Romann Weber",
            "user": "RMW",
            "type": "user"
          },
          "name": "Romann M. Weber",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-26T09:23:16.404Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-25T13:58:37.000Z",
      "submittedOnDailyAt": "2025-06-26T06:29:51.626Z",
      "title": "Hiwab: Génération d'images à haute résolution sans entraînement par échantillonnage basé sur la dispersion d'échantillons Wavelet",
      "submittedOnDailyBy": {
        "_id": "63b4b02a103617b0a5b0ee2e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b4b02a103617b0a5b0ee2e/LasBC-pCnPJ6XuCt6qqcp.jpeg",
        "isPro": false,
        "fullname": "Seyedmorteza Sadat",
        "user": "msadat97",
        "type": "user"
      },
      "summary": "Le module de diffusion a apparu comme une approche avancée dans la synthèse d'images, démontrant une excellente réalisme et diversité. Cependant, l'entraînement de modules de diffusion à haute résolution est complexe en termes de calcul, et les techniques actuelles de génération de tests tendent à inclure des répétitions d'objets et des artefacts spatiaux discontinus dans la synthèse d'images de résolution d'entraînement plus élevée. Dans cet article, nous présentons un méthode pour améliorer significativement la fiabilité visuelle et la cohérence structurale de la synthèse d'images à haute résolution sans entraînement, en utilisant un module de diffusion préalablement entraîné avec HiWave. Notre méthode utilise une pipeline de deux étapes : une image de base est générée dans le module d'entraînement précédent, et le pas inverse DDIM est connecté à un nouveau module d'amplification détaillé basé sur les ondelettes. En particulier, nous utilisons le méthode inverse pour obtenir un vecteur de bruit initial qui maintient la cohérence globale dans l'image de base. Ensuite, lors de l'échantillonnage, notre module d'amplification détaillé dans le domaine des ondelettes maintient les composantes basses de l'image de base, garantissant la cohérence structurale et, optionnellement, guide les composantes hautes pour améliorer les détails et les textures. L'évaluation avec l'expansion de Stable Diffusion XL montre que HiWave réduit effectivement les artefacts généraux des méthodes existantes et atteint une qualité visuelle supérieure. Dans la phase utilisateur, nous confirmons le rendement de HiWave en le comparant à l'état de l'art, et nous le valorisons avec un rendement supérieur de au moins 80%, soulignant l'efficacité de la synthèse d'images à haute résolution de haute qualité.",
      "upvotes": 4,
      "discussionId": "685cfd22696820ba1f28f30e",
      "ai_summary": "HiWave enhances ultra-high-resolution image synthesis using pretrained diffusion models through a two-stage pipeline involving DDIM inversion and wavelet-based detail enhancement, improving visual fidelity and reducing artifacts.",
      "ai_keywords": [
        "diffusion models",
        "image synthesis",
        "photorealism",
        "high resolutions",
        "zero-shot generation",
        "artifacts",
        "object duplication",
        "spatial incoherence",
        "pretrained diffusion models",
        "two-stage pipeline",
        "DDIM inversion",
        "wavelet-based detail enhancer",
        "structural consistency",
        "fine details",
        "textures",
        "perceptual quality",
        "user study"
      ]
    },
    "publishedAt": "2025-06-25T09:58:37.000Z",
    "title": "HiWave: Training-Free High-Resolution Image Generation via Wavelet-Based\n  Diffusion Sampling",
    "summary": "Diffusion models have emerged as the leading approach for image synthesis,\ndemonstrating exceptional photorealism and diversity. However, training\ndiffusion models at high resolutions remains computationally prohibitive, and\nexisting zero-shot generation techniques for synthesizing images beyond\ntraining resolutions often produce artifacts, including object duplication and\nspatial incoherence. In this paper, we introduce HiWave, a training-free,\nzero-shot approach that substantially enhances visual fidelity and structural\ncoherence in ultra-high-resolution image synthesis using pretrained diffusion\nmodels. Our method employs a two-stage pipeline: generating a base image from\nthe pretrained model followed by a patch-wise DDIM inversion step and a novel\nwavelet-based detail enhancer module. Specifically, we first utilize inversion\nmethods to derive initial noise vectors that preserve global coherence from the\nbase image. Subsequently, during sampling, our wavelet-domain detail enhancer\nretains low-frequency components from the base image to ensure structural\nconsistency, while selectively guiding high-frequency components to enrich fine\ndetails and textures. Extensive evaluations using Stable Diffusion XL\ndemonstrate that HiWave effectively mitigates common visual artifacts seen in\nprior methods, achieving superior perceptual quality. A user study confirmed\nHiWave's performance, where it was preferred over the state-of-the-art\nalternative in more than 80% of comparisons, highlighting its effectiveness for\nhigh-quality, ultra-high-resolution image synthesis without requiring\nretraining or architectural modifications.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.20452.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63b4b02a103617b0a5b0ee2e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b4b02a103617b0a5b0ee2e/LasBC-pCnPJ6XuCt6qqcp.jpeg",
      "fullname": "Seyedmorteza Sadat",
      "name": "msadat97",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.18403",
      "authors": [
        {
          "_id": "685a555f0e4ad7e2197586b1",
          "user": {
            "_id": "65eef9ce7443c09267513796",
            "avatarUrl": "/avatars/62547f99130557f54093b2ff4d6c9c24.svg",
            "isPro": false,
            "fullname": "Muntasir Adnan",
            "user": "adnaan525",
            "type": "user"
          },
          "name": "Muntasir Adnan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-26T09:33:02.852Z",
          "hidden": false
        },
        {
          "_id": "685a555f0e4ad7e2197586b2",
          "name": "Carlos C. N. Kuhn",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-23T08:40:45.000Z",
      "submittedOnDailyAt": "2025-06-26T01:42:45.705Z",
      "title": "Index de désagrégation de purification : révision de la stratégie de purification du code d'un LLM",
      "submittedOnDailyBy": {
        "_id": "65eef9ce7443c09267513796",
        "avatarUrl": "/avatars/62547f99130557f54093b2ff4d6c9c24.svg",
        "isPro": false,
        "fullname": "Muntasir Adnan",
        "user": "adnaan525",
        "type": "user"
      },
      "summary": "L'effet de la débogage d'IA est reconnu comme un modèle de refroidissement d'une fonction exponentielle prévisible, et la majorité des modèles perdent 60-80% de leur capacité débogage dans 2-3 essais, reconnaissant l'importance de répéter les essais pour un système efficace de génération de code. Nous quantifions les cas où la débogage n'est pas efficace et présentons un cadre mathématique appelé Indice de Débogage de Chute (DDI) pour prédire les points d'accent qui peuvent améliorer l'efficacité de la débogage. Notre nouveau départ stratégique explore les aspects stratégiques de l'introduction de la débogage et montre comment un approche appropriée à ce moment peut restaurer l'efficacité de la débogage. L'DDI identifie clairement les limitations fondamentales de la débogage d'IA et fournit pour la première fois un cadre quantitatif pour optimiser la stratégie de génération de code qui répète des essais multiples.",
      "upvotes": 2,
      "discussionId": "685a555f0e4ad7e2197586b3",
      "ai_summary": "The Debugging Decay Index (DDI) quantifies and optimizes the effectiveness of iterative AI debugging by predicting intervention points to revive and enhance debugging capability.",
      "ai_keywords": [
        "AI debugging",
        "Debugging Decay Index (DDI)",
        "iterative debugging",
        "code generation",
        "effectiveness",
        "intervention points"
      ]
    },
    "publishedAt": "2025-06-23T04:40:45.000Z",
    "title": "The Debugging Decay Index: Rethinking Debugging Strategies for Code LLMs",
    "summary": "The effectiveness of AI debugging follows a predictable exponential decay\npattern; most models lose 60-80% of their debugging capability within just 2-3\nattempts, despite iterative debugging being a critical capability for practical\ncode generation systems. We introduce the Debugging Decay Index (DDI), a\nmathematical framework that quantifies when debugging becomes ineffective and\npredicts intervention points. Our strategic fresh start approach shifts from\nexploitation to exploration at strategic points in the debugging process,\ndemonstrating that well-timed interventions can rescue the effectiveness of\ndebugging. DDI reveals a fundamental limitation in current AI debugging and\nprovides the first quantitative framework for optimising iterative code\ngeneration strategies.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18403.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65eef9ce7443c09267513796",
      "avatarUrl": "/avatars/62547f99130557f54093b2ff4d6c9c24.svg",
      "fullname": "Muntasir Adnan",
      "name": "adnaan525",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.20512",
      "authors": [
        {
          "_id": "685cb8d7696820ba1f28f296",
          "name": "Zengzhi Wang",
          "hidden": false
        },
        {
          "_id": "685cb8d7696820ba1f28f297",
          "name": "Fan Zhou",
          "hidden": false
        },
        {
          "_id": "685cb8d7696820ba1f28f298",
          "name": "Xuefeng Li",
          "hidden": false
        },
        {
          "_id": "685cb8d7696820ba1f28f299",
          "name": "Pengfei Liu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/62cbeb2d72dfd24b86bdf977/cZoks9vqpXBnkpA9PgVRV.png"
      ],
      "publishedAt": "2025-06-25T14:58:13.000Z",
      "submittedOnDailyAt": "2025-06-26T07:21:37.577Z",
      "title": "Octotihneer : Promeut l'échellabilité de l'apprentissage par renforcement dans l'apprentissage intermédiaire.",
      "submittedOnDailyBy": {
        "_id": "62cbeb2d72dfd24b86bdf977",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62cbeb2d72dfd24b86bdf977/UcGYYSBNrCvPM5K9v-sro.png",
        "isPro": false,
        "fullname": "Zengzhi Wang",
        "user": "SinclairWang",
        "type": "user"
      },
      "summary": "Différentes familles de modèles de langage de base, comme par exemple Llama et Qwen, montrent différentes actions lors du pas de modification après l'apprentissage par renforcement (RL). Pourquoi sont-ils les modèles de langage de base appropriés pour l'apprentissage par renforcement ? La compréhension profonde de ces problèmes est cruciale pour le développement des prochains générateurs RL échelonnables de modèles de base. Dans cette étude, on examine comment la stratégie d'apprentissage intermédiaire affecte RL de manière dynamique, en se concentrant sur deux familles de modèles représentatifs, Qwen et Llama. L'étude présente les résultats suivants : (1) les corpus de haute qualité tels que MegaMath-Web-Pro améliorent significativement le rendement des modèles de base et RL, démontrant que les corpus actuels (par exemple, FineMath-4plus) ne réussissent pas à atteindre ces améliorations ; (2) l'ajout de données de type QA, en particulier des données qui incluent des exemples d'inférence longue de chaîne de pensée (CoT), améliorent les résultats du RL et montrent que les données directrices ont ces effets ; (3) la CoT augmente la profondeur de l'inférence mais également entraîne une augmentation de la longueur de la réponse du modèle et de l'instabilité de l'apprentissage RL, soulignant l'importance du format des données ; (4) l'échelle de la stratégie d'apprentissage intermédiaire montre un rendement fort en RL. En se basant sur ces observations, on introduit la stratégie d'apprentissage intermédiaire en deux étapes, Stable-then-Decay, où les modèles de base sont entraînés à un rythme d'apprentissage fixe de 200B tokens, puis entraînés dans 3 branches de focalisation sur CoT avec 20B tokens, réduisant le rythme d'apprentissage. De cette manière, la famille d'OctoThinker montre une forte compatibilité avec le RL et réduit la différence de rendement avec les familles de modèles plus adaptés pour le RL. Cette étude espère former les stratégies d'entraînement préalable des modèles de base dans l'ère du RL. Pour encourager davantage de recherche, cette étude publie des modèles de code ouverts et un corpus de plus de 700B tokens avec une forte capacité théorique en mathématiques (MegaMath-Web-Pro-Max).",
      "upvotes": 1,
      "discussionId": "685cb8d7696820ba1f28f29a",
      "githubRepo": "https://github.com/GAIR-NLP/OctoThinker",
      "ai_summary": "Investigating mid-training strategies reveals that high-quality mathematical corpora and well-formatted chain-of-thought reasoning examples enhance reinforcement learning performance in language models, leading to the development of OctoThinker.",
      "ai_keywords": [
        "reinforcement learning",
        "base language model",
        "mid-training strategy",
        "MegaMath-Web-Pro",
        "QA-style data",
        "chain-of-thought (CoT) reasoning",
        "data formatting",
        "learning rate decay",
        "OctoThinker",
        "MegaMath-Web-Pro-Max"
      ],
      "githubStars": 66
    },
    "publishedAt": "2025-06-25T10:58:13.000Z",
    "title": "OctoThinker: Mid-training Incentivizes Reinforcement Learning Scaling",
    "summary": "Different base language model families, such as Llama and Qwen, exhibit\ndivergent behaviors during post-training with reinforcement learning (RL),\nespecially on reasoning-intensive tasks. What makes a base language model\nsuitable for reinforcement learning? Gaining deeper insight into this question\nis essential for developing RL-scalable foundation models of the next\ngeneration. In this work, we investigate how mid-training strategies shape RL\ndynamics, focusing on two representative model families: Qwen and Llama. Our\nstudy reveals that (1) high-quality mathematical corpora, such as\nMegaMath-Web-Pro, significantly improve both base model and RL performance,\nwhile existing alternatives (e.g., FineMath-4plus) fail to do so; (2) further\nadding QA-style data, particularly long chain-of-thought (CoT) reasoning\nexamples, enhances RL outcomes, and instruction data further unlocks this\neffect; (3) while long-CoT improves reasoning depth, it can also induce\nverbosity of model responses and unstability of RL training, underscoring the\nimportance of data formatting; (4) scaling mid-training consistently leads to\nstronger downstream RL performance. Building on these insights, we introduce a\ntwo-stage mid-training strategy, Stable-then-Decay, in which base models are\nfirst trained on 200B tokens with a constant learning rate, followed by 20B\ntokens across three CoT-focused branches with learning rate decay. This yields\nOctoThinker, a family of models demonstrating strong RL compatibility and\nclosing the performance gap with more RL-friendly model families, i.e., Qwen.\nWe hope our work will help shape pre-training strategies for foundation models\nin the RL era. To support further research, we release our open-source models\nalong with a curated math reasoning-intensive corpus of over 70 billion tokens\n(i.e., MegaMath-Web-Pro-Max).",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62cbeb2d72dfd24b86bdf977/cZoks9vqpXBnkpA9PgVRV.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.20512.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62cbeb2d72dfd24b86bdf977",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62cbeb2d72dfd24b86bdf977/UcGYYSBNrCvPM5K9v-sro.png",
      "fullname": "Zengzhi Wang",
      "name": "SinclairWang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 19
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.19502",
      "authors": [
        {
          "_id": "685c02aadf8a0d6c70bbf918",
          "user": {
            "_id": "674ed490fc7f50ef61c3a7bd",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/674ed490fc7f50ef61c3a7bd/j260ICQmNl42aXehBEd6P.jpeg",
            "isPro": false,
            "fullname": "Aleksandr Algazinov",
            "user": "AleksandrAlgazinov",
            "type": "user"
          },
          "name": "Aleksandr Algazinov",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-25T20:59:09.301Z",
          "hidden": false
        },
        {
          "_id": "685c02aadf8a0d6c70bbf919",
          "name": "Matt Laing",
          "hidden": false
        },
        {
          "_id": "685c02aadf8a0d6c70bbf91a",
          "name": "Paul Laban",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-24T10:40:23.000Z",
      "submittedOnDailyAt": "2025-06-26T01:26:07.991Z",
      "title": "MATE : Ingénierie de robots mobiles Terminal de programmation Entrepreneuriat Accès à l'aide d'autres Applications Premium",
      "submittedOnDailyBy": {
        "_id": "674ed490fc7f50ef61c3a7bd",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/674ed490fc7f50ef61c3a7bd/j260ICQmNl42aXehBEd6P.jpeg",
        "isPro": false,
        "fullname": "Aleksandr Algazinov",
        "user": "AleksandrAlgazinov",
        "type": "user"
      },
      "summary": "L'accessibilité est un problème important dans la société moderne, car de nombreuses technologies ne satisfont pas les besoins complets des utilisateurs. Les systèmes multi-agent (SMA) existants, en raison de leur conception fermée, ne proposent pas suffisamment d'options de personnalisation, ce qui fait que de nombreux utilisateurs ne reçoivent pas l'aide nécessaire. Cela constitue un grand défi pour les personnes handicapées, surtout lorsqu'elles interagissent avec des environnements digitaux. Nous présentons un approche multi-modèle pour un SMA qui permet la conversion de modèles basée sur les besoins de l'utilisateur. Ce système transforme les données en formats compréhensibles et offre un soutien aux personnes handicapées. Par exemple, si un utilisateur a une perte de vision, le système convertit les images en descriptions vocales. Le système Multi-Agent MATE peut être appliqué dans une large gamme d'domaines, comme l'attention médicale, et peut être une outil utile pour de nombreux groupes d'utilisateurs. Le système supporte une variété de modèles, allant des appels à l'API de grands modèles de langage (LLM) jusqu'à l'utilisation de ML personnalisé par chatbots. Cette flexibilité permet que le système soit adaptable à différentes besoins et maintienne une compatibilité avec divers dispositifs. On suppose que le système fonctionne localement, garantissant la confidentialité et la sécurité de l'information importante. De plus, le cadre de travail peut être intégré efficacement avec la technologie des institutions (par exemple, les services digitaux d'attention médicale) et offrir une assistance en temps réel. Nous présentons également ModCon-Task-Identifier, un modèle qui extrait des tâches de conversion précises à partir des entrées de l'utilisateur. Plusieurs expériences ont montré que ModCon-Task-Identifier montre un rendement constant sur nos données d'utilisateurs, améliorant les autres modèles LLM ou modèles statistiques. Notre code et nos données sont disponibles sur https://github.com/AlgazinovAleksandr/Multi-Agent-MATE.",
      "upvotes": 1,
      "discussionId": "685c02abdf8a0d6c70bbf91b",
      "ai_summary": "MATE, a multimodal accessibility multi-agent system, converts data into understandable formats based on user needs, supporting various disabilities and integrating with institutional technologies.",
      "ai_keywords": [
        "MULTIAGENT SYSTEMS",
        "MAS",
        "MODALITY CONVERSIONS",
        "LLM API",
        "CUSTOM MACHINE LEARNING CLASSIFIERS",
        "MODCON-TASK-IDENTIFIER",
        "LLM",
        "STATISTICAL MODELS"
      ]
    },
    "publishedAt": "2025-06-24T06:40:23.000Z",
    "title": "MATE: LLM-Powered Multi-Agent Translation Environment for Accessibility\n  Applications",
    "summary": "Accessibility remains a critical concern in today's society, as many\ntechnologies are not developed to support the full range of user needs.\nExisting multi-agent systems (MAS) often cannot provide comprehensive\nassistance for users in need due to the lack of customization stemming from\nclosed-source designs. Consequently, individuals with disabilities frequently\nencounter significant barriers when attempting to interact with digital\nenvironments. We introduce MATE, a multimodal accessibility MAS, which performs\nthe modality conversions based on the user's needs. The system is useful for\nassisting people with disabilities by ensuring that data will be converted to\nan understandable format. For instance, if the user cannot see well and\nreceives an image, the system converts this image to its audio description.\nMATE can be applied to a wide range of domains, industries, and areas, such as\nhealthcare, and can become a useful assistant for various groups of users. The\nsystem supports multiple types of models, ranging from LLM API calling to using\ncustom machine learning (ML) classifiers. This flexibility ensures that the\nsystem can be adapted to various needs and is compatible with a wide variety of\nhardware. Since the system is expected to run locally, it ensures the privacy\nand security of sensitive information. In addition, the framework can be\neffectively integrated with institutional technologies (e.g., digital\nhealthcare service) for real-time user assistance. Furthermore, we introduce\nModCon-Task-Identifier, a model that is capable of extracting the precise\nmodality conversion task from the user input. Numerous experiments show that\nModCon-Task-Identifier consistently outperforms other LLMs and statistical\nmodels on our custom data. Our code and data are publicly available at\nhttps://github.com/AlgazinovAleksandr/Multi-Agent-MATE.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.19502.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "674ed490fc7f50ef61c3a7bd",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/674ed490fc7f50ef61c3a7bd/j260ICQmNl42aXehBEd6P.jpeg",
      "fullname": "Aleksandr Algazinov",
      "name": "AleksandrAlgazinov",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.20331",
      "authors": [
        {
          "_id": "685d0b5d696820ba1f28f349",
          "user": {
            "_id": "62a9b0acf6708cb85014f9dc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62a9b0acf6708cb85014f9dc/Sem1qcBt1lJjFEPK-xz4_.jpeg",
            "isPro": false,
            "fullname": "Rian Touchent",
            "user": "rntc",
            "type": "user"
          },
          "name": "Rian Touchent",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-26T09:23:08.325Z",
          "hidden": false
        },
        {
          "_id": "685d0b5d696820ba1f28f34a",
          "name": "Nathan Godey",
          "hidden": false
        },
        {
          "_id": "685d0b5d696820ba1f28f34b",
          "name": "Eric de la Clergerie",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-25T11:30:25.000Z",
      "submittedOnDailyAt": "2025-06-26T07:44:08.382Z",
      "title": "Biomedi Enriched : Extension de la Base de Données Biomedi pour la Pré-entraînement et l'Extraction de Contenu Rare ou Cache à l'Utilisation de LLMs",
      "submittedOnDailyBy": {
        "_id": "62a9b0acf6708cb85014f9dc",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62a9b0acf6708cb85014f9dc/Sem1qcBt1lJjFEPK-xz4_.jpeg",
        "isPro": false,
        "fullname": "Rian Touchent",
        "user": "rntc",
        "type": "user"
      },
      "summary": "Biomedic Enity, un ensemble de données biomédicales construit à partir de PubMed, est présenté. Ce ensemble de données a été construit au cours d'un processus d'annotation en deux étapes. Dans la première étape, un modèle de langage grand a annoté 400K pages d'articles scientifiques de PubMed, attribuant à chaque page un score de qualité éducative (évalué de 1 à 5), ainsi que des catégories telles que revue, recherche, cas clinique et d'autres, et des domaines comme clinique, biomédical et d'autres. Ces scores de qualité éducative sont utilisés pour calibrer un modèle de langage de moindre échelle, ce qui permet de propager des étiquettes à tout le corpus PMC-OA. Ainsi, il est possible d'extraire un sous-ensemble de cas cliniques et de construire un sous-ensemble de plus de 450K articles de haute qualité avec des autorisations de usage commercial. De plus, grâce au filtrage de qualité et au échantillonnage par domaine, plusieurs versions de l'ensemble de données sont construites.",
      "upvotes": 0,
      "discussionId": "685d0b5d696820ba1f28f34c",
      "ai_summary": "A biomedical text dataset, constructed from PubMed, uses a two-stage annotation process involving large and small language models to fine-tune and extract subsets for clinical NLP, improving pretraining efficiency and performance.",
      "ai_keywords": [
        "Biomed-Enriched",
        "PubMed",
        "large language model",
        "small language model",
        "fine-tuning",
        "PMC-OA corpus",
        "educational quality",
        "clinical cases",
        "biomedical NLP",
        "continual-pretraining",
        "OLMo2",
        "MMLU ProfMed",
        "MedQA",
        "MedMCQA",
        "training tokens"
      ]
    },
    "publishedAt": "2025-06-25T07:30:25.000Z",
    "title": "Biomed-Enriched: A Biomedical Dataset Enriched with LLMs for Pretraining\n  and Extracting Rare and Hidden Content",
    "summary": "We introduce Biomed-Enriched, a biomedical text dataset constructed from\nPubMed via a two-stage annotation process. In the first stage, a large language\nmodel annotates 400K paragraphs from PubMed scientific articles, assigning\nscores for their type (review, study, clinical case, other), domain (clinical,\nbiomedical, other), and educational quality. The educational quality score\n(rated 1 to 5) estimates how useful a paragraph is for college-level learning.\nThese annotations are then used to fine-tune a small language model, which\npropagates the labels across the full PMC-OA corpus. The resulting metadata\nallows us to extract refined subsets, including 2M clinical case paragraphs\nwith over 450K high-quality ones from articles with commercial-use licenses,\nand to construct several variants via quality filtering and domain upsampling.\nClinical text is typically difficult to access due to privacy constraints, as\nhospital records cannot be publicly shared. Hence, our dataset provides an\nalternative large-scale, openly available collection of clinical cases from\nPubMed, making it a valuable resource for biomedical and clinical NLP.\nPreliminary continual-pretraining experiments with OLMo2 suggest these curated\nsubsets enable targeted improvements, with clinical upsampling boosting\nperformance by ~5% on MMLU ProfMed and educational quality filtering improving\nMedQA and MedMCQA by ~1%. Combinations of these techniques led to faster\nconvergence, reaching same performance with a third of training tokens,\nindicating potential for more efficient and effective biomedical pretraining\nstrategies.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.20331.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62a9b0acf6708cb85014f9dc",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62a9b0acf6708cb85014f9dc/Sem1qcBt1lJjFEPK-xz4_.jpeg",
      "fullname": "Rian Touchent",
      "name": "rntc",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 15
    },
    "isAuthorParticipating": true
  }
]