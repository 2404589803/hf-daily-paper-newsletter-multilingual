[
  {
    "paper": {
      "id": "2502.06394",
      "authors": [
        {
          "_id": "67aafead3711ca5b760f324c",
          "user": {
            "_id": "61ade264f602880813dbe10b",
            "avatarUrl": "/avatars/a92dea7d853bbabbf60b351c207b6875.svg",
            "isPro": false,
            "fullname": "Daniil Moskovskiy",
            "user": "etomoscow",
            "type": "user"
          },
          "name": "Daniil Moskovskiy",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T07:54:17.448Z",
          "hidden": false
        },
        {
          "_id": "67aafead3711ca5b760f324d",
          "user": {
            "_id": "634c72e6fe1bfa967d6c2b5c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/634c72e6fe1bfa967d6c2b5c/WFWIAlWl-FsiJRyGxQTTx.jpeg",
            "isPro": false,
            "fullname": "Nikita Sushko",
            "user": "chameleon-lizard",
            "type": "user"
          },
          "name": "Nikita Sushko",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T07:54:21.453Z",
          "hidden": false
        },
        {
          "_id": "67aafead3711ca5b760f324e",
          "user": {
            "_id": "5dfa8e07da6d0311fd3d5430",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1651090418656-5dfa8e07da6d0311fd3d5430.png",
            "isPro": false,
            "fullname": "Sergey Pletenev",
            "user": "memyprokotow",
            "type": "user"
          },
          "name": "Sergey Pletenev",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T09:59:47.063Z",
          "hidden": false
        },
        {
          "_id": "67aafead3711ca5b760f324f",
          "user": {
            "_id": "662f8d645c4db70c77a203b0",
            "avatarUrl": "/avatars/72f9a3c39b3ba5114388d16a35524835.svg",
            "isPro": false,
            "fullname": "Elena Tutubalina",
            "user": "tlenusik",
            "type": "user"
          },
          "name": "Elena Tutubalina",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T09:59:50.003Z",
          "hidden": false
        },
        {
          "_id": "67aafead3711ca5b760f3250",
          "name": "Alexander Panchenko",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-10T12:30:25.000Z",
      "title": "SynthDetoxM : Les modèles de langage modernes sont des annotateurs à un seul pas de conception parallèle de données de rétroaction détoxifiée.",
      "summary": "Actuellement, le traitement de la détection de toxicité dans des textes multilingues est limité par la rareté des ensembles de données parallèles multilingues. Dans cette étude, nous proposons une solution par l'implémentation d'un pipeline pour la génération de données de détection de toxicité multilingue. De plus, nous utilisons l'ensemble de données de textes multilingues de détection de toxicité, SynthDetoxM, qui a été collecté à la main et généré de manière synthétique, pour créer 16 000 pairs de requêtes de détection de toxicité de haute qualité en anglais, français, espagnol et russe. Ce jeu de données a été construit avec différents ensembles de données d'évaluation de toxicité et a été réécrit avec des paramètres d'entraînement dans de nombreux modèles de langage libre de code modernes (LLM). Les résultats des expérimentations montrent que les modèles entraînés avec ce jeu de données synthétique dépassent les modèles entraînés avec le jeu de données MultiParaDetox annotés par humains, même dans des situations avec des limitations de données. Les modèles entraînés sur SynthDetoxM dépassent tous les LLM évalués. Nous publions notre jeu de données et notre code pour contribuer davantage au domaine de la recherche en détection de toxicité dans des textes multilingues.",
      "upvotes": 55,
      "discussionId": "67aafeae3711ca5b760f3280"
    },
    "publishedAt": "2025-02-11T03:03:12.135Z",
    "title": "SynthDetoxM: Modern LLMs are Few-Shot Parallel Detoxification Data Annotators",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06394.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "61ade264f602880813dbe10b",
      "avatarUrl": "/avatars/a92dea7d853bbabbf60b351c207b6875.svg",
      "fullname": "Daniil Moskovskiy",
      "name": "etomoscow",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.06781",
      "authors": [
        {
          "_id": "67aacd7e078cdf445284f9f6",
          "name": "Chengqi Lyu",
          "hidden": false
        },
        {
          "_id": "67aacd7e078cdf445284f9f7",
          "name": "Songyang Gao",
          "hidden": false
        },
        {
          "_id": "67aacd7e078cdf445284f9f8",
          "name": "Yuzhe Gu",
          "hidden": false
        },
        {
          "_id": "67aacd7e078cdf445284f9f9",
          "user": {
            "_id": "64e8505321540e1da3226b54",
            "avatarUrl": "/avatars/18958b8406d1ce492b54c1c839f18c54.svg",
            "isPro": false,
            "fullname": "Wenwei Zhang",
            "user": "ZwwWayne",
            "type": "user"
          },
          "name": "Wenwei Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T07:54:40.279Z",
          "hidden": false
        },
        {
          "_id": "67aacd7e078cdf445284f9fa",
          "name": "Jianfei Gao",
          "hidden": false
        },
        {
          "_id": "67aacd7e078cdf445284f9fb",
          "name": "Kuikun Liu",
          "hidden": false
        },
        {
          "_id": "67aacd7e078cdf445284f9fc",
          "name": "Ziyi Wang",
          "hidden": false
        },
        {
          "_id": "67aacd7e078cdf445284f9fd",
          "name": "Shuaibin Li",
          "hidden": false
        },
        {
          "_id": "67aacd7e078cdf445284f9fe",
          "name": "Qian Zhao",
          "hidden": false
        },
        {
          "_id": "67aacd7e078cdf445284f9ff",
          "name": "Haian Huang",
          "hidden": false
        },
        {
          "_id": "67aacd7e078cdf445284fa00",
          "name": "Weihan Cao",
          "hidden": false
        },
        {
          "_id": "67aacd7e078cdf445284fa01",
          "name": "Jiangning Liu",
          "hidden": false
        },
        {
          "_id": "67aacd7e078cdf445284fa02",
          "name": "Hongwei Liu",
          "hidden": false
        },
        {
          "_id": "67aacd7e078cdf445284fa03",
          "name": "Junnan Liu",
          "hidden": false
        },
        {
          "_id": "67aacd7e078cdf445284fa04",
          "user": {
            "_id": "630716d11801ecc7d2595021",
            "avatarUrl": "/avatars/2d36a880ce4a3cf7efc5ff3987dbeaf3.svg",
            "isPro": false,
            "fullname": "Songyang Zhang",
            "user": "zsytony",
            "type": "user"
          },
          "name": "Songyang Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T07:54:37.733Z",
          "hidden": false
        },
        {
          "_id": "67aacd7e078cdf445284fa05",
          "name": "Dahua Lin",
          "hidden": false
        },
        {
          "_id": "67aacd7e078cdf445284fa06",
          "name": "Kai Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-10T18:57:29.000Z",
      "title": "Le limite de la mathématique de l'apprentissage des réussites et des récompenses",
      "summary": "Un des éléments clés de l'intelligence artificielle, la capacité à faire des inférences, et plus particulièrement la capacité à résoudre des problèmes mathématiques complexes, a montré un développement récent qui a attiré l'attention, comme dans les modèles de séries d'OpenAI. Cependant, la technologie complète n'est pas publiée, et on suppose que la technologie utilisée est l'apprentissage par renforcement (RL) et le pensée à long terme. Dans cet article, nous proposons un nouveau cadre d'RL appelé OREAL pour atteindre le limite de la performance possible dans des tâches d'inférence mathématique basées sur des récompenses basées sur les résultats. Ce cadre d'RL fournit une démonstration théorique que l'apprentissage de stratégies optimales KL-normalisées peut être suffisant lorsque des récompenses binaires de résultats peuvent être obtenues. Cette formule signifie également qu'il faut garantir la consistance du gradient entre les positifs et les négatifs. Pour atténuer le problème des récompenses rares qui a existé dans l'RL pendant de longtemps, nous appliquons un modèle de récompenses basé sur des tokens supplémentaires, qui montre des tokens importants pour l'apprentissage dans les chemins d'inférence, en considérant la précision partielle de la pensée à long terme dans des tâches d'inférence mathématique. Grâce à OREAL, un modèle de 7B a réalisé pour la première fois une précision de 94,0 pass@1 sur MATH-500, ce qui peut être comparé à un modèle de 32B. L'OREAL-32B a atteint une précision de 95,0 pass@1 sur MATH-500, dépassant un modèle de 32B entraîné précédemment par distillation. Notre étude met en avant l'importance des modèles de stratégies initiales et des consultations d'entraînement d'RL. Le code, les modèles et les données seront publiés pour encourager des recherches futures. https://github.com/InternLM/OREAL.",
      "upvotes": 30,
      "discussionId": "67aacd7f078cdf445284fa4b"
    },
    "publishedAt": "2025-02-10T23:18:11.727Z",
    "title": "Exploring the Limit of Outcome Reward for Learning Mathematical Reasoning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06781.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6601196cc91ba4c08ad6e270",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6601196cc91ba4c08ad6e270/X2YPNzUOQXBz5Gv-xR9LW.jpeg",
      "fullname": "yuzhe gu",
      "name": "vanilla1116",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.06703",
      "authors": [
        {
          "_id": "67aabf93c0f8648f68c68ce4",
          "user": {
            "_id": "667187ba9ab144eb3ac43a1b",
            "avatarUrl": "/avatars/db5558aa1c5160b9aee8b58573271959.svg",
            "isPro": false,
            "fullname": "Runze Liu",
            "user": "RyanLiu112",
            "type": "user"
          },
          "name": "Runze Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T07:55:22.940Z",
          "hidden": false
        },
        {
          "_id": "67aabf93c0f8648f68c68ce5",
          "name": "Junqi Gao",
          "hidden": false
        },
        {
          "_id": "67aabf93c0f8648f68c68ce6",
          "name": "Jian Zhao",
          "hidden": false
        },
        {
          "_id": "67aabf93c0f8648f68c68ce7",
          "user": {
            "_id": "60bc94cd85a3ab33829b6211",
            "avatarUrl": "/avatars/b57d36c7577fbbb42ea5b963eef4144a.svg",
            "isPro": false,
            "fullname": "Kaiyan Zhang",
            "user": "iseesaw",
            "type": "user"
          },
          "name": "Kaiyan Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T07:55:18.725Z",
          "hidden": false
        },
        {
          "_id": "67aabf93c0f8648f68c68ce8",
          "name": "Xiu Li",
          "hidden": false
        },
        {
          "_id": "67aabf93c0f8648f68c68ce9",
          "name": "Biqing Qi",
          "hidden": false
        },
        {
          "_id": "67aabf93c0f8648f68c68cea",
          "name": "Wanli Ouyang",
          "hidden": false
        },
        {
          "_id": "67aabf93c0f8648f68c68ceb",
          "name": "Bowen Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-10T17:30:23.000Z",
      "title": "Le 1B LLM peut-il surpasser le 405B LLM ? Considérez l'optimisation du temps de test pour augmenter la capacité.",
      "summary": "TTS (Test Time Scaling) est un méthode importante pour améliorer le rendement des grands modèles de langage (LLMs) lors de la phase d'inférence, en utilisant des calculs supplémentaires. Cependant, actuellement, les études ne réalisent pas un analyse systématique sur la façon dont TTS affecte les modèles de politique, Process Reward Models (PRMs) et les problèmes. Cette analyse insuffisante limite la compréhension et l'application pratique de TTS. Dans cet article, nous nous concentrons sur deux questions clés : (1) Quel est le meilleur approche pour échelonner les calculs de TTS, en considérant le niveau de difficulté des modèles de politique, PRMs et des problèmes ? (2) Comment affecte-t-il les calculs supplémentaires l'amélioration du rendement des LLMs dans des tâches complexes, et est-il possible que un petit modèle de langage surpasse un grand modèle avec cette approximation ? Nous avons effectué des expériences détaillées sur MATH-500 et des tâches défiantes comme AIME24, et nous avons obtenu les résultats suivants : (1) La stratégie de TTS optimisée par calcul dépend fortement du modèle de politique, PRM et du niveau de difficulté du problème. (2) En utilisant la stratégie de TTS optimisée par calcul, un petit modèle de politique peut surpasser un modèle grand. Par exemple, un 1B LLM surpasse un 405B LLM sur MATH-500. De plus, sur MATH-500 et AIME24, un 0.5B LLM surpasse GPT-4o, un 3B LLM surpasse un 405B LLM, et un 7B LLM surpasse o1 et DeepSeek-R1, montrant une haute efficacité d'inférence. Ces résultats montrent que il est crucial que la stratégie de TTS se adapte aux caractéristiques spécifiques de chaque tâche et de modèle, et que TTS est un approche potentiellement prometteuse pour améliorer le rendement des LLMs.",
      "upvotes": 25,
      "discussionId": "67aabf94c0f8648f68c68d19"
    },
    "publishedAt": "2025-02-11T00:36:11.270Z",
    "title": "Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06703.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6017
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.05609",
      "authors": [
        {
          "_id": "67aacaaaa03eecbc2d72835f",
          "user": {
            "_id": "64ec4c04c782d648d28d70fc",
            "avatarUrl": "/avatars/6975526fcf4b513cc934b5bc45370a48.svg",
            "isPro": false,
            "fullname": "Sukmin Cho",
            "user": "zomss",
            "type": "user"
          },
          "name": "Sukmin Cho",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T07:54:43.377Z",
          "hidden": false
        },
        {
          "_id": "67aacaaaa03eecbc2d728360",
          "name": "Sangjin Choi",
          "hidden": false
        },
        {
          "_id": "67aacaaaa03eecbc2d728361",
          "user": {
            "_id": "64d1e70a84f205869017703b",
            "avatarUrl": "/avatars/215d0d4db5f79cb74df4d888b18c6a0d.svg",
            "isPro": false,
            "fullname": "Taeho Hwang",
            "user": "doubleyyh",
            "type": "user"
          },
          "name": "Taeho Hwang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T07:54:45.737Z",
          "hidden": false
        },
        {
          "_id": "67aacaaaa03eecbc2d728362",
          "name": "Jeongyeon Seo",
          "hidden": false
        },
        {
          "_id": "67aacaaaa03eecbc2d728363",
          "name": "Soyeong Jeong",
          "hidden": false
        },
        {
          "_id": "67aacaaaa03eecbc2d728364",
          "name": "Huije Lee",
          "hidden": false
        },
        {
          "_id": "67aacaaaa03eecbc2d728365",
          "name": "Hoyun Song",
          "hidden": false
        },
        {
          "_id": "67aacaaaa03eecbc2d728366",
          "name": "Jong C. Park",
          "hidden": false
        },
        {
          "_id": "67aacaaaa03eecbc2d728367",
          "name": "Youngjin Kwon",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-08T15:32:53.000Z",
      "title": "Une écriture hiérarchique basée sur la proximité temporelle pour accélérer le modèle de langage à grande échelle sans distorsion.",
      "summary": "L'amélioration de la vitesse d'inférence dans les modèles de langage grands (LLMs) est cruciale dans les interactions en temps réel. Une des solutions algorithmiques qui a reçu beaucoup d'attention pour améliorer la vitesse d'inférence est le méthode de \"preview\" de tokens, qui permet de générer plusieurs tokens en une seule caché vers l'avenir après avoir vérifié leur précision précédemment. Cependant, les stratégies actuelles de \"preview\" nécessitent souvent un ajustement micro important ou présentent des performances déséquilibrées pour chaque tâche. Pour résoudre ces problèmes, nous proposons un approche de \"preview\" sans perte basée sur la localité temporelle, appelée Hierarchy Drafting (HD), qui utilise une structure multi-couche pour configurer différentes sources de tokens. Dans le pas de \"preview\", HD accède séquentiellement à plusieurs bases de données, de la plus grande localité à la moins grande, garantissant une amélioration constante dans différentes tâches et minimisant le délai du \"preview\". Les expériences sur Spec-Bench montrent que, en utilisant des modèles de LLMs avec 7B et 13B paramètres, HD réalise une amélioration significative de la vitesse d'inférence, dépassant les méthodes de \"preview\" de base de données existantes et montrant un fort accroissement de la vitesse d'inférence indépendamment du taille du modèle, de la tâche et de la température.",
      "upvotes": 12,
      "discussionId": "67aacaaca03eecbc2d728394"
    },
    "publishedAt": "2025-02-10T22:58:41.471Z",
    "title": "Lossless Acceleration of Large Language Models with Hierarchical Drafting based on Temporal Locality in Speculative Decoding",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05609.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ec4c04c782d648d28d70fc",
      "avatarUrl": "/avatars/6975526fcf4b513cc934b5bc45370a48.svg",
      "fullname": "Sukmin Cho",
      "name": "zomss",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.05415",
      "authors": [
        {
          "_id": "67aaea0a0acaa007694aed73",
          "user": {
            "_id": "65708920806dee337da0eef5",
            "avatarUrl": "/avatars/945e328dedc8e1e3111f48c344ad5b03.svg",
            "isPro": false,
            "fullname": "xuchenkai",
            "user": "UnhurriedDawn",
            "type": "user"
          },
          "name": "Chenkai Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T07:54:28.861Z",
          "hidden": false
        },
        {
          "_id": "67aaea0a0acaa007694aed74",
          "user": {
            "_id": "6644548a3a16452261cdb173",
            "avatarUrl": "/avatars/4643db904204e3a60202a29e8c884139.svg",
            "isPro": false,
            "fullname": "wangxu",
            "user": "asunalove",
            "type": "user"
          },
          "name": "Xu Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T07:54:26.432Z",
          "hidden": false
        },
        {
          "_id": "67aaea0a0acaa007694aed75",
          "name": "Zhenyi Liao",
          "hidden": false
        },
        {
          "_id": "67aaea0a0acaa007694aed76",
          "name": "Yishun Li",
          "hidden": false
        },
        {
          "_id": "67aaea0a0acaa007694aed77",
          "name": "Tianqi Hou",
          "hidden": false
        },
        {
          "_id": "67aaea0a0acaa007694aed78",
          "user": {
            "_id": "64bba541da140e461924dfed",
            "avatarUrl": "/avatars/367993765b0ca3734b2b100db33ed787.svg",
            "isPro": false,
            "fullname": "zhijie deng",
            "user": "zhijie3",
            "type": "user"
          },
          "name": "Zhijie Deng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T07:54:24.089Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-08T02:52:25.000Z",
      "title": "Show Ottoverbal : Accélérer la compréhension et la génération de monomodaux intégrés",
      "summary": "L'intérêt pour la recherche dans certains domaines est en augmentation, et notamment, Show-o reçoit beaucoup d'attention, notant son excellent rendement dans la génération d'images à partir de texte et inversement. L'apprentissage de Show-o se heurte à des problèmes dans l'évolution des tokens d'image et la récupération automatique des tokens de texte, ces deux aspects confrontant des défis. Dans cet article, nous présentons Show-o Turbo pour améliorer la connexion entre ces groupes. Tout d'abord, nous identifions une perspective unifiée de désencouragement basée sur l'explication parallèle des tokens de texte dans la génération d'images et de texte à partir de Show-o. Ensuite, nous étendons le design de désencouragement multimodal de Show-o avec un design de style cohérent (CD) pour fournir une approximation appropriée pour réduire le processus de désencouragement du modèle de diffusion. Nous introduisons une stratégie de division du projet et un processus de tri pour améliorer la convergence de l'entraînement. Expérimentalement, dans la génération d'images à partir de texte, l'utilisation de CFG sans utilisation se réalise un score de GenEval de 0,625 en 4 étapes, dépassant Show-o original. Dans la génération de texte à partir d'images, nous montrons une amélioration de 1,5 fois en vitesse sans perdre significativement le rendement. Le code est disponible sur https://github.com/zhijie-group/Show-o-Turbo.",
      "upvotes": 9,
      "discussionId": "67aaea100acaa007694aeea5"
    },
    "publishedAt": "2025-02-11T02:09:27.778Z",
    "title": "Show-o Turbo: Towards Accelerated Unified Multimodal Understanding and Generation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05415.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64bba541da140e461924dfed",
      "avatarUrl": "/avatars/367993765b0ca3734b2b100db33ed787.svg",
      "fullname": "zhijie deng",
      "name": "zhijie3",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.06772",
      "authors": [
        {
          "_id": "67aac8adfe33f6d8d695bc40",
          "name": "Ling Yang",
          "hidden": false
        },
        {
          "_id": "67aac8adfe33f6d8d695bc41",
          "name": "Zhaochen Yu",
          "hidden": false
        },
        {
          "_id": "67aac8adfe33f6d8d695bc42",
          "name": "Bin Cui",
          "hidden": false
        },
        {
          "_id": "67aac8adfe33f6d8d695bc43",
          "name": "Mengdi Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-10T18:51:47.000Z",
      "title": "ReasonFlux : Échelonnage de modèles pour la logique de LLM hiérarchique",
      "summary": "Nous proposons la théorie logique d'un modèle de langage génératif (LLM) heuristique en utilisant un simple modèle de pensée échelonnable, démontrant que nous pouvons dépasser la capacité logique mathématique d'un modèle de langage génératif fort (comme OpenAI ou 1-preview, DeepSeek V3). Nous entraînons le modèle ReasonFlux-32B sur 8 GAFs et introduisons trois innovations : (i) une bibliothèque de modèles de pensée structurés de genres communs, comprenant environ 500 modèles de pensée de haut niveau et permettant de généraliser des problèmes logiques similaires ou liés ; (ii) nous effectuons un apprentissage récurrent heuristique avec une séquence de modèles de pensée, optimisant la planification du trajet optimal pour résoudre des problèmes complexes de manière itérative ; (iii) nous introduisons un nouveau système d'échelle de l'inférence, permettant d'adapter les modèles de pensée pour faciliter la théorie logique d'un modèle de langage génératif heuristique. En ayant un trajet de modèles de pensée qui inclut un ordre logique, ReasonFlux-32B atteint un niveau de logique mathématique de première classe. En particulier, il atteint une précision de 91,2% sur le benchmark MATH, dépassant o1-preview de 6,7%. Sur le benchmark USA Math Olympiad (AIME), ReasonFlux-32B résout en moyenne 56,7% des problèmes, dépassant o1-preview de 27% et DeepSeek-V3 de 45%. Code : https://github.com/Gen-Verse/ReasonFlux",
      "upvotes": 9,
      "discussionId": "67aac8affe33f6d8d695bcbd"
    },
    "publishedAt": "2025-02-10T22:49:56.390Z",
    "title": "ReasonFlux: Hierarchical LLM Reasoning via Scaling Thought Templates",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06772.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64fde4e252e82dd432b74ce9",
      "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
      "fullname": "Ling Yang",
      "name": "Lingaaaaaaa",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.06049",
      "authors": [
        {
          "_id": "67aac01bd7b18841e7c266df",
          "name": "Jikun Kang",
          "hidden": false
        },
        {
          "_id": "67aac01bd7b18841e7c266e0",
          "name": "Wenqi Wu",
          "hidden": false
        },
        {
          "_id": "67aac01bd7b18841e7c266e1",
          "name": "Filippos Christianos",
          "hidden": false
        },
        {
          "_id": "67aac01bd7b18841e7c266e2",
          "name": "Alex J. Chan",
          "hidden": false
        },
        {
          "_id": "67aac01bd7b18841e7c266e3",
          "name": "Fraser Greenlee",
          "hidden": false
        },
        {
          "_id": "67aac01bd7b18841e7c266e4",
          "name": "George Thomas",
          "hidden": false
        },
        {
          "_id": "67aac01bd7b18841e7c266e5",
          "name": "Marvin Purtorab",
          "hidden": false
        },
        {
          "_id": "67aac01bd7b18841e7c266e6",
          "name": "Andy Toulis",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-09T22:11:42.000Z",
      "title": "Dae Jiyeo Mudeol",
      "summary": "Dans cet article, nous présentons le modèle de mémoire grande (LM2). LM2 est une architecture transformer basée sur un analyseur qui résout les limites de l'inférence à l'étape, la logique relationnelle et la synthèse d'information dispersée dans des contextes longs, en ajoutant un module de mémoire supplémentaire. Le LM2 exécute la fonction d'un répertoire de représentations de contexte, interagit avec les tokens d'entrée et l'attention croisée, et met à jour son état par une structure de portes. Pour maintenir les fonctions d'une transformer générale, LM2 intègre le passage du module de mémoire tout en maintenant le flux d'information initial. Les résultats des expériences sur le benchmark BABILong montrent que le modèle LM2 obtient des résultats moyens 37,1% plus élevés que le modèle RMT et 86,3% plus élevés que le modèle Llama-3.2 de base. LM2 démontre des compétences spécialisées en l'inférence à l'étape, la logique numérique et les réponses à des contextes longs. Dans le jeu de données MMLU, un améliorament de 5,0% a été réalisé par rapport à la version entraînée, démontrant que son module de mémoire ne dégrade pas le rendement dans les tâches générales. De plus, nous avons examiné l'explicabilité de la mémoire, l'effet du module de mémoire et le comportement pendant la phase de test. Nos résultats soulignent l'importance de la mémoire explicite pour renforcer l'architecture transformer.",
      "upvotes": 8,
      "discussionId": "67aac01dd7b18841e7c26739"
    },
    "publishedAt": "2025-02-10T22:13:17.117Z",
    "title": "LM2: Large Memory Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06049.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6489e10ca13f65198dc6e122",
      "avatarUrl": "/avatars/4aa9eab488157711b2f0298ddadee2f4.svg",
      "fullname": "Kang",
      "name": "JaxonK",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.03628",
      "authors": [
        {
          "_id": "67aab82e6024056209d727a8",
          "name": "Zhuowei Li",
          "hidden": false
        },
        {
          "_id": "67aab82e6024056209d727a9",
          "name": "Haizhou Shi",
          "hidden": false
        },
        {
          "_id": "67aab82e6024056209d727aa",
          "name": "Yunhe Gao",
          "hidden": false
        },
        {
          "_id": "67aab82e6024056209d727ab",
          "name": "Di Liu",
          "hidden": false
        },
        {
          "_id": "67aab82e6024056209d727ac",
          "name": "Zhenting Wang",
          "hidden": false
        },
        {
          "_id": "67aab82e6024056209d727ad",
          "name": "Yuxiao Chen",
          "hidden": false
        },
        {
          "_id": "67aab82e6024056209d727ae",
          "name": "Ting Liu",
          "hidden": false
        },
        {
          "_id": "67aab82e6024056209d727af",
          "name": "Long Zhao",
          "hidden": false
        },
        {
          "_id": "67aab82e6024056209d727b0",
          "name": "Hao Wang",
          "hidden": false
        },
        {
          "_id": "67aab82e6024056209d727b1",
          "name": "Dimitris N. Metaxas",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-05T21:34:02.000Z",
      "title": "Le secret des tokens : la réduction de la hashmision dans les grands modèles de langue par le contrôle de l'information visuelle",
      "summary": "Le modèle de langue de vision de l'écran (LVLMs) est capable de déduire efficacement le contexte et les entrées visuelles, mais tend à imaginer des contenus grammaticalement cohérents mais pas basés sur des informations visuelles. Dans cet article, on étudie l'ordre des tokens de la racine pour révéler trois patrons principaux qui se présentent dans le traitement de l'information dans les LVLMs : 1) perte d'information visuelle en cours - les tokens basés sur des informations visuelles ont une priorité graduellement réduite pendant le processus de génération. 2) anticipation initiale - les tokens avec un sens ont un pic d'activité plus rapide que les couches finales. 3) information cachée - les tokens basés sur des informations visuelles sont décidés finalement, mais maintiennent une position relativement élevée pendant l'inférence. En se basant sur ces patrons, on propose VISTA (cadre d'intervention de l'information visuelle par l'augmentation des tokens de la racine). VISTA est un cadre d'inférence qui ne nécessite pas un superviseur externe, réduit l'imagination et promeut l'information réelle. VISTA utilise le renforcement de l'information visuelle dans l'espace actif et l'activité dans les couches initiales pour promouvoir la décodification avec sens. Comparé aux méthodes existantes, VISTA peut être appliquée à diverses stratégies de décodification sans nécessiter un superviseur externe. A travers d'expériences étendues, VISTA a réduit l'imagination d'environ 40% et obtenu des résultats exceptionnels dans quatre cadres de référence, en combinant trois stratégies de décodification, dépassant les méthodes existantes.",
      "upvotes": 8,
      "discussionId": "67aab82f6024056209d727f6"
    },
    "publishedAt": "2025-02-10T21:38:53.032Z",
    "title": "The Hidden Life of Tokens: Reducing Hallucination of Large Vision-Language Models via Visual Information Steering",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.03628.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64dfcc62e8b6f3f3baa950e0",
      "avatarUrl": "/avatars/21bbff67d46c08044efe2406575aa77e.svg",
      "fullname": "Zhenting Wang",
      "name": "ztwang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.06786",
      "authors": [
        {
          "_id": "67aae91b83b1182df7c0cf54",
          "name": "Pranav Nair",
          "hidden": false
        },
        {
          "_id": "67aae91b83b1182df7c0cf55",
          "name": "Puranjay Datta",
          "hidden": false
        },
        {
          "_id": "67aae91b83b1182df7c0cf56",
          "name": "Jeff Dean",
          "hidden": false
        },
        {
          "_id": "67aae91b83b1182df7c0cf57",
          "name": "Prateek Jain",
          "hidden": false
        },
        {
          "_id": "67aae91b83b1182df7c0cf58",
          "name": "Aditya Kusupati",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-10T18:59:10.000Z",
      "title": "MATRIZOHA KUANCHITUSHONG",
      "summary": "La quantification des poids d'un modèle a un sens fondamental pour réduire les coûts de communication et d'inférence dans les grands modèles. Cependant, la quantification du modèle (en particulier à des niveaux de précision bas, comme int4 ou int2) peut avoir un effet négatif sur la qualité du modèle. En particulier, int2 a été connu pour affecter significativement la qualité du modèle. Par conséquent, les professionnels doivent généralement gérer plusieurs modèles à différents niveaux de quantification ou proposer un modèle qui satisfait à la fois la qualité et la perte de latence. En revanche, les types de données entières (comme int8) ont la structure 'martyrical' qui enveloppe les entiers de petit taille (comme int4 ou int2) dans les plus significatifs. Cet article propose une nouvelle technologie de quantification à différentes échelles appelée 'MatQuant', avec l'objectif de répondre à la nécessité de modèles quantifiés à différents niveaux de précision. Cette technologie permet d'entraîner un modèle et de le fournir à différents niveaux de précision. De plus, grâce à l'entraînement collaboratif et à la normalisation dans MatQuant, les modèles obtenus avec int2 dans MatQuant peuvent avoir une précision supérieure de 10% par rapport aux modèles quantifiés de manière standard avec int2 (comme QAT ou OmniQuant). Cela montre que, en utilisant la même méthode, un modèle Gemma-2 9B avec la FFN quantifiée à int2 peut avoir une précision supérieure à un modèle Gemma-2 2B avec la FFN quantifiée à int8, démontrant les progrès dans la quantification des modèles.",
      "upvotes": 7,
      "discussionId": "67aae91d83b1182df7c0cff6"
    },
    "publishedAt": "2025-02-11T01:07:50.116Z",
    "title": "Matryoshka Quantization",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06786.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6017
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.06788",
      "authors": [
        {
          "_id": "67aac64de37429ebdbdafc40",
          "name": "Haiwen Diao",
          "hidden": false
        },
        {
          "_id": "67aac64de37429ebdbdafc41",
          "name": "Xiaotong Li",
          "hidden": false
        },
        {
          "_id": "67aac64de37429ebdbdafc42",
          "name": "Yufeng Cui",
          "hidden": false
        },
        {
          "_id": "67aac64de37429ebdbdafc43",
          "name": "Yueze Wang",
          "hidden": false
        },
        {
          "_id": "67aac64de37429ebdbdafc44",
          "name": "Haoge Deng",
          "hidden": false
        },
        {
          "_id": "67aac64de37429ebdbdafc45",
          "user": {
            "_id": "6565bc5ee5aac326bfc98e39",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/vIfHy9Y1yAK6A96UCHNBH.jpeg",
            "isPro": false,
            "fullname": "Ting Pan",
            "user": "PhyscalX",
            "type": "user"
          },
          "name": "Ting Pan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T07:55:09.401Z",
          "hidden": false
        },
        {
          "_id": "67aac64de37429ebdbdafc46",
          "name": "Wenxuan Wang",
          "hidden": false
        },
        {
          "_id": "67aac64de37429ebdbdafc47",
          "name": "Huchuan Lu",
          "hidden": false
        },
        {
          "_id": "67aac64de37429ebdbdafc48",
          "name": "Xinlong Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-10T18:59:58.000Z",
      "title": "EVEv2 : Amélioration de la base de données de base sans codage pour les modèles de vision-langue",
      "summary": "Les modèles de langue visuelle (VLMs) sans codage actuels réduisent rapidement la différence de performance par rapport aux modèles basés sur un codage. Cela fournit une preuve claire de la possibilité de modèles simples et efficaces structurellement. Nous avons analysé en profondeur le rendement des VLMs sans codage, construits à partir d'un codage préalablement entraîné, d'un tokenisateur discret et de couches minimalistes de modèle visuel. Nous avons développé une stratégie efficace permettant aux VLMs sans codage de concourir aux modèles basés sur un codage en termes de performance. À travers une recherche détaillée, nous avons présenté à la communauté le nouveau et meilleur VLM sans codage, EVEv2.0. Nous montrons : (i) la capacité de décomposer et de relier adéquatement la vision et le langage dans un seul modèle, réduisant ainsi l'entaillement entre modèles. (ii) Une bonne stratégie d'entraînement facilite l'optimisation efficace des VLMs sans codage. Par des évaluations larges, nous démontrons que notre EVEv2.0 présente une architecture développée uniquement dans le décodageur, avec une grande efficacité en données et un fort rendement en reconnaissance visuelle. Le code est disponible pour l'utilisation publique : https://github.com/baaivision/EVE.",
      "upvotes": 6,
      "discussionId": "67aac64ee37429ebdbdafc96"
    },
    "publishedAt": "2025-02-10T22:40:39.442Z",
    "title": "EVEv2: Improved Baselines for Encoder-Free Vision-Language Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06788.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b4a717aa03b6520839e9b8",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b4a717aa03b6520839e9b8/Rt3ERG-6BVEA4hAwOz0_I.jpeg",
      "fullname": "Haiwen Diao",
      "name": "Paranioar",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.06782",
      "authors": [
        {
          "_id": "67aae76c71a9983f50e134ef",
          "name": "Dongyang Liu",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e134f0",
          "name": "Shicheng Li",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e134f1",
          "name": "Yutong Liu",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e134f2",
          "name": "Zhen Li",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e134f3",
          "name": "Kai Wang",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e134f4",
          "name": "Xinyue Li",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e134f5",
          "name": "Qi Qin",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e134f6",
          "name": "Yufei Liu",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e134f7",
          "name": "Yi Xin",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e134f8",
          "name": "Zhongyu Li",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e134f9",
          "name": "Bin Fu",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e134fa",
          "name": "Chenyang Si",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e134fb",
          "name": "Yuewen Cao",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e134fc",
          "name": "Conghui He",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e134fd",
          "name": "Ziwei Liu",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e134fe",
          "name": "Yu Qiao",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e134ff",
          "name": "Qibin Hou",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e13500",
          "name": "Hongsheng Li",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e13501",
          "name": "Peng Gao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-10T18:58:11.000Z",
      "title": "Lumina-Video : Next-DiT pour la génération de vidéos efficaces et flexibles à travers des échelles multiples",
      "summary": "Le dernier développement a mis les Transformers de Diffusion (DiTs) au cœur du modèle de génération. En s'appuyant sur ce succès, Lumina-Next a atteint un rendement élevé dans la génération d'images réalistes en utilisant Next-DiT. Cependant, la possibilité de générer des vidéos n'a pas été significativement développée, car il existe de grands défis à relever pour modéliser la complexité spatio-temporelle inhérente aux données vidéo. En réponse à cette question, nous présentons Lumina-Video. Ce cadre de travail utilise les forces de Next-DiT et introduit des solutions adaptées pour la synthèse de vidéos. Lumina-Video utilise une architecture multi-échelle de Next-DiT, ce qui permet de garantir à la fois l'efficacité et la flexibilité lors de l'entraînement. De plus, en utilisant le score de mouvement comme condition explicite, Lumina-Video peut contrôler directement la dynamique des vidéos générées. Cet approche combine un schéma d'entraînement avancé, qui augmente la résolution et la FPS, avec un schéma d'entraînement plus flexible pour entraîner avec des données de synthèse naturelle, assurant une efficacité élevée tant pendant l'entraînement que l'inférence, ainsi qu'une qualité artistique et fluidité excellentes. De plus, nous proposons Lumina-V2A, un modèle basé sur Next-DiT pour générer de l'audio à partir de vidéos. Ce modèle vise à générer des vidéos et des sons synchronisés. Le code est disponible sur https://www.github.com/Alpha-VLLM/Lumina-Video.",
      "upvotes": 5,
      "discussionId": "67aae76e71a9983f50e1357d"
    },
    "publishedAt": "2025-02-11T01:00:25.383Z",
    "title": "Lumina-Video: Efficient and Flexible Video Generation with Multi-scale Next-DiT",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06782.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6017
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.05431",
      "authors": [
        {
          "_id": "67aac392385da1f07cc7fcbd",
          "user": {
            "_id": "64f58b970b24e548a85522bc",
            "avatarUrl": "/avatars/c8ca1294b5a1edd609694877e335b22f.svg",
            "isPro": false,
            "fullname": "Xinyu Yang",
            "user": "Hanyuezhuohua",
            "type": "user"
          },
          "name": "Xinyu Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T07:55:13.131Z",
          "hidden": false
        },
        {
          "_id": "67aac392385da1f07cc7fcbe",
          "name": "Tianqi Chen",
          "hidden": false
        },
        {
          "_id": "67aac392385da1f07cc7fcbf",
          "name": "Beidi Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-08T03:41:16.000Z",
      "title": "APE: Génération d'Action et de Perception dans les Environnements Publics et Privés",
      "summary": "Le méthode de génération avec augmentation de contexte (CAG), incluant RAG et ICL, nécessite une combinaison efficace de multiples contextes pour générer des réponses aux demandes du utilisateur. L'entrée séquentielle de ces contextes peut imposer une grande charge de calcul en raison de la combinaison de contextes, ce qui nécessite une recodéification de chaque demande. Pour résoudre ce problème, on examine la possibilité d'une codification parallèle, où les états KV de chaque contexte sont calculés et cachés indépendamment. Cet approche permet que, lors de l'inférence, les états cacheés soient lu directement, ce qui permet d'augmenter la quantité de contextes. Cependant, l'asymétrie de la distribution de l'attention peut significativement affecter le rendement si la codification parallèle est appliquée directement. Pour aborder ce problème, on propose l'Adaptive Parallel Encoding (APE), qui ajuste la distribution de la codification parallèle à celle de la codification séquentielle en utilisant un préfixe commun, une température d'attention et un facteur d'échelle. Dans les résultats des tâches de RAG et ICL, on observe que, avec le même input, la codification séquentielle maintient un rendement de 98% et 93%, tandis que la codification parallèle dépasse de 3,6% et 7,9%. De plus, l'APE peut être appliquée à plusieurs exemples de CAG et codifier des centaines de contextes en même temps. Selon l'évaluation de l'efficacité, l'APE réduit le temps de prédiction pour des contextes de 128K de longueur de 28 fois et peut augmenter la vitesse jusqu'à 4,5 fois.",
      "upvotes": 5,
      "discussionId": "67aac393385da1f07cc7fd17"
    },
    "publishedAt": "2025-02-10T22:29:36.102Z",
    "title": "APE: Faster and Longer Context-Augmented Generation via Adaptive Parallel Encoding",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05431.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "64f58b970b24e548a85522bc",
      "avatarUrl": "/avatars/c8ca1294b5a1edd609694877e335b22f.svg",
      "fullname": "Xinyu Yang",
      "name": "Hanyuezhuohua",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.06155",
      "authors": [
        {
          "_id": "67aab9b4a2bf5e5ea03d4c19",
          "user": {
            "_id": "643a451ee2b979ae6141329d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643a451ee2b979ae6141329d/HN3M5vyroanQoUEiXJFyB.jpeg",
            "isPro": false,
            "fullname": "Hangliang Ding",
            "user": "foreverpiano",
            "type": "user"
          },
          "name": "Hangliang Ding",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T07:55:29.115Z",
          "hidden": false
        },
        {
          "_id": "67aab9b4a2bf5e5ea03d4c1a",
          "name": "Dacheng Li",
          "hidden": false
        },
        {
          "_id": "67aab9b4a2bf5e5ea03d4c1b",
          "name": "Runlong Su",
          "hidden": false
        },
        {
          "_id": "67aab9b4a2bf5e5ea03d4c1c",
          "name": "Peiyuan Zhang",
          "hidden": false
        },
        {
          "_id": "67aab9b4a2bf5e5ea03d4c1d",
          "user": {
            "_id": "64bba541da140e461924dfed",
            "avatarUrl": "/avatars/367993765b0ca3734b2b100db33ed787.svg",
            "isPro": false,
            "fullname": "zhijie deng",
            "user": "zhijie3",
            "type": "user"
          },
          "name": "Zhijie Deng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T07:55:25.471Z",
          "hidden": false
        },
        {
          "_id": "67aab9b4a2bf5e5ea03d4c1e",
          "name": "Ion Stoica",
          "hidden": false
        },
        {
          "_id": "67aab9b4a2bf5e5ea03d4c1f",
          "name": "Hao Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-10T05:00:56.000Z",
      "title": "Efficient-vDiT : Transformateur d'Attention Vidéo Dipénsion Efficace",
      "summary": "DiTs dispose d'une structure entièrement en attention 3D, ce qui permet la synthèse de vidéos de haute qualité, mais son coût de calcul d'attention et de nombreux pas d'échantillonnage le rend coûteux. Par exemple, le modèle Open-Sora-Plan, très populaire, nécessite plus de 9 minutes pour générer une seule vidéo de 29 frames. Dans cet article, nous abordons ces inconvénients à partir de deux perspectives : 1) nous réduisons l'attention complète 3D en se basant sur l'absence de données vidéo. Nous identifions des motifs de création similaires à la mosaïque qui sont visibles sur les cartes d'attention 3D des données vidéo, et proposons une nouvelle famille d'attentions 3D épars avec une complexité linéaire par rapport au nombre de frames de vidéo. 2) Nous introduisons un design de multiples pas de cohérence pour réduire le processus d'échantillonnage. Nous divisons le tracé d'échantillonnage en étapes pouvant être résolues, et dans chaque étape, nous effectuons un design de cohérence pour activer la capacité de multiples pas de génération. De plus, nous concevons un processus d'apprentissage en trois étapes pour intégrer l'attention de faible complexité et la capacité de génération de multiples pas. En particulier, en utilisant seulement 0,1% de données préalablement entraînées, nous pouvons créer le modèle Open-Sora-Plan-1.2 7,4 à 7,8 fois plus rapide et ajuster son rendement sur VBench. De plus, notre approche permet l'application de la calcul distribué et, en utilisant la programmation parallèle de séquences sur 4 graphiques, nous pouvons atteindre un accroissement supplémentaire de vitesse de 3,91x.",
      "upvotes": 5,
      "discussionId": "67aab9bca2bf5e5ea03d4e3c"
    },
    "publishedAt": "2025-02-10T22:09:58.181Z",
    "title": "Efficient-vDiT: Efficient Video Diffusion Transformers With Attention Tile",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06155.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63565cc56d7fcf1bedb7d347",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63565cc56d7fcf1bedb7d347/XGcHP4VkO_oieA1gZ4IAX.jpeg",
      "fullname": "Zhang Peiyuan",
      "name": "PY007",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 82
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.06527",
      "authors": [
        {
          "_id": "67aae4128d478dcb4b39a097",
          "name": "D. She",
          "hidden": false
        },
        {
          "_id": "67aae4128d478dcb4b39a098",
          "name": "Mushui Liu",
          "hidden": false
        },
        {
          "_id": "67aae4128d478dcb4b39a099",
          "name": "Jingxuan Pang",
          "hidden": false
        },
        {
          "_id": "67aae4128d478dcb4b39a09a",
          "name": "Jin Wang",
          "hidden": false
        },
        {
          "_id": "67aae4128d478dcb4b39a09b",
          "name": "Zhen Yang",
          "hidden": false
        },
        {
          "_id": "67aae4128d478dcb4b39a09c",
          "name": "Wanggui He",
          "hidden": false
        },
        {
          "_id": "67aae4128d478dcb4b39a09d",
          "name": "Guanghao Zhang",
          "hidden": false
        },
        {
          "_id": "67aae4128d478dcb4b39a09e",
          "name": "Yi Wang",
          "hidden": false
        },
        {
          "_id": "67aae4128d478dcb4b39a09f",
          "name": "Qihan Huang",
          "hidden": false
        },
        {
          "_id": "67aae4128d478dcb4b39a0a0",
          "name": "Haobin Tang",
          "hidden": false
        },
        {
          "_id": "67aae4128d478dcb4b39a0a1",
          "name": "Yunlong Yu",
          "hidden": false
        },
        {
          "_id": "67aae4128d478dcb4b39a0a2",
          "name": "Siming Fu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-10T14:50:32.000Z",
      "title": "CustomVideoX : Référence 3D avec Adaptation Dynamique pour le Transformer de Diffusion Personnalisé en 0 Photos",
      "summary": "La génération personnalisée a réalisé des progrès significatifs dans le domaine de la synthèse d'images, mais la génération de vidéos personnalisées devient difficile en raison de l'incertitude temporelle et de la dégradation de la qualité. Dans cet article, nous présentons un nouveau cadre appelé \"CustomVideoX\" pour générer des vidéos personnalisées à partir d'images de référence. CustomVideoX utilise une réseau de vidéos entraînée préalablement, extrayant des caractéristiques de référence à travers des paramètres d'entraînement indépendants de LoRA, garantissant ainsi une efficacité et une adaptabilité. Pour promouvoir l'interaction infinie entre l'image de référence et le contenu de la vidéo, nous proposons l'Attention de Référence 3D. Cela permet que les caractéristiques de l'image de référence correspondent directement et simultanément aux dimensions spatiales et temporelles de tous les cadres de la vidéo. Pour réduire l'influence excessive des caractéristiques de l'image de référence et du contexte sur le contenu de la vidéo générée lors de l'inférence, nous mettons en œuvre la stratégie de l'Attention de Référence avec Bias Temporel-Aware (TAB). Cette stratégie ajuste le biais de référence de manière dynamique en fonction des étapes temporelles. De plus, nous présentons le module Entity Region-Aware Enhancement (ERAE). Ce module ajuste le biais d'attention en fonction de l'injection de caractéristiques de référence, améliorant les régions d'activité élevée des marques d'entités clés, améliorant ainsi l'effet. Pour évaluer de manière détaillée la génération de vidéos personnalisées, nous avons construit un nouveau standard de référence qui comprend plus de 50 objets et 100 pistes, appelé \"VideoBench\". Les résultats expérimentaux montrent que CustomVideoX dépasse considérablement les méthodes existantes en termes de cohérence et de qualité des vidéos.",
      "upvotes": 4,
      "discussionId": "67aae4178d478dcb4b39a1e7"
    },
    "publishedAt": "2025-02-11T00:46:11.168Z",
    "title": "CustomVideoX: 3D Reference Attention Driven Dynamic Adaptation for Zero-Shot Customized Video Diffusion Transformers",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06527.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6017
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.06635",
      "authors": [
        {
          "_id": "67aac0ba91e6f5eb5476ea76",
          "name": "Qingshui Gu",
          "hidden": false
        },
        {
          "_id": "67aac0ba91e6f5eb5476ea77",
          "name": "Shu Li",
          "hidden": false
        },
        {
          "_id": "67aac0ba91e6f5eb5476ea78",
          "user": {
            "_id": "64ab99dcb76bfd863eba64c1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ab99dcb76bfd863eba64c1/UBXwDPx17X-gl-SzBPvrc.jpeg",
            "isPro": false,
            "fullname": "TY.Zheng",
            "user": "aaabiao",
            "type": "user"
          },
          "name": "Tianyu Zheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T07:55:15.968Z",
          "hidden": false
        },
        {
          "_id": "67aac0ba91e6f5eb5476ea79",
          "name": "Zhaoxiang Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-10T16:31:37.000Z",
      "title": "Steel-LLM : Démarrage à partir du zéro - Une passion personnelle pour la construction d'un LLM axé sur le chinois",
      "summary": "Steel-LLM est un modèle de langage développé en Chine avec l'objectif de créer un modèle de haute qualité de code open, malgré les limites des ressources informatiques. Ce projet, publié en mars 2024, se concentre sur la formation d'un modèle à 100 millions de paramètres, en prioritisant la transparence et la partage d'informations pratiques pour aider d'autres membres de la communauté. Le processus d'entraînement se fait principalement avec des données chinoises, y compris certaines données en anglais, et complète les lacunes des modèles open de langage actuels, en fournissant une description plus détaillée du processus de conception du modèle. Steel-LLM a démontré un rendement compétitif dans des référentiels comme CEVAL et CMMLU, et a dépassé les modèles initiaux d'institutions de grande échelle. Ce texte résume les principales contributions du projet. Les ressources pour le développement du modèle sont fournies aux chercheurs et praticiens comme une outil utile. Les points de contrôle du modèle et les scripts d'entraînement sont disponibles sur https://github.com/zhanshijinwat/Steel-LLM.",
      "upvotes": 4,
      "discussionId": "67aac0bb91e6f5eb5476eab8"
    },
    "publishedAt": "2025-02-10T22:20:38.168Z",
    "title": "Steel-LLM:From Scratch to Open Source -- A Personal Journey in Building a Chinese-Centric LLM",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06635.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ab99dcb76bfd863eba64c1",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ab99dcb76bfd863eba64c1/UBXwDPx17X-gl-SzBPvrc.jpeg",
      "fullname": "TY.Zheng",
      "name": "aaabiao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.04370",
      "authors": [
        {
          "_id": "67aafd90141fac22732a79b3",
          "name": "Zhenglin Zhou",
          "hidden": false
        },
        {
          "_id": "67aafd90141fac22732a79b4",
          "name": "Xiaobo Xia",
          "hidden": false
        },
        {
          "_id": "67aafd90141fac22732a79b5",
          "name": "Fan Ma",
          "hidden": false
        },
        {
          "_id": "67aafd90141fac22732a79b6",
          "name": "Hehe Fan",
          "hidden": false
        },
        {
          "_id": "67aafd90141fac22732a79b7",
          "name": "Yi Yang",
          "hidden": false
        },
        {
          "_id": "67aafd90141fac22732a79b8",
          "name": "Tat-Seng Chua",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-05T11:03:08.000Z",
      "title": "DreamDPO : Optimisation Directe de la Préférence pour la Génération de Texte 3D qui Coincide avec les Préférences Humaines",
      "summary": "La génération 3D à partir de texte réalise l'automatique de contenu 3D à partir de descriptions textuelles, présentant un grand potentiel d'innovation dans diverses domaines. Cependant, les méthodes actuelles rencontrent des difficultés pour aligner le contenu généré aux préférences personnelles des individus, limitant ainsi l'application et la flexibilité. Pour résoudre ces problèmes, cet article propose un cadre de travail basé sur l'optimisation qui intègre directement les préférences personnelles dans le processus de génération 3D, nommé \"DreamDPO\". De manière pratique, DreamDPO construit un exemple de couple de personnes et compare ses préférences avec des fonctions d'alignement et des grands scénarios de diffusion, finalement optimisant la représentation 3D en utilisant une fonction de perte orientée vers les préférences. En comparant avec les couples de personnes, DreamDPO augmente la précision des évaluations ponctuelles sans dépendre de ces derniers, et améliore la capacité de contrôle micro-contrôlé des préférences. Les expériences montrent que DreamDPO produit des résultats exceptionnels par rapport aux méthodes actuelles et offre des contenus 3D de haute qualité avec un contrôle possible. Les codes et modèles sont disponibles sous licence ouverte.",
      "upvotes": 3,
      "discussionId": "67aafd94141fac22732a7adc"
    },
    "publishedAt": "2025-02-11T02:46:33.870Z",
    "title": "DreamDPO: Aligning Text-to-3D Generation with Human Preferences via Direct Preference Optimization",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6425318d175bd2952281065e/R7cMLIsmYovAMtL1vhsDn.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04370.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6425318d175bd2952281065e",
      "avatarUrl": "/avatars/37deb6ceb1552dece43a1c8c13c1c871.svg",
      "fullname": "ZhenglinZhou",
      "name": "zhenglin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.05957",
      "authors": [
        {
          "_id": "67aaecec114e64d6e15e7f41",
          "name": "Jiabin Tang",
          "hidden": false
        },
        {
          "_id": "67aaecec114e64d6e15e7f42",
          "name": "Tianyu Fan",
          "hidden": false
        },
        {
          "_id": "67aaecec114e64d6e15e7f43",
          "name": "Chao Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-09T16:53:56.000Z",
      "title": "MetaChain : Créez des agents de LLM en utilisant un cadre de travail entièrement automatique et sans code.",
      "summary": "Les modèles de langage grand (LLM) d'agents ont démontré des capacités impressionnantes dans l'automatisation des tâches et la prise de décisions intelligentes, impulsant l'introduction large de cadres de développement d'agents comme LangChain et AutoGen. Cependant, ces cadres sont principalement destinés aux développeurs ayant un niveau avancé de connaissances techniques, ce qui limite leur accessibilité à une minorité de la population. Ce point de vue stricte indique que seul 0,03% de la population mondiale a accès aux techniques de programmation nécessaires, ce qui agit comme un grand limite. Cette approche soulève fondamentalement la question de savoir si les personnes sans connaissances techniques peuvent construire directement des agents de LLM. Pour relever ces défis, nous présentons MetaChain, un cadre de développement entièrement automatisé et à haut niveau d'automatisation. Ce cadre nécessite seulement du texte naturel pour générer et distribuer des agents de LLM. MetaChain fonctionne comme un système opérationnel automatique d'agents, composé de quatre composants principaux : Utilisations de Système d'Agents, Moteur d'Accès à la Base de Connaissances de LLM, Système de Fichiers Autogérés et Modularisation d'Agents de Jeu. Ce système peut générer et modifier efficacement et dynamiquement les outils, agents et flux de travail sans nécessiter de code ou de manuelité. MetaChain offre des fonctions de développement d'agents qui ne requièrent pas de programmation, maintenant ainsi la diversité dans les systèmes d'agents communs. Dans le benchmark GAIA, MetaChain a démontré son efficacité dans des tâches d'agents générales, dépassant les méthodes les plus avancées actuelles. De plus, les fonctions de générateur d'agents de révision de MetaChain (RAG) présentent un rendement constant et excellent par rapport à d'autres solutions basées sur des modèles de LLM.",
      "upvotes": 3,
      "discussionId": "67aaecef114e64d6e15e802c"
    },
    "publishedAt": "2025-02-11T01:33:35.134Z",
    "title": "MetaChain: A Fully-Automated and Zero-Code Framework for LLM Agents",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05957.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "643b751cc5f633a7fa84b325",
      "avatarUrl": "/avatars/a094b856cf3d51eb78d16a14361def62.svg",
      "fullname": "Tang",
      "name": "Jiabin99",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.06764",
      "authors": [
        {
          "_id": "67aac6052c02e43558b6b4b0",
          "name": "Kiwhan Song",
          "hidden": false
        },
        {
          "_id": "67aac6052c02e43558b6b4b1",
          "name": "Boyuan Chen",
          "hidden": false
        },
        {
          "_id": "67aac6052c02e43558b6b4b2",
          "name": "Max Simchowitz",
          "hidden": false
        },
        {
          "_id": "67aac6052c02e43558b6b4b3",
          "name": "Yilun Du",
          "hidden": false
        },
        {
          "_id": "67aac6052c02e43558b6b4b4",
          "name": "Russ Tedrake",
          "hidden": false
        },
        {
          "_id": "67aac6052c02e43558b6b4b5",
          "name": "Vincent Sitzmann",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-10T18:44:25.000Z",
      "title": "Histoire Guide de la Route de Diffusion",
      "summary": "Filterpréadventure",
      "upvotes": 3,
      "discussionId": "67aac6072c02e43558b6b543"
    },
    "publishedAt": "2025-02-11T00:55:33.866Z",
    "title": "History-Guided Video Diffusion",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06764.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6017
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.06023",
      "authors": [
        {
          "_id": "67aac3a9ef5570c0c9047095",
          "user": {
            "_id": "640f6299ef5c6dcac8b1df52",
            "avatarUrl": "/avatars/022f21183abc8a8b5ce1b198d3ba96dc.svg",
            "isPro": false,
            "fullname": "Amir",
            "user": "sahsaeedi",
            "type": "user"
          },
          "name": "Amir Saeidi",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-02-11T03:31:48.492Z",
          "hidden": false
        },
        {
          "_id": "67aac3a9ef5570c0c9047096",
          "name": "Yiran Luo",
          "hidden": false
        },
        {
          "_id": "67aac3a9ef5570c0c9047097",
          "name": "Agneet Chatterjee",
          "hidden": false
        },
        {
          "_id": "67aac3a9ef5570c0c9047098",
          "name": "Shamanthak Hegde",
          "hidden": false
        },
        {
          "_id": "67aac3a9ef5570c0c9047099",
          "name": "Bimsara Pathiraja",
          "hidden": false
        },
        {
          "_id": "67aac3a9ef5570c0c904709a",
          "name": "Yezhou Yang",
          "hidden": false
        },
        {
          "_id": "67aac3a9ef5570c0c904709b",
          "name": "Chitta Baral",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-09T20:34:43.000Z",
      "title": "Modèle d'Optimisation de Préférences Dualcapacitif",
      "summary": "Récemment, le développement de l'optimisation des distractions a été initialement associé au développement des Modèles de Langue Grands (LLMs), montrant également une possibilité considérable dans l'amélioration de modèles qui évoluent depuis des phrases jusqu'à des images. Ces méthodes visent à apprendre la distribution des distractions tout en distinguant entre différentes distributions. Cependant, les ensembles de données actuels de distractions présentent des distributions qui se chevauchent, ce qui génère des situations où les distributions en conflit sont fréquentes. De plus, des images ont été trouvées qui contiennent des informations irrélevantes aux prompts d'entrée, limitant ainsi la prédiction précise de bruit dans les méthodes d'optimisation des distractions, connues sous le nom de problèmes de prompts irrélevants. Pour résoudre ces problèmes, nous proposons un nouvel approche appelée Dual Caption Preference Optimization (DCPO), qui utilise deux captions différentes pour inhiber les prompts irrélevants. Pour aborder les distributions en conflit, nous proposons l'ensemble de données Pick-Double Caption, qui est une amélioration de Pick-a-Pic v2 et permet d'utiliser des captions pour des images liées aux distractions et pour des images irrélevantes. De plus, nous proposons trois stratégies différentes pour la génération de captions : captchising, parmibivisive et hybride. Nos expérimentations montrent que DCPO améliore significativement la qualité des images et leur relation avec les prompts, dépassant Stable Diffusion (SD) 2.1, SFT_Chosen, Diffusion-DPO et MaPO, et une rétropropagation de SD 2.1 a été effectuée en utilisant plusieurs métriques comme Pickscore, HPSv2.1, GenEval, CLIPscore et ImageReward.",
      "upvotes": 3,
      "discussionId": "67aac3b1ef5570c0c9047264"
    },
    "publishedAt": "2025-02-10T22:33:17.468Z",
    "title": "Dual Caption Preference Optimization for Diffusion Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06023.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "640f6299ef5c6dcac8b1df52",
      "avatarUrl": "/avatars/022f21183abc8a8b5ce1b198d3ba96dc.svg",
      "fullname": "Amir",
      "name": "sahsaeedi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.06060",
      "authors": [
        {
          "_id": "67ab1314385da1f07cda1271",
          "user": {
            "_id": "63abbf74ad514ca8d14a0548",
            "avatarUrl": "/avatars/b1357b73b8f9a8ff9908710ad64154ef.svg",
            "isPro": false,
            "fullname": "Bidipta Sarkar",
            "user": "bidiptas",
            "type": "user"
          },
          "name": "Bidipta Sarkar",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T09:51:17.933Z",
          "hidden": false
        },
        {
          "_id": "67ab1314385da1f07cda1272",
          "name": "Warren Xia",
          "hidden": false
        },
        {
          "_id": "67ab1314385da1f07cda1273",
          "name": "C. Karen Liu",
          "hidden": false
        },
        {
          "_id": "67ab1314385da1f07cda1274",
          "name": "Dorsa Sadigh",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-09T22:44:45.000Z",
      "title": "L'entraînement de modèles de langage pour des dialogues sociaux en utilisant l'apprentissage par récompense multi-agent",
      "summary": "La communication en langage naturel est un instrument efficace qui permet aux agents indépendants dans un environnement multi-agent à partager des informations partiellement observées et à faciliter la collaboration avec les humains dans un scénario d'apprentissage 0-shot. Cependant, les recherches précédentes ont été largement basées sur des démonstrations à grande échelle humaine ou ont présenté des limites dans la capacité à générer des stratégies de communication naturelles et efficaces. Dans ce travail, la communication est entraînée par la discussion en langage naturel sur l'environnement, y compris des démonstrations humaines. Le problème de communication est décomposé en écoute et dialogue, et l'idée fondamentale de générer des signaux de base de danse pour guider le dialogue est considérée, basée sur la prédiction d'information utile en fonction du but de l'agent. Spécifiquement, la capacité d'écoute est améliorée en entraînant le modèle avec des informations sur l'environnement prédites en fonction d'un dialogue, tandis que l'apprentissage par renforcement entre plusieurs agents est utilisé pour améliorer la capacité de dialogue grâce à des récompenses basées sur des messages qui influent entre les agents. L'importance et la nécessité de la communication dans des environnements sociaux complexes sont étudiés, en examinant des jeux d'explication social basés sur des démonstrateurs et en abordant des problèmes importants comme l'étude de personnages antagonistes irréguliers. Les phénomènes générés par notre méthode sont analysés, des questions sont posées, des preuves sont fournies pour découvrir des phénomènes, et une discussion intense est atteinte, atteignant un rendement deux fois supérieur au apprentissage par renforcement standard. Notre code et modèle sont disponibles sur la URL suivante : https://socialdeductionllm.github.io/",
      "upvotes": 2,
      "discussionId": "67ab1315385da1f07cda12a5"
    },
    "publishedAt": "2025-02-11T04:08:55.672Z",
    "title": "Training Language Models for Social Deduction with Multi-Agent Reinforcement Learning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06060.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63abbf74ad514ca8d14a0548",
      "avatarUrl": "/avatars/b1357b73b8f9a8ff9908710ad64154ef.svg",
      "fullname": "Bidipta Sarkar",
      "name": "bidiptas",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.05795",
      "authors": [
        {
          "_id": "67ab189a8087b66340398b01",
          "name": "Wenfang Sun",
          "hidden": false
        },
        {
          "_id": "67ab189a8087b66340398b02",
          "name": "Xinyuan Song",
          "hidden": false
        },
        {
          "_id": "67ab189a8087b66340398b03",
          "user": {
            "_id": "64245f2c089d5fae56b4549a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64245f2c089d5fae56b4549a/qUHFsL9Svwyj5BKpfMtaY.jpeg",
            "isPro": false,
            "fullname": "Pengxiang Li",
            "user": "pengxiang",
            "type": "user"
          },
          "name": "Pengxiang Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T09:51:15.671Z",
          "hidden": false
        },
        {
          "_id": "67ab189a8087b66340398b04",
          "name": "Lu Yin",
          "hidden": false
        },
        {
          "_id": "67ab189a8087b66340398b05",
          "name": "Yefeng Zheng",
          "hidden": false
        },
        {
          "_id": "67ab189a8087b66340398b06",
          "name": "Shiwei Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-09T07:03:36.000Z",
      "title": "Le Malefice de la Mente du Modèle de Langue",
      "summary": "Dans cet article, grâce au concept de « Depth of Depth », on dévoile et analyse le phénomène selon lequel plus de la moitié des couches dans les grands modèles de langue modernes (LLMs) produisent des effets inattendus et faibles, proposant des solutions. Tout d'abord, on confirme la présence étendue de ce phénomène dans des LLMs célèbres tels que Llama, Mistral, DeepSeek et Qwen. Grâce à un analyse théorique et expérimentale, on identifie l'utilisation large de la Normalisation pré-couche (Pre-Layer Normalization, ou Pre-LN) comme la cause fondamentale de l'inefficacité des couches profondes des LLMs. La Pre-LN stabilise l'entraînement des modèles Transformer, mais la variance de la sortie augmente exponentiellement avec la profondeur, et les dérivées des blocs Transformer profonds se transforment en matrices de mise à l'échelle, contribuant très peu à l'entraînement. Pour résoudre ces problèmes, on propose la Normalisation par Couche Scaling (LayerNorm Scaling). LayerNorm Scaling réduit la variance de la normalisation des couches en multipliant par la racine carrée inverse de la profondeur, ce qui empêche l'explosion de la variance de la sortie des couches profondes et améliore leur contribution à l'entraînement. Les résultats expérimentaux montrent que, dans un intervalle de tailles de modèle allant de 130M à 1B, LayerNorm Scaling améliore significativement le rendement de l'entraînement des LLMs par rapport à la Pre-LN, et cette amélioration est due à ce que LayerNorm Scaling permet aux couches profondes de contribuer plus efficacement pendant l'entraînement.",
      "upvotes": 1,
      "discussionId": "67ab189b8087b66340398b3b"
    },
    "publishedAt": "2025-02-11T04:30:30.043Z",
    "title": "The Curse of Depth in Large Language Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05795.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64245f2c089d5fae56b4549a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64245f2c089d5fae56b4549a/qUHFsL9Svwyj5BKpfMtaY.jpeg",
      "fullname": "Pengxiang Li",
      "name": "pengxiang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  }
]