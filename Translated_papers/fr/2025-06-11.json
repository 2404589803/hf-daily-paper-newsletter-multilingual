[
  {
    "paper": {
      "id": "2506.06751",
      "authors": [
        {
          "_id": "6848fecf42e4f9106973f315",
          "user": {
            "_id": "62bd6c6baaf1480f1aa2222e",
            "avatarUrl": "/avatars/fd92ae2986d435a47eb1e382ac11d8e0.svg",
            "isPro": false,
            "fullname": "Mikhail Salnikov",
            "user": "msalnikov",
            "type": "user"
          },
          "name": "Mikhail Salnikov",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-11T08:34:47.630Z",
          "hidden": false
        },
        {
          "_id": "6848fecf42e4f9106973f316",
          "name": "Dmitrii Korzh",
          "hidden": false
        },
        {
          "_id": "6848fecf42e4f9106973f317",
          "user": {
            "_id": "657c4a8dfb0285d857d86e4c",
            "avatarUrl": "/avatars/17635a4c2c804dd3837ae01833bb940d.svg",
            "isPro": false,
            "fullname": "Ivan",
            "user": "IvanLazichny",
            "type": "user"
          },
          "name": "Ivan Lazichny",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-11T09:26:44.876Z",
          "hidden": false
        },
        {
          "_id": "6848fecf42e4f9106973f318",
          "name": "Elvir Karimov",
          "hidden": false
        },
        {
          "_id": "6848fecf42e4f9106973f319",
          "name": "Artyom Iudin",
          "hidden": false
        },
        {
          "_id": "6848fecf42e4f9106973f31a",
          "name": "Ivan Oseledets",
          "hidden": false
        },
        {
          "_id": "6848fecf42e4f9106973f31b",
          "name": "Oleg Y. Rogov",
          "hidden": false
        },
        {
          "_id": "6848fecf42e4f9106973f31c",
          "user": {
            "_id": "605473729d7c1d4d81b7e52b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662046050710-605473729d7c1d4d81b7e52b.jpeg",
            "isPro": false,
            "fullname": "Alexander Panchenko",
            "user": "apanc",
            "type": "user"
          },
          "name": "Alexander Panchenko",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-11T09:27:06.218Z",
          "hidden": false
        },
        {
          "_id": "6848fecf42e4f9106973f31d",
          "name": "Natalia Loukachevitch",
          "hidden": false
        },
        {
          "_id": "6848fecf42e4f9106973f31e",
          "user": {
            "_id": "662f8d645c4db70c77a203b0",
            "avatarUrl": "/avatars/72f9a3c39b3ba5114388d16a35524835.svg",
            "isPro": false,
            "fullname": "Elena Tutubalina",
            "user": "tlenusik",
            "type": "user"
          },
          "name": "Elena Tutubalina",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-11T09:26:55.840Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-07T10:45:17.000Z",
      "submittedOnDailyAt": "2025-06-11T02:53:12.616Z",
      "title": "Le préjugé dans la politique internationale : les modèles de langage modernes de \"bonnes\" et \"maures\" nations",
      "submittedOnDailyBy": {
        "_id": "62bd6c6baaf1480f1aa2222e",
        "avatarUrl": "/avatars/fd92ae2986d435a47eb1e382ac11d8e0.svg",
        "isPro": false,
        "fullname": "Mikhail Salnikov",
        "user": "msalnikov",
        "type": "user"
      },
      "summary": "Cet article évalue les différences de vision mondiale dans l'interprétation des événements historiques (États-Unis, Royaume-Uni, Union européenne-Chine, Chine) pour évaluer l'inclinaison politique locale des Modèles de Langue de Grande Taille (LLMs). Nous présentons un nouveau jeu de données qui inclut des explications neutres des événements et des perspectives différentes de chaque pays. Nos résultats montrent une grande inclinaison politique locale et confirment que les modèles traitent de manière plus amicale la représentation narrative de certaines nations. De plus, un simple prompt de contrôle des biais peut réduire ces inclinaisons. Des expériences où les marqueurs des participants sont manipulés démontrent la sensibilité du modèle aux caractéristiques et confirment que les modèles peuvent amplifier ou reconnaître des perspectives inadéquates. Ce travail clairement démontre l'inclinaison narrative politique des pays dans les LLMs, doute sur l'effet des méthodes simples de contrôle des biais et fournit un cadre et un jeu de données pour futures recherches sur l'inclinaison politique locale.",
      "upvotes": 34,
      "discussionId": "6848fed042e4f9106973f31f",
      "projectPage": "https://airi-institute.github.io/geopolitical_llm_bias",
      "githubRepo": "https://github.com/AIRI-Institute/geopolitical_llm_bias",
      "ai_summary": "LLMs exhibit significant geopolitical biases in their interpretation of historical events, and simple debiasing methods have limited effectiveness; a novel dataset for further research is provided.",
      "ai_keywords": [
        "LLMs",
        "geopolitical biases",
        "historical events",
        "national narratives",
        "debiasing prompts"
      ]
    },
    "publishedAt": "2025-06-07T06:45:17.000Z",
    "title": "Geopolitical biases in LLMs: what are the \"good\" and the \"bad\" countries\n  according to contemporary language models",
    "summary": "This paper evaluates geopolitical biases in LLMs with respect to various\ncountries though an analysis of their interpretation of historical events with\nconflicting national perspectives (USA, UK, USSR, and China). We introduce a\nnovel dataset with neutral event descriptions and contrasting viewpoints from\ndifferent countries. Our findings show significant geopolitical biases, with\nmodels favoring specific national narratives. Additionally, simple debiasing\nprompts had a limited effect in reducing these biases. Experiments with\nmanipulated participant labels reveal models' sensitivity to attribution,\nsometimes amplifying biases or recognizing inconsistencies, especially with\nswapped labels. This work highlights national narrative biases in LLMs,\nchallenges the effectiveness of simple debiasing methods, and offers a\nframework and dataset for future geopolitical bias research.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.06751.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62bd6c6baaf1480f1aa2222e",
      "avatarUrl": "/avatars/fd92ae2986d435a47eb1e382ac11d8e0.svg",
      "fullname": "Mikhail Salnikov",
      "name": "msalnikov",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.08672",
      "authors": [
        {
          "_id": "684936e842e4f9106973f45e",
          "name": "Yang Liu",
          "hidden": false
        },
        {
          "_id": "684936e842e4f9106973f45f",
          "name": "Jiaqi Li",
          "hidden": false
        },
        {
          "_id": "684936e842e4f9106973f460",
          "user": {
            "_id": "63a95a6a7930fa8c7dd63d4e",
            "avatarUrl": "/avatars/d9d0420f7ddfe2f3a7e029fb05f1c89f.svg",
            "isPro": false,
            "fullname": "Zilong Zheng",
            "user": "zlzheng",
            "type": "user"
          },
          "name": "Zilong Zheng",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-11T07:57:29.119Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-10T10:31:21.000Z",
      "submittedOnDailyAt": "2025-06-11T07:49:21.042Z",
      "title": "Rul Resíguar: Inférence de Règles Basée sur un Règne Dynamiquement Sampling du Domaine",
      "submittedOnDailyBy": {
        "_id": "63a95a6a7930fa8c7dd63d4e",
        "avatarUrl": "/avatars/d9d0420f7ddfe2f3a7e029fb05f1c89f.svg",
        "isPro": false,
        "fullname": "Zilong Zheng",
        "user": "zlzheng",
        "type": "user"
      },
      "summary": "La réflexion basée sur les règles est reconnue comme l'un des problèmes fondamentaux de la logique, mais dans des applications réelles, l'inclination vers le format, la catégorie et la complexité des règles représente un défi strict. Selon des études récentes, les modèles logiques à grande échelle (LRMs) ont démontré un pouvoir logique impressionnant, et ce rendement a été considérablement amélioré par l'apprentissage par renforcement (RL). Cependant, la capacité des modèles logiques à petite échelle (SRMs) pour apprendre des réflexions basées sur les règles de forte généralisation dans diverses tâches et domaines est un problème qui suscite des débats. En réponse à ce défi, nous présentons une méthodologie simple et efficace pour la réflexion basée sur les règles par renforcement, appelée RuleReasoner. Ce méthode permet de faire des réflexions basées sur les règles dans différentes tâches modifiées par une approximation dynamique d'échantillonnage dans de nouveaux domaines. En particulier, RuleReasoner met à jour les poids d'échantillonnage dans différents domaines en fonction de récompenses historiques, ce qui permet d'étendre les domaines et d'implémenter un apprentissage en ligne flexible de RL. Cela élimine la nécessité de recettes d'apprentissage hybrides préalablement conçues par l'homme dans les méthodes existantes. Les évaluations expérimentales dans des cadres de référence de distribution interne (ID) et hors distribution (OOD) montrent que RuleReasoner dépasse considérablement les LRMs avancés (delta moyen de 4,1% sur 8 tâches ID, et 10,4% sur 3 tâches OOD). En particulier, notre approche montre une plus grande efficacité en termes de calcul par rapport aux méthodes dynamiques d'échantillonnage précédentes de RL.",
      "upvotes": 15,
      "discussionId": "684936e842e4f9106973f461",
      "githubRepo": "https://github.com/bigai-nlco/RuleReasoner",
      "ai_summary": "RuleReasoner enhances rule-based reasoning in small models through dynamic domain sampling, achieving superior performance and efficiency compared to large models.",
      "ai_keywords": [
        "reinforcement learning",
        "rule-based reasoning",
        "large reasoning models",
        "small reasoning models",
        "domain-aware dynamic sampling",
        "historical rewards",
        "in-distribution",
        "out-of-distribution",
        "computational efficiency"
      ]
    },
    "publishedAt": "2025-06-10T06:31:21.000Z",
    "title": "RuleReasoner: Reinforced Rule-based Reasoning via Domain-aware Dynamic\n  Sampling",
    "summary": "Rule-based reasoning has been acknowledged as one of the fundamental problems\nin reasoning, while deviations in rule formats, types, and complexity in\nreal-world applications pose severe challenges. Recent studies have shown that\nlarge reasoning models (LRMs) have remarkable reasoning capabilities, and their\nperformance is substantially enhanced by reinforcement learning (RL). However,\nit remains an open question whether small reasoning models (SRMs) can learn\nrule-based reasoning effectively with robust generalization across diverse\ntasks and domains. To address this, we introduce Reinforced Rule-based\nReasoning, a.k.a. RuleReasoner, a simple yet effective method to conduct\nrule-based reasoning via a wide collection of curated tasks and a novel\ndomain-aware dynamic sampling approach. Specifically, RuleReasoner resamples\neach training batch by updating the sampling weights of different domains based\non historical rewards. This facilitates domain augmentation and flexible online\nlearning schedules for RL, obviating the need for pre-hoc human-engineered\nmix-training recipes used in existing methods. Empirical evaluations on\nin-distribution (ID) and out-of-distribution (OOD) benchmarks reveal that\nRuleReasoner outperforms frontier LRMs by a significant margin (Delta4.1%\naverage points on eight ID tasks and Delta10.4% average points on three OOD\ntasks over OpenAI-o1). Notably, our approach also exhibits higher computational\nefficiency compared to prior dynamic sampling methods for RL.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08672.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a95a6a7930fa8c7dd63d4e",
      "avatarUrl": "/avatars/d9d0420f7ddfe2f3a7e029fb05f1c89f.svg",
      "fullname": "Zilong Zheng",
      "name": "zlzheng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.09040",
      "authors": [
        {
          "_id": "6848fff842e4f9106973f321",
          "name": "Dianyi Wang",
          "hidden": false
        },
        {
          "_id": "6848fff842e4f9106973f322",
          "name": "Wei Song",
          "hidden": false
        },
        {
          "_id": "6848fff842e4f9106973f323",
          "name": "Yikun Wang",
          "hidden": false
        },
        {
          "_id": "6848fff842e4f9106973f324",
          "name": "Siyuan Wang",
          "hidden": false
        },
        {
          "_id": "6848fff842e4f9106973f325",
          "name": "Kaicheng Yu",
          "hidden": false
        },
        {
          "_id": "6848fff842e4f9106973f326",
          "name": "Zhongyu Wei",
          "hidden": false
        },
        {
          "_id": "6848fff842e4f9106973f327",
          "name": "Jiaqi Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-10T17:57:50.000Z",
      "submittedOnDailyAt": "2025-06-11T05:55:58.221Z",
      "title": "La reconstruction visuelle significative de la régression automatique aide à améliorer la compréhension des VLMs.",
      "submittedOnDailyBy": {
        "_id": "64b4eec4faa3181a5eab9c46",
        "avatarUrl": "/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg",
        "isPro": true,
        "fullname": "Jiaqi Wang",
        "user": "myownskyW7",
        "type": "user"
      },
      "summary": "Les modèles grands de langage visuel standard (LVLMs) appliquent seulement un support pour la reconstruction automatique des séquences de texte et ne intègrent pas complètement le modèle visuel dans le processus d'entraînement. Cela génère trois limitations principales : 1) la capacité d'utilisation d'images n'existe pas lorsqu'elles ne sont pas capturées, 2) le risque d'omission de détails visuels importants lors de la capture, et 3) que le contenu visuel puisse être plus précisément transmis en langage est un défi. Par conséquent, les LVLMs actuels priorisent l'harmonisation du langage à partir de la vision, mais ont la possibilité de laisser de côté des informations visuelles détaillées. En contraste, les études précédentes ont investigué la génération d'images par reconstruction automatique et ont développé des défis pour améliorer la compréhension des images en utilisant efficacement le support de reconstruction visuel. Dans cet article, on présente la reconstruction automatique de langage et de vision (ASVR) et on décrit comment les modèles de vision et de langage peuvent être entraînés dans un seul cadre de reconstruction automatique. La reconstruction automatique de l'apparence originale de l'image ne améliore pas la compréhension de plusieurs types d'images, mais la reconstruction automatique de la représentation visuelle améliore la compréhension. Spécifiquement, lorsque le modèle reçoit des caractéristiques continues d'images en tant que entrée, il peut reconstruire efficacement ces caractéristiques sous forme de tokens de sens dispersés, ce qui a été confirmé par des améliorations stables dans différents cadres d'évaluation. Notre approche obtient un grand améliorament du rendement, indépendamment du taille des données (556k-2M) et de la variété des modèles de langage appelés à service (LLM). En particulier, ASVR améliore en moyenne de 5% sur 14 cadres d'évaluation par rapport à LLaVA-1.5. Le code est disponible sur https://github.com/AlenjandroWang/ASVR.",
      "upvotes": 12,
      "discussionId": "6848fff842e4f9106973f328",
      "ai_summary": "Autoregressive Semantic Visual Reconstruction (ASVR) improves multimodal understanding by focusing on semantic reconstruction rather than raw visual appearance, enhancing performance across various benchmarks.",
      "ai_keywords": [
        "autoregressive supervision",
        "large vision-language models (LVLMs)",
        "visual modality",
        "image captions",
        "autoregressive image generation",
        "multimodal learning",
        "semantic representation",
        "discrete semantic tokens",
        "multimodal understanding benchmarks",
        "LLaVA-1.5"
      ]
    },
    "publishedAt": "2025-06-10T13:57:50.000Z",
    "title": "Autoregressive Semantic Visual Reconstruction Helps VLMs Understand\n  Better",
    "summary": "Typical large vision-language models (LVLMs) apply autoregressive supervision\nsolely to textual sequences, without fully incorporating the visual modality\ninto the learning process. This results in three key limitations: (1) an\ninability to utilize images without accompanying captions, (2) the risk that\ncaptions omit critical visual details, and (3) the challenge that certain\nvision-centric content cannot be adequately conveyed through text. As a result,\ncurrent LVLMs often prioritize vision-to-language alignment while potentially\noverlooking fine-grained visual information. While some prior works have\nexplored autoregressive image generation, effectively leveraging autoregressive\nvisual supervision to enhance image understanding remains an open challenge. In\nthis paper, we introduce Autoregressive Semantic Visual Reconstruction (ASVR),\nwhich enables joint learning of visual and textual modalities within a unified\nautoregressive framework. We show that autoregressively reconstructing the raw\nvisual appearance of images does not enhance and may even impair multimodal\nunderstanding. In contrast, autoregressively reconstructing the semantic\nrepresentation of images consistently improves comprehension. Notably, we find\nthat even when models are given continuous image features as input, they can\neffectively reconstruct discrete semantic tokens, resulting in stable and\nconsistent improvements across a wide range of multimodal understanding\nbenchmarks. Our approach delivers significant performance gains across varying\ndata scales (556k-2M) and types of LLM bacbones. Specifically, ASVR improves\nLLaVA-1.5 by 5% in average scores across 14 multimodal benchmarks. The code is\navailable at https://github.com/AlenjandroWang/ASVR.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09040.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b4eec4faa3181a5eab9c46",
      "avatarUrl": "/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg",
      "fullname": "Jiaqi Wang",
      "name": "myownskyW7",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 20
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.08009",
      "authors": [
        {
          "_id": "68485e5b4fe3b60e21b258bd",
          "name": "Xun Huang",
          "hidden": false
        },
        {
          "_id": "68485e5b4fe3b60e21b258be",
          "name": "Zhengqi Li",
          "hidden": false
        },
        {
          "_id": "68485e5b4fe3b60e21b258bf",
          "user": {
            "_id": "67492ee82ad3cfc108a41bbb",
            "avatarUrl": "/avatars/7ad03e55a8791c62f1271a5c9bf8cc60.svg",
            "isPro": false,
            "fullname": "Guande He",
            "user": "gdhe17",
            "type": "user"
          },
          "name": "Guande He",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-11T08:35:40.950Z",
          "hidden": false
        },
        {
          "_id": "68485e5b4fe3b60e21b258c0",
          "name": "Mingyuan Zhou",
          "hidden": false
        },
        {
          "_id": "68485e5b4fe3b60e21b258c1",
          "name": "Eli Shechtman",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/67492ee82ad3cfc108a41bbb/bEQLc--MCz7a-4ZBIBbaJ.mp4"
      ],
      "publishedAt": "2025-06-09T17:59:55.000Z",
      "submittedOnDailyAt": "2025-06-11T04:34:32.742Z",
      "title": "Auto-Reconstructing Video Diffusers Through the Lens of Overfitting Test-to-Test Wrapping",
      "submittedOnDailyBy": {
        "_id": "67492ee82ad3cfc108a41bbb",
        "avatarUrl": "/avatars/7ad03e55a8791c62f1271a5c9bf8cc60.svg",
        "isPro": false,
        "fullname": "Guande He",
        "user": "gdhe17",
        "type": "user"
      },
      "summary": "Self Forcing introduit un nouveau paradigme d'entraînement pour des modèles de diffusion vidéo qui génèrent automatiquement le prochain frame. Cette approche résout le problème de la génération de séquences basées sur son propre output incomplet pendant l'inférence et aborde les problèmes d'entraînement qui durent longtemps en raison du contexte réel. En comparaison avec les méthodes précédentes, au lieu de débruiter les frames futures basées sur des contextes réels, Self Forcing utilise un cache de KV pour exécuter des rollouts automatiques pendant l'entraînement et génère chaque frame en se basant sur l'output généré dans le passé. Cette stratégie permet de contrôler l'entraînement au niveau vidéo à travers une perte globale et d'évaluer directement la qualité des séquences générées. Pour garantir l'efficacité de l'entraînement, des modèles de diffusion avec peu de pas et la trimation de tokens de random gandia en espagnol sont utilisés, ce qui équilibre effectivement les coûts computationnels et l'efficacité. De plus, pour exécuter des rollouts automatiques de manière efficace, une structure de cache KV est introduite. A travers des expériences étendues, notre approche a permis la génération de vidéos en temps réel, satisfaisant la qualité de génération en secondes de latin sur une seule carte de processeur, sans amélioration de la vitesse et en dépassant la qualité de génération des modèles de diffusion de base. Site web du projet : http://self-forcing.github.io/",
      "upvotes": 9,
      "discussionId": "68485e5b4fe3b60e21b258c2",
      "projectPage": "https://self-forcing.github.io/",
      "githubRepo": "https://github.com/guandeh17/Self-Forcing",
      "ai_summary": "Self Forcing, a novel training method for autoregressive video diffusion models, reduces exposure bias and improves generation quality through holistic video-level supervision and efficient caching mechanisms.",
      "ai_keywords": [
        "Self Forcing",
        "autoregressive video diffusion models",
        "exposure bias",
        "denoising",
        "key-value (KV) caching",
        "autoregressive rollout",
        "holistic loss",
        "few-step diffusion model",
        "stochastic gradient truncation",
        "rolling KV cache mechanism",
        "video extrapolation"
      ]
    },
    "publishedAt": "2025-06-09T13:59:55.000Z",
    "title": "Self Forcing: Bridging the Train-Test Gap in Autoregressive Video\n  Diffusion",
    "summary": "We introduce Self Forcing, a novel training paradigm for autoregressive video\ndiffusion models. It addresses the longstanding issue of exposure bias, where\nmodels trained on ground-truth context must generate sequences conditioned on\ntheir own imperfect outputs during inference. Unlike prior methods that denoise\nfuture frames based on ground-truth context frames, Self Forcing conditions\neach frame's generation on previously self-generated outputs by performing\nautoregressive rollout with key-value (KV) caching during training. This\nstrategy enables supervision through a holistic loss at the video level that\ndirectly evaluates the quality of the entire generated sequence, rather than\nrelying solely on traditional frame-wise objectives. To ensure training\nefficiency, we employ a few-step diffusion model along with a stochastic\ngradient truncation strategy, effectively balancing computational cost and\nperformance. We further introduce a rolling KV cache mechanism that enables\nefficient autoregressive video extrapolation. Extensive experiments demonstrate\nthat our approach achieves real-time streaming video generation with sub-second\nlatency on a single GPU, while matching or even surpassing the generation\nquality of significantly slower and non-causal diffusion models. Project\nwebsite: http://self-forcing.github.io/",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/67492ee82ad3cfc108a41bbb/bEQLc--MCz7a-4ZBIBbaJ.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08009.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67492ee82ad3cfc108a41bbb",
      "avatarUrl": "/avatars/7ad03e55a8791c62f1271a5c9bf8cc60.svg",
      "fullname": "Guande He",
      "name": "gdhe17",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.07927",
      "authors": [
        {
          "_id": "684794003ec10bdd8ab4de11",
          "name": "Jiayi Sheng",
          "hidden": false
        },
        {
          "_id": "684794003ec10bdd8ab4de12",
          "name": "Luna Lyu",
          "hidden": false
        },
        {
          "_id": "684794003ec10bdd8ab4de13",
          "name": "Jikai Jin",
          "hidden": false
        },
        {
          "_id": "684794003ec10bdd8ab4de14",
          "name": "Tony Xia",
          "hidden": false
        },
        {
          "_id": "684794003ec10bdd8ab4de15",
          "name": "Alex Gu",
          "hidden": false
        },
        {
          "_id": "684794003ec10bdd8ab4de16",
          "name": "James Zou",
          "hidden": false
        },
        {
          "_id": "684794003ec10bdd8ab4de17",
          "name": "Pan Lu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/60f5f68fa7fd83d025749234/ahvR-ZmwDrUNm3-jcQ4o1.png"
      ],
      "publishedAt": "2025-06-09T16:43:38.000Z",
      "submittedOnDailyAt": "2025-06-11T04:15:25.994Z",
      "title": "Méthodologie de Démonstration d'Inégalités en Utilisant des Modèles de Langage de Grande Échelle",
      "submittedOnDailyBy": {
        "_id": "60f5f68fa7fd83d025749234",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60f5f68fa7fd83d025749234/gCeJAZfzaANAcEvI6v5-P.jpeg",
        "isPro": true,
        "fullname": "Pan Lu",
        "user": "lupantech",
        "type": "user"
      },
      "summary": "La preu des inégalités est une zone importante dans plusieurs domaines scientifiques et mathématiques, qui évalue la capacité d'inférence haute à travers la détection de frontières strictes et l'application de théorèmes stratégiques. Par conséquent, les modèles de langage de grande taille (LLMs) offrent des profondeurs de compréhension accrues qui résolvent des problèmes mathématiques généraux, surtout dans la configuration de frontières difficiles. L'élaboration de cette zone est limitée par des ensembles de données rares, synthétiques ou avec des structures strictes. Pour résoudre ce problème, nous proposons la formation de tâches non formelles mais probablement, reconstruisant la preu des inégalités en deux sous-tâches vérifiables automatiquement : l'estimation des frontières et la prédiction des relations. En nous basant sur cela, nous lançons le jeu de données \"IneqMath\", un jeu de données de haut niveau d'Olympiade de inégalités personnalisé. Ce jeu de données comprend des ensembles de test et des corpus d'apprentissage avec des solutions pas à pas et des résumés. De plus, nous avons développé un nouveau cadre d'évaluation \"LLM-as-judge\" qui combine un juge de réponses finales avec un juge de quatre étapes pour détecter des erreurs dans l'inférence générale. Les résultats systématiques de 29 modèles avancés de LLM dans \"IneqMath\" révèlent des faits surprisants : par exemple, des modèles comme o1 atteignent une précision générale inférieure à 10% en raison de la révision pas à pas, ce qui représente une perte de 65,5% en précision comparative avec la cohérence de la réponse finale. Cette différence met en évidence une lacune cruciale dans les modèles de LLM pour construire des tests rigoureux. L'expansion du taille du modèle et l'augmentation de la quantité de calcul dans les tests ont des limites uniquement en termes de précision de la preu globale, sans effet significatif. En revanche, notre approche de recherche montre la possibilité d'inférence guidée par théorèmes et auto-revision. Les codes et les données sont disponibles sur https://ineqmath.github.io/.",
      "upvotes": 9,
      "discussionId": "684794013ec10bdd8ab4de18",
      "ai_summary": "The investigation into inequality proving using large language models uncovers significant challenges in constructing rigorous proofs, revealing gaps between finding answers and generating valid step-wise solutions.",
      "ai_keywords": [
        "LLMs",
        "IneqMath",
        "bound estimation",
        "relation prediction",
        "theorem-guided reasoning",
        "self-refinement"
      ]
    },
    "publishedAt": "2025-06-09T12:43:38.000Z",
    "title": "Solving Inequality Proofs with Large Language Models",
    "summary": "Inequality proving, crucial across diverse scientific and mathematical\nfields, tests advanced reasoning skills such as discovering tight bounds and\nstrategic theorem application. This makes it a distinct, demanding frontier for\nlarge language models (LLMs), offering insights beyond general mathematical\nproblem-solving. Progress in this area is hampered by existing datasets that\nare often scarce, synthetic, or rigidly formal. We address this by proposing an\ninformal yet verifiable task formulation, recasting inequality proving into two\nautomatically checkable subtasks: bound estimation and relation prediction.\nBuilding on this, we release IneqMath, an expert-curated dataset of\nOlympiad-level inequalities, including a test set and training corpus enriched\nwith step-wise solutions and theorem annotations. We also develop a novel\nLLM-as-judge evaluation framework, combining a final-answer judge with four\nstep-wise judges designed to detect common reasoning flaws. A systematic\nevaluation of 29 leading LLMs on IneqMath reveals a surprising reality: even\ntop models like o1 achieve less than 10% overall accuracy under step-wise\nscrutiny; this is a drop of up to 65.5% from their accuracy considering only\nfinal answer equivalence. This discrepancy exposes fragile deductive chains and\na critical gap for current LLMs between merely finding an answer and\nconstructing a rigorous proof. Scaling model size and increasing test-time\ncomputation yield limited gains in overall proof correctness. Instead, our\nfindings highlight promising research directions such as theorem-guided\nreasoning and self-refinement. Code and data are available at\nhttps://ineqmath.github.io/.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/60f5f68fa7fd83d025749234/ahvR-ZmwDrUNm3-jcQ4o1.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07927.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f5f68fa7fd83d025749234",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60f5f68fa7fd83d025749234/gCeJAZfzaANAcEvI6v5-P.jpeg",
      "fullname": "Pan Lu",
      "name": "lupantech",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.04614",
      "authors": [
        {
          "_id": "684921e342e4f9106973f3e7",
          "name": "Yuyang Wanyan",
          "hidden": false
        },
        {
          "_id": "684921e342e4f9106973f3e8",
          "name": "Xi Zhang",
          "hidden": false
        },
        {
          "_id": "684921e342e4f9106973f3e9",
          "name": "Haiyang Xu",
          "hidden": false
        },
        {
          "_id": "684921e342e4f9106973f3ea",
          "name": "Haowei Liu",
          "hidden": false
        },
        {
          "_id": "684921e342e4f9106973f3eb",
          "name": "Junyang Wang",
          "hidden": false
        },
        {
          "_id": "684921e342e4f9106973f3ec",
          "name": "Jiabo Ye",
          "hidden": false
        },
        {
          "_id": "684921e342e4f9106973f3ed",
          "name": "Yutong Kou",
          "hidden": false
        },
        {
          "_id": "684921e342e4f9106973f3ee",
          "name": "Ming Yan",
          "hidden": false
        },
        {
          "_id": "684921e342e4f9106973f3ef",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "684921e342e4f9106973f3f0",
          "name": "Xiaoshan Yang",
          "hidden": false
        },
        {
          "_id": "684921e342e4f9106973f3f1",
          "name": "Weiming Dong",
          "hidden": false
        },
        {
          "_id": "684921e342e4f9106973f3f2",
          "name": "Changsheng Xu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/645b10e80c73ea27d13f7aca/u6BK1EJr5-c6EUSfCkYqH.jpeg",
        "https://cdn-uploads.huggingface.co/production/uploads/645b10e80c73ea27d13f7aca/ySTNcOWpa_W2ZpbmkS50q.jpeg"
      ],
      "publishedAt": "2025-06-05T04:12:36.000Z",
      "submittedOnDailyAt": "2025-06-11T04:59:22.770Z",
      "title": "「Ce que le joueur peut voir lors de son jeu : Diagnostic des erreurs précédentes à la chirurgie de l'automatisation de la GUI par le modèle GUI-Critic-R1」",
      "submittedOnDailyBy": {
        "_id": "645b10e80c73ea27d13f7aca",
        "avatarUrl": "/avatars/95e565306472a15067440b5b43e07a6f.svg",
        "isPro": false,
        "fullname": "xuhaiyang",
        "user": "xhyandwyy",
        "type": "user"
      },
      "summary": "Récemment, les modèles de langage multimodal (MLLMs) sont largement utilisés dans des tâches d'automatisation de l'interface graphique (GUI) et d'autres tâches logiques. Au contraire des tâches communes des modèles en ligne, l'automatisation de la GUI se fait dans des environnements interactifs en ligne, où les décisions sont prises de manière séquentielle en fonction de l'état en temps réel de l'environnement. Cette tâche a une faible tolérance aux erreurs à chaque étape, car certaines erreurs peuvent détruire constamment le processus, et il existe la possibilité de obtenir des résultats non modifiables comme l'élimination ou le paiement, ce qui introduit une structure d'évaluation préliminaire qui fournit une rétroaction valide avant l'exécution réelle. En particulier, on propose la construction d'un modèle d'évaluation de l'interface graphique (GUI-Critic-R1) qui considère les résultats potentiels, basé sur les propositions suggérées, et on propose une stratégie d'optimisation de politiques relatives (S-GRPO). De plus, on cherche à augmenter la confiance dans la rétroaction du modèle avec la proposition de nouvelles récompenses. De plus, on développe un pipeline de collecte de données basé sur l'évidence pour résoudre les défauts existants dans les données d'évaluation de la GUI, et on crée GUI-Critic-Train et GUI-Critic-Test. Les tests statiques dans le domaine des mobiles et web dans GUI-Critic-Test montrent une grande avantage en termes de précision d'évaluation par rapport aux MLLMs actuels. L'évaluation dynamique dans le cadre de référence d'automatisation de la GUI démontre une amélioration de la taux de succès et de l'efficacité de fonctionnement, ce qui démontre clairement l'efficacité et le très bon rendement du modèle.",
      "upvotes": 9,
      "discussionId": "684921e442e4f9106973f3f3",
      "githubRepo": "https://github.com/X-PLUG/MobileAgent",
      "ai_summary": "A pre-operative critic mechanism with Suggestion-aware Gradient Relative Policy Optimization enhances the reliability of multimodal reasoning tasks in GUI automation.",
      "ai_keywords": [
        "Multimodal Large Language Models",
        "Suggestion-aware Gradient Relative Policy Optimization",
        "pre-operative critic mechanism",
        "reasoning-bootstrapping",
        "GUI automation",
        "GUI-Critic-R1",
        "GUI-Critic-Test",
        "GUI-Critic-Train"
      ]
    },
    "publishedAt": "2025-06-05T00:12:36.000Z",
    "title": "Look Before You Leap: A GUI-Critic-R1 Model for Pre-Operative Error\n  Diagnosis in GUI Automation",
    "summary": "In recent years, Multimodal Large Language Models (MLLMs) have been\nextensively utilized for multimodal reasoning tasks, including Graphical User\nInterface (GUI) automation. Unlike general offline multimodal tasks, GUI\nautomation is executed in online interactive environments, necessitating\nstep-by-step decision-making based on real-time status of the environment. This\ntask has a lower tolerance for decision-making errors at each step, as any\nmistakes may cumulatively disrupt the process and potentially lead to\nirreversible outcomes like deletions or payments. To address these issues, we\nintroduce a pre-operative critic mechanism that provides effective feedback\nprior to the actual execution, by reasoning about the potential outcome and\ncorrectness of actions. Specifically, we propose a Suggestion-aware Gradient\nRelative Policy Optimization (S-GRPO) strategy to construct our pre-operative\ncritic model GUI-Critic-R1, incorporating a novel suggestion reward to enhance\nthe reliability of the model's feedback. Furthermore, we develop a\nreasoning-bootstrapping based data collection pipeline to create a\nGUI-Critic-Train and a GUI-Critic-Test, filling existing gaps in GUI critic\ndata. Static experiments on the GUI-Critic-Test across both mobile and web\ndomains reveal that our GUI-Critic-R1 offers significant advantages in critic\naccuracy compared to current MLLMs. Dynamic evaluation on GUI automation\nbenchmark further highlights the effectiveness and superiority of our model, as\nevidenced by improved success rates and operational efficiency.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/645b10e80c73ea27d13f7aca/u6BK1EJr5-c6EUSfCkYqH.jpeg",
      "https://cdn-uploads.huggingface.co/production/uploads/645b10e80c73ea27d13f7aca/ySTNcOWpa_W2ZpbmkS50q.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04614.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645b10e80c73ea27d13f7aca",
      "avatarUrl": "/avatars/95e565306472a15067440b5b43e07a6f.svg",
      "fullname": "xuhaiyang",
      "name": "xhyandwyy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.08002",
      "authors": [
        {
          "_id": "6848f8c242e4f9106973f2f6",
          "name": "Aadarsh Sahoo",
          "hidden": false
        },
        {
          "_id": "6848f8c242e4f9106973f2f7",
          "name": "Vansh Tibrewal",
          "hidden": false
        },
        {
          "_id": "6848f8c242e4f9106973f2f8",
          "name": "Georgia Gkioxari",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/638e5fc6485360fbdfeb1301/2iX6eNaXCKBiwTpAmSk2Z.qt",
        "https://cdn-uploads.huggingface.co/production/uploads/638e5fc6485360fbdfeb1301/EUswqANZ-bRURwRZ0sv3m.qt",
        "https://cdn-uploads.huggingface.co/production/uploads/638e5fc6485360fbdfeb1301/dLiQzQHFyFye4cIgj3ivo.png"
      ],
      "publishedAt": "2025-06-09T17:59:37.000Z",
      "submittedOnDailyAt": "2025-06-11T02:05:41.608Z",
      "title": "Texte, images, structures 3D correspondent token à token.",
      "submittedOnDailyBy": {
        "_id": "638e5fc6485360fbdfeb1301",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638e5fc6485360fbdfeb1301/_8oqLMbkn_Ig-Jqa0fZCJ.png",
        "isPro": false,
        "fullname": "Aadarsh Sahoo",
        "user": "aadarsh99",
        "type": "user"
      },
      "summary": "Créer des machines capables de comprendre la réalité 3D est une tâche importante qui aide les concepteurs à gérer des environnements et des éditions en 3D. Avec le développement de modèles de langage et de modélisation d'images, il est possible d'explorer la possibilité de nouveaux modèles structurés de scènes 3D. Pour cela, on propose un cadre de travail de LLM qui intègre le langage, les images et les scènes 3D, et offre une guidance claire sur les choix de conception pour atteindre le meilleur entraînement et rendement sur des thèmes tels que la représentation des données, la modélisation d'objectifs spécifiques et d'autres questions importantes. On évalue le rendement sur quatre tâches clés de 3D (renderisation, reconnaissance, suivi d'instructions, réponse à des questions) et quatre ensembles de données de 3D (données synthétiques et de la réalité). On montre l'effet du modèle sur la tâche de reconnaissance d'objets 3D en réalité en ajoutant une codification de formes positives. Page du projet : https://glab-caltech.github.io/kyvo/",
      "upvotes": 8,
      "discussionId": "6848f8c242e4f9106973f2f9",
      "projectPage": "https://glab-caltech.github.io/kyvo/",
      "githubRepo": "https://github.com/AadSah/kyvo",
      "ai_summary": "A unified language, image, and 3D scene model framework is proposed, achieving optimal training and performance across various 3D tasks and datasets.",
      "ai_keywords": [
        "autoregressive models",
        "LLM framework",
        "data representation",
        "modality-specific objectives",
        "3D rendering",
        "3D recognition",
        "instruction-following",
        "question-answering",
        "3D datasets",
        "quantized shape encodings"
      ]
    },
    "publishedAt": "2025-06-09T13:59:37.000Z",
    "title": "Aligning Text, Images, and 3D Structure Token-by-Token",
    "summary": "Creating machines capable of understanding the world in 3D is essential in\nassisting designers that build and edit 3D environments and robots navigating\nand interacting within a three-dimensional space. Inspired by advances in\nlanguage and image modeling, we investigate the potential of autoregressive\nmodels for a new modality: structured 3D scenes. To this end, we propose a\nunified LLM framework that aligns language, images, and 3D scenes and provide a\ndetailed ''cookbook'' outlining critical design choices for achieving optimal\ntraining and performance addressing key questions related to data\nrepresentation, modality-specific objectives, and more. We evaluate performance\nacross four core 3D tasks -- rendering, recognition, instruction-following, and\nquestion-answering -- and four 3D datasets, synthetic and real-world. We extend\nour approach to reconstruct complex 3D object shapes by enriching our 3D\nmodality with quantized shape encodings, and show our model's effectiveness on\nreal-world 3D object recognition tasks. Project webpage:\nhttps://glab-caltech.github.io/kyvo/",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/638e5fc6485360fbdfeb1301/2iX6eNaXCKBiwTpAmSk2Z.qt",
      "https://cdn-uploads.huggingface.co/production/uploads/638e5fc6485360fbdfeb1301/EUswqANZ-bRURwRZ0sv3m.qt",
      "https://cdn-uploads.huggingface.co/production/uploads/638e5fc6485360fbdfeb1301/dLiQzQHFyFye4cIgj3ivo.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08002.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "638e5fc6485360fbdfeb1301",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638e5fc6485360fbdfeb1301/_8oqLMbkn_Ig-Jqa0fZCJ.png",
      "fullname": "Aadarsh Sahoo",
      "name": "aadarsh99",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.07177",
      "authors": [
        {
          "_id": "6849036342e4f9106973f32a",
          "name": "Sangwon Jang",
          "hidden": false
        },
        {
          "_id": "6849036342e4f9106973f32b",
          "user": {
            "_id": "66b57c77778c98d29446c8ec",
            "avatarUrl": "/avatars/63a7da38ee3808858f0f786a3a4a8dae.svg",
            "isPro": false,
            "fullname": "Taekyung Ki",
            "user": "tkkitkki",
            "type": "user"
          },
          "name": "Taekyung Ki",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-11T08:34:42.228Z",
          "hidden": false
        },
        {
          "_id": "6849036342e4f9106973f32c",
          "name": "Jaehyeong Jo",
          "hidden": false
        },
        {
          "_id": "6849036342e4f9106973f32d",
          "user": {
            "_id": "652066649004117947e46ed6",
            "avatarUrl": "/avatars/972c97df6f26d2c3d6ce71ec579984bb.svg",
            "isPro": false,
            "fullname": "Jaehong Yoon",
            "user": "jaehong31",
            "type": "user"
          },
          "name": "Jaehong Yoon",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-11T08:34:44.146Z",
          "hidden": false
        },
        {
          "_id": "6849036342e4f9106973f32e",
          "name": "Soo Ye Kim",
          "hidden": false
        },
        {
          "_id": "6849036342e4f9106973f32f",
          "name": "Zhe Lin",
          "hidden": false
        },
        {
          "_id": "6849036342e4f9106973f330",
          "name": "Sung Ju Hwang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63bbf972d8d676a2299cdb44/n94aSArXRHEapWS5MwzkR.mp4"
      ],
      "publishedAt": "2025-06-08T14:54:41.000Z",
      "submittedOnDailyAt": "2025-06-11T02:49:49.936Z",
      "title": "Guide des marqueurs : Guide de distribution de modèles pour le contrôle du niveau de marqueurs de vidéo sans apprentissage.",
      "submittedOnDailyBy": {
        "_id": "63bbf972d8d676a2299cdb44",
        "avatarUrl": "/avatars/cd038f11dc1007b1267324b34c165dda.svg",
        "isPro": false,
        "fullname": "Sangwon",
        "user": "agwmon",
        "type": "user"
      },
      "summary": "Le développement de modèles d'expansion a noté un grand amélioration de la qualité des images et la possibilité d'un contrôle très précis. Cependant, actuellement, de nombreux méthodes dépendent du fine-tuning de modèles d'images de grande taille pour des tâches spécifiques, mais cette approche n'est pas efficace lorsque la taille du modèle continue d'augmenter. Dans cet article, nous proposons \"Frame Guidance\", une orientation non entraînée basée sur des signaux de niveau de frame (par exemple, frame clé, image de style, schéma ou carte de profondeur, etc.) pour la génération d'images contrôlables. Pour mettre en œuvre une orientation non entraînée efficace, nous proposons une simple technique de traitement de variables potentielles qui réduit significativement l'utilisation de la mémoire, et nous appliquons une nouvelle stratégie d'optimisation de variables potentielles conçue pour la génération d'images cohérentes. Frame Guidance permet un contrôle efficace dans diverses tâches comme guider avec un frame clé, styliser ou effectuer des boucles, et générer des images de haute qualité sans nécessiter d'entraînement, et est compatible avec tout modèle d'images. Les résultats expérimentaux montrent que Frame Guidance peut générer des images de haute qualité avec un contrôle efficace dans une large variété de tâches et de signaux d'entrée.",
      "upvotes": 8,
      "discussionId": "6849036342e4f9106973f331",
      "ai_summary": "Frame Guidance offers a training-free method for controlling video generation using frame-level signals, reducing memory usage and enhancing globally coherent video output.",
      "ai_keywords": [
        "diffusion models",
        "frame-level signals",
        "keyframes",
        "style reference images",
        "sketches",
        "depth maps",
        "latent processing",
        "latent optimization",
        "globally coherent video generation",
        "video models",
        "keyframe guidance",
        "stylization",
        "looping"
      ]
    },
    "publishedAt": "2025-06-08T10:54:41.000Z",
    "title": "Frame Guidance: Training-Free Guidance for Frame-Level Control in Video\n  Diffusion Models",
    "summary": "Advancements in diffusion models have significantly improved video quality,\ndirecting attention to fine-grained controllability. However, many existing\nmethods depend on fine-tuning large-scale video models for specific tasks,\nwhich becomes increasingly impractical as model sizes continue to grow. In this\nwork, we present Frame Guidance, a training-free guidance for controllable\nvideo generation based on frame-level signals, such as keyframes, style\nreference images, sketches, or depth maps. For practical training-free\nguidance, we propose a simple latent processing method that dramatically\nreduces memory usage, and apply a novel latent optimization strategy designed\nfor globally coherent video generation. Frame Guidance enables effective\ncontrol across diverse tasks, including keyframe guidance, stylization, and\nlooping, without any training, compatible with any video models. Experimental\nresults show that Frame Guidance can produce high-quality controlled videos for\na wide range of tasks and input signals.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63bbf972d8d676a2299cdb44/n94aSArXRHEapWS5MwzkR.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07177.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63bbf972d8d676a2299cdb44",
      "avatarUrl": "/avatars/cd038f11dc1007b1267324b34c165dda.svg",
      "fullname": "Sangwon",
      "name": "agwmon",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.05167",
      "authors": [
        {
          "_id": "68468cb23ec10bdd8ab4db5b",
          "user": {
            "_id": "645aedd221ab438e732bff43",
            "avatarUrl": "/avatars/31e14fee670fd5ddd296ea0249dbf710.svg",
            "isPro": false,
            "fullname": "Yeonseok Jeong",
            "user": "yeonseokjeong",
            "type": "user"
          },
          "name": "Yeonseok Jeong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-09T10:11:11.745Z",
          "hidden": false
        },
        {
          "_id": "68468cb23ec10bdd8ab4db5c",
          "name": "Jinsu Kim",
          "hidden": false
        },
        {
          "_id": "68468cb23ec10bdd8ab4db5d",
          "name": "Dohyeon Lee",
          "hidden": false
        },
        {
          "_id": "68468cb23ec10bdd8ab4db5e",
          "name": "Seung-won Hwang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-05T15:43:49.000Z",
      "submittedOnDailyAt": "2025-06-11T00:47:14.627Z",
      "title": "ECoRAG : Guide des Evidences pour Comprimer le Contexte Long de RAG",
      "submittedOnDailyBy": {
        "_id": "645aedd221ab438e732bff43",
        "avatarUrl": "/avatars/31e14fee670fd5ddd296ea0249dbf710.svg",
        "isPro": false,
        "fullname": "Yeonseok Jeong",
        "user": "yeonseokjeong",
        "type": "user"
      },
      "summary": "Les modèles de langage grand (LLMs) montrent un comportement impressionnant dans la réponse à des questions dans des domaines ouverts (ODQA) en utilisant des documents externes et la technologie de Rechius Augur Decision (RAG). Pour réduire l'overhead de RAG, il est nécessaire de réduire le contexte long en configuré le contexte. Cependant, les méthodes de configuration récentes ne peuvent pas filtrer des informations non probées, ce qui limite le rendement de RAG basé sur des LLMs. Par conséquent, nous proposons le cadre de référence RAG basé sur les tests, ECoRAG. ECoRAG configure les documents trouvés en se basant sur les tests et vérifie si la génération de la réponse est soutenue par des tests probés. En un pas supplémentaire, ECoRAG vérifie si les tests fournissent des contenus suffisamment compressés et, si non, effectue une recherche supplémentaire. Les expériences montrent que ECoRAG améliore le rendement des LLMs dans ODQA, dépasse les méthodes de configuration existantes et minimise la quantité de tokens utilisés, réduisant ainsi le coût de la mémoire. Le code est disponible sur https://github.com/ldilab/ECoRAG.",
      "upvotes": 6,
      "discussionId": "68468cb23ec10bdd8ab4db5f",
      "githubRepo": "https://github.com/ldilab/ECoRAG",
      "ai_summary": "ECoRAG framework enhances LLM performance in ODQA by compressing retrieved documents based on evidentiality, reducing latency and token usage.",
      "ai_keywords": [
        "Retrieval-Augmented Generation (RAG)",
        "context compression",
        "evidentiality",
        "LLM",
        "Open-Domain Question Answering (ODQA)"
      ]
    },
    "publishedAt": "2025-06-05T11:43:49.000Z",
    "title": "ECoRAG: Evidentiality-guided Compression for Long Context RAG",
    "summary": "Large Language Models (LLMs) have shown remarkable performance in Open-Domain\nQuestion Answering (ODQA) by leveraging external documents through\nRetrieval-Augmented Generation (RAG). To reduce RAG overhead, from longer\ncontext, context compression is necessary. However, prior compression methods\ndo not focus on filtering out non-evidential information, which limit the\nperformance in LLM-based RAG. We thus propose Evidentiality-guided RAG, or\nECoRAG framework. ECoRAG improves LLM performance by compressing retrieved\ndocuments based on evidentiality, ensuring whether answer generation is\nsupported by the correct evidence. As an additional step, ECoRAG reflects\nwhether the compressed content provides sufficient evidence, and if not,\nretrieves more until sufficient. Experiments show that ECoRAG improves LLM\nperformance on ODQA tasks, outperforming existing compression methods.\nFurthermore, ECoRAG is highly cost-efficient, as it not only reduces latency\nbut also minimizes token usage by retaining only the necessary information to\ngenerate the correct answer. Code is available at\nhttps://github.com/ldilab/ECoRAG.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05167.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645aedd221ab438e732bff43",
      "avatarUrl": "/avatars/31e14fee670fd5ddd296ea0249dbf710.svg",
      "fullname": "Yeonseok Jeong",
      "name": "yeonseokjeong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.08887",
      "authors": [
        {
          "_id": "6848ec1542e4f9106973f2ac",
          "user": {
            "_id": "6364b81b3e248b1e28a68b26",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6364b81b3e248b1e28a68b26/9pd6zfH3HGx1gQkYuL3pR.png",
            "isPro": false,
            "fullname": "LeqiShen",
            "user": "lunar677",
            "type": "user"
          },
          "name": "Leqi Shen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-11T08:35:05.097Z",
          "hidden": false
        },
        {
          "_id": "6848ec1542e4f9106973f2ad",
          "name": "Guoqiang Gong",
          "hidden": false
        },
        {
          "_id": "6848ec1542e4f9106973f2ae",
          "name": "Tianxiang Hao",
          "hidden": false
        },
        {
          "_id": "6848ec1542e4f9106973f2af",
          "name": "Tao He",
          "hidden": false
        },
        {
          "_id": "6848ec1542e4f9106973f2b0",
          "name": "Yifeng Zhang",
          "hidden": false
        },
        {
          "_id": "6848ec1542e4f9106973f2b1",
          "name": "Pengzhang Liu",
          "hidden": false
        },
        {
          "_id": "6848ec1542e4f9106973f2b2",
          "name": "Sicheng Zhao",
          "hidden": false
        },
        {
          "_id": "6848ec1542e4f9106973f2b3",
          "name": "Jungong Han",
          "hidden": false
        },
        {
          "_id": "6848ec1542e4f9106973f2b4",
          "name": "Guiguang Ding",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-10T15:16:40.000Z",
      "submittedOnDailyAt": "2025-06-11T01:17:30.703Z",
      "title": "Discovery : Vision, langage et élimination des erreurs dans les tableaux pour une recherche de vidéos de texte efficace en paramètres",
      "submittedOnDailyBy": {
        "_id": "6364b81b3e248b1e28a68b26",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6364b81b3e248b1e28a68b26/9pd6zfH3HGx1gQkYuL3pR.png",
        "isPro": false,
        "fullname": "LeqiShen",
        "user": "lunar677",
        "type": "user"
      },
      "summary": "L'article - recherche sur l'application efficace des paramètres du modèle CLIP dans le cadre de l'apprentissage profond pour la recherche vidéo-texte est l'une des domaines les plus importants. Bien que CLIP se concentre sur l'approche de correspondance visuo-linguistique à l'échelle des images, la recherche vidéo-texte nécessite une compréhension intégrale à l'échelle du vidéo. Lorsque l'on passe des images à des vidéos, trois différences apparaissent : la vision, le langage et l'alignement. Cependant, actuellement, les méthodes se concentrent principalement sur la vision, en excluant le langage et l'alignement. Dans cet article, on propose une réduction des divergences entre vision, langage et alignement appelée Discrepancy Reduction for Vision, Language, and Alignment (DiscoVLA). Spécifiquement, on introduit l'intégration de caractéristiques d'images et de vidéo pour combiner les caractéristiques de ces deux niveaux, et on aborde efficacement les différences entre vision et langage. De plus, on génère des images fausses pour entraîner l'alignement à l'échelle des images. On propose la distillation de l'alignement d'images vers le vidéo pour réduire les divergences dans l'alignement, en utilisant le savoir d'alignement à l'échelle des images. Les expériences extensives montrent la performance excellente de DiscoVLA, avec un amélioration de 1,5% en R@1 par rapport aux méthodes précédentes, atteignant un R@1 final de 50,5% lorsque CLIP (ViT-B/16) est utilisé dans MSRVTT. Le code est disponible sur https://github.com/LunarShen/DsicoVLA.",
      "upvotes": 4,
      "discussionId": "6848ec1642e4f9106973f2b5",
      "githubRepo": "https://github.com/LunarShen/DsicoVLA",
      "ai_summary": "The paper proposes DiscoVLA to improve video-text retrieval using CLIP by addressing vision, language, and alignment discrepancies, achieving superior performance.",
      "ai_keywords": [
        "parameter-efficient adaptation",
        "image-text pretraining model",
        "CLIP",
        "video-text retrieval",
        "vision",
        "language",
        "alignment",
        "Image-Video Features Fusion",
        "pseudo image captions",
        "Image-to-Video Alignment Distillation",
        "MSRVTT",
        "R@1"
      ]
    },
    "publishedAt": "2025-06-10T11:16:40.000Z",
    "title": "DiscoVLA: Discrepancy Reduction in Vision, Language, and Alignment for\n  Parameter-Efficient Video-Text Retrieval",
    "summary": "The parameter-efficient adaptation of the image-text pretraining model CLIP\nfor video-text retrieval is a prominent area of research. While CLIP is focused\non image-level vision-language matching, video-text retrieval demands\ncomprehensive understanding at the video level. Three key discrepancies emerge\nin the transfer from image-level to video-level: vision, language, and\nalignment. However, existing methods mainly focus on vision while neglecting\nlanguage and alignment. In this paper, we propose Discrepancy Reduction in\nVision, Language, and Alignment (DiscoVLA), which simultaneously mitigates all\nthree discrepancies. Specifically, we introduce Image-Video Features Fusion to\nintegrate image-level and video-level features, effectively tackling both\nvision and language discrepancies. Additionally, we generate pseudo image\ncaptions to learn fine-grained image-level alignment. To mitigate alignment\ndiscrepancies, we propose Image-to-Video Alignment Distillation, which\nleverages image-level alignment knowledge to enhance video-level alignment.\nExtensive experiments demonstrate the superiority of our DiscoVLA. In\nparticular, on MSRVTT with CLIP (ViT-B/16), DiscoVLA outperforms previous\nmethods by 1.5% in R@1, reaching a final score of 50.5% R@1. The code is\navailable at https://github.com/LunarShen/DsicoVLA.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08887.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6364b81b3e248b1e28a68b26",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6364b81b3e248b1e28a68b26/9pd6zfH3HGx1gQkYuL3pR.png",
      "fullname": "LeqiShen",
      "name": "lunar677",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.07932",
      "authors": [
        {
          "_id": "68487f6342e4f9106973f17a",
          "user": {
            "_id": "60796959c59d9e1697fa2324",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60796959c59d9e1697fa2324/wxnDm-p3YgB95NV2p4LGF.png",
            "isPro": false,
            "fullname": "Rishit Dagli",
            "user": "rishitdagli",
            "type": "user"
          },
          "name": "Rishit Dagli",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-11T08:35:32.138Z",
          "hidden": false
        },
        {
          "_id": "68487f6342e4f9106973f17b",
          "name": "Yushi Guan",
          "hidden": false
        },
        {
          "_id": "68487f6342e4f9106973f17c",
          "name": "Sankeerth Durvasula",
          "hidden": false
        },
        {
          "_id": "68487f6342e4f9106973f17d",
          "name": "Mohammadreza Mofayezi",
          "hidden": false
        },
        {
          "_id": "68487f6342e4f9106973f17e",
          "name": "Nandita Vijaykumar",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-09T16:52:10.000Z",
      "submittedOnDailyAt": "2025-06-11T02:25:06.388Z",
      "title": "Squeeze3D : Votre modèle de génération 3D est en fait un puissant compacteur neuronal caché.",
      "submittedOnDailyBy": {
        "_id": "60796959c59d9e1697fa2324",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60796959c59d9e1697fa2324/wxnDm-p3YgB95NV2p4LGF.png",
        "isPro": false,
        "fullname": "Rishit Dagli",
        "user": "rishitdagli",
        "type": "user"
      },
      "summary": "Squeeze3D est un nouveau cadre de travail qui utilise les espaces de vecteurs cachés appris par des modèles de génération 3D préalablement entraînés pour compresser des données 3D de manière très efficace. Notre approche utilise une réseau de cartographie apprenable pour relier les espaces de vecteurs cachés d'un codificateur et d'un modèle de génération préalablement entraînés. La réseau de cartographie crée des vecteurs cachés qui représentent des modèles 3D (comme des modèles de points, des clusters de points ou des champs radiaux) et les transforme en espaces de vecteurs cachés du modèle de génération. Ces vecteurs cachés peuvent être utilisés comme une représentation très compressée de modèles de points ou de clusters de points. Squeeze3D est entraîné uniquement sur des données de synthèse générée, sans nécessité d'un ensemble complet de données 3D. La structure de Squeeze3D peut être adaptée à des codificateurs et des modèles de génération préalablement entraînés, et elle est flexible pour supporter des formats comme des modèles de points, des clusters de points ou des champs radiaux. Selon les expériences, Squeeze3D atteint des taux de compression de 2187 fois pour des modèles text-as-point, 55 fois pour des clusters de points et 619 fois pour des champs radiaux, tout en maintenant la qualité visuelle comparative avec de nombreux méthodes existantes. Squeeze3D ne nécessite pas d'entraîner une réseau spécialisé pour chaque objet, ce qui réduit la latence de compression et de décompression.",
      "upvotes": 2,
      "discussionId": "68487f6442e4f9106973f17f",
      "projectPage": "https://squeeze3d.github.io/",
      "ai_summary": "A novel framework called Squeeze3D uses pre-trained models to compress 3D data efficiently, achieving high compression ratios while maintaining visual quality.",
      "ai_keywords": [
        "pre-trained 3D generative models",
        "latent spaces",
        "encode",
        "latent code",
        "mapping networks",
        "radiance fields",
        "synthetic data",
        "compression ratios",
        "visual quality",
        "compression latency",
        "decompression latency"
      ]
    },
    "publishedAt": "2025-06-09T12:52:10.000Z",
    "title": "Squeeze3D: Your 3D Generation Model is Secretly an Extreme Neural\n  Compressor",
    "summary": "We propose Squeeze3D, a novel framework that leverages implicit prior\nknowledge learnt by existing pre-trained 3D generative models to compress 3D\ndata at extremely high compression ratios. Our approach bridges the latent\nspaces between a pre-trained encoder and a pre-trained generation model through\ntrainable mapping networks. Any 3D model represented as a mesh, point cloud, or\na radiance field is first encoded by the pre-trained encoder and then\ntransformed (i.e. compressed) into a highly compact latent code. This latent\ncode can effectively be used as an extremely compressed representation of the\nmesh or point cloud. A mapping network transforms the compressed latent code\ninto the latent space of a powerful generative model, which is then conditioned\nto recreate the original 3D model (i.e. decompression). Squeeze3D is trained\nentirely on generated synthetic data and does not require any 3D datasets. The\nSqueeze3D architecture can be flexibly used with existing pre-trained 3D\nencoders and existing generative models. It can flexibly support different\nformats, including meshes, point clouds, and radiance fields. Our experiments\ndemonstrate that Squeeze3D achieves compression ratios of up to 2187x for\ntextured meshes, 55x for point clouds, and 619x for radiance fields while\nmaintaining visual quality comparable to many existing methods. Squeeze3D only\nincurs a small compression and decompression latency since it does not involve\ntraining object-specific networks to compress an object.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07932.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60796959c59d9e1697fa2324",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60796959c59d9e1697fa2324/wxnDm-p3YgB95NV2p4LGF.png",
      "fullname": "Rishit Dagli",
      "name": "rishitdagli",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.05928",
      "authors": [
        {
          "_id": "6847b3393ec10bdd8ab4df20",
          "user": {
            "_id": "65ea90741b0d7e029a3a1fb0",
            "avatarUrl": "/avatars/7a4f861d4ead080996bb28e2b6cb8ac5.svg",
            "isPro": false,
            "fullname": "cj",
            "user": "cajie",
            "type": "user"
          },
          "name": "Jie Cao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-10T08:42:50.209Z",
          "hidden": false
        },
        {
          "_id": "6847b3393ec10bdd8ab4df21",
          "name": "Tianwei Lin",
          "hidden": false
        },
        {
          "_id": "6847b3393ec10bdd8ab4df22",
          "name": "Hongyang He",
          "hidden": false
        },
        {
          "_id": "6847b3393ec10bdd8ab4df23",
          "name": "Rolan Yan",
          "hidden": false
        },
        {
          "_id": "6847b3393ec10bdd8ab4df24",
          "name": "Wenqiao Zhang",
          "hidden": false
        },
        {
          "_id": "6847b3393ec10bdd8ab4df25",
          "name": "Juncheng Li",
          "hidden": false
        },
        {
          "_id": "6847b3393ec10bdd8ab4df26",
          "name": "Dongping Zhang",
          "hidden": false
        },
        {
          "_id": "6847b3393ec10bdd8ab4df27",
          "name": "Siliang Tang",
          "hidden": false
        },
        {
          "_id": "6847b3393ec10bdd8ab4df28",
          "name": "Yueting Zhuang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-06T09:54:19.000Z",
      "submittedOnDailyAt": "2025-06-11T01:28:46.209Z",
      "title": "Moa : Efficacité des paramètres dans l'ajustement micro de modèles hybrides de grande échelle de langage",
      "submittedOnDailyBy": {
        "_id": "65ea90741b0d7e029a3a1fb0",
        "avatarUrl": "/avatars/7a4f861d4ead080996bb28e2b6cb8ac5.svg",
        "isPro": false,
        "fullname": "cj",
        "user": "cajie",
        "type": "user"
      },
      "summary": "Les dernières recherches travaillent à améliorer le rendement des méthodes d'adaptation efficace de paramètres (PEFT) dans les applications de modèles de langage grands (LLM) en intégrant l'Adaptation de Rang Faible (LoRA) et les Experts Mixtes (MoE). Les méthodes actuelles sont composées d'une architecture uniforme MoE-LoRA avec des Experts LoRA de structures et capacités similaires. Cependant, ces approximations sont affectées par la perte de représentation et l'imbalance des charges des Experts, ce qui peut réduire le potentiel des LLM. Pour relever ces défis, nous proposons un approche hétérogène de Mixture-of-Adapters (MoA).\n\nCette méthode intègre de manière dynamique des Experts PEFT d'adaptateurs de structures différentes et exploite leur capacité d'interpolation pour promouvoir la spécialisation des Experts et transmettre de manière efficace le savoir pré-entraîné pour améliorer les tâches ultérieures. La MoA offre deux versions :\n(i) MoA douce intègre finement tous les Experts par une combinaison pondérée de leurs sorties.\n(ii) MoA épars active de manière épars les Experts en fonction de leur contribution, ce qui permet de faire des ajustements sans perdre de la performance.\n\nLes résultats des expériences montrent que la MoA hétérogène dépasse l'architecture homogène MoE-LoRA en termes de rendement et d'efficience en paramètres. Notre projet est disponible sur : https://github.com/DCDmllm/MoA.",
      "upvotes": 2,
      "discussionId": "6847b3393ec10bdd8ab4df29",
      "githubRepo": "https://github.com/DCDmllm/MoA",
      "ai_summary": "A heterogeneous Mixture-of-Adapters (MoA) approach enhances parameter-efficient fine-tuning in LLMs by integrating diverse adapter experts, outperforming homogeneous MoE-LoRA methods.",
      "ai_keywords": [
        "Low-Rank Adaptation",
        "Mixture-of-Experts",
        "parameter-efficient fine-tuning",
        "Large Language Model",
        "homogeneous",
        "representation collapse",
        "expert load imbalance",
        "heterogeneous",
        "Mixture-of-Adapters",
        "soft MoA",
        "sparse MoA"
      ]
    },
    "publishedAt": "2025-06-06T05:54:19.000Z",
    "title": "MoA: Heterogeneous Mixture of Adapters for Parameter-Efficient\n  Fine-Tuning of Large Language Models",
    "summary": "Recent studies integrate Low-Rank Adaptation (LoRA) and Mixture-of-Experts\n(MoE) to further enhance the performance of parameter-efficient fine-tuning\n(PEFT) methods in Large Language Model (LLM) applications. Existing methods\nemploy homogeneous MoE-LoRA architectures composed of LoRA experts with\neither similar or identical structures and capacities. However, these\napproaches often suffer from representation collapse and expert load imbalance,\nwhich negatively impact the potential of LLMs. To address these challenges, we\npropose a heterogeneous Mixture-of-Adapters (MoA) approach.\nThis method dynamically integrates PEFT adapter experts with diverse\nstructures, leveraging their complementary representational capabilities to\nfoster expert specialization, thereby enhancing the effective transfer of\npre-trained knowledge to downstream tasks. MoA supports two variants:\n(i) Soft MoA achieves fine-grained integration by performing\na weighted fusion of all expert outputs; (ii) Sparse MoA\nactivates adapter experts sparsely based on their contribution, achieving this\nwith negligible performance degradation. Experimental results demonstrate that\nheterogeneous MoA outperforms homogeneous MoE-LoRA methods in both performance\nand parameter efficiency. Our project is available at\nhttps://github.com/DCDmllm/MoA.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05928.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65ea90741b0d7e029a3a1fb0",
      "avatarUrl": "/avatars/7a4f861d4ead080996bb28e2b6cb8ac5.svg",
      "fullname": "cj",
      "name": "cajie",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.08300",
      "authors": [
        {
          "_id": "684954493614057188acbf5a",
          "name": "Matteo Cargnelutti",
          "hidden": false
        },
        {
          "_id": "684954493614057188acbf5b",
          "name": "Catherine Brobston",
          "hidden": false
        },
        {
          "_id": "684954493614057188acbf5c",
          "name": "John Hess",
          "hidden": false
        },
        {
          "_id": "684954493614057188acbf5d",
          "name": "Jack Cushman",
          "hidden": false
        },
        {
          "_id": "684954493614057188acbf5e",
          "name": "Kristi Mukk",
          "hidden": false
        },
        {
          "_id": "684954493614057188acbf5f",
          "name": "Aristana Scourtas",
          "hidden": false
        },
        {
          "_id": "684954493614057188acbf60",
          "name": "Kyle Courtney",
          "hidden": false
        },
        {
          "_id": "684954493614057188acbf61",
          "name": "Greg Leppert",
          "hidden": false
        },
        {
          "_id": "684954493614057188acbf62",
          "name": "Amanda Watson",
          "hidden": false
        },
        {
          "_id": "684954493614057188acbf63",
          "name": "Martha Whitehead",
          "hidden": false
        },
        {
          "_id": "684954493614057188acbf64",
          "name": "Jonathan Zittrain",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-10T00:11:30.000Z",
      "submittedOnDailyAt": "2025-06-11T08:37:11.451Z",
      "title": "Institutionnal Bug 1.0 : Collection de la Bibliothèque Harvard de 242B tokens, adaptée à la précision et à la possibilité d'utilisation.",
      "submittedOnDailyBy": {
        "_id": "5e6a3d4ea9afd5125d9ec064",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1584020801691-noauth.jpeg",
        "isPro": true,
        "fullname": "Stefan Schweter",
        "user": "stefan-it",
        "type": "user"
      },
      "summary": "Les grands modèles de langue (LLMs) ont pour objectif d'apprendre sur le monde en utilisant des données et de générer des relations significatives et des prédictions. Par conséquent, les caractéristiques, l'échelle, la qualité et la diversité des ensembles de données utilisés pour entraîner ou inférer ces modèles ont un impact direct sur leur qualité. Le rapide développement et l'introduction de modèles de haute qualité ont mis en évidence la faiblesse de la qualité des données d'entraînement disponibles publiquement, ce qui a conduit à une nécessité urgente pour que les gestionnaires de ces ensembles de données adoptent des pratiques continuelles. Cependant, ce rapport technique présente, pour la première fois depuis 2006, le service de Gitter Book, la bibliothèque de Harvard a digitalisé la collection de livres institutionnels 1.0, une vaste collection de livres dans le domaine public. En collaboration avec la bibliothèque de Harvard, ces livres ont été extraits, analysés et convertis en ensembles de données enregistrés. Cette analyse a couvert toute la collection de la bibliothèque de Harvard et a présenté pour la première fois 1,075,899 livres écrits dans plus de 250 langues, dépassant approximativement 250 milliards de tokens. Une partie de cette première version de lancement a inclus le texte extrait par OCR (original et ensuite traité) et les métadonnées (catalogue numérique, sources, générées) de 983,004 livres (242B tokens). Ce rapport explique l'objectif et les méthodes du projet, résume les résultats des analyses effectuées, et fournit des soutiens pour rendre cette collection historique plus accessible, offrant des méthodes pour filtrer, lire et utiliser tant pour les personnes que pour les machines.",
      "upvotes": 1,
      "discussionId": "684954493614057188acbf65",
      "ai_summary": "Institutional Books 1.0 provides a large dataset of public domain books from Harvard Library for training and inference of large language models, enhancing data accessibility and sustainability.",
      "ai_keywords": [
        "large language models",
        "datasets",
        "institutional books",
        "public domain",
        "harrass library",
        "google books project",
        "ocr",
        "metadata",
        "historic texts"
      ]
    },
    "publishedAt": "2025-06-09T20:11:30.000Z",
    "title": "Institutional Books 1.0: A 242B token dataset from Harvard Library's\n  collections, refined for accuracy and usability",
    "summary": "Large language models (LLMs) use data to learn about the world in order to\nproduce meaningful correlations and predictions. As such, the nature, scale,\nquality, and diversity of the datasets used to train these models, or to\nsupport their work at inference time, have a direct impact on their quality.\nThe rapid development and adoption of LLMs of varying quality has brought into\nfocus the scarcity of publicly available, high-quality training data and\nrevealed an urgent need to ground the stewardship of these datasets in\nsustainable practices with clear provenance chains. To that end, this technical\nreport introduces Institutional Books 1.0, a large collection of public domain\nbooks originally digitized through Harvard Library's participation in the\nGoogle Books project, beginning in 2006. Working with Harvard Library, we\nextracted, analyzed, and processed these volumes into an extensively-documented\ndataset of historic texts. This analysis covers the entirety of Harvard\nLibrary's collection scanned as part of that project, originally spanning\n1,075,899 volumes written in over 250 different languages for a total of\napproximately 250 billion tokens. As part of this initial release, the\nOCR-extracted text (original and post-processed) as well as the metadata\n(bibliographic, source, and generated) of the 983,004 volumes, or 242B tokens,\nidentified as being in the public domain have been made available. This report\ndescribes this project's goals and methods as well as the results of the\nanalyses we performed, all in service of making this historical collection more\naccessible and easier for humans and machines alike to filter, read and use.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08300.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "5e6a3d4ea9afd5125d9ec064",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1584020801691-noauth.jpeg",
      "fullname": "Stefan Schweter",
      "name": "stefan-it",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2742
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.07047",
      "authors": [
        {
          "_id": "68492bf142e4f9106973f411",
          "name": "Yu Xuejun",
          "hidden": false
        },
        {
          "_id": "68492bf142e4f9106973f412",
          "user": {
            "_id": "6608fa4f5baec84322ec85ea",
            "avatarUrl": "/avatars/13bdaff931676b065fa1efef06fef922.svg",
            "isPro": false,
            "fullname": "Zhong",
            "user": "Jianyuan1",
            "type": "user"
          },
          "name": "Jianyuan Zhong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-11T08:34:08.719Z",
          "hidden": false
        },
        {
          "_id": "68492bf142e4f9106973f413",
          "name": "Zijin Feng",
          "hidden": false
        },
        {
          "_id": "68492bf142e4f9106973f414",
          "name": "Pengyi Zhai",
          "hidden": false
        },
        {
          "_id": "68492bf142e4f9106973f415",
          "name": "Roozbeh Yousefzadeh",
          "hidden": false
        },
        {
          "_id": "68492bf142e4f9106973f416",
          "name": "Wei Chong Ng",
          "hidden": false
        },
        {
          "_id": "68492bf142e4f9106973f417",
          "name": "Haoxiong Liu",
          "hidden": false
        },
        {
          "_id": "68492bf142e4f9106973f418",
          "name": "Ziyi Shou",
          "hidden": false
        },
        {
          "_id": "68492bf142e4f9106973f419",
          "name": "Jing Xiong",
          "hidden": false
        },
        {
          "_id": "68492bf142e4f9106973f41a",
          "name": "Yudong Zhou",
          "hidden": false
        },
        {
          "_id": "68492bf142e4f9106973f41b",
          "name": "Claudia Beth Ong",
          "hidden": false
        },
        {
          "_id": "68492bf142e4f9106973f41c",
          "name": "Austen Jeremy Sugiarto",
          "hidden": false
        },
        {
          "_id": "68492bf142e4f9106973f41d",
          "name": "Yaoxi Zhang",
          "hidden": false
        },
        {
          "_id": "68492bf142e4f9106973f41e",
          "name": "Wai Ming Tai",
          "hidden": false
        },
        {
          "_id": "68492bf142e4f9106973f41f",
          "name": "Huan Cao",
          "hidden": false
        },
        {
          "_id": "68492bf142e4f9106973f420",
          "name": "Dongcai Lu",
          "hidden": false
        },
        {
          "_id": "68492bf142e4f9106973f421",
          "name": "Jiacheng Sun",
          "hidden": false
        },
        {
          "_id": "68492bf142e4f9106973f422",
          "name": "Qiang Xu",
          "hidden": false
        },
        {
          "_id": "68492bf142e4f9106973f423",
          "name": "Shen Xin",
          "hidden": false
        },
        {
          "_id": "68492bf142e4f9106973f424",
          "name": "Zhenguo Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-08T09:04:14.000Z",
      "submittedOnDailyAt": "2025-06-11T05:45:24.452Z",
      "title": "Maestro : Recherche sur les démonstrations formelles à partir du langage naturel",
      "submittedOnDailyBy": {
        "_id": "6608fa4f5baec84322ec85ea",
        "avatarUrl": "/avatars/13bdaff931676b065fa1efef06fef922.svg",
        "isPro": false,
        "fullname": "Zhong",
        "user": "Jianyuan1",
        "type": "user"
      },
      "summary": "Le développement récent de modèles de langue grands montre une grande possibilité grâce à leur capacité formelle. Cependant, la majorité des systèmes de démonstration basés sur des modèles de langage grands nécessitent des entrées d'explications formelles écrites par des experts, ce qui limite leur application aux problèmes exprimés de manière naturelle. Nous avons essayé de combler ce vide en introduisant un processus complet de démonstration formelle appelé \"Mathesis\". Ce système propose un premier générateur automatique de formalisation, \"Mathesis-Autoformalizer\", utilisant l'apprentissage par renforcement pour renforcer la capacité de formalisation des explications naturelles. De plus, notre nouveau cadre d'évaluation LeanScorer soutient l'évaluation de la qualité de la formalisation complexe. Nous proposons également \"Mathesis-Prover\", un système qui génère des démonstrations formelles à partir d'explications formelles. Pour évaluer la possibilité d'application des démonstrations formelles, nous avons introduit le benchmark Gaokao-Formal, composé de 488 problèmes complexes. Notre approche a été conçue soigneusement à travers une recherche détaillée de chaque composant. Les expériences montrent l'effet de Mathesis et atteignent un résultat meilleur que 22% du meilleur modèle de référence dans Gaokao-Formal. Le système complet atteint une précision de 64% sur MiniF2F et 18% plus avancé sur Gaokao-Formal.",
      "upvotes": 1,
      "discussionId": "68492bf142e4f9106973f425",
      "githubRepo": "https://github.com/Huawei-AI4Math/Mathesis"
    },
    "publishedAt": "2025-06-08T05:04:14.000Z",
    "title": "Mathesis: Towards Formal Theorem Proving from Natural Languages",
    "summary": "Recent advances in large language models show strong promise for formal\nreasoning. However, most LLM-based theorem provers have long been constrained\nby the need for expert-written formal statements as inputs, limiting their\napplicability to real-world problems expressed in natural language. We tackle\nthis gap with Mathesis, the first end-to-end theorem proving pipeline\nprocessing informal problem statements. It contributes Mathesis-Autoformalizer,\nthe first autoformalizer using reinforcement learning to enhance the\nformalization ability of natural language problems, aided by our novel\nLeanScorer framework for nuanced formalization quality assessment. It also\nproposes a Mathesis-Prover, which generates formal proofs from the formalized\nstatements. To evaluate the real-world applicability of end-to-end formal\ntheorem proving, we introduce Gaokao-Formal, a benchmark of 488 complex\nproblems from China's national college entrance exam. Our approach is carefully\ndesigned, with a thorough study of each component. Experiments demonstrate\nMathesis's effectiveness, with the autoformalizer outperforming the best\nbaseline by 22% in pass-rate on Gaokao-Formal. The full system surpasses other\nmodel combinations, achieving 64% accuracy on MiniF2F with pass@32 and a\nstate-of-the-art 18% on Gaokao-Formal.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07047.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6608fa4f5baec84322ec85ea",
      "avatarUrl": "/avatars/13bdaff931676b065fa1efef06fef922.svg",
      "fullname": "Zhong",
      "name": "Jianyuan1",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.05700",
      "authors": [
        {
          "_id": "6848de6e42e4f9106973f273",
          "user": {
            "_id": "65d76cc5b9b7b8bf88faa916",
            "avatarUrl": "/avatars/d95232cd0c307efab6197ade1a66190b.svg",
            "isPro": true,
            "fullname": "Yan Wang",
            "user": "YanAdjeNole",
            "type": "user"
          },
          "name": "Yan Wang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-11T01:39:59.328Z",
          "hidden": false
        },
        {
          "_id": "6848de6e42e4f9106973f274",
          "name": "Yueru He",
          "hidden": false
        },
        {
          "_id": "6848de6e42e4f9106973f275",
          "name": "Ruoyu Xiang",
          "hidden": false
        },
        {
          "_id": "6848de6e42e4f9106973f276",
          "name": "Jeff Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-06T03:02:52.000Z",
      "submittedOnDailyAt": "2025-06-11T00:15:04.904Z",
      "title": "Modèle de langage grandement renforcé par des connaissances de règles",
      "submittedOnDailyBy": {
        "_id": "65d76cc5b9b7b8bf88faa916",
        "avatarUrl": "/avatars/d95232cd0c307efab6197ade1a66190b.svg",
        "isPro": true,
        "fullname": "Yan Wang",
        "user": "YanAdjeNole",
        "type": "user"
      },
      "summary": "Le développement récent de grands modèles de langage (LLMs) offre un grand potentiel dans le domaine financier, cependant, les problèmes de précision et les violations dans les rapports de régulation numériques (DRR) ont été un sujet d'importance considérable. Pour faire face à ces défis, nous proposons RKEFino1, un modèle de logique financière basé sur Fino1, qui a été renforcé avec des connaissances réglementaires. Ce modèle a été ajusté aux connaissances spécifiques obtenues à partir de XBRL, CDM et MOF. Nous avons configuré deux tâches de questions et réponses basées sur le savoir et la mathématique, et nous avons introduit une nouvelle tâche de reconnaissance d'entités numériques (NER) qui couvre des entités financières dans des phrases et des tableaux. A travers nos résultats expérimentaux, nous montrons que RKEFino1 montre une efficacité et une capacité de généralisation dans des tâches financières importantes liées aux violations. Notre modèle est disponible sur Hugging Face.",
      "upvotes": 1,
      "discussionId": "6848de6e42e4f9106973f277",
      "ai_summary": "RKEFino1, a regulation-aware LLM fine-tuned with financial domain knowledge, effectively handles compliance-critical tasks including QA and numerical NER.",
      "ai_keywords": [
        "Large language models",
        "financial reasoning",
        "Digital Regulatory Reporting",
        "regulation knowledge-enhanced",
        "fine-tuning",
        "domain knowledge",
        "XBRL",
        "CDM",
        "MOF",
        "QA tasks",
        "knowledge-based reasoning",
        "mathematical reasoning",
        "Numerical NER",
        "financial entities"
      ]
    },
    "publishedAt": "2025-06-05T23:02:52.000Z",
    "title": "RKEFino1: A Regulation Knowledge-Enhanced Large Language Model",
    "summary": "Recent advances in large language models (LLMs) hold great promise for\nfinancial applications but introduce critical accuracy and compliance\nchallenges in Digital Regulatory Reporting (DRR). To address these issues, we\npropose RKEFino1, a regulation knowledge-enhanced financial reasoning model\nbuilt upon Fino1, fine-tuned with domain knowledge from XBRL, CDM, and MOF. We\nformulate two QA tasks-knowledge-based and mathematical reasoning-and introduce\na novel Numerical NER task covering financial entities in both sentences and\ntables. Experimental results demonstrate the effectiveness and generalization\ncapacity of RKEFino1 in compliance-critical financial tasks. We have released\nour model on Hugging Face.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05700.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65d76cc5b9b7b8bf88faa916",
      "avatarUrl": "/avatars/d95232cd0c307efab6197ade1a66190b.svg",
      "fullname": "Yan Wang",
      "name": "YanAdjeNole",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.07976",
      "authors": [
        {
          "_id": "68487bd642e4f9106973f16d",
          "name": "Junhong Shen",
          "hidden": false
        },
        {
          "_id": "68487bd642e4f9106973f16e",
          "user": {
            "_id": "62927c2e56fedc76e396b3ca",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678105603200-62927c2e56fedc76e396b3ca.jpeg",
            "isPro": false,
            "fullname": "HAO BAI",
            "user": "JackBAI",
            "type": "user"
          },
          "name": "Hao Bai",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-11T10:08:42.049Z",
          "hidden": false
        },
        {
          "_id": "68487bd642e4f9106973f16f",
          "name": "Lunjun Zhang",
          "hidden": false
        },
        {
          "_id": "68487bd642e4f9106973f170",
          "name": "Yifei Zhou",
          "hidden": false
        },
        {
          "_id": "68487bd642e4f9106973f171",
          "name": "Amrith Setlur",
          "hidden": false
        },
        {
          "_id": "68487bd642e4f9106973f172",
          "name": "Shengbang Tong",
          "hidden": false
        },
        {
          "_id": "68487bd642e4f9106973f173",
          "name": "Diego Caples",
          "hidden": false
        },
        {
          "_id": "68487bd642e4f9106973f174",
          "name": "Nan Jiang",
          "hidden": false
        },
        {
          "_id": "68487bd642e4f9106973f175",
          "name": "Tong Zhang",
          "hidden": false
        },
        {
          "_id": "68487bd642e4f9106973f176",
          "name": "Ameet Talwalkar",
          "hidden": false
        },
        {
          "_id": "68487bd642e4f9106973f177",
          "name": "Aviral Kumar",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-09T17:50:02.000Z",
      "submittedOnDailyAt": "2025-06-11T08:38:04.852Z",
      "title": "Thinking vs. Doing : Escalado de Interactions Interactifs en Temps de Test par des Motifs d'Escalage",
      "submittedOnDailyBy": {
        "_id": "62927c2e56fedc76e396b3ca",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678105603200-62927c2e56fedc76e396b3ca.jpeg",
        "isPro": false,
        "fullname": "HAO BAI",
        "user": "JackBAI",
        "type": "user"
      },
      "summary": "Actuellement, le paradigme de l'échelle temporelle des tests implique la génération de traces logiques complexes avant de produire une réponse. Dans les problèmes d'échange, ces traces logiques permettent de penser à des actions à réaliser dans un monde avant d'agir. Cependant, ce processus ne peut pas être exécuté à l'intérieur de la charge de sortie, qui peut réaliser diverses actions telles que l'exploration, le retour et la réorganisation dynamique. Cet article propose un échelle temporelle d'échange de temps de test. Cette échelle temporelle permet que, à l'intérieur de la charge de sortie, diverses actions soient réalisées telles que l'exploration, le retour et la réorganisation dynamique, ce qui n'est pas possible dans l'échelle temporelle des tests actuelles. Pour montrer la possibilité de ce nouvel échelle, on se concentre sur l'aire de la charge de sortie web. Tout d'abord, on montre que l'échelle d'échange basée sur des prompts sans apprentissage augmente simplement la taux de succès. À travers cela, on introduit le TTI (Interaction de Temps de Test) et on utilise un approche basée sur l'apprentissage par renforcement (RL) avec un clair pour ajuster de manière adaptative la longueur de la charge de sortie. En utilisant le modèle de rat 3 12B, le TTI génère la meilleure charge de sortie web ouverte et avec des données ouvertes dans les benchmarks de webloader et webarena. De plus, le TTI montre qu'il peut maintenir un équilibre adaptatif entre l'exploration et l'utilisation dans la charge de sortie. Nos résultats montrent que l'échelle d'échange est un puissant complément pour l'échelle de la quantité de calcul à chaque étape et offre une nouvelle voie pour entraîner la charge de sortie.",
      "upvotes": 0,
      "discussionId": "68487bd742e4f9106973f178",
      "ai_summary": "Test-Time Interaction (TTI) improves web agent performance by scaling interaction, enabling adaptive behavior and balancing exploration and exploitation without adding per-step compute.",
      "ai_keywords": [
        "test-time scaling",
        "thinking traces",
        "agent interaction",
        "interaction horizon",
        "exploration",
        "backtracking",
        "dynamic re-planning",
        "rollout",
        "curriculum-based online reinforcement learning (RL)",
        "Gemma 3 12B model",
        "WebVoyager",
        "WebArena",
        "adaptive agents"
      ]
    },
    "publishedAt": "2025-06-09T13:50:02.000Z",
    "title": "Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction",
    "summary": "The current paradigm of test-time scaling relies on generating long reasoning\ntraces (\"thinking\" more) before producing a response. In agent problems that\nrequire interaction, this can be done by generating thinking traces before\nacting in the world. However, this process does not allow agents to acquire new\ninformation from the environment or adapt their behavior over time. In this\nwork, we propose to scale test-time interaction, an untapped dimension of\ntest-time scaling that increases the agent's interaction horizon to enable\nrunning rich behaviors such as exploration, backtracking, and dynamic\nre-planning within a single rollout. To demonstrate the promise of this scaling\ndimension, we study the domain of web agents. We first show that even\nprompting-based interaction scaling without any training can improve task\nsuccess on web benchmarks non-trivially. Building on this, we introduce TTI\n(Test-Time Interaction), a curriculum-based online reinforcement learning (RL)\napproach that trains agents by adaptively adjusting their rollout lengths.\nUsing a Gemma 3 12B model, TTI produces state-of-the-art open-source, open-data\nweb agents on WebVoyager and WebArena benchmarks. We further show that TTI\nenables agents to balance exploration and exploitation adaptively. Our results\nestablish interaction scaling as a powerful, complementary axis to scaling\nper-step compute, offering new avenues for training adaptive agents.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07976.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62927c2e56fedc76e396b3ca",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678105603200-62927c2e56fedc76e396b3ca.jpeg",
      "fullname": "HAO BAI",
      "name": "JackBAI",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  }
]