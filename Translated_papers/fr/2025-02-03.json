[
  {
    "paper": {
      "id": "2501.19393",
      "authors": [
        {
          "_id": "67a02dd80e751b0476a1bcc6",
          "name": "Niklas Muennighoff",
          "hidden": false
        },
        {
          "_id": "67a02dd80e751b0476a1bcc7",
          "name": "Zitong Yang",
          "hidden": false
        },
        {
          "_id": "67a02dd80e751b0476a1bcc8",
          "name": "Weijia Shi",
          "hidden": false
        },
        {
          "_id": "67a02dd80e751b0476a1bcc9",
          "name": "Xiang Lisa Li",
          "hidden": false
        },
        {
          "_id": "67a02dd80e751b0476a1bcca",
          "name": "Li Fei-Fei",
          "hidden": false
        },
        {
          "_id": "67a02dd80e751b0476a1bccb",
          "name": "Hannaneh Hajishirzi",
          "hidden": false
        },
        {
          "_id": "67a02dd80e751b0476a1bccc",
          "name": "Luke Zettlemoyer",
          "hidden": false
        },
        {
          "_id": "67a02dd80e751b0476a1bccd",
          "name": "Percy Liang",
          "hidden": false
        },
        {
          "_id": "67a02dd80e751b0476a1bcce",
          "name": "Emmanuel Candès",
          "hidden": false
        },
        {
          "_id": "67a02dd80e751b0476a1bccf",
          "name": "Tatsunori Hashimoto",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-31T18:48:08.000Z",
      "title": "Programmation des temps simples de tests",
      "summary": "L'escalage pendant le test est un méthode pour améliorer l'efficacité d'un modèle de langage nouveau en ajoutant plus de capacités de calcul pendant l'entraînement. Récemment, le modèle o1 de OpenAI a démontré cette capacité, mais en ne publiant pas les méthodes, plusieurs efforts de reproduction ont été réalisés. Nous cherchons à trouver la manière la plus simple pour implémenter l'escalage pendant le test et un rendement théorique puissant.\n\nTout d'abord, nous choisissons un ensemble de données s1K de 1,000 ensembles, évalués en difficulté, diversité et qualité. Ce ensemble inclut des combinaisons de problèmes qui comprennent des traces théoriques et leurs réponses. Ensuite, nous développons une gestion pour contrôler la quantité de calculs pendant le test, en faisant que le modèle termine son processus de pensée forcément. Cela permet d'ajouter plusieurs \"poids\" au modèle lorsqu'il essaie de terminer sa réponse, étendant son processus de pensée. De cette manière, le modèle peut réviser sa réponse et corriger les étapes théoriques incorrectes.\n\nLe modèle de langage Qwen2.5-32B-Instruct, normalisé avec s1K et avec la gestion ajoutée, est appelé le modèle s1. Ce modèle a amélioré les problèmes de compétences plus élevées d'un pourcentage approché de 27% par rapport au modèle o1-preview (MATH et AIME24). De plus, l'utilisation de la gestion permet d'étendre le rendement du modèle s1 sans avoir à répondre aux tests, atteignant des améliorations de 50% à 57% en AIME24. Notre modèle, données et code sont disponibles sur https://github.com/simplescaling/s1.",
      "upvotes": 21,
      "discussionId": "67a02dd90e751b0476a1bd02"
    },
    "publishedAt": "2025-02-02T21:45:49.841Z",
    "title": "s1: Simple test-time scaling",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.19393.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5912
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.19324",
      "authors": [
        {
          "_id": "67a04151dd7b3a4aba880589",
          "name": "Baohao Liao",
          "hidden": false
        },
        {
          "_id": "67a04151dd7b3a4aba88058a",
          "name": "Yuhui Xu",
          "hidden": false
        },
        {
          "_id": "67a04151dd7b3a4aba88058b",
          "name": "Hanze Dong",
          "hidden": false
        },
        {
          "_id": "67a04151dd7b3a4aba88058c",
          "name": "Junnan Li",
          "hidden": false
        },
        {
          "_id": "67a04151dd7b3a4aba88058d",
          "name": "Christof Monz",
          "hidden": false
        },
        {
          "_id": "67a04151dd7b3a4aba88058e",
          "name": "Silvio Savarese",
          "hidden": false
        },
        {
          "_id": "67a04151dd7b3a4aba88058f",
          "name": "Doyen Sahoo",
          "hidden": false
        },
        {
          "_id": "67a04151dd7b3a4aba880590",
          "name": "Caiming Xiong",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-31T17:19:57.000Z",
      "title": "L'utilisation de la décodification de la guide de Ribeiro pour Doripeki dans les calculs efficaces de modèles de langage grands (LLM)",
      "summary": "Nous introduisons un nouveau cadre de travail pour améliorer l'efficacité : la Décodification Prédictive Orientée à la Récompense (RSD). Le RSD combine un modèle léger de début avec un modèle plus puissant de but, introduisant une inclination contrôlée qui privilégie les sorties avec de hautes récompenses, sans imposer un strict non-bias. Il utilise un modèle de récompense pour évaluer les étapes de décodification intermédiaires et décide dynamiquement si de l'appel au modèle de but, optimisant l'équilibre entre les coûts de calcul et la qualité de la sortie. Théoriquement, il a été démontré que stratégie de mélange basée sur des seuils atteint l'équilibre optimal entre l'utilisation de ressources et le rendement. Dans des évaluations variées, qui comprennent des tâches d'un niveau élevé comme l'Olympiade, le RSD consomme jusqu'à 4,4 fois moins de FLOPs que le décodificateur utilisant seulement le modèle de but, et atteint une précision moyenne de 3,5% plus élevée que le méthode de décodification parallèle. Ces résultats soulignent que le RSD est une forme efficace et rentable d'implémenter les LLMs dans des environnements riches en ressources.",
      "upvotes": 15,
      "discussionId": "67a04152dd7b3a4aba8805c0"
    },
    "publishedAt": "2025-02-02T23:10:16.068Z",
    "title": "Reward-Guided Speculative Decoding for Efficient LLM Reasoning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.19324.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6602869253a0518b2a98cafd",
      "avatarUrl": "/avatars/c14b5953a716f42c83ad28147f8308ae.svg",
      "fullname": "Yuhui Xu",
      "name": "yuhuixu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.18837",
      "authors": [
        {
          "_id": "67a04e7ab6fd93f91c65457b",
          "name": "Mrinank Sharma",
          "hidden": false
        },
        {
          "_id": "67a04e7ab6fd93f91c65457c",
          "name": "Meg Tong",
          "hidden": false
        },
        {
          "_id": "67a04e7ab6fd93f91c65457d",
          "name": "Jesse Mu",
          "hidden": false
        },
        {
          "_id": "67a04e7ab6fd93f91c65457e",
          "name": "Jerry Wei",
          "hidden": false
        },
        {
          "_id": "67a04e7ab6fd93f91c65457f",
          "name": "Jorrit Kruthoff",
          "hidden": false
        },
        {
          "_id": "67a04e7ab6fd93f91c654580",
          "name": "Scott Goodfriend",
          "hidden": false
        },
        {
          "_id": "67a04e7ab6fd93f91c654581",
          "name": "Euan Ong",
          "hidden": false
        },
        {
          "_id": "67a04e7ab6fd93f91c654582",
          "name": "Alwin Peng",
          "hidden": false
        },
        {
          "_id": "67a04e7ab6fd93f91c654583",
          "name": "Raj Agarwal",
          "hidden": false
        },
        {
          "_id": "67a04e7ab6fd93f91c654584",
          "name": "Cem Anil",
          "hidden": false
        },
        {
          "_id": "67a04e7ab6fd93f91c654585",
          "name": "Amanda Askell",
          "hidden": false
        },
        {
          "_id": "67a04e7ab6fd93f91c654586",
          "name": "Nathan Bailey",
          "hidden": false
        },
        {
          "_id": "67a04e7ab6fd93f91c654587",
          "name": "Joe Benton",
          "hidden": false
        },
        {
          "_id": "67a04e7ab6fd93f91c654588",
          "name": "Emma Bluemke",
          "hidden": false
        },
        {
          "_id": "67a04e7ab6fd93f91c654589",
          "name": "Samuel R. Bowman",
          "hidden": false
        },
        {
          "_id": "67a04e7ab6fd93f91c65458a",
          "name": "Eric Christiansen",
          "hidden": false
        },
        {
          "_id": "67a04e7ab6fd93f91c65458b",
          "name": "Hoagy Cunningham",
          "hidden": false
        },
        {
          "_id": "67a04e7ab6fd93f91c65458c",
          "name": "Andy Dau",
          "hidden": false
        },
        {
          "_id": "67a04e7ab6fd93f91c65458d",
          "name": "Anjali Gopal",
          "hidden": false
        },
        {
          "_id": "67a04e7ab6fd93f91c65458e",
          "name": "Rob Gilson",
          "hidden": false
        },
        {
          "_id": "67a04e7ab6fd93f91c65458f",
          "name": "Logan Graham",
          "hidden": false
        },
        {
          "_id": "67a04e7ab6fd93f91c654590",
          "name": "Logan Howard",
          "hidden": false
        },
        {
          "_id": "67a04e7ab6fd93f91c654591",
          "user": {
            "_id": "66fc4c692408eb3bdeba876f",
            "avatarUrl": "/avatars/66ba18ccb95d150e66d7b6930d4eb938.svg",
            "isPro": false,
            "fullname": "Nimit Kalra",
            "user": "nimitkalra",
            "type": "user"
          },
          "name": "Nimit Kalra",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-03T08:14:42.317Z",
          "hidden": false
        },
        {
          "_id": "67a04e7ab6fd93f91c654592",
          "name": "Taesung Lee",
          "hidden": false
        },
        {
          "_id": "67a04e7ab6fd93f91c654593",
          "name": "Kevin Lin",
          "hidden": false
        },
        {
          "_id": "67a04e7ab6fd93f91c654594",
          "name": "Peter Lofgren",
          "hidden": false
        },
        {
          "_id": "67a04e7ab6fd93f91c654595",
          "name": "Francesco Mosconi",
          "hidden": false
        },
        {
          "_id": "67a04e7ab6fd93f91c654596",
          "name": "Clare O'Hara",
          "hidden": false
        },
        {
          "_id": "67a04e7ab6fd93f91c654597",
          "name": "Catherine Olsson",
          "hidden": false
        },
        {
          "_id": "67a04e7ab6fd93f91c654598",
          "name": "Linda Petrini",
          "hidden": false
        },
        {
          "_id": "67a04e7ab6fd93f91c654599",
          "name": "Samir Rajani",
          "hidden": false
        },
        {
          "_id": "67a04e7ab6fd93f91c65459a",
          "name": "Nikhil Saxena",
          "hidden": false
        },
        {
          "_id": "67a04e7ab6fd93f91c65459b",
          "name": "Alex Silverstein",
          "hidden": false
        },
        {
          "_id": "67a04e7ab6fd93f91c65459c",
          "name": "Tanya Singh",
          "hidden": false
        },
        {
          "_id": "67a04e7ab6fd93f91c65459d",
          "name": "Theodore Sumers",
          "hidden": false
        },
        {
          "_id": "67a04e7ab6fd93f91c65459e",
          "name": "Leonard Tang",
          "hidden": false
        },
        {
          "_id": "67a04e7ab6fd93f91c65459f",
          "name": "Kevin K. Troy",
          "hidden": false
        },
        {
          "_id": "67a04e7ab6fd93f91c6545a0",
          "name": "Constantin Weisser",
          "hidden": false
        },
        {
          "_id": "67a04e7ab6fd93f91c6545a1",
          "name": "Ruiqi Zhong",
          "hidden": false
        },
        {
          "_id": "67a04e7ab6fd93f91c6545a2",
          "name": "Giulio Zhou",
          "hidden": false
        },
        {
          "_id": "67a04e7ab6fd93f91c6545a3",
          "name": "Jan Leike",
          "hidden": false
        },
        {
          "_id": "67a04e7ab6fd93f91c6545a4",
          "name": "Jared Kaplan",
          "hidden": false
        },
        {
          "_id": "67a04e7ab6fd93f91c6545a5",
          "name": "Ethan Perez",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-31T01:09:32.000Z",
      "title": "Constitution de la Ville de Seseña : Diverses tests de freins pour éviter les accidents dans le municipalité, testés par un équipe de tests locaux.",
      "summary": "Les grands modèles de langue (LLMs) sont vulnérables aux jailbreaks universels. Cela permet aux utilisateurs de contourner systématiquement les dispositions de sécurité du modèle et de réaliser des processus nuisibles via plusieurs interfaces de modèle. Par exemple, la production illégale de grandes quantités de matériaux. Pour faire face à ces menaces, nous présentons au modèle des contenus autorisés et limités, conformément aux règles grammaticales établies dans la constitution, pour entraîner un dispositif de surveillance de sécurité nommé \"Class Filter\". Pendant environ 3 000 heures de test par l'équipe rouge, les participants de l'équipe rouge n'ont pas pu trouver des jailbreaks universels permettant l'extradition d'informations dans le LLM initial surveillé par le Class Filter. Dans l'évaluation automatique, le Class Filter renforcé a démontré une forte défense face à certains jailbreaks universels hors de l'intervalle. Ces Class Filters ont augmenté la taux de rejet des données produites en absolu de 0,38% et l'overcharge d'inférence de 23,7%, mais ont maintenu leur fonctionnalité efficace. Notre étude montre que c'est possible de prévenir les jailbreaks universels tout en maintenant la fonctionnalité efficace.",
      "upvotes": 2,
      "discussionId": "67a04e7bb6fd93f91c6545bc"
    },
    "publishedAt": "2025-02-03T00:05:21.087Z",
    "title": "Constitutional Classifiers: Defending against Universal Jailbreaks across Thousands of Hours of Red Teaming",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.18837.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5912
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.18841",
      "authors": [
        {
          "_id": "67a02c75221b701e4c04da7f",
          "name": "Wojciech Zaremba",
          "hidden": false
        },
        {
          "_id": "67a02c75221b701e4c04da80",
          "name": "Evgenia Nitishinskaya",
          "hidden": false
        },
        {
          "_id": "67a02c75221b701e4c04da81",
          "name": "Boaz Barak",
          "hidden": false
        },
        {
          "_id": "67a02c75221b701e4c04da82",
          "name": "Stephanie Lin",
          "hidden": false
        },
        {
          "_id": "67a02c75221b701e4c04da83",
          "name": "Sam Toyer",
          "hidden": false
        },
        {
          "_id": "67a02c75221b701e4c04da84",
          "name": "Yaodong Yu",
          "hidden": false
        },
        {
          "_id": "67a02c75221b701e4c04da85",
          "name": "Rachel Dias",
          "hidden": false
        },
        {
          "_id": "67a02c75221b701e4c04da86",
          "name": "Eric Wallace",
          "hidden": false
        },
        {
          "_id": "67a02c75221b701e4c04da87",
          "name": "Kai Xiao",
          "hidden": false
        },
        {
          "_id": "67a02c75221b701e4c04da88",
          "name": "Johannes Heidecke",
          "hidden": false
        },
        {
          "_id": "67a02c75221b701e4c04da89",
          "name": "Amelia Glaese",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-31T01:20:44.000Z",
      "title": "Infrastructure des transactions Temps de calcul et Changement de forteresse de défense",
      "summary": "Les expériences sont menées pour étudier la robustesse face aux attaques sur les modèles d'inférence (en particulier OpenAI o1-preview et o1-mini). Augmenter la quantité de calculs lors de l'inférence améliore la robustesse. Dans la plupart des cas (sauf quelques cas importants), en augmentant la quantité de calculs lors du test, la proportion de modèles que l'attaque réussit approche zéro. L'étude ne fait pas d'entraînement contre les attaques. De plus, pour augmenter la quantité de calculs lors de l'inférence, le modèle doit utiliser beaucoup de temps de calcul. Ce résultat montre la possibilité d'améliorer la robustesse des modèles de langage grands en augmentant la quantité de calculs. De plus, des nouveaux attaques sur les modèles d'inférence sont étudiés, si la quantité de calculs lors de l'inférence n'est pas liée à l'amélioration de la confiance, et les raisons et méthodes de réponse sont examinées.",
      "upvotes": 2,
      "discussionId": "67a02c76221b701e4c04daf5"
    },
    "publishedAt": "2025-02-02T21:40:11.158Z",
    "title": "Trading Inference-Time Compute for Adversarial Robustness",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.18841.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5912
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2404.07097",
      "authors": [
        {
          "_id": "67a07a4b605a6c919dea84ec",
          "name": "Yoni Kasten",
          "hidden": false
        },
        {
          "_id": "67a07a4b605a6c919dea84ed",
          "name": "Wuyue Lu",
          "hidden": false
        },
        {
          "_id": "67a07a4b605a6c919dea84ee",
          "name": "Haggai Maron",
          "hidden": false
        }
      ],
      "publishedAt": "2024-04-10T15:37:00.000Z",
      "title": "Rapide Encodeur-Base 3D à partir de Vidéos de Manière Casuelle par Traitement de Tracage de Points",
      "summary": "Cet article aborde un défi longue et complexe : la reconstruction de la structure 3D dans des contenus dynamiques de vidéos. Les méthodes actuelles fonctionnent avec des vidéos enregistrées par des caméras standard et ne nécessitent pas de longs temps d'optimisation.\n\nDans cet article, nous proposons une nouvelle approche basée sur l'apprentissage appelée TracksTo4D, avec l'objectif de améliorer significativement l'efficacité des méthodes précédentes. TracksTo4D utilise une seule passe vers l'avance efficace pour estimer la structure 3D et la position des caméras à partir du contenu dynamique de la vidéo. Pour atteindre cet objectif, nous manipulons directement le trajet de points 2D et proposons une architecture adaptée pour ce type de trajet. Cette architecture est basée sur deux principes clés : 1. Elle considère la symétrie inhérente dans les données de trajet de points 2D. 2. Elle suppose que les patrons de mouvement peuvent être effectivement représentés par des approximations de basse fréquence. TracksTo4D est entraîné sans limites en utilisant des ensembles de données de vidéos personnalisées qui incluent des étiquettes de sous-objets 3D extraites des points 2D de la vidéo. Les résultats des expériences montrent que TracksTo4D reconstruit la structure 3D et la position des caméras avec la précision des meilleurs méthodes actuelles, réduisant le temps d'exécution de 95%. De plus, TracksTo4D est capable de répondre à des catégories de signification non vues dans des vidéos non vues lors de la phase d'inférence.",
      "upvotes": 1,
      "discussionId": "67a07a4d605a6c919dea8555"
    },
    "publishedAt": "2025-02-03T03:12:19.292Z",
    "title": "Fast Encoder-Based 3D from Casual Videos via Point Track Processing",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.07097.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5f1158120c833276f61f1a84",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
      "fullname": "Niels Rogge",
      "name": "nielsr",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 742
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2411.04983",
      "authors": [
        {
          "_id": "67a0783a1b24595484396c4d",
          "name": "Gaoyue Zhou",
          "hidden": false
        },
        {
          "_id": "67a0783a1b24595484396c4e",
          "name": "Hengkai Pan",
          "hidden": false
        },
        {
          "_id": "67a0783a1b24595484396c4f",
          "name": "Yann LeCun",
          "hidden": false
        },
        {
          "_id": "67a0783a1b24595484396c50",
          "name": "Lerrel Pinto",
          "hidden": false
        }
      ],
      "publishedAt": "2024-11-07T18:54:37.000Z",
      "title": "DINO-WM : DINO-WM utilise un modèle de caractéristiques visuelles pré-entraîné sur un modèle de monde construit pour réaliser une planification à 0-shot.",
      "summary": "La capacité de prédire des résultats futurs est essentiellement importante pour des raisons physiques. Cependant, ces modèles de prédiction, généralement appelés 'World Model', sont difficiles à entraîner et sont souvent utilisés pour résoudre des tâches spécifiques et effectuer un apprentissage en ligne. Nous affirmons que le potentiel fondamental d'un World Model est la capacité de planifier pour résoudre divers problèmes. Concrètement, un World Model nécessite trois caractéristiques : 1) être capable d'être entraîné avec des trajectoires recueillies précédemment dans l'environnement, 2) soutenir l'optimisation des actions pendant le test, et 3) encourager des solutions qui ne dépendent pas de la tâche. Pour atteindre ceci, nous proposons le World Model DINO (DINO-WM). Le DINO-WM introduit une nouvelle façon de modéliser les actions visuelles et modélise le monde visuel sans le reconstruire. En utilisant les caractéristiques des patches spatiales prédites avec DINOv2, le DINO-WM peut être entraîné avec des trajectoires d'actions dans l'environnement et prédire les caractéristiques des patches futures. Ce design permet que le DINO-WM optimise les séquences d'actions pour atteindre des objectifs observés et estime les caractéristiques des patches objectifs pour promouvoir des plans d'action indépendants de la tâche. Le DINO-WM a été évalué dans diverses domaines, comme l'exploration de labyrinthes, le positionnement sur des tables et la manipulation de particules, et a démontré la capacité de générer des solutions d'action en zero-shot. Comparé à d'autres World Models avancés, le DINO-WM montre une forte capacité de généralisation et s'adapte à différentes familles de tâches, y compris des labyrinthes de conception arbitraire, des manipulations de positions et des scénarios avec plusieurs particules.",
      "upvotes": 1,
      "discussionId": "67a0783d1b24595484396cca"
    },
    "publishedAt": "2025-02-03T03:10:08.761Z",
    "title": "DINO-WM: World Models on Pre-trained Visual Features enable Zero-shot Planning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.04983.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5f1158120c833276f61f1a84",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
      "fullname": "Niels Rogge",
      "name": "nielsr",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 742
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.18128",
      "authors": [
        {
          "_id": "679e04b792d873dfa23d0ba6",
          "user": {
            "_id": "647d79a736e109abce419102",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647d79a736e109abce419102/S8Hby6eO4WdPQrct0Ix3c.png",
            "isPro": false,
            "fullname": "Abdurrahman Odabaşı",
            "user": "odabashi",
            "type": "user"
          },
          "name": "Abdurrahman Odabaşı",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-03T08:14:51.873Z",
          "hidden": false
        },
        {
          "_id": "679e04b792d873dfa23d0ba7",
          "name": "Göksel Biricik",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-30T04:20:16.000Z",
      "title": "L'objectif de la résumé des nouvelles sur la capacité des modèles de langue est de fournir une vision concise et précise des avancées et développements dans le domaine des modèles de langue, permettant aux lecteurs de comprendre rapidement les principes et applications de ces modèles dans divers domaines, tels que le traitement du langage naturel, l'apprentissage automatique et l'intelligence artificielle.",
      "summary": "Récemment, l'introduction de plusieurs modèles de langage et l'amélioration des tâches de traitement du langage naturel (NLP) ont constitué le contexte dans lequel cette recherche a été développée. Cette recherche fournit une évaluation détaillée de 20 modèles de langage récents, avec un accent particulier sur l'évaluation de modèles petits pour aborder le problème de résumé de nouvelles. Dans cette recherche, la capacité et l'efficacité de ces modèles pour résumer des articles de nouvelles de différents styles sont vérifiées systématiquement sur trois ensembles de données différents. En particulier, l'étude se concentre sur la configuration d'entraînement sans exemples (zero-shot) et avec peu d'exemples (few-shot), en appliquant une méthode d'évaluation robuste qui intègre des indicateurs d'évaluation automatique, une évaluation humaine et l'utilisation de LLM comme jury. Intéressamment, l'inclusion d'exemples dans la configuration avec peu d'exemples n'est pas seulement capable de améliorer le rendement du modèle, mais peut également dégrader la qualité des résumés générés. Ce problème est principalement attribué à la qualité réduite des résumés de référence, ce qui a un impact négatif sur le rendement du modèle. De plus, les résultats de cette recherche montrent des performances spéciales pour GPT-3.5-Turbo et GPT-4, qui se sont consolidés dans un domaine de capacités avancées. Cependant, dans l'évaluation globale des modèles, Qwen1.5-7B, SOLAR-10.7B-Instruct-v1.0, Meta-Llama-3-8B et Zephyr-7B-Beta montrent des résultats attendus et présentent un grand potentiel pour concourir avec des modèles plus grands dans le problème de résumé de nouvelles.",
      "upvotes": 0,
      "discussionId": "679e04b892d873dfa23d0bd3"
    },
    "publishedAt": "2025-02-03T04:01:13.509Z",
    "title": "Unraveling the Capabilities of Language Models in News Summarization",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.18128.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "647d79a736e109abce419102",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647d79a736e109abce419102/S8Hby6eO4WdPQrct0Ix3c.png",
      "fullname": "Abdurrahman Odabaşı",
      "name": "odabashi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  }
]