[
  {
    "paper": {
      "id": "2506.11924",
      "authors": [
        {
          "_id": "684faeba60b4a34dbe007ae2",
          "name": "Min-Seop Kwak",
          "hidden": false
        },
        {
          "_id": "684faeba60b4a34dbe007ae3",
          "name": "Junho Kim",
          "hidden": false
        },
        {
          "_id": "684faeba60b4a34dbe007ae4",
          "name": "Sangdoo Yun",
          "hidden": false
        },
        {
          "_id": "684faeba60b4a34dbe007ae5",
          "name": "Dongyoon Han",
          "hidden": false
        },
        {
          "_id": "684faeba60b4a34dbe007ae6",
          "name": "Taekyoung Kim",
          "hidden": false
        },
        {
          "_id": "684faeba60b4a34dbe007ae7",
          "name": "Seungryong Kim",
          "hidden": false
        },
        {
          "_id": "684faeba60b4a34dbe007ae8",
          "name": "Jin-Hwa Kim",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-13T16:19:00.000Z",
      "submittedOnDailyAt": "2025-06-16T04:13:42.201Z",
      "title": "Construcción d'Images et Jeitmoritos par Fusion d'Images Nouvelles et Jeitmoritos à l'aide de la Construction d'Attention Transversale Modale",
      "submittedOnDailyBy": {
        "_id": "642673f185f26ab94af4b422",
        "avatarUrl": "/avatars/289d611e0907f02f72d4e489468e039c.svg",
        "isPro": false,
        "fullname": "Bracio",
        "user": "bracio9623",
        "type": "user"
      },
      "summary": "Ce document présente un cadre de travail basé sur l'addition de couches. Ce cadre utilise des techniques de réflexion et d'inpainting pour générer des images et des génériques de nouvelles perspectives. Les méthodes existantes utilisaient des images avec des postures denses ou des modèles génératifs pour limiter la perspective spécifique. Cependant, notre méthode utilise un prédicteur générique simple pour prédire le générique des parties observées dans une image de référence et configure la synthèse de nouvelles perspectives de l'image et du générique comme tâche d'inpainting. Pour garantir une correspondance précise entre l'image générée et le générique, nous entraînons et inférons une carte d'attention dans la branche d'addition de couches de l'image, et introduisons une branche d'addition de couches générique en parallèle. Nous proposons un désagrégation d'attention modale croisée. Cette approche multi-tâche favorise la synthèse d'images résistantes au générique et une prédiction précise du générique. De plus, nous introduisons une attribution de conditions basée sur des points proches pour intégrer la profondeur et le positif, et nous filtrons les points censurés et les prédictions incorrectes du générique pour ne pas affecter le processus de génération. Expérimentalement, notre méthode réussit à effectuer une synthèse de vues externes de haute précision et fournit une reconstruction de la correspondance dans l'espace d'interpolation, créant des points de couleur symétriques dans les détails 3D du générique. La page du projet est disponible sur https://cvlab-kaist.github.io/MoAI.",
      "upvotes": 22,
      "discussionId": "684faebb60b4a34dbe007ae9",
      "projectPage": "https://cvlab-kaist.github.io/MoAI/",
      "githubRepo": "https://github.com/cvlab-kaist/MoAI",
      "ai_summary": "A diffusion-based framework generates aligned novel views of images and geometry using warping-and-inpainting with cross-modal attention distillation and proximity-based mesh conditioning, achieving high-fidelity synthesis and 3D completion.",
      "ai_keywords": [
        "diffusion-based framework",
        "warping-and-inpainting",
        "off-the-shelf geometry predictors",
        "cross-modal attention distillation",
        "proximity-based mesh conditioning",
        "novel-view synthesis",
        "multi-task approach",
        "geometrically robust image synthesis",
        "well-defined geometry prediction",
        "extrapolative view synthesis",
        "3D completion"
      ]
    },
    "publishedAt": "2025-06-13T12:19:00.000Z",
    "title": "Aligned Novel View Image and Geometry Synthesis via Cross-modal\n  Attention Instillation",
    "summary": "We introduce a diffusion-based framework that performs aligned novel view\nimage and geometry generation via a warping-and-inpainting methodology. Unlike\nprior methods that require dense posed images or pose-embedded generative\nmodels limited to in-domain views, our method leverages off-the-shelf geometry\npredictors to predict partial geometries viewed from reference images, and\nformulates novel-view synthesis as an inpainting task for both image and\ngeometry. To ensure accurate alignment between generated images and geometry,\nwe propose cross-modal attention distillation, where attention maps from the\nimage diffusion branch are injected into a parallel geometry diffusion branch\nduring both training and inference. This multi-task approach achieves\nsynergistic effects, facilitating geometrically robust image synthesis as well\nas well-defined geometry prediction. We further introduce proximity-based mesh\nconditioning to integrate depth and normal cues, interpolating between point\ncloud and filtering erroneously predicted geometry from influencing the\ngeneration process. Empirically, our method achieves high-fidelity\nextrapolative view synthesis on both image and geometry across a range of\nunseen scenes, delivers competitive reconstruction quality under interpolation\nsettings, and produces geometrically aligned colored point clouds for\ncomprehensive 3D completion. Project page is available at\nhttps://cvlab-kaist.github.io/MoAI.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.11924.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642673f185f26ab94af4b422",
      "avatarUrl": "/avatars/289d611e0907f02f72d4e489468e039c.svg",
      "fullname": "Bracio",
      "name": "bracio9623",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.09600",
      "authors": [
        {
          "_id": "684fca8160b4a34dbe007b4f",
          "name": "Itay Nakash",
          "hidden": false
        },
        {
          "_id": "684fca8160b4a34dbe007b50",
          "name": "George Kour",
          "hidden": false
        },
        {
          "_id": "684fca8160b4a34dbe007b51",
          "name": "Koren Lazar",
          "hidden": false
        },
        {
          "_id": "684fca8160b4a34dbe007b52",
          "name": "Matan Vetzler",
          "hidden": false
        },
        {
          "_id": "684fca8160b4a34dbe007b53",
          "name": "Guy Uziel",
          "hidden": false
        },
        {
          "_id": "684fca8160b4a34dbe007b54",
          "name": "Ateret Anaby-Tavor",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/671f8106d677d3a764a6f9a5/99oCW2IrMaCeLyuyfgvbG.png"
      ],
      "publishedAt": "2025-06-11T10:59:47.000Z",
      "submittedOnDailyAt": "2025-06-16T06:16:02.507Z",
      "title": "Effectifément la thématisation des agents de conformité à la politique",
      "submittedOnDailyBy": {
        "_id": "671f8106d677d3a764a6f9a5",
        "avatarUrl": "/avatars/90b4b00058aac30c060c5eac8debb1c7.svg",
        "isPro": false,
        "fullname": "itay nakash",
        "user": "itaynakash",
        "type": "user"
      },
      "summary": "Les agents basés sur des modèles de langage grands (LLM) axés sur la tâche sont en augmentation dans des domaines strictement réglementés comme les remboursements avec publicités et les règles de cancellation. Ce défi consiste à maintenir une interaction naturelle tout en ces agents exécutent ces règles de manière cohérente et rejettent les demandes qui violent ces mêmes règles. Pour y parvenir, il est nécessaire de développer des méthodes de conception et d'évaluation permettant aux agents de maintenir leur résistance face aux actions malicieuses de utilisateurs. Nous proposons un nouveau modèle de tour pour faire face aux utilisateurs hostiles avec l'objectif de respecter les politiques. Pour cela, nous présentons le système CRAFT, un groupe d'agents rouges qui utilise des stratégies persuasives liées aux politiques pour détruire les services. Cela dépasse les méthodes traditionnelles comme le DAN, la manipulation émotionnelle et la coercition. Basé sur le tau-bench, nous introduisons le tau-break comme un cadre d'évaluation pour mesurer la résistance des agents face aux actions manipulées par les utilisateurs. Enfin, nous évaluons diverses stratégies de défense simples mais efficaces. Bien que ces mesures fournissent une certaine protection, une forte base de recherche est nécessaire pour obtenir une certitude de sécurité.",
      "upvotes": 15,
      "discussionId": "684fca8160b4a34dbe007b55",
      "ai_summary": "CRAFT, a multi-agent system using policy-aware persuasive strategies, challenges policy-adherent LLM-based agents in customer service to assess and improve their robustness against adversarial attacks.",
      "ai_keywords": [
        "LLM-based agents",
        "policy-adherence",
        "adversarial users",
        "CRAFT",
        "multi-agent red-teaming",
        "policy-aware persuasive strategies",
        "DAN prompts",
        "emotional manipulation",
        "coercive",
        "tau-break",
        "defense strategies",
        "adversarial attacks"
      ]
    },
    "publishedAt": "2025-06-11T06:59:47.000Z",
    "title": "Effective Red-Teaming of Policy-Adherent Agents",
    "summary": "Task-oriented LLM-based agents are increasingly used in domains with strict\npolicies, such as refund eligibility or cancellation rules. The challenge lies\nin ensuring that the agent consistently adheres to these rules and policies,\nappropriately refusing any request that would violate them, while still\nmaintaining a helpful and natural interaction. This calls for the development\nof tailored design and evaluation methodologies to ensure agent resilience\nagainst malicious user behavior. We propose a novel threat model that focuses\non adversarial users aiming to exploit policy-adherent agents for personal\nbenefit. To address this, we present CRAFT, a multi-agent red-teaming system\nthat leverages policy-aware persuasive strategies to undermine a\npolicy-adherent agent in a customer-service scenario, outperforming\nconventional jailbreak methods such as DAN prompts, emotional manipulation, and\ncoercive. Building upon the existing tau-bench benchmark, we introduce\ntau-break, a complementary benchmark designed to rigorously assess the agent's\nrobustness against manipulative user behavior. Finally, we evaluate several\nstraightforward yet effective defense strategies. While these measures provide\nsome protection, they fall short, highlighting the need for stronger,\nresearch-driven safeguards to protect policy-adherent agents from adversarial\nattacks",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/671f8106d677d3a764a6f9a5/99oCW2IrMaCeLyuyfgvbG.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09600.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "671f8106d677d3a764a6f9a5",
      "avatarUrl": "/avatars/90b4b00058aac30c060c5eac8debb1c7.svg",
      "fullname": "itay nakash",
      "name": "itaynakash",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.10892",
      "authors": [
        {
          "_id": "684fb2f060b4a34dbe007aeb",
          "name": "Subham Sekhar Sahoo",
          "hidden": false
        },
        {
          "_id": "684fb2f060b4a34dbe007aec",
          "name": "Justin Deschenaux",
          "hidden": false
        },
        {
          "_id": "684fb2f060b4a34dbe007aed",
          "name": "Aaron Gokaslan",
          "hidden": false
        },
        {
          "_id": "684fb2f060b4a34dbe007aee",
          "name": "Guanghan Wang",
          "hidden": false
        },
        {
          "_id": "684fb2f060b4a34dbe007aef",
          "name": "Justin Chiu",
          "hidden": false
        },
        {
          "_id": "684fb2f060b4a34dbe007af0",
          "name": "Volodymyr Kuleshov",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/661839d73b412cdc851299c1/GmIlLMVIuyWjydykQPOt2.png",
        "https://cdn-uploads.huggingface.co/production/uploads/661839d73b412cdc851299c1/TjIhoD3hxygzenitTi75x.qt"
      ],
      "publishedAt": "2025-06-12T16:55:35.000Z",
      "submittedOnDailyAt": "2025-06-16T04:40:29.065Z",
      "title": "Diffusion Duality\n\nDualité de la Diffusion",
      "submittedOnDailyBy": {
        "_id": "661839d73b412cdc851299c1",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/661839d73b412cdc851299c1/xicwANPQPTFdWfblisL2-.png",
        "isPro": false,
        "fullname": "Subham Sekhar Sahoo",
        "user": "s-sahoo",
        "type": "user"
      },
      "summary": "Le modèle de diffusion discret unifié a la capacité d'ajustement automatique pour générer des phrases rapidement, mais généralement présente un rendement inférieur en comparaison avec les modèles séquentiels automatiques et les modèles de diffusion avec masque. Dans cet article, des points clés importants sont utilisés pour réduire cette différence de rendement. Le processus de diffusion unifié développe de manière naturelle sous la diffusion bayésienne, ce qui est peu valorisé. Notre méthode, Duo, transmet des technologies puissantes provenant de la diffusion bayésienne et améliore à la fois l'entraînement et la sampling. Tout d'abord, grâce à la stratégie d'entraînement du marcheur, qui utilise le processus bayésien pour réduire la variance, la vitesse d'entraînement a été augmentée d'un facteur de 2. Le modèle entraîné par le marcheur dépasse le modèle séquentiel automatique dans 3 des 7 benchmarks en perplexité 0 shot. De plus, un algorithme de stylisation discret applicable est proposé, qui accélère la sampling en deux étapes, facilitant la génération de quelques pas dans les modèles de diffusion de langage. Le site web du projet offre le code et les checkpoints du modèle : http://s-sahoo.github.io/duo",
      "upvotes": 8,
      "discussionId": "684fb2f060b4a34dbe007af1",
      "projectPage": "https://s-sahoo.com/duo/",
      "githubRepo": "https://github.com/s-sahoo/duo",
      "ai_summary": "Duo improves uniform-state discrete diffusion models by transferring techniques from Gaussian diffusion, enhancing training speed and enabling fast few-step text generation.",
      "ai_keywords": [
        "discrete diffusion models",
        "Gaussian diffusion",
        "curriculum learning",
        "Discrete Consistency Distillation",
        "zero-shot perplexity",
        "few-step generation"
      ]
    },
    "publishedAt": "2025-06-12T12:55:35.000Z",
    "title": "The Diffusion Duality",
    "summary": "Uniform-state discrete diffusion models hold the promise of fast text\ngeneration due to their inherent ability to self-correct. However, they are\ntypically outperformed by autoregressive models and masked diffusion models. In\nthis work, we narrow this performance gap by leveraging a key insight:\nUniform-state diffusion processes naturally emerge from an underlying Gaussian\ndiffusion. Our method, Duo, transfers powerful techniques from Gaussian\ndiffusion to improve both training and sampling. First, we introduce a\ncurriculum learning strategy guided by the Gaussian process, doubling training\nspeed by reducing variance. Models trained with curriculum learning surpass\nautoregressive models in zero-shot perplexity on 3 of 7 benchmarks. Second, we\npresent Discrete Consistency Distillation, which adapts consistency\ndistillation from the continuous to the discrete setting. This algorithm\nunlocks few-step generation in diffusion language models by accelerating\nsampling by two orders of magnitude. We provide the code and model checkpoints\non the project page: http://s-sahoo.github.io/duo",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/661839d73b412cdc851299c1/GmIlLMVIuyWjydykQPOt2.png",
      "https://cdn-uploads.huggingface.co/production/uploads/661839d73b412cdc851299c1/TjIhoD3hxygzenitTi75x.qt"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10892.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "661839d73b412cdc851299c1",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/661839d73b412cdc851299c1/xicwANPQPTFdWfblisL2-.png",
      "fullname": "Subham Sekhar Sahoo",
      "name": "s-sahoo",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.11928",
      "authors": [
        {
          "_id": "684fae8d60b4a34dbe007acd",
          "name": "Zihan Zheng",
          "hidden": false
        },
        {
          "_id": "684fae8d60b4a34dbe007ace",
          "name": "Zerui Cheng",
          "hidden": false
        },
        {
          "_id": "684fae8d60b4a34dbe007acf",
          "name": "Zeyu Shen",
          "hidden": false
        },
        {
          "_id": "684fae8d60b4a34dbe007ad0",
          "name": "Shang Zhou",
          "hidden": false
        },
        {
          "_id": "684fae8d60b4a34dbe007ad1",
          "name": "Kaiyuan Liu",
          "hidden": false
        },
        {
          "_id": "684fae8d60b4a34dbe007ad2",
          "name": "Hansen He",
          "hidden": false
        },
        {
          "_id": "684fae8d60b4a34dbe007ad3",
          "name": "Dongruixuan Li",
          "hidden": false
        },
        {
          "_id": "684fae8d60b4a34dbe007ad4",
          "name": "Stanley Wei",
          "hidden": false
        },
        {
          "_id": "684fae8d60b4a34dbe007ad5",
          "name": "Hangyi Hao",
          "hidden": false
        },
        {
          "_id": "684fae8d60b4a34dbe007ad6",
          "name": "Jianzhu Yao",
          "hidden": false
        },
        {
          "_id": "684fae8d60b4a34dbe007ad7",
          "name": "Peiyao Sheng",
          "hidden": false
        },
        {
          "_id": "684fae8d60b4a34dbe007ad8",
          "name": "Zixuan Wang",
          "hidden": false
        },
        {
          "_id": "684fae8d60b4a34dbe007ad9",
          "name": "Wenhao Chai",
          "hidden": false
        },
        {
          "_id": "684fae8d60b4a34dbe007ada",
          "name": "Aleksandra Korolova",
          "hidden": false
        },
        {
          "_id": "684fae8d60b4a34dbe007adb",
          "name": "Peter Henderson",
          "hidden": false
        },
        {
          "_id": "684fae8d60b4a34dbe007adc",
          "name": "Sanjeev Arora",
          "hidden": false
        },
        {
          "_id": "684fae8d60b4a34dbe007add",
          "name": "Pramod Viswanath",
          "hidden": false
        },
        {
          "_id": "684fae8d60b4a34dbe007ade",
          "name": "Jingbo Shang",
          "hidden": false
        },
        {
          "_id": "684fae8d60b4a34dbe007adf",
          "name": "Saining Xie",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-13T16:29:09.000Z",
      "submittedOnDailyAt": "2025-06-16T04:13:30.111Z",
      "title": "LiveCodeBench Pro : Liste des Médailles de l'Olympiade sur comment les participants du programme d'évaluation des LLMs évaluent les LLMs",
      "submittedOnDailyBy": {
        "_id": "637c7503fe115289cfecbe6b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676361945047-637c7503fe115289cfecbe6b.jpeg",
        "isPro": false,
        "fullname": "Wenhao Chai",
        "user": "wchai",
        "type": "user"
      },
      "summary": "Selon un rapport récent, les modèles de langage grands (LLMs) dépassent les gagnants de la programmation compétitive. Cette affirmation est en cours d'examen en se basant sur le savoir-faire des groupes de médaillés des événements internationaux d'algorithmes. On cherche à déterminer les différences entre les LLMs et les experts humains, ainsi que les limites de ces derniers. On présente LiveCodeBench Pro, un cadre de test basé sur des problèmes de Codeforces, ICPC et IOI, mis à jour de manière continue pour réduire la contamination des données. Les équipes de médaillés notent les problèmes par catégorie d'algorithmes et présentent des modèles qui échouent. Avec ce nouveau ensemble de données et ce cadre de test, les modèles de chef atteignent un 53% de succès sur des problèmes difficiles sans utiliser des outils externes, ce qui est un domaine des experts humains. De plus, les LLMs peuvent résoudre des problèmes complexes mais souvent échouent dans l'analyse de raisons et créent des raisons incorrectes. Le haut rendement est attribué à la précision de l'implémentation et à l'étendue des outils, mais pas à la raison correcte. LiveCodeBench Pro clairement montre les différences avec les Grand Masters humains et fournit de manière continue des orientations pour l'amélioration future des raisons des LLMs dans les centres de code.",
      "upvotes": 6,
      "discussionId": "684fae8d60b4a34dbe007ae0",
      "ai_summary": "LLMs perform well on implementation-heavy competitive programming problems but struggle with nuanced algorithmic reasoning, as highlighted by LiveCodeBench Pro.",
      "ai_keywords": [
        "large language models",
        "LLMs",
        "competitive programming",
        "LiveCodeBench Pro",
        "Codeforces",
        "ICPC",
        "IOI",
        "algorithmic categories",
        "algorithmic reasoning",
        "case analysis"
      ]
    },
    "publishedAt": "2025-06-13T12:29:09.000Z",
    "title": "LiveCodeBench Pro: How Do Olympiad Medalists Judge LLMs in Competitive\n  Programming?",
    "summary": "Recent reports claim that large language models (LLMs) now outperform elite\nhumans in competitive programming. Drawing on knowledge from a group of\nmedalists in international algorithmic contests, we revisit this claim,\nexamining how LLMs differ from human experts and where limitations still\nremain. We introduce LiveCodeBench Pro, a benchmark composed of problems from\nCodeforces, ICPC, and IOI that are continuously updated to reduce the\nlikelihood of data contamination. A team of Olympiad medalists annotates every\nproblem for algorithmic categories and conducts a line-by-line analysis of\nfailed model-generated submissions. Using this new data and benchmark, we find\nthat frontier models still have significant limitations: without external\ntools, the best model achieves only 53% pass@1 on medium-difficulty problems\nand 0% on hard problems, domains where expert humans still excel. We also find\nthat LLMs succeed at implementation-heavy problems but struggle with nuanced\nalgorithmic reasoning and complex case analysis, often generating confidently\nincorrect justifications. High performance appears largely driven by\nimplementation precision and tool augmentation, not superior reasoning.\nLiveCodeBench Pro thus highlights the significant gap to human grandmaster\nlevels, while offering fine-grained diagnostics to steer future improvements in\ncode-centric LLM reasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.11928.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "637c7503fe115289cfecbe6b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676361945047-637c7503fe115289cfecbe6b.jpeg",
      "fullname": "Wenhao Chai",
      "name": "wchai",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 34
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.11997",
      "authors": [
        {
          "_id": "684fd0cb60b4a34dbe007b70",
          "user": {
            "_id": "6333650673c07e8aebb2e941",
            "avatarUrl": "/avatars/bfcc236641671e88c2fe5426740071d3.svg",
            "isPro": false,
            "fullname": "Korbinian Poeppel",
            "user": "korbip",
            "type": "user"
          },
          "name": "Korbinian Pöppel",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-16T08:35:40.461Z",
          "hidden": false
        },
        {
          "_id": "684fd0cb60b4a34dbe007b71",
          "name": "Richard Freinschlag",
          "hidden": false
        },
        {
          "_id": "684fd0cb60b4a34dbe007b72",
          "name": "Thomas Schmied",
          "hidden": false
        },
        {
          "_id": "684fd0cb60b4a34dbe007b73",
          "name": "Wei Lin",
          "hidden": false
        },
        {
          "_id": "684fd0cb60b4a34dbe007b74",
          "name": "Sepp Hochreiter",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-13T17:51:37.000Z",
      "submittedOnDailyAt": "2025-06-16T06:38:46.139Z",
      "title": "pLSTM : Réseau de Marqueurs de Mouvement de Source Linéaire Parallélisable",
      "submittedOnDailyBy": {
        "_id": "64c3849269b1a6796052eac7",
        "avatarUrl": "/avatars/9f0c832d5b51b659c7bb83074f02a648.svg",
        "isPro": false,
        "fullname": "Thomas Schmied",
        "user": "thomasschmied",
        "type": "user"
      },
      "summary": "Récemment, des architectures récentes telles que xLSTM et Mamba challengent le Transformer dans le domaine du modélisation de langues. Cependant, ces structures seulement s'appliquent à des séquences et nécessitent de traiter des données de structures multidimensionnelles comme des images ou des graphes moléculaires de manière prédéfinie. D'autre part, les Réseaux Recurrents Multidimensionnels (MDRNNs) sont adaptés aux données de haute dimension, comme les graphes 2D, les arbres et les graphes acycliques orientés (DAG). Dans cet article, nous étendons la linéarité des réseaux recurrents pour incorporer la multidimensionnalité. En utilisant des portes Source, Transition et Mark qui agissent sur des graphes linéaires généraux de DAG, nous introduisons les Réseaux Linéaires Source Transition Mark parallèlables (pLSTMs). Ainsi, nous atteignons une parallélisation similaire à celle des RNN linéaires séquentielles, mais applicable également à des DAG. Il est possible de les implémenter efficacement pour des graphes 1D et 2D en utilisant des opérations einsum, de la combinaison et du remplissage logique de temps. Pour résoudre les problèmes d'activation et de désaccélération/accélération à longue distance dans les DAG, les pLSTMs utilisent deux modes différents : le mode de propagation dirigée (P-mode) et le mode de distribution diffusive (D-mode). Pour démontrer leur capacité à longue distance, nous introduisons des tâches complexes de calcul graphique qui incluent des informations dirigées à longue distance, comme \"la prononciation des marqueurs\". Les pLSTMs s'adaptent bien aux données d'images de grande taille et sont plus efficaces que les Transformers en termes de prononciation. Ils montrent également des résultats excellents sur les benchmarks de graphes moléculaires et de calcul graphique. Le code et les ensembles de données sont disponibles sur la URL suivante : https://github.com/ml-jku/plstm_experiments.",
      "upvotes": 4,
      "discussionId": "684fd0cb60b4a34dbe007b75",
      "ai_summary": "pLSTMs are parallelizable linear RNNs designed for DAGs, demonstrating superior performance on long-range tasks and benchmarks compared to Transformers.",
      "ai_keywords": [
        "xLSTM",
        "Mamba",
        "Transformer",
        "Multi-Dimensional RNNs",
        "MDRNNs",
        "parallelizable Linear Source Transition Mark networks",
        "pLSTMs",
        "Source gates",
        "Transition gates",
        "Mark gates",
        "line graph",
        "DAGs",
        "parallel associative scans",
        "chunkwise-recurrent",
        "einsum operations",
        "concatenations",
        "padding",
        "vanishing/exploding activation/gradient problem",
        "directed propagation mode",
        "diffusive distribution mode",
        "arrow-pointing extrapolation",
        "computer vision task",
        "molecular graph",
        "performance benchmarks"
      ]
    },
    "publishedAt": "2025-06-13T13:51:37.000Z",
    "title": "pLSTM: parallelizable Linear Source Transition Mark networks",
    "summary": "Modern recurrent architectures, such as xLSTM and Mamba, have recently\nchallenged the Transformer in language modeling. However, their structure\nconstrains their applicability to sequences only or requires processing\nmulti-dimensional data structures, such as images or molecular graphs, in a\npre-defined sequential order. In contrast, Multi-Dimensional RNNs (MDRNNs) are\nwell suited for data with a higher level structure, like 2D grids, trees, and\ndirected acyclic graphs (DAGs). In this work, we extend the notion of\nmulti-dimensionality to linear RNNs. We introduce parallelizable Linear Source\nTransition Mark networks (pLSTMs) using Source, Transition, and Mark gates that\nact on the line graph of a general DAG. This enables parallelization in analogy\nto parallel associative scans and the chunkwise-recurrent form of sequential\nlinear RNNs, but for DAGs. For regular grids (1D and 2D), like images, this\nscheme can be efficiently implemented using einsum operations, concatenations,\nand padding in logarithmic time. pLSTMs tackle the vanishing/exploding\nactivation/gradient problem for long distances in DAGs via two distinct modes:\na directed propagation mode (P-mode) and a diffusive distribution mode\n(D-mode). To showcase the long-range capabilities of pLSTM, we introduce\narrow-pointing extrapolation as a synthetic computer vision task that contains\nlong-distance directional information. We demonstrate that pLSTMs generalize\nwell to larger image sizes, whereas Transformers struggle to extrapolate. On\nestablished molecular graph and computer vision benchmarks, pLSTMs also show\nstrong performance. Code and Datasets are available at:\nhttps://github.com/ml-jku/plstm_experiments.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.11997.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64c3849269b1a6796052eac7",
      "avatarUrl": "/avatars/9f0c832d5b51b659c7bb83074f02a648.svg",
      "fullname": "Thomas Schmied",
      "name": "thomasschmied",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.09427",
      "authors": [
        {
          "_id": "684fa6d060b4a34dbe007aa7",
          "user": {
            "_id": "66d94f2a36aa5055694dfe04",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/grAN83brH0E4_S0__yLdv.jpeg",
            "isPro": false,
            "fullname": "fengyukang",
            "user": "finyorko",
            "type": "user"
          },
          "name": "Yukang Feng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-16T07:48:14.381Z",
          "hidden": false
        },
        {
          "_id": "684fa6d060b4a34dbe007aa8",
          "name": "Jianwen Sun",
          "hidden": false
        },
        {
          "_id": "684fa6d060b4a34dbe007aa9",
          "user": {
            "_id": "6533f7ecb3852ed1ceb48e47",
            "avatarUrl": "/avatars/5d767c093e73f06a89f625c3a5903902.svg",
            "isPro": false,
            "fullname": "Chuanhao Li",
            "user": "cyrilli",
            "type": "user"
          },
          "name": "Chuanhao Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-16T07:48:30.156Z",
          "hidden": false
        },
        {
          "_id": "684fa6d060b4a34dbe007aaa",
          "name": "Zizhen Li",
          "hidden": false
        },
        {
          "_id": "684fa6d060b4a34dbe007aab",
          "name": "Jiaxin Ai",
          "hidden": false
        },
        {
          "_id": "684fa6d060b4a34dbe007aac",
          "user": {
            "_id": "665305eff0c8c891cae7fe01",
            "avatarUrl": "/avatars/1f372e3bc6a4eb19ef702ec96a391c96.svg",
            "isPro": false,
            "fullname": "Fanrui Zhang",
            "user": "fanrui00",
            "type": "user"
          },
          "name": "Fanrui Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-16T07:49:04.386Z",
          "hidden": false
        },
        {
          "_id": "684fa6d060b4a34dbe007aad",
          "name": "Yifan Chang",
          "hidden": false
        },
        {
          "_id": "684fa6d060b4a34dbe007aae",
          "name": "Sizhuo Zhou",
          "hidden": false
        },
        {
          "_id": "684fa6d060b4a34dbe007aaf",
          "user": {
            "_id": "6674d02914e2aebef893779e",
            "avatarUrl": "/avatars/acdbe3820462b87126c8f1e14f0d1a60.svg",
            "isPro": false,
            "fullname": "ZhangShenglin",
            "user": "ZhangShenglin",
            "type": "user"
          },
          "name": "Shenglin Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-16T07:49:28.741Z",
          "hidden": false
        },
        {
          "_id": "684fa6d060b4a34dbe007ab0",
          "name": "Yu Dai",
          "hidden": false
        },
        {
          "_id": "684fa6d060b4a34dbe007ab1",
          "user": {
            "_id": "63527f4e7d071f23d085ad45",
            "avatarUrl": "/avatars/99a51adef5673b3ac1a8c02eb47759c4.svg",
            "isPro": false,
            "fullname": "KAIPENG ZHANG",
            "user": "kpzhang",
            "type": "user"
          },
          "name": "Kaipeng Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-16T07:49:35.126Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/65f1713552c38a91e0a445e8/1FHKfzv4w4VzV4nhqKCJ7.png"
      ],
      "publishedAt": "2025-06-11T06:21:20.000Z",
      "submittedOnDailyAt": "2025-06-16T03:49:15.860Z",
      "title": "Génération en temps réel d'images de personnages basées sur des ensembles de données de haute qualité et des évaluations fiables",
      "submittedOnDailyBy": {
        "_id": "65f1713552c38a91e0a445e8",
        "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
        "isPro": false,
        "fullname": "kaipeng",
        "user": "kpzhang996",
        "type": "user"
      },
      "summary": "Le développement récent de grands modèles multimodal (LMMs) a atteint des améliorations notables dans la compréhension et la génération de ces modèles. Cependant, ces modèles rencontrent des difficultés dans la création d'images-texte répétés en raison des limites de l'échelle, de la qualité et de la richesse des ensembles de données d'entraînement actuels. Pour aborder ces limites, nous présentons un grand ensemble de données de modèles multimodal (LMMs) construit en utilisant le méthode d'autoévaluation avec refinement itératif (SEIR). InterSyn est caractérisé par des réponses qui incluent des images-texte répétés, des dialogues dirigés par instructions, une grande diversité d'objets et un contrôle stricte de l'édition automatique de la masse, ce qui l'adapte pour l'entraînement de LMMs dans des instructions futures. De plus, en réponse à la manque de outils fiables pour évaluer les résultats multimodales répétés, nous présentons SynJudge. SynJudge est un modèle d'évaluation automatique qui quantifie de manière quadratique le contenu textuel, le contenu de l'image, la qualité de l'image et l'harmonie entre l'image et le texte.\n\nA travers des études expérimentales, nous avons démontré que le méthode SEIR atteint un amélioration significative de la qualité du ensemble de données sans nécessité d'éditions. De plus, les LMMs entraînés sur InterSyn montrent une amélioration constante dans tous les indicateurs d'évaluation et confirment l'adéquation de leur développement comme système multimodal.",
      "upvotes": 4,
      "discussionId": "684fa6d060b4a34dbe007ab2",
      "ai_summary": "InterSyn, a large-scale dataset with tightly interleaved image-text outputs and automated quality refinement, improves multimodal understanding and generation through the SEIR method and SynJudge, an automatic evaluation tool.",
      "ai_keywords": [
        "Large Multimodal Models (LMMs)",
        "multimodal understanding",
        "multimodal generation",
        "Self-Evaluation with Iterative Refinement (SEIR)",
        "InterSyn",
        "image-text outputs",
        "SynJudge",
        "text content",
        "image content",
        "image quality",
        "image-text synergy"
      ]
    },
    "publishedAt": "2025-06-11T02:21:20.000Z",
    "title": "A High-Quality Dataset and Reliable Evaluation for Interleaved\n  Image-Text Generation",
    "summary": "Recent advancements in Large Multimodal Models (LMMs) have significantly\nimproved multimodal understanding and generation. However, these models still\nstruggle to generate tightly interleaved image-text outputs, primarily due to\nthe limited scale, quality and instructional richness of current training\ndatasets. To address this, we introduce InterSyn, a large-scale multimodal\ndataset constructed using our Self-Evaluation with Iterative Refinement (SEIR)\nmethod. InterSyn features multi-turn, instruction-driven dialogues with tightly\ninterleaved imagetext responses, providing rich object diversity and rigorous\nautomated quality refinement, making it well-suited for training\nnext-generation instruction-following LMMs. Furthermore, to address the lack of\nreliable evaluation tools capable of assessing interleaved multimodal outputs,\nwe introduce SynJudge, an automatic evaluation model designed to quantitatively\nassess multimodal outputs along four dimensions: text content, image content,\nimage quality, and image-text synergy.\n  Experimental studies show that the SEIR method leads to substantially higher\ndataset quality compared to an otherwise identical process without refinement.\n  Moreover, LMMs trained on InterSyn achieve uniform performance gains across\nall evaluation metrics, confirming InterSyn's utility for advancing multimodal\nsystems.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/65f1713552c38a91e0a445e8/1FHKfzv4w4VzV4nhqKCJ7.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09427.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65f1713552c38a91e0a445e8",
      "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
      "fullname": "kaipeng",
      "name": "kpzhang996",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.09366",
      "authors": [
        {
          "_id": "684ae246dbd21a9cc27b111c",
          "user": {
            "_id": "62359088a17d7271859c88f4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1647677549197-noauth.jpeg",
            "isPro": false,
            "fullname": "Yuxuan Kuang",
            "user": "yxK",
            "type": "user"
          },
          "name": "Yuxuan Kuang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-16T07:17:03.322Z",
          "hidden": false
        },
        {
          "_id": "684ae246dbd21a9cc27b111d",
          "name": "Haoran Geng",
          "hidden": false
        },
        {
          "_id": "684ae246dbd21a9cc27b111e",
          "user": {
            "_id": "64da71311d19239f50483005",
            "avatarUrl": "/avatars/d97a7177adca180d795bf0f9ec66c65c.svg",
            "isPro": false,
            "fullname": "Amine Elhafsi",
            "user": "AmineElhafsi",
            "type": "user"
          },
          "name": "Amine Elhafsi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-16T07:50:59.675Z",
          "hidden": false
        },
        {
          "_id": "684ae246dbd21a9cc27b111f",
          "name": "Tan-Dzung Do",
          "hidden": false
        },
        {
          "_id": "684ae246dbd21a9cc27b1120",
          "name": "Pieter Abbeel",
          "hidden": false
        },
        {
          "_id": "684ae246dbd21a9cc27b1121",
          "user": {
            "_id": "65369a95605a07338de78ab0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/sGFjOjLT2akN-sn5beVWL.jpeg",
            "isPro": false,
            "fullname": "Jitendra Malik ",
            "user": "jitendra1995",
            "type": "user"
          },
          "name": "Jitendra Malik",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-16T07:51:18.391Z",
          "hidden": false
        },
        {
          "_id": "684ae246dbd21a9cc27b1122",
          "name": "Marco Pavone",
          "hidden": false
        },
        {
          "_id": "684ae246dbd21a9cc27b1123",
          "name": "Yue Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-11T03:24:26.000Z",
      "submittedOnDailyAt": "2025-06-16T04:37:55.714Z",
      "title": "Bridging des Compétences : Elle est utilisée pour la manipulation générale de différents homéoïdes en utilisant des techniques de pontage des compétences.",
      "submittedOnDailyBy": {
        "_id": "62359088a17d7271859c88f4",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1647677549197-noauth.jpeg",
        "isPro": false,
        "fullname": "Yuxuan Kuang",
        "user": "yxK",
        "type": "user"
      },
      "summary": "Les robots d'animaux de petite taille ont un potentiel important en raison de leur flexibilité et de leur forme humain, ce qui leur permet de réaliser des tâches quotidiennes dans différents environnements. Récemment, les études ont utilisé le contrôle optimal et l'apprentissage par renforcement pour avancer dans le contrôle complet et le mouvement des robots. Cependant, ces méthodes nécessitent un ajustement complexe pour chaque tâche et limitent la diversité et l'échelle pour plusieurs tâches. Par conséquent, nous présentons un nouveau cadre d'apprentissage par renforcement en étapes appelé SkillBlender, qui garantit la diversité dans les tâches de mouvement des robots. SkillBlender apprend d'abord des compétences basiques indépendantes de la tâche et combine ces compétences dynamiquement, évitant la nécessité de concevoir des récompenses spécifiques pour chaque tâche. De plus, nous présentons SkillBench, une batterie de tests pour évaluer différents types de corps, qui inclut 3 corps, 4 compétences basiques et 8 tâches de mouvement difficiles, offrant un critère scientifique équilibré de précision et de possibilité. Les résultats des tests de robots d'animaux de petite taille montrent que nos méthodes dépassent significativement les limites des baselines, normalisent le comportement et évitent le renforcement de la récompense, démontrant une amélioration de la précision du mouvement dans des scénarios quotidiens. Notre code et les benchmarks seront publiés pour encourager futures recherches. Page du projet : https://usc-gvl.github.io/SkillBlender-web/",
      "upvotes": 3,
      "discussionId": "684ae246dbd21a9cc27b1124",
      "projectPage": "https://usc-gvl.github.io/SkillBlender-web/",
      "githubRepo": "https://github.com/Humanoid-SkillBlender/SkillBlender",
      "ai_summary": "SkillBlender is a hierarchical reinforcement learning framework that uses pretrained primitive skills to efficiently solve diverse loco-manipulation tasks for humanoid robots.",
      "ai_keywords": [
        "reinforcement learning",
        "SkillBlender",
        "goal-conditioned",
        "task-agnostic primitive skills",
        "hierarchical reinforcement learning",
        "SkillBench",
        "cross-embodiment",
        "simulated benchmark",
        "loco-manipulation tasks",
        "reward engineering",
        "reward hacking"
      ]
    },
    "publishedAt": "2025-06-10T23:24:26.000Z",
    "title": "SkillBlender: Towards Versatile Humanoid Whole-Body Loco-Manipulation\n  via Skill Blending",
    "summary": "Humanoid robots hold significant potential in accomplishing daily tasks\nacross diverse environments thanks to their flexibility and human-like\nmorphology. Recent works have made significant progress in humanoid whole-body\ncontrol and loco-manipulation leveraging optimal control or reinforcement\nlearning. However, these methods require tedious task-specific tuning for each\ntask to achieve satisfactory behaviors, limiting their versatility and\nscalability to diverse tasks in daily scenarios. To that end, we introduce\nSkillBlender, a novel hierarchical reinforcement learning framework for\nversatile humanoid loco-manipulation. SkillBlender first pretrains\ngoal-conditioned task-agnostic primitive skills, and then dynamically blends\nthese skills to accomplish complex loco-manipulation tasks with minimal\ntask-specific reward engineering. We also introduce SkillBench, a parallel,\ncross-embodiment, and diverse simulated benchmark containing three embodiments,\nfour primitive skills, and eight challenging loco-manipulation tasks,\naccompanied by a set of scientific evaluation metrics balancing accuracy and\nfeasibility. Extensive simulated experiments show that our method significantly\noutperforms all baselines, while naturally regularizing behaviors to avoid\nreward hacking, resulting in more accurate and feasible movements for diverse\nloco-manipulation tasks in our daily scenarios. Our code and benchmark will be\nopen-sourced to the community to facilitate future research. Project page:\nhttps://usc-gvl.github.io/SkillBlender-web/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09366.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62359088a17d7271859c88f4",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1647677549197-noauth.jpeg",
      "fullname": "Yuxuan Kuang",
      "name": "yxK",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.08477",
      "authors": [
        {
          "_id": "684fb8cb60b4a34dbe007b05",
          "name": "Fengjun Pan",
          "hidden": false
        },
        {
          "_id": "684fb8cb60b4a34dbe007b06",
          "name": "Anh Tuan Luu",
          "hidden": false
        },
        {
          "_id": "684fb8cb60b4a34dbe007b07",
          "user": {
            "_id": "64cb02869e30a46f7b80b355",
            "avatarUrl": "/avatars/81ce4ba78826b54f0e1b53eeaff87ee6.svg",
            "isPro": false,
            "fullname": "Xiaobao Wu",
            "user": "bobxwu",
            "type": "user"
          },
          "name": "Xiaobao Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-16T07:16:04.938Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-10T06:10:45.000Z",
      "submittedOnDailyAt": "2025-06-16T04:59:02.024Z",
      "title": "Détection de Mémes Péjoratifs et Guides Pratiques\n\nModèle d'Apprentissage Automatique développé à l'Institutionnelle de Recherche en Intelligence Artificielle de Shanghai. Ce modèle utilise des explications et des guides pratiques pour détecter efficacement les Mémes péjoratifs.",
      "submittedOnDailyBy": {
        "_id": "64cb02869e30a46f7b80b355",
        "avatarUrl": "/avatars/81ce4ba78826b54f0e1b53eeaff87ee6.svg",
        "isPro": false,
        "fullname": "Xiaobao Wu",
        "user": "bobxwu",
        "type": "user"
      },
      "summary": "La perception de messages perjudiciels est essentielle pour maintenir l'intégrité des environnements en ligne. Cependant, les méthodes actuelles d'accès présentent des déficits en termes d'efficacité de ressources, de flexibilité et d'explicabilité, limitant l'application pratique du système de modélisation de contenu. Pour relever ces défis, nous présentons U-CoT+, un nouveau cadre de travail pour la perception de messages perjudiciels. Ce cadre de travail développe, pour la première fois, une chaîne d'opérations de messages perjudiciels de haute précision vers le texte, en maintenant presque intacte l'entraînement ou le fine-tuning du modèle. Ce design sépare l'interprétation et la classification des messages, évitant ainsi la connexion immédiate des contenus complexes avec leurs raisons et permettant une perception de messages perjudiciels efficace en ressources en utilisant de grands modèles de langage naturel (LLMs). En se basant sur ces textes explicatifs, des guides plus spécifiques et interprétables créés par des êtres humains sont produits, guidant le raisonnement du modèle dans un traitement de raisonnement de cotes (CoT) sans exemples (0-shot). Ce cadre de travail permet une adaptation simple à différents standards de perception de préjudices selon la plateforme, la région et le temps, offrant une grande flexibilité et explicabilité. Les expériences étendues sur 7 ensembles de données de benchmark démontrent l'efficacité de ce cadre de travail et mettent en avant la possibilité d'une perception de messages perjudiciels explicative et efficace en ressources en utilisant de petits modèles de LLMs. Le code et les données sont disponibles sur la URL suivante : https://anonymous.4open.science/r/HMC-AF2B/README.md.",
      "upvotes": 2,
      "discussionId": "684fb8cb60b4a34dbe007b08",
      "ai_summary": "U-CoT+ is a novel framework for detecting harmful memes by converting them into textual descriptions and using human-crafted guidelines with zero-shot CoT prompting to achieve high flexibility and explainability with small-scale LLMs.",
      "ai_keywords": [
        "U-CoT+",
        "meme-to-text pipeline",
        "high-fidelity",
        "zero-shot CoT prompting",
        "human-crafted guidelines",
        "large language models (LLMs)",
        "harmful meme detection",
        "explainability",
        "flexibility",
        "benchmark datasets"
      ]
    },
    "publishedAt": "2025-06-10T02:10:45.000Z",
    "title": "Detecting Harmful Memes with Decoupled Understanding and Guided CoT\n  Reasoning",
    "summary": "Detecting harmful memes is essential for maintaining the integrity of online\nenvironments. However, current approaches often struggle with resource\nefficiency, flexibility, or explainability, limiting their practical deployment\nin content moderation systems. To address these challenges, we introduce\nU-CoT+, a novel framework for harmful meme detection. Instead of relying solely\non prompting or fine-tuning multimodal models, we first develop a high-fidelity\nmeme-to-text pipeline that converts visual memes into detail-preserving textual\ndescriptions. This design decouples meme interpretation from meme\nclassification, thus avoiding immediate reasoning over complex raw visual\ncontent and enabling resource-efficient harmful meme detection with general\nlarge language models (LLMs). Building on these textual descriptions, we\nfurther incorporate targeted, interpretable human-crafted guidelines to guide\nmodels' reasoning under zero-shot CoT prompting. As such, this framework allows\nfor easy adaptation to different harmfulness detection criteria across\nplatforms, regions, and over time, offering high flexibility and\nexplainability. Extensive experiments on seven benchmark datasets validate the\neffectiveness of our framework, highlighting its potential for explainable and\nlow-resource harmful meme detection using small-scale LLMs. Codes and data are\navailable at: https://anonymous.4open.science/r/HMC-AF2B/README.md.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08477.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64cb02869e30a46f7b80b355",
      "avatarUrl": "/avatars/81ce4ba78826b54f0e1b53eeaff87ee6.svg",
      "fullname": "Xiaobao Wu",
      "name": "bobxwu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.07464",
      "authors": [
        {
          "_id": "684fd9a160b4a34dbe007b93",
          "name": "Jinyoung Park",
          "hidden": false
        },
        {
          "_id": "684fd9a160b4a34dbe007b94",
          "name": "Jeehye Na",
          "hidden": false
        },
        {
          "_id": "684fd9a160b4a34dbe007b95",
          "name": "Jinyoung Kim",
          "hidden": false
        },
        {
          "_id": "684fd9a160b4a34dbe007b96",
          "name": "Hyunwoo J. Kim",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-09T06:15:54.000Z",
      "submittedOnDailyAt": "2025-06-16T07:17:50.892Z",
      "title": "Difficulté-basée Politique Globale pour Régler l'Apprentissage par Renforcement en Vidéo",
      "submittedOnDailyBy": {
        "_id": "64b6eae88ba7d6c922c0434a",
        "avatarUrl": "/avatars/6adac8242106ab12abeaa3584346c0cd.svg",
        "isPro": false,
        "fullname": "Jinyoung Park",
        "user": "jinypark",
        "type": "user"
      },
      "summary": "Recentes recherches ont montré que l'apprentissage par renforcement (RL) basé sur le traitement postérieur améliore la capacité de compréhension des modèles de langage grands (LLMs). En particulier, le Group Relative Policy Optimization (GRPO) utilise un algorithme d'apprentissage par renforcement de type PPO qui utilise une normalisation de récompenses basée sur des groupes et a eu un succès notable. Cependant, l'application du GRPO aux Modèles de Langage Grands Vidéo (Video LLMs) a été moins étudiée. Dans cet article, nous examinons le GRPO dans les Video LLMs et identifions deux problèmes principaux qui peuvent empêcher un apprentissage efficace : (1) la dépendance sur des jeux sûrs et (2) le problème de disparition de la priorité. Pour atténuer ces problèmes, nous proposons DeepVideo-R1. DeepVideo-R1 est un Modèle de Langage Grands Vidéo entraîné en utilisant le Reg-GRPO (Région de GRPO) que nous proposons et une stratégie d'expansion de données en fonction de la difficulté. Le Reg-GRPO reconstruit l'objectif de GRPO en tant que tâche de régression et prédit directement les priorités de GRPO. Cette décomposition élimine la nécessité d'utiliser des jeux sûrs comme ClipPING et min, ajuste le modèle aux valeurs de priorité et fournit une orientation plus directe pour la politique. De plus, nous avons conçu une stratégie d'expansion de données en fonction de la difficulté, qui élargit les échantillons d'entraînement dynamiquement à des niveaux de difficulté résolubles et encourage les signaux de récompense avec diverses informations. Nos résultats détaillés montrent que DeepVideo-R1 améliore significativement le rendement de la raisonnance vidéo sur plusieurs benchmarks de raisonnance vidéo.",
      "upvotes": 2,
      "discussionId": "684fd9a160b4a34dbe007b97",
      "ai_summary": "DeepVideo-R1 enhances video reasoning performance using Reg-GRPO, a regression-based GRPO approach, and difficulty-aware data augmentation for video large language models.",
      "ai_keywords": [
        "reinforcement learning",
        "Group Relative Policy Optimization",
        "GRPO",
        "Policy Optimization",
        "PPO",
        "Video Large Language Models",
        "Video LLMs",
        "DeepVideo-R1",
        "Reg-GRPO",
        "regression task",
        "advantage values",
        "difficulty-aware data augmentation",
        "video reasoning benchmarks"
      ]
    },
    "publishedAt": "2025-06-09T02:15:54.000Z",
    "title": "DeepVideo-R1: Video Reinforcement Fine-Tuning via Difficulty-aware\n  Regressive GRPO",
    "summary": "Recent works have demonstrated the effectiveness of reinforcement learning\n(RL)-based post-training in enhancing the reasoning capabilities of large\nlanguage models (LLMs). In particular, Group Relative Policy Optimization\n(GRPO) has shown impressive success by employing a PPO-style reinforcement\nalgorithm with group-based normalized rewards. However, the application of GRPO\nto Video Large Language Models (Video LLMs) has been less studied. In this\npaper, we explore GRPO for video LLMs and identify two primary issues that\nimpede its effective learning: (1) reliance on safeguards, and (2) the\nvanishing advantage problem. To mitigate these challenges, we propose\nDeepVideo-R1, a video large language model trained with our proposed Reg-GRPO\n(Regressive GRPO) and difficulty-aware data augmentation strategy. Reg-GRPO\nreformulates the GRPO objective as a regression task, directly predicting the\nadvantage in GRPO. This design eliminates the need for safeguards like clipping\nand min functions, thereby facilitating more direct policy guidance by aligning\nthe model with the advantage values. We also design the difficulty-aware data\naugmentation strategy that dynamically augments training samples at solvable\ndifficulty levels, fostering diverse and informative reward signals. Our\ncomprehensive experiments show that DeepVideo-R1 significantly improves video\nreasoning performance across multiple video reasoning benchmarks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07464.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b6eae88ba7d6c922c0434a",
      "avatarUrl": "/avatars/6adac8242106ab12abeaa3584346c0cd.svg",
      "fullname": "Jinyoung Park",
      "name": "jinypark",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.11702",
      "authors": [
        {
          "_id": "684fae8360b4a34dbe007ac9",
          "user": {
            "_id": "5fad8602b8423e1d80b8a965",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5fad8602b8423e1d80b8a965/tRqTwcZmrGka8c1vFq2wX.jpeg",
            "isPro": false,
            "fullname": "Victor Gallego",
            "user": "vicgalle",
            "type": "user"
          },
          "name": "Víctor Gallego",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-06-16T05:42:55.745Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-13T12:17:38.000Z",
      "submittedOnDailyAt": "2025-06-16T04:12:09.638Z",
      "title": "Préférence d'ajustement variable basée sur des données avec des mots-clés",
      "submittedOnDailyBy": {
        "_id": "5fad8602b8423e1d80b8a965",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5fad8602b8423e1d80b8a965/tRqTwcZmrGka8c1vFq2wX.jpeg",
        "isPro": false,
        "fullname": "Victor Gallego",
        "user": "vicgalle",
        "type": "user"
      },
      "summary": "Dans la proposition d'ajustement de modèles de rétroaction humaine, les méthodes comme l'Optimisation de Préférence Directe (DPO) incluent souvent des ensembles statiques de préférences cohérentes, limitant l'adaptabilité. Dans cet article, sans supposer la cohérence des préférences, nous introduisons l'Ajustement de Préférences Configurable (CPT) pour proposer un nouveau cadre permettant d'ajuster de manière dynamique les modèles de langage en fonction d'instructions explicites pouvant être compris par l'humain. Le CPT utilise des données de préférences générées à partir de systèmes de questions structurées et de règles délicates, pour ajuster les micro-préférences guidées par ces règles et pour entraîner à ajuster les sorties qui répondent aux systèmes de questions. Cet approche offre un contrôle plus détaillé tout en offrant une structure plus complexe pour modéliser une rétroaction plus avancée. Le code de l'entraînement, les ensembles de données générés et les modèles ajustés, ainsi que d'autres fichiers expérimentaux, sont disponibles sur https://github.com/vicgalle/configurable-preference-tuning.",
      "upvotes": 1,
      "discussionId": "684fae8460b4a34dbe007aca",
      "ai_summary": "Configurable Preference Tuning enables language models to dynamically adjust their behavior based on human-interprettable directives, using rubric-guided preference data for fine-tuning and inference-time modulation.",
      "ai_keywords": [
        "Configurable Preference Tuning",
        "Direct Preference Optimization",
        "language models",
        "fine-grained control",
        "rubric-guided preferences",
        "inference-time modulation"
      ]
    },
    "publishedAt": "2025-06-13T08:17:38.000Z",
    "title": "Configurable Preference Tuning with Rubric-Guided Synthetic Data",
    "summary": "Models of human feedback for AI alignment, such as those underpinning Direct\nPreference Optimization (DPO), often bake in a singular, static set of\npreferences, limiting adaptability. This paper challenges the assumption of\nmonolithic preferences by introducing Configurable Preference Tuning (CPT), a\nnovel framework for endowing language models with the ability to dynamically\nadjust their behavior based on explicit, human-interpretable directives. CPT\nleverages synthetically generated preference data, conditioned on system\nprompts derived from structured, fine-grained rubrics that define desired\nattributes like writing style. By fine-tuning with these rubric-guided\npreferences, the LLM learns to modulate its outputs at inference time in\nresponse to the system prompt, without retraining. This approach not only\noffers fine-grained control but also provides a mechanism for modeling more\nnuanced and context-dependent human feedback. Several experimental artifacts,\nsuch as training code, generated datasets and fine-tuned models are released at\nhttps://github.com/vicgalle/configurable-preference-tuning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.11702.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5fad8602b8423e1d80b8a965",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5fad8602b8423e1d80b8a965/tRqTwcZmrGka8c1vFq2wX.jpeg",
      "fullname": "Victor Gallego",
      "name": "vicgalle",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 129
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.10128",
      "authors": [
        {
          "_id": "684fed081d9b438aa3957a50",
          "name": "Xiyao Wang",
          "hidden": false
        },
        {
          "_id": "684fed081d9b438aa3957a51",
          "name": "Zhengyuan Yang",
          "hidden": false
        },
        {
          "_id": "684fed081d9b438aa3957a52",
          "name": "Chao Feng",
          "hidden": false
        },
        {
          "_id": "684fed081d9b438aa3957a53",
          "name": "Yongyuan Liang",
          "hidden": false
        },
        {
          "_id": "684fed081d9b438aa3957a54",
          "name": "Yuhang Zhou",
          "hidden": false
        },
        {
          "_id": "684fed081d9b438aa3957a55",
          "name": "Xiaoyu Liu",
          "hidden": false
        },
        {
          "_id": "684fed081d9b438aa3957a56",
          "name": "Ziyi Zang",
          "hidden": false
        },
        {
          "_id": "684fed081d9b438aa3957a57",
          "name": "Ming Li",
          "hidden": false
        },
        {
          "_id": "684fed081d9b438aa3957a58",
          "name": "Chung-Ching Lin",
          "hidden": false
        },
        {
          "_id": "684fed081d9b438aa3957a59",
          "name": "Kevin Lin",
          "hidden": false
        },
        {
          "_id": "684fed081d9b438aa3957a5a",
          "name": "Linjie Li",
          "hidden": false
        },
        {
          "_id": "684fed081d9b438aa3957a5b",
          "name": "Furong Huang",
          "hidden": false
        },
        {
          "_id": "684fed081d9b438aa3957a5c",
          "name": "Lijuan Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-11T19:16:54.000Z",
      "submittedOnDailyAt": "2025-06-16T08:39:17.071Z",
      "title": "ViCrit : Tâche d'un agent apprenant par renforcement vérifiable pour des projets visuels",
      "submittedOnDailyBy": {
        "_id": "655fed9fdef5905d38b84af3",
        "avatarUrl": "/avatars/2cda4182dfd11a1e94743639e62328ea.svg",
        "isPro": false,
        "fullname": "Xiyao Wang",
        "user": "russwang",
        "type": "user"
      },
      "summary": "L'apprentissage par renforcement (RL) a été utilisé avec succès dans des tâches telles que la logique mathématique ou la génération de code, démontrant des résultats efficaces dans l'ajustement de modèles de langage grands (LLMs). Cependant, l'extension de ce succès au reconnaissance visuelle a été retardée en raison de la pénurie de tâches axées sur la vision. Pour aborder ce défi, on propose ViCrit (Critique de Hallucination de Captures Visuelles), une solution basée sur la représentation des tâches déléguées du RL. ViCrit modélise les modèles de langage et vision (VLMs) et est entraîné pour localiser les petits et synthétiques erreurs visuels qui sont insérés dans les captures d'images écrites par des humains. Il commence avec une capture de 200 mots, modifiant des objets, caractéristiques, nombres ou relations spatiales pour introduire un erreur visuel, et fournissant la capture modifiée et l'image au modèle, le modèle doit identifier l'erreur introduite. Cette configuration maintient complètement le défi visuel et fournit une compensation binaire claire et facile à calculer. Les modèles entraînés avec le travail de ViCrit montrent des améliorations notables dans différents benchmarks de VL. Un point important est que ces améliorations s'appliquent aussi à la logique abstraite des images dans des données d'images naturelles ainsi qu'à la mathématique visuelle, démontrant que la perception visuelle peut être entraînée. Pour évaluer ces résultats, on ajoute ViCrit-Bench, un benchmark diagnostique équilibré par catégories, pour explorer de manière intégrale les différents types d'erreurs visuelles. Ces résultats montrent que l'évaluation de la hallucination visuelle est un objectif efficace et généralisable pour renforcer la perception visuelle des VLMs.",
      "upvotes": 1,
      "discussionId": "684fed081d9b438aa3957a5d",
      "githubRepo": "https://github.com/si0wang/ViCrit",
      "ai_summary": "ViCrit, an RL task for fine-tuning VLMs, improves visual perception by training models to detect subtle hallucinations in image captions, with gains transferable to various visual domains.",
      "ai_keywords": [
        "reinforcement learning (RL)",
        "large language models (LLMs)",
        "vision-language models (VLMs)",
        "visual caption hallucination critic",
        "perceptual difficulty",
        "binary reward",
        "exact-match reward",
        "ViCrit-Bench",
        "abstract image reasoning",
        "visual math"
      ]
    },
    "publishedAt": "2025-06-11T15:16:54.000Z",
    "title": "ViCrit: A Verifiable Reinforcement Learning Proxy Task for Visual\n  Perception in VLMs",
    "summary": "Reinforcement learning (RL) has shown great effectiveness for fine-tuning\nlarge language models (LLMs) using tasks that are challenging yet easily\nverifiable, such as math reasoning or code generation. However, extending this\nsuccess to visual perception in vision-language models (VLMs) has been impeded\nby the scarcity of vision-centric tasks that are simultaneously challenging and\nunambiguously verifiable. To this end, we introduce ViCrit (Visual Caption\nHallucination Critic), an RL proxy task that trains VLMs to localize a subtle,\nsynthetic visual hallucination injected into paragraphs of human-written image\ncaptions. Starting from a 200-word captions, we inject a single, subtle visual\ndescription error-altering a few words on objects, attributes, counts, or\nspatial relations-and task the model to pinpoint the corrupted span given the\nimage and the modified caption. This formulation preserves the full perceptual\ndifficulty while providing a binary, exact-match reward that is easy to compute\nand unambiguous. Models trained with the ViCrit Task exhibit substantial gains\nacross a variety of VL benchmarks. Crucially, the improvements transfer beyond\nnatural-image training data to abstract image reasoning and visual math,\nshowing promises of learning to perceive rather than barely memorizing seen\nobjects. To facilitate evaluation, we further introduce ViCrit-Bench, a\ncategory-balanced diagnostic benchmark that systematically probes perception\nerrors across diverse image domains and error types. Together, our results\ndemonstrate that fine-grained hallucination criticism is an effective and\ngeneralizable objective for enhancing visual perception in VLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10128.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "655fed9fdef5905d38b84af3",
      "avatarUrl": "/avatars/2cda4182dfd11a1e94743639e62328ea.svg",
      "fullname": "Xiyao Wang",
      "name": "russwang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.11130",
      "authors": [
        {
          "_id": "684fdd8e1d9b438aa39579c6",
          "name": "Cheng Kang Chou",
          "hidden": false
        },
        {
          "_id": "684fdd8e1d9b438aa39579c7",
          "name": "Chan-Jan Hsu",
          "hidden": false
        },
        {
          "_id": "684fdd8e1d9b438aa39579c8",
          "name": "Ho-Lam Chung",
          "hidden": false
        },
        {
          "_id": "684fdd8e1d9b438aa39579c9",
          "name": "Liang-Hsuan Tseng",
          "hidden": false
        },
        {
          "_id": "684fdd8e1d9b438aa39579ca",
          "name": "Hsi-Chun Cheng",
          "hidden": false
        },
        {
          "_id": "684fdd8e1d9b438aa39579cb",
          "name": "Yu-Kuan Fu",
          "hidden": false
        },
        {
          "_id": "684fdd8e1d9b438aa39579cc",
          "name": "Kuan Po Huang",
          "hidden": false
        },
        {
          "_id": "684fdd8e1d9b438aa39579cd",
          "name": "Hung-Yi Lee",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-10T17:30:32.000Z",
      "submittedOnDailyAt": "2025-06-16T07:32:50.759Z",
      "title": "Utilisant le cadre transformé pour la génération de données TTS pour l'apprentissage renforcé de l'ASR",
      "submittedOnDailyBy": {
        "_id": "6213410828005421265b27d3",
        "avatarUrl": "/avatars/930ac20daf640ca31fab713bf00c3268.svg",
        "isPro": false,
        "fullname": "許湛然",
        "user": "Splend1dchan",
        "type": "user"
      },
      "summary": "Nous proposons un cadre d'amélioration automatique pour améliorer le rendement de l'ASR en utilisant des ensembles de données non étiquetées. Le processus commence par la génération de labels virtuels par le modèle ASR pour des données vocales non étiquetées, ce qui est utilisé pour entraîner un système de conversion de texte en voix (TTS) de haute qualité. Ensuite, les paires de voix synthétique et de texte sont réutilisées dans le système original d'ASR, complétant le cycle d'amélioration automatique dans un cycle fermé. Nous avons démontré l'effet de ce cadre de travail en utilisant une voix générale dans le taingó. Nous avons utilisé 6 000 heures de données de signal non étiquetées, une quantité significative de données de texte et de contenu généré par un modèle d'IA pour spécialiser Whisper-large-v2 et créer le modèle Twister. Twister a réduit l'erreur de 20% dans le taingó par rapport à Whisper et le benchmark de changement de code entre le taingó et l'anglais de 50%. Ces résultats démontrent l'efficacité du méthode d'adaptation de labels virtuels et fournissent une clé efficace pour améliorer le rendement de l'ASR dans des environnements avec de faibles ressources ou configurations spécifiques.",
      "upvotes": 1,
      "discussionId": "684fdd8f1d9b438aa39579ce",
      "ai_summary": "A self-refining framework enhances ASR performance using unlabeled datasets by integrating pseudo-labeling, TTS, and synthesized speech to create a specialized model.",
      "ai_keywords": [
        "self-refining framework",
        "ASR",
        "pseudo-labels",
        "TTS",
        "synthesized speech",
        "Whisper-large-v2",
        "Twister",
        "error rates",
        "Mandarin",
        "Mandarin-English code-switching"
      ]
    },
    "publishedAt": "2025-06-10T13:30:32.000Z",
    "title": "A Self-Refining Framework for Enhancing ASR Using TTS-Synthesized Data",
    "summary": "We propose a self-refining framework that enhances ASR performance with only\nunlabeled datasets. The process starts with an existing ASR model generating\npseudo-labels on unannotated speech, which are then used to train a\nhigh-fidelity text-to-speech (TTS) system. Then, synthesized speech text pairs\nare bootstrapped into the original ASR system, completing the closed-loop\nself-improvement cycle. We demonstrated the effectiveness of the framework on\nTaiwanese Mandarin speech. Leveraging 6,000 hours of unlabeled speech, a\nmoderate amount of text data, and synthetic content from the AI models, we\nadapt Whisper-large-v2 into a specialized model, Twister. Twister reduces error\nrates by up to 20% on Mandarin and 50% on Mandarin-English code-switching\nbenchmarks compared to Whisper. Results highlight the framework as a compelling\nalternative to pseudo-labeling self-distillation approaches and provides a\npractical pathway for improving ASR performance in low-resource or\ndomain-specific settings.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.11130.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6213410828005421265b27d3",
      "avatarUrl": "/avatars/930ac20daf640ca31fab713bf00c3268.svg",
      "fullname": "許湛然",
      "name": "Splend1dchan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 13
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.08592",
      "authors": [
        {
          "_id": "684cfefc3b733ba3336873a6",
          "user": {
            "_id": "650f0fac11f3210cf7a8a849",
            "avatarUrl": "/avatars/687d56c3a6d4f5cdb34e424cdcff954d.svg",
            "isPro": false,
            "fullname": "Leon Xu",
            "user": "lxucs",
            "type": "user"
          },
          "name": "Liyan Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-16T07:50:16.178Z",
          "hidden": false
        },
        {
          "_id": "684cfefc3b733ba3336873a7",
          "name": "Zhenlin Su",
          "hidden": false
        },
        {
          "_id": "684cfefc3b733ba3336873a8",
          "name": "Mo Yu",
          "hidden": false
        },
        {
          "_id": "684cfefc3b733ba3336873a9",
          "name": "Jiangnan Li",
          "hidden": false
        },
        {
          "_id": "684cfefc3b733ba3336873aa",
          "name": "Fandong Meng",
          "hidden": false
        },
        {
          "_id": "684cfefc3b733ba3336873ab",
          "name": "Jie Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-10T09:00:33.000Z",
      "submittedOnDailyAt": "2025-06-16T06:09:56.672Z",
      "title": "Dense Retrievers Can Fail on Simple Queries: Revealing The Granularity Dilemma of Embeddings",
      "submittedOnDailyBy": {
        "_id": "650f0fac11f3210cf7a8a849",
        "avatarUrl": "/avatars/687d56c3a6d4f5cdb34e424cdcff954d.svg",
        "isPro": false,
        "fullname": "Leon Xu",
        "user": "lxucs",
        "type": "user"
      },
      "summary": "Cette étude met en lumière les limitations observées dans les encodeurs de texte : les embeddings ne peuvent pas reconnaître des entités ou des événements très petits ou délicats dans un contexte de sens, ce qui peut entraîner des erreurs dans les recherches proches, même dans des cas simples. Pour étudier ces conditions, on présente d'abord un nouveau jeu de données d'évaluation. Ce jeu de données comprend des captures d'images, avec des questions formulées sous forme d'énoncés qui soulèvent des entités ou des événements. Selon l'évaluation 0-shot, l'encodeur a un risque élevé de faillir dans ces petites coincidences. Pour améliorer cette situation, on propose une stratégie de génération de données pour ajuster minutieusement l'encodeur et atteindre le meilleur rendement dans la CapRetrieval. Pendant ce processus, deux problèmes complexes ont été identifiés : l'embedding doit représenter des signaux très petits de manière à coincidir avec le sens général, ce qui nécessite une grande granularité. Les jeux de données, le code et les modèles utilisés dans cette étude sont disponibles sur https://github.com/lxucs/CapRetrieval.",
      "upvotes": 1,
      "discussionId": "684cfefc3b733ba3336873ac",
      "ai_summary": "A new dataset named CapRetrieval is introduced to evaluate the ability of text encoders to recognize fine-grained entities and events, highlighting challenges in dense retrieval tasks.",
      "ai_keywords": [
        "text encoders",
        "embeddings",
        "fine-grained entities",
        "events",
        "dense retrieval",
        "zero-shot evaluation",
        "data generation strategies",
        "granularity dilemma"
      ]
    },
    "publishedAt": "2025-06-10T05:00:33.000Z",
    "title": "Dense Retrievers Can Fail on Simple Queries: Revealing The Granularity\n  Dilemma of Embeddings",
    "summary": "This work focuses on an observed limitation of text encoders: embeddings may\nnot be able to recognize fine-grained entities or events within the semantics,\nresulting in failed dense retrieval on even simple cases. To examine such\nbehaviors, we first introduce a new evaluation dataset in Chinese, named\nCapRetrieval, whose passages are image captions, and queries are phrases\ninquiring entities or events in various forms. Zero-shot evaluation suggests\nthat encoders may fail on these fine-grained matching, regardless of training\nsources or model sizes. Aiming for enhancement, we proceed to finetune encoders\nwith our proposed data generation strategies, which obtains the best\nperformance on CapRetrieval. Within this process, we further identify an issue\nof granularity dilemma, a challenge for embeddings to express fine-grained\nsalience while aligning with overall semantics. Our dataset, code and models in\nthis work are publicly released at https://github.com/lxucs/CapRetrieval.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08592.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "650f0fac11f3210cf7a8a849",
      "avatarUrl": "/avatars/687d56c3a6d4f5cdb34e424cdcff954d.svg",
      "fullname": "Leon Xu",
      "name": "lxucs",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.08915",
      "authors": [
        {
          "_id": "684fe3711d9b438aa39579da",
          "user": {
            "_id": "6508647f0c87331947c4a46d",
            "avatarUrl": "/avatars/9ddaf4ec53729cb69f65b314a7f4a9a0.svg",
            "isPro": false,
            "fullname": "Ananthu Aniraj",
            "user": "ananthu-aniraj",
            "type": "user"
          },
          "name": "Ananthu Aniraj",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-16T09:52:38.316Z",
          "hidden": false
        },
        {
          "_id": "684fe3711d9b438aa39579db",
          "name": "Cassio F. Dantas",
          "hidden": false
        },
        {
          "_id": "684fe3711d9b438aa39579dc",
          "name": "Dino Ienco",
          "hidden": false
        },
        {
          "_id": "684fe3711d9b438aa39579dd",
          "name": "Diego Marcos",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-10T15:41:22.000Z",
      "submittedOnDailyAt": "2025-06-16T08:04:59.344Z",
      "title": "Mapeo de Atención con Confiance en la Race pour les Vision Transformers",
      "submittedOnDailyBy": {
        "_id": "6508647f0c87331947c4a46d",
        "avatarUrl": "/avatars/9ddaf4ec53729cb69f65b314a7f4a9a0.svg",
        "isPro": false,
        "fullname": "Ananthu Aniraj",
        "user": "ananthu-aniraj",
        "type": "user"
      },
      "summary": "Voici la traduction en français :\n\nOn présente un méthode basée sur l'attention qui utilise une masque d'attention binaire entraînée pour que seulement affecte la prédiction aux zones de l'image trouvées. Le contexte a un impact significatif sur la reconnaissance d'objets. Parfois, il appelle à des expressions qui peuvent causer des biais, surtout lorsque les objets apparaissent dans des fonds différents de leur distribution naturelle. D'autre part, dans les problèmes de niveau d'image axés sur les objets, il est nécessaire de spécifier des zones liées. Pour résoudre ce problème, nous proposons un cadre de travail en deux étapes : l'étape 1 traite l'image complète pour détecter des parties des objets et spécifier les zones liées au problème. L'étape 2 utilise la masque d'attention d'entrée pour concentrer l'analyse sur ces zones et filtrer l'information potentiellement de passage court. Les deux étapes sont entraînées ensemble, et l'étape 2 peut affiner l'étape 1. Les expériences sur différents benchmarks montrent que notre approche améliore significativement la robustesse face aux corrélations indirectes perdues et aux fonds différents de la distribution naturelle.",
      "upvotes": 0,
      "discussionId": "684fe3711d9b438aa39579de",
      "githubRepo": "https://github.com/ananthu-aniraj/ifam",
      "ai_summary": "An attention-based method using learned binary masks improves robustness in object perception by focusing on relevant image regions while filtering out spurious information.",
      "ai_keywords": [
        "attention-based method",
        "learned binary attention masks",
        "object perception",
        "context",
        "out-of-distribution backgrounds",
        "image-level object-centric tasks",
        "task-relevant regions",
        "two-stage framework",
        "receptive field",
        "joint training",
        "robustness",
        "spurious correlations"
      ]
    },
    "publishedAt": "2025-06-10T11:41:22.000Z",
    "title": "Inherently Faithful Attention Maps for Vision Transformers",
    "summary": "We introduce an attention-based method that uses learned binary attention\nmasks to ensure that only attended image regions influence the prediction.\nContext can strongly affect object perception, sometimes leading to biased\nrepresentations, particularly when objects appear in out-of-distribution\nbackgrounds. At the same time, many image-level object-centric tasks require\nidentifying relevant regions, often requiring context. To address this\nconundrum, we propose a two-stage framework: stage 1 processes the full image\nto discover object parts and identify task-relevant regions, while stage 2\nleverages input attention masking to restrict its receptive field to these\nregions, enabling a focused analysis while filtering out potentially spurious\ninformation. Both stages are trained jointly, allowing stage 2 to refine stage\n1. Extensive experiments across diverse benchmarks demonstrate that our\napproach significantly improves robustness against spurious correlations and\nout-of-distribution backgrounds.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08915.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6508647f0c87331947c4a46d",
      "avatarUrl": "/avatars/9ddaf4ec53729cb69f65b314a7f4a9a0.svg",
      "fullname": "Ananthu Aniraj",
      "name": "ananthu-aniraj",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]