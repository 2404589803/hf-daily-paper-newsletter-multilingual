[
  {
    "paper": {
      "id": "2505.03318",
      "authors": [
        {
          "_id": "681aac4fd31f567552f0cc0e",
          "name": "Yibin Wang",
          "hidden": false
        },
        {
          "_id": "681aac4fd31f567552f0cc0f",
          "name": "Zhimin Li",
          "hidden": false
        },
        {
          "_id": "681aac4fd31f567552f0cc10",
          "user": {
            "_id": "63859cf3b2906edaf83af9f0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63859cf3b2906edaf83af9f0/mCgynEtoXdILvzyoPexii.png",
            "isPro": false,
            "fullname": "Yuhang Zang",
            "user": "yuhangzang",
            "type": "user"
          },
          "name": "Yuhang Zang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-07T08:09:33.194Z",
          "hidden": false
        },
        {
          "_id": "681aac4fd31f567552f0cc11",
          "name": "Chunyu Wang",
          "hidden": false
        },
        {
          "_id": "681aac4fd31f567552f0cc12",
          "name": "Qinglin Lu",
          "hidden": false
        },
        {
          "_id": "681aac4fd31f567552f0cc13",
          "name": "Cheng Jin",
          "hidden": false
        },
        {
          "_id": "681aac4fd31f567552f0cc14",
          "name": "Jiaqi Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-06T08:46:41.000Z",
      "submittedOnDailyAt": "2025-05-07T00:19:35.245Z",
      "title": "Ajuste micro du modèle de récompense multimodal de chose en apprentissage par renforcement",
      "submittedOnDailyBy": {
        "_id": "654c6845bac6e6e49895a5b5",
        "avatarUrl": "/avatars/ed1f140abcd4d76669e2e48db1d1193f.svg",
        "isPro": false,
        "fullname": "Yibin Wang",
        "user": "CodeGoat24",
        "type": "user"
      },
      "summary": "Le développement récent du modèle de récompense Damo (RM) montre une notable anticipation pour la fourniture de signaux de récompense alignant les modèles visuels et les préférences humaines. Cependant, actuellement, les RM sont limités par des réponses directes ou des processus de raisonnement superficiels, ce qui génère souvent de l'incertitude dans les signaux de récompense. Nous proposons que l'inclusion explicite d'une longue chaîne de pensée (CoT) dans le processus de récompense puisse significativement renforcer sa confiance et sa robustesse. De plus, nous croyons que la précision des réponses directes peut être améliorée grâce à la capacité cachée de l'RM à processer la CoT. Dans ce sens, cet article propose UnifiedReward-Think, le premier modèle de récompense basé sur la CoT dans Damo. Ce modèle peut effectuer une considération profonde de multiples niveaux dans deux domaines : la compréhension visuelle et la génération de récompense. Spécifiquement, nous adoptons un approche d'apprentissage répété à petite échelle pour développer la capacité de pensée complexe du modèle et pour stimuler la génération de récompense : (1) Tout d'abord, nous utilisons des données de préférence pour la génération de peu d'images pour entraîner le processus de pensée de GPT-4o, en utilisant cela comme point de départ de congélation pour apprendre la forme et la structure de la CoT. (2) Ensuite, nous exploitons le savoir préalable et la capacité de généralisation du modèle pour préparer de grands ensembles de données de préférence unifiées dans Damo, développant le processus de pensée du modèle sur une large gamme de tâches visuelles. Dans cette étape, nous maintenons la précision de la considération et nous entraînons le modèle en utilisant des techniques d'échantillonnage de refus. (3) De l'autre côté, les prédictions incertaines sont explorées par un apprentissage répété à petite échelle basé sur la politique de groupe et l'optimisation de GRPO, permettant d'explorer divers pas de pensée et d'optimiser des décisions précises et robustes. Les expériences larges dans la tâche de récompense visuelle démontrent l'excellence du modèle proposé dans cet article.",
      "upvotes": 50,
      "discussionId": "681aac50d31f567552f0cc5d",
      "projectPage": "https://codegoat24.github.io/UnifiedReward/think",
      "githubRepo": "https://github.com/CodeGoat24/UnifiedReward",
      "ai_keywords": [
        "Multimodal Reward Models (RMs)",
        "long chains of thought (CoT)",
        "UnifiedReward-Think",
        "exploration-driven reinforcement fine-tuning",
        "small amount of image generation preference data",
        "GPT-4o",
        "large-scale unified multimodal preference data",
        "rejection sampling",
        "Group Relative Policy Optimization (GRPO)",
        "reinforcement fine-tuning"
      ]
    },
    "publishedAt": "2025-05-06T04:46:41.000Z",
    "title": "Unified Multimodal Chain-of-Thought Reward Model through Reinforcement\n  Fine-Tuning",
    "summary": "Recent advances in multimodal Reward Models (RMs) have shown significant\npromise in delivering reward signals to align vision models with human\npreferences. However, current RMs are generally restricted to providing direct\nresponses or engaging in shallow reasoning processes with limited depth, often\nleading to inaccurate reward signals. We posit that incorporating explicit long\nchains of thought (CoT) into the reward reasoning process can significantly\nstrengthen their reliability and robustness. Furthermore, we believe that once\nRMs internalize CoT reasoning, their direct response accuracy can also be\nimproved through implicit reasoning capabilities. To this end, this paper\nproposes UnifiedReward-Think, the first unified multimodal CoT-based reward\nmodel, capable of multi-dimensional, step-by-step long-chain reasoning for both\nvisual understanding and generation reward tasks. Specifically, we adopt an\nexploration-driven reinforcement fine-tuning approach to elicit and incentivize\nthe model's latent complex reasoning ability: (1) We first use a small amount\nof image generation preference data to distill the reasoning process of GPT-4o,\nwhich is then used for the model's cold start to learn the format and structure\nof CoT reasoning. (2) Subsequently, by leveraging the model's prior knowledge\nand generalization capabilities, we prepare large-scale unified multimodal\npreference data to elicit the model's reasoning process across various vision\ntasks. During this phase, correct reasoning outputs are retained for rejection\nsampling to refine the model (3) while incorrect predicted samples are finally\nused for Group Relative Policy Optimization (GRPO) based reinforcement\nfine-tuning, enabling the model to explore diverse reasoning paths and optimize\nfor correct and robust solutions. Extensive experiments across various vision\nreward tasks demonstrate the superiority of our model.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03318.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "654c6845bac6e6e49895a5b5",
      "avatarUrl": "/avatars/ed1f140abcd4d76669e2e48db1d1193f.svg",
      "fullname": "Yibin Wang",
      "name": "CodeGoat24",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.03335",
      "authors": [
        {
          "_id": "681ab9b8f43603c60cab87a3",
          "name": "Andrew Zhao",
          "hidden": false
        },
        {
          "_id": "681ab9b8f43603c60cab87a4",
          "user": {
            "_id": "647eb24118274bce0308b2b8",
            "avatarUrl": "/avatars/463e49a89b61164ccfad85ced10658b2.svg",
            "isPro": false,
            "fullname": "Yiran Wu",
            "user": "kevinwyr",
            "type": "user"
          },
          "name": "Yiran Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-07T08:09:25.939Z",
          "hidden": false
        },
        {
          "_id": "681ab9b8f43603c60cab87a5",
          "user": {
            "_id": "649d475111592b1a765ac1a3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649d475111592b1a765ac1a3/rjORJjErJq-mthghan08U.jpeg",
            "isPro": false,
            "fullname": "Yang Yue",
            "user": "Yang130",
            "type": "user"
          },
          "name": "Yang Yue",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-07T08:09:23.436Z",
          "hidden": false
        },
        {
          "_id": "681ab9b8f43603c60cab87a6",
          "name": "Tong Wu",
          "hidden": false
        },
        {
          "_id": "681ab9b8f43603c60cab87a7",
          "name": "Quentin Xu",
          "hidden": false
        },
        {
          "_id": "681ab9b8f43603c60cab87a8",
          "name": "Yang Yue",
          "hidden": false
        },
        {
          "_id": "681ab9b8f43603c60cab87a9",
          "name": "Matthieu Lin",
          "hidden": false
        },
        {
          "_id": "681ab9b8f43603c60cab87aa",
          "user": {
            "_id": "6486dde1f74857df3f1a5828",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6486dde1f74857df3f1a5828/FgE80CpalBO5qqArdfwxA.jpeg",
            "isPro": false,
            "fullname": "Shenzhi Wang",
            "user": "shenzhi-wang",
            "type": "user"
          },
          "name": "Shenzhi Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-07T08:09:28.623Z",
          "hidden": false
        },
        {
          "_id": "681ab9b8f43603c60cab87ab",
          "name": "Qingyun Wu",
          "hidden": false
        },
        {
          "_id": "681ab9b8f43603c60cab87ac",
          "user": {
            "_id": "63a95a6a7930fa8c7dd63d4e",
            "avatarUrl": "/avatars/d9d0420f7ddfe2f3a7e029fb05f1c89f.svg",
            "isPro": false,
            "fullname": "Zilong Zheng",
            "user": "zlzheng",
            "type": "user"
          },
          "name": "Zilong Zheng",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-07T01:39:06.052Z",
          "hidden": false
        },
        {
          "_id": "681ab9b8f43603c60cab87ad",
          "name": "Gao Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-06T09:08:00.000Z",
      "submittedOnDailyAt": "2025-05-07T00:10:48.130Z",
      "title": "「Inférence de jeux d'apprentissage par renforcement basé sur les données 0」",
      "submittedOnDailyBy": {
        "_id": "630482fbce6b12280b18971d",
        "avatarUrl": "/avatars/b07f31fd970d736bdf574d56da7a5634.svg",
        "isPro": false,
        "fullname": "Andrew Zhao",
        "user": "andrewzh",
        "type": "user"
      },
      "summary": "L'apprentissage par renforcement basé sur les récompenses possibles (RLVR) montre qu'il est possible d'améliorer la capacité logique de grands modèles de langage par des récompenses basées sur les résultats. Les dernières recherches en RLVR fonctionnent dans un 0 de configuration, évitant ainsi l'étiquetage des processus logiques, mais dépendent toujours de la collecte de questions et réponses de qualité pour le jeu de données de test. L'absence de modèles de haute qualité créés par les humains conduit à des inquiétudes sur la dépendance à long terme des humains pour la sous-étiquetage. De plus, si l'avenir de l'IA dépasse l'intelligence humaine, la tâche que fournissent les humains pourrait avoir un potentiel d'apprentissage limité pour des systèmes d'intelligence de haut niveau. Pour faire face à ces inquiétudes, nous proposons que un modèle apprenne à maximiser son propre progrès dans une tâche, ce qui nous amène à proposer un nouveau paradigme RLVR appelé 'Absolute Zero', avec l'objectif d'améliorer la capacité logique. Dans ce paradigme, nous validons la tâche logique proposée en utilisant un code de vérification de codes et nous confirmons les réponses, présentant ainsi un système 'Absolute Zero Reasoner (AZR)' qui améliore sa capacité logique et son cours d'apprentissage grâce à son propre développement. AZR ne dépend pas complètement de données externes, mais atteint les meilleurs résultats dans des tâches logiques de code et de mathématiques, dépassant les modèles de 0 configuration actuels. De plus, il montre son applicabilité sur différentes échelles de modèles et de classes de modèles, démontrant son efficacité dans sa mise en œuvre.",
      "upvotes": 43,
      "discussionId": "681ab9baf43603c60cab881a",
      "projectPage": "https://andrewzh112.github.io/absolute-zero-reasoner/",
      "githubRepo": "https://github.com/LeapLabTHU/Absolute-Zero-Reasoner",
      "ai_keywords": [
        "Reinforcement learning with verifiable rewards (RLVR)",
        "zero setting",
        "outcome-based rewards",
        "manually curated collections",
        "superintelligent system",
        "Absolute Zero",
        "AzR (Absolute Zero Reasoner)",
        "training curriculum",
        "code executor",
        "verifiable reward",
        "open-ended yet grounded learning",
        "SOTA performance",
        "coding and mathematical reasoning tasks"
      ]
    },
    "publishedAt": "2025-05-06T05:08:00.000Z",
    "title": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data",
    "summary": "Reinforcement learning with verifiable rewards (RLVR) has shown promise in\nenhancing the reasoning capabilities of large language models by learning\ndirectly from outcome-based rewards. Recent RLVR works that operate under the\nzero setting avoid supervision in labeling the reasoning process, but still\ndepend on manually curated collections of questions and answers for training.\nThe scarcity of high-quality, human-produced examples raises concerns about the\nlong-term scalability of relying on human supervision, a challenge already\nevident in the domain of language model pretraining. Furthermore, in a\nhypothetical future where AI surpasses human intelligence, tasks provided by\nhumans may offer limited learning potential for a superintelligent system. To\naddress these concerns, we propose a new RLVR paradigm called Absolute Zero, in\nwhich a single model learns to propose tasks that maximize its own learning\nprogress and improves reasoning by solving them, without relying on any\nexternal data. Under this paradigm, we introduce the Absolute Zero Reasoner\n(AZR), a system that self-evolves its training curriculum and reasoning ability\nby using a code executor to both validate proposed code reasoning tasks and\nverify answers, serving as an unified source of verifiable reward to guide\nopen-ended yet grounded learning. Despite being trained entirely without\nexternal data, AZR achieves overall SOTA performance on coding and mathematical\nreasoning tasks, outperforming existing zero-setting models that rely on tens\nof thousands of in-domain human-curated examples. Furthermore, we demonstrate\nthat AZR can be effectively applied across different model scales and is\ncompatible with various model classes.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03335.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "630482fbce6b12280b18971d",
      "avatarUrl": "/avatars/b07f31fd970d736bdf574d56da7a5634.svg",
      "fullname": "Andrew Zhao",
      "name": "andrewzh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.03730",
      "authors": [
        {
          "_id": "681abf3759155282c1cb2306",
          "name": "Shiyi Zhang",
          "hidden": false
        },
        {
          "_id": "681abf3759155282c1cb2307",
          "user": {
            "_id": "64970d3d9c3b29dca8633f87",
            "avatarUrl": "/avatars/11e3c9c66d28490d6d09925f9aa47cd1.svg",
            "isPro": false,
            "fullname": "JunhaoZhuang",
            "user": "JunhaoZhuang",
            "type": "user"
          },
          "name": "Junhao Zhuang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-07T08:08:58.214Z",
          "hidden": false
        },
        {
          "_id": "681abf3759155282c1cb2308",
          "name": "Zhaoyang Zhang",
          "hidden": false
        },
        {
          "_id": "681abf3759155282c1cb2309",
          "name": "Ying Shan",
          "hidden": false
        },
        {
          "_id": "681abf3759155282c1cb230a",
          "name": "Yansong Tang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-06T17:58:02.000Z",
      "submittedOnDailyAt": "2025-05-07T00:37:06.442Z",
      "title": "FlexiAct : Contrôle des actions volatiles dans des scénarios hétérogènes",
      "submittedOnDailyBy": {
        "_id": "6315d306a9456afe2b9bf34a",
        "avatarUrl": "/avatars/7285b4e7d84b528d1a50f8ee4eb10727.svg",
        "isPro": false,
        "fullname": "ElevenZ",
        "user": "shiyi0408",
        "type": "user"
      },
      "summary": "La prise en charge personnalisée est le processus de génération de vidéos qui exécutent des actions indiquées par des signaux de contrôle d'entrée, en utilisant des guides personnalisés orientés ou des mouvements globaux personnalisés. Actuellement, les méthodes disponibles souffrent de limites en raison de la stricte restriction de la structure spectrale (par exemple, cohérence de la lumière, échelle et perspective), ce qui réduit leur capacité d'adaptation à divers thèmes et scénarios. Pour surmonter ces limites, nous proposons FlexiAct. FlexiAct vise à transférer des actions d'un vidéo de référence à une image de destination, permettant des variations dans la structure spectrale entre le thème du vidéo de référence et l'image de destination, tout en maintenant la cohérence des actions. Pour atteindre ceci, il est nécessaire un contrôle précis des actions, une adaptation spectrale et la maintenance de la cohérence des actions. Pour cela, nous introduisons RefAdapter, qui se concentre sur l'adaptation spectrale et la cohérence, surmontant les méthodes actuelles pour atteindre un équilibre entre accord et flexibilité structurale. De nos observations, nous avons conclu que le processus de débruitage nécessite une attention différente pour les mouvements lents (basses fréquences) et les détails apertures (hautes fréquences) à chaque pas temporel. Par conséquent, nous proposons FAE (Extraction d'Actions avec Connaissance de Fréquence). FAE effectue l'extraction d'actions directement lors du processus de débruitage, sans utiliser une architecture spectrale-temporelle séparée. Les expérimentations montrent que notre méthode efficacement transfère des actions à divers thèmes avec différentes cohérences de lumière, échelle et perspective. Nous publions notre code et les poids du modèle pour soutenir des recherches futures. https://shiyi-zh0408.github.io/projectpages/FlexiAct/",
      "upvotes": 17,
      "discussionId": "681abf3859155282c1cb23fa",
      "projectPage": "https://shiyi-zh0408.github.io/projectpages/FlexiAct/",
      "githubRepo": "https://github.com/shiyi-zh0408/FlexiAct",
      "ai_keywords": [
        "FlexiAct",
        "RefAdapter",
        "image-conditioned adapter",
        "spatial adaptation",
        "consistency preservation",
        "FAE",
        "Frequency-aware Action Extraction",
        "denoising process",
        "spatial-temporal architectures",
        "action extraction"
      ]
    },
    "publishedAt": "2025-05-06T13:58:02.000Z",
    "title": "FlexiAct: Towards Flexible Action Control in Heterogeneous Scenarios",
    "summary": "Action customization involves generating videos where the subject performs\nactions dictated by input control signals. Current methods use pose-guided or\nglobal motion customization but are limited by strict constraints on spatial\nstructure, such as layout, skeleton, and viewpoint consistency, reducing\nadaptability across diverse subjects and scenarios. To overcome these\nlimitations, we propose FlexiAct, which transfers actions from a reference\nvideo to an arbitrary target image. Unlike existing methods, FlexiAct allows\nfor variations in layout, viewpoint, and skeletal structure between the subject\nof the reference video and the target image, while maintaining identity\nconsistency. Achieving this requires precise action control, spatial structure\nadaptation, and consistency preservation. To this end, we introduce RefAdapter,\na lightweight image-conditioned adapter that excels in spatial adaptation and\nconsistency preservation, surpassing existing methods in balancing appearance\nconsistency and structural flexibility. Additionally, based on our\nobservations, the denoising process exhibits varying levels of attention to\nmotion (low frequency) and appearance details (high frequency) at different\ntimesteps. So we propose FAE (Frequency-aware Action Extraction), which, unlike\nexisting methods that rely on separate spatial-temporal architectures, directly\nachieves action extraction during the denoising process. Experiments\ndemonstrate that our method effectively transfers actions to subjects with\ndiverse layouts, skeletons, and viewpoints. We release our code and model\nweights to support further research at\nhttps://shiyi-zh0408.github.io/projectpages/FlexiAct/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03730.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6315d306a9456afe2b9bf34a",
      "avatarUrl": "/avatars/7285b4e7d84b528d1a50f8ee4eb10727.svg",
      "fullname": "ElevenZ",
      "name": "shiyi0408",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.02922",
      "authors": [
        {
          "_id": "681abdbef8feaef23b543877",
          "name": "Yaoqi Chen",
          "hidden": false
        },
        {
          "_id": "681abdbef8feaef23b543878",
          "name": "Jinkai Zhang",
          "hidden": false
        },
        {
          "_id": "681abdbef8feaef23b543879",
          "user": {
            "_id": "667135bdcca06def1c2599a6",
            "avatarUrl": "/avatars/d94ab99265e1970852605d344b4d69e9.svg",
            "isPro": false,
            "fullname": "Baotong Lu",
            "user": "baotonglu",
            "type": "user"
          },
          "name": "Baotong Lu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-07T01:56:16.240Z",
          "hidden": false
        },
        {
          "_id": "681abdbef8feaef23b54387a",
          "user": {
            "_id": "66e96f58de9ebeee86f5e27f",
            "avatarUrl": "/avatars/e7d692aa47f02f1858186f47186166ad.svg",
            "isPro": false,
            "fullname": "Qianxi Zhang",
            "user": "qianxizhang",
            "type": "user"
          },
          "name": "Qianxi Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-07T08:09:00.341Z",
          "hidden": false
        },
        {
          "_id": "681abdbef8feaef23b54387b",
          "name": "Chengruidong Zhang",
          "hidden": false
        },
        {
          "_id": "681abdbef8feaef23b54387c",
          "name": "Jingjia Luo",
          "hidden": false
        },
        {
          "_id": "681abdbef8feaef23b54387d",
          "name": "Di Liu",
          "hidden": false
        },
        {
          "_id": "681abdbef8feaef23b54387e",
          "name": "Huiqiang Jiang",
          "hidden": false
        },
        {
          "_id": "681abdbef8feaef23b54387f",
          "name": "Qi Chen",
          "hidden": false
        },
        {
          "_id": "681abdbef8feaef23b543880",
          "name": "Jing Liu",
          "hidden": false
        },
        {
          "_id": "681abdbef8feaef23b543881",
          "name": "Bailu Ding",
          "hidden": false
        },
        {
          "_id": "681abdbef8feaef23b543882",
          "name": "Xiao Yan",
          "hidden": false
        },
        {
          "_id": "681abdbef8feaef23b543883",
          "name": "Jiawei Jiang",
          "hidden": false
        },
        {
          "_id": "681abdbef8feaef23b543884",
          "name": "Chen Chen",
          "hidden": false
        },
        {
          "_id": "681abdbef8feaef23b543885",
          "name": "Mingxing Zhang",
          "hidden": false
        },
        {
          "_id": "681abdbef8feaef23b543886",
          "name": "Yuqing Yang",
          "hidden": false
        },
        {
          "_id": "681abdbef8feaef23b543887",
          "name": "Fan Yang",
          "hidden": false
        },
        {
          "_id": "681abdbef8feaef23b543888",
          "name": "Mao Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-05T18:01:17.000Z",
      "submittedOnDailyAt": "2025-05-07T00:36:48.274Z",
      "title": "RetroInfer : Méthode de stockage de vecteurs pour l'inférence de LLM avec contexte étendu scalable",
      "submittedOnDailyBy": {
        "_id": "6278bd42541f3d2dfa77ea70",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6278bd42541f3d2dfa77ea70/ejn49eapnB3UXQckAYdTd.jpeg",
        "isPro": true,
        "fullname": "Huiqiang Jiang",
        "user": "iofu728",
        "type": "user"
      },
      "summary": "Le développement de la longueur de contexte des modèles de langage grands (LLMs) génère des problèmes graves pour l'inférence efficace en raison des limitations de la mémoire et de la bande passante du GPU. Nous présentons un nouveau système appelé RetroInfer, qui utilise un cache de clés-valeurs (KV) basé sur l'attention rare pour accélérer l'inférence de LLMs avec un contexte long. L'essence de RetroInfer est l'index d'onde. L'index d'onde utilise des méthodes comme des approximations d'attention, des estimations d'attention avec des limites de précision et un clustering séparé pour faciliter la recherche efficace et précise de tokens importants. Associé à un buffer d'onde, RetroInfer coordonne la placement du cache de KV, parallélise les calculs et la transmission de données entre GPU et CPU, maintenant des performances élevées. A différence de d'autres méthodes basées sur la rareté, RetroInfer évite les problèmes de sélection de tokens et d'adaptation au matériel, offrant un rendement élevé sans sacrifier la précision du modèle. Les expériences dans des cadres de test avec un contexte long montrent que RetroInfer atteint un accélération de 4,5 fois par rapport à l'attention complète, en respectant les limites de la mémoire du GPU, et un accélération de 10,5 fois en étendant le cache de KV dans la mémoire du CPU, tout en maintenant la précision de l'attention complète.",
      "upvotes": 16,
      "discussionId": "681abdc0f8feaef23b543926",
      "ai_keywords": [
        "RetrInfer",
        "key-value (KV) cache",
        "vector storage system",
        "wave index",
        "Attention-aWare VEctor index",
        "tripartite attention approximation",
        "accuracy-bounded attention estimation",
        "segmented clustering",
        "wave buffer",
        "GPU memory",
        "token selection",
        "hardware coordination"
      ]
    },
    "publishedAt": "2025-05-05T14:01:17.000Z",
    "title": "RetroInfer: A Vector-Storage Approach for Scalable Long-Context LLM\n  Inference",
    "summary": "The growing context lengths of large language models (LLMs) pose significant\nchallenges for efficient inference, primarily due to GPU memory and bandwidth\nconstraints. We present RetroInfer, a novel system that reconceptualizes the\nkey-value (KV) cache as a vector storage system which exploits the inherent\nattention sparsity to accelerate long-context LLM inference. At its core is the\nwave index, an Attention-aWare VEctor index that enables efficient and accurate\nretrieval of critical tokens through techniques such as tripartite attention\napproximation, accuracy-bounded attention estimation, and segmented clustering.\nComplementing this is the wave buffer, which coordinates KV cache placement and\noverlaps computation and data transfer across GPU and CPU to sustain high\nthroughput. Unlike prior sparsity-based methods that struggle with token\nselection and hardware coordination, RetroInfer delivers robust performance\nwithout compromising model accuracy. Experiments on long-context benchmarks\nshow up to 4.5X speedup over full attention within GPU memory limits and up to\n10.5X over sparse attention baselines when KV cache is extended to CPU memory,\nall while preserving full-attention-level accuracy.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02922.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6278bd42541f3d2dfa77ea70",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6278bd42541f3d2dfa77ea70/ejn49eapnB3UXQckAYdTd.jpeg",
      "fullname": "Huiqiang Jiang",
      "name": "iofu728",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.03005",
      "authors": [
        {
          "_id": "681ac8a09f4ed2ece10fac18",
          "user": {
            "_id": "647f4bac45baf21ad709fcd0",
            "avatarUrl": "/avatars/14c04cdda95de676aeefa9ae3e7c19ba.svg",
            "isPro": false,
            "fullname": "Dan Goldstein",
            "user": "SmerkyG",
            "type": "user"
          },
          "name": "Daniel Goldstein",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-07T08:08:53.268Z",
          "hidden": false
        },
        {
          "_id": "681ac8a09f4ed2ece10fac19",
          "name": "Eric Alcaide",
          "hidden": false
        },
        {
          "_id": "681ac8a09f4ed2ece10fac1a",
          "name": "Janna Lu",
          "hidden": false
        },
        {
          "_id": "681ac8a09f4ed2ece10fac1b",
          "name": "Eugene Cheah",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/647f4bac45baf21ad709fcd0/sfcDfWZfka9sy8siQC6MV.png",
        "https://cdn-uploads.huggingface.co/production/uploads/647f4bac45baf21ad709fcd0/yXcUOgY48NS_nfIZN0Kqq.png"
      ],
      "publishedAt": "2025-05-05T20:03:28.000Z",
      "submittedOnDailyAt": "2025-05-07T01:19:36.031Z",
      "title": "RADLADS : Style d'attention rapide d'attraction linéaire sur scène",
      "submittedOnDailyBy": {
        "_id": "647f4bac45baf21ad709fcd0",
        "avatarUrl": "/avatars/14c04cdda95de676aeefa9ae3e7c19ba.svg",
        "isPro": false,
        "fullname": "Dan Goldstein",
        "user": "SmerkyG",
        "type": "user"
      },
      "summary": "Introducing RADLADS (Scale à Attention Rapide de Segmentation pour le Décodeur de l'Attention 렐러어), une extension de la Transformation Affine Laplacienne à la Décodeur d'Attention 렐러어. Ce protocole transforme un Transformateur d'Attention Softmax en un modèle de Décodeur d'Attention 렐러어, en intégrant deux nouvelles architectures variantes de RWKV et des modèles open-source spécialisés Qwen2.5 (tailles 7B, 32B, 72B). Le processus de transformation nécessite environ 350-700M tokens, soit environ 0,005% du compte de tokens du modèle enseignant original. Actuellement, le coût pour convertir un modèle d'Attention 렐러어 de 72B est inférieur à 2 000 $ USD, avec une qualité d'inférence presque identique à celle du Transformateur original. Ces modèles atteignent des performances de pointe sur les benchmarks standards pour les tailles de modèles d'Attention 렐러어. Tous les modèles sont publiés sur HuggingFace sous la licence Apache 2.0, avec le modèle 72B soumis aux mêmes limitations selon l'Accord de Licence Qwen.\n\nLes modèles sont accessibles publiquement à :\nhttps://huggingface.co/collections/recursal/radlads-6818ee69e99e729ba8a87102\n\nLe code de training est disponible à :\nhttps://github.com/recursal/RADLADS-paper",
      "upvotes": 14,
      "discussionId": "681ac8a19f4ed2ece10fac75",
      "ai_keywords": [
        "Rapid Attention Distillation",
        "RADLADS",
        "softmax attention transformers",
        "linear attention decoder models",
        "RWKV-variant",
        "Qwen2.5",
        "token count",
        "linear attention models",
        "HuggingFace",
        "Apache 2.0 license",
        "Qwen License Agreement"
      ]
    },
    "publishedAt": "2025-05-05T16:03:28.000Z",
    "title": "RADLADS: Rapid Attention Distillation to Linear Attention Decoders at\n  Scale",
    "summary": "We present Rapid Attention Distillation to Linear Attention Decoders at Scale\n(RADLADS), a protocol for rapidly converting softmax attention transformers\ninto linear attention decoder models, along with two new RWKV-variant\narchitectures, and models converted from popular Qwen2.5 open source models in\n7B, 32B, and 72B sizes. Our conversion process requires only 350-700M tokens,\nless than 0.005% of the token count used to train the original teacher models.\nConverting to our 72B linear attention model costs less than \\$2,000 USD at\ntoday's prices, yet quality at inference remains close to the original\ntransformer. These models achieve state-of-the-art downstream performance\nacross a set of standard benchmarks for linear attention models of their size.\nWe release all our models on HuggingFace under the Apache 2.0 license, with the\nexception of our 72B models which are also governed by the Qwen License\nAgreement.\n  Models at\nhttps://huggingface.co/collections/recursal/radlads-6818ee69e99e729ba8a87102\nTraining Code at https://github.com/recursal/RADLADS-paper",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/647f4bac45baf21ad709fcd0/sfcDfWZfka9sy8siQC6MV.png",
      "https://cdn-uploads.huggingface.co/production/uploads/647f4bac45baf21ad709fcd0/yXcUOgY48NS_nfIZN0Kqq.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03005.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "647f4bac45baf21ad709fcd0",
      "avatarUrl": "/avatars/14c04cdda95de676aeefa9ae3e7c19ba.svg",
      "fullname": "Dan Goldstein",
      "name": "SmerkyG",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.02872",
      "authors": [
        {
          "_id": "681b03a33bf166767107c74b",
          "name": "Cfir Avraham Hadar",
          "hidden": false
        },
        {
          "_id": "681b03a33bf166767107c74c",
          "name": "Omer Shubi",
          "hidden": false
        },
        {
          "_id": "681b03a33bf166767107c74d",
          "name": "Yoav Meiri",
          "hidden": false
        },
        {
          "_id": "681b03a33bf166767107c74e",
          "name": "Yevgeni Berzak",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-04T13:23:48.000Z",
      "submittedOnDailyAt": "2025-05-07T05:24:49.077Z",
      "title": "\"Le découverte de l'objectif de l'exploration d'information ouverte à partir du mouvement oculaire dans le processus de lecture Roading\"",
      "submittedOnDailyBy": {
        "_id": "60ef001bed64a34082bfa0dd",
        "avatarUrl": "/avatars/78e4daeac169edbf4dc42fbed9b50d59.svg",
        "isPro": false,
        "fullname": "Omer Shubi",
        "user": "scaperex",
        "type": "user"
      },
      "summary": "Durant le processus de lecture, on se trouve souvent intéressé par des informations spécifiques. Par exemple, lorsqu'on lit un article et qu'on est intéressé par les modèles de LLMs (Large Language Models), le design des expériences, ou la question \"Est-ce efficace?\", parmi d'autres. Dans une large gamme de contextes, y compris la vie quotidienne, les objectifs uniques d'un texte guident les actions de lecture. Dans cette étude, on cherche pour la première fois si on peut interpréter automatiquement les objectifs de lecture ouverts à partir des mouvements oculaires lors de la lecture. Pour résoudre ce problème, on propose une classification des objectifs et une tâche de configuration des objectifs, ainsi qu'un cadre d'évaluation. On utilise une grande quantité de données de suivi oculaire qui incluent des tâches d'exploration d'informations uniques de texte en anglais. On développe et compare séparément et générativement divers LLMs pour la classification et la configuration des objectifs. Ces expériences montrent des succès significatifs dans les deux tâches et démontrent que les LLMs peuvent extraire des informations utiles sur les objectifs uniques d'un texte des lecteurs à partir des mouvements oculaires.",
      "upvotes": 11,
      "discussionId": "681b03a43bf166767107c787",
      "ai_keywords": [
        "goal classification",
        "goal reconstruction",
        "discriminative models",
        "generative models",
        "multimodal LLMs",
        "large-scale eye tracking",
        "text-specific information seeking"
      ]
    },
    "publishedAt": "2025-05-04T09:23:48.000Z",
    "title": "Decoding Open-Ended Information Seeking Goals from Eye Movements in\n  Reading",
    "summary": "When reading, we often have specific information that interests us in a text.\nFor example, you might be reading this paper because you are curious about LLMs\nfor eye movements in reading, the experimental design, or perhaps you only care\nabout the question ``but does it work?''. More broadly, in daily life, people\napproach texts with any number of text-specific goals that guide their reading\nbehavior. In this work, we ask, for the first time, whether open-ended reading\ngoals can be automatically decoded from eye movements in reading. To address\nthis question, we introduce goal classification and goal reconstruction tasks\nand evaluation frameworks, and use large-scale eye tracking for reading data in\nEnglish with hundreds of text-specific information seeking tasks. We develop\nand compare several discriminative and generative multimodal LLMs that combine\neye movements and text for goal classification and goal reconstruction. Our\nexperiments show considerable success on both tasks, suggesting that LLMs can\nextract valuable information about the readers' text-specific goals from eye\nmovements.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02872.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "60ef001bed64a34082bfa0dd",
      "avatarUrl": "/avatars/78e4daeac169edbf4dc42fbed9b50d59.svg",
      "fullname": "Omer Shubi",
      "name": "scaperex",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.02214",
      "authors": [
        {
          "_id": "6819905519b420a2f91aa231",
          "user": {
            "_id": "64612660933afb0106a9dee3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/Ea83e0zR_2m8foWy6J0AF.jpeg",
            "isPro": false,
            "fullname": "Xingyu Zheng",
            "user": "Xingyu-Zheng",
            "type": "user"
          },
          "name": "Xingyu Zheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-07T08:09:43.859Z",
          "hidden": false
        },
        {
          "_id": "6819905519b420a2f91aa232",
          "name": "Yuye Li",
          "hidden": false
        },
        {
          "_id": "6819905519b420a2f91aa233",
          "user": {
            "_id": "68164883244ee586cb159268",
            "avatarUrl": "/avatars/856ca8731ca156f616529e427d8fd76a.svg",
            "isPro": false,
            "fullname": "初浩然",
            "user": "HaoranChu",
            "type": "user"
          },
          "name": "Haoran Chu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-07T08:09:41.155Z",
          "hidden": false
        },
        {
          "_id": "6819905519b420a2f91aa234",
          "name": "Yue Feng",
          "hidden": false
        },
        {
          "_id": "6819905519b420a2f91aa235",
          "name": "Xudong Ma",
          "hidden": false
        },
        {
          "_id": "6819905519b420a2f91aa236",
          "name": "Jie Luo",
          "hidden": false
        },
        {
          "_id": "6819905519b420a2f91aa237",
          "name": "Jinyang Guo",
          "hidden": false
        },
        {
          "_id": "6819905519b420a2f91aa238",
          "name": "Haotong Qin",
          "hidden": false
        },
        {
          "_id": "6819905519b420a2f91aa239",
          "name": "Michele Magno",
          "hidden": false
        },
        {
          "_id": "6819905519b420a2f91aa23a",
          "name": "Xianglong Liu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64612660933afb0106a9dee3/uQV5mzdTqoNatTz5fk6xC.png"
      ],
      "publishedAt": "2025-05-04T18:43:44.000Z",
      "submittedOnDailyAt": "2025-05-07T02:59:22.920Z",
      "title": "Investigation expérimentale de réduction de Qwen3",
      "submittedOnDailyBy": {
        "_id": "64612660933afb0106a9dee3",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/Ea83e0zR_2m8foWy6J0AF.jpeg",
        "isPro": false,
        "fullname": "Xingyu Zheng",
        "user": "Xingyu-Zheng",
        "type": "user"
      },
      "summary": "La série Qwen a émergé comme une famille de leaders de grands modèles de langage naturel (LLMs) ouverts, démontrant des capacités exceptionnelles dans des tâches de compréhension du langage naturel. Dans la dernière mise à jour de Qwen3, un performance supérieure a été observée sur plusieurs benchmarks, et l'intérêt pour traiter de ces modèles de manière efficace dans des environnements avec des limitations de ressources a augmenté. La quantification à faibles performances a été proposée comme une solution prometteuse, mais son impact sur le rendement de Qwen3 n'a pas encore été étudié en détail. Dans cette étude, la robustesse de Qwen3 dans différentes configurations de quantification est évaluée de manière systématique, et les opportunités et défis associés à la compression de ce modèle de chef sont recherchés. Qwen3 a été évalué rigoureusement avec cinq technologies classiques de quantification post-entraînement, utilisant un intervalle de 1 à 8 bits de précision, et plusieurs ensembles de données ont été analysés pour évaluer son effet. Nos résultats montrent que Qwen3 maintient un rendement compétitif dans les performances de précision moyenne, mais présente un déclin notable dans les tâches de langage naturelle à faible précision, révélant des problèmes persistants dans la compression de modèles de langage naturel. Ces résultats soulignent la nécessité de recherche pour réduire la perte de rendement dans des scénarios de quantification extrême. Nous espérons que cet analyse expérimentale contribuera à développer des méthodes de quantification appropriées pour Qwen3 et futurs LLMs. Ce projet a été publié sur https://github.com/Efficient-ML/Qwen3-Quantization et sur https://huggingface.co/collections/Efficient-ML/qwen3-quantization-68164450decb1c868788cb2b.",
      "upvotes": 7,
      "discussionId": "6819905519b420a2f91aa27c",
      "githubRepo": "https://github.com/Efficient-ML/Qwen3-Quantization",
      "ai_keywords": [
        "Low-bit quantization",
        "Post-training quantization techniques",
        "Bit-widths",
        "Linguistic tasks",
        "LLM compression",
        "Quantization methods"
      ]
    },
    "publishedAt": "2025-05-04T14:43:44.000Z",
    "title": "An Empirical Study of Qwen3 Quantization",
    "summary": "The Qwen series has emerged as a leading family of open-source Large Language\nModels (LLMs), demonstrating remarkable capabilities in natural language\nunderstanding tasks. With the recent release of Qwen3, which exhibits superior\nperformance across diverse benchmarks, there is growing interest in deploying\nthese models efficiently in resource-constrained environments. Low-bit\nquantization presents a promising solution, yet its impact on Qwen3's\nperformance remains underexplored. This study conducts a systematic evaluation\nof Qwen3's robustness under various quantization settings, aiming to uncover\nboth opportunities and challenges in compressing this state-of-the-art model.\nWe rigorously assess 5 existing classic post-training quantization techniques\napplied to Qwen3, spanning bit-widths from 1 to 8 bits, and evaluate their\neffectiveness across multiple datasets. Our findings reveal that while Qwen3\nmaintains competitive performance at moderate bit-widths, it experiences\nnotable degradation in linguistic tasks under ultra-low precision, underscoring\nthe persistent hurdles in LLM compression. These results emphasize the need for\nfurther research to mitigate performance loss in extreme quantization\nscenarios. We anticipate that this empirical analysis will provide actionable\ninsights for advancing quantization methods tailored to Qwen3 and future LLMs,\nultimately enhancing their practicality without compromising accuracy. Our\nproject is released on https://github.com/Efficient-ML/Qwen3-Quantization and\nhttps://huggingface.co/collections/Efficient-ML/qwen3-quantization-68164450decb1c868788cb2b.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64612660933afb0106a9dee3/uQV5mzdTqoNatTz5fk6xC.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02214.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64612660933afb0106a9dee3",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/Ea83e0zR_2m8foWy6J0AF.jpeg",
      "fullname": "Xingyu Zheng",
      "name": "Xingyu-Zheng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.03735",
      "authors": [
        {
          "_id": "681acb9c285636e830d37c8e",
          "user": {
            "_id": "6625ce8074ae2df4e3effa92",
            "avatarUrl": "/avatars/686cd85cf8d3d8f0beb0737811144294.svg",
            "isPro": false,
            "fullname": "Jiayuan Rao 饶珈源",
            "user": "Homie0609",
            "type": "user"
          },
          "name": "Jiayuan Rao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-07T08:08:50.372Z",
          "hidden": false
        },
        {
          "_id": "681acb9c285636e830d37c8f",
          "name": "Zifeng Li",
          "hidden": false
        },
        {
          "_id": "681acb9c285636e830d37c90",
          "user": {
            "_id": "632c7a0d1d303f5f9acf01b8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/632c7a0d1d303f5f9acf01b8/T010IFuCp6UaOeIyWhbCk.jpeg",
            "isPro": false,
            "fullname": "Haoning Wu",
            "user": "haoningwu",
            "type": "user"
          },
          "name": "Haoning Wu",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-07T02:56:09.106Z",
          "hidden": false
        },
        {
          "_id": "681acb9c285636e830d37c91",
          "name": "Ya Zhang",
          "hidden": false
        },
        {
          "_id": "681acb9c285636e830d37c92",
          "name": "Yanfeng Wang",
          "hidden": false
        },
        {
          "_id": "681acb9c285636e830d37c93",
          "name": "Weidi Xie",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-06T17:59:31.000Z",
      "submittedOnDailyAt": "2025-05-07T01:26:13.756Z",
      "title": "Un système d'agents pour une compréhension générale du football",
      "submittedOnDailyBy": {
        "_id": "6625ce8074ae2df4e3effa92",
        "avatarUrl": "/avatars/686cd85cf8d3d8f0beb0737811144294.svg",
        "isPro": false,
        "fullname": "Jiayuan Rao 饶珈源",
        "user": "Homie0609",
        "type": "user"
      },
      "summary": "Récemment, le développement dans le domaine de la compréhension du football dirigé par l'IA a progressé rapidement, mais les études actuelles se concentrent principalement sur des éléments séparés et sur des tâches très spécifiques. Pour combler cette lacune, on propose un cadre strict pour comprendre le football dans son ensemble. Concrètement, cet article fait les contributions suivantes : (i) on construit SoccerWiki, qui est le premier grande base de connaissances multimodale en football, intégrant des connaissances riches d'aires telles que les joueurs, les équipes, les arbitres et les fanatiques, et permettant des inférences basées sur le savoir. (ii) on présente SoccerBench, le cadre de référence le plus strict en football, qui fournit environ 10K paires de problèmes de sélection multiple de différentes tâches de compréhension du football, évalués par autocorrection et révision manuelle. (iii) on introduit SoccerAgent, un nouveau système d'agents multiagents, qui divise les problèmes complexes du football par inférence collaborative et utilise des connaissances spécifiques d'aires dans SoccerWiki pour atteindre de hauts rendements. (iv) on effectue des évaluations très larges et des tests d'élimination, benchmarkant les MLLMs les plus avancés sur SoccerBench et clairement démontrant la supériorité du système d'agents proposé. Tous les données et les codes sont disponibles sur l'URL suivante : https://jyrao.github.io/SoccerAgent/",
      "upvotes": 5,
      "discussionId": "681acb9e285636e830d37d4b",
      "projectPage": "https://jyrao.github.io/SoccerAgent/",
      "githubRepo": "https://github.com/jyrao/SoccerAgent",
      "ai_keywords": [
        "multimodal",
        "knowledge base",
        "domain knowledge",
        "multimodal (text, image, video) multi-choice QA pairs",
        "multi-agent system",
        "collaborative reasoning",
        "state-of-the-art MLLMs"
      ]
    },
    "publishedAt": "2025-05-06T13:59:31.000Z",
    "title": "Multi-Agent System for Comprehensive Soccer Understanding",
    "summary": "Recent advancements in AI-driven soccer understanding have demonstrated rapid\nprogress, yet existing research predominantly focuses on isolated or narrow\ntasks. To bridge this gap, we propose a comprehensive framework for holistic\nsoccer understanding. Specifically, we make the following contributions in this\npaper: (i) we construct SoccerWiki, the first large-scale multimodal soccer\nknowledge base, integrating rich domain knowledge about players, teams,\nreferees, and venues to enable knowledge-driven reasoning; (ii) we present\nSoccerBench, the largest and most comprehensive soccer-specific benchmark,\nfeaturing around 10K standardized multimodal (text, image, video) multi-choice\nQA pairs across 13 distinct understanding tasks, curated through automated\npipelines and manual verification; (iii) we introduce SoccerAgent, a novel\nmulti-agent system that decomposes complex soccer questions via collaborative\nreasoning, leveraging domain expertise from SoccerWiki and achieving robust\nperformance; (iv) extensive evaluations and ablations that benchmark\nstate-of-the-art MLLMs on SoccerBench, highlighting the superiority of our\nproposed agentic system. All data and code are publicly available at:\nhttps://jyrao.github.io/SoccerAgent/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03735.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6625ce8074ae2df4e3effa92",
      "avatarUrl": "/avatars/686cd85cf8d3d8f0beb0737811144294.svg",
      "fullname": "Jiayuan Rao 饶珈源",
      "name": "Homie0609",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.03368",
      "authors": [
        {
          "_id": "681b05ab8e325b4097318969",
          "user": {
            "_id": "661e841972c7030f3fd57eb9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/661e841972c7030f3fd57eb9/PysszxrOl__sZ-IToZfJU.jpeg",
            "isPro": false,
            "fullname": "Stef De Sabbata",
            "user": "sdesabbata",
            "type": "user"
          },
          "name": "Stef De Sabbata",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-07T08:08:24.829Z",
          "hidden": false
        },
        {
          "_id": "681b05ab8e325b409731896a",
          "name": "Stefano Mizzaro",
          "hidden": false
        },
        {
          "_id": "681b05ab8e325b409731896b",
          "name": "Kevin Roitero",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-06T09:40:06.000Z",
      "submittedOnDailyAt": "2025-05-07T05:33:25.973Z",
      "title": "Le Modèle de Génération de Spectres pour l'Explication Mécanique des Langages de Grande Taille",
      "submittedOnDailyBy": {
        "_id": "620cca6f06a4320dbf3b50d8",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654011487582-620cca6f06a4320dbf3b50d8.png",
        "isPro": false,
        "fullname": "Kevin Roitero",
        "user": "kevinr",
        "type": "user"
      },
      "summary": "Les modèles de langage grands (LLMs) montrent des capacités sans précédent dans diverses tâches de traitement du langage naturel. Leur habileté à processer et à générer du texte et du code est largement utilisée dans plusieurs domaines, mais son utilisation comme base de connaissances et comme outil de 'ronronner' reste un domaine de recherche. La littérature dans le domaine de la géologie est en augmentation dans l'évaluation des connaissances géographiques et des capacités spatiales des LLMs, mais il n'existe pas une conscience spécifique sur comment ces modèles traitent les données géographiques.\n\nDans cette section, un nouveau cadre est construit pour explorer la possibilité d'expliquer la structure spatiale de la Terre. Ce cadre utilise un analyse spatiale pour effectuer un analyse inverse de la manière dont les LLMs traitent les données géographiques. Notre objectif est de comprendre les représentations internes générées par les modèles lorsqu'ils traitent des données géographiques. Cela nous permettra de considérer comment ces modèles pensent sur les données géographiques, mais sans exagérer l'anthropisation.\n\nTout d'abord, on explique la façon de découvrir la structure interne des LLMs. Ensuite, on discute les rôles de la spectrométrie diodé et de l'autoencodeur à codage rare dans le domaine de l'explication structurale. Ces modèles sont analysés pour découvrir leurs représentations internes multiples et pour les décomposer en caractéristiques uniques et explicables. Dans les expériences, l'autocorrélation spatiale est utilisée pour montrer que les caractéristiques associées aux noms géographiques sont liées à des patrons spatiaux, fournissant une compréhension plus profonde de la manière dont les modèles traitent les données géographiques. Enfin, on discute comment notre cadre peut aider à la recherche et à l'utilisation de modèles de base en géologie.",
      "upvotes": 3,
      "discussionId": "681b05ad8e325b4097318a3d",
      "ai_keywords": [
        "probing",
        "mechanistic interpretability",
        "superposition hypothesis",
        "sparse autoencoders",
        "polysemantic",
        "monosemantic",
        "spatial autocorrelation",
        "placenames",
        "geospatial patterns"
      ]
    },
    "publishedAt": "2025-05-06T05:40:06.000Z",
    "title": "Geospatial Mechanistic Interpretability of Large Language Models",
    "summary": "Large Language Models (LLMs) have demonstrated unprecedented capabilities\nacross various natural language processing tasks. Their ability to process and\ngenerate viable text and code has made them ubiquitous in many fields, while\ntheir deployment as knowledge bases and \"reasoning\" tools remains an area of\nongoing research. In geography, a growing body of literature has been focusing\non evaluating LLMs' geographical knowledge and their ability to perform spatial\nreasoning. However, very little is still known about the internal functioning\nof these models, especially about how they process geographical information.\n  In this chapter, we establish a novel framework for the study of geospatial\nmechanistic interpretability - using spatial analysis to reverse engineer how\nLLMs handle geographical information. Our aim is to advance our understanding\nof the internal representations that these complex models generate while\nprocessing geographical information - what one might call \"how LLMs think about\ngeographic information\" if such phrasing was not an undue anthropomorphism.\n  We first outline the use of probing in revealing internal structures within\nLLMs. We then introduce the field of mechanistic interpretability, discussing\nthe superposition hypothesis and the role of sparse autoencoders in\ndisentangling polysemantic internal representations of LLMs into more\ninterpretable, monosemantic features. In our experiments, we use spatial\nautocorrelation to show how features obtained for placenames display spatial\npatterns related to their geographic location and can thus be interpreted\ngeospatially, providing insights into how these models process geographical\ninformation. We conclude by discussing how our framework can help shape the\nstudy and use of foundation models in geography.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03368.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "620cca6f06a4320dbf3b50d8",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654011487582-620cca6f06a4320dbf3b50d8.png",
      "fullname": "Kevin Roitero",
      "name": "kevinr",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.03164",
      "authors": [
        {
          "_id": "681ac5d2626f5a368b2a7103",
          "name": "Ji Won Chung",
          "hidden": false
        },
        {
          "_id": "681ac5d2626f5a368b2a7104",
          "name": "Tongyu Zhou",
          "hidden": false
        },
        {
          "_id": "681ac5d2626f5a368b2a7105",
          "name": "Ivy Chen",
          "hidden": false
        },
        {
          "_id": "681ac5d2626f5a368b2a7106",
          "name": "Kevin Hsu",
          "hidden": false
        },
        {
          "_id": "681ac5d2626f5a368b2a7107",
          "name": "Ryan A. Rossi",
          "hidden": false
        },
        {
          "_id": "681ac5d2626f5a368b2a7108",
          "name": "Alexa Siu",
          "hidden": false
        },
        {
          "_id": "681ac5d2626f5a368b2a7109",
          "name": "Shunan Guo",
          "hidden": false
        },
        {
          "_id": "681ac5d2626f5a368b2a710a",
          "user": {
            "_id": "62c5947524171688a9feb992",
            "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
            "isPro": false,
            "fullname": "Franck Dernoncourt",
            "user": "Franck-Dernoncourt",
            "type": "user"
          },
          "name": "Franck Dernoncourt",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-07T08:08:55.782Z",
          "hidden": false
        },
        {
          "_id": "681ac5d2626f5a368b2a710b",
          "name": "James Tompkin",
          "hidden": false
        },
        {
          "_id": "681ac5d2626f5a368b2a710c",
          "name": "Jeff Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-06T04:18:42.000Z",
      "submittedOnDailyAt": "2025-05-07T01:00:46.546Z",
      "title": "InfoVids : Réalise une réévaluation de l'expérience vidéo des utilisateurs et utilise les relations entre différents méthodes de visualisation et de présentation pour effectuer une réorganisation.",
      "submittedOnDailyBy": {
        "_id": "62c5947524171688a9feb992",
        "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
        "isPro": false,
        "fullname": "Franck Dernoncourt",
        "user": "Franck-Dernoncourt",
        "type": "user"
      },
      "summary": "La représentation traditionnelle des données sépare la présentation et la visualisation dans deux espaces différents : le monde tridimensionnel et la écran bidimensionnel, ce qui renforce la narration des créateurs de visualisations. Pour créer une expérience humaine pour les spectateurs, nous utilisons InfoVids pour connecter plus équitablement la présentation et la visualisation. Ces vidéos d'information ont été conçues pour réinstaller la relation entre la présentation et la visualisation. Pendant le processus de conception d'InfoVids, nous avons étudié comment l'expérience du spectateur change avec l'utilisation de roues, de forces et d'interactions. Avec 30 participants et 9 points de référence, nous offrons une présentation pratique et utile par rapport aux slides bidimensionnels standards. L'analyse de méthodes mixtes a démontré que ce paradigme réduit la chute de l'attention du spectateur, change la concentration de la visualisation vers la présentation, et atteint une expérience interactive, naturelle et agréable des données. Enfin, InfoVids réévalue la motivation traditionnelle entre la présentation et la visualisation pour les spectateurs.",
      "upvotes": 3,
      "discussionId": "681ac5d4626f5a368b2a71f8"
    },
    "publishedAt": "2025-05-06T00:18:42.000Z",
    "title": "InfoVids: Reimagining the Viewer Experience with Alternative\n  Visualization-Presenter Relationships",
    "summary": "Traditional data presentations typically separate the presenter and\nvisualization into two separate spaces--the 3D world and a 2D screen--enforcing\nvisualization-centric stories. To create a more human-centric viewing\nexperience, we establish a more equitable relationship between the\nvisualization and the presenter through our InfoVids. These\ninfographics-inspired informational videos are crafted to redefine\nrelationships between the presenter and visualizations. As we design InfoVids,\nwe explore how the use of layout, form, and interactions affects the viewer\nexperience. We compare InfoVids against their baseline 2D `slides' equivalents\nacross 9 metrics with 30 participants and provide practical, long-term insights\nfrom an autobiographical perspective. Our mixed methods analyses reveal that\nthis paradigm reduced viewer attention splitting, shifted the focus from the\nvisualization to the presenter, and led to more interactive, natural, and\nengaging full-body data performances for viewers. Ultimately, InfoVids helped\nviewers re-imagine traditional dynamics between the presenter and\nvisualizations.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03164.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62c5947524171688a9feb992",
      "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
      "fullname": "Franck Dernoncourt",
      "name": "Franck-Dernoncourt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.03739",
      "authors": [
        {
          "_id": "681b232d7167935b1e979e64",
          "name": "Zuwei Long",
          "hidden": false
        },
        {
          "_id": "681b232d7167935b1e979e65",
          "name": "Yunhang Shen",
          "hidden": false
        },
        {
          "_id": "681b232d7167935b1e979e66",
          "name": "Chaoyou Fu",
          "hidden": false
        },
        {
          "_id": "681b232d7167935b1e979e67",
          "name": "Heting Gao",
          "hidden": false
        },
        {
          "_id": "681b232d7167935b1e979e68",
          "name": "Lijiang Li",
          "hidden": false
        },
        {
          "_id": "681b232d7167935b1e979e69",
          "name": "Peixian Chen",
          "hidden": false
        },
        {
          "_id": "681b232d7167935b1e979e6a",
          "name": "Mengdan Zhang",
          "hidden": false
        },
        {
          "_id": "681b232d7167935b1e979e6b",
          "name": "Hang Shao",
          "hidden": false
        },
        {
          "_id": "681b232d7167935b1e979e6c",
          "name": "Jian Li",
          "hidden": false
        },
        {
          "_id": "681b232d7167935b1e979e6d",
          "name": "Jinlong Peng",
          "hidden": false
        },
        {
          "_id": "681b232d7167935b1e979e6e",
          "name": "Haoyu Cao",
          "hidden": false
        },
        {
          "_id": "681b232d7167935b1e979e6f",
          "name": "Ke Li",
          "hidden": false
        },
        {
          "_id": "681b232d7167935b1e979e70",
          "name": "Rongrong Ji",
          "hidden": false
        },
        {
          "_id": "681b232d7167935b1e979e71",
          "name": "Xing Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-06T17:59:53.000Z",
      "submittedOnDailyAt": "2025-05-07T08:07:03.177Z",
      "title": "VITA-Audio : Génération efficace de tokens pour des modèles de langage et de voix à grande échelle rapide",
      "submittedOnDailyBy": {
        "_id": "6483143902f98c3f05aff915",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6483143902f98c3f05aff915/ZhWFFgrlRsQf4MXiInh5p.jpeg",
        "isPro": true,
        "fullname": "沈云航 Yunhang Shen",
        "user": "shenyunhang",
        "type": "user"
      },
      "summary": "L'augmentation de la nécessité d'une interaction naturelle entre les humains et les ordinateurs a conduit à que les systèmes basés sur la voix reçoivent plus d'attention. Ces systèmes utilisent la parole, la forme la plus commune de communication quotidienne, pour interagir. Cependant, les modèles de voix actuels présentent des latencies élevées lors de la génération des premiers tokens de voix dans le processus de streaming, ce qui peut causer un fatigue significatif. Pour résoudre ces problèmes, nous proposons VITA-Audio, un modèle de voix à grande échelle du dispositif jusqu'au dispositif. En particulier, nous introduisons un module léger de Prédiction de Tokens Multimodal (MCTP) pour générer efficacement plusieurs tokens de voix en un seul pas de propagation du modèle, ce qui accélère la vitesse d'inférence et réduit significativement la latencie de la génération de la première voix dans les scénarios de streaming. De plus, nous examinons une stratégie d'entraînement progressif en quatre étapes pour accélérer le modèle sans perdre la qualité de la voix. Selon notre expérience, VITA-Audio est le premier modèle de voix à grande échelle capable de générer des sorties de voix dans le processus de propagation du modèle et permet une communication en temps réel avec une latencie minimale. VITA-Audio est un modèle entièrement reproductible et son dataset est open-source. Les résultats des expérimentations montrent que notre modèle atteint un accroissement de vitesse d'inférence de 3 à 5 fois pour un taille de 7B paramètres, surpassant significativement d'autres modèles open-source de même échelle dans des tests comme le Reconnaissance Automatique du Langage (ASR), la Génération de Contexte (TTS) et la Réponse à des Questions Interactives (SQA).",
      "upvotes": 2,
      "discussionId": "681b232e7167935b1e979ec6",
      "ai_keywords": [
        "end-to-end large speech model",
        "Multiple Cross-modal Token Prediction (MCTP)",
        "model forward pass",
        "inference",
        "streaming scenarios",
        "four-stage progressive training strategy",
        "multi-modal large language model",
        "real-time conversational capabilities",
        "inference speedup",
        "automatic speech recognition (ASR)",
        "text-to-speech (TTS)",
        "spoken question answering (SQA)"
      ]
    },
    "publishedAt": "2025-05-06T13:59:53.000Z",
    "title": "VITA-Audio: Fast Interleaved Cross-Modal Token Generation for Efficient\n  Large Speech-Language Model",
    "summary": "With the growing requirement for natural human-computer interaction,\nspeech-based systems receive increasing attention as speech is one of the most\ncommon forms of daily communication. However, the existing speech models still\nexperience high latency when generating the first audio token during streaming,\nwhich poses a significant bottleneck for deployment. To address this issue, we\npropose VITA-Audio, an end-to-end large speech model with fast audio-text token\ngeneration. Specifically, we introduce a lightweight Multiple Cross-modal Token\nPrediction (MCTP) module that efficiently generates multiple audio tokens\nwithin a single model forward pass, which not only accelerates the inference\nbut also significantly reduces the latency for generating the first audio in\nstreaming scenarios. In addition, a four-stage progressive training strategy is\nexplored to achieve model acceleration with minimal loss of speech quality. To\nour knowledge, VITA-Audio is the first multi-modal large language model capable\nof generating audio output during the first forward pass, enabling real-time\nconversational capabilities with minimal latency. VITA-Audio is fully\nreproducible and is trained on open-source data only. Experimental results\ndemonstrate that our model achieves an inference speedup of 3~5x at the 7B\nparameter scale, but also significantly outperforms open-source models of\nsimilar model size on multiple benchmarks for automatic speech recognition\n(ASR), text-to-speech (TTS), and spoken question answering (SQA) tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03739.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6483143902f98c3f05aff915",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6483143902f98c3f05aff915/ZhWFFgrlRsQf4MXiInh5p.jpeg",
      "fullname": "沈云航 Yunhang Shen",
      "name": "shenyunhang",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.02311",
      "authors": [
        {
          "_id": "681a06459c99f4b5ce5f2838",
          "user": {
            "_id": "658e85bb5b7553ca5c29ba89",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658e85bb5b7553ca5c29ba89/KK6UpS9agtrxevvBoup5N.jpeg",
            "isPro": false,
            "fullname": "Jihao Zhao",
            "user": "Robot2050",
            "type": "user"
          },
          "name": "Jihao Zhao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-06T16:51:29.234Z",
          "hidden": false
        },
        {
          "_id": "681a06459c99f4b5ce5f2839",
          "name": "Chunlai Zhou",
          "hidden": false
        },
        {
          "_id": "681a06459c99f4b5ce5f283a",
          "name": "Biao Qin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-05T01:45:56.000Z",
      "submittedOnDailyAt": "2025-05-07T00:06:14.265Z",
      "title": "L'Interface d'Invocation est appelée selon la nécessité : Résolution de problèmes par l'invocation adaptative de modèles de langage à grande échelle.",
      "submittedOnDailyBy": {
        "_id": "658e85bb5b7553ca5c29ba89",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658e85bb5b7553ca5c29ba89/KK6UpS9agtrxevvBoup5N.jpeg",
        "isPro": false,
        "fullname": "Jihao Zhao",
        "user": "Robot2050",
        "type": "user"
      },
      "summary": "Le paradigme de collaboration entre grands et petits modèles de langage (LM) peut équilibrer efficacement le compromis entre performance et coût, mais il est crucial d'identifier précisément le moment où apparaissent les bruits dans les petits LM. Les efforts de précédentes optimisations ont principalement porté sur des méthodes de post-traitement, qui séparent le processus d'explication du LM et associent des coûts computationnels élevés et des effets limités. Dans cet article, nous proposons une mesure d'évaluation pratique pour calculer l'accumulation et la propagation de bruits dans le processus de génération des petits LM, appelée \"AttenHScore\". Cet indicateur permet d'étendre de manière continue les erreurs potentielles d'explication, ce qui permettrait aux grands LM de réaliser des appels en temps réel de meilleure qualité. De plus, nous considérons les limites de la capacité d'explication des petits LM et utilisons la reconstruction de connaissance pour traiter les incertitudes, ce qui permet d'extraire plus efficacement des informations de différents textes de chatbots. Les expériences étendues montrent que \"AttenHScore\" améliore significativement la capacité de détection de bruits en temps réel sur plusieurs ensembles de données de questions et de réponses, surtout dans les cas de questions complexes. De plus, la stratégie de cet article élimine la nécessité d'entraînement supplémentaire de modèles et démontre son adaptabilité à différents modèles de langage basés sur les Transformers.",
      "upvotes": 2,
      "discussionId": "681a06469c99f4b5ce5f28a3",
      "ai_keywords": [
        "large language models (LMs)",
        "small language models (LMs)",
        "hallucinations",
        "post-processing techniques",
        "AttenHScore",
        "generation process",
        "reasoning errors",
        "dynamic adjustment",
        "real-time invocation",
        "reasoning capacity",
        "uncertainty-aware knowledge reorganization",
        "QA datasets",
        "complex queries",
        "transformer-based LMs"
      ]
    },
    "publishedAt": "2025-05-04T21:45:56.000Z",
    "title": "Invoke Interfaces Only When Needed: Adaptive Invocation for Large\n  Language Models in Question Answering",
    "summary": "The collaborative paradigm of large and small language models (LMs)\neffectively balances performance and cost, yet its pivotal challenge lies in\nprecisely pinpointing the moment of invocation when hallucinations arise in\nsmall LMs. Previous optimization efforts primarily focused on post-processing\ntechniques, which were separate from the reasoning process of LMs, resulting in\nhigh computational costs and limited effectiveness. In this paper, we propose a\npractical invocation evaluation metric called AttenHScore, which calculates the\naccumulation and propagation of hallucinations during the generation process of\nsmall LMs, continuously amplifying potential reasoning errors. By dynamically\nadjusting the detection threshold, we achieve more accurate real-time\ninvocation of large LMs. Additionally, considering the limited reasoning\ncapacity of small LMs, we leverage uncertainty-aware knowledge reorganization\nto assist them better capture critical information from different text chunks.\nExtensive experiments reveal that our AttenHScore outperforms most baseline in\nenhancing real-time hallucination detection capabilities across multiple QA\ndatasets, especially when addressing complex queries. Moreover, our strategies\neliminate the need for additional model training and display flexibility in\nadapting to various transformer-based LMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02311.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "658e85bb5b7553ca5c29ba89",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658e85bb5b7553ca5c29ba89/KK6UpS9agtrxevvBoup5N.jpeg",
      "fullname": "Jihao Zhao",
      "name": "Robot2050",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.21650",
      "authors": [
        {
          "_id": "681b258f23dbf6b59c5fc735",
          "name": "Haiyang Zhou",
          "hidden": false
        },
        {
          "_id": "681b258f23dbf6b59c5fc736",
          "user": {
            "_id": "63f095be6309c84d5f48848a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f095be6309c84d5f48848a/pL2CKi-r-0mMfIGhYSAsm.jpeg",
            "isPro": false,
            "fullname": "Wangbo Yu",
            "user": "Drexubery",
            "type": "user"
          },
          "name": "Wangbo Yu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-07T09:19:13.250Z",
          "hidden": false
        },
        {
          "_id": "681b258f23dbf6b59c5fc737",
          "name": "Jiawen Guan",
          "hidden": false
        },
        {
          "_id": "681b258f23dbf6b59c5fc738",
          "name": "Xinhua Cheng",
          "hidden": false
        },
        {
          "_id": "681b258f23dbf6b59c5fc739",
          "name": "Yonghong Tian",
          "hidden": false
        },
        {
          "_id": "681b258f23dbf6b59c5fc73a",
          "name": "Li Yuan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-30T13:55:28.000Z",
      "submittedOnDailyAt": "2025-05-07T07:52:52.459Z",
      "title": "HoloTiempo : En utilisant un modèle de rétroaction, on peut générer une scène panoramique 4D pour l'apprentissage profond en 2D.",
      "submittedOnDailyBy": {
        "_id": "66eeda3676a8038cb448f11d",
        "avatarUrl": "/avatars/8d6e61f4c9354c6720ccaa7be0fe1d9f.svg",
        "isPro": false,
        "fullname": "Haiyang Zhou",
        "user": "Marblueocean",
        "type": "user"
      },
      "summary": "Le rapide développement des modèles de diffusion montre la possibilité innovante d'application des technologies VR et AR. En général, ces technologies nécessitent des ressources 4D au niveau de l'échantillon pour améliorer l'expérience utilisateur. Cependant, les modèles de diffusion actuels principalement modèlent des scènes 3D statiques ou des actions au niveau de l'objet, ce qui limite la possibilité de fournir une expérience satisfaisante. Pour résoudre ces problèmes, nous proposons HoloTime. Ce cadre intègre des modèles de diffusion de vidéo pour combiner la reconstruction de scènes 360° 4D à partir d'un prompt ou d'une image de référence, transformant le vidéo 360° en un ressource 4D permettant aux utilisateurs d'expérimenter complètement en 4D. Spécifiquement, pour appliquer des modèles de diffusion de vidéo à la génération de vidéos 360°, nous présentons le dataset 360World. Ceci est le premier ensemble détaillé de vidéos 360° conçu pour la reconstruction de scènes 4D. En se basant sur cet ensemble, nous proposons Panoramic Animator, qui utilise des modèles de diffusion de vidéo en deux étapes pour convertir des qualités de vidéos 360° en vidéos 360°. De plus, nous présentons Panoramic Space-Time Reconstruction, qui utilise des méthodes d'estimation de profondeur en espace-temps pour convertir des vidéos 360° en clusters de points 4D, facilitant la reconstruction de scènes 4D en temps réel. Nous évaluons l'effet de notre méthode comparativement aux méthodes actuelles, démontrant sa supériorité dans la génération de vidéos 360° et la reconstruction de scènes 4D. Cela démontre que notre méthode, capable de maintenir l'intérêt de l'utilisateur et de créer des environnements satisfaisants en temps réel, améliore l'expérience utilisateur dans les applications VR et AR.",
      "upvotes": 1,
      "discussionId": "681b259123dbf6b59c5fc7cf",
      "projectPage": "https://zhouhyocean.github.io/holotime/",
      "githubRepo": "https://github.com/PKU-YuanGroup/HoloTime",
      "ai_keywords": [
        "diffusion models",
        "HoloTime",
        "video diffusion models",
        "panoramic videos",
        "360-degree 4D scene reconstruction",
        "360World dataset",
        "Panoramic Animator",
        "image-to-video diffusion model",
        "Panoramic Space-Time Reconstruction",
        "space-time depth estimation",
        "4D point clouds",
        "Gaussian Splatting",
        "4D scenes",
        "spatially and temporally consistent"
      ]
    },
    "publishedAt": "2025-04-30T09:55:28.000Z",
    "title": "HoloTime: Taming Video Diffusion Models for Panoramic 4D Scene\n  Generation",
    "summary": "The rapid advancement of diffusion models holds the promise of\nrevolutionizing the application of VR and AR technologies, which typically\nrequire scene-level 4D assets for user experience. Nonetheless, existing\ndiffusion models predominantly concentrate on modeling static 3D scenes or\nobject-level dynamics, constraining their capacity to provide truly immersive\nexperiences. To address this issue, we propose HoloTime, a framework that\nintegrates video diffusion models to generate panoramic videos from a single\nprompt or reference image, along with a 360-degree 4D scene reconstruction\nmethod that seamlessly transforms the generated panoramic video into 4D assets,\nenabling a fully immersive 4D experience for users. Specifically, to tame video\ndiffusion models for generating high-fidelity panoramic videos, we introduce\nthe 360World dataset, the first comprehensive collection of panoramic videos\nsuitable for downstream 4D scene reconstruction tasks. With this curated\ndataset, we propose Panoramic Animator, a two-stage image-to-video diffusion\nmodel that can convert panoramic images into high-quality panoramic videos.\nFollowing this, we present Panoramic Space-Time Reconstruction, which leverages\na space-time depth estimation method to transform the generated panoramic\nvideos into 4D point clouds, enabling the optimization of a holistic 4D\nGaussian Splatting representation to reconstruct spatially and temporally\nconsistent 4D scenes. To validate the efficacy of our method, we conducted a\ncomparative analysis with existing approaches, revealing its superiority in\nboth panoramic video generation and 4D scene reconstruction. This demonstrates\nour method's capability to create more engaging and realistic immersive\nenvironments, thereby enhancing user experiences in VR and AR applications.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.21650.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66eeda3676a8038cb448f11d",
      "avatarUrl": "/avatars/8d6e61f4c9354c6720ccaa7be0fe1d9f.svg",
      "fullname": "Haiyang Zhou",
      "name": "Marblueocean",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.18373",
      "authors": [
        {
          "_id": "681af7a58f41e99d8ca05074",
          "user": {
            "_id": "6454d2ee1a543cf97b1ba671",
            "avatarUrl": "/avatars/30edc30cf3684bf685aabffcec8a3c30.svg",
            "isPro": false,
            "fullname": "shen",
            "user": "lorashen",
            "type": "user"
          },
          "name": "Lei Shen",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-07T06:09:12.039Z",
          "hidden": false
        },
        {
          "_id": "681af7a58f41e99d8ca05075",
          "name": "Xiaoyu Shen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-25T14:17:47.000Z",
      "submittedOnDailyAt": "2025-05-07T04:49:41.738Z",
      "title": "Auto-SLURP : modèle de référence de jeu de données d'évaluation pour un cadre d'agent multi-agent intelligent et privé",
      "submittedOnDailyBy": {
        "_id": "6454d2ee1a543cf97b1ba671",
        "avatarUrl": "/avatars/30edc30cf3684bf685aabffcec8a3c30.svg",
        "isPro": false,
        "fullname": "shen",
        "user": "lorashen",
        "type": "user"
      },
      "summary": "Récemment, le développement des modèles de travail multi-agent basés sur des modèles de langage grands (LLMs) a progressé de manière rapide. Dans ce processus, en particulier, l'insuffisance de disponibilité de jeux de données de référence pour évaluer le rendement de ces modèles de travail multi-agent basés sur des LLMs a été notable. Pour corriger cette lacune, nous présentons \"Auto-SLURP\", un jeu de données de référence pour évaluer le rendement de modèles de travail multi-agent basés sur des LLMs, développé dans le contexte d'assistants personnels intelligents. Auto-SLURP étend le jeu de données original SLURP, qui a été initialement développé, grâce à la réaffectation de labels de données et à l'intégration avec des serveurs de calcul et des services externes. Dans cette extension, un processus d'évaluation end-to-end cohérent a été mis en place pour évaluer la compréhension du langage, l'exécution de tâches et la génération de réponses. Nos expériences représentent les défis les plus importants pour les modèles de travail multi-agent leader au moment, et clairement indiquent que l'assistant personnel intelligent multi-agent que l'on puisse confiance n'est encore en développement. Le jeu de données et le code associé sont disponibles sur https://github.com/lorashen/Auto-SLURP/.",
      "upvotes": 0,
      "discussionId": "681af7a68f41e99d8ca050ba",
      "githubRepo": "https://github.com/lorashen/Auto-SLURP/",
      "ai_keywords": [
        "multi-agent frameworks",
        "large language models (LLMs)",
        "benchmark datasets",
        "natural language understanding tasks",
        "simulated servers",
        "external services",
        "end-to-end evaluation pipeline",
        "language understanding",
        "task execution",
        "response generation"
      ]
    },
    "publishedAt": "2025-04-25T10:17:47.000Z",
    "title": "Auto-SLURP: A Benchmark Dataset for Evaluating Multi-Agent Frameworks in\n  Smart Personal Assistant",
    "summary": "In recent years, multi-agent frameworks powered by large language models\n(LLMs) have advanced rapidly. Despite this progress, there is still a notable\nabsence of benchmark datasets specifically tailored to evaluate their\nperformance. To bridge this gap, we introduce Auto-SLURP, a benchmark dataset\naimed at evaluating LLM-based multi-agent frameworks in the context of\nintelligent personal assistants. Auto-SLURP extends the original SLURP dataset\n-- initially developed for natural language understanding tasks -- by\nrelabeling the data and integrating simulated servers and external services.\nThis enhancement enables a comprehensive end-to-end evaluation pipeline,\ncovering language understanding, task execution, and response generation. Our\nexperiments demonstrate that Auto-SLURP presents a significant challenge for\ncurrent state-of-the-art frameworks, highlighting that truly reliable and\nintelligent multi-agent personal assistants remain a work in progress. The\ndataset and related code are available at\nhttps://github.com/lorashen/Auto-SLURP/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.18373.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6454d2ee1a543cf97b1ba671",
      "avatarUrl": "/avatars/30edc30cf3684bf685aabffcec8a3c30.svg",
      "fullname": "shen",
      "name": "lorashen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]