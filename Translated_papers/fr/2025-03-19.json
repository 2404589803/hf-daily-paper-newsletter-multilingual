[
  {
    "paper": {
      "id": "2503.14456",
      "authors": [
        {
          "_id": "67da21ed78c08b432f9fee0c",
          "user": {
            "_id": "62b3d8d651b07307bd12b7f0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1655953609090-noauth.jpeg",
            "isPro": false,
            "fullname": "BlinkDL",
            "user": "BlinkDL",
            "type": "user"
          },
          "name": "Bo Peng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-19T09:44:45.587Z",
          "hidden": false
        },
        {
          "_id": "67da21ed78c08b432f9fee0d",
          "user": {
            "_id": "6418629fd13ffa408128d7ae",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1679319546731-noauth.png",
            "isPro": false,
            "fullname": "Zhang Ruichong",
            "user": "ZhangRC",
            "type": "user"
          },
          "name": "Ruichong Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-19T09:44:54.749Z",
          "hidden": false
        },
        {
          "_id": "67da21ed78c08b432f9fee0e",
          "user": {
            "_id": "647f4bac45baf21ad709fcd0",
            "avatarUrl": "/avatars/14c04cdda95de676aeefa9ae3e7c19ba.svg",
            "isPro": false,
            "fullname": "Dan Goldstein",
            "user": "SmerkyG",
            "type": "user"
          },
          "name": "Daniel Goldstein",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-19T09:44:57.333Z",
          "hidden": false
        },
        {
          "_id": "67da21ed78c08b432f9fee0f",
          "name": "Eric Alcaide",
          "hidden": false
        },
        {
          "_id": "67da21ed78c08b432f9fee10",
          "name": "Haowen Hou",
          "hidden": false
        },
        {
          "_id": "67da21ed78c08b432f9fee11",
          "name": "Janna Lu",
          "hidden": false
        },
        {
          "_id": "67da21ed78c08b432f9fee12",
          "name": "William Merrill",
          "hidden": false
        },
        {
          "_id": "67da21ed78c08b432f9fee13",
          "user": {
            "_id": "622c062645261ac5cc0bda94",
            "avatarUrl": "/avatars/ce544b74110f7fe1ad11a3939526f5da.svg",
            "isPro": false,
            "fullname": "Guangyu Song",
            "user": "Guangyu",
            "type": "user"
          },
          "name": "Guangyu Song",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-19T09:50:42.957Z",
          "hidden": false
        },
        {
          "_id": "67da21ed78c08b432f9fee14",
          "name": "Kaifeng Tan",
          "hidden": false
        },
        {
          "_id": "67da21ed78c08b432f9fee15",
          "user": {
            "_id": "638f1fd8c4444c6ca86ff823",
            "avatarUrl": "/avatars/405807c3868663246cfe371a2034f351.svg",
            "isPro": false,
            "fullname": "saitejautpala",
            "user": "saitejautpala",
            "type": "user"
          },
          "name": "Saiteja Utpala",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-19T09:48:50.276Z",
          "hidden": false
        },
        {
          "_id": "67da21ed78c08b432f9fee16",
          "user": {
            "_id": "63cac1a50932c72f13995d6f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63cac1a50932c72f13995d6f/Bd9jEsCL9yWXdyAQCx61J.jpeg",
            "isPro": false,
            "fullname": "Nathan Wilce",
            "user": "m8than",
            "type": "user"
          },
          "name": "Nathan Wilce",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-19T09:50:59.151Z",
          "hidden": false
        },
        {
          "_id": "67da21ed78c08b432f9fee17",
          "name": "Johan S. Wind",
          "hidden": false
        },
        {
          "_id": "67da21ed78c08b432f9fee18",
          "name": "Tianyi Wu",
          "hidden": false
        },
        {
          "_id": "67da21ed78c08b432f9fee19",
          "name": "Daniel Wuttke",
          "hidden": false
        },
        {
          "_id": "67da21ed78c08b432f9fee1a",
          "user": {
            "_id": "6584f042b378d311dccea501",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/vkq1AQhcuZwIjpFDkdgPQ.png",
            "isPro": false,
            "fullname": "Christian Zhou-Zheng",
            "user": "ChristianAzinn",
            "type": "user"
          },
          "name": "Christian Zhou-Zheng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-19T09:51:20.835Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-18T17:31:05.000Z",
      "submittedOnDailyAt": "2025-03-19T00:29:42.147Z",
      "title": "RWKV-7 \"Goose\" est un modèle qui a la fonction d'évolution dynamique des états expressifs.",
      "submittedOnDailyBy": {
        "_id": "6418629fd13ffa408128d7ae",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1679319546731-noauth.png",
        "isPro": false,
        "fullname": "Zhang Ruichong",
        "user": "ZhangRC",
        "type": "user"
      },
      "summary": "RWKV-7 \"Goose\" est une nouvelle architecture de modélisation de séquences. De plus, avec une échelle de 300 millions de paramètres, il établit un nouveau rendement optimal pour des tâches multilingues, et comparé à d'autres modèles de 300 millions, il est entraîné avec un nombre significativement moins de tokens tout en maintenant l'amélioration actuelle en anglais. Cependant, le modèle RWKV-7 nécessite un usage de mémoire fixe par token et un temps d'inférence fixe. Il introduit une nouvelle généralisation de la formule deltarule qui inclut le vecteur de gestion et l'apprentissage de la vitesse de contexte, et atténue les règles d'évaluation. Le modèle RWKV-7 suit les états et reconnaît tous les langages normaux, ce qui permet d'entraîner les deux en même temps. Cela dépasse les capacités du Transformer, qui dépendent d'hypothèses de complexité standard. Pour démontrer sa capacité en modélisation de langue, un corpus multilingue de 3,1 trillions de tokens est publié, et quatre modèles RWKV-7 avec des paramètres entre 190 millions et 2,9 milliards sont entraînés sur cette base de données.\n\nLes modèles RWKV et la liste des bases de données sont disponibles sur https://huggingface.co/RWKV, et le code d'entraînement et d'inférence est disponible sur https://github.com/RWKV/RWKV-LM. Tout cela est disponible sous la licence Apache 2.0.",
      "upvotes": 65,
      "discussionId": "67da21ee78c08b432f9fee71",
      "projectPage": "https://rwkv.cn",
      "githubRepo": "https://github.com/RWKV/RWKV-LM",
      "ai_keywords": [
        "sequence modeling architecture",
        "pre-trained language models",
        "downstream performance",
        "multilingual tasks",
        "in-context learning rates",
        "delta rule",
        "vector-valued gating",
        "value replacement rule",
        "state tracking",
        "regular languages",
        "parallelizability of training",
        "Transformers",
        "$\\mathsf{TC}^0$"
      ]
    },
    "publishedAt": "2025-03-18T13:31:05.000Z",
    "title": "RWKV-7 \"Goose\" with Expressive Dynamic State Evolution",
    "summary": "We present RWKV-7 \"Goose\", a new sequence modeling architecture, along with\npre-trained language models that establish a new state-of-the-art in downstream\nperformance at the 3 billion parameter scale on multilingual tasks, and match\ncurrent SoTA English language performance despite being trained on dramatically\nfewer tokens than other top 3B models. Nevertheless, RWKV-7 models require only\nconstant memory usage and constant inference time per token. RWKV-7 introduces\na newly generalized formulation of the delta rule with vector-valued gating and\nin-context learning rates, as well as a relaxed value replacement rule. We show\nthat RWKV-7 can perform state tracking and recognize all regular languages,\nwhile retaining parallelizability of training. This exceeds the capabilities of\nTransformers under standard complexity conjectures, which are limited to\nTC^0. To demonstrate RWKV-7's language modeling capability, we also\npresent an extended open source 3.1 trillion token multilingual corpus, and\ntrain four RWKV-7 models ranging from 0.19 billion to 2.9 billion parameters on\nthis dataset.\n  To foster openness, reproduction, and adoption, we release our models and\ndataset component listing at https://huggingface.co/RWKV, and our training and\ninference code at https://github.com/RWKV/RWKV-LM all under the Apache 2.0\nLicense.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14456.png",
    "numComments": 7,
    "submittedBy": {
      "_id": "6418629fd13ffa408128d7ae",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1679319546731-noauth.png",
      "fullname": "Zhang Ruichong",
      "name": "ZhangRC",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 15
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.14378",
      "authors": [
        {
          "_id": "67da1ee1f1a4a52e8a1e0241",
          "user": {
            "_id": "64b7833aa5018e3c7c9b50d8",
            "avatarUrl": "/avatars/782415605ed786b73f484fcc86a6384f.svg",
            "isPro": false,
            "fullname": "Zechen Bai",
            "user": "ZechenBai",
            "type": "user"
          },
          "name": "Zechen Bai",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-19T09:45:09.659Z",
          "hidden": false
        },
        {
          "_id": "67da1ee1f1a4a52e8a1e0242",
          "name": "Hai Ci",
          "hidden": false
        },
        {
          "_id": "67da1ee1f1a4a52e8a1e0243",
          "user": {
            "_id": "63a55320ce5763e06f78519c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1671779060549-noauth.jpeg",
            "isPro": false,
            "fullname": "Mike Shou",
            "user": "mikeshou",
            "type": "user"
          },
          "name": "Mike Zheng Shou",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-19T01:33:23.736Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64b7833aa5018e3c7c9b50d8/1jxSsiEAyMr5fSng7GOB3.mp4"
      ],
      "publishedAt": "2025-03-18T16:10:24.000Z",
      "submittedOnDailyAt": "2025-03-19T00:11:44.927Z",
      "title": "Impossible Videos est une série de sources vidéo impossibles. Cette série développe et explore les technologies de sources vidéo personnalisées pour explorer les possibilités et les limites de ces sources. Son objectif est de favoriser le développement de la technologie de sources vidéo afin que ces technologies puissent exploiter au maximum le potentiel des sources vidéo, car le progrès technologique des sources vidéo est sa principale méta.",
      "submittedOnDailyBy": {
        "_id": "64b7833aa5018e3c7c9b50d8",
        "avatarUrl": "/avatars/782415605ed786b73f484fcc86a6384f.svg",
        "isPro": false,
        "fullname": "Zechen Bai",
        "user": "ZechenBai",
        "type": "user"
      },
      "summary": "Les vidéos synthétiques actuelles sont largement utilisées pour compléter la pénurie de données et la diversité des vidéos réelles. Les ensembles de données synthétiques actuels se fondent principalement sur la récréation d'environnements réels, ce qui entraîne une recherche insuffisante sur les contenus vidéos impossibles, exceptionnels ou différents du réel. Cet article est conçu pour répondre à deux questions : 1) Les modèles de génération de vidéos peuvent-ils créer du contenu vidéo impossible ? 2) Les modèles de compréhension de vidéos peuvent-ils comprendre des contenus vidéo impossibles ? Pour cela, nous présentons IPV-Bench, un nouveau benchmark conçu pour évaluer et encourager le développement de la compréhension et de la génération de vidéos. IPV-Bench est basé sur des scénarios détaillés qui incluent des lois physiques, biologiques, géographiques ou sociales. Avec cette technologie, nous construisons un formulaire de questions pour évaluer les modèles de génération de vidéos, ainsi que leur capacité à suivre et à créer. De plus, nous évaluons la compréhension de vidéos impossibles par la sélection d'un ensemble de tests, soulignant la nécessité d'inférences temporelles et de connaissance du monde. Les évaluations détaillées présentent des limitations et des recommandations pour l'avenir des modèles de vidéo, ouvrant ainsi un chemin clair pour le développement des prochaines générations de modèles de vidéo.",
      "upvotes": 37,
      "discussionId": "67da1ee3f1a4a52e8a1e02df",
      "projectPage": "https://showlab.github.io/Impossible-Videos/",
      "githubRepo": "https://github.com/showlab/Impossible-Videos",
      "ai_keywords": [
        "IPV-Bench",
        "taxonomy",
        "prompt suite",
        "video generation models",
        "prompt following",
        "creativity capabilities",
        "video benchmark",
        "Video-LLMs",
        "temporal dynamics",
        "world knowledge"
      ]
    },
    "publishedAt": "2025-03-18T12:10:24.000Z",
    "title": "Impossible Videos",
    "summary": "Synthetic videos nowadays is widely used to complement data scarcity and\ndiversity of real-world videos. Current synthetic datasets primarily replicate\nreal-world scenarios, leaving impossible, counterfactual and anti-reality video\nconcepts underexplored. This work aims to answer two questions: 1) Can today's\nvideo generation models effectively follow prompts to create impossible video\ncontent? 2) Are today's video understanding models good enough for\nunderstanding impossible videos? To this end, we introduce IPV-Bench, a novel\nbenchmark designed to evaluate and foster progress in video understanding and\ngeneration. IPV-Bench is underpinned by a comprehensive taxonomy, encompassing\n4 domains, 14 categories. It features diverse scenes that defy physical,\nbiological, geographical, or social laws. Based on the taxonomy, a prompt suite\nis constructed to evaluate video generation models, challenging their prompt\nfollowing and creativity capabilities. In addition, a video benchmark is\ncurated to assess Video-LLMs on their ability of understanding impossible\nvideos, which particularly requires reasoning on temporal dynamics and world\nknowledge. Comprehensive evaluations reveal limitations and insights for future\ndirections of video models, paving the way for next-generation video models.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64b7833aa5018e3c7c9b50d8/1jxSsiEAyMr5fSng7GOB3.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14378.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b7833aa5018e3c7c9b50d8",
      "avatarUrl": "/avatars/782415605ed786b73f484fcc86a6384f.svg",
      "fullname": "Zechen Bai",
      "name": "ZechenBai",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.14478",
      "authors": [
        {
          "_id": "67da34a648348387ebac36ff",
          "name": "Xinyu Fang",
          "hidden": false
        },
        {
          "_id": "67da34a648348387ebac3700",
          "name": "Zhijian Chen",
          "hidden": false
        },
        {
          "_id": "67da34a648348387ebac3701",
          "name": "Kai Lan",
          "hidden": false
        },
        {
          "_id": "67da34a648348387ebac3702",
          "name": "Shengyuan Ding",
          "hidden": false
        },
        {
          "_id": "67da34a648348387ebac3703",
          "name": "Yingji Liang",
          "hidden": false
        },
        {
          "_id": "67da34a648348387ebac3704",
          "name": "Xiangyu Zhao",
          "hidden": false
        },
        {
          "_id": "67da34a648348387ebac3705",
          "name": "Farong Wen",
          "hidden": false
        },
        {
          "_id": "67da34a648348387ebac3706",
          "name": "Zicheng Zhang",
          "hidden": false
        },
        {
          "_id": "67da34a648348387ebac3707",
          "name": "Guofeng Zhang",
          "hidden": false
        },
        {
          "_id": "67da34a648348387ebac3708",
          "name": "Haodong Duan",
          "hidden": false
        },
        {
          "_id": "67da34a648348387ebac3709",
          "name": "Kai Chen",
          "hidden": false
        },
        {
          "_id": "67da34a648348387ebac370a",
          "name": "Dahua Lin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-18T17:51:34.000Z",
      "submittedOnDailyAt": "2025-03-19T01:40:40.617Z",
      "title": "Création-MMBench : Évaluation du Savoir Créatif de Compréhension du Contexte dans les MLLM",
      "submittedOnDailyBy": {
        "_id": "64f5f8dd9b17cd59c453c57f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f5f8dd9b17cd59c453c57f/MulhwLcePFUWUQel8LQZ8.jpeg",
        "isPro": false,
        "fullname": "Xinyu Fang",
        "user": "nebulae09",
        "type": "user"
      },
      "summary": "La créativité est un aspect fondamental de l'intelligence et comprend la capacité de générer des solutions nouvelles et adaptées dans différents contextes. Dans ce domaine, l'évaluation de la créativité dans les modèles de langage multimodal (MLLMs) est encore largement explorée. Pour corriger cela, nous présentons Creation-MMBench, un cadre d'évaluation qui se concentre sur l'évaluation de la créativité des MLLMs dans des tâches basées sur des images de la réalité. Ce cadre contient 51 tâches détaillées et 765 cas de test. Pour garantir une évaluation stricte, nous avons défini des critères d'évaluation par instance et nous avons fourni des guides pour évaluer à la fois la qualité générale des réponses et leur cohérence factuelle avec l'entrée visuelle. Les résultats des expérimentations montrent que les modèles de langage multimodal ouverts actuels sont significativement affectés dans des tâches créatives par rapport aux modèles sous droit d'auteur. De plus, notre analyse démontre que l'entraînement visuel a un impact négatif sur la capacité créative du modèle de base. Creation-MMBench offre des insights précieux sur le développement de la créativité dans les MLLMs et fait partie de la base pour le futur développement de l'intelligence générative multimodal. Tous les données et codes d'évaluation sont disponibles sur https://github.com/open-compass/Creation-MMBench.",
      "upvotes": 35,
      "discussionId": "67da34ad48348387ebac3926",
      "projectPage": "https://open-compass.github.io/Creation-MMBench/",
      "githubRepo": "https://github.com/open-compass/Creation-MMBench",
      "ai_keywords": [
        "Multimodal Large Language Models (MLLMs)",
        "Creation-MMBench",
        "image-based tasks",
        "instance-specific evaluation criteria",
        "visual fine-tuning",
        "multimodal generative intelligence"
      ]
    },
    "publishedAt": "2025-03-18T13:51:34.000Z",
    "title": "Creation-MMBench: Assessing Context-Aware Creative Intelligence in MLLM",
    "summary": "Creativity is a fundamental aspect of intelligence, involving the ability to\ngenerate novel and appropriate solutions across diverse contexts. While Large\nLanguage Models (LLMs) have been extensively evaluated for their creative\ncapabilities, the assessment of Multimodal Large Language Models (MLLMs) in\nthis domain remains largely unexplored. To address this gap, we introduce\nCreation-MMBench, a multimodal benchmark specifically designed to evaluate the\ncreative capabilities of MLLMs in real-world, image-based tasks. The benchmark\ncomprises 765 test cases spanning 51 fine-grained tasks. To ensure rigorous\nevaluation, we define instance-specific evaluation criteria for each test case,\nguiding the assessment of both general response quality and factual consistency\nwith visual inputs. Experimental results reveal that current open-source MLLMs\nsignificantly underperform compared to proprietary models in creative tasks.\nFurthermore, our analysis demonstrates that visual fine-tuning can negatively\nimpact the base LLM's creative abilities. Creation-MMBench provides valuable\ninsights for advancing MLLM creativity and establishes a foundation for future\nimprovements in multimodal generative intelligence. Full data and evaluation\ncode is released on https://github.com/open-compass/Creation-MMBench.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14478.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64f5f8dd9b17cd59c453c57f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f5f8dd9b17cd59c453c57f/MulhwLcePFUWUQel8LQZ8.jpeg",
      "fullname": "Xinyu Fang",
      "name": "nebulae09",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.14476",
      "authors": [
        {
          "_id": "67da2b54e5335651349e262c",
          "name": "Qiying Yu",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e262d",
          "name": "Zheng Zhang",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e262e",
          "name": "Ruofei Zhu",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e262f",
          "name": "Yufeng Yuan",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e2630",
          "name": "Xiaochen Zuo",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e2631",
          "name": "Yu Yue",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e2632",
          "name": "Tiantian Fan",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e2633",
          "name": "Gaohong Liu",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e2634",
          "name": "Lingjun Liu",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e2635",
          "name": "Xin Liu",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e2636",
          "name": "Haibin Lin",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e2637",
          "name": "Zhiqi Lin",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e2638",
          "name": "Bole Ma",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e2639",
          "name": "Guangming Sheng",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e263a",
          "name": "Yuxuan Tong",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e263b",
          "name": "Chi Zhang",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e263c",
          "name": "Mofan Zhang",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e263d",
          "name": "Wang Zhang",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e263e",
          "name": "Hang Zhu",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e263f",
          "name": "Jinhua Zhu",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e2640",
          "name": "Jiaze Chen",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e2641",
          "name": "Jiangjie Chen",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e2642",
          "name": "Chengyi Wang",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e2643",
          "name": "Hongli Yu",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e2644",
          "name": "Weinan Dai",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e2645",
          "name": "Yuxuan Song",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e2646",
          "name": "Xiangpeng Wei",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e2647",
          "name": "Hao Zhou",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e2648",
          "name": "Jingjing Liu",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e2649",
          "name": "Wei-Ying Ma",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e264a",
          "name": "Ya-Qin Zhang",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e264b",
          "name": "Lin Yan",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e264c",
          "name": "Mu Qiao",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e264d",
          "name": "Yonghui Wu",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e264e",
          "name": "Mingxuan Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-18T17:49:06.000Z",
      "submittedOnDailyAt": "2025-03-19T00:56:39.773Z",
      "title": "DAPO : Système d'apprentissage profond de modèles de langage libre de code à grande échelle",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "La progression du raisonnement est un mécanisme qui pousse les LLMs à acquérir des capacités sans précédent, et est crucial pour l'extraction de causes complexes par apprentissage par renforcement. Cependant, les détails technologiques les plus avancés des LLMs les plus importants sont cachés (par exemple, dans le blog de OpenAI ou1 et dans le rapport technique de DeepSeek R1), et la communauté rencontre des difficultés pour reproduire les résultats d'apprentissage par renforcement. Nous proposons l'algorithme d'Optimisation de Politique de Clip Déconnectée et d'Échantillonnage Dynamique (DAPO), et présentons le système d'apprentissage par renforcement open-source le plus grand jusqu'à présent, utilisant le modèle Qwen2.5-32B, qui a atteint 50 points sur l'AIME 2024. Au contraire de d'autres études, les détails d'apprentissage ne sont pas cachés, et nous montrons comment l'algorithme comporte quatre technologies importantes qui permettent le succès dans l'apprentissage par renforcement de grands LLMs. De plus, nous avons ouvert le code de l'apprentissage construit sur verlframe et les datasets bien sélectionnés et traités. Ces composants du système open-source améliorent la reproductibilité et soutiennent futures recherches dans l'apprentissage par renforcement de grands LLMs.",
      "upvotes": 23,
      "discussionId": "67da2b55e5335651349e26c7",
      "ai_keywords": [
        "inference scaling",
        "LLMs (Large Language Models)",
        "reinforcement learning (RL)",
        "Decoupled Clip and Dynamic Sampling Action Policy Optimization (DAPO)",
        "Qwen2.5-32B",
        "AIME 2024",
        "verl framework"
      ]
    },
    "publishedAt": "2025-03-18T13:49:06.000Z",
    "title": "DAPO: An Open-Source LLM Reinforcement Learning System at Scale",
    "summary": "Inference scaling empowers LLMs with unprecedented reasoning ability, with\nreinforcement learning as the core technique to elicit complex reasoning.\nHowever, key technical details of state-of-the-art reasoning LLMs are concealed\n(such as in OpenAI o1 blog and DeepSeek R1 technical report), thus the\ncommunity still struggles to reproduce their RL training results. We propose\nthe Decoupled Clip and Dynamic sAmpling\nPolicy Optimization (DAPO) algorithm, and\nfully open-source a state-of-the-art large-scale RL system that achieves 50\npoints on AIME 2024 using Qwen2.5-32B base model. Unlike previous works that\nwithhold training details, we introduce four key techniques of our algorithm\nthat make large-scale LLM RL a success. In addition, we open-source our\ntraining code, which is built on the verl framework, along with a carefully\ncurated and processed dataset. These components of our open-source system\nenhance reproducibility and support future research in large-scale LLM RL.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14476.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6398
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.12797",
      "authors": [
        {
          "_id": "67da2c83aa2c34f7d95e46ff",
          "user": {
            "_id": "63c3b67ec7d7f4c63a4eea3a",
            "avatarUrl": "/avatars/4a5f98cb6b0c1e37a2c09af72f7a9946.svg",
            "isPro": false,
            "fullname": "Xinyu Ma",
            "user": "MaxyLee",
            "type": "user"
          },
          "name": "Xinyu Ma",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-19T09:44:31.865Z",
          "hidden": false
        },
        {
          "_id": "67da2c83aa2c34f7d95e4700",
          "user": {
            "_id": "65903c4aa78a277803bde77b",
            "avatarUrl": "/avatars/061388320ccf1a66e1e99519dd426a60.svg",
            "isPro": false,
            "fullname": "Ziyang Ding",
            "user": "sdudzy",
            "type": "user"
          },
          "name": "Ziyang Ding",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-19T09:44:35.757Z",
          "hidden": false
        },
        {
          "_id": "67da2c83aa2c34f7d95e4701",
          "name": "Zhicong Luo",
          "hidden": false
        },
        {
          "_id": "67da2c83aa2c34f7d95e4702",
          "user": {
            "_id": "642086ed290342c5df85662d",
            "avatarUrl": "/avatars/915a4d7b89455ae97b8544c79286ddf8.svg",
            "isPro": false,
            "fullname": "Chi Chen",
            "user": "carboncoo",
            "type": "user"
          },
          "name": "Chi Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-19T09:44:39.356Z",
          "hidden": false
        },
        {
          "_id": "67da2c83aa2c34f7d95e4703",
          "name": "Zonghao Guo",
          "hidden": false
        },
        {
          "_id": "67da2c83aa2c34f7d95e4704",
          "name": "Derek F. Wong",
          "hidden": false
        },
        {
          "_id": "67da2c83aa2c34f7d95e4705",
          "name": "Xiaoyi Feng",
          "hidden": false
        },
        {
          "_id": "67da2c83aa2c34f7d95e4706",
          "name": "Maosong Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-17T04:06:34.000Z",
      "submittedOnDailyAt": "2025-03-19T01:06:07.094Z",
      "title": "DeepPerception: Perception Profonde : Développement de Jeux Visuels de Connaissance par Identification Profonde",
      "submittedOnDailyBy": {
        "_id": "642086ed290342c5df85662d",
        "avatarUrl": "/avatars/915a4d7b89455ae97b8544c79286ddf8.svg",
        "isPro": false,
        "fullname": "Chi Chen",
        "user": "carboncoo",
        "type": "user"
      },
      "summary": "Les experts humains utilisent le savoir du domaine pour organiser détaillément les caractéristiques visuelles, atteignant une définition précise des termes visuels détaillés.",
      "upvotes": 20,
      "discussionId": "67da2c85aa2c34f7d95e4796",
      "projectPage": "https://deepperception-kvg.github.io",
      "githubRepo": "https://github.com/thunlp/DeepPerception",
      "ai_keywords": [
        "knowledge-intensive visual grounding (KVG)",
        "DeepPerception",
        "automated data synthesis pipeline",
        "supervised fine-tuning",
        "reinforcement learning",
        "perception-cognition synergy",
        "KVG-Bench",
        "cognitive reasoning scaffolding",
        "cross-domain generalization",
        "cognitive processes",
        "multimodal reasoning"
      ]
    },
    "publishedAt": "2025-03-17T00:06:34.000Z",
    "title": "DeepPerception: Advancing R1-like Cognitive Visual Perception in MLLMs\n  for Knowledge-Intensive Visual Grounding",
    "summary": "Human experts excel at fine-grained visual discrimination by leveraging\ndomain knowledge to refine perceptual features, a capability that remains\nunderdeveloped in current Multimodal Large Language Models (MLLMs). Despite\npossessing vast expert-level knowledge, MLLMs struggle to integrate reasoning\ninto visual perception, often generating direct responses without deeper\nanalysis. To bridge this gap, we introduce knowledge-intensive visual grounding\n(KVG), a novel visual grounding task that requires both fine-grained perception\nand domain-specific knowledge integration. To address the challenges of KVG, we\npropose DeepPerception, an MLLM enhanced with cognitive visual perception\ncapabilities. Our approach consists of (1) an automated data synthesis pipeline\nthat generates high-quality, knowledge-aligned training samples, and (2) a\ntwo-stage training framework combining supervised fine-tuning for cognitive\nreasoning scaffolding and reinforcement learning to optimize\nperception-cognition synergy. To benchmark performance, we introduce KVG-Bench\na comprehensive dataset spanning 10 domains with 1.3K manually curated test\ncases. Experimental results demonstrate that DeepPerception significantly\noutperforms direct fine-tuning, achieving +8.08\\% accuracy improvements on\nKVG-Bench and exhibiting +4.60\\% superior cross-domain generalization over\nbaseline approaches. Our findings highlight the importance of integrating\ncognitive processes into MLLMs for human-like visual perception and open new\ndirections for multimodal reasoning research. The data, codes, and models are\nreleased at https://github.com/thunlp/DeepPerception.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12797.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642086ed290342c5df85662d",
      "avatarUrl": "/avatars/915a4d7b89455ae97b8544c79286ddf8.svg",
      "fullname": "Chi Chen",
      "name": "carboncoo",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.12329",
      "authors": [
        {
          "_id": "67d8e115f55b855ae6d8f29b",
          "user": {
            "_id": "63340dbbd92c5842ae71d1e9",
            "avatarUrl": "/avatars/3a3182996bd41b526dcbfa8687d91963.svg",
            "isPro": false,
            "fullname": "Kanzhi Cheng",
            "user": "cckevinn",
            "type": "user"
          },
          "name": "Kanzhi Cheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-18T11:34:32.729Z",
          "hidden": false
        },
        {
          "_id": "67d8e115f55b855ae6d8f29c",
          "user": {
            "_id": "653f8cc9d4d0924ad2404d86",
            "avatarUrl": "/avatars/83defb31d669393447ee04fe9989b96b.svg",
            "isPro": false,
            "fullname": "Wenpo Song",
            "user": "songwp",
            "type": "user"
          },
          "name": "Wenpo Song",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-19T09:46:11.039Z",
          "hidden": false
        },
        {
          "_id": "67d8e115f55b855ae6d8f29d",
          "name": "Jiaxin Fan",
          "hidden": false
        },
        {
          "_id": "67d8e115f55b855ae6d8f29e",
          "name": "Zheng Ma",
          "hidden": false
        },
        {
          "_id": "67d8e115f55b855ae6d8f29f",
          "name": "Qiushi Sun",
          "hidden": false
        },
        {
          "_id": "67d8e115f55b855ae6d8f2a0",
          "name": "Fangzhi Xu",
          "hidden": false
        },
        {
          "_id": "67d8e115f55b855ae6d8f2a1",
          "name": "Chenyang Yan",
          "hidden": false
        },
        {
          "_id": "67d8e115f55b855ae6d8f2a2",
          "name": "Nuo Chen",
          "hidden": false
        },
        {
          "_id": "67d8e115f55b855ae6d8f2a3",
          "name": "Jianbing Zhang",
          "hidden": false
        },
        {
          "_id": "67d8e115f55b855ae6d8f2a4",
          "name": "Jiajun Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-16T02:56:09.000Z",
      "submittedOnDailyAt": "2025-03-19T02:04:01.010Z",
      "title": "CapArena : Normes et analyse des détails d'images dans l'ère des LLM",
      "submittedOnDailyBy": {
        "_id": "63340dbbd92c5842ae71d1e9",
        "avatarUrl": "/avatars/3a3182996bd41b526dcbfa8687d91963.svg",
        "isPro": false,
        "fullname": "Kanzhi Cheng",
        "user": "cckevinn",
        "type": "user"
      },
      "summary": "La caption d'images est l'un des défis les plus longs de la recherche en langage visuel, et grâce au développement des modèles de langage machine (LLMs), les modèles de langage visuel (VLMs) modernes peuvent générer des explications détaillées et complètes d'images. Cependant, les méthodes pour évaluer la qualité de ces captions n'ont pas encore été résolues. Cet article aborde deux questions importantes : 1) Quel rendement montrent les VLMs actuels dans la caption d'images, en particulier lorsqu'ils sont comparés aux personnes ? Nous avons construit une plateforme appelée CapArena pour fournir plus de 6000 résultats de captions d'images dans un format conversationnel et de préférences de personnes de haute qualité. Ce modèle d'évaluation de CapArena montre que les modèles leaders, comme GPT-4o, atteignent ou dépassent le rendement humain, révélant que de nombreux modèles open-source restent derrière. 2) Peuvent-elles des métriques automatiques évaluer de manière fiable la qualité de captions détaillées ? Dans CapArena, nous utilisons des analyses de personnes pour évaluer des métriques traditionnelles et récentes de caption, ainsi que VLM-as-a-Judge. L'analyse montre que certaines métriques (par exemple, METEOR) montrent un bon accord avec les préférences humaines et les captions, mais révèlent également un système de biais qui incertaine la classification des modèles. En contraste, VLM-as-a-Judge montre une forte capacité d'identification à la fois au niveau de la caption et du modèle. Sur la base de ces observations, nous avons lancé CapArena-Auto, un benchmark automatique précis et efficace, qui atteint un 94,3% de corrélation avec les classifications humaines et montre que les coûts des tests sont de 4 dollars. Les données et les ressources sont disponibles sur https://caparena.github.io.",
      "upvotes": 18,
      "discussionId": "67d8e118f55b855ae6d8f34e",
      "projectPage": "https://caparena.github.io/",
      "githubRepo": "https://github.com/njucckevin/CapArena",
      "ai_keywords": [
        "Vision-Language Models (VLMs)",
        "GPT-4o",
        "CapArena",
        "pairwise caption battles",
        "high-quality human preference votes",
        "VLM-as-a-Judge",
        "METEOR",
        "CapArena-Auto"
      ]
    },
    "publishedAt": "2025-03-15T22:56:09.000Z",
    "title": "CapArena: Benchmarking and Analyzing Detailed Image Captioning in the\n  LLM Era",
    "summary": "Image captioning has been a longstanding challenge in vision-language\nresearch. With the rise of LLMs, modern Vision-Language Models (VLMs) generate\ndetailed and comprehensive image descriptions. However, benchmarking the\nquality of such captions remains unresolved. This paper addresses two key\nquestions: (1) How well do current VLMs actually perform on image captioning,\nparticularly compared to humans? We built CapArena, a platform with over 6000\npairwise caption battles and high-quality human preference votes. Our\narena-style evaluation marks a milestone, showing that leading models like\nGPT-4o achieve or even surpass human performance, while most open-source models\nlag behind. (2) Can automated metrics reliably assess detailed caption quality?\nUsing human annotations from CapArena, we evaluate traditional and recent\ncaptioning metrics, as well as VLM-as-a-Judge. Our analysis reveals that while\nsome metrics (e.g., METEOR) show decent caption-level agreement with humans,\ntheir systematic biases lead to inconsistencies in model ranking. In contrast,\nVLM-as-a-Judge demonstrates robust discernment at both the caption and model\nlevels. Building on these insights, we release CapArena-Auto, an accurate and\nefficient automated benchmark for detailed captioning, achieving 94.3%\ncorrelation with human rankings at just $4 per test. Data and resources will be\nopen-sourced at https://caparena.github.io.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12329.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63340dbbd92c5842ae71d1e9",
      "avatarUrl": "/avatars/3a3182996bd41b526dcbfa8687d91963.svg",
      "fullname": "Kanzhi Cheng",
      "name": "cckevinn",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.13424",
      "authors": [
        {
          "_id": "67da22b75fe852c86d3c419b",
          "name": "Xinyu Lian",
          "hidden": false
        },
        {
          "_id": "67da22b75fe852c86d3c419c",
          "name": "Zichao Yu",
          "hidden": false
        },
        {
          "_id": "67da22b75fe852c86d3c419d",
          "name": "Ruiming Liang",
          "hidden": false
        },
        {
          "_id": "67da22b75fe852c86d3c419e",
          "name": "Yitong Wang",
          "hidden": false
        },
        {
          "_id": "67da22b75fe852c86d3c419f",
          "name": "Li Ray Luo",
          "hidden": false
        },
        {
          "_id": "67da22b75fe852c86d3c41a0",
          "name": "Kaixu Chen",
          "hidden": false
        },
        {
          "_id": "67da22b75fe852c86d3c41a1",
          "name": "Yuanzhen Zhou",
          "hidden": false
        },
        {
          "_id": "67da22b75fe852c86d3c41a2",
          "name": "Qihong Tang",
          "hidden": false
        },
        {
          "_id": "67da22b75fe852c86d3c41a3",
          "name": "Xudong Xu",
          "hidden": false
        },
        {
          "_id": "67da22b75fe852c86d3c41a4",
          "name": "Zhaoyang Lyu",
          "hidden": false
        },
        {
          "_id": "67da22b75fe852c86d3c41a5",
          "name": "Bo Dai",
          "hidden": false
        },
        {
          "_id": "67da22b75fe852c86d3c41a6",
          "name": "Jiangmiao Pang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63f2ec797ddf724fbcc75aee/7Is4N1AFDor-EuzkZEiui.mp4"
      ],
      "publishedAt": "2025-03-17T17:53:56.000Z",
      "submittedOnDailyAt": "2025-03-19T00:24:14.520Z",
      "title": "Infinite Mobility : Synthèse Scalable de Qualité Haute de Objets Connectés par Génération Structurée",
      "submittedOnDailyBy": {
        "_id": "63f2ec797ddf724fbcc75aee",
        "avatarUrl": "/avatars/e93432ad11da703d46fe5e594d69f8c0.svg",
        "isPro": false,
        "fullname": "Zhaoyang Lyu",
        "user": "ZhaoyangLyu",
        "type": "user"
      },
      "summary": "Les grands travaux d'art de qualité élevée sont en urgence nécessitant plusieurs tâches concrètes liées à différents types d'IA. Actuellement, les méthodes de fabrication d'objets d'art sont presque entièrement orientées par les données ou basées sur la simulation, et ces méthodes sont limitées par le taille, la qualité des données d'entraînement, la précision et la complexité de la simulation. Dans cet article, nous proposons un nouveau méthode appelé \"Infinite Mobility\" pour la synthèse d'objets de qualité élevée par génération de processus. En nous basant sur des études d'utilisateurs et des évaluations quantitatives, nous démontrons que notre méthode dépasse les plus avancées actuellement, et obtient des résultats similaires à ceux des ensembles de données annotés avec des caractéristiques physiques et de qualité de maillage. De plus, nos données de synthèse peuvent être utilisées comme données d'entraînement pour des modèles génératifs, facilitant l'échelle à la prochaine étape. Le code est disponible sur https://github.com/Intern-Nexus/Infinite-Mobility.",
      "upvotes": 13,
      "discussionId": "67da22bb5fe852c86d3c4304",
      "projectPage": "https://infinite-mobility.github.io/",
      "githubRepo": "https://github.com/Intern-Nexus/Infinite-Mobility",
      "ai_keywords": [
        "articulated objects",
        "high-fidelity",
        "embodied AI",
        "data-driven",
        "simulation-based",
        "procedural generation",
        "physics property",
        "mesh quality",
        "generative models"
      ]
    },
    "publishedAt": "2025-03-17T13:53:56.000Z",
    "title": "Infinite Mobility: Scalable High-Fidelity Synthesis of Articulated\n  Objects via Procedural Generation",
    "summary": "Large-scale articulated objects with high quality are desperately needed for\nmultiple tasks related to embodied AI. Most existing methods for creating\narticulated objects are either data-driven or simulation based, which are\nlimited by the scale and quality of the training data or the fidelity and heavy\nlabour of the simulation. In this paper, we propose Infinite Mobility, a novel\nmethod for synthesizing high-fidelity articulated objects through procedural\ngeneration. User study and quantitative evaluation demonstrate that our method\ncan produce results that excel current state-of-the-art methods and are\ncomparable to human-annotated datasets in both physics property and mesh\nquality. Furthermore, we show that our synthetic data can be used as training\ndata for generative models, enabling next-step scaling up. Code is available at\nhttps://github.com/Intern-Nexus/Infinite-Mobility",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63f2ec797ddf724fbcc75aee/7Is4N1AFDor-EuzkZEiui.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13424.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63f2ec797ddf724fbcc75aee",
      "avatarUrl": "/avatars/e93432ad11da703d46fe5e594d69f8c0.svg",
      "fullname": "Zhaoyang Lyu",
      "name": "ZhaoyangLyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.14125",
      "authors": [
        {
          "_id": "67da200db41738a058666623",
          "name": "Defa Zhu",
          "hidden": false
        },
        {
          "_id": "67da200db41738a058666624",
          "name": "Hongzhi Huang",
          "hidden": false
        },
        {
          "_id": "67da200db41738a058666625",
          "name": "Jundong Zhou",
          "hidden": false
        },
        {
          "_id": "67da200db41738a058666626",
          "user": {
            "_id": "65a62085576772f531e13856",
            "avatarUrl": "/avatars/72c67a60422e333ea4e323f7480ae0b7.svg",
            "isPro": false,
            "fullname": "Huang Zihao",
            "user": "FetchFortune",
            "type": "user"
          },
          "name": "Zihao Huang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-19T09:45:07.454Z",
          "hidden": false
        },
        {
          "_id": "67da200db41738a058666627",
          "user": {
            "_id": "6371128eafbe42caa5a5222b",
            "avatarUrl": "/avatars/c3b2ab35949c38aa3dfb2657a1300aac.svg",
            "isPro": false,
            "fullname": "Yutao Zeng",
            "user": "Taoer",
            "type": "user"
          },
          "name": "Yutao Zeng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-19T09:45:05.382Z",
          "hidden": false
        },
        {
          "_id": "67da200db41738a058666628",
          "name": "Banggu Wu",
          "hidden": false
        },
        {
          "_id": "67da200db41738a058666629",
          "name": "Qiyang Min",
          "hidden": false
        },
        {
          "_id": "67da200db41738a05866662a",
          "name": "Xun Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-18T10:37:50.000Z",
      "submittedOnDailyAt": "2025-03-19T00:09:57.233Z",
      "title": "Frac-Connections : Connexions Fractionnelles de l'Expansion Fractionnelle",
      "submittedOnDailyBy": {
        "_id": "667505f4361b960c79e35486",
        "avatarUrl": "/avatars/d352639c520075220f6abaae23c39376.svg",
        "isPro": false,
        "fullname": "Defa Zhu",
        "user": "mathfinder",
        "type": "user"
      },
      "summary": "Les résidus de connexions sont l'un des éléments clés dans l'architecture moderne de l'apprentissage profond, permettant l'entraînement de réseaux neuronaux très profonds en atténuant le phénomène de disparition du gradient. Les hyperconnexions, récemment proposées, étendent les résidus de connexions en introduisant plusieurs intensités de connexion à différentes profondeurs et offrent des solutions pour l'effet de disparition du gradient et la destruction de représentations, offrant ainsi une nouvelle approche. Cependant, les hyperconnexions affectent le coût d'accès à la mémoire en étendant l'amplitude des états cachés. Dans cet article, nous proposons une nouvelle approche appelée Frac-Connections, qui maintient certains avantages des hyperconnexions en divisant les états cachés en plusieurs parties, réduisant ainsi le consommateur de mémoire. Cela est démontré effectivement par des expériences à grande échelle sur des tâches de langue, montrant que Frac-Connections dépasse considérablement les résidus de connexions, démontrant son entraînement d'un modèle MoE jusqu'à 7B avec 3T tokens.",
      "upvotes": 11,
      "discussionId": "67da200eb41738a058666690",
      "ai_keywords": [
        "residual connections",
        "gradient vanishing",
        "Hyper-Connections",
        "multiple connection strengths",
        "seesaw effect",
        "representation collapse",
        "Frac-Connections",
        "hidden states",
        "language tasks",
        "MoE model"
      ]
    },
    "publishedAt": "2025-03-18T06:37:50.000Z",
    "title": "Frac-Connections: Fractional Extension of Hyper-Connections",
    "summary": "Residual connections are central to modern deep learning architectures,\nenabling the training of very deep networks by mitigating gradient vanishing.\nHyper-Connections recently generalized residual connections by introducing\nmultiple connection strengths at different depths, thereby addressing the\nseesaw effect between gradient vanishing and representation collapse. However,\nHyper-Connections increase memory access costs by expanding the width of hidden\nstates. In this paper, we propose Frac-Connections, a novel approach that\ndivides hidden states into multiple parts rather than expanding their width.\nFrac-Connections retain partial benefits of Hyper-Connections while reducing\nmemory consumption. To validate their effectiveness, we conduct large-scale\nexperiments on language tasks, with the largest being a 7B MoE model trained on\nup to 3T tokens, demonstrating that Frac-Connections significantly outperform\nresidual connections.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14125.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "667505f4361b960c79e35486",
      "avatarUrl": "/avatars/d352639c520075220f6abaae23c39376.svg",
      "fullname": "Defa Zhu",
      "name": "mathfinder",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.14504",
      "authors": [
        {
          "_id": "67da436711b6db6920802e9e",
          "name": "Tao Yu",
          "hidden": false
        },
        {
          "_id": "67da436711b6db6920802e9f",
          "user": {
            "_id": "623d8ca4c29adf5ef6175615",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623d8ca4c29adf5ef6175615/q7lHao7UPwU1u7YLSP56m.jpeg",
            "isPro": false,
            "fullname": "Yi-Fan Zhang",
            "user": "yifanzhang114",
            "type": "user"
          },
          "name": "Yi-Fan Zhang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-19T04:09:15.055Z",
          "hidden": false
        },
        {
          "_id": "67da436711b6db6920802ea0",
          "name": "Chaoyou Fu",
          "hidden": false
        },
        {
          "_id": "67da436711b6db6920802ea1",
          "name": "Junkang Wu",
          "hidden": false
        },
        {
          "_id": "67da436711b6db6920802ea2",
          "name": "Jinda Lu",
          "hidden": false
        },
        {
          "_id": "67da436711b6db6920802ea3",
          "name": "Kun Wang",
          "hidden": false
        },
        {
          "_id": "67da436711b6db6920802ea4",
          "name": "Xingyu Lu",
          "hidden": false
        },
        {
          "_id": "67da436711b6db6920802ea5",
          "name": "Yunhang Shen",
          "hidden": false
        },
        {
          "_id": "67da436711b6db6920802ea6",
          "name": "Guibin Zhang",
          "hidden": false
        },
        {
          "_id": "67da436711b6db6920802ea7",
          "name": "Dingjie Song",
          "hidden": false
        },
        {
          "_id": "67da436711b6db6920802ea8",
          "name": "Yibo Yan",
          "hidden": false
        },
        {
          "_id": "67da436711b6db6920802ea9",
          "name": "Tianlong Xu",
          "hidden": false
        },
        {
          "_id": "67da436711b6db6920802eaa",
          "name": "Qingsong Wen",
          "hidden": false
        },
        {
          "_id": "67da436711b6db6920802eab",
          "name": "Zhang Zhang",
          "hidden": false
        },
        {
          "_id": "67da436711b6db6920802eac",
          "name": "Yan Huang",
          "hidden": false
        },
        {
          "_id": "67da436711b6db6920802ead",
          "name": "Liang Wang",
          "hidden": false
        },
        {
          "_id": "67da436711b6db6920802eae",
          "name": "Tieniu Tan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-18T17:59:56.000Z",
      "submittedOnDailyAt": "2025-03-19T02:39:30.454Z",
      "title": "Damodal LLM préfère l'accord avec la préférence humaine : recherche",
      "submittedOnDailyBy": {
        "_id": "623d8ca4c29adf5ef6175615",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623d8ca4c29adf5ef6175615/q7lHao7UPwU1u7YLSP56m.jpeg",
        "isPro": false,
        "fullname": "Yi-Fan Zhang",
        "user": "yifanzhang114",
        "type": "user"
      },
      "summary": "Les modèles de langage grands (LLMs) peuvent traiter diverses tâches générales avec des schémas de programmation simples et ne nécessitent pas d'entraînement spécifique pour celles-ci. Les modèles de langage grands multimodal (MLLMs) sont construits sur les bases des LLMs et montrent une potentiel impressionnant pour aborder des tâches complexes dans les données visuelles, auditives et textuelles. Cependant, des problèmes importants en termes de précision, sécurité et inférence comme celle de O1 et l'attention vers les préférences humaines ont été identifiés, mais pas suffisamment résolus. Ce vide a impulsé l'apparition de différents algorithmes d'attention. Des études récentes ont montré que ces algorithmes d'attention peuvent aborder efficacement les problèmes mentionnés précédemment. Dans cet article, nous proposons une revue systématique sur les algorithmes d'attention dans les MLLMs. En particulier, nous étudierons les quatre domaines principaux suivants : 1) les cas de tâches couverts par les algorithmes d'attention, comme la compréhension générale d'images, plusieurs images, vidéos, voix et applications multimodales élargies ; 2) les facteurs clés dans la construction de jeux de données d'attention, sources de données, réponses du modèle et notes sur les préférences ; 3) les benchmarks utilisés pour évaluer les algorithmes d'attention ; et 4) la discussion sur les directions futures possibles dans le développement de ces algorithmes. Le site web du projet de l'article est disponible à https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Alignment.",
      "upvotes": 6,
      "discussionId": "67da436b11b6db6920803040",
      "githubRepo": "https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Alignment",
      "ai_keywords": [
        "Large language models (LLMs)",
        "Multimodal Large Language Models (MLLMs)",
        "Truthfulness",
        "Safety",
        "o1-like reasoning",
        "Alignment with human preference",
        "Alignment algorithms",
        "General image understanding",
        "Multi-image",
        "Video",
        "Audio",
        "Extended multimodal applications",
        "Alignment datasets",
        "Data sources",
        "Model responses",
        "Preference annotations",
        "Benchmarks"
      ]
    },
    "publishedAt": "2025-03-18T13:59:56.000Z",
    "title": "Aligning Multimodal LLM with Human Preference: A Survey",
    "summary": "Large language models (LLMs) can handle a wide variety of general tasks with\nsimple prompts, without the need for task-specific training. Multimodal Large\nLanguage Models (MLLMs), built upon LLMs, have demonstrated impressive\npotential in tackling complex tasks involving visual, auditory, and textual\ndata. However, critical issues related to truthfulness, safety, o1-like\nreasoning, and alignment with human preference remain insufficiently addressed.\nThis gap has spurred the emergence of various alignment algorithms, each\ntargeting different application scenarios and optimization goals. Recent\nstudies have shown that alignment algorithms are a powerful approach to\nresolving the aforementioned challenges. In this paper, we aim to provide a\ncomprehensive and systematic review of alignment algorithms for MLLMs.\nSpecifically, we explore four key aspects: (1) the application scenarios\ncovered by alignment algorithms, including general image understanding,\nmulti-image, video, and audio, and extended multimodal applications; (2) the\ncore factors in constructing alignment datasets, including data sources, model\nresponses, and preference annotations; (3) the benchmarks used to evaluate\nalignment algorithms; and (4) a discussion of potential future directions for\nthe development of alignment algorithms. This work seeks to help researchers\norganize current advancements in the field and inspire better alignment\nmethods. The project page of this paper is available at\nhttps://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Alignment.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14504.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "623d8ca4c29adf5ef6175615",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623d8ca4c29adf5ef6175615/q7lHao7UPwU1u7YLSP56m.jpeg",
      "fullname": "Yi-Fan Zhang",
      "name": "yifanzhang114",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.12505",
      "authors": [
        {
          "_id": "67d9442ec37d05ff0ab28e44",
          "name": "Zhaopan Xu",
          "hidden": false
        },
        {
          "_id": "67d9442ec37d05ff0ab28e45",
          "name": "Pengfei Zhou",
          "hidden": false
        },
        {
          "_id": "67d9442ec37d05ff0ab28e46",
          "name": "Jiaxin Ai",
          "hidden": false
        },
        {
          "_id": "67d9442ec37d05ff0ab28e47",
          "name": "Wangbo Zhao",
          "hidden": false
        },
        {
          "_id": "67d9442ec37d05ff0ab28e48",
          "name": "Kai Wang",
          "hidden": false
        },
        {
          "_id": "67d9442ec37d05ff0ab28e49",
          "name": "Xiaojiang Peng",
          "hidden": false
        },
        {
          "_id": "67d9442ec37d05ff0ab28e4a",
          "name": "Wenqi Shao",
          "hidden": false
        },
        {
          "_id": "67d9442ec37d05ff0ab28e4b",
          "name": "Hongxun Yao",
          "hidden": false
        },
        {
          "_id": "67d9442ec37d05ff0ab28e4c",
          "name": "Kaipeng Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-16T13:50:38.000Z",
      "submittedOnDailyAt": "2025-03-19T00:16:20.299Z",
      "title": "MPBench : Marqueur de logique détaillé pour la détection d'erreurs dans les processus",
      "submittedOnDailyBy": {
        "_id": "65f1713552c38a91e0a445e8",
        "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
        "isPro": false,
        "fullname": "kaipeng",
        "user": "kpzhang996",
        "type": "user"
      },
      "summary": "Ceci est l'un des capacités essentielles pour le traitement de tâches complexes dans les modèles de langage grands (LLMs), et il est crucial d'identifier les erreurs dans le processus de reconnaissance. Récemment, un modèle de récompenses par étapes (PRMs) a été proposé pour fournir des récompenses par étapes, encourager l'apprentissage par renforcement et la génération de données, et guider les LLMs à suivre des étapes d'inférence correctes pendant l'entraînement, ce qui améliore la précision de l'inférence. Cependant, les tests actuels de PRMs se basent sur le texte et se concentrent sur la détection d'erreurs, ignorant d'autres scénarios comme la recherche d'inférence. Pour résoudre ces différences, nous introduisons MPBench, qui est un référentiel intégral et multi-tâche, multi-modèle pour évaluer de manière systématique l'efficacité des PRMs dans différents scénarios. MPBench applique l'une des trois méthodes d'évaluation lors du processus d'inférence : 1) précision par étapes, évalue la précision de chaque étape d'inférence intermédiaire ; 2) ensemble de réponses, sélectionne la meilleure solution parmi plusieurs ; 3) recherche du processus d'inférence, guide la recherche des étapes d'inférence pendant la période d'inférence. De cette manière, MPBench effectue des évaluations intégrales et fournit des insights sur le développement des PRMs multi-modèles.",
      "upvotes": 6,
      "discussionId": "67d94430c37d05ff0ab28eb3",
      "projectPage": "https://mpbench.github.io/",
      "ai_keywords": [
        "process-level reward models (PRMs)",
        "reinforcement learning",
        "step-wise rewards",
        "error detection",
        "reasoning search",
        "MPBench",
        "multi-task",
        "multimodal benchmark",
        "Step Correctness",
        "Answer Aggregation",
        "Reasoning Process Search"
      ]
    },
    "publishedAt": "2025-03-16T09:50:38.000Z",
    "title": "MPBench: A Comprehensive Multimodal Reasoning Benchmark for Process\n  Errors Identification",
    "summary": "Reasoning is an essential capacity for large language models (LLMs) to\naddress complex tasks, where the identification of process errors is vital for\nimproving this ability. Recently, process-level reward models (PRMs) were\nproposed to provide step-wise rewards that facilitate reinforcement learning\nand data production during training and guide LLMs toward correct steps during\ninference, thereby improving reasoning accuracy. However, existing benchmarks\nof PRMs are text-based and focus on error detection, neglecting other scenarios\nlike reasoning search. To address this gap, we introduce MPBench, a\ncomprehensive, multi-task, multimodal benchmark designed to systematically\nassess the effectiveness of PRMs in diverse scenarios. MPBench employs three\nevaluation paradigms, each targeting a specific role of PRMs in the reasoning\nprocess: (1) Step Correctness, which assesses the correctness of each\nintermediate reasoning step; (2) Answer Aggregation, which aggregates multiple\nsolutions and selects the best one; and (3) Reasoning Process Search, which\nguides the search for optimal reasoning steps during inference. Through these\nparadigms, MPBench makes comprehensive evaluations and provides insights into\nthe development of multimodal PRMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12505.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65f1713552c38a91e0a445e8",
      "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
      "fullname": "kaipeng",
      "name": "kpzhang996",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.14492",
      "authors": [
        {
          "_id": "67da2cbde5335651349e98c8",
          "name": "NVIDIA",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98ca",
          "name": "Hassan Abu Alhaija",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98cb",
          "name": "Jose Alvarez",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98cc",
          "name": "Maciej Bala",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98cd",
          "name": "Tiffany Cai",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98ce",
          "name": "Tianshi Cao",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98cf",
          "name": "Liz Cha",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98d0",
          "name": "Joshua Chen",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98d1",
          "name": "Mike Chen",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98d2",
          "name": "Francesco Ferroni",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98d3",
          "name": "Sanja Fidler",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98d4",
          "name": "Dieter Fox",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98d5",
          "name": "Yunhao Ge",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98d6",
          "name": "Jinwei Gu",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98d7",
          "name": "Ali Hassani",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98d8",
          "name": "Michael Isaev",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98d9",
          "name": "Pooya Jannaty",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98da",
          "name": "Shiyi Lan",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98db",
          "name": "Tobias Lasser",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98dc",
          "name": "Huan Ling",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98dd",
          "name": "Ming-Yu Liu",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98de",
          "name": "Xian Liu",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98df",
          "name": "Yifan Lu",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98e0",
          "name": "Alice Luo",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98e1",
          "name": "Qianli Ma",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98e2",
          "name": "Hanzi Mao",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98e3",
          "name": "Fabio Ramos",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98e4",
          "name": "Xuanchi Ren",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98e5",
          "name": "Tianchang Shen",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98e6",
          "name": "Shitao Tang",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98e7",
          "name": "Ting-Chun Wang",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98e8",
          "name": "Jay Wu",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98e9",
          "name": "Jiashu Xu",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98ea",
          "name": "Stella Xu",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98eb",
          "name": "Kevin Xie",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98ec",
          "name": "Yuchong Ye",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98ed",
          "name": "Xiaodong Yang",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98ee",
          "name": "Xiaohui Zeng",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98ef",
          "name": "Yu Zeng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-18T17:57:54.000Z",
      "submittedOnDailyAt": "2025-03-19T01:03:48.943Z",
      "title": "Cosmos Transformer 1 : Transformateur Adaptatif Monomodal pour la Génération de Mondes Conditionnés",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "CosmoS Transfère, modèle de génération de mondes avec conditions, est présenté. Ce modèle peut générer des simulations de mondes basées sur plusieurs entrées de contrôle spatial diversifiées, comme la division, la profondeur et les bords. D'un point de vue de conception, les schémas de conditionnement spatial adaptatif et personnalisable permettent d'attribuer des poids différents aux entrées de conditionnement dans différentes positions spatiales. Cela permet un haut niveau de contrôle dans la génération de mondes, ce qui peut être utilisé dans des séquences de rétroalimentation entre mondes comme Sim2Real. On évalue en détail les fonctions du modèle et on montre son application dans l'intelligence physique. En particulier, il est efficace dans la Sim2Real de la robotique et dans l'abondance de données automobiles. De plus, on montre l'échelle de l'inférence pour la génération en temps réel en utilisant le rack NVIDIA GB200 NVL72. On promeut l'accélération du développement de la recherche en publiant le modèle et le code. Le site de publication est : https://github.com/nvidia-cosmos/cosmos-transfer1.",
      "upvotes": 5,
      "discussionId": "67da2cc1e5335651349e9a3e",
      "ai_keywords": [
        "conditional world generation model",
        "world simulations",
        "spatial control inputs",
        "segmentation",
        "depth",
        "edge",
        "spatial conditional scheme",
        "adaptive",
        "customizable",
        "high controllable world generation",
        "world-to-world transfer",
        "Sim2Real",
        "Physical AI",
        "robotics Sim2Real",
        "autonomous vehicle data enrichment",
        "inference scaling strategy",
        "real-time world generation",
        "NVIDIA GB200 NVL72 rack"
      ]
    },
    "publishedAt": "2025-03-18T13:57:54.000Z",
    "title": "Cosmos-Transfer1: Conditional World Generation with Adaptive Multimodal\n  Control",
    "summary": "We introduce Cosmos-Transfer, a conditional world generation model that can\ngenerate world simulations based on multiple spatial control inputs of various\nmodalities such as segmentation, depth, and edge. In the design, the spatial\nconditional scheme is adaptive and customizable. It allows weighting different\nconditional inputs differently at different spatial locations. This enables\nhighly controllable world generation and finds use in various world-to-world\ntransfer use cases, including Sim2Real. We conduct extensive evaluations to\nanalyze the proposed model and demonstrate its applications for Physical AI,\nincluding robotics Sim2Real and autonomous vehicle data enrichment. We further\ndemonstrate an inference scaling strategy to achieve real-time world generation\nwith an NVIDIA GB200 NVL72 rack. To help accelerate research development in the\nfield, we open-source our models and code at\nhttps://github.com/nvidia-cosmos/cosmos-transfer1.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14492.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6398
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.14499",
      "authors": [
        {
          "_id": "67da2e831bba0f73374fd5a0",
          "name": "Thomas Kwa",
          "hidden": false
        },
        {
          "_id": "67da2e831bba0f73374fd5a1",
          "name": "Ben West",
          "hidden": false
        },
        {
          "_id": "67da2e831bba0f73374fd5a2",
          "name": "Joel Becker",
          "hidden": false
        },
        {
          "_id": "67da2e831bba0f73374fd5a3",
          "name": "Amy Deng",
          "hidden": false
        },
        {
          "_id": "67da2e831bba0f73374fd5a4",
          "name": "Katharyn Garcia",
          "hidden": false
        },
        {
          "_id": "67da2e831bba0f73374fd5a5",
          "name": "Max Hasin",
          "hidden": false
        },
        {
          "_id": "67da2e831bba0f73374fd5a6",
          "name": "Sami Jawhar",
          "hidden": false
        },
        {
          "_id": "67da2e831bba0f73374fd5a7",
          "name": "Megan Kinniment",
          "hidden": false
        },
        {
          "_id": "67da2e831bba0f73374fd5a8",
          "name": "Nate Rush",
          "hidden": false
        },
        {
          "_id": "67da2e831bba0f73374fd5a9",
          "name": "Sydney Von Arx",
          "hidden": false
        },
        {
          "_id": "67da2e831bba0f73374fd5aa",
          "name": "Ryan Bloom",
          "hidden": false
        },
        {
          "_id": "67da2e831bba0f73374fd5ab",
          "name": "Thomas Broadley",
          "hidden": false
        },
        {
          "_id": "67da2e831bba0f73374fd5ac",
          "name": "Haoxing Du",
          "hidden": false
        },
        {
          "_id": "67da2e831bba0f73374fd5ad",
          "name": "Brian Goodrich",
          "hidden": false
        },
        {
          "_id": "67da2e831bba0f73374fd5ae",
          "name": "Nikola Jurkovic",
          "hidden": false
        },
        {
          "_id": "67da2e831bba0f73374fd5af",
          "name": "Luke Harold Miles",
          "hidden": false
        },
        {
          "_id": "67da2e831bba0f73374fd5b0",
          "name": "Seraphina Nix",
          "hidden": false
        },
        {
          "_id": "67da2e831bba0f73374fd5b1",
          "name": "Tao Lin",
          "hidden": false
        },
        {
          "_id": "67da2e831bba0f73374fd5b2",
          "name": "Neev Parikh",
          "hidden": false
        },
        {
          "_id": "67da2e831bba0f73374fd5b3",
          "name": "David Rein",
          "hidden": false
        },
        {
          "_id": "67da2e831bba0f73374fd5b4",
          "name": "Lucas Jun Koba Sato",
          "hidden": false
        },
        {
          "_id": "67da2e831bba0f73374fd5b5",
          "name": "Hjalmar Wijk",
          "hidden": false
        },
        {
          "_id": "67da2e831bba0f73374fd5b6",
          "name": "Daniel M. Ziegler",
          "hidden": false
        },
        {
          "_id": "67da2e831bba0f73374fd5b7",
          "name": "Elizabeth Barnes",
          "hidden": false
        },
        {
          "_id": "67da2e831bba0f73374fd5b8",
          "name": "Lawrence Chan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-18T17:59:31.000Z",
      "submittedOnDailyAt": "2025-03-19T01:10:23.636Z",
      "title": "Évaluation des capacités de résolution de tâches à long terme de l'IA",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Bien sûr, voici la traduction en français de l'article demandé, en maintenant une précision et une professionnalité :\n\nMalgré le développement rapide des benchmarks en IA, l'efficacité de ces benchmarks en termes de signification mondiale est peu claire. Je propose un nouveau métrique pour évaluer la capacité d'un système d'IA par rapport aux habiletés humaines : le \"Horizonte de 50% de Complétion\" (H50). Ce métrique mesure le temps que prend un modèle d'IA pour compléter un travail avec un 50% de succès, ce qui est approximativement le temps que prend un être humain pour accomplir le même travail. Tout d'abord, nous enregistrons le temps que prend aux experts humains pour réaliser 66 nouvelles tâches courtes en utilisant RE-Bench et HCAST. Dans cette tâche, le leader actuel en IA, Claude 3.7 Sonnet, a un H50 d'environ 50 minutes. De plus, depuis 2019, l'H50 des leaders en IA a augmenté d'environ 100% toutes les 7 mois, mais en 2024, cette tendance s'accélère. La plupart de ces améliorations sont dues à une combinaison de confiance élevée, adaptabilité face aux erreurs, améliorations de la capacité logique d'inférence et l'utilisation de outils. On discute les limites des résultats, en particulier le degré de validité externe et le risque des fonctions. Si ces résultats sont généralisés aux tâches de logiciels dans la vie réelle, on prédit que dans les 5 prochaines années, un système d'IA pourrait automatiser de nombreuses tâches de logiciels qui actuellement prennent un mois pour un être humain, ce qui serait une transformation significative dans le développement de logiciels.",
      "upvotes": 4,
      "discussionId": "67da2e8a1bba0f73374fd89e",
      "ai_keywords": [
        "RE-Bench",
        "HCAST",
        "Claude 3.7 Sonnet",
        "50%-task-completion time horizon",
        "reliability",
        "ability to adapt to mistakes",
        "logical reasoning",
        "tool use capabilities"
      ]
    },
    "publishedAt": "2025-03-18T13:59:31.000Z",
    "title": "Measuring AI Ability to Complete Long Tasks",
    "summary": "Despite rapid progress on AI benchmarks, the real-world meaning of benchmark\nperformance remains unclear. To quantify the capabilities of AI systems in\nterms of human capabilities, we propose a new metric: 50%-task-completion time\nhorizon. This is the time humans typically take to complete tasks that AI\nmodels can complete with 50% success rate. We first timed humans with relevant\ndomain expertise on a combination of RE-Bench, HCAST, and 66 novel shorter\ntasks. On these tasks, current frontier AI models such as Claude 3.7 Sonnet\nhave a 50% time horizon of around 50 minutes. Furthermore, frontier AI time\nhorizon has been doubling approximately every seven months since 2019, though\nthe trend may have accelerated in 2024. The increase in AI models' time\nhorizons seems to be primarily driven by greater reliability and ability to\nadapt to mistakes, combined with better logical reasoning and tool use\ncapabilities. We discuss the limitations of our results -- including their\ndegree of external validity -- and the implications of increased autonomy for\ndangerous capabilities. If these results generalize to real-world software\ntasks, extrapolation of this trend predicts that within 5 years, AI systems\nwill be capable of automating many software tasks that currently take humans a\nmonth.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14499.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6398
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.14495",
      "authors": [
        {
          "_id": "67da454fd5132b0eebd066ff",
          "name": "Jiacheng Guo",
          "hidden": false
        },
        {
          "_id": "67da454fd5132b0eebd06700",
          "name": "Yue Wu",
          "hidden": false
        },
        {
          "_id": "67da454fd5132b0eebd06701",
          "name": "Jiahao Qiu",
          "hidden": false
        },
        {
          "_id": "67da454fd5132b0eebd06702",
          "name": "Kaixuan Huang",
          "hidden": false
        },
        {
          "_id": "67da454fd5132b0eebd06703",
          "name": "Xinzhe Juan",
          "hidden": false
        },
        {
          "_id": "67da454fd5132b0eebd06704",
          "name": "Ling Yang",
          "hidden": false
        },
        {
          "_id": "67da454fd5132b0eebd06705",
          "name": "Mengdi Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-18T17:58:28.000Z",
      "submittedOnDailyAt": "2025-03-19T02:48:17.494Z",
      "title": "Détection d'erreurs dans le processus logique d'un modèle de langage basé sur la consistence temporelle",
      "submittedOnDailyBy": {
        "_id": "64fde4e252e82dd432b74ce9",
        "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
        "isPro": false,
        "fullname": "Ling Yang",
        "user": "Lingaaaaaaa",
        "type": "user"
      },
      "summary": "La vérification est essentielle pour garantir l'efficacité mathématique. Nous proposons un nouveau méthode de cohérence temporelle qui répète régulièrement à partir d'évaluations précédentes. En contraste avec un seul vérificateur ou l'approche de concepteurs de modèles multiples, notre méthode utilise la cohérence dans l'ordre des actions de réflexion pour améliorer la précision du vérificateur. Un améliorament significatif a été observé dans le rendement constant dans des cadres de tests d'erreurs de processus mathématiques (Mathcheck, ProcessBench, PRM800K) par rapport aux méthodes standards. En appliquant cette méthode à des modèles combinés de DeepSeek R1, nous avons démontré que les modèles de 7B/8B dépassent tous les modèles de 70B/72B et GPT-4o, en particulier, notre méthode a permis à un modèle combiné de 14B d'atteindre le rendement de DeepSeek-R1. Notre code est disponible sur https://github.com/jcguo123/Temporal-Consistency.",
      "upvotes": 4,
      "discussionId": "67da4550d5132b0eebd0673c",
      "ai_keywords": [
        "temporal consistency",
        "verifiers",
        "iterative refinement",
        "self-reflection actions",
        "verification accuracy",
        "Mathcheck",
        "ProcessBench",
        "PRM800K",
        "DeepSeek R1",
        "distilled models",
        "GPT-4o",
        "performance comparable"
      ]
    },
    "publishedAt": "2025-03-18T13:58:28.000Z",
    "title": "Temporal Consistency for LLM Reasoning Process Error Identification",
    "summary": "Verification is crucial for effective mathematical reasoning. We present a\nnew temporal consistency method where verifiers iteratively refine their\njudgments based on the previous assessment. Unlike one-round verification or\nmulti-model debate approaches, our method leverages consistency in a sequence\nof self-reflection actions to improve verification accuracy. Empirical\nevaluations across diverse mathematical process error identification benchmarks\n(Mathcheck, ProcessBench, and PRM800K) show consistent performance improvements\nover baseline methods. When applied to the recent DeepSeek R1 distilled models,\nour method demonstrates strong performance, enabling 7B/8B distilled models to\noutperform all 70B/72B models and GPT-4o on ProcessBench. Notably, the\ndistilled 14B model with our method achieves performance comparable to\nDeepseek-R1. Our codes are available at\nhttps://github.com/jcguo123/Temporal-Consistency",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14495.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64fde4e252e82dd432b74ce9",
      "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
      "fullname": "Ling Yang",
      "name": "Lingaaaaaaa",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.12545",
      "authors": [
        {
          "_id": "67d943d272843a36b74ab41c",
          "name": "Zhaopan Xu",
          "hidden": false
        },
        {
          "_id": "67d943d272843a36b74ab41d",
          "name": "Pengfei Zhou",
          "hidden": false
        },
        {
          "_id": "67d943d272843a36b74ab41e",
          "name": "Weidong Tang",
          "hidden": false
        },
        {
          "_id": "67d943d272843a36b74ab41f",
          "name": "Jiaxin Ai",
          "hidden": false
        },
        {
          "_id": "67d943d272843a36b74ab420",
          "name": "Wangbo Zhao",
          "hidden": false
        },
        {
          "_id": "67d943d272843a36b74ab421",
          "name": "Xiaojiang Peng",
          "hidden": false
        },
        {
          "_id": "67d943d272843a36b74ab422",
          "name": "Kai Wang",
          "hidden": false
        },
        {
          "_id": "67d943d272843a36b74ab423",
          "name": "Yang You",
          "hidden": false
        },
        {
          "_id": "67d943d272843a36b74ab424",
          "name": "Wenqi Shao",
          "hidden": false
        },
        {
          "_id": "67d943d272843a36b74ab425",
          "name": "Hongxun Yao",
          "hidden": false
        },
        {
          "_id": "67d943d272843a36b74ab426",
          "name": "Kaipeng Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-16T15:26:20.000Z",
      "submittedOnDailyAt": "2025-03-19T00:14:50.269Z",
      "title": "PEBench : Ensemble de données virtuelles pour évaluer l'élimination de la machine apprenante dans des modèles de langage à grande échelle.",
      "submittedOnDailyBy": {
        "_id": "65f1713552c38a91e0a445e8",
        "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
        "isPro": false,
        "fullname": "kaipeng",
        "user": "kpzhang996",
        "type": "user"
      },
      "summary": "Récemment, les modèles multimodal de langage naturel (MLLMs) ont démontré un développement impressionnant dans des problèmes tels que la réponse à des questions d'images, la compréhension d'images et le raisonnement logique. Cependant, ce développement impressionnant dépend de la collecte de beaucoup de données sur Internet, ce qui a généré de grandes préoccupations sur la vie privée et la sécurité. Pour résoudre ces problèmes, l'apprentissage non supervisé (MU) a été appliqué, ce qui permet d'éliminer les connaissances spécifiques des modèles entraînés. Cependant, l'apprentissage non supervisé des MLLMs a attiré de l'attention mais l'évaluation actuelle n'est pas parfaite, généralement considérant des problèmes de base comme des erreurs, ce qui affecte le développement de systèmes sûrs et fiables. Pour résoudre ce problème, nous présentons PEBench, un cadre de référence pour évaluer le rendement de l'apprentissage non supervisé des MLLMs. Ce cadre de référence inclut des données de vie personnelle et des scénarios généraux d'événements relatifs, conçu pour évaluer de manière intégrale le rendement de l'apprentissage non supervisé des MLLMs. Grâce à PEBench, nous proposons de fournir un cadre de référence standardisé pour la recherche sur les modèles multimodal qui protègent la vie privée et sont sûrs. Nous évaluons 6 méthodes d'apprentissage non supervisé, mettant en avant leurs avantages et limites, et révélons les principaux problèmes et opportunités dans l'apprentissage non supervisé des MLLMs.",
      "upvotes": 4,
      "discussionId": "67d943db72843a36b74ab652",
      "projectPage": "https://pebench.github.io/",
      "ai_keywords": [
        "Multimodal Large Language Models",
        "visual question answering",
        "visual understanding",
        "reasoning",
        "machine unlearning",
        "benchmark",
        "PEBench",
        "dataset",
        "personal entities",
        "general event scenes",
        "secure",
        "privacy-preserving",
        "multimodal models"
      ]
    },
    "publishedAt": "2025-03-16T11:26:20.000Z",
    "title": "PEBench: A Fictitious Dataset to Benchmark Machine Unlearning for\n  Multimodal Large Language Models",
    "summary": "In recent years, Multimodal Large Language Models (MLLMs) have demonstrated\nremarkable advancements in tasks such as visual question answering, visual\nunderstanding, and reasoning. However, this impressive progress relies on vast\namounts of data collected from the internet, raising significant concerns about\nprivacy and security. To address these issues, machine unlearning (MU) has\nemerged as a promising solution, enabling the removal of specific knowledge\nfrom an already trained model without requiring retraining from scratch.\nAlthough MU for MLLMs has gained attention, current evaluations of its efficacy\nremain incomplete, and the underlying problem is often poorly defined, which\nhinders the development of strategies for creating more secure and trustworthy\nsystems. To bridge this gap, we introduce a benchmark, named PEBench, which\nincludes a dataset of personal entities and corresponding general event scenes,\ndesigned to comprehensively assess the performance of MU for MLLMs. Through\nPEBench, we aim to provide a standardized and robust framework to advance\nresearch in secure and privacy-preserving multimodal models. We benchmarked 6\nMU methods, revealing their strengths and limitations, and shedding light on\nkey challenges and opportunities for MU in MLLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12545.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65f1713552c38a91e0a445e8",
      "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
      "fullname": "kaipeng",
      "name": "kpzhang996",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.14151",
      "authors": [
        {
          "_id": "67da71b1c26b43885226a72d",
          "name": "Yong Zhong",
          "hidden": false
        },
        {
          "_id": "67da71b1c26b43885226a72e",
          "name": "Zhuoyi Yang",
          "hidden": false
        },
        {
          "_id": "67da71b1c26b43885226a72f",
          "name": "Jiayan Teng",
          "hidden": false
        },
        {
          "_id": "67da71b1c26b43885226a730",
          "name": "Xiaotao Gu",
          "hidden": false
        },
        {
          "_id": "67da71b1c26b43885226a731",
          "name": "Chongxuan Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-18T11:17:32.000Z",
      "submittedOnDailyAt": "2025-03-19T05:57:05.443Z",
      "title": "Concat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale\n\nConcat-ID: Notification sur le vidéo de synthèse de l'identité générale",
      "submittedOnDailyBy": {
        "_id": "63468720dd6d90d82ccf3450",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
        "isPro": false,
        "fullname": "YSH",
        "user": "BestWishYsh",
        "type": "user"
      },
      "summary": "Concat-ID est un cadre uniforme pour la génération de vidéos d'identité. Concat-ID utilise des autoencodeurs variationnels pour extraire des caractéristiques d'images et combine les potentiels de vidéo et la direction de séquence. Il élimine la nécessité d'ajouts de modules en utilisant une mécanique d'attention auto-quadratique 3D. Il introduit une nouvelle stratégie de combinaison de vidéos et un ensemble d'entraînement multiniveau pour améliorer la nature de la vidéo tout en maintenant la cohérence de l'identité et la lecture faciale. Des expériences larges montrent que Concat-ID dépasse les méthodes actuelles dans la génération d'identités uniques et multiples, et maintient une scalabilité sans limites même dans des scénarios multi-personnels. Concat-ID établit un nouveau standard pour la synthèse de vidéos d'identité et offre des solutions scalables pour une large gamme d'applications.",
      "upvotes": 3,
      "discussionId": "67da71bdc26b43885226ab4e",
      "projectPage": "https://ml-gsai.github.io/Concat-ID-demo/",
      "githubRepo": "https://github.com/ML-GSAI/Concat-ID",
      "ai_keywords": [
        "Variational Autoencoders",
        "3D self-attention mechanisms",
        "cross-video pairing strategy",
        "multi-stage training regimen",
        "identity consistency",
        "facial editability",
        "video naturalness",
        "identity-preserving video synthesis"
      ]
    },
    "publishedAt": "2025-03-18T07:17:32.000Z",
    "title": "Concat-ID: Towards Universal Identity-Preserving Video Synthesis",
    "summary": "We present Concat-ID, a unified framework for identity-preserving video\ngeneration. Concat-ID employs Variational Autoencoders to extract image\nfeatures, which are concatenated with video latents along the sequence\ndimension, leveraging solely 3D self-attention mechanisms without the need for\nadditional modules. A novel cross-video pairing strategy and a multi-stage\ntraining regimen are introduced to balance identity consistency and facial\neditability while enhancing video naturalness. Extensive experiments\ndemonstrate Concat-ID's superiority over existing methods in both single and\nmulti-identity generation, as well as its seamless scalability to multi-subject\nscenarios, including virtual try-on and background-controllable generation.\nConcat-ID establishes a new benchmark for identity-preserving video synthesis,\nproviding a versatile and scalable solution for a wide range of applications.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14151.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63468720dd6d90d82ccf3450",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
      "fullname": "YSH",
      "name": "BestWishYsh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 35
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.12271",
      "authors": [
        {
          "_id": "67d926523acb37a1cfa74cf8",
          "name": "Shufan Li",
          "hidden": false
        },
        {
          "_id": "67d926523acb37a1cfa74cf9",
          "name": "Konstantinos Kallidromitis",
          "hidden": false
        },
        {
          "_id": "67d926523acb37a1cfa74cfa",
          "name": "Akash Gokul",
          "hidden": false
        },
        {
          "_id": "67d926523acb37a1cfa74cfb",
          "name": "Arsh Koneru",
          "hidden": false
        },
        {
          "_id": "67d926523acb37a1cfa74cfc",
          "name": "Yusuke Kato",
          "hidden": false
        },
        {
          "_id": "67d926523acb37a1cfa74cfd",
          "name": "Kazuki Kozuka",
          "hidden": false
        },
        {
          "_id": "67d926523acb37a1cfa74cfe",
          "name": "Aditya Grover",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-15T21:58:12.000Z",
      "submittedOnDailyAt": "2025-03-19T02:08:42.869Z",
      "title": "Reflect-DiT : Réflexion de texte en image par échelonnage dans l'inférence des transformateurs de diffusion",
      "submittedOnDailyBy": {
        "_id": "6310531914aa81e1044363ed",
        "avatarUrl": "/avatars/ae7767e591cb7199ea2f62d2db89fc7f.svg",
        "isPro": false,
        "fullname": "Shufan Li",
        "user": "jacklishufan",
        "type": "user"
      },
      "summary": "L'évolution des modèles de génération contextuel a montré que l'une des principales façons d'aborder le problème est par l'échelle pendant l'entraînement. Cette approche permet aux modèles plus grands de s'entraîner avec une plus grande quantité de données et de ressources calcul, ce qui est efficace mais coûteux en termes de calculs. Cependant, l'échelle pendant l'inférence a gagné en importance. Actuellement, l'échelle pendant l'inférence dans les modèles de génération contextuel est limitée à la génération de plusieurs images pour un prompt et à la sélection du meilleur résultat par la technique de sampling Best-of-N. En se basant sur le succès récent dans le domaine du langage comme DeepSeek-R1, on propose d'ajouter une fonction de réflexion contextuelle aux modèles de génération contextuel au lieu de la technique Best-of-N. Cette nouvelle approche est appelée Reflect-DiT. Reflect-DiT utilise des exemples d'images générées précédemment et une description des améliorations nécessaires sous forme de phrase pour réviser la génération d'un Transformer de diffusion. Au lieu de dépendre de la génération aléatoire et d'attendre des résultats meilleurs dans les générations futures, Reflect-DiT se concentre sur l'amélioration spécifiquement dans des zones spécifiques. Les résultats des expériences montrent un amélioration du rendement du benchmark GenEval (+0,19), en utilisant SANA-1.0-1,6B comme modèle de base. De plus, 20 échantillons sont générés pour un prompt et on atteint un nouveau point maximum (0,81) qui dépasse le point maximum antérieur (0,80). Ce résultat dépasse un modèle plus grand qui utilisait la technique Best-of-N (SANA-1,5-4,8B) et qui a été évalué avec 2048 échantillons.",
      "upvotes": 2,
      "discussionId": "67d926543acb37a1cfa74d9f",
      "ai_keywords": [
        "diffusion models",
        "text-to-image diffusion models",
        "best-of-N sampling",
        "in-context reflection",
        "Diffusion Transformers",
        "Reflect-DiT",
        "GenEval benchmark",
        "in-context examples",
        "textual feedback",
        "performance improvement",
        "state-of-the-art"
      ]
    },
    "publishedAt": "2025-03-15T17:58:12.000Z",
    "title": "Reflect-DiT: Inference-Time Scaling for Text-to-Image Diffusion\n  Transformers via In-Context Reflection",
    "summary": "The predominant approach to advancing text-to-image generation has been\ntraining-time scaling, where larger models are trained on more data using\ngreater computational resources. While effective, this approach is\ncomputationally expensive, leading to growing interest in inference-time\nscaling to improve performance. Currently, inference-time scaling for\ntext-to-image diffusion models is largely limited to best-of-N sampling, where\nmultiple images are generated per prompt and a selection model chooses the best\noutput. Inspired by the recent success of reasoning models like DeepSeek-R1 in\nthe language domain, we introduce an alternative to naive best-of-N sampling by\nequipping text-to-image Diffusion Transformers with in-context reflection\ncapabilities. We propose Reflect-DiT, a method that enables Diffusion\nTransformers to refine their generations using in-context examples of\npreviously generated images alongside textual feedback describing necessary\nimprovements. Instead of passively relying on random sampling and hoping for a\nbetter result in a future generation, Reflect-DiT explicitly tailors its\ngenerations to address specific aspects requiring enhancement. Experimental\nresults demonstrate that Reflect-DiT improves performance on the GenEval\nbenchmark (+0.19) using SANA-1.0-1.6B as a base model. Additionally, it\nachieves a new state-of-the-art score of 0.81 on GenEval while generating only\n20 samples per prompt, surpassing the previous best score of 0.80, which was\nobtained using a significantly larger model (SANA-1.5-4.8B) with 2048 samples\nunder the best-of-N approach.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12271.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6310531914aa81e1044363ed",
      "avatarUrl": "/avatars/ae7767e591cb7199ea2f62d2db89fc7f.svg",
      "fullname": "Shufan Li",
      "name": "jacklishufan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.09443",
      "authors": [
        {
          "_id": "67da8b794e1138ddc328de09",
          "user": {
            "_id": "630a4aaa9df54451d91cc6fa",
            "avatarUrl": "/avatars/10898c7886e25c64d58334083e3cf2d5.svg",
            "isPro": false,
            "fullname": "Julian Spravil",
            "user": "Spravil",
            "type": "user"
          },
          "name": "Julian Spravil",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-19T09:43:48.893Z",
          "hidden": false
        },
        {
          "_id": "67da8b794e1138ddc328de0a",
          "name": "Sebastian Houben",
          "hidden": false
        },
        {
          "_id": "67da8b794e1138ddc328de0b",
          "name": "Sven Behnke",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-12T14:41:10.000Z",
      "submittedOnDailyAt": "2025-03-19T07:58:17.987Z",
      "title": "Florence : Loi d'escalade pour la généralisation systématique du modèle de langue Vision-Language",
      "submittedOnDailyBy": {
        "_id": "630a4aaa9df54451d91cc6fa",
        "avatarUrl": "/avatars/10898c7886e25c64d58334083e3cf2d5.svg",
        "isPro": false,
        "fullname": "Julian Spravil",
        "user": "Spravil",
        "type": "user"
      },
      "summary": "Le traduction en français du texte fourni est :\n\n\"Le transfert de langues permet aux modèles de langage visuel (VLMs) de réaliser des tâches visuelles multilingues avec des données d'entraînement dans un seul langage. L'approche actuelle se base sur des modèles d'entraînement à grande échelle multilingues. Cependant, ceux-ci font face au 'problème' de la multilingualité, qui affecte le rendement dans des tâches de dérivation descendante tout en maintenant la capacité multilingue, surpassant l'incertitude du langage et ne pouvant pas suivre les avancées récentes. Dans cet article, on étudie l'échelle de la généralisation panoramique dans des tâches multi-tâches, en mettant l'accent sur l'impact du taille du modèle et des échantillons d'entraînement. On propose le modèle Florenz, qui combine un modèle d'entraînement préalable Florence-2 et un grand modèle de langage Gemma-2 dans un unique VLM encoder-decoder, avec un nombre de paramètres variant de 0,4B à 11,2B. Florenz est entraîné avec un ensemble de données synthétique de capture d'images qui caractérise la manque de couverture linguistique complète, ce qui permet de mesurer la généralisation dans des tâches de traduction complètement couvertes. De plus, l'apprentissage de paires de langues non vues s'adapte à l'échelle, et on utilise des proxies de génération de données et la famille de modèles Florenz proposée pour démontrer que la capacité de capture d'images est maintenue même dans des cas où seulement des données de tâches de traduction dans un langage spécifique sont disponibles. L'entraînement fine avec des ensembles de données de dérivation descendante montre un rendement adéquat et démontre les patrons d'échelle attendus en traduction multilingue (Multi30K, CoMMuTE), surpassement de l'incertitude du langage (CoMMuTE) et capture d'images (Multi30K, XM3600, COCO Karpathy).\"",
      "upvotes": 2,
      "discussionId": "67da8b7a4e1138ddc328de44"
    },
    "publishedAt": "2025-03-12T10:41:10.000Z",
    "title": "Florenz: Scaling Laws for Systematic Generalization in Vision-Language\n  Models",
    "summary": "Cross-lingual transfer enables vision-language models (VLMs) to perform\nvision tasks in various languages with training data only in one language.\nCurrent approaches rely on large pre-trained multilingual language models.\nHowever, they face the curse of multilinguality, sacrificing downstream task\nperformance for multilingual capabilities, struggling with lexical ambiguities,\nand falling behind recent advances. In this work, we study the scaling laws of\nsystematic generalization with monolingual VLMs for multilingual tasks,\nfocusing on the impact of model size and seen training samples. We propose\nFlorenz, a monolingual encoder-decoder VLM with 0.4B to 11.2B parameters\ncombining the pre-trained VLM Florence-2 and the large language model Gemma-2.\nFlorenz is trained with varying compute budgets on a synthetic dataset that\nfeatures intentionally incomplete language coverage for image captioning, thus,\ntesting generalization from the fully covered translation task. We show that\nnot only does indirectly learning unseen task-language pairs adhere to a\nscaling law, but also that with our data generation pipeline and the proposed\nFlorenz model family, image captioning abilities can emerge in a specific\nlanguage even when only data for the translation task is available. Fine-tuning\non a mix of downstream datasets yields competitive performance and demonstrates\npromising scaling trends in multimodal machine translation (Multi30K, CoMMuTE),\nlexical disambiguation (CoMMuTE), and image captioning (Multi30K, XM3600, COCO\nKarpathy).",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.09443.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "630a4aaa9df54451d91cc6fa",
      "avatarUrl": "/avatars/10898c7886e25c64d58334083e3cf2d5.svg",
      "fullname": "Julian Spravil",
      "name": "Spravil",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.13661",
      "authors": [
        {
          "_id": "67da8a9fdab8cc723c349fb0",
          "user": {
            "_id": "630a5ef0e81e1dea2cedcec0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/630a5ef0e81e1dea2cedcec0/ATtyCvYoX4z7uxsm2sJU2.png",
            "isPro": false,
            "fullname": "Hà Huy Hoàng",
            "user": "HoangHa",
            "type": "user"
          },
          "name": "Huy Hoang Ha",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-19T09:43:52.541Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/630a5ef0e81e1dea2cedcec0/9DxLIM8Ftd4OVGfuGqAEU.png"
      ],
      "publishedAt": "2025-03-17T19:09:11.000Z",
      "submittedOnDailyAt": "2025-03-19T07:43:37.853Z",
      "title": "Francés LLM reconnaissant le pensée de \"moins de données, plus d'explication\"",
      "submittedOnDailyBy": {
        "_id": "630a5ef0e81e1dea2cedcec0",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/630a5ef0e81e1dea2cedcec0/ATtyCvYoX4z7uxsm2sJU2.png",
        "isPro": false,
        "fullname": "Hà Huy Hoàng",
        "user": "HoangHa",
        "type": "user"
      },
      "summary": "Les modèles de langage grand (LLMs) montrent des capacités exceptionnelles dans de nombreuses tâches de traitement du langage naturel. Cependant, pour atteindre un rendement robuste dans des domaines spécialisés comme la logique mathématique ou les langues non anglaises, il est nécessaire d'étendre l'entraînement avec de grands ensembles de données. Dans cet article, nous examinons un approche opposée : l'ajustement stratégique micro de petits, hautement qualifiés ensembles de données bilingues (anglais-français) pour améliorer la capacité logique des modèles de langage grand et le vocabulaire du français. Nous étudions que, en évitant la dépendance à l'échelle, la personnalisation et l'entraînement optimisé des données cibles peuvent atteindre des rendements compétitifs ou meilleurs. Grâce à l'ajustement micro de 2 000 échantillons sélectionnés pour l'optimisation de post-rotation (SFT), nous observons un amélioration claire en logique mathématique. En particulier, Pensez 7B augmente la précision du modèle de base sur AIME25 d'au plus 20% et sur le benchmark de niveau 5 de mathématiques en France de 12%. Ces résultats remettent en cause l'hypothèse générale selon laquelle il est nécessaire d'avoir de grands ensembles de données pour atteindre un rendement robuste dans les modèles de langage grand et montrent que la personnalisation stratégique des données et l'ajustement micro optimisé peuvent augmenter tant les compétences spécialisées que les compétences multilingues de manière bidirectionnelle. Nos résultats ont un impact sur le développement efficace de modèles de langage grand de haute qualité, surtout dans les scénarios limités en ressources.",
      "upvotes": 1,
      "discussionId": "67da8aa1dab8cc723c34a039"
    },
    "publishedAt": "2025-03-17T15:09:11.000Z",
    "title": "Pensez: Less Data, Better Reasoning -- Rethinking French LLM",
    "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\nvarious natural language processing tasks. However, achieving strong\nperformance in specialized domains like mathematical reasoning and non-English\nlanguages often requires extensive training on massive datasets. This paper\ninvestigates a contrasting approach: strategic fine-tuning on a small,\nhigh-quality, bilingual (English-French) dataset to enhance both the reasoning\ncapabilities and French language proficiency of a large language model. Rather\nthan relying on scale, we explore the hypothesis that targeted data curation\nand optimized training can achieve competitive, or even superior, performance.\nWe demonstrate, through targeted supervised fine-tuning (SFT) on only 2,000\ncarefully selected samples, significant improvements in mathematical reasoning.\nSpecifically, Pensez 7B exhibits an increase in accuracy of the base model up\nto 20% on the AIME25 and a 12% increase on a French MATH level 5 benchmark.\nThese results challenge the prevailing assumption that massive datasets are\naprerequisite for strong reasoning performance in LLMs, highlighting the\npotential of strategic data curation and optimized fine-tuning for enhancing\nboth specialized skills and multilingual capabilities. Our findings have\nimplications for the efficient development of high-performing, multilingual\nLLMs, especially in resource-constrained scenarios.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/630a5ef0e81e1dea2cedcec0/9DxLIM8Ftd4OVGfuGqAEU.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13661.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "630a5ef0e81e1dea2cedcec0",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/630a5ef0e81e1dea2cedcec0/ATtyCvYoX4z7uxsm2sJU2.png",
      "fullname": "Hà Huy Hoàng",
      "name": "HoangHa",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 26
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.12127",
      "authors": [
        {
          "_id": "67d95fa8fb17ef1c744db2db",
          "user": {
            "_id": "64ee11c125d2bb76c06e243d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ee11c125d2bb76c06e243d/DR2NV4NYINtpRoMd0NupA.png",
            "isPro": false,
            "fullname": "tobia poppi",
            "user": "tobi1modna",
            "type": "user"
          },
          "name": "Tobia Poppi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-18T14:57:56.046Z",
          "hidden": false
        },
        {
          "_id": "67d95fa8fb17ef1c744db2dc",
          "name": "Tejaswi Kasarla",
          "hidden": false
        },
        {
          "_id": "67d95fa8fb17ef1c744db2dd",
          "name": "Pascal Mettes",
          "hidden": false
        },
        {
          "_id": "67d95fa8fb17ef1c744db2de",
          "name": "Lorenzo Baraldi",
          "hidden": false
        },
        {
          "_id": "67d95fa8fb17ef1c744db2df",
          "name": "Rita Cucchiara",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-15T13:18:04.000Z",
      "submittedOnDailyAt": "2025-03-19T07:49:10.270Z",
      "title": "Hiperbólico Safege Auribision Larguía Modelo",
      "submittedOnDailyBy": {
        "_id": "64ee11c125d2bb76c06e243d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ee11c125d2bb76c06e243d/DR2NV4NYINtpRoMd0NupA.png",
        "isPro": false,
        "fullname": "tobia poppi",
        "user": "tobi1modna",
        "type": "user"
      },
      "summary": "Pour résoudre la détection et l'élimination de contenu insecure, une étape importante est l'intégration avec la réalité, ce qui implique l'utilisation de modèles de vision-langue comme CLIP. Actuellement, des techniques d'entraînement oubliées sont utilisées pour éliminer le savoir des modèles de concepts insecures, ce qui est efficace pour réduire les sorties insatisfaisantes mais limite la différence entre insecur et sécur. Dans cet article, nous présentons un nouvel approche qui transforme l'entraînement oublié en un paradigme de reconnaissance en utilisant les propriétés stratifiées d'espaces hyperboliques. Les contenus sécurisés et insecurisés sont considérés en couches de signification et sont distribués dans différentes zones d'un espace hyperbolique. Notre HySAC (Hyperbolic Safety-Aware CLIP) utilise une fonction de perte liée au sens pour modéliser les relations stratifiées et asymétriques entre paires d'images et de texte sécurisés et insecurisés. Ce modèle permet au modèle d'adapter la reconnaissance de contenu insecur, réorienter les requêtes insecures vers des options sécures dynamiquement ou maintenir le résultat original. Les expériences extensives montrent que cet approche renforce la perception de sécurité et améliore l'adaptabilité et la compréhension de la modélisation de contenu dans les modèles de vision-langue. Le code source est disponible sur https://github.com/aimagelab/HySAC.",
      "upvotes": 1,
      "discussionId": "67d95fabfb17ef1c744db411",
      "ai_keywords": [
        "hyperbolic space",
        "entailment hierarchy",
        "entailment loss functions",
        "Euclidean embeddings",
        "multimodal unsafe classifier",
        "content retriever",
        "hyperbolic Safety-Aware CLIP",
        "HySAC"
      ]
    },
    "publishedAt": "2025-03-15T09:18:04.000Z",
    "title": "Hyperbolic Safety-Aware Vision-Language Models",
    "summary": "Addressing the retrieval of unsafe content from vision-language models such\nas CLIP is an important step towards real-world integration. Current efforts\nhave relied on unlearning techniques that try to erase the model's knowledge of\nunsafe concepts. While effective in reducing unwanted outputs, unlearning\nlimits the model's capacity to discern between safe and unsafe content. In this\nwork, we introduce a novel approach that shifts from unlearning to an awareness\nparadigm by leveraging the inherent hierarchical properties of the hyperbolic\nspace. We propose to encode safe and unsafe content as an entailment hierarchy,\nwhere both are placed in different regions of hyperbolic space. Our HySAC,\nHyperbolic Safety-Aware CLIP, employs entailment loss functions to model the\nhierarchical and asymmetrical relations between safe and unsafe image-text\npairs. This modelling, ineffective in standard vision-language models due to\ntheir reliance on Euclidean embeddings, endows the model with awareness of\nunsafe content, enabling it to serve as both a multimodal unsafe classifier and\na flexible content retriever, with the option to dynamically redirect unsafe\nqueries toward safer alternatives or retain the original output. Extensive\nexperiments show that our approach not only enhances safety recognition but\nalso establishes a more adaptable and interpretable framework for content\nmoderation in vision-language models. Our source code is available at\nhttps://github.com/aimagelab/HySAC.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12127.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ee11c125d2bb76c06e243d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ee11c125d2bb76c06e243d/DR2NV4NYINtpRoMd0NupA.png",
      "fullname": "tobia poppi",
      "name": "tobi1modna",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.10546",
      "authors": [
        {
          "_id": "67da141d6b2857e3ec1412a7",
          "name": "Zixian Liu",
          "hidden": false
        },
        {
          "_id": "67da141d6b2857e3ec1412a8",
          "name": "Mingtong Zhang",
          "hidden": false
        },
        {
          "_id": "67da141d6b2857e3ec1412a9",
          "name": "Yunzhu Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-13T16:59:17.000Z",
      "submittedOnDailyAt": "2025-03-19T00:00:05.763Z",
      "title": "KUDA : Points clés de l'intégration de l'apprentissage dynamique et de la rétroalimentation visuelle dans la manipulation de robots de mots ouverts",
      "submittedOnDailyBy": {
        "_id": "671c6a3e255aa50ebb504fc5",
        "avatarUrl": "/avatars/b549bb19990ae21690472799817f951e.svg",
        "isPro": false,
        "fullname": "Mingtong Zhang",
        "user": "Mingtongz",
        "type": "user"
      },
      "summary": "Le développement rapide des modèles de langage (LLMs) et de langage visuel (VLMs) a permis d'obtenir des avancées notables dans le développement de systèmes de manipulation de robots basés sur des environnements ouverts. Cependant, de nombreux méthodes existantes ont ignoré l'importance de la dynamique des objets et ont limité leur application dans des tâches dynamiques complexes. Dans cet article, nous présentons un système de manipulation ouvert basé sur des points clés (KUDA) qui résout ces problèmes en intégrant des modèles de dynamique basés sur l'apprentissage et un traitement visuel. Notre principal objectif est de montrer que la configuration des objectifs basée sur des points clés peut être interprétée simultanément par des VLMs et traduite efficacement en fonctions de coût dans la planification basée sur des modèles. KUDA reçoit des instructions linguistiques et des observations visuelles, attribue des points clés dans des images RGB et génère la configuration des objectifs à travers des VLMs. Cette représentation abstraite des points clés se transforme en une fonction de coût via un modèle de dynamique entraîné, ce qui génère les trajectoires du robot. KUDA a été évalué dans diverses tâches de manipulation, y compris des instructions linguistiques libres, des interactions entre plusieurs objets, des objets déformables ou des particules, démontrant l'efficacité de notre cadre de travail. Le site web du projet est disponible sur http://kuda-dynamics.github.io.",
      "upvotes": 1,
      "discussionId": "67da141e6b2857e3ec141301",
      "projectPage": "https://kuda-dynamics.github.io",
      "githubRepo": "https://github.com/StoreBlank/KUDA",
      "ai_keywords": [
        "LLMs",
        "VLMs",
        "open-vocabulary robotic manipulation systems",
        "object dynamics",
        "KUDA",
        "dynamics learning",
        "visual prompting",
        "keypoints",
        "learning-based neural dynamics models",
        "keypoint-based target specification",
        "cost functions",
        "model-based planning",
        "robotic trajectories",
        "free-form language instructions",
        "multi-object interactions",
        "deformable objects",
        "granular objects"
      ]
    },
    "publishedAt": "2025-03-13T12:59:17.000Z",
    "title": "KUDA: Keypoints to Unify Dynamics Learning and Visual Prompting for\n  Open-Vocabulary Robotic Manipulation",
    "summary": "With the rapid advancement of large language models (LLMs) and\nvision-language models (VLMs), significant progress has been made in developing\nopen-vocabulary robotic manipulation systems. However, many existing approaches\noverlook the importance of object dynamics, limiting their applicability to\nmore complex, dynamic tasks. In this work, we introduce KUDA, an\nopen-vocabulary manipulation system that integrates dynamics learning and\nvisual prompting through keypoints, leveraging both VLMs and learning-based\nneural dynamics models. Our key insight is that a keypoint-based target\nspecification is simultaneously interpretable by VLMs and can be efficiently\ntranslated into cost functions for model-based planning. Given language\ninstructions and visual observations, KUDA first assigns keypoints to the RGB\nimage and queries the VLM to generate target specifications. These abstract\nkeypoint-based representations are then converted into cost functions, which\nare optimized using a learned dynamics model to produce robotic trajectories.\nWe evaluate KUDA on a range of manipulation tasks, including free-form language\ninstructions across diverse object categories, multi-object interactions, and\ndeformable or granular objects, demonstrating the effectiveness of our\nframework. The project page is available at http://kuda-dynamics.github.io.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10546.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "671c6a3e255aa50ebb504fc5",
      "avatarUrl": "/avatars/b549bb19990ae21690472799817f951e.svg",
      "fullname": "Mingtong Zhang",
      "name": "Mingtongz",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.10410",
      "authors": [
        {
          "_id": "67d9638d92e48ed07860ecee",
          "user": {
            "_id": "67934b85c67af4a116b5594b",
            "avatarUrl": "/avatars/076cf0803b50e1ab54e5ba4f8f2a8e44.svg",
            "isPro": false,
            "fullname": "yuwendu",
            "user": "yuwendu",
            "type": "user"
          },
          "name": "Yuwen Du",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-18T14:57:49.908Z",
          "hidden": false
        },
        {
          "_id": "67d9638d92e48ed07860ecef",
          "name": "Anning Hu",
          "hidden": false
        },
        {
          "_id": "67d9638d92e48ed07860ecf0",
          "name": "Zichen Chao",
          "hidden": false
        },
        {
          "_id": "67d9638d92e48ed07860ecf1",
          "name": "Yifan Lu",
          "hidden": false
        },
        {
          "_id": "67d9638d92e48ed07860ecf2",
          "name": "Junhao Ge",
          "hidden": false
        },
        {
          "_id": "67d9638d92e48ed07860ecf3",
          "name": "Genjia Liu",
          "hidden": false
        },
        {
          "_id": "67d9638d92e48ed07860ecf4",
          "name": "Weitao Wu",
          "hidden": false
        },
        {
          "_id": "67d9638d92e48ed07860ecf5",
          "name": "Lanjun Wang",
          "hidden": false
        },
        {
          "_id": "67d9638d92e48ed07860ecf6",
          "name": "Siheng Chen",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/67934b85c67af4a116b5594b/rDLvz9Jma7IWsaGubueis.png",
        "https://cdn-uploads.huggingface.co/production/uploads/67934b85c67af4a116b5594b/1PULsx8sLyy9CqDRMuzo1.png"
      ],
      "publishedAt": "2025-03-13T14:33:42.000Z",
      "submittedOnDailyAt": "2025-03-19T00:00:36.900Z",
      "title": "RoCo-Sim : Méthode pour diriger les processus de collaboration à l'extérieur de la route vers l'avenir",
      "submittedOnDailyBy": {
        "_id": "67934b85c67af4a116b5594b",
        "avatarUrl": "/avatars/076cf0803b50e1ab54e5ba4f8f2a8e44.svg",
        "isPro": false,
        "fullname": "yuwendu",
        "user": "yuwendu",
        "type": "user"
      },
      "summary": "La reconnaissance de collaboration du côté de la route est un système qui améliore la reconnaissance de l'environnement des véhicules en partageant des données d'observation entre plusieurs unités du côté de la route. Actuellement, les méthodes d'observation du côté de la route se concentrent sur le développement de modèles, essayant d'éviter des problèmes de données tels que les erreurs standards, la manque d'information et la cohérence des points de référence, mais leur performance a diminué dans les données publiques récentes. Pour améliorer significativement la reconnaissance de collaboration du côté de la route et résoudre les problèmes de données, nous proposons le premier cadre de simulation de l'observation de collaboration du côté de la route RoCo-Sim. RoCo-Sim peut générer des données d'observation diverses et avec une cohérence de points de référence en éditant dynamiquement les bords et en transformant l'apparence d'une seule image. RoCo-Sim est composé de quatre composants : (1) l'optimisation de la caméra externe garantit la projection exacte 3D à 2D des caméras du côté de la route ; (2) le nouveau reconnaissance de masque de points de référence (MOAS) détermine la disposition de différents actifs digitaux dans l'espace 3D ; (3) DepthSAM modélise de manière innovante la relation entre les bords et le fond d'un point de vue fixe, assurant la cohérence des points de référence du bord ; (4) le paquet de post-traitement du scalateur génère des scénarios plus réalistes et riches en raison de la transformation d'apparence et d'autres extensions. RoCo-Sim comble des lacunes importantes dans la simulation de l'observation du côté de la route. Le code et les modèles pré-entraînés sont immédiatement disponibles : https://github.com/duyuwen-duen/RoCo-Sim",
      "upvotes": 1,
      "discussionId": "67d9639192e48ed07860ee1f",
      "ai_keywords": [
        "Multi-View Occlusion-Aware Sampler",
        "DepthSAM",
        "Scalable Post-Processing Toolkit",
        "3D object detection"
      ]
    },
    "publishedAt": "2025-03-13T10:33:42.000Z",
    "title": "RoCo-Sim: Enhancing Roadside Collaborative Perception through Foreground\n  Simulation",
    "summary": "Roadside Collaborative Perception refers to a system where multiple roadside\nunits collaborate to pool their perceptual data, assisting vehicles in\nenhancing their environmental awareness. Existing roadside perception methods\nconcentrate on model design but overlook data issues like calibration errors,\nsparse information, and multi-view consistency, leading to poor performance on\nrecent published datasets. To significantly enhance roadside collaborative\nperception and address critical data issues, we present the first simulation\nframework RoCo-Sim for road-side collaborative perception. RoCo-Sim is capable\nof generating diverse, multi-view consistent simulated roadside data through\ndynamic foreground editing and full-scene style transfer of a single image.\nRoCo-Sim consists of four components: (1) Camera Extrinsic Optimization ensures\naccurate 3D to 2D projection for roadside cameras; (2) A novel Multi-View\nOcclusion-Aware Sampler (MOAS) determines the placement of diverse digital\nassets within 3D space; (3) DepthSAM innovatively models foreground-background\nrelationships from single-frame fixed-view images, ensuring multi-view\nconsistency of foreground; and (4) Scalable Post-Processing Toolkit generates\nmore realistic and enriched scenes through style transfer and other\nenhancements. RoCo-Sim significantly improves roadside 3D object detection,\noutperforming SOTA methods by 83.74 on Rcooper-Intersection and 83.12 on\nTUMTraf-V2X for AP70. RoCo-Sim fills a critical gap in roadside perception\nsimulation. Code and pre-trained models will be released soon:\nhttps://github.com/duyuwen-duen/RoCo-Sim",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/67934b85c67af4a116b5594b/rDLvz9Jma7IWsaGubueis.png",
      "https://cdn-uploads.huggingface.co/production/uploads/67934b85c67af4a116b5594b/1PULsx8sLyy9CqDRMuzo1.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10410.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67934b85c67af4a116b5594b",
      "avatarUrl": "/avatars/076cf0803b50e1ab54e5ba4f8f2a8e44.svg",
      "fullname": "yuwendu",
      "name": "yuwendu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]