[
  {
    "paper": {
      "id": "2503.15265",
      "authors": [
        {
          "_id": "67db8c4c9e4f93ee46411c1d",
          "name": "Ruowen Zhao",
          "hidden": false
        },
        {
          "_id": "67db8c4c9e4f93ee46411c1e",
          "name": "Junliang Ye",
          "hidden": false
        },
        {
          "_id": "67db8c4c9e4f93ee46411c1f",
          "name": "Zhengyi Wang",
          "hidden": false
        },
        {
          "_id": "67db8c4c9e4f93ee46411c20",
          "name": "Guangce Liu",
          "hidden": false
        },
        {
          "_id": "67db8c4c9e4f93ee46411c21",
          "name": "Yiwen Chen",
          "hidden": false
        },
        {
          "_id": "67db8c4c9e4f93ee46411c22",
          "name": "Yikai Wang",
          "hidden": false
        },
        {
          "_id": "67db8c4c9e4f93ee46411c23",
          "name": "Jun Zhu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-19T14:39:30.000Z",
      "submittedOnDailyAt": "2025-03-20T03:22:40.364Z",
      "title": "DeepMesh: 強化学習による自動帰納的アーティストマッチング作成",
      "submittedOnDailyBy": {
        "_id": "6522e4fbd89bc7773ddc4b58",
        "avatarUrl": "/avatars/3e9b158af52c5f738a3eae72dcbb3824.svg",
        "isPro": false,
        "fullname": "Ruowen Zhao",
        "user": "zzzrw",
        "type": "user"
      },
      "summary": "三角形メッシュは、3Dアプリケーションでの効率的な操作とレンダリングに重要な役割を果たします。自動帰戻しメソッドは、離散なベータートークンを予測して構造化されたメッシュを生成しますが、通常は面数の制限とメッシュの不完全性に制限されます。これらの課題を解決するために、私たちはDeepMeshフレームワークを提案します。これは2つのキーのイノベーションを通じてメッシュ生成を最適化しています。1. 効率的な予えられたトークナリゼーションアルゴリズムを含む新しい予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な予えられたトークナリゼーションアルゴリズムを含む効率的な",
      "upvotes": 25,
      "discussionId": "67db8c519e4f93ee46411d60",
      "projectPage": "https://zhaorw02.github.io/DeepMesh/",
      "githubRepo": "https://github.com/zhaorw02/DeepMesh",
      "ai_keywords": [
        "triangle meshes",
        "auto-regressive methods",
        "discrete vertex tokens",
        "face counts",
        "mesh incompleteness",
        "DeepMesh",
        "tokenization algorithm",
        "data curation",
        "data processing",
        "Reinforcement Learning (RL)",
        "Direct Preference Optimization (DPO)",
        "human evaluation",
        "3D metrics",
        "point clouds",
        "intricate details",
        "precise topology",
        "state-of-the-art methods",
        "precision",
        "quality"
      ]
    },
    "publishedAt": "2025-03-19T10:39:30.000Z",
    "title": "DeepMesh: Auto-Regressive Artist-mesh Creation with Reinforcement\n  Learning",
    "summary": "Triangle meshes play a crucial role in 3D applications for efficient\nmanipulation and rendering. While auto-regressive methods generate structured\nmeshes by predicting discrete vertex tokens, they are often constrained by\nlimited face counts and mesh incompleteness. To address these challenges, we\npropose DeepMesh, a framework that optimizes mesh generation through two key\ninnovations: (1) an efficient pre-training strategy incorporating a novel\ntokenization algorithm, along with improvements in data curation and\nprocessing, and (2) the introduction of Reinforcement Learning (RL) into 3D\nmesh generation to achieve human preference alignment via Direct Preference\nOptimization (DPO). We design a scoring standard that combines human evaluation\nwith 3D metrics to collect preference pairs for DPO, ensuring both visual\nappeal and geometric accuracy. Conditioned on point clouds and images, DeepMesh\ngenerates meshes with intricate details and precise topology, outperforming\nstate-of-the-art methods in both precision and quality. Project page:\nhttps://zhaorw02.github.io/DeepMesh/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.15265.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6522e4fbd89bc7773ddc4b58",
      "avatarUrl": "/avatars/3e9b158af52c5f738a3eae72dcbb3824.svg",
      "fullname": "Ruowen Zhao",
      "name": "zzzrw",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.13288",
      "authors": [
        {
          "_id": "67dbc49d85eacb364e913c38",
          "name": "Fangzhi Xu",
          "hidden": false
        },
        {
          "_id": "67dbc49d85eacb364e913c39",
          "user": {
            "_id": "67dbe3d969655e406fda64b8",
            "avatarUrl": "/avatars/6053c84e32d0e46dd1e490c493f766ed.svg",
            "isPro": false,
            "fullname": "Mei Tuan",
            "user": "Meituannnnnn",
            "type": "user"
          },
          "name": "Hang Yan",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-20T09:48:32.179Z",
          "hidden": false
        },
        {
          "_id": "67dbc49d85eacb364e913c3a",
          "name": "Chang Ma",
          "hidden": false
        },
        {
          "_id": "67dbc49d85eacb364e913c3b",
          "name": "Haiteng Zhao",
          "hidden": false
        },
        {
          "_id": "67dbc49d85eacb364e913c3c",
          "name": "Jun Liu",
          "hidden": false
        },
        {
          "_id": "67dbc49d85eacb364e913c3d",
          "name": "Qika Lin",
          "hidden": false
        },
        {
          "_id": "67dbc49d85eacb364e913c3e",
          "name": "Zhiyong Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-17T15:38:33.000Z",
      "submittedOnDailyAt": "2025-03-20T06:08:48.330Z",
      "title": "φ-Decoding: バランスづけた推論時間の探索と採用の適応的な先見的サンプリング",
      "submittedOnDailyBy": {
        "_id": "64e6cf78ecce34cb442dc889",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e6cf78ecce34cb442dc889/qVZFiUEpBpSkmH8SQeinm.jpeg",
        "isPro": false,
        "fullname": "Fangzhi Xu",
        "user": "xufangzhi",
        "type": "user"
      },
      "summary": "推論時の最適化は、計算をスケール化し、効果的な性能を確保するための謹密な理由のステップを得ることにより、推論の効率化を実現します。過去の探索基準のスティラテジは、自動帰戻生成の短視性を解決しましたが、巨大な探索空間は、過剰な探索と十分な開発の不足により、最適なステップを得るためのより効率的なバランスを維持することが困難になりました。最適なステップを得るためのより効率的なバランスを維持するために、我々は、未来のステップを計算して全局的に最適なステップを推定するためのフォレスキャンサンプリングを採用し、これにより、新しい解碼スティラテジを提案しました。このフォレスキャンサンプリングにより、ステップの値を正確かつ表現力のある評価を得ることができます。このために、フォレスキャンとクラスタリングを用いて2つの分布を近似し、この共分布からサンプリングを行い、最適なステップを選択して開発することができます。推論の効率化を支援するために、我々は、推論の効率化を実現するための軽量ソリューションを採用し、これにより、適応的な計算アロケーションを実現するためのin-widthとin-depthのプラウンジングスティラテジを提案しました。7つのベンチマークの幅広い範囲での試験は、phi-Decodingは効率と性能の両方で強いベースラインを上回りました。進展分析は、この手法が様々なLLMsにより一般化され、広い範囲の計算バケットでのスケーラビリティを示しました。コードは、https://github.com/xufangzhi/phi-Decodingで公開され、このファイルをもつオープンソースPyPIパッケージも即ち公開されます。",
      "upvotes": 24,
      "discussionId": "67dbc49f85eacb364e913d20",
      "githubRepo": "https://github.com/xufangzhi/phi-Decoding",
      "ai_keywords": [
        "inference-time optimization",
        "auto-regressive generation",
        "foresight sampling",
        "$\\phi$-Decoding",
        "joint distribution",
        "in-width and in-depth pruning",
        "LLMs (Large Language Models)"
      ]
    },
    "publishedAt": "2025-03-17T11:38:33.000Z",
    "title": "φ-Decoding: Adaptive Foresight Sampling for Balanced Inference-Time\n  Exploration and Exploitation",
    "summary": "Inference-time optimization scales computation to derive deliberate reasoning\nsteps for effective performance. While previous search-based strategies address\nthe short-sightedness of auto-regressive generation, the vast search space\nleads to excessive exploration and insufficient exploitation. To strike an\nefficient balance to derive the optimal step, we frame the decoding strategy as\nforesight sampling, leveraging simulated future steps to obtain globally\noptimal step estimation. Built on it, we propose a novel decoding strategy,\nnamed phi-Decoding. To provide a precise and expressive estimation of step\nvalue, phi-Decoding approximates two distributions via foresight and\nclustering. Sampling from the joint distribution, the optimal steps can be\nselected for exploitation. To support adaptive computation allocation, we\npropose in-width and in-depth pruning strategies, featuring a light-weight\nsolution to achieve inference efficiency. Extensive experiments across seven\nbenchmarks show phi-Decoding outperforms strong baselines in both\nperformance and efficiency. Additional analysis demonstrates its generalization\nacross various LLMs and scalability across a wide range of computing budgets.\nThe code will be released at https://github.com/xufangzhi/phi-Decoding, and the\nopen-source PyPI package is coming soon.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13288.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64e6cf78ecce34cb442dc889",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e6cf78ecce34cb442dc889/qVZFiUEpBpSkmH8SQeinm.jpeg",
      "fullname": "Fangzhi Xu",
      "name": "xufangzhi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.15485",
      "authors": [
        {
          "_id": "67db7dd224fe67fe45b21e63",
          "name": "Zineng Tang",
          "hidden": false
        },
        {
          "_id": "67db7dd224fe67fe45b21e64",
          "name": "Long Lian",
          "hidden": false
        },
        {
          "_id": "67db7dd224fe67fe45b21e65",
          "name": "Seun Eisape",
          "hidden": false
        },
        {
          "_id": "67db7dd224fe67fe45b21e66",
          "name": "XuDong Wang",
          "hidden": false
        },
        {
          "_id": "67db7dd224fe67fe45b21e67",
          "name": "Roei Herzig",
          "hidden": false
        },
        {
          "_id": "67db7dd224fe67fe45b21e68",
          "name": "Adam Yala",
          "hidden": false
        },
        {
          "_id": "67db7dd224fe67fe45b21e69",
          "name": "Alane Suhr",
          "hidden": false
        },
        {
          "_id": "67db7dd224fe67fe45b21e6a",
          "name": "Trevor Darrell",
          "hidden": false
        },
        {
          "_id": "67db7dd224fe67fe45b21e6b",
          "name": "David M. Chan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-19T17:58:57.000Z",
      "submittedOnDailyAt": "2025-03-20T01:01:18.127Z",
      "title": "TULIP: 言語画像の統合的な予ち練習に向けて",
      "submittedOnDailyBy": {
        "_id": "6388f68c43d8b0797a09ff84",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669920369989-noauth.jpeg",
        "isPro": false,
        "fullname": "David Chan",
        "user": "davidchan",
        "type": "user"
      },
      "summary": "最近のCLIPやSigLIPの成功にもかかわらず、これらのモデルは画像理解の高精度が求められる視覚中心的なタスク（例えば、数え上げ、深さ推定、細かい物体識別）に適応しやすく、これらのモデルは語言のアライメントを行うことで、高レベルの語義を優先し、画像理解を弱化してしまう。一方、視覚フォーカスモデルは視覚情報の処理に特化しているが、語言の理解に苦戦し、言語駆動タスクの柔軟性を制限している。本研究では、TULIPというオープンソースの、CLIPようなモデルのドロップインの置換として紹介します。私たちの方法は、生成データの拡張、強化された画像-画像と語言-語言の対比的学習、画像/語言の再構成正規化を利用して、細かい視覚特徴を学習しながら、全体的な語義のアライメントを保つことを目的としています。私たちのアプローチは、10億以上のパラメーターを持つことで、複数のベンチマークで現在の最先端（SOTA）モデルを超え、ImageNet-1Kでのゼロショット性能を新たなSOTAとし、RxRx1での少ショット分類の線形プロビングでSigLIPより2倍以上の向上を収め、視覚-語言モデルの向上を実現し、MMVPでSigLIPより3倍以上のスコアを収めました。コード/チェックポイントは、https://tulip-berkeley.github.io から利用可能です。",
      "upvotes": 18,
      "discussionId": "67db7dd424fe67fe45b21ee1",
      "projectPage": "https://tulip-berkeley.github.io/",
      "ai_keywords": [
        "generative data augmentation",
        "enhanced image-image and text-text contrastive learning",
        "image/text reconstruction regularization",
        "fine-grained visual features",
        "global semantic alignment",
        "zero-shot performance",
        "few-shot classification",
        "vision-language models"
      ]
    },
    "publishedAt": "2025-03-19T13:58:57.000Z",
    "title": "TULIP: Towards Unified Language-Image Pretraining",
    "summary": "Despite the recent success of image-text contrastive models like CLIP and\nSigLIP, these models often struggle with vision-centric tasks that demand\nhigh-fidelity image understanding, such as counting, depth estimation, and\nfine-grained object recognition. These models, by performing language\nalignment, tend to prioritize high-level semantics over visual understanding,\nweakening their image understanding. On the other hand, vision-focused models\nare great at processing visual information but struggle to understand language,\nlimiting their flexibility for language-driven tasks. In this work, we\nintroduce TULIP, an open-source, drop-in replacement for existing CLIP-like\nmodels. Our method leverages generative data augmentation, enhanced image-image\nand text-text contrastive learning, and image/text reconstruction\nregularization to learn fine-grained visual features while preserving global\nsemantic alignment. Our approach, scaling to over 1B parameters, outperforms\nexisting state-of-the-art (SOTA) models across multiple benchmarks,\nestablishing a new SOTA zero-shot performance on ImageNet-1K, delivering up to\na 2times enhancement over SigLIP on RxRx1 in linear probing for few-shot\nclassification, and improving vision-language models, achieving over 3times\nhigher scores than SigLIP on MMVP. Our code/checkpoints are available at\nhttps://tulip-berkeley.github.io",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.15485.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6388f68c43d8b0797a09ff84",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669920369989-noauth.jpeg",
      "fullname": "David Chan",
      "name": "davidchan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.15475",
      "authors": [
        {
          "_id": "67db729fa720e711cff4d205",
          "name": "Foundation AI Team",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d206",
          "name": "Kiran Bhat",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d207",
          "name": "Nishchaie Khanna",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d208",
          "name": "Karun Channa",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d209",
          "name": "Tinghui Zhou",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d20a",
          "name": "Yiheng Zhu",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d20b",
          "name": "Xiaoxia Sun",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d20c",
          "name": "Charles Shang",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d20d",
          "name": "Anirudh Sudarshan",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d20e",
          "name": "Maurice Chu",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d20f",
          "name": "Daiqing Li",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d210",
          "name": "Kangle Deng",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d211",
          "name": "Jean-Philippe Fauconnier",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d212",
          "name": "Tijmen Verhulsdonck",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d213",
          "name": "Maneesh Agrawala",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d214",
          "name": "Kayvon Fatahalian",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d215",
          "name": "Alexander Weiss",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d216",
          "name": "Christian Reiser",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d217",
          "name": "Ravi Kiran Chirravuri",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d218",
          "name": "Ravali Kandur",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d219",
          "name": "Alejandro Pelaez",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d21a",
          "name": "Akash Garg",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d21b",
          "name": "Michael Palleschi",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d21c",
          "name": "Jessica Wang",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d21d",
          "name": "Skylar Litz",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d21e",
          "name": "Leon Liu",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d21f",
          "name": "Anying Li",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d220",
          "name": "David Harmon",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d221",
          "name": "Derek Liu",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d222",
          "name": "Liangjun Feng",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d223",
          "name": "Denis Goupil",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d224",
          "name": "Lukas Kuczynski",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d225",
          "name": "Jihyun Yoon",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d226",
          "name": "Naveen Marri",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d227",
          "name": "Peiye Zhuang",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d228",
          "name": "Yinan Zhang",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d229",
          "name": "Brian Yin",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d22a",
          "name": "Haomiao Jiang",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d22b",
          "name": "Marcel van Workum",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d22c",
          "name": "Thomas Lane",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d22d",
          "name": "Bryce Erickson",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d22e",
          "name": "Salil Pathare",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d22f",
          "name": "Kyle Price",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d230",
          "name": "Anupam Singh",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d231",
          "name": "David Baszucki",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-19T17:52:17.000Z",
      "submittedOnDailyAt": "2025-03-20T00:57:52.833Z",
      "title": "Cube: ロボロックで見る3次元知能",
      "submittedOnDailyBy": {
        "_id": "62cd5c43299c0c2e0e437842",
        "avatarUrl": "/avatars/8fa8511baf2d9bd95d3ba4535a5b3d69.svg",
        "isPro": false,
        "fullname": "Jean-Philippe Fauconnier",
        "user": "j4kn",
        "type": "user"
      },
      "summary": "データの大量に基づいて訓練された基盤モデルは、文章、画像、音声、ビデオの領域で驚異的な理由と生成能力を示しています。我々のRobloxの目標は、3D情報の基盤モデルを構築することです。このモデルは、3Dオブジェクトやスペースの生成、アニメーションのためのチャラクターのロープ、オブジェクトの行動を記述するプログラミングスクリプトの作成のすべての面で開発者をサポートすることを目指しています。このような3D基盤モデルの3つのキーデザイン要求について議論し、このモデルの構築の最初のステップを紹介します。3D幾何形状が核心データ型として重要であることを期待し、3D形状トーキナイザーの解決策を説明します。我々のトーキナイザーシナリオは、文章から形状、形状から文章、文章からスペースの生成のようなアプリケーションにどのように利用できるかを示します。これらのアプリケーションは、現在の大規模な言語モデル（LLMs）と協力してスペース解析と理由を行うことを示します。最後に、3D情報の完全な基盤モデルの構築のパスを議論します。",
      "upvotes": 16,
      "discussionId": "67db72a1a720e711cff4d292",
      "githubRepo": "https://github.com/Roblox/cube",
      "ai_keywords": [
        "3D foundation model",
        "3D geometric shapes",
        "3D shape tokenizer",
        "text-to-shape generation",
        "shape-to-text generation",
        "text-to-scene generation",
        "large language models (LLMs)",
        "scene analysis",
        "reasoning",
        "unified foundation model"
      ]
    },
    "publishedAt": "2025-03-19T13:52:17.000Z",
    "title": "Cube: A Roblox View of 3D Intelligence",
    "summary": "Foundation models trained on vast amounts of data have demonstrated\nremarkable reasoning and generation capabilities in the domains of text,\nimages, audio and video. Our goal at Roblox is to build such a foundation model\nfor 3D intelligence, a model that can support developers in producing all\naspects of a Roblox experience, from generating 3D objects and scenes to\nrigging characters for animation to producing programmatic scripts describing\nobject behaviors. We discuss three key design requirements for such a 3D\nfoundation model and then present our first step towards building such a model.\nWe expect that 3D geometric shapes will be a core data type and describe our\nsolution for 3D shape tokenizer. We show how our tokenization scheme can be\nused in applications for text-to-shape generation, shape-to-text generation and\ntext-to-scene generation. We demonstrate how these applications can collaborate\nwith existing large language models (LLMs) to perform scene analysis and\nreasoning. We conclude with a discussion outlining our path to building a fully\nunified foundation model for 3D intelligence.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.15475.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62cd5c43299c0c2e0e437842",
      "avatarUrl": "/avatars/8fa8511baf2d9bd95d3ba4535a5b3d69.svg",
      "fullname": "Jean-Philippe Fauconnier",
      "name": "j4kn",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.14868",
      "authors": [
        {
          "_id": "67db9f06842d8b6642a5eeaf",
          "name": "Hoigi Seo",
          "hidden": false
        },
        {
          "_id": "67db9f06842d8b6642a5eeb0",
          "name": "Wongi Jeong",
          "hidden": false
        },
        {
          "_id": "67db9f06842d8b6642a5eeb1",
          "name": "Kyungryeol Lee",
          "hidden": false
        },
        {
          "_id": "67db9f06842d8b6642a5eeb2",
          "name": "Se Young Chun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-19T03:45:37.000Z",
      "submittedOnDailyAt": "2025-03-20T03:25:51.779Z",
      "title": "効率的なバックプロパゲーションなしの減量化ディフュージョンモデルの個別化",
      "submittedOnDailyBy": {
        "_id": "633e6f07309a99325095dd42",
        "avatarUrl": "/avatars/57b91a488ac1745b3c0509c04eb6ad93.svg",
        "isPro": false,
        "fullname": "Hoigi Seo",
        "user": "Agorium",
        "type": "user"
      },
      "summary": "ディフュージョンモデルは画像合成において驚異な性能を示していますが、訓練、微調節と推論に必要な計算量とメモリリソースが大きいです。進歩的なクオンテーショナル化技術は推論時のメモリ使用量を最小限に抑えることができましたが、これらのクオンテーショナル化モデルの訓練と微調節には、勾配の正確な計算や勾配基盤アルゴリズムのバックプロパゲーションによるメモリの大量の使用が必要です。しかし、メモリ効率的な微調節は、プライベートデータを扱うモバイルデバイスなどのエッジデバイス上で実行する個人化などのアプリケーションに特に望ましいです。本論文では、テキストインバージョンによる個人化を採用したディフュージョンモデルをクオンテーショナル化し、個人化トークンのゼロスタード最適化を活用してディフォーマションモデルの個人化を実現することで、バックプロパゲーションによる勾配と活性化のメモリ使用を不要にします。ゼロスタード最適化による勾配推定は、個人化のための一つまたは複数の画像に対してはほとんどの場合噪音が多いため、過去のトークンの歴史に基づいた次元空間にプロジェクトしてノイズを除去することで推定された勾配をディフュージョンではなく、これを「Subspace Gradient」と呼んでいます。また、画像生成におけるテキスト埋め込みの影響を調査し、これにより、「Partial Uniform Timestep Sampling」と呼んでいる、効果的なディフュージョン時間ステップをサンプリングする方法を提案しました。本方法は、個人化のためのフォワードパスだけで画像とテキストの一致スコアに比較的な性能を達成し、訓練メモリの使用量を8.2倍減らしました。",
      "upvotes": 15,
      "discussionId": "67db9f11842d8b6642a5f165",
      "projectPage": "https://ignoww.github.io/ZOODiP_project/",
      "githubRepo": "https://github.com/ignoww/ZOODiP",
      "ai_keywords": [
        "diffusion models",
        "image synthesis",
        "quantization techniques",
        "dequantization",
        "gradient-based algorithms",
        "memory-efficient fine-tuning",
        "Textual Inversion",
        "zeroth-order optimization",
        "personalization tokens",
        "gradient estimation",
        "Subspace Gradient",
        "subspace projection",
        "text embedding",
        "Partial Uniform Timestep Sampling",
        "diffusion timesteps",
        "Stable Diffusion",
        "image and text alignment scores"
      ]
    },
    "publishedAt": "2025-03-18T23:45:37.000Z",
    "title": "Efficient Personalization of Quantized Diffusion Model without\n  Backpropagation",
    "summary": "Diffusion models have shown remarkable performance in image synthesis, but\nthey demand extensive computational and memory resources for training,\nfine-tuning and inference. Although advanced quantization techniques have\nsuccessfully minimized memory usage for inference, training and fine-tuning\nthese quantized models still require large memory possibly due to\ndequantization for accurate computation of gradients and/or backpropagation for\ngradient-based algorithms. However, memory-efficient fine-tuning is\nparticularly desirable for applications such as personalization that often must\nbe run on edge devices like mobile phones with private data. In this work, we\naddress this challenge by quantizing a diffusion model with personalization via\nTextual Inversion and by leveraging a zeroth-order optimization on\npersonalization tokens without dequantization so that it does not require\ngradient and activation storage for backpropagation that consumes considerable\nmemory. Since a gradient estimation using zeroth-order optimization is quite\nnoisy for a single or a few images in personalization, we propose to denoise\nthe estimated gradient by projecting it onto a subspace that is constructed\nwith the past history of the tokens, dubbed Subspace Gradient. In addition, we\ninvestigated the influence of text embedding in image generation, leading to\nour proposed time steps sampling, dubbed Partial Uniform Timestep Sampling for\nsampling with effective diffusion timesteps. Our method achieves comparable\nperformance to prior methods in image and text alignment scores for\npersonalizing Stable Diffusion with only forward passes while reducing training\nmemory demand up to 8.2times.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14868.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "633e6f07309a99325095dd42",
      "avatarUrl": "/avatars/57b91a488ac1745b3c0509c04eb6ad93.svg",
      "fullname": "Hoigi Seo",
      "name": "Agorium",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.15417",
      "authors": [
        {
          "_id": "67db8e05842d8b6642a135d0",
          "name": "Harold Haodong Chen",
          "hidden": false
        },
        {
          "_id": "67db8e05842d8b6642a135d1",
          "name": "Haojian Huang",
          "hidden": false
        },
        {
          "_id": "67db8e05842d8b6642a135d2",
          "name": "Xianfeng Wu",
          "hidden": false
        },
        {
          "_id": "67db8e05842d8b6642a135d3",
          "name": "Yexin Liu",
          "hidden": false
        },
        {
          "_id": "67db8e05842d8b6642a135d4",
          "name": "Yajing Bai",
          "hidden": false
        },
        {
          "_id": "67db8e05842d8b6642a135d5",
          "name": "Wen-Jie Shu",
          "hidden": false
        },
        {
          "_id": "67db8e05842d8b6642a135d6",
          "name": "Harry Yang",
          "hidden": false
        },
        {
          "_id": "67db8e05842d8b6642a135d7",
          "name": "Ser-Nam Lim",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-19T16:59:32.000Z",
      "submittedOnDailyAt": "2025-03-20T02:10:52.068Z",
      "title": "時間正規化がビデオジェネレーターを強くする",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "時間質はビデオ生成の重要な面であり、フレーム間の一致した動きとリアルな動作を確保します。しかし、高い時間的な一致性と多様性を達成するのは難しいです。本稿では、時間的なアウェイティングをビデオ生成に最初に調査し、FluxFlowを紹介します。FluxFlowは時間質を向上させるための戦略で、データレベルで操作し、構造設計の変更が必要とならないように設計されています。UCF-101とVBenchベンチマークに対しての拡張的な実験は、FluxFlowがU-Net、DiT、およびARベースの構造において時間的な一致性と多様性を大幅に向上させ、空間的な忠実性を保っていることを示しました。これらの発見は、時間的なアウェイティングが簡単で効果的なビデオ生成の質を向上させる手法の可能性を示しています。",
      "upvotes": 12,
      "discussionId": "67db8e07842d8b6642a1365f",
      "ai_keywords": [
        "temporal augmentation",
        "FluxFlow",
        "temporal perturbations",
        "temporal quality",
        "temporal coherence",
        "UCF-101",
        "VBench",
        "U-Net",
        "DiT",
        "AR-based architectures",
        "spatial fidelity"
      ]
    },
    "publishedAt": "2025-03-19T12:59:32.000Z",
    "title": "Temporal Regularization Makes Your Video Generator Stronger",
    "summary": "Temporal quality is a critical aspect of video generation, as it ensures\nconsistent motion and realistic dynamics across frames. However, achieving high\ntemporal coherence and diversity remains challenging. In this work, we explore\ntemporal augmentation in video generation for the first time, and introduce\nFluxFlow for initial investigation, a strategy designed to enhance temporal\nquality. Operating at the data level, FluxFlow applies controlled temporal\nperturbations without requiring architectural modifications. Extensive\nexperiments on UCF-101 and VBench benchmarks demonstrate that FluxFlow\nsignificantly improves temporal coherence and diversity across various video\ngeneration models, including U-Net, DiT, and AR-based architectures, while\npreserving spatial fidelity. These findings highlight the potential of temporal\naugmentation as a simple yet effective approach to advancing video generation\nquality.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.15417.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6408
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.12532",
      "authors": [
        {
          "_id": "67da1df040371958e1732c83",
          "name": "Fanbin Lu",
          "hidden": false
        },
        {
          "_id": "67da1df040371958e1732c84",
          "name": "Zhisheng Zhong",
          "hidden": false
        },
        {
          "_id": "67da1df040371958e1732c85",
          "name": "Ziqin Wei",
          "hidden": false
        },
        {
          "_id": "67da1df040371958e1732c86",
          "name": "Shu Liu",
          "hidden": false
        },
        {
          "_id": "67da1df040371958e1732c87",
          "name": "Chi-Wing Fu",
          "hidden": false
        },
        {
          "_id": "67da1df040371958e1732c88",
          "name": "Jiaya Jia",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-16T14:53:43.000Z",
      "submittedOnDailyAt": "2025-03-20T01:38:29.350Z",
      "title": "ステビー: コンピューター使用アグエントの訓練に向けてのステップバイステップの認証パイプライン",
      "submittedOnDailyBy": {
        "_id": "6418554a0956be7233a1023e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6418554a0956be7233a1023e/9EKN0GoOpcDbvBDmAQEJf.png",
        "isPro": false,
        "fullname": "zhang yuechen",
        "user": "julianjuaner",
        "type": "user"
      },
      "summary": "AIエージェントを自動的にグラフィックユーザーインターフェイスを操作することは長い難しい任務です。データスケーリングの最近の進展は、スケードされたインストラクションセットを用いてコンピューター使用エージェントを訓練することを促しますが、行動クローニングを用いてエージェントを訓練するには、大規模な高品質のトラジェクトが必要です。スケーラビリティの需要を満たすために、私たちは、コンピューター使用エージェントの訓練に向けてステブ（STEVE）というステップバーケットを設計しました。まず、コンピューター使用エージェントに大規模なインストラクションセットを設定し、一部のサブオプティマルなエージェントを用いてトラジェクトデータを収集しました。GPT-4oは、行動実行の前後のスクリーンに基づいてトラジェクトの各ステップの正確性を確認し、各ステップに二値ラベルを割り当てます。最後に、Kahneman and Tversky Optimizationを用いて、二値ステップラベルからエージェントを最適化しました。拡張的な実験は、トラジェクト内の正のと負の行動を両方利用することで、我々のエージェントが規範的な微調節よりも優れていることを明らかにしました。また、ステブは、大幅な効率と削減されたコストで、7Bの視覚言語モデルをコンピューター使用エージェントとして訓練することができ、難しいリビーデスクトップ環境でのWinAgentArenaで先駆的な性能を達成しました。コードとデータは、https://github.com/FanbinLu/STEVEにあります。",
      "upvotes": 8,
      "discussionId": "67da1df240371958e1732d2f",
      "githubRepo": "https://github.com/FanbinLu/STEVE",
      "ai_keywords": [
        "behavior cloning",
        "trajectory data",
        "suboptimal agents",
        "GPT-4o",
        "step verification pipeline",
        "correctness verification",
        "Kahneman and Tversky Optimization",
        "positive actions",
        "negative actions",
        "vision-language model",
        "computer-use agent",
        "live desktop environment",
        "WinAgentArena"
      ]
    },
    "publishedAt": "2025-03-16T10:53:43.000Z",
    "title": "STEVE: AStep Verification Pipeline for Computer-use Agent Training",
    "summary": "Developing AI agents to autonomously manipulate graphical user interfaces is\na long challenging task. Recent advances in data scaling law inspire us to\ntrain computer-use agents with a scaled instruction set, yet using behavior\ncloning to train agents still requires immense high-quality trajectories. To\nmeet the scalability need, we designed STEVE, a step verification pipeline for\ncomputer-use agent training. First, we establish a large instruction set for\ncomputer-use agents and collect trajectory data with some suboptimal agents.\nGPT-4o is used to verify the correctness of each step in the trajectories based\non the screens before and after the action execution, assigning each step with\na binary label. Last, we adopt the Kahneman and Tversky Optimization to\noptimize the agent from the binary stepwise labels. Extensive experiments\nmanifest that our agent outperforms supervised finetuning by leveraging both\npositive and negative actions within a trajectory. Also, STEVE enables us to\ntrain a 7B vision-language model as a computer-use agent, achieving leading\nperformance in the challenging live desktop environment WinAgentArena with\ngreat efficiency at a reduced cost. Code and data:\nhttps://github.com/FanbinLu/STEVE.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12532.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6418554a0956be7233a1023e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6418554a0956be7233a1023e/9EKN0GoOpcDbvBDmAQEJf.png",
      "fullname": "zhang yuechen",
      "name": "julianjuaner",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.15264",
      "authors": [
        {
          "_id": "67dbbe8fafd5251fc6b55730",
          "name": "Hengrui Kang",
          "hidden": false
        },
        {
          "_id": "67dbbe8fafd5251fc6b55731",
          "name": "Siwei Wen",
          "hidden": false
        },
        {
          "_id": "67dbbe8fafd5251fc6b55732",
          "name": "Zichen Wen",
          "hidden": false
        },
        {
          "_id": "67dbbe8fafd5251fc6b55733",
          "name": "Junyan Ye",
          "hidden": false
        },
        {
          "_id": "67dbbe8fafd5251fc6b55734",
          "name": "Weijia Li",
          "hidden": false
        },
        {
          "_id": "67dbbe8fafd5251fc6b55735",
          "name": "Peilin Feng",
          "hidden": false
        },
        {
          "_id": "67dbbe8fafd5251fc6b55736",
          "name": "Baichuan Zhou",
          "hidden": false
        },
        {
          "_id": "67dbbe8fafd5251fc6b55737",
          "name": "Bin Wang",
          "hidden": false
        },
        {
          "_id": "67dbbe8fafd5251fc6b55738",
          "name": "Dahua Lin",
          "hidden": false
        },
        {
          "_id": "67dbbe8fafd5251fc6b55739",
          "name": "Linfeng Zhang",
          "hidden": false
        },
        {
          "_id": "67dbbe8fafd5251fc6b5573a",
          "name": "Conghui He",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-19T14:37:21.000Z",
      "submittedOnDailyAt": "2025-03-20T05:54:43.430Z",
      "title": "レジオン: 合成画像検出のための学習、基底化、説明",
      "submittedOnDailyBy": {
        "_id": "653b8c3e97a4d71d950e2f20",
        "avatarUrl": "/avatars/b68880022e14556d0be58c69615db3be.svg",
        "isPro": false,
        "fullname": "Zichen Wen",
        "user": "zichenwen",
        "type": "user"
      },
      "summary": "生成技術の急速な進歩は、二面刃の剣として現れています。便利性を向上させる強力なツールを提供しながら、大きな社会的な懸念を抱えています。守り手として現在の合成画像検出方法は、アーティファクトレベルの文字的な解釈可能性を欠くことが多いです。また、画像操作検出に過度に焦点を当てていて、現在のデータセットは通常、更新されていないジェネレーターと細かなアノテーションの欠如で苦戦しています。本論文では、SynthScarsという高品質で多様なデータセットを介して、12,236枚の完全な合成画像を含むものを紹介します。これは4種類の異なる画像内容タイプ、3つのアーティファクトカテゴリ、ピクセルレベルの分割、詳細な文字的な解釈、アーティファクトカテゴリラベルを含む細かなアノテーションを特徴としています。また、LEGION（LEarning to Ground and explain for Synthetic Image detectiON）という多モデルの大規模な言語モデル（MLLM）に基づく画像偽造分析フレームワークを提案し、アーティファクト検出、分割、解釈を統合しています。この能力を基に、LEGIONをコントローラーとして拡張し、画像精修パイプラインに統合して、高品質で更に写実な画像の生成をガイドすることを試みます。拡張した実験は、LEGIONは多くのベンチマークで現在の方法を上回っていることを示し、SynthScarsではmIoUで3.31%、F1スコアで7.75%により最善の従来の専門家を上回っています。また、LEGIONの指導下で生成された精選画像は、人間の好みとのより強い一致を示しています。コード、モデル、データセットはリリースされます。",
      "upvotes": 6,
      "discussionId": "67dbbe92afd5251fc6b55825",
      "projectPage": "https://opendatalab.github.io/LEGION",
      "githubRepo": "https://github.com/opendatalab/LEGION",
      "ai_keywords": [
        "SynthScars",
        "LEGION",
        "multimodal large language model",
        "image forgery analysis framework",
        "artifact detection",
        "segmentation",
        "explanation",
        "mIoU",
        "F1 score"
      ]
    },
    "publishedAt": "2025-03-19T10:37:21.000Z",
    "title": "LEGION: Learning to Ground and Explain for Synthetic Image Detection",
    "summary": "The rapid advancements in generative technology have emerged as a\ndouble-edged sword. While offering powerful tools that enhance convenience,\nthey also pose significant social concerns. As defenders, current synthetic\nimage detection methods often lack artifact-level textual interpretability and\nare overly focused on image manipulation detection, and current datasets\nusually suffer from outdated generators and a lack of fine-grained annotations.\nIn this paper, we introduce SynthScars, a high-quality and diverse dataset\nconsisting of 12,236 fully synthetic images with human-expert annotations. It\nfeatures 4 distinct image content types, 3 categories of artifacts, and\nfine-grained annotations covering pixel-level segmentation, detailed textual\nexplanations, and artifact category labels. Furthermore, we propose LEGION\n(LEarning to Ground and explain for Synthetic Image detectiON), a multimodal\nlarge language model (MLLM)-based image forgery analysis framework that\nintegrates artifact detection, segmentation, and explanation. Building upon\nthis capability, we further explore LEGION as a controller, integrating it into\nimage refinement pipelines to guide the generation of higher-quality and more\nrealistic images. Extensive experiments show that LEGION outperforms existing\nmethods across multiple benchmarks, particularly surpassing the second-best\ntraditional expert on SynthScars by 3.31% in mIoU and 7.75% in F1 score.\nMoreover, the refined images generated under its guidance exhibit stronger\nalignment with human preferences. The code, model, and dataset will be\nreleased.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.15264.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "653b8c3e97a4d71d950e2f20",
      "avatarUrl": "/avatars/b68880022e14556d0be58c69615db3be.svg",
      "fullname": "Zichen Wen",
      "name": "zichenwen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.14505",
      "authors": [
        {
          "_id": "67db13f71956dcedf0b4d357",
          "name": "Susung Hong",
          "hidden": false
        },
        {
          "_id": "67db13f71956dcedf0b4d358",
          "name": "Ira Kemelmacher-Shlizerman",
          "hidden": false
        },
        {
          "_id": "67db13f71956dcedf0b4d359",
          "name": "Brian Curless",
          "hidden": false
        },
        {
          "_id": "67db13f71956dcedf0b4d35a",
          "name": "Steven M. Seitz",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-18T17:59:58.000Z",
      "submittedOnDailyAt": "2025-03-20T02:39:46.392Z",
      "title": "MusicInfuser: 音楽を含めて映画の拡散聴覚と踊りをする方法",
      "submittedOnDailyBy": {
        "_id": "635a6dd21668c4ead3ed19fa",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1679159537671-635a6dd21668c4ead3ed19fa.jpeg",
        "isPro": false,
        "fullname": "Susung Hong",
        "user": "susunghong",
        "type": "user"
      },
      "summary": "MusicInfuserは、指定された音楽トラックに同期された高品質のダンスビデオの生成を目的としています。新しい多模態音声ビデオモデルの設計と訓練を試みるより、既存のビデオディフュージョンモデルを音楽入力に対応させる方法を示しています。音楽とビデオのクロスアテンションと低レンキングアダプターを導入することで、以前の研究と異なり、モーションキャプチャデータが必要とするものではありません。MusicInfuserは、ダンスビデオだけでの微調節を通じて、音楽を駆動する高品質のビデオ生成を実現し、基礎モデルの柔軟性と生成能力を保っています。Video-LLMsを用いた評価フレームワークを導入し、ダンス生成の品質を複数の次元で評価することができます。プロジェクトページとコードは、https://susunghong.github.io/MusicInfuser から利用できます。",
      "upvotes": 5,
      "discussionId": "67db13fc1956dcedf0b4d470",
      "ai_keywords": [
        "video diffusion models",
        "multimodal audio-video model",
        "music-video cross-attention",
        "low-rank adapter",
        "dance videos",
        "motion capture data",
        "music-driven video generation",
        "Video-LLMs"
      ]
    },
    "publishedAt": "2025-03-18T13:59:58.000Z",
    "title": "MusicInfuser: Making Video Diffusion Listen and Dance",
    "summary": "We introduce MusicInfuser, an approach for generating high-quality dance\nvideos that are synchronized to a specified music track. Rather than attempting\nto design and train a new multimodal audio-video model, we show how existing\nvideo diffusion models can be adapted to align with musical inputs by\nintroducing lightweight music-video cross-attention and a low-rank adapter.\nUnlike prior work requiring motion capture data, our approach fine-tunes only\non dance videos. MusicInfuser achieves high-quality music-driven video\ngeneration while preserving the flexibility and generative capabilities of the\nunderlying models. We introduce an evaluation framework using Video-LLMs to\nassess multiple dimensions of dance generation quality. The project page and\ncode are available at https://susunghong.github.io/MusicInfuser.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14505.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "635a6dd21668c4ead3ed19fa",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1679159537671-635a6dd21668c4ead3ed19fa.jpeg",
      "fullname": "Susung Hong",
      "name": "susunghong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.12769",
      "authors": [
        {
          "_id": "67d8ded81a1b6ae91f79eb18",
          "name": "Shenghao Fu",
          "hidden": false
        },
        {
          "_id": "67d8ded81a1b6ae91f79eb19",
          "name": "Qize Yang",
          "hidden": false
        },
        {
          "_id": "67d8ded81a1b6ae91f79eb1a",
          "name": "Yuan-Ming Li",
          "hidden": false
        },
        {
          "_id": "67d8ded81a1b6ae91f79eb1b",
          "name": "Yi-Xing Peng",
          "hidden": false
        },
        {
          "_id": "67d8ded81a1b6ae91f79eb1c",
          "name": "Kun-Yu Lin",
          "hidden": false
        },
        {
          "_id": "67d8ded81a1b6ae91f79eb1d",
          "name": "Xihan Wei",
          "hidden": false
        },
        {
          "_id": "67d8ded81a1b6ae91f79eb1e",
          "name": "Jian-Fang Hu",
          "hidden": false
        },
        {
          "_id": "67d8ded81a1b6ae91f79eb1f",
          "name": "Xiaohua Xie",
          "hidden": false
        },
        {
          "_id": "67d8ded81a1b6ae91f79eb20",
          "name": "Wei-Shi Zheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-17T03:05:31.000Z",
      "submittedOnDailyAt": "2025-03-20T00:48:47.112Z",
      "title": "ViSpeak: 流れ映画のビジュアルインストラクションフィードバック",
      "submittedOnDailyBy": {
        "_id": "6686044047f2a33570e59e31",
        "avatarUrl": "/avatars/2656bf2cecd6d7cbffd0a912a54d25de.svg",
        "isPro": false,
        "fullname": "Jiaxing Zhao",
        "user": "StarJiaxing",
        "type": "user"
      },
      "summary": "最近の大規模多モーダルモデル（LMMs）の進展は主にオフラインビデオ理解に焦点を当てています。一方、ストリーミングビデオ理解は、時間的性、全モーダル性とインタラクティブな特徴により、最近のモデルに大きな課題を見せています。本研究では、ストリーミングビデオ理解を新しい視点から拡張し、モデルが視覚内容を意識し、それからインストラクションを抽出する能力を学習させる新しいタスク「Visual Instruction Feedback」を提案します。例えば、ユーザーがアウトロードに手を振ると、アウトロードは手の動きを認識し、ビエンチャー情報を含む会話を開始する必要があります。このように、視覚モーダルでのインストラクションを追跡することは、アウトロードとのユーザーインタラクションを大幅に向上させます。研究を促進するために、視覚モーダルに関連した7つのキーサブタスクを定義し、ViSpeak-Instructデータセットを学習用、ViSpeak-Benchを評価用に採集しました。また、ViSpeakモデルを提案します。ViSpeakは、GPT-4oレベルの性能を持つSOTAストリーミングビデオ理解LMMで、学習させることで基本的な視覚インストラクションフィードバック能力を持つことで、将来の研究のための堅固なベースラインとして役立ちます。",
      "upvotes": 4,
      "discussionId": "67d8ded91a1b6ae91f79eb5c",
      "ai_keywords": [
        "Large Multi-modal Models (LMMs)",
        "streaming video understanding",
        "Visual Instruction Feedback",
        "visual contents",
        "instructions",
        "gesture recognition",
        "user-agent interactions",
        "subtasks",
        "ViSpeak-Instruct dataset",
        "ViSpeak-Bench",
        "ViSpeak model",
        "GPT-4o-level performance",
        "streaming video understanding benchmarks",
        "finetuning"
      ]
    },
    "publishedAt": "2025-03-16T23:05:31.000Z",
    "title": "ViSpeak: Visual Instruction Feedback in Streaming Videos",
    "summary": "Recent advances in Large Multi-modal Models (LMMs) are primarily focused on\noffline video understanding. Instead, streaming video understanding poses great\nchallenges to recent models due to its time-sensitive, omni-modal and\ninteractive characteristics. In this work, we aim to extend the streaming video\nunderstanding from a new perspective and propose a novel task named Visual\nInstruction Feedback in which models should be aware of visual contents and\nlearn to extract instructions from them. For example, when users wave their\nhands to agents, agents should recognize the gesture and start conversations\nwith welcome information. Thus, following instructions in visual modality\ngreatly enhances user-agent interactions. To facilitate research, we define\nseven key subtasks highly relevant to visual modality and collect the\nViSpeak-Instruct dataset for training and the ViSpeak-Bench for evaluation.\nFurther, we propose the ViSpeak model, which is a SOTA streaming video\nunderstanding LMM with GPT-4o-level performance on various streaming video\nunderstanding benchmarks. After finetuning on our ViSpeak-Instruct dataset,\nViSpeak is equipped with basic visual instruction feedback ability, serving as\na solid baseline for future research.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12769.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6686044047f2a33570e59e31",
      "avatarUrl": "/avatars/2656bf2cecd6d7cbffd0a912a54d25de.svg",
      "fullname": "Jiaxing Zhao",
      "name": "StarJiaxing",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 13
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.11227",
      "authors": [
        {
          "_id": "67da533bb443470b7908a048",
          "user": {
            "_id": "658be7fe135580745c510323",
            "avatarUrl": "/avatars/830e5cec4565efdc23226a86a0fcef0e.svg",
            "isPro": false,
            "fullname": "Jian Zhang",
            "user": "VentureZJ",
            "type": "user"
          },
          "name": "Jian Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-19T09:44:13.546Z",
          "hidden": false
        },
        {
          "_id": "67da533bb443470b7908a049",
          "name": "Bifan Wei",
          "hidden": false
        },
        {
          "_id": "67da533bb443470b7908a04a",
          "name": "Shihao Qi",
          "hidden": false
        },
        {
          "_id": "67da533bb443470b7908a04b",
          "name": "haiping Zhu",
          "hidden": false
        },
        {
          "_id": "67da533bb443470b7908a04c",
          "name": "Jun Liu",
          "hidden": false
        },
        {
          "_id": "67da533bb443470b7908a04d",
          "name": "Qika Lin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-14T09:23:22.000Z",
      "submittedOnDailyAt": "2025-03-20T06:15:24.085Z",
      "title": "GKG-LLM: 一般化の知識グラフ構築の統一フレームワーク",
      "submittedOnDailyBy": {
        "_id": "658be7fe135580745c510323",
        "avatarUrl": "/avatars/830e5cec4565efdc23226a86a0fcef0e.svg",
        "isPro": false,
        "fullname": "Jian Zhang",
        "user": "VentureZJ",
        "type": "user"
      },
      "summary": "Generalized Knowledge Graph (GKG)の構築、これは知識グラフ、イベント知識グラフ、および常識知識グラフの3つのタイプを含むもので、自然言語処理の多様なタスクに基盤的な役割を果たしている。現在の研究では、これらのグラフの種類を別々に構築し、全体的なインサイトと潜在的な統合可能性を見落としている。しかし、GKGの統一フレームワークの開発においての一連の挑戦は、タスク固有の違いから生じる障害である。本研究では、この挑戦を解決するために、GKGの構築に適した統一フレームワークを提案した。まず、3つのグラフのタイプについての29データセットから15サブタスクのデータを集め、それらをin-sample、counter-task、およびout-of-distribution (OOD)データに分類した。次に、3つのグラフのキャップチャを繰り返しにLarge Language Modelsに注入し、3ステップのカレクリウム学習の微調節フレームワークを提案した。広範囲の実験により、我々の提案モデルはin-domain、OOD、およびcounter-taskデータのすべての3グラフの構築において改善を示した。",
      "upvotes": 3,
      "discussionId": "67da533db443470b7908a0e6",
      "ai_keywords": [
        "knowledge graph",
        "event knowledge graph",
        "commonsense knowledge graph",
        "natural language processing",
        "unified framework",
        "in-sample data",
        "counter-task data",
        "out-of-distribution data",
        "three-stage curriculum learning fine-tuning framework",
        "Large Language Models"
      ]
    },
    "publishedAt": "2025-03-14T05:23:22.000Z",
    "title": "GKG-LLM: A Unified Framework for Generalized Knowledge Graph\n  Construction",
    "summary": "The construction of Generalized Knowledge Graph (GKG), including knowledge\ngraph, event knowledge graph and commonsense knowledge graph, is fundamental\nfor various natural language processing tasks. Current studies typically\nconstruct these types of graph separately, overlooking holistic insights and\npotential unification that could be beneficial in computing resources and usage\nperspectives. However, a key challenge in developing a unified framework for\nGKG is obstacles arising from task-specific differences. In this study, we\npropose a unified framework for constructing generalized knowledge graphs to\naddress this challenge. First, we collect data from 15 sub-tasks in 29 datasets\nacross the three types of graphs, categorizing them into in-sample,\ncounter-task, and out-of-distribution (OOD) data. Then, we propose a\nthree-stage curriculum learning fine-tuning framework, by iteratively injecting\nknowledge from the three types of graphs into the Large Language Models.\nExtensive experiments show that our proposed model improves the construction of\nall three graph types across in-domain, OOD and counter-task data.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.11227.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "658be7fe135580745c510323",
      "avatarUrl": "/avatars/830e5cec4565efdc23226a86a0fcef0e.svg",
      "fullname": "Jian Zhang",
      "name": "VentureZJ",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.13360",
      "authors": [
        {
          "_id": "67d8e21dea26d6d743f2adde",
          "name": "Hai-Long Sun",
          "hidden": false
        },
        {
          "_id": "67d8e21dea26d6d743f2addf",
          "name": "Zhun Sun",
          "hidden": false
        },
        {
          "_id": "67d8e21dea26d6d743f2ade0",
          "name": "Houwen Peng",
          "hidden": false
        },
        {
          "_id": "67d8e21dea26d6d743f2ade1",
          "name": "Han-Jia Ye",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-17T16:45:12.000Z",
      "submittedOnDailyAt": "2025-03-20T04:52:23.426Z",
      "title": "ビジュアルフォーゲットを軽減するためのTake-along Visual Conditioningを用いた多タイプ長期コンテキスト論理",
      "submittedOnDailyBy": {
        "_id": "6623975c728f756224d4b768",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6623975c728f756224d4b768/US9n0k45Y1TF6WqOhgBhZ.jpeg",
        "isPro": false,
        "fullname": "Allen Sun",
        "user": "Allen8",
        "type": "user"
      },
      "summary": "最近の大語言モデル（LLMs）の進展は、Chain-of-Thought（CoT）プロンプティングから進化し、OpenAI o1のような先進的なプロダクト向けの解決策に至っています。このモデルの再実装において、ビジュアル入力を必要とする多タイプタスク（例：幾何問題）で、多タイプLLMs（MLLMs）がビジュアル情報に焦点を当てることが難しくなっていることを見出しました。つまり、理由の進行に伴い、MLLMsはビジュアル情報の注意度の徐々な低下を見落とし、テキストの過度依存の出力を引き起こします。これについて調査するために、長めの理由の進行中に画像入力を削除してテストしました。具體には、理由の進行を中途半端に終了させ、テキスト入力を削除して理由の進行を再開しました。MathVistaのtest-hardセットでは、この操作により精度が約2%だけ低下しましたが、これはモデルのテキスト出力が理由の進行の後続に主導していることを明らかにしました。これに基づいて、Take-along Visual Conditioning（TVC）の戦略を提案しました。この戦略では、画像入力を重要な理由のステージに移し、動的なプローディングによって冗長なビジュアルトークンを削減します。この方法は、理由の進行中にビジュアル成分の注意を保持することを助けます。我々のアプローチは、5つの数学理由ベンチマークで平均的に最先端の性能を収め、TVCが多タイプ理由システムの強化において効果的であることを示しました。",
      "upvotes": 1,
      "discussionId": "67d8e21eea26d6d743f2ae50",
      "ai_keywords": [
        "Chain-of-Thought (CoT)",
        "OpenAI o1",
        "Multimodal LLMs (MLLMs)",
        "attention to visual information",
        "text-over-relied outputs",
        "ablate image inputs",
        "long-chain reasoning",
        "MathVista's test-hard subset",
        "Take-along Visual Conditioning (TVC)",
        "critical reasoning stages",
        "dynamic pruning",
        "multimodal reasoning systems"
      ]
    },
    "publishedAt": "2025-03-17T12:45:12.000Z",
    "title": "Mitigating Visual Forgetting via Take-along Visual Conditioning for\n  Multi-modal Long CoT Reasoning",
    "summary": "Recent advancements in Large Language Models (LLMs) have demonstrated\nenhanced reasoning capabilities, evolving from Chain-of-Thought (CoT) prompting\nto advanced, product-oriented solutions like OpenAI o1. During our\nre-implementation of this model, we noticed that in multimodal tasks requiring\nvisual input (e.g., geometry problems), Multimodal LLMs (MLLMs) struggle to\nmaintain focus on the visual information, in other words, MLLMs suffer from a\ngradual decline in attention to visual information as reasoning progresses,\ncausing text-over-relied outputs. To investigate this, we ablate image inputs\nduring long-chain reasoning. Concretely, we truncate the reasoning process\nmidway, then re-complete the reasoning process with the input image removed. We\nobserve only a ~2% accuracy drop on MathVista's test-hard subset, revealing the\nmodel's textual outputs dominate the following reasoning process. Motivated by\nthis, we propose Take-along Visual Conditioning (TVC), a strategy that shifts\nimage input to critical reasoning stages and compresses redundant visual tokens\nvia dynamic pruning. This methodology helps the model retain attention to the\nvisual components throughout the reasoning. Our approach achieves\nstate-of-the-art performance on average across five mathematical reasoning\nbenchmarks (+3.4% vs previous sota), demonstrating the effectiveness of TVC in\nenhancing multimodal reasoning systems.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13360.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6623975c728f756224d4b768",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6623975c728f756224d4b768/US9n0k45Y1TF6WqOhgBhZ.jpeg",
      "fullname": "Allen Sun",
      "name": "Allen8",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.15055",
      "authors": [
        {
          "_id": "67db9586a2f164ac51f84c72",
          "user": {
            "_id": "641ee9fe632a1ec42caf1fa6",
            "avatarUrl": "/avatars/4efb005ce798ce82d4053e00e0ba9f23.svg",
            "isPro": false,
            "fullname": "Arina Razmyslovich",
            "user": "lavriz",
            "type": "user"
          },
          "name": "Arina Razmyslovich",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-03-20T04:13:32.101Z",
          "hidden": false
        },
        {
          "_id": "67db9586a2f164ac51f84c73",
          "name": "Kseniia Murasheva",
          "hidden": false
        },
        {
          "_id": "67db9586a2f164ac51f84c74",
          "name": "Sofia Sedlova",
          "hidden": false
        },
        {
          "_id": "67db9586a2f164ac51f84c75",
          "name": "Julien Capitaine",
          "hidden": false
        },
        {
          "_id": "67db9586a2f164ac51f84c76",
          "name": "Eugene Dmitriev",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-19T09:46:54.000Z",
      "submittedOnDailyAt": "2025-03-20T02:46:26.189Z",
      "title": "ELTEX: ドメインドリブンド合成データ生成のフレームワーク",
      "submittedOnDailyBy": {
        "_id": "641ee9fe632a1ec42caf1fa6",
        "avatarUrl": "/avatars/4efb005ce798ce82d4053e00e0ba9f23.svg",
        "isPro": false,
        "fullname": "Arina Razmyslovich",
        "user": "lavriz",
        "type": "user"
      },
      "summary": "エルテックス（Efficient LLM Token Extraction）は、専門分野での高品質な合成的な学習データの生成を目的とするドメイン駆動フレームワークです。大型言語モデル（LLMs）は一般的な能力を示していますが、サイバーセキュリティなどの専門分野では、特定のドメインデータの不足が性能の向上に制限をかけています。エルテックスは、専門的な知識を保持することで、効率的な生成プロセスを実現するために、明示的なドメインインディカターの抽出と動的なプロンプティングを体系的に統合しています。ブロックチェーン関連のサイバー攻撃検出のコンテキストでエルテックスの効果性を示しています。ジェマ-2Bを用いて、実際のデータとエルテックスによる生成データの組み合わせを試しています。結果として、エルテックスを拡張したモデルは、標準的なクラス分類ミーターと不確実性補正の両方でGPT-4と同等の性能を達成し、計算資源の使用量を大幅に削減しています。ブロックチェーンのサイバー攻撃検出に関するソーシャルメディアテキストのセレクションされた合成データセットを公開しています。我々の研究は、ドメイン駆動の合成データ生成が、資源効率的なモデルと大きなアーキテクチャの間の性能間違いを効果的に橋渡しできることを示しています。",
      "upvotes": 0,
      "discussionId": "67db958fa2f164ac51f84f51",
      "githubRepo": "https://github.com/1712n/eltex",
      "ai_keywords": [
        "ELTEX",
        "domain-driven framework",
        "high-quality synthetic training data",
        "Large Language Models (LLMs)",
        "cohort indicator extraction",
        "dynamic prompting",
        "critical domain knowledge",
        "blockchain-related cyberattack detection",
        "Gemma-2B",
        "performance competitive",
        "GPT-4",
        "standard classification metrics",
        "uncertainty calibration",
        "computational resources",
        "synthetic dataset",
        "social media texts",
        "domain-driven synthetic data generation",
        "performance gap",
        "resource-efficient models",
        "larger architectures"
      ]
    },
    "publishedAt": "2025-03-19T05:46:54.000Z",
    "title": "ELTEX: A Framework for Domain-Driven Synthetic Data Generation",
    "summary": "We present ELTEX (Efficient LLM Token Extraction), a domain-driven framework\nfor generating high-quality synthetic training data in specialized domains.\nWhile Large Language Models (LLMs) have shown impressive general capabilities,\ntheir performance in specialized domains like cybersecurity remains limited by\nthe scarcity of domain-specific training data. ELTEX addresses this challenge\nby systematically integrating explicit domain indicator extraction with dynamic\nprompting to preserve critical domain knowledge throughout the generation\nprocess. We demonstrate ELTEX's effectiveness in the context of\nblockchain-related cyberattack detection, where we fine-tune Gemma-2B using\nvarious combinations of real and ELTEX-generated data. Our results show that\nthe ELTEX-enhanced model achieves performance competitive with GPT-4 across\nboth standard classification metrics and uncertainty calibration, while\nrequiring significantly fewer computational resources. We release a curated\nsynthetic dataset of social media texts for cyberattack detection in\nblockchain. Our work demonstrates that domain-driven synthetic data generation\ncan effectively bridge the performance gap between resource-efficient models\nand larger architectures in specialized domains.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.15055.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "641ee9fe632a1ec42caf1fa6",
      "avatarUrl": "/avatars/4efb005ce798ce82d4053e00e0ba9f23.svg",
      "fullname": "Arina Razmyslovich",
      "name": "lavriz",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]