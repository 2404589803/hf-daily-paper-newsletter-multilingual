[
  {
    "paper": {
      "id": "2503.06053",
      "authors": [
        {
          "_id": "67cfd2d7bc539099da9ebecb",
          "name": "Runze Zhang",
          "hidden": false
        },
        {
          "_id": "67cfd2d7bc539099da9ebecc",
          "user": {
            "_id": "6474a63f7d131daf633d10f2",
            "avatarUrl": "/avatars/5e5d1ce5731987a810448835a1a69c91.svg",
            "isPro": false,
            "fullname": "GeorgeDu",
            "user": "georgedu",
            "type": "user"
          },
          "name": "Guoguang Du",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-17T08:45:36.478Z",
          "hidden": false
        },
        {
          "_id": "67cfd2d7bc539099da9ebecd",
          "user": {
            "_id": "66b01dc4e48856bb718f2ba8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66b01dc4e48856bb718f2ba8/MHZyIDd3BEtsnH9M2BvzM.jpeg",
            "isPro": false,
            "fullname": "Xiaochuan Li",
            "user": "lixiaochuan",
            "type": "user"
          },
          "name": "Xiaochuan Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-17T08:45:39.724Z",
          "hidden": false
        },
        {
          "_id": "67cfd2d7bc539099da9ebece",
          "name": "Qi Jia",
          "hidden": false
        },
        {
          "_id": "67cfd2d7bc539099da9ebecf",
          "name": "Liang Jin",
          "hidden": false
        },
        {
          "_id": "67cfd2d7bc539099da9ebed0",
          "user": {
            "_id": "66f67725cdcb9a4eaef04027",
            "avatarUrl": "/avatars/fb5f4b467cc4d73e129fa9aa60ef344d.svg",
            "isPro": false,
            "fullname": "Ellen Liu",
            "user": "EllenAP",
            "type": "user"
          },
          "name": "Lu Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-17T08:45:32.476Z",
          "hidden": false
        },
        {
          "_id": "67cfd2d7bc539099da9ebed1",
          "name": "Jingjing Wang",
          "hidden": false
        },
        {
          "_id": "67cfd2d7bc539099da9ebed2",
          "user": {
            "_id": "6297889a64501abb8d002c6b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/-tfSq05d4nkLkU_E-N75e.png",
            "isPro": false,
            "fullname": "Cong Xu",
            "user": "NeilXu",
            "type": "user"
          },
          "name": "Cong Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-18T09:16:48.232Z",
          "hidden": false
        },
        {
          "_id": "67cfd2d7bc539099da9ebed3",
          "name": "Zhenhua Guo",
          "hidden": false
        },
        {
          "_id": "67cfd2d7bc539099da9ebed4",
          "name": "Yaqian Zhao",
          "hidden": false
        },
        {
          "_id": "67cfd2d7bc539099da9ebed5",
          "name": "Xiaoli Gong",
          "hidden": false
        },
        {
          "_id": "67cfd2d7bc539099da9ebed6",
          "name": "Rengang Li",
          "hidden": false
        },
        {
          "_id": "67cfd2d7bc539099da9ebed7",
          "name": "Baoyu Fan",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/66b01dc4e48856bb718f2ba8/_R9279HPwaWKB34BDJmAv.png"
      ],
      "publishedAt": "2025-03-08T04:37:38.000Z",
      "submittedOnDailyAt": "2025-03-18T05:40:31.378Z",
      "title": "ドロップレチューバービデオ：空間時間的な一致性を探求するためのデータセットとアプローチ",
      "submittedOnDailyBy": {
        "_id": "66b01dc4e48856bb718f2ba8",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66b01dc4e48856bb718f2ba8/MHZyIDd3BEtsnH9M2BvzM.jpeg",
        "isPro": false,
        "fullname": "Xiaochuan Li",
        "user": "lixiaochuan",
        "type": "user"
      },
      "summary": "スペクトラルタイム一致性は、ビデオ生成において重要な研究課題です。正しく生成されたビデオセグメントは、プロットの可能性とコラバラティティを確保しながら、物体とシーンの可視的な一致性を保つ必要があります。先行研究では、特にオープンソースプロジェクトでは、時間的または空間的な一致性のどちらかや、その基本的な組み合わせに焦点を当てています。例えば、プロンプトの後にカメラの移動を説明するだけで、その移動の結果を制限しないようにしています。しかし、カメラの移動は、新しい物体をシーンに追加したり、既存の物体を削除したりすることができ、それによって前のナレーティブを重ねて影響を与えます。特に、カメラの移動が多いビデオでは、複数のプロットの間の相互作用は日々複雑になります。本論文では、プロットの進行とカメラの技術の間のシンプラニズムと、前の内容の長期の影響がどのように後続の生成に及ぼすかを考慮して、統合的なスペクトラルタイム一致性を紹介し、検討します。本研究は、データセットの構築からモデルの開発まで幅広く視野を広げています。最初に、Dynamic Camera MotionとObject Actionsを含む1000万のビデオを構築したDropletVideo-10Mデータセットを構築しました。各ビデオは平均206言葉のカプションで注釈され、様々なカメラの移動とプロットの進行を詳細に説明しています。その後、DropletVideoモデルを開発し、ビデオ生成の際にスペクトラルタイムの一致性を保つことが得意です。DropletVideoデータセットとモデルは、https://dropletx.github.io からアクセスできます。",
      "upvotes": 47,
      "discussionId": "67cfd2debc539099da9ec061",
      "ai_keywords": [
        "spatio-temporal consistency",
        "video generation",
        "plot plausibility",
        "visual consistency",
        "objects",
        "scenes",
        "viewpoints",
        "camera movement",
        "prompt",
        "narrative",
        "plot progression",
        "camera techniques",
        "long-term impact",
        "dataset construction",
        "DropletVideo-10M dataset",
        "dynamic camera motion",
        "object actions",
        "caption",
        "DropletVideo model",
        "spatio-temporal coherence"
      ]
    },
    "publishedAt": "2025-03-07T23:37:38.000Z",
    "title": "DropletVideo: A Dataset and Approach to Explore Integral Spatio-Temporal\n  Consistent Video Generation",
    "summary": "Spatio-temporal consistency is a critical research topic in video generation.\nA qualified generated video segment must ensure plot plausibility and coherence\nwhile maintaining visual consistency of objects and scenes across varying\nviewpoints. Prior research, especially in open-source projects, primarily\nfocuses on either temporal or spatial consistency, or their basic combination,\nsuch as appending a description of a camera movement after a prompt without\nconstraining the outcomes of this movement. However, camera movement may\nintroduce new objects to the scene or eliminate existing ones, thereby\noverlaying and affecting the preceding narrative. Especially in videos with\nnumerous camera movements, the interplay between multiple plots becomes\nincreasingly complex. This paper introduces and examines integral\nspatio-temporal consistency, considering the synergy between plot progression\nand camera techniques, and the long-term impact of prior content on subsequent\ngeneration. Our research encompasses dataset construction through to the\ndevelopment of the model. Initially, we constructed a DropletVideo-10M dataset,\nwhich comprises 10 million videos featuring dynamic camera motion and object\nactions. Each video is annotated with an average caption of 206 words,\ndetailing various camera movements and plot developments. Following this, we\ndeveloped and trained the DropletVideo model, which excels in preserving\nspatio-temporal coherence during video generation. The DropletVideo dataset and\nmodel are accessible at https://dropletx.github.io.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/66b01dc4e48856bb718f2ba8/_R9279HPwaWKB34BDJmAv.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.06053.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66b01dc4e48856bb718f2ba8",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66b01dc4e48856bb718f2ba8/MHZyIDd3BEtsnH9M2BvzM.jpeg",
      "fullname": "Xiaochuan Li",
      "name": "lixiaochuan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.12533",
      "authors": [
        {
          "_id": "67d8eadc045f869fea1ce3f2",
          "name": "Haoqi Yuan",
          "hidden": false
        },
        {
          "_id": "67d8eadc045f869fea1ce3f3",
          "name": "Yu Bai",
          "hidden": false
        },
        {
          "_id": "67d8eadc045f869fea1ce3f4",
          "user": {
            "_id": "67d92b2218de6ef86c60f7d4",
            "avatarUrl": "/avatars/9758522c99bc38bc7b60845eff8bf8d7.svg",
            "isPro": false,
            "fullname": "Yuhui Fu",
            "user": "fuyh",
            "type": "user"
          },
          "name": "Yuhui Fu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:17:46.443Z",
          "hidden": false
        },
        {
          "_id": "67d8eadc045f869fea1ce3f5",
          "name": "Bohan Zhou",
          "hidden": false
        },
        {
          "_id": "67d8eadc045f869fea1ce3f6",
          "user": {
            "_id": "6655b86e607894ea80d74910",
            "avatarUrl": "/avatars/663c0135c903c9c127fe1b8d8aaf279c.svg",
            "isPro": false,
            "fullname": "yicheng feng",
            "user": "takenpeanut",
            "type": "user"
          },
          "name": "Yicheng Feng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:17:31.771Z",
          "hidden": false
        },
        {
          "_id": "67d8eadc045f869fea1ce3f7",
          "user": {
            "_id": "653238fdcd5377e9adee0c41",
            "avatarUrl": "/avatars/78aea70cde6ab0050c7e18b5e148075c.svg",
            "isPro": false,
            "fullname": "Xinrun Xu",
            "user": "SherryXu",
            "type": "user"
          },
          "name": "Xinrun Xu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:17:05.200Z",
          "hidden": false
        },
        {
          "_id": "67d8eadc045f869fea1ce3f8",
          "name": "Yi Zhan",
          "hidden": false
        },
        {
          "_id": "67d8eadc045f869fea1ce3f9",
          "user": {
            "_id": "61e52be53d6dbb1da842316a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61e52be53d6dbb1da842316a/gx0WGPcOCClXPymoKglc4.jpeg",
            "isPro": false,
            "fullname": "Börje Karlsson",
            "user": "tellarin",
            "type": "user"
          },
          "name": "Börje F. Karlsson",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-18T08:07:34.005Z",
          "hidden": false
        },
        {
          "_id": "67d8eadc045f869fea1ce3fa",
          "user": {
            "_id": "67d905c0e27ba28109384f5c",
            "avatarUrl": "/avatars/26712594ac9d43c8d1a3e75e36b5df16.svg",
            "isPro": false,
            "fullname": "Zongqing Lu",
            "user": "chungtsing",
            "type": "user"
          },
          "name": "Zongqing Lu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:16:58.409Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-16T14:53:53.000Z",
      "submittedOnDailyAt": "2025-03-18T02:11:08.263Z",
      "title": "ビジョン・ランゲージモデルとモジュール化スキルを採用した人間型ロボットアグェント",
      "submittedOnDailyBy": {
        "_id": "61e52be53d6dbb1da842316a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61e52be53d6dbb1da842316a/gx0WGPcOCClXPymoKglc4.jpeg",
        "isPro": false,
        "fullname": "Börje Karlsson",
        "user": "tellarin",
        "type": "user"
      },
      "summary": "自動軌動機人型ロボットの研究で最終的な目標として、実世界の具象化タスクで人間レベルの性能を達成する自動軌動機人型ロボットを開発することは重要な目標です。最近の進歩は、ベースドモデル（FM）による高レベルの認知と人型ロボットの低レベルスキル開発において顕著な進展を遂げました。しかし、これらのコンポーネントの直接的な統合は、長期間タスクでの誤差の累積と各モジュールの異なる遅延により、脆弱性と効率が低下することがあります。ここでは、FMとモジュール化サキルライブラリを統合するヒューリスティックなアガントフレームワーク「Being-0」を介して、この間違いを補うことを目的としています。FMは、指示理解、タスク計画、理由のような高レベルの認知タスクを担当し、サキルライブラリは、低レベルの制御における穩やかな移動と柔軟な操作を提供します。これらのレベルの間の隙間を埋めるために、新型のConnectorモジュールを提案しています。Connectorは、軽量ビジョン言語モデル（VLM）をもち、FMの具象化能力を強化し、言語ベースの計画を行動可能なスキルコマンドに翻訳し、移動と操作の動的な協調を行い、タスクの成功率を向上させます。Being-0は、FMを除くすべてのコンポーネントが低コストのバーチャルコンピュータ裝備で実装可能であり、具象化手の持ち主とアクティブビジョンを掛け付けた全計画人型ロボットにより、効率的な時間協調を実現します。大規模な室内環境での拡散的な実験は、Being-0が複雑な、長期間タスクを解決するために必要な難しいナビゲーションと操作のサブタスクを実現する効果を示しました。詳細とビデオを確認するには、https://beingbeyond.github.io/being-0 をご覧ください。",
      "upvotes": 35,
      "discussionId": "67d8eadd045f869fea1ce44a",
      "projectPage": "https://beingbeyond.github.io/Being-0/",
      "ai_keywords": [
        "Foundation Models (FMs)",
        "modular skill library",
        "high-level cognitive tasks",
        "instruction understanding",
        "task planning",
        "reasoning",
        "stable locomotion",
        "dexterous manipulation",
        "low-level control",
        "Connector module",
        "lightweight vision-language model (VLM",
        "embodied capabilities",
        "language-based plans",
        "actionable skill commands",
        "dynamic coordination",
        "full-sized humanoid robot",
        "dexterous hands",
        "active vision",
        "complex, long-horizon tasks",
        "challenging navigation",
        "manipulation subtasks"
      ]
    },
    "publishedAt": "2025-03-16T10:53:53.000Z",
    "title": "Being-0: A Humanoid Robotic Agent with Vision-Language Models and\n  Modular Skills",
    "summary": "Building autonomous robotic agents capable of achieving human-level\nperformance in real-world embodied tasks is an ultimate goal in humanoid robot\nresearch. Recent advances have made significant progress in high-level\ncognition with Foundation Models (FMs) and low-level skill development for\nhumanoid robots. However, directly combining these components often results in\npoor robustness and efficiency due to compounding errors in long-horizon tasks\nand the varied latency of different modules. We introduce Being-0, a\nhierarchical agent framework that integrates an FM with a modular skill\nlibrary. The FM handles high-level cognitive tasks such as instruction\nunderstanding, task planning, and reasoning, while the skill library provides\nstable locomotion and dexterous manipulation for low-level control. To bridge\nthe gap between these levels, we propose a novel Connector module, powered by a\nlightweight vision-language model (VLM). The Connector enhances the FM's\nembodied capabilities by translating language-based plans into actionable skill\ncommands and dynamically coordinating locomotion and manipulation to improve\ntask success. With all components, except the FM, deployable on low-cost\nonboard computation devices, Being-0 achieves efficient, real-time performance\non a full-sized humanoid robot equipped with dexterous hands and active vision.\nExtensive experiments in large indoor environments demonstrate Being-0's\neffectiveness in solving complex, long-horizon tasks that require challenging\nnavigation and manipulation subtasks. For further details and videos, visit\nhttps://beingbeyond.github.io/being-0.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12533.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "61e52be53d6dbb1da842316a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61e52be53d6dbb1da842316a/gx0WGPcOCClXPymoKglc4.jpeg",
      "fullname": "Börje Karlsson",
      "name": "tellarin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 24
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.12885",
      "authors": [
        {
          "_id": "67d8e23afa59a8b15a9057e8",
          "user": {
            "_id": "65eaa1e2b11eeb516a973508",
            "avatarUrl": "/avatars/beecd135bb940fdc02406f9063b3fa67.svg",
            "isPro": false,
            "fullname": "Dewei Zhou",
            "user": "limuloo1999",
            "type": "user"
          },
          "name": "Dewei Zhou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:20:39.038Z",
          "hidden": false
        },
        {
          "_id": "67d8e23afa59a8b15a9057e9",
          "user": {
            "_id": "64551bc2c9c0dcc8c2484cf6",
            "avatarUrl": "/avatars/0d1ed4f4502f6f54ac6ba071e4c9a220.svg",
            "isPro": false,
            "fullname": "Mingwei Li",
            "user": "aiJojosh",
            "type": "user"
          },
          "name": "Mingwei Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:20:46.810Z",
          "hidden": false
        },
        {
          "_id": "67d8e23afa59a8b15a9057ea",
          "user": {
            "_id": "619bf9b3cbedb87e1a92fb3b",
            "avatarUrl": "/avatars/ee280db0232e21416c948ab9a9a2344e.svg",
            "isPro": false,
            "fullname": "Zongxin Yang",
            "user": "z-x-yang",
            "type": "user"
          },
          "name": "Zongxin Yang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:20:52.869Z",
          "hidden": false
        },
        {
          "_id": "67d8e23afa59a8b15a9057eb",
          "name": "Yi Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-17T07:30:16.000Z",
      "submittedOnDailyAt": "2025-03-18T01:33:30.593Z",
      "title": "ドリームレンダー：大規模テキストから画像への多インスタンス属性制御のように調達する",
      "submittedOnDailyBy": {
        "_id": "65eaa1e2b11eeb516a973508",
        "avatarUrl": "/avatars/beecd135bb940fdc02406f9063b3fa67.svg",
        "isPro": false,
        "fullname": "Dewei Zhou",
        "user": "limuloo1999",
        "type": "user"
      },
      "summary": "画像条件付き生成法の例である深さやCanny条件付きアプローチは、精確な画像合成のために驚異的な能力を示している。しかし、現在のモデルは、複数のインスタンス（または領域）の内容を正確に制御することが難しい。その中でも最先端のモデルであるFLUXと3DISも、インスタンス間の属性漏れなどの問題を見つけ、ユーザーの制御を制限している。これらの問題を解決するために、我々は、FLUXモデルに基づくトレーニング無制限アプローチであるDreamRendererを紹介します。DreamRendererは、ユーザーがボウンディングボックスまたはマスクを通じて各インスタンスの内容を制御できるようにし、全体の視覚的な和諧性を保ちます。我々は、2つのキーの革新的な提案を提案します：1) 難しい文脈属性の結合を行う画像トークンのブリッジ、2) ビタルレイヤーに限った厳格な画像属性の結合。FLUXの分析を通じて、インスタンス属性のレンダリングに責任を持つキープレイヤーを特定し、それらのビタルレイヤーでのみ厳格な画像属性の結合を適用し、その他のレイヤーではソフトな結合を行います。このアプローチは、画像の質を保つながら、精密な制御を可能にします。COCO-POSとCOCO-MIGベンチマークでの評価により、DreamRendererはFLUXより17.7%の画像成功比を向上させ、GLIGENと3DISなどの並列デザインから画像へのモデルの性能を最高26.8%向上させます。プロジェクトページ：https://limuloo.github.io/DreamRenderer/。",
      "upvotes": 24,
      "discussionId": "67d8e23cfa59a8b15a9058ba",
      "projectPage": "https://limuloo.github.io/DreamRenderer/",
      "githubRepo": "https://github.com/limuloo/DreamRenderer",
      "ai_keywords": [
        "Bridge Image Tokens",
        "Hard Text Attribute Binding",
        "Replicated image tokens",
        "T5 text embeddings",
        "Joint Attention",
        "Hard Image Attribute Binding",
        "Vital layers",
        "Soft binding",
        "Image Success Ratio",
        "COCO-POS",
        "COCO-MIG",
        "layout-to-image models",
        "GLIGEN",
        "3DIS"
      ]
    },
    "publishedAt": "2025-03-17T03:30:16.000Z",
    "title": "DreamRenderer: Taming Multi-Instance Attribute Control in Large-Scale\n  Text-to-Image Models",
    "summary": "Image-conditioned generation methods, such as depth- and canny-conditioned\napproaches, have demonstrated remarkable abilities for precise image synthesis.\nHowever, existing models still struggle to accurately control the content of\nmultiple instances (or regions). Even state-of-the-art models like FLUX and\n3DIS face challenges, such as attribute leakage between instances, which limits\nuser control. To address these issues, we introduce DreamRenderer, a\ntraining-free approach built upon the FLUX model. DreamRenderer enables users\nto control the content of each instance via bounding boxes or masks, while\nensuring overall visual harmony. We propose two key innovations: 1) Bridge\nImage Tokens for Hard Text Attribute Binding, which uses replicated image\ntokens as bridge tokens to ensure that T5 text embeddings, pre-trained solely\non text data, bind the correct visual attributes for each instance during Joint\nAttention; 2) Hard Image Attribute Binding applied only to vital layers.\nThrough our analysis of FLUX, we identify the critical layers responsible for\ninstance attribute rendering and apply Hard Image Attribute Binding only in\nthese layers, using soft binding in the others. This approach ensures precise\ncontrol while preserving image quality. Evaluations on the COCO-POS and\nCOCO-MIG benchmarks demonstrate that DreamRenderer improves the Image Success\nRatio by 17.7% over FLUX and enhances the performance of layout-to-image models\nlike GLIGEN and 3DIS by up to 26.8%. Project Page:\nhttps://limuloo.github.io/DreamRenderer/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12885.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "65eaa1e2b11eeb516a973508",
      "avatarUrl": "/avatars/beecd135bb940fdc02406f9063b3fa67.svg",
      "fullname": "Dewei Zhou",
      "name": "limuloo1999",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.13327",
      "authors": [
        {
          "_id": "67d8e00f0922c3dc8866520c",
          "user": {
            "_id": "640d704c8036cc2142299c19",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640d704c8036cc2142299c19/Wt9AslcVxWOSSc11epk8l.jpeg",
            "isPro": false,
            "fullname": "Lan Chen",
            "user": "Orannue",
            "type": "user"
          },
          "name": "Lan Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:21:18.982Z",
          "hidden": false
        },
        {
          "_id": "67d8e00f0922c3dc8866520d",
          "name": "Qi Mao",
          "hidden": false
        },
        {
          "_id": "67d8e00f0922c3dc8866520e",
          "user": {
            "_id": "63021630a35b21bd8a53305a",
            "avatarUrl": "/avatars/7a7e8b39749eda61e57d8a1908726558.svg",
            "isPro": true,
            "fullname": "Gu Yuchao",
            "user": "guyuchao",
            "type": "user"
          },
          "name": "Yuchao Gu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:21:44.702Z",
          "hidden": false
        },
        {
          "_id": "67d8e00f0922c3dc8866520f",
          "user": {
            "_id": "661ab3da2b14565c7acccf5c",
            "avatarUrl": "/avatars/fa4fc03664803e02aede4d4c3d50b393.svg",
            "isPro": false,
            "fullname": "Mike Zheng Shou",
            "user": "AnalMom",
            "type": "user"
          },
          "name": "Mike Zheng Shou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:21:29.754Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-17T16:04:44.000Z",
      "submittedOnDailyAt": "2025-03-18T01:26:49.605Z",
      "title": "エディットトランスフォーマー：視覚のインコンテクスト関係を学ぶ画像エディット",
      "submittedOnDailyBy": {
        "_id": "640d704c8036cc2142299c19",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640d704c8036cc2142299c19/Wt9AslcVxWOSSc11epk8l.jpeg",
        "isPro": false,
        "fullname": "Lan Chen",
        "user": "Orannue",
        "type": "user"
      },
      "summary": "新しい設定「Edit Transfer」を介して、モデルは単一のソースターゲット例を学習し、新しいクエリ画像に対して変換を適用する。文脈ベースの方法は、文脈プロンプトを通じて語意的な操作を優れているが、精度の高い幾何的な詳細（例：姿勢と視点の変化）に関しては難しい。対象ベースの編集とは、通常、スタイルや外見に焦点を当て、非剛性の変換には成功しない。単一のソースターゲットペアから明示的に編集の変換を学習することで、Edit Transferは文脈だけであるほか、外見も中心となる参照の制限を軽減する。大規模な言語モデルでの文脈学習をヒントに、DiTベースの文脈から画像へのモデルを基にして、視覚関係の文脈学習パラダイムを提案する。編集された例とクエリ画像を一つの4ページのコンポーネントに組み立て、最小限の例から複雑な空間的な変換を捉えるために、軽量のLoRA微調を適用する。42件の訓練サンプルしか使用しないのに対して、Edit Transferは多様な非剛性スケーラーで最先端のTIEとRIEの方法を大幅に超え、少ショットの視覚関係学習の効果を示す。",
      "upvotes": 19,
      "discussionId": "67d8e0100922c3dc88665285",
      "projectPage": "https://cuc-mipg.github.io/EditTransfer.github.io",
      "githubRepo": "https://github.com/CUC-MIPG/Edit-Transfer",
      "ai_keywords": [
        "Edit Transfer",
        "visual relation in-context learning",
        "DiT-based text-to-image model",
        "four-panel composite",
        "LoRA fine-tuning",
        "few-shot visual relation learning",
        "non-rigid transformations",
        "TIE methods",
        "RIE methods"
      ]
    },
    "publishedAt": "2025-03-17T12:04:44.000Z",
    "title": "Edit Transfer: Learning Image Editing via Vision In-Context Relations",
    "summary": "We introduce a new setting, Edit Transfer, where a model learns a\ntransformation from just a single source-target example and applies it to a new\nquery image. While text-based methods excel at semantic manipulations through\ntextual prompts, they often struggle with precise geometric details (e.g.,\nposes and viewpoint changes). Reference-based editing, on the other hand,\ntypically focuses on style or appearance and fails at non-rigid\ntransformations. By explicitly learning the editing transformation from a\nsource-target pair, Edit Transfer mitigates the limitations of both text-only\nand appearance-centric references. Drawing inspiration from in-context learning\nin large language models, we propose a visual relation in-context learning\nparadigm, building upon a DiT-based text-to-image model. We arrange the edited\nexample and the query image into a unified four-panel composite, then apply\nlightweight LoRA fine-tuning to capture complex spatial transformations from\nminimal examples. Despite using only 42 training samples, Edit Transfer\nsubstantially outperforms state-of-the-art TIE and RIE methods on diverse\nnon-rigid scenarios, demonstrating the effectiveness of few-shot visual\nrelation learning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13327.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "640d704c8036cc2142299c19",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640d704c8036cc2142299c19/Wt9AslcVxWOSSc11epk8l.jpeg",
      "fullname": "Lan Chen",
      "name": "Orannue",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.12590",
      "authors": [
        {
          "_id": "67d8de85f7809eea577c4805",
          "name": "Haoran Feng",
          "hidden": false
        },
        {
          "_id": "67d8de85f7809eea577c4806",
          "user": {
            "_id": "6375d136dee28348a9c63cbf",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6375d136dee28348a9c63cbf/gK465HBrQWIOZ-qtHb-Vh.jpeg",
            "isPro": false,
            "fullname": "zehuan-huang",
            "user": "huanngzh",
            "type": "user"
          },
          "name": "Zehuan Huang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-18T08:08:06.131Z",
          "hidden": false
        },
        {
          "_id": "67d8de85f7809eea577c4807",
          "name": "Lin Li",
          "hidden": false
        },
        {
          "_id": "67d8de85f7809eea577c4808",
          "user": {
            "_id": "674ded8ee50d988a4b9e108b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/8oQITwlb7AB8LeIJjooYc.png",
            "isPro": false,
            "fullname": "Hairong Lv",
            "user": "lvhairong",
            "type": "user"
          },
          "name": "Hairong Lv",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:22:23.294Z",
          "hidden": false
        },
        {
          "_id": "67d8de85f7809eea577c4809",
          "name": "Lu Sheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-16T17:51:16.000Z",
      "submittedOnDailyAt": "2025-03-18T01:18:31.307Z",
      "title": "Diffusion Transformerで、どのものをも自動的に個別化できます。無料です。",
      "submittedOnDailyBy": {
        "_id": "6375d136dee28348a9c63cbf",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6375d136dee28348a9c63cbf/gK465HBrQWIOZ-qtHb-Vh.jpeg",
        "isPro": false,
        "fullname": "zehuan-huang",
        "user": "huanngzh",
        "type": "user"
      },
      "summary": "個人化画像生成は、ユーザー指定の概念の画像を生成し、柔軟な編集を可能にすることを目的としています。最近のトレーニング無しアプローチは、トレーニングベースの方法よりも高い計算効率を示すことができますが、自己同一性保持、適用性、ディフュージョントランスフォーマー（DiT）との相性に関しては問題があります。本論文では、DiTの未開発の潜力を発見し、参照オブジェクトのノイズトークンを置換するだけでゼロショットの主題再構築が可能になることを示します。この簡単で効果的な特徴注入技術は、個人化から画像編集まで様々なシナリオを解決することができます。この見聞を基に、デレーニング無しフレームワーク「Personalize Anything」を提案し、DiTで個人化画像生成を実現します。1）時間ステップ適応的トークン置換で、早期階段に注入したことで主題の一致性を強制し、遅期階段で正規化して柔軟性を向上させます。2）パッチペルバチャティクスを用いて構造的な多様性を向上させます。我々の方法は、レイアウトガイドされた生成、多主題個人化、マスク制御編集をサポートします。評価は、自己同一性保持と多様性の最先端の性能を示します。我々の研究は、DiTについて新しい見聞を提供し、効率的な個人化の実用的パラダイムを提供します。",
      "upvotes": 18,
      "discussionId": "67d8de89f7809eea577c4930",
      "projectPage": "https://fenghora.github.io/Personalize-Anything-Page/",
      "githubRepo": "https://github.com/fenghora/personalize-anything",
      "ai_keywords": [
        "diffusion transformers (DiTs)",
        "denoising tokens",
        "zero-shot subject reconstruction",
        "timestep-adaptive token replacement",
        "early-stage injection",
        "late-stage regularization",
        "patch perturbation strategies",
        "layout-guided generation",
        "multi-subject personalization",
        "mask-controlled editing",
        "identity preservation",
        "versatility"
      ]
    },
    "publishedAt": "2025-03-16T13:51:16.000Z",
    "title": "Personalize Anything for Free with Diffusion Transformer",
    "summary": "Personalized image generation aims to produce images of user-specified\nconcepts while enabling flexible editing. Recent training-free approaches,\nwhile exhibit higher computational efficiency than training-based methods,\nstruggle with identity preservation, applicability, and compatibility with\ndiffusion transformers (DiTs). In this paper, we uncover the untapped potential\nof DiT, where simply replacing denoising tokens with those of a reference\nsubject achieves zero-shot subject reconstruction. This simple yet effective\nfeature injection technique unlocks diverse scenarios, from personalization to\nimage editing. Building upon this observation, we propose Personalize\nAnything, a training-free framework that achieves personalized image\ngeneration in DiT through: 1) timestep-adaptive token replacement that enforces\nsubject consistency via early-stage injection and enhances flexibility through\nlate-stage regularization, and 2) patch perturbation strategies to boost\nstructural diversity. Our method seamlessly supports layout-guided generation,\nmulti-subject personalization, and mask-controlled editing. Evaluations\ndemonstrate state-of-the-art performance in identity preservation and\nversatility. Our work establishes new insights into DiTs while delivering a\npractical paradigm for efficient personalization.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12590.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6375d136dee28348a9c63cbf",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6375d136dee28348a9c63cbf/gK465HBrQWIOZ-qtHb-Vh.jpeg",
      "fullname": "zehuan-huang",
      "name": "huanngzh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 24
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.13435",
      "authors": [
        {
          "_id": "67d8dd0b924be985c277c8f6",
          "user": {
            "_id": "64fde4e252e82dd432b74ce9",
            "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
            "isPro": false,
            "fullname": "Ling Yang",
            "user": "Lingaaaaaaa",
            "type": "user"
          },
          "name": "Ling Yang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:22:51.251Z",
          "hidden": false
        },
        {
          "_id": "67d8dd0b924be985c277c8f7",
          "user": {
            "_id": "6708920aeae29d1cd41a703b",
            "avatarUrl": "/avatars/922427a86523b0aa810412fd2d75f88e.svg",
            "isPro": false,
            "fullname": "kaixin zhu",
            "user": "czkk566",
            "type": "user"
          },
          "name": "Kaixin Zhu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-18T09:16:44.192Z",
          "hidden": false
        },
        {
          "_id": "67d8dd0b924be985c277c8f8",
          "user": {
            "_id": "670880950e79a8b46f7ff9dd",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/670880950e79a8b46f7ff9dd/hA1TLhwlQblkFsq8wLrkB.jpeg",
            "isPro": false,
            "fullname": "Juanxi Tian",
            "user": "Juanxi",
            "type": "user"
          },
          "name": "Juanxi Tian",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-18T08:08:12.128Z",
          "hidden": false
        },
        {
          "_id": "67d8dd0b924be985c277c8f9",
          "user": {
            "_id": "6671214c92412fd4640714eb",
            "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg",
            "isPro": false,
            "fullname": "bohan zeng",
            "user": "zbhpku",
            "type": "user"
          },
          "name": "Bohan Zeng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-18T08:08:15.654Z",
          "hidden": false
        },
        {
          "_id": "67d8dd0b924be985c277c8fa",
          "user": {
            "_id": "64a2a8127adb12be606ec33e",
            "avatarUrl": "/avatars/f85dc39a23727a4d50f8a5f5a3865b0d.svg",
            "isPro": false,
            "fullname": "Mingbao Lin",
            "user": "mingbao",
            "type": "user"
          },
          "name": "Mingbao Lin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:22:58.991Z",
          "hidden": false
        },
        {
          "_id": "67d8dd0b924be985c277c8fb",
          "name": "Hongjuan Pei",
          "hidden": false
        },
        {
          "_id": "67d8dd0b924be985c277c8fc",
          "name": "Wentao Zhang",
          "hidden": false
        },
        {
          "_id": "67d8dd0b924be985c277c8fd",
          "name": "Shuicheng Yan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-17T17:58:18.000Z",
      "submittedOnDailyAt": "2025-03-18T01:44:02.731Z",
      "title": "WideRange4D: 幅広い移動と場面による高品質な4D再構成を可能にする",
      "submittedOnDailyBy": {
        "_id": "64fde4e252e82dd432b74ce9",
        "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
        "isPro": false,
        "fullname": "Ling Yang",
        "user": "Lingaaaaaaa",
        "type": "user"
      },
      "summary": "3D再構造技術の急速な進歩に伴い、4D再構造の研究も進展しています。現在の4D再構造手法は高品質の4Dシーンを生成できますが、多角度ビデオデータの取得には課題があり、現在の4D再構造ベンチマークは主に限定のスケーナリオ内での行動（例えば、ダンス）を表現しています。実用的なスケーナリオでは、多くのシーンには広範囲的な空間的移動が含まれており、現在の4D再構造データセットの制限が明らかになっています。また、現在の4D再構造手法は3Dオブジェクトの動力学を評価するために変形フィールドを依存していますが、変形フィールドは広範囲的な空間的移動に対して困難を抱え、これにより広範囲的な空間的移動を含む高品質な4Dシーン再構造を実現する能力が限られています。本論文では、広範囲的な空間的移動を含む4Dシーン再構造に焦点を当て、新しい4D再構造ベンチマーク「WideRange4D」を提案します。このベンチマークは、大きな空間的変化を含む豊富な4Dシーンデータを含み、4D生成手法の生成能力の評価を更に詳細に行うことができます。また、新しい4D再構造手法「Progress4D」を紹介し、複雑な4Dシーン再構造タスクでも穩定した高品質な4D結果を生成できることを示します。WideRange4Dにおいて定量的および定性的な比較実験を行い、我々のProgress4Dが現在の最先端の4D再構造手法を上回ることを示します。プロジェクト：https://github.com/Gen-Verse/WideRange4D",
      "upvotes": 14,
      "discussionId": "67d8dd0d924be985c277c998",
      "projectPage": "https://huggingface.co/datasets/Gen-Verse/WideRange4D",
      "githubRepo": "https://github.com/Gen-Verse/WideRange4D",
      "ai_keywords": [
        "deformation fields",
        "4D reconstruction",
        "scene reconstruction",
        "multi-view video",
        "spatial movements",
        "3D objects",
        "4D scene data",
        "generation capabilities",
        "4D generation methods",
        "WideRange4D",
        "Progress4D"
      ]
    },
    "publishedAt": "2025-03-17T13:58:18.000Z",
    "title": "WideRange4D: Enabling High-Quality 4D Reconstruction with Wide-Range\n  Movements and Scenes",
    "summary": "With the rapid development of 3D reconstruction technology, research in 4D\nreconstruction is also advancing, existing 4D reconstruction methods can\ngenerate high-quality 4D scenes. However, due to the challenges in acquiring\nmulti-view video data, the current 4D reconstruction benchmarks mainly display\nactions performed in place, such as dancing, within limited scenarios. In\npractical scenarios, many scenes involve wide-range spatial movements,\nhighlighting the limitations of existing 4D reconstruction datasets.\nAdditionally, existing 4D reconstruction methods rely on deformation fields to\nestimate the dynamics of 3D objects, but deformation fields struggle with\nwide-range spatial movements, which limits the ability to achieve high-quality\n4D scene reconstruction with wide-range spatial movements. In this paper, we\nfocus on 4D scene reconstruction with significant object spatial movements and\npropose a novel 4D reconstruction benchmark, WideRange4D. This benchmark\nincludes rich 4D scene data with large spatial variations, allowing for a more\ncomprehensive evaluation of the generation capabilities of 4D generation\nmethods. Furthermore, we introduce a new 4D reconstruction method, Progress4D,\nwhich generates stable and high-quality 4D results across various complex 4D\nscene reconstruction tasks. We conduct both quantitative and qualitative\ncomparison experiments on WideRange4D, showing that our Progress4D outperforms\nexisting state-of-the-art 4D reconstruction methods. Project:\nhttps://github.com/Gen-Verse/WideRange4D",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13435.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64fde4e252e82dd432b74ce9",
      "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
      "fullname": "Ling Yang",
      "name": "Lingaaaaaaa",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.13434",
      "authors": [
        {
          "_id": "67d916b500030726e0df2a67",
          "user": {
            "_id": "6362801380c1a705a6ea54ac",
            "avatarUrl": "/avatars/041ad5abf9be42e336938f51ebb8746c.svg",
            "isPro": false,
            "fullname": "Yaowei Li",
            "user": "Yw22",
            "type": "user"
          },
          "name": "Yaowei Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:23:48.215Z",
          "hidden": false
        },
        {
          "_id": "67d916b500030726e0df2a68",
          "user": {
            "_id": "66837d3c48edefb453b0640a",
            "avatarUrl": "/avatars/b16385eaa612578728e2c6460a76b38f.svg",
            "isPro": false,
            "fullname": "Lingen Li",
            "user": "l-li",
            "type": "user"
          },
          "name": "Lingen Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:23:54.498Z",
          "hidden": false
        },
        {
          "_id": "67d916b500030726e0df2a69",
          "user": {
            "_id": "658409ceca19ccf6d9989add",
            "avatarUrl": "/avatars/3ac1dbd25e52435185babdeb3da28875.svg",
            "isPro": false,
            "fullname": "Zhaoyang Zhang",
            "user": "ZyZcuhk",
            "type": "user"
          },
          "name": "Zhaoyang Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-18T08:07:22.583Z",
          "hidden": false
        },
        {
          "_id": "67d916b500030726e0df2a6a",
          "name": "Xiaoyu Li",
          "hidden": false
        },
        {
          "_id": "67d916b500030726e0df2a6b",
          "user": {
            "_id": "6422b973ef9e8971003cdd22",
            "avatarUrl": "/avatars/8564a2e984e2e79e46d90cc9c35e5773.svg",
            "isPro": false,
            "fullname": "Guangzhi Wang",
            "user": "daoyuan98",
            "type": "user"
          },
          "name": "Guangzhi Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:24:18.787Z",
          "hidden": false
        },
        {
          "_id": "67d916b500030726e0df2a6c",
          "user": {
            "_id": "653eb3bd4a52f10eaf72fbaf",
            "avatarUrl": "/avatars/b525482b61c6f6054bf44bbc3113c29f.svg",
            "isPro": false,
            "fullname": "Hongxiang Li",
            "user": "HongxiangLi",
            "type": "user"
          },
          "name": "Hongxiang Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:24:25.700Z",
          "hidden": false
        },
        {
          "_id": "67d916b500030726e0df2a6d",
          "user": {
            "_id": "63184c517ca1b876d99b7e0e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63184c517ca1b876d99b7e0e/b-qDExoeJuDXK0cJBZKnz.jpeg",
            "isPro": false,
            "fullname": "Xiaodong Cun",
            "user": "vinthony",
            "type": "user"
          },
          "name": "Xiaodong Cun",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:24:32.118Z",
          "hidden": false
        },
        {
          "_id": "67d916b500030726e0df2a6e",
          "user": {
            "_id": "63ca3ddc04c979828310bfcb",
            "avatarUrl": "/avatars/615e0d8622950b4408b40d550f02a894.svg",
            "isPro": false,
            "fullname": "Ying Shan",
            "user": "yshan2u",
            "type": "user"
          },
          "name": "Ying Shan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:24:39.725Z",
          "hidden": false
        },
        {
          "_id": "67d916b500030726e0df2a6f",
          "name": "Yuexian Zou",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/658409ceca19ccf6d9989add/fhyGe83BiDhDf9a71kUyP.mp4"
      ],
      "publishedAt": "2025-03-17T17:58:05.000Z",
      "submittedOnDailyAt": "2025-03-18T05:20:30.708Z",
      "title": "BlobCtrl: 元素レベル画像の生成と編集の一貫したおさらいと柔軟なフレームワーク",
      "submittedOnDailyBy": {
        "_id": "658409ceca19ccf6d9989add",
        "avatarUrl": "/avatars/3ac1dbd25e52435185babdeb3da28875.svg",
        "isPro": false,
        "fullname": "Zhaoyang Zhang",
        "user": "ZyZcuhk",
        "type": "user"
      },
      "summary": "要素レベルの可視化操作は、デジタルコンテンツ作成には重要ですが、現在の拡散ベースの方法は、伝統的なツールと比べて精度と柔軟性が不足しています。本稿では、プロバビリティブなブロッブベースの表現を用いて要素レベルの生成と編集を統一するフレームワーク、BlobCtrlを紹介します。ブロッブを可視化の基本として用いることで、我々のアプローチは空間位置、語義的な内容、アイデンティティ情報を適切に離れて表現でき、精密な要素レベルの操作を可能にします。主な貢献点は以下の3つです：1) 階層的な特徴融合を用いたダブルブランシャー構造での無間のフォロゴーン・バックグラウンド統合；2) データ拡張とスコア関数をチューニングした自己観測学習パラダイム；3) ファイドティリティと多様性のバランスを調節する可能なドロップアウト戦略。進める研究のために、大規模なトレーニングを支援するBlobDataとシステマティックな評価を行うBlobBenchを紹介します。実験結果から、BlobCtrlは複数の要素レベルの操作タスクで優れている一方、計算効率を維持していることから、精密かつ柔軟な可視化コンテンツ作成の実用的な解決策としての可能性を示しています。プロジェクトページ：https://liyaowei-stu.github.io/project/BlobCtrl/",
      "upvotes": 12,
      "discussionId": "67d916bc00030726e0df2c3e",
      "projectPage": "https://liyaowei-stu.github.io/project/BlobCtrl/",
      "githubRepo": "https://github.com/TencentARC/BlobCtrl",
      "ai_keywords": [
        "probabilistic blob-based representation",
        "dual-branch diffusion architecture",
        "hierarchical feature fusion",
        "self-supervised training paradigm",
        "tailored data augmentation",
        "score functions",
        "controllable dropout strategies",
        "BlobData",
        "BlobBench"
      ]
    },
    "publishedAt": "2025-03-17T13:58:05.000Z",
    "title": "BlobCtrl: A Unified and Flexible Framework for Element-level Image\n  Generation and Editing",
    "summary": "Element-level visual manipulation is essential in digital content creation,\nbut current diffusion-based methods lack the precision and flexibility of\ntraditional tools. In this work, we introduce BlobCtrl, a framework that\nunifies element-level generation and editing using a probabilistic blob-based\nrepresentation. By employing blobs as visual primitives, our approach\neffectively decouples and represents spatial location, semantic content, and\nidentity information, enabling precise element-level manipulation. Our key\ncontributions include: 1) a dual-branch diffusion architecture with\nhierarchical feature fusion for seamless foreground-background integration; 2)\na self-supervised training paradigm with tailored data augmentation and score\nfunctions; and 3) controllable dropout strategies to balance fidelity and\ndiversity. To support further research, we introduce BlobData for large-scale\ntraining and BlobBench for systematic evaluation. Experiments show that\nBlobCtrl excels in various element-level manipulation tasks while maintaining\ncomputational efficiency, offering a practical solution for precise and\nflexible visual content creation. Project page:\nhttps://liyaowei-stu.github.io/project/BlobCtrl/",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/658409ceca19ccf6d9989add/fhyGe83BiDhDf9a71kUyP.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13434.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "658409ceca19ccf6d9989add",
      "avatarUrl": "/avatars/3ac1dbd25e52435185babdeb3da28875.svg",
      "fullname": "Zhaoyang Zhang",
      "name": "ZyZcuhk",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.11751",
      "authors": [
        {
          "_id": "67d8e861fa59a8b15a921052",
          "user": {
            "_id": "6351712b40dffad651f128c7",
            "avatarUrl": "/avatars/87708c86c1baef548ef556f5d32dca71.svg",
            "isPro": false,
            "fullname": "Zhaofeng Wu",
            "user": "ZhaofengWu",
            "type": "user"
          },
          "name": "Zhaofeng Wu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:27:29.372Z",
          "hidden": false
        },
        {
          "_id": "67d8e861fa59a8b15a921053",
          "user": {
            "_id": "621e9388345a1d9ab65391c3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/621e9388345a1d9ab65391c3/RxurNzyAWJOUdgeSHQi1R.jpeg",
            "isPro": false,
            "fullname": "Michihiro Yasunaga",
            "user": "michiyasunaga",
            "type": "user"
          },
          "name": "Michihiro Yasunaga",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:27:35.318Z",
          "hidden": false
        },
        {
          "_id": "67d8e861fa59a8b15a921054",
          "name": "Andrew Cohen",
          "hidden": false
        },
        {
          "_id": "67d8e861fa59a8b15a921055",
          "name": "Yoon Kim",
          "hidden": false
        },
        {
          "_id": "67d8e861fa59a8b15a921056",
          "name": "Asli Celikyilmaz",
          "hidden": false
        },
        {
          "_id": "67d8e861fa59a8b15a921057",
          "user": {
            "_id": "660f0fd377a1e2509aa5a679",
            "avatarUrl": "/avatars/e04ef05bed0bf6cefdc7e3e39674e2f9.svg",
            "isPro": false,
            "fullname": "Marjan Ghazvininejad",
            "user": "mghazvininejad",
            "type": "user"
          },
          "name": "Marjan Ghazvininejad",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:28:15.386Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-14T17:59:41.000Z",
      "submittedOnDailyAt": "2025-03-18T01:59:35.073Z",
      "title": "reWordBench: 変換された入力を用いた報酬モデルの強固性のベンチマークと向上",
      "submittedOnDailyBy": {
        "_id": "6351712b40dffad651f128c7",
        "avatarUrl": "/avatars/87708c86c1baef548ef556f5d32dca71.svg",
        "isPro": false,
        "fullname": "Zhaofeng Wu",
        "user": "ZhaofengWu",
        "type": "user"
      },
      "summary": "レボリュードモデルは、現代のNLPにおいて標準的な機能として使用されています。それは、スケーラブルなテキスト評価器であり、多数のアラインメントのレシピと推論時アルゴリズムの不可欠な構成部分であることを示しています。しかし、最近のレボリュードモデルは標準ベンチマークでの性能向上を示しているが、これは一部では過学習の影響によるものであり、それがその本質的な能力を理解することを妨げている可能性があることを示しています。本稿では、レボリュードモデルの強固性と過学習の程度を詳しく調査します。私たちは**reWordBench**を構築し、レボリュードモデルの入力を意味やランキングを保存するようにシステマティックに変換します。私たちは、最先端のレボリュードモデルは、少しの入力変換でも性能が大幅に低下し、時にはランダム精度よりも显著に低くなることを示し、脆弱性を示していることを明らかにします。レボリュードモデルの強固性を向上させるために、私たちは、同じスコアを語意的な変形された文に割り当てることを明確に学習させることを提案し、このアプローチは他の異なる変換にも対して強固性を向上させることも示しています。例えば、我々の強固なレボリュードモデルは、RewardBenchのChat Hardサブセットでは、この低下を約半分程度に抑えることができます。また、アラインメントに使用される場合、我々の強固なレボリュードモデルは、より良い効用を示し、高品質な出力を生成し、標準的に学習されたRMと比較して、59%の場合でも勝ちます。",
      "upvotes": 12,
      "discussionId": "67d8e866fa59a8b15a92117c",
      "ai_keywords": [
        "reward models",
        "NLP",
        "text evaluator",
        "alignment",
        "inference-time algorithms",
        "overfitting",
        "reWordBench",
        "meaning-preserving",
        "ranking-preserving",
        "state-of-the-art",
        "performance degradation",
        "below-random accuracy",
        "brittleness",
        "paraphrases",
        "robust reward model",
        "Chat Hard subset",
        "RewardBench",
        "utility",
        "higher-quality outputs"
      ]
    },
    "publishedAt": "2025-03-14T13:59:41.000Z",
    "title": "reWordBench: Benchmarking and Improving the Robustness of Reward Models\n  with Transformed Inputs",
    "summary": "Reward models have become a staple in modern NLP, serving as not only a\nscalable text evaluator, but also an indispensable component in many alignment\nrecipes and inference-time algorithms. However, while recent reward models\nincrease performance on standard benchmarks, this may partly be due to\noverfitting effects, which would confound an understanding of their true\ncapability. In this work, we scrutinize the robustness of reward models and the\nextent of such overfitting. We build **reWordBench**, which systematically\ntransforms reward model inputs in meaning- or ranking-preserving ways. We show\nthat state-of-the-art reward models suffer from substantial performance\ndegradation even with minor input transformations, sometimes dropping to\nsignificantly below-random accuracy, suggesting brittleness. To improve reward\nmodel robustness, we propose to explicitly train them to assign similar scores\nto paraphrases, and find that this approach also improves robustness to other\ndistinct kinds of transformations. For example, our robust reward model reduces\nsuch degradation by roughly half for the Chat Hard subset in RewardBench.\nFurthermore, when used in alignment, our robust reward models demonstrate\nbetter utility and lead to higher-quality outputs, winning in up to 59% of\ninstances against a standardly trained RM.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.11751.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6351712b40dffad651f128c7",
      "avatarUrl": "/avatars/87708c86c1baef548ef556f5d32dca71.svg",
      "fullname": "Zhaofeng Wu",
      "name": "ZhaofengWu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.13399",
      "authors": [
        {
          "_id": "67d8d99a0983992037cdf33f",
          "user": {
            "_id": "650871aeb44445e9b3625c7b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/650871aeb44445e9b3625c7b/mtx3EnkuNF4z29IosnhaQ.png",
            "isPro": false,
            "fullname": "James Burgess",
            "user": "jmhb",
            "type": "user"
          },
          "name": "James Burgess",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-18T08:08:18.287Z",
          "hidden": false
        },
        {
          "_id": "67d8d99a0983992037cdf340",
          "name": "Jeffrey J Nirschl",
          "hidden": false
        },
        {
          "_id": "67d8d99a0983992037cdf341",
          "name": "Laura Bravo-Sánchez",
          "hidden": false
        },
        {
          "_id": "67d8d99a0983992037cdf342",
          "name": "Alejandro Lozano",
          "hidden": false
        },
        {
          "_id": "67d8d99a0983992037cdf343",
          "name": "Sanket Rajan Gupte",
          "hidden": false
        },
        {
          "_id": "67d8d99a0983992037cdf344",
          "name": "Jesus G. Galaz-Montoya",
          "hidden": false
        },
        {
          "_id": "67d8d99a0983992037cdf345",
          "name": "Yuhui Zhang",
          "hidden": false
        },
        {
          "_id": "67d8d99a0983992037cdf346",
          "user": {
            "_id": "666aa5183263a8feca6b7003",
            "avatarUrl": "/avatars/6ac4d52e8abea0df9f83da408502c076.svg",
            "isPro": false,
            "fullname": "Yuchang Su",
            "user": "suyc21",
            "type": "user"
          },
          "name": "Yuchang Su",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:27:03.753Z",
          "hidden": false
        },
        {
          "_id": "67d8d99a0983992037cdf347",
          "name": "Disha Bhowmik",
          "hidden": false
        },
        {
          "_id": "67d8d99a0983992037cdf348",
          "name": "Zachary Coman",
          "hidden": false
        },
        {
          "_id": "67d8d99a0983992037cdf349",
          "name": "Sarina M. Hasan",
          "hidden": false
        },
        {
          "_id": "67d8d99a0983992037cdf34a",
          "name": "Alexandra Johannesson",
          "hidden": false
        },
        {
          "_id": "67d8d99a0983992037cdf34b",
          "name": "William D. Leineweber",
          "hidden": false
        },
        {
          "_id": "67d8d99a0983992037cdf34c",
          "name": "Malvika G Nair",
          "hidden": false
        },
        {
          "_id": "67d8d99a0983992037cdf34d",
          "name": "Ridhi Yarlagadda",
          "hidden": false
        },
        {
          "_id": "67d8d99a0983992037cdf34e",
          "name": "Connor Zuraski",
          "hidden": false
        },
        {
          "_id": "67d8d99a0983992037cdf34f",
          "name": "Wah Chiu",
          "hidden": false
        },
        {
          "_id": "67d8d99a0983992037cdf350",
          "user": {
            "_id": "655b8dbb83186f133f7f8a98",
            "avatarUrl": "/avatars/15f41d0efb3a59a2e389cdb5338e0c1e.svg",
            "isPro": false,
            "fullname": "Sarah Cohen",
            "user": "shcohen",
            "type": "user"
          },
          "name": "Sarah Cohen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:26:24.760Z",
          "hidden": false
        },
        {
          "_id": "67d8d99a0983992037cdf351",
          "name": "Jan N. Hansen",
          "hidden": false
        },
        {
          "_id": "67d8d99a0983992037cdf352",
          "name": "Manuel D Leonetti",
          "hidden": false
        },
        {
          "_id": "67d8d99a0983992037cdf353",
          "user": {
            "_id": "64fc5c1cc45dd732acc2ec48",
            "avatarUrl": "/avatars/b1f072cbfec014f1a054d4a433cff93c.svg",
            "isPro": false,
            "fullname": "Chad Liu",
            "user": "chadliu",
            "type": "user"
          },
          "name": "Chad Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:25:42.249Z",
          "hidden": false
        },
        {
          "_id": "67d8d99a0983992037cdf354",
          "user": {
            "_id": "678af263320331c7e008f842",
            "avatarUrl": "/avatars/e2cde80f018f3dd278000270fdbc104d.svg",
            "isPro": false,
            "fullname": "emma lundberg",
            "user": "lundbergemma",
            "type": "user"
          },
          "name": "Emma Lundberg",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:25:33.946Z",
          "hidden": false
        },
        {
          "_id": "67d8d99a0983992037cdf355",
          "name": "Serena Yeung-Levy",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-17T17:33:10.000Z",
      "submittedOnDailyAt": "2025-03-18T01:06:54.667Z",
      "title": "MicroVQA: マイクロサイズビザービジョンに基づく科学研究の多模構造論ベンチマーク",
      "submittedOnDailyBy": {
        "_id": "650871aeb44445e9b3625c7b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/650871aeb44445e9b3625c7b/mtx3EnkuNF4z29IosnhaQ.png",
        "isPro": false,
        "fullname": "James Burgess",
        "user": "jmhb",
        "type": "user"
      },
      "summary": "科学研究要求对多模态数据进行复杂的推理，这一挑战在生物学中尤为普遍。尽管最近在多模态大语言模型（MLLMs）方面取得了进展，现有的多模态推理基准测试仅针对大学水平的难度，而研究级别的基准测试则强调低层次的感知，未能满足科学发现所需的复杂多模态推理。为了填补这一空白，我们介绍了MicroVQA，这是一个视觉问答（VQA）基准测试，旨在评估研究工作流程中至关重要的三种推理能力：专家图像理解、假设生成和实验提案。MicroVQA包含1,042道多项选择题（MCQs），由来自不同显微镜模式的生物学专家精心挑选，确保VQA样本代表真实的科学实践。在构建基准测试时，我们发现标准的MCQ生成方法会导致语言捷径，因此我们提出了一个新的两阶段管道：首先，通过优化的LLM提示将问题-答案对结构化为多项选择题；然后，基于代理的`RefineBot'更新这些问题以移除捷径。在使用最先进的MLLMs进行基准测试时，我们发现最高性能为53%；使用较小的LLM的模型仅略微落后于顶级模型，这表明基于语言的推理比多模态推理更不具挑战性；并且使用科学文章进行微调可以提高性能。对链式思维响应的专家分析显示，感知错误是最常见的，其次是知识错误，然后是过度概括错误。这些见解突显了多模态科学推理的挑战，表明MicroVQA是推动人工智能驱动的生物医学研究的宝贵资源。MicroVQA可在https://huggingface.co/datasets/jmhb/microvqa上获取，项目页面位于https://jmhb0.github.io/microvqa。",
      "upvotes": 10,
      "discussionId": "67d8d99f0983992037cdf47e",
      "projectPage": "https://jmhb0.github.io/microvqa/",
      "githubRepo": "https://github.com/jmhb0/microvqa",
      "ai_keywords": [
        "multimodal large language models",
        "visual-question answering (VQA)",
        "multiple-choice questions (MCQs)",
        "biology experts",
        "microscopy modalities",
        "standard MCQ generation methods",
        "optimized LLM prompt",
        "agent-based `RefineBot'",
        "chain-of-thought responses",
        "perception errors",
        "knowledge errors",
        "overgeneralization errors"
      ]
    },
    "publishedAt": "2025-03-17T13:33:10.000Z",
    "title": "MicroVQA: A Multimodal Reasoning Benchmark for Microscopy-Based\n  Scientific Research",
    "summary": "Scientific research demands sophisticated reasoning over multimodal data, a\nchallenge especially prevalent in biology. Despite recent advances in\nmultimodal large language models (MLLMs) for AI-assisted research, existing\nmultimodal reasoning benchmarks only target up to college-level difficulty,\nwhile research-level benchmarks emphasize lower-level perception, falling short\nof the complex multimodal reasoning needed for scientific discovery. To bridge\nthis gap, we introduce MicroVQA, a visual-question answering (VQA) benchmark\ndesigned to assess three reasoning capabilities vital in research workflows:\nexpert image understanding, hypothesis generation, and experiment proposal.\nMicroVQA consists of 1,042 multiple-choice questions (MCQs) curated by biology\nexperts across diverse microscopy modalities, ensuring VQA samples represent\nreal scientific practice. In constructing the benchmark, we find that standard\nMCQ generation methods induce language shortcuts, motivating a new two-stage\npipeline: an optimized LLM prompt structures question-answer pairs into MCQs;\nthen, an agent-based `RefineBot' updates them to remove shortcuts. Benchmarking\non state-of-the-art MLLMs reveal a peak performance of 53\\%; models with\nsmaller LLMs only slightly underperform top models, suggesting that\nlanguage-based reasoning is less challenging than multimodal reasoning; and\ntuning with scientific articles enhances performance. Expert analysis of\nchain-of-thought responses shows that perception errors are the most frequent,\nfollowed by knowledge errors and then overgeneralization errors. These insights\nhighlight the challenges in multimodal scientific reasoning, showing MicroVQA\nis a valuable resource advancing AI-driven biomedical research. MicroVQA is\navailable at https://huggingface.co/datasets/jmhb/microvqa, and project page at\nhttps://jmhb0.github.io/microvqa.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13399.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "650871aeb44445e9b3625c7b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/650871aeb44445e9b3625c7b/mtx3EnkuNF4z29IosnhaQ.png",
      "fullname": "James Burgess",
      "name": "jmhb",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.12605",
      "authors": [
        {
          "_id": "67d939f6fa59a8b15aa931a8",
          "user": {
            "_id": "64ff369d9abcc85a5519b33e",
            "avatarUrl": "/avatars/4b99cdaf5f970d930b196eddf1e5e499.svg",
            "isPro": false,
            "fullname": "Yaoting Wang",
            "user": "Gh0stAR",
            "type": "user"
          },
          "name": "Yaoting Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:29:33.845Z",
          "hidden": false
        },
        {
          "_id": "67d939f6fa59a8b15aa931a9",
          "user": {
            "_id": "64c139d867eff857ea51caa8",
            "avatarUrl": "/avatars/4b7b3f41c2e2cfa21dd43bbac6e081ae.svg",
            "isPro": false,
            "fullname": "Shengqiong Wu",
            "user": "ChocoWu",
            "type": "user"
          },
          "name": "Shengqiong Wu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:29:41.386Z",
          "hidden": false
        },
        {
          "_id": "67d939f6fa59a8b15aa931aa",
          "name": "Yuecheng Zhang",
          "hidden": false
        },
        {
          "_id": "67d939f6fa59a8b15aa931ab",
          "name": "William Wang",
          "hidden": false
        },
        {
          "_id": "67d939f6fa59a8b15aa931ac",
          "user": {
            "_id": "62ab1ac1d48b4d8b048a3473",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656826685333-62ab1ac1d48b4d8b048a3473.png",
            "isPro": false,
            "fullname": "Ziwei Liu",
            "user": "liuziwei7",
            "type": "user"
          },
          "name": "Ziwei Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:30:17.710Z",
          "hidden": false
        },
        {
          "_id": "67d939f6fa59a8b15aa931ad",
          "name": "Jiebo Luo",
          "hidden": false
        },
        {
          "_id": "67d939f6fa59a8b15aa931ae",
          "user": {
            "_id": "647773a1168cb428e00e9a8f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647773a1168cb428e00e9a8f/NiRR3ScY6Plzjibfwy1hC.jpeg",
            "isPro": false,
            "fullname": "Hao Fei",
            "user": "scofield7419",
            "type": "user"
          },
          "name": "Hao Fei",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-18T09:30:49.867Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64ff369d9abcc85a5519b33e/YhPHIT3BwUWkXcOT1r1LJ.png",
        "https://cdn-uploads.huggingface.co/production/uploads/64ff369d9abcc85a5519b33e/7SvBogepNT-uNYcY5dAgv.png",
        "https://cdn-uploads.huggingface.co/production/uploads/64ff369d9abcc85a5519b33e/VqwQO1NsLKrjaKEBZxXUb.png"
      ],
      "publishedAt": "2025-03-16T18:39:13.000Z",
      "submittedOnDailyAt": "2025-03-18T07:53:47.429Z",
      "title": "多モデルコースオブシンクス実装: 完全な調査",
      "submittedOnDailyBy": {
        "_id": "64ff369d9abcc85a5519b33e",
        "avatarUrl": "/avatars/4b99cdaf5f970d930b196eddf1e5e499.svg",
        "isPro": false,
        "fullname": "Yaoting Wang",
        "user": "Gh0stAR",
        "type": "user"
      },
      "summary": "連鎖コンシュープ（CoT）論理の人間のようなステップごとのプロセスの優位性を多タイプコンテキストに拡張した多タイプCoT（MCoT）論理は、特に多タイプ大語言モデル（MLLMs）との統合において最近により重要な研究注目を集めており、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボット、医療、自動運転、および多タイプ生成のような機能の応用において極めて成功を収めています。しかし、MCoTは、ロボ",
      "upvotes": 8,
      "discussionId": "67d939f7fa59a8b15aa9322a",
      "projectPage": "https://github.com/yaotingwangofficial/Awesome-MCoT",
      "githubRepo": "https://github.com/yaotingwangofficial/Awesome-MCoT",
      "ai_keywords": [
        "chain-of-thought (CoT) reasoning",
        "multimodal CoT (MCoT) reasoning",
        "multimodal large language models (MLLMs)",
        "image",
        "video",
        "speech",
        "audio",
        "3D",
        "structured data",
        "robotics",
        "healthcare",
        "autonomous driving",
        "multimodal generation",
        "multimodal AGI"
      ]
    },
    "publishedAt": "2025-03-16T14:39:13.000Z",
    "title": "Multimodal Chain-of-Thought Reasoning: A Comprehensive Survey",
    "summary": "By extending the advantage of chain-of-thought (CoT) reasoning in human-like\nstep-by-step processes to multimodal contexts, multimodal CoT (MCoT) reasoning\nhas recently garnered significant research attention, especially in the\nintegration with multimodal large language models (MLLMs). Existing MCoT\nstudies design various methodologies and innovative reasoning paradigms to\naddress the unique challenges of image, video, speech, audio, 3D, and\nstructured data across different modalities, achieving extensive success in\napplications such as robotics, healthcare, autonomous driving, and multimodal\ngeneration. However, MCoT still presents distinct challenges and opportunities\nthat require further focus to ensure consistent thriving in this field, where,\nunfortunately, an up-to-date review of this domain is lacking. To bridge this\ngap, we present the first systematic survey of MCoT reasoning, elucidating the\nrelevant foundational concepts and definitions. We offer a comprehensive\ntaxonomy and an in-depth analysis of current methodologies from diverse\nperspectives across various application scenarios. Furthermore, we provide\ninsights into existing challenges and future research directions, aiming to\nfoster innovation toward multimodal AGI.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64ff369d9abcc85a5519b33e/YhPHIT3BwUWkXcOT1r1LJ.png",
      "https://cdn-uploads.huggingface.co/production/uploads/64ff369d9abcc85a5519b33e/7SvBogepNT-uNYcY5dAgv.png",
      "https://cdn-uploads.huggingface.co/production/uploads/64ff369d9abcc85a5519b33e/VqwQO1NsLKrjaKEBZxXUb.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12605.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ff369d9abcc85a5519b33e",
      "avatarUrl": "/avatars/4b99cdaf5f970d930b196eddf1e5e499.svg",
      "fullname": "Yaoting Wang",
      "name": "Gh0stAR",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.12937",
      "authors": [
        {
          "_id": "67d8eb0c18de6ef86c4eb457",
          "name": "Jingyi Zhang",
          "hidden": false
        },
        {
          "_id": "67d8eb0c18de6ef86c4eb458",
          "user": {
            "_id": "65237910b80dc49ba03a96d9",
            "avatarUrl": "/avatars/9d81c4c8fb2d597079e8dd9d9b79a8d8.svg",
            "isPro": false,
            "fullname": "jiaxing",
            "user": "huangjiaxing",
            "type": "user"
          },
          "name": "Jiaxing Huang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:31:01.731Z",
          "hidden": false
        },
        {
          "_id": "67d8eb0c18de6ef86c4eb459",
          "user": {
            "_id": "6590e03454f8826173ed5ee6",
            "avatarUrl": "/avatars/b2fbaaf444e1e53c5e914cd42a41389a.svg",
            "isPro": false,
            "fullname": "Huanjin Yao",
            "user": "HuanjinYao",
            "type": "user"
          },
          "name": "Huanjin Yao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:31:08.069Z",
          "hidden": false
        },
        {
          "_id": "67d8eb0c18de6ef86c4eb45a",
          "user": {
            "_id": "6713afea187a20dc579e121b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6713afea187a20dc579e121b/ELxVQLVF9ifuT-TCWPK22.jpeg",
            "isPro": false,
            "fullname": "Shunyu Liu",
            "user": "liushunyu",
            "type": "user"
          },
          "name": "Shunyu Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:31:15.587Z",
          "hidden": false
        },
        {
          "_id": "67d8eb0c18de6ef86c4eb45b",
          "user": {
            "_id": "6274a9620d11b4f675085fbc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1651812606924-noauth.jpeg",
            "isPro": false,
            "fullname": "Xikun Zhang",
            "user": "Xikun",
            "type": "user"
          },
          "name": "Xikun Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:31:21.950Z",
          "hidden": false
        },
        {
          "_id": "67d8eb0c18de6ef86c4eb45c",
          "name": "Shijian Lu",
          "hidden": false
        },
        {
          "_id": "67d8eb0c18de6ef86c4eb45d",
          "name": "Dacheng Tao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-17T08:51:44.000Z",
      "submittedOnDailyAt": "2025-03-18T02:10:10.429Z",
      "title": "R1-VL: 多モデル大語言モデルを用いた理由論学の学習を行うためのステップごとのグループ相対的なポリシー最適化法",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "最近の研究は、高品質なコンティンュースの推理データによる規範的な微調論理モデル（MLLMs）の推理能力を向上させていますが、これは通常成功した推理パスを模倣しながら何が間違っているかを理解しないことにより、そのみを模倣するだけになります。本論文では、MLLMsの推理能力を間接的に模倣することを超えることを目指しています。そのために、新しいオンライン再励励効率学習フレームワーク「Step-wise Group Relative Policy Optimization（StepGRPO）」を設計しました。このフレームワークは、単純で効果的な、密度の高いステップごとの報酬を通じて、MLLMsの自己改善の推理能力を可能にします。特に、StepGRPOは、2つの新しいルールベースの推理報酬を導入します：「Step-wise Reasoning Accuracy Reward（StepRAR）」と「Step-wise Reasoning Validity Reward（StepRVR）」。StepRARは必要な中間推理ステップを含む推理パスによる柔軟なキーステップマッチング手法を通じて報酬を与え、StepRVRはロジック的に一貫した推理プロセスを通じて、推理の完全性とロジック評価戦略を通じて報酬を与えます。提案されたStepGRPOにより、R1-VLという、ステップごとの推理の出色な能力を持つMLLMsのシリーズを導入しました。8ベンチマークでの拡張的な実験は、我々の方法の優れた性能を示しました。",
      "upvotes": 7,
      "discussionId": "67d8eb0d18de6ef86c4eb4aa",
      "ai_keywords": [
        "Step-wise Group Relative Policy Optimization (StepGRPO)",
        "online reinforcement learning",
        "Step-wise Reasoning Accuracy Reward (StepRAR)",
        "Step-wise Reasoning Validity Reward (StepRVR)",
        "soft key-step matching",
        "reasoning completeness",
        "logic evaluation",
        "R1-VL",
        "step-by-step reasoning"
      ]
    },
    "publishedAt": "2025-03-17T04:51:44.000Z",
    "title": "R1-VL: Learning to Reason with Multimodal Large Language Models via\n  Step-wise Group Relative Policy Optimization",
    "summary": "Recent studies generally enhance MLLMs' reasoning capabilities via supervised\nfine-tuning on high-quality chain-of-thought reasoning data, which often leads\nmodels to merely imitate successful reasoning paths without understanding what\nthe wrong reasoning paths are. In this work, we aim to enhance the MLLMs'\nreasoning ability beyond passively imitating positive reasoning paths. To this\nend, we design Step-wise Group Relative Policy Optimization (StepGRPO), a new\nonline reinforcement learning framework that enables MLLMs to self-improve\nreasoning ability via simple, effective and dense step-wise rewarding.\nSpecifically, StepGRPO introduces two novel rule-based reasoning rewards:\nStep-wise Reasoning Accuracy Reward (StepRAR) and Step-wise Reasoning Validity\nReward (StepRVR). StepRAR rewards the reasoning paths that contain necessary\nintermediate reasoning steps via a soft key-step matching technique, while\nStepRAR rewards reasoning paths that follow a well-structured and logically\nconsistent reasoning process through a reasoning completeness and logic\nevaluation strategy. With the proposed StepGRPO, we introduce R1-VL, a series\nof MLLMs with outstanding capabilities in step-by-step reasoning. Extensive\nexperiments over 8 benchmarks demonstrate the superiority of our methods.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12937.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6390
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.13444",
      "authors": [
        {
          "_id": "67d8eeb17e184aa2954d19f4",
          "name": "Ye Liu",
          "hidden": false
        },
        {
          "_id": "67d8eeb17e184aa2954d19f5",
          "user": {
            "_id": "64440be5af034cdfd69ca3a7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64440be5af034cdfd69ca3a7/qmx24QiDFT29vleCxL9TX.jpeg",
            "isPro": true,
            "fullname": "Qinghong (Kevin) Lin",
            "user": "KevinQHLin",
            "type": "user"
          },
          "name": "Kevin Qinghong Lin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:28:47.081Z",
          "hidden": false
        },
        {
          "_id": "67d8eeb17e184aa2954d19f6",
          "name": "Chang Wen Chen",
          "hidden": false
        },
        {
          "_id": "67d8eeb17e184aa2954d19f7",
          "user": {
            "_id": "661ab3da2b14565c7acccf5c",
            "avatarUrl": "/avatars/fa4fc03664803e02aede4d4c3d50b393.svg",
            "isPro": false,
            "fullname": "Mike Zheng Shou",
            "user": "AnalMom",
            "type": "user"
          },
          "name": "Mike Zheng Shou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:29:07.309Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-17T17:59:33.000Z",
      "submittedOnDailyAt": "2025-03-18T02:25:58.731Z",
      "title": "VideoMind: 長ビデオ論理のChain-of-LoRAアグリート",
      "submittedOnDailyBy": {
        "_id": "64440be5af034cdfd69ca3a7",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64440be5af034cdfd69ca3a7/qmx24QiDFT29vleCxL9TX.jpeg",
        "isPro": true,
        "fullname": "Qinghong (Kevin) Lin",
        "user": "KevinQHLin",
        "type": "user"
      },
      "summary": "Videos, with their unique temporal dimension, demand precise grounded understanding, where answers are directly linked to visual, interpretable evidence. Despite significant breakthroughs in reasoning capabilities within Large Language Models, multi-modal reasoning - especially for videos - remains unexplored. In this work, we introduce VideoMind, a novel video-language agent designed for temporal-grounded video understanding. VideoMind incorporates two key innovations: (i) We identify essential capabilities for video temporal reasoning and develop a role-based agentic workflow, including a planner for coordinating different roles, a grounder for temporal localization, a verifier to assess temporal interval accuracy, and an answerer for question-answering. (ii) To efficiently integrate these diverse roles, we propose a novel Chain-of-LoRA strategy, enabling seamless role-switching via lightweight LoRA adaptors while avoiding the overhead of multiple models, thus balancing efficiency and flexibility. Extensive experiments on 14 public benchmarks demonstrate that our agent achieves state-of-the-art performance on diverse video understanding tasks, including 3 on grounded video question-answering, 6 on video temporal grounding, and 5 on general video question-answering, underscoring its effectiveness in advancing video agent and long-form temporal reasoning.",
      "upvotes": 6,
      "discussionId": "67d8eeb37e184aa2954d1a39",
      "ai_keywords": [
        "VideoMind",
        "temporal-grounded video understanding",
        "role-based agentic workflow",
        "planner",
        "grounder",
        "temporale localization",
        "verifier",
        "temporal interval accuracy",
        "answerer",
        "question-answering",
        "Chain-of-LoRA",
        "LoRA adaptors",
        "grounded video question-answering",
        "video temporal grounding",
        "general video question-answering",
        "temporal reasoning"
      ]
    },
    "publishedAt": "2025-03-17T13:59:33.000Z",
    "title": "VideoMind: A Chain-of-LoRA Agent for Long Video Reasoning",
    "summary": "Videos, with their unique temporal dimension, demand precise grounded\nunderstanding, where answers are directly linked to visual, interpretable\nevidence. Despite significant breakthroughs in reasoning capabilities within\nLarge Language Models, multi-modal reasoning - especially for videos - remains\nunexplored. In this work, we introduce VideoMind, a novel video-language agent\ndesigned for temporal-grounded video understanding. VideoMind incorporates two\nkey innovations: (i) We identify essential capabilities for video temporal\nreasoning and develop a role-based agentic workflow, including a planner for\ncoordinating different roles, a grounder for temporal localization, a verifier\nto assess temporal interval accuracy, and an answerer for question-answering.\n(ii) To efficiently integrate these diverse roles, we propose a novel\nChain-of-LoRA strategy, enabling seamless role-switching via lightweight LoRA\nadaptors while avoiding the overhead of multiple models, thus balancing\nefficiency and flexibility. Extensive experiments on 14 public benchmarks\ndemonstrate that our agent achieves state-of-the-art performance on diverse\nvideo understanding tasks, including 3 on grounded video question-answering, 6\non video temporal grounding, and 5 on general video question-answering,\nunderscoring its effectiveness in advancing video agent and long-form temporal\nreasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13444.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64440be5af034cdfd69ca3a7",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64440be5af034cdfd69ca3a7/qmx24QiDFT29vleCxL9TX.jpeg",
      "fullname": "Qinghong (Kevin) Lin",
      "name": "KevinQHLin",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isMod": false,
      "followerCount": 21
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.13070",
      "authors": [
        {
          "_id": "67d8fbf641d31cc626e4d7b9",
          "user": {
            "_id": "65f7e6856bd4bac5b6a4ecc3",
            "avatarUrl": "/avatars/9c0c48310fb5e99c19700316a0ab531e.svg",
            "isPro": false,
            "fullname": "Yihong Luo",
            "user": "Luo-Yihong",
            "type": "user"
          },
          "name": "Yihong Luo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-18T08:07:30.236Z",
          "hidden": false
        },
        {
          "_id": "67d8fbf641d31cc626e4d7ba",
          "user": {
            "_id": "636a40faa6f948c4f0c62ae5",
            "avatarUrl": "/avatars/30c35b194ba84d6e274df30e91a8cc45.svg",
            "isPro": false,
            "fullname": "Tianyang Hu",
            "user": "whatlegequ",
            "type": "user"
          },
          "name": "Tianyang Hu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:33:01.489Z",
          "hidden": false
        },
        {
          "_id": "67d8fbf641d31cc626e4d7bb",
          "name": "Weijian Luo",
          "hidden": false
        },
        {
          "_id": "67d8fbf641d31cc626e4d7bc",
          "name": "Kenji Kawaguchi",
          "hidden": false
        },
        {
          "_id": "67d8fbf641d31cc626e4d7bd",
          "name": "Jing Tang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-17T11:21:43.000Z",
      "submittedOnDailyAt": "2025-03-18T03:24:24.806Z",
      "title": "Rewards は、高速な写真質の画像を生成するために十分です。",
      "submittedOnDailyBy": {
        "_id": "65f7e6856bd4bac5b6a4ecc3",
        "avatarUrl": "/avatars/9c0c48310fb5e99c19700316a0ab531e.svg",
        "isPro": false,
        "fullname": "Yihong Luo",
        "user": "Luo-Yihong",
        "type": "user"
      },
      "summary": "生成画像を複雑なテキストプラントと人間の好みに合わせることは、人工知能ジェネレーションコンテンツ（AIGC）の中心的な課題です。文から画像への変換モデルの制御可能性と忠実度を向上させるために、報酬を増強したディフュージョンディスティルーションが優れたアプローチとして発展しています。これにより、条件が特定化され、報酬シグナルが強くなることで、報酬自身が生成の主導力となる基本的なパラダイムの変更が見出されます。逆に、ディフュージョン損失は過剰な調整として見られます。この仮説を詳しく検証するために、R0という新しい条件付き生成アプローチを提案します。R0は、複雑な条件での報酬の主導力を示すことを示し、ディフュージョン損失を信頼しないで、画像生成をデータ空間での最適化問題として扱う新しい視点を提案します。ジェネレータパラメータの独自の設計と適切な調整手法をもとに、R0を用いて、シンプルなステップで最先端の文から画像への生成モデルを訓練します。この結果は、ディフュージョンの後処理と条件付き生成の伝統的な観点を挑戦し、複雑な条件での報酬の主導力を示します。私たちの研究結果は、AIGCのより広い分野での人間中心的および報酬中心的な生成パラダイムの進展に貢献したいと思います。コードは、https://github.com/Luo-Yihong/R0 に提供されています。",
      "upvotes": 5,
      "discussionId": "67d8fbf841d31cc626e4d812",
      "githubRepo": "https://github.com/Luo-Yihong/R0",
      "ai_keywords": [
        "reward-enhanced diffusion distillation",
        "diffusion losses",
        "R0",
        "regularized reward maximization",
        "optimization problem in data space",
        "compositional rewards",
        "generator parameterization",
        "state-of-the-art few-step text-to-image generative models",
        "diffusion post-training",
        "human-centric generation",
        "reward-centric generation paradigms"
      ]
    },
    "publishedAt": "2025-03-17T07:21:43.000Z",
    "title": "Rewards Are Enough for Fast Photo-Realistic Text-to-image Generation",
    "summary": "Aligning generated images to complicated text prompts and human preferences\nis a central challenge in Artificial Intelligence-Generated Content (AIGC).\nWith reward-enhanced diffusion distillation emerging as a promising approach\nthat boosts controllability and fidelity of text-to-image models, we identify a\nfundamental paradigm shift: as conditions become more specific and reward\nsignals stronger, the rewards themselves become the dominant force in\ngeneration. In contrast, the diffusion losses serve as an overly expensive form\nof regularization. To thoroughly validate our hypothesis, we introduce R0, a\nnovel conditional generation approach via regularized reward maximization.\nInstead of relying on tricky diffusion distillation losses, R0 proposes a new\nperspective that treats image generations as an optimization problem in data\nspace which aims to search for valid images that have high compositional\nrewards. By innovative designs of the generator parameterization and proper\nregularization techniques, we train state-of-the-art few-step text-to-image\ngenerative models with R0 at scales. Our results challenge the conventional\nwisdom of diffusion post-training and conditional generation by demonstrating\nthat rewards play a dominant role in scenarios with complex conditions. We hope\nour findings can contribute to further research into human-centric and\nreward-centric generation paradigms across the broader field of AIGC. Code is\navailable at https://github.com/Luo-Yihong/R0.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13070.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65f7e6856bd4bac5b6a4ecc3",
      "avatarUrl": "/avatars/9c0c48310fb5e99c19700316a0ab531e.svg",
      "fullname": "Yihong Luo",
      "name": "Luo-Yihong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.11495",
      "authors": [
        {
          "_id": "67d8ca56e94f1237cb3ba3ca",
          "user": {
            "_id": "667ee096b0fad0fdee319ed4",
            "avatarUrl": "/avatars/d9df687e8522d47f7fcefe40fd9b575b.svg",
            "isPro": false,
            "fullname": "Zixu Cheng",
            "user": "Cade921",
            "type": "user"
          },
          "name": "Zixu Cheng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:31:58.589Z",
          "hidden": false
        },
        {
          "_id": "67d8ca56e94f1237cb3ba3cb",
          "user": {
            "_id": "65e1b6e9501590df0173cbd3",
            "avatarUrl": "/avatars/a73e2139700e23eff455734c99cef5ba.svg",
            "isPro": false,
            "fullname": "Jian Hu",
            "user": "lwpyh",
            "type": "user"
          },
          "name": "Jian Hu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-18T08:08:20.643Z",
          "hidden": false
        },
        {
          "_id": "67d8ca56e94f1237cb3ba3cc",
          "name": "Ziquan Liu",
          "hidden": false
        },
        {
          "_id": "67d8ca56e94f1237cb3ba3cd",
          "user": {
            "_id": "635f8ed47c05eb9f59963d3a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/635f8ed47c05eb9f59963d3a/uQf4p9N9pSaFy87Wg9v4k.jpeg",
            "isPro": false,
            "fullname": "ChenyangSi",
            "user": "ChenyangSi",
            "type": "user"
          },
          "name": "Chenyang Si",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:32:21.924Z",
          "hidden": false
        },
        {
          "_id": "67d8ca56e94f1237cb3ba3ce",
          "name": "Wei Li",
          "hidden": false
        },
        {
          "_id": "67d8ca56e94f1237cb3ba3cf",
          "name": "Shaogang Gong",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/65e1b6e9501590df0173cbd3/A2JrOm6FBAq5keCRRdVMK.png"
      ],
      "publishedAt": "2025-03-14T15:21:44.000Z",
      "submittedOnDailyAt": "2025-03-18T01:11:08.161Z",
      "title": "V-STaR: 映像スペクトラル空間時間理由論のベンチマークを行う映像LLMs",
      "submittedOnDailyBy": {
        "_id": "65e1b6e9501590df0173cbd3",
        "avatarUrl": "/avatars/a73e2139700e23eff455734c99cef5ba.svg",
        "isPro": false,
        "fullname": "Jian Hu",
        "user": "lwpyh",
        "type": "user"
      },
      "summary": "人間は、順次空間時系列的論理で映画の理由を理解します。まず、関連するフレーム（\"どの時間であるか\"）を特定し、次に、キーオブジェクトの空間関係（\"どこにあるか\"）を分析し、最後に、これらの関係を利用して推論（\"どのことか\"）を行います。しかし、映画の大規模な言語モデル（Video-LLMs）も映画で「順次空間時系列的論理を通じて理由を見出す」ことができるかどうかは不明です。現在のVideo-LLMベンチマークは主にオブジェクトの存在を評価し、関係的な論理を飛ばしています。そのため、モデルが映画のオブジェクトの相互作用（行動/イベント）を真に理解しているか、またはそれらの共発生の「メモリ」を基に回答を生成しているかを評価することが難しいです。本稿では、これらの欠点を解決するために、映画の空間時系列的論理（V-STaR）ベンチマークを導入します。主なアイデアは、映画の理解を逆向き空間時系列的論理（RSTR）タスクに分解し、オブジェクトの存在、イベントの発生時間、その位置を同時に評価し、その下層のChain-of-thought（CoT）論理を捉えることです。この評価を支援するために、Video-LLMsの空間時系列的論理プロセスを引き出すデータセットを構築します。これは、半自動化プロセスで生成された、GPT-4を基にしたコーストフィードワークで、明示的な論理連鎖を含めて人間の認知をミニマップします。14つのVideo-LLMsの実験から、我々のV-STaRでは、現在のVideo-LLMsと強固かつ一貫した空間時系列的論理の必要との間の間違いが明らかになりました。",
      "upvotes": 5,
      "discussionId": "67d8ca59e94f1237cb3ba47c",
      "ai_keywords": [
        "Video Large Language Models (Video-LLMs)",
        "Reverse Spatio-Temporal Reasoning (RSTR)",
        "Chain-of-thought (CoT)",
        "GPT-4",
        "Video Spatio-Temporal Reasoning (V-STaR)",
        "CoT questions",
        "CoT logic",
        "human cognition"
      ]
    },
    "publishedAt": "2025-03-14T11:21:44.000Z",
    "title": "V-STaR: Benchmarking Video-LLMs on Video Spatio-Temporal Reasoning",
    "summary": "Human processes video reasoning in a sequential spatio-temporal reasoning\nlogic, we first identify the relevant frames (\"when\") and then analyse the\nspatial relationships (\"where\") between key objects, and finally leverage these\nrelationships to draw inferences (\"what\"). However, can Video Large Language\nModels (Video-LLMs) also \"reason through a sequential spatio-temporal logic\" in\nvideos? Existing Video-LLM benchmarks primarily focus on assessing object\npresence, neglecting relational reasoning. Consequently, it is difficult to\nmeasure whether a model truly comprehends object interactions (actions/events)\nin videos or merely relies on pre-trained \"memory\" of co-occurrences as biases\nin generating answers. In this work, we introduce a Video Spatio-Temporal\nReasoning (V-STaR) benchmark to address these shortcomings. The key idea is to\ndecompose video understanding into a Reverse Spatio-Temporal Reasoning (RSTR)\ntask that simultaneously evaluates what objects are present, when events occur,\nand where they are located while capturing the underlying Chain-of-thought\n(CoT) logic. To support this evaluation, we construct a dataset to elicit the\nspatial-temporal reasoning process of Video-LLMs. It contains coarse-to-fine\nCoT questions generated by a semi-automated GPT-4-powered pipeline, embedding\nexplicit reasoning chains to mimic human cognition. Experiments from 14\nVideo-LLMs on our V-STaR reveal significant gaps between current Video-LLMs and\nthe needs for robust and consistent spatio-temporal reasoning.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/65e1b6e9501590df0173cbd3/A2JrOm6FBAq5keCRRdVMK.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.11495.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65e1b6e9501590df0173cbd3",
      "avatarUrl": "/avatars/a73e2139700e23eff455734c99cef5ba.svg",
      "fullname": "Jian Hu",
      "name": "lwpyh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.11412",
      "authors": [
        {
          "_id": "67d8f8b77f61dda9ea6512b7",
          "name": "Shiyuan Yang",
          "hidden": false
        },
        {
          "_id": "67d8f8b77f61dda9ea6512b8",
          "user": {
            "_id": "678f49878af7a399877b87c0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/3IHnM4AKbIW_wmL15wdZf.png",
            "isPro": false,
            "fullname": "GuZheng",
            "user": "GuZheng",
            "type": "user"
          },
          "name": "Zheng Gu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:33:49.096Z",
          "hidden": false
        },
        {
          "_id": "67d8f8b77f61dda9ea6512b9",
          "user": {
            "_id": "64560a2aaaaf85a98fa9a4b9",
            "avatarUrl": "/avatars/e81e21f353baf48f0d91bf29ad200eea.svg",
            "isPro": false,
            "fullname": "Liang Hou",
            "user": "lianghou",
            "type": "user"
          },
          "name": "Liang Hou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:33:57.501Z",
          "hidden": false
        },
        {
          "_id": "67d8f8b77f61dda9ea6512ba",
          "name": "Xin Tao",
          "hidden": false
        },
        {
          "_id": "67d8f8b77f61dda9ea6512bb",
          "user": {
            "_id": "662f93942510ef5735d7ad00",
            "avatarUrl": "/avatars/dc9486db75869ce902d0a638eea126bd.svg",
            "isPro": false,
            "fullname": "magicwpf",
            "user": "magicwpf",
            "type": "user"
          },
          "name": "Pengfei Wan",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-03-18T04:42:06.437Z",
          "hidden": false
        },
        {
          "_id": "67d8f8b77f61dda9ea6512bc",
          "user": {
            "_id": "67be7cd21616162fc336cb44",
            "avatarUrl": "/avatars/e58cc3c2d1484419222a5ccfc11f5c48.svg",
            "isPro": false,
            "fullname": "Xiaodong Chen",
            "user": "XiaodongChen",
            "type": "user"
          },
          "name": "Xiaodong Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:34:15.858Z",
          "hidden": false
        },
        {
          "_id": "67d8f8b77f61dda9ea6512bd",
          "user": {
            "_id": "65e77726767bfc7d109c45bf",
            "avatarUrl": "/avatars/24e68c86e06055ea1209598ba49ce8b9.svg",
            "isPro": false,
            "fullname": "Jing Liao",
            "user": "CeciliaJL",
            "type": "user"
          },
          "name": "Jing Liao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:34:23.653Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63316d499e3604f3f17f5d89/jV8ETjZh0NTDoFBhSNv8I.mp4"
      ],
      "publishedAt": "2025-03-14T13:54:10.000Z",
      "submittedOnDailyAt": "2025-03-18T03:20:01.050Z",
      "title": "MTV-Inpaint: 多タスク長ビデオインプレイング",
      "submittedOnDailyBy": {
        "_id": "63316d499e3604f3f17f5d89",
        "avatarUrl": "/avatars/c7e0c8bdd7e598276cfd133d5222c5e8.svg",
        "isPro": false,
        "fullname": "catfood",
        "user": "ysy31415926",
        "type": "user"
      },
      "summary": "Video inpaintingは、映像内の局所領域を変更し、空間的および時系列的な一致性を確保することを含む。現在のメソッドでは、主にスペースンコンペリーント（スペースンコンペリーント、即、欠損領域を埋める）に焦点を当てていて、新しい物体をスペースにコントロールされたように挿入する能力が欠りている。幸いながら、最近のテキストから映像への（T2V）拡散モデルの進歩は、テキストガイドされた映像inpaintingにつながるようになった。しかし、T2Vモデルを直接inpaintingに適用することは、完了と挿入の両タスクの統合、入力のコントロール可能性、長い映像の処理において限界があり、その応用可能性と柔軟性が制限されている。これらの課題を解決するために、我々は、単一のフレームワークでスペースンコンペリーントと新しい物体挿入の両タスクを処理できる統一的な多タスク映像inpaintingフレームワーク、MTV-Inpaintを提案しています。これらの違うタスクを統合するために、T2V拡散U-Net内では、スペースアテンション機構を双層構造として設計し、スペースンコンペリーントと物体挿入の無間違なりの統合を可能にしています。テキストガイドだけでなく、MTV-Inpaintは、我々が提案した画像から映像への（I2V）inpaintingモードを組み込み、多タイプコントロールを支援しています。また、我々は、キーフレームinpaintingと間接フレームの伝播を組み合わせた2ステップパイプラインを提案し、MTV-Inpaintが数百フレームの長い映像を効果的に処理できるようにしています。拡張された実験により、MTV-Inpaintはスペースンコンペリーントと物体挿入の両タスクで最先端の性能を収めていることが示され、マルチモードinpainting、物体編集、除去、画像物体ブラシ、長い映像の処理能力など、拡張可能なアプリケーションの多様性を示しています。プロジェクトページは、https://mtv-inpaint.github.io/ です。",
      "upvotes": 5,
      "discussionId": "67d8f8bf7f61dda9ea6514a7",
      "projectPage": "https://mtv-inpaint.github.io/",
      "ai_keywords": [
        "text-to-video (T2V) diffusion models",
        "text-guided video inpainting",
        "dual-branch spatial attention mechanism",
        "T2V diffusion U-Net",
        "multimodal control",
        "image-to-video (I2V) inpainting mode",
        "keyframe inpainting",
        "in-between frame propagation",
        "multi-modal inpainting",
        "object editing",
        "object removal",
        "image object brush"
      ]
    },
    "publishedAt": "2025-03-14T09:54:10.000Z",
    "title": "MTV-Inpaint: Multi-Task Long Video Inpainting",
    "summary": "Video inpainting involves modifying local regions within a video, ensuring\nspatial and temporal consistency. Most existing methods focus primarily on\nscene completion (i.e., filling missing regions) and lack the capability to\ninsert new objects into a scene in a controllable manner. Fortunately, recent\nadvancements in text-to-video (T2V) diffusion models pave the way for\ntext-guided video inpainting. However, directly adapting T2V models for\ninpainting remains limited in unifying completion and insertion tasks, lacks\ninput controllability, and struggles with long videos, thereby restricting\ntheir applicability and flexibility. To address these challenges, we propose\nMTV-Inpaint, a unified multi-task video inpainting framework capable of\nhandling both traditional scene completion and novel object insertion tasks. To\nunify these distinct tasks, we design a dual-branch spatial attention mechanism\nin the T2V diffusion U-Net, enabling seamless integration of scene completion\nand object insertion within a single framework. In addition to textual\nguidance, MTV-Inpaint supports multimodal control by integrating various image\ninpainting models through our proposed image-to-video (I2V) inpainting mode.\nAdditionally, we propose a two-stage pipeline that combines keyframe inpainting\nwith in-between frame propagation, enabling MTV-Inpaint to effectively handle\nlong videos with hundreds of frames. Extensive experiments demonstrate that\nMTV-Inpaint achieves state-of-the-art performance in both scene completion and\nobject insertion tasks. Furthermore, it demonstrates versatility in derived\napplications such as multi-modal inpainting, object editing, removal, image\nobject brush, and the ability to handle long videos. Project page:\nhttps://mtv-inpaint.github.io/.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63316d499e3604f3f17f5d89/jV8ETjZh0NTDoFBhSNv8I.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.11412.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63316d499e3604f3f17f5d89",
      "avatarUrl": "/avatars/c7e0c8bdd7e598276cfd133d5222c5e8.svg",
      "fullname": "catfood",
      "name": "ysy31415926",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.10704",
      "authors": [
        {
          "_id": "67d7eba831dd5b46c3e6fdcb",
          "user": {
            "_id": "633c2310c0fb6fd232f0accf",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/633c2310c0fb6fd232f0accf/AnvEvxMtd-BpSZbaoA4ha.jpeg",
            "isPro": false,
            "fullname": "Wang Jing",
            "user": "k-nick",
            "type": "user"
          },
          "name": "Jing Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-18T08:09:01.169Z",
          "hidden": false
        },
        {
          "_id": "67d7eba831dd5b46c3e6fdcc",
          "user": {
            "_id": "64b8c1a995bd42c7707f7918",
            "avatarUrl": "/avatars/08c2929f8f150ecd6f8e5a06c4cb9034.svg",
            "isPro": false,
            "fullname": "Fengzhuo Zhang",
            "user": "Fengzhuo",
            "type": "user"
          },
          "name": "Fengzhuo Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:34:37.712Z",
          "hidden": false
        },
        {
          "_id": "67d7eba831dd5b46c3e6fdcd",
          "user": {
            "_id": "67aa01782183876b1ec5760f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/hZd1iBn_2yjHVoavcXPQo.png",
            "isPro": false,
            "fullname": "xiaolili",
            "user": "xiaolili",
            "type": "user"
          },
          "name": "Xiaoli Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:34:47.006Z",
          "hidden": false
        },
        {
          "_id": "67d7eba831dd5b46c3e6fdce",
          "name": "Vincent Y. F. Tan",
          "hidden": false
        },
        {
          "_id": "67d7eba831dd5b46c3e6fdcf",
          "user": {
            "_id": "661a4a556fb488fa078c60aa",
            "avatarUrl": "/avatars/c77401fa9c6d2db896b4a337bb3f8add.svg",
            "isPro": false,
            "fullname": "Tianyu Pang",
            "user": "TIanyupang",
            "type": "user"
          },
          "name": "Tianyu Pang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:35:17.732Z",
          "hidden": false
        },
        {
          "_id": "67d7eba831dd5b46c3e6fdd0",
          "user": {
            "_id": "632407c892e07e3ca20aca28",
            "avatarUrl": "/avatars/23b51b37b12b51a0947f687d1de4d3b5.svg",
            "isPro": false,
            "fullname": "Chao Du",
            "user": "duchao",
            "type": "user"
          },
          "name": "Chao Du",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:35:29.239Z",
          "hidden": false
        },
        {
          "_id": "67d7eba831dd5b46c3e6fdd1",
          "user": {
            "_id": "664aab898fa42b4fe70ebf52",
            "avatarUrl": "/avatars/a38455fd17bbc74ce3111f2c3da9aa59.svg",
            "isPro": false,
            "fullname": "Aixin Sun",
            "user": "aixinsun",
            "type": "user"
          },
          "name": "Aixin Sun",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:35:35.567Z",
          "hidden": false
        },
        {
          "_id": "67d7eba831dd5b46c3e6fdd2",
          "user": {
            "_id": "6397873ec0b27f432db8693f",
            "avatarUrl": "/avatars/1db65fe55002ad5c137c4a59bbcd239d.svg",
            "isPro": false,
            "fullname": "Zhuoran Yang",
            "user": "zhuoran",
            "type": "user"
          },
          "name": "Zhuoran Yang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-17T09:30:18.764Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-12T15:32:44.000Z",
      "submittedOnDailyAt": "2025-03-18T03:10:46.254Z",
      "title": "エラー分析の自動回帰ビデオディフュージョンモデル：一つの統合フレームワーク",
      "submittedOnDailyBy": {
        "_id": "633c2310c0fb6fd232f0accf",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/633c2310c0fb6fd232f0accf/AnvEvxMtd-BpSZbaoA4ha.jpeg",
        "isPro": false,
        "fullname": "Wang Jing",
        "user": "k-nick",
        "type": "user"
      },
      "summary": "多種の自動逆回帰ビデオ拡散モデル（ARVDM）は、実写的な長タイムビデオの生成に驚異的な成功を収めています。しかし、これらのモデルの理論的な分析は少ないです。本稿では、これらのモデルの理論的な基礎を開発し、その知見を既存のモデルの性能向上に活用します。まず、Meta-ARVDM、すなわち、現存する方法をまとめるARVDMの統合的なフレームワークを開発します。Meta-ARVDMを使用して、Meta-ARVDMが生成したビデオと真のビデオの間のKL分散を分析します。分析では、ARVDMに固有な2つの重要な現象を明らかにします -- 誤差の蓄積とメモリボトルネック。情報理論的な不可能性結果を計算し、メモリボトルネック現象は避けられないことを示します。メモリボトルネックを軽減するために、過去のフレームを明示的に使用するための様々なネットワーク構造を設計します。また、フレームを圧縮してメモリボトルネックの軽減と推論効率の改善のタイラップを大幅に向上させます。DMLabとMinecraftでの実験結果は、本稿で提案された方法の効果を証明します。さらに、誤差の蓄積とメモリボトルネックのパロードールファイアーを示し、異なる方法でのパロードールファイアーを示します。",
      "upvotes": 4,
      "discussionId": "67d7ebaa31dd5b46c3e6fe5a",
      "projectPage": "https://sail-sg.github.io/AR-Video-Diffusion",
      "ai_keywords": [
        "Auto-Regressive Video Diffusion Models",
        "Meta-ARVDM",
        "KL-divergence",
        "error accumulation",
        "memory bottleneck",
        "information-theoretic impossibility result",
        "network structures",
        "frame compression",
        "DMLab",
        "Minecraft",
        "Pareto-frontier"
      ]
    },
    "publishedAt": "2025-03-12T11:32:44.000Z",
    "title": "Error Analyses of Auto-Regressive Video Diffusion Models: A Unified\n  Framework",
    "summary": "A variety of Auto-Regressive Video Diffusion Models (ARVDM) have achieved\nremarkable successes in generating realistic long-form videos. However,\ntheoretical analyses of these models remain scant. In this work, we develop\ntheoretical underpinnings for these models and use our insights to improve the\nperformance of existing models. We first develop Meta-ARVDM, a unified\nframework of ARVDMs that subsumes most existing methods. Using Meta-ARVDM, we\nanalyze the KL-divergence between the videos generated by Meta-ARVDM and the\ntrue videos. Our analysis uncovers two important phenomena inherent to ARVDM --\nerror accumulation and memory bottleneck. By deriving an information-theoretic\nimpossibility result, we show that the memory bottleneck phenomenon cannot be\navoided. To mitigate the memory bottleneck, we design various network\nstructures to explicitly use more past frames. We also achieve a significantly\nimproved trade-off between the mitigation of the memory bottleneck and the\ninference efficiency by compressing the frames. Experimental results on DMLab\nand Minecraft validate the efficacy of our methods. Our experiments also\ndemonstrate a Pareto-frontier between the error accumulation and memory\nbottleneck across different methods.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10704.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "633c2310c0fb6fd232f0accf",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/633c2310c0fb6fd232f0accf/AnvEvxMtd-BpSZbaoA4ha.jpeg",
      "fullname": "Wang Jing",
      "name": "k-nick",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.10719",
      "authors": [
        {
          "_id": "67d91dadb533888991ade4e1",
          "user": {
            "_id": "672c6f3d4c1e2de12c6f174e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/mv-8GX6OBBpJLzVvyPYbz.png",
            "isPro": false,
            "fullname": "Yehang Zhang",
            "user": "Buzz-lightyear",
            "type": "user"
          },
          "name": "Yehang Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-18T08:07:19.937Z",
          "hidden": false
        },
        {
          "_id": "67d91dadb533888991ade4e2",
          "user": {
            "_id": "64b4ab62eec33e27dcd733b5",
            "avatarUrl": "/avatars/0a9bf220c9a5efe7279f9b287b087d36.svg",
            "isPro": false,
            "fullname": "Xinli XU",
            "user": "Xxlbigbrother",
            "type": "user"
          },
          "name": "Xinli Xu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:35:50.412Z",
          "hidden": false
        },
        {
          "_id": "67d91dadb533888991ade4e3",
          "name": "Xiaojie Xu",
          "hidden": false
        },
        {
          "_id": "67d91dadb533888991ade4e4",
          "name": "Li Liu",
          "hidden": false
        },
        {
          "_id": "67d91dadb533888991ade4e5",
          "user": {
            "_id": "655cba1d87b67834000590e8",
            "avatarUrl": "/avatars/3bd43b7c9351f65b8f38f4c8237a0146.svg",
            "isPro": false,
            "fullname": "Yingcong Chen",
            "user": "yingcongchen",
            "type": "user"
          },
          "name": "Yingcong Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:36:19.820Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-13T07:58:23.000Z",
      "submittedOnDailyAt": "2025-03-18T06:41:21.358Z",
      "title": "長ビデオアウディオ合成における多効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果",
      "submittedOnDailyBy": {
        "_id": "672c6f3d4c1e2de12c6f174e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/mv-8GX6OBBpJLzVvyPYbz.png",
        "isPro": false,
        "fullname": "Yehang Zhang",
        "user": "Buzz-lightyear",
        "type": "user"
      },
      "summary": "ビデオから音声の合成は、視覚内容に同期された音声を生成する技術で、映画や相互作用マディアにおいて視聴者の浸透感とナレーティブの一貫性を大幅に向上させる。しかし、長タイプの内容に対するビデオからの音声翻訳は、動的な意味の移行、時間的な調整不一致、および特定のデータセットの欠如により解決されていない課題である。既存の方法は短いビデオでは優れているが、長い場合（例：映画）では、合成の断片化とスペクトルの一貫性の不足により失敗する。私たちは、専門的な翻訳ワークフローを模倣する新しい多エージェントフレームワーク「LVAS-Agent」を提案しています。私たちのアプローチは、シーン分割、スクリプト生成、サウンドデザイン、音声合成の4ステップに分解しています。中心的な革新点として、シーン/スクリプトの精確化の討論修正機能と時間的・意味的な調整の生成・検索ループを含むものがあります。システム的な評価を可能にするために、私たちは、207部の専門家が編集した多様な場合の長タイプのビデオを含む最初のベンチマーク「LVAS-Bench」を紹介しています。実験は、基準方法よりも上位の音声・視覚の調整を示しています。プロジェクトページ：https://lvas-agent.github.io",
      "upvotes": 2,
      "discussionId": "67d91dafb533888991ade557",
      "projectPage": "https://lvas-agent.github.io/",
      "ai_keywords": [
        "scene segmentation",
        "script generation",
        "sound design",
        "audio synthesis",
        "discussion-correction mechanism",
        "generation-retrieval loop",
        "temporal-semantic alignment",
        "LVAS-Agent",
        "LVAS-Bench",
        "audio-visual alignment"
      ]
    },
    "publishedAt": "2025-03-13T03:58:23.000Z",
    "title": "Long-Video Audio Synthesis with Multi-Agent Collaboration",
    "summary": "Video-to-audio synthesis, which generates synchronized audio for visual\ncontent, critically enhances viewer immersion and narrative coherence in film\nand interactive media. However, video-to-audio dubbing for long-form content\nremains an unsolved challenge due to dynamic semantic shifts, temporal\nmisalignment, and the absence of dedicated datasets. While existing methods\nexcel in short videos, they falter in long scenarios (e.g., movies) due to\nfragmented synthesis and inadequate cross-scene consistency. We propose\nLVAS-Agent, a novel multi-agent framework that emulates professional dubbing\nworkflows through collaborative role specialization. Our approach decomposes\nlong-video synthesis into four steps including scene segmentation, script\ngeneration, sound design and audio synthesis. Central innovations include a\ndiscussion-correction mechanism for scene/script refinement and a\ngeneration-retrieval loop for temporal-semantic alignment. To enable systematic\nevaluation, we introduce LVAS-Bench, the first benchmark with 207\nprofessionally curated long videos spanning diverse scenarios. Experiments\ndemonstrate superior audio-visual alignment over baseline methods. Project\npage: https://lvas-agent.github.io",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10719.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "672c6f3d4c1e2de12c6f174e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/mv-8GX6OBBpJLzVvyPYbz.png",
      "fullname": "Yehang Zhang",
      "name": "Buzz-lightyear",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.13369",
      "authors": [
        {
          "_id": "67d8e38e5fde3b1be16874e4",
          "user": {
            "_id": "64b214c4f4361a032002cdcf",
            "avatarUrl": "/avatars/3188a4edec4d6c945a0ed9f36050a03c.svg",
            "isPro": false,
            "fullname": "Andrew Wan Ju Kang",
            "user": "soarhigh",
            "type": "user"
          },
          "name": "Wan Ju Kang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-18T08:08:01.967Z",
          "hidden": false
        },
        {
          "_id": "67d8e38e5fde3b1be16874e5",
          "user": {
            "_id": "628e3b87a2cb9819d4391ba6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1653488512816-noauth.jpeg",
            "isPro": false,
            "fullname": "Eunki Kim",
            "user": "eunkey",
            "type": "user"
          },
          "name": "Eunki Kim",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:36:36.532Z",
          "hidden": false
        },
        {
          "_id": "67d8e38e5fde3b1be16874e6",
          "user": {
            "_id": "65cccd8c80d3c4b865d3b262",
            "avatarUrl": "/avatars/e6f6d8f06dd54e1e7b6d686835a9c075.svg",
            "isPro": false,
            "fullname": "Na Min An",
            "user": "namin0202",
            "type": "user"
          },
          "name": "Na Min An",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:36:43.821Z",
          "hidden": false
        },
        {
          "_id": "67d8e38e5fde3b1be16874e7",
          "user": {
            "_id": "62f2638d04674e28535d40f8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1672818467177-62f2638d04674e28535d40f8.png",
            "isPro": false,
            "fullname": "Sangryul Kim",
            "user": "sangryul",
            "type": "user"
          },
          "name": "Sangryul Kim",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:36:49.867Z",
          "hidden": false
        },
        {
          "_id": "67d8e38e5fde3b1be16874e8",
          "user": {
            "_id": "6639807fc9648d06126d7ec4",
            "avatarUrl": "/avatars/d996b5102ae9092da6db5f44f5142b54.svg",
            "isPro": false,
            "fullname": "haemin choi",
            "user": "hammnii",
            "type": "user"
          },
          "name": "Haemin Choi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:36:55.951Z",
          "hidden": false
        },
        {
          "_id": "67d8e38e5fde3b1be16874e9",
          "name": "Ki Hoon Kwak",
          "hidden": false
        },
        {
          "_id": "67d8e38e5fde3b1be16874ea",
          "name": "James Thorne",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-17T16:52:46.000Z",
      "submittedOnDailyAt": "2025-03-18T06:53:48.028Z",
      "title": "シグナル数：視覚者のフィードバックを活用してBLVに合わせたディアグラム説明のデータセットの構築",
      "submittedOnDailyBy": {
        "_id": "64b214c4f4361a032002cdcf",
        "avatarUrl": "/avatars/3188a4edec4d6c945a0ed9f36050a03c.svg",
        "isPro": false,
        "fullname": "Andrew Wan Ju Kang",
        "user": "soarhigh",
        "type": "user"
      },
      "summary": "通常、アノテーターグループと最終ユーザーグループの需要と視覚能力は異なります。盲人と低視力者（BLV）のようなユーザーに詳細な図解説を生成するのは一つの難しい領域です。視力のあるアノテーターは、ビジュアルコンテンツを容易に説明できますが、既存の研究によると、それらの直接の生成はコストが高く、バイアスがあり、BLVの標準に欠けています。本研究では、視力のある人を図解説を評価することを求め、それらを生成することを求めません。ビジョン言語モデル（VLM）が多段階推論を通じて潜在的なサブジェクトでガイドされたものによって生成された図解説を評価します。視力のある人の評価は、自らBLVで視覚障害者の学習者を教える専門的な教育者にとって効果的で有用です。Sightationという、5k図解説データセットと137kサンプルを収録した集合を公開し、完了、好み、検索、問答、理由論の訓練のために使用し、それらの微調節の可能性を示します。",
      "upvotes": 1,
      "discussionId": "67d8e3905fde3b1be168759f",
      "projectPage": "https://huggingface.co/Sightation",
      "ai_keywords": [
        "vision-language models (VLM)",
        "latent supervision",
        "multi-pass inference",
        "diagram description datasets",
        "fine-tuning"
      ]
    },
    "publishedAt": "2025-03-17T12:52:46.000Z",
    "title": "Sightation Counts: Leveraging Sighted User Feedback in Building a\n  BLV-aligned Dataset of Diagram Descriptions",
    "summary": "Often, the needs and visual abilities differ between the annotator group and\nthe end user group. Generating detailed diagram descriptions for blind and\nlow-vision (BLV) users is one such challenging domain. Sighted annotators could\ndescribe visuals with ease, but existing studies have shown that direct\ngenerations by them are costly, bias-prone, and somewhat lacking by BLV\nstandards. In this study, we ask sighted individuals to assess -- rather than\nproduce -- diagram descriptions generated by vision-language models (VLM) that\nhave been guided with latent supervision via a multi-pass inference. The\nsighted assessments prove effective and useful to professional educators who\nare themselves BLV and teach visually impaired learners. We release Sightation,\na collection of diagram description datasets spanning 5k diagrams and 137k\nsamples for completion, preference, retrieval, question answering, and\nreasoning training purposes and demonstrate their fine-tuning potential in\nvarious downstream tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13369.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b214c4f4361a032002cdcf",
      "avatarUrl": "/avatars/3188a4edec4d6c945a0ed9f36050a03c.svg",
      "fullname": "Andrew Wan Ju Kang",
      "name": "soarhigh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.12530",
      "authors": [
        {
          "_id": "67d8f0fa53f713733d6c6b1c",
          "user": {
            "_id": "6737d99b728a96aa64a2b00a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/p3vMX0xkvL_4IpWwyURLM.png",
            "isPro": false,
            "fullname": "Hunter Sawyer",
            "user": "HTSawyer",
            "type": "user"
          },
          "name": "Hunter Sawyer",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:37:16.399Z",
          "hidden": false
        },
        {
          "_id": "67d8f0fa53f713733d6c6b1d",
          "user": {
            "_id": "63c19eb3a0ffa3857eae2efa",
            "avatarUrl": "/avatars/35b06ca092f615a6d11ee99683d0376a.svg",
            "isPro": false,
            "fullname": "Jesse Roberts",
            "user": "JesseTNRoberts",
            "type": "user"
          },
          "name": "Jesse Roberts",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-18T04:05:16.739Z",
          "hidden": false
        },
        {
          "_id": "67d8f0fa53f713733d6c6b1e",
          "user": {
            "_id": "675ec603e4d6d0e820ad9d3f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/ruPDRCZlcbDhZ6zp4xi_P.png",
            "isPro": false,
            "fullname": "Olzhas",
            "user": "KyleMoore",
            "type": "user"
          },
          "name": "Kyle Moore",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:37:25.148Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-16T14:50:54.000Z",
      "submittedOnDailyAt": "2025-03-18T02:37:23.781Z",
      "title": "基本カテゴリーの使用方法（Vision Language Models）",
      "submittedOnDailyBy": {
        "_id": "63c19eb3a0ffa3857eae2efa",
        "avatarUrl": "/avatars/35b06ca092f615a6d11ee99683d0376a.svg",
        "isPro": false,
        "fullname": "Jesse Roberts",
        "user": "JesseTNRoberts",
        "type": "user"
      },
      "summary": "心理学の分野は、1976年にRoschが命名した視覚刺激のラベリングにおける基本的な分類レベルを長く認識しています。この分類レベルは、プリミングを含む視覚語言タスクにおいて最も頻繁に使用され、情報密度が高く、人類の理解を促進します。ここでは、最近公開された2つの開放ソースビジョン言語モデル（VLMs）で基本的なレベルの分類を調査します。この論文では、Llama 3.2 Vision Instruct（11B）とMolmo 7B-Dが人類の行動に一致する基本的なレベルの分類を好み合い、生物学的なと非生物学的な基本的なレベルの効果や既知の専門家の基本的なレベルの変動など、複雑な人類の行動に一致します。これらの結果は、VLMsは訓練されるデータから人類の認知的な分類行動を学習していることを進一歩証明しています。",
      "upvotes": 1,
      "discussionId": "67d8f0fc53f713733d6c6b89",
      "ai_keywords": [
        "vision-language models (VLMs)",
        "basic level categorization",
        "biological versus non-biological basic level effects",
        "expert basic level shift",
        "cognitive categorization behaviors"
      ]
    },
    "publishedAt": "2025-03-16T10:50:54.000Z",
    "title": "Basic Category Usage in Vision Language Models",
    "summary": "The field of psychology has long recognized a basic level of categorization\nthat humans use when labeling visual stimuli, a term coined by Rosch in 1976.\nThis level of categorization has been found to be used most frequently, to have\nhigher information density, and to aid in visual language tasks with priming in\nhumans. Here, we investigate basic level categorization in two recently\nreleased, open-source vision-language models (VLMs). This paper demonstrates\nthat Llama 3.2 Vision Instruct (11B) and Molmo 7B-D both prefer basic level\ncategorization consistent with human behavior. Moreover, the models'\npreferences are consistent with nuanced human behaviors like the biological\nversus non-biological basic level effects and the well established expert basic\nlevel shift, further suggesting that VLMs acquire cognitive categorization\nbehaviors from the human data on which they are trained.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12530.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63c19eb3a0ffa3857eae2efa",
      "avatarUrl": "/avatars/35b06ca092f615a6d11ee99683d0376a.svg",
      "fullname": "Jesse Roberts",
      "name": "JesseTNRoberts",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.12528",
      "authors": [
        {
          "_id": "67d8f044f8b0e148f60cef0d",
          "name": "Kyle Moore",
          "hidden": false
        },
        {
          "_id": "67d8f044f8b0e148f60cef0e",
          "user": {
            "_id": "63c19eb3a0ffa3857eae2efa",
            "avatarUrl": "/avatars/35b06ca092f615a6d11ee99683d0376a.svg",
            "isPro": false,
            "fullname": "Jesse Roberts",
            "user": "JesseTNRoberts",
            "type": "user"
          },
          "name": "Jesse Roberts",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-18T04:02:14.260Z",
          "hidden": false
        },
        {
          "_id": "67d8f044f8b0e148f60cef0f",
          "name": "Daryl Watson",
          "hidden": false
        },
        {
          "_id": "67d8f044f8b0e148f60cef10",
          "name": "Pamela Wisniewski",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-16T14:45:43.000Z",
      "submittedOnDailyAt": "2025-03-18T02:34:14.582Z",
      "title": "人間対応大語言モデルの不確実性の調査",
      "submittedOnDailyBy": {
        "_id": "63c19eb3a0ffa3857eae2efa",
        "avatarUrl": "/avatars/35b06ca092f615a6d11ee99683d0376a.svg",
        "isPro": false,
        "fullname": "Jesse Roberts",
        "user": "JesseTNRoberts",
        "type": "user"
      },
      "summary": "最近の研究は、大規模言語モデルの不確実性を定量化し、モデルの制御とユーザの信頼関係を調節するために取り組んでいます。先行研究は、理論的に基盤を持つ不確実性の測定法やモデルの平均外見行動を反映する測定法に焦点を当てていました。本研究では、多様な不確実性の測定法を調査し、ユーザーのグループレベルの不確実性と関連する測定法を特定することを目的としています。我々は、ベイジアンの測定法と熵の測定法の変形版（top-k entropy）がモデルのサイズに応じてユーザーの行動と一致していることを見出しました。また、一部の強力な測定法はモデルのサイズによってユーザーの類似性が低下することを見出しましたが、複数の線形回帰分析により、複数の不確実性の測定法を組み合わせることでサイズ依存性を減らした同時に、ユーザーの対応性を比較的に高くすることが見出されました。",
      "upvotes": 1,
      "discussionId": "67d8f046f8b0e148f60cef95",
      "ai_keywords": [
        "Bayesian measures",
        "entropy measures",
        "top-k entropy",
        "human-similarity",
        "human-alignment",
        "multiple linear regression"
      ]
    },
    "publishedAt": "2025-03-16T10:45:43.000Z",
    "title": "Investigating Human-Aligned Large Language Model Uncertainty",
    "summary": "Recent work has sought to quantify large language model uncertainty to\nfacilitate model control and modulate user trust. Previous works focus on\nmeasures of uncertainty that are theoretically grounded or reflect the average\novert behavior of the model. In this work, we investigate a variety of\nuncertainty measures, in order to identify measures that correlate with human\ngroup-level uncertainty. We find that Bayesian measures and a variation on\nentropy measures, top-k entropy, tend to agree with human behavior as a\nfunction of model size. We find that some strong measures decrease in\nhuman-similarity with model size, but, by multiple linear regression, we find\nthat combining multiple uncertainty measures provide comparable human-alignment\nwith reduced size-dependency.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12528.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63c19eb3a0ffa3857eae2efa",
      "avatarUrl": "/avatars/35b06ca092f615a6d11ee99683d0376a.svg",
      "fullname": "Jesse Roberts",
      "name": "JesseTNRoberts",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]