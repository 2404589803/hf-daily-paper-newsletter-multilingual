[
  {
    "paper": {
      "id": "2505.11820",
      "authors": [
        {
          "_id": "682bf779fdfa3c5de0eb1e02",
          "user": {
            "_id": "5fc0b2b61160c47d1d438568",
            "avatarUrl": "/avatars/b355912b0ec683e73f21c8d36620e146.svg",
            "isPro": false,
            "fullname": "Kaitao Song",
            "user": "KaitaoSong",
            "type": "user"
          },
          "name": "Kaitao Song",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-20T08:07:27.158Z",
          "hidden": false
        },
        {
          "_id": "682bf779fdfa3c5de0eb1e03",
          "name": "Xiaohua Wang",
          "hidden": false
        },
        {
          "_id": "682bf779fdfa3c5de0eb1e04",
          "user": {
            "_id": "5f1040b6e9d71719e3be71d2",
            "avatarUrl": "/avatars/a2f28940236ae625ed3810ad62e343ff.svg",
            "isPro": false,
            "fullname": "Xu Tan",
            "user": "xutan",
            "type": "user"
          },
          "name": "Xu Tan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-20T07:20:36.359Z",
          "hidden": false
        },
        {
          "_id": "682bf779fdfa3c5de0eb1e05",
          "user": {
            "_id": "6278bd42541f3d2dfa77ea70",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6278bd42541f3d2dfa77ea70/ejn49eapnB3UXQckAYdTd.jpeg",
            "isPro": true,
            "fullname": "Huiqiang Jiang",
            "user": "iofu728",
            "type": "user"
          },
          "name": "Huiqiang Jiang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-20T08:08:04.470Z",
          "hidden": false
        },
        {
          "_id": "682bf779fdfa3c5de0eb1e06",
          "user": {
            "_id": "64646896884f2e3e1ced3cd5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64646896884f2e3e1ced3cd5/86-t8V8LGMNaPQRXnADiD.png",
            "isPro": false,
            "fullname": "Zhang",
            "user": "Chengruidong",
            "type": "user"
          },
          "name": "Chengruidong Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-20T08:08:14.328Z",
          "hidden": false
        },
        {
          "_id": "682bf779fdfa3c5de0eb1e07",
          "user": {
            "_id": "5e1058e9fcf41d740b69966d",
            "avatarUrl": "/avatars/ce74839ba871f2b54313a670a233ba82.svg",
            "isPro": false,
            "fullname": "Yongliang Shen",
            "user": "tricktreat",
            "type": "user"
          },
          "name": "Yongliang Shen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-20T07:23:50.224Z",
          "hidden": false
        },
        {
          "_id": "682bf779fdfa3c5de0eb1e08",
          "name": "Cen LU",
          "hidden": false
        },
        {
          "_id": "682bf779fdfa3c5de0eb1e09",
          "name": "Zihao Li",
          "hidden": false
        },
        {
          "_id": "682bf779fdfa3c5de0eb1e0a",
          "name": "Zifan Song",
          "hidden": false
        },
        {
          "_id": "682bf779fdfa3c5de0eb1e0b",
          "user": {
            "_id": "66beeca13ae330ae8b63a0c9",
            "avatarUrl": "/avatars/09c8341beb8998e4506cef09e3481e77.svg",
            "isPro": false,
            "fullname": "SHAN CAIHUA",
            "user": "sxdtgg",
            "type": "user"
          },
          "name": "Caihua Shan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-20T08:08:55.127Z",
          "hidden": false
        },
        {
          "_id": "682bf779fdfa3c5de0eb1e0c",
          "user": {
            "_id": "678e0bd1ef7630e73c4ad508",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/LlHuTu9VuUJl3jaJzuWly.png",
            "isPro": false,
            "fullname": "Yansen Wang",
            "user": "victorywys",
            "type": "user"
          },
          "name": "Yansen Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-20T08:09:02.859Z",
          "hidden": false
        },
        {
          "_id": "682bf779fdfa3c5de0eb1e0d",
          "name": "Kan Ren",
          "hidden": false
        },
        {
          "_id": "682bf779fdfa3c5de0eb1e0e",
          "user": {
            "_id": "680331764422d7ba43db26cc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/sDKtCDf71fAljNC_SAq_C.png",
            "isPro": false,
            "fullname": "zheng xiaoqing",
            "user": "Qu1zas",
            "type": "user"
          },
          "name": "Xiaoqing Zheng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-20T08:09:16.810Z",
          "hidden": false
        },
        {
          "_id": "682bf779fdfa3c5de0eb1e0f",
          "name": "Tao Qin",
          "hidden": false
        },
        {
          "_id": "682bf779fdfa3c5de0eb1e10",
          "name": "Yuqing Yang",
          "hidden": false
        },
        {
          "_id": "682bf779fdfa3c5de0eb1e11",
          "name": "Dongsheng Li",
          "hidden": false
        },
        {
          "_id": "682bf779fdfa3c5de0eb1e12",
          "name": "Lili Qiu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-17T04:06:12.000Z",
      "submittedOnDailyAt": "2025-05-20T02:47:35.698Z",
      "title": "モデル連鎖学習の言語モデル",
      "submittedOnDailyBy": {
        "_id": "5fc0b2b61160c47d1d438568",
        "avatarUrl": "/avatars/b355912b0ec683e73f21c8d36620e146.svg",
        "isPro": false,
        "fullname": "Kaitao Song",
        "user": "KaitaoSong",
        "type": "user"
      },
      "summary": "この論文では、新しい学習パラダイムを提案します。これは、Chain-of-Model（CoM）と呼ばれ、各層の隠れ状態に原因と結果の関係を連鎖的に組み込み、モデルの訓練のスケーリング効率と実行時のフレックスバイリティを導入します。Chain-of-Representation（CoR）の概念を紹介します。これは、各層の隠れ状態を、隠れ次元レベルでの複数のサブ表現（つまり、連鎖）の組み合わせとして定式化します。各層では、出力表現からの連鎖は、入力表現のすべての前の連鎖を見ることができます。このため、CoMフレームワークに基づくモデルは、前のモデル（つまり、連鎖）に基づいて連鎖を増やし、進歩的にモデルサイズを拡大し、連鎖数を変えて彈性な推論を行うための複数のサブモデルを提供できます。この原則に基づいて、Transformerアーキテクチャの各層にCoMのアイデアを採用したChain-of-Language-Model（CoLM）を提案します。CoLMに基づいて、KV共有機制を採用してCoLM-Airを追加します。この設計は、無間断なLM切り替え、プリフィルリング加速などの追加的な拡張性を示します。実験結果は、標準のTransformerと比較的な性能を達成し、同時に進歩的なスケーリング、トレーニング効率の向上、彈性な推論のための複数のモデルサイズの提供などのより多様な機能性を許すことを示します。将来、コードは以下のURLで公開されます：https://github.com/microsoft/CoLM。",
      "upvotes": 55,
      "discussionId": "682bf77afdfa3c5de0eb1e50",
      "ai_keywords": [
        "Chain-of-Model (CoM)",
        "Chain-of-Representation (CoR)",
        "hidden states",
        "sub-representations",
        "chains",
        "hidden dimension",
        "Chain-of-Language-Model (CoLM)",
        "KV sharing mechanism",
        "keys",
        "values",
        "Transformer architecture",
        "CoLM-Air",
        "seamless LM switching",
        "prefilling acceleration",
        "progressive scaling",
        "elastic inference"
      ]
    },
    "publishedAt": "2025-05-17T00:06:12.000Z",
    "title": "Chain-of-Model Learning for Language Model",
    "summary": "In this paper, we propose a novel learning paradigm, termed Chain-of-Model\n(CoM), which incorporates the causal relationship into the hidden states of\neach layer as a chain style, thereby introducing great scaling efficiency in\nmodel training and inference flexibility in deployment. We introduce the\nconcept of Chain-of-Representation (CoR), which formulates the hidden states at\neach layer as a combination of multiple sub-representations (i.e., chains) at\nthe hidden dimension level. In each layer, each chain from the output\nrepresentations can only view all of its preceding chains in the input\nrepresentations. Consequently, the model built upon CoM framework can\nprogressively scale up the model size by increasing the chains based on the\nprevious models (i.e., chains), and offer multiple sub-models at varying sizes\nfor elastic inference by using different chain numbers. Based on this\nprinciple, we devise Chain-of-Language-Model (CoLM), which incorporates the\nidea of CoM into each layer of Transformer architecture. Based on CoLM, we\nfurther introduce CoLM-Air by introducing a KV sharing mechanism, that computes\nall keys and values within the first chain and then shares across all chains.\nThis design demonstrates additional extensibility, such as enabling seamless LM\nswitching, prefilling acceleration and so on. Experimental results demonstrate\nour CoLM family can achieve comparable performance to the standard Transformer,\nwhile simultaneously enabling greater flexiblity, such as progressive scaling\nto improve training efficiency and offer multiple varying model sizes for\nelastic inference, paving a a new way toward building language models. Our code\nwill be released in the future at: https://github.com/microsoft/CoLM.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.11820.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5fc0b2b61160c47d1d438568",
      "avatarUrl": "/avatars/b355912b0ec683e73f21c8d36620e146.svg",
      "fullname": "Kaitao Song",
      "name": "KaitaoSong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.13417",
      "authors": [
        {
          "_id": "682be3e43ba4cfbca886a521",
          "user": {
            "_id": "66cdd285c51a915bd5f2d017",
            "avatarUrl": "/avatars/14e5794307e4672b1b51d26b31227e0f.svg",
            "isPro": false,
            "fullname": "Jiajie Zhang",
            "user": "NeoZ123",
            "type": "user"
          },
          "name": "Jiajie Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-20T08:10:15.004Z",
          "hidden": false
        },
        {
          "_id": "682be3e43ba4cfbca886a522",
          "user": {
            "_id": "67385497d9af4eb4c078ced3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/yP-EPaY0tUosVR4kjXQ9B.png",
            "isPro": false,
            "fullname": "Lin Nianyi",
            "user": "linny2002",
            "type": "user"
          },
          "name": "Nianyi Lin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-20T08:10:51.330Z",
          "hidden": false
        },
        {
          "_id": "682be3e43ba4cfbca886a523",
          "name": "Lei Hou",
          "hidden": false
        },
        {
          "_id": "682be3e43ba4cfbca886a524",
          "name": "Ling Feng",
          "hidden": false
        },
        {
          "_id": "682be3e43ba4cfbca886a525",
          "user": {
            "_id": "65df8cbc2705d9672f55d1aa",
            "avatarUrl": "/avatars/63e46f15bb76bd9d4508fd0f54f39829.svg",
            "isPro": false,
            "fullname": "Juanzi Li",
            "user": "juanli",
            "type": "user"
          },
          "name": "Juanzi Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-20T08:10:58.673Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-19T17:50:52.000Z",
      "submittedOnDailyAt": "2025-05-20T00:38:40.060Z",
      "title": "AdaptThink: 理由モデルは、どのように考えるかを学ぶ能力を持つ",
      "submittedOnDailyBy": {
        "_id": "66cdd285c51a915bd5f2d017",
        "avatarUrl": "/avatars/14e5794307e4672b1b51d26b31227e0f.svg",
        "isPro": false,
        "fullname": "Jiajie Zhang",
        "user": "NeoZ123",
        "type": "user"
      },
      "summary": "最近、大規模の理由論モデルは、人間のような深い思考を使用して、多様なタスクで驚異的な性能を達成しました。しかし、長期的な思考プロセスは推論オーバーヘッドを大幅に増加させ、効率性が重要なボトルネックとなります。本稿では、最初に、NoThinkingを提示して理由論モデルを思考をスキップして最終的な解決策を直接生成させることを示し、性能と効率性の両方で比較的に簡単なタスクに対してより良い選択肢として説明します。このへんられ、問題の難易度に基づいて最適な思考モードを選択することを学習させるための新しいRLアルゴリズムであるAdaptThinkを提案します。特に、AdaptThinkは2つの核心的な構成要素を特徴としています。1. 約束最適化の目標関数で、モデルが思考を選択しながら全体の性能を維持することを促します。2. 重要サンプリングの戦略で、プロジェクト訓練中に思考とNoThinkingのサンプルをバランスづけることで、冷やめのスタートを可能にし、モデルが訓練プロセス中に両方の思考モードを探索して利用することを可能にします。私たちの実験は、AdaptThinkは推論コストを大幅に減少させ、性能を進めることを示します。特に、3つの数学データセットでは、AdaptThinkはDeepSeek-R1-Distill-Qwen-1.5Bの平均の回答長を53%減少させ、正確性を2.4%向上させ、理由論の品質と効率性のバランスを最適化するための適応的な思考モード選択の可能性を明らかにします。私たちのコードとモデルは、https://github.com/THU-KEG/AdaptThinkに公開されています。",
      "upvotes": 47,
      "discussionId": "682be3e53ba4cfbca886a551",
      "ai_keywords": [
        "NoThinking",
        "AdaptThink",
        "RL algorithm",
        "constrained optimization objective",
        "importance sampling strategy",
        "on-policy training",
        "cold start"
      ]
    },
    "publishedAt": "2025-05-19T13:50:52.000Z",
    "title": "AdaptThink: Reasoning Models Can Learn When to Think",
    "summary": "Recently, large reasoning models have achieved impressive performance on\nvarious tasks by employing human-like deep thinking. However, the lengthy\nthinking process substantially increases inference overhead, making efficiency\na critical bottleneck. In this work, we first demonstrate that NoThinking,\nwhich prompts the reasoning model to skip thinking and directly generate the\nfinal solution, is a better choice for relatively simple tasks in terms of both\nperformance and efficiency. Motivated by this, we propose AdaptThink, a novel\nRL algorithm to teach reasoning models to choose the optimal thinking mode\nadaptively based on problem difficulty. Specifically, AdaptThink features two\ncore components: (1) a constrained optimization objective that encourages the\nmodel to choose NoThinking while maintaining the overall performance; (2) an\nimportance sampling strategy that balances Thinking and NoThinking samples\nduring on-policy training, thereby enabling cold start and allowing the model\nto explore and exploit both thinking modes throughout the training process. Our\nexperiments indicate that AdaptThink significantly reduces the inference costs\nwhile further enhancing performance. Notably, on three math datasets,\nAdaptThink reduces the average response length of DeepSeek-R1-Distill-Qwen-1.5B\nby 53% and improves its accuracy by 2.4%, highlighting the promise of adaptive\nthinking-mode selection for optimizing the balance between reasoning quality\nand efficiency. Our codes and models are available at\nhttps://github.com/THU-KEG/AdaptThink.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.13417.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66cdd285c51a915bd5f2d017",
      "avatarUrl": "/avatars/14e5794307e4672b1b51d26b31227e0f.svg",
      "fullname": "Jiajie Zhang",
      "name": "NeoZ123",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.11896",
      "authors": [
        {
          "_id": "682bf60625f785dbadfb3dfd",
          "user": {
            "_id": "63fc6e47ee821f4bdfab58b8",
            "avatarUrl": "/avatars/4f1e98050092e416ba543b66dd981c2e.svg",
            "isPro": false,
            "fullname": "louchenwei",
            "user": "louchenwei",
            "type": "user"
          },
          "name": "Chenwei Lou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-20T08:12:03.700Z",
          "hidden": false
        },
        {
          "_id": "682bf60625f785dbadfb3dfe",
          "user": {
            "_id": "638dbeaaf467129f49947d5b",
            "avatarUrl": "/avatars/996aa78b4edb429cbb436d48821a317b.svg",
            "isPro": false,
            "fullname": "Zewei Sun",
            "user": "sunzewei2715",
            "type": "user"
          },
          "name": "Zewei Sun",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-20T08:12:13.589Z",
          "hidden": false
        },
        {
          "_id": "682bf60625f785dbadfb3dff",
          "name": "Xinnian Liang",
          "hidden": false
        },
        {
          "_id": "682bf60625f785dbadfb3e00",
          "name": "Meng Qu",
          "hidden": false
        },
        {
          "_id": "682bf60625f785dbadfb3e01",
          "user": {
            "_id": "6468823272d9180d4ac90bdf",
            "avatarUrl": "/avatars/70cb7d65d30ecb944595000ceeeedb1b.svg",
            "isPro": false,
            "fullname": "Wei Shen",
            "user": "Swtheking",
            "type": "user"
          },
          "name": "Wei Shen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-20T08:11:52.139Z",
          "hidden": false
        },
        {
          "_id": "682bf60625f785dbadfb3e02",
          "name": "Wenqi Wang",
          "hidden": false
        },
        {
          "_id": "682bf60625f785dbadfb3e03",
          "name": "Yuntao Li",
          "hidden": false
        },
        {
          "_id": "682bf60625f785dbadfb3e04",
          "user": {
            "_id": "64d20e1821aed29b2ffd2d99",
            "avatarUrl": "/avatars/b0719319a74e8f51fc8a1404aca367e6.svg",
            "isPro": false,
            "fullname": "Qingping Yang",
            "user": "qingping95",
            "type": "user"
          },
          "name": "Qingping Yang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-20T08:23:10.993Z",
          "hidden": false
        },
        {
          "_id": "682bf60625f785dbadfb3e05",
          "user": {
            "_id": "637301f4bb66bd6b13206a25",
            "avatarUrl": "/avatars/6925439441324f6fd00d167d471edff2.svg",
            "isPro": false,
            "fullname": "Shuangzhi Wu",
            "user": "Shuangzhi",
            "type": "user"
          },
          "name": "Shuangzhi Wu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-20T08:23:43.591Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-17T08:27:00.000Z",
      "submittedOnDailyAt": "2025-05-20T01:58:16.261Z",
      "title": "AdaCoT: Pareto最適化のアダプティブなChain-of-Thoughtトリガーをリネアルファイングによって実現",
      "submittedOnDailyBy": {
        "_id": "6468823272d9180d4ac90bdf",
        "avatarUrl": "/avatars/70cb7d65d30ecb944595000ceeeedb1b.svg",
        "isPro": false,
        "fullname": "Wei Shen",
        "user": "Swtheking",
        "type": "user"
      },
      "summary": "大語言モデル（LLMs）は、頂級の能力を示していますが、複雑な理由論を必要とするタスクでは、多くの場合問題を抱えます。Chain-of-Thought（CoT）プロンプティングは、理由論を大幅に向上させることができますが、すべてのクエリに長い理由論ステップを生成し、計算コストと不適切さによる効率の低下を招きます。この重要な問題に対処するために、我々は、LLMsがCoTを適切に使用することを可能にする新しいフレームワーク、Adaptive Chain-of-Thought（AdaCoT）を紹介します。AdaCoTは、Pareto最適化問題として理由論の適切な時期を決定することを実現し、モデルの性能とCoTの呼び出しに伴うコスト（頻度と計算オーバーヘッド）をバランスすることを目指しています。強化学習（RL）に基づく方法を提案し、特にProximal Policy Optimization（PPO）を利用して、理由論の呼び出しの決定バライアを動的に制御し、クエリの隠れた複雑さに基づいてCoTの必要性を決定することを可能にします。技術的な貢献として、Multi-Stage RLトレーニング中の決定バライアの崩壊を防ぎ、強固なステーブルな適応的な呼び出しを確保するためにSelective Loss Masking（SLM）を設計しました。実験結果は、AdaCoTがPareto前沿を成功に歩み、複雑な理由論が不要なクエリに対してCoTの使用を大幅に減少させることを示しています。例えば、我々の生産データセットでは、AdaCoTはCoTの呼び出し率を3.18%まで下げ、平均応答トークンを69.06%減少させ、複雑なタスクでも高い性能を維持しました。",
      "upvotes": 38,
      "discussionId": "682bf60725f785dbadfb3e32",
      "ai_keywords": [
        "Chain-of-Thought (CoT)",
        "Adaptive Chain-of-Thought (AdaCoT)",
        "Pareto optimization problem",
        "reinforcement learning (RL)",
        "Proximal Policy Optimization (PPO)",
        "penalty coefficients",
        "Selective Loss Masking (SLM)",
        "decision boundary collapse",
        "multi-stage RL training",
        "CoT triggering decision boundary",
        "query complexity",
        "adaptive reasoning",
        "CoT usage",
        "average response tokens",
        "complex tasks"
      ]
    },
    "publishedAt": "2025-05-17T04:27:00.000Z",
    "title": "AdaCoT: Pareto-Optimal Adaptive Chain-of-Thought Triggering via\n  Reinforcement Learning",
    "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities but\noften face challenges with tasks requiring sophisticated reasoning. While\nChain-of-Thought (CoT) prompting significantly enhances reasoning, it\nindiscriminately generates lengthy reasoning steps for all queries, leading to\nsubstantial computational costs and inefficiency, especially for simpler\ninputs. To address this critical issue, we introduce AdaCoT (Adaptive\nChain-of-Thought), a novel framework enabling LLMs to adaptively decide when to\ninvoke CoT. AdaCoT framed adaptive reasoning as a Pareto optimization problem\nthat seeks to balance model performance with the costs associated with CoT\ninvocation (both frequency and computational overhead). We propose a\nreinforcement learning (RL) based method, specifically utilizing Proximal\nPolicy Optimization (PPO), to dynamically control the CoT triggering decision\nboundary by adjusting penalty coefficients, thereby allowing the model to\ndetermine CoT necessity based on implicit query complexity. A key technical\ncontribution is Selective Loss Masking (SLM), designed to counteract decision\nboundary collapse during multi-stage RL training, ensuring robust and stable\nadaptive triggering. Experimental results demonstrate that AdaCoT successfully\nnavigates the Pareto frontier, achieving substantial reductions in CoT usage\nfor queries not requiring elaborate reasoning. For instance, on our production\ntraffic testset, AdaCoT reduced CoT triggering rates to as low as 3.18\\% and\ndecreased average response tokens by 69.06%, while maintaining high performance\non complex tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.11896.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6468823272d9180d4ac90bdf",
      "avatarUrl": "/avatars/70cb7d65d30ecb944595000ceeeedb1b.svg",
      "fullname": "Wei Shen",
      "name": "Swtheking",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.11254",
      "authors": [
        {
          "_id": "682bf899ca2c97f999864e23",
          "user": {
            "_id": "654c5d6548b4741202739b73",
            "avatarUrl": "/avatars/bf1bfcf34d93136b7d3a48cebf014d45.svg",
            "isPro": false,
            "fullname": "Jeff Willette",
            "user": "jeffwillette",
            "type": "user"
          },
          "name": "Jeffrey Willette",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-20T07:20:26.628Z",
          "hidden": false
        },
        {
          "_id": "682bf899ca2c97f999864e24",
          "user": {
            "_id": "62e622d08e0b2dc6707f8794",
            "avatarUrl": "/avatars/8c47b5c862f82d4258ba707c932f7f87.svg",
            "isPro": false,
            "fullname": "Heejun Lee",
            "user": "gmlwns5176",
            "type": "user"
          },
          "name": "Heejun Lee",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-20T07:20:28.954Z",
          "hidden": false
        },
        {
          "_id": "682bf899ca2c97f999864e25",
          "name": "Sung Ju Hwang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-16T13:48:33.000Z",
      "submittedOnDailyAt": "2025-05-20T02:14:37.554Z",
      "title": "デルタ・アテンション：Delta 補正による高速と正確なスパースアテンション計算",
      "submittedOnDailyBy": {
        "_id": "62e622d08e0b2dc6707f8794",
        "avatarUrl": "/avatars/8c47b5c862f82d4258ba707c932f7f87.svg",
        "isPro": false,
        "fullname": "Heejun Lee",
        "user": "gmlwns5176",
        "type": "user"
      },
      "summary": "Transformerのアテンション機構は二次元複雑度であり、長いシーケンスに対して推論コストと遅延が高まる。しかし、アテンション行列は主にスパースであり、計算の効率化において許可されるエントリーの省略が可能である。スパースアテンション推論法はこの計算負担を減らすために設計されていますが、それらは性能の低下を伴います。私たちは、この低下の原因の一つがスパース計算がアテンション出力の分布に影響を与え、この分布的な変化が解码時のクエリと予準フィルドステージからの適切なキーとの対応が失われ、性能が低下することを発見しました。私たちは、この分布的な変化を修正するために簡単で新しいそして効果的な手順を提案します。この手順は、どのスパースアテンション方法の上に適用でき、平均36%ptの性能向上を収得し、131K RULERベンチマークでサイクリングウィンドウアテンションとサンクトークンを使用して適用した場合、サイクリングウィンドウアテンションの精度を88%回復します。私たちの方法は、完全な二次元アテンションの約98.5%のスパーシティを維持でき、1Mトークンの予準フィルドを処理するときにFlash Attention 2より32倍速くなることを示します。",
      "upvotes": 31,
      "discussionId": "682bf89aca2c97f999864e76",
      "githubRepo": "https://github.com/jeffwillette/delta-attention",
      "ai_keywords": [
        "attention mechanism",
        "transformer",
        "quadratic complexity",
        "inference costs",
        "latency",
        "long sequences",
        "sparse attention",
        "performance degradation",
        "distributional shift",
        "decoding-time queries",
        "prefill stage",
        "sink tokens",
        "sliding window attention",
        "Flash Attention 2"
      ]
    },
    "publishedAt": "2025-05-16T09:48:33.000Z",
    "title": "Delta Attention: Fast and Accurate Sparse Attention Inference by Delta\n  Correction",
    "summary": "The attention mechanism of a transformer has a quadratic complexity, leading\nto high inference costs and latency for long sequences. However, attention\nmatrices are mostly sparse, which implies that many entries may be omitted from\ncomputation for efficient inference. Sparse attention inference methods aim to\nreduce this computational burden; however, they also come with a troublesome\nperformance degradation. We discover that one reason for this degradation is\nthat the sparse calculation induces a distributional shift in the attention\noutputs. The distributional shift causes decoding-time queries to fail to align\nwell with the appropriate keys from the prefill stage, leading to a drop in\nperformance. We propose a simple, novel, and effective procedure for correcting\nthis distributional shift, bringing the distribution of sparse attention\noutputs closer to that of quadratic attention. Our method can be applied on top\nof any sparse attention method, and results in an average 36%pt performance\nincrease, recovering 88% of quadratic attention accuracy on the 131K RULER\nbenchmark when applied on top of sliding window attention with sink tokens\nwhile only adding a small overhead. Our method can maintain approximately 98.5%\nsparsity over full quadratic attention, making our model 32 times faster than\nFlash Attention 2 when processing 1M token prefills.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.11254.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62e622d08e0b2dc6707f8794",
      "avatarUrl": "/avatars/8c47b5c862f82d4258ba707c932f7f87.svg",
      "fullname": "Heejun Lee",
      "name": "gmlwns5176",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 16
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.13227",
      "authors": [
        {
          "_id": "682c12b44040343163ca7e2a",
          "user": {
            "_id": "618767e4238063b4615d042b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1636263880877-noauth.jpeg",
            "isPro": true,
            "fullname": "Tianbao Xie",
            "user": "tianbaoxiexxx",
            "type": "user"
          },
          "name": "Tianbao Xie",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-20T07:20:09.634Z",
          "hidden": false
        },
        {
          "_id": "682c12b44040343163ca7e2b",
          "user": {
            "_id": "66eeeb2ae65d94c88e9af620",
            "avatarUrl": "/avatars/a25657d634878e9d53ada19feb38149a.svg",
            "isPro": false,
            "fullname": "Jiaqi Deng",
            "user": "MillanK",
            "type": "user"
          },
          "name": "Jiaqi Deng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-20T07:20:06.695Z",
          "hidden": false
        },
        {
          "_id": "682c12b44040343163ca7e2c",
          "user": {
            "_id": "64b103cf372d434077206750",
            "avatarUrl": "/avatars/ba0eb4fc712a8b9b93ceb30d11859ec2.svg",
            "isPro": false,
            "fullname": "Xiaochuan Li",
            "user": "lixiaochuan2020",
            "type": "user"
          },
          "name": "Xiaochuan Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-20T07:20:04.603Z",
          "hidden": false
        },
        {
          "_id": "682c12b44040343163ca7e2d",
          "user": {
            "_id": "66ed083acaf696884760729a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/RgPe99BqsJsHUWoXO1qtS.jpeg",
            "isPro": false,
            "fullname": "Nick Yang",
            "user": "RadioBlue",
            "type": "user"
          },
          "name": "Junlin Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-20T07:20:02.029Z",
          "hidden": false
        },
        {
          "_id": "682c12b44040343163ca7e2e",
          "name": "Haoyuan Wu",
          "hidden": false
        },
        {
          "_id": "682c12b44040343163ca7e2f",
          "user": {
            "_id": "6465941d0e6c7618f615675b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6465941d0e6c7618f615675b/W4EHqlCucz_bojFLFEeV_.jpeg",
            "isPro": false,
            "fullname": "Jixuan Chen",
            "user": "Mayome",
            "type": "user"
          },
          "name": "Jixuan Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-20T08:24:36.390Z",
          "hidden": false
        },
        {
          "_id": "682c12b44040343163ca7e30",
          "name": "Wenjing Hu",
          "hidden": false
        },
        {
          "_id": "682c12b44040343163ca7e31",
          "user": {
            "_id": "63eb133a91a1b8ec4fbc4c2f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63eb133a91a1b8ec4fbc4c2f/dmaD56RAqkovB4izizv5m.png",
            "isPro": false,
            "fullname": "Xinyuan Wang",
            "user": "buaa42wxy",
            "type": "user"
          },
          "name": "Xinyuan Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-20T08:24:59.036Z",
          "hidden": false
        },
        {
          "_id": "682c12b44040343163ca7e32",
          "user": {
            "_id": "6602869253a0518b2a98cafd",
            "avatarUrl": "/avatars/c14b5953a716f42c83ad28147f8308ae.svg",
            "isPro": false,
            "fullname": "Yuhui Xu",
            "user": "yuhuixu",
            "type": "user"
          },
          "name": "Yuhui Xu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-20T08:25:12.279Z",
          "hidden": false
        },
        {
          "_id": "682c12b44040343163ca7e33",
          "user": {
            "_id": "656832dfbd65fd41ee7aa8cd",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656832dfbd65fd41ee7aa8cd/HHkyetTqNq1wIBPipzjQA.jpeg",
            "isPro": false,
            "fullname": "Zekun Wang",
            "user": "kugwzk",
            "type": "user"
          },
          "name": "Zekun Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-20T08:25:30.953Z",
          "hidden": false
        },
        {
          "_id": "682c12b44040343163ca7e34",
          "user": {
            "_id": "601d29ab913ad3afd7b7ddb8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1620447944896-601d29ab913ad3afd7b7ddb8.jpeg",
            "isPro": true,
            "fullname": "Yiheng Xu",
            "user": "ranpox",
            "type": "user"
          },
          "name": "Yiheng Xu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-20T08:25:52.551Z",
          "hidden": false
        },
        {
          "_id": "682c12b44040343163ca7e35",
          "name": "Junli Wang",
          "hidden": false
        },
        {
          "_id": "682c12b44040343163ca7e36",
          "user": {
            "_id": "65f84fd980481173afd91233",
            "avatarUrl": "/avatars/6ac7bd6beba24d1476c5179b88c9e3fa.svg",
            "isPro": false,
            "fullname": "Doyen",
            "user": "doyensahoo",
            "type": "user"
          },
          "name": "Doyen Sahoo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-20T08:26:21.701Z",
          "hidden": false
        },
        {
          "_id": "682c12b44040343163ca7e37",
          "name": "Tao Yu",
          "hidden": false
        },
        {
          "_id": "682c12b44040343163ca7e38",
          "user": {
            "_id": "649dbcc4e0fff1ed099dc80a",
            "avatarUrl": "/avatars/c87c273ca628dbcddccbf1ee19b2ce33.svg",
            "isPro": false,
            "fullname": "Caiming Xiong",
            "user": "cxiong",
            "type": "user"
          },
          "name": "Caiming Xiong",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-20T08:26:15.939Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-19T15:09:23.000Z",
      "submittedOnDailyAt": "2025-05-20T03:59:32.853Z",
      "title": "コンピューター使用の基礎を拡大するためのユーザーインターフェース分解と合成",
      "submittedOnDailyBy": {
        "_id": "618767e4238063b4615d042b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1636263880877-noauth.jpeg",
        "isPro": true,
        "fullname": "Tianbao Xie",
        "user": "tianbaoxiexxx",
        "type": "user"
      },
      "summary": "GUI 基礎化、自然言語指令をグラフィックユーザインターフェイス（GUI）の特定の行動にマッピングする能力は、コンピューター使用アガント開発の重要なボトルネックである。現在のベンチマークは、短い参照表現による基礎化タスクを簡単化し、ソフトウェアのコモンセンス、レイアウト理解、細かい操作能力の複雑な実世界のインタラクションを捉えずにいる。これらの制限を解決するために、私たちは OSWorld-G を紹介し、これは、テキストマッチング、要素認識、レイアウト理解、精密な操作を含む多様なタスクタイプの 564 件の細かく注記されたサンプルからなる詳細なベンチマークである。また、私たちは、最大のコンピューター使用基礎化データセット Jedi を合成し、リリースし、これは、タスクの多角度の離れ合いにより 4 百万例を含むものである。Jedi で訓練された多スケールモデルは、ScreenSpot-v2、ScreenSpot-Pro と OSWorld-G で現在のアプローチを上回ることで、その効果を示す。また、Jedi での改善された基礎化は、一般的なファンダメンタルモデルのアガント能力を複雑なコンピュータータスクで向上させ、OSWorld では 5% から 27% に達し、これを示している。詳細な ablation ステージにより、基礎化性能に寄与する要因を特定し、異なるインターフェイス要素の専門的なデータの組み合わせによる構成的な一般化の可能性を確認している。すべてのベンチマーク、データ、チェックポイント、コードは、https://osworld-grounding.github.io でオープンソースで利用可能である。",
      "upvotes": 30,
      "discussionId": "682c12ba4040343163ca7fd4",
      "projectPage": "https://osworld-grounding.github.io/",
      "githubRepo": "https://github.com/xlang-ai/OSWorld-G",
      "ai_keywords": [
        "GUI grounding",
        "natural language instructions",
        "software commonsense",
        "layout understanding",
        "fine-grained manipulation capabilities",
        "OSWorld-G",
        "text matching",
        "element recognition",
        "precise manipulation",
        "Jedi",
        "multi-perspective decoupling",
        "multi-scale models",
        "ScreenSpot-v2",
        "ScreenSpot-Pro",
        "agentic capabilities",
        "general foundation models",
        "compositional generalization",
        "novel interfaces"
      ]
    },
    "publishedAt": "2025-05-19T11:09:23.000Z",
    "title": "Scaling Computer-Use Grounding via User Interface Decomposition and\n  Synthesis",
    "summary": "Graphical user interface (GUI) grounding, the ability to map natural language\ninstructions to specific actions on graphical user interfaces, remains a\ncritical bottleneck in computer use agent development. Current benchmarks\noversimplify grounding tasks as short referring expressions, failing to capture\nthe complexity of real-world interactions that require software commonsense,\nlayout understanding, and fine-grained manipulation capabilities. To address\nthese limitations, we introduce OSWorld-G, a comprehensive benchmark comprising\n564 finely annotated samples across diverse task types including text matching,\nelement recognition, layout understanding, and precise manipulation.\nAdditionally, we synthesize and release the largest computer use grounding\ndataset Jedi, which contains 4 million examples through multi-perspective\ndecoupling of tasks. Our multi-scale models trained on Jedi demonstrate its\neffectiveness by outperforming existing approaches on ScreenSpot-v2,\nScreenSpot-Pro, and our OSWorld-G. Furthermore, we demonstrate that improved\ngrounding with Jedi directly enhances agentic capabilities of general\nfoundation models on complex computer tasks, improving from 5% to 27% on\nOSWorld. Through detailed ablation studies, we identify key factors\ncontributing to grounding performance and verify that combining specialized\ndata for different interface elements enables compositional generalization to\nnovel interfaces. All benchmark, data, checkpoints, and code are open-sourced\nand available at https://osworld-grounding.github.io.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.13227.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "618767e4238063b4615d042b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1636263880877-noauth.jpeg",
      "fullname": "Tianbao Xie",
      "name": "tianbaoxiexxx",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 17
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.13379",
      "authors": [
        {
          "_id": "682bf32f09ce6055262b42ec",
          "user": {
            "_id": "646a1939c37ca1e12308fe81",
            "avatarUrl": "/avatars/752e9d86018e7d33ad8bcd741203fd86.svg",
            "isPro": false,
            "fullname": "Gongfan Fang",
            "user": "Vinnnf",
            "type": "user"
          },
          "name": "Gongfan Fang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-20T07:20:41.314Z",
          "hidden": false
        },
        {
          "_id": "682bf32f09ce6055262b42ed",
          "user": {
            "_id": "64396ebc21221ac7411852b3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64396ebc21221ac7411852b3/SR0dC8N0bdj9tZFxYPpSf.jpeg",
            "isPro": false,
            "fullname": "Xinyin Ma",
            "user": "horseee",
            "type": "user"
          },
          "name": "Xinyin Ma",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-20T08:26:33.691Z",
          "hidden": false
        },
        {
          "_id": "682bf32f09ce6055262b42ee",
          "user": {
            "_id": "63fc03a50aab060792ffef39",
            "avatarUrl": "/avatars/9d5b1bb2a41928e08176b703935133ab.svg",
            "isPro": false,
            "fullname": "Wangxinchao",
            "user": "wxcTest",
            "type": "user"
          },
          "name": "Xinchao Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-20T08:27:00.912Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-19T17:24:16.000Z",
      "submittedOnDailyAt": "2025-05-20T02:01:08.741Z",
      "title": "Thinkless: LLM Learns When to Think",
      "submittedOnDailyBy": {
        "_id": "646a1939c37ca1e12308fe81",
        "avatarUrl": "/avatars/752e9d86018e7d33ad8bcd741203fd86.svg",
        "isPro": false,
        "fullname": "Gongfan Fang",
        "user": "Vinnnf",
        "type": "user"
      },
      "summary": "推理モデル、長期的なチェーンオフサインロジック推理を可能にしているものは、複雑なロジカル推論を必要とするタスクにおいて卓越した性能を示しています。しかし、すべてのクエリに詳細な理由を適用することは、計算的なエフィシェンスの低下を招くことになります、特に、多くの問題には簡単な解決策がある場合もあります。これにより、LLMsが学ぶべきのはどのようなものかという問題が開かれています。これを答えるために、Thinklessという学習可能なフレームワークを提案します。これは、タスクの複雑さとモデルの能力に基づいて短形と長形の理由を適切に選択することを可能にします。Thinklessは、強化学習パラダイムで訓練され、<short>で簡潔な回答を、<think>で詳細な理由を提供するコントロールトークンを使用しています。我々の方法の核心は、Decoupled Group Relative Policy Optimization (DeGRPO)アルゴリズムです。これは、ハイブリッド理由の学習オブジェクティブを2つの要素に分解します：1) 理由モードの選択を制御するコントロールトークンロスと、2) 生成される回答の精度を向上させる回答ロス。このデコープレートの構成は、各オブジェクトの貢献度をフィングラインに制御することを可能にし、トレーニングの安定化とvanilla GRPOにおける崩壊を防ぎ止めることにより、効果的です。実験的には、Minerva Algebra、MATH-500、GSM8Kなどの評価ベンチマークで、Thinklessは長期チェーンサインの使用を50% - 90%削減でき、Reasoning Language Modelsのエフィシェンスを大幅に向上させます。コードは、https://github.com/VainF/Thinklessに提供されています。",
      "upvotes": 23,
      "discussionId": "682bf33309ce6055262b43fd",
      "githubRepo": "https://github.com/VainF/Thinkless",
      "ai_keywords": [
        "Thinkless",
        "Decoupled Group Relative Policy Optimization (DeGRPO)",
        "control token loss",
        "response loss",
        "hybrid reasoning",
        "long-chain thinking",
        "Minerva Algebra",
        "MATH-500",
        "GSM8K"
      ]
    },
    "publishedAt": "2025-05-19T13:24:16.000Z",
    "title": "Thinkless: LLM Learns When to Think",
    "summary": "Reasoning Language Models, capable of extended chain-of-thought reasoning,\nhave demonstrated remarkable performance on tasks requiring complex logical\ninference. However, applying elaborate reasoning for all queries often results\nin substantial computational inefficiencies, particularly when many problems\nadmit straightforward solutions. This motivates an open question: Can LLMs\nlearn when to think? To answer this, we propose Thinkless, a learnable\nframework that empowers an LLM to adaptively select between short-form and\nlong-form reasoning, based on both task complexity and the model's ability.\nThinkless is trained under a reinforcement learning paradigm and employs two\ncontrol tokens, <short> for concise responses and <think> for detailed\nreasoning. At the core of our method is a Decoupled Group Relative Policy\nOptimization (DeGRPO) algorithm, which decomposes the learning objective of\nhybrid reasoning into two components: (1) a control token loss that governs the\nselection of the reasoning mode, and (2) a response loss that improves the\naccuracy of the generated answers. This decoupled formulation enables\nfine-grained control over the contributions of each objective, stabilizing\ntraining and effectively preventing collapse observed in vanilla GRPO.\nEmpirically, on several benchmarks such as Minerva Algebra, MATH-500, and\nGSM8K, Thinkless is able to reduce the usage of long-chain thinking by 50% -\n90%, significantly improving the efficiency of Reasoning Language Models. The\ncode is available at https://github.com/VainF/Thinkless",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.13379.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "646a1939c37ca1e12308fe81",
      "avatarUrl": "/avatars/752e9d86018e7d33ad8bcd741203fd86.svg",
      "fullname": "Gongfan Fang",
      "name": "Vinnnf",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.13427",
      "authors": [
        {
          "_id": "682bfa77444a7d5f589a8769",
          "user": {
            "_id": "666fe1a5b07525f0bde69c27",
            "avatarUrl": "/avatars/bb98ab0b974c8fe011739baa8dadd91a.svg",
            "isPro": false,
            "fullname": "Lingxiao Du",
            "user": "Cierra0506",
            "type": "user"
          },
          "name": "Lingxiao Du",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-20T08:27:27.006Z",
          "hidden": false
        },
        {
          "_id": "682bfa77444a7d5f589a876a",
          "user": {
            "_id": "640b37b2bab5ca8fbe7df8f2",
            "avatarUrl": "/avatars/c7bef45efad6a0d911a720e2236fcba5.svg",
            "isPro": false,
            "fullname": "fanqing meng",
            "user": "FanqingM",
            "type": "user"
          },
          "name": "Fanqing Meng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-20T08:27:33.802Z",
          "hidden": false
        },
        {
          "_id": "682bfa77444a7d5f589a876b",
          "name": "Zongkai Liu",
          "hidden": false
        },
        {
          "_id": "682bfa77444a7d5f589a876c",
          "user": {
            "_id": "674bfdf227f531cdc248bb5c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/674bfdf227f531cdc248bb5c/xh4gw89sr8MzNzRdiTjFx.jpeg",
            "isPro": false,
            "fullname": "Zhixiang Zhou",
            "user": "SuperposedWave",
            "type": "user"
          },
          "name": "Zhixiang Zhou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-20T08:28:03.869Z",
          "hidden": false
        },
        {
          "_id": "682bfa77444a7d5f589a876d",
          "name": "Ping Luo",
          "hidden": false
        },
        {
          "_id": "682bfa77444a7d5f589a876e",
          "user": {
            "_id": "63cf4ecdc1dedf59c8f8362e",
            "avatarUrl": "/avatars/cede885854d6a1551860080d55c87568.svg",
            "isPro": false,
            "fullname": "Qiaosheng ZHANG",
            "user": "Domingo12",
            "type": "user"
          },
          "name": "Qiaosheng Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-20T08:28:11.669Z",
          "hidden": false
        },
        {
          "_id": "682bfa77444a7d5f589a876f",
          "user": {
            "_id": "64b3fd42eec33e27dcc4c941",
            "avatarUrl": "/avatars/5aa1a99468fa61d4b8b0e80b592c4e55.svg",
            "isPro": false,
            "fullname": "Wenqi Shao",
            "user": "wqshao126",
            "type": "user"
          },
          "name": "Wenqi Shao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-20T08:28:17.437Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-19T17:55:08.000Z",
      "submittedOnDailyAt": "2025-05-20T02:15:09.304Z",
      "title": "MM-PRM: スケーラブルなステップレベルのサバイエンスを活用した多タイプ数学論理の向上",
      "submittedOnDailyBy": {
        "_id": "666fe1a5b07525f0bde69c27",
        "avatarUrl": "/avatars/bb98ab0b974c8fe011739baa8dadd91a.svg",
        "isPro": false,
        "fullname": "Lingxiao Du",
        "user": "Cierra0506",
        "type": "user"
      },
      "summary": "マルチモーダル大語言モデル（MLLMs）は、視覚言語理解において驚異的な進歩を達成していますが、複雑な多段階推理においては、論理的に不一致したものから部分的に正しい解決策を生成することで困難を見出しています。主な制限は、中間的な推理ステップに対する細かいサブジェクトの欠如です。これに対して、私たちはMM-PRM（プロセス報酬モデル）を提案します。これは完全自動化されたスケーラブルなフレームワーク内で訓練されたものです。まず、多様な数学推理データによって強力なマルチモーダルモデルMM-Policyを構築し、次に、10,000問のマルチモーダル数学問題のセレクションデータMM-K12を構築します。これは証明的に正しい答えがあるもので、シードデータとして使用されます。モンテカルロ木探索（MCTS）に基づくパイプラインを利用して、人間のラベルを必要とさせずに70万以上のステップレベルのアノテーションを生成します。その結果、PRMはBest-of-N推論セットで候補の推理パスをスコアすることで、モデル内でのデータ（MM-K12テストセット）とモデル外でのデータ（OlympiadBench、MathVistaなど）の両方で显著な向上を達成します。進一づ分析は、ソフトラベル、小さな学習率、パスの多様性がPRMの性能を最適化することを確認します。MM-PRMは、プロセスサブジェクトはマルチモーダル推理システムの論理的な強固性を向上させる強力なツールであることを示します。すべてのコードとデータはhttps://github.com/ModalMinds/MM-PRMで公開しています。",
      "upvotes": 18,
      "discussionId": "682bfa78444a7d5f589a879a",
      "githubRepo": "https://github.com/ModalMinds/MM-PRM",
      "ai_keywords": [
        "Multimodal Large Language Models (MLLMs)",
        "vision-language understanding",
        "multi-step reasoning",
        "fine-grained supervision",
        "process reward model (PRM)",
        "MM-Policy",
        "multimodal math problems",
        "verifiable answers",
        "MM-K12",
        "Monte Carlo Tree Search (MCTS)",
        "step-level annotations",
        "Best-of-N inference setup",
        "OlympiadBench",
        "MathVista",
        "logical robustness",
        "multimodal reasoning systems"
      ]
    },
    "publishedAt": "2025-05-19T13:55:08.000Z",
    "title": "MM-PRM: Enhancing Multimodal Mathematical Reasoning with Scalable\n  Step-Level Supervision",
    "summary": "While Multimodal Large Language Models (MLLMs) have achieved impressive\nprogress in vision-language understanding, they still struggle with complex\nmulti-step reasoning, often producing logically inconsistent or partially\ncorrect solutions. A key limitation lies in the lack of fine-grained\nsupervision over intermediate reasoning steps. To address this, we propose\nMM-PRM, a process reward model trained within a fully automated, scalable\nframework. We first build MM-Policy, a strong multimodal model trained on\ndiverse mathematical reasoning data. Then, we construct MM-K12, a curated\ndataset of 10,000 multimodal math problems with verifiable answers, which\nserves as seed data. Leveraging a Monte Carlo Tree Search (MCTS)-based\npipeline, we generate over 700k step-level annotations without human labeling.\nThe resulting PRM is used to score candidate reasoning paths in the Best-of-N\ninference setup and achieves significant improvements across both in-domain\n(MM-K12 test set) and out-of-domain (OlympiadBench, MathVista, etc.)\nbenchmarks. Further analysis confirms the effectiveness of soft labels, smaller\nlearning rates, and path diversity in optimizing PRM performance. MM-PRM\ndemonstrates that process supervision is a powerful tool for enhancing the\nlogical robustness of multimodal reasoning systems. We release all our codes\nand data at https://github.com/ModalMinds/MM-PRM.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.13427.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "666fe1a5b07525f0bde69c27",
      "avatarUrl": "/avatars/bb98ab0b974c8fe011739baa8dadd91a.svg",
      "fullname": "Lingxiao Du",
      "name": "Cierra0506",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.13308",
      "authors": [
        {
          "_id": "682c154830991f1cf6291a79",
          "user": {
            "_id": "62649e2b1ed8d81e47ad9b4e",
            "avatarUrl": "/avatars/f33a0b727822fd2ea99dce37fbda3d17.svg",
            "isPro": false,
            "fullname": "Li",
            "user": "henry12348",
            "type": "user"
          },
          "name": "Hengli Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-20T07:19:59.597Z",
          "hidden": false
        },
        {
          "_id": "682c154830991f1cf6291a7a",
          "name": "Chenxi Li",
          "hidden": false
        },
        {
          "_id": "682c154830991f1cf6291a7b",
          "name": "Tong Wu",
          "hidden": false
        },
        {
          "_id": "682c154830991f1cf6291a7c",
          "user": {
            "_id": "647ffddeb82adfa7cc1a10d9",
            "avatarUrl": "/avatars/26aa168d6b2068298ebb16584aa52b6c.svg",
            "isPro": false,
            "fullname": "zhu",
            "user": "xuekai",
            "type": "user"
          },
          "name": "Xuekai Zhu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-20T08:29:34.137Z",
          "hidden": false
        },
        {
          "_id": "682c154830991f1cf6291a7d",
          "user": {
            "_id": "60b9e6837946aff342f734ae",
            "avatarUrl": "/avatars/a711a6aa35757dfd7b78b26098a964fc.svg",
            "isPro": false,
            "fullname": "Yuxuan Wang",
            "user": "ColorfulAI",
            "type": "user"
          },
          "name": "Yuxuan Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-20T08:29:49.123Z",
          "hidden": false
        },
        {
          "_id": "682c154830991f1cf6291a7e",
          "name": "Zhaoxin Yu",
          "hidden": false
        },
        {
          "_id": "682c154830991f1cf6291a7f",
          "name": "Eric Hanchen Jiang",
          "hidden": false
        },
        {
          "_id": "682c154830991f1cf6291a80",
          "name": "Song-Chun Zhu",
          "hidden": false
        },
        {
          "_id": "682c154830991f1cf6291a81",
          "user": {
            "_id": "64b7ae6cf53ae848e72b997d",
            "avatarUrl": "/avatars/b55dd3d6fcb3ccac2e3880d01a9bdc63.svg",
            "isPro": false,
            "fullname": "Zixia Jia",
            "user": "vickyandkekey",
            "type": "user"
          },
          "name": "Zixia Jia",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-20T08:30:26.630Z",
          "hidden": false
        },
        {
          "_id": "682c154830991f1cf6291a82",
          "name": "Ying Nian Wu",
          "hidden": false
        },
        {
          "_id": "682c154830991f1cf6291a83",
          "user": {
            "_id": "63a95a6a7930fa8c7dd63d4e",
            "avatarUrl": "/avatars/d9d0420f7ddfe2f3a7e029fb05f1c89f.svg",
            "isPro": false,
            "fullname": "Zilong Zheng",
            "user": "zlzheng",
            "type": "user"
          },
          "name": "Zilong Zheng",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-20T05:38:17.771Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-19T16:26:02.000Z",
      "submittedOnDailyAt": "2025-05-20T05:49:18.858Z",
      "title": "暗闇中探求：時間テスト時インスタンスレベルのポリシー勾配を用いた推論",
      "submittedOnDailyBy": {
        "_id": "63a95a6a7930fa8c7dd63d4e",
        "avatarUrl": "/avatars/d9d0420f7ddfe2f3a7e029fb05f1c89f.svg",
        "isPro": false,
        "fullname": "Zilong Zheng",
        "user": "zlzheng",
        "type": "user"
      },
      "summary": "推理能力，作为人类智能的核心组成部分，在追求通用人工智能（AGI）的过程中，对大型语言模型（LLMs）构成了重大挑战。尽管在训练缩放定律下模型的性能有所提升，但仍存在许多重大挑战，尤其是在训练算法方面，如灾难性遗忘，以及新颖训练数据的有限可用性。作为一种替代方案，测试时间缩放通过增加测试时间的计算量而非参数更新，从而增强推理性能。与该范式中先前专注于标记空间的方法不同，我们提出利用潜在空间进行更有效的推理和更好的测试时间缩放定律遵循。我们引入了LatentSeek，这是一个新的框架，通过模型潜在空间内的测试时间实例级适应（TTIA）来增强LLM的推理能力。具体而言，LatentSeek利用策略梯度迭代更新潜在表示，由自生成的奖励信号引导。LatentSeek在包括GSM8K、MATH-500和AIME2024在内的一系列推理基准测试中进行了评估，涵盖了多种LLM架构。结果表明，LatentSeek始终优于强大的基线，如思维链提示和基于微调的方法。此外，我们的分析表明，LatentSeek高度高效，通常在几个迭代内收敛到平均复杂度问题，同时从额外的迭代中受益，从而突出了测试时间缩放在潜在空间中的潜力。这些发现将LatentSeek定位为增强LLM推理能力的轻量级、可扩展且有效的解决方案。",
      "upvotes": 18,
      "discussionId": "682c154930991f1cf6291b02",
      "projectPage": "https://bigai-nlco.github.io/LatentSeek/",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "AGI",
        "catastrophic forgetting",
        "token space",
        "latent space",
        "LatentSeek",
        "Test-Time Instance-level Adaptation (TTIA)",
        "policy gradient",
        "latent representations",
        "self-generated reward signals",
        "GSM8K",
        "MATH-500",
        "AIME2024",
        "Chain-of-Thought prompting",
        "fine-tuning-based methods"
      ]
    },
    "publishedAt": "2025-05-19T12:26:02.000Z",
    "title": "Seek in the Dark: Reasoning via Test-Time Instance-Level Policy Gradient\n  in Latent Space",
    "summary": "Reasoning ability, a core component of human intelligence, continues to pose\na significant challenge for Large Language Models (LLMs) in the pursuit of AGI.\nAlthough model performance has improved under the training scaling law,\nsignificant challenges remain, particularly with respect to training\nalgorithms, such as catastrophic forgetting, and the limited availability of\nnovel training data. As an alternative, test-time scaling enhances reasoning\nperformance by increasing test-time computation without parameter updating.\nUnlike prior methods in this paradigm focused on token space, we propose\nleveraging latent space for more effective reasoning and better adherence to\nthe test-time scaling law. We introduce LatentSeek, a novel framework that\nenhances LLM reasoning through Test-Time Instance-level Adaptation (TTIA)\nwithin the model's latent space. Specifically, LatentSeek leverages policy\ngradient to iteratively update latent representations, guided by self-generated\nreward signals. LatentSeek is evaluated on a range of reasoning benchmarks,\nincluding GSM8K, MATH-500, and AIME2024, across multiple LLM architectures.\nResults show that LatentSeek consistently outperforms strong baselines, such as\nChain-of-Thought prompting and fine-tuning-based methods. Furthermore, our\nanalysis demonstrates that LatentSeek is highly efficient, typically converging\nwithin a few iterations for problems of average complexity, while also\nbenefiting from additional iterations, thereby highlighting the potential of\ntest-time scaling in the latent space. These findings position LatentSeek as a\nlightweight, scalable, and effective solution for enhancing the reasoning\ncapabilities of LLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.13308.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "63a95a6a7930fa8c7dd63d4e",
      "avatarUrl": "/avatars/d9d0420f7ddfe2f3a7e029fb05f1c89f.svg",
      "fullname": "Zilong Zheng",
      "name": "zlzheng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.13215",
      "authors": [
        {
          "_id": "682bedb2fdfa3c5de0e86a0d",
          "user": {
            "_id": "672b66744efad666d2efb0c8",
            "avatarUrl": "/avatars/9c00a67e9d5b74694759849cca32b015.svg",
            "isPro": false,
            "fullname": "Oh Seungjun",
            "user": "ohseungjun",
            "type": "user"
          },
          "name": "Seungjun Oh",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-20T07:20:43.581Z",
          "hidden": false
        },
        {
          "_id": "682bedb2fdfa3c5de0e86a0e",
          "user": {
            "_id": "66a4a1a7d8e85b03deddfa59",
            "avatarUrl": "/avatars/56dbec2101717ad9471e08a03ae51f0c.svg",
            "isPro": false,
            "fullname": "Young geun Lee",
            "user": "LeeYG",
            "type": "user"
          },
          "name": "Younggeun Lee",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-20T08:28:33.215Z",
          "hidden": false
        },
        {
          "_id": "682bedb2fdfa3c5de0e86a0f",
          "user": {
            "_id": "64c0d2f962983511b95c38d6",
            "avatarUrl": "/avatars/68d9d3002d7f5d39aa9a7e2a49d25532.svg",
            "isPro": false,
            "fullname": "JeonHyejin",
            "user": "Heyjin",
            "type": "user"
          },
          "name": "Hyejin Jeon",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-20T08:28:47.259Z",
          "hidden": false
        },
        {
          "_id": "682bedb2fdfa3c5de0e86a10",
          "user": {
            "_id": "655e0141d36a195f663ee4b0",
            "avatarUrl": "/avatars/97bb695ccefdcb2139b94bcae808cf99.svg",
            "isPro": false,
            "fullname": "Eunbyung Park",
            "user": "epark",
            "type": "user"
          },
          "name": "Eunbyung Park",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-20T08:28:53.220Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-19T14:59:58.000Z",
      "submittedOnDailyAt": "2025-05-20T01:21:32.887Z",
      "title": "混合3D-4Dガウススプレッティングの高速ダイナミックシーン表現",
      "submittedOnDailyBy": {
        "_id": "672b66744efad666d2efb0c8",
        "avatarUrl": "/avatars/9c00a67e9d5b74694759849cca32b015.svg",
        "isPro": false,
        "fullname": "Oh Seungjun",
        "user": "ohseungjun",
        "type": "user"
      },
      "summary": "最近の動的3Dスケーン構成の進展は、高品質な3D新視点合成を可能にし、時間的な一致性を向上させた優れた結果を示しています。これらの中で、4次元ガウススプレッティング（4DGS）は、高品質な空間的および時間的な変化をモデル化する能力により、吸引的なアプローチとして現れています。しかし、現在の方法は、静的領域に冗須的に4次元ガウスを割り当てるために、計算量とメモリーオーバーヘッドを大きく課題になっており、画像の品質も低下する可能性があります。本稿では、3次元-4次元ガウススプレッティング（3D-4DGS）という新しいフレームワークを導入し、静的領域を3次元ガウスで適応的に表現し、4次元ガウスを動的な要素に残すことで解決策を提案します。我々の方法は、完全な4次元ガウス表現から始まり、時間的に変化しないガウスを3次元に変換し、パラメータ数を大幅に削減し、計算効率を向上させます。一方、動的なガウスは完全な4次元表現を維持し、高品質で複雑な動きを捉えます。我々のアプローチは、ベースライン4次元ガウススプレッティング方法と比較して、学習時間を大幅に短縮し、画像の品質を維持または向上させることができます。",
      "upvotes": 18,
      "discussionId": "682bedb6fdfa3c5de0e86b64",
      "projectPage": "https://ohsngjun.github.io/3D-4DGS/",
      "githubRepo": "https://github.com/ohsngjun/3D-4DGS",
      "ai_keywords": [
        "Gaussian Splatting",
        "4DGS",
        "4D Gaussian Splatting",
        "3D-4D Gaussian Splatting",
        "3D-4DGS",
        "3D Gaussians",
        "4D Gaussians",
        "temporal invariant",
        "computational efficiency",
        "visual quality",
        "training times"
      ]
    },
    "publishedAt": "2025-05-19T10:59:58.000Z",
    "title": "Hybrid 3D-4D Gaussian Splatting for Fast Dynamic Scene Representation",
    "summary": "Recent advancements in dynamic 3D scene reconstruction have shown promising\nresults, enabling high-fidelity 3D novel view synthesis with improved temporal\nconsistency. Among these, 4D Gaussian Splatting (4DGS) has emerged as an\nappealing approach due to its ability to model high-fidelity spatial and\ntemporal variations. However, existing methods suffer from substantial\ncomputational and memory overhead due to the redundant allocation of 4D\nGaussians to static regions, which can also degrade image quality. In this\nwork, we introduce hybrid 3D-4D Gaussian Splatting (3D-4DGS), a novel framework\nthat adaptively represents static regions with 3D Gaussians while reserving 4D\nGaussians for dynamic elements. Our method begins with a fully 4D Gaussian\nrepresentation and iteratively converts temporally invariant Gaussians into 3D,\nsignificantly reducing the number of parameters and improving computational\nefficiency. Meanwhile, dynamic Gaussians retain their full 4D representation,\ncapturing complex motions with high fidelity. Our approach achieves\nsignificantly faster training times compared to baseline 4D Gaussian Splatting\nmethods while maintaining or improving the visual quality.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.13215.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "672b66744efad666d2efb0c8",
      "avatarUrl": "/avatars/9c00a67e9d5b74694759849cca32b015.svg",
      "fullname": "Oh Seungjun",
      "name": "ohseungjun",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.12805",
      "authors": [
        {
          "_id": "682bfcec8081928badd176e7",
          "user": {
            "_id": "64ad5f59b7e4b2c1ce47eb43",
            "avatarUrl": "/avatars/1f13ebe21a90d8c99920aa2c8cd9ac45.svg",
            "isPro": false,
            "fullname": "Seanie Lee",
            "user": "Seanie-lee",
            "type": "user"
          },
          "name": "Seanie Lee",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-20T07:20:22.246Z",
          "hidden": false
        },
        {
          "_id": "682bfcec8081928badd176e8",
          "name": "Sangwoo Park",
          "hidden": false
        },
        {
          "_id": "682bfcec8081928badd176e9",
          "user": {
            "_id": "64f000769e7770db74d44bba",
            "avatarUrl": "/avatars/d015820380ffb823b1b35df64dcd3457.svg",
            "isPro": false,
            "fullname": "Dong-Bok Lee",
            "user": "dongboklee",
            "type": "user"
          },
          "name": "Dong Bok Lee",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-20T08:31:04.999Z",
          "hidden": false
        },
        {
          "_id": "682bfcec8081928badd176ea",
          "user": {
            "_id": "6311ba6f05cc08a1408d910a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662997515866-6311ba6f05cc08a1408d910a.png",
            "isPro": false,
            "fullname": "Dominik Wagner",
            "user": "dwgnr",
            "type": "user"
          },
          "name": "Dominik Wagner",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-20T08:31:33.334Z",
          "hidden": false
        },
        {
          "_id": "682bfcec8081928badd176eb",
          "user": {
            "_id": "63a9379e2e05ca32e352d93b",
            "avatarUrl": "/avatars/6cda37befc873a92ed6d5dcba507954a.svg",
            "isPro": false,
            "fullname": "Haebin Seong",
            "user": "hbseong",
            "type": "user"
          },
          "name": "Haebin Seong",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-20T08:31:39.425Z",
          "hidden": false
        },
        {
          "_id": "682bfcec8081928badd176ec",
          "name": "Tobias Bocklet",
          "hidden": false
        },
        {
          "_id": "682bfcec8081928badd176ed",
          "name": "Juho Lee",
          "hidden": false
        },
        {
          "_id": "682bfcec8081928badd176ee",
          "name": "Sung Ju Hwang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-19T07:32:56.000Z",
      "submittedOnDailyAt": "2025-05-20T03:02:05.528Z",
      "title": "FedSVD: ローラーベースの個人的なフェデレーテッドラーニングの適応的正交化",
      "submittedOnDailyBy": {
        "_id": "638716c14e00d7fc0902fef4",
        "avatarUrl": "/avatars/5fa8152f8c0e4e600d1a64802c3e0103.svg",
        "isPro": false,
        "fullname": "Sangwoo Park",
        "user": "Sangsang",
        "type": "user"
      },
      "summary": "低ランクアダプタイション（LoRA）は、2つの学習可能な低ランク行列の積を冻結された事前学習済み重みに追加し、連邦学習（FL）で言語モデルの効率的な微調節に広く使用されています。しかし、差分隠蔽の乱数化された勾配降下法（DP-SGD）と組み合わせると、LoRAは大きな乱数拡大を課題にされます：DP-SGDはサンプル毎の勾配を乱数化し、LoRAの更新（BA）の行列積がこの効果を強めることになります。1つの行列（例えばA）を冻結させることで、乱数が減少するが、モデルの表現力が制限され、通常、最適なアダプティングにならないことが多いです。これを解決するために、私たちは、固有値分解（SVD）に基づくグローバル再パラメトリゼーションを採用する簡単で効果的な方法、FedSVDを提案します。私たちのアプローチでは、各クライアントはそのみB行列を最適化し、サーバーに送信します。サーバーはB行列を集約し、前のAを使用してBAの積を計算し、SVDで結果を再分解します。これにより、新しいアダプティブなAがBAの正規正交行列の右固有ベクトルからなるものと、更新されたBが残りのSVDコンポーネントを含みます。この再パラメトリゼーションは、二次的な乱数拡大を避けながらも、Aが集約更新の主な方向をより良く捉えることができます。また、Aの正規正交構造は、DP-SGDでもより多くの信号を保存し、Bの勾配の大きさを制限することができます。これを理論的な分析により確認しました。このように、FedSVDは多様なプライバシー設定とベンチマークで安定した性能を収め、両方公開と非公開の仕様で関連する基準を超えます。",
      "upvotes": 17,
      "discussionId": "682bfcef8081928badd177c0",
      "ai_keywords": [
        "Low-Rank Adaptation (LoRA)",
        "pre-trained weights",
        "federated learning (FL)",
        "differentially private stochastic gradient descent (DP-SGD)",
        "matrix multiplication",
        "singular value decomposition (SVD)",
        "reparameterization",
        "orthonormal right singular vectors",
        "orthonormal structure",
        "gradient norms"
      ]
    },
    "publishedAt": "2025-05-19T03:32:56.000Z",
    "title": "FedSVD: Adaptive Orthogonalization for Private Federated Learning with\n  LoRA",
    "summary": "Low-Rank Adaptation (LoRA), which introduces a product of two trainable\nlow-rank matrices into frozen pre-trained weights, is widely used for efficient\nfine-tuning of language models in federated learning (FL). However, when\ncombined with differentially private stochastic gradient descent (DP-SGD), LoRA\nfaces substantial noise amplification: DP-SGD perturbs per-sample gradients,\nand the matrix multiplication of the LoRA update (BA) intensifies this\neffect. Freezing one matrix (e.g., A) reduces the noise but restricts model\nexpressiveness, often resulting in suboptimal adaptation. To address this, we\npropose FedSVD, a simple yet effective method that introduces a global\nreparameterization based on singular value decomposition (SVD). In our\napproach, each client optimizes only the B matrix and transmits it to the\nserver. The server aggregates the B matrices, computes the product BA using\nthe previous A, and refactorizes the result via SVD. This yields a new\nadaptive A composed of the orthonormal right singular vectors of BA, and an\nupdated B containing the remaining SVD components. This reparameterization\navoids quadratic noise amplification, while allowing A to better capture the\nprincipal directions of the aggregate updates. Moreover, the orthonormal\nstructure of A bounds the gradient norms of B and preserves more signal\nunder DP-SGD, as confirmed by our theoretical analysis. As a result, FedSVD\nconsistently improves stability and performance across a variety of privacy\nsettings and benchmarks, outperforming relevant baselines under both private\nand non-private regimes.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.12805.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "638716c14e00d7fc0902fef4",
      "avatarUrl": "/avatars/5fa8152f8c0e4e600d1a64802c3e0103.svg",
      "fullname": "Sangwoo Park",
      "name": "Sangsang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.12504",
      "authors": [
        {
          "_id": "682bf9090080c5ce0c1b43a1",
          "user": {
            "_id": "674d42a03a4b7e31a1707218",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/3DIez-RYnDMYe1U-m0qBZ.png",
            "isPro": false,
            "fullname": "kkkai",
            "user": "Zkkkai",
            "type": "user"
          },
          "name": "Zongkai Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-20T08:32:24.351Z",
          "hidden": false
        },
        {
          "_id": "682bf9090080c5ce0c1b43a2",
          "user": {
            "_id": "640b37b2bab5ca8fbe7df8f2",
            "avatarUrl": "/avatars/c7bef45efad6a0d911a720e2236fcba5.svg",
            "isPro": false,
            "fullname": "fanqing meng",
            "user": "FanqingM",
            "type": "user"
          },
          "name": "Fanqing Meng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-20T08:32:31.174Z",
          "hidden": false
        },
        {
          "_id": "682bf9090080c5ce0c1b43a3",
          "user": {
            "_id": "666fe1a5b07525f0bde69c27",
            "avatarUrl": "/avatars/bb98ab0b974c8fe011739baa8dadd91a.svg",
            "isPro": false,
            "fullname": "Lingxiao Du",
            "user": "Cierra0506",
            "type": "user"
          },
          "name": "Lingxiao Du",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-20T08:32:46.648Z",
          "hidden": false
        },
        {
          "_id": "682bf9090080c5ce0c1b43a4",
          "user": {
            "_id": "674bfdf227f531cdc248bb5c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/674bfdf227f531cdc248bb5c/xh4gw89sr8MzNzRdiTjFx.jpeg",
            "isPro": false,
            "fullname": "Zhixiang Zhou",
            "user": "SuperposedWave",
            "type": "user"
          },
          "name": "Zhixiang Zhou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-20T08:32:54.272Z",
          "hidden": false
        },
        {
          "_id": "682bf9090080c5ce0c1b43a5",
          "name": "Chao Yu",
          "hidden": false
        },
        {
          "_id": "682bf9090080c5ce0c1b43a6",
          "user": {
            "_id": "64b3fd42eec33e27dcc4c941",
            "avatarUrl": "/avatars/5aa1a99468fa61d4b8b0e80b592c4e55.svg",
            "isPro": false,
            "fullname": "Wenqi Shao",
            "user": "wqshao126",
            "type": "user"
          },
          "name": "Wenqi Shao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-20T08:33:04.198Z",
          "hidden": false
        },
        {
          "_id": "682bf9090080c5ce0c1b43a7",
          "user": {
            "_id": "63cf4ecdc1dedf59c8f8362e",
            "avatarUrl": "/avatars/cede885854d6a1551860080d55c87568.svg",
            "isPro": false,
            "fullname": "Qiaosheng ZHANG",
            "user": "Domingo12",
            "type": "user"
          },
          "name": "Qiaosheng Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-20T08:33:09.646Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-18T17:44:53.000Z",
      "submittedOnDailyAt": "2025-05-20T02:10:10.274Z",
      "title": "CPGD: 言語モデル向けの安定化ルールベース強化学習のための研究",
      "submittedOnDailyBy": {
        "_id": "674d42a03a4b7e31a1707218",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/3DIez-RYnDMYe1U-m0qBZ.png",
        "isPro": false,
        "fullname": "kkkai",
        "user": "Zkkkai",
        "type": "user"
      },
      "summary": "最近のルールベースの強化学習（RL）の進展は、ルールベースの報酬を用いた言語モデル（LMs）の理由能力を大幅に向上させました。しかし、現在のRL手法（例：GRPO、REINFORCE++、RLOO）は、大きなポリシー更新と適切なクリッピングがトレーニング崩壊を招くことを通じてトレーニング不穩定に苦しむことがあります。この問題を解決するために、Clipped Policy Gradient Optimization with Policy Drift（CPGD）という新しいアルゴリズムを提案します。CPGDは、KL分散に基づくポリシードリフト制約を導入して、ポリシー更新を動的に正規化し、比率の対数に対してクリップ機能を利用して過度なポリシー更新を防ぐように設計されています。CPGDの理論的な正当性を提供し、先行手法で見落とされていた不穩定を軽減することを証明します。また、CPGDはトレーニングの安定性を維持することで性能を大幅に向上させることを示します。我々の実装は理論的な厳密性と実用的な利用可能性をバランスに持ち、LMsのトレーニング後のRLの強力な代替となるものです。我々のコードはhttps://github.com/ModalMinds/MM-EUREKAに公開しています。",
      "upvotes": 17,
      "discussionId": "682bf90a0080c5ce0c1b43c7",
      "ai_keywords": [
        "Clipped Policy Gradient Optimization with Policy Drift (CPGD)",
        "policy drift constraint",
        "KL divergence",
        "policy updates",
        "training instability",
        "training collapse",
        "theoretical justification",
        "empirical analysis",
        "performance improvement",
        "robust alternative"
      ]
    },
    "publishedAt": "2025-05-18T13:44:53.000Z",
    "title": "CPGD: Toward Stable Rule-based Reinforcement Learning for Language\n  Models",
    "summary": "Recent advances in rule-based reinforcement learning (RL) have significantly\nimproved the reasoning capability of language models (LMs) with rule-based\nrewards. However, existing RL methods -- such as GRPO, REINFORCE++, and RLOO --\noften suffer from training instability, where large policy updates and improper\nclipping can lead to training collapse. To address this issue, we propose\nClipped Policy Gradient Optimization with Policy Drift (CPGD), a novel\nalgorithm designed to stabilize policy learning in LMs. CPGD introduces a\npolicy drift constraint based on KL divergence to dynamically regularize policy\nupdates, and leverages a clip mechanism on the logarithm of the ratio to\nprevent excessive policy updates. We provide theoretical justification for CPGD\nand demonstrate through empirical analysis that it mitigates the instability\nobserved in prior approaches. Furthermore, we show that CPGD significantly\nimproves performance while maintaining training stability. Our implementation\nbalances theoretical rigor with practical usability, offering a robust\nalternative for RL in the post-training of LMs. We release our code at\nhttps://github.com/ModalMinds/MM-EUREKA.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.12504.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "674d42a03a4b7e31a1707218",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/3DIez-RYnDMYe1U-m0qBZ.png",
      "fullname": "kkkai",
      "name": "Zkkkai",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.13389",
      "authors": [
        {
          "_id": "682c27e2fffb36958f8cd84e",
          "user": {
            "_id": "63565cc56d7fcf1bedb7d347",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63565cc56d7fcf1bedb7d347/XGcHP4VkO_oieA1gZ4IAX.jpeg",
            "isPro": false,
            "fullname": "Zhang Peiyuan",
            "user": "PY007",
            "type": "user"
          },
          "name": "Peiyuan Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-20T08:36:24.007Z",
          "hidden": false
        },
        {
          "_id": "682c27e2fffb36958f8cd84f",
          "user": {
            "_id": "67ea1f6693f71dd8167a2d22",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/H_upra_XVG1AoBKUe9ArV.png",
            "isPro": false,
            "fullname": "haofeng huang",
            "user": "haofeng666",
            "type": "user"
          },
          "name": "Haofeng Huang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-20T08:36:29.763Z",
          "hidden": false
        },
        {
          "_id": "682c27e2fffb36958f8cd850",
          "user": {
            "_id": "65416817271d3bc4d70f6745",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65416817271d3bc4d70f6745/1YkW0MpuufejvxqksVMIx.jpeg",
            "isPro": false,
            "fullname": "Yongqi Chen",
            "user": "BrianChen1129",
            "type": "user"
          },
          "name": "Yongqi Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-20T08:36:36.258Z",
          "hidden": false
        },
        {
          "_id": "682c27e2fffb36958f8cd851",
          "name": "Will Lin",
          "hidden": false
        },
        {
          "_id": "682c27e2fffb36958f8cd852",
          "user": {
            "_id": "62fbdc67c776fd8821ae3f2d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62fbdc67c776fd8821ae3f2d/cI7iAZOL40RUYluo5ZVTU.png",
            "isPro": false,
            "fullname": "Zhengzhong Liu",
            "user": "hunterhector",
            "type": "user"
          },
          "name": "Zhengzhong Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-20T08:36:46.746Z",
          "hidden": false
        },
        {
          "_id": "682c27e2fffb36958f8cd853",
          "name": "Ion Stoica",
          "hidden": false
        },
        {
          "_id": "682c27e2fffb36958f8cd854",
          "user": {
            "_id": "64ff67722ad36636be6c4542",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/sLIrNelAWPVOy4e3oo5LB.jpeg",
            "isPro": false,
            "fullname": "Eric Xing",
            "user": "EricX003",
            "type": "user"
          },
          "name": "Eric P. Xing",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-20T08:37:02.570Z",
          "hidden": false
        },
        {
          "_id": "682c27e2fffb36958f8cd855",
          "name": "Hao Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-19T17:30:13.000Z",
      "submittedOnDailyAt": "2025-05-20T05:27:46.441Z",
      "title": "Faster Video Diffusion with Trainable Sparse Attention",
      "submittedOnDailyBy": {
        "_id": "63565cc56d7fcf1bedb7d347",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63565cc56d7fcf1bedb7d347/XGcHP4VkO_oieA1gZ4IAX.jpeg",
        "isPro": false,
        "fullname": "Zhang Peiyuan",
        "user": "PY007",
        "type": "user"
      },
      "summary": "スカラインドバイデオディフュージョントランスフォーマー（DiTs）は、その二次元3D注意機能によって制限されていますが、ほとんどの注意量が小さな位置のサブセットに集中していることが多いです。この観察をビジネスソース（VSA）として、訓練および推論で全ての注意を置換する訓練可能で、ハードウェア効率的なスパース注意に変換します。VSAでは、軽量コアステージはトークンをテーブルにまとめ、高ウェイトの重要なトークンを特定し、ファインステージはブロック計算レイアウトによってテーブル内でのトークンレベルの注意を計算します。これにより、一つの微分可変カーネルが構築され、終端から訓練でき、後処理プロファイリングが必要なく、FlashAttention3 MFUの85%を維持します。DiTsの60Mから1.4Bパラメータの予ちりに大きなスワープの消滅研究とスケーリングラーの実験を行い、VSAは2.53倍の訓練FLOPS削減を実現し、ディフュージョン損失の低下はありません。オープンソースのWan-2.1モデルを再構築すると、注意時間を6倍速くし、31秒から18秒に設定時間を短縮し、相当の品質を維持します。これらの結果は、訓練可能なスパース注意が全注意の実用的な代替として、ビデオディフュージョンモデルの進一步なスケーリングの鍵となることを確立します。",
      "upvotes": 13,
      "discussionId": "682c27e3fffb36958f8cd8c2",
      "ai_keywords": [
        "diffusion transformers",
        "3D attention",
        "sparse attention",
        "token-level attention",
        "block computing",
        "differentiable kernel",
        "training FLOPS",
        "diffusion loss",
        "open-source Wan-2.1 model",
        "attention time",
        "end-to-end generation time"
      ]
    },
    "publishedAt": "2025-05-19T13:30:13.000Z",
    "title": "Faster Video Diffusion with Trainable Sparse Attention",
    "summary": "Scaling video diffusion transformers (DiTs) is limited by their quadratic 3D\nattention, even though most of the attention mass concentrates on a small\nsubset of positions. We turn this observation into VSA, a trainable,\nhardware-efficient sparse attention that replaces full attention at both\ntraining and inference. In VSA, a lightweight coarse stage pools tokens into\ntiles and identifies high-weight critical tokens; a fine stage computes\ntoken-level attention only inside those tiles subjecting to block computing\nlayout to ensure hard efficiency. This leads to a single differentiable kernel\nthat trains end-to-end, requires no post-hoc profiling, and sustains 85\\% of\nFlashAttention3 MFU. We perform a large sweep of ablation studies and\nscaling-law experiments by pretraining DiTs from 60M to 1.4B parameters. VSA\nreaches a Pareto point that cuts training FLOPS by 2.53times with no drop in\ndiffusion loss. Retrofitting the open-source Wan-2.1 model speeds up attention\ntime by 6times and lowers end-to-end generation time from 31s to 18s with\ncomparable quality. These results establish trainable sparse attention as a\npractical alternative to full attention and a key enabler for further scaling\nof video diffusion models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.13389.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63565cc56d7fcf1bedb7d347",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63565cc56d7fcf1bedb7d347/XGcHP4VkO_oieA1gZ4IAX.jpeg",
      "fullname": "Zhang Peiyuan",
      "name": "PY007",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 85
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.12992",
      "authors": [
        {
          "_id": "682c12290f622b7afc1fc98f",
          "user": {
            "_id": "62c414354ce7250560a1f67f",
            "avatarUrl": "/avatars/28fd73973d1703c84f4f59644fef8a80.svg",
            "isPro": false,
            "fullname": "Baohao Liao",
            "user": "baohao",
            "type": "user"
          },
          "name": "Baohao Liao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-20T08:33:43.331Z",
          "hidden": false
        },
        {
          "_id": "682c12290f622b7afc1fc990",
          "user": {
            "_id": "63a3ff69f91ad3ea5703841d",
            "avatarUrl": "/avatars/69227c4bce01d33747c1377b6f9672db.svg",
            "isPro": false,
            "fullname": "Hanze Dong",
            "user": "hendrydong",
            "type": "user"
          },
          "name": "Hanze Dong",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-20T08:33:50.212Z",
          "hidden": false
        },
        {
          "_id": "682c12290f622b7afc1fc991",
          "user": {
            "_id": "6602869253a0518b2a98cafd",
            "avatarUrl": "/avatars/c14b5953a716f42c83ad28147f8308ae.svg",
            "isPro": false,
            "fullname": "Yuhui Xu",
            "user": "yuhuixu",
            "type": "user"
          },
          "name": "Yuhui Xu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-20T08:33:37.033Z",
          "hidden": false
        },
        {
          "_id": "682c12290f622b7afc1fc992",
          "user": {
            "_id": "65f84fd980481173afd91233",
            "avatarUrl": "/avatars/6ac7bd6beba24d1476c5179b88c9e3fa.svg",
            "isPro": false,
            "fullname": "Doyen",
            "user": "doyensahoo",
            "type": "user"
          },
          "name": "Doyen Sahoo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-20T08:33:57.766Z",
          "hidden": false
        },
        {
          "_id": "682c12290f622b7afc1fc993",
          "name": "Christof Monz",
          "hidden": false
        },
        {
          "_id": "682c12290f622b7afc1fc994",
          "user": {
            "_id": "61f9d3b54ac99e8a1bae85f4",
            "avatarUrl": "/avatars/ac47d13204dd22452e4bc46e280842d5.svg",
            "isPro": false,
            "fullname": "JunnanLi",
            "user": "JunnanLi",
            "type": "user"
          },
          "name": "Junnan Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-20T08:34:18.715Z",
          "hidden": false
        },
        {
          "_id": "682c12290f622b7afc1fc995",
          "user": {
            "_id": "649dbcc4e0fff1ed099dc80a",
            "avatarUrl": "/avatars/c87c273ca628dbcddccbf1ee19b2ce33.svg",
            "isPro": false,
            "fullname": "Caiming Xiong",
            "user": "cxiong",
            "type": "user"
          },
          "name": "Caiming Xiong",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-20T08:34:27.628Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-19T11:30:41.000Z",
      "submittedOnDailyAt": "2025-05-20T03:55:28.140Z",
      "title": "Fractured Chain-of-Thought Reasoning",
      "submittedOnDailyBy": {
        "_id": "6602869253a0518b2a98cafd",
        "avatarUrl": "/avatars/c14b5953a716f42c83ad28147f8308ae.svg",
        "isPro": false,
        "fullname": "Yuhui Xu",
        "user": "yuhuixu",
        "type": "user"
      },
      "summary": "推論時のスケーリングテクニックは、再学習を避けながら追加的な計算努力を利用して、大規模な言語モデル（LLMs）の推理能力を大幅に向上させました。同様に、Chain-of-Thought（CoT）プロンプティングとその拡張版、Long CoTは、複雑な中間的な推理トラジェクトリーを生成して正確性を向上させますが、これらのアプローチは、トークンコストが高く、遅延敏感な設定での実装を妨げています。本研究では、まずトランク化されたCoT（truncated CoT）を使用して、理由の完成までの前に最終的な答えを直接生成することで、通常のCoTサンプリングと同等の性能を示すことを示します。この見通しに基づいて、我々は、全CoTサンプリングと解答だけのサンプリングの間で3つの直交する軸（1）理由トラジェクトの数、（2）それぞれのトラジェクトの最終的な解答の数、（3）理由トレースのトランク化の深さを挙げるフレクチャーサンプリング（Fractured Sampling）という統一的な推論時の戦略を導入します。5つの多様な推理ベンチマークと数々のモデルサイズでの拡張的な実験を通じて、我々は、Fractured Samplingが常に優れた精度-コストの調整を実現し、Pass@kとトークンバジェットの対数線形スケーリング効果を示します。分析では、これらの次元での計算量の割り当てを最適化する方法を明らかにし、より効率的かつスケーラブルなLLM推理を実現する道が開かれます。",
      "upvotes": 13,
      "discussionId": "682c122a0f622b7afc1fc9b7",
      "ai_keywords": [
        "truncated CoT",
        "Fractured Sampling",
        "reasoning trajectories",
        "solution-only sampling",
        "orthogonal axes",
        "depth of reasoning traces",
        "Pass@k",
        "token budget",
        "performance",
        "computational allocation"
      ]
    },
    "publishedAt": "2025-05-19T07:30:41.000Z",
    "title": "Fractured Chain-of-Thought Reasoning",
    "summary": "Inference-time scaling techniques have significantly bolstered the reasoning\ncapabilities of large language models (LLMs) by harnessing additional\ncomputational effort at inference without retraining. Similarly,\nChain-of-Thought (CoT) prompting and its extension, Long CoT, improve accuracy\nby generating rich intermediate reasoning trajectories, but these approaches\nincur substantial token costs that impede their deployment in latency-sensitive\nsettings. In this work, we first show that truncated CoT, which stops reasoning\nbefore completion and directly generates the final answer, often matches full\nCoT sampling while using dramatically fewer tokens. Building on this insight,\nwe introduce Fractured Sampling, a unified inference-time strategy that\ninterpolates between full CoT and solution-only sampling along three orthogonal\naxes: (1) the number of reasoning trajectories, (2) the number of final\nsolutions per trajectory, and (3) the depth at which reasoning traces are\ntruncated. Through extensive experiments on five diverse reasoning benchmarks\nand several model scales, we demonstrate that Fractured Sampling consistently\nachieves superior accuracy-cost trade-offs, yielding steep log-linear scaling\ngains in Pass@k versus token budget. Our analysis reveals how to allocate\ncomputation across these dimensions to maximize performance, paving the way for\nmore efficient and scalable LLM reasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.12992.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6602869253a0518b2a98cafd",
      "avatarUrl": "/avatars/c14b5953a716f42c83ad28147f8308ae.svg",
      "fullname": "Yuhui Xu",
      "name": "yuhuixu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.12081",
      "authors": [
        {
          "_id": "682be7b7a1a5d85b0537de81",
          "user": {
            "_id": "669cefd6119595d21b55a995",
            "avatarUrl": "/avatars/bafc2387ee70b263bf45c42159381da8.svg",
            "isPro": false,
            "fullname": "Yuqi Liu",
            "user": "Ricky06662",
            "type": "user"
          },
          "name": "Yuqi Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-20T07:21:05.792Z",
          "hidden": false
        },
        {
          "_id": "682be7b7a1a5d85b0537de82",
          "user": {
            "_id": "66e79b3c1c79fc2e51dc1d60",
            "avatarUrl": "/avatars/8706336e9e7a417505c9bb32583a662f.svg",
            "isPro": false,
            "fullname": "QU Tianyuan",
            "user": "TainU",
            "type": "user"
          },
          "name": "Tianyuan Qu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-20T08:34:44.348Z",
          "hidden": false
        },
        {
          "_id": "682be7b7a1a5d85b0537de83",
          "user": {
            "_id": "65d882d30f35ed3f52d3ae2c",
            "avatarUrl": "/avatars/22cda67c3fcd7150320ec3551eda90f5.svg",
            "isPro": false,
            "fullname": "Zhisheng Zhong",
            "user": "zszhong",
            "type": "user"
          },
          "name": "Zhisheng Zhong",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-20T08:35:08.921Z",
          "hidden": false
        },
        {
          "_id": "682be7b7a1a5d85b0537de84",
          "user": {
            "_id": "673a10f911b7efeeedabc252",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/T7ySn7F0pTVCvRdcvMz3d.png",
            "isPro": false,
            "fullname": "Bohao Peng",
            "user": "BoHao0326",
            "type": "user"
          },
          "name": "Bohao Peng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-20T08:35:16.720Z",
          "hidden": false
        },
        {
          "_id": "682be7b7a1a5d85b0537de85",
          "name": "Shu Liu",
          "hidden": false
        },
        {
          "_id": "682be7b7a1a5d85b0537de86",
          "name": "Bei Yu",
          "hidden": false
        },
        {
          "_id": "682be7b7a1a5d85b0537de87",
          "name": "Jiaya Jia",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-17T16:51:47.000Z",
      "submittedOnDailyAt": "2025-05-20T00:54:15.427Z",
      "title": "VisionReasoner: 統合的視覚認識と理由論を通じた強化学習",
      "submittedOnDailyBy": {
        "_id": "65d882d30f35ed3f52d3ae2c",
        "avatarUrl": "/avatars/22cda67c3fcd7150320ec3551eda90f5.svg",
        "isPro": false,
        "fullname": "Zhisheng Zhong",
        "user": "zszhong",
        "type": "user"
      },
      "summary": "大視覚言語モデルは、多様な視覚認知タスクを処理する固有の能力を持っています。本文では、VisionReasonerという一つの統一的なフレームワークを介して、複数の視覚認知タスクを理由論と解決することができることを紹介します。特に、新しい多物体認知学習戦略とシステマティックなタスク再設定を設計し、VisionReasonerは理由論能力を向上させ、視覚入力を分析し、統一的なフレームワークで多様な認知タスクを解決することができます。モデルは、ユーザーのクエリに対して望みの出力を提供する前に、構造化された理由論プロセスを生成します。統一的な視覚認知能力を厳密に評価するために、VisionReasonerは検出、分割、カウントの3つの重要な領域にわたる10種類の多様なタスクにおいて評価されます。実験結果によると、VisionReasonerは統一的なモデルとして上位の性能を達成し、Qwen2.5VLを対比して、COCO（検出）で29.1%、ReasonSeg（分割）で22.1%、CountBench（カウント）で15.3%の相対的な差異で上位を越えました。",
      "upvotes": 13,
      "discussionId": "682be7b8a1a5d85b0537dea8",
      "githubRepo": "https://github.com/dvlab-research/VisionReasoner",
      "ai_keywords": [
        "VisionReasoner",
        "multi-object cognitive learning strategies",
        "task reformulation",
        "structured reasoning process",
        "unified framework"
      ]
    },
    "publishedAt": "2025-05-17T12:51:47.000Z",
    "title": "VisionReasoner: Unified Visual Perception and Reasoning via\n  Reinforcement Learning",
    "summary": "Large vision-language models exhibit inherent capabilities to handle diverse\nvisual perception tasks. In this paper, we introduce VisionReasoner, a unified\nframework capable of reasoning and solving multiple visual perception tasks\nwithin a shared model. Specifically, by designing novel multi-object cognitive\nlearning strategies and systematic task reformulation, VisionReasoner enhances\nits reasoning capabilities to analyze visual inputs, and addresses diverse\nperception tasks in a unified framework. The model generates a structured\nreasoning process before delivering the desired outputs responding to user\nqueries. To rigorously assess unified visual perception capabilities, we\nevaluate VisionReasoner on ten diverse tasks spanning three critical domains:\ndetection, segmentation, and counting. Experimental results show that\nVisionReasoner achieves superior performance as a unified model, outperforming\nQwen2.5VL by relative margins of 29.1% on COCO (detection), 22.1% on ReasonSeg\n(segmentation), and 15.3% on CountBench (counting).",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.12081.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65d882d30f35ed3f52d3ae2c",
      "avatarUrl": "/avatars/22cda67c3fcd7150320ec3551eda90f5.svg",
      "fullname": "Zhisheng Zhong",
      "name": "zszhong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.11932",
      "authors": [
        {
          "_id": "682bf7363e041a44f23afcea",
          "user": {
            "_id": "64bdfa1a1a62149c5e80ef6f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Wjc9gPFzlARBkdoTAOZm8.png",
            "isPro": false,
            "fullname": "Yuyao Zhang",
            "user": "KeriaZhang",
            "type": "user"
          },
          "name": "Yuyao Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-20T08:38:04.060Z",
          "hidden": false
        },
        {
          "_id": "682bf7363e041a44f23afceb",
          "user": {
            "_id": "66f0bf59e9d50ec57febf751",
            "avatarUrl": "/avatars/be97941e60064e5dd806c6fe9db3c537.svg",
            "isPro": false,
            "fullname": "Zhicheng Dou",
            "user": "douzc",
            "type": "user"
          },
          "name": "Zhicheng Dou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-20T08:37:46.972Z",
          "hidden": false
        },
        {
          "_id": "682bf7363e041a44f23afcec",
          "user": {
            "_id": "66e03eace17fb5ff054b7686",
            "avatarUrl": "/avatars/2b739ff11e43dd9e701c647a92617f20.svg",
            "isPro": false,
            "fullname": "Xiaoxi Li",
            "user": "lixiaoxi45",
            "type": "user"
          },
          "name": "Xiaoxi Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-20T08:37:41.072Z",
          "hidden": false
        },
        {
          "_id": "682bf7363e041a44f23afced",
          "name": "Jiajie Jin",
          "hidden": false
        },
        {
          "_id": "682bf7363e041a44f23afcee",
          "user": {
            "_id": "62f3a590261bc5fb2e072a5f",
            "avatarUrl": "/avatars/d65d362ddc32aca3d6c564252d81e109.svg",
            "isPro": false,
            "fullname": "YongkangWu",
            "user": "wuyongkang",
            "type": "user"
          },
          "name": "Yongkang Wu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-20T08:38:33.785Z",
          "hidden": false
        },
        {
          "_id": "682bf7363e041a44f23afcef",
          "name": "Zhonghua Li",
          "hidden": false
        },
        {
          "_id": "682bf7363e041a44f23afcf0",
          "name": "Qi Ye",
          "hidden": false
        },
        {
          "_id": "682bf7363e041a44f23afcf1",
          "user": {
            "_id": "64b8c89052b7353d8c6a1013",
            "avatarUrl": "/avatars/cd59fffe81f6b07b4519540b8ff3d95f.svg",
            "isPro": false,
            "fullname": "Ji-Rong Wen",
            "user": "jrwen",
            "type": "user"
          },
          "name": "Ji-Rong Wen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-20T08:38:51.063Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-17T09:36:03.000Z",
      "submittedOnDailyAt": "2025-05-20T02:02:22.305Z",
      "title": "Neuro-Symbolic Query Compiler\n\nニューロシンボリッククエリーコンパイラ",
      "submittedOnDailyBy": {
        "_id": "66e03eace17fb5ff054b7686",
        "avatarUrl": "/avatars/2b739ff11e43dd9e701c647a92617f20.svg",
        "isPro": false,
        "fullname": "Xiaoxi Li",
        "user": "lixiaoxi45",
        "type": "user"
      },
      "summary": "検索インテントの精密認識は、リフェレンスアウガーデーション（RAG）システムでは特に資源制約のある状況や複雑なクエリにおいて難しい目標です。本論文では、言語文法ルールとコンパイラ設計によるインスピレーションを受けたニューロシンボリックフレームワーク「QCompiler」を提案し、この間違いをつなぎ合わせています。理論的には、複雑なクエリを形式化するために最小であり十分なバクスナーフォーム（BNF）文法G[q]を設計します。前の方法と異なり、この文法は完全性を維持しながら冗長を最小化しています。これに基づいて、QCompilerはクエリ表現翻訳機能、語法解析機能、再帰降下処理機能を含み、クエリを抽象的なシンボリック木（AST）に変換して実行することができます。葉ノードの子クエリの原子性が、ドキュメントの検索とレスポンスの生成においてより精密な結果を得ることを保証し、RAGシステムが複雑なクエリを対処する能力を大幅に向上させます。",
      "upvotes": 11,
      "discussionId": "682bf7373e041a44f23afd25",
      "githubRepo": "https://github.com/YuyaoZhangQAQ/QCompiler",
      "ai_keywords": [
        "Retrieval-Augmented Generation (RAG)",
        "neuro-symbolic framework",
        "Backus-Naur Form (BNF)",
        "Query Expression Translator",
        "Lexical Syntax Parser",
        "Recursive Descent Processor",
        "Abstract Syntax Trees (ASTs)",
        "document retrieval"
      ]
    },
    "publishedAt": "2025-05-17T05:36:03.000Z",
    "title": "Neuro-Symbolic Query Compiler",
    "summary": "Precise recognition of search intent in Retrieval-Augmented Generation (RAG)\nsystems remains a challenging goal, especially under resource constraints and\nfor complex queries with nested structures and dependencies. This paper\npresents QCompiler, a neuro-symbolic framework inspired by linguistic grammar\nrules and compiler design, to bridge this gap. It theoretically designs a\nminimal yet sufficient Backus-Naur Form (BNF) grammar G[q] to formalize\ncomplex queries. Unlike previous methods, this grammar maintains completeness\nwhile minimizing redundancy. Based on this, QCompiler includes a Query\nExpression Translator, a Lexical Syntax Parser, and a Recursive Descent\nProcessor to compile queries into Abstract Syntax Trees (ASTs) for execution.\nThe atomicity of the sub-queries in the leaf nodes ensures more precise\ndocument retrieval and response generation, significantly improving the RAG\nsystem's ability to address complex queries.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.11932.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "66e03eace17fb5ff054b7686",
      "avatarUrl": "/avatars/2b739ff11e43dd9e701c647a92617f20.svg",
      "fullname": "Xiaoxi Li",
      "name": "lixiaoxi45",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.13180",
      "authors": [
        {
          "_id": "682c389bc19ea9cd7d822b5c",
          "user": {
            "_id": "644555c72d91b15b4c7ebd1c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644555c72d91b15b4c7ebd1c/28zmmIkLHUUiQXQ3RQlPM.jpeg",
            "isPro": false,
            "fullname": "Matteo Merler",
            "user": "merlerm",
            "type": "user"
          },
          "name": "Matteo Merler",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-20T08:39:09.385Z",
          "hidden": false
        },
        {
          "_id": "682c389bc19ea9cd7d822b5d",
          "user": {
            "_id": "6382346663e3fab40c8c66f9",
            "avatarUrl": "/avatars/bcdba23952ff465b8488bd68a61005e5.svg",
            "isPro": false,
            "fullname": "Nicola Dainese",
            "user": "dainesn1",
            "type": "user"
          },
          "name": "Nicola Dainese",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-20T08:39:30.830Z",
          "hidden": false
        },
        {
          "_id": "682c389bc19ea9cd7d822b5e",
          "user": {
            "_id": "64c268c4b57937d56d65e163",
            "avatarUrl": "/avatars/bf290d81983703e457e709fec1a2300e.svg",
            "isPro": false,
            "fullname": "Minttu Alakuijala",
            "user": "minttusofia",
            "type": "user"
          },
          "name": "Minttu Alakuijala",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-20T08:23:26.845Z",
          "hidden": false
        },
        {
          "_id": "682c389bc19ea9cd7d822b5f",
          "user": {
            "_id": "60d9e5b71fa5d458da777550",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676481484908-60d9e5b71fa5d458da777550.png",
            "isPro": false,
            "fullname": "Giovanni Bonetta",
            "user": "giobin",
            "type": "user"
          },
          "name": "Giovanni Bonetta",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-20T08:39:37.830Z",
          "hidden": false
        },
        {
          "_id": "682c389bc19ea9cd7d822b60",
          "user": {
            "_id": "6512b0e48c0f10eedb296c65",
            "avatarUrl": "/avatars/46cf7ddf5f94468b7cf39a787741ca2d.svg",
            "isPro": false,
            "fullname": "Pietro Ferrazzi",
            "user": "Pietroferr",
            "type": "user"
          },
          "name": "Pietro Ferrazzi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-20T08:40:03.086Z",
          "hidden": false
        },
        {
          "_id": "682c389bc19ea9cd7d822b61",
          "name": "Yu Tian",
          "hidden": false
        },
        {
          "_id": "682c389bc19ea9cd7d822b62",
          "user": {
            "_id": "666d3b2bb955b0e655473ffe",
            "avatarUrl": "/avatars/de83261afe1655b857a34f3c9f1d0bcc.svg",
            "isPro": false,
            "fullname": "Bernardo Magnini",
            "user": "magnini",
            "type": "user"
          },
          "name": "Bernardo Magnini",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-20T08:40:11.231Z",
          "hidden": false
        },
        {
          "_id": "682c389bc19ea9cd7d822b63",
          "name": "Pekka Marttinen",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/644555c72d91b15b4c7ebd1c/78QkuWLqE7ymFCANRaoMM.png",
        "https://cdn-uploads.huggingface.co/production/uploads/644555c72d91b15b4c7ebd1c/oaFbpbvdWQvVFhQbSnXcF.png",
        "https://cdn-uploads.huggingface.co/production/uploads/644555c72d91b15b4c7ebd1c/2y26ftHdYf0mP6NhVSc-b.png",
        "https://cdn-uploads.huggingface.co/production/uploads/644555c72d91b15b4c7ebd1c/z1TeQqgR8lGqfgfzQJF3L.png"
      ],
      "publishedAt": "2025-05-19T14:38:15.000Z",
      "submittedOnDailyAt": "2025-05-20T06:44:23.174Z",
      "title": "ViPlan: 符号講句とビジョン・ラングラジュモデルを用いた可視計画のベンチマーク",
      "submittedOnDailyBy": {
        "_id": "644555c72d91b15b4c7ebd1c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644555c72d91b15b4c7ebd1c/28zmmIkLHUUiQXQ3RQlPM.jpeg",
        "isPro": false,
        "fullname": "Matteo Merler",
        "user": "merlerm",
        "type": "user"
      },
      "summary": "大語言モデルと符号的なプランナーの統合は、自然語言でのプランニングに比べて、確認可能で地道なプランを得るための有望な方向である。最近の研究は、ビジョン-言語モデル（VLM）を使用してこのアイデアをビジョンドメインに拡張している。しかし、VLMに基づく符号的なアプローチとVLMを直接用いたプランニング手法の厳密な比較は、共通環境、評価プロトコルとモデルのカバーの不足により妨害されていた。我々は、ViPlanという最初のオープンソースベンチマークを紹介し、符号的な予想とVLMを用いたビジョンプランニングを評価する。ViPlanは、2つの領域での進歩的な課題群を特徴として、古典的なBlocksworldプランニング問題のビジョン版とシミュレーションされた家庭用ロボット環境を含む。我々は、9つのオープンソースVLMフamilesの複数のサイズを検討し、選択された閉じたモデルとともに評価し、VLMに基づく符号的なプランニングとVLMを直接用いた行動提案を評価する。Blocksworldでは、正確な画像基底が重要であるため、符号的なプランニングが直接のVLMプランニングよりも優れている。一方で、家庭用ロボットテキストでは、常識知識と誤り復元能力が有利であるため、逆のことがそうである。最後に、Chain-of-Thoughtプロンプティングの使用による顕著な利益は、現在のVLMが視覚的な推理に難しいことを示している。",
      "upvotes": 8,
      "discussionId": "682c389bc19ea9cd7d822b92",
      "githubRepo": "https://github.com/merlerm/ViPlan",
      "ai_keywords": [
        "symbolic planners",
        "Vision-Language Models (VLMs)",
        "visual domains",
        "Visual Planning",
        "symbolic predicates",
        "ViPlan",
        "Benchmark",
        "Blocksworld planning problem",
        "simulated household robotics environment",
        "Chain-of-Thought prompting",
        "visual reasoning"
      ]
    },
    "publishedAt": "2025-05-19T10:38:15.000Z",
    "title": "ViPlan: A Benchmark for Visual Planning with Symbolic Predicates and\n  Vision-Language Models",
    "summary": "Integrating Large Language Models with symbolic planners is a promising\ndirection for obtaining verifiable and grounded plans compared to planning in\nnatural language, with recent works extending this idea to visual domains using\nVision-Language Models (VLMs). However, rigorous comparison between\nVLM-grounded symbolic approaches and methods that plan directly with a VLM has\nbeen hindered by a lack of common environments, evaluation protocols and model\ncoverage. We introduce ViPlan, the first open-source benchmark for Visual\nPlanning with symbolic predicates and VLMs. ViPlan features a series of\nincreasingly challenging tasks in two domains: a visual variant of the classic\nBlocksworld planning problem and a simulated household robotics environment. We\nbenchmark nine open-source VLM families across multiple sizes, along with\nselected closed models, evaluating both VLM-grounded symbolic planning and\nusing the models directly to propose actions. We find symbolic planning to\noutperform direct VLM planning in Blocksworld, where accurate image grounding\nis crucial, whereas the opposite is true in the household robotics tasks, where\ncommonsense knowledge and the ability to recover from errors are beneficial.\nFinally, we show that across most models and methods, there is no significant\nbenefit to using Chain-of-Thought prompting, suggesting that current VLMs still\nstruggle with visual reasoning.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/644555c72d91b15b4c7ebd1c/78QkuWLqE7ymFCANRaoMM.png",
      "https://cdn-uploads.huggingface.co/production/uploads/644555c72d91b15b4c7ebd1c/oaFbpbvdWQvVFhQbSnXcF.png",
      "https://cdn-uploads.huggingface.co/production/uploads/644555c72d91b15b4c7ebd1c/2y26ftHdYf0mP6NhVSc-b.png",
      "https://cdn-uploads.huggingface.co/production/uploads/644555c72d91b15b4c7ebd1c/z1TeQqgR8lGqfgfzQJF3L.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.13180.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "644555c72d91b15b4c7ebd1c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644555c72d91b15b4c7ebd1c/28zmmIkLHUUiQXQ3RQlPM.jpeg",
      "fullname": "Matteo Merler",
      "name": "merlerm",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.12849",
      "authors": [
        {
          "_id": "682bedba4be8e1707067bdb2",
          "user": {
            "_id": "682459b20ee49a8c3822a525",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/FCAtfQ40wZU3zai3DoAyq.png",
            "isPro": false,
            "fullname": "Ben",
            "user": "encoreus",
            "type": "user"
          },
          "name": "Ben Liu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-20T02:49:33.265Z",
          "hidden": false
        },
        {
          "_id": "682bedba4be8e1707067bdb3",
          "user": {
            "_id": "649014b91d71e55664838d2d",
            "avatarUrl": "/avatars/f0e0f2830c5cb7428cbbc9634d95c34b.svg",
            "isPro": false,
            "fullname": "Zhen Qin",
            "user": "zhenqincn",
            "type": "user"
          },
          "name": "Zhen Qin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-20T08:40:41.443Z",
          "hidden": true
        }
      ],
      "publishedAt": "2025-05-19T08:35:44.000Z",
      "submittedOnDailyAt": "2025-05-20T01:50:07.916Z",
      "title": "GS-Jacobi Iterationを用いてTarFlow Samplingを加速する",
      "submittedOnDailyBy": {
        "_id": "642e63a53c2cf43f6d6dc5ce",
        "avatarUrl": "/avatars/dfd78c8d55485c22be6e616670a633e5.svg",
        "isPro": false,
        "fullname": "zhenqin",
        "user": "Doreamonzzz",
        "type": "user"
      },
      "summary": "画像生成モデルは広範囲で応用されています。例えば、TarFlowモデルはTransformerアーキテクチャとNormalizing Flowモデルを組み合わせて、複数のベンチマークで最先端の結果を収めています。しかし、注意の因果形式が順番的な計算を必要とするため、TarFlowのサンプリングプロセスは非常に遅いです。本論文では、Gauss-Seidel-Jacobi（GS-Jacobi）イテレーション法を用いてサンプリングを大幅に加速することを示します。特に、TarFlowモデルのブロックには異なる重要性があります：少数のブロックが画像生成タスクにおいて主な役割を果たし、その他のブロックは相対的に少しだけ貢献します。また、一部のブロックは初期値に敏感で数値のオーバーフローに容易になり、その他のブロックは相対的に強固です。これらの2つの特徴に基づいて、Convergence Ranking Metric（CRM）とInitial Guessing Metric（IGM）を提案します：CRMはTarFlowブロックが「簡単」（少ないイテレーションで収束）か「難しい」（多くのイテレーションを必要）かを識別するために使用され、IGMはイテレーションの初期値が良いかを評価するために使用されます。4つのTarFlowモデルに対する実験は、GS-Jacobiサンプリングは画像の生成質量（FIDで測定）を維持する同時にサンプリング効率を大幅に向上させ、Img128condで4.53倍、AFHQで5.32倍、Img64uncondで2.96倍、Img64condで2.51倍のスピードアップを実現しました。コードとチェックポイントは以下のURLからアクセス可能です。https://github.com/encoreus/GS-Jacobi_for_TarFlow",
      "upvotes": 7,
      "discussionId": "682bedbd4be8e1707067be54",
      "githubRepo": "https://github.com/encoreus/GS-Jacobi_for_TarFlow",
      "ai_keywords": [
        "TarFlow model",
        "transformer architecture",
        "Normalizing Flow models",
        "causal form of attention",
        "Gauss-Seidel-Jacobi (GS-Jacobi) iteration method",
        "Convergence Ranking Metric (CRM)",
        "Initial Guessing Metric (IGM)",
        "FID"
      ]
    },
    "publishedAt": "2025-05-19T04:35:44.000Z",
    "title": "Accelerate TarFlow Sampling with GS-Jacobi Iteration",
    "summary": "Image generation models have achieved widespread applications. As an\ninstance, the TarFlow model combines the transformer architecture with\nNormalizing Flow models, achieving state-of-the-art results on multiple\nbenchmarks. However, due to the causal form of attention requiring sequential\ncomputation, TarFlow's sampling process is extremely slow. In this paper, we\ndemonstrate that through a series of optimization strategies, TarFlow sampling\ncan be greatly accelerated by using the Gauss-Seidel-Jacobi (abbreviated as\nGS-Jacobi) iteration method. Specifically, we find that blocks in the TarFlow\nmodel have varying importance: a small number of blocks play a major role in\nimage generation tasks, while other blocks contribute relatively little; some\nblocks are sensitive to initial values and prone to numerical overflow, while\nothers are relatively robust. Based on these two characteristics, we propose\nthe Convergence Ranking Metric (CRM) and the Initial Guessing Metric (IGM): CRM\nis used to identify whether a TarFlow block is \"simple\" (converges in few\niterations) or \"tough\" (requires more iterations); IGM is used to evaluate\nwhether the initial value of the iteration is good. Experiments on four TarFlow\nmodels demonstrate that GS-Jacobi sampling can significantly enhance sampling\nefficiency while maintaining the quality of generated images (measured by FID),\nachieving speed-ups of 4.53x in Img128cond, 5.32x in AFHQ, 2.96x in\nImg64uncond, and 2.51x in Img64cond without degrading FID scores or sample\nquality. Code and checkpoints are accessible on\nhttps://github.com/encoreus/GS-Jacobi_for_TarFlow",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.12849.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642e63a53c2cf43f6d6dc5ce",
      "avatarUrl": "/avatars/dfd78c8d55485c22be6e616670a633e5.svg",
      "fullname": "zhenqin",
      "name": "Doreamonzzz",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.11855",
      "authors": [
        {
          "_id": "682c11fe08d047591841ebf1",
          "user": {
            "_id": "60d3e619b8448e1785bbda2a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60d3e619b8448e1785bbda2a/q2re5u1HNwsCCyIMtid_I.jpeg",
            "isPro": false,
            "fullname": "GUIJIN SON",
            "user": "amphora",
            "type": "user"
          },
          "name": "Guijin Son",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-20T08:40:55.774Z",
          "hidden": false
        },
        {
          "_id": "682c11fe08d047591841ebf2",
          "user": {
            "_id": "6415c043486c7c9a5d151583",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6415c043486c7c9a5d151583/fUdYFh6iVh57swCkBEy-y.jpeg",
            "isPro": false,
            "fullname": "Jiwoo Hong",
            "user": "JW17",
            "type": "user"
          },
          "name": "Jiwoo Hong",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-20T08:41:14.541Z",
          "hidden": false
        },
        {
          "_id": "682c11fe08d047591841ebf3",
          "name": "Honglu Fan",
          "hidden": false
        },
        {
          "_id": "682c11fe08d047591841ebf4",
          "user": {
            "_id": "659f9445d5c4ea912705aa4d",
            "avatarUrl": "/avatars/1d3297c3ccad48e5eb6c01e0640dc06d.svg",
            "isPro": false,
            "fullname": "Heejeong Nam",
            "user": "HazelNam",
            "type": "user"
          },
          "name": "Heejeong Nam",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-20T08:41:30.411Z",
          "hidden": false
        },
        {
          "_id": "682c11fe08d047591841ebf5",
          "user": {
            "_id": "63e087b6a98d931aa90c1b9c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e087b6a98d931aa90c1b9c/96c6IT3f1pWGLbRdRDB2U.png",
            "isPro": false,
            "fullname": "Hyunwoo Ko",
            "user": "Cartinoe5930",
            "type": "user"
          },
          "name": "Hyunwoo Ko",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-20T07:20:13.177Z",
          "hidden": false
        },
        {
          "_id": "682c11fe08d047591841ebf6",
          "user": {
            "_id": "63be1cd13b0665ad51d29c37",
            "avatarUrl": "/avatars/5acc9b9bbecac3d567e927e2d8667b00.svg",
            "isPro": false,
            "fullname": "Seungwon Lim",
            "user": "sngwon",
            "type": "user"
          },
          "name": "Seungwon Lim",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-20T08:41:49.745Z",
          "hidden": false
        },
        {
          "_id": "682c11fe08d047591841ebf7",
          "name": "Jinyeop Song",
          "hidden": false
        },
        {
          "_id": "682c11fe08d047591841ebf8",
          "name": "Jinha Choi",
          "hidden": false
        },
        {
          "_id": "682c11fe08d047591841ebf9",
          "name": "Gonçalo Paulo",
          "hidden": false
        },
        {
          "_id": "682c11fe08d047591841ebfa",
          "name": "Youngjae Yu",
          "hidden": false
        },
        {
          "_id": "682c11fe08d047591841ebfb",
          "name": "Stella Biderman",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-17T05:45:16.000Z",
      "submittedOnDailyAt": "2025-05-20T04:18:15.709Z",
      "title": "AIコーラボレーティブサイエンティストたちが失敗するとき：SPOT - 科学研究の自動化確認のベンチマーク",
      "submittedOnDailyBy": {
        "_id": "60d3e619b8448e1785bbda2a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60d3e619b8448e1785bbda2a/q2re5u1HNwsCCyIMtid_I.jpeg",
        "isPro": false,
        "fullname": "GUIJIN SON",
        "user": "amphora",
        "type": "user"
      },
      "summary": "最近の大規模言語モデル（LLMs）の進展は、自動化科学発見の観念を燃やし、これを「AI コサイエンティスト」と呼ばれています。これまでの研究では、これらのシステムは、仮説の作成、コードの合成または論文の草稿を担当する生成的な共同著者として扱われていました。本稿では、補助的なアプリケーションを検討します：LLMs を証明者として、科学論文の学術的な証明を自動化することを試みます。そのために、SPOT データセットを紹介します。SPOT は、83 篇の出版論文と 91 件の関係のある誤りのペアを含み、実際の著者と人間のアノテーターとの交差検証を行います。SPOT 上で最先端の LLMs を評価した結果、どれも 21.1% の再現率または 6.1% の精度を超えないことがわかりました（o3 が最も良いスコアを達成し、それ以外はゼロ近くです）。また、信頼度の推定は一律的に低く、8 回の独立モデル実験では同じ誤りを再現することが稀で、その信頼性を軽減します。最後に、領域の専門家との質的な分析では、最も強いモデルも、学生レベルの誤解を基にしたような誤読をしていることが明らかになりました。これらの発見は、現在の LLMs の能力と、信頼性のある AI アシスタントの学術的な証明に必要な要求の間の極めて大きな間違いを明らかにしています。",
      "upvotes": 7,
      "discussionId": "682c11ff08d047591841ec50",
      "ai_keywords": [
        "large language models (LLMs)",
        "AI Co-Scientists",
        "generative co-authors",
        "academic verification",
        "SPOT",
        "published papers",
        "errata",
        "retraction",
        "cross-validated",
        "human annotators",
        "recall",
        "precision",
        "confidence estimates"
      ]
    },
    "publishedAt": "2025-05-17T01:45:16.000Z",
    "title": "When AI Co-Scientists Fail: SPOT-a Benchmark for Automated Verification\n  of Scientific Research",
    "summary": "Recent advances in large language models (LLMs) have fueled the vision of\nautomated scientific discovery, often called AI Co-Scientists. To date, prior\nwork casts these systems as generative co-authors responsible for crafting\nhypotheses, synthesizing code, or drafting manuscripts. In this work, we\nexplore a complementary application: using LLMs as verifiers to automate the\nacademic verification of scientific manuscripts. To that end, we\nintroduce SPOT, a dataset of 83 published papers paired with 91 errors\nsignificant enough to prompt errata or retraction, cross-validated with actual\nauthors and human annotators. Evaluating state-of-the-art LLMs on SPOT, we find\nthat none surpasses 21.1\\% recall or 6.1\\% precision (o3 achieves the best\nscores, with all others near zero). Furthermore, confidence estimates are\nuniformly low, and across eight independent runs, models rarely rediscover the\nsame errors, undermining their reliability. Finally, qualitative analysis with\ndomain experts reveals that even the strongest models make mistakes resembling\nstudent-level misconceptions derived from misunderstandings. These findings\nhighlight the substantial gap between current LLM capabilities and the\nrequirements for dependable AI-assisted academic verification.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.11855.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60d3e619b8448e1785bbda2a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60d3e619b8448e1785bbda2a/q2re5u1HNwsCCyIMtid_I.jpeg",
      "fullname": "GUIJIN SON",
      "name": "amphora",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 54
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.13444",
      "authors": [
        {
          "_id": "682bf33a6f59c839338ffdd0",
          "user": {
            "_id": "62c70672e7d825deaae41e5e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62c70672e7d825deaae41e5e/ICCpeBwmQ1NsgWcjG-MEZ.png",
            "isPro": true,
            "fullname": "Liyan Tang",
            "user": "lytang",
            "type": "user"
          },
          "name": "Liyan Tang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-20T07:20:38.292Z",
          "hidden": false
        },
        {
          "_id": "682bf33a6f59c839338ffdd1",
          "name": "Grace Kim",
          "hidden": false
        },
        {
          "_id": "682bf33a6f59c839338ffdd2",
          "name": "Xinyu Zhao",
          "hidden": false
        },
        {
          "_id": "682bf33a6f59c839338ffdd3",
          "user": {
            "_id": "64a87c60b76bfd863e715cab",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a87c60b76bfd863e715cab/cpAUTOTEwhgP29aw6AOWA.jpeg",
            "isPro": false,
            "fullname": "Thom Lake",
            "user": "thomlake",
            "type": "user"
          },
          "name": "Thom Lake",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-20T08:43:44.062Z",
          "hidden": false
        },
        {
          "_id": "682bf33a6f59c839338ffdd4",
          "name": "Wenxuan Ding",
          "hidden": false
        },
        {
          "_id": "682bf33a6f59c839338ffdd5",
          "user": {
            "_id": "64efa8748602335a044cd97f",
            "avatarUrl": "/avatars/0ab5df922cb0ce4abe7aed35e7b9100c.svg",
            "isPro": false,
            "fullname": "Fangcong Yin",
            "user": "fcyin",
            "type": "user"
          },
          "name": "Fangcong Yin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-20T08:43:29.872Z",
          "hidden": false
        },
        {
          "_id": "682bf33a6f59c839338ffdd6",
          "user": {
            "_id": "613cb3e1c7a43c281cd417a2",
            "avatarUrl": "/avatars/69123ba49c2aa1cd9f3cc5746f4839dc.svg",
            "isPro": false,
            "fullname": "Prasann Singhal",
            "user": "PrasannSinghal",
            "type": "user"
          },
          "name": "Prasann Singhal",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-20T08:43:24.263Z",
          "hidden": false
        },
        {
          "_id": "682bf33a6f59c839338ffdd7",
          "user": {
            "_id": "655ab2ccc11dee7f7e6db119",
            "avatarUrl": "/avatars/1c13e338cd4cc0b4eb681ed8f33abf19.svg",
            "isPro": false,
            "fullname": "Manya Wadhwa",
            "user": "wadhma",
            "type": "user"
          },
          "name": "Manya Wadhwa",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-20T08:43:18.026Z",
          "hidden": false
        },
        {
          "_id": "682bf33a6f59c839338ffdd8",
          "user": {
            "_id": "6607b0d29d2edd43f74dec98",
            "avatarUrl": "/avatars/437b5cbc555bf6906c3f07495a903ab4.svg",
            "isPro": false,
            "fullname": "Zeyu Leo Liu",
            "user": "leo-liuzy",
            "type": "user"
          },
          "name": "Zeyu Leo Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-20T08:42:57.568Z",
          "hidden": false
        },
        {
          "_id": "682bf33a6f59c839338ffdd9",
          "user": {
            "_id": "64e78a03e3953cd90bcad620",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e78a03e3953cd90bcad620/Rj_-xLJUsxdRmNhvRbssq.jpeg",
            "isPro": false,
            "fullname": "Zayne Sprague",
            "user": "Zaynes",
            "type": "user"
          },
          "name": "Zayne Sprague",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-20T08:42:49.622Z",
          "hidden": false
        },
        {
          "_id": "682bf33a6f59c839338ffdda",
          "name": "Ramya Namuduri",
          "hidden": false
        },
        {
          "_id": "682bf33a6f59c839338ffddb",
          "name": "Bodun Hu",
          "hidden": false
        },
        {
          "_id": "682bf33a6f59c839338ffddc",
          "name": "Juan Diego Rodriguez",
          "hidden": false
        },
        {
          "_id": "682bf33a6f59c839338ffddd",
          "user": {
            "_id": "6480706f5409aa3e3bbaee16",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/qi2IrGGu7rgQVB_cfPNhh.png",
            "isPro": false,
            "fullname": "Puyuan Peng",
            "user": "pyp1",
            "type": "user"
          },
          "name": "Puyuan Peng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-20T08:42:13.748Z",
          "hidden": false
        },
        {
          "_id": "682bf33a6f59c839338ffdde",
          "user": {
            "_id": "65be9918b54ab5b37d1b67a7",
            "avatarUrl": "/avatars/9953707affb6881724c8efb2abf0c668.svg",
            "isPro": false,
            "fullname": "Greg Durrett",
            "user": "gregdurrett",
            "type": "user"
          },
          "name": "Greg Durrett",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-20T08:42:07.638Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-19T17:59:27.000Z",
      "submittedOnDailyAt": "2025-05-20T02:45:41.647Z",
      "title": "ChartMuseum: 大視覚言語モデルの視覚推論能力のテスト",
      "submittedOnDailyBy": {
        "_id": "62c70672e7d825deaae41e5e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62c70672e7d825deaae41e5e/ICCpeBwmQ1NsgWcjG-MEZ.png",
        "isPro": true,
        "fullname": "Liyan Tang",
        "user": "lytang",
        "type": "user"
      },
      "summary": "チャート理解は、大規模な視覚言語モデル（LVLMs）にとって特別な課題であり、複雑な文字的および視覚的理由能力の統合が必要です。しかし、現在のLVLMsは、視覚的理由において文脈では困難なことを行うことができないことによって、これらのスキルの間に明らかな不平衡があります。私たちは、視覚的理由にだけより解決可能な合成データセットを用いて場合の研究を行い、モデルの性能が視覚的複雑性が増加するにつれて显著に低下し、その対照的に人間の性能は強固であることを示しました。次に、ChartMuseumという新しいチャート問答（QA）ベンチマークを紹介します。これは1,162件の専門家注釈された問題を含み、184つのリソースからの実世界的なチャートから選ばれ、複雑な視覚的および文字的理由を評価するために特に作られました。先行のチャート理解ベンチマークと異なり、これらのモデルと人間の性能の間に大きな間違いがあり、モデルの能力を効果的に区別できます：人間は93%の精度を達成しますが、最も高い性能を示すジェミニー-2.5-Proは63.0%を達成し、リードプロジェクトLVLM Qwen2.5-VL-72B-Instructは38.5%を達成します。また、主に視覚的理由が必要な問題においては、全モデルが文脈的理由が重視された問題の性能から35%-55%の低下を認めます。最後に、定性的誤り分析により、現在のLVLMsにとって難しい具体的な視覚的理由のカテゴリーが明らかになりました。",
      "upvotes": 4,
      "discussionId": "682bf33e6f59c839338ffee5",
      "projectPage": "https://chartmuseum-leaderboard.github.io",
      "githubRepo": "https://github.com/Liyan06/ChartMuseum",
      "ai_keywords": [
        "Chart Question Answering (QA)",
        "ChartMuseum",
        "LVLMs (large vision-language models)",
        "synthetic dataset",
        "visual reasoning",
        "textual reasoning",
        "expert-annotated questions",
        "real-world charts",
        "Gemini-2.5-Pro",
        "Qwen2.5-VL-72B-Instruct"
      ]
    },
    "publishedAt": "2025-05-19T13:59:27.000Z",
    "title": "ChartMuseum: Testing Visual Reasoning Capabilities of Large\n  Vision-Language Models",
    "summary": "Chart understanding presents a unique challenge for large vision-language\nmodels (LVLMs), as it requires the integration of sophisticated textual and\nvisual reasoning capabilities. However, current LVLMs exhibit a notable\nimbalance between these skills, falling short on visual reasoning that is\ndifficult to perform in text. We conduct a case study using a synthetic dataset\nsolvable only through visual reasoning and show that model performance degrades\nsignificantly with increasing visual complexity, while human performance\nremains robust. We then introduce ChartMuseum, a new Chart Question Answering\n(QA) benchmark containing 1,162 expert-annotated questions spanning multiple\nreasoning types, curated from real-world charts across 184 sources,\nspecifically built to evaluate complex visual and textual reasoning. Unlike\nprior chart understanding benchmarks -- where frontier models perform similarly\nand near saturation -- our benchmark exposes a substantial gap between model\nand human performance, while effectively differentiating model capabilities:\nalthough humans achieve 93% accuracy, the best-performing model Gemini-2.5-Pro\nattains only 63.0%, and the leading open-source LVLM Qwen2.5-VL-72B-Instruct\nachieves only 38.5%. Moreover, on questions requiring primarily visual\nreasoning, all models experience a 35%-55% performance drop from\ntext-reasoning-heavy question performance. Lastly, our qualitative error\nanalysis reveals specific categories of visual reasoning that are challenging\nfor current LVLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.13444.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62c70672e7d825deaae41e5e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62c70672e7d825deaae41e5e/ICCpeBwmQ1NsgWcjG-MEZ.png",
      "fullname": "Liyan Tang",
      "name": "lytang",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.10238",
      "authors": [
        {
          "_id": "682bfefa73f0db9ddd6c73f7",
          "user": {
            "_id": "65c09224a9c1b20e69a61569",
            "avatarUrl": "/avatars/78c73be711f2c7a889acb088507ca0aa.svg",
            "isPro": false,
            "fullname": "YANBO DING",
            "user": "yanboding",
            "type": "user"
          },
          "name": "Yanbo Ding",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-20T07:20:19.748Z",
          "hidden": false
        },
        {
          "_id": "682bfefa73f0db9ddd6c73f8",
          "name": "Xirui Hu",
          "hidden": false
        },
        {
          "_id": "682bfefa73f0db9ddd6c73f9",
          "name": "Zhizhi Guo",
          "hidden": false
        },
        {
          "_id": "682bfefa73f0db9ddd6c73fa",
          "name": "Yali Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-15T12:50:29.000Z",
      "submittedOnDailyAt": "2025-05-20T05:58:35.243Z",
      "title": "MTVCrafter: 4D モーショントークン化を用いた開放ウールド中の人間画像アニメーション",
      "submittedOnDailyBy": {
        "_id": "65c09224a9c1b20e69a61569",
        "avatarUrl": "/avatars/78c73be711f2c7a889acb088507ca0aa.svg",
        "isPro": false,
        "fullname": "YANBO DING",
        "user": "yanboding",
        "type": "user"
      },
      "summary": "人間画像アニメーションは、デジタル人間において広く応用され、急速に発展しています。しかし、現在の方法は主に2D渲染された姿勢画像を基に運動のガイドに依存しており、一般化能力を制限し、開放ワールドのアニメーションに必要な3D情報を失います。この問題に対処するために、私たちは、最初のフレームワークであるMTVCrafter（Motion Tokenization Video Crafter）を提案します。これは、人間画像アニメーションのために、裸の3D運動シーケンス（つまり、4D運動）を直接モデル化するものです。特に、4DMoT（4D運動トークナイザー）を導入し、3D運動シーケンスを4D運動トークンに圧縮します。2D渲染された姿勢画像と比較して、4D運動トークンはより強固な空間時間的コマンドを提供し、姿勢画像とキャラクターのピクセルレベルの厳密なアライメントを避け、より柔軟かつ独立した制御を可能にします。そして、MV-DiT（Motion-aware Video DiT）を導入します。4D位置付けエンコーディングを用いた特別な運動アタションを設計し、MV-DiTは複雑な3Dワールドでの人間画像アニメーションにおいて、運動トークンを4Dの総括的で表現的なコンテキストとして効果的に利用できます。これは、この分野における重要なステップ進み、姿勢ガイドされた人間ビデオ生成の新しい方向を開拓します。実験は、私たちのMTVCrafterはFID-VID 6.98で最先端の結果を収め、2番目に良いものを65%超えています。強固な運動トークンをポートにしたMTVCrafterは、多様な開放ワールドのキャラクター（単一/複数、全身/半身）を構成し、多様なスタイルとスケーナrioに対してより広範囲に一般化できます。私たちのビデオデモとコードは以下のURLで公開されています：https://github.com/DINGYANB/MTVCrafter。",
      "upvotes": 4,
      "discussionId": "682bfefd73f0db9ddd6c747f",
      "ai_keywords": [
        "MTVCrafter",
        "4DMoT",
        "4D motion tokenizer",
        "4D motion tokens",
        "4D positional encodings",
        "MV-DiT",
        "Motion-aware Video DiT",
        "motion attention",
        "FID-VID",
        "pose-guided human video generation"
      ]
    },
    "publishedAt": "2025-05-15T08:50:29.000Z",
    "title": "MTVCrafter: 4D Motion Tokenization for Open-World Human Image Animation",
    "summary": "Human image animation has gained increasing attention and developed rapidly\ndue to its broad applications in digital humans. However, existing methods rely\nlargely on 2D-rendered pose images for motion guidance, which limits\ngeneralization and discards essential 3D information for open-world animation.\nTo tackle this problem, we propose MTVCrafter (Motion Tokenization Video\nCrafter), the first framework that directly models raw 3D motion sequences\n(i.e., 4D motion) for human image animation. Specifically, we introduce 4DMoT\n(4D motion tokenizer) to quantize 3D motion sequences into 4D motion tokens.\nCompared to 2D-rendered pose images, 4D motion tokens offer more robust\nspatio-temporal cues and avoid strict pixel-level alignment between pose image\nand character, enabling more flexible and disentangled control. Then, we\nintroduce MV-DiT (Motion-aware Video DiT). By designing unique motion attention\nwith 4D positional encodings, MV-DiT can effectively leverage motion tokens as\n4D compact yet expressive context for human image animation in the complex 3D\nworld. Hence, it marks a significant step forward in this field and opens a new\ndirection for pose-guided human video generation. Experiments show that our\nMTVCrafter achieves state-of-the-art results with an FID-VID of 6.98,\nsurpassing the second-best by 65%. Powered by robust motion tokens, MTVCrafter\nalso generalizes well to diverse open-world characters (single/multiple,\nfull/half-body) across various styles and scenarios. Our video demos and code\nare on: https://github.com/DINGYANB/MTVCrafter.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.10238.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65c09224a9c1b20e69a61569",
      "avatarUrl": "/avatars/78c73be711f2c7a889acb088507ca0aa.svg",
      "fullname": "YANBO DING",
      "name": "yanboding",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.13437",
      "authors": [
        {
          "_id": "682bfc257f2ade8dcbef284d",
          "name": "Dian Shao",
          "hidden": false
        },
        {
          "_id": "682bfc257f2ade8dcbef284e",
          "name": "Mingfei Shi",
          "hidden": false
        },
        {
          "_id": "682bfc257f2ade8dcbef284f",
          "name": "Shengda Xu",
          "hidden": false
        },
        {
          "_id": "682bfc257f2ade8dcbef2850",
          "user": {
            "_id": "6570450a78d7aca0c361a177",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6570450a78d7aca0c361a177/z0GrnXEsjK2_G-hFfQhKv.jpeg",
            "isPro": false,
            "fullname": "Harold Chen",
            "user": "Harold328",
            "type": "user"
          },
          "name": "Haodong Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-20T07:20:24.383Z",
          "hidden": false
        },
        {
          "_id": "682bfc257f2ade8dcbef2851",
          "user": {
            "_id": "673e1ae7c90f9c7fbe4298d7",
            "avatarUrl": "/avatars/a6f0e64af7c502beb4c1d91ff4c4ea56.svg",
            "isPro": false,
            "fullname": "Yongle Huang",
            "user": "Jason-Huang824",
            "type": "user"
          },
          "name": "Yongle Huang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-20T08:44:22.708Z",
          "hidden": false
        },
        {
          "_id": "682bfc257f2ade8dcbef2852",
          "name": "Binglu Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-19T17:58:11.000Z",
      "submittedOnDailyAt": "2025-05-20T02:21:36.948Z",
      "title": "FinePhys: 物理法則を明記して骨格ガイドを効果的に行う微分化の人間アクション生成",
      "submittedOnDailyBy": {
        "_id": "6570450a78d7aca0c361a177",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6570450a78d7aca0c361a177/z0GrnXEsjK2_G-hFfQhKv.jpeg",
        "isPro": false,
        "fullname": "Harold Chen",
        "user": "Harold328",
        "type": "user"
      },
      "summary": "物理的に可能な人間の動作の合成は、現在の技術では長期的な課題であり、特により細かいセマンティクスと複雑な時系列的な動作のモデリングにおいて尤も難しい。例えば、「0.5回転のスイッチラップ」のような体操プログラムの生成は、現在の方法にとっては大きな難関で、満足しない結果を多く出すことがある。この隙を埋めるために、私たちはFinePhysというFine-grained人間の動作生成フレームワークを提案し、物理的な骨格のガイドを得ることを目的としています。特に、FinePhysは最初にオンラインで2Dポーズを推定し、その後、in-context learningを用いて2Dから3Dの次元転換を行います。また、3Dポーズのデータ駆動の不穩定さと解釈性の限界を軽減するために、Euler-Lagrange方程式による物理的な動作再推定モジュールを追加し、双方向的な時系列更新を用いて関節加速度を計算します。物理的に予測された3Dポーズは、データ駆動のものと融合され、拡散プロセスによる多スケール2Dヒートマップガイドを提供します。FineGymからのFX-JUMP、FX-TURN、FX-SALTOの3つのFine-grainedアクションの部分に対して評価した結果、FinePhysは相競合ベースラインに比べて显著に優れています。詳細な質的な結果は、FinePhysが自然で物理的に可能なFine-grained人間の動作を生成する能力を示しています。",
      "upvotes": 3,
      "discussionId": "682bfc277f2ade8dcbef28bc",
      "projectPage": "https://smartdianlab.github.io/projects-FinePhys/",
      "githubRepo": "https://github.com/SmartDianLab/FinePhys",
      "ai_keywords": [
        "FinePhys",
        "Fine-grained human action generation framework",
        "Euler-Lagrange equations",
        "bidirectional temporal updating",
        "diffusion process",
        "2D poses",
        "3D poses",
        "2D-to-3D dimension lifting",
        "in-context learning",
        "multi-scale 2D heatmap guidance"
      ]
    },
    "publishedAt": "2025-05-19T13:58:11.000Z",
    "title": "FinePhys: Fine-grained Human Action Generation by Explicitly\n  Incorporating Physical Laws for Effective Skeletal Guidance",
    "summary": "Despite significant advances in video generation, synthesizing physically\nplausible human actions remains a persistent challenge, particularly in\nmodeling fine-grained semantics and complex temporal dynamics. For instance,\ngenerating gymnastics routines such as \"switch leap with 0.5 turn\" poses\nsubstantial difficulties for current methods, often yielding unsatisfactory\nresults. To bridge this gap, we propose FinePhys, a Fine-grained human action\ngeneration framework that incorporates Physics to obtain effective skeletal\nguidance. Specifically, FinePhys first estimates 2D poses in an online manner\nand then performs 2D-to-3D dimension lifting via in-context learning. To\nmitigate the instability and limited interpretability of purely data-driven 3D\nposes, we further introduce a physics-based motion re-estimation module\ngoverned by Euler-Lagrange equations, calculating joint accelerations via\nbidirectional temporal updating. The physically predicted 3D poses are then\nfused with data-driven ones, offering multi-scale 2D heatmap guidance for the\ndiffusion process. Evaluated on three fine-grained action subsets from FineGym\n(FX-JUMP, FX-TURN, and FX-SALTO), FinePhys significantly outperforms\ncompetitive baselines. Comprehensive qualitative results further demonstrate\nFinePhys's ability to generate more natural and plausible fine-grained human\nactions.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.13437.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6570450a78d7aca0c361a177",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6570450a78d7aca0c361a177/z0GrnXEsjK2_G-hFfQhKv.jpeg",
      "fullname": "Harold Chen",
      "name": "Harold328",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.12996",
      "authors": [
        {
          "_id": "682c1f8b47e6c8a0c0fd5b5f",
          "user": {
            "_id": "6051e3f145db307eddc0c962",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676443438507-6051e3f145db307eddc0c962.jpeg",
            "isPro": false,
            "fullname": "Jiaan Wang",
            "user": "Krystalan",
            "type": "user"
          },
          "name": "Jiaan Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-20T08:46:01.647Z",
          "hidden": false
        },
        {
          "_id": "682c1f8b47e6c8a0c0fd5b60",
          "user": {
            "_id": "64cb254871a7bbb60c17d5fa",
            "avatarUrl": "/avatars/5121fd5b7b55d275eba3947f3f4c034d.svg",
            "isPro": false,
            "fullname": "Fandong Meng",
            "user": "fandong",
            "type": "user"
          },
          "name": "Fandong Meng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-20T08:51:05.844Z",
          "hidden": false
        },
        {
          "_id": "682c1f8b47e6c8a0c0fd5b61",
          "name": "Jie Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-19T11:34:47.000Z",
      "submittedOnDailyAt": "2025-05-20T04:53:13.355Z",
      "title": "ExTrans: マルチラング語深層理由訳譯を例示強化学習によって実現する",
      "submittedOnDailyBy": {
        "_id": "6051e3f145db307eddc0c962",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676443438507-6051e3f145db307eddc0c962.jpeg",
        "isPro": false,
        "fullname": "Jiaan Wang",
        "user": "Krystalan",
        "type": "user"
      },
      "summary": "近年、OpenAI-o1やDeepSeek-R1などの大規模な理由論モデル（LRMs）の出現は、複雑な問題における卓越した能力を示していることがわかった。例えば、数学やコーディングなど。先進的研究では、LRMsが神経機械翻訳（MT）における成功を収めることを試みている。それらは、強化学習（RL）を通じて深い理由論的なMT能力を持つLRMsを構築することを試みている。しかし、進歩はあるが、これらの試みは主に英語と中国語などの豊富な資源言語に焦点を当て、他の言語の性能は明確ではない。また、先行研究で用いられた報酬モデリング方法は、MTの強化学習の潛力を完全に発揮させることはできていない。本研究では、最初に、政策MTモデルの翻訳結果と強力なLRM（即、DeepSeek-R1-671B）との比較を行い、比較を定量化して報酬を提供する新しい報酬モデリング方法を設計する。実験結果は、報酬モデリング方法の優れた性能を示している。Qwen2.5-7B-Instructをベースとして学習されたモデルは、文学翻訳の新しい最先端性能を達成し、OpenAI-o1やDeepSeeK-R1などの強力なLRMsを上回ることができている。また、11言語の多言語設定においても、厳密に設計された軽量級報酬モデリングを用いて、強力なMT能力を1方向から複数方向（即、90方向）に簡単に転移し、評価の高い多言語MT性能を達成することができる。",
      "upvotes": 3,
      "discussionId": "682c1f8b47e6c8a0c0fd5b82",
      "githubRepo": "https://github.com/krystalan/DRT",
      "ai_keywords": [
        "large reasoning models (LRMs)",
        "reinforcement learning (RL)",
        "neural machine translation (MT)",
        "policy MT model",
        "reward modeling",
        "Qwen2.5-7B-Instruct",
        "strong MT ability",
        "multilingual settings",
        "multilingual MT performance"
      ]
    },
    "publishedAt": "2025-05-19T07:34:47.000Z",
    "title": "ExTrans: Multilingual Deep Reasoning Translation via Exemplar-Enhanced\n  Reinforcement Learning",
    "summary": "In recent years, the emergence of large reasoning models (LRMs), such as\nOpenAI-o1 and DeepSeek-R1, has shown impressive capabilities in complex\nproblems, e.g., mathematics and coding. Some pioneering studies attempt to\nbring the success of LRMs in neural machine translation (MT). They try to build\nLRMs with deep reasoning MT ability via reinforcement learning (RL). Despite\nsome progress that has been made, these attempts generally focus on several\nhigh-resource languages, e.g., English and Chinese, leaving the performance on\nother languages unclear. Besides, the reward modeling methods in previous work\ndo not fully unleash the potential of reinforcement learning in MT. In this\nwork, we first design a new reward modeling method that compares the\ntranslation results of the policy MT model with a strong LRM (i.e.,\nDeepSeek-R1-671B), and quantifies the comparisons to provide rewards.\nExperimental results demonstrate the superiority of the reward modeling method.\nUsing Qwen2.5-7B-Instruct as the backbone, the trained model achieves the new\nstate-of-the-art performance in literary translation, and outperforms strong\nLRMs including OpenAI-o1 and DeepSeeK-R1. Furthermore, we extend our method to\nthe multilingual settings with 11 languages. With a carefully designed\nlightweight reward modeling in RL, we can simply transfer the strong MT ability\nfrom a single direction into multiple (i.e., 90) translation directions and\nachieve impressive multilingual MT performance.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.12996.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6051e3f145db307eddc0c962",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676443438507-6051e3f145db307eddc0c962.jpeg",
      "fullname": "Jiaan Wang",
      "name": "Krystalan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.11484",
      "authors": [
        {
          "_id": "682b7826e9f4a26b02e74091",
          "user": {
            "_id": "6448d7e5e87a77e872e47982",
            "avatarUrl": "/avatars/7405ceef3bf7468cb3e977c4669d81a4.svg",
            "isPro": false,
            "fullname": "Yige Xu",
            "user": "xuyige",
            "type": "user"
          },
          "name": "Yige Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-20T07:22:07.334Z",
          "hidden": false
        },
        {
          "_id": "682b7826e9f4a26b02e74092",
          "name": "Xu Guo",
          "hidden": false
        },
        {
          "_id": "682b7826e9f4a26b02e74093",
          "user": {
            "_id": "664b5d83edcadf9fa5e0615d",
            "avatarUrl": "/avatars/5fdfc87a78b68f1eb54e1ed7d144952a.svg",
            "isPro": false,
            "fullname": "zeng zhiwei",
            "user": "Aver3",
            "type": "user"
          },
          "name": "Zhiwei Zeng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-20T08:52:04.331Z",
          "hidden": false
        },
        {
          "_id": "682b7826e9f4a26b02e74094",
          "name": "Chunyan Miao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-16T17:47:50.000Z",
      "submittedOnDailyAt": "2025-05-20T05:56:06.427Z",
      "title": "SoftCoT++: テストタイムスケーリングと軟体の連鎖的思考",
      "submittedOnDailyBy": {
        "_id": "6448d7e5e87a77e872e47982",
        "avatarUrl": "/avatars/7405ceef3bf7468cb3e977c4669d81a4.svg",
        "isPro": false,
        "fullname": "Yige Xu",
        "user": "xuyige",
        "type": "user"
      },
      "summary": "テスト時スケーリング（TTS）は、推論時に追加的な計算を割り当てることでモデルのパラメータを変更しないことで理由論の性能を向上させるアプローチです。現在のTTS方法は、離散トークン空間で操作し、多くの中間ステップを生成しますが、CoconutとSoftCoTの最近の研究では、連続的な潜在空間で考えることが理由論の性能を進めることができることを示しています。このような潜在的な考え方は、自動後退生成に伴う情報損失を伴っていないことから情報的な考え方を記録し、連続空間の理由論に興味を引き起こしています。離散的な解碼と違い、異なる理由論のパスを探索するための繰り返しサンプリングは、特定の入力に対して固定された潜在的な表現であることにより、多様な探索が制限されます。この制限を克服するために、SoftCoT++を導入し、SoftCoTをテスト時スケーリングパラダイムに拡張し、考えのパスの多様な探索を可能にします。特に、複数の特別な初期トークンを用いて潜在的な考え方を摂動し、ソフトな考えの表現の多様性を促進するための対比的学習を適用します。5つの理由論ベンチマークと2つの異なるLLMアーキテクチャの実験は、SoftCoT++がSoftCoTを大幅に向上させ、自統一スケーリングを含むSoftCoTよりも優れていることを示し、また自統一スケーリングとよく合致しています。ソースコードは、https://github.com/xuyige/SoftCoTから利用可能です。",
      "upvotes": 3,
      "discussionId": "682b7827e9f4a26b02e740ee",
      "ai_keywords": [
        "Test-Time Scaling (TTS)",
        "continuous latent space",
        "autoregressive token generation",
        "discrete decoding",
        "SoftCoT++",
        "contrastive learning",
        "reasoning benchmarks",
        "LLM architectures",
        "self-consistency scaling"
      ]
    },
    "publishedAt": "2025-05-16T13:47:50.000Z",
    "title": "SoftCoT++: Test-Time Scaling with Soft Chain-of-Thought Reasoning",
    "summary": "Test-Time Scaling (TTS) refers to approaches that improve reasoning\nperformance by allocating extra computation during inference, without altering\nthe model's parameters. While existing TTS methods operate in a discrete token\nspace by generating more intermediate steps, recent studies in Coconut and\nSoftCoT have demonstrated that thinking in the continuous latent space can\nfurther enhance the reasoning performance. Such latent thoughts encode\ninformative thinking without the information loss associated with\nautoregressive token generation, sparking increased interest in\ncontinuous-space reasoning. Unlike discrete decoding, where repeated sampling\nenables exploring diverse reasoning paths, latent representations in continuous\nspace are fixed for a given input, which limits diverse exploration, as all\ndecoded paths originate from the same latent thought. To overcome this\nlimitation, we introduce SoftCoT++ to extend SoftCoT to the Test-Time Scaling\nparadigm by enabling diverse exploration of thinking paths. Specifically, we\nperturb latent thoughts via multiple specialized initial tokens and apply\ncontrastive learning to promote diversity among soft thought representations.\nExperiments across five reasoning benchmarks and two distinct LLM architectures\ndemonstrate that SoftCoT++ significantly boosts SoftCoT and also outperforms\nSoftCoT with self-consistency scaling. Moreover, it shows strong compatibility\nwith conventional scaling techniques such as self-consistency. Source code is\navailable at https://github.com/xuyige/SoftCoT.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.11484.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6448d7e5e87a77e872e47982",
      "avatarUrl": "/avatars/7405ceef3bf7468cb3e977c4669d81a4.svg",
      "fullname": "Yige Xu",
      "name": "xuyige",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.12872",
      "authors": [
        {
          "_id": "682c51889f83963d2d41998c",
          "name": "Maytus Piriyajitakonkij",
          "hidden": false
        },
        {
          "_id": "682c51889f83963d2d41998d",
          "name": "Rujikorn Charakorn",
          "hidden": false
        },
        {
          "_id": "682c51889f83963d2d41998e",
          "name": "Weicheng Tao",
          "hidden": false
        },
        {
          "_id": "682c51889f83963d2d41998f",
          "name": "Wei Pan",
          "hidden": false
        },
        {
          "_id": "682c51889f83963d2d419990",
          "name": "Mingfei Sun",
          "hidden": false
        },
        {
          "_id": "682c51889f83963d2d419991",
          "name": "Cheston Tan",
          "hidden": false
        },
        {
          "_id": "682c51889f83963d2d419992",
          "name": "Mengmi Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-19T08:57:30.000Z",
      "submittedOnDailyAt": "2025-05-20T08:26:15.837Z",
      "title": "グランツから文法に至るまで：協力狩猟からの発生語言",
      "submittedOnDailyBy": {
        "_id": "64d98ef7a4839890b25eb78b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64d98ef7a4839890b25eb78b/215-CSVLl81z6CAq0ECWU.jpeg",
        "isPro": true,
        "fullname": "Fangyuan Yu",
        "user": "Ksgk-fy",
        "type": "user"
      },
      "summary": "早期の洞窟人は手势、語り声、簡単な信号を使用して、協調、計画、捕食者を避けること、資源を共有することを可能にしました。現在、人類は複雑な言語を使用して、驚異的な成果を達成します。このコミュニケーションの進化はどのように駆動されているのか？言語はどのように発生し、適応し、チームワークに重要な役割になるのか？言語の起源を理解するのは難しいです。言語学とアンタロポロジーでの主な仮説は、言語は早期の人類の協力の生態的および社会的な要求に対応して進化したと仮定しています。言語は孤立して発生しませんでしたが、共有した生存の目標によって発生しました。\n\nこの観点に受け感じられ、マルチエージェントのフォーガウェイゲームでの言語の発生を調査しています。これらの環境は、言語の進化に影響した認知的および生態学的制約を反映して設計されています。エージェントは、他のエージェントと環境についての部分の知識しか持ち、高価値のターゲットを拾ったり、時系列に並べられた行動を実行するように協調する必要があります。エンドツーエンドの深層学習で強化調教を使用して、エージェントはどのように行動を学ぶかと、どのようにコミュニケーションの戦略を学ぶかを学びます。私たちは、エージェントが自然言語のハラマー特徴を持つコミュニケーションプロトコルを開発したことを発見しました：任意性、交換可能性、置換性、文化伝達、構成性。これらの性質を定量化し、人口サイズや時系列的依存関係などの異なる要因がどのように言語の特定の面で影響を与えるかを分析します。私たちのフレームワークは、機体化マルチエージェントの設定で、部分の観測性、時系列的推理、協力の目標によって言語がどのように進化するかを研究するプラットフォームとして役立ちます。私たちは、すべてのデータ、コード、モデルを公開します。",
      "upvotes": 1,
      "discussionId": "682c51899f83963d2d4199fe"
    },
    "publishedAt": "2025-05-19T04:57:30.000Z",
    "title": "From Grunts to Grammar: Emergent Language from Cooperative Foraging",
    "summary": "Early cavemen relied on gestures, vocalizations, and simple signals to\ncoordinate, plan, avoid predators, and share resources. Today, humans\ncollaborate using complex languages to achieve remarkable results. What drives\nthis evolution in communication? How does language emerge, adapt, and become\nvital for teamwork? Understanding the origins of language remains a challenge.\nA leading hypothesis in linguistics and anthropology posits that language\nevolved to meet the ecological and social demands of early human cooperation.\nLanguage did not arise in isolation, but through shared survival goals.\nInspired by this view, we investigate the emergence of language in multi-agent\nForaging Games. These environments are designed to reflect the cognitive and\necological constraints believed to have influenced the evolution of\ncommunication. Agents operate in a shared grid world with only partial\nknowledge about other agents and the environment, and must coordinate to\ncomplete games like picking up high-value targets or executing temporally\nordered actions. Using end-to-end deep reinforcement learning, agents learn\nboth actions and communication strategies from scratch. We find that agents\ndevelop communication protocols with hallmark features of natural language:\narbitrariness, interchangeability, displacement, cultural transmission, and\ncompositionality. We quantify each property and analyze how different factors,\nsuch as population size and temporal dependencies, shape specific aspects of\nthe emergent language. Our framework serves as a platform for studying how\nlanguage can evolve from partial observability, temporal reasoning, and\ncooperative goals in embodied multi-agent settings. We will release all data,\ncode, and models publicly.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.12872.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64d98ef7a4839890b25eb78b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64d98ef7a4839890b25eb78b/215-CSVLl81z6CAq0ECWU.jpeg",
      "fullname": "Fangyuan Yu",
      "name": "Ksgk-fy",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 15
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.12058",
      "authors": [
        {
          "_id": "682c49699953a079cc8964a0",
          "user": {
            "_id": "643bc6ea5ec6af9c331ad3f9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643bc6ea5ec6af9c331ad3f9/ZFppIidaJ_dKgk70bU6f6.png",
            "isPro": false,
            "fullname": "Vincent Koc",
            "user": "vincentkoc",
            "type": "user"
          },
          "name": "Vincent Koc",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-20T09:20:49.606Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/643bc6ea5ec6af9c331ad3f9/ZckIkxLgGEHjJy8E5qa59.png"
      ],
      "publishedAt": "2025-05-17T15:40:03.000Z",
      "submittedOnDailyAt": "2025-05-20T08:16:30.585Z",
      "title": "ティニー QA ベンチマークプラスプラス：超軽量、合成的な多言語データセット\nジェネレーションと煙のテストを用いた継続的なLLM評価",
      "submittedOnDailyBy": {
        "_id": "643bc6ea5ec6af9c331ad3f9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643bc6ea5ec6af9c331ad3f9/ZFppIidaJ_dKgk70bU6f6.png",
        "isPro": false,
        "fullname": "Vincent Koc",
        "user": "vincentkoc",
        "type": "user"
      },
      "summary": "Tiny QA Benchmark++ (TQB++)は、超エルギー、多言語のスモークテストセットで、大語言モデル（LLM）パイプラインにユニットテストスタイルの安全ネットデータセットを提供します。このセットは秒毎に実行でき、最小限のコストを負担します。Comet Opikのプロンプト最適化SDKの緊密なフィードバックループの要求から生まれたTQB++は、重めなベンチマークを待つことで開発フローを妨げることを避けるために、52サイズの英語ゴールドセット（20kB未満）と、LiteLLMに基づく提供者無関係なシンテティックデータ生成パッケージを組み合わせて作成されました。このジェネレータは、実践者がどの言語、ドメイン、それまでの難易度で自分のティニーパックを作成することを可能にします。既に10つのプレインディードパックは、アラビア語、中国語、フランス語、ドイツ語、日本語、韓国語、ポルトガル語、ロシア語、スペイン語、トルコ語をカバーしています。各データセットは、CroissantメタデータとOpenAI-Evals、LangChain、標準的なCIツールのプラグインとパックを揃えて配布されており、チームはGPUブジュエルを負担せずに、確定的なマイクロベンチマークを直接リプライセスゲート、プロンプト工学ループ、プロダクションダッシュボードにドロップできます。TQB++の完全なランニングはパイプラインラテンシーにそのほんの数秒の追加がかかりますが、MMLUやBIG-Benchのような全視点ベンチマークが設定完了する前に、プロンプトテンプレートエラー、トーキナイザーの漂流、微調校の副作用を信頼的にフラグします。フレームワーク全体は、生成AIエコシステム全体で継続的な、リソース効率的な品質保証を加速するために公開されています。",
      "upvotes": 1,
      "discussionId": "682c496a9953a079cc8964df",
      "projectPage": "https://huggingface.co/datasets/vincentkoc/tiny_qa_benchmark",
      "githubRepo": "https://github.com/vincentkoc/tinyqa_benchmark_pp",
      "ai_keywords": [
        "large-language-model (LLM)",
        "prompt-optimization SDK",
        "synthetic-data generator",
        "provider-agnostic",
        "LiteLLM",
        "Croissant metadata",
        "OpenAI-Evals",
        "LangChain",
        "CI tools",
        "micro-benchmarks",
        "prompt-template errors",
        "tokenizer drift",
        "fine-tuning side-effects",
        "MMLU",
        "BIG-Bench",
        "generative-AI ecosystem"
      ]
    },
    "publishedAt": "2025-05-17T11:40:03.000Z",
    "title": "Tiny QA Benchmark++: Ultra-Lightweight, Synthetic Multilingual Dataset\n  Generation & Smoke-Tests for Continuous LLM Evaluation",
    "summary": "Tiny QA Benchmark++ (TQB++) presents an ultra-lightweight, multilingual\nsmoke-test suite designed to give large-language-model (LLM) pipelines a\nunit-test style safety net dataset that runs in seconds with minimal cost. Born\nout of the tight feedback-loop demands building the Comet Opik\nprompt-optimization SDK, where waiting on heavyweight benchmarks breaks\ndeveloper flow. TQB++ couples a 52-item English gold set (less than 20 kB) with\na tiny synthetic-data generator pypi package built on provider-agnostic\nLiteLLM. The generator lets practitioners mint their own tiny packs in any\nlanguage, domain, or difficulty, while ten ready-made packs already cover\nArabic, Chinese, French, German, Japanese, Korean, Portuguese, Russian,\nSpanish, and Turkish. Every dataset ships with Croissant metadata and\nplug-and-play files for OpenAI-Evals, LangChain, and standard CI tools, so\nteams can drop deterministic micro-benchmarks directly into pull-request gates,\nprompt-engineering loops, and production dashboards without touching GPU\nbudgets. A complete TQB++ run adds only a few seconds to pipeline latency yet\nreliably flags prompt-template errors, tokenizer drift, and fine-tuning\nside-effects long before full-scale suites like MMLU or BIG-Bench would finish\nconfiguring. The entire framework is released to accelerate continuous,\nresource-efficient quality assurance across the generative-AI ecosystem.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/643bc6ea5ec6af9c331ad3f9/ZckIkxLgGEHjJy8E5qa59.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.12058.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "643bc6ea5ec6af9c331ad3f9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643bc6ea5ec6af9c331ad3f9/ZFppIidaJ_dKgk70bU6f6.png",
      "fullname": "Vincent Koc",
      "name": "vincentkoc",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.11497",
      "authors": [
        {
          "_id": "682c27b10f622b7afc25df1f",
          "user": {
            "_id": "64b500fdf460afaefc5c64b3",
            "avatarUrl": "/avatars/0cb90e3fdd116e1a49209b222125c76e.svg",
            "isPro": false,
            "fullname": "Yushi Huang",
            "user": "Harahan",
            "type": "user"
          },
          "name": "Yushi Huang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-20T08:07:25.265Z",
          "hidden": false
        },
        {
          "_id": "682c27b10f622b7afc25df20",
          "user": {
            "_id": "648876a7063b5020501479f0",
            "avatarUrl": "/avatars/0a8a0c1d4ebf8e444d151e634d55e91f.svg",
            "isPro": false,
            "fullname": "Gong",
            "user": "Ruihao",
            "type": "user"
          },
          "name": "Ruihao Gong",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-20T08:53:49.624Z",
          "hidden": false
        },
        {
          "_id": "682c27b10f622b7afc25df21",
          "name": "Jing Liu",
          "hidden": false
        },
        {
          "_id": "682c27b10f622b7afc25df22",
          "name": "Yifu Ding",
          "hidden": false
        },
        {
          "_id": "682c27b10f622b7afc25df23",
          "user": {
            "_id": "64e9bfc3f494f8b2a061a010",
            "avatarUrl": "/avatars/e55cfea55b45b03d1abfa38db6af58b6.svg",
            "isPro": false,
            "fullname": "吕呈滔",
            "user": "lvchengtao",
            "type": "user"
          },
          "name": "Chengtao Lv",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-20T08:53:14.848Z",
          "hidden": false
        },
        {
          "_id": "682c27b10f622b7afc25df24",
          "user": {
            "_id": "65c49589c0b1921e19260a8d",
            "avatarUrl": "/avatars/7ce9af8c627f2a0c3db6bde82290ee1f.svg",
            "isPro": false,
            "fullname": "Haotong Qin",
            "user": "HaotongQin",
            "type": "user"
          },
          "name": "Haotong Qin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-20T08:52:56.938Z",
          "hidden": false
        },
        {
          "_id": "682c27b10f622b7afc25df25",
          "name": "Jun Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-16T17:59:40.000Z",
      "submittedOnDailyAt": "2025-05-20T06:04:24.681Z",
      "title": "QVGen: 量子化ビデオ生成モデルの限界を突き抜ける",
      "submittedOnDailyBy": {
        "_id": "64b500fdf460afaefc5c64b3",
        "avatarUrl": "/avatars/0cb90e3fdd116e1a49209b222125c76e.svg",
        "isPro": false,
        "fullname": "Yushi Huang",
        "user": "Harahan",
        "type": "user"
      },
      "summary": "ビデオディフュージョンモデル（DMs）は、高品質のビデオ合成を可能にしました。しかし、それらの計算量とメモリの要求は、高級グラフィックスプロジェクタでも実世界の採用に重大な課題をもたらしています。一般的な解決策として、量化は画像DMsにおいてコスト削減において顕著な成功を収めましたが、直接ビデオDMsに適用することは効果的ではありません。本論文では、極端に低ビットの量化（例：4ビットまたはそれ以下）で高性能と推論効率的なビデオDMsを扱うための新しい量化に関する訓練（QAT）フレームワーク、QVGenを紹介します。まず、QATの収束を促すために勾配の大きさを減少することの重要性を理論的に分析します。そこで、大きな量化誤差を軽減するための助手モジュール（Phi）を導入し、収束を大幅に向上させます。Phiの推論オーバーヘッドを除去するために、順次Phiを除去するスケープディエーション戦略を提案します。特に、固有値分解（SVD）と提案された順位ベースの正則化gammaを繰り返し使用し、低貢献のコンポーネントを識別して衰減します。この戦略は性能を維持する一方で、推論オーバーヘッドをゼロにすることを実現します。4つの最先端（SOTA）のビデオDMsを採用し、パラメータサイズが1.3Bから14Bまでの範囲で検証した拡張なされた実験により、QVGenは4ビット設定で全精度と比較可能な品質を達成することが初めてです。また、現在の方法よりも显著に優れています。例えば、我々の3ビットCogVideoX-2Bは、VBenchでDynamic Degreeに+25.28、Scene Consistencyに+8.43の向上を収めました。",
      "upvotes": 1,
      "discussionId": "682c27b20f622b7afc25df76",
      "ai_keywords": [
        "Video diffusion models (DMs)",
        "quantization",
        "quantization-aware training (QAT)",
        "gradient norm",
        "auxiliary modules ($\\Phi$)",
        "singular value decomposition (SVD)",
        "rank-based regularization $\\mathbf{\\gamma}$",
        "Dynamic Degree",
        "Scene Consistency",
        "VBench"
      ]
    },
    "publishedAt": "2025-05-16T13:59:40.000Z",
    "title": "QVGen: Pushing the Limit of Quantized Video Generative Models",
    "summary": "Video diffusion models (DMs) have enabled high-quality video synthesis. Yet,\ntheir substantial computational and memory demands pose serious challenges to\nreal-world deployment, even on high-end GPUs. As a commonly adopted solution,\nquantization has proven notable success in reducing cost for image DMs, while\nits direct application to video DMs remains ineffective. In this paper, we\npresent QVGen, a novel quantization-aware training (QAT) framework tailored for\nhigh-performance and inference-efficient video DMs under extremely low-bit\nquantization (e.g., 4-bit or below). We begin with a theoretical analysis\ndemonstrating that reducing the gradient norm is essential to facilitate\nconvergence for QAT. To this end, we introduce auxiliary modules (Phi) to\nmitigate large quantization errors, leading to significantly enhanced\nconvergence. To eliminate the inference overhead of Phi, we propose a\nrank-decay strategy that progressively eliminates Phi. Specifically, we\nrepeatedly employ singular value decomposition (SVD) and a proposed rank-based\nregularization gamma to identify and decay low-contributing\ncomponents. This strategy retains performance while zeroing out inference\noverhead. Extensive experiments across 4 state-of-the-art (SOTA) video DMs,\nwith parameter sizes ranging from 1.3B sim14B, show that QVGen is the\nfirst to reach full-precision comparable quality under 4-bit settings.\nMoreover, it significantly outperforms existing methods. For instance, our\n3-bit CogVideoX-2B achieves improvements of +25.28 in Dynamic Degree and\n+8.43 in Scene Consistency on VBench.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.11497.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b500fdf460afaefc5c64b3",
      "avatarUrl": "/avatars/0cb90e3fdd116e1a49209b222125c76e.svg",
      "fullname": "Yushi Huang",
      "name": "Harahan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.12257",
      "authors": [
        {
          "_id": "682c105927a587e5a6ebacdd",
          "user": {
            "_id": "68264aa0e6a0ae8670403081",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68264aa0e6a0ae8670403081/a6V9yE1cf6-lFf7G8Ih-H.png",
            "isPro": false,
            "fullname": "Evgeny Markhasin",
            "user": "PChemGuy",
            "type": "user"
          },
          "name": "Evgeny Markhasin",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-20T07:20:15.756Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-18T06:33:08.000Z",
      "submittedOnDailyAt": "2025-05-20T03:52:49.831Z",
      "title": "LLMコンテキスト条件付きとPWPプロンプティングによる化学式の多モデル検証",
      "submittedOnDailyBy": {
        "_id": "68264aa0e6a0ae8670403081",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68264aa0e6a0ae8670403081/a6V9yE1cf6-lFf7G8Ih-H.png",
        "isPro": false,
        "fullname": "Evgeny Markhasin",
        "user": "PChemGuy",
        "type": "user"
      },
      "summary": "複雑な科学技術記録内の微妙な技術的誤差の特定、特に画像に含まれる公式の複数の観点からの解釈を求める場合、大語言モデル（LLMs）の固有の誤差補正傾向が不正確性を隠すという重大な課題があります。この探索的な証明の概念（PoC）研究では、持続的なワークフロープランティング（PWP）原理に基づく構造化されたLLMコンテキスト条件調整を方法学的な戦略として、推論時にLLMのこの行動を調節することを調査しています。このアプローチは、標準のチャットインターフェースをみたし、APIアクセスやモデルの変更を除き、利用できる一般的なLLMs（特にGemini 2.5 ProとChatGPT Plus o3）の精度の向上を効果的に行うために設計されています。この方法学を調査するために、知られた文脈的および画像ベースの誤りを含む単一の複雑なテストペーパー内の化学公式の検証を焦点としました。複数のプロンプティングストラテジーが評価されました：基本的なプロンプトは信頼できなかったが、PWP構造を変更した分析的なモードに厳密な条件調整を行うアプローチが両モデルで文脈的誤差の特定を改善したことがわかりました。特に、この方法は、手動レビューで隠されていた微妙な画像ベースの公式誤差を再現的に特定することができ、このタスクではChatGPT Plus o3が失敗したことが特に注目されました。これらの予備的な発見は、詳細的な検証においてLLMの操作モードが妨げることを明らかにし、PWPに基づくコンテキスト条件調整が科学技術記録で細かい誤差検出が必要なタスクに対して強力な分析ワークフローの開発においてプロモーション的で高度にアクセス可能な技術としての可能性を示しています。この限定的なPoCを超えた拡張的な検証が必要であり、その広範な適用可能性を確認することが必要です。",
      "upvotes": 0,
      "discussionId": "682c105a27a587e5a6ebad2e"
    },
    "publishedAt": "2025-05-18T02:33:08.000Z",
    "title": "LLM Context Conditioning and PWP Prompting for Multimodal Validation of\n  Chemical Formulas",
    "summary": "Identifying subtle technical errors within complex scientific and technical\ndocuments, especially those requiring multimodal interpretation (e.g., formulas\nin images), presents a significant hurdle for Large Language Models (LLMs)\nwhose inherent error-correction tendencies can mask inaccuracies. This\nexploratory proof-of-concept (PoC) study investigates structured LLM context\nconditioning, informed by Persistent Workflow Prompting (PWP) principles, as a\nmethodological strategy to modulate this LLM behavior at inference time. The\napproach is designed to enhance the reliability of readily available,\ngeneral-purpose LLMs (specifically Gemini 2.5 Pro and ChatGPT Plus o3) for\nprecise validation tasks, crucially relying only on their standard chat\ninterfaces without API access or model modifications. To explore this\nmethodology, we focused on validating chemical formulas within a single,\ncomplex test paper with known textual and image-based errors. Several prompting\nstrategies were evaluated: while basic prompts proved unreliable, an approach\nadapting PWP structures to rigorously condition the LLM's analytical mindset\nappeared to improve textual error identification with both models. Notably,\nthis method also guided Gemini 2.5 Pro to repeatedly identify a subtle\nimage-based formula error previously overlooked during manual review, a task\nwhere ChatGPT Plus o3 failed in our tests. These preliminary findings highlight\nspecific LLM operational modes that impede detail-oriented validation and\nsuggest that PWP-informed context conditioning offers a promising and highly\naccessible technique for developing more robust LLM-driven analytical\nworkflows, particularly for tasks requiring meticulous error detection in\nscientific and technical documents. Extensive validation beyond this limited\nPoC is necessary to ascertain broader applicability.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.12257.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "68264aa0e6a0ae8670403081",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68264aa0e6a0ae8670403081/a6V9yE1cf6-lFf7G8Ih-H.png",
      "fullname": "Evgeny Markhasin",
      "name": "PChemGuy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.11988",
      "authors": [
        {
          "_id": "682c2a1b09ce6055263a5094",
          "user": {
            "_id": "6458ac92c16ecb4815dd1d10",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6458ac92c16ecb4815dd1d10/llkaZ8U-4IEWgtopk2BJs.jpeg",
            "isPro": false,
            "fullname": "Ahmed Lekssays",
            "user": "lekssays",
            "type": "user"
          },
          "name": "Ahmed Lekssays",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-20T07:19:55.468Z",
          "hidden": false
        },
        {
          "_id": "682c2a1b09ce6055263a5095",
          "user": {
            "_id": "65c094edb54ab5b37d9d883b",
            "avatarUrl": "/avatars/81f75c49d31335ff74e24bd37cb89bcb.svg",
            "isPro": false,
            "fullname": "utsav shukla",
            "user": "utsavshukla",
            "type": "user"
          },
          "name": "Utsav Shukla",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-20T08:54:03.070Z",
          "hidden": false
        },
        {
          "_id": "682c2a1b09ce6055263a5096",
          "user": {
            "_id": "66c6e7ced707a52f9d102f66",
            "avatarUrl": "/avatars/e7fc2e78babee4765276978aeb42b1aa.svg",
            "isPro": false,
            "fullname": "Husrev Taha Sencar",
            "user": "TahaSencar",
            "type": "user"
          },
          "name": "Husrev Taha Sencar",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-20T08:54:09.925Z",
          "hidden": false
        },
        {
          "_id": "682c2a1b09ce6055263a5097",
          "user": {
            "_id": "65ae1c4468139e3c42973fe4",
            "avatarUrl": "/avatars/b065a857dd763410caadea37a2dc01c4.svg",
            "isPro": false,
            "fullname": "Md Rizwan Parvez",
            "user": "mparvez",
            "type": "user"
          },
          "name": "Md Rizwan Parvez",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-20T08:54:21.776Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-17T12:46:10.000Z",
      "submittedOnDailyAt": "2025-05-20T05:37:48.776Z",
      "title": "TechniqueRAG: 戦闘手法の検索アウグメント生成技術\nサイバー敵情報のテキストの注釈",
      "submittedOnDailyBy": {
        "_id": "6458ac92c16ecb4815dd1d10",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6458ac92c16ecb4815dd1d10/llkaZ8U-4IEWgtopk2BJs.jpeg",
        "isPro": false,
        "fullname": "Ahmed Lekssays",
        "user": "lekssays",
        "type": "user"
      },
      "summary": "敵意手法の正確な識別は、セキュリティテキストの有効なサイバー防御には重要です。しかし、現在の方法は基本的な調整を面見ています：それらは、限定的なドメイン精度を持つジャンルシンプルなモデルを依存しているか、大きなラベル付きデータセットとタスク専門的な最適化（例えば、カスタムの難読マイニングとデノイズ）に依存していますが、これらは専門的なドメインでは少なくありません。\n\n我々は、テクニックRAG（TechniqueRAG）を提案します。これは、ドメイン専門的な検索拡充生成（RAG）フレームワークです。これは、シンプルな検索ツール、インストラクションチューニングされたLLM、最小限のテキストとテクニックのペアを統合して、この間違いをコンクリートにします。我々のアプローチは、データの不足を解決し、有限なドメインサンプルでの生成コンポーネントの微調節を依存し、資源豊かな検索トレーニングの必要性を回避します。従来のRAGは、検索と生成の連携を通じてハロキニングを軽減しますが、ジャンルシンプルな検索ツールに依存しているため、ノイズの多いキャンディドを導入し、ドメイン専門的な精度を制限します。これを解決するために、ゼロショットLLMでの再スコア化を使用して、検索キャンディドを敵意手法に明確に対応させます。\n\n複数のセキュリティベンチマーク上の実験は、技術RAGは、複数のタスク専門的な最適化やラベル付きデータを必要としない状態の最先端の性能を達成します。また、詳細な分析は、さらなる見解を提供します。",
      "upvotes": 0,
      "discussionId": "682c2a1c09ce6055263a50da",
      "githubRepo": "https://github.com/qcri/TechniqueRAG",
      "ai_keywords": [
        "retrieval-augmented generation (RAG)",
        "off-the-shelf retrievers",
        "instruction-tuned LLMs",
        "minimal text-technique pairs",
        "domain-specific retrieval",
        "zero-shot LLM re-ranking",
        "hallucination"
      ]
    },
    "publishedAt": "2025-05-17T08:46:10.000Z",
    "title": "TechniqueRAG: Retrieval Augmented Generation for Adversarial Technique\n  Annotation in Cyber Threat Intelligence Text",
    "summary": "Accurately identifying adversarial techniques in security texts is critical\nfor effective cyber defense. However, existing methods face a fundamental\ntrade-off: they either rely on generic models with limited domain precision or\nrequire resource-intensive pipelines that depend on large labeled datasets and\ntask-specific optimizations, such as custom hard-negative mining and denoising,\nresources rarely available in specialized domains.\n  We propose TechniqueRAG, a domain-specific retrieval-augmented generation\n(RAG) framework that bridges this gap by integrating off-the-shelf retrievers,\ninstruction-tuned LLMs, and minimal text-technique pairs. Our approach\naddresses data scarcity by fine-tuning only the generation component on limited\nin-domain examples, circumventing the need for resource-intensive retrieval\ntraining. While conventional RAG mitigates hallucination by coupling retrieval\nand generation, its reliance on generic retrievers often introduces noisy\ncandidates, limiting domain-specific precision. To address this, we enhance\nretrieval quality and domain specificity through zero-shot LLM re-ranking,\nwhich explicitly aligns retrieved candidates with adversarial techniques.\n  Experiments on multiple security benchmarks demonstrate that TechniqueRAG\nachieves state-of-the-art performance without extensive task-specific\noptimizations or labeled data, while comprehensive analysis provides further\ninsights.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.11988.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6458ac92c16ecb4815dd1d10",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6458ac92c16ecb4815dd1d10/llkaZ8U-4IEWgtopk2BJs.jpeg",
      "fullname": "Ahmed Lekssays",
      "name": "lekssays",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.03332",
      "authors": [
        {
          "_id": "68263e83543459fc150218d3",
          "user": {
            "_id": "68264aa0e6a0ae8670403081",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68264aa0e6a0ae8670403081/a6V9yE1cf6-lFf7G8Ih-H.png",
            "isPro": false,
            "fullname": "Evgeny Markhasin",
            "user": "PChemGuy",
            "type": "user"
          },
          "name": "Evgeny Markhasin",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-16T07:12:13.763Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-06T09:06:18.000Z",
      "submittedOnDailyAt": "2025-05-20T03:56:15.822Z",
      "title": "AI駆動の学術的な同儕レビューを長期的なワークフロープライン、メタプライン、およびメタラジィングによって実現する",
      "submittedOnDailyBy": {
        "_id": "68264aa0e6a0ae8670403081",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68264aa0e6a0ae8670403081/a6V9yE1cf6-lFf7G8Ih-H.png",
        "isPro": false,
        "fullname": "Evgeny Markhasin",
        "user": "PChemGuy",
        "type": "user"
      },
      "summary": "科学論文の批判的な同行評価における大規模言語モデル（LLM）については、データの制限と専門家の理由の複雑性により重要な課題となっています。本報告では、標準的なLLMチャットインターフェース（コード無し、API無し）を使用してこの隙を埋めるために、機能的なワークフロープラティング（PWP）という提示工程の方法を導入します。実験化学論文の批判的な分析に適したPWP提示を提案し、構造化されたマークダウンで定義された階層的、モジュール化アーキテクチャを特徴としています。このPWP提示は、元提示技術と元理由の連続的な適用により、専門家のレビューワークフローをシステマティックにコーディネートし、シークエンス知識も含むものです。会話の開始時に1度に提出されることで、このPWP提示は、後続のクエリによって起動される持続的なワークフローをLLMに提供し、現代の理由LLMをシステマティックな多モーダル評価にガイドします。テストケースでの主要な方法学的な欠陥を識別し、LLMの入力バイアスを抑制し、複雑なタスクを実行することを示し、主張と証拠を区別し、テキスト/写真/図の分析を統合してパラメータを推定、定量的な可能性検証を実行、証拠と主張を比較、先驚きの可能性を評価することを含みます。透明性と再現性の確保のため、完全な提示、詳細な示唆分析、インタラクティブチャットのログを補助リソースとして提供します。特定のアプリケーションを除き、この研究は、詳細なワークフローの形式化に基づくPWPの可能性を明らかにし、複雑な科学タスクに対して手に入れられる専門的な分析を可能にするための複雑なLLMの使用を促すことを示します。",
      "upvotes": 0,
      "discussionId": "68263e84543459fc150218f3"
    },
    "publishedAt": "2025-05-06T05:06:18.000Z",
    "title": "AI-Driven Scholarly Peer Review via Persistent Workflow Prompting,\n  Meta-Prompting, and Meta-Reasoning",
    "summary": "Critical peer review of scientific manuscripts presents a significant\nchallenge for Large Language Models (LLMs), partly due to data limitations and\nthe complexity of expert reasoning. This report introduces Persistent Workflow\nPrompting (PWP), a potentially broadly applicable prompt engineering\nmethodology designed to bridge this gap using standard LLM chat interfaces\n(zero-code, no APIs). We present a proof-of-concept PWP prompt for the critical\nanalysis of experimental chemistry manuscripts, featuring a hierarchical,\nmodular architecture (structured via Markdown) that defines detailed analysis\nworkflows. We develop this PWP prompt through iterative application of\nmeta-prompting techniques and meta-reasoning aimed at systematically codifying\nexpert review workflows, including tacit knowledge. Submitted once at the start\nof a session, this PWP prompt equips the LLM with persistent workflows\ntriggered by subsequent queries, guiding modern reasoning LLMs through\nsystematic, multimodal evaluations. Demonstrations show the PWP-guided LLM\nidentifying major methodological flaws in a test case while mitigating LLM\ninput bias and performing complex tasks, including distinguishing claims from\nevidence, integrating text/photo/figure analysis to infer parameters, executing\nquantitative feasibility checks, comparing estimates against claims, and\nassessing a priori plausibility. To ensure transparency and facilitate\nreplication, we provide full prompts, detailed demonstration analyses, and logs\nof interactive chats as supplementary resources. Beyond the specific\napplication, this work offers insights into the meta-development process\nitself, highlighting the potential of PWP, informed by detailed workflow\nformalization, to enable sophisticated analysis using readily available LLMs\nfor complex scientific tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03332.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "68264aa0e6a0ae8670403081",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68264aa0e6a0ae8670403081/a6V9yE1cf6-lFf7G8Ih-H.png",
      "fullname": "Evgeny Markhasin",
      "name": "PChemGuy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]