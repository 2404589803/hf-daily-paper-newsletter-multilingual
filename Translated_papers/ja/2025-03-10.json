[
  {
    "paper": {
      "id": "2503.05236",
      "authors": [
        {
          "_id": "67ce37239f9aaaae837f3894",
          "user": {
            "_id": "654c6845bac6e6e49895a5b5",
            "avatarUrl": "/avatars/ed1f140abcd4d76669e2e48db1d1193f.svg",
            "isPro": false,
            "fullname": "Yibin Wang",
            "user": "CodeGoat24",
            "type": "user"
          },
          "name": "Yibin Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T09:37:51.835Z",
          "hidden": false
        },
        {
          "_id": "67ce37239f9aaaae837f3895",
          "user": {
            "_id": "63859cf3b2906edaf83af9f0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63859cf3b2906edaf83af9f0/iUQm5FAomzqYi6fkqIn9F.jpeg",
            "isPro": false,
            "fullname": "Yuhang Zang",
            "user": "yuhangzang",
            "type": "user"
          },
          "name": "Yuhang Zang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-10T08:01:24.660Z",
          "hidden": false
        },
        {
          "_id": "67ce37239f9aaaae837f3896",
          "name": "Hao Li",
          "hidden": false
        },
        {
          "_id": "67ce37239f9aaaae837f3897",
          "name": "Cheng Jin",
          "hidden": false
        },
        {
          "_id": "67ce37239f9aaaae837f3898",
          "user": {
            "_id": "64638c4d51fa6e63060521b5",
            "avatarUrl": "/avatars/c863ace5b1dc788a341bcf4ddbdfaec1.svg",
            "isPro": false,
            "fullname": "JIaqi",
            "user": "Jiaqiwang",
            "type": "user"
          },
          "name": "Jiaqi Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T09:38:17.938Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-07T08:36:05.000Z",
      "title": "統合賞奨モデルの多モード理解と生成",
      "summary": "最近の人間の好みの調整の進歩は、多モデルの生成と理解にとって大幅に向上されました。鍵のアプローチとして、報酬モデルの訓練をガイドすることで好み最適化を行うことが重要です。しかし、現在のモデルは通常、特定のタスクに特化されており、多様な可視アプリケーションに適応することが限られています。また、複数のタスクを同時に評価することで、相互の効果を促進することができると主張しています。そのために、本論文では、最初のユニットマイナスモデル「UnifiedReward」を提案します。これは、多モデルの理解と生成の評価を可能にし、ペアランキングとポイントスコアを行うことができます。具体的には、(1) 私たちは、自分たちが構築した大規模な人間の好みデータセットにユニットマイナスモデルを開発しました。このデータセットには、画像とビデオの生成/理解タスクが含まれています。 (2) 次に、これらのビジョンモデルに基づいて高品質の好みペアデータを自動的に構築し、ペアランキングと点シフトでその出力をフィルタリングします。 (3) 最後に、これらのデータを直接な好み最適化（DPO）に用いて好みの調整を行います。実験結果は、複数の可視タスクを評価することで相互の利益を大きく引き出すことができることを示し、画像とビデオの理解/生成タスクにこのパイプラインを適用し、各領域での性能を大幅に向上させました。",
      "upvotes": 72,
      "discussionId": "67ce37259f9aaaae837f3948",
      "projectPage": "https://codegoat24.github.io/UnifiedReward/",
      "githubRepo": "https://github.com/CodeGoat24/UnifiedReward"
    },
    "publishedAt": "2025-03-09T22:20:09.137Z",
    "title": "Unified Reward Model for Multimodal Understanding and Generation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05236.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "654c6845bac6e6e49895a5b5",
      "avatarUrl": "/avatars/ed1f140abcd4d76669e2e48db1d1193f.svg",
      "fullname": "Yibin Wang",
      "name": "CodeGoat24",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.05179",
      "authors": [
        {
          "_id": "67ce4bff5847e4787a7ebedd",
          "user": {
            "_id": "65f4060754ecda1ecb5797a0",
            "avatarUrl": "/avatars/f8b44524d36b505673cb538fd7895a82.svg",
            "isPro": false,
            "fullname": "Simon Aytes",
            "user": "saytes",
            "type": "user"
          },
          "name": "Simon A. Aytes",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-10T08:01:10.363Z",
          "hidden": false
        },
        {
          "_id": "67ce4bff5847e4787a7ebede",
          "user": {
            "_id": "63036b6c5c70c21d0ea79d48",
            "avatarUrl": "/avatars/a7eb03f5cbd4eaa09fe807bbed8bc0f7.svg",
            "isPro": false,
            "fullname": "Jinheon Baek",
            "user": "jinheon",
            "type": "user"
          },
          "name": "Jinheon Baek",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-10T08:01:13.328Z",
          "hidden": false
        },
        {
          "_id": "67ce4bff5847e4787a7ebedf",
          "name": "Sung Ju Hwang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-07T06:57:17.000Z",
      "title": "スキートオブスモード：エフィシェントLLM論理によるアダプティブな認知をモデルとしたスキート",
      "summary": "最近の大語言モデルの進展は、Chain of Thought (CoT) プロンプティングによる魅力的な理由論的な能力を示したが、その代わりに中間出力の過剰な長さが計算オーバーヘッドを増やしていました。我々は、Sketch-of-Thought (SoT) という新しいプロンプティングフレームワークを紹介し、これは認知科学に基づく理由論的パラダイムと言語制約を組み合わせて、理由論的精度を維持するにあたり、トークンの使用量を最小限に抑えることを目的としています。SoT は、認知科学に基づく任意のカスタム理由論的パラダイムを組み込むことができる柔軟なフレームワークとして設計されています。私たちは、Conceptual Chaining、Chunked Symbolism、Expert Lexicons の 3つのパラダイムを実装し、それぞれの理由論的タスクに適しており、軽量のルーティングモデルによって動的に選択されます。15つの理由論的データセットを構成する複数の言語と多モデルスケーナrios での検証を通じて、SoT はトークン削減率が 76% に達し、誤り率の影響は微視できることを示しました。数学や多段階理由のような特定の領域では、それほど少ないトークンを使用して精度を向上させることができます。我々のコードは公開的に利用可能です：https://www.github.com/SimonAytes/SoT。",
      "upvotes": 27,
      "discussionId": "67ce4c035847e4787a7ebf4c",
      "projectPage": "https://huggingface.co/saytes/SoT_DistilBERT",
      "githubRepo": "https://github.com/SimonAytes/SoT"
    },
    "publishedAt": "2025-03-09T22:25:52.244Z",
    "title": "Sketch-of-Thought: Efficient LLM Reasoning with Adaptive Cognitive-Inspired Sketching",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05179.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "63036b6c5c70c21d0ea79d48",
      "avatarUrl": "/avatars/a7eb03f5cbd4eaa09fe807bbed8bc0f7.svg",
      "fullname": "Jinheon Baek",
      "name": "jinheon",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.05500",
      "authors": [
        {
          "_id": "67ce9626e5cdfda52b9e8839",
          "user": {
            "_id": "62be186a5f59ff2320e6e32b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62be186a5f59ff2320e6e32b/W_emoC2uItM-MJZyCfIKI.png",
            "isPro": false,
            "fullname": "Nicolas-BZRD",
            "user": "Nicolas-BZRD",
            "type": "user"
          },
          "name": "Nicolas Boizard",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T09:42:06.860Z",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e883a",
          "user": {
            "_id": "65fa95405355a52c784633fc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65fa95405355a52c784633fc/rSfBUHPa7eSAsLd8DuOq4.png",
            "isPro": false,
            "fullname": "Hippolyte Gisserot-Boukhlef",
            "user": "hgissbkh",
            "type": "user"
          },
          "name": "Hippolyte Gisserot-Boukhlef",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T09:42:13.176Z",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e883b",
          "user": {
            "_id": "64132452d8a418df415a6ded",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64132452d8a418df415a6ded/qkjL5G89uldHUXlCI3n4f.jpeg",
            "isPro": false,
            "fullname": "Duarte Alves",
            "user": "DuarteMRAlves",
            "type": "user"
          },
          "name": "Duarte M. Alves",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T09:42:23.055Z",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e883c",
          "name": "André Martins",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e883d",
          "user": {
            "_id": "63937b399762cdd66be2a32f",
            "avatarUrl": "/avatars/7aefd888a3c54673d5881dcef61f771b.svg",
            "isPro": false,
            "fullname": "Ayoub Hammal",
            "user": "ayoubhammal",
            "type": "user"
          },
          "name": "Ayoub Hammal",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T09:42:42.527Z",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e883e",
          "user": {
            "_id": "677bedd522ca8585ede98470",
            "avatarUrl": "/avatars/54bca410c446610f02aca55918c74518.svg",
            "isPro": false,
            "fullname": "Caio Corro",
            "user": "caiocorro",
            "type": "user"
          },
          "name": "Caio Corro",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T09:42:48.603Z",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e883f",
          "user": {
            "_id": "61efea03a57920a251ec19b8",
            "avatarUrl": "/avatars/f47c8e3cb17a2bf7d43f2c152bb86885.svg",
            "isPro": false,
            "fullname": "Celine Hudelot",
            "user": "CelineH",
            "type": "user"
          },
          "name": "Céline Hudelot",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T09:51:41.273Z",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e8840",
          "user": {
            "_id": "66f2d6a684a241caac8e16dc",
            "avatarUrl": "/avatars/81acb87c2b07bea938251b40a2139911.svg",
            "isPro": false,
            "fullname": "Emmanuel Malherbe",
            "user": "emmanuelmalherbe",
            "type": "user"
          },
          "name": "Emmanuel Malherbe",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T09:51:47.996Z",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e8841",
          "name": "Etienne Malaboeuf",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e8842",
          "user": {
            "_id": "6708db59caf70ddea8e1355d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6708db59caf70ddea8e1355d/C6T16AdpqoeWCk7Gg9wSH.jpeg",
            "isPro": false,
            "fullname": "Fanny Jourdan",
            "user": "Fannyjrd",
            "type": "user"
          },
          "name": "Fanny Jourdan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T09:52:09.223Z",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e8843",
          "user": {
            "_id": "67cafedda972115e89972cd7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/P_xComqG9IttvluN-6tyB.png",
            "isPro": false,
            "fullname": "Gabriel Hautreux",
            "user": "GabrielHau",
            "type": "user"
          },
          "name": "Gabriel Hautreux",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T09:52:15.512Z",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e8844",
          "user": {
            "_id": "6772bde5c997eeb5550e80ea",
            "avatarUrl": "/avatars/8134a4d9330317e748dc7b33e1bb25f6.svg",
            "isPro": false,
            "fullname": "João Alves",
            "user": "albusonrails",
            "type": "user"
          },
          "name": "João Alves",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T09:52:22.630Z",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e8845",
          "user": {
            "_id": "66e2c22d7cc3edd60d725267",
            "avatarUrl": "/avatars/b217c5708c7dba8b1c220f37984ccc1e.svg",
            "isPro": false,
            "fullname": "Kevin El Haddad",
            "user": "kelhad",
            "type": "user"
          },
          "name": "Kevin El-Haddad",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T09:52:31.191Z",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e8846",
          "user": {
            "_id": "60f2e021adf471cbdf8bb660",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654090481550-60f2e021adf471cbdf8bb660.jpeg",
            "isPro": false,
            "fullname": "Manuel Faysse",
            "user": "manu",
            "type": "user"
          },
          "name": "Manuel Faysse",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T09:52:38.114Z",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e8847",
          "user": {
            "_id": "6369394dd322a76e1ea4bdf6",
            "avatarUrl": "/avatars/a4e5ab0167025fbbfc970d54630ce754.svg",
            "isPro": false,
            "fullname": "Maxime Peyrard",
            "user": "peyrardm",
            "type": "user"
          },
          "name": "Maxime Peyrard",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T09:52:44.389Z",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e8848",
          "user": {
            "_id": "67b622d2df3a86fbca306c43",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/lNlshrl56oaKslArMzSzj.png",
            "isPro": false,
            "fullname": "Nuno  Guerreiro",
            "user": "nunogj",
            "type": "user"
          },
          "name": "Nuno M. Guerreiro",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T09:52:54.367Z",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e8849",
          "name": "Patrick Fernandes",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e884a",
          "name": "Ricardo Rei",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e884b",
          "user": {
            "_id": "644a900e3a619fe72b14af0f",
            "avatarUrl": "/avatars/e2d5dac3d92757ed48e37e126a3464a3.svg",
            "isPro": false,
            "fullname": "Colombo",
            "user": "PierreColombo",
            "type": "user"
          },
          "name": "Pierre Colombo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T09:41:26.353Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-07T15:13:58.000Z",
      "title": "EuroBERT: ユーロベルト：東洋語言のエンコーダーを拡大する",
      "summary": "一般用語の多言語ベクトル表現は、検索、回帰と分類に使用されています。これらは、双方向エンコーダモデルから得られます。それもその広く応用可能であるが、最近、エンコーダーは生成的なデコーダモデルの進歩によって優れています。しかし、この進歩の多くのイノベーションは、デコーダーと固有に結びつかないです。本論文では、これらの進歩をめぐって、多言語エンコーダーの開発を再評価し、EuroBERT、つまり、歐フォーランド諸国と世界で広く使用される言語をカバーする多言語エンコーダーの家族を紹介します。我々のモデルは、多言語能力、数学、コーディングの幅広い範囲の諸多のタスクで、現在のものよりも優れています。また、8,192トークンの長さのシーケンスを原則的にサポートします。また、EuroBERTの設計決定についても検討し、データセットの構成とトレーニングパイプラインについてのフィードバックを提供します。EuroBERTモデルを公開し、中間的なトレーニングチェックポイントやトレーニングフレームワークを含めて公開します。",
      "upvotes": 26,
      "discussionId": "67ce9627e5cdfda52b9e88a4"
    },
    "publishedAt": "2025-03-10T03:42:45.848Z",
    "title": "EuroBERT: Scaling Multilingual Encoders for European Languages",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62be186a5f59ff2320e6e32b/NxwS9WJrRc9D3q9awbn_X.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05500.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "62be186a5f59ff2320e6e32b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62be186a5f59ff2320e6e32b/W_emoC2uItM-MJZyCfIKI.png",
      "fullname": "Nicolas-BZRD",
      "name": "Nicolas-BZRD",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.02130",
      "authors": [
        {
          "_id": "67cc697fa029f09af72cca01",
          "user": {
            "_id": "6694cc1009326cb83f2d11bb",
            "avatarUrl": "/avatars/1ddaaed70a16ac475a9404848aef5d48.svg",
            "isPro": false,
            "fullname": "Zhixuan Lin",
            "user": "zhixuan-lin",
            "type": "user"
          },
          "name": "Zhixuan Lin",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-03-08T16:00:21.933Z",
          "hidden": false
        },
        {
          "_id": "67cc697fa029f09af72cca02",
          "user": {
            "_id": "64234eadd654afd6931a288b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/UTU-XcO_ssKpIYr5MBujK.jpeg",
            "isPro": false,
            "fullname": "Evgenii Nikishin",
            "user": "nikishin",
            "type": "user"
          },
          "name": "Evgenii Nikishin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T09:54:09.945Z",
          "hidden": false
        },
        {
          "_id": "67cc697fa029f09af72cca03",
          "user": {
            "_id": "66906c4e37eadb9c577984d3",
            "avatarUrl": "/avatars/b81765472942fdf94c0ee885ca62df2d.svg",
            "isPro": false,
            "fullname": "Owen He",
            "user": "littleowen",
            "type": "user"
          },
          "name": "Xu Owen He",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-08T16:00:01.392Z",
          "hidden": false
        },
        {
          "_id": "67cc697fa029f09af72cca04",
          "name": "Aaron Courville",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-03T23:35:23.000Z",
      "title": "Forgetting Transformer: Softmax Attention with a Forget Gate",
      "summary": "現代のリカレントシーケンスモデルの重要な構成要素の1つは、忘れゲートです。Transformerは明示的なリカレント形式を持たないのに、データ依存的な方法で未正規化された注意スコアを下位にして忘れゲートを自然に機能的に挿入できることを示します。この注意機能を「忘れゲート注意」と名付け、その結果となるモデルを「忘れゲートTransformer（FoX）」と呼びます。FoXは、長文脈の言語モデリング、長文脈の長さ推計、短文脈の下流タスクでTransformerよりも優れていることを示し、同時に長文脈の下流タスクではTransformerと同等の性能を示します。また、FlashAttentionアルゴリズムと相容し、位置ベクトルは不要です。「ハイスタックのニードルテスト」などの分析を含め、FoXはリカレントシーケンスモデルのようなMamba-2、HGRN2、DeltaNetと比較してTransformerの優れた長文脈能力を維持していることが示されます。また、「Pro」ブロックデザインを導入し、リカレントシーケンスモデルにおける共通なアーキテクチャコンポーネントを挿入し、FoXおよびTransformerの両方の性能を大幅に向上させることを見出しました。コードは以下のURLから利用できます。\nhttps://github.com/zhixuan-lin/forgetting-transformer",
      "upvotes": 12,
      "discussionId": "67cc6981a029f09af72ccac1",
      "githubRepo": "https://github.com/zhixuan-lin/forgetting-transformer"
    },
    "publishedAt": "2025-03-09T22:02:39.842Z",
    "title": "Forgetting Transformer: Softmax Attention with a Forget Gate",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.02130.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6694cc1009326cb83f2d11bb",
      "avatarUrl": "/avatars/1ddaaed70a16ac475a9404848aef5d48.svg",
      "fullname": "Zhixuan Lin",
      "name": "zhixuan-lin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.05639",
      "authors": [
        {
          "_id": "67ce5ad85847e4787a82242d",
          "user": {
            "_id": "650447dd52ca06fef957f05d",
            "avatarUrl": "/avatars/511c11ac9b3cc7a162bda5e07f6ee0a3.svg",
            "isPro": true,
            "fullname": "Yuxuan BIAN",
            "user": "BianYx",
            "type": "user"
          },
          "name": "Yuxuan Bian",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-10T03:22:04.947Z",
          "hidden": false
        },
        {
          "_id": "67ce5ad85847e4787a82242e",
          "name": "Zhaoyang Zhang",
          "hidden": false
        },
        {
          "_id": "67ce5ad85847e4787a82242f",
          "user": {
            "_id": "62d4577bc85b0fcf7fde39bb",
            "avatarUrl": "/avatars/a3a5729e33ae89ce9ba408830db3c835.svg",
            "isPro": false,
            "fullname": "Xuan Ju",
            "user": "juxuan27",
            "type": "user"
          },
          "name": "Xuan Ju",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T09:57:06.955Z",
          "hidden": false
        },
        {
          "_id": "67ce5ad85847e4787a822430",
          "user": {
            "_id": "6374a02d0856ac905bfc6113",
            "avatarUrl": "/avatars/2cbe75c9cc818a647ca6e416f129c96f.svg",
            "isPro": false,
            "fullname": "Mingdeng Cao",
            "user": "Ljzycmd",
            "type": "user"
          },
          "name": "Mingdeng Cao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T09:56:45.412Z",
          "hidden": false
        },
        {
          "_id": "67ce5ad85847e4787a822431",
          "name": "Liangbin Xie",
          "hidden": false
        },
        {
          "_id": "67ce5ad85847e4787a822432",
          "user": {
            "_id": "63ca3ddc04c979828310bfcb",
            "avatarUrl": "/avatars/615e0d8622950b4408b40d550f02a894.svg",
            "isPro": false,
            "fullname": "Ying Shan",
            "user": "yshan2u",
            "type": "user"
          },
          "name": "Ying Shan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T09:56:34.001Z",
          "hidden": false
        },
        {
          "_id": "67ce5ad85847e4787a822433",
          "name": "Qiang Xu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-07T17:59:46.000Z",
      "title": "VideoPainter: ネーブル長さのビデオインプレイングとエディティングにプラグアンプレイング\nコンテキスト制御",
      "summary": "Video inpainting, バイオフィードバックを復元する目的でのビデオ処理技術は進歩しています。これらの進歩にもかかわらず、現在の方法では、光学フローと受容領域の先行知識を用いて隠れられた領域のピクセルを広げるか、または画像inpaintingモデルを時間的に延長することで、完全に隠れられたオブジェクトの生成や、背景のコンテキストの保存とフォロコントの生成の調和が難しい問題が残っています。これらの制限を解決するために、私たちは新しいダブルストリームパラダイムのVideoPainterを提案します。これは、6%のバックボーンパラメータを持つ効率的なコンテキストエンコーダーを組み込み、隠れられたビデオを処理し、バックボーンによる背景コンテキスト情報をビデオディーティープモデルに注入し、セマンティックに一貫した内容を生成することを目指しています。このアーキテクチャの分離は、学習複雑性を大幅に減少し、重要な背景コンテキストの複雑な統合を可能にします。また、我々は、ビデオinpaintingの実用的な応用を大幅に向上させるために、新しい目標領域IDのリサンプリングテクニックを導入し、その長いビデオも処理できるようにしました。また、現在の視覚理解モデルを利用してスケーラブルなデータセットパイプラインを構築し、VPDataとVPBenchを提供し、セグメンテーションベースのinpaintingの訓練と評価を促進し、現在まで最大のビデオinpaintingデータセットとベンチマークとして390K以上の多様なクリップを含むものを提供しました。inpaintingをパイプラインの基盤に、ビデオ編集とビデオ編集ペアデータの生成などの下流アプリケーションも検討し、競争的な性能と実用的な可能性を示しました。拡大的な実験は、8つのキーメトリクスでのVideoPainterの優れた性能を示し、それにより、ビデオinpaintingやビデオ編集の両方での効果的な応用を示しました。",
      "upvotes": 10,
      "discussionId": "67ce5adc5847e4787a822524",
      "projectPage": "https://yxbian23.github.io/project/video-painter/",
      "githubRepo": "https://github.com/TencentARC/VideoPainter"
    },
    "publishedAt": "2025-03-10T04:30:00.983Z",
    "title": "VideoPainter: Any-length Video Inpainting and Editing with Plug-and-Play Context Control",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/650447dd52ca06fef957f05d/VSg-Ti5epQJbVp20s1ILN.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05639.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "650447dd52ca06fef957f05d",
      "avatarUrl": "/avatars/511c11ac9b3cc7a162bda5e07f6ee0a3.svg",
      "fullname": "Yuxuan BIAN",
      "name": "BianYx",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.05592",
      "authors": [
        {
          "_id": "67ce5fd2e5cdfda52b9123a4",
          "user": {
            "_id": "66163dc8c7f45b3f893ff40b",
            "avatarUrl": "/avatars/801043dac0caae90bbca8c9d3e2e203b.svg",
            "isPro": false,
            "fullname": "Song Huatong",
            "user": "XXsongLALA",
            "type": "user"
          },
          "name": "Huatong Song",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T10:03:49.730Z",
          "hidden": false
        },
        {
          "_id": "67ce5fd2e5cdfda52b9123a5",
          "user": {
            "_id": "61b8405b516a20acdf3b85ff",
            "avatarUrl": "/avatars/3d2eae7c163a80b73260087b05a4230b.svg",
            "isPro": false,
            "fullname": "Jinhao Jiang",
            "user": "Boru",
            "type": "user"
          },
          "name": "Jinhao Jiang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T10:04:22.446Z",
          "hidden": false
        },
        {
          "_id": "67ce5fd2e5cdfda52b9123a6",
          "user": {
            "_id": "6703ac76ea890f0ca5b225eb",
            "avatarUrl": "/avatars/5f56c49a1940143d47dd484782a4abbf.svg",
            "isPro": false,
            "fullname": "Yingqian Min",
            "user": "EliverQ",
            "type": "user"
          },
          "name": "Yingqian Min",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-10T09:40:54.171Z",
          "hidden": false
        },
        {
          "_id": "67ce5fd2e5cdfda52b9123a7",
          "name": "Jie Chen",
          "hidden": false
        },
        {
          "_id": "67ce5fd2e5cdfda52b9123a8",
          "name": "Zhipeng Chen",
          "hidden": false
        },
        {
          "_id": "67ce5fd2e5cdfda52b9123a9",
          "name": "Wayne Xin Zhao",
          "hidden": false
        },
        {
          "_id": "67ce5fd2e5cdfda52b9123aa",
          "name": "Lei Fang",
          "hidden": false
        },
        {
          "_id": "67ce5fd2e5cdfda52b9123ab",
          "user": {
            "_id": "64b8c89052b7353d8c6a1013",
            "avatarUrl": "/avatars/cd59fffe81f6b07b4519540b8ff3d95f.svg",
            "isPro": false,
            "fullname": "Ji-Rong Wen",
            "user": "jrwen",
            "type": "user"
          },
          "name": "Ji-Rong Wen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T10:04:33.194Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-07T17:14:44.000Z",
      "title": "R1-Searcher: 強化学習によるLLMの検索能力の奨励",
      "summary": "現在の大規模推論モデル（LRMs）は、強化学習（RL）が大規模言語モデル（LLMs）の複雑な推論能力を強化する可能性を示しています。それらは数学やコーディングのような難しいタスクでは卓越した性能を示しますが、時間応急や知識密集型の質問に対しては、内部の知識を信じて問題を解決することを多く依存し、不十分であることがあります。これに対処し、私たちはR1-Searcherを提案します。これは新しい2段階の結果ベースのRLアプローチで、LLMsの検索能力を強化するために設計されています。この方法は、推論プロセス中に外部の検索システムを自動的に呼び出すことを可能にします。我々のフレームワークは、初期化時にプロセス報酬や煙霧化を必要としません。% この方法は、外れのデータセットにも効果的に一般化し、ベースモデルとインストラクションモデルをどちらにしてもサポートします。私たちの実験は、先行の強力なRAGメソッドよりも显著に優れていることを示し、閉源のGPT-4o-miniと比較しても同様です。",
      "upvotes": 9,
      "discussionId": "67ce5fd3e5cdfda52b912436",
      "githubRepo": "https://github.com/SsmallSong/R1-Searcher"
    },
    "publishedAt": "2025-03-09T23:43:27.151Z",
    "title": "R1-Searcher: Incentivizing the Search Capability in LLMs via Reinforcement Learning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05592.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6317
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.05638",
      "authors": [
        {
          "_id": "67ce8388764226f050ad18b3",
          "name": "Mark YU",
          "hidden": false
        },
        {
          "_id": "67ce8388764226f050ad18b4",
          "user": {
            "_id": "657a7458afbb0117ba15c59f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/657a7458afbb0117ba15c59f/8_iwTS1UG_mKnfylFbLsY.jpeg",
            "isPro": false,
            "fullname": "Wenbo Hu",
            "user": "wbhu-tc",
            "type": "user"
          },
          "name": "Wenbo Hu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-10T08:00:42.681Z",
          "hidden": false
        },
        {
          "_id": "67ce8388764226f050ad18b5",
          "user": {
            "_id": "64770e86d7cf39f2e937ae9a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64770e86d7cf39f2e937ae9a/pLqGg2z1KzQxCGpMwds-9.jpeg",
            "isPro": false,
            "fullname": "Jinbo Xing",
            "user": "Doubiiu",
            "type": "user"
          },
          "name": "Jinbo Xing",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-10T08:00:40.328Z",
          "hidden": false
        },
        {
          "_id": "67ce8388764226f050ad18b6",
          "user": {
            "_id": "63ca3ddc04c979828310bfcb",
            "avatarUrl": "/avatars/615e0d8622950b4408b40d550f02a894.svg",
            "isPro": false,
            "fullname": "Ying Shan",
            "user": "yshan2u",
            "type": "user"
          },
          "name": "Ying Shan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T10:06:56.510Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-07T17:57:53.000Z",
      "title": "トラジェクトライナー：ディフュージョンモデルをよってモノラルビデオのカメラトラジェクトをリダイレクトする",
      "summary": "こちらの英語テキストを日本語に翻訳します。\n\nここでは、TrajectoryCrafterという新しいアプローチを紹介します。これは、単目のビデオのカメラの軌跡を再定向するための方法です。確定的な視点変換とストロークな内容生成を分離し、ユーザー指定されたカメラの軌跡に厳密な制御を実現します。新しいダブルストリーム条件付きビデオディフュージョンモデルを提案します。これは、点云レンダリングと元のビデオを両方とも条件として統合し、正確な視点変換とコラーな4D内容生成を確保します。多視点のビデオを利用することではなく、ネットワークサイズの単目のビデオと静的な多視点データセットを組み合わせたハイブリッドトレーニングデータセットを作成します。これは、我々の独自のダブルリプロジェクションスタラテジーを通じて、多様なスケーンに対する強固な一般化を促進します。多視点と大規模な単目のビデオ上での拡張的な評価は、我々の方法の上位の性能を示します。",
      "upvotes": 8,
      "discussionId": "67ce838a764226f050ad1952",
      "projectPage": "https://trajectorycrafter.github.io/",
      "githubRepo": "https://github.com/TrajectoryCrafter/TrajectoryCrafter"
    },
    "publishedAt": "2025-03-10T02:24:39.763Z",
    "title": "TrajectoryCrafter: Redirecting Camera Trajectory for Monocular Videos via Diffusion Models",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/657a7458afbb0117ba15c59f/lpXbCmGz-upwRVSEUzBjV.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05638.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "657a7458afbb0117ba15c59f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/657a7458afbb0117ba15c59f/8_iwTS1UG_mKnfylFbLsY.jpeg",
      "fullname": "Wenbo Hu",
      "name": "wbhu-tc",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.05652",
      "authors": [
        {
          "_id": "67ce524ee969bc5fd69c9388",
          "user": {
            "_id": "61e9f5398c237a147a3f4ab5",
            "avatarUrl": "/avatars/afd4ec17cb132b5ab56e50a678c4786d.svg",
            "isPro": false,
            "fullname": "Yunfan Jiang",
            "user": "yunfanj",
            "type": "user"
          },
          "name": "Yunfan Jiang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-10T08:01:07.901Z",
          "hidden": false
        },
        {
          "_id": "67ce524ee969bc5fd69c9389",
          "name": "Ruohan Zhang",
          "hidden": false
        },
        {
          "_id": "67ce524ee969bc5fd69c938a",
          "name": "Josiah Wong",
          "hidden": false
        },
        {
          "_id": "67ce524ee969bc5fd69c938b",
          "name": "Chen Wang",
          "hidden": false
        },
        {
          "_id": "67ce524ee969bc5fd69c938c",
          "user": {
            "_id": "63509bc859bfa9a85d4220aa",
            "avatarUrl": "/avatars/ca2cc9b87f5ca5cd51606b2f9edf89d0.svg",
            "isPro": false,
            "fullname": "Yanjie Ze",
            "user": "yjze",
            "type": "user"
          },
          "name": "Yanjie Ze",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-10T08:01:05.242Z",
          "hidden": false
        },
        {
          "_id": "67ce524ee969bc5fd69c938d",
          "name": "Hang Yin",
          "hidden": false
        },
        {
          "_id": "67ce524ee969bc5fd69c938e",
          "name": "Cem Gokmen",
          "hidden": false
        },
        {
          "_id": "67ce524ee969bc5fd69c938f",
          "name": "Shuran Song",
          "hidden": false
        },
        {
          "_id": "67ce524ee969bc5fd69c9390",
          "name": "Jiajun Wu",
          "hidden": false
        },
        {
          "_id": "67ce524ee969bc5fd69c9391",
          "name": "Li Fei-Fei",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-07T18:15:21.000Z",
      "title": "ボーイアウトロボットシュート：日常家賃活動の全身操作をシンプルにする",
      "summary": "実世界的家庭仕事は、モバイル操作ロボットにとって重大な課題を見せます。現存するロボティクスベンチマークの分析から、成功した仕事の実行は、3つの主な全体的な身体制御能力に依存します：バイマニューコオダション、安定した且つ精密なナビゲーション、そして拡散的な端部エフェクターの可及性。これらの能力を達成するには、謹密なハードウェアデザインが必要ですが、その結果として、ビジョニムポリシー学習がより複雑になります。これらの課題に対処するために、私たちは、多様な家庭仕事での全体的な操作を行うための、厳密なフレームワークを介して、BEHAVIORロボットシュート（BRS）を紹介します。バイマニュー、ホーダーロボットとして構成され、4自由度のタロスを持つことで、BRSは、データの収集と全体的なテレオプロープラインエフェクターの収集を行うための、コストエフェクティブな全体的なテレオプロープラインエフェクターのインターフェースを統合し、新しいアルゴリズムを用いて、全体的なビジョニムポリシーの学習を行うためのアルゴリズムを統合しています。BRSは、3つの核心的な能力を強調しながら、長距離ナビゲーション、アーチュレートおよび可塑性の対象物との相互作用、そして狭い空間での操作など、追加の複雑性を導入した5つの難しい家庭仕事において評価されます。BRSの統合されたロボットエフェメバム、データの収集インターフェース、そして学習フレームワークが、日常的な家庭仕事の実世界的な全体的な操作を可能にするために、重要なステップとして認められています。BRSは、https://behavior-robot-suite.github.io/でオープンソースとして提供されています。",
      "upvotes": 7,
      "discussionId": "67ce5294e969bc5fd69c9a2c",
      "projectPage": "https://behavior-robot-suite.github.io/",
      "githubRepo": "https://github.com/behavior-robot-suite/brs-algo"
    },
    "publishedAt": "2025-03-09T22:51:04.616Z",
    "title": "BEHAVIOR Robot Suite: Streamlining Real-World Whole-Body Manipulation for Everyday Household Activities",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/61e9f5398c237a147a3f4ab5/WD3cV-QLiHRgh4IUOrikN.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05652.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "61e9f5398c237a147a3f4ab5",
      "avatarUrl": "/avatars/afd4ec17cb132b5ab56e50a678c4786d.svg",
      "fullname": "Yunfan Jiang",
      "name": "yunfanj",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.05379",
      "authors": [
        {
          "_id": "67ce5f2389663abdbc364495",
          "name": "Jiaxing Zhao",
          "hidden": false
        },
        {
          "_id": "67ce5f2389663abdbc364496",
          "name": "Xihan Wei",
          "hidden": false
        },
        {
          "_id": "67ce5f2389663abdbc364497",
          "name": "Liefeng Bo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-07T12:46:42.000Z",
      "title": "R1-Omni: 可解釈的オムニモダル感情認識と強化学習を用いた",
      "summary": "この研究では、感情認識のセンターでの可証明可能な報酬を持つ強化学習（RLVR）の最初の応用をOmni-multimodal大語言モデルに応用します。このタスクでは、視覚と音声のモデルが重要な役割を果たします。RLVRを活用してOmniモデルを最適化し、理由論能力、感情認識精度、一般化能力の3つのキー面での性能を大幅に向上させます。RLVRの導入は、分布内データの全体的な性能を向上させるだけでなく、分布外データセットでの優れた強固性を示します。より重要なのは、向上させた理由論能力が感情認識プロセスでの異なるモデルの貢献を明確に分析することができることです。これは、多モデル大語言モデルの最適化によくあるような貢献を提供します。",
      "upvotes": 6,
      "discussionId": "67ce5f2489663abdbc3644d0"
    },
    "publishedAt": "2025-03-09T23:40:46.906Z",
    "title": "R1-Omni: Explainable Omni-Multimodal Emotion Recognition with Reinforcing Learning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05379.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6317
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.04872",
      "authors": [
        {
          "_id": "67ce5deedb623d45a95deb72",
          "user": {
            "_id": "632c30576bcb864974cc40a8",
            "avatarUrl": "/avatars/96aa948ad1dd35d355e20b5765a2563a.svg",
            "isPro": false,
            "fullname": "sunlin",
            "user": "lincharliesun",
            "type": "user"
          },
          "name": "Lin Sun",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-10T08:00:47.601Z",
          "hidden": false
        },
        {
          "_id": "67ce5deedb623d45a95deb73",
          "name": "Guangxiang Zhao",
          "hidden": false
        },
        {
          "_id": "67ce5deedb623d45a95deb74",
          "name": "Xiaoqi Jian",
          "hidden": false
        },
        {
          "_id": "67ce5deedb623d45a95deb75",
          "name": "Yuhan Wu",
          "hidden": false
        },
        {
          "_id": "67ce5deedb623d45a95deb76",
          "name": "Weihong Lin",
          "hidden": false
        },
        {
          "_id": "67ce5deedb623d45a95deb77",
          "name": "Yongfu Zhu",
          "hidden": false
        },
        {
          "_id": "67ce5deedb623d45a95deb78",
          "name": "Change Jia",
          "hidden": false
        },
        {
          "_id": "67ce5deedb623d45a95deb79",
          "name": "Linglin Zhang",
          "hidden": false
        },
        {
          "_id": "67ce5deedb623d45a95deb7a",
          "name": "Jinzhu Wu",
          "hidden": false
        },
        {
          "_id": "67ce5deedb623d45a95deb7b",
          "name": "Junfeng Ran",
          "hidden": false
        },
        {
          "_id": "67ce5deedb623d45a95deb7c",
          "name": "Sai-er Hu",
          "hidden": false
        },
        {
          "_id": "67ce5deedb623d45a95deb7d",
          "name": "Zihan Jiang",
          "hidden": false
        },
        {
          "_id": "67ce5deedb623d45a95deb7e",
          "name": "Junting Zhou",
          "hidden": false
        },
        {
          "_id": "67ce5deedb623d45a95deb7f",
          "name": "Wenrui Liu",
          "hidden": false
        },
        {
          "_id": "67ce5deedb623d45a95deb80",
          "name": "Bin Cui",
          "hidden": false
        },
        {
          "_id": "67ce5deedb623d45a95deb81",
          "name": "Tong Yang",
          "hidden": false
        },
        {
          "_id": "67ce5deedb623d45a95deb82",
          "name": "Xiangzheng Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-06T16:25:53.000Z",
      "title": "TinyR1-32B-Preview: ブランチーム結合転移学習で精度向上",
      "summary": "LLMのサイズを減らしながら性能を維持する課題は関注を集めています。しかし、現在の手法では、モデルの煉熱とトランスファーム学習などが高い精度を達成することが難しいことが多いです。この制限を解決するために、Branch-Mergeの煉熱アプローチを提案します。このアプローチは、2つのステップでモデルの圧縮を強化します。まずは、(1) Branch Phaseで、大きな教師モデルからの知識を選択的に、領域専門的な観察学習(SFT)を通じて、特別化された学生モデルに煉熱させます。次に、(2) Merge Phaseで、これらの学生モデルを統合し、領域間の知識移行を可能にし、一般化を向上させます。DeepSeek-R1を教師モデルとし、DeepSeek-R1-Distill-Qwen-32Bを学生モデルとして、この煉熱アプローチを検証しました。その結果、合成されたモデルであるTinyR1-32B-Previewは、数学(+5.5点)、コーディング(+4.4点)、科学(+2.9点)の複数のベンチマークで、DeepSeek-R1-Distill-Qwen-32Bよりも優れています。また、AIME 2024ではDeepSeek-R1と近い性能を達成しました。Branch-Mergeの煉熱アプローチは、計算コストと時間を減らしながら、小さくなった高性能のLLMの作成にスケーラブルな解決策を提供します。",
      "upvotes": 5,
      "discussionId": "67ce5df0db623d45a95dec1f"
    },
    "publishedAt": "2025-03-09T23:35:58.424Z",
    "title": "TinyR1-32B-Preview: Boosting Accuracy with Branch-Merge Distillation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04872.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6317
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.05447",
      "authors": [
        {
          "_id": "67ceab4132a6585cecad2c36",
          "user": {
            "_id": "6246bb33da617c00b48e4d92",
            "avatarUrl": "/avatars/0304a9f6eb7f5dee4d933d03222f94e9.svg",
            "isPro": false,
            "fullname": "Weigao Sun",
            "user": "weigao266",
            "type": "user"
          },
          "name": "Weigao Sun",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-03-10T09:09:22.436Z",
          "hidden": false
        },
        {
          "_id": "67ceab4132a6585cecad2c37",
          "name": "Disen Lan",
          "hidden": false
        },
        {
          "_id": "67ceab4132a6585cecad2c38",
          "name": "Tong Zhu",
          "hidden": false
        },
        {
          "_id": "67ceab4132a6585cecad2c39",
          "name": "Xiaoye Qu",
          "hidden": false
        },
        {
          "_id": "67ceab4132a6585cecad2c3a",
          "name": "Yu Cheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-07T14:17:45.000Z",
      "title": "Linear-MoE: 線形シーケンスモデリングとミックスオブエキスポーダーの融合",
      "summary": "線形シーケンスモデリング（LSM）というような線形アタチン、状態空間モデル、線形RNN、そしてミックスオブエクスプレッショナー（MoE）が最近重要なアーキテクチャ改善として広く採用されています。本論文では、LSMとMoEを統合した大規模モデルのモデリングと訓練のための生産レベルシステム「Linear-MoE」を紹介します。Linear-MoEは、線形複雑度のシーケンスモデリングの優れた性能と稀疏な活性化を実現するMoE層の利点を活用し、効率的な訓練により高い性能を提供します。Linear-MoEシステムは、1) モデリングサブシステムと2) 訓練サブシステムから構成されています。モデリングサブシステムは、すべてのLSMのインスタンスをサポートする統一的なフレームワークを提供し、訓練サブシステムは、進捗的な並列化テクノロジーを採用して効率的な訓練を促進します。特に、Linear-MoEモデルに向けて設計されたシーケンスパラレリズムを採用しています。また、Linear-MoE層と標準的なTransformer-MoE層を組み合わせたハイブリッドモデルも検討し、そのシーケンスパラレリズムを用いてモデルの柔軟性と性能を進めることを試みています。A0.3B-2BとA1B-7Bの2つのモデルシリーズに対する評価は、Linear-MoEが訓練の効率を向上させながら、多様なベンチマークで競争的な性能を維持することを示し、次世代の基盤モデルアーキテクチャの可能性を示しています。コードは、https://github.com/OpenSparseLLMs/Linear-MoE。",
      "upvotes": 3,
      "discussionId": "67ceab4232a6585cecad2c82",
      "githubRepo": "https://github.com/OpenSparseLLMs/Linear-MoE"
    },
    "publishedAt": "2025-03-10T05:05:22.522Z",
    "title": "Linear-MoE: Linear Sequence Modeling Meets Mixture-of-Experts",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05447.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6246bb33da617c00b48e4d92",
      "avatarUrl": "/avatars/0304a9f6eb7f5dee4d933d03222f94e9.svg",
      "fullname": "Weigao Sun",
      "name": "weigao266",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.01713",
      "authors": [
        {
          "_id": "67c75e18cb29e2a4b0eb0293",
          "user": {
            "_id": "66c0a08bac74db25de8427ec",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c0a08bac74db25de8427ec/9D-piDBZqSt6KNkHImmkv.jpeg",
            "isPro": false,
            "fullname": "Jintao Zhang",
            "user": "jt-zhang",
            "type": "user"
          },
          "name": "Jintao Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-07T09:26:50.367Z",
          "hidden": false
        },
        {
          "_id": "67c75e18cb29e2a4b0eb0294",
          "name": "Guoliang Li",
          "hidden": false
        },
        {
          "_id": "67c75e18cb29e2a4b0eb0295",
          "name": "Jinyang Su",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-03T16:25:58.000Z",
      "title": "SAGE: 適切なレビューのフレームワーク",
      "summary": "レタイブレーディングアウガイナション（RAG）は、特定のコーパス内での質問回答（QA）タスクに関して显著な効果を示しています。しかし、RAGのQAでは多くの失敗例が存在しています。これらの失敗は、大規模言語モデル（LLMs）の制限だけでなく、以下の2つの制限によって主に原因となっています：1.現在のRAG手法は、コーパスをセマンティクスを考慮しないように分割しているため、問題とセグメントの間の関連性が損なわれ、関連性のあるコンテキストを特定することが難しくなります。2.コンテキストの量とエキサクタブルなコンテキストの量の間に転換があります。この論文では、これらの制限を克服するためにRAGフレームワーク（SAGE）を紹介します。まず、セマンティクスを考慮しない分割問題を解決するために、セマンティクス分割モデルを提案します。このモデルは、コーパスをセマンティクス的に完全なコンテキストに分割するように訓練されています。2.最適なコンテキストのみを取得し、関係ないコンテキストを無視することを確保するために、関連性スコアの減少速度に基づいてコンテキストを動的に選択するコンテキスト選択アルゴリズムを設計します。3.コンテキストの精度を進めるために、LLMsが取得したコンテキストが過剰か不足しているかを評価し、その量を調整することを提案します。実験は、SAGEはQAの品質において平均で61.25%の改善率を示しています。また、ノイズのあるコンテキストの取得を避けることで、LLM推論で使用されるトークンのコストを下げ、平均で49.41%のコスト効率向上を実現します。また、我々の研究は、RAGの強化における有價値な見解を提供しています。",
      "upvotes": 3,
      "discussionId": "67c75e1ccb29e2a4b0eb03a9"
    },
    "publishedAt": "2025-03-10T03:23:51.482Z",
    "title": "SAGE: A Framework of Precise Retrieval for RAG",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01713.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66c0a08bac74db25de8427ec",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c0a08bac74db25de8427ec/9D-piDBZqSt6KNkHImmkv.jpeg",
      "fullname": "Jintao Zhang",
      "name": "jt-zhang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.05315",
      "authors": [
        {
          "_id": "67ce6db07110b8bedb3344a7",
          "name": "Saumya Chaturvedi",
          "hidden": false
        },
        {
          "_id": "67ce6db07110b8bedb3344a8",
          "user": {
            "_id": "63a4754927f1f64ed7238dac",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
            "isPro": false,
            "fullname": "Aman Chadha",
            "user": "amanchadha",
            "type": "user"
          },
          "name": "Aman Chadha",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-10T08:00:45.150Z",
          "hidden": false
        },
        {
          "_id": "67ce6db07110b8bedb3344a9",
          "name": "Laurent Bindschaedler",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-07T10:50:45.000Z",
      "title": "LoRACode: LoRA アダプターズ for Code エンベディング",
      "summary": "コード埋め込みは、セマンティックなコード検索に重要ですが、現在のアプローチは、コードに固有した正確な句法的およびコンテキスト的なニュアンスを捉えやすいようには難しいことが多いです。Open-sourceモデルのコードBERTやUniXcoderは、スケーラビリティと効率性において制限があり、高性能のプロプライターシステムは、計算費用を大幅に課金します。私たちは、Low-Rank Adaptation（LoRA）に基づくパラメータ効率的な微調節方法を導入し、コード検索に適用可能なタスク専用アダプターを構築します。我々のアプローチは、基盤モデルの学習パラメータ数を2%以下に抑え、2つのH100GPUで25分で200万サンプルの極めて幅広いコードコーパスに効率的に微調節できます。実験は、Code2Code検索のMean Reciprocal Rank（MRR）が最大9.1%増加し、多くのプログラミング言語でのText2Code検索タスクでは最大86.69%増加しました。タスクごとおよび言語ごとの適応性の差異をもって、コード検索における句法的および言語的な変化の敏感性を調査します。",
      "upvotes": 2,
      "discussionId": "67ce6db17110b8bedb3344c5"
    },
    "publishedAt": "2025-03-10T00:51:02.203Z",
    "title": "LoRACode: LoRA Adapters for Code Embeddings",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05315.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a4754927f1f64ed7238dac",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
      "fullname": "Aman Chadha",
      "name": "amanchadha",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.05132",
      "authors": [
        {
          "_id": "67ce5ec17c6e6ea1cc5649c2",
          "name": "Hengguang Zhou",
          "hidden": false
        },
        {
          "_id": "67ce5ec17c6e6ea1cc5649c3",
          "name": "Xirui Li",
          "hidden": false
        },
        {
          "_id": "67ce5ec17c6e6ea1cc5649c4",
          "name": "Ruochen Wang",
          "hidden": false
        },
        {
          "_id": "67ce5ec17c6e6ea1cc5649c5",
          "name": "Minhao Cheng",
          "hidden": false
        },
        {
          "_id": "67ce5ec17c6e6ea1cc5649c6",
          "name": "Tianyi Zhou",
          "hidden": false
        },
        {
          "_id": "67ce5ec17c6e6ea1cc5649c7",
          "name": "Cho-Jui Hsieh",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-07T04:21:47.000Z",
      "title": "R1-Zeroの「Ahaモーメント」、2B非SFTモデル上での可視化理由論の研究",
      "summary": "最近、DeepSeek R1は、簡単なルールベースの報酬を用いた強化学習が、大規模な言語モデルにおける複雑な理由論を自動的に開発することを示し、「エイホーモーメント」という特徴を持つことを示した。しかし、この成功を多めの理由論に拡張する試みは、これらの重要な特徴を再現することに失敗した。本報告では、非SFT 2Bモデルでのこの出現した特徴の最初の成功再現を報告します。Qwen2-VL-2Bを始め、SATデータセットに直接強化学習を適用し、CVBenchでは59.47%の精度を達成し、基礎モデルより約30%以上の改善を収め、SFT設定より約2%以上の改善を収めました。また、R1モデルのような理由論を実現するための強化学習を使用した試みの失敗とフィードバックを共有し、課題を明らかにするために試みました。主要な見解には、(1)強化学習を導入された指示モデルにおいて、論理の軌跡が軽くなることが多いと、(2)ランダムな長さ報酬は理由論の能力を引き出すことに効果がないというものがあります。プロジェクトコードは、https://github.com/turningpoint-ai/VisualThinker-R1-Zero に公開されています。",
      "upvotes": 2,
      "discussionId": "67ce5ec27c6e6ea1cc564a01"
    },
    "publishedAt": "2025-03-09T23:39:12.374Z",
    "title": "R1-Zero's \"Aha Moment\" in Visual Reasoning on a 2B Non-SFT Model",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05132.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6317
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.04808",
      "authors": [
        {
          "_id": "67ce5c7065b141ae6b0d3957",
          "name": "Stephen Chung",
          "hidden": false
        },
        {
          "_id": "67ce5c7065b141ae6b0d3958",
          "name": "Wenyu Du",
          "hidden": false
        },
        {
          "_id": "67ce5c7065b141ae6b0d3959",
          "name": "Jie Fu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-04T02:53:39.000Z",
      "title": "学習失敗からの多回試行学習",
      "summary": "最近の大語言モデル（LLM）の強化学習（RL）の進展、DeepSeek R1を例にして、簡単な質問回答タスクでもLLMの理由能力を大幅に向上させることが示されています。本稿では、このアプローチを拡張し、多タイプ設定に変更します。問題に対して1つの回答を生成するよりは、モデルは誤答後にフィードバックを提供されるように、複数の試行を行うことができます。多タイプタスクは、モデルが前の試行を改良し、検索エフィシェンスを向上させるように促します。実験結果によると、数学ベンチマークで評価されたとき、1タイプでは45.6%から2タイプでは52.5%までの精度が大幅に向上します。対照的に、同じLLMが標準の1タイプタスクで訓練された場合、評価時にもより多くの試行を与えるとそれほどの改善が見られます。結果は、標準の1タイプタスクに比べて、多タイプタスクで訓練されたLLMは数学ベンチマークでよりも少しより良い性能を達成し、同時にユーザーのフィードバックに基づいてより効果的に回答を改良することを学ぶことがわかります。完全なコードは、https://github.com/DualityRL/multi-attempt にあります。",
      "upvotes": 2,
      "discussionId": "67ce5c7165b141ae6b0d39c6"
    },
    "publishedAt": "2025-03-09T23:29:35.505Z",
    "title": "Learning from Failures in Multi-Attempt Reinforcement Learning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04808.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6317
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.04548",
      "authors": [
        {
          "_id": "67cbff8e4dedec48bdec8a99",
          "name": "Zhipeng Chen",
          "hidden": false
        },
        {
          "_id": "67cbff8e4dedec48bdec8a9a",
          "user": {
            "_id": "6703ac76ea890f0ca5b225eb",
            "avatarUrl": "/avatars/5f56c49a1940143d47dd484782a4abbf.svg",
            "isPro": false,
            "fullname": "Yingqian Min",
            "user": "EliverQ",
            "type": "user"
          },
          "name": "Yingqian Min",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-10T08:02:04.349Z",
          "hidden": false
        },
        {
          "_id": "67cbff8e4dedec48bdec8a9b",
          "name": "Beichen Zhang",
          "hidden": false
        },
        {
          "_id": "67cbff8e4dedec48bdec8a9c",
          "name": "Jie Chen",
          "hidden": false
        },
        {
          "_id": "67cbff8e4dedec48bdec8a9d",
          "name": "Jinhao Jiang",
          "hidden": false
        },
        {
          "_id": "67cbff8e4dedec48bdec8a9e",
          "name": "Daixuan Cheng",
          "hidden": false
        },
        {
          "_id": "67cbff8e4dedec48bdec8a9f",
          "name": "Wayne Xin Zhao",
          "hidden": false
        },
        {
          "_id": "67cbff8e4dedec48bdec8aa0",
          "name": "Zheng Liu",
          "hidden": false
        },
        {
          "_id": "67cbff8e4dedec48bdec8aa1",
          "name": "Xu Miao",
          "hidden": false
        },
        {
          "_id": "67cbff8e4dedec48bdec8aa2",
          "name": "Yang Lu",
          "hidden": false
        },
        {
          "_id": "67cbff8e4dedec48bdec8aa3",
          "name": "Lei Fang",
          "hidden": false
        },
        {
          "_id": "67cbff8e4dedec48bdec8aa4",
          "name": "Zhongyuan Wang",
          "hidden": false
        },
        {
          "_id": "67cbff8e4dedec48bdec8aa5",
          "name": "Ji-Rong Wen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-06T15:34:27.000Z",
      "title": "「R1モデルの誘発と向上に関する実験的研究」",
      "summary": "このレポートでは、STILLプロジェクトの一部としてのスローティニングモデルの開発に関する第三タイプの技術レポートを提供します。技術のパターンが明確になるにつれ、RL訓練のスケーリングはこのような論理モデルの実装の中心的な技術となりました。RL訓練に影響する様々な要因の効果をシステマティックに実験し、記録しました。基礎モデルと微調節モデルの両方に対して実験を行いました。特に、私たちのRL訓練アプローチがQwen2.5-32Bの基礎モデルを一貫して改善し、回答の長さとテスト精度を向上させることを示しました。また、DeepSeek-R1-Distill-Qwen-1.5Bのようなモデルが高い性能レベルを達成している場合も、RL訓練によって進化させ、AIME 2024で39.33%の精度を達成することを示しました。RL訓練を超えて、ツール操作の使用も検討し、大規模な論理モデルの論理性能を大幅に向上させることを見出しました。このアプローチは、AIME 2024でgreedy searchを使用して86.67%の精度を達成し、モデルの能力を向上させる効果性を明らかにしました。STILLプロジェクトのウェブサイトでリソースを公開しています：https://github.com/RUCAIBox/Slow_Thinking_with_LLMs。",
      "upvotes": 1,
      "discussionId": "67cbff8f4dedec48bdec8af3"
    },
    "publishedAt": "2025-03-10T05:23:26.375Z",
    "title": "An Empirical Study on Eliciting and Improving R1-like Reasoning Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04548.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6703ac76ea890f0ca5b225eb",
      "avatarUrl": "/avatars/5f56c49a1940143d47dd484782a4abbf.svg",
      "fullname": "Yingqian Min",
      "name": "EliverQ",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.04504",
      "authors": [
        {
          "_id": "67cb8e882cfa481bcee9455e",
          "user": {
            "_id": "66a07c07b7f0bb64d3b35497",
            "avatarUrl": "/avatars/c38af1ddb7a5b625e26b7ff05957ff7c.svg",
            "isPro": false,
            "fullname": "SunghyunAhn",
            "user": "SkiddieAhn",
            "type": "user"
          },
          "name": "Sunghyun Ahn",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-10T08:02:12.798Z",
          "hidden": false
        },
        {
          "_id": "67cb8e882cfa481bcee9455f",
          "user": {
            "_id": "673d7b70713e4b8db2d5ca94",
            "avatarUrl": "/avatars/b9e89eba62eb939ddd93c1cb91744e93.svg",
            "isPro": false,
            "fullname": "Youngwan Jo",
            "user": "jyy1551",
            "type": "user"
          },
          "name": "Youngwan Jo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-10T08:02:08.710Z",
          "hidden": false
        },
        {
          "_id": "67cb8e882cfa481bcee94560",
          "name": "Kijung Lee",
          "hidden": false
        },
        {
          "_id": "67cb8e882cfa481bcee94561",
          "name": "Sein Kwon",
          "hidden": false
        },
        {
          "_id": "67cb8e882cfa481bcee94562",
          "name": "Inpyo Hong",
          "hidden": false
        },
        {
          "_id": "67cb8e882cfa481bcee94563",
          "name": "Sanghyun Park",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-06T14:52:34.000Z",
      "title": "AnyAnomaly: LVLMを用いたゼロショットでのカスタマイズ可能なビデオ異常検出",
      "summary": "ビデオ異常検出（VAD）は、コンピュータビジョンのビデオ分析と視聴衛生に重要です。しかし、現在のVADモデルは学習された正常パターンに依存しているため、多様な環境に適用することが難しい。その結果、ユーザーは新しい環境に対応するためにモデルを再学習したり、別のAIモデルを開発する必要があります。これには機械学習の専門知識、高性能のハードウェア、および拡張されたデータの集め方が必要で、VADの実用的な利用可能なものに限ります。これらの課題に対処するために、本研究では、カスタマイズ可能なビデオ異常検出（C-VAD）テクニックとAnyAnomalyモデルを提案します。C-VADは、ユーザー定義されたテキストを異常事件として扱い、ビデオ内の特定のイベントを含むフレームを検出します。AnyAnomalyは、大規模なビジョン言語モデルの微調節を除き、コンテキストに関する視覚問答によって効果的に実装されました。提案されたモデルの有効性を証明するために、C-VADデータセットを構築し、AnyAnomalyの優越性を示しました。また、我々のアプローチはVADベンチマークデータセットにおいて競合的な性能を示し、UBnormalデータセットで最先端の結果を達成し、すべてのデータセットの拡張性において他の方法を超えました。我々のコードは、github.com/SkiddieAhn/Paper-AnyAnomalyで公開されています。",
      "upvotes": 0,
      "discussionId": "67cb8e8a2cfa481bcee945cd",
      "githubRepo": "https://github.com/SkiddieAhn/Paper-AnyAnomaly"
    },
    "publishedAt": "2025-03-10T04:06:49.447Z",
    "title": "AnyAnomaly: Zero-Shot Customizable Video Anomaly Detection with LVLM",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/66a07c07b7f0bb64d3b35497/TNPrQD3FdFBVVW2pUoEnU.gif",
      "https://cdn-uploads.huggingface.co/production/uploads/66a07c07b7f0bb64d3b35497/cA30QnSF_7AeHciWhCFSN.gif",
      "https://cdn-uploads.huggingface.co/production/uploads/66a07c07b7f0bb64d3b35497/T9EwE4Ea7DrH3XVZ_eJ1g.gif",
      "https://cdn-uploads.huggingface.co/production/uploads/66a07c07b7f0bb64d3b35497/Duuqk_Ph1GNfz6HW2Qv5g.gif",
      "https://cdn-uploads.huggingface.co/production/uploads/66a07c07b7f0bb64d3b35497/-Dze7JwIaBTbCfXUetsCd.gif"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04504.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66a07c07b7f0bb64d3b35497",
      "avatarUrl": "/avatars/c38af1ddb7a5b625e26b7ff05957ff7c.svg",
      "fullname": "SunghyunAhn",
      "name": "SkiddieAhn",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]