[
  {
    "paper": {
      "id": "2503.18878",
      "authors": [
        {
          "_id": "67e25fe88e6c927eb7794abd",
          "name": "Andrey Galichin",
          "hidden": false
        },
        {
          "_id": "67e25fe88e6c927eb7794abe",
          "user": {
            "_id": "60cd95ee15ecba5f2200304a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60cd95ee15ecba5f2200304a/3gMYeWm8wQO5KfqE5RmEe.jpeg",
            "isPro": false,
            "fullname": "Alexey Dontsov",
            "user": "therem",
            "type": "user"
          },
          "name": "Alexey Dontsov",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-25T08:18:43.467Z",
          "hidden": false
        },
        {
          "_id": "67e25fe88e6c927eb7794abf",
          "name": "Polina Druzhinina",
          "hidden": false
        },
        {
          "_id": "67e25fe88e6c927eb7794ac0",
          "user": {
            "_id": "6172aaeec8e66e2aa84c06b9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6172aaeec8e66e2aa84c06b9/ZdRZSp3P1SU6CIDbvQwkv.jpeg",
            "isPro": false,
            "fullname": "Anton Razzhigaev",
            "user": "razzant",
            "type": "user"
          },
          "name": "Anton Razzhigaev",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-25T09:05:58.409Z",
          "hidden": false
        },
        {
          "_id": "67e25fe88e6c927eb7794ac1",
          "name": "Oleg Y. Rogov",
          "hidden": false
        },
        {
          "_id": "67e25fe88e6c927eb7794ac2",
          "user": {
            "_id": "662f8d645c4db70c77a203b0",
            "avatarUrl": "/avatars/72f9a3c39b3ba5114388d16a35524835.svg",
            "isPro": false,
            "fullname": "Elena Tutubalina",
            "user": "tlenusik",
            "type": "user"
          },
          "name": "Elena Tutubalina",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-25T09:05:56.401Z",
          "hidden": false
        },
        {
          "_id": "67e25fe88e6c927eb7794ac3",
          "name": "Ivan Oseledets",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-24T16:54:26.000Z",
      "submittedOnDailyAt": "2025-03-25T06:45:16.781Z",
      "title": "ここですべての基礎を覆いました：Sparse Autoencodersを用いて大規模言語モデルの理由論的特徴を解釈する",
      "submittedOnDailyBy": {
        "_id": "60cd95ee15ecba5f2200304a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60cd95ee15ecba5f2200304a/3gMYeWm8wQO5KfqE5RmEe.jpeg",
        "isPro": false,
        "fullname": "Alexey Dontsov",
        "user": "therem",
        "type": "user"
      },
      "summary": "大語言モデル（LLMs）は自然言語処理において驚異的な成功を収めています。最近の進歩は、新しいクラスの理由論的なLLMsの開発につながりました。例えば、オープンソースのDeepSeek-R1は深い思考と複雑な理由論を統合して最先端の性能を達成しました。これらの驚異的な能力に関しては、このようなモデルの内部の理由論的機構はまだ調べられていません。本稿では、Sparse Autoencoders（SAEs）を用いて、ニューラルネットワークの潜在表現を読解可能な特徴量に分解する方法であることを用いて、DeepSeek-R1シリーズのモデルの理由論における特徴量を識別します。まず、SAE表現から候補の「理由論的特徴量」を抽出するアプローチを提案します。これらの特徴量を実験的な分析と解釈可能な方法で検証し、モデルの理由論的性能に直接的な関連性を示します。重要なことに、これらの特徴量をシステマティックに制御することで理由論的性能を向上させ、LLMsの理由論について最初の機構的な解説を提供します。コードは、https://github.com/AIRI-Institute/SAE-Reasoning から利用できます。",
      "upvotes": 63,
      "discussionId": "67e25fea8e6c927eb7794b25",
      "ai_keywords": [
        "Sparse Autoencoders (SAEs)",
        "latent representations",
        "interpretable features",
        "reasoning features",
        "empirical analysis",
        "interpretability methods",
        "systematic enhancement"
      ]
    },
    "publishedAt": "2025-03-24T12:54:26.000Z",
    "title": "I Have Covered All the Bases Here: Interpreting Reasoning Features in\n  Large Language Models via Sparse Autoencoders",
    "summary": "Large Language Models (LLMs) have achieved remarkable success in natural\nlanguage processing. Recent advances have led to the developing of a new class\nof reasoning LLMs; for example, open-source DeepSeek-R1 has achieved\nstate-of-the-art performance by integrating deep thinking and complex\nreasoning. Despite these impressive capabilities, the internal reasoning\nmechanisms of such models remain unexplored. In this work, we employ Sparse\nAutoencoders (SAEs), a method to learn a sparse decomposition of latent\nrepresentations of a neural network into interpretable features, to identify\nfeatures that drive reasoning in the DeepSeek-R1 series of models. First, we\npropose an approach to extract candidate ''reasoning features'' from SAE\nrepresentations. We validate these features through empirical analysis and\ninterpretability methods, demonstrating their direct correlation with the\nmodel's reasoning abilities. Crucially, we demonstrate that steering these\nfeatures systematically enhances reasoning performance, offering the first\nmechanistic account of reasoning in LLMs. Code available at\nhttps://github.com/AIRI-Institute/SAE-Reasoning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18878.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60cd95ee15ecba5f2200304a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60cd95ee15ecba5f2200304a/3gMYeWm8wQO5KfqE5RmEe.jpeg",
      "fullname": "Alexey Dontsov",
      "name": "therem",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.17359",
      "authors": [
        {
          "_id": "67e16a266280a70b45b8a16c",
          "user": {
            "_id": "64105a6d14215c0775dfdd14",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64105a6d14215c0775dfdd14/-VX-cUYOLjHIg7QnWhRGG.jpeg",
            "isPro": false,
            "fullname": "Jiwen Yu",
            "user": "VictorYuki",
            "type": "user"
          },
          "name": "Jiwen Yu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:05:33.251Z",
          "hidden": false
        },
        {
          "_id": "67e16a266280a70b45b8a16d",
          "name": "Yiran Qin",
          "hidden": false
        },
        {
          "_id": "67e16a266280a70b45b8a16e",
          "user": {
            "_id": "652404d0050781c16f1c51b0",
            "avatarUrl": "/avatars/4ad62f2c65406dd0af36c6d0697ae599.svg",
            "isPro": false,
            "fullname": "Haoxuan Che",
            "user": "chehx",
            "type": "user"
          },
          "name": "Haoxuan Che",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:06:13.592Z",
          "hidden": false
        },
        {
          "_id": "67e16a266280a70b45b8a16f",
          "name": "Quande Liu",
          "hidden": false
        },
        {
          "_id": "67e16a266280a70b45b8a170",
          "user": {
            "_id": "60e272ca6c78a8c122b12127",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60e272ca6c78a8c122b12127/xldEGBzGrU-bX6IwAw0Ie.jpeg",
            "isPro": false,
            "fullname": "Xintao Wang",
            "user": "Xintao",
            "type": "user"
          },
          "name": "Xintao Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:06:29.181Z",
          "hidden": false
        },
        {
          "_id": "67e16a266280a70b45b8a171",
          "name": "Pengfei Wan",
          "hidden": false
        },
        {
          "_id": "67e16a266280a70b45b8a172",
          "user": {
            "_id": "644c8324f02250233d0d67d9",
            "avatarUrl": "/avatars/feb39d281457c1750f3eada3c060a23e.svg",
            "isPro": false,
            "fullname": "Di Zhang",
            "user": "dizhang",
            "type": "user"
          },
          "name": "Di Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:06:50.971Z",
          "hidden": false
        },
        {
          "_id": "67e16a266280a70b45b8a173",
          "user": {
            "_id": "65d5ec74cd05bc1eaa125040",
            "avatarUrl": "/avatars/2de1b1539a86452c2c89570eeb02f5ab.svg",
            "isPro": false,
            "fullname": "Xihui Liu",
            "user": "XihuiLiu",
            "type": "user"
          },
          "name": "Xihui Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:06:56.864Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-21T17:59:22.000Z",
      "submittedOnDailyAt": "2025-03-25T01:42:15.880Z",
      "title": "Interactive Generative Video as Next-Generation Game Engine",
      "submittedOnDailyBy": {
        "_id": "64105a6d14215c0775dfdd14",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64105a6d14215c0775dfdd14/-VX-cUYOLjHIg7QnWhRGG.jpeg",
        "isPro": false,
        "fullname": "Jiwen Yu",
        "user": "VictorYuki",
        "type": "user"
      },
      "summary": "現代のゲーム開発は、創造性とコストに関する大きな課題を抱えています。従来のゲームエンジンでは、決定的な内容があるためです。最近、写真生成モデルにおける進歩が、実写的で相互作用可能な虚像環境を合成することができることを示し、ゲームの作成を革命的に変える機会を提供しています。この立場論文では、Interactive Generative Video (IGV) を Generative Game Engines (GGE) の基盤として提案し、次世代ゲームで無限に新しいコンテンツを生成できるようにすることを目指しています。GGEは、IGVの無限な高品質なコンテンツ合成、物理的知識を持つ世界モデリング、ユーザー制御可能な相互作用、長期記憶能力、因果推理の特徴を活用しています。GGEの核心モジュールを詳細に説明し、進化をガイドするための分階段成熟マップ（L0-L4）を提出します。私たちの仕事は、AI時代のゲーム開発に新たな道を導き、将来、AIドライバーの生成システムがゲームの作成と体験を根本的に変形する未来を想像しています。",
      "upvotes": 47,
      "discussionId": "67e16a276280a70b45b8a214",
      "ai_keywords": [
        "Interactive Generative Video (IGV)",
        "Generative Game Engines (GGE)",
        "video generation models",
        "high-quality content synthesis",
        "physics-aware world modeling",
        "user-controlled interactivity",
        "long-term memory capabilities",
        "causal reasoning",
        "hierarchical maturity roadmap (L0-L4)"
      ]
    },
    "publishedAt": "2025-03-21T13:59:22.000Z",
    "title": "Position: Interactive Generative Video as Next-Generation Game Engine",
    "summary": "Modern game development faces significant challenges in creativity and cost\ndue to predetermined content in traditional game engines. Recent breakthroughs\nin video generation models, capable of synthesizing realistic and interactive\nvirtual environments, present an opportunity to revolutionize game creation. In\nthis position paper, we propose Interactive Generative Video (IGV) as the\nfoundation for Generative Game Engines (GGE), enabling unlimited novel content\ngeneration in next-generation gaming. GGE leverages IGV's unique strengths in\nunlimited high-quality content synthesis, physics-aware world modeling,\nuser-controlled interactivity, long-term memory capabilities, and causal\nreasoning. We present a comprehensive framework detailing GGE's core modules\nand a hierarchical maturity roadmap (L0-L4) to guide its evolution. Our work\ncharts a new course for game development in the AI era, envisioning a future\nwhere AI-powered generative systems fundamentally reshape how games are created\nand experienced.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17359.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64105a6d14215c0775dfdd14",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64105a6d14215c0775dfdd14/-VX-cUYOLjHIg7QnWhRGG.jpeg",
      "fullname": "Jiwen Yu",
      "name": "VictorYuki",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.18942",
      "authors": [
        {
          "_id": "67e226039cd910bee045e38f",
          "user": {
            "_id": "6505a02f9310ce8c400edc63",
            "avatarUrl": "/avatars/bbf781594fc8c812316711aa8e2797aa.svg",
            "isPro": false,
            "fullname": "Fangfu Liu",
            "user": "Liuff23",
            "type": "user"
          },
          "name": "Fangfu Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:07:42.279Z",
          "hidden": false
        },
        {
          "_id": "67e226039cd910bee045e390",
          "name": "Hanyang Wang",
          "hidden": false
        },
        {
          "_id": "67e226039cd910bee045e391",
          "name": "Yimo Cai",
          "hidden": false
        },
        {
          "_id": "67e226039cd910bee045e392",
          "user": {
            "_id": "60bc94cd85a3ab33829b6211",
            "avatarUrl": "/avatars/b57d36c7577fbbb42ea5b963eef4144a.svg",
            "isPro": false,
            "fullname": "Kaiyan Zhang",
            "user": "iseesaw",
            "type": "user"
          },
          "name": "Kaiyan Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-25T08:20:20.614Z",
          "hidden": false
        },
        {
          "_id": "67e226039cd910bee045e393",
          "user": {
            "_id": "6528fc319474946b8541b36f",
            "avatarUrl": "/avatars/08ea388cbcd7c0f1361980127a8d33c3.svg",
            "isPro": false,
            "fullname": "Xiaohang Zhan",
            "user": "xhangzhan",
            "type": "user"
          },
          "name": "Xiaohang Zhan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:08:05.983Z",
          "hidden": false
        },
        {
          "_id": "67e226039cd910bee045e394",
          "user": {
            "_id": "66c8131afafc0fc87ca99650",
            "avatarUrl": "/avatars/a6eeba2ccf011d5c9964fd38f85bd671.svg",
            "isPro": false,
            "fullname": "Yueqi Duan",
            "user": "duanyueqi",
            "type": "user"
          },
          "name": "Yueqi Duan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:08:11.759Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-24T17:59:04.000Z",
      "submittedOnDailyAt": "2025-03-25T02:12:44.893Z",
      "title": "Video-T1: ビデオ生成のテスト時スケーリング",
      "submittedOnDailyBy": {
        "_id": "6505a02f9310ce8c400edc63",
        "avatarUrl": "/avatars/bbf781594fc8c812316711aa8e2797aa.svg",
        "isPro": false,
        "fullname": "Fangfu Liu",
        "user": "Liuff23",
        "type": "user"
      },
      "summary": "ビデオ生成において、訓練データの量、モデルサイズ、計算コストのスケーリング能力を増やすことで、数値創造において驚異的な成果を収め、ユーザーが多様な領域で創造性を表現することができるようになった。最近、大規模言語モデル（LLMs）の研究者は、テスト時にスケーリングを拡張し、推論時の計算を使用することでLLMの性能を大幅に向上させることができることを示した。ビデオベースモデルのスケーリングを高額な訓練コストで行う代わりに、テスト時スケーリング（TTS）の力をビデオ生成に拡張し、テキストプロンプトが難しい場合において、推論時の計算量を使用できることによって、ビデオの生成質量がどれだけ向上するかを解決することを目的としている。本研究では、ビデオ生成のテスト時スケーリングを、ガウスノイズ空間からの目標ビデオ分布へのより良いトラジェクトをサンプリングする問題とし、特に、テスト時バリデーターとヒューリスティックアルゴリズムを用いて探索スペースを構築している。テキストプロンプトを与えると、最初に、推論時にノイズ候補を増やす直感的な線形探索戦略を検討し、全ステップのデノイズ化が同時に全フレームを必要とするための重いテスト時計算コストを抑えるために、ビデオ生成に適したより効率的なTTS方法を設計し、フレームのタブレース（ToF）という名前で、自動帰ロジック的なビデオブランチを適応的に拡張し、プリングすることで、ビデオ生成の質量を向上させる。テキスト条件付きビデオ生成ベンチマーク上での拡張などの詳細な実験は、テスト時の計算量を増やすことがビデオの質量の大幅な向上を収めることを示した。プロジェクトページ：https://liuff19.github.io/Video-T1",
      "upvotes": 41,
      "discussionId": "67e226059cd910bee045e42b",
      "projectPage": "https://liuff19.github.io/Video-T1/",
      "githubRepo": "https://github.com/liuff19/Video-T1",
      "ai_keywords": [
        "Test-Time Scaling (TTS)",
        "video foundation models",
        "inference-time computation",
        "Gaussian noise space",
        "target video distribution",
        "test-time verifiers",
        "heuristic algorithms",
        "linear search strategy",
        "noise candidates",
        "full-step denoising",
        "inference time",
        "Tree-of-Frames (ToF)",
        "autoregressive manner",
        "text-conditioned video generation benchmarks"
      ]
    },
    "publishedAt": "2025-03-24T13:59:04.000Z",
    "title": "Video-T1: Test-Time Scaling for Video Generation",
    "summary": "With the scale capability of increasing training data, model size, and\ncomputational cost, video generation has achieved impressive results in digital\ncreation, enabling users to express creativity across various domains.\nRecently, researchers in Large Language Models (LLMs) have expanded the scaling\nto test-time, which can significantly improve LLM performance by using more\ninference-time computation. Instead of scaling up video foundation models\nthrough expensive training costs, we explore the power of Test-Time Scaling\n(TTS) in video generation, aiming to answer the question: if a video generation\nmodel is allowed to use non-trivial amount of inference-time compute, how much\ncan it improve generation quality given a challenging text prompt. In this\nwork, we reinterpret the test-time scaling of video generation as a searching\nproblem to sample better trajectories from Gaussian noise space to the target\nvideo distribution. Specifically, we build the search space with test-time\nverifiers to provide feedback and heuristic algorithms to guide searching\nprocess. Given a text prompt, we first explore an intuitive linear search\nstrategy by increasing noise candidates at inference time. As full-step\ndenoising all frames simultaneously requires heavy test-time computation costs,\nwe further design a more efficient TTS method for video generation called\nTree-of-Frames (ToF) that adaptively expands and prunes video branches in an\nautoregressive manner. Extensive experiments on text-conditioned video\ngeneration benchmarks demonstrate that increasing test-time compute\nconsistently leads to significant improvements in the quality of videos.\nProject page: https://liuff19.github.io/Video-T1",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18942.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6505a02f9310ce8c400edc63",
      "avatarUrl": "/avatars/bbf781594fc8c812316711aa8e2797aa.svg",
      "fullname": "Fangfu Liu",
      "name": "Liuff23",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.18945",
      "authors": [
        {
          "_id": "67e22eca9455abdd1d257263",
          "name": "Aether Team",
          "hidden": false
        },
        {
          "_id": "67e22eca9455abdd1d257264",
          "user": {
            "_id": "6283546209aa80237c6c482c",
            "avatarUrl": "/avatars/0d6fc5846c0456d5282d82d5bf4d7056.svg",
            "isPro": false,
            "fullname": "Haoyi Zhu",
            "user": "HaoyiZhu",
            "type": "user"
          },
          "name": "Haoyi Zhu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:15:13.586Z",
          "hidden": false
        },
        {
          "_id": "67e22eca9455abdd1d257265",
          "name": "Yifan Wang",
          "hidden": false
        },
        {
          "_id": "67e22eca9455abdd1d257266",
          "user": {
            "_id": "667e81565934c9fae29207ef",
            "avatarUrl": "/avatars/431e777c71fccf7cf48ce013e5f6f1cb.svg",
            "isPro": false,
            "fullname": "Zhou",
            "user": "ZhouTimeMachine",
            "type": "user"
          },
          "name": "Jianjun Zhou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-25T09:05:54.719Z",
          "hidden": false
        },
        {
          "_id": "67e22eca9455abdd1d257267",
          "user": {
            "_id": "67a5b0fe5a8652514e67c38c",
            "avatarUrl": "/avatars/28da8e93ee00fd77c7e62d16f9b94045.svg",
            "isPro": false,
            "fullname": "Wenzheng Chang",
            "user": "AmberHeart",
            "type": "user"
          },
          "name": "Wenzheng Chang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-25T08:20:10.947Z",
          "hidden": false
        },
        {
          "_id": "67e22eca9455abdd1d257268",
          "name": "Yang Zhou",
          "hidden": false
        },
        {
          "_id": "67e22eca9455abdd1d257269",
          "user": {
            "_id": "65e7eb86c7a0617cc71d3df4",
            "avatarUrl": "/avatars/01020b6b5ccb08bf8aa10fd5f8b2701d.svg",
            "isPro": false,
            "fullname": "lizizun",
            "user": "lizizun",
            "type": "user"
          },
          "name": "Zizun Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:16:06.912Z",
          "hidden": false
        },
        {
          "_id": "67e22eca9455abdd1d25726a",
          "user": {
            "_id": "6679bb85972a0f224cde335c",
            "avatarUrl": "/avatars/bf0649645458e206ba5224b001723641.svg",
            "isPro": false,
            "fullname": "Junyi Chen",
            "user": "Junyichen",
            "type": "user"
          },
          "name": "Junyi Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:16:13.751Z",
          "hidden": false
        },
        {
          "_id": "67e22eca9455abdd1d25726b",
          "name": "Chunhua Shen",
          "hidden": false
        },
        {
          "_id": "67e22eca9455abdd1d25726c",
          "user": {
            "_id": "65783ee6ee33d547aecc3ffc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65783ee6ee33d547aecc3ffc/lWZX88c-0dCsN-yB9Jhlf.jpeg",
            "isPro": false,
            "fullname": "Jiangmiao Pang",
            "user": "Jiangmiao",
            "type": "user"
          },
          "name": "Jiangmiao Pang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:16:26.855Z",
          "hidden": false
        },
        {
          "_id": "67e22eca9455abdd1d25726d",
          "user": {
            "_id": "64478c64e2148488340229db",
            "avatarUrl": "/avatars/f5c23489a068e896381cdc25836ce3dd.svg",
            "isPro": false,
            "fullname": "he",
            "user": "tonghe",
            "type": "user"
          },
          "name": "Tong He",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:16:35.824Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-24T17:59:51.000Z",
      "submittedOnDailyAt": "2025-03-25T02:50:34.610Z",
      "title": "Aether: 幾何学に関心を持つ統合的世界モデリング",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "フィードバックを受け付けました。以下の日本語翻訳を返します：\n\n幾何的再構造と生成モデリングの統合は、人間のような空間的理由能力を持つAIシステムの開発において重要な課題です。本論文では、世界モデルにおける幾何的知識を持つ理由を可能にするために、3つの核心能力を共同最適化するユニーバーセットフレームワーク「Aether」を提案します。これらの能力は、(1) 4D動的再構造、(2) 行動条件付きのビデオ予測、(3) 目標条件付きの視覚プランニングです。タスク交換された特徴量学習を通じて、Aetherは再構造、予測、プランニングの目標におけるシンプレックス的知識共有を実現します。ビデオ生成モデルの基礎上で、本フレームワークは訓練中に実世界データを見ることなく、前所未聞の合成データからの実世界データの拡張性を示します。また、本アプローチは、行動の追跡と再構造タスクにおいてゼロショットの拡張性を実現し、固有の幾何モデリングにより可能です。特に、実世界データを見ることなくも、再構造の性能は領域専門的なモデルよりも大幅に高いことが驚異的です。また、Aetherは幾何的知見をもつ行動空間を利用して、予測を行動に平滑に変換し、自動軌道プランニングを可能にします。私たちの研究は、物理的に合理的な世界モデリングの新たな境界への探索によるコミュニティを促進することを望んでいます。",
      "upvotes": 18,
      "discussionId": "67e22ecb9455abdd1d2572af",
      "projectPage": "https://aether-world.github.io/",
      "githubRepo": "https://github.com/OpenRobotLab/Aether",
      "ai_keywords": [
        "Aether",
        "4D dynamic reconstruction",
        "action-conditioned video prediction",
        "goal-conditioned visual planning",
        "task-interleaved feature learning",
        "video generation models",
        "synthetic-to-real generalization",
        "zero-shot generalization",
        "geometric modeling",
        "geometry-informed action space",
        "autonomous trajectory planning",
        "physically-reasonable world modeling"
      ]
    },
    "publishedAt": "2025-03-24T13:59:51.000Z",
    "title": "Aether: Geometric-Aware Unified World Modeling",
    "summary": "The integration of geometric reconstruction and generative modeling remains a\ncritical challenge in developing AI systems capable of human-like spatial\nreasoning. This paper proposes Aether, a unified framework that enables\ngeometry-aware reasoning in world models by jointly optimizing three core\ncapabilities: (1) 4D dynamic reconstruction, (2) action-conditioned video\nprediction, and (3) goal-conditioned visual planning. Through task-interleaved\nfeature learning, Aether achieves synergistic knowledge sharing across\nreconstruction, prediction, and planning objectives. Building upon video\ngeneration models, our framework demonstrates unprecedented synthetic-to-real\ngeneralization despite never observing real-world data during training.\nFurthermore, our approach achieves zero-shot generalization in both action\nfollowing and reconstruction tasks, thanks to its intrinsic geometric modeling.\nRemarkably, even without real-world data, its reconstruction performance far\nexceeds that of domain-specific models. Additionally, Aether leverages a\ngeometry-informed action space to seamlessly translate predictions into\nactions, enabling effective autonomous trajectory planning. We hope our work\ninspires the community to explore new frontiers in physically-reasonable world\nmodeling and its applications.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18945.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6456
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.18892",
      "authors": [
        {
          "_id": "67e22ce1155ea10f2fdbe5d2",
          "user": {
            "_id": "62751082b43ccfeef483424f",
            "avatarUrl": "/avatars/fec83e4478e7d1731ba6033328131852.svg",
            "isPro": false,
            "fullname": "WeihaoZeng",
            "user": "AndrewZeng",
            "type": "user"
          },
          "name": "Weihao Zeng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-25T08:20:18.367Z",
          "hidden": false
        },
        {
          "_id": "67e22ce1155ea10f2fdbe5d3",
          "user": {
            "_id": "6462def82a83863b97c0611e",
            "avatarUrl": "/avatars/c03e9cc7d75b0266fcc56ecb6ee62148.svg",
            "isPro": false,
            "fullname": "Yuzhen Huang",
            "user": "yuzhen17",
            "type": "user"
          },
          "name": "Yuzhen Huang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-25T08:20:13.781Z",
          "hidden": false
        },
        {
          "_id": "67e22ce1155ea10f2fdbe5d4",
          "user": {
            "_id": "612ee6a7b960e78c6d2319d4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/612ee6a7b960e78c6d2319d4/2Hu9BaAyXbyh1vt0v1Qui.jpeg",
            "isPro": false,
            "fullname": "Qian Liu",
            "user": "SivilTaram",
            "type": "user"
          },
          "name": "Qian Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-25T08:20:15.927Z",
          "hidden": false
        },
        {
          "_id": "67e22ce1155ea10f2fdbe5d5",
          "name": "Wei Liu",
          "hidden": false
        },
        {
          "_id": "67e22ce1155ea10f2fdbe5d6",
          "user": {
            "_id": "64bf71792915a87970c07446",
            "avatarUrl": "/avatars/b24403f9fa699e0143e441b56528e6af.svg",
            "isPro": false,
            "fullname": "Keqing He",
            "user": "HelicHe",
            "type": "user"
          },
          "name": "Keqing He",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:17:09.770Z",
          "hidden": false
        },
        {
          "_id": "67e22ce1155ea10f2fdbe5d7",
          "name": "Zejun Ma",
          "hidden": false
        },
        {
          "_id": "67e22ce1155ea10f2fdbe5d8",
          "user": {
            "_id": "615f34ec3f6d24d67c1b5c78",
            "avatarUrl": "/avatars/6dcff6477993d9e57c5cb92b6f95eb66.svg",
            "isPro": false,
            "fullname": "Junxian He",
            "user": "jxhe",
            "type": "user"
          },
          "name": "Junxian He",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:16:49.208Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-24T17:06:10.000Z",
      "submittedOnDailyAt": "2025-03-25T02:41:40.812Z",
      "title": "SimpleRL-Zoo: 野生の開放ベースモデルにおけるゼロ強化学習の調査と制御",
      "submittedOnDailyBy": {
        "_id": "62751082b43ccfeef483424f",
        "avatarUrl": "/avatars/fec83e4478e7d1731ba6033328131852.svg",
        "isPro": false,
        "fullname": "WeihaoZeng",
        "user": "AndrewZeng",
        "type": "user"
      },
      "summary": "DeepSeek-R1は、簡単なルールベース報酬を持つ強化学習（RL）フレームワークで自然的に長い連鎖的思考（CoT）論理が現れることを示した。この学習は、基礎モデルから直接始まるパラダイムとして「zero RL トレーニング」と呼ばれている。最近の効果的な実験は、Qwen2.5モデルシリーズを中心に行われており、これらのモデルは既に強い指示従いと自己反省能力を示しているため、代表的ではないという点に注目している。本研究では、10種類の異なる基礎モデルを対象に、LLama3-8B、Mistral-7B/24B、DeepSeek-Math-7B、Qwen2.5-math-7B、そしてQwen2.5モデルシリーズ（0.5Bから32B）を含む多様なモデルを検討した。フォーマット報酬の調整とクエリの難易度の制御などの鍵の設計戦略を活用し、理由論理の正確性と回答の長さの両方に大幅な向上を実現した。しかし、学習のダイナミクスを観察すると、各基礎モデルは学習中に異なるパターンを示すことがわかった。例えば、回答の長さの増加は、特定の認知行動の現れ（例えば、「おおきなコンシェクト」）との関連が常にあることではない。特に、QwenファMIリーからの小さなモデルで「おおきなコンシェクト」が初めて見られたことが注目される。本研究では、成功なzero RLトレーニングを可能にする鍵の設計、発見および実践を共有し、進展の促進を目的にコード、モデル、分析ツールをオープンソース化している。",
      "upvotes": 16,
      "discussionId": "67e22ce3155ea10f2fdbe6c0",
      "githubRepo": "https://github.com/hkust-nlp/simpleRL-reason",
      "ai_keywords": [
        "reinforcement learning",
        "rule-based rewards",
        "zero RL training",
        "long chain-of-thought (CoT) reasoning",
        "instruction-following",
        "self-reflection",
        "base models",
        "Qwen2.5 model series",
        "LLama3-8B",
        "Mistral-7B/24B",
        "DeepSeek-Math-7B",
        "Qwen2.5-math-7B",
        "response length",
        "reasoning accuracy",
        "cognitive behaviors",
        "verification",
        "training dynamics"
      ]
    },
    "publishedAt": "2025-03-24T13:06:10.000Z",
    "title": "SimpleRL-Zoo: Investigating and Taming Zero Reinforcement Learning for\n  Open Base Models in the Wild",
    "summary": "DeepSeek-R1 has shown that long chain-of-thought (CoT) reasoning can\nnaturally emerge through a simple reinforcement learning (RL) framework with\nrule-based rewards, where the training may directly start from the base\nmodels-a paradigm referred to as zero RL training. Most recent efforts to\nreproduce zero RL training have primarily focused on the Qwen2.5 model series,\nwhich may not be representative as we find the base models already exhibit\nstrong instruction-following and self-reflection abilities. In this work, we\ninvestigate zero RL training across 10 diverse base models, spanning different\nfamilies and sizes including LLama3-8B, Mistral-7B/24B, DeepSeek-Math-7B,\nQwen2.5-math-7B, and all Qwen2.5 models from 0.5B to 32B. Leveraging several\nkey design strategies-such as adjusting format reward and controlling query\ndifficulty-we achieve substantial improvements in both reasoning accuracy and\nresponse length across most settings. However, by carefully monitoring the\ntraining dynamics, we observe that different base models exhibit distinct\npatterns during training. For instance, the increased response length does not\nalways correlate with the emergence of certain cognitive behaviors such as\nverification (i.e., the \"aha moment\"). Notably, we observe the \"aha moment\" for\nthe first time in small models not from the Qwen family. We share the key\ndesigns that enable successful zero RL training, along with our findings and\npractices. To facilitate further research, we open-source the code, models, and\nanalysis tools.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18892.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62751082b43ccfeef483424f",
      "avatarUrl": "/avatars/fec83e4478e7d1731ba6033328131852.svg",
      "fullname": "WeihaoZeng",
      "name": "AndrewZeng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.17439",
      "authors": [
        {
          "_id": "67e21f63fb4213c53714be08",
          "user": {
            "_id": "6565e24fe5aac326bfd15a9d",
            "avatarUrl": "/avatars/28ad90df0e0dbc10ef25ee6499a50dec.svg",
            "isPro": false,
            "fullname": "Zhuoshi Pan",
            "user": "panzs",
            "type": "user"
          },
          "name": "Zhuoshi Pan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:17:23.682Z",
          "hidden": false
        },
        {
          "_id": "67e21f63fb4213c53714be09",
          "name": "Yu Li",
          "hidden": false
        },
        {
          "_id": "67e21f63fb4213c53714be0a",
          "user": {
            "_id": "640d99628512ec51d7ef71c7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640d99628512ec51d7ef71c7/fcBkqnxfxuuuZTqfN_BGy.jpeg",
            "isPro": false,
            "fullname": "Honglin Lin",
            "user": "LHL3341",
            "type": "user"
          },
          "name": "Honglin Lin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:17:30.635Z",
          "hidden": false
        },
        {
          "_id": "67e21f63fb4213c53714be0b",
          "user": {
            "_id": "6397f6081323f19c578f142e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6397f6081323f19c578f142e/it7FYYKjlLX8wSsMLm8EO.jpeg",
            "isPro": false,
            "fullname": "QizhiPei",
            "user": "QizhiPei",
            "type": "user"
          },
          "name": "Qizhi Pei",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:17:42.836Z",
          "hidden": false
        },
        {
          "_id": "67e21f63fb4213c53714be0c",
          "user": {
            "_id": "66580d3d80ee5b1e11a94e57",
            "avatarUrl": "/avatars/1a88e7337f9095c40c6d402fab797d83.svg",
            "isPro": false,
            "fullname": "Zinan Tang",
            "user": "Word2Li",
            "type": "user"
          },
          "name": "Zinan Tang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:17:50.791Z",
          "hidden": false
        },
        {
          "_id": "67e21f63fb4213c53714be0d",
          "name": "Wei Wu",
          "hidden": false
        },
        {
          "_id": "67e21f63fb4213c53714be0e",
          "user": {
            "_id": "677e133ee86d0754dc7ce296",
            "avatarUrl": "/avatars/c16511c1876b50c2d049925c5f320d15.svg",
            "isPro": false,
            "fullname": "mingchenlin",
            "user": "mingchenlin2025",
            "type": "user"
          },
          "name": "Chenlin Ming",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:18:12.485Z",
          "hidden": false
        },
        {
          "_id": "67e21f63fb4213c53714be0f",
          "name": "H. Vicky Zhao",
          "hidden": false
        },
        {
          "_id": "67e21f63fb4213c53714be10",
          "user": {
            "_id": "63f9fca8d4349b157a109eec",
            "avatarUrl": "/avatars/fa1f2ae7972d7cde99dab178136ccbb0.svg",
            "isPro": false,
            "fullname": "Conghui He",
            "user": "conghui",
            "type": "user"
          },
          "name": "Conghui He",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:18:25.368Z",
          "hidden": false
        },
        {
          "_id": "67e21f63fb4213c53714be11",
          "name": "Lijun Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-21T17:59:10.000Z",
      "submittedOnDailyAt": "2025-03-25T01:47:52.213Z",
      "title": "ライブラリーマシンでの数学的進歩を目指して誤りから学ぶ学習法",
      "submittedOnDailyBy": {
        "_id": "6397f6081323f19c578f142e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6397f6081323f19c578f142e/it7FYYKjlLX8wSsMLm8EO.jpeg",
        "isPro": false,
        "fullname": "QizhiPei",
        "user": "QizhiPei",
        "type": "user"
      },
      "summary": "大型言語モデル（LLMs）は、数學問題の解決において驚異的な推理能力を示しています。しかし、現在のアプローチは主に正しいトレーニングデータの品質向上に焦点を当てています。例えば、先進モデルから高品質の正しい解決策をディスティルすることで、誤りデータに含まれる価値を無視し、モデルの反省能力を潜在的に妨げています。しかし、一部の研究は誤りデータを活用しようと試みていますが、これらは複雑な機構を含むことが多いです。例えば、モンテカルロ木検索（MCTS）を用いて誤りノードを探索することです。本稿では、数學の進歩を促進するための誤りから学習（LEMMA）を提案します。LEMMAは不正な解決策と誤りステップ、正しい解決策への反省へのつながりを含むデータを構築し、微調節します。特に、モデルが生成した誤りタイプをシステマティックに分析し、多様的で有代表性的な誤りを集めるための誤りタイプベースの誤り拡大メソッドを導入します。正しい解決策は、誤りの修正か新たな始めになるものです。モデルによる平滑な反省へのつながりを通じて、誤りの解決策は正しい解決策に移行します。構築されたデータセットによる微調節を通じて、モデルは生成プロセス内で自動的に誤りを自動調整できるようになり、外部の批判モデルに依存しないようにします。実験結果は、LEMMAが他の強いベースラインより显著な性能向上を達成していることを示しています。",
      "upvotes": 12,
      "discussionId": "67e21f64fb4213c53714be6b",
      "githubRepo": "https://github.com/pzs19/LEMMA",
      "ai_keywords": [
        "Learning from Errors for Mathematical Advancement (LEMMA)",
        "mistake augmentation",
        "model-aware smooth reflection connection",
        "autonomous error correction"
      ]
    },
    "publishedAt": "2025-03-21T13:59:10.000Z",
    "title": "LEMMA: Learning from Errors for MatheMatical Advancement in LLMs",
    "summary": "Large language models (LLMs) have demonstrated remarkable reasoning\ncapability in solving mathematical problems. However, existing approaches\nprimarily focus on improving the quality of correct training data, e.g.,\ndistilling high-quality correct solutions from advanced models, neglecting the\nvalue contained in error data, potentially hindering the model's reflective\nability. Though some studies attempt to leverage error data, they often involve\ncomplex mechanisms, such as Monte Carlo Tree Search (MCTS) to explore error\nnodes. In this work, we propose to enhance LLMs' reasoning ability by Learning\nfrom Errors for Mathematical Advancement (LEMMA). LEMMA constructs data\nconsisting of an incorrect solution with an erroneous step and a reflection\nconnection to a correct solution for fine-tuning. Specifically, we\nsystematically analyze the model-generated error types and introduce an\nerror-type grounded mistake augmentation method to collect diverse and\nrepresentative errors. Correct solutions are either from fixing the errors or\ngenerating a fresh start. Through a model-aware smooth reflection connection,\nthe erroneous solution is transferred to the correct one. By fine-tuning on the\nconstructed dataset, the model is able to self-correct errors autonomously\nwithin the generation process without relying on external critique models.\nExperimental results demonstrate that LEMMA achieves significant performance\nimprovements over other strong baselines.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17439.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6397f6081323f19c578f142e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6397f6081323f19c578f142e/it7FYYKjlLX8wSsMLm8EO.jpeg",
      "fullname": "QizhiPei",
      "name": "QizhiPei",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 17
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.18940",
      "authors": [
        {
          "_id": "67e21b305d20ec3277dac34a",
          "user": {
            "_id": "64e357dd825f4133e7427bf8",
            "avatarUrl": "/avatars/aeb6869d075f65a581797df2aabfb02f.svg",
            "isPro": false,
            "fullname": "tyfeld",
            "user": "tyfeld",
            "type": "user"
          },
          "name": "Ye Tian",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-25T08:20:24.659Z",
          "hidden": false
        },
        {
          "_id": "67e21b305d20ec3277dac34b",
          "name": "Xin Xia",
          "hidden": false
        },
        {
          "_id": "67e21b305d20ec3277dac34c",
          "user": {
            "_id": "6618d5e83b412cdc85334ca8",
            "avatarUrl": "/avatars/5fe356d58c4c822a60370dbee8d78a69.svg",
            "isPro": false,
            "fullname": "renyuxi",
            "user": "renyuxi",
            "type": "user"
          },
          "name": "Yuxi Ren",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:19:25.116Z",
          "hidden": false
        },
        {
          "_id": "67e21b305d20ec3277dac34d",
          "name": "Shanchuan Lin",
          "hidden": false
        },
        {
          "_id": "67e21b305d20ec3277dac34e",
          "name": "Xing Wang",
          "hidden": false
        },
        {
          "_id": "67e21b305d20ec3277dac34f",
          "user": {
            "_id": "646b7f71df2609a541c1ab9f",
            "avatarUrl": "/avatars/48b82e5fd9b06f41ff825507c36816cd.svg",
            "isPro": false,
            "fullname": "Xuefeng Xiao",
            "user": "xiaoxuefeng",
            "type": "user"
          },
          "name": "Xuefeng Xiao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:19:05.020Z",
          "hidden": false
        },
        {
          "_id": "67e21b305d20ec3277dac350",
          "name": "Yunhai Tong",
          "hidden": false
        },
        {
          "_id": "67e21b305d20ec3277dac351",
          "user": {
            "_id": "64fde4e252e82dd432b74ce9",
            "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
            "isPro": false,
            "fullname": "Ling Yang",
            "user": "Lingaaaaaaa",
            "type": "user"
          },
          "name": "Ling Yang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:19:44.463Z",
          "hidden": false
        },
        {
          "_id": "67e21b305d20ec3277dac352",
          "user": {
            "_id": "67b2795f0bd4ddcd84426bb4",
            "avatarUrl": "/avatars/d4346ac5a0ebbaeb828d832cc6ca9f0b.svg",
            "isPro": false,
            "fullname": "Bin Cui",
            "user": "lazybone128",
            "type": "user"
          },
          "name": "Bin Cui",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:18:48.809Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-24T17:59:02.000Z",
      "submittedOnDailyAt": "2025-03-25T01:26:48.606Z",
      "title": "トレーニングなしディフュージョン加速によるボトルネックサンプリング",
      "submittedOnDailyBy": {
        "_id": "64fde4e252e82dd432b74ce9",
        "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
        "isPro": false,
        "fullname": "Ling Yang",
        "user": "Lingaaaaaaa",
        "type": "user"
      },
      "summary": "Diffusionモデルは、視覚内容の生成において驚異的な能力を示していますが、推論時の高い計算コストにより実装が難しい問題があります。この計算ボレーンは、画像またはビデオの解像度に対するself-attentionの二次元複雑性によって主に原因となります。既存の加速方法は、出力の品質を補損させるか、高コストの再学習を必要とすることが多いですが、私たちは、多くのDiffusionモデルが低解像度で予えられていることを見出し、これらの低解像度の先駆を利用して、性能の低下を避けながらより効率的な推論を実現する機会を見出しました。本論文では、低解像度の先駆を利用して計算オーバーヘッドを減らし、出力の忠実性を維持するためのトレーニング無制限のフレームワーク「Bottleneck Sampling」を紹介します。Bottleneck Samplingは、高解像度のデノイズ処理を初期と最終段階で行い、中間段階では低解像度で処理する「高-低-高」デノイズワークフローを実行します。アライアンスやブラーリングのアーテキュルを軽減するために、解像度の転移点を進歩的に改善し、各段階でデノイズタイムステップを適応的に変更します。Bottleneck Samplingは、画像とビデオの生成タスクで検証され、様々な評価指標で標準の全解像度サンプリングプロセスと比較的な出力品質を維持しながら、画像生成では3倍、ビデオ生成では2.5倍の推論速度アクセルレーションを実現しました。コードは以下のURLで提供されています：https://github.com/tyfeld/Bottleneck-Sampling",
      "upvotes": 10,
      "discussionId": "67e21b365d20ec3277dac500",
      "projectPage": "https://tyfeld.github.io/BottleneckSampling.github.io",
      "githubRepo": "https://github.com/tyfeld/Bottleneck-Sampling",
      "ai_keywords": [
        "diffusion models",
        "self-attention",
        "computational overhead",
        "low-resolution priors",
        "Bottleneck Sampling",
        "denoising workflow",
        "high-resolution denoising",
        "aliasing",
        "blurring artifacts",
        "resolution transition points",
        "adaptive timesteps"
      ]
    },
    "publishedAt": "2025-03-24T13:59:02.000Z",
    "title": "Training-free Diffusion Acceleration with Bottleneck Sampling",
    "summary": "Diffusion models have demonstrated remarkable capabilities in visual content\ngeneration but remain challenging to deploy due to their high computational\ncost during inference. This computational burden primarily arises from the\nquadratic complexity of self-attention with respect to image or video\nresolution. While existing acceleration methods often compromise output quality\nor necessitate costly retraining, we observe that most diffusion models are\npre-trained at lower resolutions, presenting an opportunity to exploit these\nlow-resolution priors for more efficient inference without degrading\nperformance. In this work, we introduce Bottleneck Sampling, a training-free\nframework that leverages low-resolution priors to reduce computational overhead\nwhile preserving output fidelity. Bottleneck Sampling follows a high-low-high\ndenoising workflow: it performs high-resolution denoising in the initial and\nfinal stages while operating at lower resolutions in intermediate steps. To\nmitigate aliasing and blurring artifacts, we further refine the resolution\ntransition points and adaptively shift the denoising timesteps at each stage.\nWe evaluate Bottleneck Sampling on both image and video generation tasks, where\nextensive experiments demonstrate that it accelerates inference by up to\n3times for image generation and 2.5times for video generation, all while\nmaintaining output quality comparable to the standard full-resolution sampling\nprocess across multiple evaluation metrics. Code is available at:\nhttps://github.com/tyfeld/Bottleneck-Sampling",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18940.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64fde4e252e82dd432b74ce9",
      "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
      "fullname": "Ling Yang",
      "name": "Lingaaaaaaa",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.17489",
      "authors": [
        {
          "_id": "67e21f300e6b6fcc3eb38ae1",
          "name": "Shu Pu",
          "hidden": false
        },
        {
          "_id": "67e21f300e6b6fcc3eb38ae2",
          "name": "Yaochen Wang",
          "hidden": false
        },
        {
          "_id": "67e21f300e6b6fcc3eb38ae3",
          "user": {
            "_id": "65e2be1e630e2db23829ee8d",
            "avatarUrl": "/avatars/294f9ba909037f03669dc0bb80cabfe3.svg",
            "isPro": false,
            "fullname": "Dongping Chen",
            "user": "fjchendp",
            "type": "user"
          },
          "name": "Dongping Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:20:55.986Z",
          "hidden": false
        },
        {
          "_id": "67e21f300e6b6fcc3eb38ae4",
          "user": {
            "_id": "64964aae457f60023c6a6f9d",
            "avatarUrl": "/avatars/342603e0028204f33fe7f5e3f3da1aa3.svg",
            "isPro": false,
            "fullname": "Yuhang Chen",
            "user": "yuhangchen",
            "type": "user"
          },
          "name": "Yuhang Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:21:02.911Z",
          "hidden": false
        },
        {
          "_id": "67e21f300e6b6fcc3eb38ae5",
          "user": {
            "_id": "67c94fd48670a35a7c05f36c",
            "avatarUrl": "/avatars/a59a7872bcc58fec7747225f2d3da3f9.svg",
            "isPro": false,
            "fullname": "Guohao Wang",
            "user": "NiuniuWang",
            "type": "user"
          },
          "name": "Guohao Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:21:09.345Z",
          "hidden": false
        },
        {
          "_id": "67e21f300e6b6fcc3eb38ae6",
          "name": "Qi Qin",
          "hidden": false
        },
        {
          "_id": "67e21f300e6b6fcc3eb38ae7",
          "name": "Zhongyi Zhang",
          "hidden": false
        },
        {
          "_id": "67e21f300e6b6fcc3eb38ae8",
          "name": "Zhiyuan Zhang",
          "hidden": false
        },
        {
          "_id": "67e21f300e6b6fcc3eb38ae9",
          "user": {
            "_id": "6697e7e55ef2828a1ff371c3",
            "avatarUrl": "/avatars/b361ea817760f7cb5c5d39028ee6b507.svg",
            "isPro": false,
            "fullname": "Zetong Zhou",
            "user": "Frywind",
            "type": "user"
          },
          "name": "Zetong Zhou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:21:29.118Z",
          "hidden": false
        },
        {
          "_id": "67e21f300e6b6fcc3eb38aea",
          "user": {
            "_id": "67575cac2f7acf9a8b4626fa",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/1OkoZh8A4jPKHpTg5iSXP.png",
            "isPro": false,
            "fullname": "Shuang Gong",
            "user": "shuang72",
            "type": "user"
          },
          "name": "Shuang Gong",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:21:35.465Z",
          "hidden": false
        },
        {
          "_id": "67e21f300e6b6fcc3eb38aeb",
          "name": "Yi Gui",
          "hidden": false
        },
        {
          "_id": "67e21f300e6b6fcc3eb38aec",
          "name": "Yao Wan",
          "hidden": false
        },
        {
          "_id": "67e21f300e6b6fcc3eb38aed",
          "name": "Philip S. Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-21T18:59:20.000Z",
      "submittedOnDailyAt": "2025-03-25T01:46:07.985Z",
      "title": "Judge Anything: MLLM は、どのモデラーでも判定を行うような機能を持つものです。",
      "submittedOnDailyBy": {
        "_id": "643be8879f5d314db2d9ed23",
        "avatarUrl": "/avatars/64e9bb2c4e10fbe03e2b81afedf40865.svg",
        "isPro": false,
        "fullname": "Chen Dongping",
        "user": "shuaishuaicdp",
        "type": "user"
      },
      "summary": "多模态理解（MMU）と生成（MMG）の開放的なタスクにおける生成的基盤モデルの評価は、クロスモディアルインタラクションの複雑性により重大な課題を抱えています。この点に対して、多模态LLMs（MLLMs）を自動化ジャッジとして利用するアイデアが発展し、視覚言語理解タスクの評価に極めて優れた結果を示しています。さらに、この論文は、TaskAnythingとJudgeAnythingの2つのベンチマークを介して、MLLMsの統一的な評価をモデルバイモデルのタスクに拡張します。TaskAnythingは、15つのモデルバイモデルのカテゴリにおけるMMUとMMGの能力を評価し、1,500件のクエリを使用しています。また、JudgeAnythingは、Pair ComparisonとScore Evaluationの観点から5つの先進モデル（例：GPT-4oとGemini-2.0-Flash）の判断能力を評価し、人間の判断と詳細なレビューガイドを含む標準化テストベンダを提供しています。我々の拡大試験による結果から、これらのMLLMsはMMUの評価に望ましい効果を示します（Pair Comparisonで平均66.55%、Score Evaluationで平均42.79%）が、MMGの評価には大きな課題を抱えています（Pair Comparisonで平均53.37%、Score Evaluationで平均30.05%）、クロスモディアルバイアスとハウジング問題を明らかにします。これに対して、OmniArenaという自動化プラットフォームを提出し、これらのモデルの評価と多模态報酬モデルの評価を行うことを目的としています。我々の研究は、公平な評価プロトコルと人間の好みとの強い一致の必要性を強調しています。ソースコードとデータセットは以下のURLで公開されています：https://urrealhero.github.io/judgeanythingweb/",
      "upvotes": 10,
      "discussionId": "67e21f350e6b6fcc3eb38c35",
      "ai_keywords": [
        "Multimodal LLMs (MLLMs)",
        "TaskAnything",
        "JudgeAnything",
        "open-ended multimodal understanding (MMU)",
        "open-ended multimodal generation (MMG)",
        "cross-modal interactions",
        "vision-language understanding tasks",
        "any-to-any modality tasks",
        "Pair Comparison",
        "Score Evaluation",
        "omni-models",
        "multimodal reward models",
        "cross-modality biases",
        "hallucination issues"
      ]
    },
    "publishedAt": "2025-03-21T14:59:20.000Z",
    "title": "Judge Anything: MLLM as a Judge Across Any Modality",
    "summary": "Evaluating generative foundation models on open-ended multimodal\nunderstanding (MMU) and generation (MMG) tasks across diverse modalities (e.g.,\nimages, audio, video) poses significant challenges due to the complexity of\ncross-modal interactions. To this end, the idea of utilizing Multimodal LLMs\n(MLLMs) as automated judges has emerged, with encouraging results in assessing\nvision-language understanding tasks. Moving further, this paper extends\nMLLM-as-a-Judge across modalities to a unified manner by introducing two\nbenchmarks, TaskAnything and JudgeAnything, to respectively evaluate the\noverall performance and judging capabilities of MLLMs across any-to-any\nmodality tasks. Specifically, TaskAnything evaluates the MMU and MMG\ncapabilities across 15 any-to-any modality categories, employing 1,500 queries\ncurated from well-established benchmarks. Furthermore, JudgeAnything evaluates\nthe judging capabilities of 5 advanced (e.g., GPT-4o and Gemini-2.0-Flash) from\nthe perspectives of Pair Comparison and Score Evaluation, providing a\nstandardized testbed that incorporates human judgments and detailed rubrics.\nOur extensive experiments reveal that while these MLLMs show promise in\nassessing MMU (i.e., achieving an average of 66.55% in Pair Comparison setting\nand 42.79% in Score Evaluation setting), they encounter significant challenges\nwith MMG tasks (i.e., averaging only 53.37% in Pair Comparison setting and\n30.05% in Score Evaluation setting), exposing cross-modality biases and\nhallucination issues. To address this, we present OmniArena, an automated\nplatform for evaluating omni-models and multimodal reward models. Our work\nhighlights the need for fairer evaluation protocols and stronger alignment with\nhuman preferences. The source code and dataset are publicly available at:\nhttps://urrealhero.github.io/judgeanythingweb/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17489.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "643be8879f5d314db2d9ed23",
      "avatarUrl": "/avatars/64e9bb2c4e10fbe03e2b81afedf40865.svg",
      "fullname": "Chen Dongping",
      "name": "shuaishuaicdp",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.18948",
      "authors": [
        {
          "_id": "67e24217db11e1d382285cd4",
          "user": {
            "_id": "6447a5806ffed6ece1fcf723",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/NjOA7G_QCa3bCluA69hSs.jpeg",
            "isPro": false,
            "fullname": "Ruixiao Dong",
            "user": "dongruixiao",
            "type": "user"
          },
          "name": "Ruixiao Dong",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:20:32.045Z",
          "hidden": false
        },
        {
          "_id": "67e24217db11e1d382285cd5",
          "user": {
            "_id": "63f5993afcf95ecac2b419b5",
            "avatarUrl": "/avatars/a8c020080a84d9a663789c4fb19270e9.svg",
            "isPro": false,
            "fullname": "Mengde Xu",
            "user": "Mendel192",
            "type": "user"
          },
          "name": "Mengde Xu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:20:25.156Z",
          "hidden": false
        },
        {
          "_id": "67e24217db11e1d382285cd6",
          "name": "Zigang Geng",
          "hidden": false
        },
        {
          "_id": "67e24217db11e1d382285cd7",
          "name": "Li Li",
          "hidden": false
        },
        {
          "_id": "67e24217db11e1d382285cd8",
          "user": {
            "_id": "665d88640e92f92b0e7eb17f",
            "avatarUrl": "/avatars/ff3a410e1e7bfb00ff0ec8ce4d5b1463.svg",
            "isPro": false,
            "fullname": "han hu",
            "user": "hanhu2",
            "type": "user"
          },
          "name": "Han Hu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:20:10.271Z",
          "hidden": false
        },
        {
          "_id": "67e24217db11e1d382285cd9",
          "name": "Shuyang Gu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-24T17:59:57.000Z",
      "submittedOnDailyAt": "2025-03-25T05:57:15.975Z",
      "title": "等方的画像モデリング",
      "submittedOnDailyBy": {
        "_id": "64c38fcf573c5a427e12cd37",
        "avatarUrl": "/avatars/2b9de06f29147ed2c212e920afba0eaf.svg",
        "isPro": false,
        "fullname": "cientgu",
        "user": "cientgu",
        "type": "user"
      },
      "summary": "現在の生成モデル（例：自動復元モデルとディフュージョンアプローチ）は、高次元データ分布学習を簡単なサブタスクの列に分解しています。しかし、これらのサブタスクの共通最適化において固有の衝突が発生し、現在の解決策は効率やスケーラビリティを失ってもそれらの衝突を解決できません。私たちは、自然的な視覚信号の移動不変性を活用してサブタスク間の最適化ターゲットを内在的に一致させる新しい等対称画像モデリングフレームワークを提案します。私たちの方法は、(1) カラムワイズトークニゼーションを導入し、横軸方向の移動対称性を強化し、(2) ウィンドウサイズの因果注意を導入し、位置間の一貫的なコンテキスト関係を強制します。クラス条件付きのImageNet生成（256x256解像度）で評価したところ、私たちのアプローチは状態の最先端のARモデルと比較的な性能を達成し、計算コストを少なく使用しています。システマティックな分析により、拡大された等対称性がタスク間の衝突を減らし、ゼロショット拡張性を大幅に向上させ、超長期画像合成を可能にします。この研究は、生成モデリングのタスク一致性分解の最初のフレームワークを立て、効率的なパラメータ共有と衝突無し最適化についての見解を提供します。コードとモデルは、https://github.com/drx-code/EquivariantModeling で公開されています。",
      "upvotes": 9,
      "discussionId": "67e2421edb11e1d382285f9b",
      "ai_keywords": [
        "autoregressive",
        "diffusion approaches",
        "high-dimensional data distribution learning",
        "subtasks",
        "joint optimization",
        "equivariant image modeling framework",
        "translation invariance",
        "column-wise tokenization",
        "translational symmetry",
        "windowed causal attention",
        "contextual relationships",
        "class-conditioned ImageNet generation",
        "state-of-the-art AR models",
        "computational resources",
        "enhanced equivariance",
        "zero-shot generalization",
        "ultra-long image synthesis",
        "task-aligned decomposition",
        "efficient parameter sharing",
        "conflict-free optimization"
      ]
    },
    "publishedAt": "2025-03-24T13:59:57.000Z",
    "title": "Equivariant Image Modeling",
    "summary": "Current generative models, such as autoregressive and diffusion approaches,\ndecompose high-dimensional data distribution learning into a series of simpler\nsubtasks. However, inherent conflicts arise during the joint optimization of\nthese subtasks, and existing solutions fail to resolve such conflicts without\nsacrificing efficiency or scalability. We propose a novel equivariant image\nmodeling framework that inherently aligns optimization targets across subtasks\nby leveraging the translation invariance of natural visual signals. Our method\nintroduces (1) column-wise tokenization which enhances translational symmetry\nalong the horizontal axis, and (2) windowed causal attention which enforces\nconsistent contextual relationships across positions. Evaluated on\nclass-conditioned ImageNet generation at 256x256 resolution, our approach\nachieves performance comparable to state-of-the-art AR models while using fewer\ncomputational resources. Systematic analysis demonstrates that enhanced\nequivariance reduces inter-task conflicts, significantly improving zero-shot\ngeneralization and enabling ultra-long image synthesis. This work establishes\nthe first framework for task-aligned decomposition in generative modeling,\noffering insights into efficient parameter sharing and conflict-free\noptimization. The code and models are publicly available at\nhttps://github.com/drx-code/EquivariantModeling.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18948.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64c38fcf573c5a427e12cd37",
      "avatarUrl": "/avatars/2b9de06f29147ed2c212e920afba0eaf.svg",
      "fullname": "cientgu",
      "name": "cientgu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.18886",
      "authors": [
        {
          "_id": "67e21d3484513315a9169aae",
          "user": {
            "_id": "6481764e8af4675862efb22e",
            "avatarUrl": "/avatars/fc2e076bc861693f598a528a068a696e.svg",
            "isPro": true,
            "fullname": "weichenfan",
            "user": "weepiess2383",
            "type": "user"
          },
          "name": "Weichen Fan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-25T08:20:22.659Z",
          "hidden": false
        },
        {
          "_id": "67e21d3484513315a9169aaf",
          "name": "Amber Yijia Zheng",
          "hidden": false
        },
        {
          "_id": "67e21d3484513315a9169ab0",
          "name": "Raymond A. Yeh",
          "hidden": false
        },
        {
          "_id": "67e21d3484513315a9169ab1",
          "name": "Ziwei Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-24T16:59:57.000Z",
      "submittedOnDailyAt": "2025-03-25T06:34:48.098Z",
      "title": "CFG-Zero*: フローマッチングモデルの改善されたクラスファイザーフリーガイドニング",
      "submittedOnDailyBy": {
        "_id": "6481764e8af4675862efb22e",
        "avatarUrl": "/avatars/fc2e076bc861693f598a528a068a696e.svg",
        "isPro": true,
        "fullname": "weichenfan",
        "user": "weepiess2383",
        "type": "user"
      },
      "summary": "Classifier-Free Guidance (CFG)は、ディフュージョンモデルやフローモデルで画像の品質と制御性を向上させるために広く採用されている技術です。本研究では、ガウス混合物によって訓練されたフローマッチングモデルに対してCFGの影響を分析的に調査します。訓練の初期段階で、フローの推定が不正確である場合、CFGはサンプルを不正な軌道に向けます。この観察に基づき、CFG-Zero*を提案します。CFG-Zero*は、2つの貢献を持ちます。1つは、調整されたスケールで、スカラーを最適化して推定された速度の不正確さを補正します。2つ目は、zero-initで、ODEソルバーの最初の数ステップをゼロにします。テキストから画像（Lumina-Next、Stable Diffusion 3、Flux）およびテキストから動画（Wan-2.1）の生成において、CFG-Zero*はCFGを超える結果を示し、フローマッチングモデルのガイドにおいて効果的であることを明らかにします。コードはgithub.com/WeichenFan/CFG-Zero-starに公開されています。",
      "upvotes": 7,
      "discussionId": "67e21d3884513315a9169bba",
      "projectPage": "https://weichenfan.github.io/webpage-cfg-zero-star/",
      "githubRepo": "https://github.com/WeichenFan/CFG-Zero-star",
      "ai_keywords": [
        "Classifier-Free Guidance (CFG)",
        "diffusion/flow models",
        "image fidelity",
        "controllability",
        "flow matching models",
        "Gaussian mixtures",
        "ground-truth flow",
        "flow estimation",
        "estimated velocity",
        "scalar optimization",
        "ODE solver",
        "text-to-image",
        "Lumina-Next",
        "Stable Diffusion 3",
        "Flux",
        "text-to-video",
        "Wan-2.1",
        "CFG-Zero*"
      ]
    },
    "publishedAt": "2025-03-24T12:59:57.000Z",
    "title": "CFG-Zero*: Improved Classifier-Free Guidance for Flow Matching Models",
    "summary": "Classifier-Free Guidance (CFG) is a widely adopted technique in\ndiffusion/flow models to improve image fidelity and controllability. In this\nwork, we first analytically study the effect of CFG on flow matching models\ntrained on Gaussian mixtures where the ground-truth flow can be derived. We\nobserve that in the early stages of training, when the flow estimation is\ninaccurate, CFG directs samples toward incorrect trajectories. Building on this\nobservation, we propose CFG-Zero*, an improved CFG with two contributions: (a)\noptimized scale, where a scalar is optimized to correct for the inaccuracies in\nthe estimated velocity, hence the * in the name; and (b) zero-init, which\ninvolves zeroing out the first few steps of the ODE solver. Experiments on both\ntext-to-image (Lumina-Next, Stable Diffusion 3, and Flux) and text-to-video\n(Wan-2.1) generation demonstrate that CFG-Zero* consistently outperforms CFG,\nhighlighting its effectiveness in guiding Flow Matching models. (Code is\navailable at github.com/WeichenFan/CFG-Zero-star)",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18886.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6481764e8af4675862efb22e",
      "avatarUrl": "/avatars/fc2e076bc861693f598a528a068a696e.svg",
      "fullname": "weichenfan",
      "name": "weepiess2383",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.18923",
      "authors": [
        {
          "_id": "67e226f401cdb8cf3a1c7cd8",
          "user": {
            "_id": "640222f83e3d0f2745b097b2",
            "avatarUrl": "/avatars/c5dbac84734855369a7f57b051f16caa.svg",
            "isPro": false,
            "fullname": "Meng Cao",
            "user": "mengcao",
            "type": "user"
          },
          "name": "Meng Cao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:22:38.738Z",
          "hidden": false
        },
        {
          "_id": "67e226f401cdb8cf3a1c7cd9",
          "name": "Pengfei Hu",
          "hidden": false
        },
        {
          "_id": "67e226f401cdb8cf3a1c7cda",
          "name": "Yingyao Wang",
          "hidden": false
        },
        {
          "_id": "67e226f401cdb8cf3a1c7cdb",
          "user": {
            "_id": "65733c1b244aefdfc45cc771",
            "avatarUrl": "/avatars/7223cedbeed065c28a400e130cea30ae.svg",
            "isPro": false,
            "fullname": "Jihao Guo",
            "user": "grejioh",
            "type": "user"
          },
          "name": "Jihao Gu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:22:23.718Z",
          "hidden": false
        },
        {
          "_id": "67e226f401cdb8cf3a1c7cdc",
          "name": "Haoran Tang",
          "hidden": false
        },
        {
          "_id": "67e226f401cdb8cf3a1c7cdd",
          "name": "Haoze Zhao",
          "hidden": false
        },
        {
          "_id": "67e226f401cdb8cf3a1c7cde",
          "name": "Jiahua Dong",
          "hidden": false
        },
        {
          "_id": "67e226f401cdb8cf3a1c7cdf",
          "user": {
            "_id": "63f095be6309c84d5f48848a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f095be6309c84d5f48848a/pL2CKi-r-0mMfIGhYSAsm.jpeg",
            "isPro": false,
            "fullname": "Wangbo Yu",
            "user": "Drexubery",
            "type": "user"
          },
          "name": "Wangbo Yu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:23:02.455Z",
          "hidden": false
        },
        {
          "_id": "67e226f401cdb8cf3a1c7ce0",
          "user": {
            "_id": "638efcf4c67af472d316d424",
            "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
            "isPro": false,
            "fullname": "Ge Zhang",
            "user": "zhangysk",
            "type": "user"
          },
          "name": "Ge Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:22:08.334Z",
          "hidden": false
        },
        {
          "_id": "67e226f401cdb8cf3a1c7ce1",
          "name": "Ian Reid",
          "hidden": false
        },
        {
          "_id": "67e226f401cdb8cf3a1c7ce2",
          "name": "Xiaodan Liang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-24T17:46:09.000Z",
      "submittedOnDailyAt": "2025-03-25T02:17:41.453Z",
      "title": "Video SimpleQA: 大規模ビデオ言語モデルの事実性評価に向けて",
      "submittedOnDailyBy": {
        "_id": "638efcf4c67af472d316d424",
        "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
        "isPro": false,
        "fullname": "Ge Zhang",
        "user": "zhangysk",
        "type": "user"
      },
      "summary": "最近の大型ビデオ言語モデル（LVLMs）の進歩は、多モード理解の可能性を明らかにしたが、ビデオコンテキストでの事実的な基礎の評価は、重要な未解決の課題である。この空間を填ぐために、私たちはVideo SimpleQAを紹介します。これは、LVLMsの事実性評価に適した最初の詳細なベンチマークです。私たちの研究は、以下の主要な特徴から異なります：1）知識の必要性：外部知識の追加統合を要求し、明示的なナレーションを超える；2）事実探求の質問：主観的な解釈を避けた目標的的、議論のないイベントや関係を標的とします；3）確定的で短形の回答：回答は、短形で無難で確定的に正確な形で作成され、LLM-as-a-judgeフレームワークを通じて自動評価可能で、最小限のスコアの変動を伴う；4）外部ソースの検証：すべての注釈は、権威的な外部リソースとの厳密な検証を通じて信頼性を確保します；5）時系列推理の必要性：注釈された質問の種類は、静的な単一フレームの理解と動的な時系列推理を含む、長期のコンテキスト依存関係でLVLMsの事実性を明確に評価します。私たちは41つの最先端のLVLMsを拡張的に評価し、以下の要約された主要な発見をまとめます：1）現在のLVLMsは、特にオープンソースモデルで事実的な従れば性に顕著な欠点を見出します。最良の性能を示すモデル、Gemini-1.5-Proは、Fスコアの54.4%だけです；2）テスト時の計算パラダイムは、有意な性能の向上は見られません、事実性の向上を後払い計算を通じて実現するための基本的な制限を明らかにします；3）検索アウジェニングは、追加の推論時間オーバーヘッドを費やしながら、一貫した改善を示し、重要な効率性と性能のトレードオフを示します。",
      "upvotes": 6,
      "discussionId": "67e226f601cdb8cf3a1c7d73",
      "projectPage": "https://videosimpleqa.github.io",
      "ai_keywords": [
        "Large Video Language Models (LVLMs)",
        "multi-modal understanding",
        "factuality evaluation",
        "Video SimpleQA",
        "external knowledge",
        "objective events",
        "relationships",
        "short-form answer",
        "LLM-as-a-judge",
        "automated evaluation",
        "scQUIre",
        "authoritative external references",
        "temporal reasoning",
        "long-context dependencies",
        "F-score",
        "test-time compute",
        "Retrieval-Augmented Generation",
        "inference time overhead",
        "efficiency-performance trade-off"
      ]
    },
    "publishedAt": "2025-03-24T13:46:09.000Z",
    "title": "Video SimpleQA: Towards Factuality Evaluation in Large Video Language\n  Models",
    "summary": "Recent advancements in Large Video Language Models (LVLMs) have highlighted\ntheir potential for multi-modal understanding, yet evaluating their factual\ngrounding in video contexts remains a critical unsolved challenge. To address\nthis gap, we introduce Video SimpleQA, the first comprehensive benchmark\ntailored for factuality evaluation of LVLMs. Our work distinguishes from\nexisting video benchmarks through the following key features: 1) Knowledge\nrequired: demanding integration of external knowledge beyond the explicit\nnarrative; 2) Fact-seeking question: targeting objective, undisputed events or\nrelationships, avoiding subjective interpretation; 3) Definitive & short-form\nanswer: Answers are crafted as unambiguous and definitively correct in a short\nformat, enabling automated evaluation through LLM-as-a-judge frameworks with\nminimal scoring variance; 4) External-source verified: All annotations undergo\nrigorous validation against authoritative external references to ensure the\nreliability; 5) Temporal reasoning required: The annotated question types\nencompass both static single-frame understanding and dynamic temporal\nreasoning, explicitly evaluating LVLMs factuality under the long-context\ndependencies. We extensively evaluate 41 state-of-the-art LVLMs and summarize\nkey findings as follows: 1) Current LVLMs exhibit notable deficiencies in\nfactual adherence, particularly for open-source models. The best-performing\nmodel Gemini-1.5-Pro achieves merely an F-score of 54.4%; 2) Test-time compute\nparadigms show insignificant performance gains, revealing fundamental\nconstraints for enhancing factuality through post-hoc computation; 3)\nRetrieval-Augmented Generation demonstrates consistent improvements at the cost\nof additional inference time overhead, presenting a critical\nefficiency-performance trade-off.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18923.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "638efcf4c67af472d316d424",
      "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
      "fullname": "Ge Zhang",
      "name": "zhangysk",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 43
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.14428",
      "authors": [
        {
          "_id": "67e217941cb9bded659267f0",
          "name": "Hongyu Zhang",
          "hidden": false
        },
        {
          "_id": "67e217941cb9bded659267f1",
          "user": {
            "_id": "64210d1fd039a891a914986d",
            "avatarUrl": "/avatars/b178a768657eb223bdbfbd9e0a2000ff.svg",
            "isPro": false,
            "fullname": "Yufan Deng",
            "user": "dyf",
            "type": "user"
          },
          "name": "Yufan Deng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:28:50.390Z",
          "hidden": false
        },
        {
          "_id": "67e217941cb9bded659267f2",
          "user": {
            "_id": "63468720dd6d90d82ccf3450",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
            "isPro": false,
            "fullname": "YSH",
            "user": "BestWishYsh",
            "type": "user"
          },
          "name": "Shenghai Yuan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-25T08:20:33.315Z",
          "hidden": false
        },
        {
          "_id": "67e217941cb9bded659267f3",
          "user": {
            "_id": "63ad0b04e3b217fb36d36c13",
            "avatarUrl": "/avatars/5a3715ba20859052ba04c048db9e03c2.svg",
            "isPro": false,
            "fullname": "Peng Jin",
            "user": "Pengjin",
            "type": "user"
          },
          "name": "Peng Jin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:29:15.403Z",
          "hidden": false
        },
        {
          "_id": "67e217941cb9bded659267f4",
          "user": {
            "_id": "65b2529285b6c21448a10d65",
            "avatarUrl": "/avatars/1b09e2742aecce1bbdc57f0c4504cf38.svg",
            "isPro": false,
            "fullname": "Zesen Cheng",
            "user": "ClownRat",
            "type": "user"
          },
          "name": "Zesen Cheng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:28:42.575Z",
          "hidden": false
        },
        {
          "_id": "67e217941cb9bded659267f5",
          "name": "Yian Zhao",
          "hidden": false
        },
        {
          "_id": "67e217941cb9bded659267f6",
          "name": "Chang Liu",
          "hidden": false
        },
        {
          "_id": "67e217941cb9bded659267f7",
          "name": "Jie Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-18T17:02:14.000Z",
      "submittedOnDailyAt": "2025-03-25T01:10:50.245Z",
      "title": "マジックコンポーネント: トレーニング無しの二段階精調整を用いた組成ビデオ生成",
      "submittedOnDailyBy": {
        "_id": "63468720dd6d90d82ccf3450",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
        "isPro": false,
        "fullname": "YSH",
        "user": "BestWishYsh",
        "type": "user"
      },
      "summary": "テキストからビデオ（T2V）の生成は、拡散モデルを用いて進歩しています。しかし、現在の方法は、属性の正確な結合、空間関係の決定、複雑な行動相互作用の捉え方に難問を抱えています。これらの制限を解決するために、我々は、二段階の精修を通じた構成的なT2V生成を強化するためのトレーニング無制限の方法、MagicCompを提案します。特に、(1)条件化ステージでは、Semantic Anchor Disambiguationを導入し、主題特有の意味を強化し、主題間の不明確性を進歩的に解決するために、セマンティックアンカーの方向ベクトルを元のテキスト埋め込みに注入します。(2)デノイズステージでは、Dynamic Layout Fusion Attentionを提案し、地元先行知識とモデル適応的な空間認識を統合し、マスク付きアテンション調節を通じて、主題をスペクトラルタイム領域に柔軟に結び付けます。また、MagicCompはモデル無依存で機能的であり、現在のT2Vアーキテクチャに無難に統合可能です。T2V-CompBenchとVBenchの拡張的な実験により、MagicCompは最先端の方法を超える性能を示し、複雑なプロンプトベースや軌道制御可能なビデオ生成などの應用の可能性を強調しています。プロジェクトページ：https://hong-yu-zhang.github.io/MagicComp-Page/。",
      "upvotes": 6,
      "discussionId": "67e217981cb9bded65926978",
      "projectPage": "https://hong-yu-zhang.github.io/MagicComp-Page/",
      "githubRepo": "https://github.com/Hong-yu-Zhang/MagicComp",
      "ai_keywords": [
        "Semantic Anchor Disambiguation",
        "Dynamic Layout Fusion Attention",
        "grounding priors",
        "model-adaptive spatial perception",
        "masked attention modulation"
      ]
    },
    "publishedAt": "2025-03-18T13:02:14.000Z",
    "title": "MagicComp: Training-free Dual-Phase Refinement for Compositional Video\n  Generation",
    "summary": "Text-to-video (T2V) generation has made significant strides with diffusion\nmodels. However, existing methods still struggle with accurately binding\nattributes, determining spatial relationships, and capturing complex action\ninteractions between multiple subjects. To address these limitations, we\npropose MagicComp, a training-free method that enhances compositional T2V\ngeneration through dual-phase refinement. Specifically, (1) During the\nConditioning Stage: We introduce the Semantic Anchor Disambiguation to\nreinforces subject-specific semantics and resolve inter-subject ambiguity by\nprogressively injecting the directional vectors of semantic anchors into\noriginal text embedding; (2) During the Denoising Stage: We propose Dynamic\nLayout Fusion Attention, which integrates grounding priors and model-adaptive\nspatial perception to flexibly bind subjects to their spatiotemporal regions\nthrough masked attention modulation. Furthermore, MagicComp is a model-agnostic\nand versatile approach, which can be seamlessly integrated into existing T2V\narchitectures. Extensive experiments on T2V-CompBench and VBench demonstrate\nthat MagicComp outperforms state-of-the-art methods, highlighting its potential\nfor applications such as complex prompt-based and trajectory-controllable video\ngeneration. Project page: https://hong-yu-zhang.github.io/MagicComp-Page/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14428.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63468720dd6d90d82ccf3450",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
      "fullname": "YSH",
      "name": "BestWishYsh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 35
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.18908",
      "authors": [
        {
          "_id": "67e230fd4b9f234b60d06389",
          "user": {
            "_id": "66857bd849a4ed9de4c31936",
            "avatarUrl": "/avatars/f6f016bf36fad5b29f30fbec6cde3e4d.svg",
            "isPro": false,
            "fullname": "Akhiad Bercovich",
            "user": "abercovich",
            "type": "user"
          },
          "name": "Akhiad Bercovich",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:30:49.482Z",
          "hidden": false
        },
        {
          "_id": "67e230fd4b9f234b60d0638a",
          "user": {
            "_id": "6756aa3741b39ab0d327de52",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/VjjYWljIgPn9HEMDyKtft.png",
            "isPro": false,
            "fullname": "Mohammad Dabbah",
            "user": "mdabbah-nvidia",
            "type": "user"
          },
          "name": "Mohammad Dabbah",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:31:05.696Z",
          "hidden": false
        },
        {
          "_id": "67e230fd4b9f234b60d0638b",
          "user": {
            "_id": "6509a96c61c4bb4636fd0fd2",
            "avatarUrl": "/avatars/8ffa9b4dd698469f7d70d4d9144aac82.svg",
            "isPro": false,
            "fullname": "Omri Puny",
            "user": "omripuny",
            "type": "user"
          },
          "name": "Omri Puny",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:31:13.161Z",
          "hidden": false
        },
        {
          "_id": "67e230fd4b9f234b60d0638c",
          "name": "Ido Galil",
          "hidden": false
        },
        {
          "_id": "67e230fd4b9f234b60d0638d",
          "user": {
            "_id": "65006cac12c1442d993d6d51",
            "avatarUrl": "/avatars/6700109303b902d453f3d8e2b45a103f.svg",
            "isPro": false,
            "fullname": "Geifman",
            "user": "AmnonGeifman",
            "type": "user"
          },
          "name": "Amnon Geifman",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:31:22.938Z",
          "hidden": false
        },
        {
          "_id": "67e230fd4b9f234b60d0638e",
          "user": {
            "_id": "604bc69e0fe8ff3ec13d71cd",
            "avatarUrl": "/avatars/fe4b14b24befdbed02eecb43a25c67f4.svg",
            "isPro": false,
            "fullname": "Yonatan Geifman",
            "user": "geifmany",
            "type": "user"
          },
          "name": "Yonatan Geifman",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:31:30.392Z",
          "hidden": false
        },
        {
          "_id": "67e230fd4b9f234b60d0638f",
          "name": "Izhak Golan",
          "hidden": false
        },
        {
          "_id": "67e230fd4b9f234b60d06390",
          "name": "Ehud Karpas",
          "hidden": false
        },
        {
          "_id": "67e230fd4b9f234b60d06391",
          "user": {
            "_id": "668578fdd24e614fec97eac8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/668578fdd24e614fec97eac8/n5xYnqo5nQbVX2tgaRfEi.jpeg",
            "isPro": false,
            "fullname": "Itay Levy",
            "user": "itlevy",
            "type": "user"
          },
          "name": "Itay Levy",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:31:50.928Z",
          "hidden": false
        },
        {
          "_id": "67e230fd4b9f234b60d06392",
          "user": {
            "_id": "61ee58f1af500c0acfc4d8eb",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1643010228464-noauth.png",
            "isPro": false,
            "fullname": "Zach Moshe",
            "user": "zachmoshe",
            "type": "user"
          },
          "name": "Zach Moshe",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:31:56.910Z",
          "hidden": false
        },
        {
          "_id": "67e230fd4b9f234b60d06393",
          "user": {
            "_id": "63a16d5d5d09b819fee9a350",
            "avatarUrl": "/avatars/d1a3fef0131688e92e272cbd80856fc3.svg",
            "isPro": false,
            "fullname": "Najeeb Nabwani",
            "user": "NajeebDeci",
            "type": "user"
          },
          "name": "Najeeb Nabwani",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:32:02.613Z",
          "hidden": false
        },
        {
          "_id": "67e230fd4b9f234b60d06394",
          "user": {
            "_id": "6671634f1820f293a9995b12",
            "avatarUrl": "/avatars/50c8f7b4bfb00f2169b808f3c72c7686.svg",
            "isPro": false,
            "fullname": "Tomer Ronen",
            "user": "tomer-nv",
            "type": "user"
          },
          "name": "Tomer Ronen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:32:10.072Z",
          "hidden": false
        },
        {
          "_id": "67e230fd4b9f234b60d06395",
          "user": {
            "_id": "665f0a46f065b1d42806000d",
            "avatarUrl": "/avatars/927f042a3c95c5846621e2a381c66bbf.svg",
            "isPro": false,
            "fullname": "Itamar Schen",
            "user": "ischen-nvidia",
            "type": "user"
          },
          "name": "Itamar Schen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:32:17.605Z",
          "hidden": false
        },
        {
          "_id": "67e230fd4b9f234b60d06396",
          "user": {
            "_id": "5f5b0efe10b2753d9000c888",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1628140531144-5f5b0efe10b2753d9000c888.jpeg",
            "isPro": false,
            "fullname": "Elad Segal",
            "user": "eladsegal",
            "type": "user"
          },
          "name": "Elad Segal",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:32:24.511Z",
          "hidden": false
        },
        {
          "_id": "67e230fd4b9f234b60d06397",
          "user": {
            "_id": "666ef13c14f1c262feeb706c",
            "avatarUrl": "/avatars/7dca59acf5e069d96bdbb98dace9199b.svg",
            "isPro": false,
            "fullname": "Ido Shahaf",
            "user": "ishahaf",
            "type": "user"
          },
          "name": "Ido Shahaf",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:32:30.927Z",
          "hidden": false
        },
        {
          "_id": "67e230fd4b9f234b60d06398",
          "user": {
            "_id": "66b089f14ae4ae811218cdb6",
            "avatarUrl": "/avatars/a50fe725922dfdbe0e731fade381b22e.svg",
            "isPro": false,
            "fullname": "Oren Tropp",
            "user": "otropp",
            "type": "user"
          },
          "name": "Oren Tropp",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:32:38.161Z",
          "hidden": false
        },
        {
          "_id": "67e230fd4b9f234b60d06399",
          "user": {
            "_id": "666027917c3f9c72113cc75c",
            "avatarUrl": "/avatars/a276ebe8e2731b6a05e3c61c2ae0ddae.svg",
            "isPro": false,
            "fullname": "Ran Zilberstein",
            "user": "RanZilberstein-Nvidia",
            "type": "user"
          },
          "name": "Ran Zilberstein",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:32:44.768Z",
          "hidden": false
        },
        {
          "_id": "67e230fd4b9f234b60d0639a",
          "user": {
            "_id": "65758349983403462a54ac06",
            "avatarUrl": "/avatars/4f337c732f31bd748738c2717b50a99c.svg",
            "isPro": false,
            "fullname": "Ran El-Yaniv",
            "user": "ranielyaniv",
            "type": "user"
          },
          "name": "Ran El-Yaniv",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:32:56.726Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-24T17:20:35.000Z",
      "submittedOnDailyAt": "2025-03-25T02:59:12.174Z",
      "title": "FFN Fusion: リティフィング シーケンシャルな計算を再考して 大規模言語モデル",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "FFN Fusionは、大規模言語モデルの順次計算を削減するアーキテクチャ最適化手法です。これは、自然な並列化の機会を識別し、利用することで効果的です。私たちの主な見解は、Feed-Forward Network (FFN) 層の順列が、特定のアテンション層を削除後に残った場合、通常は精度の影響が最小限で、並列化可能であることです。私たちは、このような順列の識別と結合の原則的な方法を開発し、それらを並列操作に変換し、推論時間を大幅に削減しながらモデルの挙動を保持することができます。Llama-3.1-405B-Instructにこれらの手法を適用し、Llama-Nemotron-Ultra-253B-Base (Ultra-253B-Base)を作成しました。これは、効率的で、将来公開になるモデルで、推論時間が1.71倍高速化し、1ターゲットごとのコストが35倍低くなり、ベンチマークで強い性能を維持します。49Bから253Bパラメータのモデルに対して拡散的な実験を行い、FFN Fusionは大きなサイズで効果的になり、既存の最適化手法（例えば、量化と削減）に補完できることを示しました。特に興味深いながら、アテンションとFFN層を含む完全なトランスフォーマーブロックが、時にも並列化可能であることを見出しました。これは、神経アーキテクチャ設計の新しい方向を示しています。",
      "upvotes": 5,
      "discussionId": "67e230fe4b9f234b60d063ec",
      "ai_keywords": [
        "FFN Fusion",
        "Feed-Forward Network (FFN)",
        "parallelization",
        "inference latency",
        "Llama-3.1-405B-Instruct",
        "Ultra-253B-Base",
        "model behavior",
        "per-token cost",
        "benchmarks",
        "transformer blocks",
        "quantization",
        "pruning"
      ]
    },
    "publishedAt": "2025-03-24T13:20:35.000Z",
    "title": "FFN Fusion: Rethinking Sequential Computation in Large Language Models",
    "summary": "We introduce FFN Fusion, an architectural optimization technique that reduces\nsequential computation in large language models by identifying and exploiting\nnatural opportunities for parallelization. Our key insight is that sequences of\nFeed-Forward Network (FFN) layers, particularly those remaining after the\nremoval of specific attention layers, can often be parallelized with minimal\naccuracy impact. We develop a principled methodology for identifying and fusing\nsuch sequences, transforming them into parallel operations that significantly\nreduce inference latency while preserving model behavior. Applying these\ntechniques to Llama-3.1-405B-Instruct, we create Llama-Nemotron-Ultra-253B-Base\n(Ultra-253B-Base), an efficient and soon-to-be publicly available model that\nachieves a 1.71X speedup in inference latency and 35X lower per-token cost\nwhile maintaining strong performance across benchmarks. Through extensive\nexperiments on models from 49B to 253B parameters, we demonstrate that FFN\nFusion becomes increasingly effective at larger scales and can complement\nexisting optimization techniques like quantization and pruning. Most\nintriguingly, we find that even full transformer blocks containing both\nattention and FFN layers can sometimes be parallelized, suggesting new\ndirections for neural architecture design.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18908.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6456
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.18102",
      "authors": [
        {
          "_id": "67e22206ddc9b120cbde6fbe",
          "name": "Samuel Schmidgall",
          "hidden": false
        },
        {
          "_id": "67e22206ddc9b120cbde6fbf",
          "user": {
            "_id": "6438d1d843d932c462404500",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6438d1d843d932c462404500/6UrtJbed9N5ETbImgIh-C.png",
            "isPro": false,
            "fullname": "Michael Moor",
            "user": "mdmoor",
            "type": "user"
          },
          "name": "Michael Moor",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:34:00.877Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-23T15:16:42.000Z",
      "submittedOnDailyAt": "2025-03-25T05:09:36.523Z",
      "title": "AgentRxiv: コラボレーションモデルのための自動調査のためのアウトライン",
      "submittedOnDailyBy": {
        "_id": "6438d1d843d932c462404500",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6438d1d843d932c462404500/6UrtJbed9N5ETbImgIh-C.png",
        "isPro": false,
        "fullname": "Michael Moor",
        "user": "mdmoor",
        "type": "user"
      },
      "summary": "科学発見の進歩は、少なくとも一つの「エフリーカ」（Eureka）の瞬時的なモーメントではなく、数百人の科学者が共有の目標を手に入れるために段階的に協力していく結果である。現在のアガントワークフローは、自動的に研究を行うことができるが、それは孤立して、前の研究結果について継続的に改善する能力を持っていない。これらの課題に対処するために、我々はAgentRxivというフレームワークを紹介し、LLMアガント実験室が共有予稿サーバーから報告書をアップロードし、取り出すことを可能にし、協力、インサイトの共有、相互に研究を築くことを可能にします。アガント実験室には、新しい理由論とプロンプティングテクニックの開発を課題にし、前の研究にアクセス可能なアガントが孤立した状態でのアガントに比べて高い性能向上（基準値に対する11.4%の相対的な向上）を収めることを見出しました。最も優れた実績を示す戦略は、その他の領域のベンチマークにも一般化でき、平均で3.3%の向上を収めました。AgentRxivを通じて研究を共有する複数のアガント実験室は、共有の目標を手に入れるために協力し、孤立した実験室よりも速く進むことができ、全体的な精度（基準値に対する13.7%の相対的な向上）を達成します。これらの発見は、自動的なアガントは将来のAIシステムの設計にヒューマンとともに役立つことができることを示しています。我々は、AgentRxivがアガントが研究の目標に向かって協力し、研究者の発見を加速させることを望みます。",
      "upvotes": 5,
      "discussionId": "67e22207ddc9b120cbde702c",
      "ai_keywords": [
        "LLM (Large Language Model)",
        "agent laboratories",
        "preprint server",
        "reasoning techniques",
        "prompting techniques",
        "performance improvements",
        "benchmarks",
        "accuracy"
      ]
    },
    "publishedAt": "2025-03-23T11:16:42.000Z",
    "title": "AgentRxiv: Towards Collaborative Autonomous Research",
    "summary": "Progress in scientific discovery is rarely the result of a single \"Eureka\"\nmoment, but is rather the product of hundreds of scientists incrementally\nworking together toward a common goal. While existing agent workflows are\ncapable of producing research autonomously, they do so in isolation, without\nthe ability to continuously improve upon prior research results. To address\nthese challenges, we introduce AgentRxiv-a framework that lets LLM agent\nlaboratories upload and retrieve reports from a shared preprint server in order\nto collaborate, share insights, and iteratively build on each other's research.\nWe task agent laboratories to develop new reasoning and prompting techniques\nand find that agents with access to their prior research achieve higher\nperformance improvements compared to agents operating in isolation (11.4%\nrelative improvement over baseline on MATH-500). We find that the best\nperforming strategy generalizes to benchmarks in other domains (improving on\naverage by 3.3%). Multiple agent laboratories sharing research through\nAgentRxiv are able to work together towards a common goal, progressing more\nrapidly than isolated laboratories, achieving higher overall accuracy (13.7%\nrelative improvement over baseline on MATH-500). These findings suggest that\nautonomous agents may play a role in designing future AI systems alongside\nhumans. We hope that AgentRxiv allows agents to collaborate toward research\ngoals and enables researchers to accelerate discovery.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18102.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6438d1d843d932c462404500",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6438d1d843d932c462404500/6UrtJbed9N5ETbImgIh-C.png",
      "fullname": "Michael Moor",
      "name": "mdmoor",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.15879",
      "authors": [
        {
          "_id": "67dea7cc5b44ace7a30e237e",
          "user": {
            "_id": "6540fbf9cb7fffd683942b43",
            "avatarUrl": "/avatars/d4a64fbde511d0949e1c339179586850.svg",
            "isPro": false,
            "fullname": "DongGeon Lee",
            "user": "oneonlee",
            "type": "user"
          },
          "name": "DongGeon Lee",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-24T08:52:18.850Z",
          "hidden": false
        },
        {
          "_id": "67dea7cc5b44ace7a30e237f",
          "name": "Ahjeong Park",
          "hidden": false
        },
        {
          "_id": "67dea7cc5b44ace7a30e2380",
          "user": {
            "_id": "666a8be869a08ea4aac5e73e",
            "avatarUrl": "/avatars/be42632414bafb0af74b5f4d4f03d223.svg",
            "isPro": false,
            "fullname": "keira lee",
            "user": "keirahrlee",
            "type": "user"
          },
          "name": "Hyeri Lee",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-22T12:06:36.938Z",
          "hidden": false
        },
        {
          "_id": "67dea7cc5b44ace7a30e2381",
          "name": "Hyeonseo Nam",
          "hidden": false
        },
        {
          "_id": "67dea7cc5b44ace7a30e2382",
          "name": "Yunho Maeng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-20T06:04:12.000Z",
      "submittedOnDailyAt": "2025-03-25T03:48:51.972Z",
      "title": "Typed-RAG: タイプに関係する多面分解による非ファクトオードクエスチョンの答えを求める",
      "submittedOnDailyBy": {
        "_id": "6540fbf9cb7fffd683942b43",
        "avatarUrl": "/avatars/d4a64fbde511d0949e1c339179586850.svg",
        "isPro": false,
        "fullname": "DongGeon Lee",
        "user": "oneonlee",
        "type": "user"
      },
      "summary": "非ファクトイド問題回答（NFQA）は、その開放的な性質、多様なインテントと多面的な理由の必要性により、単なるファクトイドQAアプローチ（特に、リビューアグメント生成（RAG））による手法が不十分であるという重大な課題を抱えています。ファクトイド問題に比べ、非ファクトイド問題（NFQ）は確定的な答えを持たないことで、複数のソースからの情報を統合し、多様な理由の次元での理由を求める必要があります。これらの制限を解決するために、私たちはRAGパラダイム内でのタイプに関する多面分解フレームワーク「Typed-RAG」を紹介します。Typed-RAGは、デバイル、経験、比較などの特定のタイプにNFQを分類し、アスペクトベースの分解を適用してリビューと生成の戦略を精確化します。多面のNFQを単一のアスペクトのサブクエリに分解し、結果を集計することで、Typed-RAGは情報量の多くのより情報的でコンテキストに適した回答を生成します。Typed-RAGの評価において、私たちは、多様なNFQタイプをカバーするベンチマークデータセット「Wiki-NFQA」を紹介します。実験結果は、Typed-RAGがベースラインを上回っていることを示し、NFQAでの有效なリビューと生成におけるタイプに関する分解の重要性を明らかにしています。私たちのコードとデータセットは以下のURLで提供されています。\nhttps://github.com/TeamNLP/Typed-RAG{https://github.com/TeamNLP/Typed-RAG}",
      "upvotes": 5,
      "discussionId": "67dea7cc5b44ace7a30e23b8",
      "githubRepo": "https://github.com/TeamNLP/Typed-RAG",
      "ai_keywords": [
        "retrieval-augmented generation (RAG)",
        "non-factoid question-answering (NFQA)",
        "multi-aspect reasoning",
        "type-aware multi-aspect decomposition framework",
        "single-aspect sub-queries",
        "Wiki-NFQA",
        "type-aware decomposition"
      ]
    },
    "publishedAt": "2025-03-20T02:04:12.000Z",
    "title": "Typed-RAG: Type-aware Multi-Aspect Decomposition for Non-Factoid\n  Question Answering",
    "summary": "Non-factoid question-answering (NFQA) poses a significant challenge due to\nits open-ended nature, diverse intents, and the need for multi-aspect\nreasoning, which renders conventional factoid QA approaches, including\nretrieval-augmented generation (RAG), inadequate. Unlike factoid questions,\nnon-factoid questions (NFQs) lack definitive answers and require synthesizing\ninformation from multiple sources across various reasoning dimensions. To\naddress these limitations, we introduce Typed-RAG, a type-aware multi-aspect\ndecomposition framework within the RAG paradigm for NFQA. Typed-RAG classifies\nNFQs into distinct types -- such as debate, experience, and comparison -- and\napplies aspect-based decomposition to refine retrieval and generation\nstrategies. By decomposing multi-aspect NFQs into single-aspect sub-queries and\naggregating the results, Typed-RAG generates more informative and contextually\nrelevant responses. To evaluate Typed-RAG, we introduce Wiki-NFQA, a benchmark\ndataset covering diverse NFQ types. Experimental results demonstrate that\nTyped-RAG outperforms baselines, thereby highlighting the importance of\ntype-aware decomposition for effective retrieval and generation in NFQA. Our\ncode and dataset are available at\nhttps://github.com/TeamNLP/Typed-RAG{https://github.com/TeamNLP/Typed-RAG}.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.15879.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6540fbf9cb7fffd683942b43",
      "avatarUrl": "/avatars/d4a64fbde511d0949e1c339179586850.svg",
      "fullname": "DongGeon Lee",
      "name": "oneonlee",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.18866",
      "authors": [
        {
          "_id": "67e2290da4525cbb1d718ae2",
          "user": {
            "_id": "65619949d2e4352d64365606",
            "avatarUrl": "/avatars/4241b6e901e146a9ff8b5bc7de3f960a.svg",
            "isPro": true,
            "fullname": "Yangjun Ruan",
            "user": "ryoungj",
            "type": "user"
          },
          "name": "Yangjun Ruan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:33:10.423Z",
          "hidden": false
        },
        {
          "_id": "67e2290da4525cbb1d718ae3",
          "user": {
            "_id": "630bc38809eceb8fafe5ed7f",
            "avatarUrl": "/avatars/5f2a1268f8a7b51cca8446ef0be6445f.svg",
            "isPro": true,
            "fullname": "Neil Band",
            "user": "nband",
            "type": "user"
          },
          "name": "Neil Band",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:33:16.895Z",
          "hidden": false
        },
        {
          "_id": "67e2290da4525cbb1d718ae4",
          "user": {
            "_id": "66a7f54fbb22d7e78a2aeaf4",
            "avatarUrl": "/avatars/3ab8899935f8f7b14e89c623cc6c0fd2.svg",
            "isPro": false,
            "fullname": "Chris J. Maddison",
            "user": "cmaddis",
            "type": "user"
          },
          "name": "Chris J. Maddison",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:33:22.633Z",
          "hidden": false
        },
        {
          "_id": "67e2290da4525cbb1d718ae5",
          "name": "Tatsunori Hashimoto",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-24T16:41:23.000Z",
      "submittedOnDailyAt": "2025-03-25T02:25:31.127Z",
      "title": "レーテンテイクスから学ぶ理由",
      "submittedOnDailyBy": {
        "_id": "65619949d2e4352d64365606",
        "avatarUrl": "/avatars/4241b6e901e146a9ff8b5bc7de3f960a.svg",
        "isPro": true,
        "fullname": "Yangjun Ruan",
        "user": "ryoungj",
        "type": "user"
      },
      "summary": "LM予備学習のスケーリングは、人間書いた本文の増加よりも速く進み、データがLMスケーリングのボトルネックとなることを懸念している。このデータ制限の環境でLM予備学習を継続するために、我々は、文章生成プロセスにおける潜在的な思いを明記して推定することが予備学習データの効率化に大幅に効果的であることを提案します。直感的に、我々のアプローチは、ウェブテキストを詳しい人間の思いの圧縮された最終的な結果と見なし、潜在的な思いはデータ効率的な学習に重要なコンテキスト知識と理由化ステップを含んでいることを示します。我々は、データ制限の継続予備学習で我々のアプローチの効果性を実験的に示します。まず、合成データアプローチで潜在的な思いを推定することが予備学習データの効率化を大幅に向上させ、同量の裸データでの学習を上回ることを示します（MATHでは5.7%→25.4%）。さらに、強いティーチャーがない状態で潜在的な思いを推定することを示します。LMはEMアルゴリズムを用いて、学習されたLMの能力と思いを加算した予備学習データの質を連続的に向上させ、自らの性能をスタートアップします。1BのLMは少なくとも3回のイテレーションで性能をスケールすることを示し、裸データでの基準を大幅に上回り、Eステップでの進捗を行うことで追加の推定コンピューティングからの効果を増加させます。推定スケーリングとEMイテレーションからの効果は、データ制限の予備学習のスケーリングに新たな機会を示していることを示します。",
      "upvotes": 4,
      "discussionId": "67e2290ea4525cbb1d718b18",
      "ai_keywords": [
        "latent thoughts",
        "data-efficient learning",
        "web text",
        "verbose human thought process",
        "synthetic data",
        "data-constrained regime",
        "EM algorithm",
        "thought-augmented pretraining data",
        "inference compute",
        "data-constrained pretraining"
      ]
    },
    "publishedAt": "2025-03-24T12:41:23.000Z",
    "title": "Reasoning to Learn from Latent Thoughts",
    "summary": "Compute scaling for language model (LM) pretraining has outpaced the growth\nof human-written texts, leading to concerns that data will become the\nbottleneck to LM scaling. To continue scaling pretraining in this\ndata-constrained regime, we propose that explicitly modeling and inferring the\nlatent thoughts that underlie the text generation process can significantly\nimprove pretraining data efficiency. Intuitively, our approach views web text\nas the compressed final outcome of a verbose human thought process and that the\nlatent thoughts contain important contextual knowledge and reasoning steps that\nare critical to data-efficient learning. We empirically demonstrate the\neffectiveness of our approach through data-constrained continued pretraining\nfor math. We first show that synthetic data approaches to inferring latent\nthoughts significantly improve data efficiency, outperforming training on the\nsame amount of raw data (5.7\\% rightarrow 25.4\\% on MATH). Furthermore, we\ndemonstrate latent thought inference without a strong teacher, where an LM\nbootstraps its own performance by using an EM algorithm to iteratively improve\nthe capability of the trained LM and the quality of thought-augmented\npretraining data. We show that a 1B LM can bootstrap its performance across at\nleast three iterations and significantly outperform baselines trained on raw\ndata, with increasing gains from additional inference compute when performing\nthe E-step. The gains from inference scaling and EM iterations suggest new\nopportunities for scaling data-constrained pretraining.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18866.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65619949d2e4352d64365606",
      "avatarUrl": "/avatars/4241b6e901e146a9ff8b5bc7de3f960a.svg",
      "fullname": "Yangjun Ruan",
      "name": "ryoungj",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.18013",
      "authors": [
        {
          "_id": "67e22902af6628c90b525a2b",
          "name": "Yufei Zhan",
          "hidden": false
        },
        {
          "_id": "67e22902af6628c90b525a2c",
          "name": "Yousong Zhu",
          "hidden": false
        },
        {
          "_id": "67e22902af6628c90b525a2d",
          "name": "Shurong Zheng",
          "hidden": false
        },
        {
          "_id": "67e22902af6628c90b525a2e",
          "name": "Hongyin Zhao",
          "hidden": false
        },
        {
          "_id": "67e22902af6628c90b525a2f",
          "name": "Fan Yang",
          "hidden": false
        },
        {
          "_id": "67e22902af6628c90b525a30",
          "name": "Ming Tang",
          "hidden": false
        },
        {
          "_id": "67e22902af6628c90b525a31",
          "name": "Jinqiao Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-23T10:21:14.000Z",
      "submittedOnDailyAt": "2025-03-25T02:25:32.811Z",
      "title": "Vision-R1: 大視覚言語モデルにおける人間無しのアラインメントの進化をビジョンガイドされた再強化学習をもとに",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "大視覚言語モデル（LVLMs）通常は2段階の訓練パラダイムを採用します：予ち練習と規範制御の微調節。最近、言語領域から派生した偏好最適化は、LVLMsの能力を強化する効果的な訓練後の強化戦略として登場しました。しかし、高品質の人間注釈された偏好データの構築と、これらの偏好を模倣する強力な報酬モデルの開発は、両方とも費用の高いだけでなく難しいです。この観察に基づき、我々はVision-R1を提案します。Vision-R1は、LVLMs向けの新しい視覚ガイドされたR1ライクの強化学習アルゴリズムで、確定的な視覚フィードバックを受けるモデルに報酬を与えます。これは、専門的な報酬モデルと手作りの偏好データセットの必要性を排除します。また、視覚タスクのロジックに基づいた評価を行うために、多次元のフィードバックを統合した規則駆動の報酬関数を採用します。さらに、進歩的なルール精練戦略を導入し、訓練中に報酬の評価基準を動的に調整し、モデルの連続的な向上と報酬ハッキングの抑制を実現します。分布内と分布外のベンチマークでの拡張的な実験により、Vision-R1での7B LVLMsの微調節は、統一的な性能向上を収め、50%程度の向上さえあり、最先端の10倍サイズモデルを超えました。",
      "upvotes": 4,
      "discussionId": "67e22903af6628c90b525a71",
      "ai_keywords": [
        "Large Vision-Language Models (LVLMs)",
        "pretraining",
        "supervised fine-tuning",
        "preference optimization",
        "reinforcement learning",
        "Vision-R1",
        "criterion-driven reward function",
        "progressive rule refinement strategy",
        "reward criteria",
        "model completions",
        "vision task logic",
        "reward hacking",
        "state-of-the-art"
      ]
    },
    "publishedAt": "2025-03-23T06:21:14.000Z",
    "title": "Vision-R1: Evolving Human-Free Alignment in Large Vision-Language Models\n  via Vision-Guided Reinforcement Learning",
    "summary": "Large Vision-Language Models (LVLMs) typically follow a two-stage training\nparadigm-pretraining and supervised fine-tuning. Recently, preference\noptimization, derived from the language domain, has emerged as an effective\npost-training reinforcement strategy to enhance capabilities of LVLMs. However,\nconstructing high-quality human-annotated preference data and developing robust\nreward models to mimic these preferences are both costly and challenging.\nMotivated by this observation, we propose Vision-R1, a novel vision-guided\nR1-like reinforcement learning algorithm for LVLMs that rewards models with\ndefinitive vision feedback. It only leverages curated instruction data,\neliminating the need for specialized reward models and handcrafted preference\ndatasets. We incorporate a criterion-driven reward function that further\nintegrates multi-dimensional feedback to evaluate model completions\ncomprehensively based on the vision task logic. Furthermore, we introduce a\nprogressive rule refinement strategy that dynamically adjusts the reward\ncriteria during training, enabling continuous model improvement and mitigating\nreward hacking. Extensive experiments on both in-distribution and\nout-of-distribution benchmarks demonstrate that fine-tuning the 7B LVLMs with\nVision-R1 achieves consistent performance gains, with even up to 50%\nimprovement and surpassing the state-of-the-art 10x size model.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18013.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6456
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.17422",
      "authors": [
        {
          "_id": "67e226b3b1acaf8a7680e926",
          "name": "Javier J. Poveda Rodrigo",
          "hidden": false
        },
        {
          "_id": "67e226b3b1acaf8a7680e927",
          "name": "Mohamed Amine Ahmdi",
          "hidden": false
        },
        {
          "_id": "67e226b3b1acaf8a7680e928",
          "name": "Alessio Burrello",
          "hidden": false
        },
        {
          "_id": "67e226b3b1acaf8a7680e929",
          "name": "Daniele Jahier Pagliari",
          "hidden": false
        },
        {
          "_id": "67e226b3b1acaf8a7680e92a",
          "name": "Luca Benini",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-21T09:00:19.000Z",
      "submittedOnDailyAt": "2025-03-25T02:15:09.508Z",
      "title": "V-Seek: オープンハードウェアサーバークラスのRISC-VプラットフォームでLLMの論理を加速する",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "最近のLarge Language Models (LLMs)の指数的な成長は、GPUベースのシステムに依存していましたが、CPUは推論および論理ワークロードを目指している場合には柔軟で低コストな選択肢として広く採用され始めています。RISC-Vはこの領域で急速に拡大しています。RISC-Vは開放的でビジネスバージョンに依存しないインストークシズム（ISA）を持っているためですが、LLMワークロードに適したRISC-Vハードウェアと対応するソフトウェアエコシステムは、ドメイン専門的なチューニングの必要性により完全に成熟しているわけではありません。この論文はこの隙を埋めることを目的としています。特に、最初の市販された多コアRISC-V CPUであるSophon SG2042上でのLLM推論の最適化を焦点としています。\n\n最近の最先端のLLMsで、論理向けに最適化されたDeepSeek R1 Distill Llama 8BとDeepSeek R1 Distill QWEN 14Bを対象にして、トークン生成では4.32/2.29トークン/秒、プロンプト処理では6.54/3.68トークン/秒を実現し、基線に比べて2.9倍/3.0倍のスピードアップを収めました。",
      "upvotes": 3,
      "discussionId": "67e226b3b1acaf8a7680e96b",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "GPU-based systems",
        "CPUs",
        "RISC-V",
        "ISA",
        "RISC-V hardware",
        "software ecosystem",
        "domain-specific tuning",
        "Sophon SG2042",
        "many-core RISC-V CPU",
        "vector processing capabilities",
        "DeepSeek R1 Distill Llama 8B",
        "DeepSeek R1 Distill QWEN 14B",
        "token generation",
        "prompt processing"
      ]
    },
    "publishedAt": "2025-03-21T05:00:19.000Z",
    "title": "V-Seek: Accelerating LLM Reasoning on Open-hardware Server-class RISC-V\n  Platforms",
    "summary": "The recent exponential growth of Large Language Models (LLMs) has relied on\nGPU-based systems. However, CPUs are emerging as a flexible and lower-cost\nalternative, especially when targeting inference and reasoning workloads.\nRISC-V is rapidly gaining traction in this area, given its open and\nvendor-neutral ISA. However, the RISC-V hardware for LLM workloads and the\ncorresponding software ecosystem are not fully mature and streamlined, given\nthe requirement of domain-specific tuning. This paper aims at filling this gap,\nfocusing on optimizing LLM inference on the Sophon SG2042, the first\ncommercially available many-core RISC-V CPU with vector processing\ncapabilities.\n  On two recent state-of-the-art LLMs optimized for reasoning, DeepSeek R1\nDistill Llama 8B and DeepSeek R1 Distill QWEN 14B, we achieve 4.32/2.29 token/s\nfor token generation and 6.54/3.68 token/s for prompt processing, with a speed\nup of up 2.9x/3.0x compared to our baseline.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17422.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6456
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.18813",
      "authors": [
        {
          "_id": "67e24a997210beea5ecae330",
          "user": {
            "_id": "631dd96f6d6a5870f3d42528",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/631dd96f6d6a5870f3d42528/YMsxboRfIBBGE-z-_DeNx.jpeg",
            "isPro": false,
            "fullname": "Edoardo Debenedetti",
            "user": "dedeswim",
            "type": "user"
          },
          "name": "Edoardo Debenedetti",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:26:38.111Z",
          "hidden": false
        },
        {
          "_id": "67e24a997210beea5ecae331",
          "user": {
            "_id": "6475c2794766357252e69e9f",
            "avatarUrl": "/avatars/db428715dfd2239df2aeaaff1282323f.svg",
            "isPro": false,
            "fullname": "i",
            "user": "iliashum",
            "type": "user"
          },
          "name": "Ilia Shumailov",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:27:10.362Z",
          "hidden": false
        },
        {
          "_id": "67e24a997210beea5ecae332",
          "name": "Tianqi Fan",
          "hidden": false
        },
        {
          "_id": "67e24a997210beea5ecae333",
          "name": "Jamie Hayes",
          "hidden": false
        },
        {
          "_id": "67e24a997210beea5ecae334",
          "user": {
            "_id": "6303fa9ba362e7e8b51d8f2a",
            "avatarUrl": "/avatars/53e53c84f987989deb351dd2ae6ee558.svg",
            "isPro": false,
            "fullname": "Nicholas Carlini",
            "user": "carlini",
            "type": "user"
          },
          "name": "Nicholas Carlini",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:27:26.752Z",
          "hidden": false
        },
        {
          "_id": "67e24a997210beea5ecae335",
          "name": "Daniel Fabian",
          "hidden": false
        },
        {
          "_id": "67e24a997210beea5ecae336",
          "name": "Christoph Kern",
          "hidden": false
        },
        {
          "_id": "67e24a997210beea5ecae337",
          "name": "Chongyang Shi",
          "hidden": false
        },
        {
          "_id": "67e24a997210beea5ecae338",
          "name": "Andreas Terzis",
          "hidden": false
        },
        {
          "_id": "67e24a997210beea5ecae339",
          "user": {
            "_id": "63568f18ba90b4ea9fe91cb5",
            "avatarUrl": "/avatars/3e8b3c573e20cf80d329a312bfc34728.svg",
            "isPro": false,
            "fullname": "Florian Tramer",
            "user": "ftramer",
            "type": "user"
          },
          "name": "Florian Tramèr",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:27:51.364Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-24T15:54:10.000Z",
      "submittedOnDailyAt": "2025-03-25T04:48:52.478Z",
      "title": "プロンプトインジェクションを奪い戻すための設計",
      "submittedOnDailyBy": {
        "_id": "6475c2794766357252e69e9f",
        "avatarUrl": "/avatars/db428715dfd2239df2aeaaff1282323f.svg",
        "isPro": false,
        "fullname": "i",
        "user": "iliashum",
        "type": "user"
      },
      "summary": "大語言モデル（LLMs）は、外部環境と相互作用するアガントシステムについては、増加して使用されています。しかし、LLMアガントは、不信頼なデータを処理する際に、プロンプトインジェクション攻撃に脆弱です。本論文では、CaMeLという強固な防御戦略を提案します。これは、LLMの周りに保護システム層を作成し、基礎モデルが攻撃に脆弱であっても安全に保護します。CaMeLは、処理するデータの制御フローとデータフローを明記的に抽出し、LLMが取得した不信頼なデータは、プログラムフローに影響を与えることはできません。さらに安全性を向上させるために、CaMeLは、非公開データの流出を防ぐために、能力の概念を利用しています。CaMeLの効果性を示すために、最近のアガントセキュリティベンチマークであるAgentDojo [NeurIPS 2024]で、67%のタスクを確実に安全に解決できることを示しました。",
      "upvotes": 2,
      "discussionId": "67e24a9b7210beea5ecae3a0",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "prompt injection attacks",
        "protective system layer",
        "trusted query",
        "untrusted data",
        "program flow",
        "capability",
        "private data exfiltration",
        "unauthorized data flows",
        "AgentDojo"
      ]
    },
    "publishedAt": "2025-03-24T11:54:10.000Z",
    "title": "Defeating Prompt Injections by Design",
    "summary": "Large Language Models (LLMs) are increasingly deployed in agentic systems\nthat interact with an external environment. However, LLM agents are vulnerable\nto prompt injection attacks when handling untrusted data. In this paper we\npropose CaMeL, a robust defense that creates a protective system layer around\nthe LLM, securing it even when underlying models may be susceptible to attacks.\nTo operate, CaMeL explicitly extracts the control and data flows from the\n(trusted) query; therefore, the untrusted data retrieved by the LLM can never\nimpact the program flow. To further improve security, CaMeL relies on a notion\nof a capability to prevent the exfiltration of private data over unauthorized\ndata flows. We demonstrate effectiveness of CaMeL by solving 67% of tasks\nwith provable security in AgentDojo [NeurIPS 2024], a recent agentic security\nbenchmark.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18813.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6475c2794766357252e69e9f",
      "avatarUrl": "/avatars/db428715dfd2239df2aeaaff1282323f.svg",
      "fullname": "i",
      "name": "iliashum",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.18769",
      "authors": [
        {
          "_id": "67e2177c77d32fd1ed8a496b",
          "user": {
            "_id": "62d7b2339b629105a5d6888a",
            "avatarUrl": "/avatars/c3f164fde6b8f9a671890e08ce8a3e75.svg",
            "isPro": false,
            "fullname": "Alan Dao",
            "user": "alandao",
            "type": "user"
          },
          "name": "Alan Dao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-25T08:20:35.578Z",
          "hidden": false
        },
        {
          "_id": "67e2177c77d32fd1ed8a496c",
          "name": "Dinh Bach Vu",
          "hidden": false
        },
        {
          "_id": "67e2177c77d32fd1ed8a496d",
          "name": "Bui Quang Huy",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/62d7b2339b629105a5d6888a/505sM0N1UIUHUOunaHtAG.mp4"
      ],
      "publishedAt": "2025-03-24T15:16:51.000Z",
      "submittedOnDailyAt": "2025-03-25T01:11:03.544Z",
      "title": "AlphaSpace: セマンティックトークン化と記号的推理によるロボットアクションの可能化",
      "submittedOnDailyBy": {
        "_id": "62d7b2339b629105a5d6888a",
        "avatarUrl": "/avatars/c3f164fde6b8f9a671890e08ce8a3e75.svg",
        "isPro": false,
        "fullname": "Alan Dao",
        "user": "alandao",
        "type": "user"
      },
      "summary": "この論文では、AlphaSpaceという新しい手法を紹介しています。AlphaSpaceは、大規模な言語モデル（LLMs）の3次元カーテシアン空間ナビゲーションに対する空間的認知能力を向上させるために設計されています。AlphaSpaceは、セマンティクスに基づくトークン化戦略を使用し、特別なセマンティクストークンでの高さ情報をエンコードし、主に符号的な合成的な認知データを統合しています。このアプローチにより、LLMsは特定の[x, y, z]座標で物体を位置づけて正確に操作できます。実験結果によると、AlphaSpaceは既存のモデルよりも大幅に優れています。操作の部分タスクでの精度は、GPT-4oの37.5%よりも66.67%を達成し、Claude 3.5 Sonnetの29.17%よりも高いということです。",
      "upvotes": 2,
      "discussionId": "67e2177d77d32fd1ed8a49ac",
      "ai_keywords": [
        "semantics-based tokenization",
        "semantic tokens",
        "Cartesian space",
        "3D Cartesian space",
        "positioning",
        "manipulation subtasks"
      ]
    },
    "publishedAt": "2025-03-24T11:16:51.000Z",
    "title": "AlphaSpace: Enabling Robotic Actions through Semantic Tokenization and\n  Symbolic Reasoning",
    "summary": "This paper presents AlphaSpace, a novel methodology designed to enhance the\nspatial reasoning capabilities of large language models (LLMs) for 3D Cartesian\nspace navigation. AlphaSpace employs a semantics-based tokenization strategy,\nencoding height information through specialized semantic tokens, and integrates\nprimarily symbolic synthetic reasoning data. This approach enables LLMs to\naccurately manipulate objects by positioning them at specific [x, y, z]\ncoordinates. Experimental results demonstrate that AlphaSpace significantly\noutperforms existing models on manipulation subtasks, achieving a total\naccuracy of 66.67%, compared to 37.5% for GPT-4o and 29.17% for Claude 3.5\nSonnet.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62d7b2339b629105a5d6888a/505sM0N1UIUHUOunaHtAG.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18769.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62d7b2339b629105a5d6888a",
      "avatarUrl": "/avatars/c3f164fde6b8f9a671890e08ce8a3e75.svg",
      "fullname": "Alan Dao",
      "name": "alandao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.18559",
      "authors": [
        {
          "_id": "67e22d5236076dc847989434",
          "name": "Takashi Isobe",
          "hidden": false
        },
        {
          "_id": "67e22d5236076dc847989435",
          "name": "He Cui",
          "hidden": false
        },
        {
          "_id": "67e22d5236076dc847989436",
          "name": "Dong Zhou",
          "hidden": false
        },
        {
          "_id": "67e22d5236076dc847989437",
          "user": {
            "_id": "6463685fd2044cd1d7c74b81",
            "avatarUrl": "/avatars/334637e2d63efb7cc2129fec6ea54725.svg",
            "isPro": false,
            "fullname": "gemengmeng",
            "user": "gemengmeng",
            "type": "user"
          },
          "name": "Mengmeng Ge",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:30:24.973Z",
          "hidden": false
        },
        {
          "_id": "67e22d5236076dc847989438",
          "name": "Dong Li",
          "hidden": false
        },
        {
          "_id": "67e22d5236076dc847989439",
          "user": {
            "_id": "65adc9d086f88a686be41215",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65adc9d086f88a686be41215/xizVHuZPkE0Gu8_ulx0Fm.jpeg",
            "isPro": false,
            "fullname": "Emad Barsoum",
            "user": "ebarsoum",
            "type": "user"
          },
          "name": "Emad Barsoum",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:30:09.267Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-24T11:13:33.000Z",
      "submittedOnDailyAt": "2025-03-25T02:43:21.795Z",
      "title": "AMD-Hummingbird: 効率的なテキストから動画へのモデルへの向け方",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Text-to-Video (T2V)生成は、文脈から写実的なビデオを合成する能力で注目を集めています。しかし、現在のモデルは計算効率と高い視覚品質のバランスを保つに苦労しています、特にリソース制限されたデバイスであるiGPUや携帯電話において。先行研究は、視覚の忠実度を優先しながら、実世界的な機械学習モデルの小ささと効率性の必要性を飛ばしていました。この挑戦に対して、私たちは、現在のモデルを削減し、視覚の品質を向上させるための視覚的反応学習を通じた軽量T2Vフレームワーク「Hummingbird」を提案します。私たちのアプローチは、U-Netのパラメータ数を14億から7億に減らし、効率性を大幅に向上させながら高品質のビデオ生成を維持します。また、私たちは、大規模言語モデル（LLMs）とビデオ品質評価（VQA）モデルを活用した新しいデータ処理プイルプリングを導入し、文脈プラントとビデオデータの品質を向上させます。ユーザー駆動の訓練とスタイルカスタマイズのために、私たちは、完全な訓練コードを公開し、データ処理とモデル訓練を含みます。広範囲の実験により、私たちの方法は、VideoCrafter2などの最先端モデルと比較して31倍のスピードアップを達成し、VBenchで最も高い総合スコアを達成しました。また、私たちの方法は、現在のU-Netベースの方法の長ビデオ生成の制限を解決し、26フレームのビデオの生成を支援します。特に、全体の訓練プロセスは、4台のGPUでだけで可能で、現在の先進的な方法と同等の性能を提供します。Hummingbirdは、高性能、スケーラビリティ、柔軟性を組み合わせた実用的な解決策で、T2V生成の実世界的なアプリケーションに向けて提案します。",
      "upvotes": 2,
      "discussionId": "67e22d5836076dc84798964e",
      "ai_keywords": [
        "U-Net",
        "Visual feedback learning",
        "Large Language Models (LLMs)",
        "Video Quality Assessment (VQA)",
        "VBench"
      ]
    },
    "publishedAt": "2025-03-24T07:13:33.000Z",
    "title": "AMD-Hummingbird: Towards an Efficient Text-to-Video Model",
    "summary": "Text-to-Video (T2V) generation has attracted significant attention for its\nability to synthesize realistic videos from textual descriptions. However,\nexisting models struggle to balance computational efficiency and high visual\nquality, particularly on resource-limited devices, e.g.,iGPUs and mobile\nphones. Most prior work prioritizes visual fidelity while overlooking the need\nfor smaller, more efficient models suitable for real-world deployment. To\naddress this challenge, we propose a lightweight T2V framework, termed\nHummingbird, which prunes existing models and enhances visual quality through\nvisual feedback learning. Our approach reduces the size of the U-Net from 1.4\nbillion to 0.7 billion parameters, significantly improving efficiency while\npreserving high-quality video generation. Additionally, we introduce a novel\ndata processing pipeline that leverages Large Language Models (LLMs) and Video\nQuality Assessment (VQA) models to enhance the quality of both text prompts and\nvideo data. To support user-driven training and style customization, we\npublicly release the full training code, including data processing and model\ntraining. Extensive experiments show that our method achieves a 31X speedup\ncompared to state-of-the-art models such as VideoCrafter2, while also attaining\nthe highest overall score on VBench. Moreover, our method supports the\ngeneration of videos with up to 26 frames, addressing the limitations of\nexisting U-Net-based methods in long video generation. Notably, the entire\ntraining process requires only four GPUs, yet delivers performance competitive\nwith existing leading methods. Hummingbird presents a practical and efficient\nsolution for T2V generation, combining high performance, scalability, and\nflexibility for real-world applications.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18559.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6456
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.18033",
      "authors": [
        {
          "_id": "67e2706af6cf2764a534d4a5",
          "user": {
            "_id": "630f0d48982455e61cc4cc08",
            "avatarUrl": "/avatars/eea6ed2e112e830effa98a4661c5474f.svg",
            "isPro": false,
            "fullname": "Samuel",
            "user": "Dvir",
            "type": "user"
          },
          "name": "Dvir Samuel",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-03-25T09:00:01.542Z",
          "hidden": false
        },
        {
          "_id": "67e2706af6cf2764a534d4a6",
          "user": {
            "_id": "66633be10875aaaa9153c963",
            "avatarUrl": "/avatars/f47aaaf7b029ad3e99f49676a8f9a479.svg",
            "isPro": false,
            "fullname": "Matan Levy",
            "user": "m98levy",
            "type": "user"
          },
          "name": "Matan Levy",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:25:56.024Z",
          "hidden": false
        },
        {
          "_id": "67e2706af6cf2764a534d4a7",
          "name": "Nir Darshan",
          "hidden": false
        },
        {
          "_id": "67e2706af6cf2764a534d4a8",
          "user": {
            "_id": "6493393f357b252af72196c5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6493393f357b252af72196c5/EWSy18XRcMRa_4XMM3Fu-.jpeg",
            "isPro": false,
            "fullname": "Gal Chechik",
            "user": "galchechik",
            "type": "user"
          },
          "name": "Gal Chechik",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:26:05.156Z",
          "hidden": false
        },
        {
          "_id": "67e2706af6cf2764a534d4a9",
          "user": {
            "_id": "64c5f22c2581696666ebed88",
            "avatarUrl": "/avatars/e85cd2d82f16ec10cad2b63929b2f05a.svg",
            "isPro": false,
            "fullname": "Rami Ben-Ari",
            "user": "ramiben",
            "type": "user"
          },
          "name": "Rami Ben-Ari",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:26:11.853Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-23T11:26:48.000Z",
      "submittedOnDailyAt": "2025-03-25T07:31:06.785Z",
      "title": "OmnimatteZero: トレーニング無しの実時的なOmnimatteを用いた事前学習されたビデオディフューションモデル",
      "submittedOnDailyBy": {
        "_id": "630f0d48982455e61cc4cc08",
        "avatarUrl": "/avatars/eea6ed2e112e830effa98a4661c5474f.svg",
        "isPro": false,
        "fullname": "Samuel",
        "user": "Dvir",
        "type": "user"
      },
      "summary": "Omnimatteは、与えられたビデオを意味的に意味深いレイヤーに分解することを目指しています。これには背景と個々の物体そしてそれらに関連するエフェクト（例えば影と反射）を含むものです。現在の方法は、極めて複雑な訓練や高額な自動調整最適化を必要としています。本論文では、OmnimatteZeroという訓練不要のアプローチを提出しています。これは、シンプルなビデオディフフェーションモデルを利用してOmnimatteを行うことを目的としています。これは、ビデオから物体を削除し、物体そしてそれらのエフェクトを抽出し、新しいビデオにそれらの物体を組み込むことができます。これを実現するためには、ビデオ物体削除のためのゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し、それらがビデオへの応用では効果的に対応できないことを補正しています。そして、ゼロショット画像補完手法を適用し",
      "upvotes": 2,
      "discussionId": "67e2706df6cf2764a534d570",
      "ai_keywords": [
        "diffusion models",
        "zero-shot image inpainting",
        "self-attention maps",
        "latent arithmetic",
        "real-time performance",
        "frame runtime"
      ]
    },
    "publishedAt": "2025-03-23T07:26:48.000Z",
    "title": "OmnimatteZero: Training-free Real-time Omnimatte with Pre-trained Video\n  Diffusion Models",
    "summary": "Omnimatte aims to decompose a given video into semantically meaningful\nlayers, including the background and individual objects along with their\nassociated effects, such as shadows and reflections. Existing methods often\nrequire extensive training or costly self-supervised optimization. In this\npaper, we present OmnimatteZero, a training-free approach that leverages\noff-the-shelf pre-trained video diffusion models for omnimatte. It can remove\nobjects from videos, extract individual object layers along with their effects,\nand composite those objects onto new videos. We accomplish this by adapting\nzero-shot image inpainting techniques for video object removal, a task they\nfail to handle effectively out-of-the-box. We then show that self-attention\nmaps capture information about the object and its footprints and use them to\ninpaint the object's effects, leaving a clean background. Additionally, through\nsimple latent arithmetic, object layers can be isolated and recombined\nseamlessly with new video layers to produce new videos. Evaluations show that\nOmnimatteZero not only achieves superior performance in terms of background\nreconstruction but also sets a new record for the fastest Omnimatte approach,\nachieving real-time performance with minimal frame runtime.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18033.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "630f0d48982455e61cc4cc08",
      "avatarUrl": "/avatars/eea6ed2e112e830effa98a4661c5474f.svg",
      "fullname": "Samuel",
      "name": "Dvir",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.17500",
      "authors": [
        {
          "_id": "67e23d503ef5318b1550f1bc",
          "user": {
            "_id": "6071c4b270e11b30cfcfd7a3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6071c4b270e11b30cfcfd7a3/-1ekCBzSTpqxkkul0bgmI.jpeg",
            "isPro": false,
            "fullname": "Louis Owen",
            "user": "louisowen6",
            "type": "user"
          },
          "name": "Louis Owen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-25T08:19:59.211Z",
          "hidden": false
        },
        {
          "_id": "67e23d503ef5318b1550f1bd",
          "user": {
            "_id": "62cd4b03c5cc157be82f0b56",
            "avatarUrl": "/avatars/351e963c1c763d507ae78cbcd62966a3.svg",
            "isPro": false,
            "fullname": "Abhay kumar",
            "user": "akanyaani",
            "type": "user"
          },
          "name": "Abhay Kumar",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-25T09:23:34.033Z",
          "hidden": false
        },
        {
          "_id": "67e23d503ef5318b1550f1be",
          "user": {
            "_id": "645a0d3dd6648853107c5fdc",
            "avatarUrl": "/avatars/1e3b6a4f5ce81a707ba7cbdf81631091.svg",
            "isPro": false,
            "fullname": "Nilabhra Roy Chowdhury",
            "user": "nilabhra",
            "type": "user"
          },
          "name": "Nilabhra Roy Chowdhury",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-25T09:15:43.961Z",
          "hidden": false
        },
        {
          "_id": "67e23d503ef5318b1550f1bf",
          "user": {
            "_id": "65e4be59e8b017ee1310a1b6",
            "avatarUrl": "/avatars/c3f7cdf5d0859cb80bfb2b970a675dfa.svg",
            "isPro": false,
            "fullname": "Fabian",
            "user": "gueraf",
            "type": "user"
          },
          "name": "Fabian Güra",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-25T08:19:56.909Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6071c4b270e11b30cfcfd7a3/AaY617TTwvPiB-ub2B9bA.png"
      ],
      "publishedAt": "2025-03-21T19:23:08.000Z",
      "submittedOnDailyAt": "2025-03-25T03:52:18.158Z",
      "title": "LLM予約学習時の重み再規格化による分散制御",
      "submittedOnDailyBy": {
        "_id": "6071c4b270e11b30cfcfd7a3",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6071c4b270e11b30cfcfd7a3/-1ekCBzSTpqxkkul0bgmI.jpeg",
        "isPro": false,
        "fullname": "Louis Owen",
        "user": "louisowen6",
        "type": "user"
      },
      "summary": "LLMの事前学習の結果は重み初期化と分散制御戦略によって強く影響されます。一般的な神経ネットワークでは、初期分散制御の重要性は既によく記録されていますが、特にLLMの事前学習時の初期化とその成長の管理に関する文献は少ないです。本論文では、Layer Index Rescaling (LIR) の重み初期化スキームとTarget Variance Rescaling (TVR) の分散制御戦略を介紹します。1BパラメータのLLaMAモデルに対する実験は、これらの手法を使用した分散管理の改善が下流タスクの性能に大幅な向上を帯び、極端な活性値を減少し、量化と低精度学習に関連する課題を軽減することを示します。コードは以下のURLで公開されています：https://github.com/bluorion-com/weight_rescaling。",
      "upvotes": 2,
      "discussionId": "67e23d513ef5318b1550f22c",
      "githubRepo": "https://github.com/bluorion-com/weight_rescaling",
      "ai_keywords": [
        "Layer Index Rescaling (LIR)",
        "Target Variance Rescaling (TVR)",
        "weight initialization",
        "variance control"
      ]
    },
    "publishedAt": "2025-03-21T15:23:08.000Z",
    "title": "Variance Control via Weight Rescaling in LLM Pre-training",
    "summary": "The outcome of Large Language Model (LLM) pre-training strongly depends on\nweight initialization and variance control strategies. Although the importance\nof initial variance control has been well documented in neural networks in\ngeneral, the literature on initialization and management of its growth during\nLLM pre-training, specifically, is somewhat sparse. In this paper, we introduce\nthe Layer Index Rescaling (LIR) weight initialization scheme, and the Target\nVariance Rescaling (TVR) variance control strategy. Experiments on a 1B\nparameter LLaMA model demonstrate that better variance management using these\ntechniques yields substantial improvements in downstream task performance (up\nto 4.6% on common pre-training benchmarks) and reduces extreme activation\nvalues, thus mitigating challenges associated with quantization and\nlow-precision training. Our code is available at:\nhttps://github.com/bluorion-com/weight_rescaling.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6071c4b270e11b30cfcfd7a3/AaY617TTwvPiB-ub2B9bA.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17500.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6071c4b270e11b30cfcfd7a3",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6071c4b270e11b30cfcfd7a3/-1ekCBzSTpqxkkul0bgmI.jpeg",
      "fullname": "Louis Owen",
      "name": "louisowen6",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.18470",
      "authors": [
        {
          "_id": "67e23d7ddb11e1d38226cafd",
          "user": {
            "_id": "669794c5813d96b4eb0b3fd6",
            "avatarUrl": "/avatars/b4e2bb7b07cc281932d783dcbb64f211.svg",
            "isPro": true,
            "fullname": "Zhenyu Pan",
            "user": "zhenyupan",
            "type": "user"
          },
          "name": "Zhenyu Pan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-25T08:19:54.569Z",
          "hidden": false
        },
        {
          "_id": "67e23d7ddb11e1d38226cafe",
          "name": "Han Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-24T09:18:01.000Z",
      "submittedOnDailyAt": "2025-03-25T06:54:21.536Z",
      "title": "MetaSpatial: Metaベースの3次元空間認識を強化したVLMsの開発",
      "submittedOnDailyBy": {
        "_id": "669794c5813d96b4eb0b3fd6",
        "avatarUrl": "/avatars/b4e2bb7b07cc281932d783dcbb64f211.svg",
        "isPro": true,
        "fullname": "Zhenyu Pan",
        "user": "zhenyupan",
        "type": "user"
      },
      "summary": "MetaSpatialは、最初の強化学習(RL)ベースのフレームワークです。これは、視覚言語モデル(VLMs)に3D空間認識を強化し、硬コーディングされた最適化を必要とさせないように3Dスケーンの時間実時生成を可能にします。MetaSpatialは2つの核心的な課題を解決しています： (i) VLMsにおける内部化された3D空間認識の欠如、これがリアリスティックな配置の生成能力を制限しているということです。 (ii) 価格生成タスクに対する伝統的な超フィーチング(SFT)の不適切さ、これは完全な真のデータのアノテーションが存在しないということです。メインの革新点として、物理的な制約と渲染された画像の評価を統合した多段階のRLベースの最適化機構があります。これにより、生成される3D配置は一致し、物理的に可能で美術的に一致しています。方法論的には、MetaSpatialは適応的なイテレーション的な理由過程を導入し、VLMは渲染された出力を分析してスペースアレンジを進歩的に改善します。実験的な評価により、MetaSpatialは様々なスケールのモデルの空間一致性とフォーマットの安定性を大幅に向上させます。トレーニング後、物体の配置はリアリスティックで、位置付けや機能的に一致しています。これは、RLがメタバージュ、AR/VR、デジタルツイン、ゲーム開発における3D空間認識においての効果を証明しています。コード、データ、トレーニングパイプラインは、https://github.com/PzySeere/MetaSpatial で公開しています。",
      "upvotes": 1,
      "discussionId": "67e23d7fdb11e1d38226cb7b",
      "ai_keywords": [
        "reinforcement learning (RL)",
        "vision-language models (VLMs)",
        "3D spatial reasoning",
        "real-time 3D scene generation",
        "internalized 3D spatial reasoning",
        "supervised fine-tuning (SFT)",
        "multi-turn RL-based optimization",
        "physics-aware constraints",
        "rendered image evaluations",
        "adaptive, iterative reasoning process",
        "scene coherence",
        "spatial consistency",
        "formatting stability",
        "object placements",
        "metaverse",
        "AR/VR",
        "digital twins",
        "game development",
        "empirical evaluations"
      ]
    },
    "publishedAt": "2025-03-24T05:18:01.000Z",
    "title": "MetaSpatial: Reinforcing 3D Spatial Reasoning in VLMs for the Metaverse",
    "summary": "We present MetaSpatial, the first reinforcement learning (RL)-based framework\ndesigned to enhance 3D spatial reasoning in vision-language models (VLMs),\nenabling real-time 3D scene generation without the need for hard-coded\noptimizations. MetaSpatial addresses two core challenges: (i) the lack of\ninternalized 3D spatial reasoning in VLMs, which limits their ability to\ngenerate realistic layouts, and (ii) the inefficiency of traditional supervised\nfine-tuning (SFT) for layout generation tasks, as perfect ground truth\nannotations are unavailable. Our key innovation is a multi-turn RL-based\noptimization mechanism that integrates physics-aware constraints and rendered\nimage evaluations, ensuring generated 3D layouts are coherent, physically\nplausible, and aesthetically consistent. Methodologically, MetaSpatial\nintroduces an adaptive, iterative reasoning process, where the VLM refines\nspatial arrangements over multiple turns by analyzing rendered outputs,\nimproving scene coherence progressively. Empirical evaluations demonstrate that\nMetaSpatial significantly enhances the spatial consistency and formatting\nstability of various scale models. Post-training, object placements are more\nrealistic, aligned, and functionally coherent, validating the effectiveness of\nRL for 3D spatial reasoning in metaverse, AR/VR, digital twins, and game\ndevelopment applications. Our code, data, and training pipeline are publicly\navailable at https://github.com/PzySeere/MetaSpatial.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18470.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "669794c5813d96b4eb0b3fd6",
      "avatarUrl": "/avatars/b4e2bb7b07cc281932d783dcbb64f211.svg",
      "fullname": "Zhenyu Pan",
      "name": "zhenyupan",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.18352",
      "authors": [
        {
          "_id": "67e217a272e17348c5b3f0a2",
          "name": "Jinjin Zhang",
          "hidden": false
        },
        {
          "_id": "67e217a272e17348c5b3f0a3",
          "user": {
            "_id": "6708e399672d9dcd31575fbc",
            "avatarUrl": "/avatars/0f947f17b5426186aadaa4224571f47b.svg",
            "isPro": false,
            "fullname": "qiuyuhuang",
            "user": "qiuyuhuang",
            "type": "user"
          },
          "name": "Qiuyu Huang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:24:20.054Z",
          "hidden": false
        },
        {
          "_id": "67e217a272e17348c5b3f0a4",
          "name": "Junjie Liu",
          "hidden": false
        },
        {
          "_id": "67e217a272e17348c5b3f0a5",
          "user": {
            "_id": "64905cd589f22918ecaca080",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/2S7I7uZL49CXbUN2T7p63.jpeg",
            "isPro": false,
            "fullname": "Xiefan Guo",
            "user": "xiefan-guo",
            "type": "user"
          },
          "name": "Xiefan Guo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:24:02.736Z",
          "hidden": false
        },
        {
          "_id": "67e217a272e17348c5b3f0a6",
          "user": {
            "_id": "62c581177b48ba0bb8cdb737",
            "avatarUrl": "/avatars/d1a85c28f13bb86481b6be80824eb1fa.svg",
            "isPro": false,
            "fullname": "di huang",
            "user": "dihuang",
            "type": "user"
          },
          "name": "Di Huang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:23:56.412Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-24T05:25:07.000Z",
      "submittedOnDailyAt": "2025-03-25T07:48:32.671Z",
      "title": "Diffusion-4K: 潜在ディフュージョンモデルを用いた超高解像度画像合成",
      "submittedOnDailyBy": {
        "_id": "5f1158120c833276f61f1a84",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
        "isPro": false,
        "fullname": "Niels Rogge",
        "user": "nielsr",
        "type": "user"
      },
      "summary": "この論文では、Diffusion-4Kという新しいフレームワークを提案し、テキストから画像へのディフュージョンモデルを用いて直接の超高解像度画像合成を行うことを報告します。核心の進歩点は以下の通りです：\n\n（1）Aesthetic-4K Benchmark：4K画像合成データセットが公開できていないことを解決するために、Aesthetic-4Kという総合的なベンチマークを構築しました。これは、GPT-4oから選ばれたような高品質な4K画像とキャプションを用いたデータセットです。また、GLCMスコアとコンピューション比率のメトリックを用いて、細かい詳細を評価し、FID、美術性とCLIPScoreなどの全体的な評価指標を組み合わせて、超高解像度画像の詳細な評価を行います。\n\n（2）ワブレットベースの微調節：ディフュージョンモデルを直接的に写実的な4K画像と訓練するためのワブレットベースの微調節アプローチを提案しました。これは、多様な潜在ディフュージョンモデルに適用可能で、高度な詳細の4K画像の合成において効果的です。このため、Diffusion-4Kは、高品質画像合成とテキストプラントの従順性において、特に現代の大規模なディフュージョンモデル（例：SD3-2BとFlux-12B）を用いて優れた性能を達成します。本ベンチマークからの拡大的な実験結果は、Diffusion-4Kの超高解像度画像合成の優れた性能を示しています。",
      "upvotes": 1,
      "discussionId": "67e217a772e17348c5b3f20a",
      "ai_keywords": [
        "diffusion models",
        "text-to-image diffusion models",
        "Aesthetic-4K Benchmark",
        "wavelet-based fine-tuning",
        "latent diffusion models",
        "SD3-2B",
        "Flux-12B",
        "GLCM Score",
        "Compression Ratio",
        "FID",
        "Aesthetics",
        "CLIPScore",
        "ultra-high-resolution image synthesis",
        "photorealistic 4K images",
        "high-quality image synthesis",
        "text prompt adherence"
      ]
    },
    "publishedAt": "2025-03-24T01:25:07.000Z",
    "title": "Diffusion-4K: Ultra-High-Resolution Image Synthesis with Latent\n  Diffusion Models",
    "summary": "In this paper, we present Diffusion-4K, a novel framework for direct\nultra-high-resolution image synthesis using text-to-image diffusion models. The\ncore advancements include: (1) Aesthetic-4K Benchmark: addressing the absence\nof a publicly available 4K image synthesis dataset, we construct Aesthetic-4K,\na comprehensive benchmark for ultra-high-resolution image generation. We\ncurated a high-quality 4K dataset with carefully selected images and captions\ngenerated by GPT-4o. Additionally, we introduce GLCM Score and Compression\nRatio metrics to evaluate fine details, combined with holistic measures such as\nFID, Aesthetics and CLIPScore for a comprehensive assessment of\nultra-high-resolution images. (2) Wavelet-based Fine-tuning: we propose a\nwavelet-based fine-tuning approach for direct training with photorealistic 4K\nimages, applicable to various latent diffusion models, demonstrating its\neffectiveness in synthesizing highly detailed 4K images. Consequently,\nDiffusion-4K achieves impressive performance in high-quality image synthesis\nand text prompt adherence, especially when powered by modern large-scale\ndiffusion models (e.g., SD3-2B and Flux-12B). Extensive experimental results\nfrom our benchmark demonstrate the superiority of Diffusion-4K in\nultra-high-resolution image synthesis.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18352.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5f1158120c833276f61f1a84",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
      "fullname": "Niels Rogge",
      "name": "nielsr",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 799
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.17735",
      "authors": [
        {
          "_id": "67e22bc349edf14060e5747a",
          "name": "Zhiqiang Yuan",
          "hidden": false
        },
        {
          "_id": "67e22bc349edf14060e5747b",
          "name": "Ting Zhang",
          "hidden": false
        },
        {
          "_id": "67e22bc349edf14060e5747c",
          "name": "Ying Deng",
          "hidden": false
        },
        {
          "_id": "67e22bc349edf14060e5747d",
          "name": "Jiapei Zhang",
          "hidden": false
        },
        {
          "_id": "67e22bc349edf14060e5747e",
          "name": "Yeshuang Zhu",
          "hidden": false
        },
        {
          "_id": "67e22bc349edf14060e5747f",
          "name": "Zexi Jia",
          "hidden": false
        },
        {
          "_id": "67e22bc349edf14060e57480",
          "name": "Jie Zhou",
          "hidden": false
        },
        {
          "_id": "67e22bc349edf14060e57481",
          "name": "Jinchao Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-22T11:28:25.000Z",
      "submittedOnDailyAt": "2025-03-25T02:36:41.359Z",
      "title": "RDTF: リソース効率的ダブルマスクトレーニングフレームワークの多フレームアニメーションスタップ生成",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "最近、映像生成技術には大きな進歩があり、学者たちに広く注目を集めています。この技術を資源制限のある下流アプリケーションに適用するためには、研究者たちは通常パラメーター効率的な調整方法その他のように基礎学習モデルを微調整しています。これらの方法は源データドメインからの知識をターゲットデータドメインへと伝えることができますが、少ないトレーニングパラメーターは汎化能力が悪く、源データドメインからの知識が推論プロセスからターゲットデータドメインへと偏りを引き起こす可能性があります。本論文では、資源制限のある場合、100万レベルのサンプルをだけ使ってスタートから学習した小さな映像生成モデルを訓練することが大きなモデルのパラメーター効率的な調整よりも下流アプリケーションで上回ることができることを主張しています：これはデータの有効利用とカレクリウム戦略の有効利用にあることが核心です。アニメーションスティッカー生成（ASG）を実験的な場合に取り上げ、低フレームレートでスティッカーの離散フレーム生成ネットワークを構築し、モデルトレーニングの資源制限下でのパラメーターの要求を満たすようにします。スタートから学習されるモデルのデータサポートを提供するためには、ディスクリティブフレーム生成ネットワークを構築し、データの有効利用を改善し、限られたデータの多様性を拡大するためのダブルマスクベースのデータ利用戦略を提案します。ダブルマスク状況下での収束を促進するためには、難易度適応的なカレクリウム学習法を提案し、サンプルエントロピーを静的的と適応的のカバー成分に分解し、サンプルを容易から難しいように取得することです。実験は、I2V-AdapterやSimDAといったパラメーター効率的な調整法と比較して定量的・定性的に上回ることを示し、資源制限のある下流タスクでの本方法の可能性を証明します。コードは提供されます。",
      "upvotes": 1,
      "discussionId": "67e22bc449edf14060e574e3",
      "ai_keywords": [
        "parameter-efficient tuning",
        "Adapter",
        "Lora",
        "discrete frame generation network",
        "dual-mask based data utilization strategy",
        "curriculum learning method",
        "difficulty-adaptive curriculum learning",
        "sample entropy",
        "I2V-Adapter",
        "SimDA"
      ]
    },
    "publishedAt": "2025-03-22T07:28:25.000Z",
    "title": "RDTF: Resource-efficient Dual-mask Training Framework for Multi-frame\n  Animated Sticker Generation",
    "summary": "Recently, great progress has been made in video generation technology,\nattracting the widespread attention of scholars. To apply this technology to\ndownstream applications under resource-constrained conditions, researchers\nusually fine-tune the pre-trained models based on parameter-efficient tuning\nmethods such as Adapter or Lora. Although these methods can transfer the\nknowledge from the source domain to the target domain, fewer training\nparameters lead to poor fitting ability, and the knowledge from the source\ndomain may lead to the inference process deviating from the target domain. In\nthis paper, we argue that under constrained resources, training a smaller video\ngeneration model from scratch using only million-level samples can outperform\nparameter-efficient tuning on larger models in downstream applications: the\ncore lies in the effective utilization of data and curriculum strategy. Take\nanimated sticker generation (ASG) as a case study, we first construct a\ndiscrete frame generation network for stickers with low frame rates, ensuring\nthat its parameters meet the requirements of model training under constrained\nresources. In order to provide data support for models trained from scratch, we\ncome up with a dual-mask based data utilization strategy, which manages to\nimprove the availability and expand the diversity of limited data. To\nfacilitate convergence under dual-mask situation, we propose a\ndifficulty-adaptive curriculum learning method, which decomposes the sample\nentropy into static and adaptive components so as to obtain samples from easy\nto difficult. The experiment demonstrates that our resource-efficient dual-mask\ntraining framework is quantitatively and qualitatively superior to\nefficient-parameter tuning methods such as I2V-Adapter and SimDA, verifying the\nfeasibility of our method on downstream tasks under constrained resources. Code\nwill be available.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17735.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6456
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.16924",
      "authors": [
        {
          "_id": "67e230f384513315a91c5602",
          "user": {
            "_id": "664207e5af62c6c26653b369",
            "avatarUrl": "/avatars/031453340e114efb10b7d650e1d1c384.svg",
            "isPro": false,
            "fullname": "Joo Chan Lee",
            "user": "maincold2",
            "type": "user"
          },
          "name": "Joo Chan Lee",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-25T08:20:07.628Z",
          "hidden": false
        },
        {
          "_id": "67e230f384513315a91c5603",
          "name": "Jong Hwan Ko",
          "hidden": false
        },
        {
          "_id": "67e230f384513315a91c5604",
          "user": {
            "_id": "655e0141d36a195f663ee4b0",
            "avatarUrl": "/avatars/97bb695ccefdcb2139b94bcae808cf99.svg",
            "isPro": false,
            "fullname": "Eunbyung Park",
            "user": "epark",
            "type": "user"
          },
          "name": "Eunbyung Park",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:25:09.995Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-21T07:41:45.000Z",
      "submittedOnDailyAt": "2025-03-25T02:59:24.076Z",
      "title": "最適化最小3次元ガウススプレッティング",
      "submittedOnDailyBy": {
        "_id": "664207e5af62c6c26653b369",
        "avatarUrl": "/avatars/031453340e114efb10b7d650e1d1c384.svg",
        "isPro": false,
        "fullname": "Joo Chan Lee",
        "user": "maincold2",
        "type": "user"
      },
      "summary": "3D Gaussian Splatting (3DGS)は、時間効率的な高性能のレンダリングを可能にし、広い範囲のアプリケーションに応用可能な強力な表現として現れてきました。しかし、3Dシーンを明示的なGaussian primitivesで表現することは、大きなストレージやメモリオーバーヘッドを課します。最近の研究により、高精度の属性を持つ場合、高品質のレンダリングを実現するために、大幅にGaussianの数を減らすことができることが示されました。しかし、現在の3DGSの圧縮方法は、主に属性の圧縮に焦点を当てて、相対的に大きな数のGaussianを使用しています。これは、小さなGaussianの集合が、無効的な属性圧縮に脆弱になり、品質の低下が厳しいことになるためです。Gaussianの数は、計算コストに直接関係しており、優れたストレージを実現するよりも、Gaussianの数を効果的に減らすことが重要です。本論文では、Optimized Minimal Gaussians representation (OMG)を提案します。OMGは、ストレージを大幅に減らしながら、最小限のprimitivesを使用します。まず、近くのGaussianから違いがあるGaussianを特定し、品質を犠牲にしないまま冗長を最小化します。次に、primitivesの連続性と不連続性を効率的に捉える総当たりと精密な属性表現を提案します。また、不連続性の表現を改善するために、sub-vector quantization技術を提案し、コードブックサイズを無視できる高速な訓練を維持します。拡大的な実験により、OMGは前の最先端と比べて近似50%のストレージ要求を減らし、高品質のレンダリングを維持する同時に600FPS以上のレンダリングを可能にします。ソースコードは、https://maincold2.github.io/omg/に公開されています。",
      "upvotes": 1,
      "discussionId": "67e230f484513315a91c5678",
      "projectPage": "https://maincold2.github.io/omg/",
      "githubRepo": "https://github.com/maincold2/OMG",
      "ai_keywords": [
        "Gaussian Splatting (3DGS)",
        "real-time",
        "high-performance rendering",
        "3D scenes",
        "explicit Gaussian primitives",
        "storage",
        "memory overhead",
        "high-quality rendering",
        "attribute compression",
        "quality degradation",
        "computational costs",
        "Optimized Minimal Gaussians representation (OMG)",
        "distinct Gaussian",
        "redundancy",
        "attribute representation",
        "continuity",
        "irregularity",
        "sub-vector quantization",
        "codebook size",
        "FPS rendering",
        "rendering quality"
      ]
    },
    "publishedAt": "2025-03-21T03:41:45.000Z",
    "title": "Optimized Minimal 3D Gaussian Splatting",
    "summary": "3D Gaussian Splatting (3DGS) has emerged as a powerful representation for\nreal-time, high-performance rendering, enabling a wide range of applications.\nHowever, representing 3D scenes with numerous explicit Gaussian primitives\nimposes significant storage and memory overhead. Recent studies have shown that\nhigh-quality rendering can be achieved with a substantially reduced number of\nGaussians when represented with high-precision attributes. Nevertheless,\nexisting 3DGS compression methods still rely on a relatively large number of\nGaussians, focusing primarily on attribute compression. This is because a\nsmaller set of Gaussians becomes increasingly sensitive to lossy attribute\ncompression, leading to severe quality degradation. Since the number of\nGaussians is directly tied to computational costs, it is essential to reduce\nthe number of Gaussians effectively rather than only optimizing storage. In\nthis paper, we propose Optimized Minimal Gaussians representation (OMG), which\nsignificantly reduces storage while using a minimal number of primitives.\nFirst, we determine the distinct Gaussian from the near ones, minimizing\nredundancy without sacrificing quality. Second, we propose a compact and\nprecise attribute representation that efficiently captures both continuity and\nirregularity among primitives. Additionally, we propose a sub-vector\nquantization technique for improved irregularity representation, maintaining\nfast training with a negligible codebook size. Extensive experiments\ndemonstrate that OMG reduces storage requirements by nearly 50% compared to the\nprevious state-of-the-art and enables 600+ FPS rendering while maintaining high\nrendering quality. Our source code is available at\nhttps://maincold2.github.io/omg/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16924.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "664207e5af62c6c26653b369",
      "avatarUrl": "/avatars/031453340e114efb10b7d650e1d1c384.svg",
      "fullname": "Joo Chan Lee",
      "name": "maincold2",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.18494",
      "authors": [
        {
          "_id": "67e21a81e2e69ea26eee4f67",
          "user": {
            "_id": "65bef46337491e7adc5ee7c9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/bQ2WY0bU65IQ3IBn6SrnA.png",
            "isPro": false,
            "fullname": "Hao-Yuan Chen",
            "user": "MarkChenX",
            "type": "user"
          },
          "name": "Hao-Yuan Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-25T08:20:30.277Z",
          "hidden": false
        },
        {
          "_id": "67e21a81e2e69ea26eee4f68",
          "name": "Cheng-Pong Huang",
          "hidden": false
        },
        {
          "_id": "67e21a81e2e69ea26eee4f69",
          "name": "Jui-Ming Yao",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/65bef46337491e7adc5ee7c9/kVdnqHbvP4VXfEWb2MAs_.jpeg",
        "https://cdn-uploads.huggingface.co/production/uploads/65bef46337491e7adc5ee7c9/amlMqiikDojWAwsnS3d5e.png",
        "https://cdn-uploads.huggingface.co/production/uploads/65bef46337491e7adc5ee7c9/cndDykrtJViO6gRavkF9D.png"
      ],
      "publishedAt": "2025-03-24T09:48:59.000Z",
      "submittedOnDailyAt": "2025-03-25T06:52:38.289Z",
      "title": "言語プロセスの視聴によるコーディングアグエントの改善",
      "submittedOnDailyBy": {
        "_id": "65bef46337491e7adc5ee7c9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/bQ2WY0bU65IQ3IBn6SrnA.png",
        "isPro": false,
        "fullname": "Hao-Yuan Chen",
        "user": "MarkChenX",
        "type": "user"
      },
      "summary": "大語言モデルの出現およびAIアガントの應用は、最先端のコード生成ベンチマークにおいて大幅に進展し、ソフトウェア開発のタスクを変形しました。しかし、テスト時に計算される理由論モデルを用いても、これらのシステムは複雑なソフトウェア開発の挑戦に頑張ります。本稿では、言語的なプロセス視聴（VPS）を機能として増強したコード理解と理由論アガントシステムCURAを紹介し、BigCodeBenchのような難しいベンチマークで基準モデルに対して3.65％の改善を収めました。また、CURAとo3-miniモデルとVPS手法を組み合わせると、最先端の性能を達成します。この研究は、LLMベースのコード生成と理由論駆動のアーキテクチャの統合に向けて進展し、言語モデルのアガント的な理由論を用いて複雑なソフトウェア開発のタスクを解決することができるようにします。",
      "upvotes": 0,
      "discussionId": "67e21a82e2e69ea26eee4fba",
      "ai_keywords": [
        "large language models (LLMs)",
        "code generation",
        "software engineering tasks",
        "CURA",
        "code understanding and reasoning agent system",
        "verbal process supervision (VPS)",
        "BigCodeBench",
        "o3-mini model",
        "reasoning-driven architectures",
        "agentic reasoning"
      ]
    },
    "publishedAt": "2025-03-24T05:48:59.000Z",
    "title": "Verbal Process Supervision Elicits Better Coding Agents",
    "summary": "The emergence of large language models and their applications as AI agents\nhave significantly advanced state-of-the-art code generation benchmarks,\ntransforming modern software engineering tasks. However, even with test-time\ncomputed reasoning models, these systems still struggle with complex software\nengineering challenges. This work introduces CURA, a code understanding and\nreasoning agent system enhanced with verbal process supervision (VPS),\nachieving a 3.65\\% improvement over baseline models on challenging benchmarks\nlike BigCodeBench. Furthermore, CURA, when paired with the o3-mini model and\nVPS techniques, attains state-of-the-art performance. This work represents a\nstep forward in integrating reasoning-driven architectures with LLM-based code\ngeneration, enabling agentic reasoning for language models to solve complex\nsoftware engineering tasks.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/65bef46337491e7adc5ee7c9/kVdnqHbvP4VXfEWb2MAs_.jpeg",
      "https://cdn-uploads.huggingface.co/production/uploads/65bef46337491e7adc5ee7c9/amlMqiikDojWAwsnS3d5e.png",
      "https://cdn-uploads.huggingface.co/production/uploads/65bef46337491e7adc5ee7c9/cndDykrtJViO6gRavkF9D.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18494.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65bef46337491e7adc5ee7c9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/bQ2WY0bU65IQ3IBn6SrnA.png",
      "fullname": "Hao-Yuan Chen",
      "name": "MarkChenX",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]