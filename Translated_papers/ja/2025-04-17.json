[
  {
    "paper": {
      "id": "2504.10514",
      "authors": [
        {
          "_id": "67ffedb8b0c26d6ec0b608cf",
          "name": "Yijun Liang",
          "hidden": false
        },
        {
          "_id": "67ffedb8b0c26d6ec0b608d0",
          "name": "Ming Li",
          "hidden": false
        },
        {
          "_id": "67ffedb8b0c26d6ec0b608d1",
          "user": {
            "_id": "64a8121e35fab7cd04c30ed0",
            "avatarUrl": "/avatars/48849b84703158772f1022932331b143.svg",
            "isPro": false,
            "fullname": "Chenrui Fan",
            "user": "Fcr09",
            "type": "user"
          },
          "name": "Chenrui Fan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-17T08:05:05.662Z",
          "hidden": false
        },
        {
          "_id": "67ffedb8b0c26d6ec0b608d2",
          "name": "Ziyue Li",
          "hidden": false
        },
        {
          "_id": "67ffedb8b0c26d6ec0b608d3",
          "name": "Dang Nguyen",
          "hidden": false
        },
        {
          "_id": "67ffedb8b0c26d6ec0b608d4",
          "user": {
            "_id": "63f546e0fcf95ecac2b0ee3e",
            "avatarUrl": "/avatars/02a401bcff91cc473d9946bbb771a985.svg",
            "isPro": false,
            "fullname": "Kwesi Cobbina",
            "user": "kweCobi",
            "type": "user"
          },
          "name": "Kwesi Cobbina",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:08:36.225Z",
          "hidden": false
        },
        {
          "_id": "67ffedb8b0c26d6ec0b608d5",
          "user": {
            "_id": "639d4b8d860db464ae35c3ab",
            "avatarUrl": "/avatars/ec0fa3e91593a03fc9fb611e66b30553.svg",
            "isPro": false,
            "fullname": "Shweta Bhardwaj",
            "user": "shweta12",
            "type": "user"
          },
          "name": "Shweta Bhardwaj",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:08:29.680Z",
          "hidden": false
        },
        {
          "_id": "67ffedb8b0c26d6ec0b608d6",
          "user": {
            "_id": "6393847e3e30234ae798b7be",
            "avatarUrl": "/avatars/daeb8c37dff4432d837a69b87c196521.svg",
            "isPro": true,
            "fullname": "JiuhaiChen",
            "user": "jiuhai",
            "type": "user"
          },
          "name": "Jiuhai Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:08:22.786Z",
          "hidden": false
        },
        {
          "_id": "67ffedb8b0c26d6ec0b608d7",
          "name": "Fuxiao Liu",
          "hidden": false
        },
        {
          "_id": "67ffedb8b0c26d6ec0b608d8",
          "user": {
            "_id": "647f5af5b0e96764589f3b2a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
            "isPro": false,
            "fullname": "Tianyi Zhou",
            "user": "zhoutianyi",
            "type": "user"
          },
          "name": "Tianyi Zhou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-17T08:05:07.396Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-10T16:36:26.000Z",
      "submittedOnDailyAt": "2025-04-17T00:58:33.032Z",
      "title": "ColorBench: VLMsは色の世界を見ることと理解することができるか？色覚の見込み、理由論と強固性の評価基準",
      "submittedOnDailyBy": {
        "_id": "647f5af5b0e96764589f3b2a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
        "isPro": false,
        "fullname": "Tianyi Zhou",
        "user": "zhoutianyi",
        "type": "user"
      },
      "summary": "色調は人間の認識に重要な役割を果たし、通常、ビジュアル推論において重要なヒントを提供します。しかし、視覚言語モデル（VLMs）が色を人間のように認識、理解し、利用することができるかどうかは明確ではありません。本論文では、色認識の能力を評価するための新しいベンチマーク「ColorBench」を紹介します。これは、色認識、理由の成立、そして色の変換に対する強固性を含む色理解の能力を評価するために詳細に作成されました。多様なテストスケnarioを選び、実世界的なアプリケーションに基づいてコレクトしたことで、ColorBenchはこれらのモデルが色をどのように認識し、色ベースのコードから意味を推論し、色の変換に対して一貫した性能を維持するかを評価します。32つのVLMsを構成する異なる言語モデルと視覚エンコーダーを構成し、詳細な評価を行い、本論文では以下の新しい発見を明らかにします： (i) スケーリングラーム（大きなモデルが良い）はColorBenchでも維持され、言語モデルが視覚エンコーダーよりも重要な役割を果たしています。 (ii) しかし、モデル間の性能間隔は相対的に小さく、既存のVLMsが色理解を大きく遺棄していることを示しています。 (iii) CoT推論は色理解の精度と強固性を向上させますが、それは視覚センシティータスクです。 (iv) VLMsはColorBenchで色ベースのコードを利用しますが、それらはそのようなタスクでモデルを誤導することもあります。これらの発見は現在のVLMsの重要な制限を明らかにし、色理解の向上が必要としています。ColorBenchは、多タイプAIの人間レベルの色理解の研究を進めるための基盤的なツールとして役立ちます。",
      "upvotes": 18,
      "discussionId": "67ffedbeb0c26d6ec0b60a5b",
      "projectPage": "https://huggingface.co/datasets/umd-zhou-lab/ColorBench",
      "githubRepo": "https://github.com/tianyi-lab/ColorBench",
      "ai_keywords": [
        "vision-language models (VLMs)",
        "color understanding",
        "color perception",
        "color-based cues",
        "color transformations",
        "scaling law",
        "language model",
        "vision encoder",
        "CoT reasoning",
        "multimodal AI"
      ]
    },
    "publishedAt": "2025-04-10T12:36:26.000Z",
    "title": "ColorBench: Can VLMs See and Understand the Colorful World? A\n  Comprehensive Benchmark for Color Perception, Reasoning, and Robustness",
    "summary": "Color plays an important role in human perception and usually provides\ncritical clues in visual reasoning. However, it is unclear whether and how\nvision-language models (VLMs) can perceive, understand, and leverage color as\nhumans. This paper introduces ColorBench, an innovative benchmark meticulously\ncrafted to assess the capabilities of VLMs in color understanding, including\ncolor perception, reasoning, and robustness. By curating a suite of diverse\ntest scenarios, with grounding in real applications, ColorBench evaluates how\nthese models perceive colors, infer meanings from color-based cues, and\nmaintain consistent performance under varying color transformations. Through an\nextensive evaluation of 32 VLMs with varying language models and vision\nencoders, our paper reveals some undiscovered findings: (i) The scaling law\n(larger models are better) still holds on ColorBench, while the language model\nplays a more important role than the vision encoder. (ii) However, the\nperformance gaps across models are relatively small, indicating that color\nunderstanding has been largely neglected by existing VLMs. (iii) CoT reasoning\nimproves color understanding accuracies and robustness, though they are\nvision-centric tasks. (iv) Color clues are indeed leveraged by VLMs on\nColorBench but they can also mislead models in some tasks. These findings\nhighlight the critical limitations of current VLMs and underscore the need to\nenhance color comprehension. Our ColorBenchcan serve as a foundational tool for\nadvancing the study of human-level color understanding of multimodal AI.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10514.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "647f5af5b0e96764589f3b2a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
      "fullname": "Tianyi Zhou",
      "name": "zhoutianyi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.12240",
      "authors": [
        {
          "_id": "680057b49031335df49732fc",
          "user": {
            "_id": "64970d3d9c3b29dca8633f87",
            "avatarUrl": "/avatars/11e3c9c66d28490d6d09925f9aa47cd1.svg",
            "isPro": false,
            "fullname": "JunhaoZhuang",
            "user": "JunhaoZhuang",
            "type": "user"
          },
          "name": "Junhao Zhuang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-17T08:04:09.222Z",
          "hidden": false
        },
        {
          "_id": "680057b49031335df49732fd",
          "user": {
            "_id": "66837d3c48edefb453b0640a",
            "avatarUrl": "/avatars/b16385eaa612578728e2c6460a76b38f.svg",
            "isPro": false,
            "fullname": "Lingen Li",
            "user": "l-li",
            "type": "user"
          },
          "name": "Lingen Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:09:48.492Z",
          "hidden": false
        },
        {
          "_id": "680057b49031335df49732fe",
          "user": {
            "_id": "62d4577bc85b0fcf7fde39bb",
            "avatarUrl": "/avatars/a3a5729e33ae89ce9ba408830db3c835.svg",
            "isPro": false,
            "fullname": "Xuan Ju",
            "user": "juxuan27",
            "type": "user"
          },
          "name": "Xuan Ju",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:09:55.870Z",
          "hidden": false
        },
        {
          "_id": "680057b49031335df49732ff",
          "name": "Zhaoyang Zhang",
          "hidden": false
        },
        {
          "_id": "680057b49031335df4973300",
          "name": "Chun Yuan",
          "hidden": false
        },
        {
          "_id": "680057b49031335df4973301",
          "user": {
            "_id": "63ca3ddc04c979828310bfcb",
            "avatarUrl": "/avatars/615e0d8622950b4408b40d550f02a894.svg",
            "isPro": false,
            "fullname": "Ying Shan",
            "user": "yshan2u",
            "type": "user"
          },
          "name": "Ying Shan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:12:47.014Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64970d3d9c3b29dca8633f87/VweYE_xmPVmixUo3dh0Wu.png",
        "https://cdn-uploads.huggingface.co/production/uploads/64970d3d9c3b29dca8633f87/J9PY5rVuEtD_-ZLibCR1W.png"
      ],
      "publishedAt": "2025-04-16T16:45:19.000Z",
      "submittedOnDailyAt": "2025-04-17T00:32:24.205Z",
      "title": "コブラ: 広範囲参照を用いた効率的な線画上色法",
      "submittedOnDailyBy": {
        "_id": "64970d3d9c3b29dca8633f87",
        "avatarUrl": "/avatars/11e3c9c66d28490d6d09925f9aa47cd1.svg",
        "isPro": false,
        "fullname": "JunhaoZhuang",
        "user": "JunhaoZhuang",
        "type": "user"
      },
      "summary": "コミック制作業界では、高精度、効率、コンテキストの一貫性、柔軟な制御を要求する基準に基づく線画の着色が必要とされています。コミックの一ページには、多様な人物、物体、背景が含まれているため、着色プロセスが複雑になります。画像生成のための拡散モデルの進歩はあるが、線画の着色においては、拡散モデルの応用が限られ、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モデルの応用には、拡散モ",
      "upvotes": 16,
      "discussionId": "680057b89031335df497343e",
      "projectPage": "https://zhuang2002.github.io/Cobra/",
      "githubRepo": "https://github.com/zhuang2002/Cobra",
      "ai_keywords": [
        "diffusion models",
        "line art colorization",
        "contextual image guidance",
        "color hints",
        "Causal Sparse DiT architecture",
        "positional encodings",
        "causal sparse attention",
        "Key-Value Cache",
        "long-context references",
        "color identity consistency"
      ]
    },
    "publishedAt": "2025-04-16T12:45:19.000Z",
    "title": "Cobra: Efficient Line Art COlorization with BRoAder References",
    "summary": "The comic production industry requires reference-based line art colorization\nwith high accuracy, efficiency, contextual consistency, and flexible control. A\ncomic page often involves diverse characters, objects, and backgrounds, which\ncomplicates the coloring process. Despite advancements in diffusion models for\nimage generation, their application in line art colorization remains limited,\nfacing challenges related to handling extensive reference images,\ntime-consuming inference, and flexible control. We investigate the necessity of\nextensive contextual image guidance on the quality of line art colorization. To\naddress these challenges, we introduce Cobra, an efficient and versatile method\nthat supports color hints and utilizes over 200 reference images while\nmaintaining low latency. Central to Cobra is a Causal Sparse DiT architecture,\nwhich leverages specially designed positional encodings, causal sparse\nattention, and Key-Value Cache to effectively manage long-context references\nand ensure color identity consistency. Results demonstrate that Cobra achieves\naccurate line art colorization through extensive contextual reference,\nsignificantly enhancing inference speed and interactivity, thereby meeting\ncritical industrial demands. We release our codes and models on our project\npage: https://zhuang2002.github.io/Cobra/.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64970d3d9c3b29dca8633f87/VweYE_xmPVmixUo3dh0Wu.png",
      "https://cdn-uploads.huggingface.co/production/uploads/64970d3d9c3b29dca8633f87/J9PY5rVuEtD_-ZLibCR1W.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.12240.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64970d3d9c3b29dca8633f87",
      "avatarUrl": "/avatars/11e3c9c66d28490d6d09925f9aa47cd1.svg",
      "fullname": "JunhaoZhuang",
      "name": "JunhaoZhuang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 32
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.12285",
      "authors": [
        {
          "_id": "68006e6e175c8dce4ec17f7a",
          "user": {
            "_id": "613f07f40153aafa379775f2",
            "avatarUrl": "/avatars/3965175b320d753d9a5ccb0c7d9298a4.svg",
            "isPro": false,
            "fullname": "Shuming Ma",
            "user": "shumingma",
            "type": "user"
          },
          "name": "Shuming Ma",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:13:17.676Z",
          "hidden": false
        },
        {
          "_id": "68006e6e175c8dce4ec17f7b",
          "user": {
            "_id": "63f71771d36951307fcb4dcd",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f71771d36951307fcb4dcd/1c-GSQmSSuDXyVWjxZFy_.jpeg",
            "isPro": false,
            "fullname": "Hongyu Wang",
            "user": "hongyuw",
            "type": "user"
          },
          "name": "Hongyu Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:14:45.597Z",
          "hidden": false
        },
        {
          "_id": "68006e6e175c8dce4ec17f7c",
          "user": {
            "_id": "632bd2f72d6a805eeb4bc601",
            "avatarUrl": "/avatars/6e1533e8a599f3068290aa69ac82cab7.svg",
            "isPro": false,
            "fullname": "HUANG SHAOHAN",
            "user": "buaahsh",
            "type": "user"
          },
          "name": "Shaohan Huang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:14:59.750Z",
          "hidden": false
        },
        {
          "_id": "68006e6e175c8dce4ec17f7d",
          "user": {
            "_id": "64abbcff6cadc7aca584f71b",
            "avatarUrl": "/avatars/fc6e85ad4a8befd133a37b411712c648.svg",
            "isPro": false,
            "fullname": "Xingxing Zhang",
            "user": "THU-CHUNXIA",
            "type": "user"
          },
          "name": "Xingxing Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:15:06.359Z",
          "hidden": false
        },
        {
          "_id": "68006e6e175c8dce4ec17f7e",
          "name": "Ying Hu",
          "hidden": false
        },
        {
          "_id": "68006e6e175c8dce4ec17f7f",
          "name": "Ting Song",
          "hidden": false
        },
        {
          "_id": "68006e6e175c8dce4ec17f80",
          "name": "Yan Xia",
          "hidden": false
        },
        {
          "_id": "68006e6e175c8dce4ec17f81",
          "user": {
            "_id": "6368c512fbfe97c16a40baba",
            "avatarUrl": "/avatars/1c23bc7c0b6d9225699ce27647623d7a.svg",
            "isPro": false,
            "fullname": "Furu Wei",
            "user": "thegenerality",
            "type": "user"
          },
          "name": "Furu Wei",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:15:42.250Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-16T17:51:43.000Z",
      "submittedOnDailyAt": "2025-04-17T01:29:56.744Z",
      "title": "BitNet b1.58 2B4T 技術報告",
      "submittedOnDailyBy": {
        "_id": "63f71771d36951307fcb4dcd",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f71771d36951307fcb4dcd/1c-GSQmSSuDXyVWjxZFy_.jpeg",
        "isPro": false,
        "fullname": "Hongyu Wang",
        "user": "hongyuw",
        "type": "user"
      },
      "summary": "ビットネット b1.58 2B4T、最初の開放ソース、原生1ビットの大規模言語モデル（LLM）を紹介します。このモデルは2億パラメータスケールであり、4兆トークンのコーパスにより訓練されています。モデルは言語理解、数学的推理、コーディング能力、および会話能力のベンチマークを掲載し、厳密な評価を受けました。我々の結果は、ビットネット b1.58 2B4Tが同じサイズの領先する開放ウェイト、全精度LLMと同等の性能を達成し、計算効率の大幅な優勢を提供していることを示しています。特にメモリ使用量、エネルギー消費量、解確定時間が大幅に減少しています。また、進める研究および採用を促進するため、モデルの重みはHugging Faceで公開され、両方GPUとCPUアーキテクチャの開放ソース推論実装も提供されています。",
      "upvotes": 12,
      "discussionId": "68006e70175c8dce4ec17fc0",
      "ai_keywords": [
        "BitNet b1.58 2B4T",
        "Large Language Model (LLM)",
        "1-bit Large Language Model",
        "Train",
        "Corpus",
        "4 trillion tokens",
        "Benchmarks",
        "Language understanding",
        "Mathematical reasoning",
        "Coding proficiency",
        "Conversational ability"
      ]
    },
    "publishedAt": "2025-04-16T13:51:43.000Z",
    "title": "BitNet b1.58 2B4T Technical Report",
    "summary": "We introduce BitNet b1.58 2B4T, the first open-source, native 1-bit Large\nLanguage Model (LLM) at the 2-billion parameter scale. Trained on a corpus of 4\ntrillion tokens, the model has been rigorously evaluated across benchmarks\ncovering language understanding, mathematical reasoning, coding proficiency,\nand conversational ability. Our results demonstrate that BitNet b1.58 2B4T\nachieves performance on par with leading open-weight, full-precision LLMs of\nsimilar size, while offering significant advantages in computational\nefficiency, including substantially reduced memory footprint, energy\nconsumption, and decoding latency. To facilitate further research and adoption,\nthe model weights are released via Hugging Face along with open-source\ninference implementations for both GPU and CPU architectures.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.12285.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63f71771d36951307fcb4dcd",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f71771d36951307fcb4dcd/1c-GSQmSSuDXyVWjxZFy_.jpeg",
      "fullname": "Hongyu Wang",
      "name": "hongyuw",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 13
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.10326",
      "authors": [
        {
          "_id": "67ff772061373fdf16ce1d38",
          "user": {
            "_id": "66486ba1640bc89c93bcc8a2",
            "avatarUrl": "/avatars/07e07bd768339495e55b7b17f2f8bc59.svg",
            "isPro": false,
            "fullname": "Yangshen Deng",
            "user": "YangshenDeng",
            "type": "user"
          },
          "name": "Yangshen Deng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-16T11:57:58.302Z",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d39",
          "name": "Zhengxin You",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d3a",
          "name": "Long Xiang",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d3b",
          "user": {
            "_id": "66d6be0bddf54fd90923c727",
            "avatarUrl": "/avatars/7bb82c8c339db944d79d47b3b9b35aa8.svg",
            "isPro": false,
            "fullname": "Li",
            "user": "Qilong00",
            "type": "user"
          },
          "name": "Qilong Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:16:16.665Z",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d3c",
          "user": {
            "_id": "66efe50667c4ce2c9024cd45",
            "avatarUrl": "/avatars/7b7f650953c371f08a5beecc500b6a43.svg",
            "isPro": false,
            "fullname": "peiqiyuan",
            "user": "YuanPeiqi",
            "type": "user"
          },
          "name": "Peiqi Yuan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:16:22.829Z",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d3d",
          "name": "Zhaoyang Hong",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d3e",
          "user": {
            "_id": "66d6c339b61dd11022907252",
            "avatarUrl": "/avatars/d2ed3cc003e94b2e5204ce0f8a481dcf.svg",
            "isPro": false,
            "fullname": "Yitao Zheng",
            "user": "FeTieTer",
            "type": "user"
          },
          "name": "Yitao Zheng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:16:36.808Z",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d3f",
          "name": "Wanting Li",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d40",
          "user": {
            "_id": "671a7bce2b10d343bab18637",
            "avatarUrl": "/avatars/af1c10a59236d953b42e67d3955eecc4.svg",
            "isPro": false,
            "fullname": "runzhong",
            "user": "runzhongli",
            "type": "user"
          },
          "name": "Runzhong Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:17:02.031Z",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d41",
          "user": {
            "_id": "63898b61ec1f539adc0f4da2",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674248167280-63898b61ec1f539adc0f4da2.jpeg",
            "isPro": false,
            "fullname": "Haotian Liu",
            "user": "liuhaotian",
            "type": "user"
          },
          "name": "Haotian Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:17:25.728Z",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d42",
          "name": "Kyriakos Mouratidis",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d43",
          "name": "Man Lung Yiu",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d44",
          "name": "Huan Li",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d45",
          "name": "Qiaomu Shen",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d46",
          "name": "Rui Mao",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d47",
          "user": {
            "_id": "66b02a2642c34e7a212133c0",
            "avatarUrl": "/avatars/737a69b095e8c427ecd08f870b173635.svg",
            "isPro": false,
            "fullname": "Bo Tang",
            "user": "BO1022",
            "type": "user"
          },
          "name": "Bo Tang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:18:01.460Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-14T15:34:26.000Z",
      "submittedOnDailyAt": "2025-04-17T06:00:20.705Z",
      "title": "AlayaDB: 長文脈LLM推論の効率的かつ有効なデータ基盤",
      "submittedOnDailyBy": {
        "_id": "66486ba1640bc89c93bcc8a2",
        "avatarUrl": "/avatars/07e07bd768339495e55b7b17f2f8bc59.svg",
        "isPro": false,
        "fullname": "Yangshen Deng",
        "user": "YangshenDeng",
        "type": "user"
      },
      "summary": "アライアDBは、アライアDB AIでの大脳言語モデル（LLMs）の長文脈推論に効率的かつ有効な構造を原生に構築した最先端のベクトルデータベースシステムです。特に、LLM推論システムからKVキャッシュと注意計算を分離し、新しいベクトルデータベースシステムに収まります。モデルアジェンス提供者（MaaS）に対して、アライアDBは現在の代替ソリューション（例：KVキャッシュの分離、検索ベースの稀疏注意）と比較して、異なるサービスレベル目標（SLOs）に対する多様なワークロードでは、軽い設備資源の使用量と高品質の生成を提供します。アライアDBの核心は、LLM推論の注意計算とキャッシュ管理をクエリ処理プロセスに抽象化し、ネイティブなクエリオペレーターで性能を最適化します。本論文中、我々は、業界パートナーからの3つのケーススタディとLLM推論ベンチマークの拡大的な実験結果を通じて、アライアDBの効果性を示します。",
      "upvotes": 12,
      "discussionId": "67ff772261373fdf16ce1d93",
      "ai_keywords": [
        "vector database",
        "long-context inference",
        "Large Language Models (LLMs)",
        "KV cache",
        "attention computation",
        "Model as a Service (MaaS)",
        "Service Level Objectives (SLOs)",
        "KV cache disaggregation",
        "retrieval-based sparse attention",
        "query processing procedure",
        "native query optimizer",
        "LLM inference benchmarks"
      ]
    },
    "publishedAt": "2025-04-14T11:34:26.000Z",
    "title": "AlayaDB: The Data Foundation for Efficient and Effective Long-context\n  LLM Inference",
    "summary": "AlayaDB is a cutting-edge vector database system natively architected for\nefficient and effective long-context inference for Large Language Models (LLMs)\nat AlayaDB AI. Specifically, it decouples the KV cache and attention\ncomputation from the LLM inference systems, and encapsulates them into a novel\nvector database system. For the Model as a Service providers (MaaS), AlayaDB\nconsumes fewer hardware resources and offers higher generation quality for\nvarious workloads with different kinds of Service Level Objectives (SLOs), when\ncomparing with the existing alternative solutions (e.g., KV cache\ndisaggregation, retrieval-based sparse attention). The crux of AlayaDB is that\nit abstracts the attention computation and cache management for LLM inference\ninto a query processing procedure, and optimizes the performance via a native\nquery optimizer. In this work, we demonstrate the effectiveness of AlayaDB via\n(i) three use cases from our industry partners, and (ii) extensive experimental\nresults on LLM inference benchmarks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10326.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66486ba1640bc89c93bcc8a2",
      "avatarUrl": "/avatars/07e07bd768339495e55b7b17f2f8bc59.svg",
      "fullname": "Yangshen Deng",
      "name": "YangshenDeng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.09081",
      "authors": [
        {
          "_id": "67fdbfcccaa65039e8c3d8ae",
          "user": {
            "_id": "67dcd93f73e2178fe917b893",
            "avatarUrl": "/avatars/86ec25d32c45da3937280ef9a619f19e.svg",
            "isPro": false,
            "fullname": "Prabhat Pandey",
            "user": "panprabh",
            "type": "user"
          },
          "name": "Prabhat Pandey",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-16T09:23:52.370Z",
          "hidden": false
        },
        {
          "_id": "67fdbfcccaa65039e8c3d8af",
          "name": "Rupak Vignesh Swaminathan",
          "hidden": false
        },
        {
          "_id": "67fdbfcccaa65039e8c3d8b0",
          "user": {
            "_id": "66279810009f1bfdc6bf71bf",
            "avatarUrl": "/avatars/97f561c91b92e1abb4fe6f0b5c688126.svg",
            "isPro": false,
            "fullname": "Girish",
            "user": "vijaygirish2001",
            "type": "user"
          },
          "name": "K V Vijay Girish",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-16T09:23:49.809Z",
          "hidden": false
        },
        {
          "_id": "67fdbfcccaa65039e8c3d8b1",
          "user": {
            "_id": "66367dfb2d6b86ff193dbbe0",
            "avatarUrl": "/avatars/0a0c230bb5fc81a28a166691146cf807.svg",
            "isPro": false,
            "fullname": "Arunasish Sen",
            "user": "svinxz",
            "type": "user"
          },
          "name": "Arunasish Sen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-16T11:58:01.767Z",
          "hidden": false
        },
        {
          "_id": "67fdbfcccaa65039e8c3d8b2",
          "name": "Jian Xie",
          "hidden": false
        },
        {
          "_id": "67fdbfcccaa65039e8c3d8b3",
          "name": "Grant P. Strimel",
          "hidden": false
        },
        {
          "_id": "67fdbfcccaa65039e8c3d8b4",
          "name": "Andreas Schwarz",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-12T04:45:48.000Z",
      "submittedOnDailyAt": "2025-04-17T07:32:43.212Z",
      "title": "SIFT-50M: 音声指令の大規模多言語データセットでの微調節",
      "submittedOnDailyBy": {
        "_id": "67dcd93f73e2178fe917b893",
        "avatarUrl": "/avatars/86ec25d32c45da3937280ef9a619f19e.svg",
        "isPro": false,
        "fullname": "Prabhat Pandey",
        "user": "panprabh",
        "type": "user"
      },
      "summary": "SIFT（スピーチインストラクティングファインチューニング）を紹介します。SIFT-50Mは、インストラクションファインチューニングとスピーチ-テキスト大語言モデル（LLMs）の事前学習に適した50M例のデータセットです。SIFT-50Mは公開で利用可能なスピーチコーパスから構築され、合計14,000時間のスピーチを含み、LLMsと商用の専門モデルを活用しています。このデータセットは5言語を拡張し、多様なスピーチ理解および制御可能なスピーチ生成インストラクションを含みます。SIFT-50Mを使用して、SIFT-LLMを訓練し、インストラクション従いベンチマークで現在のスピーチ-テキストLLMsを超え、基盤的なスピーチタスクで優れた性能を収めました。また、さらなる研究のために、EvalSIFTという、スピーチ-テキストLLMsのインストラクション従い能力を評価するために特に設計されたベンチマークデータセットを紹介します。",
      "upvotes": 9,
      "discussionId": "67fdbfe0caa65039e8c3de4b",
      "ai_keywords": [
        "SIFT (Speech Instruction Fine-Tuning)",
        "speech-text large language models (LLMs)",
        "instruction fine-tuning",
        "pre-training",
        "speech corpora",
        "off-the-shelf expert models",
        "speech understanding",
        "controllable speech generation",
        "SIFT-LLM",
        "instruction-following benchmarks",
        "foundational speech tasks",
        "EvalSIFT"
      ]
    },
    "publishedAt": "2025-04-12T00:45:48.000Z",
    "title": "SIFT-50M: A Large-Scale Multilingual Dataset for Speech Instruction\n  Fine-Tuning",
    "summary": "We introduce SIFT (Speech Instruction Fine-Tuning), a 50M-example dataset\ndesigned for instruction fine-tuning and pre-training of speech-text large\nlanguage models (LLMs). SIFT-50M is built from publicly available speech\ncorpora, which collectively contain 14K hours of speech, and leverages LLMs\nalong with off-the-shelf expert models. The dataset spans five languages,\nencompassing a diverse range of speech understanding as well as controllable\nspeech generation instructions. Using SIFT-50M, we train SIFT-LLM, which\noutperforms existing speech-text LLMs on instruction-following benchmarks while\nachieving competitive performance on foundational speech tasks. To support\nfurther research, we also introduce EvalSIFT, a benchmark dataset specifically\ndesigned to evaluate the instruction-following capabilities of speech-text\nLLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.09081.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67dcd93f73e2178fe917b893",
      "avatarUrl": "/avatars/86ec25d32c45da3937280ef9a619f19e.svg",
      "fullname": "Prabhat Pandey",
      "name": "panprabh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.10483",
      "authors": [
        {
          "_id": "67fdd3e3913c97aa32f94e9b",
          "user": {
            "_id": "641480554b1701c01cdb36c4",
            "avatarUrl": "/avatars/f1f6b294e0236d76a68c099164c81f36.svg",
            "isPro": false,
            "fullname": "Xingjian Leng",
            "user": "xingjianleng",
            "type": "user"
          },
          "name": "Xingjian Leng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-16T09:23:36.449Z",
          "hidden": false
        },
        {
          "_id": "67fdd3e3913c97aa32f94e9c",
          "name": "Jaskirat Singh",
          "hidden": false
        },
        {
          "_id": "67fdd3e3913c97aa32f94e9d",
          "user": {
            "_id": "6752870ec63bc5b670b1b27e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6752870ec63bc5b670b1b27e/3CdHxnyTKbGup-1V67nEV.jpeg",
            "isPro": false,
            "fullname": "Yunzhong Hou",
            "user": "yunzhong-hou",
            "type": "user"
          },
          "name": "Yunzhong Hou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:34:11.946Z",
          "hidden": false
        },
        {
          "_id": "67fdd3e3913c97aa32f94e9e",
          "user": {
            "_id": "63bf533f4a2beec65568f813",
            "avatarUrl": "/avatars/bba583653b5cada8dd4a3ff2281e9dec.svg",
            "isPro": false,
            "fullname": "Xing",
            "user": "Zhenchang",
            "type": "user"
          },
          "name": "Zhenchang Xing",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:34:21.190Z",
          "hidden": false
        },
        {
          "_id": "67fdd3e3913c97aa32f94e9f",
          "user": {
            "_id": "6596422646624a86ff3b3bda",
            "avatarUrl": "/avatars/216e12b77e45ac5f1fa20932f5745411.svg",
            "isPro": false,
            "fullname": "Saining Xie",
            "user": "sainx",
            "type": "user"
          },
          "name": "Saining Xie",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:34:39.700Z",
          "hidden": false
        },
        {
          "_id": "67fdd3e3913c97aa32f94ea0",
          "user": {
            "_id": "666351ebd86c026caa135e5c",
            "avatarUrl": "/avatars/50a37f7e999f660c69f518b71577eb7d.svg",
            "isPro": false,
            "fullname": "Liang Zheng",
            "user": "liangzheng06",
            "type": "user"
          },
          "name": "Liang Zheng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:34:56.041Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-14T17:59:53.000Z",
      "submittedOnDailyAt": "2025-04-17T05:08:17.990Z",
      "title": "REPA-E: 潜在ディフュージョンを用いた終端から終端までのチューニングでVAEを解放する\nTransformers",
      "submittedOnDailyBy": {
        "_id": "5f1158120c833276f61f1a84",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
        "isPro": false,
        "fullname": "Niels Rogge",
        "user": "nielsr",
        "type": "user"
      },
      "summary": "この論文では、基本的な問題を課題に取り入れています: \"潜在的なディフュージョンモデルと変分オートエンコーダー（VAE）トーキナイザーを統一して終始一斉で訓練できるか？\" 傳統的な深層学習の知識は、可能な限り終始一斉の訓練は好ましいと教えています。しかし、潜在的なディフュージョントランスフォーマーの場合、標準的なディフュージョン損失を用いてVAEとディフュージョンモデルを終始一斉で訓練することは、最終的な性能が低下してしまうことが見られています。私たちは、ディフュージョン損失が無効であることを示し、終始一斉訓練を可能にするために、表現の調整（REPA）損失を使用することができることを示します。その簡単な REPA-E の訓練ペションは、REPA とベージャーの訓練ペションに比べてディフュージョンモデルの訓練速度を17倍以上、45倍以上高速化します。興味深いながら、REPA-E での終始一斉訓練は、VAE自体も改善し、潜在的な空間構造とダウンストリーム生成性能も向上させます。最終的な性能において、我々のアプローチは新しい最先端となります。ImageNet 256 x 256では、カラスフレードガイド無しと有しの場合、FIDが1.26と1.83を達成します。コードは https://end2end-diffusion.github.io に提供されています。",
      "upvotes": 3,
      "discussionId": "67fdd3e5913c97aa32f94ee3",
      "projectPage": "https://end2end-diffusion.github.io/",
      "githubRepo": "https://github.com/End2End-Diffusion/REPA-E",
      "ai_keywords": [
        "latent diffusion models",
        "variational auto-encoder (VAE) tokenizer",
        "end-to-end training",
        "diffusion-loss",
        "representation-alignment (REPA) loss",
        "diffusion model training",
        "VAE",
        "latent space structure",
        "downstream generation performance",
        "FID",
        "ImageNet 256 x 256"
      ]
    },
    "publishedAt": "2025-04-14T13:59:53.000Z",
    "title": "REPA-E: Unlocking VAE for End-to-End Tuning with Latent Diffusion\n  Transformers",
    "summary": "In this paper we tackle a fundamental question: \"Can we train latent\ndiffusion models together with the variational auto-encoder (VAE) tokenizer in\nan end-to-end manner?\" Traditional deep-learning wisdom dictates that\nend-to-end training is often preferable when possible. However, for latent\ndiffusion transformers, it is observed that end-to-end training both VAE and\ndiffusion-model using standard diffusion-loss is ineffective, even causing a\ndegradation in final performance. We show that while diffusion loss is\nineffective, end-to-end training can be unlocked through the\nrepresentation-alignment (REPA) loss -- allowing both VAE and diffusion model\nto be jointly tuned during the training process. Despite its simplicity, the\nproposed training recipe (REPA-E) shows remarkable performance; speeding up\ndiffusion model training by over 17x and 45x over REPA and vanilla training\nrecipes, respectively. Interestingly, we observe that end-to-end tuning with\nREPA-E also improves the VAE itself; leading to improved latent space structure\nand downstream generation performance. In terms of final performance, our\napproach sets a new state-of-the-art; achieving FID of 1.26 and 1.83 with and\nwithout classifier-free guidance on ImageNet 256 x 256. Code is available at\nhttps://end2end-diffusion.github.io.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10483.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5f1158120c833276f61f1a84",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
      "fullname": "Niels Rogge",
      "name": "nielsr",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 820
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.11952",
      "authors": [
        {
          "_id": "6800646e0679d4ec4b9d01a7",
          "user": {
            "_id": "645c60dd7d655680b57ddbff",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645c60dd7d655680b57ddbff/MTMtkV6sy44oD-zwkGX0E.png",
            "isPro": true,
            "fullname": "Ram Kadiyala",
            "user": "1024m",
            "type": "user"
          },
          "name": "Ram Mohan Rao Kadiyala",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-17T08:04:05.627Z",
          "hidden": false
        },
        {
          "_id": "6800646e0679d4ec4b9d01a8",
          "user": {
            "_id": "63da8ba3f03c3d71ef32408c",
            "avatarUrl": "/avatars/1b2e6f3ea2bac5ab35dbd53edb7f8cf2.svg",
            "isPro": false,
            "fullname": "Siddartha Pullakhandam",
            "user": "Siddartha10",
            "type": "user"
          },
          "name": "Siddartha Pullakhandam",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:18:54.938Z",
          "hidden": false
        },
        {
          "_id": "6800646e0679d4ec4b9d01a9",
          "name": "Kanwal Mehreen",
          "hidden": false
        },
        {
          "_id": "6800646e0679d4ec4b9d01aa",
          "user": {
            "_id": "618c1ad1c74578e0a4a4d074",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/618c1ad1c74578e0a4a4d074/8u_AkeHt4d6xtQ8hzaffU.jpeg",
            "isPro": true,
            "fullname": "Drishti Sharma",
            "user": "DrishtiSharma",
            "type": "user"
          },
          "name": "Drishti Sharma",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:19:22.573Z",
          "hidden": false
        },
        {
          "_id": "6800646e0679d4ec4b9d01ab",
          "name": "Siddhant Gupta",
          "hidden": false
        },
        {
          "_id": "6800646e0679d4ec4b9d01ac",
          "user": {
            "_id": "66c578770a22b2f9ab575847",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c578770a22b2f9ab575847/-zfNho1DR3yZHDazq669-.png",
            "isPro": false,
            "fullname": "Jebish Purbey",
            "user": "jebish7",
            "type": "user"
          },
          "name": "Jebish Purbey",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:23:00.066Z",
          "hidden": false
        },
        {
          "_id": "6800646e0679d4ec4b9d01ad",
          "user": {
            "_id": "653d84f13fc9c706fa755d03",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653d84f13fc9c706fa755d03/F_jYbeuLLM9EX8hKXcbHD.png",
            "isPro": false,
            "fullname": "Ashay Srivastava",
            "user": "ashay-sriv",
            "type": "user"
          },
          "name": "Ashay Srivastava",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:23:15.588Z",
          "hidden": false
        },
        {
          "_id": "6800646e0679d4ec4b9d01ae",
          "user": {
            "_id": "669a745a4bbe8ad52ee287cf",
            "avatarUrl": "/avatars/245644aa638b45a17ff71124bd5bbe0f.svg",
            "isPro": false,
            "fullname": "Subhasya Tippareddy",
            "user": "subhasyar",
            "type": "user"
          },
          "name": "Subhasya TippaReddy",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:23:23.946Z",
          "hidden": false
        },
        {
          "_id": "6800646e0679d4ec4b9d01af",
          "user": {
            "_id": "669a7383c9111326dc596f5e",
            "avatarUrl": "/avatars/8bbd307fd4bb2d7055b2c8fc9140dc81.svg",
            "isPro": false,
            "fullname": "Arvind Reddy Bobbili",
            "user": "Arvindreddy",
            "type": "user"
          },
          "name": "Arvind Reddy Bobbili",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:23:32.804Z",
          "hidden": false
        },
        {
          "_id": "6800646e0679d4ec4b9d01b0",
          "name": "Suraj Telugara Chandrashekhar",
          "hidden": false
        },
        {
          "_id": "6800646e0679d4ec4b9d01b1",
          "user": {
            "_id": "668db0bfb09a05f3d7cc796f",
            "avatarUrl": "/avatars/969c511cca4d129b99eca6252a468385.svg",
            "isPro": false,
            "fullname": "Modabbir Adeeb",
            "user": "moda10",
            "type": "user"
          },
          "name": "Modabbir Adeeb",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:23:46.461Z",
          "hidden": false
        },
        {
          "_id": "6800646e0679d4ec4b9d01b2",
          "user": {
            "_id": "641c3337f0b71a9743629985",
            "avatarUrl": "/avatars/820b124887f173c263a675728baf99c6.svg",
            "isPro": false,
            "fullname": "Srinadh Vura",
            "user": "SriV",
            "type": "user"
          },
          "name": "Srinadh Vura",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:23:53.315Z",
          "hidden": false
        },
        {
          "_id": "6800646e0679d4ec4b9d01b3",
          "name": "Hamza Farooq",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-16T10:29:30.000Z",
      "submittedOnDailyAt": "2025-04-17T00:47:10.039Z",
      "title": "ロバストなフィンギアドエッテクスのAI生成テキスト検出",
      "submittedOnDailyBy": {
        "_id": "645c60dd7d655680b57ddbff",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645c60dd7d655680b57ddbff/MTMtkV6sy44oD-zwkGX0E.png",
        "isPro": true,
        "fullname": "Ram Kadiyala",
        "user": "1024m",
        "type": "user"
      },
      "summary": "理想なマシン生成内容の検出システムは、いくつかのジェネレーター上でも良く動作するべきである。既存のシステムは、短いテキストでAI生成内容を正確に識別することが難しい。また、全てのテキストは人間かLLMが完全に著手したものではないので、これらの部分ケースに焦点を当てています。我々の論文では、トークンクラス分類の任務に向けて構築されたモデルのセットを紹介し、これらのモデルは、見たことのないドメイン、見たことのないジェネレーター、非母語者のテキスト、そして対抗的な入力に対しても良く動作しました。また、我々は、23言語でのショートポップのLLMが共著したほとんど全てのテキストを2.4M以上のデータセットを紹介しました。また、各ドメインとジェネレーターのそれぞれのテキストにおけるモデルの性能を示し、対抗的な手法に対する性能比較、入力テキストの長さ、生成テキストと元の人間著手テキストの特徴を比較した結果も追加しました。",
      "upvotes": 2,
      "discussionId": "680064710679d4ec4b9d0224",
      "ai_keywords": [
        "token classification",
        "adversarial inputs"
      ]
    },
    "publishedAt": "2025-04-16T06:29:30.000Z",
    "title": "Robust and Fine-Grained Detection of AI Generated Texts",
    "summary": "An ideal detection system for machine generated content is supposed to work\nwell on any generator as many more advanced LLMs come into existence day by\nday. Existing systems often struggle with accurately identifying AI-generated\ncontent over shorter texts. Further, not all texts might be entirely authored\nby a human or LLM, hence we focused more over partial cases i.e human-LLM\nco-authored texts. Our paper introduces a set of models built for the task of\ntoken classification which are trained on an extensive collection of\nhuman-machine co-authored texts, which performed well over texts of unseen\ndomains, unseen generators, texts by non-native speakers and those with\nadversarial inputs. We also introduce a new dataset of over 2.4M such texts\nmostly co-authored by several popular proprietary LLMs over 23 languages. We\nalso present findings of our models' performance over each texts of each domain\nand generator. Additional findings include comparison of performance against\neach adversarial method, length of input texts and characteristics of generated\ntexts compared to the original human authored texts.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11952.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645c60dd7d655680b57ddbff",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645c60dd7d655680b57ddbff/MTMtkV6sy44oD-zwkGX0E.png",
      "fullname": "Ram Kadiyala",
      "name": "1024m",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 19
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.11092",
      "authors": [
        {
          "_id": "6800a43e1fd95d7dc21d6b83",
          "user": {
            "_id": "63e367d3fae035bdc4c347fc",
            "avatarUrl": "/avatars/239da0c9287bf965348aff4890e94722.svg",
            "isPro": false,
            "fullname": "Jiaxin Huang",
            "user": "JaceyH919",
            "type": "user"
          },
          "name": "Jiaxin Huang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-17T08:03:59.476Z",
          "hidden": false
        },
        {
          "_id": "6800a43e1fd95d7dc21d6b84",
          "name": "Sheng Miao",
          "hidden": false
        },
        {
          "_id": "6800a43e1fd95d7dc21d6b85",
          "name": "BangBnag Yang",
          "hidden": false
        },
        {
          "_id": "6800a43e1fd95d7dc21d6b86",
          "name": "Yuewen Ma",
          "hidden": false
        },
        {
          "_id": "6800a43e1fd95d7dc21d6b87",
          "name": "Yiyi Liao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-15T11:38:14.000Z",
      "submittedOnDailyAt": "2025-04-17T07:32:38.400Z",
      "title": "Vivid4D: ビデオによるモノカメラビデオからの4D再構築の改善",
      "submittedOnDailyBy": {
        "_id": "63e367d3fae035bdc4c347fc",
        "avatarUrl": "/avatars/239da0c9287bf965348aff4890e94722.svg",
        "isPro": false,
        "fullname": "Jiaxin Huang",
        "user": "JaceyH919",
        "type": "user"
      },
      "summary": "4D動的シーンをシンプルに撮影した単目のビデオから再構築することは有價ですが、非常に難しいです。時間スライドの各時間点は単一の視点から見取れています。私たちは、Vivid4Dという新しいアプローチを紹介します。これは、観察視点を増強して4D単目のビデオ合成を向上させることを目指しています。既存の方法と違って、これは単目の深さ先驅を用いて観察視点を増強し、その他の視点のビデオを合成します。これは、観察視点を新しい視点に移動させるようなビデオのカットオペーティングタスクとして再構築されます。これを実現するために、合成されたマスクを用いた無動作ウェブビデオでビデオのカットオペーティングモデルを訓練します。これは、欠損領域の空間的および時間的な一貫性の確保を確認します。また、単目の深さ先驅の不正確性を軽減するために、観察視点の増強を行うイテレーション的なアプローチと強固な再構築損失を導入します。実験は、私たちの方法が単目の4Dシーンの再構築と完成を効果的に向上させることを示しています。",
      "upvotes": 2,
      "discussionId": "6800a4441fd95d7dc21d6d46",
      "projectPage": "https://xdimlab.github.io/Vivid4D/",
      "ai_keywords": [
        "Vivid4D",
        "4D monocular video synthesis",
        "view augmentation",
        "video inpainting",
        "monocular depth priors",
        "unposed web videos",
        "synthetically generated masks",
        "iterative view augmentation strategy",
        "robust reconstruction loss"
      ]
    },
    "publishedAt": "2025-04-15T07:38:14.000Z",
    "title": "Vivid4D: Improving 4D Reconstruction from Monocular Video by Video\n  Inpainting",
    "summary": "Reconstructing 4D dynamic scenes from casually captured monocular videos is\nvaluable but highly challenging, as each timestamp is observed from a single\nviewpoint. We introduce Vivid4D, a novel approach that enhances 4D monocular\nvideo synthesis by augmenting observation views - synthesizing multi-view\nvideos from a monocular input. Unlike existing methods that either solely\nleverage geometric priors for supervision or use generative priors while\noverlooking geometry, we integrate both. This reformulates view augmentation as\na video inpainting task, where observed views are warped into new viewpoints\nbased on monocular depth priors. To achieve this, we train a video inpainting\nmodel on unposed web videos with synthetically generated masks that mimic\nwarping occlusions, ensuring spatially and temporally consistent completion of\nmissing regions. To further mitigate inaccuracies in monocular depth priors, we\nintroduce an iterative view augmentation strategy and a robust reconstruction\nloss. Experiments demonstrate that our method effectively improves monocular 4D\nscene reconstruction and completion.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11092.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63e367d3fae035bdc4c347fc",
      "avatarUrl": "/avatars/239da0c9287bf965348aff4890e94722.svg",
      "fullname": "Jiaxin Huang",
      "name": "JaceyH919",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.11536",
      "authors": [
        {
          "_id": "6800cc7159e20f50cc282e87",
          "name": "Jiazhan Feng",
          "hidden": false
        },
        {
          "_id": "6800cc7159e20f50cc282e88",
          "name": "Shijue Huang",
          "hidden": false
        },
        {
          "_id": "6800cc7159e20f50cc282e89",
          "name": "Xingwei Qu",
          "hidden": false
        },
        {
          "_id": "6800cc7159e20f50cc282e8a",
          "name": "Ge Zhang",
          "hidden": false
        },
        {
          "_id": "6800cc7159e20f50cc282e8b",
          "name": "Yujia Qin",
          "hidden": false
        },
        {
          "_id": "6800cc7159e20f50cc282e8c",
          "name": "Baoquan Zhong",
          "hidden": false
        },
        {
          "_id": "6800cc7159e20f50cc282e8d",
          "name": "Chengquan Jiang",
          "hidden": false
        },
        {
          "_id": "6800cc7159e20f50cc282e8e",
          "name": "Jinxin Chi",
          "hidden": false
        },
        {
          "_id": "6800cc7159e20f50cc282e8f",
          "name": "Wanjun Zhong",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-15T18:10:22.000Z",
      "submittedOnDailyAt": "2025-04-17T08:10:39.383Z",
      "title": "ReTool: 戦略的なツール使用のための強化学習",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Reasoningモデル（例：DeepSeek R1）は、強化学習（RL）を用いて訓練されたもので、自然言語の理由論理に優れていますが、構造化された問題解決に適していません。例えば、幾何的理由論理、簡潔な計算、複雑な方程式解法のような場合では、計算ツールのようにコードインタプレーター（CI）が優れています。この隙間を埋めるために、ReToolを提案します。ReToolは、ツールを統合した学習を用いた長文理由論理を強化し、2つの主な機能を持ちます。1. 自然言語の理由論理プロセス内での実時間のコード実行の動的な間離し、2. 自動化されたRLパラダイムで、多ターンの実時間のコード実行を含むポリシーロールアウトを許可し、モデルがツールをどのように呼び出すかを学習させる。ReToolは、合成的な冷やめデータの生成を始め、コード追加の長文理由論理トレースを生成し、基盤モデルの微調節を行います。次に、RL訓練は、タスクの結果を報酬として、モデルのツール使用戦略を連続的に改善し、人間の事前知識を除き、最適なツール呼び出しパターンを自動的に発見することができます。厳しいMATHオリンピックベンチマークのAIMEでの実験により、ReToolの優れた性能が示されます：私たちの32Bモデルは400トレーニングステップで67%の精度を達成し、テキストベースのRLベースライン（40%精度、1080ステップ）を優れて、効率と性能にも優れています。特に、ReTool-32Bは拡張された設定で72.5%の精度を達成し、OpenAIのo1-previewを27.9%超えます。進む分析は、コード自動調整のような発見された行動を示し、モデルが自動的に適応的なツール使用を習得するような「あっ」モーメントを意味します。これらの発見は、結果に基づくツール統合の可能性を示し、複雑な数学的理由論理の進歩について新しい視点を提供します。",
      "upvotes": 0,
      "discussionId": "6800cc7359e20f50cc282f43",
      "ai_keywords": [
        "reinforcement learning",
        "dynamic interleaving",
        "real-time code execution",
        "natural language reasoning processes",
        "automated RL paradigm",
        "policy rollouts",
        "multi-turn real-time code execution",
        "synthetic cold-start data generation",
        "code-augmented long-form reasoning traces",
        "fine-tuning",
        "RL training",
        "task outcomes as rewards",
        "autonomous discovery",
        "optimal tool invocation patterns",
        "MATH Olympiad benchmark",
        "AIME",
        "accuracy",
        "training steps",
        "OpenAI's o1-preview",
        "code self-correction",
        "adaptive tool use",
        "hybrid neuro-symbolic systems"
      ]
    },
    "publishedAt": "2025-04-15T14:10:22.000Z",
    "title": "ReTool: Reinforcement Learning for Strategic Tool Use in LLMs",
    "summary": "While reasoning models (e.g., DeepSeek R1) trained with reinforcement\nlearning (RL), excel in textual reasoning, they struggle in scenarios requiring\nstructured problem-solving, such as geometric reasoning, concise computation,\nor complex equation solving-areas where computational tools like code\ninterpreters (CI) demonstrate distinct advantages. To bridge this gap, we\npropose ReTool, which enhances long-form reasoning with tool-integrated\nlearning, including two key features: (1) dynamic interleaving of real-time\ncode execution within natural language reasoning processes, and (2) an\nautomated RL paradigm that allows policy rollouts with multi-turn real-time\ncode execution and teaches the model in learning when and how to invoke tools\nbased on outcome feedback. ReTool employs a systematic training framework,\nbeginning with synthetic cold-start data generation to produce code-augmented\nlong-form reasoning traces for fine-tuning base models. Subsequent RL training\nleverages task outcomes as rewards to iteratively refine the model's tool use\nstrategy, enabling autonomous discovery of optimal tool invocation patterns\nwithout human priors. Experiments on the challenging MATH Olympiad benchmark\nAIME demonstrate ReTool's superiority: Our 32B model achieves 67% accuracy with\n400 training steps, outperforming text-based RL baseline (40% accuracy, 1080\nsteps) in efficiency and performance. Remarkably, ReTool-32B attains 72.5%\naccuracy in extended settings, surpassing OpenAI's o1-preview by 27.9%. Further\nanalysis reveals emergent behaviors such as code self-correction, signaling an\n''aha moment'' in which the model autonomously masters adaptive tool use. These\nfindings highlight the promise of outcome-driven tool integration for advancing\ncomplex mathematical reasoning and offer new insights into hybrid\nneuro-symbolic systems.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11536.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6669
    },
    "isAuthorParticipating": false
  }
]