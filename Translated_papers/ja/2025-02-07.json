[
  {
    "paper": {
      "id": "2502.03032",
      "authors": [
        {
          "_id": "67a59c4e7ffacd843a56404a",
          "user": {
            "_id": "634c5f8cfb80cc6bcaf42c03",
            "avatarUrl": "/avatars/1f37db0e70cbaf9707f4c8cbcee37ca0.svg",
            "isPro": false,
            "fullname": "Daniil Laptev",
            "user": "dlaptev",
            "type": "user"
          },
          "name": "Daniil Laptev",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-07T09:58:04.546Z",
          "hidden": false
        },
        {
          "_id": "67a59c4e7ffacd843a56404b",
          "user": {
            "_id": "60b364e7f88532cd79eaff7b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654185363389-60b364e7f88532cd79eaff7b.jpeg",
            "isPro": false,
            "fullname": "Nikita Balagansky",
            "user": "elephantmipt",
            "type": "user"
          },
          "name": "Nikita Balagansky",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-07T09:58:02.693Z",
          "hidden": false
        },
        {
          "_id": "67a59c4e7ffacd843a56404c",
          "name": "Yaroslav Aksenov",
          "hidden": false
        },
        {
          "_id": "67a59c4e7ffacd843a56404d",
          "user": {
            "_id": "62a9c8edc19f92ae443ab37f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669110208492-62a9c8edc19f92ae443ab37f.png",
            "isPro": false,
            "fullname": "Daniil Gavrilov",
            "user": "kefirski",
            "type": "user"
          },
          "name": "Daniil Gavrilov",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-07T09:58:06.718Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-05T09:39:34.000Z",
      "title": "特徴フローの分析を通じて、言語モデルの解釈とマニュアルを向上させる",
      "summary": "新しいアプローチを介して、大規模な言語モデルの連続した層における、エスパース自動エンコーダーによって発見された特徴量をシステマティックにマッピングします。先行研究では、層間の特徴量リンクを検討したものと続き、データ無しのコサイン類似度手法を用いて、特定の特徴量がどのように持続し、変形し、または最初に出現するかを跡跡追います。この方法は、特徴量の進化を詳細に表現するグラフを生成し、モデルの計算における機構的なインサイトを提供します。重要なことに、これらの層間の特徴量マップは、選択した特徴量を増強したり抑えたりすることで、モデルの行動を直接制御することを示し、文章生成における目標的な主題制御を実現します。これらの発見は、因果的な、層間の説明性の枠組みの有用性を明らかにし、特徴量が順伝播の過程で開発されることを明確にし、大規模な言語モデルの透明な操作に新たな手段を提供します。",
      "upvotes": 34,
      "discussionId": "67a59c4f7ffacd843a56408f"
    },
    "publishedAt": "2025-02-07T01:29:53.798Z",
    "title": "Analyze Feature Flow to Enhance Interpretation and Steering in Language Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.03032.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62a9c8edc19f92ae443ab37f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669110208492-62a9c8edc19f92ae443ab37f.png",
      "fullname": "Daniil Gavrilov",
      "name": "kefirski",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.04153",
      "authors": [
        {
          "_id": "67a57b1fdea89ffe80d9fe56",
          "user": {
            "_id": "66c89152d33e34fbc29497d7",
            "avatarUrl": "/avatars/bbddabf6532393951c4759e5915a065b.svg",
            "isPro": false,
            "fullname": "KaikaiAn",
            "user": "kkk-an",
            "type": "user"
          },
          "name": "Kaikai An",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-07T09:58:18.320Z",
          "hidden": false
        },
        {
          "_id": "67a57b1fdea89ffe80d9fe57",
          "name": "Li Sheng",
          "hidden": false
        },
        {
          "_id": "67a57b1fdea89ffe80d9fe58",
          "name": "Ganqu Cui",
          "hidden": false
        },
        {
          "_id": "67a57b1fdea89ffe80d9fe59",
          "user": {
            "_id": "637c99bbfe115289cfedfb44",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637c99bbfe115289cfedfb44/344NN9KKF_XXTlVYaGaMW.png",
            "isPro": false,
            "fullname": "ssz",
            "user": "ssz1111",
            "type": "user"
          },
          "name": "Shuzheng Si",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-07T09:58:16.229Z",
          "hidden": false
        },
        {
          "_id": "67a57b1fdea89ffe80d9fe5a",
          "name": "Ning Ding",
          "hidden": false
        },
        {
          "_id": "67a57b1fdea89ffe80d9fe5b",
          "name": "Yu Cheng",
          "hidden": false
        },
        {
          "_id": "67a57b1fdea89ffe80d9fe5c",
          "name": "Baobao Chang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-06T15:39:16.000Z",
      "title": "UltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める\n\nUltraIF: 野生の指令追踪を進める",
      "summary": "インストラクション従いがモダンな大規模な言語モデル（LLMs）を役立つ助手にしています。しかし、複雑なインストラクションでLLMsを制御するための鍵は秘密で、オープンソースコミュニティとリーディング企業が訓練したモデルの間には大きな隙間があります。この隙間を埋めるために、私たちは、オープンソースデータを用いて複雑なインストラクションを従うLLMsを構築するための簡単かつスケーラブルなアプローチ「UltraIF」を提案します。UltraIFは、まず、実世界的なユーザーのプロンプトを簡単なクエリ、制約、そして制約に対応する評価クエスチョンに分解します。次に、UltraComposerを訓練し、制約に関連付けられたプロンプトと評価クエスチョンを構成します。このプロンプト構成器は、複雑なインストラクションを合成し、評価クエスチョンを用いてレスポンスをフィルタリングできます。私たちの実験では、まずましくは、5つのインストラクション従いベンチマークでLLaMA-3.1-8B-Baseをそのインストラクションバージョンに追いつけることができました。これは、ベンチマーク情報を使用しないものの、8Bモデルをそのレスポンスジェネレーターと評価者として使用しました。連携されたモデルは、他のベンチマークでも相競合的なスコアを達成しました。また、UltraIFは、自動的な連携を通じてLLaMA-3.1-8B-Instructを進化させることができることを示し、この方法の広範囲的な使用場面を促進します。私たちのコードは、https://github.com/kkk-an/UltraIF に公開されます。",
      "upvotes": 13,
      "discussionId": "67a57b1fdea89ffe80d9fe93"
    },
    "publishedAt": "2025-02-06T22:27:51.425Z",
    "title": "UltraIF: Advancing Instruction Following from the Wild",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04153.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66c89152d33e34fbc29497d7",
      "avatarUrl": "/avatars/bbddabf6532393951c4759e5915a065b.svg",
      "fullname": "KaikaiAn",
      "name": "kkk-an",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.04328",
      "authors": [
        {
          "_id": "67a586fad177de2eeba7de7b",
          "user": {
            "_id": "64f001bfabd9fb1914398bd5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f001bfabd9fb1914398bd5/9teH82hkBI4csIz_WQh5q.jpeg",
            "isPro": false,
            "fullname": "liuzuyan",
            "user": "Zuyan",
            "type": "user"
          },
          "name": "Zuyan Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-07T09:58:10.679Z",
          "hidden": false
        },
        {
          "_id": "67a586fad177de2eeba7de7c",
          "name": "Yuhao Dong",
          "hidden": false
        },
        {
          "_id": "67a586fad177de2eeba7de7d",
          "name": "Jiahui Wang",
          "hidden": false
        },
        {
          "_id": "67a586fad177de2eeba7de7e",
          "name": "Ziwei Liu",
          "hidden": false
        },
        {
          "_id": "67a586fad177de2eeba7de7f",
          "name": "Winston Hu",
          "hidden": false
        },
        {
          "_id": "67a586fad177de2eeba7de80",
          "name": "Jiwen Lu",
          "hidden": false
        },
        {
          "_id": "67a586fad177de2eeba7de81",
          "name": "Yongming Rao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-06T18:59:55.000Z",
      "title": "オーラ: 進歩的なモディバリティアラインメントを用いた全方位言語モデルの境界を突き抜ける",
      "summary": "最近、大語言モデルの進展、特にGPT-4oを後続して、多様性の理解能力を持つモデルの開発に対して興味が高まりました。その一方、性能的な進歩は専門的な単一モデルに比べて明らかに遅れています。本論文では、OlaというOmni-modal語言モデルを紹介します。Olaは、画像、映像、音声の理解に対して専門的なモデルと同等の性能を達成します。Olaの核心設計は、進歩的なモデルのモデルアライメント戦略です。この戦略では、画像と本文を最も違ったモデルから始め、語言と音声の知識を結びつける語り口語データ、または全モデルを結びつける映像データを使用してモデルのスキルセットを進歩的に拡大します。この進歩的な学習パイプラインは、クロスモデルアライメントデータのサイズを維持し、現有の視覚語言モデルからOmni-modalモデルの開発を容易かつ費用軽く行うことを可能にします。また、GPT-4oのような高度なインタラクティブな体験を達成するために、Olaは流れ語生成の文毎の解像戦略を進めます。拡散的な実験は、すべてのモデルにおいて現在の開放モデルを超え、同じサイズの最先端の専門的なモデルと同等の高度な性能を達成します。Olaは、この新興分野の将来的研究を進めるために完全に開放したOmni-modal理解ソリューションとして目標を設定しています。モデルの重み、コード、データはhttps://github.com/Ola-Omni/Olaで開放されています。",
      "upvotes": 8,
      "discussionId": "67a586fbd177de2eeba7deae"
    },
    "publishedAt": "2025-02-07T00:54:43.254Z",
    "title": "Ola: Pushing the Frontiers of Omni-Modal Language Model with Progressive Modality Alignment",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04328.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64f001bfabd9fb1914398bd5",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f001bfabd9fb1914398bd5/9teH82hkBI4csIz_WQh5q.jpeg",
      "fullname": "liuzuyan",
      "name": "Zuyan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.03621",
      "authors": [
        {
          "_id": "67a59e5298f41a0460ee5282",
          "name": "Danah Yatim",
          "hidden": false
        },
        {
          "_id": "67a59e5298f41a0460ee5283",
          "name": "Rafail Fridman",
          "hidden": false
        },
        {
          "_id": "67a59e5298f41a0460ee5284",
          "name": "Omer Bar-Tal",
          "hidden": false
        },
        {
          "_id": "67a59e5298f41a0460ee5285",
          "name": "Tali Dekel",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-05T21:14:55.000Z",
      "title": "DynVFX: 動的コンテンツを追加した実写映像",
      "summary": "私たちは、新しく生成された動的な内容を実世界のビデオに追加する方法を提案します。入力ビデオと、望む内容を説明する簡単なユーザー提供テキスト指示が与えられた場合、我々の方法は時間にわたって自然に既存のスペースと相互作用する動的なオブジェクトまたは複雑なスペース効果を合成します。新しい内容の位置、外観、動きはカメラの動き、遮蔽、スペース内の他の動的なオブジェクトとの相互作用を考慮して、元の映像に無間なように統合されます。これにより、一貫した仮想したリアルな出力ビデオが得られます。これは、事前学習されたテキストからビデオへの拡散変換器と事前学習された視覚言語モデルを利用したゼロショット、訓練無しのフレームワークで実現されます。特に、我々は、新しい内容の正確な位置付けと無間な統合を可能にするために、注意機構内の特徴量を操作する新しい推論ベースの方法を導入します。この方法は完全に自動化されていますが、シンプルなユーザー指示しか必要ありません。私たちは、実世界的なビデオに対して様々な編集を適用した場合のこの方法の効果を示します。これは、カメラおよびオブジェクトの動きによる多様なオブジェクトとスケーナーによる異なるシナリオを含むものです。",
      "upvotes": 8,
      "discussionId": "67a59e5798f41a0460ee5389"
    },
    "publishedAt": "2025-02-07T00:48:49.217Z",
    "title": "DynVFX: Augmenting Real Videos with Dynamic Content",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.03621.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6181c72cdcc1df2c9de8a4d8",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1655248010394-6181c72cdcc1df2c9de8a4d8.jpeg",
      "fullname": "Hila Chefer",
      "name": "Hila",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.02358",
      "authors": [
        {
          "_id": "67a43546f6caedc30f9d8c71",
          "user": {
            "_id": "659faf1d874e583fed79d09b",
            "avatarUrl": "/avatars/178a18686426908b9496ce71f6550655.svg",
            "isPro": false,
            "fullname": "Ziyan Guo",
            "user": "ZiyanGuo",
            "type": "user"
          },
          "name": "Ziyan Guo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-06T14:15:01.599Z",
          "hidden": false
        },
        {
          "_id": "67a43546f6caedc30f9d8c72",
          "name": "Zeyu Hu",
          "hidden": false
        },
        {
          "_id": "67a43546f6caedc30f9d8c73",
          "name": "Na Zhao",
          "hidden": false
        },
        {
          "_id": "67a43546f6caedc30f9d8c74",
          "name": "De Wen Soh",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-04T14:43:26.000Z",
      "title": "MotionLab: 動き条件動きパラダイムによる統一的な人間動き生成と編集",
      "summary": "人間の動き生成と編集は、コンピューターグラフィックと視覚の重要な構成要素です。しかし、この分野での現在のアプローチは、特定のタスクにタイドマップされた孤立した解決策を提供していますが、実世界的なアプリケーションにはよく適用できないことがあります。一方、一部の努力は、動き関連のタスクを統合することを目指していますが、これらの方法は単に異なるモデライズを条件として動き生成をガイドするだけです。その結果、編集機能、細かい制御が欠けて、タスク間の知識共有を促進しないことがあります。これらの制限を克服し、人間の動き生成と編集を両方扱う機能的で統一的なフレームワークを提供することを目的として、モーション条件モーションという新しいパラダイムを提案します。このパラダイムに基づいて、モーションラボを提案します。モーションラボは、源モーションからターゲットモーションへのマッピングを学習するために、指定された条件によってガイドされる正規化フローを組み込みます。モーションラボでは、1) モーションフローチャネラーセッターを導入し、タスク専用モジュールを除く条件下で生成と編集を強化することを、2) アラインドンローションポジションエンコーディングを用いて、源モーションとターゲットモーションの時間同期を保証することを、3) タスク指定インストラクションモデレーションを、4) モーションカレンカルラーニングを用いて、効果的な多タスク学習とタスク間の知識共有を促進することを実現します。特に、我々のモーションラボは、人間の動きに関する複数のベンチマークでの有望な一般化能力と推論効率を示しています。我々のコードと追加のビデオ結果は以下のURLから利用できます：https://diouo.github.io/motionlab.github.io/。",
      "upvotes": 8,
      "discussionId": "67a43547f6caedc30f9d8c9b"
    },
    "publishedAt": "2025-02-06T23:38:19.926Z",
    "title": "MotionLab: Unified Human Motion Generation and Editing via the Motion-Condition-Motion Paradigm",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.02358.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "659faf1d874e583fed79d09b",
      "avatarUrl": "/avatars/178a18686426908b9496ce71f6550655.svg",
      "fullname": "Ziyan Guo",
      "name": "ZiyanGuo",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.04313",
      "authors": [
        {
          "_id": "67a5b9107897c8f5406155e0",
          "name": "Shashwat Goel",
          "hidden": false
        },
        {
          "_id": "67a5b9107897c8f5406155e1",
          "name": "Joschka Struber",
          "hidden": false
        },
        {
          "_id": "67a5b9107897c8f5406155e2",
          "name": "Ilze Amanda Auzina",
          "hidden": false
        },
        {
          "_id": "67a5b9107897c8f5406155e3",
          "name": "Karuna K Chandra",
          "hidden": false
        },
        {
          "_id": "67a5b9107897c8f5406155e4",
          "name": "Ponnurangam Kumaraguru",
          "hidden": false
        },
        {
          "_id": "67a5b9107897c8f5406155e5",
          "name": "Douwe Kiela",
          "hidden": false
        },
        {
          "_id": "67a5b9107897c8f5406155e6",
          "name": "Ameya Prabhu",
          "hidden": false
        },
        {
          "_id": "67a5b9107897c8f5406155e7",
          "name": "Matthias Bethge",
          "hidden": false
        },
        {
          "_id": "67a5b9107897c8f5406155e8",
          "name": "Jonas Geiping",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-06T18:56:01.000Z",
      "title": "グレートモデルは同じように思い、これはAIの視聴を軽減します。",
      "summary": "LMの能力が進化するにつれて、スケール上での評価と監督を人間が行うことは難しくなってきました。他のLMがこれらの仕事を自動化することが望ましいことになり、これを「AI 監視」と呼びます。LMの類似性がAI監視の両方の面でどのように影響を及ぼすかを調査するために、モデル誤りの重複に基づくLMの類似性の確率計量を提案しました。この計量を用いて、まずLLM-as-a-judgeスコアが判官に類似したモデルに優しくなることを示し、最近の自動好み結果を一般化しています。次に、LMの注釈を用いた訓練を調査し、弱い監督者と強い学生モデルの補間知識が「弱いから強い一般化」の効果に重要な役割を果たしていることを見出しました。モデルの能力が増加するにつれて、モデルの誤りを見つけることが難しくなり、AI監視による依頼が増加することが予想されます。しかし、懸念の趨勢が見出されています。モデルの誤りが能力が増加するにつれてより類似になり、相関関係の失敗からのリスクがあることを示しています。本論文では、AI監視の新しいパラダイムでモデルの類似性を報告し、補正する重要性を強調しています。",
      "upvotes": 7,
      "discussionId": "67a5b9137897c8f540615673"
    },
    "publishedAt": "2025-02-07T02:46:29.675Z",
    "title": "Great Models Think Alike and this Undermines AI Oversight",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6506832221ac448013f94995/pXBCc2dpWXCw6JinTbiFP.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04313.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6506832221ac448013f94995",
      "avatarUrl": "/avatars/0a86f64cb502a04ab1487d78f63bf3fd.svg",
      "fullname": "Shashwat Goel",
      "name": "shash42",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.03544",
      "authors": [
        {
          "_id": "67a589ebb16fabcdd2dea1eb",
          "name": "Yuri Chervonyi",
          "hidden": false
        },
        {
          "_id": "67a589ebb16fabcdd2dea1ec",
          "name": "Trieu H. Trinh",
          "hidden": false
        },
        {
          "_id": "67a589ebb16fabcdd2dea1ed",
          "name": "Miroslav Olšák",
          "hidden": false
        },
        {
          "_id": "67a589ebb16fabcdd2dea1ee",
          "name": "Xiaomeng Yang",
          "hidden": false
        },
        {
          "_id": "67a589ebb16fabcdd2dea1ef",
          "name": "Hoang Nguyen",
          "hidden": false
        },
        {
          "_id": "67a589ebb16fabcdd2dea1f0",
          "name": "Marcelo Menegali",
          "hidden": false
        },
        {
          "_id": "67a589ebb16fabcdd2dea1f1",
          "name": "Junehyuk Jung",
          "hidden": false
        },
        {
          "_id": "67a589ebb16fabcdd2dea1f2",
          "name": "Vikas Verma",
          "hidden": false
        },
        {
          "_id": "67a589ebb16fabcdd2dea1f3",
          "name": "Quoc V. Le",
          "hidden": false
        },
        {
          "_id": "67a589ebb16fabcdd2dea1f4",
          "name": "Thang Luong",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-05T19:02:03.000Z",
      "title": "オリンピック問題の幾何学を解くためのAlphaGeometry2でのゴールドメダリストの実績",
      "summary": "AlphaGeometry2 は、Trinh et al. (2024) によって紹介された AlphaGeometry の大幅に改善されたバージョンです。これは、オリンピックジョイミーティージャー問題を解くことで平均金メダリストを超えたことを示しています。まず、オリンピックジョイミーティージャー問題を解くことを目的としたより難しい問題に対応するため、AlphaGeometry の言語を拡張しました。これにより、角度の線形方程式、比率、距離を含む問題も対応できました。これらの拡張と他の追加機能により、AlphaGeometry の言語が国際数学オリンピック（IMO）2000-2024 の問題に対するカバー率は、66% から 88% に上がりました。また、Gemini アーキテクチャを用いた言語モデリングの改善と新しい知識共有機制（複数の探索木を組み合わせたもの）の導入により、AlphaGeometry2 の探索プロセスも大幅に向上しました。さらに、符号演算エンジンと合成データ生成の改善により、AlphaGeometry2 の全体の解答率は、25年間のすべての問題に対して 84% に上がりました（以前は 54%）。AlphaGeometry2 は、IMO 2024 で銀メダルの標準を達成したシステムの一環でした（https://dpmd.ai/imo-silver）。最後に、AlphaGeometry2 を自然言語入力から直接問題を解くための完全自動化システムの一部として使用することに向けての進展状況を報告しています。",
      "upvotes": 7,
      "discussionId": "67a589ecb16fabcdd2dea259"
    },
    "publishedAt": "2025-02-06T23:20:09.641Z",
    "title": "Gold-medalist Performance in Solving Olympiad Geometry with AlphaGeometry2",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.03544.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5968
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.04306",
      "authors": [
        {
          "_id": "67a57f334e50b2956b13f4e0",
          "user": {
            "_id": "6730dc8df84c8aac97451e57",
            "avatarUrl": "/avatars/4f2cf5363b17744daca41d2a18ddfeb8.svg",
            "isPro": false,
            "fullname": "Yinjie Wang",
            "user": "yinjiewang",
            "type": "user"
          },
          "name": "Yinjie Wang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-02-07T03:34:13.176Z",
          "hidden": false
        },
        {
          "_id": "67a57f334e50b2956b13f4e1",
          "name": "Ling Yang",
          "hidden": false
        },
        {
          "_id": "67a57f334e50b2956b13f4e2",
          "name": "Guohao Li",
          "hidden": false
        },
        {
          "_id": "67a57f334e50b2956b13f4e3",
          "name": "Mengdi Wang",
          "hidden": false
        },
        {
          "_id": "67a57f334e50b2956b13f4e4",
          "name": "Bryon Aragam",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-06T18:47:49.000Z",
      "title": "スコアフロー：スコアベースの好み最適化をもとにLLMアガントワークフローを掌握する",
      "summary": "最近の研究は、複雑な問題解決に向けて、手動でのモデルの構築に必要な努力を削減するために、大規模言語モデル多アガントシステムを活用しています。これにより、自動化されたアガントワークフロー最適化手法の開発が進められています。しかし、現在の手法は、表現の制限、適応性の欠如、離散最適化手法のようなスケーラビリティの低さにより、柔軟性が不足しています。ScoreFlowという簡単で高性能なフレームワークを用いて、これらの課題を解決しています。ScoreFlowは、連続空間での効率的な勾配基礎最適化を活用しています。ScoreFlowは、定量的なフィードバックを考慮した新しい直接好み最適化手法の変体であるScore-DPOを採用しています。6つのベンチマークを通じて、ScoreFlowは既存のベースラインより8.2%の改善を収めました。また、それらの小さなモデルは、低い推論コストで大きなモデルを超えることができます。プロジェクト：https://github.com/Gen-Verse/ScoreFlow",
      "upvotes": 7,
      "discussionId": "67a57f354e50b2956b13f53d"
    },
    "publishedAt": "2025-02-06T22:34:42.483Z",
    "title": "ScoreFlow: Mastering LLM Agent Workflows via Score-based Preference Optimization",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04306.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64fde4e252e82dd432b74ce9",
      "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
      "fullname": "Ling Yang",
      "name": "Lingaaaaaaa",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.04128",
      "authors": [
        {
          "_id": "67a5894db16fabcdd2de5459",
          "user": {
            "_id": "645f172d7c6bff8577353d1a",
            "avatarUrl": "/avatars/a83682e1343809257b082b78d58c582a.svg",
            "isPro": false,
            "fullname": "ZhenYE",
            "user": "ZhenYe234",
            "type": "user"
          },
          "name": "Zhen Ye",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-07T09:58:08.787Z",
          "hidden": false
        },
        {
          "_id": "67a5894db16fabcdd2de545a",
          "name": "Xinfa Zhu",
          "hidden": false
        },
        {
          "_id": "67a5894db16fabcdd2de545b",
          "name": "Chi-Min Chan",
          "hidden": false
        },
        {
          "_id": "67a5894db16fabcdd2de545c",
          "name": "Xinsheng Wang",
          "hidden": false
        },
        {
          "_id": "67a5894db16fabcdd2de545d",
          "name": "Xu Tan",
          "hidden": false
        },
        {
          "_id": "67a5894db16fabcdd2de545e",
          "name": "Jiahe Lei",
          "hidden": false
        },
        {
          "_id": "67a5894db16fabcdd2de545f",
          "name": "Yi Peng",
          "hidden": false
        },
        {
          "_id": "67a5894db16fabcdd2de5460",
          "name": "Haohe Liu",
          "hidden": false
        },
        {
          "_id": "67a5894db16fabcdd2de5461",
          "name": "Yizhu Jin",
          "hidden": false
        },
        {
          "_id": "67a5894db16fabcdd2de5462",
          "name": "Zheqi DAI",
          "hidden": false
        },
        {
          "_id": "67a5894db16fabcdd2de5463",
          "name": "Hongzhan Lin",
          "hidden": false
        },
        {
          "_id": "67a5894db16fabcdd2de5464",
          "name": "Jianyi Chen",
          "hidden": false
        },
        {
          "_id": "67a5894db16fabcdd2de5465",
          "name": "Xingjian Du",
          "hidden": false
        },
        {
          "_id": "67a5894db16fabcdd2de5466",
          "name": "Liumeng Xue",
          "hidden": false
        },
        {
          "_id": "67a5894db16fabcdd2de5467",
          "name": "Yunlin Chen",
          "hidden": false
        },
        {
          "_id": "67a5894db16fabcdd2de5468",
          "name": "Zhifei Li",
          "hidden": false
        },
        {
          "_id": "67a5894db16fabcdd2de5469",
          "name": "Lei Xie",
          "hidden": false
        },
        {
          "_id": "67a5894db16fabcdd2de546a",
          "name": "Qiuqiang Kong",
          "hidden": false
        },
        {
          "_id": "67a5894db16fabcdd2de546b",
          "name": "Yike Guo",
          "hidden": false
        },
        {
          "_id": "67a5894db16fabcdd2de546c",
          "user": {
            "_id": "6628adb14277eae0da5eee28",
            "avatarUrl": "/avatars/6cb41b80cc5e014e455dfc2a22682e64.svg",
            "isPro": true,
            "fullname": "HKUST Audio",
            "user": "HKUST-Audio",
            "type": "user"
          },
          "name": "Wei Xue",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-02-07T04:17:17.888Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-06T15:04:00.000Z",
      "title": "ラザーナ：ラマーベースの音声合成に対してのトレーニング時と推論時の計算機能の拡大",
      "summary": "最近の文書基盤の大規模言語モデル（LLMs）の進展、特にGPTシリーズとo1モデルにおける進展は、訓練時と推論時の計算量のスケーリングの効果性を示しています。しかし、現在の最先端のLLMsを利用したTTSシステムは、多ステップで構成されており、別のモデル（例：LLM後のディフュージョンモデル）を必要とすることが多いため、訓練時やテスト時に特定のモデルをスケーリングするか否かの判断が複雑になっています。本研究では、次の貢献を行います：最初に、訓練時と推論時の計算量のスケーリングを語調合成に適用して調査します。第二に、簡単なフレームワークLlasaを提案し、一つのレイヤーのベクトル定量化（VQ）コーデックと一つのTransformerアーキテクチャを使用して、標準的なLLMs（例：Llama）と完全に一致することを目指しています。実験により、Llasaの訓練時の計算量のスケーリングは合成された語調の自然性を一貫して向上させ、複雑なおよび正確な語調パターンの生成を可能にしています。また、推論時の計算量のスケーリングの観点から、語調理解モデルを検定器として利用して検索を行い、推論時の計算量のスケーリングが特定の検定器の好みに向かうサンプリングモードへと変わり、感情表現力、音色の一貫性、および内容の正確性を向上させることを見出しました。また、我々のTTSモデル（1B、3B、8B）とコーデックモデルのチェックポイントと訓練コードを公開的に提供しています。",
      "upvotes": 5,
      "discussionId": "67a5894db16fabcdd2de54d3"
    },
    "publishedAt": "2025-02-06T23:17:40.725Z",
    "title": "Llasa: Scaling Train-Time and Inference-Time Compute for Llama-based Speech Synthesis",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04128.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5968
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.04299",
      "authors": [
        {
          "_id": "67a591234020a3bfdb8cb2e5",
          "name": "Jinbo Xing",
          "hidden": false
        },
        {
          "_id": "67a591234020a3bfdb8cb2e6",
          "name": "Long Mai",
          "hidden": false
        },
        {
          "_id": "67a591234020a3bfdb8cb2e7",
          "name": "Cusuh Ham",
          "hidden": false
        },
        {
          "_id": "67a591234020a3bfdb8cb2e8",
          "name": "Jiahui Huang",
          "hidden": false
        },
        {
          "_id": "67a591234020a3bfdb8cb2e9",
          "name": "Aniruddha Mahapatra",
          "hidden": false
        },
        {
          "_id": "67a591234020a3bfdb8cb2ea",
          "name": "Chi-Wing Fu",
          "hidden": false
        },
        {
          "_id": "67a591234020a3bfdb8cb2eb",
          "name": "Tien-Tsin Wong",
          "hidden": false
        },
        {
          "_id": "67a591234020a3bfdb8cb2ec",
          "name": "Feng Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-06T18:41:04.000Z",
      "title": "MotionCanvas: 映画的ショットデザインと制御可能な画像から動画への生成",
      "summary": "この論文では、画像から動画を生成するシステムのコンテキストでビデオショットを設計する方法を提案します。ショットデザインは映画製作の重要な面で、シーン内のカメラの移動とオブジェクトの動きを細かく計画することが含まれます。しかし、現代の画像から動画を生成するシステムで直感的なショットデザインを可能にするには、2つの主な課題があります：1. ユーザーの意図を適切に捉えるために、カメラの移動とシーンスペースのオブジェクトの動きを共に指定することが必要です。2. 動画ディフュージョンモデルが合成する画像アニメーションに役立つ動き情報を表現することができるようにすることが必要です。これらの課題に対処するために、モーションキャンバス（MotionCanvas）という方法を提案します。この方法では、画像から動画（I2V）を生成するモデルにユーザー駆動の制御を統合し、シーンによるオブジェクトとカメラの動きを制御できるようにします。古典的なコンピューターグラフィックと現代の動画生成技術の見解を統合し、I2V合成で3Dに関する費用の高いトレーニングデータを必要とされないように3D関連の動き制御を実現することを示します。モーションキャンバスでは、ユーザーが直感的にシーンスペースの動きを表現し、それを動画ディフュージョンモデルに利用できる空間時間的な動き条件シグナルに翻訳することができます。実際の画像内容とショットデザインの様々なシナリオで、我々の方法の効果性を示し、デジタルコンテンツ作成の創造的なワークフローを高めることや、画像と動画編集アプリケーションに適用できることを示します。",
      "upvotes": 3,
      "discussionId": "67a5912b4020a3bfdb8cb4d5"
    },
    "publishedAt": "2025-02-06T23:50:54.836Z",
    "title": "MotionCanvas: Cinematic Shot Design with Controllable Image-to-Video Generation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04299.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5968
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.03860",
      "authors": [
        {
          "_id": "67a5880c886a1e223b1d57ec",
          "name": "Bo Pang",
          "hidden": false
        },
        {
          "_id": "67a5880c886a1e223b1d57ed",
          "name": "Hanze Dong",
          "hidden": false
        },
        {
          "_id": "67a5880c886a1e223b1d57ee",
          "name": "Jiacheng Xu",
          "hidden": false
        },
        {
          "_id": "67a5880c886a1e223b1d57ef",
          "name": "Silvio Savarese",
          "hidden": false
        },
        {
          "_id": "67a5880c886a1e223b1d57f0",
          "name": "Yingbo Zhou",
          "hidden": false
        },
        {
          "_id": "67a5880c886a1e223b1d57f1",
          "name": "Caiming Xiong",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-06T08:19:59.000Z",
      "title": "BOLT: 言語モデルにおける長い思考連鎖のブーストストップでの軽減を無くしての設計",
      "summary": "大語言モデル（LLMs）の例に、OpenAIのo1が示した議論能力が驚異的である。o1は問い合わせを答える前に長い思考連鎖（LongCoT）を生成する。LongCoTはLLMsが問題を分析、計画、反省、バックトラックすることを可能にします。これらの行動がLLMsが複雑な問題を解決することを可能にします。o1のリリース後、多くのチームはLongCoTと議論能力を再現することを試みました。方法としては、現存しているLongCoT能力を持つモデル（例：OpenAI-o1、Qwen-QwQ、DeepSeek-R1-Preview）からの知識の経験を主に依存し、これらの議論能力のシステマチ的な開発における重要な不確実性が残っています。データ領域としては、数学を中心に焦点を当て、少数の場合はコーディングを含み、一般化能力を制限しています。本論文では、o1みたいなモデルからの経験または高価な人間のアノテーションを除いて、LLMsのLongCoT能力を設定する新しいアプローチを紹介します。これは、標準のインストラクツナルモデルからのLongCoTをスタートアップします。BOLTは3つのステージからなります：1）標準のインストラクツナルモデルにおけるLongCoTデータのスタートアップ；2）LongCoTのサブジェクト調整；3）オンライントレーニングでLongCoT能力を進歩的に改善する。BOLTでは、スタートアップステージでは、ただの数のインタカースエグザムを構築する必要がありますが、我々の実験では10例を作成し、このアプローチの可能性を示しました。Llama-3.1-70B-Instructを使用してLongCoTをスタートアップし、様々なモデルスケール（7B、8B、70B）にこの方法を適用しました。Arena-Hard、MT-Bench、WildBench、ZebraLogic、MATH500の多様なベンチマークで、多様なタスク解決と議論能力を評価することで、驚異的な性能を達成しました。",
      "upvotes": 3,
      "discussionId": "67a5880e886a1e223b1d58ca"
    },
    "publishedAt": "2025-02-06T23:12:15.874Z",
    "title": "BOLT: Bootstrap Long Chain-of-Thought in Language Models without Distillation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.03860.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5968
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.04295",
      "authors": [
        {
          "_id": "67a57d32bc587f5b57a3f24f",
          "name": "Yuanye Liu",
          "hidden": false
        },
        {
          "_id": "67a57d32bc587f5b57a3f250",
          "user": {
            "_id": "62abdf657b037eafffc48808",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1655430982462-noauth.jpeg",
            "isPro": false,
            "fullname": "Jiahang Xu",
            "user": "Jiahang",
            "type": "user"
          },
          "name": "Jiahang Xu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-02-07T03:25:39.760Z",
          "hidden": false
        },
        {
          "_id": "67a57d32bc587f5b57a3f251",
          "name": "Li Lyna Zhang",
          "hidden": false
        },
        {
          "_id": "67a57d32bc587f5b57a3f252",
          "name": "Qi Chen",
          "hidden": false
        },
        {
          "_id": "67a57d32bc587f5b57a3f253",
          "name": "Xuan Feng",
          "hidden": false
        },
        {
          "_id": "67a57d32bc587f5b57a3f254",
          "name": "Yang Chen",
          "hidden": false
        },
        {
          "_id": "67a57d32bc587f5b57a3f255",
          "name": "Zhongxin Guo",
          "hidden": false
        },
        {
          "_id": "67a57d32bc587f5b57a3f256",
          "name": "Yuqing Yang",
          "hidden": false
        },
        {
          "_id": "67a57d32bc587f5b57a3f257",
          "name": "Cheng Peng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-06T18:36:44.000Z",
      "title": "プロンプト内容を超えて：コンテンツフォーマットを用いたLLM性能向上の統合プロンプト最適化",
      "summary": "大型言語モデル（LLMs）は、多様なタスクで显著な能力を示し、実世界的な効果性は通常、プロンプトデザインによって駆動されています。近年の研究は、プロンプト内容の最適化に焦点を当てていますが、プロンプトのフォーマット化、重要ながっちりしてもよく見落とされる次元、については限られた系統的な調査が行われていません。本論文では、Content-Format Integrated Prompt Optimization（CFPO）という創新的なメソッドを介して、プロンプト内容とフォーマット化を同時に最適化するイテレーション的な精練プロセスを採用します。CFPOは自然言語の変異を活用して内容の変化を探索し、プロンプトのフォーマットをシステマティックに評価するための動的なフォーマット探索戦略を採用しています。多様なタスクと開放ソースLLMsにおいて広範囲で評価を行い、内容だけの最適化手法に比べて可視的な性能向上が見られます。これは、統合的な内容フォーマット最適化の重要性を強調し、LLMの性能向上に対する実用的な、モデル無依存なアプローチを提供します。コードは、https://github.com/HenryLau7/CFPO から利用可能です。",
      "upvotes": 3,
      "discussionId": "67a57d33bc587f5b57a3f29d"
    },
    "publishedAt": "2025-02-06T22:27:24.284Z",
    "title": "Beyond Prompt Content: Enhancing LLM Performance via Content-Format Integrated Prompt Optimization",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04295.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62abdf657b037eafffc48808",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1655430982462-noauth.jpeg",
      "fullname": "Jiahang Xu",
      "name": "Jiahang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.00989",
      "authors": [
        {
          "_id": "67a5c7601e6db426653ebc3d",
          "name": "Kanika Goswami",
          "hidden": false
        },
        {
          "_id": "67a5c7601e6db426653ebc3e",
          "name": "Puneet Mathur",
          "hidden": false
        },
        {
          "_id": "67a5c7601e6db426653ebc3f",
          "name": "Ryan Rossi",
          "hidden": false
        },
        {
          "_id": "67a5c7601e6db426653ebc40",
          "name": "Franck Dernoncourt",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-03T02:00:51.000Z",
      "title": "ChartCitor: 多エージェントフレームワークでの細かいチャート可視化の責任归属",
      "summary": "大語言モデル（LLMs）は、チャートにおける質問回答タスクを行うことができますが、常に証拠がないハロウィングされた回答を生成します。現在の回答証拠評価方法は、チャートの視覚語意的コンテキストが限られ、複雑な視覚文の対応要求や複雑なレイアウトにおけるボックス予測の難しさにより、チャートの証拠をもつようにしようができません。私たちは、チャート画像内の証拠を特定して、細かいボックス引用を提供するマルチアガントフレームワーク「ChartCitor」を紹介します。このシステムは、LLMアガントを協調して、チャートからテーブルの抽出、回答の再形成、テーブルの拡張、予測と再スコアリングを通じた証拠の検索、テーブルからチャートのマッピングを行います。「ChartCitor」は、異なるチャートタイプで現在のベースラインを超えることができます。質的なユーザーステージスでは、「ChartCitor」はLLMによるチャートQAの解釈性を高めることで生成AIにおけるユーザー信頼を高め、専門家の生産性を高めることができることが示されます。",
      "upvotes": 2,
      "discussionId": "67a5c7621e6db426653ebc8a"
    },
    "publishedAt": "2025-02-07T03:42:17.799Z",
    "title": "ChartCitor: Multi-Agent Framework for Fine-Grained Chart Visual Attribution",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.00989.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62c5947524171688a9feb992",
      "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
      "fullname": "Franck Dernoncourt",
      "name": "Franck-Dernoncourt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.04322",
      "authors": [
        {
          "_id": "67a5a9357415f9155e9b4b58",
          "name": "Yik Siu Chan",
          "hidden": false
        },
        {
          "_id": "67a5a9357415f9155e9b4b59",
          "user": {
            "_id": "64698ed0dcbb937d56b9dd02",
            "avatarUrl": "/avatars/835ce9bf6e2cd1d4b7a709cf41a884e2.svg",
            "isPro": false,
            "fullname": "Edward Ri",
            "user": "narutatsuri",
            "type": "user"
          },
          "name": "Narutatsu Ri",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-07T09:57:58.519Z",
          "hidden": false
        },
        {
          "_id": "67a5a9357415f9155e9b4b5a",
          "user": {
            "_id": "64bf072bae436c8813494ba3",
            "avatarUrl": "/avatars/afb96d2bbf90411f4b1a030ebebff300.svg",
            "isPro": false,
            "fullname": "Yuxin Xiao",
            "user": "YuxinXiao",
            "type": "user"
          },
          "name": "Yuxin Xiao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-07T09:58:00.910Z",
          "hidden": false
        },
        {
          "_id": "67a5a9357415f9155e9b4b5b",
          "name": "Marzyeh Ghassemi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-06T18:59:02.000Z",
      "title": "スピーク・イージー: シンプルなインタラクションでLLMから有害なジャイルブレイクを引き出す",
      "summary": "その英語のテキストを日本語に翻訳します。\n\n広範囲での安全性の調整により、大規模な言語モデル（LLMs）は、有害な行動を招き、ジャイルブレイク攻撃に脆弱である。既存の研究は、技術的知識が必要な攻撃方法に焦点を当てているが、2つの重要な問題が調査されていません：1）ジャイルブレイクのレスポンスは、平均のユーザーが有害な行動を行うことを可能にするか？2）より一般的な、簡単な人間とLLMの相互作用における安全性の脆弱性が存在するか？この論文では、LLMのレスポンスが有害な行動を最も効果的に促進するためには、行動可能で情報豊富な2つの属性が必要であることを示します。この見解を基に、有害な行動をどのように促進するかを評価するためのジャイルブレイクメトリック「HarmScore」と、簡単な多ステップ、多言語攻撃フレームワーク「Speak Easy」を提案します。特に、「Speak Easy」を直接のリクエストとジャイルブレイクの基準に追加した場合、4つの安全性ベンチマークの開源モデルと販売モデルで、攻撃成功率は平均で0.319の絶対増加、HarmScoreは0.426の絶対増加を見出します。我々の研究は、重要なければならない脆弱性を明らかにします：悪意のあるユーザーは、有害な目的に対して、一般的な相互作用パターンを容易に利用できることを示します。",
      "upvotes": 2,
      "discussionId": "67a5a9367415f9155e9b4bbb"
    },
    "publishedAt": "2025-02-07T01:37:25.953Z",
    "title": "Speak Easy: Eliciting Harmful Jailbreaks from LLMs with Simple Interactions",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04322.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64bf072bae436c8813494ba3",
      "avatarUrl": "/avatars/afb96d2bbf90411f4b1a030ebebff300.svg",
      "fullname": "Yuxin Xiao",
      "name": "YuxinXiao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.04235",
      "authors": [
        {
          "_id": "67a56af6d7c26c7497a86308",
          "user": {
            "_id": "64b764bffdb702b3d8640610",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b764bffdb702b3d8640610/lpHg0AX_NOmzw-ZxeOa1s.png",
            "isPro": false,
            "fullname": "haoxintong",
            "user": "haoxintong",
            "type": "user"
          },
          "name": "Xintong Hao",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-02-07T04:41:11.249Z",
          "hidden": false
        },
        {
          "_id": "67a56af6d7c26c7497a86309",
          "name": "Ke Shen",
          "hidden": false
        },
        {
          "_id": "67a56af6d7c26c7497a8630a",
          "name": "Chenggang Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-06T17:19:55.000Z",
      "title": "MAGA: MAジャーネージャー・アドミニスターリフォーマティブテクニックとしての予習データセットの拡張",
      "summary": "大語言モデルは、多様なタスクでの卓越した能力を示していますが、その継続的なスケーリングは、高品質の事前学習データの不足が大きな課題となっています。モデルアーキテクチャは継続的に進化していますが、自然言語データはスケーリングに遅れています。このボトルネックを解決するために、私たちはMAssive Genre-Audience~(MAGA) 改設計法を提案します。これは、現在のコーパスから多様的でコンテキスト豊富な事前学習データをシステマ的に合成する方法です。この研究は三つの主な貢献をまとめます： (1) MAGA 改設計法を提案し、事前学習コーパス拡張の軽量でスケーリング可能なアプローチを構築し、770BトークンのMAGACorpusを構築しました。 (2) MAGACorpusを異なるデータバッジスケーリング戦略で評価し、様々なモデルサイズ（134M-13B）で一致した改善を示し、次世代の大規模な合成事前学習言語モデルの必要性を明らかにしました。 (3) 詳細な分析を通じて、プロンプトエンジニアリングが合成トレーニング崩壊にどのような影響を与えるかを調査し、評価損失を用いた伝統的な崩壊検出メトリクスの制限を明らかにしました。私たちの研究は、MAGAが訓練データセットを大幅に拡張しながらも品質を維持することを示し、データの制限を超えるモデルスケーリングの信頼的なパスワードを提供します。",
      "upvotes": 1,
      "discussionId": "67a56af8d7c26c7497a86359"
    },
    "publishedAt": "2025-02-07T00:56:20.873Z",
    "title": "MAGA: MAssive Genre-Audience Reformulation to Pretraining Corpus Expansion",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04235.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b764bffdb702b3d8640610",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b764bffdb702b3d8640610/lpHg0AX_NOmzw-ZxeOa1s.png",
      "fullname": "haoxintong",
      "name": "haoxintong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.04270",
      "authors": [
        {
          "_id": "67a5882fa8e877ef10b8d1fd",
          "name": "Yunzhen Feng",
          "hidden": false
        },
        {
          "_id": "67a5882fa8e877ef10b8d1fe",
          "name": "Ariel Kwiatkowski",
          "hidden": false
        },
        {
          "_id": "67a5882fa8e877ef10b8d1ff",
          "name": "Kunhao Zheng",
          "hidden": false
        },
        {
          "_id": "67a5882fa8e877ef10b8d200",
          "name": "Julia Kempe",
          "hidden": false
        },
        {
          "_id": "67a5882fa8e877ef10b8d201",
          "name": "Yaqi Duan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-06T18:09:00.000Z",
      "title": "PILAF: 最適な人間の好みサンプリングによる報酬モデリング",
      "summary": "大語言モデルが実世界的アプリケーションを駆動することにより、人間の価値観との一致が重要な課題になります。人間の反饋からの強化学習（RLHF）が重要な技術として現れ、神経な人間の価値観がアクセス不可の場合に好みデータを報酬モデルに翻訳します。実践的には、RLHFは大きく近似した報酬モデルを基にしていますが、これらのモデルは政策をユーザーの価値観を最大化することに向けて一貫してガイドすることはできません。我々は、PILAF（Policy-Interpolated Learning for Aligned Feedback）を提案します。PILAFは、偏好ラベルの応答サンプリングスニペットで、明示的に偏好学習を人間の価値観を最大化することに一致させる新しい技術です。PILAFは理論的に立ちつくり、最適性を最適化と統計的な観点から示します。この方法は実装が簡単であり、フィードバックの編集が重要なイテレーティブおよびオンラインRLHFセットティングで強力な性能を示します。",
      "upvotes": 1,
      "discussionId": "67a58830a8e877ef10b8d226"
    },
    "publishedAt": "2025-02-06T23:13:23.158Z",
    "title": "PILAF: Optimal Human Preference Sampling for Reward Modeling",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04270.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65cbfa6c968742be942e6cba",
      "avatarUrl": "/avatars/1a6cc0983edc28fa92178d3abc283ba1.svg",
      "fullname": "Feng",
      "name": "Yunzhen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.03639",
      "authors": [
        {
          "_id": "67a59193f86e1b9d7ae7cd55",
          "name": "Yunuo Chen",
          "hidden": false
        },
        {
          "_id": "67a59193f86e1b9d7ae7cd56",
          "name": "Junli Cao",
          "hidden": false
        },
        {
          "_id": "67a59193f86e1b9d7ae7cd57",
          "name": "Anil Kag",
          "hidden": false
        },
        {
          "_id": "67a59193f86e1b9d7ae7cd58",
          "name": "Vidit Goel",
          "hidden": false
        },
        {
          "_id": "67a59193f86e1b9d7ae7cd59",
          "name": "Sergei Korolev",
          "hidden": false
        },
        {
          "_id": "67a59193f86e1b9d7ae7cd5a",
          "name": "Chenfanfu Jiang",
          "hidden": false
        },
        {
          "_id": "67a59193f86e1b9d7ae7cd5b",
          "name": "Sergey Tulyakov",
          "hidden": false
        },
        {
          "_id": "67a59193f86e1b9d7ae7cd5c",
          "name": "Jian Ren",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-05T21:49:06.000Z",
      "title": "物理的認識を目指した映像生成における3D点の正規化手法",
      "summary": "ここでは、新しいビデオ生成フレームワークを紹介します。このフレームワークは3次元ジェムトリーと動的認識を統合しています。これを実現するために、2次元ビデオに3次元点の移動軌跡を追加し、ピクセル空間でそれらを対象対して調整します。このようにして得られた3次元認識付きビデオデータセット、PointVidを用いて、潜在ディフュージョンモデルを微調節し、2次元オブジェクトを3次元カーテシアン座標で追跡することができます。これに基づき、ビデオ中のオブジェクトの形状と動作を正規化し、不適切なエリアを除去します。例えば、物理的な変形などの不適切なエリアを除去します。このようにして、生成されるRGBビデオの質を向上させ、現在のビデオモデルにおける形状認識の欠落によるオブジェクトの形の変形などの一般的な問題を解決します。我々の3次元追加と正規化により、モデルは接触豊富なシナリオを扱うことができます。これらのビデオは、3次元情報が形の理解と接触の認識に必要な固体の複雑な相互作用を含みます。さらに、モデルは動きの3次元一致性を促進し、形状と動作の突然な変化を減らし、ビデオ生成の全体の質を向上させます。",
      "upvotes": 0,
      "discussionId": "67a59195f86e1b9d7ae7cd97"
    },
    "publishedAt": "2025-02-06T23:52:49.331Z",
    "title": "Towards Physical Understanding in Video Generation: A 3D Point Regularization Approach",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.03639.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5968
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.04296",
      "authors": [
        {
          "_id": "67a57a4637e2abc28667ec1b",
          "name": "Lirui Wang",
          "hidden": false
        },
        {
          "_id": "67a57a4637e2abc28667ec1c",
          "name": "Kevin Zhao",
          "hidden": false
        },
        {
          "_id": "67a57a4637e2abc28667ec1d",
          "name": "Chaoqi Liu",
          "hidden": false
        },
        {
          "_id": "67a57a4637e2abc28667ec1e",
          "name": "Xinlei Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-06T18:38:26.000Z",
      "title": "学習ハイブリッドマスク付きのアクションビデオ動力学",
      "summary": "ホモジョニーマスクされた自動回帰（HMA）を提案します。HMAは、アクションビデオのダイナミクスをモデル化し、ロボット学習のスケーリングにおける高品質データ生成と評価を実現します。インタラクティブなビデオワールドモデルとロボット用ポリシーの構築は、多様な設定を処理しながら時間的な計算効率を維持することが難しいため、これが難しいです。HMAは、異なるロボットの構成、ドメイン、タスクにおける観測とアクションシーケンスからのヘテロジネーションプレトレーニングを使用しています。HMAは、ビデオ予測においてキャラクティズドまたはソフトトークンを生成するためにマスクされた自動回帰を使用しています。HMAは、前回のロボットビデオ生成モデルと比較してもっとも良い視覚的な忠実性と制御可能性を実現し、実世界では15倍速く動作します。ポストトレーニング後、このモデルは、低レベルアクション入力からのビデオシミュレータとして使用でき、ポリシーの評価と合成データの生成に役立ちます。より詳細な情報はこちらのリンクを参照してください: https://liruiw.github.io/hma",
      "upvotes": 0,
      "discussionId": "67a57a4737e2abc28667ec58"
    },
    "publishedAt": "2025-02-06T22:17:36.193Z",
    "title": "Learning Real-World Action-Video Dynamics with Heterogeneous Masked Autoregression",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04296.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63151385b031f7b1c7c0871c",
      "avatarUrl": "/avatars/0088eb929866face5f95218943e3f478.svg",
      "fullname": "Lirui Wang",
      "name": "liruiw",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  }
]