[
  {
    "paper": {
      "id": "2502.06394",
      "authors": [
        {
          "_id": "67aafead3711ca5b760f324c",
          "user": {
            "_id": "61ade264f602880813dbe10b",
            "avatarUrl": "/avatars/a92dea7d853bbabbf60b351c207b6875.svg",
            "isPro": false,
            "fullname": "Daniil Moskovskiy",
            "user": "etomoscow",
            "type": "user"
          },
          "name": "Daniil Moskovskiy",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T07:54:17.448Z",
          "hidden": false
        },
        {
          "_id": "67aafead3711ca5b760f324d",
          "user": {
            "_id": "634c72e6fe1bfa967d6c2b5c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/634c72e6fe1bfa967d6c2b5c/WFWIAlWl-FsiJRyGxQTTx.jpeg",
            "isPro": false,
            "fullname": "Nikita Sushko",
            "user": "chameleon-lizard",
            "type": "user"
          },
          "name": "Nikita Sushko",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T07:54:21.453Z",
          "hidden": false
        },
        {
          "_id": "67aafead3711ca5b760f324e",
          "user": {
            "_id": "5dfa8e07da6d0311fd3d5430",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1651090418656-5dfa8e07da6d0311fd3d5430.png",
            "isPro": false,
            "fullname": "Sergey Pletenev",
            "user": "memyprokotow",
            "type": "user"
          },
          "name": "Sergey Pletenev",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T09:59:47.063Z",
          "hidden": false
        },
        {
          "_id": "67aafead3711ca5b760f324f",
          "user": {
            "_id": "662f8d645c4db70c77a203b0",
            "avatarUrl": "/avatars/72f9a3c39b3ba5114388d16a35524835.svg",
            "isPro": false,
            "fullname": "Elena Tutubalina",
            "user": "tlenusik",
            "type": "user"
          },
          "name": "Elena Tutubalina",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T09:59:50.003Z",
          "hidden": false
        },
        {
          "_id": "67aafead3711ca5b760f3250",
          "name": "Alexander Panchenko",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-10T12:30:25.000Z",
      "title": "SynthDetoxM: 現代LLMsは、フィードバックデトキシファクションデータのシングルショット平行設計のアノテーター",
      "summary": "現在の多言語テキストのデトキシファイシングにおける既存のアプローチは、平行多言語データセットの不足により制限されています。本研究では、多言語平行デトキシファイシングデータの生成に向けたパイプラインを介して解決策を提案します。また、手動的に収集され、合成的に生成された多言語平行テキストデトキシファイシングデータセットSynthDetoxMを介して、ドローン、フランス語、スペイン語とロシア語の高品質の16,000件のデトキシファイシング文のペアを構成しています。このデータは、異なる毒性評価データセットから構築され、9つの現代の開放ソースLLMにより幾ピーシー設定で再書きにより実装されました。実験結果によると、この合成データセットによって訓練されたモデルは、ヒューマンアノテーションされたMultiParaDetoxデータセットによって訓練されたモデルよりも、データの制限がある状況でも優れた性能を示します。SynthDetoxMによって訓練されたモデルは、幾ピーシー設定ですべての評価されたLLMを上回ります。私たちのデータセットとコードを公開し、さらなる多言語テキストデトキシファイシング研究に貢献していきます。",
      "upvotes": 55,
      "discussionId": "67aafeae3711ca5b760f3280"
    },
    "publishedAt": "2025-02-11T03:03:12.135Z",
    "title": "SynthDetoxM: Modern LLMs are Few-Shot Parallel Detoxification Data Annotators",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06394.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "61ade264f602880813dbe10b",
      "avatarUrl": "/avatars/a92dea7d853bbabbf60b351c207b6875.svg",
      "fullname": "Daniil Moskovskiy",
      "name": "etomoscow",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.06781",
      "authors": [
        {
          "_id": "67aacd7e078cdf445284f9f6",
          "name": "Chengqi Lyu",
          "hidden": false
        },
        {
          "_id": "67aacd7e078cdf445284f9f7",
          "name": "Songyang Gao",
          "hidden": false
        },
        {
          "_id": "67aacd7e078cdf445284f9f8",
          "name": "Yuzhe Gu",
          "hidden": false
        },
        {
          "_id": "67aacd7e078cdf445284f9f9",
          "user": {
            "_id": "64e8505321540e1da3226b54",
            "avatarUrl": "/avatars/18958b8406d1ce492b54c1c839f18c54.svg",
            "isPro": false,
            "fullname": "Wenwei Zhang",
            "user": "ZwwWayne",
            "type": "user"
          },
          "name": "Wenwei Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T07:54:40.279Z",
          "hidden": false
        },
        {
          "_id": "67aacd7e078cdf445284f9fa",
          "name": "Jianfei Gao",
          "hidden": false
        },
        {
          "_id": "67aacd7e078cdf445284f9fb",
          "name": "Kuikun Liu",
          "hidden": false
        },
        {
          "_id": "67aacd7e078cdf445284f9fc",
          "name": "Ziyi Wang",
          "hidden": false
        },
        {
          "_id": "67aacd7e078cdf445284f9fd",
          "name": "Shuaibin Li",
          "hidden": false
        },
        {
          "_id": "67aacd7e078cdf445284f9fe",
          "name": "Qian Zhao",
          "hidden": false
        },
        {
          "_id": "67aacd7e078cdf445284f9ff",
          "name": "Haian Huang",
          "hidden": false
        },
        {
          "_id": "67aacd7e078cdf445284fa00",
          "name": "Weihan Cao",
          "hidden": false
        },
        {
          "_id": "67aacd7e078cdf445284fa01",
          "name": "Jiangning Liu",
          "hidden": false
        },
        {
          "_id": "67aacd7e078cdf445284fa02",
          "name": "Hongwei Liu",
          "hidden": false
        },
        {
          "_id": "67aacd7e078cdf445284fa03",
          "name": "Junnan Liu",
          "hidden": false
        },
        {
          "_id": "67aacd7e078cdf445284fa04",
          "user": {
            "_id": "630716d11801ecc7d2595021",
            "avatarUrl": "/avatars/2d36a880ce4a3cf7efc5ff3987dbeaf3.svg",
            "isPro": false,
            "fullname": "Songyang Zhang",
            "user": "zsytony",
            "type": "user"
          },
          "name": "Songyang Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T07:54:37.733Z",
          "hidden": false
        },
        {
          "_id": "67aacd7e078cdf445284fa05",
          "name": "Dahua Lin",
          "hidden": false
        },
        {
          "_id": "67aacd7e078cdf445284fa06",
          "name": "Kai Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-10T18:57:29.000Z",
      "title": "探索成果報酬の学習数学論理の限界",
      "summary": "推理能力，特别是解决复杂数学问题的能力，是普遍智力的关键组成部分。最近，像OpenAI的o-series模型这样的专有公司取得了令人瞩目的进展。然而，完整的技术细节尚未公开，被认为肯定采用的技术只有强化学习（RL）和长链思维。本文提出了一种新的RL框架，称为OREAL，以追求通过基于结果的奖励强化学习在数学推理任务中可实现的性能极限，其中仅二元结果奖励易于获取。理论上证明，从N中最佳样本（BoN）采样正向轨迹的行为克隆足以在二元反馈环境中学习KL正则化的最优策略。这一公式进一步意味着负样本的奖励应被重新塑造，以确保正向和负向样本之间的梯度一致性。为了缓解RL中长期存在的稀疏奖励问题，这些问题甚至因推理任务中长链思维的部分正确性而加剧，我们进一步应用了基于标记的奖励模型，以在推理轨迹中采样重要标记进行学习。通过OREAL，7B模型首次在RL上实现了MATH-500的94.0 pass@1准确率，与32B模型相当。OREAL-32B在MATH-500上也以95.0 pass@1的准确率超越了之前通过蒸馏训练的32B模型。我们的研究还表明了初始策略模型和RL训练查询的重要性。代码、模型和数据将发布，以促进未来的研究。https://github.com/InternLM/OREAL.",
      "upvotes": 30,
      "discussionId": "67aacd7f078cdf445284fa4b"
    },
    "publishedAt": "2025-02-10T23:18:11.727Z",
    "title": "Exploring the Limit of Outcome Reward for Learning Mathematical Reasoning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06781.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6601196cc91ba4c08ad6e270",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6601196cc91ba4c08ad6e270/X2YPNzUOQXBz5Gv-xR9LW.jpeg",
      "fullname": "yuzhe gu",
      "name": "vanilla1116",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.06703",
      "authors": [
        {
          "_id": "67aabf93c0f8648f68c68ce4",
          "user": {
            "_id": "667187ba9ab144eb3ac43a1b",
            "avatarUrl": "/avatars/db5558aa1c5160b9aee8b58573271959.svg",
            "isPro": false,
            "fullname": "Runze Liu",
            "user": "RyanLiu112",
            "type": "user"
          },
          "name": "Runze Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T07:55:22.940Z",
          "hidden": false
        },
        {
          "_id": "67aabf93c0f8648f68c68ce5",
          "name": "Junqi Gao",
          "hidden": false
        },
        {
          "_id": "67aabf93c0f8648f68c68ce6",
          "name": "Jian Zhao",
          "hidden": false
        },
        {
          "_id": "67aabf93c0f8648f68c68ce7",
          "user": {
            "_id": "60bc94cd85a3ab33829b6211",
            "avatarUrl": "/avatars/b57d36c7577fbbb42ea5b963eef4144a.svg",
            "isPro": false,
            "fullname": "Kaiyan Zhang",
            "user": "iseesaw",
            "type": "user"
          },
          "name": "Kaiyan Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T07:55:18.725Z",
          "hidden": false
        },
        {
          "_id": "67aabf93c0f8648f68c68ce8",
          "name": "Xiu Li",
          "hidden": false
        },
        {
          "_id": "67aabf93c0f8648f68c68ce9",
          "name": "Biqing Qi",
          "hidden": false
        },
        {
          "_id": "67aabf93c0f8648f68c68cea",
          "name": "Wanli Ouyang",
          "hidden": false
        },
        {
          "_id": "67aabf93c0f8648f68c68ceb",
          "name": "Bowen Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-10T17:30:23.000Z",
      "title": "1B LLMが405B LLMを超えることはできるか？テストタイムの計算最適化サイズアップを再考する",
      "summary": "テストタイムスケーリング（TTS）は、推論フェーズで追加計算を使用して大規模言語モデル（LLMs）の性能を向上させる重要な方法です。しかし、現在の研究は、政策モデル、Process Reward Models（PRMs）、問題の難易度がどのようにTTSに影響を与えるかをシステマティックに分析していません。この分析の欠如は、TTS手法の理解と実用的な利用を制限しています。本論文では、2つの核心的な質問に焦点を当てます：（1）政策モデル、PRMs、問題の難易度レベルを統一してテストタイム計算をスケールする最適なアプローチは何か？（2）拡張計算がLLMsの複雑なタスクの性能向上にどのような影響を与え、このアプローチで小さな言語モデルが大きなモデルを超えることができるか？MATH-500と挑戦的なAIME24タスクにおいて詳細な実験を行い、以下の結論を得ました：（1）計算最適化されたTTS戦略は、政策モデル、PRM、問題の難易度の選択により高度に依存します。（2）我々の計算最適化されたTTS戦略を用いると、非常に小さな政策モデルが大きなモデルを超えることができます。例えば、1B LLMはMATH-500で405B LLMを超えます。また、MATH-500とAIME24では、0.5B LLMはGPT-4oを超え、3B LLMは405B LLMを超え、7B LLMはo1とDeepSeek-R1を超え、同時に推論効率が高いことを示します。これらの結果は、TTS戦略を各タスクとモデルの特定の特徴に適したものにする重要性を示し、TTSはLLMsの理由能力を向上させる有望なアプローチであることを示しています。",
      "upvotes": 25,
      "discussionId": "67aabf94c0f8648f68c68d19"
    },
    "publishedAt": "2025-02-11T00:36:11.270Z",
    "title": "Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06703.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6017
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.05609",
      "authors": [
        {
          "_id": "67aacaaaa03eecbc2d72835f",
          "user": {
            "_id": "64ec4c04c782d648d28d70fc",
            "avatarUrl": "/avatars/6975526fcf4b513cc934b5bc45370a48.svg",
            "isPro": false,
            "fullname": "Sukmin Cho",
            "user": "zomss",
            "type": "user"
          },
          "name": "Sukmin Cho",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T07:54:43.377Z",
          "hidden": false
        },
        {
          "_id": "67aacaaaa03eecbc2d728360",
          "name": "Sangjin Choi",
          "hidden": false
        },
        {
          "_id": "67aacaaaa03eecbc2d728361",
          "user": {
            "_id": "64d1e70a84f205869017703b",
            "avatarUrl": "/avatars/215d0d4db5f79cb74df4d888b18c6a0d.svg",
            "isPro": false,
            "fullname": "Taeho Hwang",
            "user": "doubleyyh",
            "type": "user"
          },
          "name": "Taeho Hwang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T07:54:45.737Z",
          "hidden": false
        },
        {
          "_id": "67aacaaaa03eecbc2d728362",
          "name": "Jeongyeon Seo",
          "hidden": false
        },
        {
          "_id": "67aacaaaa03eecbc2d728363",
          "name": "Soyeong Jeong",
          "hidden": false
        },
        {
          "_id": "67aacaaaa03eecbc2d728364",
          "name": "Huije Lee",
          "hidden": false
        },
        {
          "_id": "67aacaaaa03eecbc2d728365",
          "name": "Hoyun Song",
          "hidden": false
        },
        {
          "_id": "67aacaaaa03eecbc2d728366",
          "name": "Jong C. Park",
          "hidden": false
        },
        {
          "_id": "67aacaaaa03eecbc2d728367",
          "name": "Youngjin Kwon",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-08T15:32:53.000Z",
      "title": "無失真な大規模言語モデルの加速における時系列的近接性に基づく階層的起草法",
      "summary": "大語言モデル（LLMs）の推論を加速することは、実時間的なインタラクションにおいて重要です。推論速度を改善するために注目されてきたアルゴリズム的な解決策の一つとして、推論を加速するためにトークンをプレビューし、その正確性を確認することで、一回の前向キャッシュで複数のトークンを生成する手法があります。しかし、現在のプレビュー戦略は通常、重要な微調編集が必要であるか、タスク毎に不均一的な性能を示すことが多いです。これらの課題を解決するために、我々は、時間的な局在性に基づいた多層構造のフレームワークで、複数のデータベースによって異なるトークンソースを組み立てる無失損プレビューアプローチ、Hierarchy Drafting（HD）を提案します。プレビューステップでは、HDは、最高の局在性から最低の局在性に沿って複数のデータベースを順番にアクセスし、多様なタスクにおいても一貫した加速を確保し、プレビューの遅延を最小化します。Spec-Bench上での実験により、7Bと13BパラメータのLLMsを使用した結果、HDは既存のデータベースプレビュー手法を上回り、モデルサイズ、タスク、温度による強固な推論速度アップを実現しました。",
      "upvotes": 12,
      "discussionId": "67aacaaca03eecbc2d728394"
    },
    "publishedAt": "2025-02-10T22:58:41.471Z",
    "title": "Lossless Acceleration of Large Language Models with Hierarchical Drafting based on Temporal Locality in Speculative Decoding",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05609.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ec4c04c782d648d28d70fc",
      "avatarUrl": "/avatars/6975526fcf4b513cc934b5bc45370a48.svg",
      "fullname": "Sukmin Cho",
      "name": "zomss",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.05415",
      "authors": [
        {
          "_id": "67aaea0a0acaa007694aed73",
          "user": {
            "_id": "65708920806dee337da0eef5",
            "avatarUrl": "/avatars/945e328dedc8e1e3111f48c344ad5b03.svg",
            "isPro": false,
            "fullname": "xuchenkai",
            "user": "UnhurriedDawn",
            "type": "user"
          },
          "name": "Chenkai Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T07:54:28.861Z",
          "hidden": false
        },
        {
          "_id": "67aaea0a0acaa007694aed74",
          "user": {
            "_id": "6644548a3a16452261cdb173",
            "avatarUrl": "/avatars/4643db904204e3a60202a29e8c884139.svg",
            "isPro": false,
            "fullname": "wangxu",
            "user": "asunalove",
            "type": "user"
          },
          "name": "Xu Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T07:54:26.432Z",
          "hidden": false
        },
        {
          "_id": "67aaea0a0acaa007694aed75",
          "name": "Zhenyi Liao",
          "hidden": false
        },
        {
          "_id": "67aaea0a0acaa007694aed76",
          "name": "Yishun Li",
          "hidden": false
        },
        {
          "_id": "67aaea0a0acaa007694aed77",
          "name": "Tianqi Hou",
          "hidden": false
        },
        {
          "_id": "67aaea0a0acaa007694aed78",
          "user": {
            "_id": "64bba541da140e461924dfed",
            "avatarUrl": "/avatars/367993765b0ca3734b2b100db33ed787.svg",
            "isPro": false,
            "fullname": "zhijie deng",
            "user": "zhijie3",
            "type": "user"
          },
          "name": "Zhijie Deng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T07:54:24.089Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-08T02:52:25.000Z",
      "title": "ショーオツーボール：加速された統合モノモーダル理解と生成のために",
      "summary": "ユニークな多モーダル理解と生成モデルの開発に対する研究興味が増加しています。その中でも、Show-oが特に注目を集めており、テキストから画像へと画像からテキストへの生成に優れています。Show-oの推論は、画像トークンの進歩的なデノイズとテキストトークンの自動復元記述を含むため、両方での不適切なエフィシェンス問題に苦しむのです。本論文では、このグループとの間を結ぶためにShow-o Turboを紹介します。まず、Show-oでの画像とテキストの生成において、テキストトークンの並列記述に基づいた統一的なデノイズ観点を識別します。次に、ディフュージョンモデルのデノイズプロセスを短縮するための適切なアプローチである一致性ディスタイルしょン（CD）を、Show-oの多モーダルデノイズトラジェクトに拡張します。トラジェクト分割戦略とカレクルラーニングプロセスを導入し、トレーニングの収束を改善します。実験的には、テキストから画像への生成では、4スタップでカスタマイズフリーガイド（CFG）を使用しないと同時にGenEvalスコアが0.625となり、8スタップとCFGを使用した元のShow-oを超えます。画像からテキストへの生成では、性能を大幅に損ねることなく1.5倍のスピードアップを示します。コードは、https://github.com/zhijie-group/Show-o-Turboにアクセスできます。",
      "upvotes": 9,
      "discussionId": "67aaea100acaa007694aeea5"
    },
    "publishedAt": "2025-02-11T02:09:27.778Z",
    "title": "Show-o Turbo: Towards Accelerated Unified Multimodal Understanding and Generation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05415.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64bba541da140e461924dfed",
      "avatarUrl": "/avatars/367993765b0ca3734b2b100db33ed787.svg",
      "fullname": "zhijie deng",
      "name": "zhijie3",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.06772",
      "authors": [
        {
          "_id": "67aac8adfe33f6d8d695bc40",
          "name": "Ling Yang",
          "hidden": false
        },
        {
          "_id": "67aac8adfe33f6d8d695bc41",
          "name": "Zhaochen Yu",
          "hidden": false
        },
        {
          "_id": "67aac8adfe33f6d8d695bc42",
          "name": "Bin Cui",
          "hidden": false
        },
        {
          "_id": "67aac8adfe33f6d8d695bc43",
          "name": "Mengdi Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-10T18:51:47.000Z",
      "title": "ReasonFlux: スケーリングされたテンプレートを通じた階層的LLM論理",
      "summary": "私たちは、スケーリングされたシンプルな考え方テンプレートを用いたヒューリスティックなLLMの理由論理を提案し、パワフルなLLM（例えばOpenAI o1-previewやDeepSeek V3）の数学的な理由論理能力を優越することができることを示します。私たちは、ReasonFlux-32Bモデルを8ガフプロカスにして訓練し、3つのイノベーションを導入します： （i）構造化されたジャンル共通の考え方テンプレートライブラリ、およそ500つの高レベルの考え方テンプレートを含み、類似または関連する理由論理問題に一般化可能です；（ii）長いCoTsよりも順番の考え方テンプレートの列でのヒューリスティックな強化学習を行い、基礎LLMを最適なテンプレートトラジェクトに計画し、複雑な問題を段階的に解決することを最適化します；（iii）新しい推論スケーリングシステムを導入し、推論時に考え方テンプレートを適応的にスケーリングしてヒューリスティックなLLMの理由論理を可能にします。順番の考え方テンプレートを含むテンプレートトラジェクトを持つことで、ReasonFlux-32Bの数学的な理由論理能力は最先端レベルに達します。特に、MATHベンチマークでは、正確性91.2%を達成し、o1-previewを6.7%超えます。USA Math Olympiad（AIME）ベンチマークでは、ReasonFlux-32Bは平均56.7%の問題を解くことができ、o1-previewとDeepSeek-V3をそれぞれ27%と45%超えます。コード：https://github.com/Gen-Verse/ReasonFlux",
      "upvotes": 9,
      "discussionId": "67aac8affe33f6d8d695bcbd"
    },
    "publishedAt": "2025-02-10T22:49:56.390Z",
    "title": "ReasonFlux: Hierarchical LLM Reasoning via Scaling Thought Templates",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06772.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64fde4e252e82dd432b74ce9",
      "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
      "fullname": "Ling Yang",
      "name": "Lingaaaaaaa",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.06049",
      "authors": [
        {
          "_id": "67aac01bd7b18841e7c266df",
          "name": "Jikun Kang",
          "hidden": false
        },
        {
          "_id": "67aac01bd7b18841e7c266e0",
          "name": "Wenqi Wu",
          "hidden": false
        },
        {
          "_id": "67aac01bd7b18841e7c266e1",
          "name": "Filippos Christianos",
          "hidden": false
        },
        {
          "_id": "67aac01bd7b18841e7c266e2",
          "name": "Alex J. Chan",
          "hidden": false
        },
        {
          "_id": "67aac01bd7b18841e7c266e3",
          "name": "Fraser Greenlee",
          "hidden": false
        },
        {
          "_id": "67aac01bd7b18841e7c266e4",
          "name": "George Thomas",
          "hidden": false
        },
        {
          "_id": "67aac01bd7b18841e7c266e5",
          "name": "Marvin Purtorab",
          "hidden": false
        },
        {
          "_id": "67aac01bd7b18841e7c266e6",
          "name": "Andy Toulis",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-09T22:11:42.000Z",
      "title": "LM2: 大記憶モデル",
      "summary": "この論文では、Large Memory Model (LM2) を紹介します。LM2 は、標準的な Transformer の多段階推理、関係論理、長文脈で分散した情報の合成における制限を解決するために、アシスタントメモリモジュールを追加した解釈器だけの Transformer アーキテクチャです。提案された LM2 は、コンテキスト表現のリポジトリとして機能するメモリモジュールを含み、入力トークンとクロスアテンションを通じて相互作用し、ゲート機構を通じて更新されます。Transformer の一般的な機能を維持するために、LM2 は、補助的なメモリパスウェイを統合しながら元の情報流を保持しています。BABILong ベンチマークでの実験結果によると、LM2 モデルは、RMT モデルを 37.1% よりも、ベースライン Llama-3.2 モデルを 86.3% よりも平均的により良い結果を示します。LM2 は、多段階推論、数値論理、大脈動文脈の質問回答に特別な能力を示します。MMLU データセットでは、学習済みのバージョンモデルより 5.0% の改善を達成し、そのメモリモジュールが一般的なタスクに対する性能を低下させないことを示しています。また、分析では、メモリの説明性、メモリモジュールの効果、テスト時の行動を検討しました。我々の発見は、Transformer アーキテクチャを強化するための明示的なメモリの重要性を強調しています。",
      "upvotes": 8,
      "discussionId": "67aac01dd7b18841e7c26739"
    },
    "publishedAt": "2025-02-10T22:13:17.117Z",
    "title": "LM2: Large Memory Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06049.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6489e10ca13f65198dc6e122",
      "avatarUrl": "/avatars/4aa9eab488157711b2f0298ddadee2f4.svg",
      "fullname": "Kang",
      "name": "JaxonK",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.03628",
      "authors": [
        {
          "_id": "67aab82e6024056209d727a8",
          "name": "Zhuowei Li",
          "hidden": false
        },
        {
          "_id": "67aab82e6024056209d727a9",
          "name": "Haizhou Shi",
          "hidden": false
        },
        {
          "_id": "67aab82e6024056209d727aa",
          "name": "Yunhe Gao",
          "hidden": false
        },
        {
          "_id": "67aab82e6024056209d727ab",
          "name": "Di Liu",
          "hidden": false
        },
        {
          "_id": "67aab82e6024056209d727ac",
          "name": "Zhenting Wang",
          "hidden": false
        },
        {
          "_id": "67aab82e6024056209d727ad",
          "name": "Yuxiao Chen",
          "hidden": false
        },
        {
          "_id": "67aab82e6024056209d727ae",
          "name": "Ting Liu",
          "hidden": false
        },
        {
          "_id": "67aab82e6024056209d727af",
          "name": "Long Zhao",
          "hidden": false
        },
        {
          "_id": "67aab82e6024056209d727b0",
          "name": "Hao Wang",
          "hidden": false
        },
        {
          "_id": "67aab82e6024056209d727b1",
          "name": "Dimitris N. Metaxas",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-05T21:34:02.000Z",
      "title": "トークンの隠れた生活：ビジュアル情報制御による大規模ビジョン言語モデルのハウシャーミューションの減少",
      "summary": "大視野ビジョン言語モデル（LVLMs）は、文脈と視覚入力を両方に効果的に推理することができますが、それらは文法的に一致しているが視覚的に基底がない内容を幻覚化する傾向があります。本論文では、生成プロセス中のトークンロジットの順位を調査して、LVLMsが情報を処理する際に現れる3つのキーパターンを明らかにします：1) 進行中における視覚情報の損失 -- 視覚的に基底があるトークンは生成の進行中にその優先順位が徐々に低下します。2) 早期のエクスケイション -- 語意的に意味のあるトークンは最終層よりも早く層内で活性化の高潮を達成します。3) 隠れた真の情報 -- 視覚的に基底があるトークンは最終的に決定されないが、推論時には相対的に高い順位を保ちます。これらのヒントに基づいて、VISTA（Visual Information Steering with Token-logit Augmentation）を提案します。VISTAは、外部のスーパーバイザーを必要としない推論時のインターベーションフレームワークで、幻覚化を減らしながら真の情報を促進します。VISTAは、活性化スペースでの視覚情報の強化と早期層の活性化を活用して、語意的に意味のある解碼を促進します。既存の方法と比較して、VISTAは外部のスーパーバイザーを必要としないことで、多様な解碼戦略に適用可能です。拡張的な実験により、VISTAは平均で40%程度の幻覚化を減らし、4つのベンチマークで3つの解碼戦略を組み合わせて、既存の方法を超える結果を収めました。",
      "upvotes": 8,
      "discussionId": "67aab82f6024056209d727f6"
    },
    "publishedAt": "2025-02-10T21:38:53.032Z",
    "title": "The Hidden Life of Tokens: Reducing Hallucination of Large Vision-Language Models via Visual Information Steering",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.03628.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64dfcc62e8b6f3f3baa950e0",
      "avatarUrl": "/avatars/21bbff67d46c08044efe2406575aa77e.svg",
      "fullname": "Zhenting Wang",
      "name": "ztwang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.06786",
      "authors": [
        {
          "_id": "67aae91b83b1182df7c0cf54",
          "name": "Pranav Nair",
          "hidden": false
        },
        {
          "_id": "67aae91b83b1182df7c0cf55",
          "name": "Puranjay Datta",
          "hidden": false
        },
        {
          "_id": "67aae91b83b1182df7c0cf56",
          "name": "Jeff Dean",
          "hidden": false
        },
        {
          "_id": "67aae91b83b1182df7c0cf57",
          "name": "Prateek Jain",
          "hidden": false
        },
        {
          "_id": "67aae91b83b1182df7c0cf58",
          "name": "Aditya Kusupati",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-10T18:59:10.000Z",
      "title": "マトリゾハクアンチューション",
      "summary": "モデルの重みを量子化することは、大きなモデルの通信コストと推論コストの削減に重要です。しかし、モデルの量子化（特にint4またはint2のような低精度について）はモデルの質の補損として負の効果を示します。特にint2は、モデルの質を厳しく低下させることがあることが知られています。そのため、実践者は通常は、異なる量子化レベルの複数のモデルを保有するか、最も良い質と遅延の補損を満たすモデルを提供する必要があります。一方、整数データ型（例えばint8）は、小さなビット幅の整数（例えばint4またはint2）が最も有意なビットに内包されるような隠れ箱（Matryoshka）構造を持ちます。本論文では、Matryoshka Quantization（MatQuant）という新しい多スケール量子化技術を提案し、複数の量子化モデルの必要性を解決することを目指しています。これは、一つのモデルのみを訓練し、それを違う精度レベルで提供できるようにします。また、MatQuantによる共学習と共煙薬正規化のため、MatQuantから得られるint2精度のモデルは、標準的なint2量子化（QATやOmniQuantなどの技術を使用して）よりも10%以上の精度を向上させることができます。これは、同じレシピを使用して、int2のFFN量子化されたGemma-2 9Bモデルがint8のFFN量子化されたGemma-2 2Bモデルよりもより高精度であることを示し、モデル量子化の進歩を示しています。",
      "upvotes": 7,
      "discussionId": "67aae91d83b1182df7c0cff6"
    },
    "publishedAt": "2025-02-11T01:07:50.116Z",
    "title": "Matryoshka Quantization",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06786.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6017
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.06788",
      "authors": [
        {
          "_id": "67aac64de37429ebdbdafc40",
          "name": "Haiwen Diao",
          "hidden": false
        },
        {
          "_id": "67aac64de37429ebdbdafc41",
          "name": "Xiaotong Li",
          "hidden": false
        },
        {
          "_id": "67aac64de37429ebdbdafc42",
          "name": "Yufeng Cui",
          "hidden": false
        },
        {
          "_id": "67aac64de37429ebdbdafc43",
          "name": "Yueze Wang",
          "hidden": false
        },
        {
          "_id": "67aac64de37429ebdbdafc44",
          "name": "Haoge Deng",
          "hidden": false
        },
        {
          "_id": "67aac64de37429ebdbdafc45",
          "user": {
            "_id": "6565bc5ee5aac326bfc98e39",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/vIfHy9Y1yAK6A96UCHNBH.jpeg",
            "isPro": false,
            "fullname": "Ting Pan",
            "user": "PhyscalX",
            "type": "user"
          },
          "name": "Ting Pan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T07:55:09.401Z",
          "hidden": false
        },
        {
          "_id": "67aac64de37429ebdbdafc46",
          "name": "Wenxuan Wang",
          "hidden": false
        },
        {
          "_id": "67aac64de37429ebdbdafc47",
          "name": "Huchuan Lu",
          "hidden": false
        },
        {
          "_id": "67aac64de37429ebdbdafc48",
          "name": "Xinlong Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-10T18:59:58.000Z",
      "title": "EVEv2: ビジョン-言語モデルのエンコーダー無しベースラインの改善",
      "summary": "現在のエンコーダー無しの視覚言語モデル（VLMs）は、エンコーダーベースのモデルとの性能間隔を急速に狭めています。これは、構造的な簡単性と効率的な採用での統合モデルの可能性を明らかにしています。私たちは、エンコーダー無しのVLMsの性能間隔をシステマティックに明確化し、プレトレーンされた視覚エンコーダー、離散トーキナイザー、そしてビジュアル層のミニマリストライフより構築されたVLMsの視覚と言語の特性を深く掘り下げました。私たちは、エンコーダー無しのVLMsと主流のエンコーダーベースのモデルと同等の性能を競い合わせる効率的な戦略を開発しました。詳細な調査を通じて、私たちはEVEv2.0という新しい、改善されたエンコーダー無しのVLMsの家族を発表しました。私たちは以下のことを示します： (i) ビジュアルと言語を統一的なモデル内で適切に分解し、階層的に連結することで、モデル間の干渉を減らすことができます。 (ii) 良い訓練戦略が、エンコーダー無しのVLMsの効果的な最適化を可能にします。拡張的な評価を通じて、私たちのEVEv2.0は、モデル間のデコーダーだけのアーキテクチャの開発を示し、高度なデータエフィシェンスと強力的な視覚認識能力を示します。コードは公開的に利用できます：https://github.com/baaivision/EVE。",
      "upvotes": 6,
      "discussionId": "67aac64ee37429ebdbdafc96"
    },
    "publishedAt": "2025-02-10T22:40:39.442Z",
    "title": "EVEv2: Improved Baselines for Encoder-Free Vision-Language Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06788.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b4a717aa03b6520839e9b8",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b4a717aa03b6520839e9b8/Rt3ERG-6BVEA4hAwOz0_I.jpeg",
      "fullname": "Haiwen Diao",
      "name": "Paranioar",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.06782",
      "authors": [
        {
          "_id": "67aae76c71a9983f50e134ef",
          "name": "Dongyang Liu",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e134f0",
          "name": "Shicheng Li",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e134f1",
          "name": "Yutong Liu",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e134f2",
          "name": "Zhen Li",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e134f3",
          "name": "Kai Wang",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e134f4",
          "name": "Xinyue Li",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e134f5",
          "name": "Qi Qin",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e134f6",
          "name": "Yufei Liu",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e134f7",
          "name": "Yi Xin",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e134f8",
          "name": "Zhongyu Li",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e134f9",
          "name": "Bin Fu",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e134fa",
          "name": "Chenyang Si",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e134fb",
          "name": "Yuewen Cao",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e134fc",
          "name": "Conghui He",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e134fd",
          "name": "Ziwei Liu",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e134fe",
          "name": "Yu Qiao",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e134ff",
          "name": "Qibin Hou",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e13500",
          "name": "Hongsheng Li",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e13501",
          "name": "Peng Gao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-10T18:58:11.000Z",
      "title": "Lumina-Video: 多スケールでの効率的かつ柔軟なビデオ生成におけるNext-DiT",
      "summary": "最近の進歩は、Diffusion Transformers (DiTs) を生成モデリングの主導的なフレームワークとして立ち上げました。この成功に基づき、Lumina-Next は Next-DiT を用いて写実的な画像の生成に極めて高い性能を収めました。しかし、ビデオ生成の可能性は大きく未開発であり、ビデオデータに固有の空間時間複雑性のモデリングにおいて重大な課題があります。これに対して、私たちは Lumina-Video を紹介します。このフレームワークは Next-DiT の強みを活用しながら、ビデオ合成に適した解決策を導入しています。Lumina-Video は、多スケールの Next-DiT アーキテクチャを採用し、それらのパッチ化を共に学習して、エフィシェンスと柔軟性を両立させます。また、動きスコアを明示的な条件として採用することで、Lumina-Video は生成されたビデオの動的な程度を直接制御できます。これは、進歩的な学習スキームとして、高い解像度と FPS を増やしながら、自然と合成データを混在した多ソース学習スキームを組み合わせることにより、高い学習と推論のエフィシェンスを伴う、魅力的な美術質と流れの良さを収めます。また、私たちは Next-DiT に基づくビデオから音頻へのモデルである Lumina-V2A を提案します。これは生成されたビデオに同期された音声を作成することを目的としています。コードは https://www.github.com/Alpha-VLLM/Lumina-Video でリリースされています。",
      "upvotes": 5,
      "discussionId": "67aae76e71a9983f50e1357d"
    },
    "publishedAt": "2025-02-11T01:00:25.383Z",
    "title": "Lumina-Video: Efficient and Flexible Video Generation with Multi-scale Next-DiT",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06782.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6017
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.05431",
      "authors": [
        {
          "_id": "67aac392385da1f07cc7fcbd",
          "user": {
            "_id": "64f58b970b24e548a85522bc",
            "avatarUrl": "/avatars/c8ca1294b5a1edd609694877e335b22f.svg",
            "isPro": false,
            "fullname": "Xinyu Yang",
            "user": "Hanyuezhuohua",
            "type": "user"
          },
          "name": "Xinyu Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T07:55:13.131Z",
          "hidden": false
        },
        {
          "_id": "67aac392385da1f07cc7fcbe",
          "name": "Tianqi Chen",
          "hidden": false
        },
        {
          "_id": "67aac392385da1f07cc7fcbf",
          "name": "Beidi Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-08T03:41:16.000Z",
      "title": "APE: 適応的並列エンコーディングによる高速化と長期コンテキスト付加の生成",
      "summary": "Context-augmented generation (CAG)手法、RAGやICLを含む、は、複数のコンテキストを効率的に組み合わせてユーザーのクエリに対する回答を生成するために必要とされます。これらのコンテキストを順番に入力することは、コンテキストの組み合わせをごく大きな計算負担を引き起こすため、その結果はそれぞれのリクエストに対して再エンコーディングされます。この問題を解決するために、ポアラレルエンコーディングの可能性を検討し、各コンテキストのKV状態を独立して事前計算し、キャッシュします。このアプローチは、推論時にキャッシュされた状態を直接読み込むことを可能にし、コンテキストの数を増やすことができます。しかし、注目分布の不対称性により、直接ポアラレルエンコーディングを適用すると性能が大幅に低下することがあります。これを解決するために、Adaptive Parallel Encoding (APE)を提案し、共通のプレフィクス、注目温度、スケーリング因子を使用し、ポアラレルエンコーディングの分布をシーケンシャルエンコーディングの分布と一致させることを目指します。RAGおよびICLのタスクでの結果は、同じ入力を使用してシーケンシャルエンコーディングの性能を98%と93%で保持し、ポアラレルエンコーディングを3.6%と7.9%より上回ることを示します。また、複数ショットのCAGにも適用でき、数百のコンテキストを同時にエンコーディングできます。効率評価によると、APEは128K長のコンテキストの予り時間を28倍削減し、終端からの4.5倍速さアップを実現できます。",
      "upvotes": 5,
      "discussionId": "67aac393385da1f07cc7fd17"
    },
    "publishedAt": "2025-02-10T22:29:36.102Z",
    "title": "APE: Faster and Longer Context-Augmented Generation via Adaptive Parallel Encoding",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05431.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "64f58b970b24e548a85522bc",
      "avatarUrl": "/avatars/c8ca1294b5a1edd609694877e335b22f.svg",
      "fullname": "Xinyu Yang",
      "name": "Hanyuezhuohua",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.06155",
      "authors": [
        {
          "_id": "67aab9b4a2bf5e5ea03d4c19",
          "user": {
            "_id": "643a451ee2b979ae6141329d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643a451ee2b979ae6141329d/HN3M5vyroanQoUEiXJFyB.jpeg",
            "isPro": false,
            "fullname": "Hangliang Ding",
            "user": "foreverpiano",
            "type": "user"
          },
          "name": "Hangliang Ding",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T07:55:29.115Z",
          "hidden": false
        },
        {
          "_id": "67aab9b4a2bf5e5ea03d4c1a",
          "name": "Dacheng Li",
          "hidden": false
        },
        {
          "_id": "67aab9b4a2bf5e5ea03d4c1b",
          "name": "Runlong Su",
          "hidden": false
        },
        {
          "_id": "67aab9b4a2bf5e5ea03d4c1c",
          "name": "Peiyuan Zhang",
          "hidden": false
        },
        {
          "_id": "67aab9b4a2bf5e5ea03d4c1d",
          "user": {
            "_id": "64bba541da140e461924dfed",
            "avatarUrl": "/avatars/367993765b0ca3734b2b100db33ed787.svg",
            "isPro": false,
            "fullname": "zhijie deng",
            "user": "zhijie3",
            "type": "user"
          },
          "name": "Zhijie Deng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T07:55:25.471Z",
          "hidden": false
        },
        {
          "_id": "67aab9b4a2bf5e5ea03d4c1e",
          "name": "Ion Stoica",
          "hidden": false
        },
        {
          "_id": "67aab9b4a2bf5e5ea03d4c1f",
          "name": "Hao Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-10T05:00:56.000Z",
      "title": "Efficient-vDiT: エフィシェントビデオ・ディフォーション・トランスフォーマーズ エフィシェントビデオ・ディフォーション・トランスフォーマーズ アテンション",
      "summary": "ディフュージョントランスフォーマー（DiTs）は、3D全注意機構を持つことで高品質のビデオの合成が可能ですが、注意計算の複雑さと多くのサンプリングステップによりインフェリンスが高額になります。例えば、人気のOpen-Sora-Planモデルは、1枚の29フレームのビデオを生成するには9分以上の時間を要します。本論文は、2つの方面からこの不適切さ問題を解決します：1）ビデオデータ内の冗餘性に基づいて3D全注意を削減する。ビデオデータの3D注意マップにおいて広く見られるタイルタイプの再現的パターンを識別し、ビデオフレーム数に対する線形複雑度を持つ新しい家族のスパース3D注意を提唱します。2）既存の多ステップ一致性ディスタイルテーションを採用してサンプリングプロセスを短縮する。全体のサンプリングトラジェクトを数えられる段階に分割し、各段階内で一致性ディスタイルテーションを行い、少数ステップジェネレーション能力を活性化します。また、低複雑度の注意と少数ステップジェネレーション能力を統合するために3段階トレーニングプロセスを設計します。特に、0.1%の事前学習データを使用すると、Open-Sora-Plan-1.2モデルを7.4x-7.8x速くすることができ、VBenchでは性能の微調節が見られます。また、我々のアプローチは分散インフェリンスに適応でき、4グラフィックス上でシーケンスパラレルプログラミングを使用して3.91xの追加スピードアップを実現できます。",
      "upvotes": 5,
      "discussionId": "67aab9bca2bf5e5ea03d4e3c"
    },
    "publishedAt": "2025-02-10T22:09:58.181Z",
    "title": "Efficient-vDiT: Efficient Video Diffusion Transformers With Attention Tile",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06155.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63565cc56d7fcf1bedb7d347",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63565cc56d7fcf1bedb7d347/XGcHP4VkO_oieA1gZ4IAX.jpeg",
      "fullname": "Zhang Peiyuan",
      "name": "PY007",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 82
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.06527",
      "authors": [
        {
          "_id": "67aae4128d478dcb4b39a097",
          "name": "D. She",
          "hidden": false
        },
        {
          "_id": "67aae4128d478dcb4b39a098",
          "name": "Mushui Liu",
          "hidden": false
        },
        {
          "_id": "67aae4128d478dcb4b39a099",
          "name": "Jingxuan Pang",
          "hidden": false
        },
        {
          "_id": "67aae4128d478dcb4b39a09a",
          "name": "Jin Wang",
          "hidden": false
        },
        {
          "_id": "67aae4128d478dcb4b39a09b",
          "name": "Zhen Yang",
          "hidden": false
        },
        {
          "_id": "67aae4128d478dcb4b39a09c",
          "name": "Wanggui He",
          "hidden": false
        },
        {
          "_id": "67aae4128d478dcb4b39a09d",
          "name": "Guanghao Zhang",
          "hidden": false
        },
        {
          "_id": "67aae4128d478dcb4b39a09e",
          "name": "Yi Wang",
          "hidden": false
        },
        {
          "_id": "67aae4128d478dcb4b39a09f",
          "name": "Qihan Huang",
          "hidden": false
        },
        {
          "_id": "67aae4128d478dcb4b39a0a0",
          "name": "Haobin Tang",
          "hidden": false
        },
        {
          "_id": "67aae4128d478dcb4b39a0a1",
          "name": "Yunlong Yu",
          "hidden": false
        },
        {
          "_id": "67aae4128d478dcb4b39a0a2",
          "name": "Siming Fu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-10T14:50:32.000Z",
      "title": "CustomVideoX: 3D参照関心駆動ダイナミックアダプテーションを用いたゼロショットカスタマイズされたビデオディフュージョントランスフォーマー",
      "summary": "カスタマイズされた生成は、画像合成において显著な進歩を達していますが、個人化ビデオ生成は、時間的な不確実性と質の低下により難しいです。本論文では、参考画像から個人化ビデオを生成するための新しいフレームワーク「CustomVideoX」を紹介します。CustomVideoXは、事前学習されたビデオネットワークを利用し、参考特徴を抽出するために独自にLoRAパラメータを訓練し、これにより効率と適応性を確保します。参考画像とビデオ内容の無間違ったインタラクションを促進するために、3D Reference Attentionを提案します。これは、参考画像の特徴と全ビデオフレームの空間と時間の両方の次元で直接的で同時的な対応を可能にします。推論時に生成されるビデオ内容に参考画像の特徴と文脈ガイドニングが過剰な影響を与えることを抑えるために、Time-Aware Reference Attention Bias (TAB) ステラテジを実装します。これは、時間ステップごとに参考バイアスを動的に調節します。また、Entity Region-Aware Enhancement (ERAE) モジュールを紹介します。これは、鍵エンティティトークンの高度な活性化領域を参考特徴の注入に合わせて、注意バイアスを調整し、これにより効果を高めます。個人化ビデオ生成の評価を詳細に行うために、50以上の物体と100以上のプロンプトを含む新しいベンチマーク「VideoBench」を構築します。実験結果によると、CustomVideoXはビデオの一貫性と質において現在の方法を大幅に超えています。",
      "upvotes": 4,
      "discussionId": "67aae4178d478dcb4b39a1e7"
    },
    "publishedAt": "2025-02-11T00:46:11.168Z",
    "title": "CustomVideoX: 3D Reference Attention Driven Dynamic Adaptation for Zero-Shot Customized Video Diffusion Transformers",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06527.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6017
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.06635",
      "authors": [
        {
          "_id": "67aac0ba91e6f5eb5476ea76",
          "name": "Qingshui Gu",
          "hidden": false
        },
        {
          "_id": "67aac0ba91e6f5eb5476ea77",
          "name": "Shu Li",
          "hidden": false
        },
        {
          "_id": "67aac0ba91e6f5eb5476ea78",
          "user": {
            "_id": "64ab99dcb76bfd863eba64c1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ab99dcb76bfd863eba64c1/UBXwDPx17X-gl-SzBPvrc.jpeg",
            "isPro": false,
            "fullname": "TY.Zheng",
            "user": "aaabiao",
            "type": "user"
          },
          "name": "Tianyu Zheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T07:55:15.968Z",
          "hidden": false
        },
        {
          "_id": "67aac0ba91e6f5eb5476ea79",
          "name": "Zhaoxiang Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-10T16:31:37.000Z",
      "title": "Steel-LLM:スクラッチからオープンソースへ -- 中国語中心的なLLMの建設の個人の旅程",
      "summary": "スチール-LLMは、計算資源の限りでも高品質のオープンソースモデルを作成するために、スタートから開発された中国中心的な言語モデルです。2024年3月に発表されたこのプロジェクトは、大規模なデータセットで10億パラメーターモデルを訓練することを目標とし、透明性と実用的なインサイトの共有を優先して、コミュニティの他の人々に役立つように設計されました。訓練プロセスは主に中国のデータを対象にし、一部の英語データを含め、現在のオープンソースLLMの欠点を補完し、モデル作成の旅についてより詳細な実用的な説明を提供しました。スチール-LLMは、CEVALやCMMLUなどのベンチマークで競争的な性能を示し、大規模な機関からの初期モデルを超えました。この論文は、プロジェクトの主な貢献を包括的にまとめたもので、モデル開発に向けた研究者や実践者に有効なリソースとして提供します。モデルチェックポイントと訓練スクリプトは、https://github.com/zhanshijinwat/Steel-LLM から利用できます。",
      "upvotes": 4,
      "discussionId": "67aac0bb91e6f5eb5476eab8"
    },
    "publishedAt": "2025-02-10T22:20:38.168Z",
    "title": "Steel-LLM:From Scratch to Open Source -- A Personal Journey in Building a Chinese-Centric LLM",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06635.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ab99dcb76bfd863eba64c1",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ab99dcb76bfd863eba64c1/UBXwDPx17X-gl-SzBPvrc.jpeg",
      "fullname": "TY.Zheng",
      "name": "aaabiao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.04370",
      "authors": [
        {
          "_id": "67aafd90141fac22732a79b3",
          "name": "Zhenglin Zhou",
          "hidden": false
        },
        {
          "_id": "67aafd90141fac22732a79b4",
          "name": "Xiaobo Xia",
          "hidden": false
        },
        {
          "_id": "67aafd90141fac22732a79b5",
          "name": "Fan Ma",
          "hidden": false
        },
        {
          "_id": "67aafd90141fac22732a79b6",
          "name": "Hehe Fan",
          "hidden": false
        },
        {
          "_id": "67aafd90141fac22732a79b7",
          "name": "Yi Yang",
          "hidden": false
        },
        {
          "_id": "67aafd90141fac22732a79b8",
          "name": "Tat-Seng Chua",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-05T11:03:08.000Z",
      "title": "DreamDPO: ヒューマンの好みと一致するText-to-3D生成についての直接な好み最適化",
      "summary": "テキストから3D生成は、テキストの説明から3Dコンテンツの作成を自動化し、様々な分野において変革的な可能性を持つ。しかし、現在の方法は、生成されたコンテンツとの人間の好みの一致を難しく、適用範囲と柔軟性に限られている。これらの制限を解決するために、本論文では、直接的な好み最適化を通じて3D生成プロセスに人間の好みを統合する最適化ベースのフレームワーク「DreamDPO」を提案します。実用上、DreamDPOは最初にペアウイスの例を構築し、それらの人間の好みとの一致を賞与機能または大規模な多モデルで比較し、最終的に好みをドライブした損失関数を用いて3D表現を最適化します。ペアウイスの比較を利用して好みを反映することで、DreamDPOは精度の高い点ごとの評価に依存しなく、好みをガイドする最適化によって微妙な制御可能さを増やします。実験は、DreamDPOが現在の方法に比べて優れた結果を実現し、高品質で制御可能な3Dコンテンツを提供することを示しました。コードとモデルはオープンソースになります。",
      "upvotes": 3,
      "discussionId": "67aafd94141fac22732a7adc"
    },
    "publishedAt": "2025-02-11T02:46:33.870Z",
    "title": "DreamDPO: Aligning Text-to-3D Generation with Human Preferences via Direct Preference Optimization",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6425318d175bd2952281065e/R7cMLIsmYovAMtL1vhsDn.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04370.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6425318d175bd2952281065e",
      "avatarUrl": "/avatars/37deb6ceb1552dece43a1c8c13c1c871.svg",
      "fullname": "ZhenglinZhou",
      "name": "zhenglin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.05957",
      "authors": [
        {
          "_id": "67aaecec114e64d6e15e7f41",
          "name": "Jiabin Tang",
          "hidden": false
        },
        {
          "_id": "67aaecec114e64d6e15e7f42",
          "name": "Tianyu Fan",
          "hidden": false
        },
        {
          "_id": "67aaecec114e64d6e15e7f43",
          "name": "Chao Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-09T16:53:56.000Z",
      "title": "MetaChain: 完全自動化およびコードなしフレームワークでLLMアガントを構築する",
      "summary": "大語言モデル（LLM）アガントは、タスク自動化と知能的な決定において驚異的な能力を示し、LangChainやAutoGenのようなアガント開発フレームワークの広泛な採用を促しています。しかし、これらのフレームワークは主に技術的な専門知識が豊富な開発者にだけ提供されているため、技術背景がある人々のみが利用できる状況が厳しい。この厳しいアクセス性の間違いは、世界人口の0.03%の人々が必要なプログラミングスキルを持つことを意味し、これを考慮して、この間違いは大きな制限となっています。この厳しいアクセス性の間違いは、技術背景がなくてもLLMアガントを自分で構築することができるかという基本的な問題を引き起こしています。この挑戦に対処するために、私たちはMetaChainという完全自動化した、高度な自動開発フレームワークを紹介します。このフレームワークは、自然語テキストのみを通じてLLMアガントを作成し、部署することができるようにします。自動エージェントオペレーティングシステムとして動作し、Agentic System Utilities、LLMポートドライブングエンジン、Self-Managing File System、Self-Play Agent Customizationモジュールの4つのキーコンポーネントから構成されています。このライトウEightだがパワーフルなシステムは、コーディング要求や手動干渉がなくても、ツール、アガント、ワークフローの効率的かつ動的な作成と変更を可能にします。MetaChainは、コーディングを必要としないアガント開発機能を持つことで、それよりも、一般的な多エージェントシステムとしても多様性を持ちます。GAIAベンチマークにおける詳細な評価は、MetaChainの一般的な多エージェントタスクに対する効果性を示し、現在の最先端の方法を超えています。また、MetaChainのレビューアジュンテーションラジエンシャポン（RAG）に関連する機能は、複数のオルタナティブLLMベースのソリューションと比較しても、一貫して優れた性能を示しています。",
      "upvotes": 3,
      "discussionId": "67aaecef114e64d6e15e802c"
    },
    "publishedAt": "2025-02-11T01:33:35.134Z",
    "title": "MetaChain: A Fully-Automated and Zero-Code Framework for LLM Agents",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05957.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "643b751cc5f633a7fa84b325",
      "avatarUrl": "/avatars/a094b856cf3d51eb78d16a14361def62.svg",
      "fullname": "Tang",
      "name": "Jiabin99",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.06764",
      "authors": [
        {
          "_id": "67aac6052c02e43558b6b4b0",
          "name": "Kiwhan Song",
          "hidden": false
        },
        {
          "_id": "67aac6052c02e43558b6b4b1",
          "name": "Boyuan Chen",
          "hidden": false
        },
        {
          "_id": "67aac6052c02e43558b6b4b2",
          "name": "Max Simchowitz",
          "hidden": false
        },
        {
          "_id": "67aac6052c02e43558b6b4b3",
          "name": "Yilun Du",
          "hidden": false
        },
        {
          "_id": "67aac6052c02e43558b6b4b4",
          "name": "Russ Tedrake",
          "hidden": false
        },
        {
          "_id": "67aac6052c02e43558b6b4b5",
          "name": "Vincent Sitzmann",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-10T18:44:25.000Z",
      "title": "歴史ガイドドライブディフューション",
      "summary": "フィルターフリーガイド（CFG）は、ディフュージョンモデルで条件付き生成を改善する重要な技術で、より正確な制御を可能にしながらサンプルの質を向上させる。この技術をビデオディフュージョンに拡張するのは自然なことであり、ビデオは変数の数のコンテキストフレームに基づいて生成され、全体として「歴史」と呼ばれるものです。しかし、歴史の長さが変動する場合のガイドを行うには、2つの重要な課題があります：構造が固定サイズの条件付きモデルをみたすものと、CFG風の歴史ドロップアウトが不良になる実験的な観察です。これらの問題に対処するために、ディフュージョンフォーシングトランスフォーマー（DFoT）を提案します。DFoTは、歴史フレームの数の柔軟な条件付きを可能にするビデオディフュージョン構造と理論的に基づく訓練目的を共有しています。次に、DFoTによって特有に可能なガイドメソッドの家族「歴史ガイド」を紹介します。その最も簡単な形式であるベージャー歴史ガイドでは、ビデオ生成の質と時間的な一貫性が大幅に向上します。より先進的な方法では、時間と周波数の間での歴史ガイドは動作の動的性を進め、分布外の歴史に対する構成的な一般化を可能にし、非常に長いビデオを安定して生成することができます。ウェブサイト：https://boyuan.space/history-guidance",
      "upvotes": 3,
      "discussionId": "67aac6072c02e43558b6b543"
    },
    "publishedAt": "2025-02-11T00:55:33.866Z",
    "title": "History-Guided Video Diffusion",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06764.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6017
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.06023",
      "authors": [
        {
          "_id": "67aac3a9ef5570c0c9047095",
          "user": {
            "_id": "640f6299ef5c6dcac8b1df52",
            "avatarUrl": "/avatars/022f21183abc8a8b5ce1b198d3ba96dc.svg",
            "isPro": false,
            "fullname": "Amir",
            "user": "sahsaeedi",
            "type": "user"
          },
          "name": "Amir Saeidi",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-02-11T03:31:48.492Z",
          "hidden": false
        },
        {
          "_id": "67aac3a9ef5570c0c9047096",
          "name": "Yiran Luo",
          "hidden": false
        },
        {
          "_id": "67aac3a9ef5570c0c9047097",
          "name": "Agneet Chatterjee",
          "hidden": false
        },
        {
          "_id": "67aac3a9ef5570c0c9047098",
          "name": "Shamanthak Hegde",
          "hidden": false
        },
        {
          "_id": "67aac3a9ef5570c0c9047099",
          "name": "Bimsara Pathiraja",
          "hidden": false
        },
        {
          "_id": "67aac3a9ef5570c0c904709a",
          "name": "Yezhou Yang",
          "hidden": false
        },
        {
          "_id": "67aac3a9ef5570c0c904709b",
          "name": "Chitta Baral",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-09T20:34:43.000Z",
      "title": "ディュアルキャプション好み最適化モデル",
      "summary": "最近、人間の好み最適化の進展は、本来、Large Language Models (LLMs) に開発されたもので、文から画像の拡散モデルの改善にも显著な可能性を示している。これらの方法は、好みの分布を学習しながら、それらとそれ以外の分布を区別することを目的としている。しかし、現在の好みデータセットは、これらの分布の重なりがあり、衝突分布となることが多い。また、我々は、入力プロンプトには、ノンプレフェレント画像にとっての関係ない情報が含まれていることを見出し、好み最適化手法でのノイズの正確な予測を制限することによって、ノンプレフェレントプロンプト問題として知られている。これらの課題を解決するために、我々は、Dual Caption Preference Optimization (DCPO) という新しいアプローチを提案している。これは、関係ないプロンプトを抑えるために、2つの異なるキャプションを使用する。衝突分布を解決するためには、Pick-Double Captionデータセットを提案している。これは、Pick-a-Pic v2の改良版で、好みの画像とノンプレフェレント画像に対して別々のキャプションを使用する。また、3つの異なるキャプション生成戦略を提案している：キャプチング、パーミューバティブ、ハイブリッドメソッド。我々の実験は、DCPOは画像の質とプロンプトの関連性を显著に改善し、Stable Diffusion (SD) 2.1、SFT_Chosen、Diffusion-DPO、MaPOを超えることを示し、Pickscore、HPSv2.1、GenEval、CLIPscore、ImageReward の複数のメトリックで、SD 2.1をバックボードとして微調節した。",
      "upvotes": 3,
      "discussionId": "67aac3b1ef5570c0c9047264"
    },
    "publishedAt": "2025-02-10T22:33:17.468Z",
    "title": "Dual Caption Preference Optimization for Diffusion Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06023.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "640f6299ef5c6dcac8b1df52",
      "avatarUrl": "/avatars/022f21183abc8a8b5ce1b198d3ba96dc.svg",
      "fullname": "Amir",
      "name": "sahsaeedi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.06060",
      "authors": [
        {
          "_id": "67ab1314385da1f07cda1271",
          "user": {
            "_id": "63abbf74ad514ca8d14a0548",
            "avatarUrl": "/avatars/b1357b73b8f9a8ff9908710ad64154ef.svg",
            "isPro": false,
            "fullname": "Bidipta Sarkar",
            "user": "bidiptas",
            "type": "user"
          },
          "name": "Bidipta Sarkar",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T09:51:17.933Z",
          "hidden": false
        },
        {
          "_id": "67ab1314385da1f07cda1272",
          "name": "Warren Xia",
          "hidden": false
        },
        {
          "_id": "67ab1314385da1f07cda1273",
          "name": "C. Karen Liu",
          "hidden": false
        },
        {
          "_id": "67ab1314385da1f07cda1274",
          "name": "Dorsa Sadigh",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-09T22:44:45.000Z",
      "title": "多アガント強化学習によるソーシャルデュケクション向け言語モデルの訓練",
      "summary": "自然言語でのコミュニケーションは、多効果的なツールであり、多エージェントの設定で、独立したエージェントが部分観測での情報を共有し、人間とのゼロショット協調を可能にします。しかし、先行研究は、ほとんどが、大規模な人間のデモンストレーションを基に訓練するか、自然的で役立つコミュニケーション戦略を生成する能力がないことに限定されています。本論文では、人間のデモンストレーションを含むものなさらに、自然言語で環境についての討論を行うことで、コミュニケーションを行うことを訓練します。コミュニケーション問題を聞き取りと話し方に分解し、エージェントの目標を利用して、世界についての有用な情報を予測することで、話し方をガイドするデンスリュートシグナルを生成することを基本的なアイデアとしています。特に、聞き取りのスキルを向上させるために、討論に基づいた環境についての情報を予測することで、モデルを訓練し、同時に、多エージェントの強化学習を用いて、エージェント間の影響に基づいたメッセージを報酬することで、話し方のスキルを向上させます。複雑な社会的な設定でのコミュニケーションの役割と必要性を調べるために、サンプラーブに基づく社会の説明ゲームを研究し、対抗的な不規則者の身分を調べることが重要な問題となります。我々の手法によって発生した現象を分析し、疑問者を指摘し、証拠を提供することなどの現象を見出し、これにより強い討論を可能にし、標準的なRLに比べて勝利率を倍にしました。我々のコードとモデルを以下のURLからリリースしています。https://socialdeductionllm.github.io/",
      "upvotes": 2,
      "discussionId": "67ab1315385da1f07cda12a5"
    },
    "publishedAt": "2025-02-11T04:08:55.672Z",
    "title": "Training Language Models for Social Deduction with Multi-Agent Reinforcement Learning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06060.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63abbf74ad514ca8d14a0548",
      "avatarUrl": "/avatars/b1357b73b8f9a8ff9908710ad64154ef.svg",
      "fullname": "Bidipta Sarkar",
      "name": "bidiptas",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.05795",
      "authors": [
        {
          "_id": "67ab189a8087b66340398b01",
          "name": "Wenfang Sun",
          "hidden": false
        },
        {
          "_id": "67ab189a8087b66340398b02",
          "name": "Xinyuan Song",
          "hidden": false
        },
        {
          "_id": "67ab189a8087b66340398b03",
          "user": {
            "_id": "64245f2c089d5fae56b4549a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64245f2c089d5fae56b4549a/qUHFsL9Svwyj5BKpfMtaY.jpeg",
            "isPro": false,
            "fullname": "Pengxiang Li",
            "user": "pengxiang",
            "type": "user"
          },
          "name": "Pengxiang Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T09:51:15.671Z",
          "hidden": false
        },
        {
          "_id": "67ab189a8087b66340398b04",
          "name": "Lu Yin",
          "hidden": false
        },
        {
          "_id": "67ab189a8087b66340398b05",
          "name": "Yefeng Zheng",
          "hidden": false
        },
        {
          "_id": "67ab189a8087b66340398b06",
          "name": "Shiwei Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-09T07:03:36.000Z",
      "title": "「大語言モデルの深さの詛咒」",
      "summary": "この論文では、Depth of Depthという概念を介して、現代の大規模な言語モデル（LLMs）で近半数のレイヤーが予期されないように低効果的であることを明らかにし、解釈し、解決策を提案します。まず、Llama、Mistral、DeepSeek、Qwenなどの最も人気のあるLLMsの家族でこの現象の広く存在することを確認します。理論的および実験的な分析により、LLMsの深いレイヤーの無効性の根本的な原因として、Pre-Layer Normalization（Pre-LN）の広く使用されていることを識別します。Pre-LNはTransformer LLMsの訓練を安定化しますが、その出力分散はモデルの深さに指数的に増加し、深いTransformerブロックの微分が恒等行列になり、訓練にほとんど貢献しないようになります。この訓練のポイントを解決するために、LayerNorm Scalingを提案します。LayerNorm Scalingはレイヤーノーマライゼーションの出力分散を深さの平方根の逆にスケールすることで、深いTransformerレイヤーの出力分散の爆発を抑え、貢献を向上させます。実験結果は130Mから1Bのモデルサイズを範囲にわたって、Pre-LNと比較してLayerNorm ScalingがLLMの予め学習性能を大幅に向上させることを示します。また、この向上は、LayerNorm Scalingが訓練中に深いレイヤーがより効果的に貢献することを許可していることによるものです。",
      "upvotes": 1,
      "discussionId": "67ab189b8087b66340398b3b"
    },
    "publishedAt": "2025-02-11T04:30:30.043Z",
    "title": "The Curse of Depth in Large Language Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05795.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64245f2c089d5fae56b4549a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64245f2c089d5fae56b4549a/qUHFsL9Svwyj5BKpfMtaY.jpeg",
      "fullname": "Pengxiang Li",
      "name": "pengxiang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  }
]