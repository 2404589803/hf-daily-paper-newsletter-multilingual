[
  {
    "paper": {
      "id": "2506.15675",
      "authors": [
        {
          "_id": "6853946599bf39f9665c79e0",
          "name": "Zhen Li",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79e1",
          "name": "Chuanhao Li",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79e2",
          "name": "Xiaofeng Mao",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79e3",
          "name": "Shaoheng Lin",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79e4",
          "name": "Ming Li",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79e5",
          "name": "Shitian Zhao",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79e6",
          "name": "Zhaopan Xu",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79e7",
          "name": "Xinyue Li",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79e8",
          "name": "Yukang Feng",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79e9",
          "name": "Jianwen Sun",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79ea",
          "name": "Zizhen Li",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79eb",
          "name": "Fanrui Zhang",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79ec",
          "name": "Jiaxin Ai",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79ed",
          "name": "Zhixiang Wang",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79ee",
          "name": "Yuwei Wu",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79ef",
          "name": "Tong He",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79f0",
          "name": "Jiangmiao Pang",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79f1",
          "name": "Yu Qiao",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79f2",
          "name": "Yunde Jia",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79f3",
          "user": {
            "_id": "63527f4e7d071f23d085ad45",
            "avatarUrl": "/avatars/99a51adef5673b3ac1a8c02eb47759c4.svg",
            "isPro": false,
            "fullname": "KAIPENG ZHANG",
            "user": "kpzhang",
            "type": "user"
          },
          "name": "Kaipeng Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-19T10:09:38.100Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/65f1713552c38a91e0a445e8/GFrjhPvZnILgeTd_w2kvc.mp4"
      ],
      "publishedAt": "2025-06-18T17:57:06.000Z",
      "submittedOnDailyAt": "2025-06-19T03:16:13.113Z",
      "title": "世界: 世界探査向けのビデオデータセット",
      "submittedOnDailyBy": {
        "_id": "65f1713552c38a91e0a445e8",
        "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
        "isPro": false,
        "fullname": "kaipeng",
        "user": "kpzhang996",
        "type": "user"
      },
      "summary": "ビデオ生成技術は驚異的な進歩を遂げ、相互作用世界探索の基盤となるべきである。しかし、現在のビデオ生成データセットは、世界探索トレーニングに適していません。それらは、限られた場所、短い時間、静的な場面、探索と世界に関する注釈の欠如などの制限を見られます。本論文では、日本語で「世界」という意味を持つ「Sekai」という高品質な第一人称視点の世界ビデオデータセットを紹介します。これは、100以上の国や地域から750以上の市での歩行やドライネーの視点（FPVとUVA）のビデオを5,000時間以上にわたし、豊富な世界探索に関する注釈を含むものです。ビデオの収集、前処理、注釈によるアノテーションに効率的かつ有効なツールボックスを開発しました。実験はデータセットの品質を示し、一部を用いて、名前が「YUME」（日本語で「夢」という意味）という名前の相互作用ビデオ世界探索モデルをトレーニングしました。Sekaiは、ビデオ生成と世界探索の領域に裨益を与え、有價価のアプリケーションを励ますことを信じています。",
      "upvotes": 26,
      "discussionId": "6853946599bf39f9665c79f4",
      "projectPage": "https://lixsp11.github.io/sekai-project/",
      "githubRepo": "https://github.com/Lixsp11/sekai-codebase",
      "ai_summary": "Sekai, a worldwide video dataset with comprehensive annotations, is introduced to support world exploration applications, enhancing video generation models.",
      "ai_keywords": [
        "first-person view",
        "worldwide video dataset",
        "rich annotations",
        "FPV",
        "UVA",
        "video collection",
        "pre-processing",
        "camera trajectories",
        "interactive video world exploration model"
      ]
    },
    "publishedAt": "2025-06-18T13:57:06.000Z",
    "title": "Sekai: A Video Dataset towards World Exploration",
    "summary": "Video generation techniques have made remarkable progress, promising to be\nthe foundation of interactive world exploration. However, existing video\ngeneration datasets are not well-suited for world exploration training as they\nsuffer from some limitations: limited locations, short duration, static scenes,\nand a lack of annotations about exploration and the world. In this paper, we\nintroduce Sekai (meaning ``world'' in Japanese), a high-quality first-person\nview worldwide video dataset with rich annotations for world exploration. It\nconsists of over 5,000 hours of walking or drone view (FPV and UVA) videos from\nover 100 countries and regions across 750 cities. We develop an efficient and\neffective toolbox to collect, pre-process and annotate videos with location,\nscene, weather, crowd density, captions, and camera trajectories. Experiments\ndemonstrate the quality of the dataset. And, we use a subset to train an\ninteractive video world exploration model, named YUME (meaning ``dream'' in\nJapanese). We believe Sekai will benefit the area of video generation and world\nexploration, and motivate valuable applications.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/65f1713552c38a91e0a445e8/GFrjhPvZnILgeTd_w2kvc.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.15675.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65f1713552c38a91e0a445e8",
      "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
      "fullname": "kaipeng",
      "name": "kpzhang996",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.15681",
      "authors": [
        {
          "_id": "68536fc899bf39f9665c7961",
          "user": {
            "_id": "657152eb12f162153b50ec9d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/657152eb12f162153b50ec9d/qnldHP35PclV0pDz_05q8.jpeg",
            "isPro": false,
            "fullname": "Byung-Kwan Lee",
            "user": "BK-Lee",
            "type": "user"
          },
          "name": "Byung-Kwan Lee",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-19T09:10:25.415Z",
          "hidden": false
        },
        {
          "_id": "68536fc899bf39f9665c7962",
          "user": {
            "_id": "65b33e5f7cd0069ad648c4e8",
            "avatarUrl": "/avatars/1a746ea535cffa92ea08006e05ea414a.svg",
            "isPro": false,
            "fullname": "Ryo Hachiuma",
            "user": "rhachiuma",
            "type": "user"
          },
          "name": "Ryo Hachiuma",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-19T02:02:49.546Z",
          "hidden": false
        },
        {
          "_id": "68536fc899bf39f9665c7963",
          "name": "Yong Man Ro",
          "hidden": false
        },
        {
          "_id": "68536fc899bf39f9665c7964",
          "name": "Yu-Chiang Frank Wang",
          "hidden": false
        },
        {
          "_id": "68536fc899bf39f9665c7965",
          "name": "Yueh-Hua Wu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/657152eb12f162153b50ec9d/2xNUivkkqkJWJLWtIPNvb.mp4"
      ],
      "publishedAt": "2025-06-18T17:59:49.000Z",
      "submittedOnDailyAt": "2025-06-19T00:36:10.331Z",
      "title": "GenRecal: 大きさから小さきへの再調整後の生成\nビジョン-言語モデル",
      "submittedOnDailyBy": {
        "_id": "657152eb12f162153b50ec9d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/657152eb12f162153b50ec9d/qnldHP35PclV0pDz_05q8.jpeg",
        "isPro": false,
        "fullname": "Byung-Kwan Lee",
        "user": "BK-Lee",
        "type": "user"
      },
      "summary": "最近の視覚言語モデル（VLMs）の進展は、GPT-4Vといったクローズドソースシステムと同等の性能を達成するために、大規模言語モデル（LLMs）を活用しています。しかし、これらのモデルがリアルウェアモデルでの実際の場合に機能すること、特に資源制限されたデバイス上での機能を実現することは、大規模な計算要求により難しいことです。これにより、知識を大規模なVLMsから小さくしたり、効率的にすることに興味がありました。VLMsの構造の多様性がここで重要な課題となります。それは、異なるLLMsを基に構築され、単語サイズ、単語分割、単語インデックス順序が異なるターゲット型の変化を含むものです。特定のVLMタイプに限定された制限を解決するために、我々は、Generation after Recalibration（GenRecal）を紹介します。GenRecalは、異なるVLMsの間で特徴表現を調整し、適応させるリカレイターを挟む新しい一般的な用途の発熱フレームワークです。GenRecalは、多くの難しいベンチマークでの拡散的な実験を通じて、基準性能を大幅に向上させ、最終的に、大規模な開放ソースやクローズドソースのVLMsを上回ることを示しました。",
      "upvotes": 13,
      "discussionId": "68536fc899bf39f9665c7966",
      "projectPage": "https://byungkwanlee.github.io/GenRecal-page/",
      "ai_summary": "GenRecal, a novel distillation framework, improves performance of vision-language models by aligning feature representations across different architectures.",
      "ai_keywords": [
        "vision-language models",
        "large language models",
        "distillation framework",
        "GenerRecal",
        "recalibration",
        "feature representations",
        "heterogeneous VLMs"
      ]
    },
    "publishedAt": "2025-06-18T13:59:49.000Z",
    "title": "GenRecal: Generation after Recalibration from Large to Small\n  Vision-Language Models",
    "summary": "Recent advancements in vision-language models (VLMs) have leveraged large\nlanguage models (LLMs) to achieve performance on par with closed-source systems\nlike GPT-4V. However, deploying these models in real-world scenarios,\nparticularly on resource-constrained devices, remains challenging due to their\nsubstantial computational demands. This has spurred interest in distilling\nknowledge from large VLMs into smaller, more efficient counterparts. A key\nchallenge arises here from the diversity of VLM architectures, which are built\non different LLMs and employ varying token types-differing in vocabulary size,\ntoken splits, and token index ordering. To address this challenge of limitation\nto a specific VLM type, we present Generation after Recalibration (GenRecal), a\nnovel, general-purpose distillation framework for VLMs. GenRecal incorporates a\nRecalibrator that aligns and adapts feature representations between\nheterogeneous VLMs, enabling effective knowledge transfer across different\ntypes of VLMs. Through extensive experiments on multiple challenging\nbenchmarks, we demonstrate that GenRecal significantly improves baseline\nperformances, eventually outperforming large-scale open- and closed-source\nVLMs.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/657152eb12f162153b50ec9d/2xNUivkkqkJWJLWtIPNvb.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.15681.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "657152eb12f162153b50ec9d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/657152eb12f162153b50ec9d/qnldHP35PclV0pDz_05q8.jpeg",
      "fullname": "Byung-Kwan Lee",
      "name": "BK-Lee",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 56
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.15677",
      "authors": [
        {
          "_id": "68536b2399bf39f9665c794c",
          "name": "Yining Hong",
          "hidden": false
        },
        {
          "_id": "68536b2399bf39f9665c794d",
          "name": "Rui Sun",
          "hidden": false
        },
        {
          "_id": "68536b2399bf39f9665c794e",
          "name": "Bingxuan Li",
          "hidden": false
        },
        {
          "_id": "68536b2399bf39f9665c794f",
          "name": "Xingcheng Yao",
          "hidden": false
        },
        {
          "_id": "68536b2399bf39f9665c7950",
          "name": "Maxine Wu",
          "hidden": false
        },
        {
          "_id": "68536b2399bf39f9665c7951",
          "name": "Alexander Chien",
          "hidden": false
        },
        {
          "_id": "68536b2399bf39f9665c7952",
          "name": "Da Yin",
          "hidden": false
        },
        {
          "_id": "68536b2399bf39f9665c7953",
          "name": "Ying Nian Wu",
          "hidden": false
        },
        {
          "_id": "68536b2399bf39f9665c7954",
          "name": "Zhecan James Wang",
          "hidden": false
        },
        {
          "_id": "68536b2399bf39f9665c7955",
          "name": "Kai-Wei Chang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-18T17:58:17.000Z",
      "submittedOnDailyAt": "2025-06-19T00:14:22.240Z",
      "title": "Embodied Web Agents: 物理デジタル領域を結ぶ統合的なアガント知能",
      "submittedOnDailyBy": {
        "_id": "6431b64df76c34519e93d1ba",
        "avatarUrl": "/avatars/ea577762b6b4798f87a7a3f1d53d082c.svg",
        "isPro": true,
        "fullname": "Yining Hong",
        "user": "evelynhong",
        "type": "user"
      },
      "summary": "現在のAIアガントは主にセコード化されています：それは、オンラインから得られる巨大なデジタル情報と知識を検索し、理由を論じることができます；また、体験的な認識、計画、アクションを通じて物理的な世界と相互作用することができますが、両方ともできることはよくありません。この分離は、物理的とデジタルの知能を統合してタスクを解決する能力を制限しています。例えば、オンラインレシピをもとに烹飪すること、動的なマップデータを用いてナビゲートすること、またはウェブデータを用いてリアルウォールの地標を解釈することなどです。私たちは、体験的な知識とホームセールススケールの理由を流暢に結びつける新しいパラダイムを導入します。この概念を実行するためには、まず、写実的な3D室内と外の環境と機能的なホームセールスインターフェースを緊密に結合した統一的なシミュレーションプラットフォームを開発します。このプラットフォームに基づいて、私たちは、烹飪、ナビゲート、購入、観光、地理位置などの多様なタスクを構築し、リリースします。これらのタスクは、物理的とデジタルの領域を統合した理由を必要とし、横断領域の知能のシステマティックな評価を行うために必要とされます。実験結果は、最先端のAIシステムと人間の能力の間に大きな性能間違いを示し、体験的な認識とホームセールススケールの知識アクセスの交点での課題と機会を明らかにします。すべてのデータセット、コード、ウェブサイトは、私たちのプロジェクトページ（https://embodied-web-agent.github.io/）で公開しています。",
      "upvotes": 7,
      "discussionId": "68536b2399bf39f9665c7956",
      "ai_summary": "Embodied Web Agents integrate physical interaction and web-scale reasoning to assess cross-domain intelligence in a novel benchmark environment.",
      "ai_keywords": [
        "Embodied Web Agents",
        "task environments",
        "simulation platform",
        "3D indoor and outdoor environments",
        "functional web interfaces",
        "Embodied Web Agents Benchmark",
        "systematic assessment",
        "cross-domain intelligence",
        "embodied cognition"
      ]
    },
    "publishedAt": "2025-06-18T13:58:17.000Z",
    "title": "Embodied Web Agents: Bridging Physical-Digital Realms for Integrated\n  Agent Intelligence",
    "summary": "AI agents today are mostly siloed - they either retrieve and reason over vast\namount of digital information and knowledge obtained online; or interact with\nthe physical world through embodied perception, planning and action - but\nrarely both. This separation limits their ability to solve tasks that require\nintegrated physical and digital intelligence, such as cooking from online\nrecipes, navigating with dynamic map data, or interpreting real-world landmarks\nusing web knowledge. We introduce Embodied Web Agents, a novel paradigm for AI\nagents that fluidly bridge embodiment and web-scale reasoning. To\noperationalize this concept, we first develop the Embodied Web Agents task\nenvironments, a unified simulation platform that tightly integrates realistic\n3D indoor and outdoor environments with functional web interfaces. Building\nupon this platform, we construct and release the Embodied Web Agents Benchmark,\nwhich encompasses a diverse suite of tasks including cooking, navigation,\nshopping, tourism, and geolocation - all requiring coordinated reasoning across\nphysical and digital realms for systematic assessment of cross-domain\nintelligence. Experimental results reveal significant performance gaps between\nstate-of-the-art AI systems and human capabilities, establishing both\nchallenges and opportunities at the intersection of embodied cognition and\nweb-scale knowledge access. All datasets, codes and websites are publicly\navailable at our project page https://embodied-web-agent.github.io/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.15677.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6431b64df76c34519e93d1ba",
      "avatarUrl": "/avatars/ea577762b6b4798f87a7a3f1d53d082c.svg",
      "fullname": "Yining Hong",
      "name": "evelynhong",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.15068",
      "authors": [
        {
          "_id": "68536bf399bf39f9665c7958",
          "name": "Zongxia Li",
          "hidden": false
        },
        {
          "_id": "68536bf399bf39f9665c7959",
          "name": "Yapei Chang",
          "hidden": false
        },
        {
          "_id": "68536bf399bf39f9665c795a",
          "name": "Yuhang Zhou",
          "hidden": false
        },
        {
          "_id": "68536bf399bf39f9665c795b",
          "name": "Xiyang Wu",
          "hidden": false
        },
        {
          "_id": "68536bf399bf39f9665c795c",
          "name": "Zichao Liang",
          "hidden": false
        },
        {
          "_id": "68536bf399bf39f9665c795d",
          "name": "Yoo Yeon Sung",
          "hidden": false
        },
        {
          "_id": "68536bf399bf39f9665c795e",
          "name": "Jordan Lee Boyd-Graber",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-18T02:16:53.000Z",
      "submittedOnDailyAt": "2025-06-19T04:04:25.155Z",
      "title": "セマンティクスに関心のある報酬の設定と自由形式の開放的な学習のR1トレーニングにおける対応",
      "submittedOnDailyBy": {
        "_id": "64ea62f918d79efd533c93fe",
        "avatarUrl": "/avatars/9985a789ce11b788de2cba12adfb72fc.svg",
        "isPro": false,
        "fullname": "Xiyang Wu",
        "user": "wuxiyang",
        "type": "user"
      },
      "summary": "開放終端の長文生成の評価は難しいです。それは、良いと悪い出力を明確に区別することが難しいからです。現在の方法は、一致性、スタイル、関連性などのキー的な面を間違えてしまい、またトレーニングデータによって偏っていることも多いです。このようなものが存在するため、開放終端の長文生成の評価は調査不足の問題です。この空間を填ぐために、私たちはPrefBERTを提案します。PrefBERTは、GRPOでの開放終端の長文生成を評価するスコアモデルで、良いと悪い出力に対して異なる報酬を与えることでトレーニングをガイドします。PrefBERTは、二つの多様な長文スタイルの回答評価データセットとLikert評価の品質をもって訓練されています。これにより、PrefBERTは、ローカルやBERTScoreよりもより良い意味的な報酬フィードバックを提供し、GRPOをより効果的にサポートします。LLM-as-a-judge、人間の評価、そして定性的な分析を通じて、PrefBERTは、多文句と段落長の回答によって訓練され、長いパスケースにも穩健であり、GRPOに必要な確認可能な報酬と一致していることを示します。人間の評価は、PrefBERTを報酬信号としてトレーニングポリシーモデルを行うことで、人間の好みによりよく合わせた回答を得ることができることを確認しました。コードは、https://github.com/zli12321/long_form_rlに公開されています。",
      "upvotes": 7,
      "discussionId": "68536bf399bf39f9665c795f",
      "ai_summary": "PrefBERT, a scoring model, improves open-ended long-form generation by providing better semantic reward feedback than traditional metrics.",
      "ai_keywords": [
        "PrefBERT",
        "GRPO",
        "multi-sentence responses",
        "paragraph-length responses",
        "Likert-rated quality",
        "LLM-as-a-judge",
        "human ratings",
        "qualitative analysis",
        "verifiable rewards"
      ]
    },
    "publishedAt": "2025-06-17T22:16:53.000Z",
    "title": "Semantically-Aware Rewards for Open-Ended R1 Training in Free-Form\n  Generation",
    "summary": "Evaluating open-ended long-form generation is challenging because it is hard\nto define what clearly separates good from bad outputs. Existing methods often\nmiss key aspects like coherence, style, or relevance, or are biased by\npretraining data, making open-ended long-form evaluation an underexplored\nproblem. To address this gap, we propose PrefBERT, a scoring model for\nevaluating open-ended long-form generation in GRPO and guiding its training\nwith distinct rewards for good and bad outputs. Trained on two response\nevaluation datasets with diverse long-form styles and Likert-rated quality,\nPrefBERT effectively supports GRPO by offering better semantic reward feedback\nthan traditional metrics ROUGE-L and BERTScore do. Through comprehensive\nevaluations, including LLM-as-a-judge, human ratings, and qualitative analysis,\nwe show that PrefBERT, trained on multi-sentence and paragraph-length\nresponses, remains reliable across varied long passages and aligns well with\nthe verifiable rewards GRPO needs. Human evaluations confirm that using\nPrefBERT as the reward signal to train policy models yields responses better\naligned with human preferences than those trained with traditional metrics. Our\ncode is available at https://github.com/zli12321/long_form_rl.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.15068.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ea62f918d79efd533c93fe",
      "avatarUrl": "/avatars/9985a789ce11b788de2cba12adfb72fc.svg",
      "fullname": "Xiyang Wu",
      "name": "wuxiyang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.15569",
      "authors": [
        {
          "_id": "68537ca999bf39f9665c799a",
          "name": "Chengye Wang",
          "hidden": false
        },
        {
          "_id": "68537ca999bf39f9665c799b",
          "name": "Yifei Shen",
          "hidden": false
        },
        {
          "_id": "68537ca999bf39f9665c799c",
          "name": "Zexi Kuang",
          "hidden": false
        },
        {
          "_id": "68537ca999bf39f9665c799d",
          "name": "Arman Cohan",
          "hidden": false
        },
        {
          "_id": "68537ca999bf39f9665c799e",
          "user": {
            "_id": "62f662bcc58915315c4eccea",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
            "isPro": true,
            "fullname": "Yilun Zhao",
            "user": "yilunzhao",
            "type": "user"
          },
          "name": "Yilun Zhao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-19T09:10:23.732Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-18T15:43:26.000Z",
      "submittedOnDailyAt": "2025-06-19T01:28:40.934Z",
      "title": "SciVer: 科学プロテストの多模構造モデルの評価",
      "submittedOnDailyBy": {
        "_id": "62f662bcc58915315c4eccea",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
        "isPro": true,
        "fullname": "Yilun Zhao",
        "user": "yilunzhao",
        "type": "user"
      },
      "summary": "SciVerは、多モデル科学的なコンテキストでの主張の証明能力を評価するために設計された最初のベンチマークです。SciVerは、1,113 論文にわたる3,000例の専門家のアノテーションを含み、多モデル科学的な主張の証明での共通の理由論の4つのサブセットを収めています。フィンエグライン評価を可能にするために、各例に専門家のアノテーションされた補助証拠を含みます。21つの最先端の多モデルの性能を評価します。o4-mini、Gemini-2.5-Flash、Llama-3.2-Vision、Qwen2.5-VLを含みます。SciVerでのモデルと人間の専門家の間では、大きな性能間隔が見られます。検索アウゲーション生成（RAG）の詳細な分析と人間が行った誤り評価を通じて、現在のオープンソースモデルの重要な制限を特定し、多モデル科学文献タスクでのモデルの理解と理由論の進歩についてのキーのヒントを提供します。",
      "upvotes": 6,
      "discussionId": "68537ca999bf39f9665c799f",
      "githubRepo": "https://github.com/QDRhhhh/SciVer",
      "ai_summary": "A benchmark named SciVer evaluates multimodal foundation models' claim verification capabilities within scientific contexts, revealing performance gaps and limitations in current models.",
      "ai_keywords": [
        "retrieval-augmented generation (RAG)"
      ]
    },
    "publishedAt": "2025-06-18T11:43:26.000Z",
    "title": "SciVer: Evaluating Foundation Models for Multimodal Scientific Claim\n  Verification",
    "summary": "We introduce SciVer, the first benchmark specifically designed to evaluate\nthe ability of foundation models to verify claims within a multimodal\nscientific context. SciVer consists of 3,000 expert-annotated examples over\n1,113 scientific papers, covering four subsets, each representing a common\nreasoning type in multimodal scientific claim verification. To enable\nfine-grained evaluation, each example includes expert-annotated supporting\nevidence. We assess the performance of 21 state-of-the-art multimodal\nfoundation models, including o4-mini, Gemini-2.5-Flash, Llama-3.2-Vision, and\nQwen2.5-VL. Our experiment reveals a substantial performance gap between these\nmodels and human experts on SciVer. Through an in-depth analysis of\nretrieval-augmented generation (RAG), and human-conducted error evaluations, we\nidentify critical limitations in current open-source models, offering key\ninsights to advance models' comprehension and reasoning in multimodal\nscientific literature tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.15569.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62f662bcc58915315c4eccea",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
      "fullname": "Yilun Zhao",
      "name": "yilunzhao",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 13
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.15050",
      "authors": [
        {
          "_id": "68539c6199bf39f9665c79f6",
          "name": "Tiantian Fan",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c79f7",
          "name": "Lingjun Liu",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c79f8",
          "name": "Yu Yue",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c79f9",
          "name": "Jiaze Chen",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c79fa",
          "name": "Chengyi Wang",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c79fb",
          "name": "Qiying Yu",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c79fc",
          "name": "Chi Zhang",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c79fd",
          "name": "Zhiqi Lin",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c79fe",
          "name": "Ruofei Zhu",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c79ff",
          "name": "Yufeng Yuan",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c7a00",
          "name": "Xiaochen Zuo",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c7a01",
          "name": "Bole Ma",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c7a02",
          "name": "Mofan Zhang",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c7a03",
          "name": "Gaohong Liu",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c7a04",
          "name": "Ru Zhang",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c7a05",
          "name": "Haotian Zhou",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c7a06",
          "name": "Cong Xie",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c7a07",
          "name": "Ruidong Zhu",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c7a08",
          "name": "Zhi Zhang",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c7a09",
          "name": "Xin Liu",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c7a0a",
          "name": "Mingxuan Wang",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c7a0b",
          "name": "Lin Yan",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c7a0c",
          "name": "Yonghui Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-18T01:21:38.000Z",
      "submittedOnDailyAt": "2025-06-19T03:43:49.620Z",
      "title": "Truncated Proximal Policy Optimization",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "最近、検証時スケーリングラージュ・ラング・ラング・モデル（LLMs）は、科学と専門的な仕事において、長いコンショーズ（CoT）を生成して、例外的な理由論能力を示しています。これらの理由論モデルの開発の重要な構成要素として、Proximal Policy Optimization（PPO）やその変異体を示す強化学習（RL）が、モデルが試行錯誤で学習することを許可しています。しかし、PPOは固有のon-policyなことで時間がかかることがあり、これは長い回答の長さが増加してさらに悪化しています。本稿では、Truncated Proximal Policy Optimization（T-PPO）を提案します。T-PPOは、政策更新と長さ制限された回答の生成をストリーミングして、学習効率を向上させるPPOの新しい拡張です。T-PPOは、完全同期化された長い生成プロセスの固有の欠点である低いハードウェア利用率の問題を軽減します。貢献は二つです。まず、不完全な回答からの優位度の推定を行うExtended Generalized Advantage Estimation（EGAE）を提案し、政策学習の整一体を維持します。次に、政策モデルと価値モデルの独立な最適化を可能にし、計算的に最適化された機構を設計します。この機構は、選択的なプロンプトとトークンのフィルタリングを通じて、冗長な計算を減らし、収束性能を犠牲にしないまま学習プロセスを加速します。AIME 2024での実験結果から、T-PPOは理由論LLMsの学習効率を2.5倍程度に向上させ、現在のコンピューターでは見られない効果を示しています。",
      "upvotes": 4,
      "discussionId": "68539c6199bf39f9665c7a0d",
      "ai_summary": "T-PPO, an extension of PPO, improves training efficiency for Large Language Models by optimizing policy updates and utilizing hardware resources more effectively.",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "chains-of-thought (CoT)",
        "reinforcement learning (RL)",
        "Proximal Policy Optimization (PPO)",
        "Truncated Proximal Policy Optimization (T-PPO)",
        "Extended Generalized Advantage Estimation (EGAE)",
        "advantage estimation",
        "policy and value models",
        "independent optimization",
        "prompt and truncated tokens",
        "AIME 2024",
        "base model"
      ]
    },
    "publishedAt": "2025-06-17T21:21:38.000Z",
    "title": "Truncated Proximal Policy Optimization",
    "summary": "Recently, test-time scaling Large Language Models (LLMs) have demonstrated\nexceptional reasoning capabilities across scientific and professional tasks by\ngenerating long chains-of-thought (CoT). As a crucial component for developing\nthese reasoning models, reinforcement learning (RL), exemplified by Proximal\nPolicy Optimization (PPO) and its variants, allows models to learn through\ntrial and error. However, PPO can be time-consuming due to its inherent\non-policy nature, which is further exacerbated by increasing response lengths.\nIn this work, we propose Truncated Proximal Policy Optimization (T-PPO), a\nnovel extension to PPO that improves training efficiency by streamlining policy\nupdate and length-restricted response generation. T-PPO mitigates the issue of\nlow hardware utilization, an inherent drawback of fully synchronized\nlong-generation procedures, where resources often sit idle during the waiting\nperiods for complete rollouts. Our contributions are two-folds. First, we\npropose Extended Generalized Advantage Estimation (EGAE) for advantage\nestimation derived from incomplete responses while maintaining the integrity of\npolicy learning. Second, we devise a computationally optimized mechanism that\nallows for the independent optimization of the policy and value models. By\nselectively filtering prompt and truncated tokens, this mechanism reduces\nredundant computations and accelerates the training process without sacrificing\nconvergence performance. We demonstrate the effectiveness and efficacy of T-PPO\non AIME 2024 with a 32B base model. The experimental results show that T-PPO\nimproves the training efficiency of reasoning LLMs by up to 2.5x and\noutperforms its existing competitors.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.15050.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 88
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.06279",
      "authors": [
        {
          "_id": "6850e0285e07650ecce890f3",
          "user": {
            "_id": "637f22d27119bd030dfd4af8",
            "avatarUrl": "/avatars/55c468c40ad3bd3218d086759fb1be3c.svg",
            "isPro": false,
            "fullname": "Shi Liu",
            "user": "CLLBJ16",
            "type": "user"
          },
          "name": "Shi Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-18T12:16:48.071Z",
          "hidden": false
        },
        {
          "_id": "6850e0285e07650ecce890f4",
          "user": {
            "_id": "63e4562f9db5da2dc1f3b520",
            "avatarUrl": "/avatars/f4eecf1396b05e1c72436e7026d85cef.svg",
            "isPro": false,
            "fullname": "Weijie Su",
            "user": "jackroos",
            "type": "user"
          },
          "name": "Weijie Su",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-19T09:10:33.557Z",
          "hidden": false
        },
        {
          "_id": "6850e0285e07650ecce890f5",
          "name": "Xizhou Zhu",
          "hidden": false
        },
        {
          "_id": "6850e0285e07650ecce890f6",
          "name": "Wenhai Wang",
          "hidden": false
        },
        {
          "_id": "6850e0285e07650ecce890f7",
          "name": "Jifeng Dai",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-06T17:59:06.000Z",
      "submittedOnDailyAt": "2025-06-19T00:24:25.743Z",
      "title": "CoMemo: LVLMs は画像コンテキストと画像メモリを必要とする",
      "submittedOnDailyBy": {
        "_id": "637f22d27119bd030dfd4af8",
        "avatarUrl": "/avatars/55c468c40ad3bd3218d086759fb1be3c.svg",
        "isPro": false,
        "fullname": "Shi Liu",
        "user": "CLLBJ16",
        "type": "user"
      },
      "summary": "最新の大規模ビジュアル言語モデル（LVLM）は、大規模言語モデル（LLM）に基づいて進化し、視覚特徴とLLMの表現を一致させることを主なパラダイムとして確立しました。しかし、これらのLVLMは、多タイプ処理に最適な構造を持っていない傾向があります。まず、LVLMはアテンション割り当てに二つの分布を示し、コンテキストが拡大するにつれて中間の視覚内容が進行的に視覚情報を無視することになります。そして、単なる位置付けエンコーディングシナプスは、動的な高解像度画像の処理において2次元の構造的関係を保存することができません。これらの制限を解決するために、私たちはCoMemoというダブルパス構造を提案します。これは、コンテキスト画像パスと画像メモパスを組み合わせたもので、視覚情報の無視を効果的に軽減します。また、私たちはRoPE-DHRという新しい位置付けエンコーディング機構を導入します。これは、サムネイルベースの位置付けアグレギュレーションを使用して、長いシーケンスでの遠隔的な減衰を抑えながら2次元の空間意識を維持します。長コンテキスト理解、多画像推理、視覚問答テストなどの7つのベンチマークでの評価により、CoMemoは傳統的なLVLM構造に比べて上位の性能を示します。プロジェクトページは、https://lalbj.github.io/projects/CoMemo/ から利用できます。",
      "upvotes": 4,
      "discussionId": "6850e0295e07650ecce890f8",
      "projectPage": "https://lalbj.github.io/projects/CoMemo/",
      "githubRepo": "https://github.com/LALBJ/CoMemo",
      "ai_summary": "CoMemo addresses visual information neglect and spatial awareness in multimodal processing by using a dual-path architecture and a novel positional encoding mechanism.",
      "ai_keywords": [
        "Large Vision-Language Models",
        "Large Language Models",
        "multimodal processing",
        "bimodal distribution",
        "attention allocation",
        "middle visual content",
        "positional encoding",
        "visual processing",
        "image Memory path",
        "RoPE-DHR",
        "positional aggregation",
        "2D structural relationships",
        "spatial awareness",
        "remote decay",
        "long-context comprehension",
        "multi-image reasoning",
        "visual question answering"
      ]
    },
    "publishedAt": "2025-06-06T13:59:06.000Z",
    "title": "CoMemo: LVLMs Need Image Context with Image Memory",
    "summary": "Recent advancements in Large Vision-Language Models built upon Large Language\nModels have established aligning visual features with LLM representations as\nthe dominant paradigm. However, inherited LLM architectural designs introduce\nsuboptimal characteristics for multimodal processing. First, LVLMs exhibit a\nbimodal distribution in attention allocation, leading to the progressive\nneglect of middle visual content as context expands. Second, conventional\npositional encoding schemes fail to preserve vital 2D structural relationships\nwhen processing dynamic high-resolution images. To address these limitations,\nwe propose CoMemo - a dual-path architecture that combines a Context image path\nwith an image Memory path for visual processing, effectively alleviating visual\ninformation neglect. Additionally, we introduce RoPE-DHR, a novel positional\nencoding mechanism that employs thumbnail-based positional aggregation to\nmaintain 2D spatial awareness while mitigating remote decay in extended\nsequences. Evaluations across seven benchmarks,including long-context\ncomprehension, multi-image reasoning, and visual question answering,\ndemonstrate CoMemo's superior performance compared to conventional LVLM\narchitectures. Project page is available at\nhttps://lalbj.github.io/projects/CoMemo/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.06279.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "637f22d27119bd030dfd4af8",
      "avatarUrl": "/avatars/55c468c40ad3bd3218d086759fb1be3c.svg",
      "fullname": "Shi Liu",
      "name": "CLLBJ16",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.15672",
      "authors": [
        {
          "_id": "6853bbcc99bf39f9665c7a50",
          "name": "Yao Zhang",
          "hidden": false
        },
        {
          "_id": "6853bbcc99bf39f9665c7a51",
          "name": "Chenyang Lin",
          "hidden": false
        },
        {
          "_id": "6853bbcc99bf39f9665c7a52",
          "name": "Shijie Tang",
          "hidden": false
        },
        {
          "_id": "6853bbcc99bf39f9665c7a53",
          "name": "Haokun Chen",
          "hidden": false
        },
        {
          "_id": "6853bbcc99bf39f9665c7a54",
          "name": "Shijie Zhou",
          "hidden": false
        },
        {
          "_id": "6853bbcc99bf39f9665c7a55",
          "name": "Yunpu Ma",
          "hidden": false
        },
        {
          "_id": "6853bbcc99bf39f9665c7a56",
          "name": "Volker Tresp",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-18T17:54:55.000Z",
      "submittedOnDailyAt": "2025-06-19T06:26:51.897Z",
      "title": "セワームアゲンティック：セワームインテリジェンスをもとに完全自動化されたアゲンティックシステム生成への向け",
      "submittedOnDailyBy": {
        "_id": "648cbea3dee03837c823cbf2",
        "avatarUrl": "/avatars/3f8c36436a5cbff2948df099ae604418.svg",
        "isPro": false,
        "fullname": "Shuo Chen",
        "user": "ShuoChen99",
        "type": "user"
      },
      "summary": "大語言モデルの急速な進歩は、決策、協調、タスク実行におけるアウトプットシステムを進化させています。しかし、現在のアウトプットシステムの生成フレームワークは完全な自律性を欠く、スクラッチからアウトプットの生成、自動最適化のアウトプット機能、コラボレーションの欠落により、適応性とスケーラビリティを制限しています。私たちは、全自動化のアウトプットシステムの生成を実現するSwarmAgenticフレームワークを提案します。このフレームワークは、言語駆動の探索により、スクラッチからアウトプットシステムを構築し、アウトプット機能とコラボレーションを相互依存した成分として共同最適化します。システムレベル構造の効率的な探索を可能にするために、SwarmAgenticは候補システムの集団を保持し、フィードバックガイドされた更新により進化させ、パーティクルソース最適化（PSO）からのインスピレーションを受けます。私たちの方法は、高レベルのプランニング、システムレベルの協調、創造的な理由に関する6つの実世界的な、開放終端的な、探索的なタスクにおいて評価されます。タスクの説明と目的関数のみを与えると、SwarmAgenticはすべてのベースラインを超え、TravelPlannerベンチマークでADASに対して+261.8%の相対改善を達成し、構造的に制限されないタスクでの全自動化の効果を強調します。このフレームワークは、可換性と自律性のあるアウトプットシステムの設計に向けて重要なステップを示し、集団知識と全自動化のシステム多アウトプット生成をつなげています。私たちのコードは、https://yaoz720.github.io/SwarmAgentic/ で公開されています。",
      "upvotes": 2,
      "discussionId": "6853bbcc99bf39f9665c7a57",
      "ai_summary": "SwarmAgentic is a framework for automated agentic system generation that optimize agent functionality and collaboration through language-driven exploration, outperforming existing baselines in unconstrained tasks.",
      "ai_keywords": [
        "Large Language Models",
        "agentic systems",
        "from-scratch agent generation",
        "self-optimizing agent functionality",
        "collaboration",
        "Particle Swarm Optimization (PSO)",
        "TravelPlanner benchmark",
        "system-level coordination",
        "creative reasoning"
      ]
    },
    "publishedAt": "2025-06-18T13:54:55.000Z",
    "title": "SwarmAgentic: Towards Fully Automated Agentic System Generation via\n  Swarm Intelligence",
    "summary": "The rapid progress of Large Language Models has advanced agentic systems in\ndecision-making, coordination, and task execution. Yet, existing agentic system\ngeneration frameworks lack full autonomy, missing from-scratch agent\ngeneration, self-optimizing agent functionality, and collaboration, limiting\nadaptability and scalability. We propose SwarmAgentic, a framework for fully\nautomated agentic system generation that constructs agentic systems from\nscratch and jointly optimizes agent functionality and collaboration as\ninterdependent components through language-driven exploration. To enable\nefficient search over system-level structures, SwarmAgentic maintains a\npopulation of candidate systems and evolves them via feedback-guided updates,\ndrawing inspiration from Particle Swarm Optimization (PSO). We evaluate our\nmethod on six real-world, open-ended, and exploratory tasks involving\nhigh-level planning, system-level coordination, and creative reasoning. Given\nonly a task description and an objective function, SwarmAgentic outperforms all\nbaselines, achieving a +261.8% relative improvement over ADAS on the\nTravelPlanner benchmark, highlighting the effectiveness of full automation in\nstructurally unconstrained tasks. This framework marks a significant step\ntoward scalable and autonomous agentic system design, bridging swarm\nintelligence with fully automated system multi-agent generation. Our code is\npublicly released at https://yaoz720.github.io/SwarmAgentic/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.15672.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "648cbea3dee03837c823cbf2",
      "avatarUrl": "/avatars/3f8c36436a5cbff2948df099ae604418.svg",
      "fullname": "Shuo Chen",
      "name": "ShuoChen99",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.14435",
      "authors": [
        {
          "_id": "68537b2a99bf39f9665c7990",
          "name": "Hongyu Wang",
          "hidden": false
        },
        {
          "_id": "68537b2a99bf39f9665c7991",
          "name": "Jiayu Xu",
          "hidden": false
        },
        {
          "_id": "68537b2a99bf39f9665c7992",
          "name": "Ruiping Wang",
          "hidden": false
        },
        {
          "_id": "68537b2a99bf39f9665c7993",
          "name": "Yan Feng",
          "hidden": false
        },
        {
          "_id": "68537b2a99bf39f9665c7994",
          "name": "Yitao Zhai",
          "hidden": false
        },
        {
          "_id": "68537b2a99bf39f9665c7995",
          "name": "Peng Pei",
          "hidden": false
        },
        {
          "_id": "68537b2a99bf39f9665c7996",
          "name": "Xunliang Cai",
          "hidden": false
        },
        {
          "_id": "68537b2a99bf39f9665c7997",
          "name": "Xilin Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-17T11:53:49.000Z",
      "submittedOnDailyAt": "2025-06-19T01:22:11.793Z",
      "title": "MoTE: メモリエフェクティブな大規模多モダルモデル向けの三元のエキスパートの混在",
      "submittedOnDailyBy": {
        "_id": "63f71771d36951307fcb4dcd",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f71771d36951307fcb4dcd/1c-GSQmSSuDXyVWjxZFy_.jpeg",
        "isPro": false,
        "fullname": "Hongyu Wang",
        "user": "hongyuw",
        "type": "user"
      },
      "summary": "大規模多モーダル混合エキスパート（MoEs）は、モデルサイズを増やしながら性能を向上させるために活性パラメータを固定しています。しかし、以前の研究では、スパースアップサイクリングの際に全精度のエキスパートを主に使用していました。それらは終了タスクで上位の性能を示しますが、多くのエキスパートがメモリフットプリントを高くし、エッジデバイスの採用に大きな課題をもたらします。本論文では、MoTE（Mixture-of-Ternary-Experts）を提案します。これは、稠密チェックポイントから学習するためのスケーラブルでメモリエフェクティブなアプローチです。高精度のエキスパートを学習するのではなく、低精度のエキスパートを増やしてスパースアップサイクリングを行います。特に、予ティニングされたFFNを共有エキスパートとし、パラメータが{-1, 0, 1}の三値ルートエキスパートを学習します。拡張的な実験は、我々のアプローチはモデルサイズに沿った望ましいスケーリングテンドを示しています。MoTEは、全精度ベースラインMoE-LLaVAと比較して相当の性能を示し、より小さなメモリフットプリントを提供します。また、我々のアプローチは後学習減量化手法と相容し、メモリ制約が低くなると優位がより大きくなります。同じエキスパートメモリフットプリント（3.4GB）を持つものと、後学習減量化を組み合わせると、MoTEは終了タスクの平均精度で4.3%の増加を示し、メモリ制約されたデバイスの効果および可能性を示しています。",
      "upvotes": 2,
      "discussionId": "68537b2b99bf39f9665c7998",
      "ai_summary": "MoTE, a scalable and memory-efficient method, improves Mixture-of-Experts models using low-precision ternary experts, enhancing performance and reducing memory footprint for deployment on edge devices.",
      "ai_keywords": [
        "Mixture-of-Experts",
        "MoEs",
        "sparse up-cycling",
        "low-precision",
        "ternary experts",
        "shared expert",
        "FFN",
        "pre-trained",
        "post-training quantization",
        "memory-constrained",
        "end tasks"
      ]
    },
    "publishedAt": "2025-06-17T07:53:49.000Z",
    "title": "MoTE: Mixture of Ternary Experts for Memory-efficient Large Multimodal\n  Models",
    "summary": "Large multimodal Mixture-of-Experts (MoEs) effectively scale the model size\nto boost performance while maintaining fixed active parameters. However,\nprevious works primarily utilized full-precision experts during sparse\nup-cycling. Despite they show superior performance on end tasks, the large\namount of experts introduces higher memory footprint, which poses significant\nchallenges for the deployment on edge devices. In this work, we propose MoTE, a\nscalable and memory-efficient approach to train Mixture-of-Ternary-Experts\nmodels from dense checkpoint. Instead of training fewer high-precision experts,\nwe propose to train more low-precision experts during up-cycling. Specifically,\nwe use the pre-trained FFN as a shared expert and train ternary routed experts\nwith parameters in {-1, 0, 1}. Extensive experiments show that our approach has\npromising scaling trend along model size. MoTE achieves comparable performance\nto full-precision baseline MoE-LLaVA while offering lower memory footprint.\nFurthermore, our approach is compatible with post-training quantization methods\nand the advantage further amplifies when memory-constraint goes lower. Given\nthe same amount of expert memory footprint of 3.4GB and combined with\npost-training quantization, MoTE outperforms MoE-LLaVA by a gain of 4.3%\naverage accuracy on end tasks, demonstrating its effectiveness and potential\nfor memory-constrained devices.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14435.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63f71771d36951307fcb4dcd",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f71771d36951307fcb4dcd/1c-GSQmSSuDXyVWjxZFy_.jpeg",
      "fullname": "Hongyu Wang",
      "name": "hongyuw",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 18
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.14866",
      "authors": [
        {
          "_id": "6853db4199bf39f9665c7ae5",
          "name": "Thomas Kuntz",
          "hidden": false
        },
        {
          "_id": "6853db4199bf39f9665c7ae6",
          "name": "Agatha Duzan",
          "hidden": false
        },
        {
          "_id": "6853db4199bf39f9665c7ae7",
          "name": "Hao Zhao",
          "hidden": false
        },
        {
          "_id": "6853db4199bf39f9665c7ae8",
          "name": "Francesco Croce",
          "hidden": false
        },
        {
          "_id": "6853db4199bf39f9665c7ae9",
          "name": "Zico Kolter",
          "hidden": false
        },
        {
          "_id": "6853db4199bf39f9665c7aea",
          "name": "Nicolas Flammarion",
          "hidden": false
        },
        {
          "_id": "6853db4199bf39f9665c7aeb",
          "name": "Maksym Andriushchenko",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-17T17:59:31.000Z",
      "submittedOnDailyAt": "2025-06-19T08:15:06.478Z",
      "title": "OS-Harm: コンピューター使用アグエントの安全性を測定するためのベンチマーク",
      "submittedOnDailyBy": {
        "_id": "64c225f0129617dbaba5ae88",
        "avatarUrl": "/avatars/b12db433cd6b37c8e5299e575bdf98f9.svg",
        "isPro": false,
        "fullname": "Maksym Andriushchenko",
        "user": "MaksymAndriushchenko",
        "type": "user"
      },
      "summary": "コンピューター使用アガントは、スクリーンショットまたはアクセシビリティツリーを処理してグラフィックユーザーインターフェースと直接的に相互作用するLLMベースのアガントです。これらのシステムは普及し始めていますが、その安全性は大きく見落とされており、有害な行動の可能性を評価してもらうことが広くなる採用のために重要です。この空間を填ぐために、OS-Harmという新しいベンチマークを紹介します。OS-HarmはOSWorld環境の上に構築されており、故意のユーザーの不正使用、プロンプトインジェクション攻撃、モデルの不正行動を含む3つのハムリーカテゴリーでモデルを検証します。これらの場合をカバーするために、150タスクを作成し、複数のサイティエジェントとの交互作用を求めます（メールクライアント、コードエディタ、ブラウザなど）。また、精度と安全性を評価するための自動チェックボックスを提案し、人間のアノテーションと高い一致率（F1スコア0.76と0.79）を達成します。OS-Harmは、o4-mini、Claude 3.7 Sonnet、Gemini 2.5 Proなどの先端モデルを基にコンピューター使用アガントの安全性を評価し、特に、すべてのモデルは故意の不正使用の質問に直接的に対応し、静的プロンプトインジェクションに相対的に脆弱で、時々不安全な行動を行います。OS-Harmベンチマークは、https://github.com/tml-epfl/os-harmで利用できます。",
      "upvotes": 1,
      "discussionId": "6853db4199bf39f9665c7aec",
      "githubRepo": "https://github.com/tml-epfl/os-harm",
      "ai_summary": "A new benchmark called OS-Harm measures the safety of computer use agents interacting with GUIs, evaluating their susceptibility to misuse, prompt injection attacks, and misbehavior across various safety violations and applications.",
      "ai_keywords": [
        "LLM-based agents",
        "OS-Harm",
        "OSWorld environment",
        "deliberate user misuse",
        "prompt injection attacks",
        "model misbehavior",
        "harassment",
        "copyright infringement",
        "disinformation",
        "data exfiltration",
        "automated judge",
        "F1 score",
        "GUI"
      ]
    },
    "publishedAt": "2025-06-17T13:59:31.000Z",
    "title": "OS-Harm: A Benchmark for Measuring Safety of Computer Use Agents",
    "summary": "Computer use agents are LLM-based agents that can directly interact with a\ngraphical user interface, by processing screenshots or accessibility trees.\nWhile these systems are gaining popularity, their safety has been largely\noverlooked, despite the fact that evaluating and understanding their potential\nfor harmful behavior is essential for widespread adoption. To address this gap,\nwe introduce OS-Harm, a new benchmark for measuring safety of computer use\nagents. OS-Harm is built on top of the OSWorld environment and aims to test\nmodels across three categories of harm: deliberate user misuse, prompt\ninjection attacks, and model misbehavior. To cover these cases, we create 150\ntasks that span several types of safety violations (harassment, copyright\ninfringement, disinformation, data exfiltration, etc.) and require the agent to\ninteract with a variety of OS applications (email client, code editor, browser,\netc.). Moreover, we propose an automated judge to evaluate both accuracy and\nsafety of agents that achieves high agreement with human annotations (0.76 and\n0.79 F1 score). We evaluate computer use agents based on a range of frontier\nmodels - such as o4-mini, Claude 3.7 Sonnet, Gemini 2.5 Pro - and provide\ninsights into their safety. In particular, all models tend to directly comply\nwith many deliberate misuse queries, are relatively vulnerable to static prompt\ninjections, and occasionally perform unsafe actions. The OS-Harm benchmark is\navailable at https://github.com/tml-epfl/os-harm.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14866.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64c225f0129617dbaba5ae88",
      "avatarUrl": "/avatars/b12db433cd6b37c8e5299e575bdf98f9.svg",
      "fullname": "Maksym Andriushchenko",
      "name": "MaksymAndriushchenko",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.14824",
      "authors": [
        {
          "_id": "6853c3af99bf39f9665c7a89",
          "name": "Yao Zhang",
          "hidden": false
        },
        {
          "_id": "6853c3af99bf39f9665c7a8a",
          "name": "Hewei Gao",
          "hidden": false
        },
        {
          "_id": "6853c3af99bf39f9665c7a8b",
          "name": "Haokun Chen",
          "hidden": false
        },
        {
          "_id": "6853c3af99bf39f9665c7a8c",
          "name": "Weiguo Li",
          "hidden": false
        },
        {
          "_id": "6853c3af99bf39f9665c7a8d",
          "name": "Yunpu Ma",
          "hidden": false
        },
        {
          "_id": "6853c3af99bf39f9665c7a8e",
          "name": "Volker Tresp",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-12T17:50:50.000Z",
      "submittedOnDailyAt": "2025-06-19T06:31:06.085Z",
      "title": "FedNano: プレトレインドモデル向けの軽量フェデレーションチューニングへの進展",
      "submittedOnDailyBy": {
        "_id": "648cbea3dee03837c823cbf2",
        "avatarUrl": "/avatars/3f8c36436a5cbff2948df099ae604418.svg",
        "isPro": false,
        "fullname": "Shuo Chen",
        "user": "ShuoChen99",
        "type": "user"
      },
      "summary": "多モデル大語言モデル（MLLMs）は、多モデル推理とクロスモデル検索などの複雑なタスクで優れていますが、実世界のシナリオでの機能には、分散された多モデルデータと厳格なプライバシー要求による機能障害があります。Federated Learning（FL）は、データを中央化しないようにモデルの共同学習を可能にして解決策を提供していますが、MLLMsのFLの実現には高い計算要求、クライアントの容量の限界、大きな通信コスト、そしてヒテロノミーなクライアントデータによる問題があります。現在のFL手法は、クライアント側でフルモデルの機械学習を行う前提としていますが、MLLMsの巨大さと通信要求によりこの前提が破綻しています。これらの制限を解決するために、FedNanoという最初のFLフレームワークを提案しています。FedNanoは、サーバー側でLLMを中央化し、クライアント側の特定のアダプタイションのためのNanoEdgeという軽量モジュールを導入しています。NanoEdgeは、モデル特有のエンコーダー、コネクター、そして低ランクアダプターを用いて、LLMのクライアント側機械学習の必要性を除去し、クライアント側のストレージを95%削減し、モデルパラメーターの通信コストを0.01%に抑えることができます。NanoAdapterのみを伝送することで、FedNanoはヒテロノミーなクライアントデータとリソースの制限を扱い、プライバシーを維持しながら、MLLMサイズとFLの可能性の間のギャップを埋め、可換的、分散された多モデルAIシステムを可能にします。実験は、FedNanoが先行のFLベースラインショットを上回り、MLLMサイズとFLの可能性の間の隙間を埋め、可換的、分散された多モデルAIシステムを可能にします。",
      "upvotes": 1,
      "discussionId": "6853c3af99bf39f9665c7a8f",
      "ai_summary": "FedNano is a federated learning framework that centralizes large language models on servers and uses NanoEdge modules for client-specific adaptation, addressing scalability and privacy issues.",
      "ai_keywords": [
        "Multimodal Large Language Models",
        "Federated Learning",
        "NanoEdge",
        "modality-specific encoders",
        "connectors",
        "NanoAdapters",
        "low-rank adaptation",
        "client-specific adaptation",
        "compact NanoAdapter updates",
        "decentralized multimodal AI systems"
      ]
    },
    "publishedAt": "2025-06-12T13:50:50.000Z",
    "title": "FedNano: Toward Lightweight Federated Tuning for Pretrained Multimodal\n  Large Language Models",
    "summary": "Multimodal Large Language Models (MLLMs) excel in tasks like multimodal\nreasoning and cross-modal retrieval but face deployment challenges in\nreal-world scenarios due to distributed multimodal data and strict privacy\nrequirements. Federated Learning (FL) offers a solution by enabling\ncollaborative model training without centralizing data. However, realizing FL\nfor MLLMs presents significant challenges, including high computational\ndemands, limited client capacity, substantial communication costs, and\nheterogeneous client data. Existing FL methods assume client-side deployment of\nfull models, an assumption that breaks down for large-scale MLLMs due to their\nmassive size and communication demands. To address these limitations, we\npropose FedNano, the first FL framework that centralizes the LLM on the server\nwhile introducing NanoEdge, a lightweight module for client-specific\nadaptation. NanoEdge employs modality-specific encoders, connectors, and\ntrainable NanoAdapters with low-rank adaptation. This design eliminates the\nneed to deploy LLM on clients, reducing client-side storage by 95%, and\nlimiting communication overhead to only 0.01% of the model parameters. By\ntransmitting only compact NanoAdapter updates, FedNano handles heterogeneous\nclient data and resource constraints while preserving privacy. Experiments\ndemonstrate that FedNano outperforms prior FL baselines, bridging the gap\nbetween MLLM scale and FL feasibility, and enabling scalable, decentralized\nmultimodal AI systems.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14824.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "648cbea3dee03837c823cbf2",
      "avatarUrl": "/avatars/3f8c36436a5cbff2948df099ae604418.svg",
      "fullname": "Shuo Chen",
      "name": "ShuoChen99",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  }
]