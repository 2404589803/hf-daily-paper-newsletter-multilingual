[
  {
    "paper": {
      "id": "2506.17450",
      "authors": [
        {
          "_id": "68620adf9e7509383d29ab98",
          "user": {
            "_id": "655bca95360e4f90cb61ba83",
            "avatarUrl": "/avatars/1a187beb91a5e2fdc2303620b742aab1.svg",
            "isPro": true,
            "fullname": "Jiacheng Chen",
            "user": "cccjc",
            "type": "user"
          },
          "name": "Jiacheng Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-30T06:22:01.226Z",
          "hidden": false
        },
        {
          "_id": "68620adf9e7509383d29ab99",
          "name": "Ramin Mehran",
          "hidden": false
        },
        {
          "_id": "68620adf9e7509383d29ab9a",
          "name": "Xuhui Jia",
          "hidden": false
        },
        {
          "_id": "68620adf9e7509383d29ab9b",
          "name": "Saining Xie",
          "hidden": false
        },
        {
          "_id": "68620adf9e7509383d29ab9c",
          "name": "Sanghyun Woo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-20T19:38:34.000Z",
      "submittedOnDailyAt": "2025-06-30T02:33:26.106Z",
      "title": "BlenderFusion: 3Dベースの可視化編集と生成的合成",
      "submittedOnDailyBy": {
        "_id": "655bca95360e4f90cb61ba83",
        "avatarUrl": "/avatars/1a187beb91a5e2fdc2303620b742aab1.svg",
        "isPro": true,
        "fullname": "Jiacheng Chen",
        "user": "cccjc",
        "type": "user"
      },
      "summary": "BlenderFusionは、物体、カメラ、バックグラウンドを再組み合わせて新しいシーンを合成する生成的な可視的合成フレームワークです。そのパイプラインは、(i) 可視的入力を可視的3Dエンティティに変換する(レイヤー処理)、(ii) Blenderで3Dベースの制御で編集する(編集)、(iii) 生成的合成器を用いてシンフェレーションして一貫したシーンを作成する(合成)です。我々の生成的合成器は、プレトレーンドディフュージョンモデルを拡張して、元の(ソース)と編集された(ターゲット)シーンを並列で処理することができます。これは、(i) ソースマスク、(ii) シミュレーションされた物体ジャイヤーを用いた2つの主要な学習戦略で微調節されています。BlenderFusionは、複雑な合成的なシーン編集タスクで先行方法に比べて大幅に優れています。",
      "upvotes": 28,
      "discussionId": "68620adf9e7509383d29ab9d",
      "projectPage": "https://blenderfusion.github.io/",
      "ai_summary": "A generative visual compositing framework using a diffusion model for scene editing and composition with source masking and simulated object jittering.",
      "ai_keywords": [
        "diffusion model",
        "source masking",
        "simulated object jittering"
      ]
    },
    "publishedAt": "2025-06-20T15:38:34.000Z",
    "title": "BlenderFusion: 3D-Grounded Visual Editing and Generative Compositing",
    "summary": "We present BlenderFusion, a generative visual compositing framework that\nsynthesizes new scenes by recomposing objects, camera, and background. It\nfollows a layering-editing-compositing pipeline: (i) segmenting and converting\nvisual inputs into editable 3D entities (layering), (ii) editing them in\nBlender with 3D-grounded control (editing), and (iii) fusing them into a\ncoherent scene using a generative compositor (compositing). Our generative\ncompositor extends a pre-trained diffusion model to process both the original\n(source) and edited (target) scenes in parallel. It is fine-tuned on video\nframes with two key training strategies: (i) source masking, enabling flexible\nmodifications like background replacement; (ii) simulated object jittering,\nfacilitating disentangled control over objects and camera. BlenderFusion\nsignificantly outperforms prior methods in complex compositional scene editing\ntasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.17450.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "655bca95360e4f90cb61ba83",
      "avatarUrl": "/avatars/1a187beb91a5e2fdc2303620b742aab1.svg",
      "fullname": "Jiacheng Chen",
      "name": "cccjc",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.21862",
      "authors": [
        {
          "_id": "6861eea79e7509383d29ab2f",
          "name": "Boyuan Sun",
          "hidden": false
        },
        {
          "_id": "6861eea79e7509383d29ab30",
          "name": "Jiaxing Zhao",
          "hidden": false
        },
        {
          "_id": "6861eea79e7509383d29ab31",
          "name": "Xihan Wei",
          "hidden": false
        },
        {
          "_id": "6861eea79e7509383d29ab32",
          "name": "Qibin Hou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-27T02:29:58.000Z",
      "submittedOnDailyAt": "2025-06-30T00:31:12.107Z",
      "title": "LLaVA-Scissor: トークン圧縮における語意的な連結成分を用いて\n  ビデオLLM",
      "submittedOnDailyBy": {
        "_id": "6686044047f2a33570e59e31",
        "avatarUrl": "/avatars/2656bf2cecd6d7cbffd0a912a54d25de.svg",
        "isPro": false,
        "fullname": "Jiaxing Zhao",
        "user": "StarJiaxing",
        "type": "user"
      },
      "summary": "この論文では、ビデオ多タイプ大語言モデルに適したトークン圧縮戦略LLaVA-Scissorを提案します。先行の方法は、主にアテンションスコアに基づいてトークンを圧縮していますが、すべてのセマンティック領域を有効に捉えることができなかったこともあり、トークンの冗長性が生じました。別に、セマンティック連結成分（SCC）アプローチを活用し、トークンセット内の違うセマンティック領域にトークンを割り当て、全体的なセマンティック的な被覆を確保します。結果として、スペクトラルと時間的なドメインでSCCを利用する2ステップのスペクトラル・タイムスペクトラルトークン圧縮戦略が実現されます。この戦略は、非重複のセマンティックトークンのセットでビデオ全体を表現することで、トークンを効果的に圧縮できます。LLaVA-Scissorのトークン圧縮能力を評価するために、ビデオ問答、長ビデオ理解、詳細な多選挙ベンチマークを含む多様なビデオ理解ベンチマークで検証しました。実験結果によると、提案されたLLaVA-Scissorは他のトークン圧縮方法を上回り、低トークン保持比でも多様なビデオ理解ベンチマークで上位の性能を収めました。プロジェクトページ：https://github.com/HumanMLLM/LLaVA-Scissor。",
      "upvotes": 26,
      "discussionId": "6861eea89e7509383d29ab33",
      "ai_summary": "LLaVA-Scissor, a token compression strategy for video multimodal large language models, uses Semantic Connected Components to compress tokens effectively while maintaining semantic coverage and outperforming other methods.",
      "ai_keywords": [
        "token compression strategy",
        "Semantic Connected Components (SCC)",
        "spatio-temporal token compression strategy",
        "video question answering",
        "long video understanding",
        "comprehensive multi-choice benchmarks"
      ]
    },
    "publishedAt": "2025-06-26T22:29:58.000Z",
    "title": "LLaVA-Scissor: Token Compression with Semantic Connected Components for\n  Video LLMs",
    "summary": "In this paper, we present LLaVA-Scissor, a training-free token compression\nstrategy designed for video multimodal large language models. Previous methods\nmostly attempt to compress tokens based on attention scores, but fail to\neffectively capture all semantic regions and often lead to token redundancy.\nDifferently, we propose to leverage the Semantic Connected Components (SCC)\napproach that assigns tokens to distinct semantic regions within the token set,\nensuring comprehensive semantic coverage. The outcome is a two-step\nspatio-temporal token compression strategy that utilizes SCC in both spatial\nand temporal domains. This strategy can effectively compress tokens by\nrepresenting the entire video with a set of non-overlapping semantic tokens. We\nconduct extensive evaluations of the token compression capabilities of\nLLaVA-Scissor across diverse video understanding benchmarks, including video\nquestion answering, long video understanding, and comprehensive multi-choices\nbenchmarks. Experimental results show that the proposed LLaVA-Scissor\noutperforms other token compression methods, achieving superior performance in\nvarious video understanding benchmarks, particularly at low token retention\nratios. Project page: https://github.com/HumanMLLM/LLaVA-Scissor.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.21862.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "6686044047f2a33570e59e31",
      "avatarUrl": "/avatars/2656bf2cecd6d7cbffd0a912a54d25de.svg",
      "fullname": "Jiaxing Zhao",
      "name": "StarJiaxing",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 17
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.21416",
      "authors": [
        {
          "_id": "685e084071131fa43be08acc",
          "user": {
            "_id": "6361dd166945df7441b893fa",
            "avatarUrl": "/avatars/b3ae6888a41aab8c2a7ef9f7320565c4.svg",
            "isPro": false,
            "fullname": "Bowen Chen ",
            "user": "chenbowen",
            "type": "user"
          },
          "name": "Bowen Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-30T06:22:45.351Z",
          "hidden": false
        },
        {
          "_id": "685e084071131fa43be08acd",
          "name": "Mengyi Zhao",
          "hidden": false
        },
        {
          "_id": "685e084071131fa43be08ace",
          "name": "Haomiao Sun",
          "hidden": false
        },
        {
          "_id": "685e084071131fa43be08acf",
          "name": "Li Chen",
          "hidden": false
        },
        {
          "_id": "685e084071131fa43be08ad0",
          "name": "Xu Wang",
          "hidden": false
        },
        {
          "_id": "685e084071131fa43be08ad1",
          "name": "Kang Du",
          "hidden": false
        },
        {
          "_id": "685e084071131fa43be08ad2",
          "name": "Xinglong Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-26T16:04:16.000Z",
      "submittedOnDailyAt": "2025-06-30T04:43:14.606Z",
      "title": "XVerse: 識別とセマンティック属性の一致的な多変数制御を行うDiT調節",
      "submittedOnDailyBy": {
        "_id": "6498038ece9190ebb8693034",
        "avatarUrl": "/avatars/06ec2457932e05572d917ba286cdef25.svg",
        "isPro": false,
        "fullname": "Zhao",
        "user": "Mengyi",
        "type": "user"
      },
      "summary": "Achieving fine-grained control over subject identity and semantic attributes (pose, style, lighting) in text-to-image generation, particularly for multiple subjects, often undermines the editability and coherence of Diffusion Transformers (DiTs). Many approaches introduce artifacts or suffer from attribute entanglement. To overcome these challenges, we propose a novel multi-subject controlled generation model XVerse. By transforming reference images into offsets for token-specific text-stream modulation, XVerse allows for precise and independent control for specific subjects without disrupting image latents or features. Consequently, XVerse offers high-fidelity, editable multi-subject image synthesis with robust control over individual subject characteristics and semantic attributes. This advancement significantly improves personalized and complex scene generation capabilities.",
      "upvotes": 18,
      "discussionId": "685e084071131fa43be08ad3",
      "projectPage": "https://bytedance.github.io/XVerse/",
      "githubRepo": "https://github.com/bytedance/XVerse",
      "ai_summary": "XVerse enhances text-to-image generation by enabling precise and independent control over multiple subjects using token-specific text-stream modulation, improving image coherence and fidelity.",
      "ai_keywords": [
        "Diffusion Transformers",
        "DiTs",
        "text-to-image generation",
        "multi-subject controlled generation",
        "reference images",
        "token-specific text-stream modulation",
        "image latents",
        "multi-subject image synthesis",
        "semantic attributes"
      ],
      "githubStars": 68
    },
    "publishedAt": "2025-06-26T12:04:16.000Z",
    "title": "XVerse: Consistent Multi-Subject Control of Identity and Semantic\n  Attributes via DiT Modulation",
    "summary": "Achieving fine-grained control over subject identity and semantic attributes\n(pose, style, lighting) in text-to-image generation, particularly for multiple\nsubjects, often undermines the editability and coherence of Diffusion\nTransformers (DiTs). Many approaches introduce artifacts or suffer from\nattribute entanglement. To overcome these challenges, we propose a novel\nmulti-subject controlled generation model XVerse. By transforming reference\nimages into offsets for token-specific text-stream modulation, XVerse allows\nfor precise and independent control for specific subject without disrupting\nimage latents or features. Consequently, XVerse offers high-fidelity, editable\nmulti-subject image synthesis with robust control over individual subject\ncharacteristics and semantic attributes. This advancement significantly\nimproves personalized and complex scene generation capabilities.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.21416.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6498038ece9190ebb8693034",
      "avatarUrl": "/avatars/06ec2457932e05572d917ba286cdef25.svg",
      "fullname": "Zhao",
      "name": "Mengyi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.21356",
      "authors": [
        {
          "_id": "6861fb7a9e7509383d29ab4b",
          "name": "Hongbo Liu",
          "hidden": false
        },
        {
          "_id": "6861fb7a9e7509383d29ab4c",
          "name": "Jingwen He",
          "hidden": false
        },
        {
          "_id": "6861fb7a9e7509383d29ab4d",
          "name": "Yi Jin",
          "hidden": false
        },
        {
          "_id": "6861fb7a9e7509383d29ab4e",
          "name": "Dian Zheng",
          "hidden": false
        },
        {
          "_id": "6861fb7a9e7509383d29ab4f",
          "name": "Yuhao Dong",
          "hidden": false
        },
        {
          "_id": "6861fb7a9e7509383d29ab50",
          "name": "Fan Zhang",
          "hidden": false
        },
        {
          "_id": "6861fb7a9e7509383d29ab51",
          "name": "Ziqi Huang",
          "hidden": false
        },
        {
          "_id": "6861fb7a9e7509383d29ab52",
          "name": "Yinan He",
          "hidden": false
        },
        {
          "_id": "6861fb7a9e7509383d29ab53",
          "name": "Yangguang Li",
          "hidden": false
        },
        {
          "_id": "6861fb7a9e7509383d29ab54",
          "name": "Weichao Chen",
          "hidden": false
        },
        {
          "_id": "6861fb7a9e7509383d29ab55",
          "name": "Yu Qiao",
          "hidden": false
        },
        {
          "_id": "6861fb7a9e7509383d29ab56",
          "name": "Wanli Ouyang",
          "hidden": false
        },
        {
          "_id": "6861fb7a9e7509383d29ab57",
          "name": "Shengjie Zhao",
          "hidden": false
        },
        {
          "_id": "6861fb7a9e7509383d29ab58",
          "name": "Ziwei Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-26T15:09:21.000Z",
      "submittedOnDailyAt": "2025-06-30T04:32:34.261Z",
      "title": "ShotBench: エキスパートレベルの映画的な理解を持つ視覚言語モデル",
      "submittedOnDailyBy": {
        "_id": "652965773a416e1f2173443b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652965773a416e1f2173443b/y9MB8YgHzbwCXAc4EI9T3.jpeg",
        "isPro": false,
        "fullname": "Yuhao Dong",
        "user": "THUdyh",
        "type": "user"
      },
      "summary": "映画の基本的な視覚的言語である映画記録技術は、物語の伝達、感情、美術質の伝達に不可欠である。最近の視覚言語モデル（VLMs）は、広泛な視覚的理解力を示しているが、個々のショット内に含まれる微妙な映画記録文法の理解については、大きな調査がなく、強い評価が欠けている。この重要な欠点は、細かい視覚的理解とAIによる映像生成の精度を制限している。これに対して、我々は、映画記録言語理解に特化した詳細なベンチマークとしてShotBenchを紹介する。これは、200部以上の評価された映画（主にオスカー受賞予選映画）から抜粋した8つの映画記録の重要な次元を通して、3500点以上の専門家の注釈されたQAペアを準備している。ShotBenchで24つの先進的なVLMsの評価により、それらの大きな制限が明らかになった：最も高い性能を示すモデルでも、平均正答率が60%未満であり、特に細かい視覚的コードと複雑な空間的理由論に難しい。この分野の進歩を促進するために、我々は、約70,000点の映画のQAペアを含む大規模な多様的データセットとしてShotQAを構築する。ShotQAを活用して、我々は、監督的調整とグループ相対的ポリシー最適化を通じてShotVLを開発する。ShotVLは、すべての現在の開放ソースや所有権モデルを上回り、ShotBenchで新しい最先端の性能を設定する。我々は、モデル、データ、コードを開放させ、AIによる映画記録の理解と生成の重要な領域の急速な進歩を促進する。",
      "upvotes": 15,
      "discussionId": "6861fb7a9e7509383d29ab59",
      "projectPage": "https://vchitect.github.io/ShotBench-project/",
      "githubRepo": "https://github.com/Vchitect/ShotBench/tree/main",
      "ai_summary": "ShotBench and ShotQA datasets, along with ShotVL model, enhance AI's understanding and generation capabilities by specifically targeting nuanced cinematic language comprehension.",
      "ai_keywords": [
        "Vision-Language Models",
        "VLMs",
        "ShotBench",
        "QA pairs",
        "cinematic grammar",
        "fine-grained visual comprehension",
        "AI-assisted video generation",
        "ShotQA",
        "multimodal dataset",
        "supervised fine-tuning",
        "Group Relative Policy Optimization",
        "ShotVL",
        "AI-driven cinematic understanding",
        "state-of-the-art performance"
      ],
      "githubStars": 5
    },
    "publishedAt": "2025-06-26T11:09:21.000Z",
    "title": "ShotBench: Expert-Level Cinematic Understanding in Vision-Language\n  Models",
    "summary": "Cinematography, the fundamental visual language of film, is essential for\nconveying narrative, emotion, and aesthetic quality. While recent\nVision-Language Models (VLMs) demonstrate strong general visual understanding,\ntheir proficiency in comprehending the nuanced cinematic grammar embedded\nwithin individual shots remains largely unexplored and lacks robust evaluation.\nThis critical gap limits both fine-grained visual comprehension and the\nprecision of AI-assisted video generation. To address this, we introduce\nShotBench, a comprehensive benchmark specifically designed for cinematic\nlanguage understanding. It features over 3.5k expert-annotated QA pairs from\nimages and video clips, meticulously curated from over 200 acclaimed\n(predominantly Oscar-nominated) films and spanning eight key cinematography\ndimensions. Our evaluation of 24 leading VLMs on ShotBench reveals their\nsubstantial limitations: even the top-performing model achieves less than 60%\naverage accuracy, particularly struggling with fine-grained visual cues and\ncomplex spatial reasoning. To catalyze advancement in this domain, we construct\nShotQA, a large-scale multimodal dataset comprising approximately 70k cinematic\nQA pairs. Leveraging ShotQA, we develop ShotVL through supervised fine-tuning\nand Group Relative Policy Optimization. ShotVL significantly outperforms all\nexisting open-source and proprietary models on ShotBench, establishing new\nstate-of-the-art performance. We open-source our models, data, and code to\nfoster rapid progress in this crucial area of AI-driven cinematic understanding\nand generation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.21356.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "652965773a416e1f2173443b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652965773a416e1f2173443b/y9MB8YgHzbwCXAc4EI9T3.jpeg",
      "fullname": "Yuhao Dong",
      "name": "THUdyh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 43
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.20279",
      "authors": [
        {
          "_id": "686218679e7509383d29abb3",
          "name": "Changliang Xia",
          "hidden": false
        },
        {
          "_id": "686218679e7509383d29abb4",
          "name": "Chengyou Jia",
          "hidden": false
        },
        {
          "_id": "686218679e7509383d29abb5",
          "name": "Zhuohang Dang",
          "hidden": false
        },
        {
          "_id": "686218679e7509383d29abb6",
          "name": "Minnan Luo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-25T09:40:50.000Z",
      "submittedOnDailyAt": "2025-06-30T03:24:47.090Z",
      "title": "理想から実際に：実世界のスキャンニングへのデータ効率的な統一化の密集予測",
      "submittedOnDailyBy": {
        "_id": "6602548a68d519ed324b47c5",
        "avatarUrl": "/avatars/5ab411f87440cc2a98c7a1c6a3ed5548.svg",
        "isPro": false,
        "fullname": "ChengyouJia",
        "user": "ChengyouJia",
        "type": "user"
      },
      "summary": "Dense predictionタスクは、コンピュータビジョンにとって重要な役割を果たし、入力画像にピクセル毎の標識ラベルを学習することを目指しています。この分野での進展はあるが、現在の方法は理想的な条件に焦点を当て、実世界的なシナリオに限られた一般化能力を持ち、実世界的なデータの不足に直面しています。この問題を体系的に研究するために、まずDenseWorldというベンチマークを紹介します。DenseWorldは、25つの効果的な実世界的なアプリケーションに対応した幅広い範囲の密集予測タスクを構成し、タスク全体での統一評価を特徴としています。次に、DenseDiTというモデルを提案します。DenseDiTは、生成モデルの視覚的な先驅を最大限に活用し、一つの統一的な戦略で多様な実世界的な密集予測タスクを行うことを目指しています。DenseDiTは、パラメーター再利用機構と2つの軽量ブランチを組み合わせ、多スケールコンテキストを適応的に統合することで、0.1%以下の追加パラメーターを使用して動作します。DenseWorldでの評価は、現在の一般的と特化されたベースラインの性能低下を明らかにし、彼らの実世界的な一般化能力の限界を示しています。対照的に、DenseDiTは、基準データの0.01%以下を使用して優れた結果を収め、実世界的な機能の実践的な価値を強調しています。データ、チェックポイントとコードは、https://xcltql666.github.io/DenseDiTProjから利用可能です。",
      "upvotes": 13,
      "discussionId": "686218689e7509383d29abb7",
      "projectPage": "https://xcltql666.github.io/DenseDiTProj/",
      "githubRepo": "https://github.com/xcltql666/DenseDiT",
      "ai_summary": "DenseDiT, a generative model-based approach, achieves superior performance in real-world dense prediction tasks using minimal training data compared to existing methods.",
      "ai_keywords": [
        "dense prediction",
        "generative models",
        "visual priors",
        "parameter-reuse mechanism",
        "lightweight branches",
        "multi-scale context",
        "DenseWorld",
        "DenseDiT"
      ],
      "githubStars": 15
    },
    "publishedAt": "2025-06-25T05:40:50.000Z",
    "title": "From Ideal to Real: Unified and Data-Efficient Dense Prediction for\n  Real-World Scenarios",
    "summary": "Dense prediction tasks hold significant importance of computer vision, aiming\nto learn pixel-wise annotated label for an input image. Despite advances in\nthis field, existing methods primarily focus on idealized conditions, with\nlimited generalization to real-world scenarios and facing the challenging\nscarcity of real-world data. To systematically study this problem, we first\nintroduce DenseWorld, a benchmark spanning a broad set of 25 dense prediction\ntasks that correspond to urgent real-world applications, featuring unified\nevaluation across tasks. Then, we propose DenseDiT, which maximally exploits\ngenerative models' visual priors to perform diverse real-world dense prediction\ntasks through a unified strategy. DenseDiT combines a parameter-reuse mechanism\nand two lightweight branches that adaptively integrate multi-scale context,\nworking with less than 0.1% additional parameters. Evaluations on DenseWorld\nreveal significant performance drops in existing general and specialized\nbaselines, highlighting their limited real-world generalization. In contrast,\nDenseDiT achieves superior results using less than 0.01% training data of\nbaselines, underscoring its practical value for real-world deployment. Our\ndata, and checkpoints and codes are available at\nhttps://xcltql666.github.io/DenseDiTProj",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.20279.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6602548a68d519ed324b47c5",
      "avatarUrl": "/avatars/5ab411f87440cc2a98c7a1c6a3ed5548.svg",
      "fullname": "ChengyouJia",
      "name": "ChengyouJia",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.22434",
      "authors": [
        {
          "_id": "686205ad9e7509383d29ab80",
          "name": "Xi Chen",
          "hidden": false
        },
        {
          "_id": "686205ad9e7509383d29ab81",
          "name": "Mingkang Zhu",
          "hidden": false
        },
        {
          "_id": "686205ad9e7509383d29ab82",
          "name": "Shaoteng Liu",
          "hidden": false
        },
        {
          "_id": "686205ad9e7509383d29ab83",
          "name": "Xiaoyang Wu",
          "hidden": false
        },
        {
          "_id": "686205ad9e7509383d29ab84",
          "name": "Xiaogang Xu",
          "hidden": false
        },
        {
          "_id": "686205ad9e7509383d29ab85",
          "name": "Yu Liu",
          "hidden": false
        },
        {
          "_id": "686205ad9e7509383d29ab86",
          "name": "Xiang Bai",
          "hidden": false
        },
        {
          "_id": "686205ad9e7509383d29ab87",
          "name": "Hengshuang Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-27T17:59:27.000Z",
      "submittedOnDailyAt": "2025-06-30T02:04:48.511Z",
      "title": "ミコ: 複数画像対比を用いた強化可視説明",
      "submittedOnDailyBy": {
        "_id": "644a1b6401e18bf93a6f45c1",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644a1b6401e18bf93a6f45c1/P0i_CgCrIzOS2tYRlxoE9.png",
        "isPro": false,
        "fullname": "xichen",
        "user": "xichenhku",
        "type": "user"
      },
      "summary": "この研究は、複数の画像間でのChain-of-Thought（CoT）推理を可能にする方法を調査しています。直感的な解決策としては、Vision-Language Models（VLMs）に適用するルールベースの強化学習を採用することが考えられます。しかし、これらの方法は通常、手動でカレーテッドされた質問・答えペアをもとに基づくことが多いため、画像の細かい視覚的な詳細や画像間の複雑な論理を処理する場合に特に難しいです。自動調節的な視覚表現学習をモデルとして参考にして、画像は内在的な制約を持っていることを見出しました。このヒントに基づき、同じ画像の2つの拡張された視点と第三の類似しているが異なる画像から構成される画像タプルを構築しました。学習の間、モデルはこれらの画像を比較するための理由を生成するよう促します（つまり、同じか異なるかを判断する）。そして、ルールベースの強化学習を用いてモデルを最適化します。高い視覚的な類似性と拡張の存在により、モデルは微妙な視覚的な変化を関心に持ち、論理的な論理を行う必要があります。実験は、視覚的な比較タスクにだけ学習された理由を学ぶことができることを示し、それが広範囲の質問に広義的に一般化します。人間の記録された質問・答えペアに依存しないように、本方法は複数の画像の論理を処理するベンチマークで顕著な改善を達成し、一般的な視覚的なタスクに強い性能を示しています。",
      "upvotes": 8,
      "discussionId": "686205ad9e7509383d29ab88",
      "ai_summary": "Self-supervised learning using image triplets enhances the reasoning ability of Vision-Language Models (VLMs) on multi-image tasks without the need for human-annotated question-answer pairs.",
      "ai_keywords": [
        "Vision-Language Models",
        "self-supervised learning",
        "image triplets",
        "reasoning ability",
        "multi-image reasoning benchmarks",
        "general vision tasks"
      ]
    },
    "publishedAt": "2025-06-27T13:59:27.000Z",
    "title": "MiCo: Multi-image Contrast for Reinforcement Visual Reasoning",
    "summary": "This work explores enabling Chain-of-Thought (CoT) reasoning to link visual\ncues across multiple images. A straightforward solution is to adapt rule-based\nreinforcement learning for Vision-Language Models (VLMs). However, such methods\ntypically rely on manually curated question-answer pairs, which can be\nparticularly challenging when dealing with fine grained visual details and\ncomplex logic across images. Inspired by self-supervised visual representation\nlearning, we observe that images contain inherent constraints that can serve as\nsupervision. Based on this insight, we construct image triplets comprising two\naugmented views of the same image and a third, similar but distinct image.\nDuring training, the model is prompted to generate a reasoning process to\ncompare these images (i.e., determine same or different). Then we optimize the\nmodel with rule-based reinforcement learning. Due to the high visual similarity\nand the presence of augmentations, the model must attend to subtle visual\nchanges and perform logical reasoning to succeed. Experiments show that,\nalthough trained solely on visual comparison tasks, the learned reasoning\nability generalizes effectively to a wide range of questions. Without relying\non any human-annotated question-answer pairs, our method achieves significant\nimprovements on multi-image reasoning benchmarks and shows strong performance\non general vision tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.22434.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "644a1b6401e18bf93a6f45c1",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644a1b6401e18bf93a6f45c1/P0i_CgCrIzOS2tYRlxoE9.png",
      "fullname": "xichen",
      "name": "xichenhku",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 43
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.21656",
      "authors": [
        {
          "_id": "6861f2b89e7509383d29ab35",
          "name": "Yifan Shen",
          "hidden": false
        },
        {
          "_id": "6861f2b89e7509383d29ab36",
          "name": "Yuanzhe Liu",
          "hidden": false
        },
        {
          "_id": "6861f2b89e7509383d29ab37",
          "name": "Jingyuan Zhu",
          "hidden": false
        },
        {
          "_id": "6861f2b89e7509383d29ab38",
          "name": "Xu Cao",
          "hidden": false
        },
        {
          "_id": "6861f2b89e7509383d29ab39",
          "name": "Xiaofeng Zhang",
          "hidden": false
        },
        {
          "_id": "6861f2b89e7509383d29ab3a",
          "name": "Yixiao He",
          "hidden": false
        },
        {
          "_id": "6861f2b89e7509383d29ab3b",
          "name": "Wenming Ye",
          "hidden": false
        },
        {
          "_id": "6861f2b89e7509383d29ab3c",
          "name": "James Matthew Rehg",
          "hidden": false
        },
        {
          "_id": "6861f2b89e7509383d29ab3d",
          "name": "Ismini Lourentzou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-26T18:00:00.000Z",
      "submittedOnDailyAt": "2025-06-30T00:44:37.025Z",
      "title": "Fine-Grained Preference Optimization は、VLMs の空間認識能力を向上させる",
      "submittedOnDailyBy": {
        "_id": "65e387095132c2edd193ae49",
        "avatarUrl": "/avatars/39278e5b026bcbdde88c560fc54018c5.svg",
        "isPro": false,
        "fullname": "Yifan Shen",
        "user": "SivanSX",
        "type": "user"
      },
      "summary": "現在の視覚言語モデル（VLMs）は、特に多ステップのロジックと精確な空間的なアラインメントが必要な場合、細かい空間的な理由論を取り扱うことが難しい。本研究では、これらの制限を解決するために、SpatialReasoner-R1という視覚言語理由論モデルを導入しています。高品質なスペース的な理由論の規範を構築するために、多モデルモンテカルロ木探索（M3CTS）メソッドを設計し、多様的でロジック的に一貫した長いチャインオブコンシェント（LongCoT）の理由論トラジェクトを生成します。また、細かい直接な好み最適化（fDPO）を提案し、観測基底とロジック的な理由論に関するセグメント特有の好みのグラニュラリティを導入し、空間的な報酬機制によって視覚的一致性、空間的な基底、ロジック的な一貫性に基づいて候補のレスポンスを評価します。実験結果によると、fDPOはスペース的な品質タスクで標準DPOより平均的に4.1%の改善を達成し、スペース的な量タスクでは9.0%の効果を発揮します。fDPOで訓練されたSpatialReasoner-R1はSPATIALRGPT-Benchで新たなSoTAを設定し、最強な基準と比べて平均的な精度で9.8%の改善を収め、一般的な視覚言語タスクでも競争的な性能を維持しています。",
      "upvotes": 6,
      "discussionId": "6861f2b99e7509383d29ab3e",
      "ai_summary": "SpatialReasoner-R1, a vision-language reasoning model, uses Multi-Model Monte Carlo Tree Search and fine-grained Direct Preference Optimization to improve spatial reasoning, setting a new state-of-the-art on SPATIALRGPT-Bench.",
      "ai_keywords": [
        "vision-language models",
        "SpatialReasoner-R1",
        "Multi-Model Monte Carlo Tree Search",
        "M3CTS",
        "Long Chain-of-Thought",
        "LongCoT",
        "fine-grained Direct Preference Optimization",
        "fDPO",
        "segment-specific preference granularity",
        "descriptive grounding",
        "logical reasoning",
        "spatial reward mechanism",
        "visual consistency",
        "spatial grounding",
        "logical coherence",
        "SPATIALRGPT-Bench"
      ]
    },
    "publishedAt": "2025-06-26T14:00:00.000Z",
    "title": "Fine-Grained Preference Optimization Improves Spatial Reasoning in VLMs",
    "summary": "Current Vision-Language Models (VLMs) struggle with fine-grained spatial\nreasoning, particularly when multi-step logic and precise spatial alignment are\nrequired. In this work, we introduce SpatialReasoner-R1, a vision-language\nreasoning model designed to address these limitations. To construct\nhigh-quality supervision for spatial reasoning, we design a Multi-Model Monte\nCarlo Tree Search (M3CTS) method that generates diverse, logically consistent\nLong Chain-of-Thought (LongCoT) reasoning trajectories. In addition, we propose\nfine-grained Direct Preference Optimization (fDPO), which introduces\nsegment-specific preference granularity for descriptive grounding and logical\nreasoning, guided by a spatial reward mechanism that evaluates candidate\nresponses based on visual consistency, spatial grounding, and logical\ncoherence. Experimental results demonstrate that fDPO achieves an average\nimprovement of 4.1% over standard DPO across spatial quality tasks, and a 9.0%\ngain in spatial quantity tasks. SpatialReasoner-R1, trained with fDPO, sets a\nnew SoTA on SPATIALRGPT-Bench, outperforming the strongest baseline by 9.8% in\naverage accuracy, while maintaining competitive performance on general\nvision-language tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.21656.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65e387095132c2edd193ae49",
      "avatarUrl": "/avatars/39278e5b026bcbdde88c560fc54018c5.svg",
      "fullname": "Yifan Shen",
      "name": "SivanSX",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.21628",
      "authors": [
        {
          "_id": "686261739e7509383d29ac6e",
          "name": "Magnus Dierking",
          "hidden": false
        },
        {
          "_id": "686261739e7509383d29ac6f",
          "name": "Christopher E. Mower",
          "hidden": false
        },
        {
          "_id": "686261739e7509383d29ac70",
          "name": "Sarthak Das",
          "hidden": false
        },
        {
          "_id": "686261739e7509383d29ac71",
          "name": "Huang Helong",
          "hidden": false
        },
        {
          "_id": "686261739e7509383d29ac72",
          "name": "Jiacheng Qiu",
          "hidden": false
        },
        {
          "_id": "686261739e7509383d29ac73",
          "name": "Cody Reading",
          "hidden": false
        },
        {
          "_id": "686261739e7509383d29ac74",
          "name": "Wei Chen",
          "hidden": false
        },
        {
          "_id": "686261739e7509383d29ac75",
          "name": "Huidong Liang",
          "hidden": false
        },
        {
          "_id": "686261739e7509383d29ac76",
          "name": "Huang Guowei",
          "hidden": false
        },
        {
          "_id": "686261739e7509383d29ac77",
          "name": "Jan Peters",
          "hidden": false
        },
        {
          "_id": "686261739e7509383d29ac78",
          "name": "Quan Xingyue",
          "hidden": false
        },
        {
          "_id": "686261739e7509383d29ac79",
          "name": "Jun Wang",
          "hidden": false
        },
        {
          "_id": "686261739e7509383d29ac7a",
          "name": "Haitham Bou-Ammar",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-24T20:23:39.000Z",
      "submittedOnDailyAt": "2025-06-30T08:38:41.700Z",
      "title": "Ark: ロボット学習向けのオープンソースPythonフレームワーク",
      "submittedOnDailyBy": {
        "_id": "631c375768f7da9ad2496bf6",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/631c375768f7da9ad2496bf6/1sDOoecA6e1v_hn_VAgUq.jpeg",
        "isPro": false,
        "fullname": "Haitham Bou Ammar",
        "user": "hba123",
        "type": "user"
      },
      "summary": "ロボティクスは驚異的なハードウェアの進歩を遂げてきました。DARPAの都市とロボットチャレンジや最初の人間型ロボットキャップボクシングターナメントを含む。しかし、コマーシャルアノマチは機械学習の進歩に追いつかない。ロジックブロックとしてのソフトウェアが大きなブロックを形成しています。現在のロボットスタックは、高い学習曲線、低レベルのC/C++の知識、ツーリングの分散、複雑なハードウェアインターグレーションを要求しています。これは、Pythonを中心にした、詳細に書かれたエコシステムと対照して、現代のAIを駆動するものではありません。ARKというオープンソース、Pythonを先頭とするロボティクスフレームワークを紹介します。ARKは、データの収集、前処理、プロジェクトの学習を行うための最先端の見学学習アルゴリズム（例：ACT、Diffusion Policy）を使用するようにGymタイプの環境インターフェースを提供し、高精度のシミュレーションと物理的なロボットの間でシーズラスに切り替えることができます。軽量級のクライアントサーバーアーキテクチャが、ネットワークプロバイダー・サブサブシャリバリーコミュニケーションを提供し、選択可能なC/C++バインディングも実時間の性能を確保します。ARKは、制御、SLAM、移動計画、システム識別、可視化のための再利用可能なモジュールを含み、ネイティブなROSの互換性も提供しています。詳細な記述と場合の研究を含む、操作や移動検索のプロトタイプ、無痛なハードウェアの交換、端末からのパイプラインの構築を示し、主流の機械学習ワークフローの便利性に匹敵します。ARKは、Pythonを共有フラップ下でロボティクスとAIの実践を統合し、自動ロボットの研究とコマーシャル部署の入門バリューを下げ、加速します。",
      "upvotes": 4,
      "discussionId": "686261739e7509383d29ac7b",
      "ai_summary": "ARK is an open-source Python-first framework that integrates modern imitation-learning algorithms and seamless simulation-physical robot interactions to simplify robotics development and deployment.",
      "ai_keywords": [
        "Gym-style environment interface",
        "imitation-learning algorithms",
        "ACT",
        "Diffusion Policy",
        "lightweight client-server architecture",
        "publisher-subscriber communication",
        "reusable modules",
        "control",
        "SLAM",
        "motion planning",
        "system identification",
        "visualization",
        "native ROS interoperability",
        "end-to-end pipelines"
      ]
    },
    "publishedAt": "2025-06-24T16:23:39.000Z",
    "title": "Ark: An Open-source Python-based Framework for Robot Learning",
    "summary": "Robotics has made remarkable hardware strides-from DARPA's Urban and Robotics\nChallenges to the first humanoid-robot kickboxing tournament-yet commercial\nautonomy still lags behind progress in machine learning. A major bottleneck is\nsoftware: current robot stacks demand steep learning curves, low-level C/C++\nexpertise, fragmented tooling, and intricate hardware integration, in stark\ncontrast to the Python-centric, well-documented ecosystems that propelled\nmodern AI. We introduce ARK, an open-source, Python-first robotics framework\ndesigned to close that gap. ARK presents a Gym-style environment interface that\nallows users to collect data, preprocess it, and train policies using\nstate-of-the-art imitation-learning algorithms (e.g., ACT, Diffusion Policy)\nwhile seamlessly toggling between high-fidelity simulation and physical robots.\nA lightweight client-server architecture provides networked\npublisher-subscriber communication, and optional C/C++ bindings ensure\nreal-time performance when needed. ARK ships with reusable modules for control,\nSLAM, motion planning, system identification, and visualization, along with\nnative ROS interoperability. Comprehensive documentation and case studies-from\nmanipulation to mobile navigation-demonstrate rapid prototyping, effortless\nhardware swapping, and end-to-end pipelines that rival the convenience of\nmainstream machine-learning workflows. By unifying robotics and AI practices\nunder a common Python umbrella, ARK lowers entry barriers and accelerates\nresearch and commercial deployment of autonomous robots.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.21628.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "631c375768f7da9ad2496bf6",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/631c375768f7da9ad2496bf6/1sDOoecA6e1v_hn_VAgUq.jpeg",
      "fullname": "Haitham Bou Ammar",
      "name": "hba123",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 16
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.19741",
      "authors": [
        {
          "_id": "686209869e7509383d29ab92",
          "name": "Yihong Luo",
          "hidden": false
        },
        {
          "_id": "686209869e7509383d29ab93",
          "name": "Shuchen Xue",
          "hidden": false
        },
        {
          "_id": "686209869e7509383d29ab94",
          "name": "Tianyang Hu",
          "hidden": false
        },
        {
          "_id": "686209869e7509383d29ab95",
          "name": "Jing Tang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-24T15:58:55.000Z",
      "submittedOnDailyAt": "2025-06-30T02:20:48.977Z",
      "title": "ノイズ一貫性トレーニング：一歩ジェネレータの原生アプローチでの追加制御学習",
      "submittedOnDailyBy": {
        "_id": "65f7e6856bd4bac5b6a4ecc3",
        "avatarUrl": "/avatars/9c0c48310fb5e99c19700316a0ab531e.svg",
        "isPro": false,
        "fullname": "Yihong Luo",
        "user": "Luo-Yihong",
        "type": "user"
      },
      "summary": "高品質内容生成の効率化と制御可能性の追求は、人工知能生成内容（AIGC）の中心的な課題です。一ステップジェネレーターは、拡散熱蒸発手法によって優れた生成質と計算効率を提供しますが、新しい制御条件への適応（構造的制約、語義ガイドライン、外部入力など）には大きな課題があります。傳統的なアプローチは、基盤モデルの計算量の高い変更と後続の拡散熱蒸発に必要とします。本論文では、新しい制御信号を直接応用できる新しい、軽量のアプローチであるノイズ一貫性トレーニング（NCT）を紹介します。NCTは、元のトレーニング画像のアクセスや基盤拡散モデルの再トレーニングを必要としません。NCTは、アダプタモジュールを挿入し、ジェネレーターのノイズ空間でのノイズ一貫性損失を使用して効果的に実現します。この損失は、制御条件によって程度異なる条件付きノイズでの適応モデルの生成行為を一致させ、新しい制御に従うように暗黙的にガイドします。理論的には、このトレーニング目標は、適応モデルと新しい条件による条件付き分布の分布離差の最小化として理解できます。NCTは、モジュール化されている、データ効率的で、簡単に実装可能で、そのみにプレトレーンされた一ステップジェネレーターと制御信号モデルを依存します。詳細な実験は、NCTは一ステップで最先端の制御可能な生成を実現し、生成質と計算効率において現在の多ステップや拡散熱蒸発ベースの方法を超えることを示します。コードは、https://github.com/Luo-Yihong/NCT にアクセスできます。",
      "upvotes": 4,
      "discussionId": "686209869e7509383d29ab96",
      "ai_summary": "A novel Noise Consistency Training approach integrates new control signals into pre-trained one-step generators efficiently without retraining, outperforming existing methods in quality and computational efficiency.",
      "ai_keywords": [
        "diffusion distillation",
        "Noise Consistency Training",
        "NCT",
        "one-step generators",
        "adapter module",
        "noise consistency loss",
        "noise space",
        "conditional distribution",
        "generative modeling",
        "data-efficient",
        "computational efficiency"
      ]
    },
    "publishedAt": "2025-06-24T11:58:55.000Z",
    "title": "Noise Consistency Training: A Native Approach for One-Step Generator in\n  Learning Additional Controls",
    "summary": "The pursuit of efficient and controllable high-quality content generation\nremains a central challenge in artificial intelligence-generated content\n(AIGC). While one-step generators, enabled by diffusion distillation\ntechniques, offer excellent generation quality and computational efficiency,\nadapting them to new control conditions--such as structural constraints,\nsemantic guidelines, or external inputs--poses a significant challenge.\nConventional approaches often necessitate computationally expensive\nmodifications to the base model and subsequent diffusion distillation. This\npaper introduces Noise Consistency Training (NCT), a novel and lightweight\napproach to directly integrate new control signals into pre-trained one-step\ngenerators without requiring access to original training images or retraining\nthe base diffusion model. NCT operates by introducing an adapter module and\nemploys a noise consistency loss in the noise space of the generator. This loss\naligns the adapted model's generation behavior across noises that are\nconditionally dependent to varying degrees, implicitly guiding it to adhere to\nthe new control. Theoretically, this training objective can be understood as\nminimizing the distributional distance between the adapted generator and the\nconditional distribution induced by the new conditions. NCT is modular,\ndata-efficient, and easily deployable, relying only on the pre-trained one-step\ngenerator and a control signal model. Extensive experiments demonstrate that\nNCT achieves state-of-the-art controllable generation in a single forward pass,\nsurpassing existing multi-step and distillation-based methods in both\ngeneration quality and computational efficiency. Code is available at\nhttps://github.com/Luo-Yihong/NCT",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.19741.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65f7e6856bd4bac5b6a4ecc3",
      "avatarUrl": "/avatars/9c0c48310fb5e99c19700316a0ab531e.svg",
      "fullname": "Yihong Luo",
      "name": "Luo-Yihong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.21411",
      "authors": [
        {
          "_id": "686237f69e7509383d29abe9",
          "user": {
            "_id": "64d5deb154bb9eb704f83122",
            "avatarUrl": "/avatars/86ce09bcca903319051e2307581a43f4.svg",
            "isPro": false,
            "fullname": "Yehui Tang",
            "user": "tangyehui",
            "type": "user"
          },
          "name": "Yehui Tang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-30T07:11:35.262Z",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abea",
          "name": "Xiaosong Li",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abeb",
          "user": {
            "_id": "64b78295479b934973e2c40e",
            "avatarUrl": "/avatars/9213e385964132fa50859264a838d891.svg",
            "isPro": false,
            "fullname": "liu",
            "user": "Fangcheng2",
            "type": "user"
          },
          "name": "Fangcheng Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-30T07:11:52.170Z",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abec",
          "name": "Wei Guo",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abed",
          "name": "Hang Zhou",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abee",
          "name": "Yaoyuan Wang",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abef",
          "name": "Kai Han",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abf0",
          "name": "Xianzhi Yu",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abf1",
          "name": "Jinpeng Li",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abf2",
          "name": "Hui Zang",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abf3",
          "name": "Fei Mi",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abf4",
          "name": "Xiaojun Meng",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abf5",
          "name": "Zhicheng Liu",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abf6",
          "name": "Hanting Chen",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abf7",
          "name": "Binfan Zheng",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abf8",
          "name": "Can Chen",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abf9",
          "name": "Youliang Yan",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abfa",
          "name": "Ruiming Tang",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abfb",
          "name": "Peifeng Qin",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abfc",
          "name": "Xinghao Chen",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abfd",
          "name": "Dacheng Tao",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abfe",
          "user": {
            "_id": "658bdf7b925aadd43304f05c",
            "avatarUrl": "/avatars/64d9e9dea27c376c3bc7b2a54efc2a46.svg",
            "isPro": false,
            "fullname": "Yunhe Wang",
            "user": "MightyCrane",
            "type": "user"
          },
          "name": "Yunhe Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-30T07:11:59.146Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-27T16:40:21.000Z",
      "submittedOnDailyAt": "2025-06-30T05:41:23.309Z",
      "title": "Pangu Pro MoE: グループごとのエクスパートの混雑による効率的なスパース性",
      "submittedOnDailyBy": {
        "_id": "63a369d98c0c89dcae3b8329",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a369d98c0c89dcae3b8329/AiH2zjy1cnt9OADAAZMLD.jpeg",
        "isPro": true,
        "fullname": "Adina Yakefu",
        "user": "AdinaY",
        "type": "user"
      },
      "summary": "ミックスオブエキスパート（MoE）が大規模言語モデルに登場し、モデルパラメータ数と学習能力の大幅な増大に伴い、実行コストの小さな代替が期待されることを示しています。しかし、これらのエキスパートの活性化回数が偏りがあり、異なるデバイスで並列実行した際にシステムの効率化が失われることが通常見られています。そこで、ミックスオブグループエキスパート（MoGE）を導入し、エキスパートの選択時にグループ化し、MoEに比べてエキスパートの負担をより均等に調整することができることを示しています。MoGEは、各予約されたエキスパートグループ内で同じ数のエキスパートを活性化させることを制限し、モデル実行が複数のデバイスに分布される場合、このアーキテクチャ設計はデバイス間の計算負荷のバランスを保ち、特に推論フェーズでのトランスポートを大幅に向上させることを保証します。また、Ascend NPUs上でPangu Pro MoEを構築し、MoGEに基づく稀疏モデルで、総パラメータ数720億、各トークンで活性化されるパラメータ数160億を満たしています。Pangu Pro MoEの設計は、Ascend 300I Duoと800I A2を通じて拡張されたシステムシミュレーション研究によって最適化されています。実験結果から、MoGEはモデルの訓練および推論でエキスパートの負担のバランスをより良く、効率的な実行を実現することを示しています。Pangu Pro MoEの推論性能は、1カード当たり1148トークン/秒を達成し、スペクティブアクセルレーションによりさらに1528トークン/秒に達し、比較的Denseモデルを上回ることができます。また、Ascend 300I Duoでのモデル推論のコスト性能比率は非常に良く、Pangu Pro MoEはマジックスペース平行化を用いて訓練できることを示し、100B総パラメータクラス内で先駆的なモデルとして位置を固め、GLM-Z1-32BとQwen3-32Bなどの開放ソースモデルを上回ることを示しています。",
      "upvotes": 4,
      "discussionId": "686237f79e7509383d29abff",
      "ai_summary": "Mixture of Grouped Experts (MoGE) improves expert load balancing and execution efficiency for large language models, enhancing throughput and cost-to-performance on Ascend NPUs.",
      "ai_keywords": [
        "Mixture of Experts (MoE)",
        "Mixture of Grouped Experts (MoGE)",
        "large language models",
        "expert load balancing",
        "computational load",
        "inference phase",
        "sparse model",
        "Ascend NPUs",
        "system simulation",
        "speculative acceleration",
        "Dense models",
        "GLM-Z1-32B",
        "Qwen3-32B"
      ]
    },
    "publishedAt": "2025-05-27T12:40:21.000Z",
    "title": "Pangu Pro MoE: Mixture of Grouped Experts for Efficient Sparsity",
    "summary": "The surgence of Mixture of Experts (MoE) in Large Language Models promises a\nsmall price of execution cost for a much larger model parameter count and\nlearning capacity, because only a small fraction of parameters are activated\nfor each input token. However, it is commonly observed that some experts are\nactivated far more often than others, leading to system inefficiency when\nrunning the experts on different devices in parallel. Therefore, we introduce\nMixture of Grouped Experts (MoGE), which groups the experts during selection\nand balances the expert workload better than MoE in nature. It constrains\ntokens to activate an equal number of experts within each predefined expert\ngroup. When a model execution is distributed on multiple devices, this\narchitectural design ensures a balanced computational load across devices,\nsignificantly enhancing throughput, particularly for the inference phase.\nFurther, we build Pangu Pro MoE on Ascend NPUs, a sparse model based on MoGE\nwith 72 billion total parameters, 16 billion of which are activated for each\ntoken. The configuration of Pangu Pro MoE is optimized for Ascend 300I Duo and\n800I A2 through extensive system simulation studies. Our experiments indicate\nthat MoGE indeed leads to better expert load balancing and more efficient\nexecution for both model training and inference on Ascend NPUs. The inference\nperformance of Pangu Pro MoE achieves 1148 tokens/s per card and can be further\nimproved to 1528 tokens/s per card by speculative acceleration, outperforming\ncomparable 32B and 72B Dense models. Furthermore, we achieve an excellent\ncost-to-performance ratio for model inference on Ascend 300I Duo. Our studies\nshow that Ascend NPUs are capable of training Pangu Pro MoE with massive\nparallelization to make it a leading model within the sub-100B total parameter\nclass, outperforming prominent open-source models like GLM-Z1-32B and\nQwen3-32B.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21411.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a369d98c0c89dcae3b8329",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a369d98c0c89dcae3b8329/AiH2zjy1cnt9OADAAZMLD.jpeg",
      "fullname": "Adina Yakefu",
      "name": "AdinaY",
      "type": "user",
      "isPro": true,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 774
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.22419",
      "authors": [
        {
          "_id": "686229249e7509383d29abd0",
          "name": "Bingchen Zhao",
          "hidden": false
        },
        {
          "_id": "686229249e7509383d29abd1",
          "name": "Despoina Magka",
          "hidden": false
        },
        {
          "_id": "686229249e7509383d29abd2",
          "name": "Minqi Jiang",
          "hidden": false
        },
        {
          "_id": "686229249e7509383d29abd3",
          "name": "Xian Li",
          "hidden": false
        },
        {
          "_id": "686229249e7509383d29abd4",
          "name": "Roberta Raileanu",
          "hidden": false
        },
        {
          "_id": "686229249e7509383d29abd5",
          "name": "Tatiana Shavrina",
          "hidden": false
        },
        {
          "_id": "686229249e7509383d29abd6",
          "name": "Jean-Christophe Gagnon-Audet",
          "hidden": false
        },
        {
          "_id": "686229249e7509383d29abd7",
          "name": "Kelvin Niu",
          "hidden": false
        },
        {
          "_id": "686229249e7509383d29abd8",
          "name": "Shagun Sodhani",
          "hidden": false
        },
        {
          "_id": "686229249e7509383d29abd9",
          "name": "Michael Shvartsman",
          "hidden": false
        },
        {
          "_id": "686229249e7509383d29abda",
          "name": "Andrei Lupu",
          "hidden": false
        },
        {
          "_id": "686229249e7509383d29abdb",
          "name": "Alisia Lupidi",
          "hidden": false
        },
        {
          "_id": "686229249e7509383d29abdc",
          "name": "Edan Toledo",
          "hidden": false
        },
        {
          "_id": "686229249e7509383d29abdd",
          "name": "Karen Hambardzumyan",
          "hidden": false
        },
        {
          "_id": "686229249e7509383d29abde",
          "name": "Martin Josifoski",
          "hidden": false
        },
        {
          "_id": "686229249e7509383d29abdf",
          "name": "Thomas Foster",
          "hidden": false
        },
        {
          "_id": "686229249e7509383d29abe0",
          "name": "Lucia Cipolina-Kun",
          "hidden": false
        },
        {
          "_id": "686229249e7509383d29abe1",
          "name": "Abhishek Charnalia",
          "hidden": false
        },
        {
          "_id": "686229249e7509383d29abe2",
          "name": "Derek Dunfield",
          "hidden": false
        },
        {
          "_id": "686229249e7509383d29abe3",
          "name": "Alexander H. Miller",
          "hidden": false
        },
        {
          "_id": "686229249e7509383d29abe4",
          "name": "Oisin Mac Aodha",
          "hidden": false
        },
        {
          "_id": "686229249e7509383d29abe5",
          "name": "Jakob Foerster",
          "hidden": false
        },
        {
          "_id": "686229249e7509383d29abe6",
          "name": "Yoram Bachrach",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-27T17:44:32.000Z",
      "submittedOnDailyAt": "2025-06-30T06:29:15.385Z",
      "title": "自動化LLMスピードランニングベンチマーク：NanoGPTの再現と改善",
      "submittedOnDailyBy": {
        "_id": "62dcd71075e9787ec5aa41ba",
        "avatarUrl": "/avatars/f37ce036b76180ed0fa004f9c8c09363.svg",
        "isPro": true,
        "fullname": "Bingchen Zhao",
        "user": "tennant",
        "type": "user"
      },
      "summary": "大語言モデル（LLMs）の急速な進歩は科学の進歩に役立つことができる。この試みの重要な能力は、既存の仕事を再現する能力です。AIアグエントが活発な研究領域で結果を再現する能力を評価するために、我々は、NanoGPT speedrunの研究コミュニティの貢献を活用したAutomated LLM Speedrunning Benchmarkを介して導入します。このコンテストでは、最短の時間でGPT-2モデルを訓練することを目指しています。19つのspeedrunタスクのそれぞれについて、アグエントは前のレコード、訓練スクリプトを提供され、その3つのヒントフォーマットのどれかを選択的に付与されます。これらのヒントは、プセフコードから論文のような記述までの範囲があります。レコードは設計上速やかに実行され、speedrunの改善は、アルゴリズムの高レベルの進歩からハードウェアに関連付けられた最適化までの多様なコードレベルの変更を含みます。これらの機能は、LLMの訓練を改善する先鋒の問題に対して、価値のあることと実際的であることを示します。我々は、最近の理由モデルとSoTAスキフトの組み合わせが、ヒントを与えられても我々のベンチマークですでに知られているイノベーションを再実装することが難しいことを見出しました。このベンチマークは、LLMの自動化サイエンス再現の能力を簡単に、必要な（しかし十分なものではない）スキルであることを示します。",
      "upvotes": 2,
      "discussionId": "686229249e7509383d29abe7",
      "ai_summary": "An Automated LLM Speedrunning Benchmark evaluates AI agents' ability to reproduce scientific results by leveraging NanoGPT speedrun tasks, indicating that even recent reasoning LLMs struggle with re-implementing known improvements.",
      "ai_keywords": [
        "large language models",
        "LLMs",
        "AI agents",
        "Automated LLM Speedrunning Benchmark",
        "NanoGPT speedrun",
        "GPT-2",
        "high-level algorithmic advancements",
        "hardware-aware optimizations"
      ]
    },
    "publishedAt": "2025-06-27T13:44:32.000Z",
    "title": "The Automated LLM Speedrunning Benchmark: Reproducing NanoGPT\n  Improvements",
    "summary": "Rapid advancements in large language models (LLMs) have the potential to\nassist in scientific progress. A critical capability toward this endeavor is\nthe ability to reproduce existing work. To evaluate the ability of AI agents to\nreproduce results in an active research area, we introduce the Automated LLM\nSpeedrunning Benchmark, leveraging the research community contributions on the\nNanoGPT speedrun, a competition to train a GPT-2 model in the shortest time.\nEach of the 19 speedrun tasks provides the agent with the previous records\ntraining script, optionally paired with one of three hint formats, ranging from\npseudocode to paper-like descriptions of the new records improvements. Records\nexecute quickly by design and speedrun improvements encompass diverse\ncode-level changes, ranging from high-level algorithmic advancements to\nhardware-aware optimizations. These features make the benchmark both accessible\nand realistic for the frontier problem of improving LLM training. We find that\nrecent reasoning LLMs combined with SoTA scaffolds struggle to reimplement\nalready-known innovations in our benchmark, even when given detailed hints. Our\nbenchmark thus provides a simple, non-saturated measure of an LLMs ability to\nautomate scientific reproduction, a necessary (but not sufficient) skill for an\nautonomous research agent.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.22419.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62dcd71075e9787ec5aa41ba",
      "avatarUrl": "/avatars/f37ce036b76180ed0fa004f9c8c09363.svg",
      "fullname": "Bingchen Zhao",
      "name": "tennant",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.21594",
      "authors": [
        {
          "_id": "68625a4c9e7509383d29ac4c",
          "name": "Ahmed M. Adly",
          "hidden": false
        },
        {
          "_id": "68625a4c9e7509383d29ac4d",
          "name": "Mostafa Samy",
          "hidden": false
        },
        {
          "_id": "68625a4c9e7509383d29ac4e",
          "name": "Amr Fawzy",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-18T09:44:21.000Z",
      "submittedOnDailyAt": "2025-06-30T08:07:09.494Z",
      "title": "ガゼル-R1: 最先端の医療論理を達成するためのパラメータ効率的な2段階トレーニング",
      "submittedOnDailyBy": {
        "_id": "63aca106e3b217fb36cf1950",
        "avatarUrl": "/avatars/b37cc9102f875b6ce0c55a294c052078.svg",
        "isPro": false,
        "fullname": "Ahmed Mostafa",
        "user": "AhmedMostafa",
        "type": "user"
      },
      "summary": "ガザル-R1、320億パラメータの言語モデルを紹介します。このモデルは医学論理の最先端の性能を達成し、臨床判断のための転がりの通り、ステップごとの説明を提供します。Qwen3 32Bに基づいて構築されたこのモデルは、戦略的な訓練が専門領域で中間サイズのモデルが大幅に大きなコンペナントを上回ることを示します。新しい2段階訓練プインプルを開発しました。最初に、107,033例の合成的な医学論理例のコレクションによるサブプロモーション調整を行い、構造化された臨床的な思考を教え、Weight-Decomposed Low-Rank Adaptation (DoRA) や Rank-Stabilized LoRA (rsLoRA) などのパラメータ効率的な技術を加味しました。次に、Group Relative Policy Optimization (GRPO) を用いた強化学習を行い、精度、フォーマットの遵守、論理の品質を精練する複雑な多コンポーネント賞与システムを使用しました。ガザル-R1は医学ベンチマークで特に優秀な性能を収め、MedQAで87.1%、MMLU Pro (Medical)で81.6%、PubMedQAで79.6%のスコアを達成し、12倍以上の大きなモデルを上回ります。この研究は、専門領域で理由論理能力を持つモデルの訓練における課題について詳細な見解を提供し、賞与ハッキング、訓練の不穩定、事実的な記憶と詳細な論理の基本的な緊張関係などの問題について説明します。我々の方法論は、性能、効率、説明性をバランスとした高性能的な専門領域の言語モデルの開発において再現可能なフレームワークを提供します。",
      "upvotes": 2,
      "discussionId": "68625a4c9e7509383d29ac4f",
      "ai_summary": "Gazal-R1, a 32-billion-parameter language model, achieves top performance in medical reasoning through strategic training, including advanced parameter-efficient techniques and reinforcement learning, providing detailed explanations for clinical decisions.",
      "ai_keywords": [
        "Weight-Decomposed Low-Rank Adaptation (DoRA)",
        "Rank-Stabilized LoRA (rsLoRA)",
        "Group Relative Policy Optimization (GRPO)",
        "MedQA",
        "MMLU Pro (Medical)",
        "PubMedQA",
        "reasoning-capable models",
        "reward hacking",
        "training instability"
      ]
    },
    "publishedAt": "2025-06-18T05:44:21.000Z",
    "title": "Gazal-R1: Achieving State-of-the-Art Medical Reasoning with\n  Parameter-Efficient Two-Stage Training",
    "summary": "We present Gazal-R1, a 32-billion-parameter language model that achieves\nstate-of-the-art performance in medical reasoning while providing transparent,\nstep-by-step explanations for clinical decision-making. Built upon Qwen3 32B,\nour model demonstrates that strategic training can enable mid-sized models to\noutperform significantly larger counterparts in specialized domains. We\ndeveloped a novel two-stage training pipeline: first, supervised fine-tuning on\na carefully curated dataset of 107,033 synthetic medical reasoning examples\nthat teaches structured clinical thinking, enhanced by advanced\nparameter-efficient techniques including Weight-Decomposed Low-Rank Adaptation\n(DoRA) and Rank-Stabilized LoRA (rsLoRA); second, reinforcement learning using\nGroup Relative Policy Optimization (GRPO) with a sophisticated multi-component\nreward system that refines accuracy, format adherence, and reasoning quality.\nGazal-R1 achieves exceptional performance across medical benchmarks, scoring\n87.1% on MedQA, 81.6% on MMLU Pro (Medical), and 79.6% on PubMedQA, surpassing\nmodels up to 12x larger. Beyond its strong empirical results, this work\nprovides detailed insights into the challenges of training reasoning-capable\nmodels in specialized domains, including issues with reward hacking, training\ninstability, and the fundamental tension between factual recall and detailed\nreasoning. Our methodology offers a reproducible framework for developing\nhigh-capability, domain-specific language models that balance performance,\nefficiency, and explainability.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.21594.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63aca106e3b217fb36cf1950",
      "avatarUrl": "/avatars/b37cc9102f875b6ce0c55a294c052078.svg",
      "fullname": "Ahmed Mostafa",
      "name": "AhmedMostafa",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.22149",
      "authors": [
        {
          "_id": "68625f5d9e7509383d29ac62",
          "name": "Ronald Fecso",
          "hidden": false
        },
        {
          "_id": "68625f5d9e7509383d29ac63",
          "name": "José Morano",
          "hidden": false
        },
        {
          "_id": "68625f5d9e7509383d29ac64",
          "name": "Ursula Schmidt-Erfurth",
          "hidden": false
        },
        {
          "_id": "68625f5d9e7509383d29ac65",
          "name": "Hrvoje Bogunović",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-27T11:53:54.000Z",
      "submittedOnDailyAt": "2025-06-30T08:31:01.610Z",
      "title": "RetFiner: 視覚言語の精進戦略による睫状体基盤モデル",
      "submittedOnDailyBy": {
        "_id": "655b3383ed8df831286969f0",
        "avatarUrl": "/avatars/38f9a73c6ec40ba0e00de5bffec03bc0.svg",
        "isPro": false,
        "fullname": "José Morano",
        "user": "j-morano",
        "type": "user"
      },
      "summary": "ビデオインセンシング技術の進歩と深層学習（DL）の進展により、医療機関員や研究者たちは結膜病のステージ分類をストリームライン化できるようになりました。プロパイドなデータを学習するためのディープ学習のプロキシマイド学習（SSL）が人気があり、これは大量の無ラベルデータを利用してコストの計算を避けることができます。SSLは基盤モデル（FM）の開発を可能にしましたが、現在のOCT用FMは画像データだけで訓練されているため、画像の詳細な語意的理解が欠落しています。これは、下流データの性能（特に複雑なタスクに対して）により明らかにされています。これらのFMは特定のアプリケーションや人口に適合するためには、チェックフィードバック（可能ではあるが、実現できない場合もある）が必要となります。これを解決するために、私たちはRetFinerを提案します。RetFinerはSSLを用いた視覚言語精調整シェームで、現在のFMの表現を改善し、特定の人口に効率的に直接適用できるようにします。私たちの方法は、豊富なテキストデータにおける豊富なサブジェクト信号を利用して、多様なトレーニングオブジェクティブを使用しています。RetFinerはOCT用FMのRETFound、UrFound、VisionFMに対して検証され、7つの非常に多様なOCT分類タスクにおいて線形プロビング性能に显著な向上を示しました。平均的には、基準データよりそれぞれ5.8、3.9、2.1パーセント点の増加を示しました。私たちのコードとモデル重みは、https://github.com/ronnief1/RetFinerで公開しています。",
      "upvotes": 1,
      "discussionId": "68625f5d9e7509383d29ac66",
      "githubRepo": "https://github.com/ronnief1/RetFiner",
      "ai_summary": "RetFiner, a vision-language refinement scheme, enhances self-supervised foundation models for OCT by leveraging textual data, improving their downstream performance in retinal disease classification tasks.",
      "ai_keywords": [
        "optical coherence tomography (OCT)",
        "deep learning (DL)",
        "self-supervised learning (SSL)",
        "foundation models (FMs)",
        "supervised fine-tuning",
        "RetFiner",
        "vision-language refinement",
        "linear probing performance"
      ],
      "githubStars": 2
    },
    "publishedAt": "2025-06-27T07:53:54.000Z",
    "title": "RetFiner: A Vision-Language Refinement Scheme for Retinal Foundation\n  Models",
    "summary": "The rise of imaging techniques such as optical coherence tomography (OCT) and\nadvances in deep learning (DL) have enabled clinicians and researchers to\nstreamline retinal disease staging. A popular DL approach is self-supervised\nlearning (SSL), where models learn from vast amounts of unlabeled data,\navoiding costly annotation. SSL has allowed the development of foundation\nmodels (FMs), large models that can be used for a variety of downstream tasks.\nHowever, existing FMs for OCT, trained solely on image data, lack a\ncomprehensive and robust semantic understanding of images, as evidenced by\ntheir downstream performance (especially for complex tasks), and thus require\nsupervised fine-tuning (which may be unfeasible) to better adapt to specific\napplications and populations. To address this, we propose RetFiner, an SSL\nvision-language refinement scheme that improves the representations of existing\nFMs and enables their efficient and direct adaptation to specific populations\nfor improved downstream performance. Our method uses a diverse set of training\nobjectives which take advantage of the rich supervisory signal found in textual\ndata. We tested RetFiner on the retinal FMs RETFound, UrFound, and VisionFM,\nshowing significant improvements in linear probing performance on seven highly\ndiverse OCT classification tasks, with an average increase of 5.8, 3.9, and 2.1\npercentage points over their baselines, respectively. Our code and model\nweights are publicly available at https://github.com/ronnief1/RetFiner.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.22149.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "655b3383ed8df831286969f0",
      "avatarUrl": "/avatars/38f9a73c6ec40ba0e00de5bffec03bc0.svg",
      "fullname": "José Morano",
      "name": "j-morano",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  }
]