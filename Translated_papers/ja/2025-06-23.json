[
  {
    "paper": {
      "id": "2506.16406",
      "authors": [
        {
          "_id": "6858d099c0c8e29df8ea3ccb",
          "name": "Zhiyuan Liang",
          "hidden": false
        },
        {
          "_id": "6858d099c0c8e29df8ea3ccc",
          "name": "Dongwen Tang",
          "hidden": false
        },
        {
          "_id": "6858d099c0c8e29df8ea3ccd",
          "name": "Yuhao Zhou",
          "hidden": false
        },
        {
          "_id": "6858d099c0c8e29df8ea3cce",
          "name": "Xuanlei Zhao",
          "hidden": false
        },
        {
          "_id": "6858d099c0c8e29df8ea3ccf",
          "name": "Mingjia Shi",
          "hidden": false
        },
        {
          "_id": "6858d099c0c8e29df8ea3cd0",
          "name": "Wangbo Zhao",
          "hidden": false
        },
        {
          "_id": "6858d099c0c8e29df8ea3cd1",
          "name": "Zekai Li",
          "hidden": false
        },
        {
          "_id": "6858d099c0c8e29df8ea3cd2",
          "name": "Peihao Wang",
          "hidden": false
        },
        {
          "_id": "6858d099c0c8e29df8ea3cd3",
          "name": "Konstantin Schürholt",
          "hidden": false
        },
        {
          "_id": "6858d099c0c8e29df8ea3cd4",
          "name": "Damian Borth",
          "hidden": false
        },
        {
          "_id": "6858d099c0c8e29df8ea3cd5",
          "name": "Michael M. Bronstein",
          "hidden": false
        },
        {
          "_id": "6858d099c0c8e29df8ea3cd6",
          "name": "Yang You",
          "hidden": false
        },
        {
          "_id": "6858d099c0c8e29df8ea3cd7",
          "name": "Zhangyang Wang",
          "hidden": false
        },
        {
          "_id": "6858d099c0c8e29df8ea3cd8",
          "user": {
            "_id": "655452b8432af1b1116394d1",
            "avatarUrl": "/avatars/85860fb3c2d09c9c23e7677d7129cca3.svg",
            "isPro": false,
            "fullname": "Kai Wang",
            "user": "VictorKai1996NUS",
            "type": "user"
          },
          "name": "Kai Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-23T08:14:35.657Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/655452b8432af1b1116394d1/ZdZe7m_v8V6alBd1NZa-m.mp4"
      ],
      "publishedAt": "2025-06-19T15:38:21.000Z",
      "submittedOnDailyAt": "2025-06-23T02:39:37.807Z",
      "title": "Drag-and-Drop LLMs: Zero-Shot Prompt-to-Weights",
      "submittedOnDailyBy": {
        "_id": "655452b8432af1b1116394d1",
        "avatarUrl": "/avatars/85860fb3c2d09c9c23e7677d7129cca3.svg",
        "isPro": false,
        "fullname": "Kai Wang",
        "user": "VictorKai1996NUS",
        "type": "user"
      },
      "summary": "現代のパラメーター効率的調整（PEFT）手法のサンプルとして、低レンジアダプタイション（LoRA）が大規模言語モデル（LLMs）のカスタマイズコストを減らしますが、それらはダウンストリームデータセットごとに別の最適化実行が必要です。我々は、Drag-and-Drop LLMs（DnD）を紹介します。DnDは、少数の無ラベルタスクプロンプトを直接LoRAの重み更新にマッピングするプロンプト条件付きパラメータージェネレーターで、タスクごとのトレーニングを排除します。軽量なテキストエンコーダーは、各プロンプトバッチを条件埋めベクトルに変換し、これらは連続ハイパーコンバニショナルデコーダーで全てのLoRA行列に変換されます。多様なプロンプトチェックポイントペアの集合で訓練されたDnDは、タスク専用パラメーターを秒毎に生成し、i）全ファイルチューニングより12,000倍のオーバーヘッドを減らし、ii）見たことのない常識推理、数学、コーディング、モデル多様性ベンチマークで最強の訓練LoRAより平均30％の性能向上を収め、iii）目標データまたはラベルを見たことのない場合でも強力的なクロスデータ域拡張性を示します。我々の結果は、プロンプト条件付きパラメーター生成は、勾配基準的調整によるLLMsの迅速な特殊化の代替手段であることを示しています。我々のプロジェクトは、https://jerryliang24.github.io/DnD{https://jerryliang24.github.io/DnD}にアクセスできます。",
      "upvotes": 56,
      "discussionId": "6858d099c0c8e29df8ea3cd9",
      "projectPage": "https://jerryliang24.github.io/DnD/",
      "ai_summary": "Drag-and-Drop LLMs generate task-specific parameters through prompt-conditioned parameter generation, achieving significant efficiency gains and cross-domain generalization without per-task training.",
      "ai_keywords": [
        "Parameter-Efficient Fine-Tuning",
        "PEFT",
        "low-rank adaptation",
        "LoRA",
        "large language models",
        "prompts",
        "condition embeddings",
        "hyper-convolutional decoder",
        "LoRA matrices",
        "common-sense reasoning",
        "math",
        "coding",
        "multimodal benchmarks"
      ]
    },
    "publishedAt": "2025-06-19T11:38:21.000Z",
    "title": "Drag-and-Drop LLMs: Zero-Shot Prompt-to-Weights",
    "summary": "Modern Parameter-Efficient Fine-Tuning (PEFT) methods such as low-rank\nadaptation (LoRA) reduce the cost of customizing large language models (LLMs),\nyet still require a separate optimization run for every downstream dataset. We\nintroduce Drag-and-Drop LLMs (\\textit{DnD)}, a prompt-conditioned\nparameter generator that eliminates per-task training by mapping a handful of\nunlabeled task prompts directly to LoRA weight updates. A lightweight text\nencoder distills each prompt batch into condition embeddings, which are then\ntransformed by a cascaded hyper-convolutional decoder into the full set of LoRA\nmatrices. Once trained in a diverse collection of prompt-checkpoint pairs, DnD\nproduces task-specific parameters in seconds, yielding i) up to\n12,000times lower overhead than full fine-tuning, ii) average gains\nup to 30\\% in performance over the strongest training LoRAs on unseen\ncommon-sense reasoning, math, coding, and multimodal benchmarks, and iii)\nrobust cross-domain generalization despite never seeing the target data or\nlabels. Our results demonstrate that prompt-conditioned parameter generation is\na viable alternative to gradient-based adaptation for rapidly specializing\nLLMs. Our project is available at\nhttps://jerryliang24.github.io/DnD{https://jerryliang24.github.io/DnD}.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/655452b8432af1b1116394d1/ZdZe7m_v8V6alBd1NZa-m.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.16406.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "655452b8432af1b1116394d1",
      "avatarUrl": "/avatars/85860fb3c2d09c9c23e7677d7129cca3.svg",
      "fullname": "Kai Wang",
      "name": "VictorKai1996NUS",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.16054",
      "authors": [
        {
          "_id": "6858e225c0c8e29df8ea3d0f",
          "user": {
            "_id": "6454568636821f6860fed410",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6454568636821f6860fed410/VsKn0GawOBZo1hmqHnmV1.png",
            "isPro": false,
            "fullname": "Tianchen Zhao",
            "user": "A-suozhang",
            "type": "user"
          },
          "name": "Tianchen Zhao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-23T08:14:13.425Z",
          "hidden": false
        },
        {
          "_id": "6858e225c0c8e29df8ea3d10",
          "name": "Ke Hong",
          "hidden": false
        },
        {
          "_id": "6858e225c0c8e29df8ea3d11",
          "name": "Xinhao Yang",
          "hidden": false
        },
        {
          "_id": "6858e225c0c8e29df8ea3d12",
          "name": "Xuefeng Xiao",
          "hidden": false
        },
        {
          "_id": "6858e225c0c8e29df8ea3d13",
          "name": "Huixia Li",
          "hidden": false
        },
        {
          "_id": "6858e225c0c8e29df8ea3d14",
          "name": "Feng Ling",
          "hidden": false
        },
        {
          "_id": "6858e225c0c8e29df8ea3d15",
          "name": "Ruiqi Xie",
          "hidden": false
        },
        {
          "_id": "6858e225c0c8e29df8ea3d16",
          "name": "Siqi Chen",
          "hidden": false
        },
        {
          "_id": "6858e225c0c8e29df8ea3d17",
          "name": "Hongyu Zhu",
          "hidden": false
        },
        {
          "_id": "6858e225c0c8e29df8ea3d18",
          "name": "Yichong Zhang",
          "hidden": false
        },
        {
          "_id": "6858e225c0c8e29df8ea3d19",
          "name": "Yu Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-19T06:25:02.000Z",
      "submittedOnDailyAt": "2025-06-23T03:52:52.372Z",
      "title": "パロアテンション：パターン覚え取りのための効率的なスパースおよびキュアテーションされたアテンションを実現する",
      "submittedOnDailyBy": {
        "_id": "6454568636821f6860fed410",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6454568636821f6860fed410/VsKn0GawOBZo1hmqHnmV1.png",
        "isPro": false,
        "fullname": "Tianchen Zhao",
        "user": "A-suozhang",
        "type": "user"
      },
      "summary": "ビジュアル生成では、アテンション機構の二次元複雑性がメモリと計算コストを高くする問題があり、高解像度画像や多フレームビデオ生成に必要な長いトークンシーケンスに対して特に厳しい。これに対して、先行研究ではスパーシファイゼーションとクォンティシャイゼーションなどの手法を試みているが、低密度と減少したビット幅の状況下では大きな課題がある。システム的な分析を通じて、我々はアテンションパターンの分散および不規則な特徴が本質的な難問であることを認識した。したがって、このようなパターンを満たすための特殊化されたスパーシファイゼーションやクォンティシャイゼーションの設計を導入するよりも、アテンションパターンを再組織するような戦略を提案している。ビジュアル特徴抽出の局所的な集約性によるインスピレーションを受け、新しい**パターン観的なトークン再ソート（PARO）**技術を設計し、多様なアテンションパターンをハードウェアに近いブロックウィスエージョンパターンに統合する。この統合はスパーシファイゼーションとクォンティシャイゼーションを大幅に簡単化し、強化する。各々の設計選択の性能・効率の転換を評価し、統一的なパターンに適した方法学を最終的に決定する。我々のアプローチ、**PAROAttention**、は無失格メトリックでのビデオおよび画像生成を実現し、近似した結果をFPベースラインで得ることができ、顕著に低い密度（約20%-30%）とビット幅（**INT8/INT4**）で動作し、終端からのラテンティースピードアップが**1.9x**から**2.7x**まで達成する。",
      "upvotes": 35,
      "discussionId": "6858e225c0c8e29df8ea3d1a",
      "projectPage": "https://a-suozhang.xyz/paroattn.github.io/",
      "ai_summary": "PAROAttention reorganizes visual attention patterns to enable efficient sparsification and quantization, reducing memory and computational costs with minimal impact on performance.",
      "ai_keywords": [
        "attention mechanisms",
        "sparsification",
        "quantization",
        "visual attention patterns",
        "Pattern-Aware token ReOrdering (PARO)",
        "local aggregation",
        "hardware-friendly block-wise pattern",
        "end-to-end latency speedup",
        "INT8/INT4",
        "PAROAttention"
      ]
    },
    "publishedAt": "2025-06-19T02:25:02.000Z",
    "title": "PAROAttention: Pattern-Aware ReOrdering for Efficient Sparse and\n  Quantized Attention in Visual Generation Models",
    "summary": "In visual generation, the quadratic complexity of attention mechanisms\nresults in high memory and computational costs, especially for longer token\nsequences required in high-resolution image or multi-frame video generation. To\naddress this, prior research has explored techniques such as sparsification and\nquantization. However, these techniques face significant challenges under low\ndensity and reduced bitwidths. Through systematic analysis, we identify that\nthe core difficulty stems from the dispersed and irregular characteristics of\nvisual attention patterns. Therefore, instead of introducing specialized\nsparsification and quantization design to accommodate such patterns, we propose\nan alternative strategy: *reorganizing* the attention pattern to alleviate the\nchallenges. Inspired by the local aggregation nature of visual feature\nextraction, we design a novel **Pattern-Aware token ReOrdering (PARO)**\ntechnique, which unifies the diverse attention patterns into a\nhardware-friendly block-wise pattern. This unification substantially simplifies\nand enhances both sparsification and quantization. We evaluate the\nperformance-efficiency trade-offs of various design choices and finalize a\nmethodology tailored for the unified pattern. Our approach, **PAROAttention**,\nachieves video and image generation with lossless metrics, and nearly identical\nresults from full-precision (FP) baselines, while operating at notably lower\ndensity (~20%-30%) and bitwidth (**INT8/INT4**), achieving a **1.9x** to\n**2.7x** end-to-end latency speedup.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.16054.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6454568636821f6860fed410",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6454568636821f6860fed410/VsKn0GawOBZo1hmqHnmV1.png",
      "fullname": "Tianchen Zhao",
      "name": "A-suozhang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.16035",
      "authors": [
        {
          "_id": "6858d76cc0c8e29df8ea3cdb",
          "user": {
            "_id": "638828121901766b88076aa1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638828121901766b88076aa1/rXlOO7eewmmaSN_hQIVz7.jpeg",
            "isPro": false,
            "fullname": "Vishesh Tripathi",
            "user": "vishesh-t27",
            "type": "user"
          },
          "name": "Vishesh Tripathi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-23T08:14:30.878Z",
          "hidden": false
        },
        {
          "_id": "6858d76cc0c8e29df8ea3cdc",
          "name": "Tanmay Odapally",
          "hidden": false
        },
        {
          "_id": "6858d76cc0c8e29df8ea3cdd",
          "name": "Indraneel Das",
          "hidden": false
        },
        {
          "_id": "6858d76cc0c8e29df8ea3cde",
          "user": {
            "_id": "64103f66928400b4164308f0",
            "avatarUrl": "/avatars/6799d4a365776f83cecf7b9f468f3d4f.svg",
            "isPro": false,
            "fullname": "uday allu",
            "user": "udayallu",
            "type": "user"
          },
          "name": "Uday Allu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-23T08:14:33.310Z",
          "hidden": false
        },
        {
          "_id": "6858d76cc0c8e29df8ea3cdf",
          "name": "Biddwan Ahmed",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-19T05:11:43.000Z",
      "submittedOnDailyAt": "2025-06-23T03:13:41.130Z",
      "title": "ビジョンガイドドラッグはすべてです：多モデル文書理解を用いたRAGの拡張",
      "submittedOnDailyBy": {
        "_id": "638828121901766b88076aa1",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638828121901766b88076aa1/rXlOO7eewmmaSN_hQIVz7.jpeg",
        "isPro": false,
        "fullname": "Vishesh Tripathi",
        "user": "vishesh-t27",
        "type": "user"
      },
      "summary": "レタイブレーディングアウゲンザション（RAG）システムは、情報検索と質問回答に革命的な影響を及ぼしていますが、伝統的なテキストベースのチャンクメソッドは、複雑なドキュメント構造、多ページテーブル、埋め込みの図形、ページ境界を超えるコンテキスト依存関係に対して苦労しています。私たちは、大規模多モーダルモデル（LMMs）を活用した新しい多モーダルドキュメントチャンクアプローチを提案し、PDFドキュメントをバッチ処理しながら意味的な一貫性と構造的な整備を維持することを目的としています。私たちの方法は、構築可能なページバッチでドキュメントを処理し、バッチ間のコンテキスト保存を実現し、多ページのテーブル、埋め込みの可視要素、プロセス内容の正確な処理を可能にします。私たちのアプローチは、手動で作成されたクエリを含むカレーラーデータセット上で評価され、チャンク品質とRAGシステムの下流性能における向上を示しています。我々の視覚ガイドされたアプローチは、伝統的なバンドルRAGシステムに比べてより高い精度を達成し、質的な分析では、ドキュメント構造と意味的な一貫性の更なる保存を示しています。",
      "upvotes": 33,
      "discussionId": "6858d76cc0c8e29df8ea3ce0",
      "ai_summary": "A novel multimodal document chunking approach using Large Multimodal Models (LMMs) enhances RAG performance by accurately processing complex PDF documents, including multi-page tables and embedded visuals.",
      "ai_keywords": [
        "Retrieval-Augmented Generation (RAG)",
        "Large Multimodal Models (LMMs)",
        "document chunking",
        "semantic coherence",
        "structural integrity",
        "cross-batch context preservation",
        "vision-guided approach"
      ]
    },
    "publishedAt": "2025-06-19T01:11:43.000Z",
    "title": "Vision-Guided Chunking Is All You Need: Enhancing RAG with Multimodal\n  Document Understanding",
    "summary": "Retrieval-Augmented Generation (RAG) systems have revolutionized information\nretrieval and question answering, but traditional text-based chunking methods\nstruggle with complex document structures, multi-page tables, embedded figures,\nand contextual dependencies across page boundaries. We present a novel\nmultimodal document chunking approach that leverages Large Multimodal Models\n(LMMs) to process PDF documents in batches while maintaining semantic coherence\nand structural integrity. Our method processes documents in configurable page\nbatches with cross-batch context preservation, enabling accurate handling of\ntables spanning multiple pages, embedded visual elements, and procedural\ncontent. We evaluate our approach on a curated dataset of PDF documents with\nmanually crafted queries, demonstrating improvements in chunk quality and\ndownstream RAG performance. Our vision-guided approach achieves better accuracy\ncompared to traditional vanilla RAG systems, with qualitative analysis showing\nsuperior preservation of document structure and semantic coherence.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.16035.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "638828121901766b88076aa1",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638828121901766b88076aa1/rXlOO7eewmmaSN_hQIVz7.jpeg",
      "fullname": "Vishesh Tripathi",
      "name": "vishesh-t27",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.09049",
      "authors": [
        {
          "_id": "6858c341c0c8e29df8ea3c7f",
          "user": {
            "_id": "64eadcb03d76028d805a7818",
            "avatarUrl": "/avatars/528e4fded4419caf08589b2ed40437bc.svg",
            "isPro": false,
            "fullname": "Li Kang",
            "user": "FACEONG",
            "type": "user"
          },
          "name": "Li Kang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-23T08:14:50.165Z",
          "hidden": false
        },
        {
          "_id": "6858c341c0c8e29df8ea3c80",
          "name": "Xiufeng Song",
          "hidden": false
        },
        {
          "_id": "6858c341c0c8e29df8ea3c81",
          "name": "Heng Zhou",
          "hidden": false
        },
        {
          "_id": "6858c341c0c8e29df8ea3c82",
          "name": "Yiran Qin",
          "hidden": false
        },
        {
          "_id": "6858c341c0c8e29df8ea3c83",
          "name": "Jie Yang",
          "hidden": false
        },
        {
          "_id": "6858c341c0c8e29df8ea3c84",
          "name": "Xiaohong Liu",
          "hidden": false
        },
        {
          "_id": "6858c341c0c8e29df8ea3c85",
          "name": "Philip Torr",
          "hidden": false
        },
        {
          "_id": "6858c341c0c8e29df8ea3c86",
          "name": "Lei Bai",
          "hidden": false
        },
        {
          "_id": "6858c341c0c8e29df8ea3c87",
          "name": "Zhenfei Yin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-10T17:59:44.000Z",
      "submittedOnDailyAt": "2025-06-23T01:31:50.738Z",
      "title": "VIKI-R: 強化学習による体化マルチアグリエントの協調合作",
      "submittedOnDailyBy": {
        "_id": "64eadcb03d76028d805a7818",
        "avatarUrl": "/avatars/528e4fded4419caf08589b2ed40437bc.svg",
        "isPro": false,
        "fullname": "Li Kang",
        "user": "FACEONG",
        "type": "user"
      },
      "summary": "多機体実験体の協調が動的な環境では人工知能の核心的な課題であり、視覚ドリブンの推論とスケーラブルな協力戦略が必要です。最近の研究は、多機体計画にラングスファイアモデル（LLMs）を利用していますが、視覚言語モデル（VLMs）を視覚推論に用いることを始めています。しかし、VLMベースのアプローチは多様な実験体タイプのサポートに限られています。本研究では、VIKI-Benchを紹介します。VIKI-Benchは、実験体多機体協調に調整された最初の階層的ベンチマークで、機体活性化、タスク計画、トラジェクト視覚認識の3つの構造化されたレベルを特徴としています。VIKI-Benchは、多様なロボット実験体、多角度の視覚観測、構造化されたサブジェクション信号を含み、視覚入力に基づく推論を評価します。VIKI-Rという2ステップフレームワークを提案し、連鎖オブショート注釈付きのデモンストレーションで預習された視覚言語モデル（VLM）を最適化し、多レベル報酬信号の下で強化学習を行います。拡大的な実験により、VIKI-Rはすべてのタスクレベルで基準法に比べて显著に優れています。また、強化学習により、異なる機体間の構成複合的な協力パターンの現れ方を示します。VIKI-BenchとVIKI-Rは、実験体AIシステムでの多機体、視覚ドリブンの協調を進めるための統一テストベンチと方法を提供します。",
      "upvotes": 24,
      "discussionId": "6858c341c0c8e29df8ea3c88",
      "projectPage": "https://faceong.github.io/VIKI-R/",
      "githubRepo": "https://github.com/MARS-EAI/VIKI-R",
      "ai_summary": "VIKI-Bench and VIKI-R provide a benchmark and framework for evaluating and improving visual-driven cooperation among diverse embodied agents using vision-language models and reinforcement learning.",
      "ai_keywords": [
        "embodied agents",
        "VIKI-Bench",
        "multi-agent cooperation",
        "vision-language models",
        "VIKI-R",
        "Chain-of-Thought",
        "reinforcement learning",
        "compositional cooperation",
        "multi-level reward signals",
        "robot embodiments",
        "multi-view visual observations",
        "structured supervision signals"
      ]
    },
    "publishedAt": "2025-06-10T13:59:44.000Z",
    "title": "VIKI-R: Coordinating Embodied Multi-Agent Cooperation via Reinforcement\n  Learning",
    "summary": "Coordinating multiple embodied agents in dynamic environments remains a core\nchallenge in artificial intelligence, requiring both perception-driven\nreasoning and scalable cooperation strategies. While recent works have\nleveraged large language models (LLMs) for multi-agent planning, a few have\nbegun to explore vision-language models (VLMs) for visual reasoning. However,\nthese VLM-based approaches remain limited in their support for diverse\nembodiment types. In this work, we introduce VIKI-Bench, the first hierarchical\nbenchmark tailored for embodied multi-agent cooperation, featuring three\nstructured levels: agent activation, task planning, and trajectory perception.\nVIKI-Bench includes diverse robot embodiments, multi-view visual observations,\nand structured supervision signals to evaluate reasoning grounded in visual\ninputs. To demonstrate the utility of VIKI-Bench, we propose VIKI-R, a\ntwo-stage framework that fine-tunes a pretrained vision-language model (VLM)\nusing Chain-of-Thought annotated demonstrations, followed by reinforcement\nlearning under multi-level reward signals. Our extensive experiments show that\nVIKI-R significantly outperforms baselines method across all task levels.\nFurthermore, we show that reinforcement learning enables the emergence of\ncompositional cooperation patterns among heterogeneous agents. Together,\nVIKI-Bench and VIKI-R offer a unified testbed and method for advancing\nmulti-agent, visual-driven cooperation in embodied AI systems.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09049.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64eadcb03d76028d805a7818",
      "avatarUrl": "/avatars/528e4fded4419caf08589b2ed40437bc.svg",
      "fullname": "Li Kang",
      "name": "FACEONG",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.17206",
      "authors": [
        {
          "_id": "6858c5b5c0c8e29df8ea3c95",
          "name": "Yukun Huang",
          "hidden": false
        },
        {
          "_id": "6858c5b5c0c8e29df8ea3c96",
          "name": "Yanning Zhou",
          "hidden": false
        },
        {
          "_id": "6858c5b5c0c8e29df8ea3c97",
          "name": "Jianan Wang",
          "hidden": false
        },
        {
          "_id": "6858c5b5c0c8e29df8ea3c98",
          "name": "Kaiyi Huang",
          "hidden": false
        },
        {
          "_id": "6858c5b5c0c8e29df8ea3c99",
          "name": "Xihui Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-20T17:55:06.000Z",
      "submittedOnDailyAt": "2025-06-23T01:41:47.575Z",
      "title": "DreamCube: 3Dパナラム生成による多平面同期",
      "submittedOnDailyBy": {
        "_id": "638ee900ee7e45e0474a5712",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638ee900ee7e45e0474a5712/KLli_eCbWwffKR7oLDmV3.jpeg",
        "isPro": false,
        "fullname": "Yukun Huang",
        "user": "KevinHuang",
        "type": "user"
      },
      "summary": "3Dパノラマ合成は、高品質で多様な可視性とジオメトリーを要求する望ましいであるが、難しい任務です。現在の方法は、3Dパノラマデータの不足を回避するために、事前学習された2Dベースモデルからの豊富な画像プロイヤーを利用していますが、3Dパノラマと2D単一ビューの不適合性が、その効果性を制限しています。本稿では、2Dベースモデルからのオペレーターに多平面同期を適用することで、その能力を無間に360度領域に拡張できることを示します。この設計に基づいて、また、DreamCubeという3Dパノラマ生成用の多平面RGB-Dディフュージョンモデルを紹介します。このモデルは、2Dベースモデルのプロイヤーを最大限に再利用し、多様な外観と正確なジオメトリーを達成しながら、多ウェイの一致性を維持することを目指しています。幅広い実験は、パノラマ画像生成、パノラマデプストイメーション、3Dシーン生成において、我々のアプローチの効果性を示しています。",
      "upvotes": 13,
      "discussionId": "6858c5b6c0c8e29df8ea3c9a",
      "projectPage": "https://yukun-huang.github.io/DreamCube/",
      "githubRepo": "https://github.com/yukun-huang/DreamCube",
      "ai_summary": "Multi-plane synchronization extends 2D foundation models to 3D panorama generation, introducing DreamCube to achieve diverse appearances and accurate geometry.",
      "ai_keywords": [
        "multi-plane synchronization",
        "2D foundation models",
        "DreamCube",
        "RGB-D diffusion model",
        "panoramic image generation",
        "panoramic depth estimation",
        "3D scene generation"
      ]
    },
    "publishedAt": "2025-06-20T13:55:06.000Z",
    "title": "DreamCube: 3D Panorama Generation via Multi-plane Synchronization",
    "summary": "3D panorama synthesis is a promising yet challenging task that demands\nhigh-quality and diverse visual appearance and geometry of the generated\nomnidirectional content. Existing methods leverage rich image priors from\npre-trained 2D foundation models to circumvent the scarcity of 3D panoramic\ndata, but the incompatibility between 3D panoramas and 2D single views limits\ntheir effectiveness. In this work, we demonstrate that by applying multi-plane\nsynchronization to the operators from 2D foundation models, their capabilities\ncan be seamlessly extended to the omnidirectional domain. Based on this design,\nwe further introduce DreamCube, a multi-plane RGB-D diffusion model for 3D\npanorama generation, which maximizes the reuse of 2D foundation model priors to\nachieve diverse appearances and accurate geometry while maintaining multi-view\nconsistency. Extensive experiments demonstrate the effectiveness of our\napproach in panoramic image generation, panoramic depth estimation, and 3D\nscene generation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.17206.png",
    "numComments": 4,
    "submittedBy": {
      "_id": "638ee900ee7e45e0474a5712",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638ee900ee7e45e0474a5712/KLli_eCbWwffKR7oLDmV3.jpeg",
      "fullname": "Yukun Huang",
      "name": "KevinHuang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.17201",
      "authors": [
        {
          "_id": "6858c46fc0c8e29df8ea3c8a",
          "name": "Jiaqi Li",
          "hidden": false
        },
        {
          "_id": "6858c46fc0c8e29df8ea3c8b",
          "name": "Junshu Tang",
          "hidden": false
        },
        {
          "_id": "6858c46fc0c8e29df8ea3c8c",
          "name": "Zhiyong Xu",
          "hidden": false
        },
        {
          "_id": "6858c46fc0c8e29df8ea3c8d",
          "name": "Longhuang Wu",
          "hidden": false
        },
        {
          "_id": "6858c46fc0c8e29df8ea3c8e",
          "name": "Yuan Zhou",
          "hidden": false
        },
        {
          "_id": "6858c46fc0c8e29df8ea3c8f",
          "name": "Shuai Shao",
          "hidden": false
        },
        {
          "_id": "6858c46fc0c8e29df8ea3c90",
          "name": "Tianbao Yu",
          "hidden": false
        },
        {
          "_id": "6858c46fc0c8e29df8ea3c91",
          "name": "Zhiguo Cao",
          "hidden": false
        },
        {
          "_id": "6858c46fc0c8e29df8ea3c92",
          "name": "Qinglin Lu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-20T17:50:37.000Z",
      "submittedOnDailyAt": "2025-06-23T01:35:50.876Z",
      "title": "フンュァン・ゲームクラフト: ハイダイナミック・インタラクティブ・ゲームビデオ生成にわたって、ハイバード・ヒストリー条件を用いる",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": true,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "最近の拡散ベースと制御可能なビデオ生成の進展は、高品質と時間的な一致性を持つビデオ合成に成功し、満喫的なインタラクティブなゲーム体験の基盤を築いた。しかし、現在の方法は動作、一般性、長期的な一致性と効率性に制限があり、多様なゲームプレイビデオの作成能力を制限している。これらの欠点を解決するために、私たちは、ゲーム環境での高動作性のインタラクティブなビデオ生成の新しいフレームワーク「獻元・ゲームクラフト」を紹介します。動作制御の詳細化を実現するために、標準のキーボードとマウスの入力を共有したカメラ表現空間に統一し、さまざまなカメラと移動操作の間の平滑なインタープローテーションを促進します。次に、ビデオシーケンスを自動回帰的に拡大しながらゲームシーン情報を保存するための混合ヒストリー条件付きの訓練戦略を提案します。また、推論効率と遊び性を向上させるために、モデルの転写を行い、長期的な時系列の一致性を維持しながら計算オーバーヘッドを減らし、複雑なインタラクティブな環境での実時間デプロイメントに適したものにします。モデルは、100以上のAAAゲームのゲームプレイ記録の100万以上を含む大規模なデータセットで訓練され、広く覆われたさまざまなデータセットを確保し、さらに調整された合成データセットで精確性と制御性を向上させます。編集されたゲームシーンデータは、視覚的なフィデリティ、リアリティとアクション制御可能さを大幅に向上させます。拡張された実験は、獻元・ゲームクラフトは既存のモデルを大幅に上回り、インタラクティブなゲームビデオ生成のリアリズムと遊び性を進めることを示しました。",
      "upvotes": 11,
      "discussionId": "6858c46fc0c8e29df8ea3c93",
      "ai_summary": "Hunyuan-GameCraft is a novel framework for high-dynamic interactive video generation in game environments that addresses limitations in dynamics, generality, and efficiency through unified input representation, hybrid history-conditioned training, and model distillation.",
      "ai_keywords": [
        "diffusion-based",
        "controllable video generation",
        "temporally coherent video synthesis",
        "high-dynamic interactive video generation",
        "shared camera representation space",
        "hybrid history-conditioned training strategy",
        "model distillation",
        "real-time deployment",
        "large-scale dataset",
        "synthetic dataset",
        "game scene data"
      ]
    },
    "publishedAt": "2025-06-20T13:50:37.000Z",
    "title": "Hunyuan-GameCraft: High-dynamic Interactive Game Video Generation with\n  Hybrid History Condition",
    "summary": "Recent advances in diffusion-based and controllable video generation have\nenabled high-quality and temporally coherent video synthesis, laying the\ngroundwork for immersive interactive gaming experiences. However, current\nmethods face limitations in dynamics, generality, long-term consistency, and\nefficiency, which limit the ability to create various gameplay videos. To\naddress these gaps, we introduce Hunyuan-GameCraft, a novel framework for\nhigh-dynamic interactive video generation in game environments. To achieve\nfine-grained action control, we unify standard keyboard and mouse inputs into a\nshared camera representation space, facilitating smooth interpolation between\nvarious camera and movement operations. Then we propose a hybrid\nhistory-conditioned training strategy that extends video sequences\nautoregressively while preserving game scene information. Additionally, to\nenhance inference efficiency and playability, we achieve model distillation to\nreduce computational overhead while maintaining consistency across long\ntemporal sequences, making it suitable for real-time deployment in complex\ninteractive environments. The model is trained on a large-scale dataset\ncomprising over one million gameplay recordings across over 100 AAA games,\nensuring broad coverage and diversity, then fine-tuned on a carefully annotated\nsynthetic dataset to enhance precision and control. The curated game scene data\nsignificantly improves the visual fidelity, realism and action controllability.\nExtensive experiments demonstrate that Hunyuan-GameCraft significantly\noutperforms existing models, advancing the realism and playability of\ninteractive game video generation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.17201.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": true,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7170
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.16504",
      "authors": [
        {
          "_id": "6858c1fcc0c8e29df8ea3c63",
          "name": "Zeqiang Lai",
          "hidden": false
        },
        {
          "_id": "6858c1fcc0c8e29df8ea3c64",
          "name": "Yunfei Zhao",
          "hidden": false
        },
        {
          "_id": "6858c1fcc0c8e29df8ea3c65",
          "name": "Haolin Liu",
          "hidden": false
        },
        {
          "_id": "6858c1fcc0c8e29df8ea3c66",
          "name": "Zibo Zhao",
          "hidden": false
        },
        {
          "_id": "6858c1fcc0c8e29df8ea3c67",
          "name": "Qingxiang Lin",
          "hidden": false
        },
        {
          "_id": "6858c1fcc0c8e29df8ea3c68",
          "name": "Huiwen Shi",
          "hidden": false
        },
        {
          "_id": "6858c1fcc0c8e29df8ea3c69",
          "name": "Xianghui Yang",
          "hidden": false
        },
        {
          "_id": "6858c1fcc0c8e29df8ea3c6a",
          "name": "Mingxin Yang",
          "hidden": false
        },
        {
          "_id": "6858c1fcc0c8e29df8ea3c6b",
          "name": "Shuhui Yang",
          "hidden": false
        },
        {
          "_id": "6858c1fcc0c8e29df8ea3c6c",
          "name": "Yifei Feng",
          "hidden": false
        },
        {
          "_id": "6858c1fcc0c8e29df8ea3c6d",
          "name": "Sheng Zhang",
          "hidden": false
        },
        {
          "_id": "6858c1fcc0c8e29df8ea3c6e",
          "name": "Xin Huang",
          "hidden": false
        },
        {
          "_id": "6858c1fcc0c8e29df8ea3c6f",
          "name": "Di Luo",
          "hidden": false
        },
        {
          "_id": "6858c1fcc0c8e29df8ea3c70",
          "name": "Fan Yang",
          "hidden": false
        },
        {
          "_id": "6858c1fcc0c8e29df8ea3c71",
          "name": "Fang Yang",
          "hidden": false
        },
        {
          "_id": "6858c1fcc0c8e29df8ea3c72",
          "name": "Lifu Wang",
          "hidden": false
        },
        {
          "_id": "6858c1fcc0c8e29df8ea3c73",
          "name": "Sicong Liu",
          "hidden": false
        },
        {
          "_id": "6858c1fcc0c8e29df8ea3c74",
          "name": "Yixuan Tang",
          "hidden": false
        },
        {
          "_id": "6858c1fcc0c8e29df8ea3c75",
          "name": "Yulin Cai",
          "hidden": false
        },
        {
          "_id": "6858c1fcc0c8e29df8ea3c76",
          "name": "Zebin He",
          "hidden": false
        },
        {
          "_id": "6858c1fcc0c8e29df8ea3c77",
          "name": "Tian Liu",
          "hidden": false
        },
        {
          "_id": "6858c1fcc0c8e29df8ea3c78",
          "name": "Yuhong Liu",
          "hidden": false
        },
        {
          "_id": "6858c1fcc0c8e29df8ea3c79",
          "name": "Jie Jiang",
          "hidden": false
        },
        {
          "_id": "6858c1fcc0c8e29df8ea3c7a",
          "name": "Linus",
          "hidden": false
        },
        {
          "_id": "6858c1fcc0c8e29df8ea3c7b",
          "name": "Jingwei Huang",
          "hidden": false
        },
        {
          "_id": "6858c1fcc0c8e29df8ea3c7c",
          "name": "Chunchao Guo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-19T17:57:40.000Z",
      "submittedOnDailyAt": "2025-06-23T01:25:30.602Z",
      "title": "ファイナルデテイル向けの高品質3Dアセット生成への向けて、ハンュウユウン3D 2.5",
      "submittedOnDailyBy": {
        "_id": "63044b89eedc089484c995ad",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/T4mOIQLaQdM5_oviaw_Cp.png",
        "isPro": false,
        "fullname": "Zeqiang Lai",
        "user": "ZeqiangLai",
        "type": "user"
      },
      "summary": "このレポートでは、Hunyuan3D 2.5という強力な3Dディフュージョンモデルシートを紹介します。これらのモデルは、高品質で詳細な3Dアセットを生成することを目的としています。Hunyuan3D 2.5は、前のバージョンHunyuan3D 2.0と同様の2ステップパイプラインを採用しながら、形状とテクスチャの生成において大幅な進歩を示しています。形状生成においては、新しい形状ファンデーションモデルLATTICEを導入し、スケールされた高品質データセット、モデルサイズ、コンピューティングを用いて訓練されています。我々の最大のモデルは10Bパラメータを達成し、画像-3Dの適切な追跡を保ったまま、シャープで詳細な3D形状を生成し、メッシュ表面がクリーンで滑らかであることを示し、生成された3D形状と手作りの3D形状の間の間違いを大幅に狭めています。テクスチャ生成においては、Hunyuan3D 2.0のPaintモデルから拡張された新しい多角度アーキテクチャをもとに物理ベース渲染（PBR）を導入し、我々の拡張評価により、Hunyuan3D 2.5は形状と端末から端末までのテクスチャ生成において以前の方法よりも显著に優れています。",
      "upvotes": 9,
      "discussionId": "6858c1fdc0c8e29df8ea3c7d",
      "ai_summary": "Hunyuan3D 2.5, a suite of 3D diffusion models, advances shape and texture generation with a new LATTICE model and physical-based rendering in a multi-view architecture.",
      "ai_keywords": [
        "3D diffusion models",
        "LATTICE",
        "scaled high-quality datasets",
        "model-size",
        "compute",
        "parameters",
        "sharp and detailed 3D shape",
        "mesh surface",
        "precise image-3D",
        "physical-based rendering",
        "multi-view architecture",
        "end-to-end texture generation"
      ]
    },
    "publishedAt": "2025-06-19T13:57:40.000Z",
    "title": "Hunyuan3D 2.5: Towards High-Fidelity 3D Assets Generation with Ultimate\n  Details",
    "summary": "In this report, we present Hunyuan3D 2.5, a robust suite of 3D diffusion\nmodels aimed at generating high-fidelity and detailed textured 3D assets.\nHunyuan3D 2.5 follows two-stages pipeline of its previous version Hunyuan3D\n2.0, while demonstrating substantial advancements in both shape and texture\ngeneration. In terms of shape generation, we introduce a new shape foundation\nmodel -- LATTICE, which is trained with scaled high-quality datasets,\nmodel-size, and compute. Our largest model reaches 10B parameters and generates\nsharp and detailed 3D shape with precise image-3D following while keeping mesh\nsurface clean and smooth, significantly closing the gap between generated and\nhandcrafted 3D shapes. In terms of texture generation, it is upgraded with\nphyiscal-based rendering (PBR) via a novel multi-view architecture extended\nfrom Hunyuan3D 2.0 Paint model. Our extensive evaluation shows that Hunyuan3D\n2.5 significantly outperforms previous methods in both shape and end-to-end\ntexture generation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.16504.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63044b89eedc089484c995ad",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/T4mOIQLaQdM5_oviaw_Cp.png",
      "fullname": "Zeqiang Lai",
      "name": "ZeqiangLai",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.15745",
      "authors": [
        {
          "_id": "685911a30e4ad7e2197582f3",
          "name": "Minsoo Kim",
          "hidden": false
        },
        {
          "_id": "685911a30e4ad7e2197582f4",
          "name": "Kyuhong Shim",
          "hidden": false
        },
        {
          "_id": "685911a30e4ad7e2197582f5",
          "name": "Jungwook Choi",
          "hidden": false
        },
        {
          "_id": "685911a30e4ad7e2197582f6",
          "name": "Simyung Chang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-18T02:22:14.000Z",
      "submittedOnDailyAt": "2025-06-23T07:10:57.325Z",
      "title": "InfiniPot-V: メモリ制限付きKVキャッシュの圧縮技術を活用したストリーミングビデオの理解",
      "submittedOnDailyBy": {
        "_id": "63c0e2503bdc86f8108da51b",
        "avatarUrl": "/avatars/7d47f11992f030b3d831e45102581d1f.svg",
        "isPro": false,
        "fullname": "Minsoo Kim",
        "user": "minsoo2333",
        "type": "user"
      },
      "summary": "現代の多モーダル大語言モデル（MLLMs）は、時間長い映像を理由的に論理することができますが、そのキー値（KV）キャッシュは時間に比例して線形に増大し、携帯電話、ARグラス、エッジロボットの固定メモリを超えることが速くなります。先行の圧縮スキームは、ビデオ全体とユーザークエリがオフラインで利用可能であることを前提としていたり、または、完全なキャッシュを構築する必要があり、その結果としてメモリはストリームの長さに比例して増大します。InfiniPot-Vは、最初のトレーニング不要、クエリ無関係なフレームワークであり、流れ映像の理解に対して厳格な長さ無関係なメモリカップを強制しています。映像をエンコーディングする間に、キャッシュを監視し、ユーザー設定のスロープを達成したら、軽量の圧縮パスを実行します。これは（i）時間的な冗長テキストをTemporal-axis Redundancy（TaR）メトリックを用いて削除し、または（ii）Value-Norm（VaN）ランキングを用いて意味的に重要なテキストを保持します。4つの開源MLLMと4つの長時間映像ベンチマーク、2つの流れ映像ベンチマークを通じて、InfiniPot-VはGPUメモリの高峰を94%まで削減し、実時間生成を維持し、完全なキャッシュの精度を満たし、または超えます。トレーニングやクエリ知識を必要とさせないことで、KVキャッシュのボトルネックを解決し、オンデモストリーミング映像アシスタントの間違いを閉じています。",
      "upvotes": 4,
      "discussionId": "685911a30e4ad7e2197582f7",
      "ai_summary": "InfiniPot-V is a training-free, query-agnostic framework that compresses the key-value cache during video encoding to maintain a fixed memory cap for streaming video understanding, enhancing real-time performance and accuracy.",
      "ai_keywords": [
        "multimodal large language models",
        "key-value cache",
        "Temporal-axis Redundancy",
        "Value-Norm ranking",
        "long-video benchmarks",
        "streaming-video benchmarks",
        "real-time generation",
        "multi-turn dialogues",
        "on-device streaming video assistants"
      ]
    },
    "publishedAt": "2025-06-17T22:22:14.000Z",
    "title": "InfiniPot-V: Memory-Constrained KV Cache Compression for Streaming Video\n  Understanding",
    "summary": "Modern multimodal large language models (MLLMs) can reason over hour-long\nvideo, yet their key-value (KV) cache grows linearly with time--quickly\nexceeding the fixed memory of phones, AR glasses, and edge robots. Prior\ncompression schemes either assume the whole video and user query are available\noffline or must first build the full cache, so memory still scales with stream\nlength. InfiniPot-V is the first training-free, query-agnostic framework that\nenforces a hard, length-independent memory cap for streaming video\nunderstanding. During video encoding it monitors the cache and, once a user-set\nthreshold is reached, runs a lightweight compression pass that (i) removes\ntemporally redundant tokens via Temporal-axis Redundancy (TaR) metric and (ii)\nkeeps semantically significant tokens via Value-Norm (VaN) ranking. Across four\nopen-source MLLMs and four long-video and two streaming-video benchmarks,\nInfiniPot-V cuts peak GPU memory by up to 94%, sustains real-time generation,\nand matches or surpasses full-cache accuracy--even in multi-turn dialogues. By\ndissolving the KV cache bottleneck without retraining or query knowledge,\nInfiniPot-V closes the gap for on-device streaming video assistants.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.15745.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63c0e2503bdc86f8108da51b",
      "avatarUrl": "/avatars/7d47f11992f030b3d831e45102581d1f.svg",
      "fullname": "Minsoo Kim",
      "name": "minsoo2333",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.17213",
      "authors": [
        {
          "_id": "685914860e4ad7e219758301",
          "name": "Xiuyu Yang",
          "hidden": false
        },
        {
          "_id": "685914860e4ad7e219758302",
          "name": "Shuhan Tan",
          "hidden": false
        },
        {
          "_id": "685914860e4ad7e219758303",
          "name": "Philipp Krähenbühl",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-20T17:59:21.000Z",
      "submittedOnDailyAt": "2025-06-23T07:17:59.369Z",
      "title": "長期交通シミュレーションにおける交差自動帰納的移動とスケーナー生成",
      "submittedOnDailyBy": {
        "_id": "634aab35dcf125e4dafc87b1",
        "avatarUrl": "/avatars/aa2db84fd423e9eefe3ef3167c9d3999.svg",
        "isPro": false,
        "fullname": "YangXiuyu",
        "user": "gzzyyxy",
        "type": "user"
      },
      "summary": "理想な交通シミュレーターは、自動運転システムが部署中に経験する写実的な長期の点から点までの移動を再現します。先行のモデルとベンチマークは、場所の初期効果エージェントの閉路モーションシミュレーションに焦点を当てています。これは長期シミュレーションに問題があります。効果エージェントは、効果ビークが新しい領域に入った際に場所に入り出します。我々は、閉路モーションシミュレーションと場所生成を交換させるユニットモデル、InfGenを提案します。InfGenは、閉路モーションシミュレーションと場所生成モードの間を自動的に切り替えます。これにより、長期のロールアウトシミュレーションが安定します。InfGenは短期間（9秒）の交通シミュレーションで最先端となり、長期間（30秒）のシミュレーションではすべての方法を大幅に超えます。InfGenのコードとモデルは、https://orangesodahub.github.io/InfGenで公開されます。",
      "upvotes": 2,
      "discussionId": "685914860e4ad7e219758304",
      "projectPage": "https://orangesodahub.github.io/InfGen/",
      "githubRepo": "https://github.com/OrangeSodahub/infgen/",
      "ai_summary": "InfGen, a unified next-token prediction model, enables stable long-term traffic simulation by interleaving closed-loop motion simulation and scene generation.",
      "ai_keywords": [
        "next-token prediction",
        "closed-loop motion simulation",
        "scene generation",
        "long-term traffic simulation"
      ]
    },
    "publishedAt": "2025-06-20T13:59:21.000Z",
    "title": "Long-term Traffic Simulation with Interleaved Autoregressive Motion and\n  Scenario Generation",
    "summary": "An ideal traffic simulator replicates the realistic long-term point-to-point\ntrip that a self-driving system experiences during deployment. Prior models and\nbenchmarks focus on closed-loop motion simulation for initial agents in a\nscene. This is problematic for long-term simulation. Agents enter and exit the\nscene as the ego vehicle enters new regions. We propose InfGen, a unified\nnext-token prediction model that performs interleaved closed-loop motion\nsimulation and scene generation. InfGen automatically switches between\nclosed-loop motion simulation and scene generation mode. It enables stable\nlong-term rollout simulation. InfGen performs at the state-of-the-art in\nshort-term (9s) traffic simulation, and significantly outperforms all other\nmethods in long-term (30s) simulation. The code and model of InfGen will be\nreleased at https://orangesodahub.github.io/InfGen",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.17213.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "634aab35dcf125e4dafc87b1",
      "avatarUrl": "/avatars/aa2db84fd423e9eefe3ef3167c9d3999.svg",
      "fullname": "YangXiuyu",
      "name": "gzzyyxy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.17202",
      "authors": [
        {
          "_id": "68591c310e4ad7e219758306",
          "name": "Teng Li",
          "hidden": false
        },
        {
          "_id": "68591c310e4ad7e219758307",
          "name": "Quanfeng Lu",
          "hidden": false
        },
        {
          "_id": "68591c310e4ad7e219758308",
          "name": "Lirui Zhao",
          "hidden": false
        },
        {
          "_id": "68591c310e4ad7e219758309",
          "name": "Hao Li",
          "hidden": false
        },
        {
          "_id": "68591c310e4ad7e21975830a",
          "name": "Xizhou Zhu",
          "hidden": false
        },
        {
          "_id": "68591c310e4ad7e21975830b",
          "name": "Yu Qiao",
          "hidden": false
        },
        {
          "_id": "68591c310e4ad7e21975830c",
          "name": "Jun Zhang",
          "hidden": false
        },
        {
          "_id": "68591c310e4ad7e21975830d",
          "name": "Wenqi Shao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-20T17:52:31.000Z",
      "submittedOnDailyAt": "2025-06-23T07:51:10.698Z",
      "title": "UniFork: モデルダイバーシティのアライメントを調査して、統一モノモデルでの多様な理解と生成を実現する",
      "submittedOnDailyBy": {
        "_id": "64897b1f0ec897cfe579a399",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64897b1f0ec897cfe579a399/ICR_75b877BaSE94gjBuj.jpeg",
        "isPro": false,
        "fullname": "wenq",
        "user": "wenqsun",
        "type": "user"
      },
      "summary": "統合画像理解と生成は、多モデル人工知能の中で望ましいパラダイムとして登場しました。最近の進歩にもかかわらず、このような統合モデルの最適アーキテクチャ設計は開放的な挑戦です。本研究では、タスク専門的なエクスプエリエンスモデルのモデル対応行為を分析し、現在の統合モデルを含むことで始めます。分析により、重要な見つけが明らかになりました：理解タスクは、ネットワークの深さで進歩的にモデル対応が増加し、これは意味情報の構築によりより良い理解を促進します。反対に、生成タスクは異なる傾向を示しています：早期の層でモデル対応が増加し、深い層では減少し、空間の詳細を回復します。これらの異なるモデル対応パターンは、完全に共有されたTransformerバックボーンでの基本的な衝突を生み出します。この発見に基づき、UniForkという新しいY字形アーキテクチャを介して、浅い層でタスク間の表現学習を共有し、深い層でタスク専門的なバ分を使用してタスク干渉を避けることを提案します。この設計は共有学習とタスク特殊化をより均等に調和します。拡大した消滅実験を通じて、UniForkは通常の完全に共有されたTransformerアーキテクチャを超え、タスク専門的なモデルと同等またはより良い性能を収めることを示しました。",
      "upvotes": 2,
      "discussionId": "68591c310e4ad7e21975830e",
      "ai_summary": "A Y-shaped architecture, UniFork, balances shared learning and task specialization for unified image understanding and generation, outperforming conventional fully shared Transformer models.",
      "ai_keywords": [
        "modality alignment",
        "network depth",
        "semantic information",
        "spatial details",
        "Transformer backbones",
        "Y-shaped architecture",
        "task-specific branches",
        "task interference",
        "ablation experiments",
        "fully shared Transformer architectures",
        "task-specific models"
      ]
    },
    "publishedAt": "2025-06-20T13:52:31.000Z",
    "title": "UniFork: Exploring Modality Alignment for Unified Multimodal\n  Understanding and Generation",
    "summary": "Unified image understanding and generation has emerged as a promising\nparadigm in multimodal artificial intelligence. Despite recent progress, the\noptimal architectural design for such unified models remains an open challenge.\nIn this work, we start by analyzing the modality alignment behaviors of\ntask-specific expert models for understanding and generation, as well as\ncurrent unified models. Our analysis reveals a crucial observation:\nunderstanding tasks benefit from a progressively increasing modality alignment\nacross network depth, which helps build up semantic information for better\ncomprehension; In contrast, generation tasks follow a different trend: modality\nalignment increases in the early layers but decreases in the deep layers to\nrecover spatial details. These divergent alignment patterns create a\nfundamental conflict in fully shared Transformer backbones, where a uniform\nrepresentational flow often leads to performance compromises across two tasks.\nMotivated by this finding, we introduce UniFork, a novel Y-shaped architecture\nthat shares the shallow layers for cross-task representation learning, while\nemploying task-specific branches in deeper layers to avoid task interference.\nThis design effectively balances shared learning and task specialization.\nThrough extensive ablation experiments, we demonstrate that Unifork\nconsistently outperforms conventional fully shared Transformer architectures,\nand achieves performance on par with or better than task-specific models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.17202.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64897b1f0ec897cfe579a399",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64897b1f0ec897cfe579a399/ICR_75b877BaSE94gjBuj.jpeg",
      "fullname": "wenq",
      "name": "wenqsun",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.15442",
      "authors": [
        {
          "_id": "68552b394f1add9d4c5c5cd4",
          "name": "Team Hunyuan3D",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5cd5",
          "name": "Shuhui Yang",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5cd6",
          "name": "Mingxin Yang",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5cd7",
          "name": "Yifei Feng",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5cd8",
          "name": "Xin Huang",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5cd9",
          "name": "Sheng Zhang",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5cda",
          "name": "Zebin He",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5cdb",
          "name": "Di Luo",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5cdc",
          "name": "Haolin Liu",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5cdd",
          "name": "Yunfei Zhao",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5cde",
          "name": "Qingxiang Lin",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5cdf",
          "name": "Zeqiang Lai",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5ce0",
          "name": "Xianghui Yang",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5ce1",
          "name": "Huiwen Shi",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5ce2",
          "name": "Zibo Zhao",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5ce3",
          "name": "Bowen Zhang",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5ce4",
          "name": "Hongyu Yan",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5ce5",
          "name": "Lifu Wang",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5ce6",
          "name": "Sicong Liu",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5ce7",
          "name": "Jihong Zhang",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5ce8",
          "name": "Meng Chen",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5ce9",
          "name": "Liang Dong",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5cea",
          "name": "Yiwen Jia",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5ceb",
          "name": "Yulin Cai",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5cec",
          "name": "Jiaao Yu",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5ced",
          "name": "Yixuan Tang",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5cee",
          "name": "Dongyuan Guo",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5cef",
          "name": "Junlin Yu",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5cf0",
          "name": "Hao Zhang",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5cf1",
          "name": "Zheng Ye",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5cf2",
          "name": "Peng He",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5cf3",
          "name": "Runzhou Wu",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5cf4",
          "name": "Shida Wei",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5cf5",
          "name": "Chao Zhang",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5cf6",
          "name": "Yonghao Tan",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5cf7",
          "name": "Yifu Sun",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5cf8",
          "name": "Lin Niu",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5cf9",
          "name": "Shirui Huang",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5cfa",
          "name": "Bojian Zheng",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5cfb",
          "name": "Shu Liu",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5cfc",
          "name": "Shilin Chen",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5cfd",
          "name": "Xiang Yuan",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5cfe",
          "name": "Xiaofeng Yang",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5cff",
          "name": "Kai Liu",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5d00",
          "name": "Jianchen Zhu",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5d01",
          "name": "Peng Chen",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5d02",
          "name": "Tian Liu",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5d03",
          "name": "Di Wang",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5d04",
          "name": "Yuhong Liu",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5d05",
          "name": "Linus",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5d06",
          "name": "Jie Jiang",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5d07",
          "name": "Jingwei Huang",
          "hidden": false
        },
        {
          "_id": "68552b394f1add9d4c5c5d08",
          "name": "Chunchao Guo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-18T13:14:46.000Z",
      "submittedOnDailyAt": "2025-06-23T07:17:03.771Z",
      "title": "フンユウン3D 2.1: 写真から高品質な3Dアセットへ、生産準拠のPBRマテリアルを使用して",
      "submittedOnDailyBy": {
        "_id": "647d9e881a1fcad2fdbf4954",
        "avatarUrl": "/avatars/92ee8727d5c9063d852d3537b7690843.svg",
        "isPro": false,
        "fullname": "SeanYoung",
        "user": "SeanYoungxh",
        "type": "user"
      },
      "summary": "3D AIジェネレードコンテンツ（AIGC）は、ゲーム、映画、デザインにおいて3Dモデルの作成を大幅に加速している激情された分野です。数多くの革命的なモデルが開発され、3D生成においてディレクトリバリジョンを変えたが、この分野は、3Dモデルの収集、処理、トレーニングに関する複雑さにより、研究者、開発者、デザイナーだけに限ります。これらの挑戦に対処するために、このチュートリアルでは、Hunyuan3D 2.1を用いたケーススタディを紹介します。このチュートリアルは、Hunyuan3D 2.1（高解像度、テクスチャー付き3Dアセットを生成する高度なシステム）を用いて、3Dデータの処理、3D生成モデルのトレーニング、そして性能評価を行うための一連のステップごとのガイドを提供します。このシステムは、形状生成のためのHunyuan3D-DiTと、テクスチャ合成のためのHunyuan3D-Paintの2つの核心コンポーネントからなります。データ準備、モデルアーキテクチャ、トレーニング戦略、評価指標、デプロイメントのワークフロー全体を調べ、このチュートリアルの終わりには、ゲーム、バーチャルリアル、工業デザインに適した強力な3D生成モデルの微調節または開発に必要な知識を持っていることを求めます。",
      "upvotes": 2,
      "discussionId": "68552b394f1add9d4c5c5d09",
      "ai_summary": "The tutorial provides a comprehensive guide on using Hunyuan3D 2.1 for generating high-resolution, textured 3D models, covering data preparation, model architecture, training, evaluation, and deployment.",
      "ai_keywords": [
        "Hunyuan3D-DiT",
        "Hunyuan3D-Paint",
        "3D generative model",
        "texture synthesis",
        "data preparation",
        "model architecture",
        "training strategies",
        "evaluation metrics",
        "deployment"
      ]
    },
    "publishedAt": "2025-06-18T09:14:46.000Z",
    "title": "Hunyuan3D 2.1: From Images to High-Fidelity 3D Assets with\n  Production-Ready PBR Material",
    "summary": "3D AI-generated content (AIGC) is a passionate field that has significantly\naccelerated the creation of 3D models in gaming, film, and design. Despite the\ndevelopment of several groundbreaking models that have revolutionized 3D\ngeneration, the field remains largely accessible only to researchers,\ndevelopers, and designers due to the complexities involved in collecting,\nprocessing, and training 3D models. To address these challenges, we introduce\nHunyuan3D 2.1 as a case study in this tutorial. This tutorial offers a\ncomprehensive, step-by-step guide on processing 3D data, training a 3D\ngenerative model, and evaluating its performance using Hunyuan3D 2.1, an\nadvanced system for producing high-resolution, textured 3D assets. The system\ncomprises two core components: the Hunyuan3D-DiT for shape generation and the\nHunyuan3D-Paint for texture synthesis. We will explore the entire workflow,\nincluding data preparation, model architecture, training strategies, evaluation\nmetrics, and deployment. By the conclusion of this tutorial, you will have the\nknowledge to finetune or develop a robust 3D generative model suitable for\napplications in gaming, virtual reality, and industrial design.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.15442.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "647d9e881a1fcad2fdbf4954",
      "avatarUrl": "/avatars/92ee8727d5c9063d852d3537b7690843.svg",
      "fullname": "SeanYoung",
      "name": "SeanYoungxh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.15925",
      "authors": [
        {
          "_id": "6858c714c0c8e29df8ea3c9c",
          "user": {
            "_id": "64698ed0dcbb937d56b9dd02",
            "avatarUrl": "/avatars/407afd70356f9d0669d4e2d39b9a8740.svg",
            "isPro": false,
            "fullname": "Narutatsu Ri",
            "user": "narutatsuri",
            "type": "user"
          },
          "name": "Narutatsu Ri",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-23T08:14:39.250Z",
          "hidden": false
        },
        {
          "_id": "6858c714c0c8e29df8ea3c9d",
          "name": "Nicholas Deas",
          "hidden": false
        },
        {
          "_id": "6858c714c0c8e29df8ea3c9e",
          "name": "Kathleen McKeown",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-19T00:01:43.000Z",
      "submittedOnDailyAt": "2025-06-23T01:47:16.594Z",
      "title": "Rerankingベースの生成による偏った視点の摘要生成",
      "submittedOnDailyBy": {
        "_id": "64698ed0dcbb937d56b9dd02",
        "avatarUrl": "/avatars/407afd70356f9d0669d4e2d39b9a8740.svg",
        "isPro": false,
        "fullname": "Narutatsu Ri",
        "user": "narutatsuri",
        "type": "user"
      },
      "summary": "政治などの実世界の設定で無偏視的な要約を生成することは、大規模言語モデル（LLMs）の重要なアプリケーションである。しかし、現在の評価フレームワークは、覆縞や忠実性などのキー属性を測定するために伝統的なメトリックを使用し、その適用可能性を確認していない。また、改善された要約ツールの開発に向けての努力はまだ幼稚期にある。我々は、これらの欠陥を解決するために、(1) 観点要約の品質を測定するための信頼性のあるメトリックを特定し、(2) LLM ベースの方法の効果を零ショット推論よりも進めることを調査する。特に、人間の注釈を用いてメトリックの信頼性をベンチマークするためのテストセットを構築し、伝統的なメトリックが言語モデルベースのメトリックに比べて劣り、強力な評価者として証明されることを示す。これらのメトリックを使用して、リランクベースの方法が強力な結果を得ることを示し、合成的に生成されたデータとリランクラベルを用いた偏好調整は性能を進めることを進める。我々の見つけたものは、観点要約の方法の信頼的な評価と開発に貢献することを目的とする。",
      "upvotes": 1,
      "discussionId": "6858c714c0c8e29df8ea3c9f",
      "githubRepo": "https://github.com/narutatsuri/Unbiased-Perspective-Summarization",
      "ai_summary": "Reranking and preference tuning improve the quality of perspective summaries generated by LLMs, as measured by language model-based metrics that outperform traditional ones.",
      "ai_keywords": [
        "Large Language Models",
        "perspective summarization",
        "coverage",
        "faithfulness",
        "metric reliability",
        "reranking-based methods",
        "preference tuning",
        "synthetically generated data",
        "reranking-labeled data"
      ]
    },
    "publishedAt": "2025-06-18T20:01:43.000Z",
    "title": "Reranking-based Generation for Unbiased Perspective Summarization",
    "summary": "Generating unbiased summaries in real-world settings such as political\nperspective summarization remains a crucial application of Large Language\nModels (LLMs). Yet, existing evaluation frameworks rely on traditional metrics\nfor measuring key attributes such as coverage and faithfulness without\nverifying their applicability, and efforts to develop improved summarizers are\nstill nascent. We address these gaps by (1) identifying reliable metrics for\nmeasuring perspective summary quality, and (2) investigating the efficacy of\nLLM-based methods beyond zero-shot inference. Namely, we build a test set for\nbenchmarking metric reliability using human annotations and show that\ntraditional metrics underperform compared to language model-based metrics,\nwhich prove to be strong evaluators. Using these metrics, we show that\nreranking-based methods yield strong results, and preference tuning with\nsynthetically generated and reranking-labeled data further boosts performance.\nOur findings aim to contribute to the reliable evaluation and development of\nperspective summarization methods.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.15925.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64698ed0dcbb937d56b9dd02",
      "avatarUrl": "/avatars/407afd70356f9d0669d4e2d39b9a8740.svg",
      "fullname": "Narutatsu Ri",
      "name": "narutatsuri",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  }
]