[
  {
    "paper": {
      "id": "2505.02707",
      "authors": [
        {
          "_id": "6819982f17007d963b9d4166",
          "name": "Yemin Shi",
          "hidden": false
        },
        {
          "_id": "6819982f17007d963b9d4167",
          "name": "Yu Shu",
          "hidden": false
        },
        {
          "_id": "6819982f17007d963b9d4168",
          "name": "Siwei Dong",
          "hidden": false
        },
        {
          "_id": "6819982f17007d963b9d4169",
          "user": {
            "_id": "6108ae87823007eaf0c7bd1e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6108ae87823007eaf0c7bd1e/dKjdx9I5waJs6oUQ0_mmT.png",
            "isPro": false,
            "fullname": "Guangyi Liu",
            "user": "guangyil",
            "type": "user"
          },
          "name": "Guangyi Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:46:52.667Z",
          "hidden": false
        },
        {
          "_id": "6819982f17007d963b9d416a",
          "user": {
            "_id": "6438a9027de34e8ea7e4b257",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6438a9027de34e8ea7e4b257/vib8QSd1AWMr_bR9ig_xJ.jpeg",
            "isPro": false,
            "fullname": "Jaward Sesay",
            "user": "Jaward",
            "type": "user"
          },
          "name": "Jaward Sesay",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-06T08:32:48.746Z",
          "hidden": false
        },
        {
          "_id": "6819982f17007d963b9d416b",
          "name": "Jingwen Li",
          "hidden": false
        },
        {
          "_id": "6819982f17007d963b9d416c",
          "user": {
            "_id": "665bfa1b0d71762b8613282d",
            "avatarUrl": "/avatars/edbde7b1b47032339a1ecc59f8ea8f1a.svg",
            "isPro": false,
            "fullname": "Zhiting Hu",
            "user": "zhitinghu",
            "type": "user"
          },
          "name": "Zhiting Hu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:46:15.191Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/665bfa1b0d71762b8613282d/zbWarqt8nFt0AwhF0gElE.mp4"
      ],
      "publishedAt": "2025-05-05T15:05:01.000Z",
      "submittedOnDailyAt": "2025-05-06T03:36:16.945Z",
      "title": "Voila: 声語言ファンダメンタルモデルによる時間単位の自動軌道運転インタラクションと声語言役職プレイ",
      "submittedOnDailyBy": {
        "_id": "665bfa1b0d71762b8613282d",
        "avatarUrl": "/avatars/edbde7b1b47032339a1ecc59f8ea8f1a.svg",
        "isPro": false,
        "fullname": "Zhiting Hu",
        "user": "zhitinghu",
        "type": "user"
      },
      "summary": "ヴォイラ（Voila）は、日常生活にセミナスムライフにブレンドし、自動転換的、リアルタイム的、感情的に表現的なモードで人間と相互作用する声AIアガントです。それらの命令に対しての単純な反応を超え、連続的に聴き、理由をつけ、主動的に反応し、流れ通り的な、動的な、感情的に共鳴するインタラクションを促進します。ヴォイラは、このビジョンへのステップを踏み出し、家族の大規模な声言語基礎モデルを紹介しています。ヴォイラは、従来のパイプラインシステムを超え、全ダブルクロス、低ラテンシーの会話を可能にする新しい端末から端末までのアーキテクチャを採用し、音調、リズム、感情などの豊富な声の調和を保っています。195ミリ秒のレスポンスラテンシーを達成し、平均の人間のレスポンス時間を超えています。その階層的な多スケールTransformerは、大規模な言語モデル（LLMs）の理由能力と強力な音響モデリングを統合し、自然的でプロフェッショナルに覚えられる声の生成を可能にします。ユーザーは、スピカーの認識、音調、その他の特徴を定義するための簡単なテキスト指示を書くだけです。また、ヴォイラは、10秒程度の短い音声サンプルから新しい声を適切にカスタマイズできることを奨励し、超100万の事前構築された声をサポートしています。会話だけではなく、自動語音認識（ASR）、テキストオンソブティ（TTS）、そして最小限の適応を通じて多言語語学翻訳にも対応しています。ヴォイラは、完全にオープンソースであり、公開研究をサポートし、次世代の人間マシンインタラクションへの進歩を促進することを目指しています。",
      "upvotes": 49,
      "discussionId": "6819983117007d963b9d4247",
      "projectPage": "https://voila.maitrix.org",
      "githubRepo": "https://github.com/maitrix-org/Voila",
      "ai_keywords": [
        "full-duplex",
        "low-latency conversations",
        "hierarchical multi-scale Transformer",
        "reasoning capabilities",
        "large language models (LLMs)",
        "acoustic modeling",
        "persona-aware voice generation",
        "automatic speech recognition (ASR)",
        "Text-to-Speech (TTS)",
        "multilingual speech translation",
        "pre-built voices",
        "efficient customization"
      ]
    },
    "publishedAt": "2025-05-05T11:05:01.000Z",
    "title": "Voila: Voice-Language Foundation Models for Real-Time Autonomous\n  Interaction and Voice Role-Play",
    "summary": "A voice AI agent that blends seamlessly into daily life would interact with\nhumans in an autonomous, real-time, and emotionally expressive manner. Rather\nthan merely reacting to commands, it would continuously listen, reason, and\nrespond proactively, fostering fluid, dynamic, and emotionally resonant\ninteractions. We introduce Voila, a family of large voice-language foundation\nmodels that make a step towards this vision. Voila moves beyond traditional\npipeline systems by adopting a new end-to-end architecture that enables\nfull-duplex, low-latency conversations while preserving rich vocal nuances such\nas tone, rhythm, and emotion. It achieves a response latency of just 195\nmilliseconds, surpassing the average human response time. Its hierarchical\nmulti-scale Transformer integrates the reasoning capabilities of large language\nmodels (LLMs) with powerful acoustic modeling, enabling natural, persona-aware\nvoice generation -- where users can simply write text instructions to define\nthe speaker's identity, tone, and other characteristics. Moreover, Voila\nsupports over one million pre-built voices and efficient customization of new\nones from brief audio samples as short as 10 seconds. Beyond spoken dialogue,\nVoila is designed as a unified model for a wide range of voice-based\napplications, including automatic speech recognition (ASR), Text-to-Speech\n(TTS), and, with minimal adaptation, multilingual speech translation. Voila is\nfully open-sourced to support open research and accelerate progress toward\nnext-generation human-machine interactions.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/665bfa1b0d71762b8613282d/zbWarqt8nFt0AwhF0gElE.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02707.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "665bfa1b0d71762b8613282d",
      "avatarUrl": "/avatars/edbde7b1b47032339a1ecc59f8ea8f1a.svg",
      "fullname": "Zhiting Hu",
      "name": "zhitinghu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.02387",
      "authors": [
        {
          "_id": "681988d6d6a5fee26b52ac28",
          "user": {
            "_id": "6270ff726417aed8a7340c8b",
            "avatarUrl": "/avatars/3f14913c55cc4fc78678ac43fb603e80.svg",
            "isPro": false,
            "fullname": "Xiusi Chen",
            "user": "XtremSup",
            "type": "user"
          },
          "name": "Xiusi Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:47:11.654Z",
          "hidden": false
        },
        {
          "_id": "681988d6d6a5fee26b52ac29",
          "user": {
            "_id": "654d784d71a30c4bca09a319",
            "avatarUrl": "/avatars/ab9f93122903ccd662267232bab30ad8.svg",
            "isPro": false,
            "fullname": "Gaotang Li",
            "user": "gaotang",
            "type": "user"
          },
          "name": "Gaotang Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-06T08:33:13.258Z",
          "hidden": false
        },
        {
          "_id": "681988d6d6a5fee26b52ac2a",
          "name": "Ziqi Wang",
          "hidden": false
        },
        {
          "_id": "681988d6d6a5fee26b52ac2b",
          "name": "Bowen Jin",
          "hidden": false
        },
        {
          "_id": "681988d6d6a5fee26b52ac2c",
          "name": "Cheng Qian",
          "hidden": false
        },
        {
          "_id": "681988d6d6a5fee26b52ac2d",
          "name": "Yu Wang",
          "hidden": false
        },
        {
          "_id": "681988d6d6a5fee26b52ac2e",
          "user": {
            "_id": "65f906e5c3dbdcae83ff7aac",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65f906e5c3dbdcae83ff7aac/mdjiVkLDJgJcGLwv0rMe4.jpeg",
            "isPro": false,
            "fullname": "Hongru Wang",
            "user": "Merlin-Hongru",
            "type": "user"
          },
          "name": "Hongru Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-06T08:33:11.136Z",
          "hidden": false
        },
        {
          "_id": "681988d6d6a5fee26b52ac2f",
          "name": "Yu Zhang",
          "hidden": false
        },
        {
          "_id": "681988d6d6a5fee26b52ac30",
          "user": {
            "_id": "66285acb73af5913c6bbf1ec",
            "avatarUrl": "/avatars/8969e3a6ae2dcc0b1c49768fd044b9e0.svg",
            "isPro": false,
            "fullname": "Denghui Zhang",
            "user": "zhangdenghui123",
            "type": "user"
          },
          "name": "Denghui Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:48:00.793Z",
          "hidden": false
        },
        {
          "_id": "681988d6d6a5fee26b52ac31",
          "name": "Tong Zhang",
          "hidden": false
        },
        {
          "_id": "681988d6d6a5fee26b52ac32",
          "name": "Hanghang Tong",
          "hidden": false
        },
        {
          "_id": "681988d6d6a5fee26b52ac33",
          "name": "Heng Ji",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-05T06:11:12.000Z",
      "submittedOnDailyAt": "2025-05-06T02:32:05.558Z",
      "title": "RM-R1: 報酬モデリングを理由論として",
      "submittedOnDailyBy": {
        "_id": "654d784d71a30c4bca09a319",
        "avatarUrl": "/avatars/ab9f93122903ccd662267232bab30ad8.svg",
        "isPro": false,
        "fullname": "Gaotang Li",
        "user": "gaotang",
        "type": "user"
      },
      "summary": "報酬モデリングは、大規模な言語モデル（LLMs）と人間の好みを一致させるために重要です、特に、人間のフィードバックからの強化学習（RLHF）を通じて。正確な報酬信号を提供するために、報酬モデル（RM）は、スコアまたは判断を割り当てる前に深い思考を促し、解釈可能な理由を行うべきです。しかし、現在のRMは、不透明的スカラースコアを生成しているか、好ましい答えの予測を直接生成しているため、自然言語の批判を統合することが難しく、解釈性が欠けています。\n\n長い連鎖の思い出（CoT）の最近の進歩により励まして、我々は、報酬モデリングに理由の能力を統合することがRMの解釈性と性能を大幅に向上させることを仮定し、証明しました。本稿では、新しいクラスの生成的な報酬モデル——Reasoning Reward Models（ReasRMs）——を紹介し、報酬モデリングを理由の任務として構成します。我々は、理由に向けた訓練パイプラインを提案し、ReasRMsの家族——RM-R1——を訓練します。訓練は、2つの主なステージからなります：（1）高品質の理由の連鎖の精煉、（2）検証可能な報酬を用いた強化学習。RM-R1は、自動的に理由のトレースまたはチャット特有のレビューガイドを生成し、候補の回答を比較してLLMのロードアウトを向上させます。実験的に、我々のモデルは複数の詳細な報酬モデルベンチマークで生成的なRMの最先端または近い最先端の性能を達成し、大きな開放ウェイトモデル（例：Llama3.1-405B）や所有権モデル（例：GPT-4o）を上回ります（最高で13.8%以上）。最終的な性能を超えることを超えて、我々は成功したReasRM訓練の鍵の要素を理解するために詳細な実験的解析を行います。将来の研究のために、我々は6つのReasRMモデルやコードおよびデータを公開します（https://github.com/RM-R1-UIUC/RM-R1）。",
      "upvotes": 28,
      "discussionId": "681988d7d6a5fee26b52ac7e",
      "githubRepo": "https://github.com/RM-R1-UIUC/RM-R1",
      "ai_keywords": [
        "reward modeling",
        "reinforcement learning from human feedback (RLHF)",
        "reward model (RM)",
        "scalar scores",
        "preferred answer",
        "natural language critiques",
        "long chain-of-thought (CoT)",
        "reasoning capabilities",
        "Reasoning Reward Models (ReasRMs)",
        "reasoning-oriented training pipeline",
        "distillation",
        "high-quality reasoning chains",
        "reinforcement learning",
        "verifiable rewards",
        "LLM rollouts",
        "self-generating reasoning traces",
        "chat-specific rubrics",
        "candidate responses",
        "generative reward models",
        "state-of-the-art",
        "near state-of-the-art",
        "reward model benchmarks",
        "open-weight models",
        "proprietary models",
        "empirical analysis",
        "ReasRM models"
      ]
    },
    "publishedAt": "2025-05-05T02:11:12.000Z",
    "title": "RM-R1: Reward Modeling as Reasoning",
    "summary": "Reward modeling is essential for aligning large language models (LLMs) with\nhuman preferences, especially through reinforcement learning from human\nfeedback (RLHF). To provide accurate reward signals, a reward model (RM) should\nstimulate deep thinking and conduct interpretable reasoning before assigning a\nscore or a judgment. However, existing RMs either produce opaque scalar scores\nor directly generate the prediction of a preferred answer, making them struggle\nto integrate natural language critiques, thus lacking interpretability.\nInspired by recent advances of long chain-of-thought (CoT) on\nreasoning-intensive tasks, we hypothesize and validate that integrating\nreasoning capabilities into reward modeling significantly enhances RM's\ninterpretability and performance. In this work, we introduce a new class of\ngenerative reward models -- Reasoning Reward Models (ReasRMs) -- which\nformulate reward modeling as a reasoning task. We propose a reasoning-oriented\ntraining pipeline and train a family of ReasRMs, RM-R1. The training consists\nof two key stages: (1) distillation of high-quality reasoning chains and (2)\nreinforcement learning with verifiable rewards. RM-R1 improves LLM rollouts by\nself-generating reasoning traces or chat-specific rubrics and evaluating\ncandidate responses against them. Empirically, our models achieve\nstate-of-the-art or near state-of-the-art performance of generative RMs across\nmultiple comprehensive reward model benchmarks, outperforming much larger\nopen-weight models (e.g., Llama3.1-405B) and proprietary ones (e.g., GPT-4o) by\nup to 13.8%. Beyond final performance, we perform thorough empirical analysis\nto understand the key ingredients of successful ReasRM training. To facilitate\nfuture research, we release six ReasRM models along with code and data at\nhttps://github.com/RM-R1-UIUC/RM-R1.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02387.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "654d784d71a30c4bca09a319",
      "avatarUrl": "/avatars/ab9f93122903ccd662267232bab30ad8.svg",
      "fullname": "Gaotang Li",
      "name": "gaotang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.20752",
      "authors": [
        {
          "_id": "6818c145daa8955b2085667d",
          "name": "Roman Abramov",
          "hidden": false
        },
        {
          "_id": "6818c145daa8955b2085667e",
          "user": {
            "_id": "6679882913c63ebaa8ff62fe",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6679882913c63ebaa8ff62fe/zufYEHw7QNp50pfZx9SmF.jpeg",
            "isPro": false,
            "fullname": "Felix Steinbauer",
            "user": "fsteinbauer",
            "type": "user"
          },
          "name": "Felix Steinbauer",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-05T13:46:46.742Z",
          "hidden": false
        },
        {
          "_id": "6818c145daa8955b2085667f",
          "name": "Gjergji Kasneci",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-29T13:33:29.000Z",
      "submittedOnDailyAt": "2025-05-06T03:38:21.809Z",
      "title": "ジャンプフィールドにおけるジョークフィールド：実世界の多段階推論を行うトランスフォーマーのデータ拡張",
      "submittedOnDailyBy": {
        "_id": "6679882913c63ebaa8ff62fe",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6679882913c63ebaa8ff62fe/zufYEHw7QNp50pfZx9SmF.jpeg",
        "isPro": false,
        "fullname": "Felix Steinbauer",
        "user": "fsteinbauer",
        "type": "user"
      },
      "summary": "Transformersは、数多くのNLPタスクで大成功を収めていますが、多段階的事実的推理において明らかな欠点が残っています。特に、実世界的知識が稀少な場合にはそれらの欠点が明らかになります。最近のgrokkingの進展により、神経ネットワークが潜在的なロジックパターンを検出したら、記憶から完全に一般化することができることが示されました。しかし、これらの研究は主に小さな合成タスクを使用していました。この論文では、まずに、実世界的事実的データにgrokkingを拡張し、データセットの稀薄性の挑戦を解決するために、細心に設計された合成データを既存の知識グラフに追加し、推論された事実の比率phi_rを原子事実よりも閾値を超えることに成功しました。驚き的に、私たちは事実的に不正な合成データが強化してくれる推理サイクルを増強し、記憶によるものよりも関係的構造を信じることを強制します。多段階推理ベンチマークで評価されたところで、我々のアプローチは2WikiMultiHopQAでは95-100%の精度を達成し、強いベースラインを大幅に向上させ、現在の最先端の結果を追い越すことができます。さらに、phi_rの増加によるTransformers内での一般化サイクルの形成を詳細に分析します。我々の発見は、grokkingに基づくデータ増強が隠れた多段階推理能力を解放し、大規模な言語モデルでより強固かつ解釈可能な事実的推理を可能にします。",
      "upvotes": 19,
      "discussionId": "6818c146daa8955b208566f1",
      "ai_keywords": [
        "Transformers",
        "multi-step factual reasoning",
        "grokking",
        "neural networks",
        "perfect generalization",
        "logical patterns",
        "real-world factual data",
        "dataset sparsity",
        "knowledge graphs",
        "synthetic data",
        "inferred facts",
        "atomic facts",
        "factually incorrect synthetic data",
        "relational structure",
        "memorization",
        "multi-hop reasoning",
        "benchmarks",
        "2WikiMultiHopQA",
        "baselines",
        "state-of-the-art results",
        "generalizing circuits",
        "grokking-based data augmentation",
        "implicit multi-hop reasoning capabilities",
        "robust",
        "interpretable factual reasoning"
      ]
    },
    "publishedAt": "2025-04-29T09:33:29.000Z",
    "title": "Grokking in the Wild: Data Augmentation for Real-World Multi-Hop\n  Reasoning with Transformers",
    "summary": "Transformers have achieved great success in numerous NLP tasks but continue\nto exhibit notable gaps in multi-step factual reasoning, especially when\nreal-world knowledge is sparse. Recent advances in grokking have demonstrated\nthat neural networks can transition from memorizing to perfectly generalizing\nonce they detect underlying logical patterns - yet these studies have primarily\nused small, synthetic tasks. In this paper, for the first time, we extend\ngrokking to real-world factual data and address the challenge of dataset\nsparsity by augmenting existing knowledge graphs with carefully designed\nsynthetic data to raise the ratio phi_r of inferred facts to atomic facts\nabove the threshold required for grokking. Surprisingly, we find that even\nfactually incorrect synthetic data can strengthen emergent reasoning circuits\nrather than degrade accuracy, as it forces the model to rely on relational\nstructure rather than memorization. When evaluated on multi-hop reasoning\nbenchmarks, our approach achieves up to 95-100% accuracy on 2WikiMultiHopQA -\nsubstantially improving over strong baselines and matching or exceeding current\nstate-of-the-art results. We further provide an in-depth analysis of how\nincreasing phi_r drives the formation of generalizing circuits inside\nTransformers. Our findings suggest that grokking-based data augmentation can\nunlock implicit multi-hop reasoning capabilities, opening the door to more\nrobust and interpretable factual reasoning in large-scale language models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.20752.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6679882913c63ebaa8ff62fe",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6679882913c63ebaa8ff62fe/zufYEHw7QNp50pfZx9SmF.jpeg",
      "fullname": "Felix Steinbauer",
      "name": "fsteinbauer",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.02819",
      "authors": [
        {
          "_id": "6819b5da3d9c61444380f4c5",
          "user": {
            "_id": "66465dfa508db0bde50d95f2",
            "avatarUrl": "/avatars/8b4a583dc0f3cab0f1cd9a1be3daa01b.svg",
            "isPro": false,
            "fullname": "Dmitry Shophoev",
            "user": "dimitriish",
            "type": "user"
          },
          "name": "Dmitriy Shopkhoev",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-06T07:10:19.519Z",
          "hidden": false
        },
        {
          "_id": "6819b5da3d9c61444380f4c6",
          "user": {
            "_id": "6166db59f78a267701a78c2a",
            "avatarUrl": "/avatars/8784efc36f67719e9455b1f081340ed9.svg",
            "isPro": false,
            "fullname": "Ammar Ali",
            "user": "ammarali32",
            "type": "user"
          },
          "name": "Ammar Ali",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-06T08:32:17.870Z",
          "hidden": false
        },
        {
          "_id": "6819b5da3d9c61444380f4c7",
          "name": "Magauiya Zhussip",
          "hidden": false
        },
        {
          "_id": "6819b5da3d9c61444380f4c8",
          "user": {
            "_id": "66b1ce4ca14db5aac3e5e755",
            "avatarUrl": "/avatars/ab55ef112fba091813e1cc1f43857cf9.svg",
            "isPro": false,
            "fullname": "Valentin Malykh",
            "user": "madrugado",
            "type": "user"
          },
          "name": "Valentin Malykh",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:04:42.358Z",
          "hidden": false
        },
        {
          "_id": "6819b5da3d9c61444380f4c9",
          "user": {
            "_id": "6683cc62b466c0d8e60e1bbc",
            "avatarUrl": "/avatars/d781cfb113263f88eaa3250bef521c53.svg",
            "isPro": false,
            "fullname": "Stamatis Lefkimmiatis",
            "user": "stamatisl",
            "type": "user"
          },
          "name": "Stamatios Lefkimmiatis",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-06T08:32:13.923Z",
          "hidden": false
        },
        {
          "_id": "6819b5da3d9c61444380f4ca",
          "name": "Nikos Komodakis",
          "hidden": false
        },
        {
          "_id": "6819b5da3d9c61444380f4cb",
          "user": {
            "_id": "667e7f968c6d7aede7ecb94b",
            "avatarUrl": "/avatars/d6dabd9b909b1f20f661dc4bc07af23f.svg",
            "isPro": false,
            "fullname": "Sergey Zagoruyko",
            "user": "szagoruyko121",
            "type": "user"
          },
          "name": "Sergey Zagoruyko",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:04:52.244Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-05T17:47:42.000Z",
      "submittedOnDailyAt": "2025-05-06T07:03:26.032Z",
      "title": "レイヤープリニングと線形変換によるネットワークの簡約化",
      "submittedOnDailyBy": {
        "_id": "610e8c12119bebecb4d807b6",
        "avatarUrl": "/avatars/7230b1584ec45585c12eb5703fd80ff3.svg",
        "isPro": false,
        "fullname": "Ivan Sedykh",
        "user": "idsedykh",
        "type": "user"
      },
      "summary": "ReplaceMeは、一般化された訓練無制限の深さ削減手法です。これは、低い圧縮比で高い性能を維持しながら、transformerブロックを線形操作で有効に置き換えることができます。従来の削減手法と異なり、追加の訓練や微調節が必要なものではありません。我々の手法は、小さな補正データセットを使用して、削減ブロックを近似する線形変換を推定するだけです。この推定された線形マッピングは、残りのtransformerブロックと無間違く統合でき、追加のネットワークパラメーターが必要となることはありません。実験により、ReplaceMeは、他の訓練無制限手法を一致していることを示し、複雑な再訓練やファイナルチューニングやアーキテクチャ変更を含む最先端の削減手法と比較しても、高度な競争力を持っています。数えられる大規模な言語モデル（LLMs）に対して、ReplaceMeは、訓練や癒しステップを除き、開放ベンチマークでは約90%の元モデルの性能を維持しながら、25%の削減を実現します。これは、最小限の計算オーバーヘッドを伴っています（参照Fig.1）。ReplaceMeの実装と最先端の深さ削減手法の数つを含む開放ソースライブラリを提供しています。",
      "upvotes": 16,
      "discussionId": "6819b5db3d9c61444380f518",
      "githubRepo": "https://github.com/mts-ai/ReplaceMe",
      "ai_keywords": [
        "training-free depth pruning",
        "transformer blocks",
        "linear operation",
        "calibration dataset",
        "linear transformation",
        "computational overhead",
        "large language models (LLMs)",
        "open benchmarks",
        "open-source library"
      ]
    },
    "publishedAt": "2025-05-05T13:47:42.000Z",
    "title": "ReplaceMe: Network Simplification via Layer Pruning and Linear\n  Transformations",
    "summary": "We introduce ReplaceMe, a generalized training-free depth pruning method that\neffectively replaces transformer blocks with a linear operation, while\nmaintaining high performance for low compression ratios. In contrast to\nconventional pruning approaches that require additional training or\nfine-tuning, our approach requires only a small calibration dataset that is\nused to estimate a linear transformation to approximate the pruned blocks. This\nestimated linear mapping can be seamlessly merged with the remaining\ntransformer blocks, eliminating the need for any additional network parameters.\nOur experiments show that ReplaceMe consistently outperforms other\ntraining-free approaches and remains highly competitive with state-of-the-art\npruning methods that involve extensive retraining/fine-tuning and architectural\nmodifications. Applied to several large language models (LLMs), ReplaceMe\nachieves up to 25% pruning while retaining approximately 90% of the original\nmodel's performance on open benchmarks - without any training or healing steps,\nresulting in minimal computational overhead (see Fig.1). We provide an\nopen-source library implementing ReplaceMe alongside several state-of-the-art\ndepth pruning techniques, available at this repository.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02819.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "610e8c12119bebecb4d807b6",
      "avatarUrl": "/avatars/7230b1584ec45585c12eb5703fd80ff3.svg",
      "fullname": "Ivan Sedykh",
      "name": "idsedykh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.02735",
      "authors": [
        {
          "_id": "6819742e0d1c56fe9124fe3a",
          "user": {
            "_id": "62a80fe3ac97233f1625235a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62a80fe3ac97233f1625235a/_rGtpqdY7OEBz3pyqb6fE.jpeg",
            "isPro": false,
            "fullname": "Zhouliang Yu",
            "user": "zhouliang",
            "type": "user"
          },
          "name": "Zhouliang Yu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-06T08:34:10.190Z",
          "hidden": false
        },
        {
          "_id": "6819742e0d1c56fe9124fe3b",
          "user": {
            "_id": "662f2c8435ab6df959b005de",
            "avatarUrl": "/avatars/3e30053ecbe9cc14b5e1eb2b014755de.svg",
            "isPro": false,
            "fullname": "ruotian peng",
            "user": "prt66",
            "type": "user"
          },
          "name": "Ruotian Peng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:48:20.491Z",
          "hidden": false
        },
        {
          "_id": "6819742e0d1c56fe9124fe3c",
          "name": "Keyi Ding",
          "hidden": false
        },
        {
          "_id": "6819742e0d1c56fe9124fe3d",
          "name": "Yizhe Li",
          "hidden": false
        },
        {
          "_id": "6819742e0d1c56fe9124fe3e",
          "name": "Zhongyuan Peng",
          "hidden": false
        },
        {
          "_id": "6819742e0d1c56fe9124fe3f",
          "user": {
            "_id": "6417d9ea8f689506e7148417",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6417d9ea8f689506e7148417/bAYcruWNw4WvmuQcGgcwC.jpeg",
            "isPro": false,
            "fullname": "minghao",
            "user": "Liam-Liu",
            "type": "user"
          },
          "name": "Minghao Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-06T08:33:31.975Z",
          "hidden": false
        },
        {
          "_id": "6819742e0d1c56fe9124fe40",
          "user": {
            "_id": "623d8ca4c29adf5ef6175615",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623d8ca4c29adf5ef6175615/q7lHao7UPwU1u7YLSP56m.jpeg",
            "isPro": false,
            "fullname": "Yi-Fan Zhang",
            "user": "yifanzhang114",
            "type": "user"
          },
          "name": "Yifan Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:49:14.785Z",
          "hidden": false
        },
        {
          "_id": "6819742e0d1c56fe9124fe41",
          "user": {
            "_id": "649da6b4599302cdb9bc232b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/DxQT6LCDTZvyGUUe2t19c.jpeg",
            "isPro": false,
            "fullname": "Zheng Yuan",
            "user": "ZhengYuan",
            "type": "user"
          },
          "name": "Zheng Yuan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:49:20.735Z",
          "hidden": false
        },
        {
          "_id": "6819742e0d1c56fe9124fe42",
          "user": {
            "_id": "6532a060a78e70d19c669103",
            "avatarUrl": "/avatars/3cc9309b0e31da0fb83f1c3ef87dbe9f.svg",
            "isPro": false,
            "fullname": "HuajianXin",
            "user": "HuajianXin",
            "type": "user"
          },
          "name": "Huajian Xin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:49:28.104Z",
          "hidden": false
        },
        {
          "_id": "6819742e0d1c56fe9124fe43",
          "user": {
            "_id": "641e5bf65f274a0a92c2f6a2",
            "avatarUrl": "/avatars/c15a54c51998c0e6367685e8e1737ec9.svg",
            "isPro": false,
            "fullname": "Wenhao Huang",
            "user": "EZ-hwh",
            "type": "user"
          },
          "name": "Wenhao Huang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:49:44.482Z",
          "hidden": false
        },
        {
          "_id": "6819742e0d1c56fe9124fe44",
          "user": {
            "_id": "643c21735fcffe09fb68a46f",
            "avatarUrl": "/avatars/76aabacd318aa954d4c53094ad456056.svg",
            "isPro": false,
            "fullname": "Yandong Wen",
            "user": "ydwen",
            "type": "user"
          },
          "name": "Yandong Wen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:49:51.642Z",
          "hidden": false
        },
        {
          "_id": "6819742e0d1c56fe9124fe45",
          "user": {
            "_id": "638efcf4c67af472d316d424",
            "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
            "isPro": false,
            "fullname": "Ge Zhang",
            "user": "zhangysk",
            "type": "user"
          },
          "name": "Ge Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:49:59.764Z",
          "hidden": false
        },
        {
          "_id": "6819742e0d1c56fe9124fe46",
          "user": {
            "_id": "648905d1a15c43c791d4381f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/648905d1a15c43c791d4381f/GpqGBzsLiMHX0gWZEz3qn.jpeg",
            "isPro": false,
            "fullname": "Weiyang Liu",
            "user": "wy1iu",
            "type": "user"
          },
          "name": "Weiyang Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:50:07.063Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-05T15:37:00.000Z",
      "submittedOnDailyAt": "2025-05-06T01:00:48.636Z",
      "title": "正式数学推理的大型语言模型基准测试",
      "submittedOnDailyBy": {
        "_id": "62a80fe3ac97233f1625235a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62a80fe3ac97233f1625235a/_rGtpqdY7OEBz3pyqb6fE.jpeg",
        "isPro": false,
        "fullname": "Zhouliang Yu",
        "user": "zhouliang",
        "type": "user"
      },
      "summary": "形式的数学推理は人工知能にとって重要な課題であり、現在のベンチマークの範囲とスケールの制限によって論理化されています。これに対して、私たちはFormalMATHを紹介します。FormalMATHは、5,560問の正式的に検証された問題を含む大規模なLean4ベンチマークです。これらの問題は、中学生オリンピックの挑戦から学部レベルの定理まで、多様な分野（例えば、代数学、応用数学、微積学、数論、離散数学など）にわたります。手動での論理化の不適切さを軽減するために、私たちは、以下の3つの要素を組み合わせた新しい人間がロープ内の自動論理化プイプラインを導入します。1) 論理文の自動論理化を行う特殊化された大規模言語モデル（LLMs）、2) 多エンドモデルの語義検証、3) オフショットラインのLLMベースの証明プロバーを用いた否定基準の証明のフィルタリング戦略。このアプローチは、手動検証前に72.09%の論理文を残し、自然言語の問題に対する忠実性を確保します。最先端のLLMベースの定理証明プロバーの評価により、重要な制限が明らかになりました。実用的なサンプリングバッジの下で最も強いモデルも、16.46%の成功率を達成し、領域バイアス（例えば、代数学で優れているが、微積学では失敗する）と簡易化された自動化タクティクスの過度依存が見られます。特に、自然言語の解決策のガイドと証明の成功との逆の関係を見出し、人間が書いた非正式的な論理が形式的な論理の設定では噪音を引き起こし、それによってはわかりやすさを引き起こしていないことを示しました。私たちは、FormalMATHが形式的な数学論理の評価において強力なベンチマークとして提供することを信じています。",
      "upvotes": 16,
      "discussionId": "6819742f0d1c56fe9124fe8a",
      "projectPage": "https://spherelab.ai/FormalMATH/",
      "githubRepo": "https://github.com/Sphere-AI-Lab/FormalMATH-Bench"
    },
    "publishedAt": "2025-05-05T11:37:00.000Z",
    "title": "FormalMATH: Benchmarking Formal Mathematical Reasoning of Large Language\n  Models",
    "summary": "Formal mathematical reasoning remains a critical challenge for artificial\nintelligence, hindered by limitations of existing benchmarks in scope and\nscale. To address this, we present FormalMATH, a large-scale Lean4 benchmark\ncomprising 5,560 formally verified problems spanning from high-school Olympiad\nchallenges to undergraduate-level theorems across diverse domains (e.g.,\nalgebra, applied mathematics, calculus, number theory, and discrete\nmathematics). To mitigate the inefficiency of manual formalization, we\nintroduce a novel human-in-the-loop autoformalization pipeline that integrates:\n(1) specialized large language models (LLMs) for statement autoformalization,\n(2) multi-LLM semantic verification, and (3) negation-based disproof filtering\nstrategies using off-the-shelf LLM-based provers. This approach reduces expert\nannotation costs by retaining 72.09% of statements before manual verification\nwhile ensuring fidelity to the original natural-language problems. Our\nevaluation of state-of-the-art LLM-based theorem provers reveals significant\nlimitations: even the strongest models achieve only 16.46% success rate under\npractical sampling budgets, exhibiting pronounced domain bias (e.g., excelling\nin algebra but failing in calculus) and over-reliance on simplified automation\ntactics. Notably, we identify a counterintuitive inverse relationship between\nnatural-language solution guidance and proof success in chain-of-thought\nreasoning scenarios, suggesting that human-written informal reasoning\nintroduces noise rather than clarity in the formal reasoning settings. We\nbelieve that FormalMATH provides a robust benchmark for benchmarking formal\nmathematical reasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02735.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62a80fe3ac97233f1625235a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62a80fe3ac97233f1625235a/_rGtpqdY7OEBz3pyqb6fE.jpeg",
      "fullname": "Zhouliang Yu",
      "name": "zhouliang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.02835",
      "authors": [
        {
          "_id": "6819762e64ae18f1b6fde347",
          "user": {
            "_id": "623d8ca4c29adf5ef6175615",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623d8ca4c29adf5ef6175615/q7lHao7UPwU1u7YLSP56m.jpeg",
            "isPro": false,
            "fullname": "Yi-Fan Zhang",
            "user": "yifanzhang114",
            "type": "user"
          },
          "name": "Yi-Fan Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:57:15.220Z",
          "hidden": false
        },
        {
          "_id": "6819762e64ae18f1b6fde348",
          "user": {
            "_id": "664ba004bfd9b93ba4bfb353",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/UHaEcXmMSKvvFDsY3hCnb.jpeg",
            "isPro": false,
            "fullname": "LuXingyu",
            "user": "XingyuLu",
            "type": "user"
          },
          "name": "Xingyu Lu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:57:24.963Z",
          "hidden": false
        },
        {
          "_id": "6819762e64ae18f1b6fde349",
          "name": "Xiao Hu",
          "hidden": false
        },
        {
          "_id": "6819762e64ae18f1b6fde34a",
          "name": "Chaoyou Fu",
          "hidden": false
        },
        {
          "_id": "6819762e64ae18f1b6fde34b",
          "name": "Bin Wen",
          "hidden": false
        },
        {
          "_id": "6819762e64ae18f1b6fde34c",
          "name": "Tianke Zhang",
          "hidden": false
        },
        {
          "_id": "6819762e64ae18f1b6fde34d",
          "user": {
            "_id": "673421bf18caf8e877861cc6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/a8UfIZTUTFaCnWmJ_Bztr.png",
            "isPro": false,
            "fullname": "Changyi Liu",
            "user": "bhsc24",
            "type": "user"
          },
          "name": "Changyi Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:58:28.151Z",
          "hidden": false
        },
        {
          "_id": "6819762e64ae18f1b6fde34e",
          "user": {
            "_id": "63774c47455f6ad89ac41be1",
            "avatarUrl": "/avatars/e7d6048155cdf4497d58aa18523e745e.svg",
            "isPro": false,
            "fullname": "Kaiyu Jiang",
            "user": "KaiyuValley",
            "type": "user"
          },
          "name": "Kaiyu Jiang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:58:21.743Z",
          "hidden": false
        },
        {
          "_id": "6819762e64ae18f1b6fde34f",
          "name": "Kaibing Chen",
          "hidden": false
        },
        {
          "_id": "6819762e64ae18f1b6fde350",
          "user": {
            "_id": "66c605e808fee728d0dd94f5",
            "avatarUrl": "/avatars/d2ff37fedc5ac1b5b817543b80bf5256.svg",
            "isPro": false,
            "fullname": "Kaiyu Tang",
            "user": "KevinTowne",
            "type": "user"
          },
          "name": "Kaiyu Tang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:58:06.587Z",
          "hidden": false
        },
        {
          "_id": "6819762e64ae18f1b6fde351",
          "user": {
            "_id": "6610f64ee94d9046b71e19c8",
            "avatarUrl": "/avatars/11cc11199669129a740956d12c7214e8.svg",
            "isPro": false,
            "fullname": "Haojie Ding",
            "user": "haojieding",
            "type": "user"
          },
          "name": "Haojie Ding",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:57:59.222Z",
          "hidden": false
        },
        {
          "_id": "6819762e64ae18f1b6fde352",
          "user": {
            "_id": "6433abff546e16f17a0f1cd8",
            "avatarUrl": "/avatars/7c9bbcba69b823834eb0232da12cc7a9.svg",
            "isPro": false,
            "fullname": "chen",
            "user": "jiankang",
            "type": "user"
          },
          "name": "Jiankang Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:57:51.408Z",
          "hidden": false
        },
        {
          "_id": "6819762e64ae18f1b6fde353",
          "name": "Fan Yang",
          "hidden": false
        },
        {
          "_id": "6819762e64ae18f1b6fde354",
          "name": "Zhang Zhang",
          "hidden": false
        },
        {
          "_id": "6819762e64ae18f1b6fde355",
          "user": {
            "_id": "656453832bdaccfcd5379431",
            "avatarUrl": "/avatars/a0d764ce6b3fd05532c7a9cb2f263e33.svg",
            "isPro": false,
            "fullname": "Gao Ting",
            "user": "TingTingGao",
            "type": "user"
          },
          "name": "Tingting Gao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:57:35.059Z",
          "hidden": false
        },
        {
          "_id": "6819762e64ae18f1b6fde356",
          "name": "Liang Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-05T17:59:50.000Z",
      "submittedOnDailyAt": "2025-05-06T01:09:45.446Z",
      "title": "R1-Reward: 穩定な強化学習を通じた多模態報酬モデルの訓練",
      "submittedOnDailyBy": {
        "_id": "623d8ca4c29adf5ef6175615",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623d8ca4c29adf5ef6175615/q7lHao7UPwU1u7YLSP56m.jpeg",
        "isPro": false,
        "fullname": "Yi-Fan Zhang",
        "user": "yifanzhang114",
        "type": "user"
      },
      "summary": "多モデル報酬モデル（MRMs）は、多モデル大語言モデル（MLLMs）の性能向上に重要な役割を果たしています。最近の進展は主にモデル構造と学習データの改善に焦点を当てていますが、長期的な理由能力の効果性とその能力をMRMsで活性化する方法については限られた探索が行われていません。本論文では、報酬モデリングにおける強化学習（RL）の利用方法について検討します。特に、報酬モデリング問題をルールベースのRLタスクと再定式化します。しかし、現在のRLアルゴリズム（例えばReinforce++）を直接報酬モデリングに適用すると、これらのアルゴリズムの内在的な制限によりトレーニング不穩定または崩壊を招きます。この問題を解決するために、StableReinforceアルゴリズムを提案します。このアルゴリズムは、現在のRL方法のトレーニング損失、アdvantage推定戦略、報酬設計を改良し、これらの改良はトレーニングダイナミクスの安定化と上位の性能を収得します。MRMのトレーニングを支援するために、多様なデータセットから200Kの偏好データを収集します。StableReinforceアルゴリズムでこのデータセットを用いてトレーニングされた報酬モデル、R1-Rewardは、多モデル報酬モデリングベンチマーク上で显著な性能向上を収得します。前回のSOTAモデルと比較して、VL Reward-Benchで8.4%の改善率、Multimodal Reward Benchで14.3%の改善率を収得します。また、より多くの推論計算を設定すると、R1-Rewardの性能が進化し、RLアルゴリズムがMRMsを最適化する可能性を明らかにします。",
      "upvotes": 15,
      "discussionId": "6819762f64ae18f1b6fde387",
      "projectPage": "https://github.com/yfzhang114/r1_reward",
      "githubRepo": "https://github.com/yfzhang114/r1_reward",
      "ai_keywords": [
        "Multimodal Reward Models (MRMs)",
        "Multimodal Large Language Models (MLLMs)",
        "Reinforcement Learning (RL)",
        "rule-based RL task",
        "Reinforce++",
        "StableReinforce",
        "training loss",
        "advantage estimation strategy",
        "reward design",
        "preference data",
        "VL Reward-Bench",
        "Multimodal Reward Bench"
      ]
    },
    "publishedAt": "2025-05-05T13:59:50.000Z",
    "title": "R1-Reward: Training Multimodal Reward Model Through Stable Reinforcement\n  Learning",
    "summary": "Multimodal Reward Models (MRMs) play a crucial role in enhancing the\nperformance of Multimodal Large Language Models (MLLMs). While recent\nadvancements have primarily focused on improving the model structure and\ntraining data of MRMs, there has been limited exploration into the\neffectiveness of long-term reasoning capabilities for reward modeling and how\nto activate these capabilities in MRMs. In this paper, we explore how\nReinforcement Learning (RL) can be used to improve reward modeling.\nSpecifically, we reformulate the reward modeling problem as a rule-based RL\ntask. However, we observe that directly applying existing RL algorithms, such\nas Reinforce++, to reward modeling often leads to training instability or even\ncollapse due to the inherent limitations of these algorithms. To address this\nissue, we propose the StableReinforce algorithm, which refines the training\nloss, advantage estimation strategy, and reward design of existing RL methods.\nThese refinements result in more stable training dynamics and superior\nperformance. To facilitate MRM training, we collect 200K preference data from\ndiverse datasets. Our reward model, R1-Reward, trained using the\nStableReinforce algorithm on this dataset, significantly improves performance\non multimodal reward modeling benchmarks. Compared to previous SOTA models,\nR1-Reward achieves a 8.4% improvement on the VL Reward-Bench and a 14.3%\nimprovement on the Multimodal Reward Bench. Moreover, with more inference\ncompute, R1-Reward's performance is further enhanced, highlighting the\npotential of RL algorithms in optimizing MRMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02835.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "623d8ca4c29adf5ef6175615",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623d8ca4c29adf5ef6175615/q7lHao7UPwU1u7YLSP56m.jpeg",
      "fullname": "Yi-Fan Zhang",
      "name": "yifanzhang114",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.02391",
      "authors": [
        {
          "_id": "6819a63c64ae18f1b60a5c43",
          "user": {
            "_id": "66f8689725464a7989b75845",
            "avatarUrl": "/avatars/43a61a528c5779103eaf5687ba44ee14.svg",
            "isPro": false,
            "fullname": "Jiarui Yao",
            "user": "FlippyDora",
            "type": "user"
          },
          "name": "Jiarui Yao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-06T08:32:24.344Z",
          "hidden": false
        },
        {
          "_id": "6819a63c64ae18f1b60a5c44",
          "name": "Yifan Hao",
          "hidden": false
        },
        {
          "_id": "6819a63c64ae18f1b60a5c45",
          "user": {
            "_id": "6470e0f1cfd57849519033a5",
            "avatarUrl": "/avatars/7ffefee3e36a4e37b9f4510bc6b689d1.svg",
            "isPro": false,
            "fullname": "Hanning Zhang",
            "user": "HanningZhang",
            "type": "user"
          },
          "name": "Hanning Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:59:20.459Z",
          "hidden": false
        },
        {
          "_id": "6819a63c64ae18f1b60a5c46",
          "user": {
            "_id": "63a3ff69f91ad3ea5703841d",
            "avatarUrl": "/avatars/69227c4bce01d33747c1377b6f9672db.svg",
            "isPro": false,
            "fullname": "Hanze Dong",
            "user": "hendrydong",
            "type": "user"
          },
          "name": "Hanze Dong",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:59:27.200Z",
          "hidden": false
        },
        {
          "_id": "6819a63c64ae18f1b60a5c47",
          "user": {
            "_id": "6319b29809baf858241f05de",
            "avatarUrl": "/avatars/29eef2c52814abea82e2aa9bf37a7f9c.svg",
            "isPro": false,
            "fullname": "Xiong",
            "user": "WeiXiong",
            "type": "user"
          },
          "name": "Wei Xiong",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:59:34.381Z",
          "hidden": false
        },
        {
          "_id": "6819a63c64ae18f1b60a5c48",
          "user": {
            "_id": "64b8922ca1827cc8d04ae919",
            "avatarUrl": "/avatars/0aaa83e3d09a82434e1d6af724aaa485.svg",
            "isPro": false,
            "fullname": "Nan Jiang",
            "user": "nanjiang",
            "type": "user"
          },
          "name": "Nan Jiang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:59:47.400Z",
          "hidden": false
        },
        {
          "_id": "6819a63c64ae18f1b60a5c49",
          "name": "Tong Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-05T06:26:00.000Z",
      "submittedOnDailyAt": "2025-05-06T04:34:14.120Z",
      "title": "チェーンオフサスタンス推論機の最適化による勾配分散最小化\n拒否サンプリングとRLにおける",
      "submittedOnDailyBy": {
        "_id": "64d45451c34a346181b130dd",
        "avatarUrl": "/avatars/9bb8205b889337df5d321539c9b5d69d.svg",
        "isPro": false,
        "fullname": "Rui Yang",
        "user": "Ray2333",
        "type": "user"
      },
      "summary": "Chain-of-thought (CoT) 推理在大規模言語モデル (LLMs) 中可以形式化為潜在変数問題，モデルは中間的な理由ステップを生成する必要がある。先行のアプローチのように、迭り返し賞奨評価の微調節 (RAFT) などはこの構成を依存しているが、通常はプロンプト間で一貫した推論バジェットを適用していますが、難易度と収束行為の変動を考慮していません。本稿では、CoT トレーニングの主なボトルネックとして、静的なサンプリング戦略による無作為的な勾配推定の無効化を識別します。GVM-RAFT を提案し、プロンプトに対応する動的なサンプリング割当戦略を設計し、計算バジェット制約の下で標準偏差を最小化するものです。プロンプト受容率と無作為的な勾配のノルムを観測して計算リソースを動的に割り当て、その結果の勾配の標準偏差を最小化します。理論的な分析により、提案された動的なサンプリング戦略は適切な条件の下で加速された収束保証を示します。数学的な理由の実験において、GVM-RAFT は vanilla RAFT より 2-4 倍のスピードアップと相当の精度向上を収めました。提案された動的なサンプリング戦略は一般的で、GRPO などの他の強化学習アルゴリズムにも組み込め、同様の収束とテスト精度の向上を収めることができます。コードは https://github.com/RLHFlow/GVM に公開されています。",
      "upvotes": 15,
      "discussionId": "6819a63d64ae18f1b60a5c75",
      "ai_keywords": [
        "Chain-of-thought (CoT)",
        "latent variable problem",
        "iterative reward-ranked fine-tuning (RAFT)",
        "inference budget",
        "static sampling strategies",
        "GVM-RAFT",
        "Dynamic Sample Allocation Strategy",
        "prompt-specific",
        "computational budget constraint",
        "prompt acceptance rates",
        "stochastic gradient norms",
        "stochastic gradient variance",
        "accelerated convergence guarantees",
        "GRPO",
        "convergence",
        "test accuracy"
      ]
    },
    "publishedAt": "2025-05-05T02:26:00.000Z",
    "title": "Optimizing Chain-of-Thought Reasoners via Gradient Variance Minimization\n  in Rejection Sampling and RL",
    "summary": "Chain-of-thought (CoT) reasoning in large language models (LLMs) can be\nformalized as a latent variable problem, where the model needs to generate\nintermediate reasoning steps. While prior approaches such as iterative\nreward-ranked fine-tuning (RAFT) have relied on such formulations, they\ntypically apply uniform inference budgets across prompts, which fails to\naccount for variability in difficulty and convergence behavior. This work\nidentifies the main bottleneck in CoT training as inefficient stochastic\ngradient estimation due to static sampling strategies. We propose GVM-RAFT, a\nprompt-specific Dynamic Sample Allocation Strategy designed to minimize\nstochastic gradient variance under a computational budget constraint. The\nmethod dynamically allocates computational resources by monitoring prompt\nacceptance rates and stochastic gradient norms, ensuring that the resulting\ngradient variance is minimized. Our theoretical analysis shows that the\nproposed dynamic sampling strategy leads to accelerated convergence guarantees\nunder suitable conditions. Experiments on mathematical reasoning show that\nGVM-RAFT achieves a 2-4x speedup and considerable accuracy improvements over\nvanilla RAFT. The proposed dynamic sampling strategy is general and can be\nincorporated into other reinforcement learning algorithms, such as GRPO,\nleading to similar improvements in convergence and test accuracy. Our code is\navailable at https://github.com/RLHFlow/GVM.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02391.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64d45451c34a346181b130dd",
      "avatarUrl": "/avatars/9bb8205b889337df5d321539c9b5d69d.svg",
      "fullname": "Rui Yang",
      "name": "Ray2333",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.02222",
      "authors": [
        {
          "_id": "6819780dc3d212ad5b48cc07",
          "name": "Essential AI",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc09",
          "user": {
            "_id": "65ef97d0e5fc4abe66c05ed0",
            "avatarUrl": "/avatars/1d601a22639b3136bfb3519826451ddb.svg",
            "isPro": false,
            "fullname": "Ishaan Shah",
            "user": "ishaan-essential",
            "type": "user"
          },
          "name": "Ishaan Shah",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-06T08:33:24.884Z",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc0a",
          "user": {
            "_id": "6675e3ed66c4fa6d0c10e229",
            "avatarUrl": "/avatars/f73c347d824a56079729c82d60d3edc3.svg",
            "isPro": false,
            "fullname": "Anthony Polloreno",
            "user": "ampolloreno",
            "type": "user"
          },
          "name": "Anthony M. Polloreno",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:01:47.445Z",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc0b",
          "user": {
            "_id": "64d9ac38badf1110f7fcf030",
            "avatarUrl": "/avatars/c55c61af8dd52e6b4856684638b850a6.svg",
            "isPro": false,
            "fullname": "Karl Stratos",
            "user": "karlstratos",
            "type": "user"
          },
          "name": "Karl Stratos",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:01:53.826Z",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc0c",
          "user": {
            "_id": "66622dacec18341b268f97a6",
            "avatarUrl": "/avatars/8bc7d6a7c28c83aacdbeeb770716b1c0.svg",
            "isPro": false,
            "fullname": "Philip Monk",
            "user": "monk-essential",
            "type": "user"
          },
          "name": "Philip Monk",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:01:59.992Z",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc0d",
          "user": {
            "_id": "67bfd6daca6e3c22b6de31ee",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/TTt6_o9tYEgeWK26DgI_7.png",
            "isPro": false,
            "fullname": "Adarsh Chaluvaraju",
            "user": "cadarsh-essential",
            "type": "user"
          },
          "name": "Adarsh Chaluvaraju",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:02:16.867Z",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc0e",
          "user": {
            "_id": "6408e4f93461c51cf7345060",
            "avatarUrl": "/avatars/328b508e2de9e50dca2412adeb3542f5.svg",
            "isPro": false,
            "fullname": "Andrew Hojel",
            "user": "andrewhojel",
            "type": "user"
          },
          "name": "Andrew Hojel",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:02:24.655Z",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc0f",
          "user": {
            "_id": "62e24efc3a616d16e2f426ea",
            "avatarUrl": "/avatars/a2433c971f80e6cf738c03e843666cff.svg",
            "isPro": false,
            "fullname": "Andrew Ma",
            "user": "AndrewMa",
            "type": "user"
          },
          "name": "Andrew Ma",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:02:30.884Z",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc10",
          "name": "Anil Thomas",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc11",
          "name": "Ashish Tanwer",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc12",
          "name": "Darsh J Shah",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc13",
          "user": {
            "_id": "67ed7aa9290a7f9d33113fb5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/jMMTObGZktuXWJ3wVVSxj.png",
            "isPro": false,
            "fullname": "Khoi Nguyen",
            "user": "KTLK",
            "type": "user"
          },
          "name": "Khoi Nguyen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-06T08:33:20.971Z",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc14",
          "name": "Kurt Smith",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc15",
          "name": "Michael Callahan",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc16",
          "user": {
            "_id": "66cd078ea796074d428fde0f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/qThw3H6ukqIAuRy7aJTN1.jpeg",
            "isPro": false,
            "fullname": "Michael Pust",
            "user": "essentialpust",
            "type": "user"
          },
          "name": "Michael Pust",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:03:14.427Z",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc17",
          "user": {
            "_id": "674c2737d369a6de1f8f58e1",
            "avatarUrl": "/avatars/40af3aa9b9d574cc63dc328c3a465fff.svg",
            "isPro": false,
            "fullname": "Parmar Mohit",
            "user": "mohitparmar",
            "type": "user"
          },
          "name": "Mohit Parmar",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:03:21.257Z",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc18",
          "name": "Peter Rushton",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc19",
          "user": {
            "_id": "67f4ced58c4cbc2f5d95cd17",
            "avatarUrl": "/avatars/3f440a59c38f5c0c7a77746ef54ed0a5.svg",
            "isPro": false,
            "fullname": "Platon Mazarakis",
            "user": "Platona",
            "type": "user"
          },
          "name": "Platon Mazarakis",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:03:32.983Z",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc1a",
          "user": {
            "_id": "654bdcf2e06d25def57cc54b",
            "avatarUrl": "/avatars/2d2612bd7072edd60876b504345fbf25.svg",
            "isPro": false,
            "fullname": "Ritvik Kapila",
            "user": "rkapila",
            "type": "user"
          },
          "name": "Ritvik Kapila",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:03:51.154Z",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc1b",
          "name": "Saurabh Srivastava",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc1c",
          "user": {
            "_id": "679bc0b23e12a166672e5275",
            "avatarUrl": "/avatars/fe9d0e79c21c9d3594420e69e3809f0f.svg",
            "isPro": false,
            "fullname": "Somanshu Singla",
            "user": "somanshu-essential",
            "type": "user"
          },
          "name": "Somanshu Singla",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-06T08:33:27.349Z",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc1d",
          "user": {
            "_id": "67101a7165442ddc48cb4b07",
            "avatarUrl": "/avatars/551777fcd1638998ad9fd16804b313ec.svg",
            "isPro": false,
            "fullname": "Tim Romanski",
            "user": "tim-essential",
            "type": "user"
          },
          "name": "Tim Romanski",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:04:09.387Z",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc1e",
          "user": {
            "_id": "66f5f3f99ef08fe3c1f4c35a",
            "avatarUrl": "/avatars/41b8b6f90eb87e685b74587317296a1b.svg",
            "isPro": false,
            "fullname": "Yash Vanjani",
            "user": "yash-essential",
            "type": "user"
          },
          "name": "Yash Vanjani",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:04:16.859Z",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc1f",
          "name": "Ashish Vaswani",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-04T19:14:43.000Z",
      "submittedOnDailyAt": "2025-05-06T07:30:25.714Z",
      "title": "μ子の予習学習の実用的な効率",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "モーン（Muon）は、二階モジュール最終形である最も簡単なオプティマイザーであり、計算時間の負担との貪欲フロンティアをAdamWより明らかに拡張しています。モーンは、大バッチサイズでデータエフィシェンスを維持することでAdamWよりも効果的であり、計算的にも効率的であることを見出しました。これにより、モーンはより資源効率的な訓練を可能にします。また、モーンと最大更新パラメータ化（muP）の組み合わせについて、効率的な超パラメータ転移を調査し、muPのすべての誤差の原因を考慮しながらも、資源のマニュアルオーバーヘッドを引き起こすことの少ない効率的なアルゴリズムを提案しました。これらの発見を検証するために、モデルサイズが400億パラメータまでの実験やデータ分布とアーキテクチャの消去試験を行いました。",
      "upvotes": 15,
      "discussionId": "6819780fc3d212ad5b48cc89",
      "ai_keywords": [
        "second-order optimizer",
        "Pareto frontier",
        "AdamW",
        "data efficiency",
        "critical batch size",
        "computationally efficient",
        "maximal update parameterization",
        "telescoping algorithm",
        "hyperparameter transfer",
        "error sources",
        "model sizes",
        "data distribution",
        "architecture"
      ]
    },
    "publishedAt": "2025-05-04T15:14:43.000Z",
    "title": "Practical Efficiency of Muon for Pretraining",
    "summary": "We demonstrate that Muon, the simplest instantiation of a second-order\noptimizer, explicitly expands the Pareto frontier over AdamW on the\ncompute-time tradeoff. We find that Muon is more effective than AdamW in\nretaining data efficiency at large batch sizes, far beyond the so-called\ncritical batch size, while remaining computationally efficient, thus enabling\nmore economical training. We study the combination of Muon and the maximal\nupdate parameterization (muP) for efficient hyperparameter transfer and present\na simple telescoping algorithm that accounts for all sources of error in muP\nwhile introducing only a modest overhead in resources. We validate our findings\nthrough extensive experiments with model sizes up to four billion parameters\nand ablations on the data distribution and architecture.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02222.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6784
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.02094",
      "authors": [
        {
          "_id": "681992911e0fae3880173d43",
          "user": {
            "_id": "66d59dc9b005ad82ca6fc61d",
            "avatarUrl": "/avatars/0ba424690afd1144a89665c5bacdfde7.svg",
            "isPro": false,
            "fullname": "Runyi YU",
            "user": "IngridYU",
            "type": "user"
          },
          "name": "Runyi Yu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:00:09.333Z",
          "hidden": false
        },
        {
          "_id": "681992911e0fae3880173d44",
          "name": "Yinhuai Wang",
          "hidden": false
        },
        {
          "_id": "681992911e0fae3880173d45",
          "user": {
            "_id": "64341911546e16f17a129733",
            "avatarUrl": "/avatars/ae12aafc8932a7537838e6d3964858cb.svg",
            "isPro": false,
            "fullname": "QiHan Zhao",
            "user": "Crimnos",
            "type": "user"
          },
          "name": "Qihan Zhao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:00:32.182Z",
          "hidden": false
        },
        {
          "_id": "681992911e0fae3880173d46",
          "name": "Hok Wai Tsui",
          "hidden": false
        },
        {
          "_id": "681992911e0fae3880173d47",
          "name": "Jingbo Wang",
          "hidden": false
        },
        {
          "_id": "681992911e0fae3880173d48",
          "name": "Ping Tan",
          "hidden": false
        },
        {
          "_id": "681992911e0fae3880173d49",
          "user": {
            "_id": "6467b121e7a6a374fd19b44b",
            "avatarUrl": "/avatars/3f2874d58986d651aef55e3408b05700.svg",
            "isPro": false,
            "fullname": "Qifeng Chen",
            "user": "cqf",
            "type": "user"
          },
          "name": "Qifeng Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:01:21.751Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-04T13:00:29.000Z",
      "submittedOnDailyAt": "2025-05-06T03:11:28.738Z",
      "title": "スキルミモク-V2: 稀少でノイズの多いデモストレーションから強靭かつ一般化可能な相互作用スキルを学習する",
      "submittedOnDailyBy": {
        "_id": "66d59dc9b005ad82ca6fc61d",
        "avatarUrl": "/avatars/0ba424690afd1144a89665c5bacdfde7.svg",
        "isPro": false,
        "fullname": "Runyi YU",
        "user": "IngridYU",
        "type": "user"
      },
      "summary": "We address a fundamental challenge in Reinforcement Learning from Interaction\nDemonstration (RLID): demonstration noise and coverage limitations. While\nexisting data collection approaches provide valuable interaction\ndemonstrations, they often yield sparse, disconnected, and noisy trajectories\nthat fail to capture the full spectrum of possible skill variations and\ntransitions. Our key insight is that despite noisy and sparse demonstrations,\nthere exist infinite physically feasible trajectories that naturally bridge\nbetween demonstrated skills or emerge from their neighboring states, forming a\ncontinuous space of possible skill variations and transitions. Building upon\nthis insight, we present two data augmentation techniques: a Stitched\nTrajectory Graph (STG) that discovers potential transitions between\ndemonstration skills, and a State Transition Field (STF) that establishes\nunique connections for arbitrary states within the demonstration neighborhood.\nTo enable effective RLID with augmented data, we develop an Adaptive Trajectory\nSampling (ATS) strategy for dynamic curriculum generation and a historical\nencoding mechanism for memory-dependent skill learning. Our approach enables\nrobust skill acquisition that significantly generalizes beyond the reference\ndemonstrations. Extensive experiments across diverse interaction tasks\ndemonstrate substantial improvements over state-of-the-art methods in terms of\nconvergence stability, generalization capability, and recovery robustness.",
      "upvotes": 12,
      "discussionId": "681992931e0fae3880173dcf",
      "ai_keywords": [
        "Reinforcement Learning from Interaction Demonstration (RLID)",
        "demonstration noise",
        "coverage limitations",
        "interaction demonstrations",
        "sparse trajectories",
        "disconnected trajectories",
        "noise",
        "skill variations",
        "transitions",
        "physically feasible trajectories",
        "Stitched Trajectory Graph (STG)",
        "State Transition Field (STF)",
        "Adaptive Trajectory Sampling (ATS)",
        "dynamic curriculum generation",
        "historical encoding mechanism",
        "skill acquisition",
        "convergence stability",
        "generalization capability",
        "recovery robustness"
      ]
    },
    "publishedAt": "2025-05-04T09:00:29.000Z",
    "title": "SkillMimic-V2: Learning Robust and Generalizable Interaction Skills from\n  Sparse and Noisy Demonstrations",
    "summary": "We address a fundamental challenge in Reinforcement Learning from Interaction\nDemonstration (RLID): demonstration noise and coverage limitations. While\nexisting data collection approaches provide valuable interaction\ndemonstrations, they often yield sparse, disconnected, and noisy trajectories\nthat fail to capture the full spectrum of possible skill variations and\ntransitions. Our key insight is that despite noisy and sparse demonstrations,\nthere exist infinite physically feasible trajectories that naturally bridge\nbetween demonstrated skills or emerge from their neighboring states, forming a\ncontinuous space of possible skill variations and transitions. Building upon\nthis insight, we present two data augmentation techniques: a Stitched\nTrajectory Graph (STG) that discovers potential transitions between\ndemonstration skills, and a State Transition Field (STF) that establishes\nunique connections for arbitrary states within the demonstration neighborhood.\nTo enable effective RLID with augmented data, we develop an Adaptive Trajectory\nSampling (ATS) strategy for dynamic curriculum generation and a historical\nencoding mechanism for memory-dependent skill learning. Our approach enables\nrobust skill acquisition that significantly generalizes beyond the reference\ndemonstrations. Extensive experiments across diverse interaction tasks\ndemonstrate substantial improvements over state-of-the-art methods in terms of\nconvergence stability, generalization capability, and recovery robustness.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02094.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66d59dc9b005ad82ca6fc61d",
      "avatarUrl": "/avatars/0ba424690afd1144a89665c5bacdfde7.svg",
      "fullname": "Runyi YU",
      "name": "IngridYU",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.02156",
      "authors": [
        {
          "_id": "681975a9fdcf582e6d0effdb",
          "user": {
            "_id": "64bcc373ef8c0e42bf16acc5",
            "avatarUrl": "/avatars/873308203d28115ae1a9e4d0e26508f4.svg",
            "isPro": false,
            "fullname": "mz.w",
            "user": "iiiiwis",
            "type": "user"
          },
          "name": "Minzheng Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-06T08:33:29.678Z",
          "hidden": false
        },
        {
          "_id": "681975a9fdcf582e6d0effdc",
          "user": {
            "_id": "66641b2fd8e1e34bc621e688",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66641b2fd8e1e34bc621e688/csPETwnx2zCIHSWi9uAi-.png",
            "isPro": false,
            "fullname": "Yongbin Li",
            "user": "Yongbin-Li",
            "type": "user"
          },
          "name": "Yongbin Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:05:06.051Z",
          "hidden": false
        },
        {
          "_id": "681975a9fdcf582e6d0effdd",
          "name": "Haobo Wang",
          "hidden": false
        },
        {
          "_id": "681975a9fdcf582e6d0effde",
          "name": "Xinghua Zhang",
          "hidden": false
        },
        {
          "_id": "681975a9fdcf582e6d0effdf",
          "name": "Nan Xu",
          "hidden": false
        },
        {
          "_id": "681975a9fdcf582e6d0effe0",
          "user": {
            "_id": "668bd45044ab5de7e4c5b1e7",
            "avatarUrl": "/avatars/9b087cfcac65a649a12568b601d5ca53.svg",
            "isPro": false,
            "fullname": "bingli wu",
            "user": "bingliwu",
            "type": "user"
          },
          "name": "Bingli Wu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:05:37.861Z",
          "hidden": false
        },
        {
          "_id": "681975a9fdcf582e6d0effe1",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "681975a9fdcf582e6d0effe2",
          "name": "Haiyang Yu",
          "hidden": false
        },
        {
          "_id": "681975a9fdcf582e6d0effe3",
          "name": "Wenji Mao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-04T15:39:58.000Z",
      "submittedOnDailyAt": "2025-05-06T01:07:27.275Z",
      "title": "Think on Your Feet: ソーシャルアガントにおけるリニューアルローディングによる適応的思考",
      "submittedOnDailyBy": {
        "_id": "64bcc373ef8c0e42bf16acc5",
        "avatarUrl": "/avatars/873308203d28115ae1a9e4d0e26508f4.svg",
        "isPro": false,
        "fullname": "mz.w",
        "user": "iiiiwis",
        "type": "user"
      },
      "summary": "効果的ソーシャルインテリジェンスシミュレーションには、言語アガントが理由の深さを動的に調整する能力が必要です。現在のアプローチではこのような理由の能力が特に欠けています。既存の方法は、このような理由の能力を欠けているか、すべてのシナリオで一貫した長いチャインオフサインエモジションを強制しています。これにより、トークンの使用が過剰で、適切なソーシャルシミュレーションにならない場合があります。本論文では、時間のコンテキストに基づいて4つの思考モード（直感的な反応→深い見込み）から戦略的に選択するAdaptive Mode Learning（AML）を提案します。本フレームワークの核心的な革新として、Adaptive Mode Policy Optimization（AMPO）アルゴリズムが、既存の方法に対して3つの進展を導入します。それは、(1) 多粒度の思考モードの設計、(2) ソーシャルインタラクションのコンテキストに関わるモードの切り替え、(3) トークン効率的な理由による深さ適応的な処理です。ソーシャルインテリジェンスタスクにおいて幅広い実験を行い、AMLは最先端の方法より15.6%よりも高いタスク性能を実現しました。特に、我々の方法はGRPOを7.0%よりも優秀に、理由のチャインが32.8%より短くなります。これらの結果は、AMPOで実装されるコンテキスト適応的な思考モード選択が、GRPOの固定の深さアプローチよりも人間のような適応的な理由を可能にします。",
      "upvotes": 11,
      "discussionId": "681975a9fdcf582e6d0f0014",
      "githubRepo": "https://github.com/MozerWang/AMPO",
      "ai_keywords": [
        "Adaptive Mode Learning (AML)",
        "Adaptive Mode Policy Optimization (AMPO)",
        "multi-granular thinking mode design",
        "context-aware mode switching",
        "token-efficient reasoning",
        "depth-adaptive processing",
        "intuitive reaction",
        "deep contemplation",
        "social interaction",
        "reasoning chains",
        "fixed-depth approach"
      ]
    },
    "publishedAt": "2025-05-04T11:39:58.000Z",
    "title": "Think on your Feet: Adaptive Thinking via Reinforcement Learning for\n  Social Agents",
    "summary": "Effective social intelligence simulation requires language agents to\ndynamically adjust reasoning depth, a capability notably absent in current\napproaches. While existing methods either lack this kind of reasoning\ncapability or enforce uniform long chain-of-thought reasoning across all\nscenarios, resulting in excessive token usage and inappropriate social\nsimulation. In this paper, we propose Adaptive Mode\nLearning (AML) that strategically selects from four\nthinking modes (intuitive reaction rightarrow deep contemplation) based on\nreal-time context. Our framework's core innovation, the Adaptive\nMode Policy Optimization (AMPO)\nalgorithm, introduces three key advancements over existing methods: (1)\nMulti-granular thinking mode design, (2) Context-aware mode switching across\nsocial interaction, and (3) Token-efficient reasoning via depth-adaptive\nprocessing. Extensive experiments on social intelligence tasks confirm that AML\nachieves 15.6% higher task performance than state-of-the-art methods. Notably,\nour method outperforms GRPO by 7.0% with 32.8% shorter reasoning chains. These\nresults demonstrate that context-sensitive thinking mode selection, as\nimplemented in AMPO, enables more human-like adaptive reasoning than GRPO's\nfixed-depth approach",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02156.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64bcc373ef8c0e42bf16acc5",
      "avatarUrl": "/avatars/873308203d28115ae1a9e4d0e26508f4.svg",
      "fullname": "mz.w",
      "name": "iiiiwis",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.01658",
      "authors": [
        {
          "_id": "6819950bd55db085708dd2e5",
          "user": {
            "_id": "670cb786e73576f33a339144",
            "avatarUrl": "/avatars/c172887c32878aebafd786061680ea1e.svg",
            "isPro": false,
            "fullname": "Sihyeong Park",
            "user": "inputsh",
            "type": "user"
          },
          "name": "Sihyeong Park",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:09:00.902Z",
          "hidden": false
        },
        {
          "_id": "6819950bd55db085708dd2e6",
          "name": "Sungryeol Jeon",
          "hidden": false
        },
        {
          "_id": "6819950bd55db085708dd2e7",
          "user": {
            "_id": "64aaa12a04e7b379fed24327",
            "avatarUrl": "/avatars/327482e569c24ee4c97064f07ddd6de7.svg",
            "isPro": false,
            "fullname": "Chaelyn Lee",
            "user": "oos2",
            "type": "user"
          },
          "name": "Chaelyn Lee",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:09:13.239Z",
          "hidden": false
        },
        {
          "_id": "6819950bd55db085708dd2e8",
          "user": {
            "_id": "6719f17ac5837d514cfff13b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/GnL4RCj7xncVhCIFN5y35.png",
            "isPro": false,
            "fullname": "Seokhun Jeon",
            "user": "Devcow",
            "type": "user"
          },
          "name": "Seokhun Jeon",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:09:18.870Z",
          "hidden": false
        },
        {
          "_id": "6819950bd55db085708dd2e9",
          "name": "Byung-Soo Kim",
          "hidden": false
        },
        {
          "_id": "6819950bd55db085708dd2ea",
          "user": {
            "_id": "65b9dee19c4955ae7aee4954",
            "avatarUrl": "/avatars/263f129605c7763185c49076174b891b.svg",
            "isPro": false,
            "fullname": "Jemin Lee",
            "user": "leejaymin",
            "type": "user"
          },
          "name": "Jemin Lee",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-06T08:32:52.950Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-03T02:47:43.000Z",
      "submittedOnDailyAt": "2025-05-06T03:21:53.083Z",
      "title": "大語言モデルの推論エンジンに関する調査：最適化と効率的さの視点",
      "submittedOnDailyBy": {
        "_id": "65b9dee19c4955ae7aee4954",
        "avatarUrl": "/avatars/263f129605c7763185c49076174b891b.svg",
        "isPro": false,
        "fullname": "Jemin Lee",
        "user": "leejaymin",
        "type": "user"
      },
      "summary": "大語言モデル（LLMs）は、チャットボット、コード生成サービス、サーチエンジンなどに広く応用されています。チャインオフショート、複雑な論理、アジェントサービスなどのワークロードは、モデルを再度と呼び出して推論コストを大幅に上昇させます。並列化、圧縮、キャッシュなどの最適化手法が採用されており、これらの手法を選択するのに難しいことは、多様なサービスの要求により生じます。最近、特化されたLLM推論エンジンがサービス向けインフラ構造に最適化手法を統合するための重要な構成要素として登場しました。しかし、推論エンジンに関する系統的な研究はまだ不足しています。本論文では、25つの開放ソースや商業推論エンジンについての詳細な評価を提供します。各推論エンジンの使用手順、デプロイメント手順、一般用語サポート、スケーラビリティ、トランスポートとラテンシーに関する計算に適した性質について調査します。また、それぞれの推論エンジンの設計目標を明確にするために、それぞれにサポートされる最適化テクニックを調査します。また、開放ソース推論エンジンのエコシステムの成熟度を評価し、商業解決策の性能とコストポリシーも評価します。将来の研究方向を明確にし、複雑なLLMベースのサービスや多様なハードウェアのサポート、セキュリティの向上を含む、研究者や開発者に実用的なガイドラインを提供します。また、この急速に進化的している分野の開発を継続的に追跡するための公開リポジトリを提供します：https://github.com/sihyeong/Awesome-LLM-Inference-Engine",
      "upvotes": 9,
      "discussionId": "6819950cd55db085708dd32a",
      "ai_keywords": [
        "chain-of-thought",
        "complex reasoning",
        "agent services",
        "inference cost",
        "parallelism",
        "compression",
        "caching",
        "LLM inference engines",
        "ease-of-use",
        "ease-of-deployment",
        "general-purpose support",
        "scalability",
        "throughput-aware computation",
        "latency-aware computation",
        "optimization techniques",
        "ecosystem maturity",
        "performance",
        "cost policy",
        "LLM-based services",
        "enhanced security"
      ]
    },
    "publishedAt": "2025-05-02T22:47:43.000Z",
    "title": "A Survey on Inference Engines for Large Language Models: Perspectives on\n  Optimization and Efficiency",
    "summary": "Large language models (LLMs) are widely applied in chatbots, code generators,\nand search engines. Workloads such as chain-of-thought, complex reasoning, and\nagent services significantly increase the inference cost by invoking the model\nrepeatedly. Optimization methods such as parallelism, compression, and caching\nhave been adopted to reduce costs, but the diverse service requirements make it\nhard to select the right method. Recently, specialized LLM inference engines\nhave emerged as a key component for integrating the optimization methods into\nservice-oriented infrastructures. However, a systematic study on inference\nengines is still lacking. This paper provides a comprehensive evaluation of 25\nopen-source and commercial inference engines. We examine each inference engine\nin terms of ease-of-use, ease-of-deployment, general-purpose support,\nscalability, and suitability for throughput- and latency-aware computation.\nFurthermore, we explore the design goals of each inference engine by\ninvestigating the optimization techniques it supports. In addition, we assess\nthe ecosystem maturity of open source inference engines and handle the\nperformance and cost policy of commercial solutions. We outline future research\ndirections that include support for complex LLM-based services, support of\nvarious hardware, and enhanced security, offering practical guidance to\nresearchers and developers in selecting and designing optimized LLM inference\nengines. We also provide a public repository to continually track developments\nin this fast-evolving field:\nhttps://github.com/sihyeong/Awesome-LLM-Inference-Engine",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.01658.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65b9dee19c4955ae7aee4954",
      "avatarUrl": "/avatars/263f129605c7763185c49076174b891b.svg",
      "fullname": "Jemin Lee",
      "name": "leejaymin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.01441",
      "authors": [
        {
          "_id": "68198aea57d4de18fb3e69d6",
          "user": {
            "_id": "61ffaa2943eb0913fa2df74a",
            "avatarUrl": "/avatars/a19971f830abb8a8ae95e5800beb9fcd.svg",
            "isPro": false,
            "fullname": "Singh",
            "user": "joykirat",
            "type": "user"
          },
          "name": "Joykirat Singh",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-06T08:33:08.255Z",
          "hidden": false
        },
        {
          "_id": "68198aea57d4de18fb3e69d7",
          "user": {
            "_id": "622ca32345261ac5cc0bdade",
            "avatarUrl": "/avatars/7e1d633be69cf86a3affb9168b1cc27b.svg",
            "isPro": false,
            "fullname": "Raghav Magazine",
            "user": "Raghav2002",
            "type": "user"
          },
          "name": "Raghav Magazine",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:06:32.498Z",
          "hidden": false
        },
        {
          "_id": "68198aea57d4de18fb3e69d8",
          "user": {
            "_id": "64aba383fddf117e6e5ba818",
            "avatarUrl": "/avatars/ee7d25d865b34be5902872d060ad9153.svg",
            "isPro": false,
            "fullname": "Akshay  Nambi",
            "user": "akshaynambi",
            "type": "user"
          },
          "name": "Yash Pandya",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-06T04:08:10.843Z",
          "hidden": false
        },
        {
          "_id": "68198aea57d4de18fb3e69d9",
          "user": {
            "_id": "64aba383fddf117e6e5ba818",
            "avatarUrl": "/avatars/ee7d25d865b34be5902872d060ad9153.svg",
            "isPro": false,
            "fullname": "Akshay  Nambi",
            "user": "akshaynambi",
            "type": "user"
          },
          "name": "Akshay Nambi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:06:46.024Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-28T10:42:49.000Z",
      "submittedOnDailyAt": "2025-05-06T02:43:42.049Z",
      "title": "Agentic ReasoningとTool IntegrationをLLMsにおけるReinforcement Learningを通じて実現する方法",
      "submittedOnDailyBy": {
        "_id": "64aba383fddf117e6e5ba818",
        "avatarUrl": "/avatars/ee7d25d865b34be5902872d060ad9153.svg",
        "isPro": false,
        "fullname": "Akshay  Nambi",
        "user": "akshaynambi",
        "type": "user"
      },
      "summary": "大語言モデル（LLMs）は複雑な理由論問題で驚異的な進歩を達成しましたが、静的な内部知識と文脈のみの理由論に依存しているため、本質的に限界があります。実世界的な問題解決は、動的な、多段階の理由論、適応的な決策、外部ツールと環境との相互作用の能力を求めます。本論文では、Agentic Reasoning and Tool Integration in Self-improving Transformers（ARTIST）という統一的なフレームワークを紹介します。このフレームワークは、LLMsのエージェント的な理由論、強化学習、ツール統合を厳密に結びつけています。ARTISTは、多段階の理由論チェーン内で自動的にどのタイミングで、どのように、どのツールを呼び出すかを決定することを可能にし、結果ベースの強化学習を利用して、ツールの使用と環境の相互作用の強固な戦略を学習することができます。数学的な理由論と多段階の関数呼び出しベンチマークにおいての拡散的な実験は、ARTISTが最先端のベースラインを一致していることを示し、基礎モデルより最大で22%の絶対的な向上を示し、最難問のタスクにおいても強い向上を示しました。詳細な研究とメトリック分析は、エージェント的な強化学習の訓練が深い理由論、更有効なツールの使用、高品質な解決策を実現することを示しました。我々の結果は、LLMsでの強固な、解釈可能な、一般化可能な問題解決の新たな前進方向として、ツール統合と強化学習を組み合わせたものを有力な新たな境地として確立しました。",
      "upvotes": 9,
      "discussionId": "68198aec57d4de18fb3e6a30",
      "projectPage": "https://www.microsoft.com/en-us/research/people/akshayn/unlocking-agentic-reasoning-in-llms/",
      "ai_keywords": [
        "agentic reasoning",
        "reinforcement learning",
        "tool integration",
        "ARTIST",
        "multi-turn reasoning chains",
        "outcome-based RL",
        "mathematical reasoning",
        "function calling",
        "agentic RL",
        "tool use",
        "environment interaction"
      ]
    },
    "publishedAt": "2025-04-28T06:42:49.000Z",
    "title": "Agentic Reasoning and Tool Integration for LLMs via Reinforcement\n  Learning",
    "summary": "Large language models (LLMs) have achieved remarkable progress in complex\nreasoning tasks, yet they remain fundamentally limited by their reliance on\nstatic internal knowledge and text-only reasoning. Real-world problem solving\noften demands dynamic, multi-step reasoning, adaptive decision making, and the\nability to interact with external tools and environments. In this work, we\nintroduce ARTIST (Agentic Reasoning and Tool Integration in Self-improving\nTransformers), a unified framework that tightly couples agentic reasoning,\nreinforcement learning, and tool integration for LLMs. ARTIST enables models to\nautonomously decide when, how, and which tools to invoke within multi-turn\nreasoning chains, leveraging outcome-based RL to learn robust strategies for\ntool use and environment interaction without requiring step-level supervision.\nExtensive experiments on mathematical reasoning and multi-turn function calling\nbenchmarks show that ARTIST consistently outperforms state-of-the-art\nbaselines, with up to 22% absolute improvement over base models and strong\ngains on the most challenging tasks. Detailed studies and metric analyses\nreveal that agentic RL training leads to deeper reasoning, more effective tool\nuse, and higher-quality solutions. Our results establish agentic RL with tool\nintegration as a powerful new frontier for robust, interpretable, and\ngeneralizable problem-solving in LLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.01441.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64aba383fddf117e6e5ba818",
      "avatarUrl": "/avatars/ee7d25d865b34be5902872d060ad9153.svg",
      "fullname": "Akshay  Nambi",
      "name": "akshaynambi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.02370",
      "authors": [
        {
          "_id": "68197c200e4203d6bc84cdfb",
          "user": {
            "_id": "637f0eb22438d7485b8ef5d7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637f0eb22438d7485b8ef5d7/70h7dekqj7LuBobOXckmJ.jpeg",
            "isPro": false,
            "fullname": "Ming Li",
            "user": "limingcv",
            "type": "user"
          },
          "name": "Ming Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-06T08:33:18.243Z",
          "hidden": false
        },
        {
          "_id": "68197c200e4203d6bc84cdfc",
          "name": "Xin Gu",
          "hidden": false
        },
        {
          "_id": "68197c200e4203d6bc84cdfd",
          "name": "Fan Chen",
          "hidden": false
        },
        {
          "_id": "68197c200e4203d6bc84cdfe",
          "user": {
            "_id": "64ca92f738837b12d5f63729",
            "avatarUrl": "/avatars/a361be3a5ccf9368717980d1faf69df0.svg",
            "isPro": false,
            "fullname": "Xiaoying Xing",
            "user": "xiaoying0505",
            "type": "user"
          },
          "name": "Xiaoying Xing",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:06:58.770Z",
          "hidden": false
        },
        {
          "_id": "68197c200e4203d6bc84cdff",
          "user": {
            "_id": "644df7eacfb40c94eae71186",
            "avatarUrl": "/avatars/1daa4967efd34d54c59aa95970093dbd.svg",
            "isPro": false,
            "fullname": "Longyin Wen",
            "user": "lionwen",
            "type": "user"
          },
          "name": "Longyin Wen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:07:05.196Z",
          "hidden": false
        },
        {
          "_id": "68197c200e4203d6bc84ce00",
          "name": "Chen Chen",
          "hidden": false
        },
        {
          "_id": "68197c200e4203d6bc84ce01",
          "user": {
            "_id": "65cbdea6d6c974694f09249a",
            "avatarUrl": "/avatars/a317a1f545117e0699e1c56258980fd8.svg",
            "isPro": false,
            "fullname": "Sijie Zhu",
            "user": "Zilence006",
            "type": "user"
          },
          "name": "Sijie Zhu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-06T03:04:04.536Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-05T05:19:40.000Z",
      "submittedOnDailyAt": "2025-05-06T01:34:41.608Z",
      "title": "スーパーエディット：インストラクションベースの画像編集の視聴調整と効率的化",
      "submittedOnDailyBy": {
        "_id": "637f0eb22438d7485b8ef5d7",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637f0eb22438d7485b8ef5d7/70h7dekqj7LuBobOXckmJ.jpeg",
        "isPro": false,
        "fullname": "Ming Li",
        "user": "limingcv",
        "type": "user"
      },
      "summary": "データセットの構築において、手動で正確な編集データを収集する難関があるため、現在のデータセットは通常、様々な自動化手法を用いて構築されています。これにより、編集指示と元画像・編集後画像のマッチングによるノイズを原因とした不純なサブジェクション信号が発生します。最近の努力は、編集モデルの向上を達成するために、高品質の編集画像の生成、認識タスクの事前学習、または視覚言語モデル（VLMs）の導入を試みていますが、この基本的な問題を解決することはできません。本論文では、与えられた画像ペアに対してより有效な編集指示を構築する新しい解決策を提案します。これには、編集指示を正確化し、元画像・編集後画像によりより合わせることと、比較的な編集指示を使用してその効果を進一步に高めることを含みます。具体的には、編集モデルは、文脈に依存しないように、推論ステップごとに特定の生成属性を示しています。これらの先行属性に基づき、VLMsに対して編集指示を正確化するための統一的なガイドを定義します。しかし、一部の難しい編集シナリオは、これらの正確化された指示でそれだけで解決できない場合があります。そこで、正の指示と負の指示を用いて比較的なサブジェクション信号を構築し、これらをディープラーニングモデルの訓練にタプル損失を使用して挿入し、これによりより効果的なサブジェクションを促進することを試みます。我々の方法は、先行研究で使用されていたVLMモジュールや事前学習タスクを必要としません、より直接で効率的な方法で、より良いサブジェクション信号を提供することができ、インストラクションベースの画像編集において新しい、簡単で効果的な解決策を提供します。複数のベンチマーク上での結果は、我々の方法が現在のアプローチより显著に優れていることを示しています。前のSOTA SmartEditと比較して、Real-Editベンチマークでは30倍少ない学習データと13倍小さなモデルサイズで9.19%の向上を達成しました。",
      "upvotes": 8,
      "discussionId": "68197c240e4203d6bc84cee9",
      "projectPage": "https://liming-ai.github.io/SuperEdit/",
      "githubRepo": "https://github.com/bytedance/SuperEdit",
      "ai_keywords": [
        "contrastive editing instructions",
        "triplet loss",
        "instruction-based image editing",
        "contrastive supervision signals",
        "generation attributes",
        "unified guide",
        "vision-language models (VLMs)",
        "real-edit benchmark",
        "smartedit"
      ]
    },
    "publishedAt": "2025-05-05T01:19:40.000Z",
    "title": "SuperEdit: Rectifying and Facilitating Supervision for Instruction-Based\n  Image Editing",
    "summary": "Due to the challenges of manually collecting accurate editing data, existing\ndatasets are typically constructed using various automated methods, leading to\nnoisy supervision signals caused by the mismatch between editing instructions\nand original-edited image pairs. Recent efforts attempt to improve editing\nmodels through generating higher-quality edited images, pre-training on\nrecognition tasks, or introducing vision-language models (VLMs) but fail to\nresolve this fundamental issue. In this paper, we offer a novel solution by\nconstructing more effective editing instructions for given image pairs. This\nincludes rectifying the editing instructions to better align with the\noriginal-edited image pairs and using contrastive editing instructions to\nfurther enhance their effectiveness. Specifically, we find that editing models\nexhibit specific generation attributes at different inference steps,\nindependent of the text. Based on these prior attributes, we define a unified\nguide for VLMs to rectify editing instructions. However, there are some\nchallenging editing scenarios that cannot be resolved solely with rectified\ninstructions. To this end, we further construct contrastive supervision signals\nwith positive and negative instructions and introduce them into the model\ntraining using triplet loss, thereby further facilitating supervision\neffectiveness. Our method does not require the VLM modules or pre-training\ntasks used in previous work, offering a more direct and efficient way to\nprovide better supervision signals, and providing a novel, simple, and\neffective solution for instruction-based image editing. Results on multiple\nbenchmarks demonstrate that our method significantly outperforms existing\napproaches. Compared with previous SOTA SmartEdit, we achieve 9.19%\nimprovements on the Real-Edit benchmark with 30x less training data and 13x\nsmaller model size.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02370.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "637f0eb22438d7485b8ef5d7",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637f0eb22438d7485b8ef5d7/70h7dekqj7LuBobOXckmJ.jpeg",
      "fullname": "Ming Li",
      "name": "limingcv",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 22
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.01043",
      "authors": [
        {
          "_id": "68196e23d9cad0bb5c90dd9b",
          "user": {
            "_id": "64a62e3302e46deb19a7937e",
            "avatarUrl": "/avatars/43553a80f2c5f6c91742c4ce2d23fe21.svg",
            "isPro": false,
            "fullname": "Zhiwei Hao",
            "user": "Zhiwei840",
            "type": "user"
          },
          "name": "Zhiwei Hao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-06T08:34:14.150Z",
          "hidden": false
        },
        {
          "_id": "68196e23d9cad0bb5c90dd9c",
          "user": {
            "_id": "65c4a574d2db41f74ab2a808",
            "avatarUrl": "/avatars/997a8a51996e909eeb318dc592b6c67a.svg",
            "isPro": false,
            "fullname": "Jianyuan Guo",
            "user": "GGJY",
            "type": "user"
          },
          "name": "Jianyuan Guo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:08:12.386Z",
          "hidden": false
        },
        {
          "_id": "68196e23d9cad0bb5c90dd9d",
          "name": "Li Shen",
          "hidden": false
        },
        {
          "_id": "68196e23d9cad0bb5c90dd9e",
          "user": {
            "_id": "6306dc1fd37ce67e0e53c202",
            "avatarUrl": "/avatars/d53a29925511a516495b1597fd5dc764.svg",
            "isPro": false,
            "fullname": "Yong Luo",
            "user": "csdvT",
            "type": "user"
          },
          "name": "Yong Luo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:08:19.043Z",
          "hidden": false
        },
        {
          "_id": "68196e23d9cad0bb5c90dd9f",
          "name": "Han Hu",
          "hidden": false
        },
        {
          "_id": "68196e23d9cad0bb5c90dda0",
          "user": {
            "_id": "662520a75480987954af60b5",
            "avatarUrl": "/avatars/75d2509d21901c4bb187e93b23540e19.svg",
            "isPro": false,
            "fullname": "Guoxia Wang",
            "user": "Guoxia",
            "type": "user"
          },
          "name": "Guoxia Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:08:27.139Z",
          "hidden": false
        },
        {
          "_id": "68196e23d9cad0bb5c90dda1",
          "name": "Dianhai Yu",
          "hidden": false
        },
        {
          "_id": "68196e23d9cad0bb5c90dda2",
          "name": "Yonggang Wen",
          "hidden": false
        },
        {
          "_id": "68196e23d9cad0bb5c90dda3",
          "name": "Dacheng Tao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-02T06:33:25.000Z",
      "submittedOnDailyAt": "2025-05-06T00:36:54.063Z",
      "title": "大規模言語モデルの低精度訓練：方法、課題と機会",
      "submittedOnDailyBy": {
        "_id": "64a62e3302e46deb19a7937e",
        "avatarUrl": "/avatars/43553a80f2c5f6c91742c4ce2d23fe21.svg",
        "isPro": false,
        "fullname": "Zhiwei Hao",
        "user": "Zhiwei840",
        "type": "user"
      },
      "summary": "大語言モデル（LLMs）は、様々な領域で驚異的な性能を達成しました。しかし、それらの訓練に必要な大規模なハードウェアリソースは、効率性とスケーラビリティに重大な壁障となっています。この挑戦を軽減するために、低精度訓練技術が広く採用され、訓練効率に関しては顕著な進展を遂げました。これらの効果に加えて、低精度訓練は重み、活性化、勾配などの複数の成分を含み、それぞれは異なる数値フォーマットで表現できます。このような多様性は、低精度訓練研究の分断した価値観を創成し、研究者がこの分野について一貫した概要を得ることが難しくなりました。この調査では、現在の低精度訓練手法について一詳しいレビューを提供します。これらの手法をシステマティックに整理するために、ハードウェアの相性、計算効率、および読者向けの読みやすさに影響を与える基礎的な数値フォーマットに基づいて3つの主なグループに分類します。これらのグループは、(1) 固定点と整数基の方法、(2) 浮動点基の方法、(3) カスタマイズされたフォーマット基の方法です。また、量規認識訓練アプローチについても議論し、これらは進行方向の訓練と低精度訓練との関連性があることを示しています。最後に、この分野を進めるための数多くの望ましい研究方向を挙げます。この調査で議論された論文のコレクションは、https://github.com/Hao840/Awesome-Low-Precision-Trainingに提供されています。",
      "upvotes": 8,
      "discussionId": "68196e24d9cad0bb5c90de08",
      "githubRepo": "https://github.com/Hao840/Awesome-Low-Precision-Training",
      "ai_keywords": [
        "low-precision training",
        "weights",
        "activations",
        "gradients",
        "fixed-point",
        "integer-based methods",
        "floating-point-based methods",
        "customized format-based methods",
        "quantization-aware training"
      ]
    },
    "publishedAt": "2025-05-02T02:33:25.000Z",
    "title": "Low-Precision Training of Large Language Models: Methods, Challenges,\n  and Opportunities",
    "summary": "Large language models (LLMs) have achieved impressive performance across\nvarious domains. However, the substantial hardware resources required for their\ntraining present a significant barrier to efficiency and scalability. To\nmitigate this challenge, low-precision training techniques have been widely\nadopted, leading to notable advancements in training efficiency. Despite these\ngains, low-precision training involves several componentsx2013such\nas weights, activations, and gradientsx2013each of which can be\nrepresented in different numerical formats. The resulting diversity has created\na fragmented landscape in low-precision training research, making it difficult\nfor researchers to gain a unified overview of the field. This survey provides a\ncomprehensive review of existing low-precision training methods. To\nsystematically organize these approaches, we categorize them into three primary\ngroups based on their underlying numerical formats, which is a key factor\ninfluencing hardware compatibility, computational efficiency, and ease of\nreference for readers. The categories are: (1) fixed-point and integer-based\nmethods, (2) floating-point-based methods, and (3) customized format-based\nmethods. Additionally, we discuss quantization-aware training approaches, which\nshare key similarities with low-precision training during forward propagation.\nFinally, we highlight several promising research directions to advance this\nfield. A collection of papers discussed in this survey is provided in\nhttps://github.com/Hao840/Awesome-Low-Precision-Training.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.01043.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64a62e3302e46deb19a7937e",
      "avatarUrl": "/avatars/43553a80f2c5f6c91742c4ce2d23fe21.svg",
      "fullname": "Zhiwei Hao",
      "name": "Zhiwei840",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.02471",
      "authors": [
        {
          "_id": "681973cfa70a4728958323aa",
          "user": {
            "_id": "644fcbea4f7316588267dc80",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644fcbea4f7316588267dc80/w8-2Gkaw9BN9VzppNXrTP.jpeg",
            "isPro": false,
            "fullname": "Biao Gong",
            "user": "BiaoGong",
            "type": "user"
          },
          "name": "Biao Gong",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:10:35.774Z",
          "hidden": false
        },
        {
          "_id": "681973cfa70a4728958323ab",
          "name": "Cheng Zou",
          "hidden": false
        },
        {
          "_id": "681973cfa70a4728958323ac",
          "user": {
            "_id": "65dd699a89a2a760d15f7d35",
            "avatarUrl": "/avatars/e098b56c413d147d1f38cf33a4b0ecde.svg",
            "isPro": false,
            "fullname": "Dandan Zheng",
            "user": "zhengdd0422",
            "type": "user"
          },
          "name": "Dandan Zheng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:10:11.829Z",
          "hidden": false
        },
        {
          "_id": "681973cfa70a4728958323ad",
          "name": "Hu Yu",
          "hidden": false
        },
        {
          "_id": "681973cfa70a4728958323ae",
          "user": {
            "_id": "64575ac8cd935d48a47774ec",
            "avatarUrl": "/avatars/5d211e2c13d6c4e011e5e58b738413f7.svg",
            "isPro": false,
            "fullname": "chenjingdong ",
            "user": "chenjingdong",
            "type": "user"
          },
          "name": "Jingdong Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:10:51.440Z",
          "hidden": false
        },
        {
          "_id": "681973cfa70a4728958323af",
          "user": {
            "_id": "6417cd278f689506e71439ac",
            "avatarUrl": "/avatars/0993d834c6c3bbc53081aa139ee14a12.svg",
            "isPro": false,
            "fullname": "jianxinsun",
            "user": "jianxinsun",
            "type": "user"
          },
          "name": "Jianxin Sun",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:10:02.728Z",
          "hidden": false
        },
        {
          "_id": "681973cfa70a4728958323b0",
          "name": "Junbo Zhao",
          "hidden": false
        },
        {
          "_id": "681973cfa70a4728958323b1",
          "name": "Jun Zhou",
          "hidden": false
        },
        {
          "_id": "681973cfa70a4728958323b2",
          "name": "Kaixiang Ji",
          "hidden": false
        },
        {
          "_id": "681973cfa70a4728958323b3",
          "name": "Lixiang Ru",
          "hidden": false
        },
        {
          "_id": "681973cfa70a4728958323b4",
          "name": "Libin Wang",
          "hidden": false
        },
        {
          "_id": "681973cfa70a4728958323b5",
          "name": "Qingpei Guo",
          "hidden": false
        },
        {
          "_id": "681973cfa70a4728958323b6",
          "name": "Rui Liu",
          "hidden": false
        },
        {
          "_id": "681973cfa70a4728958323b7",
          "name": "Weilong Chai",
          "hidden": false
        },
        {
          "_id": "681973cfa70a4728958323b8",
          "user": {
            "_id": "67cc852d2cfa481bce2dd07e",
            "avatarUrl": "/avatars/0c1c32ec066a8de9148b083b39d1fab8.svg",
            "isPro": false,
            "fullname": "xinyu xiao",
            "user": "bear-xxy",
            "type": "user"
          },
          "name": "Xinyu Xiao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:11:49.316Z",
          "hidden": false
        },
        {
          "_id": "681973cfa70a4728958323b9",
          "name": "Ziyuan Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-05T08:56:12.000Z",
      "submittedOnDailyAt": "2025-05-06T01:00:49.692Z",
      "title": "明々ユニ：自然多様化インタラクションの統合アーキテクチャの進歩",
      "submittedOnDailyBy": {
        "_id": "644fcbea4f7316588267dc80",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644fcbea4f7316588267dc80/w8-2Gkaw9BN9VzppNXrTP.jpeg",
        "isPro": false,
        "fullname": "Biao Gong",
        "user": "BiaoGong",
        "type": "user"
      },
      "summary": "明-Lite-Uniは、新設計の統一ビジュアルジェネレータと、視覚と言語を統一するために設計された原生多モデルを掲げるオープンソースマルチモデルフレームワークです。特に、このプロジェクトは、MetaQueriesとM2-omniフレームワークの統合を実装し、新しい多スケール学習可能トークンと多スケール表現アライメントスティラテジを導入しています。固定モデルと学習可能なディフュージョンモデルを利用することで、明-Lite-Uniは、テキストから画像生成およびインストラクションベースの画像編集タスクを行うことができ、ビジュアル理解だけでなく機能を拡張します。実験結果は、明-Lite-Uniの強力な性能を示し、インタラクティブプロセスの流れの評価を提供しています。すべてのコードとモデル重みはオープンソースとして提供されており、コミュニティ内での進歩を促進します。特に、本プロジェクトは、2025年3月25日に画像生成を更新したChatGPT-4oとの同期の多モデルAIのマークストーンとして、明-Lite-Uniのような統一モデルの広範な意義を強調しています。明-Lite-Uniはアルファステージにあり、これから進化させる予定です。",
      "upvotes": 6,
      "discussionId": "681973d2a70a47289583249d",
      "projectPage": "https://github.com/inclusionAI/Ming/tree/main/Ming-unify",
      "githubRepo": "https://github.com/inclusionAI/Ming/tree/main/Ming-unify",
      "ai_keywords": [
        "unified visual generator",
        "multimodal autoregressive model",
        "MetaQueries",
        "M2-omni framework",
        "multi-scale learnable tokens",
        "multi-scale representation alignment strategy",
        "MLLM",
        "learnable diffusion model",
        "text-to-image generation",
        "instruction based image editing"
      ]
    },
    "publishedAt": "2025-05-05T04:56:12.000Z",
    "title": "Ming-Lite-Uni: Advancements in Unified Architecture for Natural\n  Multimodal Interaction",
    "summary": "We introduce Ming-Lite-Uni, an open-source multimodal framework featuring a\nnewly designed unified visual generator and a native multimodal autoregressive\nmodel tailored for unifying vision and language. Specifically, this project\nprovides an open-source implementation of the integrated MetaQueries and\nM2-omni framework, while introducing the novel multi-scale learnable tokens and\nmulti-scale representation alignment strategy. By leveraging a fixed MLLM and a\nlearnable diffusion model, Ming-Lite-Uni enables native multimodal AR models to\nperform both text-to-image generation and instruction based image editing\ntasks, expanding their capabilities beyond pure visual understanding. Our\nexperimental results demonstrate the strong performance of Ming-Lite-Uni and\nillustrate the impressive fluid nature of its interactive process. All code and\nmodel weights are open-sourced to foster further exploration within the\ncommunity. Notably, this work aligns with concurrent multimodal AI milestones -\nsuch as ChatGPT-4o with native image generation updated in March 25, 2025 -\nunderscoring the broader significance of unified models like Ming-Lite-Uni on\nthe path toward AGI. Ming-Lite-Uni is in alpha stage and will soon be further\nrefined.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02471.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "644fcbea4f7316588267dc80",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644fcbea4f7316588267dc80/w8-2Gkaw9BN9VzppNXrTP.jpeg",
      "fullname": "Biao Gong",
      "name": "BiaoGong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.01583",
      "authors": [
        {
          "_id": "6819814653612b577df718e7",
          "user": {
            "_id": "65cd4d6256671dee8ee46392",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65cd4d6256671dee8ee46392/SH30XVQnGiYqYQIeDk3na.jpeg",
            "isPro": false,
            "fullname": "Jen-Hao (Andy) Cheng",
            "user": "andaba",
            "type": "user"
          },
          "name": "Jen-Hao Cheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-06T08:33:15.728Z",
          "hidden": false
        },
        {
          "_id": "6819814653612b577df718e8",
          "name": "Vivian Wang",
          "hidden": false
        },
        {
          "_id": "6819814653612b577df718e9",
          "name": "Huayu Wang",
          "hidden": false
        },
        {
          "_id": "6819814653612b577df718ea",
          "name": "Huapeng Zhou",
          "hidden": false
        },
        {
          "_id": "6819814653612b577df718eb",
          "name": "Yi-Hao Peng",
          "hidden": false
        },
        {
          "_id": "6819814653612b577df718ec",
          "name": "Hou-I Liu",
          "hidden": false
        },
        {
          "_id": "6819814653612b577df718ed",
          "user": {
            "_id": "647e4e8da49bffab5d72fbe0",
            "avatarUrl": "/avatars/c5fb00019c7cea23fe3351ecb1e43195.svg",
            "isPro": false,
            "fullname": "Hsiang-Wei Huang",
            "user": "hsiangwei0903",
            "type": "user"
          },
          "name": "Hsiang-Wei Huang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:13:19.727Z",
          "hidden": false
        },
        {
          "_id": "6819814653612b577df718ee",
          "name": "Kuang-Ming Chen",
          "hidden": false
        },
        {
          "_id": "6819814653612b577df718ef",
          "name": "Cheng-Yen Yang",
          "hidden": false
        },
        {
          "_id": "6819814653612b577df718f0",
          "user": {
            "_id": "637c7503fe115289cfecbe6b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676361945047-637c7503fe115289cfecbe6b.jpeg",
            "isPro": false,
            "fullname": "Wenhao Chai",
            "user": "wchai",
            "type": "user"
          },
          "name": "Wenhao Chai",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:13:37.240Z",
          "hidden": false
        },
        {
          "_id": "6819814653612b577df718f1",
          "user": {
            "_id": "65f8cb651e0c65c13a2b906a",
            "avatarUrl": "/avatars/ffc8ac8f29ab1a3142fe5fab1b2302ca.svg",
            "isPro": false,
            "fullname": "Yi-Ling Chen",
            "user": "yilche",
            "type": "user"
          },
          "name": "Yi-Ling Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:13:43.313Z",
          "hidden": false
        },
        {
          "_id": "6819814653612b577df718f2",
          "user": {
            "_id": "63c8527becdb7c9fdd9cacc6",
            "avatarUrl": "/avatars/c8a3f5e1e5159ae5ead41bd9fc2b9b34.svg",
            "isPro": false,
            "fullname": "Vibhav Vineet",
            "user": "vibhav-vineet",
            "type": "user"
          },
          "name": "Vibhav Vineet",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:13:49.482Z",
          "hidden": false
        },
        {
          "_id": "6819814653612b577df718f3",
          "name": "Qin Cai",
          "hidden": false
        },
        {
          "_id": "6819814653612b577df718f4",
          "name": "Jenq-Neng Hwang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-02T21:00:17.000Z",
      "submittedOnDailyAt": "2025-05-06T01:56:09.960Z",
      "title": "TEMPURA: 時系列イベントマスク付き予測と理解による行動の理由論",
      "submittedOnDailyBy": {
        "_id": "637c7503fe115289cfecbe6b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676361945047-637c7503fe115289cfecbe6b.jpeg",
        "isPro": false,
        "fullname": "Wenhao Chai",
        "user": "wchai",
        "type": "user"
      },
      "summary": "理解因果事件关系和在视频中实现细粒度时间定位对于视觉语言模型来说仍然具有挑战性。现有的方法要么压缩视频标记以降低时间分辨率，要么将视频视为未分割的流，这模糊了细粒度事件边界并限制了因果依赖关系的建模。我们提出了TEMPURA（用于行动推理的时序事件掩码预测与理解），这是一个增强视频时序理解的两阶段训练框架。TEMPURA首先应用掩码事件预测推理来重建缺失的事件，并从密集事件注释生成逐步的因果解释，借鉴了有效的填充技术。TEMPURA然后学习执行视频分割和密集字幕，将视频分解为具有详细描述和时间戳对齐的非重叠事件。我们在VER上训练TEMPURA，这是一个由我们精心策划的大规模数据集，包含1M个训练实例和500K个带有时间对齐事件描述和结构化推理步骤的视频。在时序定位和高亮检测基准测试上的实验表明，TEMPURA优于强大的基线模型，证实了将因果推理与细粒度时间分割相结合可以提高视频理解。",
      "upvotes": 5,
      "discussionId": "6819814853612b577df71943",
      "ai_keywords": [
        "TEMPURA",
        "masked event prediction",
        "causal explanations",
        "dense event annotations",
        "infilling techniques",
        "video segmentation",
        "dense captioning",
        "non-overlapping events",
        "timestamp-aligned descriptions",
        "VER",
        "temporal grounding",
        "highlight detection",
        "baseline models",
        "causal reasoning",
        "fine-grained temporal segmentation"
      ]
    },
    "publishedAt": "2025-05-02T17:00:17.000Z",
    "title": "TEMPURA: Temporal Event Masked Prediction and Understanding for\n  Reasoning in Action",
    "summary": "Understanding causal event relationships and achieving fine-grained temporal\ngrounding in videos remain challenging for vision-language models. Existing\nmethods either compress video tokens to reduce temporal resolution, or treat\nvideos as unsegmented streams, which obscures fine-grained event boundaries and\nlimits the modeling of causal dependencies. We propose TEMPURA (Temporal Event\nMasked Prediction and Understanding for Reasoning in Action), a two-stage\ntraining framework that enhances video temporal understanding. TEMPURA first\napplies masked event prediction reasoning to reconstruct missing events and\ngenerate step-by-step causal explanations from dense event annotations, drawing\ninspiration from effective infilling techniques. TEMPURA then learns to perform\nvideo segmentation and dense captioning to decompose videos into\nnon-overlapping events with detailed, timestamp-aligned descriptions. We train\nTEMPURA on VER, a large-scale dataset curated by us that comprises 1M training\ninstances and 500K videos with temporally aligned event descriptions and\nstructured reasoning steps. Experiments on temporal grounding and highlight\ndetection benchmarks demonstrate that TEMPURA outperforms strong baseline\nmodels, confirming that integrating causal reasoning with fine-grained temporal\nsegmentation leads to improved video understanding.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.01583.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "637c7503fe115289cfecbe6b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676361945047-637c7503fe115289cfecbe6b.jpeg",
      "fullname": "Wenhao Chai",
      "name": "wchai",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 30
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.02823",
      "authors": [
        {
          "_id": "6819893117007d963b997a0b",
          "user": {
            "_id": "66b2e5f5523bf90aa7057467",
            "avatarUrl": "/avatars/ccdb58c2e56cf861e9dcec50c85d7778.svg",
            "isPro": false,
            "fullname": "Guo",
            "user": "Zinan123212",
            "type": "user"
          },
          "name": "Zinan Guo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:14:34.748Z",
          "hidden": false
        },
        {
          "_id": "6819893117007d963b997a0c",
          "name": "Pengze Zhang",
          "hidden": false
        },
        {
          "_id": "6819893117007d963b997a0d",
          "user": {
            "_id": "639709c2be8a14bb9eeea8f6",
            "avatarUrl": "/avatars/c142d71b541dccff91fcfd08a2cc0ce0.svg",
            "isPro": false,
            "fullname": "Yanze Wu",
            "user": "yanze",
            "type": "user"
          },
          "name": "Yanze Wu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:15:02.034Z",
          "hidden": false
        },
        {
          "_id": "6819893117007d963b997a0e",
          "name": "Chong Mou",
          "hidden": false
        },
        {
          "_id": "6819893117007d963b997a0f",
          "name": "Songtao Zhao",
          "hidden": false
        },
        {
          "_id": "6819893117007d963b997a10",
          "user": {
            "_id": "645dcad7a19f3e64bbf35e6c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/rV1uHDSnZv7jAvFq4ftj4.jpeg",
            "isPro": false,
            "fullname": "Qian He",
            "user": "heqian",
            "type": "user"
          },
          "name": "Qian He",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:15:28.996Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-05T17:50:24.000Z",
      "submittedOnDailyAt": "2025-05-06T02:30:32.888Z",
      "title": "MUSAR: 単一主題データセットからの多主題カスタマイズ検討を通じてアテンションルーティングを用いる",
      "submittedOnDailyBy": {
        "_id": "639709c2be8a14bb9eeea8f6",
        "avatarUrl": "/avatars/c142d71b541dccff91fcfd08a2cc0ce0.svg",
        "isPro": false,
        "fullname": "Yanze Wu",
        "user": "yanze",
        "type": "user"
      },
      "summary": "現在の多論理カスタマイズアプローチには2つの重要な課題がある：多様な多論理トレーニングデータの取得の難しさと、異なる論理間の属性の結合。これらの間違いを経由するために、私たちはMUSAR（多論理カスタマイズアプローチ）を提案します。これは単一論理のトレーニングデータでも強力な多論理カスタマイズを実現するための簡単で効果的なフレームワークです。まず、データの制限を解決するために、私たちはデバイスバイアスを補正したディプチック学習を導入します。これは単一論理の画像からディプチックトレーニングペアを構築し、ディプチック構築による分布バイアスを静的アテンションルーティングと双分野LoRAで主動的に補正します。次に、異なる論理間の結合を排除するために、私たちは動的アテンションルーティング機構を導入します。これは生成された画像と条件付き論理間の1対1の対応を適応的に確立します。この設計は、多論理表現のデコープリングを実現し、参照論理間の増加に伴い可換的な一般化性能を維持します。詳細な実験は、MUSARが単一論理データでも既存の方法より画質、論理一致性、コミュニケーションの自然性に優れていることを示します。",
      "upvotes": 2,
      "discussionId": "6819893317007d963b997ab1",
      "githubRepo": "https://github.com/guozinan126/MUSAR",
      "ai_keywords": [
        "debiased diptych learning",
        "diptych training pairs",
        "static attention routing",
        "dual-branch LoRA",
        "dynamic attention routing mechanism",
        "bijective mappings",
        "multi-subject representations",
        "scalable generalization performance"
      ]
    },
    "publishedAt": "2025-05-05T13:50:24.000Z",
    "title": "MUSAR: Exploring Multi-Subject Customization from Single-Subject Dataset\n  via Attention Routing",
    "summary": "Current multi-subject customization approaches encounter two critical\nchallenges: the difficulty in acquiring diverse multi-subject training data,\nand attribute entanglement across different subjects. To bridge these gaps, we\npropose MUSAR - a simple yet effective framework to achieve robust\nmulti-subject customization while requiring only single-subject training data.\nFirstly, to break the data limitation, we introduce debiased diptych learning.\nIt constructs diptych training pairs from single-subject images to facilitate\nmulti-subject learning, while actively correcting the distribution bias\nintroduced by diptych construction via static attention routing and dual-branch\nLoRA. Secondly, to eliminate cross-subject entanglement, we introduce dynamic\nattention routing mechanism, which adaptively establishes bijective mappings\nbetween generated images and conditional subjects. This design not only\nachieves decoupling of multi-subject representations but also maintains\nscalable generalization performance with increasing reference subjects.\nComprehensive experiments demonstrate that our MUSAR outperforms existing\nmethods - even those trained on multi-subject dataset - in image quality,\nsubject consistency, and interaction naturalness, despite requiring only\nsingle-subject dataset.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02823.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "639709c2be8a14bb9eeea8f6",
      "avatarUrl": "/avatars/c142d71b541dccff91fcfd08a2cc0ce0.svg",
      "fullname": "Yanze Wu",
      "name": "yanze",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 140
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.02625",
      "authors": [
        {
          "_id": "681975abba26bf20601bb7ca",
          "user": {
            "_id": "65b7573482d384513443875e",
            "avatarUrl": "/avatars/0f2175e4adf507f5ccb0636c1cb647de.svg",
            "isPro": false,
            "fullname": "Qingkai Fang",
            "user": "poeroz",
            "type": "user"
          },
          "name": "Qingkai Fang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:15:43.683Z",
          "hidden": false
        },
        {
          "_id": "681975abba26bf20601bb7cb",
          "name": "Yan Zhou",
          "hidden": false
        },
        {
          "_id": "681975abba26bf20601bb7cc",
          "user": {
            "_id": "66680c0505c407bfea87667c",
            "avatarUrl": "/avatars/e3c26d2eb13fe8ad2b3fd16897e61e6d.svg",
            "isPro": false,
            "fullname": "Shoutao Guo",
            "user": "guoshoutao",
            "type": "user"
          },
          "name": "Shoutao Guo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:15:53.154Z",
          "hidden": false
        },
        {
          "_id": "681975abba26bf20601bb7cd",
          "user": {
            "_id": "64803e5dc57f629056c601f1",
            "avatarUrl": "/avatars/a9e9c97c70714e3a29bef2cf929ee6b3.svg",
            "isPro": false,
            "fullname": "Shaolei Zhang",
            "user": "zhangshaolei",
            "type": "user"
          },
          "name": "Shaolei Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:16:00.139Z",
          "hidden": false
        },
        {
          "_id": "681975abba26bf20601bb7ce",
          "name": "Yang Feng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-05T12:53:09.000Z",
      "submittedOnDailyAt": "2025-05-06T01:07:20.259Z",
      "title": "LLaMA-Omni2: 時系列的語言モデルに基づく実時間の話し言葉チャットボットと自動後退語合成",
      "submittedOnDailyBy": {
        "_id": "65b7573482d384513443875e",
        "avatarUrl": "/avatars/0f2175e4adf507f5ccb0636c1cb647de.svg",
        "isPro": false,
        "fullname": "Qingkai Fang",
        "user": "poeroz",
        "type": "user"
      },
      "summary": "実時間的、知能的な、自然なスピーチインタラクションは次世代の人間コンピュータインタラクションの重要な部分です。最近の進歩は、大規模言語モデル（LLMs）に基づいた智能な語彙チャットボットの構築の可能性を示しました。本論文では、LLaMA-Omni 2、パラメータ数が0.5Bから14Bまでのスピーチ言語モデル（SpeechLMs）のシリーズを紹介します。これらは、高品質の実時間スピーチインタラクションを実現する能力を持ちます。LLaMA-Omni 2はQwen2.5シリーズモデルを基礎に、スピーチエンコーダと自動復元ストリーミングスピーチデコーダを統合して構築されています。200Kの多ターンスピーチダイアログサンプルでのみ訓練されていることにもかかわらず、LLaMA-Omni 2は複数の語彙問答テストとスピーチ指示従いテストで強力な性能を示し、GLM-4-Voiceという、数百万時間のスピーチデータで訓練された先進的なSpeechLMsを超えています。",
      "upvotes": 2,
      "discussionId": "681975abba26bf20601bb7f2",
      "ai_keywords": [
        "speech language models (SpeechLMs)",
        "Qwen2.5",
        "speech encoder",
        "autoregressive streaming speech decoder",
        "spoken question answering",
        "speech instruction following",
        "GLM-4-Voice"
      ]
    },
    "publishedAt": "2025-05-05T08:53:09.000Z",
    "title": "LLaMA-Omni2: LLM-based Real-time Spoken Chatbot with Autoregressive\n  Streaming Speech Synthesis",
    "summary": "Real-time, intelligent, and natural speech interaction is an essential part\nof the next-generation human-computer interaction. Recent advancements have\nshowcased the potential of building intelligent spoken chatbots based on large\nlanguage models (LLMs). In this paper, we introduce LLaMA-Omni 2, a series of\nspeech language models (SpeechLMs) ranging from 0.5B to 14B parameters, capable\nof achieving high-quality real-time speech interaction. LLaMA-Omni 2 is built\nupon the Qwen2.5 series models, integrating a speech encoder and an\nautoregressive streaming speech decoder. Despite being trained on only 200K\nmulti-turn speech dialogue samples, LLaMA-Omni 2 demonstrates strong\nperformance on several spoken question answering and speech instruction\nfollowing benchmarks, surpassing previous state-of-the-art SpeechLMs like\nGLM-4-Voice, which was trained on millions of hours of speech data.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02625.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65b7573482d384513443875e",
      "avatarUrl": "/avatars/0f2175e4adf507f5ccb0636c1cb647de.svg",
      "fullname": "Qingkai Fang",
      "name": "poeroz",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.01456",
      "authors": [
        {
          "_id": "6819c7577c36c576e9cb6bfa",
          "user": {
            "_id": "64f64da90efa33bfe0a3d9ba",
            "avatarUrl": "/avatars/c45fb015433e46a2eeb9518910f75d35.svg",
            "isPro": false,
            "fullname": "Vaidehi Patil",
            "user": "vaidehi99",
            "type": "user"
          },
          "name": "Vaidehi Patil",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:16:35.372Z",
          "hidden": false
        },
        {
          "_id": "6819c7577c36c576e9cb6bfb",
          "user": {
            "_id": "654ffe334d9e71e17becc660",
            "avatarUrl": "/avatars/022b7a77051d26c4e5cbf254b7352eb9.svg",
            "isPro": false,
            "fullname": "Yi-Lin Sung",
            "user": "a2889184",
            "type": "user"
          },
          "name": "Yi-Lin Sung",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:16:41.799Z",
          "hidden": false
        },
        {
          "_id": "6819c7577c36c576e9cb6bfc",
          "name": "Peter Hase",
          "hidden": false
        },
        {
          "_id": "6819c7577c36c576e9cb6bfd",
          "name": "Jie Peng",
          "hidden": false
        },
        {
          "_id": "6819c7577c36c576e9cb6bfe",
          "name": "Tianlong Chen",
          "hidden": false
        },
        {
          "_id": "6819c7577c36c576e9cb6bff",
          "user": {
            "_id": "665d9d3a057f7c508f98c625",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/665d9d3a057f7c508f98c625/u1R9P9sJoAl4zEIcetbPy.jpeg",
            "isPro": false,
            "fullname": "Mohit Bansal",
            "user": "mohitbansal",
            "type": "user"
          },
          "name": "Mohit Bansal",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:17:14.823Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-01T01:54:00.000Z",
      "submittedOnDailyAt": "2025-05-06T06:55:27.299Z",
      "title": "多タイプLLMにおける敏感情報の学習済みデータの削除：ベンチマークと攻撃防御評価",
      "submittedOnDailyBy": {
        "_id": "64f64da90efa33bfe0a3d9ba",
        "avatarUrl": "/avatars/c45fb015433e46a2eeb9518910f75d35.svg",
        "isPro": false,
        "fullname": "Vaidehi Patil",
        "user": "vaidehi99",
        "type": "user"
      },
      "summary": "LLMs は巨大なデータセットを学習していると、無意識にセンシティブな情報（個人情報や潜在的に有害な内容）を取得するリスクがある。このリスクは、多様化モデル（多様化LLMs）では、画像と文字の情報を統合しているためにさらに高まる。アドバーサリーは、この知識を多様化プロンプトを通じて、センシティブな詳細を抽出することができる。MLLMs から特定の情報を削除することの効果を評価するためには、高品質でよりよく注釈された画像-文字ペアの作成が必要である。先行研究は主に文字に焦点を当てていたが、多様化ユニフォームは調査が不足している。この空間を補うために、私たちは最初に、多様化ユニフォームの忘却ベンチマーク（UnLOK-VQA）、攻撃と防御のフレームワークを紹介し、MLLMs から特定の多様化知識を削除する方法を評価するためのものを作成する。私たちは、ビジュアルクエストアンサーショップデータセットを自動化プロセスで拡張し、一般化と特異性の検証に使用するための、プロクセスに近いサンプルを生成し、手動でフィルタリングして高品質を維持する。その後、6つの防御オブジェクティブを7つの攻撃（4つのホワイトボックス、3つのブラックボックス）に対して評価し、新しいホワイトボックスメソッドを利用した解釈性を活用した方法を含む。結果は、多様化攻撃はテキストや画像だけの攻撃よりも優れてい、最も効果的な防御は内部モデル状態からの回答情報を削除するものであることがわかる。また、大きなモデルは後編集ロバスト性が高く、スケールが安全性を高めることを示している。UnLOK-VQAは、MLLMs の忘却の進歩を支える厳密なベンチマークとなる。",
      "upvotes": 0,
      "discussionId": "6819c7597c36c576e9cb6c6b",
      "githubRepo": "https://github.com/Vaidehi99/UnLOK-VQA",
      "ai_keywords": [
        "multimodal LLMs",
        "multimodal prompts",
        "targeted unlearning",
        "high-quality, well-annotated image-text pairs",
        "multimodal unlearning",
        "UnLOK-VQA (Unlearning Outside Knowledge VQA)",
        "visual question-answering dataset",
        "varying-proximity samples",
        "whitebox attacks",
        "blackbox attacks",
        "interpretability of hidden states",
        "multimodal attacks",
        "post-editing robustness"
      ]
    },
    "publishedAt": "2025-04-30T21:54:00.000Z",
    "title": "Unlearning Sensitive Information in Multimodal LLMs: Benchmark and\n  Attack-Defense Evaluation",
    "summary": "LLMs trained on massive datasets may inadvertently acquire sensitive\ninformation such as personal details and potentially harmful content. This risk\nis further heightened in multimodal LLMs as they integrate information from\nmultiple modalities (image and text). Adversaries can exploit this knowledge\nthrough multimodal prompts to extract sensitive details. Evaluating how\neffectively MLLMs can forget such information (targeted unlearning)\nnecessitates the creation of high-quality, well-annotated image-text pairs.\nWhile prior work on unlearning has focused on text, multimodal unlearning\nremains underexplored. To address this gap, we first introduce a multimodal\nunlearning benchmark, UnLOK-VQA (Unlearning Outside Knowledge VQA), as well as\nan attack-and-defense framework to evaluate methods for deleting specific\nmultimodal knowledge from MLLMs. We extend a visual question-answering dataset\nusing an automated pipeline that generates varying-proximity samples for\ntesting generalization and specificity, followed by manual filtering for\nmaintaining high quality. We then evaluate six defense objectives against seven\nattacks (four whitebox, three blackbox), including a novel whitebox method\nleveraging interpretability of hidden states. Our results show multimodal\nattacks outperform text- or image-only ones, and that the most effective\ndefense removes answer information from internal model states. Additionally,\nlarger models exhibit greater post-editing robustness, suggesting that scale\nenhances safety. UnLOK-VQA provides a rigorous benchmark for advancing\nunlearning in MLLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.01456.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64f64da90efa33bfe0a3d9ba",
      "avatarUrl": "/avatars/c45fb015433e46a2eeb9518910f75d35.svg",
      "fullname": "Vaidehi Patil",
      "name": "vaidehi99",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]