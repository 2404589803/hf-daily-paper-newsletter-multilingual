[
  {
    "paper": {
      "id": "2504.01990",
      "authors": [
        {
          "_id": "67ef8723d325fe100f36107e",
          "user": {
            "_id": "654a97282d2fcd6bf2851173",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/654a97282d2fcd6bf2851173/9zXf940gr4WNt4e-oOt4k.png",
            "isPro": false,
            "fullname": "Bang Liu",
            "user": "Bang-UdeM-Mila",
            "type": "user"
          },
          "name": "Bang Liu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-04T07:15:51.456Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f36107f",
          "user": {
            "_id": "65af5f8f3db2280ece7fac79",
            "avatarUrl": "/avatars/66d88b2d744c8d00e11d39a55ab86c2e.svg",
            "isPro": false,
            "fullname": "Xin-Feng Li",
            "user": "xinfeng1i",
            "type": "user"
          },
          "name": "Xinfeng Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:26:45.785Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361080",
          "user": {
            "_id": "64b78a39954ae43365984448",
            "avatarUrl": "/avatars/6de9d4bf320a69eca6b758e718ee116c.svg",
            "isPro": false,
            "fullname": "Zhang",
            "user": "Peiyan",
            "type": "user"
          },
          "name": "Jiayi Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:28:49.444Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361081",
          "user": {
            "_id": "64324bb3034ecbefddd99863",
            "avatarUrl": "/avatars/3b8cdc2066251999a3a7e6d5565dceb5.svg",
            "isPro": false,
            "fullname": "Jinlin Wang",
            "user": "JinlinW",
            "type": "user"
          },
          "name": "Jinlin Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:27:02.727Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361082",
          "name": "Tanjin He",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361083",
          "name": "Sirui Hong",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361084",
          "name": "Hongzhang Liu",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361085",
          "name": "Shaokun Zhang",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361086",
          "user": {
            "_id": "5fc0b2b61160c47d1d438568",
            "avatarUrl": "/avatars/90beea6b452c662d579197dbf592423a.svg",
            "isPro": false,
            "fullname": "Kaitao Song",
            "user": "KaitaoSong",
            "type": "user"
          },
          "name": "Kaitao Song",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:27:47.151Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361087",
          "user": {
            "_id": "64c090a9f613170e7be93d2f",
            "avatarUrl": "/avatars/ccbdf444e1f2386d2281e8e42059ebb0.svg",
            "isPro": false,
            "fullname": "KunlunZhu",
            "user": "KunlunZhu",
            "type": "user"
          },
          "name": "Kunlun Zhu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:28:03.582Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361088",
          "user": {
            "_id": "6664783b0ab8b63cbb4a3156",
            "avatarUrl": "/avatars/71859e6f76c157191bd2e968061f08b0.svg",
            "isPro": false,
            "fullname": "cyh",
            "user": "chengyuheng",
            "type": "user"
          },
          "name": "Yuheng Cheng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:28:14.543Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361089",
          "user": {
            "_id": "62bb1e0f3ff437e49a3088e5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62bb1e0f3ff437e49a3088e5/bcUQmH8tKfI6DIWH9IcYp.jpeg",
            "isPro": true,
            "fullname": "Suyuchen Wang",
            "user": "sheryc",
            "type": "user"
          },
          "name": "Suyuchen Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:28:21.351Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f36108a",
          "user": {
            "_id": "655c092183186f133f959108",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/WMVgGhjQbVJXK9eh4EuT9.jpeg",
            "isPro": false,
            "fullname": "Xiaoqiang Wang",
            "user": "qindomitable",
            "type": "user"
          },
          "name": "Xiaoqiang Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:28:26.707Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f36108b",
          "name": "Yuyu Luo",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f36108c",
          "user": {
            "_id": "648ee5fe9ae7cc4fcffa9aef",
            "avatarUrl": "/avatars/9bcc5eb91452c1360b9a0a4f9def8af8.svg",
            "isPro": false,
            "fullname": "Haibo Jin",
            "user": "Nick233",
            "type": "user"
          },
          "name": "Haibo Jin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:28:40.177Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f36108d",
          "user": {
            "_id": "64b78a39954ae43365984448",
            "avatarUrl": "/avatars/6de9d4bf320a69eca6b758e718ee116c.svg",
            "isPro": false,
            "fullname": "Zhang",
            "user": "Peiyan",
            "type": "user"
          },
          "name": "Peiyan Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:29:02.679Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f36108e",
          "user": {
            "_id": "66197a8afeb55cbe39e50ae8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/pcSS1TsCoHxRcAEkcMNm0.png",
            "isPro": false,
            "fullname": "Ollie Liu",
            "user": "oliu-io",
            "type": "user"
          },
          "name": "Ollie Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:29:10.114Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f36108f",
          "name": "Jiaqi Chen",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361090",
          "user": {
            "_id": "6719d581a6cad13741b8bc7f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6719d581a6cad13741b8bc7f/w4EttqfXRgWZJc6HpYOS9.jpeg",
            "isPro": false,
            "fullname": "Huan Zhang",
            "user": "huanzhang12",
            "type": "user"
          },
          "name": "Huan Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:29:24.614Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361091",
          "user": {
            "_id": "640dc84b474aa6f89554d518",
            "avatarUrl": "/avatars/64f47f76d97c5e91b7ab8380bcada61c.svg",
            "isPro": false,
            "fullname": "Zhaoyang Yu",
            "user": "MoshiQAQ",
            "type": "user"
          },
          "name": "Zhaoyang Yu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:29:38.969Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361092",
          "name": "Haochen Shi",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361093",
          "name": "Boyan Li",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361094",
          "name": "Dekun Wu",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361095",
          "user": {
            "_id": "6402e8fb06c715b93407442d",
            "avatarUrl": "/avatars/12b67f0632be5a53b56d8a68586a7f98.svg",
            "isPro": false,
            "fullname": "Fengwei Teng",
            "user": "leavendough",
            "type": "user"
          },
          "name": "Fengwei Teng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:30:09.492Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361096",
          "name": "Xiaojun Jia",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361097",
          "name": "Jiawei Xu",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361098",
          "name": "Jinyu Xiang",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361099",
          "name": "Yizhang Lin",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f36109a",
          "name": "Tianming Liu",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f36109b",
          "name": "Tongliang Liu",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f36109c",
          "name": "Yu Su",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f36109d",
          "name": "Huan Sun",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f36109e",
          "user": {
            "_id": "66a8fa5fd909c30167f1f5cd",
            "avatarUrl": "/avatars/c9b26d5b2dd78bed9661df429012fd97.svg",
            "isPro": false,
            "fullname": "Glen Berseth",
            "user": "gberseth",
            "type": "user"
          },
          "name": "Glen Berseth",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:30:19.433Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f36109f",
          "name": "Jianyun Nie",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f3610a0",
          "name": "Ian Foster",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f3610a1",
          "name": "Logan Ward",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f3610a2",
          "name": "Qingyun Wu",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f3610a3",
          "user": {
            "_id": "5e7e595230dc073f817a2bb5",
            "avatarUrl": "/avatars/d5ff36e45555d9e169cf56c845736444.svg",
            "isPro": false,
            "fullname": "Yu Gu",
            "user": "entslscheia",
            "type": "user"
          },
          "name": "Yu Gu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:50:40.603Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f3610a4",
          "user": {
            "_id": "64403daae44f30a72323e4ca",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64403daae44f30a72323e4ca/skJ9h0pdNfmE4VbQL8xDR.png",
            "isPro": false,
            "fullname": "mingchen zhuge",
            "user": "tjpxiaoming",
            "type": "user"
          },
          "name": "Mingchen Zhuge",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:26:28.011Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f3610a5",
          "name": "Xiangru Tang",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f3610a6",
          "name": "Haohan Wang",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f3610a7",
          "name": "Jiaxuan You",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f3610a8",
          "name": "Chi Wang",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f3610a9",
          "user": {
            "_id": "670918b02806bda07e44780c",
            "avatarUrl": "/avatars/c08ba5048d9e911ef488862e8869792f.svg",
            "isPro": false,
            "fullname": "Jian Pei",
            "user": "StrawHat2333",
            "type": "user"
          },
          "name": "Jian Pei",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:36:21.496Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f3610aa",
          "name": "Qiang Yang",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f3610ab",
          "user": {
            "_id": "6342829eb9454d65a2e7a4c4",
            "avatarUrl": "/avatars/4438abdf189dbe26a52948800d79a7c5.svg",
            "isPro": false,
            "fullname": "Xiaoliang Qi",
            "user": "phynics",
            "type": "user"
          },
          "name": "Xiaoliang Qi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:30:59.175Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f3610ac",
          "name": "Chenglin Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-31T18:00:29.000Z",
      "submittedOnDailyAt": "2025-04-04T05:46:58.338Z",
      "title": "基盤アグェントの進展と課題：ブレインインスピレーションから進化的、協力的、安全なシステムへ",
      "submittedOnDailyBy": {
        "_id": "64403daae44f30a72323e4ca",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64403daae44f30a72323e4ca/skJ9h0pdNfmE4VbQL8xDR.png",
        "isPro": false,
        "fullname": "mingchen zhuge",
        "user": "tjpxiaoming",
        "type": "user"
      },
      "summary": "LLMの到来は人工知能において変革的な変化を促進し、複雑な理由論、強固な認識、多様な領域での変換的な行動を可能にする高度な智能エージェントを開発することを可能にしました。これらのエージェントがAIの研究と実用的な応用についてより多くの影響を与えることを前提に、エージェントの設計、評価、連続的な改善は複雑で多面性のある挑戦になります。この調査では、認知科学、神経科学、計算研究の原理を統合したモジュール化された、人間の脳機能に類似したアーキテクチャに基づいて智能エージェントを構成し、その設計、評価、連続的な改善の詳細な概要を提供します。私たちの探索は4つの相互関係のある部分に分けられます。最初に、智能エージェントのモジュール化の基盤について詳しく調べ、コガッチャル、認識、操作のモジュールを人間の脳の機能に相当するものにマッピングし、メモリ、世界モデリング、報酬処理、エモーションシステムのような核心成分を解明します。次に、自動補完と適応的な進化機構について議論し、エージェントが自動的に能力を精査し、動的な環境に適応し、自動化最適化パラダイムを通じて継続的な学習を達成することを調べます。これには、新しいAutoMLとLLM駆動の最適化ステラテジーも含むものです。第三に、コラボレーションと進化の多エージェントシステムについて調べ、エージェントの相互作用、協力、ソシャル構造から生じる集団的な知能を調べ、人間の社会動態についての平行関係を明らかにします。最後に、安全、安全、ベネフィシャルなAIシステムの構築の重要な義務について議論し、内関係的および外関係的な安全脅害、倫理的な合意、強固性、信頼性のための実用的な軽減戦略を強調します。",
      "upvotes": 55,
      "discussionId": "67ef8727d325fe100f3611aa",
      "githubRepo": "https://github.com/FoundationAgents/awesome-foundation-agents"
    },
    "publishedAt": "2025-03-31T14:00:29.000Z",
    "title": "Advances and Challenges in Foundation Agents: From Brain-Inspired\n  Intelligence to Evolutionary, Collaborative, and Safe Systems",
    "summary": "The advent of large language models (LLMs) has catalyzed a transformative\nshift in artificial intelligence, paving the way for advanced intelligent\nagents capable of sophisticated reasoning, robust perception, and versatile\naction across diverse domains. As these agents increasingly drive AI research\nand practical applications, their design, evaluation, and continuous\nimprovement present intricate, multifaceted challenges. This survey provides a\ncomprehensive overview, framing intelligent agents within a modular,\nbrain-inspired architecture that integrates principles from cognitive science,\nneuroscience, and computational research. We structure our exploration into\nfour interconnected parts. First, we delve into the modular foundation of\nintelligent agents, systematically mapping their cognitive, perceptual, and\noperational modules onto analogous human brain functionalities, and elucidating\ncore components such as memory, world modeling, reward processing, and\nemotion-like systems. Second, we discuss self-enhancement and adaptive\nevolution mechanisms, exploring how agents autonomously refine their\ncapabilities, adapt to dynamic environments, and achieve continual learning\nthrough automated optimization paradigms, including emerging AutoML and\nLLM-driven optimization strategies. Third, we examine collaborative and\nevolutionary multi-agent systems, investigating the collective intelligence\nemerging from agent interactions, cooperation, and societal structures,\nhighlighting parallels to human social dynamics. Finally, we address the\ncritical imperative of building safe, secure, and beneficial AI systems,\nemphasizing intrinsic and extrinsic security threats, ethical alignment,\nrobustness, and practical mitigation strategies necessary for trustworthy\nreal-world deployment.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.01990.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64403daae44f30a72323e4ca",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64403daae44f30a72323e4ca/skJ9h0pdNfmE4VbQL8xDR.png",
      "fullname": "mingchen zhuge",
      "name": "tjpxiaoming",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.02826",
      "authors": [
        {
          "_id": "67ef4be0985aa66b67021ddc",
          "user": {
            "_id": "6530e62f536dbca918e71c3e",
            "avatarUrl": "/avatars/efc93bc767e561c6c6d429f65c23382d.svg",
            "isPro": false,
            "fullname": "Xiangyu Z",
            "user": "PhoenixZ",
            "type": "user"
          },
          "name": "Xiangyu Zhao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:09:07.389Z",
          "hidden": false
        },
        {
          "_id": "67ef4be0985aa66b67021ddd",
          "user": {
            "_id": "6710be3e6d1b33cf24417e38",
            "avatarUrl": "/avatars/f60bc9a67bb58f5997cbcc28cb93c079.svg",
            "isPro": false,
            "fullname": "zpy",
            "user": "zpy777",
            "type": "user"
          },
          "name": "Peiyuan Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:09:11.816Z",
          "hidden": false
        },
        {
          "_id": "67ef4be0985aa66b67021dde",
          "user": {
            "_id": "662516d72419feed62fb3a0a",
            "avatarUrl": "/avatars/24c4157829e70a4e346aa984885aa5ad.svg",
            "isPro": false,
            "fullname": "Dian",
            "user": "KexianTang",
            "type": "user"
          },
          "name": "Kexian Tang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:21:18.584Z",
          "hidden": false
        },
        {
          "_id": "67ef4be0985aa66b67021ddf",
          "name": "Hao Li",
          "hidden": false
        },
        {
          "_id": "67ef4be0985aa66b67021de0",
          "name": "Zicheng Zhang",
          "hidden": false
        },
        {
          "_id": "67ef4be0985aa66b67021de1",
          "user": {
            "_id": "65535b125413c1a54e6fb243",
            "avatarUrl": "/avatars/03bcf1d58865f5406aff49a415e78bdc.svg",
            "isPro": false,
            "fullname": "Guangtao Zhai",
            "user": "GTZhai",
            "type": "user"
          },
          "name": "Guangtao Zhai",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:22:23.057Z",
          "hidden": false
        },
        {
          "_id": "67ef4be0985aa66b67021de2",
          "user": {
            "_id": "667289f903c802764985d8c6",
            "avatarUrl": "/avatars/916befcbf0e52ce56be49617f31c7bb2.svg",
            "isPro": false,
            "fullname": "Junchi Yan",
            "user": "Rethinker",
            "type": "user"
          },
          "name": "Junchi Yan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:22:30.311Z",
          "hidden": false
        },
        {
          "_id": "67ef4be0985aa66b67021de3",
          "name": "Hua Yang",
          "hidden": false
        },
        {
          "_id": "67ef4be0985aa66b67021de4",
          "user": {
            "_id": "648e77184cae4f6921dbb382",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/648e77184cae4f6921dbb382/zAAJRvOStC0wZplqVWrk_.jpeg",
            "isPro": false,
            "fullname": "Xue Yang",
            "user": "yangxue",
            "type": "user"
          },
          "name": "Xue Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:09:09.267Z",
          "hidden": false
        },
        {
          "_id": "67ef4be0985aa66b67021de5",
          "user": {
            "_id": "63ee1379190ddd6214efd73a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676546883247-noauth.png",
            "isPro": false,
            "fullname": "HAODONG DUAN",
            "user": "KennyUTC",
            "type": "user"
          },
          "name": "Haodong Duan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:22:37.146Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63ee1379190ddd6214efd73a/PQFr3_6S3BeUSNb79jMSO.png",
        "https://cdn-uploads.huggingface.co/production/uploads/63ee1379190ddd6214efd73a/0rGBBJT_JcUPBk5cE_Te0.png"
      ],
      "publishedAt": "2025-04-03T17:59:56.000Z",
      "submittedOnDailyAt": "2025-04-04T01:35:35.280Z",
      "title": "ピクセルを超えての想像：理由に基づく可視的編集のベンチマーク",
      "submittedOnDailyBy": {
        "_id": "63ee1379190ddd6214efd73a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676546883247-noauth.png",
        "isPro": false,
        "fullname": "HAODONG DUAN",
        "user": "KennyUTC",
        "type": "user"
      },
      "summary": "大規模多モデール（LMMs）は、視覚理解と生成において進歩していますが、一般的な視覚編集において、複雑な指示を追うこと、外観の一貫性を維持すること、柔軟な入力フォーマットをサポートすることにおいて、まだ挑戦があります。この空間を閉じるために、私たちは、Reasoning-Informed viSual Editing（RISE）の評価用ベンチマークとしてRISEBenchを紹介します。RISEBenchは、時間的、因果的、空間的、ロジック的な理由の4つのキーの理由を重点的に挙げます。各カテゴリに対して高品質のテストケースをカレーライズし、人間の判断者とLMM-as-a-judgeアプローチを用いて、指示の理由、外観の一貫性、視覚的な可能性を評価するフレームワークを提案します。実験により、GPT-4o-Nativeは他の開放ソースや専有モデルを大幅に上回っていますが、この最先端のシステムはロジック的な理由のタスクにおいても難しいと認識し、この領域がまだ調査が不足していることが明らかになりました。初期の努力として、RISEBenchは、理由による視覚編集における基盤的な見解を提供し、将来の研究を促進することを目的としています。まだ初期段階ですが、私たちは、次世代の多モデールシステムの評価において、より厳密でスケーラブルな評価を支援するために、ベンチマークを継続的に拡張し、改良することに取り組みます。コードとデータは、https://github.com/PhoenixZ810/RISEBenchで公開します。",
      "upvotes": 40,
      "discussionId": "67ef4be4985aa66b67021ef2",
      "githubRepo": "https://github.com/PhoenixZ810/RISEBench",
      "ai_keywords": [
        "Large Multi-modality Models (LMMs)",
        "General Visual Editing",
        "Temporal Reasoning",
        "Causal Reasoning",
        "Spatial Reasoning",
        "Logical Reasoning",
        "RISEBench",
        "Instruction Reasoning",
        "Appearance Consistency",
        "Visual Plausibility",
        "GPT-4o-Native",
        "multimodal systems"
      ]
    },
    "publishedAt": "2025-04-03T13:59:56.000Z",
    "title": "Envisioning Beyond the Pixels: Benchmarking Reasoning-Informed Visual\n  Editing",
    "summary": "Large Multi-modality Models (LMMs) have made significant progress in visual\nunderstanding and generation, but they still face challenges in General Visual\nEditing, particularly in following complex instructions, preserving appearance\nconsistency, and supporting flexible input formats. To address this gap, we\nintroduce RISEBench, the first benchmark for evaluating Reasoning-Informed\nviSual Editing (RISE). RISEBench focuses on four key reasoning types: Temporal,\nCausal, Spatial, and Logical Reasoning. We curate high-quality test cases for\neach category and propose an evaluation framework that assesses Instruction\nReasoning, Appearance Consistency, and Visual Plausibility with both human\njudges and an LMM-as-a-judge approach. Our experiments reveal that while\nGPT-4o-Native significantly outperforms other open-source and proprietary\nmodels, even this state-of-the-art system struggles with logical reasoning\ntasks, highlighting an area that remains underexplored. As an initial effort,\nRISEBench aims to provide foundational insights into reasoning-aware visual\nediting and to catalyze future research. Though still in its early stages, we\nare committed to continuously expanding and refining the benchmark to support\nmore comprehensive, reliable, and scalable evaluations of next-generation\nmultimodal systems. Our code and data will be released at\nhttps://github.com/PhoenixZ810/RISEBench.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63ee1379190ddd6214efd73a/PQFr3_6S3BeUSNb79jMSO.png",
      "https://cdn-uploads.huggingface.co/production/uploads/63ee1379190ddd6214efd73a/0rGBBJT_JcUPBk5cE_Te0.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02826.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63ee1379190ddd6214efd73a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676546883247-noauth.png",
      "fullname": "HAODONG DUAN",
      "name": "KennyUTC",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 24
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.02782",
      "authors": [
        {
          "_id": "67ef502ce803d818f00e1b94",
          "name": "Zhiyuan Yan",
          "hidden": false
        },
        {
          "_id": "67ef502ce803d818f00e1b95",
          "user": {
            "_id": "66978ee0b8656f6506b4acb2",
            "avatarUrl": "/avatars/298acb8222e189fce4368985ee5374a1.svg",
            "isPro": false,
            "fullname": "Junyan Ye",
            "user": "Yejy53",
            "type": "user"
          },
          "name": "Junyan Ye",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:17:10.032Z",
          "hidden": false
        },
        {
          "_id": "67ef502ce803d818f00e1b96",
          "user": {
            "_id": "66d5b56c77a026c3d2086a79",
            "avatarUrl": "/avatars/45da07fd82fd455955faa05b27a6393f.svg",
            "isPro": false,
            "fullname": "Weijia Li",
            "user": "liweijia",
            "type": "user"
          },
          "name": "Weijia Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:16:44.819Z",
          "hidden": false
        },
        {
          "_id": "67ef502ce803d818f00e1b97",
          "user": {
            "_id": "6487e158f675b4a7867f45fa",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6487e158f675b4a7867f45fa/J0sls6zZ682o-SH7iQs7B.jpeg",
            "isPro": false,
            "fullname": "Zilong Huang",
            "user": "SereinH",
            "type": "user"
          },
          "name": "Zilong Huang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:08:56.501Z",
          "hidden": false
        },
        {
          "_id": "67ef502ce803d818f00e1b98",
          "user": {
            "_id": "63468720dd6d90d82ccf3450",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
            "isPro": false,
            "fullname": "YSH",
            "user": "BestWishYsh",
            "type": "user"
          },
          "name": "Shenghai Yuan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:09:05.246Z",
          "hidden": false
        },
        {
          "_id": "67ef502ce803d818f00e1b99",
          "user": {
            "_id": "67ef53656c7ba428e7c2e605",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/fFVJTpMRKF15Bf63yZEG_.png",
            "isPro": false,
            "fullname": "He",
            "user": "shawnxyh",
            "type": "user"
          },
          "name": "Xiangyang He",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:09:02.573Z",
          "hidden": false
        },
        {
          "_id": "67ef502ce803d818f00e1b9a",
          "user": {
            "_id": "6459a47e4fe72fae522b4fc9",
            "avatarUrl": "/avatars/a4139f8e348081e45b28dd99d96908d3.svg",
            "isPro": false,
            "fullname": "Kaiqing.Lin",
            "user": "lin6123",
            "type": "user"
          },
          "name": "Kaiqing Lin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:16:28.452Z",
          "hidden": false
        },
        {
          "_id": "67ef502ce803d818f00e1b9b",
          "user": {
            "_id": "670ddb69d6ac6394419d88c5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/XxnGNaX3FWug4aiZVjg93.png",
            "isPro": false,
            "fullname": "Jun He",
            "user": "JunHe0915",
            "type": "user"
          },
          "name": "Jun He",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:08:59.877Z",
          "hidden": false
        },
        {
          "_id": "67ef502ce803d818f00e1b9c",
          "user": {
            "_id": "63f9fca8d4349b157a109eec",
            "avatarUrl": "/avatars/fa1f2ae7972d7cde99dab178136ccbb0.svg",
            "isPro": false,
            "fullname": "Conghui He",
            "user": "conghui",
            "type": "user"
          },
          "name": "Conghui He",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:17:18.123Z",
          "hidden": false
        },
        {
          "_id": "67ef502ce803d818f00e1b9d",
          "user": {
            "_id": "66135a5e50350afe76beebce",
            "avatarUrl": "/avatars/370a4b83949355feb050c2cb0425c264.svg",
            "isPro": false,
            "fullname": "yl2488",
            "user": "yl2488",
            "type": "user"
          },
          "name": "Li Yuan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:08:40.281Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-03T17:23:16.000Z",
      "submittedOnDailyAt": "2025-04-04T01:51:34.697Z",
      "title": "GPT-ImgEval: 画像生成でのGPT4oの診断用の詳細なベンチマーク",
      "submittedOnDailyBy": {
        "_id": "63468720dd6d90d82ccf3450",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
        "isPro": false,
        "fullname": "YSH",
        "user": "BestWishYsh",
        "type": "user"
      },
      "summary": "最近OpenAIのGPT4oモデルの進歩は、画像生成と編集において非常に良い能力を示し、コミュニティにとっては非常に興味深い結果を収めています。本技術報告書では、GPT-ImgEvalという名前をつけた最初の見通し評価ベンチマークを用いて、画像生成、編集、世界知識に基づく語意合成の3つの重要な次元でGPT-4oの性能を定量的・定性的に評価します。3つのタスクでは、GPT-4oは強力な性能を示し、画像生成の制御と出力の品質において既存の方法を大幅に超え、特に知識推理能力を示しています。また、GPT-4oが生成したデータに基づいて、モデルの潜在的なアーキテクチャを調査するために、クラス分類モデルベースのアプローチを提案し、実験結果からモデルは自動回帰（AR）とディフュージョンベースのヘッドを組み合わせた構造であることを示しています。また、GPT-4oの画像生成で見られる具体的な制限と合成されたフィードバックを識別し、可視化しました。また、GPT-4oとGemini 2.0 Flashの多回繰り返し画像編集の比較研究を行い、GPT-4oの出力の安全性の影響を議論しました。GPT-4oの評価に使用されたコードとデータセットは、https://github.com/PicoTrex/GPT-ImgEvalから取得できます。",
      "upvotes": 28,
      "discussionId": "67ef502fe803d818f00e1c70",
      "githubRepo": "https://github.com/PicoTrex/GPT-ImgEval",
      "ai_keywords": [
        "auto-regressive (AR)",
        "diffusion-based head",
        "VAR-like architectures",
        "multi-round image editing",
        "image forensic models"
      ]
    },
    "publishedAt": "2025-04-03T13:23:16.000Z",
    "title": "GPT-ImgEval: A Comprehensive Benchmark for Diagnosing GPT4o in Image\n  Generation",
    "summary": "The recent breakthroughs in OpenAI's GPT4o model have demonstrated\nsurprisingly good capabilities in image generation and editing, resulting in\nsignificant excitement in the community. This technical report presents the\nfirst-look evaluation benchmark (named GPT-ImgEval), quantitatively and\nqualitatively diagnosing GPT-4o's performance across three critical dimensions:\n(1) generation quality, (2) editing proficiency, and (3) world\nknowledge-informed semantic synthesis. Across all three tasks, GPT-4o\ndemonstrates strong performance, significantly surpassing existing methods in\nboth image generation control and output quality, while also showcasing\nexceptional knowledge reasoning capabilities. Furthermore, based on the\nGPT-4o's generated data, we propose a classification-model-based approach to\ninvestigate the underlying architecture of GPT-4o, where our empirical results\nsuggest the model consists of an auto-regressive (AR) combined with a\ndiffusion-based head for image decoding, rather than the VAR-like\narchitectures. We also provide a complete speculation on GPT-4o's overall\narchitecture. In addition, we conduct a series of analyses to identify and\nvisualize GPT-4o's specific limitations and the synthetic artifacts commonly\nobserved in its image generation. We also present a comparative study of\nmulti-round image editing between GPT-4o and Gemini 2.0 Flash, and discuss the\nsafety implications of GPT-4o's outputs, particularly their detectability by\nexisting image forensic models. We hope that our work can offer valuable\ninsight and provide a reliable benchmark to guide future research, foster\nreproducibility, and accelerate innovation in the field of image generation and\nbeyond. The codes and datasets used for evaluating GPT-4o can be found at\nhttps://github.com/PicoTrex/GPT-ImgEval.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02782.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63468720dd6d90d82ccf3450",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
      "fullname": "YSH",
      "name": "BestWishYsh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 36
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.02587",
      "authors": [
        {
          "_id": "67ef3f9804be7fba0c882738",
          "user": {
            "_id": "633fc70529b5a95f6e15a6b7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/633fc70529b5a95f6e15a6b7/Fzh7wWuqU-fBbzdupOUtF.jpeg",
            "isPro": false,
            "fullname": "Yan Ma",
            "user": "ManTle",
            "type": "user"
          },
          "name": "Yan Ma",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:17:53.820Z",
          "hidden": false
        },
        {
          "_id": "67ef3f9804be7fba0c882739",
          "user": {
            "_id": "64b370fe6d953e7c75ede314",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b370fe6d953e7c75ede314/RdP2q3hGXWE4E2zfSv0KU.png",
            "isPro": false,
            "fullname": "Steffi Chern",
            "user": "steffichern",
            "type": "user"
          },
          "name": "Steffi Chern",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:09:14.660Z",
          "hidden": false
        },
        {
          "_id": "67ef3f9804be7fba0c88273a",
          "user": {
            "_id": "642e4d4d6748dd4f8eeb7732",
            "avatarUrl": "/avatars/fd911e9143d1a7aedd21a7d611543fcc.svg",
            "isPro": false,
            "fullname": "Xuyang Shen",
            "user": "Ryan1122",
            "type": "user"
          },
          "name": "Xuyang Shen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:17:40.397Z",
          "hidden": false
        },
        {
          "_id": "67ef3f9804be7fba0c88273b",
          "user": {
            "_id": "64c525e4d68946edad6c7067",
            "avatarUrl": "/avatars/1b108661634af602717a4ab4b66a151f.svg",
            "isPro": false,
            "fullname": "Yiran Zhong",
            "user": "IanZhong",
            "type": "user"
          },
          "name": "Yiran Zhong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:09:16.707Z",
          "hidden": false
        },
        {
          "_id": "67ef3f9804be7fba0c88273c",
          "user": {
            "_id": "6144a0c4ff1146bbd84d9865",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1661715958139-6144a0c4ff1146bbd84d9865.png",
            "isPro": false,
            "fullname": "Pengfei Liu",
            "user": "Pengfei",
            "type": "user"
          },
          "name": "Pengfei Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:17:34.472Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-03T13:53:28.000Z",
      "submittedOnDailyAt": "2025-04-04T00:42:23.044Z",
      "title": "再考えるRLスケーリングを視覚言語モデルに対して：透明な、スタートから始めるフレームワークと詳細な評価スキーム",
      "submittedOnDailyBy": {
        "_id": "633fc70529b5a95f6e15a6b7",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/633fc70529b5a95f6e15a6b7/Fzh7wWuqU-fBbzdupOUtF.jpeg",
        "isPro": false,
        "fullname": "Yan Ma",
        "user": "ManTle",
        "type": "user"
      },
      "summary": "強化學習（RL）は、最近、大規模言語モデルの理由能力を向上させる強力な可能性を示し、これを視覚言語モデル（VLMs）に活用するように進められています。しかし、現在のVLMsにおけるRLアプリケーションは、再現性とアクセス性を妨げるための重いエンジニアリングフレームワークを多く使用しており、標準化された評価プロトコルが欠けており、結果の比較や学習ダイナミクスの解釈が難しい問題があります。本論文では、VLMsにおけるRLのシンプルで機能的な4ステップパイプラインを提供し、複数のモデルとデータセットで検証された透明なフレームワークを介して、評価プロトコルを標準化し、学習ダイナミクスと反省的な行動を評価するための標準化された評価スキームを提案します。視覚理由タスクにおける拡散的な実験では、以下のキー的な実験発見が明らかになりました：回答の長さはランダムシードに敏感であり、反省は出力の長さと相関し、RLは高品質のデータでも一般化性能においてSFTよりも一貫して優れています。これらの発見と提案されたフレームワークは、再現性のあるベースラインを確立し、RLベースのVLM研究の広範囲の協力を支援することを目指しています。",
      "upvotes": 19,
      "discussionId": "67ef3f9904be7fba0c882772",
      "ai_keywords": [
        "reinforcement learning",
        "reasoning capabilities",
        "large language models",
        "vision-language models",
        "reproducibility",
        "accessibility",
        "standardized evaluation protocols",
        "transparent framework",
        "four-step pipeline",
        "training dynamics",
        "reflective behaviors",
        "visual reasoning tasks",
        "response length",
        "reflection",
        "supervised fine-tuning"
      ]
    },
    "publishedAt": "2025-04-03T09:53:28.000Z",
    "title": "Rethinking RL Scaling for Vision Language Models: A Transparent,\n  From-Scratch Framework and Comprehensive Evaluation Scheme",
    "summary": "Reinforcement learning (RL) has recently shown strong potential in improving\nthe reasoning capabilities of large language models and is now being actively\nextended to vision-language models (VLMs). However, existing RL applications in\nVLMs often rely on heavily engineered frameworks that hinder reproducibility\nand accessibility, while lacking standardized evaluation protocols, making it\ndifficult to compare results or interpret training dynamics. This work\nintroduces a transparent, from-scratch framework for RL in VLMs, offering a\nminimal yet functional four-step pipeline validated across multiple models and\ndatasets. In addition, a standardized evaluation scheme is proposed to assess\ntraining dynamics and reflective behaviors. Extensive experiments on visual\nreasoning tasks uncover key empirical findings: response length is sensitive to\nrandom seeds, reflection correlates with output length, and RL consistently\noutperforms supervised fine-tuning (SFT) in generalization, even with\nhigh-quality data. These findings, together with the proposed framework, aim to\nestablish a reproducible baseline and support broader engagement in RL-based\nVLM research.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02587.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "633fc70529b5a95f6e15a6b7",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/633fc70529b5a95f6e15a6b7/Fzh7wWuqU-fBbzdupOUtF.jpeg",
      "fullname": "Yan Ma",
      "name": "ManTle",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.02436",
      "authors": [
        {
          "_id": "67ef3dfae8b932ae7a832950",
          "user": {
            "_id": "617ba1820e4237bd1731b867",
            "avatarUrl": "/avatars/f9de06363e64bddd7dc977e96e85df8a.svg",
            "isPro": false,
            "fullname": "zhengcong fei",
            "user": "onion",
            "type": "user"
          },
          "name": "Zhengcong Fei",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:19:16.548Z",
          "hidden": false
        },
        {
          "_id": "67ef3dfae8b932ae7a832951",
          "user": {
            "_id": "65dc3a850af7e21ba40e939f",
            "avatarUrl": "/avatars/e129c64617675edd05d4317d39604318.svg",
            "isPro": false,
            "fullname": "Li",
            "user": "Debang",
            "type": "user"
          },
          "name": "Debang Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:19:27.042Z",
          "hidden": false
        },
        {
          "_id": "67ef3dfae8b932ae7a832952",
          "user": {
            "_id": "65bef422fdb8d33cefeaccc3",
            "avatarUrl": "/avatars/d40b0d7dda21fa1a68c291d11bc357ec.svg",
            "isPro": false,
            "fullname": "Qiu Di",
            "user": "diqiu7",
            "type": "user"
          },
          "name": "Di Qiu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:19:41.458Z",
          "hidden": false
        },
        {
          "_id": "67ef3dfae8b932ae7a832953",
          "name": "Jiahua Wang",
          "hidden": false
        },
        {
          "_id": "67ef3dfae8b932ae7a832954",
          "name": "Yikun Dou",
          "hidden": false
        },
        {
          "_id": "67ef3dfae8b932ae7a832955",
          "user": {
            "_id": "62e0f1314db2175cd270ad08",
            "avatarUrl": "/avatars/1d3d6af6c63557f4abf0484e028fa942.svg",
            "isPro": false,
            "fullname": "Rui Wang",
            "user": "ruiwang",
            "type": "user"
          },
          "name": "Rui Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:20:11.206Z",
          "hidden": false
        },
        {
          "_id": "67ef3dfae8b932ae7a832956",
          "user": {
            "_id": "666a674967c686801acf25bb",
            "avatarUrl": "/avatars/c1f3edd63fd378dfb555e6413a966932.svg",
            "isPro": false,
            "fullname": "jingtao xu",
            "user": "raul678",
            "type": "user"
          },
          "name": "Jingtao Xu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:20:20.880Z",
          "hidden": false
        },
        {
          "_id": "67ef3dfae8b932ae7a832957",
          "user": {
            "_id": "634672bfb7b4e71c7f45360f",
            "avatarUrl": "/avatars/4b646fc3e271be90b9ec619d42ce3e99.svg",
            "isPro": false,
            "fullname": "Fan Mingyuan",
            "user": "MichaelFan",
            "type": "user"
          },
          "name": "Mingyuan Fan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:20:32.597Z",
          "hidden": false
        },
        {
          "_id": "67ef3dfae8b932ae7a832958",
          "name": "Guibin Chen",
          "hidden": false
        },
        {
          "_id": "67ef3dfae8b932ae7a832959",
          "name": "Yang Li",
          "hidden": false
        },
        {
          "_id": "67ef3dfae8b932ae7a83295a",
          "name": "Yahui Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-03T09:50:50.000Z",
      "submittedOnDailyAt": "2025-04-04T00:33:57.000Z",
      "title": "SkyReels-A2: 映画ディフュージョントランスフォーマーで何でも作れる",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "この論文では、SkyReels-A2という制御可能なビデオ生成フレームワークを紹介します。このフレームワークは、文字列プロンプトに基づいて任意の可視要素（例：人物、物体、背景）を合成ビデオに組み立てることができ、各要素に対して参照画像との厳密な一致性を維持します。このタスクを要素からビデオ（E2V）と呼び、主な課題は、参照要素の忠実性の維持、写真の連続性の確保、自然な出力の達成にあります。これらの課題に対して、まずは、モデルの訓練に用いるプロンプト-参照画像-ビデオのタプルを構築するための詳細なデータプイプラインを設計します。次に、画像-文字の共通埋め込みモデルを提案し、生成プロセスに多要素の表現を注入し、要素の特定的な一致性とグローバルの連続性と文脈の対応をバランスづけることを目指します。また、推論プイプラインを速度と出力の安定性による最適化します。また、システマティックな評価のために、A2 Benchというカリティングされたベンチマークを紹介します。実験は、我々のフレームワークが多様性と高品質のビデオを生成でき、要素の精密な制御を可能にしますことを示しています。SkyReels-A2は、E2Vの生成において最初の開放ソースのコマーシャルレベルモデルであり、進歩したクローズドソースコマーシャルモデルと比較してもよく評価されています。我々は、SkyReels-A2がドラマやバーチャルエコマースなどの創造的なアプリケーションにおいてビデオ生成の制御可能性の境界を超えることを期待しています。",
      "upvotes": 17,
      "discussionId": "67ef3dfee8b932ae7a832a97",
      "ai_keywords": [
        "elements-to-video (E2V)",
        "image-text joint embedding model",
        "prompt-reference-video triplets",
        "generative process",
        "multi-element representations",
        "strict consistency",
        "coherent composition",
        "natural outputs",
        "output stability",
        "A2 Bench (benchmark)",
        "high-quality videos",
        "precise element control",
        "open-source commercial grade model"
      ]
    },
    "publishedAt": "2025-04-03T05:50:50.000Z",
    "title": "SkyReels-A2: Compose Anything in Video Diffusion Transformers",
    "summary": "This paper presents SkyReels-A2, a controllable video generation framework\ncapable of assembling arbitrary visual elements (e.g., characters, objects,\nbackgrounds) into synthesized videos based on textual prompts while maintaining\nstrict consistency with reference images for each element. We term this task\nelements-to-video (E2V), whose primary challenges lie in preserving the\nfidelity of each reference element, ensuring coherent composition of the scene,\nand achieving natural outputs. To address these, we first design a\ncomprehensive data pipeline to construct prompt-reference-video triplets for\nmodel training. Next, we propose a novel image-text joint embedding model to\ninject multi-element representations into the generative process, balancing\nelement-specific consistency with global coherence and text alignment. We also\noptimize the inference pipeline for both speed and output stability. Moreover,\nwe introduce a carefully curated benchmark for systematic evaluation, i.e, A2\nBench. Experiments demonstrate that our framework can generate diverse,\nhigh-quality videos with precise element control. SkyReels-A2 is the first\nopen-source commercial grade model for the generation of E2V, performing\nfavorably against advanced closed-source commercial models. We anticipate\nSkyReels-A2 will advance creative applications such as drama and virtual\ne-commerce, pushing the boundaries of controllable video generation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02436.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6573
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.00502",
      "authors": [
        {
          "_id": "67ef72898667ee5c99026d16",
          "user": {
            "_id": "67014d33126f9ab39fc52481",
            "avatarUrl": "/avatars/60d1f791e7f3201ce1aef72e9216ff78.svg",
            "isPro": false,
            "fullname": "Qianhao Yuan",
            "user": "yuanqianhao",
            "type": "user"
          },
          "name": "Qianhao Yuan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:23:28.149Z",
          "hidden": false
        },
        {
          "_id": "67ef72898667ee5c99026d17",
          "name": "Qingyu Zhang",
          "hidden": false
        },
        {
          "_id": "67ef72898667ee5c99026d18",
          "name": "Yanjiang Liu",
          "hidden": false
        },
        {
          "_id": "67ef72898667ee5c99026d19",
          "user": {
            "_id": "654c7fbe6b51714c2a6ff590",
            "avatarUrl": "/avatars/db217415c56730872b9a807f3afb4e5b.svg",
            "isPro": false,
            "fullname": "Jiawei Chen",
            "user": "chenjiawei-icip",
            "type": "user"
          },
          "name": "Jiawei Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:08:04.134Z",
          "hidden": false
        },
        {
          "_id": "67ef72898667ee5c99026d1a",
          "user": {
            "_id": "6216496a9b34d2fb49144599",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6216496a9b34d2fb49144599/41CKA_h1Ffj3RzVabSAkm.jpeg",
            "isPro": false,
            "fullname": "Yaojie Lu",
            "user": "luyaojie",
            "type": "user"
          },
          "name": "Yaojie Lu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:24:09.511Z",
          "hidden": false
        },
        {
          "_id": "67ef72898667ee5c99026d1b",
          "user": {
            "_id": "6711c702f858a456b4b9f3a4",
            "avatarUrl": "/avatars/178e9567c3111ab22717c3c0dd003a6a.svg",
            "isPro": false,
            "fullname": "Hongyu  Lin",
            "user": "sanmusunrise",
            "type": "user"
          },
          "name": "Hongyu Lin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:24:15.188Z",
          "hidden": false
        },
        {
          "_id": "67ef72898667ee5c99026d1c",
          "name": "Jia Zheng",
          "hidden": false
        },
        {
          "_id": "67ef72898667ee5c99026d1d",
          "user": {
            "_id": "65e99a77e71555ed193609cf",
            "avatarUrl": "/avatars/38ceb127883944677665da967d17dd18.svg",
            "isPro": false,
            "fullname": "Xianpei Han",
            "user": "xphan",
            "type": "user"
          },
          "name": "Xianpei Han",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:24:23.046Z",
          "hidden": false
        },
        {
          "_id": "67ef72898667ee5c99026d1e",
          "name": "Le Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-01T07:47:55.000Z",
      "submittedOnDailyAt": "2025-04-04T04:19:46.946Z",
      "title": "ShortV: 無効な層での画像トークンのフリーズでの効率的な多モデル大語言モデル",
      "submittedOnDailyBy": {
        "_id": "67014d33126f9ab39fc52481",
        "avatarUrl": "/avatars/60d1f791e7f3201ce1aef72e9216ff78.svg",
        "isPro": false,
        "fullname": "Qianhao Yuan",
        "user": "yuanqianhao",
        "type": "user"
      },
      "summary": "多モダル大語言モデル（MLLMs）は、巨大なサイズと大規模な可視トークンにより高い計算コストを伴う。本論文では、層ごとの冗黴を調査するために、層の変換が可視トークンと文脈トークンにどのような影響を与えるかを定量化する新しいメトリック「層の貢献度（LC）」を導入した。LCの計算には、指定されたトークンに対する層の変換を除去したものとのモデル出力の離散度を測定することによって行われる。ピロット実験では、MLLMの多くの層が可視トークンの処理中に最小限の貢献を示していることが明らかになった。この見つかりに基づき、LCを利用して無効な層を特定し、その層での可視トークンの更新を停止させる無学習方法「ShortV」を提案した。実験結果によると、ShortVはMLLMの約60%の層で可視トークンの更新を停止させ、可視トークンの更新に関する計算コストを大幅に減少させることができる。例えば、LLaVA-NeXT-13Bでは、性能を維持する同時にFLOPsを50%削減することができる。コードは、https://github.com/icip-cas/ShortV で公開される。",
      "upvotes": 11,
      "discussionId": "67ef728a8667ee5c99026d69",
      "githubRepo": "https://github.com/icip-cas/ShortV",
      "ai_keywords": [
        "Multimodal Large Language Models (MLLMs)",
        "Layer Contribution (LC)",
        "visual tokens",
        "transformations",
        "layer-wise redundancy",
        "model output",
        "divergence",
        "ineffective layers",
        "training-free method",
        "visual token updates",
        "computational costs",
        "FLOPs",
        "LLaVA-NeXT-13B"
      ]
    },
    "publishedAt": "2025-04-01T03:47:55.000Z",
    "title": "ShortV: Efficient Multimodal Large Language Models by Freezing Visual\n  Tokens in Ineffective Layers",
    "summary": "Multimodal Large Language Models (MLLMs) suffer from high computational costs\ndue to their massive size and the large number of visual tokens. In this paper,\nwe investigate layer-wise redundancy in MLLMs by introducing a novel metric,\nLayer Contribution (LC), which quantifies the impact of a layer's\ntransformations on visual and text tokens, respectively. The calculation of LC\ninvolves measuring the divergence in model output that results from removing\nthe layer's transformations on the specified tokens. Our pilot experiment\nreveals that many layers of MLLMs exhibit minimal contribution during the\nprocessing of visual tokens. Motivated by this observation, we propose ShortV,\na training-free method that leverages LC to identify ineffective layers, and\nfreezes visual token updates in these layers. Experiments show that ShortV can\nfreeze visual token in approximately 60\\% of the MLLM layers, thereby\ndramatically reducing computational costs related to updating visual tokens.\nFor example, it achieves a 50\\% reduction in FLOPs on LLaVA-NeXT-13B while\nmaintaining superior performance. The code will be publicly available at\nhttps://github.com/icip-cas/ShortV",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00502.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67014d33126f9ab39fc52481",
      "avatarUrl": "/avatars/60d1f791e7f3201ce1aef72e9216ff78.svg",
      "fullname": "Qianhao Yuan",
      "name": "yuanqianhao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.02542",
      "authors": [
        {
          "_id": "67ef3773ac0c701df7fd98aa",
          "user": {
            "_id": "6264a7dfc39850dc093eb68a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1650763566575-noauth.png",
            "isPro": false,
            "fullname": "Fa-Ting Hong",
            "user": "HarlanHong",
            "type": "user"
          },
          "name": "Fa-Ting Hong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:09:38.641Z",
          "hidden": false
        },
        {
          "_id": "67ef3773ac0c701df7fd98ab",
          "user": {
            "_id": "6481523b3fb124fc9850afed",
            "avatarUrl": "/avatars/ddde178c88713662800aafd2343647a4.svg",
            "isPro": false,
            "fullname": "Zunnan Xu",
            "user": "xuzn",
            "type": "user"
          },
          "name": "Zunnan Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:09:23.733Z",
          "hidden": false
        },
        {
          "_id": "67ef3773ac0c701df7fd98ac",
          "name": "Zixiang Zhou",
          "hidden": false
        },
        {
          "_id": "67ef3773ac0c701df7fd98ad",
          "name": "Jun Zhou",
          "hidden": false
        },
        {
          "_id": "67ef3773ac0c701df7fd98ae",
          "name": "Xiu Li",
          "hidden": false
        },
        {
          "_id": "67ef3773ac0c701df7fd98af",
          "name": "Qin Lin",
          "hidden": false
        },
        {
          "_id": "67ef3773ac0c701df7fd98b0",
          "name": "Qinglin Lu",
          "hidden": false
        },
        {
          "_id": "67ef3773ac0c701df7fd98b1",
          "user": {
            "_id": "66feab48651e00e22f33222e",
            "avatarUrl": "/avatars/7344377e2c796c7ec85194bb2fc78521.svg",
            "isPro": false,
            "fullname": "Dan Xu",
            "user": "danxuhk",
            "type": "user"
          },
          "name": "Dan Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:09:20.987Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/66feab48651e00e22f33222e/VOMaQBXsDjs1295R5NVOh.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/66feab48651e00e22f33222e/tksGfiG1zEDZ4QUg4IorF.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/66feab48651e00e22f33222e/aUp8tmNM2pdPYCa3A3Noh.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/66feab48651e00e22f33222e/MpCNSPEP6OSjZR_0m70PP.mp4"
      ],
      "publishedAt": "2025-04-03T12:44:41.000Z",
      "submittedOnDailyAt": "2025-04-04T01:37:45.934Z",
      "title": "音声ビデオ制御バイアスモデリング マスクされた選択的状態ステージ空間モデリング 自然なトークンヘッド生成",
      "submittedOnDailyBy": {
        "_id": "66feab48651e00e22f33222e",
        "avatarUrl": "/avatars/7344377e2c796c7ec85194bb2fc78521.svg",
        "isPro": false,
        "fullname": "Dan Xu",
        "user": "danxuhk",
        "type": "user"
      },
      "summary": "Talking head synthesisは、ビュータービューと人間・コンピュータインタラクションにとって重要ですが、現在の方法は通常、1つの主なモデルからの制御を受けることに限定されていて、実用的な役割を果たすことが難しい。この点に対して、我々はACTalkerを紹介します。ACTalkerは、多信号制御と単一信号制御をサポートする端から端までのビデオディフュージョンフレームワークです。多信号制御のためには、平行なmamba構造を設計し、各分枝が別々の駆動信号を使用して特定の顔部領域を制御します。すべての分枝においてゲート機構を適用し、ビデオ生成に対する柔軟な制御を提供します。時間的および空間的な自然な連携を確保するために、mamba構造を使用し、各分枝で駆動信号が両方の次元で特徴トークンを操作できます。また、マスクドロップ戦略を導入し、各駆動信号が独立にmamba構造内の対応する顔部領域を制御できるようにし、制御の衝突を防ぎます。実験結果によると、我々の方法は多様な信号で駆動される自然な顔のビデオを生成し、mamba層が衝突なく複数の駆動モデルティーブルを無間に統合します。",
      "upvotes": 8,
      "discussionId": "67ef3775ac0c701df7fd994c",
      "projectPage": "https://harlanhong.github.io/publications/actalker/index.html",
      "githubRepo": "https://github.com/harlanhong/ACTalker",
      "ai_keywords": [
        "ACTalker",
        "video diffusion framework",
        "multi-signals control",
        "parallel mamba structure",
        "driving signals",
        "gate mechanism",
        "temporal coordination",
        "spatial coordination",
        "feature tokens",
        "mask-drop strategy",
        "facial videos",
        "multiple driving modalities"
      ]
    },
    "publishedAt": "2025-04-03T08:44:41.000Z",
    "title": "Audio-visual Controlled Video Diffusion with Masked Selective State\n  Spaces Modeling for Natural Talking Head Generation",
    "summary": "Talking head synthesis is vital for virtual avatars and human-computer\ninteraction. However, most existing methods are typically limited to accepting\ncontrol from a single primary modality, restricting their practical utility. To\nthis end, we introduce ACTalker, an end-to-end video diffusion\nframework that supports both multi-signals control and single-signal control\nfor talking head video generation. For multiple control, we design a parallel\nmamba structure with multiple branches, each utilizing a separate driving\nsignal to control specific facial regions. A gate mechanism is applied across\nall branches, providing flexible control over video generation. To ensure\nnatural coordination of the controlled video both temporally and spatially, we\nemploy the mamba structure, which enables driving signals to manipulate feature\ntokens across both dimensions in each branch. Additionally, we introduce a\nmask-drop strategy that allows each driving signal to independently control its\ncorresponding facial region within the mamba structure, preventing control\nconflicts. Experimental results demonstrate that our method produces\nnatural-looking facial videos driven by diverse signals and that the mamba\nlayer seamlessly integrates multiple driving modalities without conflict.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/66feab48651e00e22f33222e/VOMaQBXsDjs1295R5NVOh.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/66feab48651e00e22f33222e/tksGfiG1zEDZ4QUg4IorF.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/66feab48651e00e22f33222e/aUp8tmNM2pdPYCa3A3Noh.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/66feab48651e00e22f33222e/MpCNSPEP6OSjZR_0m70PP.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02542.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66feab48651e00e22f33222e",
      "avatarUrl": "/avatars/7344377e2c796c7ec85194bb2fc78521.svg",
      "fullname": "Dan Xu",
      "name": "danxuhk",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.02507",
      "authors": [
        {
          "_id": "67ef5a3d4417508df8d99dad",
          "user": {
            "_id": "62cd4b03c5cc157be82f0b56",
            "avatarUrl": "/avatars/351e963c1c763d507ae78cbcd62966a3.svg",
            "isPro": false,
            "fullname": "Abhay kumar",
            "user": "akanyaani",
            "type": "user"
          },
          "name": "Abhay Kumar",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:08:18.512Z",
          "hidden": false
        },
        {
          "_id": "67ef5a3d4417508df8d99dae",
          "user": {
            "_id": "6071c4b270e11b30cfcfd7a3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6071c4b270e11b30cfcfd7a3/-1ekCBzSTpqxkkul0bgmI.jpeg",
            "isPro": false,
            "fullname": "Louis Owen",
            "user": "louisowen6",
            "type": "user"
          },
          "name": "Louis Owen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:08:21.051Z",
          "hidden": false
        },
        {
          "_id": "67ef5a3d4417508df8d99daf",
          "user": {
            "_id": "645a0d3dd6648853107c5fdc",
            "avatarUrl": "/avatars/1e3b6a4f5ce81a707ba7cbdf81631091.svg",
            "isPro": false,
            "fullname": "Nilabhra Roy Chowdhury",
            "user": "nilabhra",
            "type": "user"
          },
          "name": "Nilabhra Roy Chowdhury",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:08:12.764Z",
          "hidden": false
        },
        {
          "_id": "67ef5a3d4417508df8d99db0",
          "user": {
            "_id": "65e4be59e8b017ee1310a1b6",
            "avatarUrl": "/avatars/c3f7cdf5d0859cb80bfb2b970a675dfa.svg",
            "isPro": false,
            "fullname": "Fabian",
            "user": "gueraf",
            "type": "user"
          },
          "name": "Fabian Güra",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:08:16.132Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-03T11:41:55.000Z",
      "submittedOnDailyAt": "2025-04-04T02:34:36.631Z",
      "title": "ZClip: モデルの適応的なスパイク軽減によるLLMの事前学習",
      "submittedOnDailyBy": {
        "_id": "6071c4b270e11b30cfcfd7a3",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6071c4b270e11b30cfcfd7a3/-1ekCBzSTpqxkkul0bgmI.jpeg",
        "isPro": false,
        "fullname": "Louis Owen",
        "user": "louisowen6",
        "type": "user"
      },
      "summary": "大語言モデル（LLMs）の訓練には、勾配不穩定と損失のスパイクなどの複数の課題があります。これらの現象は、破壊的なデジバンジュを引き起こし、費用の高いチェックポイントの復元やデータバッチのスキップが必要となります。常に固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の",
      "upvotes": 7,
      "discussionId": "67ef5a3e4417508df8d99dfc",
      "githubRepo": "https://github.com/bluorion-com/ZClip/",
      "ai_keywords": [
        "large language models (LLMs)",
        "gradient instability",
        "loss spikes",
        "catastrophic divergence",
        "checkpoint restoration",
        "data batch skipping",
        "traditional gradient clipping techniques",
        "norm-based methods",
        "adaptive gradient clipping",
        "clipping threshold",
        "statistical properties of gradient norms",
        "z-score-based anomaly detection",
        "malignant loss spikes",
        "convergence"
      ]
    },
    "publishedAt": "2025-04-03T07:41:55.000Z",
    "title": "ZClip: Adaptive Spike Mitigation for LLM Pre-Training",
    "summary": "Training large language models (LLMs) presents numerous challenges, including\ngradient instability and loss spikes. These phenomena can lead to catastrophic\ndivergence, requiring costly checkpoint restoration and data batch skipping.\nTraditional gradient clipping techniques, such as constant or norm-based\nmethods, fail to address these issues effectively due to their reliance on\nfixed thresholds or heuristics, leading to inefficient learning and requiring\nfrequent manual intervention. In this work, we propose ZClip, an adaptive\ngradient clipping algorithm that dynamically adjusts the clipping threshold\nbased on statistical properties of gradient norms over time. Unlike prior\nreactive strategies, ZClip proactively adapts to training dynamics without\nmaking any prior assumptions on the scale and the temporal evolution of\ngradient norms. At its core, it leverages z-score-based anomaly detection to\nidentify and mitigate large gradient spikes, preventing malignant loss spikes\nwhile not interfering with convergence otherwise. Our code is available at:\nhttps://github.com/bluorion-com/ZClip.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02507.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6071c4b270e11b30cfcfd7a3",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6071c4b270e11b30cfcfd7a3/-1ekCBzSTpqxkkul0bgmI.jpeg",
      "fullname": "Louis Owen",
      "name": "louisowen6",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.02398",
      "authors": [
        {
          "_id": "67ef63b5e8b932ae7a8d3043",
          "user": {
            "_id": "66b9bc2dacdbc1d0b39c3b50",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/hwR0pVfP_E8XjimXIxDOU.jpeg",
            "isPro": false,
            "fullname": "Gallil Maimon",
            "user": "gallilmaimon",
            "type": "user"
          },
          "name": "Gallil Maimon",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:08:06.509Z",
          "hidden": false
        },
        {
          "_id": "67ef63b5e8b932ae7a8d3044",
          "user": {
            "_id": "6547411a9295970f878aa52e",
            "avatarUrl": "/avatars/6e240f0add27bf1a6c04a9618eccdf83.svg",
            "isPro": false,
            "fullname": "Michael Hassid",
            "user": "hassid",
            "type": "user"
          },
          "name": "Michael Hassid",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:24:39.934Z",
          "hidden": false
        },
        {
          "_id": "67ef63b5e8b932ae7a8d3045",
          "user": {
            "_id": "64b7b7b38ba7d6c922d753d6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b7b7b38ba7d6c922d753d6/rt0thjYa84VZHy1BEcW4p.jpeg",
            "isPro": false,
            "fullname": "Amit Roth",
            "user": "MajoRoth",
            "type": "user"
          },
          "name": "Amit Roth",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:24:48.483Z",
          "hidden": false
        },
        {
          "_id": "67ef63b5e8b932ae7a8d3046",
          "user": {
            "_id": "6481e135578646b5c2386728",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6481e135578646b5c2386728/SPva4iNw0pORiCXD45cx9.jpeg",
            "isPro": false,
            "fullname": "Yossi Adi",
            "user": "adiyoss",
            "type": "user"
          },
          "name": "Yossi Adi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:24:54.851Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/66b9bc2dacdbc1d0b39c3b50/wqUq-bT-DvKoNybPX46uL.png"
      ],
      "publishedAt": "2025-04-03T08:46:56.000Z",
      "submittedOnDailyAt": "2025-04-04T03:52:15.607Z",
      "title": "インターライブドスピーチ-テキスト言語モデルのスケーリング分析",
      "submittedOnDailyBy": {
        "_id": "66b9bc2dacdbc1d0b39c3b50",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/hwR0pVfP_E8XjimXIxDOU.jpeg",
        "isPro": false,
        "fullname": "Gallil Maimon",
        "user": "gallilmaimon",
        "type": "user"
      },
      "summary": "現在の言語モデル（SLM）スケーリング分析は暗黒なプレイヤーを描く。\n彼らは、SLMは文書に比べて大幅に多くの計算量とデータを必要とすることを予測し、これにより高品質のSLMの訓練の可能性が疑問になるような人々を引き起こしている。\nしかし、現代のSLMは通常、言語と文字の間を交じり合った方法で事前学習されたTextLMから初期化され、知識の伝達を許可するように設計されている。\nこれは、間隙付きのSLMが無文字のSLMよりもより効率的にスケーリングするかどうかの問題を浮かび上げる。\nこの論文では、この問題を厳密に解決し、結果は積極的であることを示している。\n私たちは、間隙付きのSLMのスケーリング分析を行い、数十つのモデルを訓練し、スケーリングの傾向を分析している。\nこの設定では、SLMは計算量によりより効率的にスケーリングすることがわかった。\nまた、私たちの結果は、スケーリングの動態が無文字のSLMと大幅に異なることを示し、モデルサイズの拡大に対して計算量マナーセットを大幅に増やすべきであることを示している。\nまた、合成データとTextLMモデルの家族の役割も研究し、この潛力を解放することを試みている。\n結果は、我々のスケーリングモデルは、他のアプローチよりも計算量とデータを少なくとも使用しながら、言語語意評価においてリーディングモデルと同等の性能を達成することを示している。\nモデル、サンプル、データを公開しています - https://pages.cs.huji.ac.il/adiyoss-lab/sims.",
      "upvotes": 7,
      "discussionId": "67ef63b6e8b932ae7a8d306d",
      "projectPage": "https://pages.cs.huji.ac.il/adiyoss-lab/sims/",
      "githubRepo": "https://github.com/slp-rl/slamkit",
      "ai_keywords": [
        "Speech Language Model (SLM)",
        "TextLMs",
        "speech-text interleaving",
        "scaling analysis",
        "compute",
        "knowledge transfer",
        "textless-SLMs",
        "scaling trends",
        "scaling-dynamics",
        "training tokens",
        "synthetic data",
        "model families",
        "speech semantic metrics"
      ]
    },
    "publishedAt": "2025-04-03T04:46:56.000Z",
    "title": "Scaling Analysis of Interleaved Speech-Text Language Models",
    "summary": "Existing Speech Language Model (SLM) scaling analysis paints a bleak picture.\nThey predict that SLMs require much more compute and data compared to text,\nleading some to question the feasibility of training high-quality SLMs.\nHowever, modern SLMs are often initialised from pre-trained TextLMs using\nspeech-text interleaving to allow knowledge transfer. This raises the question\n- Do interleaved SLMs scale more efficiently than textless-SLMs? In this paper\nwe answer a resounding, yes! We conduct scaling analysis of interleaved SLMs by\ntraining several dozen and analysing the scaling trends. We see that under this\nsetup SLMs scale more efficiently with compute. Additionally, our results\nindicate that the scaling-dynamics are significantly different than\ntextless-SLMs, suggesting one should allocate notably more of the compute\nbudget for increasing model size over training tokens. We also study the role\nof synthetic data and TextLM model families in unlocking this potential.\nResults suggest, that our scaled up model achieves comparable performance with\nleading models on speech semantic metrics while using less compute and data\nthan other approaches. We open source models, samples, and data -\nhttps://pages.cs.huji.ac.il/adiyoss-lab/sims.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/66b9bc2dacdbc1d0b39c3b50/wqUq-bT-DvKoNybPX46uL.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02398.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66b9bc2dacdbc1d0b39c3b50",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/hwR0pVfP_E8XjimXIxDOU.jpeg",
      "fullname": "Gallil Maimon",
      "name": "gallilmaimon",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.02012",
      "authors": [
        {
          "_id": "67ef5af0724d484dd41afe5c",
          "user": {
            "_id": "66189b980da4c017c401fb5d",
            "avatarUrl": "/avatars/d5184741e4a333435022bcb9a4a9b9d8.svg",
            "isPro": false,
            "fullname": "soro bedio",
            "user": "bedio",
            "type": "user"
          },
          "name": "Soro Bedionita",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:51:20.621Z",
          "hidden": false
        },
        {
          "_id": "67ef5af0724d484dd41afe5d",
          "name": "Bruno Andreis",
          "hidden": false
        },
        {
          "_id": "67ef5af0724d484dd41afe5e",
          "name": "Song Chong",
          "hidden": false
        },
        {
          "_id": "67ef5af0724d484dd41afe5f",
          "name": "Sung Ju Hwang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-02T05:50:19.000Z",
      "submittedOnDailyAt": "2025-04-04T02:38:11.321Z",
      "title": "インストラクションガイドドルノライブアーガレックネットワークパラメーター生成",
      "submittedOnDailyBy": {
        "_id": "66189b980da4c017c401fb5d",
        "avatarUrl": "/avatars/d5184741e4a333435022bcb9a4a9b9d8.svg",
        "isPro": false,
        "fullname": "soro bedio",
        "user": "bedio",
        "type": "user"
      },
      "summary": "学習ツークルールによって条件付きにニューラルネットワークパラメータを生成することは、モデルの適応性とトレンジャー学習の進歩に重要です。現在の方法では、特にディフュージョンモデルに基づいたものは、大きなアーキテクチャに対するスケーラブルさの限界、異なるネットワークの深さを処理する際の剛性、および間接層のコラバレーションを破壊するパラメータ生成の離れた性質があります。本研究では、多様なタスクとアーキテクチャでパラメータ合成を統一する自動復元フレームワークであるIGPG（インストラクションガイドペラメータ生成）を提案します。IGPGはVQ-VAEと自動復元モデルを利用し、タスク指示、データセット、アーキテクチャの詳細に基づいてニューラルネットワークパラメータを生成します。自動復元的にネットワークの重みのトークンを生成することで、IGPGは間接層のコラバレーションを確保し、モデルとデータセットの間で効率的な適応性を許可します。トークンレベルで動作するIGPGは、広い範囲の学習済みモデルから集約された複雑なパラメータ分布を有効に捉えます。多くの視覚データセットにおいての拡散的な実験は、IGPGが多様な学習済みモデルを一つの柔軟な生成フレームワークに統合し、状態の最先端の方法と比較して競争的または更に上位の性能を収めることを示します。特に、大きなアーキテクチャに対してのスケーラブルさと効率においては、これらの結果は、IGPGが学習済み重みの検索、モデル選択、および迅速なタスク特有の微調節において強力なツールとしての可能性を強調します。",
      "upvotes": 5,
      "discussionId": "67ef5af1724d484dd41afef3",
      "ai_keywords": [
        "diffusion models",
        "IGPG (Instruction Guided Parameter Generation)",
        "VQ-VAE",
        "autoregressive framework",
        "token level",
        "parameter synthesis",
        "inter-layer coherence",
        "vision datasets",
        "pretrained models",
        "pretrained weight retrieval",
        "model selection",
        "task-specific fine-tuning"
      ]
    },
    "publishedAt": "2025-04-02T01:50:19.000Z",
    "title": "Instruction-Guided Autoregressive Neural Network Parameter Generation",
    "summary": "Learning to generate neural network parameters conditioned on task\ndescriptions and architecture specifications is pivotal for advancing model\nadaptability and transfer learning. Existing methods especially those based on\ndiffusion models suffer from limited scalability to large architectures,\nrigidity in handling varying network depths, and disjointed parameter\ngeneration that undermines inter-layer coherence. In this work, we propose IGPG\n(Instruction Guided Parameter Generation), an autoregressive framework that\nunifies parameter synthesis across diverse tasks and architectures. IGPG\nleverages a VQ-VAE and an autoregressive model to generate neural network\nparameters, conditioned on task instructions, dataset, and architecture\ndetails. By autoregressively generating neural network weights' tokens, IGPG\nensures inter-layer coherence and enables efficient adaptation across models\nand datasets. Operating at the token level, IGPG effectively captures complex\nparameter distributions aggregated from a broad spectrum of pretrained models.\nExtensive experiments on multiple vision datasets demonstrate that IGPG\nconsolidates diverse pretrained models into a single, flexible generative\nframework. The synthesized parameters achieve competitive or superior\nperformance relative to state-of-the-art methods, especially in terms of\nscalability and efficiency when applied to large architectures. These results\nunderscore ICPG potential as a powerful tool for pretrained weight retrieval,\nmodel selection, and rapid task-specific fine-tuning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02012.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66189b980da4c017c401fb5d",
      "avatarUrl": "/avatars/d5184741e4a333435022bcb9a4a9b9d8.svg",
      "fullname": "soro bedio",
      "name": "bedio",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.02119",
      "authors": [
        {
          "_id": "67ef41e7efcb0a2fbfbb6a32",
          "user": {
            "_id": "670826649e319cca029ff240",
            "avatarUrl": "/avatars/6d12b3abf75f714d75d1775d88885345.svg",
            "isPro": false,
            "fullname": "rtfvbhkuj",
            "user": "wwdd7718",
            "type": "user"
          },
          "name": "Wang Wei",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-04T02:20:24.253Z",
          "hidden": false
        },
        {
          "_id": "67ef41e7efcb0a2fbfbb6a33",
          "user": {
            "_id": "66e4e50a52356419c4a1ad14",
            "avatarUrl": "/avatars/4be3ce17671785cbe7126b9c1141478b.svg",
            "isPro": false,
            "fullname": "Tiankai Yang",
            "user": "tiankaiy",
            "type": "user"
          },
          "name": "Tiankai Yang",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-04-04T02:25:15.747Z",
          "hidden": false
        },
        {
          "_id": "67ef41e7efcb0a2fbfbb6a34",
          "name": "Hongjie Chen",
          "hidden": false
        },
        {
          "_id": "67ef41e7efcb0a2fbfbb6a35",
          "user": {
            "_id": "62a3ab83e4dd6252344d27cd",
            "avatarUrl": "/avatars/7ca8510f70a58dc207b104240e30c35c.svg",
            "isPro": false,
            "fullname": "Ryan A. Rossi",
            "user": "ryanrossi",
            "type": "user"
          },
          "name": "Ryan A. Rossi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:23:05.421Z",
          "hidden": false
        },
        {
          "_id": "67ef41e7efcb0a2fbfbb6a36",
          "name": "Yue Zhao",
          "hidden": false
        },
        {
          "_id": "67ef41e7efcb0a2fbfbb6a37",
          "user": {
            "_id": "62c5947524171688a9feb992",
            "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
            "isPro": false,
            "fullname": "Franck Dernoncourt",
            "user": "Franck-Dernoncourt",
            "type": "user"
          },
          "name": "Franck Dernoncourt",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-04-04T02:34:51.212Z",
          "hidden": false
        },
        {
          "_id": "67ef41e7efcb0a2fbfbb6a38",
          "name": "Hoda Eldardiry",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-02T20:33:27.000Z",
      "submittedOnDailyAt": "2025-04-04T00:50:35.167Z",
      "title": "時系列予測におけるLLMsを用いた効率的なモデル選択法",
      "submittedOnDailyBy": {
        "_id": "62c5947524171688a9feb992",
        "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
        "isPro": false,
        "fullname": "Franck Dernoncourt",
        "user": "Franck-Dernoncourt",
        "type": "user"
      },
      "summary": "モデル選択は、時系列予測の重要なステップであり、これまでは、多様なデータセットにおいて極めて厳密な性能評価が必要とされました。メタラーニングアプローチはこのプロセスを自動化することを目指していますが、通常は、これらの性能マトリックスの事前構築に依存しており、これは費用が高いです。この研究では、Large Language Models（LLMs）をモデル選択の軽量な代替として活用することを提案しています。我々の方法は、LLMsの固有の知識と推理能力を利用して、明示的な性能マトリックスの必要性を排除します。LLaMA、GPTとGeminiを用いた拡大の実験では、我々のアプローチが傳統的なメタラーニング手法とヒューリスティックベースラインワークを超え、計算オーバーヘッドを大幅に減少することを示しました。これらの発見は、LLMsが時系列予測の効率的なモデル選択における可能性を強調しています。",
      "upvotes": 4,
      "discussionId": "67ef41e8efcb0a2fbfbb6a93",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "model selection",
        "time series forecasting",
        "meta-learning approaches",
        "pre-constructed performance matrices",
        "reasoning capabilities",
        "experiments",
        "LLaMA",
        "GPT",
        "Gemini",
        "heuristic baselines",
        "computational overhead"
      ]
    },
    "publishedAt": "2025-04-02T16:33:27.000Z",
    "title": "Efficient Model Selection for Time Series Forecasting via LLMs",
    "summary": "Model selection is a critical step in time series forecasting, traditionally\nrequiring extensive performance evaluations across various datasets.\nMeta-learning approaches aim to automate this process, but they typically\ndepend on pre-constructed performance matrices, which are costly to build. In\nthis work, we propose to leverage Large Language Models (LLMs) as a lightweight\nalternative for model selection. Our method eliminates the need for explicit\nperformance matrices by utilizing the inherent knowledge and reasoning\ncapabilities of LLMs. Through extensive experiments with LLaMA, GPT and Gemini,\nwe demonstrate that our approach outperforms traditional meta-learning\ntechniques and heuristic baselines, while significantly reducing computational\noverhead. These findings underscore the potential of LLMs in efficient model\nselection for time series forecasting.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02119.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62c5947524171688a9feb992",
      "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
      "fullname": "Franck Dernoncourt",
      "name": "Franck-Dernoncourt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.00891",
      "authors": [
        {
          "_id": "67ef62342a18e60aeee0ea02",
          "name": "Jian Zhao",
          "hidden": false
        },
        {
          "_id": "67ef62342a18e60aeee0ea03",
          "user": {
            "_id": "667187ba9ab144eb3ac43a1b",
            "avatarUrl": "/avatars/db5558aa1c5160b9aee8b58573271959.svg",
            "isPro": false,
            "fullname": "Runze Liu",
            "user": "RyanLiu112",
            "type": "user"
          },
          "name": "Runze Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:08:09.923Z",
          "hidden": false
        },
        {
          "_id": "67ef62342a18e60aeee0ea04",
          "user": {
            "_id": "60bc94cd85a3ab33829b6211",
            "avatarUrl": "/avatars/b57d36c7577fbbb42ea5b963eef4144a.svg",
            "isPro": false,
            "fullname": "Kaiyan Zhang",
            "user": "iseesaw",
            "type": "user"
          },
          "name": "Kaiyan Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:25:22.709Z",
          "hidden": false
        },
        {
          "_id": "67ef62342a18e60aeee0ea05",
          "name": "Zhimu Zhou",
          "hidden": false
        },
        {
          "_id": "67ef62342a18e60aeee0ea06",
          "user": {
            "_id": "67ab05fe4c6ca2d5db4c0c52",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/QpGUNDkeuKjX71s2GXlXF.png",
            "isPro": false,
            "fullname": "Junqi Gao",
            "user": "ChetKao",
            "type": "user"
          },
          "name": "Junqi Gao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:25:38.624Z",
          "hidden": false
        },
        {
          "_id": "67ef62342a18e60aeee0ea07",
          "name": "Dong Li",
          "hidden": false
        },
        {
          "_id": "67ef62342a18e60aeee0ea08",
          "user": {
            "_id": "6562db314e8918182da42706",
            "avatarUrl": "/avatars/b113bbbb496bf4dac254f0e840f08e10.svg",
            "isPro": false,
            "fullname": "Jiafei Lyu",
            "user": "dmux",
            "type": "user"
          },
          "name": "Jiafei Lyu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:25:45.394Z",
          "hidden": false
        },
        {
          "_id": "67ef62342a18e60aeee0ea09",
          "user": {
            "_id": "65b34c5785b6c2144807db37",
            "avatarUrl": "/avatars/4c1cb03cda250d4ec760ebf7815a3bce.svg",
            "isPro": false,
            "fullname": "Qianzhouyi",
            "user": "Saputello",
            "type": "user"
          },
          "name": "Zhouyi Qian",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:26:00.763Z",
          "hidden": false
        },
        {
          "_id": "67ef62342a18e60aeee0ea0a",
          "user": {
            "_id": "645d9c3058f9ee315148116d",
            "avatarUrl": "/avatars/165e18f27b5a50738bf1d22857118478.svg",
            "isPro": false,
            "fullname": "Biqing Qi",
            "user": "jackqi7",
            "type": "user"
          },
          "name": "Biqing Qi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:26:06.517Z",
          "hidden": false
        },
        {
          "_id": "67ef62342a18e60aeee0ea0b",
          "name": "Xiu Li",
          "hidden": false
        },
        {
          "_id": "67ef62342a18e60aeee0ea0c",
          "user": {
            "_id": "669f614b59adf5b56e05bce3",
            "avatarUrl": "/avatars/ffd4189efbceb0e63a03db273065a44b.svg",
            "isPro": false,
            "fullname": "BowenZhou",
            "user": "bowenZhou",
            "type": "user"
          },
          "name": "Bowen Zhou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:26:14.312Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-01T15:21:05.000Z",
      "submittedOnDailyAt": "2025-04-04T03:13:15.991Z",
      "title": "GenPRM: テスト時の計算量を拡大するプロセス報酬モデルにおける生成的な理由論法",
      "submittedOnDailyBy": {
        "_id": "667187ba9ab144eb3ac43a1b",
        "avatarUrl": "/avatars/db5558aa1c5160b9aee8b58573271959.svg",
        "isPro": false,
        "fullname": "Runze Liu",
        "user": "RyanLiu112",
        "type": "user"
      },
      "summary": "最近の大語言モデル（LLMs）の進展により、プロセス報酬モデル（PRMs）を検証器として利用してLLMsの性能を向上させることが望ましいと示されています。しかし、現在のPRMsは3つの重要な課題に直面しています：1）プロセスの制御と一般化能力の限界、2）スカラー値予測に依存し、LLMsの生成能力を活用しない、3）テスト時の計算量のスケーリングできない。本稿では、Chain-of-Thought（CoT）論理とコード検証を行い、各論理ステップに対して判断を提供する生成的プロセス報酬モデルGenPRMを介して、プロセスの制御ラベルと理由データを高品質に得るためには、相対的進歩評価（RPE）と理由の合成フレームワークを提案します。ProcessBenchと数学的論理タスクの実験結果から、GenPRMはMATHデータセットからの23Kの訓練データを使って、先行のPRMsを大幅に超えることが示されました。テスト時のスケーリングにより、1.5BのGenPRMはGPT-4oを超え、7BのGenPRMはProcessBenchでQwen2.5-Math-PRM-72Bを超えました。また、GenPRMは政策モデルの精練のための批判モデルとして強い能力を示しています。本稿は、PRMsと批判モデルとの間の隙間を埋める新しいパラダイムを奨励し、コード、モデル、データはhttps://ryanliu112.github.io/GenPRMにアクセスできます。",
      "upvotes": 4,
      "discussionId": "67ef62352a18e60aeee0ea4b",
      "projectPage": "https://ryanliu112.github.io/GenPRM",
      "githubRepo": "https://github.com/RyanLiu112/GenPRM",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "Process Reward Models (PRMs)",
        "Chain-of-Thought (CoT) reasoning",
        "Relative Progress Estimation (RPE)",
        "ProcessBench",
        "MATH dataset",
        "GPT-4",
        "Qwen2.5-Math-PRM-72B",
        "critic model",
        "policy model refinement"
      ]
    },
    "publishedAt": "2025-04-01T11:21:05.000Z",
    "title": "GenPRM: Scaling Test-Time Compute of Process Reward Models via\n  Generative Reasoning",
    "summary": "Recent advancements in Large Language Models (LLMs) have shown that it is\npromising to utilize Process Reward Models (PRMs) as verifiers to enhance the\nperformance of LLMs. However, current PRMs face three key challenges: (1)\nlimited process supervision and generalization capabilities, (2) dependence on\nscalar value prediction without leveraging the generative abilities of LLMs,\nand (3) inability to scale the test-time compute of PRMs. In this work, we\nintroduce GenPRM, a generative process reward model that performs explicit\nChain-of-Thought (CoT) reasoning with code verification before providing\njudgment for each reasoning step. To obtain high-quality process supervision\nlabels and rationale data, we propose Relative Progress Estimation (RPE) and a\nrationale synthesis framework that incorporates code verification. Experimental\nresults on ProcessBench and several mathematical reasoning tasks show that\nGenPRM significantly outperforms prior PRMs with only 23K training data from\nMATH dataset. Through test-time scaling, a 1.5B GenPRM outperforms GPT-4o, and\na 7B GenPRM surpasses Qwen2.5-Math-PRM-72B on ProcessBench. Additionally,\nGenPRM demonstrates strong abilities to serve as a critic model for policy\nmodel refinement. This work establishes a new paradigm for process supervision\nthat bridges the gap between PRMs and critic models in LLMs. Our code, model,\nand data will be available in https://ryanliu112.github.io/GenPRM.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00891.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "667187ba9ab144eb3ac43a1b",
      "avatarUrl": "/avatars/db5558aa1c5160b9aee8b58573271959.svg",
      "fullname": "Runze Liu",
      "name": "RyanLiu112",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.22444",
      "authors": [
        {
          "_id": "67ef33a4456bcf30fa95b2f1",
          "user": {
            "_id": "655fb8a122ce47e5fa491c72",
            "avatarUrl": "/avatars/d320fe777649b68fbd1372865a2f4def.svg",
            "isPro": false,
            "fullname": "Pengsong Zhang",
            "user": "universea",
            "type": "user"
          },
          "name": "Pengsong Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:09:42.152Z",
          "hidden": false
        },
        {
          "_id": "67ef33a4456bcf30fa95b2f2",
          "name": "Heng Zhang",
          "hidden": false
        },
        {
          "_id": "67ef33a4456bcf30fa95b2f3",
          "name": "Huazhe Xu",
          "hidden": false
        },
        {
          "_id": "67ef33a4456bcf30fa95b2f4",
          "name": "Renjun Xu",
          "hidden": false
        },
        {
          "_id": "67ef33a4456bcf30fa95b2f5",
          "name": "Zhenting Wang",
          "hidden": false
        },
        {
          "_id": "67ef33a4456bcf30fa95b2f6",
          "name": "Cong Wang",
          "hidden": false
        },
        {
          "_id": "67ef33a4456bcf30fa95b2f7",
          "name": "Animesh Garg",
          "hidden": false
        },
        {
          "_id": "67ef33a4456bcf30fa95b2f8",
          "name": "Zhibin Li",
          "hidden": false
        },
        {
          "_id": "67ef33a4456bcf30fa95b2f9",
          "name": "Arash Ajoudani",
          "hidden": false
        },
        {
          "_id": "67ef33a4456bcf30fa95b2fa",
          "name": "Xinyu Liu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/655fb8a122ce47e5fa491c72/d-38XQKRw3vnyFhzZFTEd.jpeg"
      ],
      "publishedAt": "2025-03-28T14:00:27.000Z",
      "submittedOnDailyAt": "2025-04-04T06:29:27.852Z",
      "title": "AIとロボット科学者による科学発見のスケーリング法則",
      "submittedOnDailyBy": {
        "_id": "655fb8a122ce47e5fa491c72",
        "avatarUrl": "/avatars/d320fe777649b68fbd1372865a2f4def.svg",
        "isPro": false,
        "fullname": "Pengsong Zhang",
        "user": "universea",
        "type": "user"
      },
      "summary": "科学発見は、高度なロボット技術と人工知能によって急速に進展することが予想されています。現在の科学研究は、手動的な実験が時間とリソースに費やしがちであり、多様な科学分野の研究は個々の研究者の専門知識の範囲を超えた知識統合が必要となっています。ここで、私たちは自動転換型一般的な科学者（AGS）の概念を想像しています。この概念は、効果的なAIと具象化ロボットを組み合わせて、全研究周期を自動化することを目指しています。このシステムは、物理的およびビュータル環境との動的な相互作用を促進し、多様な科学分野の知識統合を支援することができます。これらのテクノロジーを研究の全ステップからそのまま拡張すること（文献検索、仮説生成、実験、マンシュリー書き上げ）と、内部反省と外部フィードバックの採用により、科学発見に必要な時間とリソースを大幅に削減することを目指しています。バーチャルAIサイエンティストから機能的な一般的なAIベースのロボットサイエンティストへの進化の基礎に立って、AGSはグローバル的な可能性を提供します。これらの自動転換システムが研究プロセスにさらに厳密に組み込まれることを予想し、科学発見が新しいスケーリング法則に従う可能性があり、これは知識の生成と進化を新しい視点から考えることができるものであることを仮定しています。具象化ロボットの極端な環境に対する適応性と、科学知識の積極的な増加によるフリーベル効果は、物理的および知的な境界を超えることを継続的に目指しています。",
      "upvotes": 4,
      "discussionId": "67ef33a5456bcf30fa95b35e",
      "githubRepo": "https://github.com/openags/Awesome-AI-Scientist-Papers"
    },
    "publishedAt": "2025-03-28T10:00:27.000Z",
    "title": "Scaling Laws in Scientific Discovery with AI and Robot Scientists",
    "summary": "Scientific discovery is poised for rapid advancement through advanced\nrobotics and artificial intelligence. Current scientific practices face\nsubstantial limitations as manual experimentation remains time-consuming and\nresource-intensive, while multidisciplinary research demands knowledge\nintegration beyond individual researchers' expertise boundaries. Here, we\nenvision an autonomous generalist scientist (AGS) concept combines agentic AI\nand embodied robotics to automate the entire research lifecycle. This system\ncould dynamically interact with both physical and virtual environments while\nfacilitating the integration of knowledge across diverse scientific\ndisciplines. By deploying these technologies throughout every research stage --\nspanning literature review, hypothesis generation, experimentation, and\nmanuscript writing -- and incorporating internal reflection alongside external\nfeedback, this system aims to significantly reduce the time and resources\nneeded for scientific discovery. Building on the evolution from virtual AI\nscientists to versatile generalist AI-based robot scientists, AGS promises\ngroundbreaking potential. As these autonomous systems become increasingly\nintegrated into the research process, we hypothesize that scientific discovery\nmight adhere to new scaling laws, potentially shaped by the number and\ncapabilities of these autonomous systems, offering novel perspectives on how\nknowledge is generated and evolves. The adaptability of embodied robots to\nextreme environments, paired with the flywheel effect of accumulating\nscientific knowledge, holds the promise of continually pushing beyond both\nphysical and intellectual frontiers.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/655fb8a122ce47e5fa491c72/d-38XQKRw3vnyFhzZFTEd.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.22444.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "655fb8a122ce47e5fa491c72",
      "avatarUrl": "/avatars/d320fe777649b68fbd1372865a2f4def.svg",
      "fullname": "Pengsong Zhang",
      "name": "universea",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.01871",
      "authors": [
        {
          "_id": "67eea9e5117231f8bb04402b",
          "user": {
            "_id": "65d0c00b0954f06e472909f4",
            "avatarUrl": "/avatars/7dd76d922b781ed9895c7f4e62fefd9c.svg",
            "isPro": false,
            "fullname": "tom bush",
            "user": "tuphs",
            "type": "user"
          },
          "name": "Thomas Bush",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-03T19:20:34.085Z",
          "hidden": false
        },
        {
          "_id": "67eea9e5117231f8bb04402c",
          "name": "Stephen Chung",
          "hidden": false
        },
        {
          "_id": "67eea9e5117231f8bb04402d",
          "name": "Usman Anwar",
          "hidden": false
        },
        {
          "_id": "67eea9e5117231f8bb04402e",
          "user": {
            "_id": "645ecd18f0f92653b9f33d4e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645ecd18f0f92653b9f33d4e/nHDMWtM9ZHrji0c4Y4XW1.jpeg",
            "isPro": false,
            "fullname": "Adrià Garriga-Alonso",
            "user": "agaralon",
            "type": "user"
          },
          "name": "Adrià Garriga-Alonso",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-03T15:31:53.577Z",
          "hidden": false
        },
        {
          "_id": "67eea9e5117231f8bb04402f",
          "name": "David Krueger",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-02T16:24:23.000Z",
      "submittedOnDailyAt": "2025-04-04T07:00:29.802Z",
      "title": "モデル無しの強化学習でのエピキュリックプランニングの解釈",
      "submittedOnDailyBy": {
        "_id": "65d0c00b0954f06e472909f4",
        "avatarUrl": "/avatars/7dd76d922b781ed9895c7f4e62fefd9c.svg",
        "isPro": false,
        "fullname": "tom bush",
        "user": "tuphs",
        "type": "user"
      },
      "summary": "ここでは、モデル無しの強化学習アガントが計画を学習することを機械的な証拠として提供します。これは、コンピューターゲーム「ソコバン」でのモデル無しアガントに概念ベースの説明性に基づくメソッドを適用して実現されました。特に、DRC（Guez et al. (2019)によって導入されたジャンルのモデル無しアガント）が学習された概念表現を内部的に計画を構成し、行動の長期的な環境効果を予測し、行動選択に影響を与えることを示します。我々のメソッドは、(1)計画に関係する概念の検出、(2)アガントの表現内での計画形成の調査、(3)発見された計画（アガントの表現内で）がアガントの行動に因果的な影響を与えることを確認する3つのステップから成ることがあります。また、これらの計画の発見は、計画的な性能のようなものの発見と一致し、テスト時の追加コンピューティングの利点を受ける能力として見ることができます。最後に、アガントが学習した計画アルゴリズムについて質的な分析を行い、並列化された双向探索に強い類似性を発見しました。これらの発見は、アガントの計画行動に基づく内部機構の理解を深めることができ、最近のLLMの強化学習による計画と推理能力の現象を理解することが重要です。",
      "upvotes": 3,
      "discussionId": "67eea9e9117231f8bb044167",
      "ai_keywords": [
        "model-free reinforcement learning",
        "concept-based interpretability",
        "Sokoban",
        "DRC",
        "learned concept representations",
        "plan formation",
        "causal effect",
        "parallelized bidirectional search",
        "LLMs",
        "emergent planning",
        "reasoning capabilities"
      ]
    },
    "publishedAt": "2025-04-02T12:24:23.000Z",
    "title": "Interpreting Emergent Planning in Model-Free Reinforcement Learning",
    "summary": "We present the first mechanistic evidence that model-free reinforcement\nlearning agents can learn to plan. This is achieved by applying a methodology\nbased on concept-based interpretability to a model-free agent in Sokoban -- a\ncommonly used benchmark for studying planning. Specifically, we demonstrate\nthat DRC, a generic model-free agent introduced by Guez et al. (2019), uses\nlearned concept representations to internally formulate plans that both predict\nthe long-term effects of actions on the environment and influence action\nselection. Our methodology involves: (1) probing for planning-relevant\nconcepts, (2) investigating plan formation within the agent's representations,\nand (3) verifying that discovered plans (in the agent's representations) have a\ncausal effect on the agent's behavior through interventions. We also show that\nthe emergence of these plans coincides with the emergence of a planning-like\nproperty: the ability to benefit from additional test-time compute. Finally, we\nperform a qualitative analysis of the planning algorithm learned by the agent\nand discover a strong resemblance to parallelized bidirectional search. Our\nfindings advance understanding of the internal mechanisms underlying planning\nbehavior in agents, which is important given the recent trend of emergent\nplanning and reasoning capabilities in LLMs through RL",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.01871.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65d0c00b0954f06e472909f4",
      "avatarUrl": "/avatars/7dd76d922b781ed9895c7f4e62fefd9c.svg",
      "fullname": "tom bush",
      "name": "tuphs",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.02821",
      "authors": [
        {
          "_id": "67ef9a8885ea9d1d7db3fdb5",
          "name": "Mateusz Pach",
          "hidden": false
        },
        {
          "_id": "67ef9a8885ea9d1d7db3fdb6",
          "name": "Shyamgopal Karthik",
          "hidden": false
        },
        {
          "_id": "67ef9a8885ea9d1d7db3fdb7",
          "name": "Quentin Bouniot",
          "hidden": false
        },
        {
          "_id": "67ef9a8885ea9d1d7db3fdb8",
          "name": "Serge Belongie",
          "hidden": false
        },
        {
          "_id": "67ef9a8885ea9d1d7db3fdb9",
          "name": "Zeynep Akata",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-03T17:58:35.000Z",
      "submittedOnDailyAt": "2025-04-04T07:08:47.568Z",
      "title": "稀疏自编码器在视觉语言模型中学習単一意味の特徴",
      "submittedOnDailyBy": {
        "_id": "6254599b6e36fe62e141c8f9",
        "avatarUrl": "/avatars/08d6b68b92c7bfd0a8be022ba9f2f289.svg",
        "isPro": false,
        "fullname": "Shyamgopal Karthik",
        "user": "shyamgopal",
        "type": "user"
      },
      "summary": "Sparse Autoencoders (SAEs) は最近 Large Language Models (LLMs) の解釈性と操作性を向上させることが示されています。本稿では、SAEs の応用範囲を Vision-Language Models (VLMs) に拡張し、視覚表現の単一意味性の評価に適した詳細なフレームワークを提案します。実験結果から、VLMs で訓練された SAEs は個々のニューロンの単一意味性を大幅に向上させ、専門家により定義された構造と一致する階層的表現を示します（例：iNaturalist 分類法）。特に、CLIP の視覚エンコーダーに SAEs を適用し、多タイプモデル（例：LLaVA）の出力を直接操作できることを示しました。これらの発見は、SAEs が VLMs の解釈性と制御性を向上させる無チャレンジの無制限的なアプローチの実用的で効果的であることを強調します。",
      "upvotes": 2,
      "discussionId": "67ef9a8985ea9d1d7db3fe20",
      "ai_keywords": [
        "Sparse Autoencoders (SAEs)",
        "Large Language Models (LLMs)",
        "Vision-Language Models (VLMs)",
        "CLIP",
        "monosemanticity",
        "vision representations",
        "hierarchical representations",
        "iNaturalist taxonomy",
        "multimodal LLMs",
        "LLaVA",
        "unsupervised approach"
      ]
    },
    "publishedAt": "2025-04-03T13:58:35.000Z",
    "title": "Sparse Autoencoders Learn Monosemantic Features in Vision-Language\n  Models",
    "summary": "Sparse Autoencoders (SAEs) have recently been shown to enhance\ninterpretability and steerability in Large Language Models (LLMs). In this\nwork, we extend the application of SAEs to Vision-Language Models (VLMs), such\nas CLIP, and introduce a comprehensive framework for evaluating monosemanticity\nin vision representations. Our experimental results reveal that SAEs trained on\nVLMs significantly enhance the monosemanticity of individual neurons while also\nexhibiting hierarchical representations that align well with expert-defined\nstructures (e.g., iNaturalist taxonomy). Most notably, we demonstrate that\napplying SAEs to intervene on a CLIP vision encoder, directly steer output from\nmultimodal LLMs (e.g., LLaVA) without any modifications to the underlying\nmodel. These findings emphasize the practicality and efficacy of SAEs as an\nunsupervised approach for enhancing both the interpretability and control of\nVLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02821.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6254599b6e36fe62e141c8f9",
      "avatarUrl": "/avatars/08d6b68b92c7bfd0a8be022ba9f2f289.svg",
      "fullname": "Shyamgopal Karthik",
      "name": "shyamgopal",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  }
]