[
  {
    "paper": {
      "id": "2506.04308",
      "authors": [
        {
          "_id": "68424dc48d0422fce0273e99",
          "user": {
            "_id": "63f08dc79cf89c9ed1bb89cd",
            "avatarUrl": "/avatars/37290358ad00bbd752f519cfdec02f3e.svg",
            "isPro": false,
            "fullname": "Zhoues",
            "user": "Zhoues",
            "type": "user"
          },
          "name": "Enshen Zhou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-06T07:41:21.339Z",
          "hidden": false
        },
        {
          "_id": "68424dc48d0422fce0273e9a",
          "name": "Jingkun An",
          "hidden": false
        },
        {
          "_id": "68424dc48d0422fce0273e9b",
          "name": "Cheng Chi",
          "hidden": false
        },
        {
          "_id": "68424dc48d0422fce0273e9c",
          "name": "Yi Han",
          "hidden": false
        },
        {
          "_id": "68424dc48d0422fce0273e9d",
          "name": "Shanyu Rong",
          "hidden": false
        },
        {
          "_id": "68424dc48d0422fce0273e9e",
          "name": "Chi Zhang",
          "hidden": false
        },
        {
          "_id": "68424dc48d0422fce0273e9f",
          "name": "Pengwei Wang",
          "hidden": false
        },
        {
          "_id": "68424dc48d0422fce0273ea0",
          "name": "Zhongyuan Wang",
          "hidden": false
        },
        {
          "_id": "68424dc48d0422fce0273ea1",
          "name": "Tiejun Huang",
          "hidden": false
        },
        {
          "_id": "68424dc48d0422fce0273ea2",
          "name": "Lu Sheng",
          "hidden": false
        },
        {
          "_id": "68424dc48d0422fce0273ea3",
          "name": "Shanghang Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-04T17:59:27.000Z",
      "submittedOnDailyAt": "2025-06-06T00:41:30.786Z",
      "title": "RoboRefer: 空間参照を理由にした視覚言語モデルの向こうへ",
      "submittedOnDailyBy": {
        "_id": "63f08dc79cf89c9ed1bb89cd",
        "avatarUrl": "/avatars/37290358ad00bbd752f519cfdec02f3e.svg",
        "isPro": false,
        "fullname": "Zhoues",
        "user": "Zhoues",
        "type": "user"
      },
      "summary": "空間参照は、具象化ロボットが3次元物理世界と相互作用するための基本的な能力です。しかし、最近のアプローチは、強力な事前学習ビジョン言語モデル（VLMs）を使用しても、複雑な3次元スケーンを正確に理解し、インストラクションに示された場所を動的に理由的に考えることができません。そこで、我々は、SFT（サブジェクトフィードバック調整）を通じて、分離されたものであるものとしての専門的な深度エンコーダを統計的に結合することで、精密な空間理解を実現できる3D適応ビジョン言語モデル（VLM）を提案しています。また、RoboReferは、RFT（強化学習調整）を通じて、メトリックに関連付けられたプロセス報酬関数を用いて、拡張された多ステップ空間理由論を進めます。SFTとRFTの訓練を支援するために、我々は、20M QAペア（先頭の2倍）を持つ大規模なデータセットRefSpatialを導入しました。このデータセットは、31種類の空間関係（先頭の15種類）をカバーし、複雑な理由論プロセス（5ステップまで）をサポートします。また、我々は、RefSpatial-Benchという難しいベンチマークを導入し、多ステップ理由論での空間参照の評価の欠点を埋めました。実験は、SFT訓練されたRoboReferが状態の最先端の空間理解を実現し、平均成功率が89.6%でした。RFT訓練されたRoboReferは、平均精度ではジェミニ-2.5-Proを17.4%より上回り、すべてのベースラインを大幅に上回りました。特に、RoboReferは、多様なロボット（例えばUR5、G1ヒューマノイド）での長期間的、動的なタスクを実行するために、異なるコントロールポリシーと統合できます。",
      "upvotes": 30,
      "discussionId": "68424dc88d0422fce0273fb5",
      "githubRepo": "https://github.com/Zhoues/RoboRefer",
      "ai_summary": "RoboRefer, a 3D-aware vision language model, enhances spatial understanding and multi-step reasoning in embodied robots through supervised and reinforcement fine-tuning, using the RefSpatial dataset and RefSpatial-Bench benchmark.",
      "ai_keywords": [
        "3D-aware VLM",
        "disentangled depth encoder",
        "supervised fine-tuning (SFT)",
        "reinforcement fine-tuning (RFT)",
        "metric-sensitive reward functions",
        "RefSpatial",
        "RefSpatial-Bench",
        "spatial referring tasks",
        "multi-step reasoning",
        "state-of-the-art spatial understanding",
        "long-horizon",
        "dynamic tasks"
      ]
    },
    "publishedAt": "2025-06-04T13:59:27.000Z",
    "title": "RoboRefer: Towards Spatial Referring with Reasoning in Vision-Language\n  Models for Robotics",
    "summary": "Spatial referring is a fundamental capability of embodied robots to interact\nwith the 3D physical world. However, even with the powerful pretrained vision\nlanguage models (VLMs), recent approaches are still not qualified to accurately\nunderstand the complex 3D scenes and dynamically reason about the\ninstruction-indicated locations for interaction. To this end, we propose\nRoboRefer, a 3D-aware VLM that can first achieve precise spatial understanding\nby integrating a disentangled but dedicated depth encoder via supervised\nfine-tuning (SFT). Moreover, RoboRefer advances generalized multi-step spatial\nreasoning via reinforcement fine-tuning (RFT), with metric-sensitive process\nreward functions tailored for spatial referring tasks. To support SFT and RFT\ntraining, we introduce RefSpatial, a large-scale dataset of 20M QA pairs (2x\nprior), covering 31 spatial relations (vs. 15 prior) and supporting complex\nreasoning processes (up to 5 steps). In addition, we introduce\nRefSpatial-Bench, a challenging benchmark filling the gap in evaluating spatial\nreferring with multi-step reasoning. Experiments show that SFT-trained\nRoboRefer achieves state-of-the-art spatial understanding, with an average\nsuccess rate of 89.6%. RFT-trained RoboRefer further outperforms all other\nbaselines by a large margin, even surpassing Gemini-2.5-Pro by 17.4% in average\naccuracy on RefSpatial-Bench. Notably, RoboRefer can be integrated with various\ncontrol policies to execute long-horizon, dynamic tasks across diverse robots\n(e,g., UR5, G1 humanoid) in cluttered real-world scenes.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04308.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "63f08dc79cf89c9ed1bb89cd",
      "avatarUrl": "/avatars/37290358ad00bbd752f519cfdec02f3e.svg",
      "fullname": "Zhoues",
      "name": "Zhoues",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.05301",
      "authors": [
        {
          "_id": "68428f675738dda052f724d3",
          "name": "Jianyi Wang",
          "hidden": false
        },
        {
          "_id": "68428f675738dda052f724d4",
          "name": "Shanchuan Lin",
          "hidden": false
        },
        {
          "_id": "68428f675738dda052f724d5",
          "name": "Zhijie Lin",
          "hidden": false
        },
        {
          "_id": "68428f675738dda052f724d6",
          "name": "Yuxi Ren",
          "hidden": false
        },
        {
          "_id": "68428f675738dda052f724d7",
          "name": "Meng Wei",
          "hidden": false
        },
        {
          "_id": "68428f675738dda052f724d8",
          "name": "Zongsheng Yue",
          "hidden": false
        },
        {
          "_id": "68428f675738dda052f724d9",
          "name": "Shangchen Zhou",
          "hidden": false
        },
        {
          "_id": "68428f675738dda052f724da",
          "name": "Hao Chen",
          "hidden": false
        },
        {
          "_id": "68428f675738dda052f724db",
          "name": "Yang Zhao",
          "hidden": false
        },
        {
          "_id": "68428f675738dda052f724dc",
          "name": "Ceyuan Yang",
          "hidden": false
        },
        {
          "_id": "68428f675738dda052f724dd",
          "name": "Xuefeng Xiao",
          "hidden": false
        },
        {
          "_id": "68428f675738dda052f724de",
          "name": "Chen Change Loy",
          "hidden": false
        },
        {
          "_id": "68428f675738dda052f724df",
          "name": "Lu Jiang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63043db17373aacccd89f49d/OfOznZMmLTV2VmgcLA0fG.mp4"
      ],
      "publishedAt": "2025-06-05T17:51:05.000Z",
      "submittedOnDailyAt": "2025-06-06T06:38:37.104Z",
      "title": "SeedVR2: 1ステップビデオリペイメントによる敵対的なディフュージョン後学習",
      "submittedOnDailyBy": {
        "_id": "63043db17373aacccd89f49d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63043db17373aacccd89f49d/jzP_fPCFXeYJvAD8uA_N7.jpeg",
        "isPro": false,
        "fullname": "JIANYI WANG",
        "user": "Iceclear",
        "type": "user"
      },
      "summary": "最近のディフュージョンベースのビデオリストレーション（VR）の進展は、視覚質量の大幅な向上を示しましたが、推論時の計算コストが高得ないことになります。一方で、数多くのディスタイル化ベースのアプローチは、一ステップ画像リストレーションの可能性を示していますが、現在のアプローチをVRに拡張するのは難しいことであり、特に実世界的な高解像度ビデオでの処理においては、まだ調査が不足しています。本論文では、対戦的なVRトレーニングを行う一ステップディフュージョンベースのVRモデルを提案し、これをSeedVR2と呼びます。高解像度のVRを一ステップで処理するためには、モデル構造とトレーニング手順にも数多くの改良を導入しました。特に、適応的なウィンドウアテンション機構を提案し、出力解像度に合わせてウィンドウサイズを動的に調整し、高解像度のVRでプレディーデフィネドウィンドウサイズを用いたウィンドウアテンションで見落としたウィンドウの不確実性を避けることができます。対戦的なトレーニングの安定化と向上を促進するためには、損失関数の系列の効果を確認し、特にトレーニング効率を大幅に損なわない限りの特に提案した特徴マッチング損失の効果を確認しました。拡張した実験は、SeedVR2は現在のVRアプローチと比較して比較的またはより良い性能を実現できることを示しました。",
      "upvotes": 28,
      "discussionId": "68428f6a5738dda052f72569",
      "projectPage": "https://iceclear.github.io/projects/seedvr2/",
      "githubRepo": "https://github.com/IceClear/SeedVR2",
      "ai_summary": "SeedVR2 is a one-step diffusion-based video restoration model that uses adaptive window attention and feature matching loss to achieve high visual quality with reduced computational cost compared to existing methods.",
      "ai_keywords": [
        "diffusion-based video restoration",
        "VR",
        "adversarial VR training",
        "adaptive window attention",
        "feature matching loss"
      ]
    },
    "publishedAt": "2025-06-05T13:51:05.000Z",
    "title": "SeedVR2: One-Step Video Restoration via Diffusion Adversarial\n  Post-Training",
    "summary": "Recent advances in diffusion-based video restoration (VR) demonstrate\nsignificant improvement in visual quality, yet yield a prohibitive\ncomputational cost during inference. While several distillation-based\napproaches have exhibited the potential of one-step image restoration,\nextending existing approaches to VR remains challenging and underexplored,\nparticularly when dealing with high-resolution video in real-world settings. In\nthis work, we propose a one-step diffusion-based VR model, termed as SeedVR2,\nwhich performs adversarial VR training against real data. To handle the\nchallenging high-resolution VR within a single step, we introduce several\nenhancements to both model architecture and training procedures. Specifically,\nan adaptive window attention mechanism is proposed, where the window size is\ndynamically adjusted to fit the output resolutions, avoiding window\ninconsistency observed under high-resolution VR using window attention with a\npredefined window size. To stabilize and improve the adversarial post-training\ntowards VR, we further verify the effectiveness of a series of losses,\nincluding a proposed feature matching loss without significantly sacrificing\ntraining efficiency. Extensive experiments show that SeedVR2 can achieve\ncomparable or even better performance compared with existing VR approaches in a\nsingle step.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63043db17373aacccd89f49d/OfOznZMmLTV2VmgcLA0fG.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05301.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63043db17373aacccd89f49d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63043db17373aacccd89f49d/jzP_fPCFXeYJvAD8uA_N7.jpeg",
      "fullname": "JIANYI WANG",
      "name": "Iceclear",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 23
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.05284",
      "authors": [
        {
          "_id": "6842929c46106f29d78635ad",
          "name": "Tong Wu",
          "hidden": false
        },
        {
          "_id": "6842929c46106f29d78635ae",
          "name": "Shuai Yang",
          "hidden": false
        },
        {
          "_id": "6842929c46106f29d78635af",
          "name": "Ryan Po",
          "hidden": false
        },
        {
          "_id": "6842929c46106f29d78635b0",
          "name": "Yinghao Xu",
          "hidden": false
        },
        {
          "_id": "6842929c46106f29d78635b1",
          "name": "Ziwei Liu",
          "hidden": false
        },
        {
          "_id": "6842929c46106f29d78635b2",
          "name": "Dahua Lin",
          "hidden": false
        },
        {
          "_id": "6842929c46106f29d78635b3",
          "name": "Gordon Wetzstein",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64b4eec4faa3181a5eab9c46/58qSYbX-_UzJE5ubWpM9W.mp4"
      ],
      "publishedAt": "2025-06-05T17:42:34.000Z",
      "submittedOnDailyAt": "2025-06-06T05:48:31.006Z",
      "title": "Video World Models with Long-term Spatial Memory",
      "submittedOnDailyBy": {
        "_id": "64b4eec4faa3181a5eab9c46",
        "avatarUrl": "/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg",
        "isPro": true,
        "fullname": "Jiaqi Wang",
        "user": "myownskyW7",
        "type": "user"
      },
      "summary": "新しい世界モデルは、カメラの移動やテキストプラントなどの制御信号に対して、順次生成ビデオフレームを行う。時間的なコンテキストウィンドウサイズが限られているため、これらのモデルは再訪問時にシーンの一貫性を維持するのに苦労し、前回生成した環境の忘れが厳しいことが多い。人間の記憶機構をモデル化したものを参考に、長期間の一貫性を高めるために、ビデオモデルの長期間の空間的な記憶を実現するための新しいフレームワークを提案します。このフレームワークは、長期間の空間的な記憶から情報を保存して取り出す機構を含み、3Dメモリ機構を明記して世界モデルを訓練し評価するためのカスタムデータセットを選び出します。評価結果によると、関連するベースラインと比較して、質の向上、一貫性、コンテキストの長さが改善され、長期間的な一貫性の世界生成への道が開かれます。",
      "upvotes": 24,
      "discussionId": "684292a046106f29d7863732",
      "ai_summary": "A new framework enhances video world models' long-term consistency by integrating a geometry-grounded long-term spatial memory mechanism.",
      "ai_keywords": [
        "world models",
        "autoregressive generation",
        "video frames",
        "control signals",
        "temporal context window",
        "scene consistency",
        "long-term spatial memory",
        "custom datasets",
        "3D memory mechanisms"
      ]
    },
    "publishedAt": "2025-06-05T13:42:34.000Z",
    "title": "Video World Models with Long-term Spatial Memory",
    "summary": "Emerging world models autoregressively generate video frames in response to\nactions, such as camera movements and text prompts, among other control\nsignals. Due to limited temporal context window sizes, these models often\nstruggle to maintain scene consistency during revisits, leading to severe\nforgetting of previously generated environments. Inspired by the mechanisms of\nhuman memory, we introduce a novel framework to enhancing long-term consistency\nof video world models through a geometry-grounded long-term spatial memory. Our\nframework includes mechanisms to store and retrieve information from the\nlong-term spatial memory and we curate custom datasets to train and evaluate\nworld models with explicitly stored 3D memory mechanisms. Our evaluations show\nimproved quality, consistency, and context length compared to relevant\nbaselines, paving the way towards long-term consistent world generation.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64b4eec4faa3181a5eab9c46/58qSYbX-_UzJE5ubWpM9W.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05284.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b4eec4faa3181a5eab9c46",
      "avatarUrl": "/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg",
      "fullname": "Jiaqi Wang",
      "name": "myownskyW7",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 20
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.05010",
      "authors": [
        {
          "_id": "6842632d542c9011f1bebf46",
          "user": {
            "_id": "639c379cdb7c5f35004066cb",
            "avatarUrl": "/avatars/3e435506ee85aa7d2d0ec2174a07462f.svg",
            "isPro": false,
            "fullname": "Zhenran Xu",
            "user": "imryanxu",
            "type": "user"
          },
          "name": "Zhenran Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-06T07:40:31.325Z",
          "hidden": false
        },
        {
          "_id": "6842632d542c9011f1bebf47",
          "name": "Xue Yang",
          "hidden": false
        },
        {
          "_id": "6842632d542c9011f1bebf48",
          "name": "Yiyu Wang",
          "hidden": false
        },
        {
          "_id": "6842632d542c9011f1bebf49",
          "name": "Qingli Hu",
          "hidden": false
        },
        {
          "_id": "6842632d542c9011f1bebf4a",
          "name": "Zijiao Wu",
          "hidden": false
        },
        {
          "_id": "6842632d542c9011f1bebf4b",
          "name": "Longyue Wang",
          "hidden": false
        },
        {
          "_id": "6842632d542c9011f1bebf4c",
          "name": "Weihua Luo",
          "hidden": false
        },
        {
          "_id": "6842632d542c9011f1bebf4d",
          "name": "Kaifu Zhang",
          "hidden": false
        },
        {
          "_id": "6842632d542c9011f1bebf4e",
          "name": "Baotian Hu",
          "hidden": false
        },
        {
          "_id": "6842632d542c9011f1bebf4f",
          "name": "Min Zhang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/639c379cdb7c5f35004066cb/wG9VdZ8vQGyG76QYWS4NS.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/639c379cdb7c5f35004066cb/VrBvS8a9t6I462AYXI2uc.png"
      ],
      "publishedAt": "2025-06-05T13:20:50.000Z",
      "submittedOnDailyAt": "2025-06-06T02:21:39.406Z",
      "title": "ComfyUI-Copilot: 自動化ワークフローの智能アシスタントの開発",
      "submittedOnDailyBy": {
        "_id": "639c379cdb7c5f35004066cb",
        "avatarUrl": "/avatars/3e435506ee85aa7d2d0ec2174a07462f.svg",
        "isPro": false,
        "fullname": "Zhenran Xu",
        "user": "imryanxu",
        "type": "user"
      },
      "summary": "ComfyUI-Copilot を紹介します。これは、AI 駆動のアート創作のオープンソースプラットフォームである ComfyUI を強化するために、大規模な言語モデルを機能として持つプラグインです。ComfyUI は、フレックスフレームワークとシンプルな UI を特徴としていますが、新入者にとっては、記述書きの不足、モデルの不正確な設定、ワークフロー設計の複雑性などの課題があります。ComfyUI-Copilot は、これらの課題を解決するために、智能なノードとモデルのリコメンド、ワークフローの自動化ワークフロー構築を提供しています。その核心は、タスクの委託を行う中心的なアシスタントアグエントと、各種使用に対応する専門的なワーカーアグエントを構成したヒューリスティックな多アグエントフレームワークです。これらのアグエントは、我々のカスタマイズされた ComfyUI 知識ベースをもとに、ダブルチェックやデプロイメントをストリーミング化しています。ComfyUI-Copilot の効果性は、オフライン定量評価とオンラインユーザーのフィードバックを通じて証明されています。これにより、ノードの正確なリコメンドとワークフローの開発を加速します。また、使用例は、ComfyUI-Copilot が新入者の入門バリアーを下げ、経験者のワークフローの効率化を促進していることを示しています。ComfyUI-Copilot のインストールパッケージとデモビデオは、https://github.com/AIDC-AI/ComfyUI-Copilot から利用できます。",
      "upvotes": 23,
      "discussionId": "6842632e542c9011f1bebfa3",
      "projectPage": "https://x.com/wangly0229/status/1923515826713526583",
      "githubRepo": "https://github.com/AIDC-AI/ComfyUI-Copilot",
      "ai_summary": "ComfyUI-Copilot uses a large language model and multi-agent system to enhance the usability and efficiency of the AI-driven art creation platform ComfyUI.",
      "ai_keywords": [
        "large language model",
        "multi-agent framework",
        "central assistant agent",
        "specialized worker agents",
        "knowledge bases"
      ]
    },
    "publishedAt": "2025-06-05T09:20:50.000Z",
    "title": "ComfyUI-Copilot: An Intelligent Assistant for Automated Workflow\n  Development",
    "summary": "We introduce ComfyUI-Copilot, a large language model-powered plugin designed\nto enhance the usability and efficiency of ComfyUI, an open-source platform for\nAI-driven art creation. Despite its flexibility and user-friendly interface,\nComfyUI can present challenges to newcomers, including limited documentation,\nmodel misconfigurations, and the complexity of workflow design. ComfyUI-Copilot\naddresses these challenges by offering intelligent node and model\nrecommendations, along with automated one-click workflow construction. At its\ncore, the system employs a hierarchical multi-agent framework comprising a\ncentral assistant agent for task delegation and specialized worker agents for\ndifferent usages, supported by our curated ComfyUI knowledge bases to\nstreamline debugging and deployment. We validate the effectiveness of\nComfyUI-Copilot through both offline quantitative evaluations and online user\nfeedback, showing that it accurately recommends nodes and accelerates workflow\ndevelopment. Additionally, use cases illustrate that ComfyUI-Copilot lowers\nentry barriers for beginners and enhances workflow efficiency for experienced\nusers. The ComfyUI-Copilot installation package and a demo video are available\nat https://github.com/AIDC-AI/ComfyUI-Copilot.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/639c379cdb7c5f35004066cb/wG9VdZ8vQGyG76QYWS4NS.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/639c379cdb7c5f35004066cb/VrBvS8a9t6I462AYXI2uc.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05010.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "639c379cdb7c5f35004066cb",
      "avatarUrl": "/avatars/3e435506ee85aa7d2d0ec2174a07462f.svg",
      "fullname": "Zhenran Xu",
      "name": "imryanxu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.02865",
      "authors": [
        {
          "_id": "683fefbb7ed0da422d1ab676",
          "name": "Mathieu Andreux",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab677",
          "name": "Breno Baldas Skuk",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab678",
          "user": {
            "_id": "6808a8cf6b8c599b583d0fe9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6808a8cf6b8c599b583d0fe9/QPgERtP3Nl2n0BCBlp89D.jpeg",
            "isPro": false,
            "fullname": "Hamza Benchekroun",
            "user": "hamza-hcompany",
            "type": "user"
          },
          "name": "Hamza Benchekroun",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T15:03:30.496Z",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab679",
          "name": "Emilien Biré",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab67a",
          "name": "Antoine Bonnet",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab67b",
          "name": "Riaz Bordie",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab67c",
          "name": "Matthias Brunel",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab67d",
          "name": "Pierre-Louis Cedoz",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab67e",
          "name": "Antoine Chassang",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab67f",
          "name": "Mickaël Chen",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab680",
          "name": "Alexandra D. Constantinou",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab681",
          "name": "Antoine d'Andigné",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab682",
          "name": "Hubert de La Jonquière",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab683",
          "name": "Aurélien Delfosse",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab684",
          "name": "Ludovic Denoyer",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab685",
          "name": "Alexis Deprez",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab686",
          "name": "Augustin Derupti",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab687",
          "name": "Michael Eickenberg",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab688",
          "name": "Mathïs Federico",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab689",
          "name": "Charles Kantor",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab68a",
          "name": "Xavier Koegler",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab68b",
          "name": "Yann Labbé",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab68c",
          "name": "Matthew C. H. Lee",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab68d",
          "name": "Erwan Le Jumeau de Kergaradec",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab68e",
          "name": "Amir Mahla",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab68f",
          "name": "Avshalom Manevich",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab690",
          "name": "Adrien Maret",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab691",
          "name": "Charles Masson",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab692",
          "name": "Rafaël Maurin",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab693",
          "name": "Arturo Mena",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab694",
          "name": "Philippe Modard",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab695",
          "name": "Axel Moyal",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab696",
          "name": "Axel Nguyen Kerbel",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab697",
          "name": "Julien Revelle",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab698",
          "name": "Mats L. Richter",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab699",
          "name": "María Santos",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab69a",
          "name": "Laurent Sifre",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab69b",
          "name": "Maxime Theillard",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab69c",
          "name": "Marc Thibault",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab69d",
          "name": "Louis Thiry",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab69e",
          "name": "Léo Tronchon",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab69f",
          "name": "Nicolas Usunier",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab6a0",
          "user": {
            "_id": "6264f9655f6f2e14d6ac981c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1650784534234-noauth.png",
            "isPro": false,
            "fullname": "Tony Wu",
            "user": "tonywu71",
            "type": "user"
          },
          "name": "Tony Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:28:00.518Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T13:29:03.000Z",
      "submittedOnDailyAt": "2025-06-06T07:18:18.574Z",
      "title": "Surfer-H は、Open Weights を基にした低コストのウェブアウトロードエージェントです。Holo1 との結合により、より効率的なサービスが提供されます。",
      "submittedOnDailyBy": {
        "_id": "6808a8cf6b8c599b583d0fe9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6808a8cf6b8c599b583d0fe9/QPgERtP3Nl2n0BCBlp89D.jpeg",
        "isPro": false,
        "fullname": "Hamza Benchekroun",
        "user": "hamza-hcompany",
        "type": "user"
      },
      "summary": "Surfer-Hは、コスト効率的なウェブアガントです。これは、ユーザーが定義したタスクを行うために、ビジョン言語モデル（VLM）を統合しています。Surfer-Hは、新しい開放ウェイトコレクションのVLM、Holo1と組み合わされています。Holo1は、ウェブナビゲーションと情報抽出に特化したものです。Holo1は、慎重に選択されたデータソースで訓練されています。これには、開放アクセスウェブコンテンツ、合成例、サインプロダクトデータなどが含まれています。Holo1は、一般的なユーザーインターフェース（UI）ベンチマークと新しいウェブUIロケオライゼーションベンチマーク、WebClickで最も優秀な性能を示しています。Holo1をポーチによって、Surfer-HはWebVoyagerで92.2%の最先端の性能を達成し、精度とコスト効率のパロート最適なバランスを調整しています。アガントシステムの研究進歩を加速するために、WebClickの評価データセットとHolo1モデルウェイトを公開しています。",
      "upvotes": 22,
      "discussionId": "683fefbd7ed0da422d1ab718",
      "ai_summary": "Surfer-H, paired with Holo1, an open-weight collection of Vision-Language Models, achieves top performance in web navigation tasks with high cost-efficiency.",
      "ai_keywords": [
        "Vision-Language Models",
        "VLM",
        "web navigation",
        "information extraction",
        "generalist User Interface",
        "UI",
        "WebClick",
        "WebVoyager",
        "open-sourcing",
        "model weights"
      ]
    },
    "publishedAt": "2025-06-03T09:29:03.000Z",
    "title": "Surfer-H Meets Holo1: Cost-Efficient Web Agent Powered by Open Weights",
    "summary": "We present Surfer-H, a cost-efficient web agent that integrates\nVision-Language Models (VLM) to perform user-defined tasks on the web. We pair\nit with Holo1, a new open-weight collection of VLMs specialized in web\nnavigation and information extraction. Holo1 was trained on carefully curated\ndata sources, including open-access web content, synthetic examples, and\nself-produced agentic data. Holo1 tops generalist User Interface (UI)\nbenchmarks as well as our new web UI localization benchmark, WebClick. When\npowered by Holo1, Surfer-H achieves a 92.2% state-of-the-art performance on\nWebVoyager, striking a Pareto-optimal balance between accuracy and\ncost-efficiency. To accelerate research advancement in agentic systems, we are\nopen-sourcing both our WebClick evaluation dataset and the Holo1 model weights.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.02865.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6808a8cf6b8c599b583d0fe9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6808a8cf6b8c599b583d0fe9/QPgERtP3Nl2n0BCBlp89D.jpeg",
      "fullname": "Hamza Benchekroun",
      "name": "hamza-hcompany",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.23656",
      "authors": [
        {
          "_id": "6842520f05049fa51eed0e9f",
          "user": {
            "_id": "656d8d4b1f8d9b618de91369",
            "avatarUrl": "/avatars/884dba9e56936241034b179d11a513b9.svg",
            "isPro": false,
            "fullname": "Xiangdong Zhang",
            "user": "aHapBean",
            "type": "user"
          },
          "name": "Xiangdong Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-06T07:41:15.290Z",
          "hidden": false
        },
        {
          "_id": "6842520f05049fa51eed0ea0",
          "name": "Jiaqi Liao",
          "hidden": false
        },
        {
          "_id": "6842520f05049fa51eed0ea1",
          "name": "Shaofeng Zhang",
          "hidden": false
        },
        {
          "_id": "6842520f05049fa51eed0ea2",
          "name": "Fanqing Meng",
          "hidden": false
        },
        {
          "_id": "6842520f05049fa51eed0ea3",
          "name": "Xiangpeng Wan",
          "hidden": false
        },
        {
          "_id": "6842520f05049fa51eed0ea4",
          "name": "Junchi Yan",
          "hidden": false
        },
        {
          "_id": "6842520f05049fa51eed0ea5",
          "name": "Yu Cheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T17:06:44.000Z",
      "submittedOnDailyAt": "2025-06-06T00:59:51.401Z",
      "title": "VideoREPA: ビデオ生成に向けた関係的なアラインメントを基礎モデルと統合して物理学を学ぶ",
      "submittedOnDailyBy": {
        "_id": "63a2a51ef30c464227924fc6",
        "avatarUrl": "/avatars/e109e85abd25b97bb29dbbe007119e34.svg",
        "isPro": false,
        "fullname": "Haoyu Sun",
        "user": "Mikivis",
        "type": "user"
      },
      "summary": "最近の文字から映像への(T2V) ディフュージョンモデルの進展は、高品質で実感的な映像合成を可能にしました。しかし、現在のT2Vモデルは、物理的に正当な内容を生成することに苦労します。私たちは、T2Vモデル内の表現は物理理解の一部の能力を持っていることを見出しましたが、最近の映像の自動認識学習方法のものよりも显著に落ちていることを認識しました。この点に対して、私たちは、Tokenレベルの関係をアラインして物理理解の能力をT2Vモデルに導入する新しいフレームワークを提案します。これは、物理理解の間違いを閉じ、物理的に正当な生成を可能にします。特に、私たちは、トークン関係の導出(TRD)損失を導入し、空間時間のアラインメントを利用して、強力な事前学習されたT2Vモデルの最適化に適した軟いガイドニングを提供します。これは、先行の表現アラインメント(REPA)方法とは異なる重要な進展です。私たちの知識によると、VideoREPAは、T2Vモデルの最適化に特化し、特に物理的な知識を注入するREPA方法の最初のものであると考えています。実験的な評価は、VideoREPAはベースライン方法、CogVideoXの物理的な常識を大幅に向上させ、関連ベンチマークで顕著な向上を収め、直感的な物理に一致する映像の生成に強い能力を示しました。より多くの映像結果は、https://videorepa.github.io/にあります。",
      "upvotes": 18,
      "discussionId": "6842521205049fa51eed0f67",
      "projectPage": "https://videorepa.github.io/",
      "githubRepo": "https://github.com/aHapBean/VideoREPA",
      "ai_summary": "VideoREPA enhances text-to-video synthesis by aligning token-level relations and distilling physics understanding from foundation models into T2V models.",
      "ai_keywords": [
        "T2V diffusion models",
        "physics understanding",
        "video self-supervised learning",
        "Token Relation Distillation (TRD) loss",
        "spatio-temporal alignment",
        "representation alignment (REPA)",
        "CogVideoX",
        "physics commonsense",
        "intuitive physics"
      ]
    },
    "publishedAt": "2025-05-29T13:06:44.000Z",
    "title": "VideoREPA: Learning Physics for Video Generation through Relational\n  Alignment with Foundation Models",
    "summary": "Recent advancements in text-to-video (T2V) diffusion models have enabled\nhigh-fidelity and realistic video synthesis. However, current T2V models often\nstruggle to generate physically plausible content due to their limited inherent\nability to accurately understand physics. We found that while the\nrepresentations within T2V models possess some capacity for physics\nunderstanding, they lag significantly behind those from recent video\nself-supervised learning methods. To this end, we propose a novel framework\ncalled VideoREPA, which distills physics understanding capability from video\nunderstanding foundation models into T2V models by aligning token-level\nrelations. This closes the physics understanding gap and enable more\nphysics-plausible generation. Specifically, we introduce the Token Relation\nDistillation (TRD) loss, leveraging spatio-temporal alignment to provide soft\nguidance suitable for finetuning powerful pre-trained T2V models, a critical\ndeparture from prior representation alignment (REPA) methods. To our knowledge,\nVideoREPA is the first REPA method designed for finetuning T2V models and\nspecifically for injecting physical knowledge. Empirical evaluations show that\nVideoREPA substantially enhances the physics commonsense of baseline method,\nCogVideoX, achieving significant improvement on relevant benchmarks and\ndemonstrating a strong capacity for generating videos consistent with intuitive\nphysics. More video results are available at https://videorepa.github.io/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23656.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a2a51ef30c464227924fc6",
      "avatarUrl": "/avatars/e109e85abd25b97bb29dbbe007119e34.svg",
      "fullname": "Haoyu Sun",
      "name": "Mikivis",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.05240",
      "authors": [
        {
          "_id": "684249e23fb0b2ecb854594a",
          "user": {
            "_id": "630b094f8b327c7b8b94d24c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/630b094f8b327c7b8b94d24c/Fd0ugLOHJZIi8oUZScOmX.jpeg",
            "isPro": false,
            "fullname": "Yizhuo Li",
            "user": "liyz",
            "type": "user"
          },
          "name": "Yizhuo Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-06T07:41:23.836Z",
          "hidden": false
        },
        {
          "_id": "684249e23fb0b2ecb854594b",
          "name": "Yuying Ge",
          "hidden": false
        },
        {
          "_id": "684249e23fb0b2ecb854594c",
          "name": "Yixiao Ge",
          "hidden": false
        },
        {
          "_id": "684249e23fb0b2ecb854594d",
          "name": "Ying Shan",
          "hidden": false
        },
        {
          "_id": "684249e23fb0b2ecb854594e",
          "name": "Ping Luo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-05T16:59:53.000Z",
      "submittedOnDailyAt": "2025-06-06T00:26:53.631Z",
      "title": "Latentスペースの対位とフロー先頭の対位",
      "submittedOnDailyBy": {
        "_id": "630b094f8b327c7b8b94d24c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/630b094f8b327c7b8b94d24c/Fd0ugLOHJZIi8oUZScOmX.jpeg",
        "isPro": false,
        "fullname": "Yizhuo Li",
        "user": "liyz",
        "type": "user"
      },
      "summary": "この論文は、任意の目標分布に対して学習可能な潜在空間を調整するための新しいフレームワークを提案します。この方法は、フローベースの生成モデルを事前学習した目標特徴量により、潜在空間を正規化します。この固定フローモデルは、潜在空間を最適化ターゲットとして調整するための調整損失を用いて正規化します。我々は、この調整損失の最小化が、目標分布の潜在空間の対数尤度の変分下限を最大化するための計算的に手間のない代理オブジェクトを設定することを正式的に証明します。特に、提案された方法は、計算的に高コストの尤度評価を排除し、最適化中のODE解法を避けます。プロシージャルな証明では、調整損失の関数面が目標分布の負対数尤度を近似していることを示します。また、ImageNet上の多様な目標分布による大規模な画像生成実験を通じて、我々のアプローチの効果を評価し、詳細な議論と消滅実験を行います。理論的および実験的な証明をもって、我々のフレームワークは潜在空間の調整に新たな道を開拓します。",
      "upvotes": 16,
      "discussionId": "684249e73fb0b2ecb8545afb",
      "projectPage": "https://liyizhuo.com/align/",
      "githubRepo": "https://github.com/liyz15/Aligning-Latent-Spaces-with-Flow-Priors",
      "ai_summary": "A novel framework using flow-based generative models aligns learnable latent spaces to target distributions, reducing computational expense and improving log-likelihood maximization.",
      "ai_keywords": [
        "flow-based generative models",
        "latent spaces",
        "alignment loss",
        "flow matching objective",
        "variational lower bound",
        "log-likelihood",
        "ImageNet"
      ]
    },
    "publishedAt": "2025-06-05T12:59:53.000Z",
    "title": "Aligning Latent Spaces with Flow Priors",
    "summary": "This paper presents a novel framework for aligning learnable latent spaces to\narbitrary target distributions by leveraging flow-based generative models as\npriors. Our method first pretrains a flow model on the target features to\ncapture the underlying distribution. This fixed flow model subsequently\nregularizes the latent space via an alignment loss, which reformulates the flow\nmatching objective to treat the latents as optimization targets. We formally\nprove that minimizing this alignment loss establishes a computationally\ntractable surrogate objective for maximizing a variational lower bound on the\nlog-likelihood of latents under the target distribution. Notably, the proposed\nmethod eliminates computationally expensive likelihood evaluations and avoids\nODE solving during optimization. As a proof of concept, we demonstrate in a\ncontrolled setting that the alignment loss landscape closely approximates the\nnegative log-likelihood of the target distribution. We further validate the\neffectiveness of our approach through large-scale image generation experiments\non ImageNet with diverse target distributions, accompanied by detailed\ndiscussions and ablation studies. With both theoretical and empirical\nvalidation, our framework paves a new way for latent space alignment.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05240.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "630b094f8b327c7b8b94d24c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/630b094f8b327c7b8b94d24c/Fd0ugLOHJZIi8oUZScOmX.jpeg",
      "fullname": "Yizhuo Li",
      "name": "liyz",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.05176",
      "authors": [
        {
          "_id": "6842521939f41e76fd96ae38",
          "name": "Yanzhao Zhang",
          "hidden": false
        },
        {
          "_id": "6842521939f41e76fd96ae39",
          "name": "Mingxin Li",
          "hidden": false
        },
        {
          "_id": "6842521939f41e76fd96ae3a",
          "user": {
            "_id": "616adb8578833ce5997e441a",
            "avatarUrl": "/avatars/bf5c04a6032709f35e3fb48e1be6976f.svg",
            "isPro": false,
            "fullname": "Dingkun Long",
            "user": "thenlper",
            "type": "user"
          },
          "name": "Dingkun Long",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-06T07:41:13.249Z",
          "hidden": false
        },
        {
          "_id": "6842521939f41e76fd96ae3b",
          "user": {
            "_id": "63b6dbc8ccebeadccc888456",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1673396893898-63b6dbc8ccebeadccc888456.jpeg",
            "isPro": false,
            "fullname": "Xin Zhang",
            "user": "izhx",
            "type": "user"
          },
          "name": "Xin Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-06T07:41:10.698Z",
          "hidden": false
        },
        {
          "_id": "6842521939f41e76fd96ae3c",
          "name": "Huan Lin",
          "hidden": false
        },
        {
          "_id": "6842521939f41e76fd96ae3d",
          "name": "Baosong Yang",
          "hidden": false
        },
        {
          "_id": "6842521939f41e76fd96ae3e",
          "name": "Pengjun Xie",
          "hidden": false
        },
        {
          "_id": "6842521939f41e76fd96ae3f",
          "name": "An Yang",
          "hidden": false
        },
        {
          "_id": "6842521939f41e76fd96ae40",
          "name": "Dayiheng Liu",
          "hidden": false
        },
        {
          "_id": "6842521939f41e76fd96ae41",
          "name": "Junyang Lin",
          "hidden": false
        },
        {
          "_id": "6842521939f41e76fd96ae42",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "6842521939f41e76fd96ae43",
          "name": "Jingren Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-05T15:49:48.000Z",
      "submittedOnDailyAt": "2025-06-06T01:01:32.740Z",
      "title": "Qwen3 Embedding: 基盤モデルを通じて文の埋め込みと再検索の進歩",
      "submittedOnDailyBy": {
        "_id": "616adb8578833ce5997e441a",
        "avatarUrl": "/avatars/bf5c04a6032709f35e3fb48e1be6976f.svg",
        "isPro": false,
        "fullname": "Dingkun Long",
        "user": "thenlper",
        "type": "user"
      },
      "summary": "この作品では、Qwen3 Embedding シリーズを紹介します。これは、前作のGTE-Qwen シリーズに比べて、文脈埋め込みと再検索の能力に重大な進歩を収めたものです。Qwen3 ベースモデルに基づいて構築されています。Qwen3 LLMs の強力な多言語文脈理解と生成能力を活用し、新たな多段階訓練プイルプインを導入しました。これは、大規模な無制限的な事前訓練と高品質データセットに基づく規制付きの微調節を組み合わせて行います。さらに、効果的なモデル統合戦略が、Qwen3 Embedding シリーズの強固さと適応性を確保しています。訓練プロセス中、Qwen3 LLMs はバックボーンモデルとしての役割を果たし、また、複数の領域と言語の幅広い高品質、豊富な、多様な訓練データの合成に重要な役割を果たしています。Qwen3 Embedding シリーズは、埋め込みと再検索の仕事に対して、モデルサイズの範囲（0.6B、4B、8B）を提供し、ユーザーがエフエクティブさまたはエフィシェンスを最適化できる多様な採用シナリオを扱います。実験的な評価により、Qwen3 Embedding シリーズは多様なベンチマークで最先端の結果を収めています。特に、MTEB の多言語評価ベンチマークでの文脈埋め込みにおいて、そしてコード検索、クロス言語検索、多言語検索などの検索タスクにおいて優れています。再現性の促進とコミュニティ駆動の研究開発の推進を促進するために、Qwen3 Embedding モデルはApache 2.0 ライセンスの下で公開されています。",
      "upvotes": 16,
      "discussionId": "6842521a39f41e76fd96ae6f",
      "projectPage": "https://qwenlm.github.io/blog/qwen3-embedding/",
      "githubRepo": "https://github.com/QwenLM/Qwen3-Embedding",
      "ai_summary": "The Qwen3 Embedding series, built on Qwen3 foundation models, offers advanced text embedding and reranking capabilities through a multi-stage training pipeline, achieving state-of-the-art performance across multilingual and retrieval benchmarks.",
      "ai_keywords": [
        "Qwen3 Embedding series",
        "GTE-Qwen series",
        "Qwen3 LLMs",
        "multilingual text understanding",
        "unsupervised pre-training",
        "supervised fine-tuning",
        "model merging",
        "embedding",
        "reranking",
        "MTEB",
        "code retrieval",
        "cross-lingual retrieval",
        "multilingual retrieval"
      ]
    },
    "publishedAt": "2025-06-05T11:49:48.000Z",
    "title": "Qwen3 Embedding: Advancing Text Embedding and Reranking Through\n  Foundation Models",
    "summary": "In this work, we introduce the Qwen3 Embedding series, a significant\nadvancement over its predecessor, the GTE-Qwen series, in text embedding and\nreranking capabilities, built upon the Qwen3 foundation models. Leveraging the\nQwen3 LLMs' robust capabilities in multilingual text understanding and\ngeneration, our innovative multi-stage training pipeline combines large-scale\nunsupervised pre-training with supervised fine-tuning on high-quality datasets.\nEffective model merging strategies further ensure the robustness and\nadaptability of the Qwen3 Embedding series. During the training process, the\nQwen3 LLMs serve not only as backbone models but also play a crucial role in\nsynthesizing high-quality, rich, and diverse training data across multiple\ndomains and languages, thus enhancing the training pipeline. The Qwen3\nEmbedding series offers a spectrum of model sizes (0.6B, 4B, 8B) for both\nembedding and reranking tasks, addressing diverse deployment scenarios where\nusers can optimize for either efficiency or effectiveness. Empirical\nevaluations demonstrate that the Qwen3 Embedding series achieves\nstate-of-the-art results across diverse benchmarks. Notably, it excels on the\nmultilingual evaluation benchmark MTEB for text embedding, as well as in\nvarious retrieval tasks, including code retrieval, cross-lingual retrieval and\nmultilingual retrieval. To facilitate reproducibility and promote\ncommunity-driven research and development, the Qwen3 Embedding models are\npublicly available under the Apache 2.0 license.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05176.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "616adb8578833ce5997e441a",
      "avatarUrl": "/avatars/bf5c04a6032709f35e3fb48e1be6976f.svg",
      "fullname": "Dingkun Long",
      "name": "thenlper",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 96
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.04633",
      "authors": [
        {
          "_id": "68426296b8d07a60074e866a",
          "name": "Linjie Li",
          "hidden": false
        },
        {
          "_id": "68426296b8d07a60074e866b",
          "name": "Mahtab Bigverdi",
          "hidden": false
        },
        {
          "_id": "68426296b8d07a60074e866c",
          "user": {
            "_id": "645b4819f9d4ec91fdd54852",
            "avatarUrl": "/avatars/e12efb8e030688a0afcc72176b453fb3.svg",
            "isPro": false,
            "fullname": "Jiawei Gu",
            "user": "kuvvi",
            "type": "user"
          },
          "name": "Jiawei Gu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-06T07:40:38.249Z",
          "hidden": false
        },
        {
          "_id": "68426296b8d07a60074e866d",
          "name": "Zixian Ma",
          "hidden": false
        },
        {
          "_id": "68426296b8d07a60074e866e",
          "name": "Yinuo Yang",
          "hidden": false
        },
        {
          "_id": "68426296b8d07a60074e866f",
          "name": "Ziang Li",
          "hidden": false
        },
        {
          "_id": "68426296b8d07a60074e8670",
          "name": "Yejin Choi",
          "hidden": false
        },
        {
          "_id": "68426296b8d07a60074e8671",
          "name": "Ranjay Krishna",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-05T05:09:46.000Z",
      "submittedOnDailyAt": "2025-06-06T02:08:57.401Z",
      "title": "スペクトラル認知の展開：ビジュアルシミュレーション上での多モーダルモデルの評価",
      "submittedOnDailyBy": {
        "_id": "645b4819f9d4ec91fdd54852",
        "avatarUrl": "/avatars/e12efb8e030688a0afcc72176b453fb3.svg",
        "isPro": false,
        "fullname": "Jiawei Gu",
        "user": "kuvvi",
        "type": "user"
      },
      "summary": "空間認知は人類の知能の重要な部分であり、問題解決を視覚的シミュレーションによって行うことで、語的推理によるみたいなことを主に依存しないようにしています。しかし、現在のAIベンチマークは主に語的推理を評価し、非語的、多ステップの視覚的シミュレーションの複雑性を間違えています。私たちは、STARE（空間変換と理由論の評価）を紹介します。STAREは、多ステップの視覚的シミュレーションを通じて解決できるようなタスクで厳密に多タイプの大語言モデルを評価するベンチマークです。STAREは、基盤的な幾何的変換（2次元と3次元）、統合的な空間的理由論（キューブネット折り方と唐詰ぼしパズル）、実世界的空間的理由論（視点と時間的理由論）を含む4Kタスクを特徴付けています。これらのタスクは、物体の組み立て、機械図の解釈、日常の空間的移動などの実用的な認知的課題を反映しています。私たちの評価によると、モデルは簡単な2次元変換の理由論に優れていますが、3次元キューブネット折り方と唐詰ぼしパズルのような複雑なタスクでは、多ステップの視覚的シミュレーションが必要な場合にその近似の乱数の運命のように行動します。人間は近準確な精度を達成しますが、複雑なタスクにおいては相当の時間（最高28.9秒）を要しますが、中間的な視覚的シミュレーションを使用することで平均で7.5秒速くなります。反対に、モデルは視覚的シミュレーションからの性能の向上は不確実で、多くのタスクで改善しますが、特定のケースでは、唐詰ぼしパズル（GPT-4o, o1）とキューブネット折り方（Claude-3.5, Gemini-2.0 Flash）では減少します。これは、モデルが中間的な視覚的情報を効果的に活用する方法がないことを示しています。",
      "upvotes": 15,
      "discussionId": "68426298b8d07a60074e86eb",
      "projectPage": "https://huggingface.co/datasets/kuvvi/STARE",
      "githubRepo": "https://github.com/STARE-bench/STARE/",
      "ai_summary": "A new benchmark evaluates multimodal models on visual simulation tasks, revealing varying model performances compared to human accuracy and the impact of intermediate visual simulations.",
      "ai_keywords": [
        "spatial cognition",
        "visual simulations",
        "verbal reasoning",
        "multimodal large language models",
        "STARE",
        "spatial transformations",
        "geometric transformations",
        "integrated spatial reasoning",
        "real-world spatial reasoning",
        "visual reasoning",
        "intermediate visual simulations"
      ]
    },
    "publishedAt": "2025-06-05T01:09:46.000Z",
    "title": "Unfolding Spatial Cognition: Evaluating Multimodal Models on Visual\n  Simulations",
    "summary": "Spatial cognition is essential for human intelligence, enabling\nproblem-solving through visual simulations rather than solely relying on verbal\nreasoning. However, existing AI benchmarks primarily assess verbal reasoning,\nneglecting the complexities of non-verbal, multi-step visual simulation. We\nintroduce STARE(Spatial Transformations and Reasoning Evaluation), a benchmark\ndesigned to rigorously evaluate multimodal large language models on tasks\nbetter solved through multi-step visual simulation. STARE features 4K tasks\nspanning foundational geometric transformations (2D and 3D), integrated spatial\nreasoning (cube net folding and tangram puzzles), and real-world spatial\nreasoning (perspective and temporal reasoning), reflecting practical cognitive\nchallenges like object assembly, mechanical diagram interpretation, and\neveryday spatial navigation. Our evaluations show that models excel at\nreasoning over simpler 2D transformations, but perform close to random chance\non more complex tasks like 3D cube net folding and tangram puzzles that require\nmulti-step visual simulations. Humans achieve near-perfect accuracy but take\nconsiderable time (up to 28.9s) on complex tasks, significantly speeding up\n(down by 7.5 seconds on average) with intermediate visual simulations. In\ncontrast, models exhibit inconsistent performance gains from visual\nsimulations, improving on most tasks but declining in specific cases like\ntangram puzzles (GPT-4o, o1) and cube net folding (Claude-3.5, Gemini-2.0\nFlash), indicating that models may not know how to effectively leverage\nintermediate visual information.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04633.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645b4819f9d4ec91fdd54852",
      "avatarUrl": "/avatars/e12efb8e030688a0afcc72176b453fb3.svg",
      "fullname": "Jiawei Gu",
      "name": "kuvvi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.05344",
      "authors": [
        {
          "_id": "68424fe9bdc448822b31beac",
          "name": "Jiahui Wang",
          "hidden": false
        },
        {
          "_id": "68424fe9bdc448822b31bead",
          "user": {
            "_id": "64f001bfabd9fb1914398bd5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f001bfabd9fb1914398bd5/9teH82hkBI4csIz_WQh5q.jpeg",
            "isPro": false,
            "fullname": "liuzuyan",
            "user": "Zuyan",
            "type": "user"
          },
          "name": "Zuyan Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-06T07:41:17.520Z",
          "hidden": false
        },
        {
          "_id": "68424fe9bdc448822b31beae",
          "name": "Yongming Rao",
          "hidden": false
        },
        {
          "_id": "68424fe9bdc448822b31beaf",
          "name": "Jiwen Lu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-05T17:59:55.000Z",
      "submittedOnDailyAt": "2025-06-06T00:48:21.379Z",
      "title": "SparseMM: ヘッドのスパース性は、MLLMの視覚概念の応答から現れる",
      "submittedOnDailyBy": {
        "_id": "64f001bfabd9fb1914398bd5",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f001bfabd9fb1914398bd5/9teH82hkBI4csIz_WQh5q.jpeg",
        "isPro": false,
        "fullname": "liuzuyan",
        "user": "Zuyan",
        "type": "user"
      },
      "summary": "多モダル大語言モデル（MLLMs）は、ビジュアル能力を加えた予習された大語言モデル（LLMs）から一般的に得られます。本研究では、MLLMsがビジュアル入力をどのように処理しているかを調べ、注意機構を分析しています。その結果、驚くべき稀疏性現象が明らかになりました：LLMsの注意ヘッドの中でビジュアル理解に活躍しているヘッドは、約5%未満の小さな部分です。これらのヘッドを効率的に特定するために、我々は、特定のレスポンス分析によってヘッドレベルのビジュアル関連性を定量化するトレーニング無制限フレームワークを設計しました。この発見に基づき、我々は、ビジュアルスコアに基づいてLLMsのヘッドに不均衡な計算バジョンを割り当てるKV-Cache最適化スティラテジ、SparseMMを紹介しました。これは、ビジュアルヘッドの稀疏性を利用してMLLMの推論を加速することを目的としています。先行のKV-Cache加速方法と比較して、SparseMMはビジュアルの特徴を無視している点に注目し、解像力と記憶領域の削減を優先しています。主流の多モダルベンチマークでの拡張的な評価により、SparseMMは精度と効率のバランスを優れています。特に、SparseMMは性能の対等性を維持する同時に、生成時に1.38倍の実時間加速と52%のメモリ削減を実現しました。我々のプロジェクトは、https://github.com/CR400AF-A/SparseMM でオープンソースに提供されています。",
      "upvotes": 14,
      "discussionId": "68424febbdc448822b31bf2c",
      "projectPage": "https://cr400af-a.github.io/SparseMM/",
      "githubRepo": "https://github.com/CR400AF-A/SparseMM",
      "ai_summary": "MLLLMs achieve enhanced efficiency through SparseMM, a KV-Cache optimization strategy that identifies and prioritizes visual heads, leading to significant real-time acceleration and memory reduction without compromising performance.",
      "ai_keywords": [
        "multimodal large language models",
        "LLMs",
        "visual capabilities",
        "attention mechanisms",
        "visual heads",
        "targeted response analysis",
        "KV-Cache optimization",
        "SparseMM",
        "head-level visual relevance",
        "visual semantics",
        "multimodal benchmarks"
      ]
    },
    "publishedAt": "2025-06-05T13:59:55.000Z",
    "title": "SparseMM: Head Sparsity Emerges from Visual Concept Responses in MLLMs",
    "summary": "Multimodal Large Language Models (MLLMs) are commonly derived by extending\npre-trained Large Language Models (LLMs) with visual capabilities. In this\nwork, we investigate how MLLMs process visual inputs by analyzing their\nattention mechanisms. We reveal a surprising sparsity phenomenon: only a small\nsubset (approximately less than 5%) of attention heads in LLMs actively\ncontribute to visual understanding, termed visual heads. To identify these\nheads efficiently, we design a training-free framework that quantifies\nhead-level visual relevance through targeted response analysis. Building on\nthis discovery, we introduce SparseMM, a KV-Cache optimization strategy that\nallocates asymmetric computation budgets to heads in LLMs based on their visual\nscores, leveraging the sparity of visual heads for accelerating the inference\nof MLLMs. Compared with prior KV-Cache acceleration methods that ignore the\nparticularity of visual, SparseMM prioritizes stress and retaining visual\nsemantics during decoding. Extensive evaluations across mainstream multimodal\nbenchmarks demonstrate that SparseMM achieves superior accuracy-efficiency\ntrade-offs. Notably, SparseMM delivers 1.38x real-time acceleration and 52%\nmemory reduction during generation while maintaining performance parity on\nefficiency test. Our project is open sourced at\nhttps://github.com/CR400AF-A/SparseMM.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05344.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "64f001bfabd9fb1914398bd5",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f001bfabd9fb1914398bd5/9teH82hkBI4csIz_WQh5q.jpeg",
      "fullname": "liuzuyan",
      "name": "Zuyan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.05331",
      "authors": [
        {
          "_id": "684260765bfed1b94a9cc307",
          "user": {
            "_id": "647c7a4ed412b3b376572a00",
            "avatarUrl": "/avatars/9cc310fd3f9e3f211475816ed9b0cdaa.svg",
            "isPro": false,
            "fullname": "Xinyan Chen",
            "user": "xy06",
            "type": "user"
          },
          "name": "Xinyan Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-06T07:40:42.546Z",
          "hidden": false
        },
        {
          "_id": "684260765bfed1b94a9cc308",
          "name": "Renrui Zhang",
          "hidden": false
        },
        {
          "_id": "684260765bfed1b94a9cc309",
          "user": {
            "_id": "6349214f8146350b3a4c5cdf",
            "avatarUrl": "/avatars/cfd24caac9a87efb528d0f4c375932bc.svg",
            "isPro": false,
            "fullname": "Dongzhi Jiang",
            "user": "CaraJ",
            "type": "user"
          },
          "name": "Dongzhi Jiang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-06T07:40:40.308Z",
          "hidden": false
        },
        {
          "_id": "684260765bfed1b94a9cc30a",
          "name": "Aojun Zhou",
          "hidden": false
        },
        {
          "_id": "684260765bfed1b94a9cc30b",
          "name": "Shilin Yan",
          "hidden": false
        },
        {
          "_id": "684260765bfed1b94a9cc30c",
          "name": "Weifeng Lin",
          "hidden": false
        },
        {
          "_id": "684260765bfed1b94a9cc30d",
          "name": "Hongsheng Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-05T17:59:02.000Z",
      "submittedOnDailyAt": "2025-06-06T02:07:44.094Z",
      "title": "MINT-CoT: 数学のChain-of-Thought Reasoningでの間接ビジュアルトークンの使用を可能にする",
      "submittedOnDailyBy": {
        "_id": "647c7a4ed412b3b376572a00",
        "avatarUrl": "/avatars/9cc310fd3f9e3f211475816ed9b0cdaa.svg",
        "isPro": false,
        "fullname": "Xinyan Chen",
        "user": "xy06",
        "type": "user"
      },
      "summary": "Chain-of-Thought (CoT)は、大規模言語モデル（LLMs）の数学的推理により広く効果的に改善されていますが、多タイプ領域に拡張することは難しいです。現在の研究は、画像入力に対して同様の文脈的推理を採用しているか、数学コンテンツに対する視覚エンコーダーの視覚認識の限界を克服するために視覚信号を数学コツの間に挟むことを試みています。しかし、数学問題解決において、3つの主要な制限があります：粗粒さの画像領域の依存関係、視覚エンコーダーによる数学コンテンツの認識の限界、視覚変更の外部能力の依存関係です。本論文では、数学的INterleaved Tokensを用いたChain-of-Thoughtの視覚推理を導入し、MINT-CoTとして紹介します。MINT-CoTは、Interleave Tokenを用いて、数学図形内の任意の形状の視覚領域を動的に選択して、文脈的推理ステップに適切な視覚トークンを間引きします。この能力を実現するために、MINT-CoTデータセットを構築し、54Kの数学問題を含み、各推理ステップに対してトークンレベルでの視覚領域と対応し、厳密なデータ生成プイルプリンを伴います。また、MINT-CoT-7Bモデルを構築するために、文脳だけのCoT SFT、間引きされたCoT SFT、間引きされたCoT RLの3段階的トレーニング戦略を提出します。拡張された実験は、数学領域での有効な視覚間引き推理の効果を示し、MINT-CoT-7Bは、MathVistaでは+34.08%、GeoQAでは+28.78%、MMStarでは+23.2%の効果を示します。コードとデータは、https://github.com/xinyan-cxy/MINT-CoTに公開されています。",
      "upvotes": 12,
      "discussionId": "684260775bfed1b94a9cc346",
      "githubRepo": "https://github.com/xinyan-cxy/MINT-CoT",
      "ai_summary": "MINT-CoT enhances multimodal mathematical reasoning by interleaving visual tokens into textual chain-of-thought steps, enabling flexible visual perception and improved problem-solving.",
      "ai_keywords": [
        "Chain-of-Thought",
        "Large Language Models",
        "multimodal domains",
        "textual reasoning",
        "visual signals",
        "image input",
        "vision encoders",
        "math content",
        "visual modification",
        "Mathematical INterleaved Tokens",
        "Interleave Token",
        "visual regions",
        "token level",
        "MINT-CoT dataset",
        "text-only CoT SFT",
        "interleaved CoT SFT",
        "interleaved CoT RL",
        "MINT-CoT-7B",
        "MathVista",
        "GeoQA",
        "MMStar"
      ]
    },
    "publishedAt": "2025-06-05T13:59:02.000Z",
    "title": "MINT-CoT: Enabling Interleaved Visual Tokens in Mathematical\n  Chain-of-Thought Reasoning",
    "summary": "Chain-of-Thought (CoT) has widely enhanced mathematical reasoning in Large\nLanguage Models (LLMs), but it still remains challenging for extending it to\nmultimodal domains. Existing works either adopt a similar textual reasoning for\nimage input, or seek to interleave visual signals into mathematical CoT.\nHowever, they face three key limitations for math problem-solving: reliance on\ncoarse-grained box-shaped image regions, limited perception of vision encoders\non math content, and dependence on external capabilities for visual\nmodification. In this paper, we propose MINT-CoT, introducing Mathematical\nINterleaved Tokens for Chain-of-Thought visual reasoning. MINT-CoT adaptively\ninterleaves relevant visual tokens into textual reasoning steps via an\nInterleave Token, which dynamically selects visual regions of any shapes within\nmath figures. To empower this capability, we construct the MINT-CoT dataset,\ncontaining 54K mathematical problems aligning each reasoning step with visual\nregions at the token level, accompanied by a rigorous data generation pipeline.\nWe further present a three-stage MINT-CoT training strategy, progressively\ncombining text-only CoT SFT, interleaved CoT SFT, and interleaved CoT RL, which\nderives our MINT-CoT-7B model. Extensive experiments demonstrate the\neffectiveness of our method for effective visual interleaved reasoning in\nmathematical domains, where MINT-CoT-7B outperforms the baseline model by\n+34.08% on MathVista, +28.78% on GeoQA, and +23.2% on MMStar, respectively. Our\ncode and data are available at https://github.com/xinyan-cxy/MINT-CoT",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05331.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "647c7a4ed412b3b376572a00",
      "avatarUrl": "/avatars/9cc310fd3f9e3f211475816ed9b0cdaa.svg",
      "fullname": "Xinyan Chen",
      "name": "xy06",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.05328",
      "authors": [
        {
          "_id": "68424822f0c91a7dcb64193b",
          "user": {
            "_id": "64a3de701698ad2985277148",
            "avatarUrl": "/avatars/09eebadbbea53ed2800591564ff5c931.svg",
            "isPro": false,
            "fullname": "lulidong",
            "user": "lulidong",
            "type": "user"
          },
          "name": "Lidong Lu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-06T07:41:34.755Z",
          "hidden": false
        },
        {
          "_id": "68424822f0c91a7dcb64193c",
          "user": {
            "_id": "6392c73390b8e99a6779a7b0",
            "avatarUrl": "/avatars/9ff824ab02848120aec5e8de6780bcf1.svg",
            "isPro": false,
            "fullname": "Guo Chen",
            "user": "cg1177",
            "type": "user"
          },
          "name": "Guo Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-06T07:41:32.158Z",
          "hidden": false
        },
        {
          "_id": "68424822f0c91a7dcb64193d",
          "name": "Zhiqi Li",
          "hidden": false
        },
        {
          "_id": "68424822f0c91a7dcb64193e",
          "name": "Yicheng Liu",
          "hidden": false
        },
        {
          "_id": "68424822f0c91a7dcb64193f",
          "name": "Tong Lu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-05T17:58:33.000Z",
      "submittedOnDailyAt": "2025-06-06T00:16:44.777Z",
      "title": "AV-Reasoner: クールガイドされた音声-視覚の数え上げを改善し、MLLMにおけるベンチマークを行う",
      "submittedOnDailyBy": {
        "_id": "64a3de701698ad2985277148",
        "avatarUrl": "/avatars/09eebadbbea53ed2800591564ff5c931.svg",
        "isPro": false,
        "fullname": "lulidong",
        "user": "lulidong",
        "type": "user"
      },
      "summary": "電視理解の進歩に伴い、現在のMLLMは数え上げタスクに苦戦しています。既存のベンチマークは、短い電視、近接セットのクエリ、カテゴリーの欠如、および弱い多モデルカバレーを受け続けています。本論文では、CG-AV-Countingという、1,027件の多モデルクエスタンと5,845件のカテゴリー付けされたカラーベースの数え上げベンチマークを紹介します。これは、黒箱評価と白箱評価をサポートし、エンドツーエンドや理由基づきの数え上げのための機能を確認するための厳密なテストベンチマークです。数え上げ能力を向上させる方法を探求するために、GRPOとカレクルラーニングを用いて訓練されたAV-Reasonerモデルを提案します。AV-Reasonerは複数のベンチマークで最先端の結果を収め、強化学習の効果性を示します。しかし、ドメイン外のベンチマークでの実験は、言語空間での理由をもつことが性能向上に役立つことを示しません。コードとベンチマークは、https://av-reasoner.github.ioに公開されています。",
      "upvotes": 12,
      "discussionId": "68424823f0c91a7dcb6419c7",
      "projectPage": "https://AV-Reasoner.github.io",
      "githubRepo": "https://github.com/AV-Reasoner/AV-Reasoner",
      "ai_summary": "CG-AV-Counting is a new benchmark for video counting tasks that includes multimodal data and supports end-to-end and reasoning-based models. AV-Reasoner, trained with GRPO and curriculum learning, achieves top results but shows limitations on out-of-domain tasks.",
      "ai_keywords": [
        "MLLMs",
        "CG-AV-Counting",
        "multimodal questions",
        "clue-grounded",
        "black-box evaluation",
        "white-box evaluation",
        "AV-Reasoner",
        "GRPO",
        "curriculum learning",
        "reinforcement learning",
        "out-of-domain benchmarks"
      ]
    },
    "publishedAt": "2025-06-05T13:58:33.000Z",
    "title": "AV-Reasoner: Improving and Benchmarking Clue-Grounded Audio-Visual\n  Counting for MLLMs",
    "summary": "Despite progress in video understanding, current MLLMs struggle with counting\ntasks. Existing benchmarks are limited by short videos, close-set queries, lack\nof clue annotations, and weak multimodal coverage. In this paper, we introduce\nCG-AV-Counting, a manually-annotated clue-grounded counting benchmark with\n1,027 multimodal questions and 5,845 annotated clues over 497 long videos. It\nsupports both black-box and white-box evaluation, serving as a comprehensive\ntestbed for both end-to-end and reasoning-based counting. To explore ways to\nimprove model's counting capability, we propose AV-Reasoner, a model trained\nwith GRPO and curriculum learning to generalize counting ability from related\ntasks. AV-Reasoner achieves state-of-the-art results across multiple\nbenchmarks, demonstrating the effectiveness of reinforcement learning. However,\nexperiments show that on out-of-domain benchmarks, reasoning in the language\nspace fails to bring performance gains. The code and benchmark have been\nrealeased on https://av-reasoner.github.io.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05328.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64a3de701698ad2985277148",
      "avatarUrl": "/avatars/09eebadbbea53ed2800591564ff5c931.svg",
      "fullname": "lulidong",
      "name": "lulidong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.03077",
      "authors": [
        {
          "_id": "683fc07a1de14546d5decf19",
          "name": "Qijun Luo",
          "hidden": false
        },
        {
          "_id": "683fc07a1de14546d5decf1a",
          "user": {
            "_id": "65a521af90b5e87bcd343828",
            "avatarUrl": "/avatars/3bbe83c1ba47df17d9c05a049147e5cc.svg",
            "isPro": false,
            "fullname": "Mengqi Li",
            "user": "Kullpar",
            "type": "user"
          },
          "name": "Mengqi Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:55:10.607Z",
          "hidden": false
        },
        {
          "_id": "683fc07a1de14546d5decf1b",
          "name": "Lei Zhao",
          "hidden": false
        },
        {
          "_id": "683fc07a1de14546d5decf1c",
          "name": "Xiao Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T16:54:15.000Z",
      "submittedOnDailyAt": "2025-06-06T02:05:55.529Z",
      "title": "StreamBP: 長シークエンスの訓練におけるメモリ効率的な正確な反復計算法",
      "submittedOnDailyBy": {
        "_id": "65a521af90b5e87bcd343828",
        "avatarUrl": "/avatars/3bbe83c1ba47df17d9c05a049147e5cc.svg",
        "isPro": false,
        "fullname": "Mengqi Li",
        "user": "Kullpar",
        "type": "user"
      },
      "summary": "長シークエンスデータを用いて言語モデルを訓練することは、複雑なタスク（例：長鎖論理）におけるモデルの能力を向上させるための要求ですが、シークエンスの長さが増えるにつれ、バックプロパゲーション（BP）プロセス中の活性値のメモリコストが大きくなります。この課題を解決するために、StreamBPというメモリ効率的なエキシサイトBPメソッドを提案します。StreamBPは、シークエンス次元に沿って鎖ロールの線形分解を行い、活性値とロジットのメモリコストを大幅に削減します。提案された方法はSFT、GRPO、DPOなどの一般的なオブジェクティブに適用可能です。実装の観点から、StreamBPは言語モデルの因果構造を活用して計算コストのFLOPsを減少させ、BP速度を高速化します。Gradient Checkpointingと比較して、StreamBPはBPの最大シークエンス長を2.8～5.5倍に増やし、BP時間が比較的または少なくなります。StreamBPのシークエンス長スケーリング能力は、トレーニングを加速するためにバッチサイズスケーリングに直接移転できます。また、コミュニケーション効率的な分散StreamBPを開発し、多GPUトレーニングを効果的に支援し、適用範囲を広げました。我々のコードは、任意のTransformerモデルのトレーニングパイプラインに簡単に統合可能で、https://github.com/Ledzy/StreamBPで利用可能です。",
      "upvotes": 12,
      "discussionId": "683fc07e1de14546d5decfe2",
      "githubRepo": "https://github.com/Ledzy/StreamBP",
      "ai_summary": "StreamBP, a memory-efficient and exact backpropagation method, decomposes the chain rule to reduce memory costs, enabling longer sequence lengths and faster training speeds for language models compared to gradient checkpointing.",
      "ai_keywords": [
        "backpropagation (BP)",
        "memory-efficient",
        "exact BP",
        "gradient checkpointing",
        "chain rule",
        "sequence dimension",
        "layer-wise",
        "activation values",
        "logits",
        "SFT",
        "GRPO",
        "DPO",
        "computational FLOPs",
        "BP speed",
        "causal structure",
        "language model",
        "multi-GPU training",
        "distributed StreamBP"
      ]
    },
    "publishedAt": "2025-06-03T12:54:15.000Z",
    "title": "StreamBP: Memory-Efficient Exact Backpropagation for Long Sequence\n  Training of LLMs",
    "summary": "Training language models on long sequence data is a demanding requirement for\nenhancing the model's capability on complex tasks, e.g., long-chain reasoning.\nHowever, as the sequence length scales up, the memory cost for storing\nactivation values becomes huge during the Backpropagation (BP) process, even\nwith the application of gradient checkpointing technique. To tackle this\nchallenge, we propose a memory-efficient and exact BP method called StreamBP,\nwhich performs a linear decomposition of the chain rule along the sequence\ndimension in a layer-wise manner, significantly reducing the memory cost of\nactivation values and logits. The proposed method is applicable to common\nobjectives such as SFT, GRPO, and DPO. From an implementation perspective,\nStreamBP achieves less computational FLOPs and faster BP speed by leveraging\nthe causal structure of the language model. Compared to gradient checkpointing,\nStreamBP scales up the maximum sequence length of BP by 2.8-5.5 times larger,\nwhile using comparable or even less BP time. Note that StreamBP's sequence\nlength scaling ability can be directly transferred to batch size scaling for\naccelerating training. We further develop a communication-efficient distributed\nStreamBP to effectively support multi-GPU training and broaden its\napplicability. Our code can be easily integrated into the training pipeline of\nany transformer models and is available at https://github.com/Ledzy/StreamBP.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03077.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65a521af90b5e87bcd343828",
      "avatarUrl": "/avatars/3bbe83c1ba47df17d9c05a049147e5cc.svg",
      "fullname": "Mengqi Li",
      "name": "Kullpar",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.05327",
      "authors": [
        {
          "_id": "6842591962047f5641b3b650",
          "user": {
            "_id": "661d1f83ea3df2195a7c2924",
            "avatarUrl": "/avatars/dec49fc1d79913b07b57ccbef079198f.svg",
            "isPro": false,
            "fullname": "dcshi",
            "user": "dc-walker",
            "type": "user"
          },
          "name": "Duochao Shi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-06T07:40:58.621Z",
          "hidden": false
        },
        {
          "_id": "6842591962047f5641b3b651",
          "user": {
            "_id": "66699aa8a33847217b5a49c7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/u8Z-6U8U7ARXOpdBDI7Qm.png",
            "isPro": false,
            "fullname": "Weijie Wang",
            "user": "lhmd",
            "type": "user"
          },
          "name": "Weijie Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-06T07:41:00.966Z",
          "hidden": false
        },
        {
          "_id": "6842591962047f5641b3b652",
          "name": "Donny Y. Chen",
          "hidden": false
        },
        {
          "_id": "6842591962047f5641b3b653",
          "name": "Zeyu Zhang",
          "hidden": false
        },
        {
          "_id": "6842591962047f5641b3b654",
          "name": "Jia-Wang Bian",
          "hidden": false
        },
        {
          "_id": "6842591962047f5641b3b655",
          "name": "Bohan Zhuang",
          "hidden": false
        },
        {
          "_id": "6842591962047f5641b3b656",
          "name": "Chunhua Shen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-05T17:58:23.000Z",
      "submittedOnDailyAt": "2025-06-06T01:28:04.514Z",
      "title": "Revisiting 深さ表現を用いたFeed-Forward 3Dガウススプレッティング",
      "submittedOnDailyBy": {
        "_id": "66699aa8a33847217b5a49c7",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/u8Z-6U8U7ARXOpdBDI7Qm.png",
        "isPro": false,
        "fullname": "Weijie Wang",
        "user": "lhmd",
        "type": "user"
      },
      "summary": "デプスマップは、新視点合成に向けて3次元ガウススプレッティング（3DGS）パイプラインで広く使用されています。この手法では、デプスマップを3次元点雲に変換して新視点合成を行います。このアプローチは、効率的なトレーニング、既知のカメラ姿勢の使用、正確なジオメトリー推定などの利点を提供します。しかし、物体の境界におけるデプス不連続は、点雲の破続や稀疏化を招き、レンダリングの質を低下させます。この問題を解決するために、私たちは、事前学習されたTransformerが予測したポイントマップに基づく新しい正規化損失、PM-Lossを紹介します。ポイントマップ自身は、デプスマップよりも正確ではありませんが、特に物体の境界周辺では、ジオメトリーの平滑性を強制し、デプスマップを改善します。このデプスマップの改善により、私たちの方法は、多様なアーキテクチャとスケーンで3DGSを大幅に向上させ、一貫して良いレンダリング結果を提供します。私たちのプロジェクトページは、https://aim-uofa.github.io/PMLoss です。",
      "upvotes": 10,
      "discussionId": "6842591a62047f5641b3b6bc",
      "projectPage": "https://aim-uofa.github.io/PMLoss",
      "githubRepo": "https://github.com/aim-uofa/PM-Loss",
      "ai_summary": "PM-Loss, a regularization technique using pointmaps from a pre-trained transformer, enhances feed-forward 3D Gaussian Splatting by improving depth map accuracy and rendering quality.",
      "ai_keywords": [
        "3D Gaussian Splatting",
        "3DGS",
        "depth maps",
        "point clouds",
        "novel view synthesis",
        "PM-Loss",
        "pre-trained transformer",
        "pointmap",
        "geometric smoothness"
      ]
    },
    "publishedAt": "2025-06-05T13:58:23.000Z",
    "title": "Revisiting Depth Representations for Feed-Forward 3D Gaussian Splatting",
    "summary": "Depth maps are widely used in feed-forward 3D Gaussian Splatting (3DGS)\npipelines by unprojecting them into 3D point clouds for novel view synthesis.\nThis approach offers advantages such as efficient training, the use of known\ncamera poses, and accurate geometry estimation. However, depth discontinuities\nat object boundaries often lead to fragmented or sparse point clouds, degrading\nrendering quality -- a well-known limitation of depth-based representations. To\ntackle this issue, we introduce PM-Loss, a novel regularization loss based on a\npointmap predicted by a pre-trained transformer. Although the pointmap itself\nmay be less accurate than the depth map, it effectively enforces geometric\nsmoothness, especially around object boundaries. With the improved depth map,\nour method significantly improves the feed-forward 3DGS across various\narchitectures and scenes, delivering consistently better rendering results. Our\nproject page: https://aim-uofa.github.io/PMLoss",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05327.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66699aa8a33847217b5a49c7",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/u8Z-6U8U7ARXOpdBDI7Qm.png",
      "fullname": "Weijie Wang",
      "name": "lhmd",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.05349",
      "authors": [
        {
          "_id": "68424bed54a0d0e4b906baca",
          "name": "Hanoona Rasheed",
          "hidden": false
        },
        {
          "_id": "68424bed54a0d0e4b906bacb",
          "name": "Abdelrahman Shaker",
          "hidden": false
        },
        {
          "_id": "68424bed54a0d0e4b906bacc",
          "name": "Anqi Tang",
          "hidden": false
        },
        {
          "_id": "68424bed54a0d0e4b906bacd",
          "name": "Muhammad Maaz",
          "hidden": false
        },
        {
          "_id": "68424bed54a0d0e4b906bace",
          "name": "Ming-Hsuan Yang",
          "hidden": false
        },
        {
          "_id": "68424bed54a0d0e4b906bacf",
          "name": "Salman Khan",
          "hidden": false
        },
        {
          "_id": "68424bed54a0d0e4b906bad0",
          "name": "Fahad Khan",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64636b2551fa6e6306046293/rVaEoWuqnZnoewKuZDe09.mp4"
      ],
      "publishedAt": "2025-06-05T17:59:58.000Z",
      "submittedOnDailyAt": "2025-06-06T04:23:10.625Z",
      "title": "VideoMathQA: ビデオでの多タイプ理解による数学論理検定",
      "submittedOnDailyBy": {
        "_id": "64636b2551fa6e6306046293",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64636b2551fa6e6306046293/Uuz6z2MZb_LKLGM8uxF9s.jpeg",
        "isPro": false,
        "fullname": "Hanoona Rasheed",
        "user": "Hanoona",
        "type": "user"
      },
      "summary": "数学ビデオセットでの実世界における数学的推論は、静的画像や文字に比べて構成的に異なる課題です。これは、細かい視覚情報を解釈し、手書きまたはデジタルテキストを正確に読み取り、時間に沿って非線形に分散した語彙を統合する必要にあります。このような多モーダルのコンテキストでの成功は、視覚的な認識だけでなく、複雑なニュース流れから選択的に正しいコンテキスト詳細を特定し、統合することに依存します。この点について、ビデオマジェックスQA（VideoMathQA）というベンチマークを介して、モデルがビデオ上でこの時間的に延長されたクロスモーダル推論を行うことができるかを評価するためのプロジェクトを導入します。このベンチマークは10種類の数学分野を拡張し、10秒から1時間以上のビデオを対象としています。これは、構造化された視覚内容を解釈し、教訓的なナレーションを理解し、視覚的、音声的、テキストのモーダルファイルを統合することを求めます。このために、大学院レベルの専門家を雇用し、高品質の記録を確保し、920マン・ハーブール以上の記録時間を確保しました。実世界的なスキャンを反映するため、質問は3つの核心の推論課題を中心に設計されています：直接な問題解決、概念的なトランスファー、および深い教訓的理解、これは、拡張された説明と一部の解答を含む多段階の理由を統合することにより行われます。各質問には、多段階の理由の記録が含まれ、モデルの能力を細かく診断できます。このベンチマークを通じて、現在のアプローチの限界を明らかにし、時間的に延長されたモーダル豊富な数学問題設定での理由を行うモデルのシステム的な評価フレームワークを構築します。ビデオマジェックスQAのベンチマークと評価コードは、https://mbzuai-oryx.github.io/VideoMathQA から利用できます。",
      "upvotes": 9,
      "discussionId": "68424bef54a0d0e4b906bb3e",
      "ai_summary": "VideoMathQA evaluates models' ability to perform temporally extended cross-modal reasoning across various mathematical domains in video settings, addressing direct problem solving, conceptual transfer, and deep instructional comprehension.",
      "ai_keywords": [
        "VideoMathQA",
        "temporally extended cross-modal reasoning",
        "structured visual content",
        "instructional narratives",
        "modality-rich",
        "multi-step reasoning",
        "partial solutions",
        "multi-step reasoning annotations",
        "system evaluation framework"
      ]
    },
    "publishedAt": "2025-06-05T13:59:58.000Z",
    "title": "VideoMathQA: Benchmarking Mathematical Reasoning via Multimodal\n  Understanding in Videos",
    "summary": "Mathematical reasoning in real-world video settings presents a fundamentally\ndifferent challenge than in static images or text. It requires interpreting\nfine-grained visual information, accurately reading handwritten or digital\ntext, and integrating spoken cues, often dispersed non-linearly over time. In\nsuch multimodal contexts, success hinges not just on perception, but on\nselectively identifying and integrating the right contextual details from a\nrich and noisy stream of content. To this end, we introduce VideoMathQA, a\nbenchmark designed to evaluate whether models can perform such temporally\nextended cross-modal reasoning on videos. The benchmark spans 10 diverse\nmathematical domains, covering videos ranging from 10 seconds to over 1 hour.\nIt requires models to interpret structured visual content, understand\ninstructional narratives, and jointly ground concepts across visual, audio, and\ntextual modalities. We employ graduate-level experts to ensure high quality,\ntotaling over 920 man-hours of annotation. To reflect real-world scenarios,\nquestions are designed around three core reasoning challenges: direct problem\nsolving, where answers are grounded in the presented question; conceptual\ntransfer, which requires applying learned methods to new problems; and deep\ninstructional comprehension, involving multi-step reasoning over extended\nexplanations and partially worked-out solutions. Each question includes\nmulti-step reasoning annotations, enabling fine-grained diagnosis of model\ncapabilities. Through this benchmark, we highlight the limitations of existing\napproaches and establish a systematic evaluation framework for models that must\nreason, rather than merely perceive, across temporally extended and\nmodality-rich mathematical problem settings. Our benchmark and evaluation code\nare available at: https://mbzuai-oryx.github.io/VideoMathQA",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64636b2551fa6e6306046293/rVaEoWuqnZnoewKuZDe09.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05349.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64636b2551fa6e6306046293",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64636b2551fa6e6306046293/Uuz6z2MZb_LKLGM8uxF9s.jpeg",
      "fullname": "Hanoona Rasheed",
      "name": "Hanoona",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.05345",
      "authors": [
        {
          "_id": "6842a3cb9393cafb521855aa",
          "name": "Adrian Łańcucki",
          "hidden": false
        },
        {
          "_id": "6842a3cb9393cafb521855ab",
          "name": "Konrad Staniszewski",
          "hidden": false
        },
        {
          "_id": "6842a3cb9393cafb521855ac",
          "name": "Piotr Nawrot",
          "hidden": false
        },
        {
          "_id": "6842a3cb9393cafb521855ad",
          "name": "Edoardo M. Ponti",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-05T17:59:55.000Z",
      "submittedOnDailyAt": "2025-06-06T06:46:43.733Z",
      "title": "推論時のハイパースケーリングとKVキャッシュの圧縮",
      "submittedOnDailyBy": {
        "_id": "640deb5d3c82bd463ee44735",
        "avatarUrl": "/avatars/0e748d7c91d97526b280e40ccb25c9e0.svg",
        "isPro": false,
        "fullname": "Piotr Nawrot",
        "user": "pnawrot",
        "type": "user"
      },
      "summary": "推論時のスケーリングは、長いまたは並列なシーケンスの生成により理由の精度を向上させ、効率を交換します。しかし、Transformer LLMsでは、生成コストはキーバリュー（KV）キャッシュのサイズによって制限され、生成されたトークンの数による制限ではないため、これを調査しています。このため、推論時のハイパースケーリングを調査します：KVキャッシュを圧縮することで、同じ計算バジュード内で生成できるトークン数を増やし、スケーリング推論の精度を進めます。しかし、このアプローチの成功は、圧縮メソッドが高い圧縮比でも精度を保つことに依存しています。ハイパースケーリングの実用性を実現するために、Dynamic Memory Sparsification（DMS）を紹介します。DMSは新しい方法で、KVキャッシュをスパースにするためのもので、1Kトレーニングステップで8倍の圧縮を達成し、トレーニングフリーのスパースアタションよりもより良い精度を維持します。DMSは、キャッシュされたトークンを遅延して削除し、隠れに表現を統合し、重要な情報を保存します。DMSを用いた推論時のハイパースケーリングの効果を証明します。複数のLLMsの家族において、比較的推論時間とメモリ負荷で精度を向上させます。例えば、Qwen-R1 32BをAIME 24で平均9.1点、GPQAで7.6点、LiveCodeBenchで9.6点の精度向上を実現します。",
      "upvotes": 9,
      "discussionId": "6842a3cc9393cafb521855dd",
      "ai_summary": "Inference-time hyper-scaling with Dynamic Memory Sparsification in Transformer LLMs allows for increased token generation within the same compute budget by compressing the key-value cache, thereby enhancing accuracy.",
      "ai_keywords": [
        "inference-time hyper-scaling",
        "key-value (KV) cache",
        "Dynamic Memory Sparsification (DMS)",
        "token eviction",
        "representation merging",
        "AIME 24",
        "GPQA",
        "LiveCodeBench",
        "Qwen-R1 32B"
      ]
    },
    "publishedAt": "2025-06-05T13:59:55.000Z",
    "title": "Inference-Time Hyper-Scaling with KV Cache Compression",
    "summary": "Inference-time scaling trades efficiency for increased reasoning accuracy by\ngenerating longer or more parallel sequences. However, in Transformer LLMs,\ngeneration cost is bottlenecked by the size of the key-value (KV) cache, rather\nthan the number of generated tokens. Hence, we explore inference-time\nhyper-scaling: by compressing the KV cache, we can generate more tokens within\nthe same compute budget and further improve the accuracy of scaled inference.\nThe success of this approach, however, hinges on the ability of compression\nmethods to preserve accuracy even at high compression ratios. To make\nhyper-scaling practical, we introduce Dynamic Memory Sparsification (DMS), a\nnovel method for sparsifying KV caches that only requires 1K training steps to\nachieve 8times compression, while maintaining better accuracy than\ntraining-free sparse attention. Instead of prematurely discarding cached\ntokens, DMS delays token eviction, implicitly merging representations and\npreserving critical information. We demonstrate the effectiveness of\ninference-time hyper-scaling with DMS on multiple families of LLMs, showing\nthat it boosts accuracy for comparable inference runtime and memory load. For\ninstance, we enhance Qwen-R1 32B by an average of 9.1 points on AIME 24, 7.6 on\nGPQA, and 9.6 on LiveCodeBench across compute budgets.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05345.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "640deb5d3c82bd463ee44735",
      "avatarUrl": "/avatars/0e748d7c91d97526b280e40ccb25c9e0.svg",
      "fullname": "Piotr Nawrot",
      "name": "pnawrot",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.05287",
      "authors": [
        {
          "_id": "68425719ba04d3ceff5bea29",
          "user": {
            "_id": "64a3fe3dde901eb01df12398",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a3fe3dde901eb01df12398/Js2bEx4rxKuEKVt5z9I2D.jpeg",
            "isPro": false,
            "fullname": "YuqianYuan",
            "user": "CircleRadon",
            "type": "user"
          },
          "name": "Yuqian Yuan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-06T07:41:05.846Z",
          "hidden": false
        },
        {
          "_id": "68425719ba04d3ceff5bea2a",
          "name": "Ronghao Dang",
          "hidden": false
        },
        {
          "_id": "68425719ba04d3ceff5bea2b",
          "name": "Long Li",
          "hidden": false
        },
        {
          "_id": "68425719ba04d3ceff5bea2c",
          "name": "Wentong Li",
          "hidden": false
        },
        {
          "_id": "68425719ba04d3ceff5bea2d",
          "name": "Dian Jiao",
          "hidden": false
        },
        {
          "_id": "68425719ba04d3ceff5bea2e",
          "name": "Xin Li",
          "hidden": false
        },
        {
          "_id": "68425719ba04d3ceff5bea2f",
          "name": "Deli Zhao",
          "hidden": false
        },
        {
          "_id": "68425719ba04d3ceff5bea30",
          "name": "Fan Wang",
          "hidden": false
        },
        {
          "_id": "68425719ba04d3ceff5bea31",
          "name": "Wenqiao Zhang",
          "hidden": false
        },
        {
          "_id": "68425719ba04d3ceff5bea32",
          "name": "Jun Xiao",
          "hidden": false
        },
        {
          "_id": "68425719ba04d3ceff5bea33",
          "name": "Yueting Zhuang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64a3fe3dde901eb01df12398/bFTB1kqlo-Oj0gQM7yzfe.png"
      ],
      "publishedAt": "2025-06-05T17:44:12.000Z",
      "submittedOnDailyAt": "2025-06-06T01:27:53.697Z",
      "title": "EOC-Bench: MLLMsは、自中心的な世界で物体を識別、記憶、予測できるか？",
      "submittedOnDailyBy": {
        "_id": "64a3fe3dde901eb01df12398",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a3fe3dde901eb01df12398/Js2bEx4rxKuEKVt5z9I2D.jpeg",
        "isPro": false,
        "fullname": "YuqianYuan",
        "user": "CircleRadon",
        "type": "user"
      },
      "summary": "多モダル大語言モデル（MLLMs）の出現は、自中心的な視覚アプリケーションにおける突破を促進しました。これらのアプリケーションは、ユーザが動的で雑多な環境でツールと相互作用する際に、物体の持続的な、コンテキストに関わった理解を必要としています。しかし、現在の具象的なベンチマークは、静的なスケーンの探索を中心として、物体の外観と空間属性を優先し、ユーザの相互作用による動的な変化の評価を遺しています。この欠点を解決するために、私たちはEOC-Benchを紹介します。EOC-Benchは、動的な自中心的なスケーニングでの物体中心的な具象的な認知をシステマティックに評価するために設計された革新的なベンチマークです。特に、EOC-Benchは、Past、Present、Futureの3つの時系列カテゴリに分類された3,277個の細かく注釈されたQAペアを特徴として持ち、11の細かい評価ディメンションと3種類の可視的な物体参照を被覆しています。評価の完璧性を確保するために、私たちは4つのクエストのマイクロフォーマットの人間がロープ内のアノテーションフレームワークを開発し、開放エンドプローチの時系列評価に向けた新しい多スケール時系列正確性メトリックを設計しました。EOC-Benchに基づいて、私たちは、権利者のプロプライエージェント、オープンソース、物体レベルのMLLMsの評価を実施しました。EOC-Benchは、MLLMsの具象的な物体認知能力の進歩を促進する重要なツールであり、具象的システムの信頼性のあるコアモデルの開発に強い基盤を提供します。",
      "upvotes": 9,
      "discussionId": "6842571dba04d3ceff5beb34",
      "projectPage": "https://circleradon.github.io/EOCBench/",
      "githubRepo": "https://github.com/alibaba-damo-academy/EOCBench",
      "ai_summary": "EOC-Bench introduces a benchmark to evaluate dynamic object-centric cognition in egocentric vision applications, focusing on temporal and interactive aspects not covered by existing benchmarks.",
      "ai_keywords": [
        "multimodal large language models (MLLMs)",
        "egocentric vision",
        "embodied benchmarks",
        "object-centric embodied cognition",
        "QA pairs",
        "temporal accuracy metric"
      ]
    },
    "publishedAt": "2025-06-05T13:44:12.000Z",
    "title": "EOC-Bench: Can MLLMs Identify, Recall, and Forecast Objects in an\n  Egocentric World?",
    "summary": "The emergence of multimodal large language models (MLLMs) has driven\nbreakthroughs in egocentric vision applications. These applications necessitate\npersistent, context-aware understanding of objects, as users interact with\ntools in dynamic and cluttered environments. However, existing embodied\nbenchmarks primarily focus on static scene exploration, emphasizing object's\nappearance and spatial attributes while neglecting the assessment of dynamic\nchanges arising from users' interactions. To address this gap, we introduce\nEOC-Bench, an innovative benchmark designed to systematically evaluate\nobject-centric embodied cognition in dynamic egocentric scenarios. Specially,\nEOC-Bench features 3,277 meticulously annotated QA pairs categorized into three\ntemporal categories: Past, Present, and Future, covering 11 fine-grained\nevaluation dimensions and 3 visual object referencing types. To ensure thorough\nassessment, we develop a mixed-format human-in-the-loop annotation framework\nwith four types of questions and design a novel multi-scale temporal accuracy\nmetric for open-ended temporal evaluation. Based on EOC-Bench, we conduct\ncomprehensive evaluations of various proprietary, open-source, and object-level\nMLLMs. EOC-Bench serves as a crucial tool for advancing the embodied object\ncognitive capabilities of MLLMs, establishing a robust foundation for\ndeveloping reliable core models for embodied systems.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64a3fe3dde901eb01df12398/bFTB1kqlo-Oj0gQM7yzfe.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05287.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64a3fe3dde901eb01df12398",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a3fe3dde901eb01df12398/Js2bEx4rxKuEKVt5z9I2D.jpeg",
      "fullname": "YuqianYuan",
      "name": "CircleRadon",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.02620",
      "authors": [
        {
          "_id": "68425ef33b5bb39c456487e0",
          "user": {
            "_id": "64049ae20ab5e22719f35103",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678023295407-noauth.jpeg",
            "isPro": false,
            "fullname": "Dongyu Yan",
            "user": "StarYDY",
            "type": "user"
          },
          "name": "Dongyu Yan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-06T07:40:56.223Z",
          "hidden": false
        },
        {
          "_id": "68425ef33b5bb39c456487e1",
          "name": "Leyi Wu",
          "hidden": false
        },
        {
          "_id": "68425ef33b5bb39c456487e2",
          "name": "Jiantao Lin",
          "hidden": false
        },
        {
          "_id": "68425ef33b5bb39c456487e3",
          "name": "Luozhou Wang",
          "hidden": false
        },
        {
          "_id": "68425ef33b5bb39c456487e4",
          "name": "Tianshuo Xu",
          "hidden": false
        },
        {
          "_id": "68425ef33b5bb39c456487e5",
          "name": "Zhifei Chen",
          "hidden": false
        },
        {
          "_id": "68425ef33b5bb39c456487e6",
          "name": "Zhen Yang",
          "hidden": false
        },
        {
          "_id": "68425ef33b5bb39c456487e7",
          "name": "Lie Xu",
          "hidden": false
        },
        {
          "_id": "68425ef33b5bb39c456487e8",
          "name": "Shunsi Zhang",
          "hidden": false
        },
        {
          "_id": "68425ef33b5bb39c456487e9",
          "user": {
            "_id": "655cba1d87b67834000590e8",
            "avatarUrl": "/avatars/3bd43b7c9351f65b8f38f4c8237a0146.svg",
            "isPro": false,
            "fullname": "Yingcong Chen",
            "user": "yingcongchen",
            "type": "user"
          },
          "name": "Yingcong Chen",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-06T03:22:29.750Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T08:36:03.000Z",
      "submittedOnDailyAt": "2025-06-06T02:21:59.705Z",
      "title": "FlexPainter: 柔軟かつ多角度一致性のあるテクスチャ生成",
      "submittedOnDailyBy": {
        "_id": "64049ae20ab5e22719f35103",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678023295407-noauth.jpeg",
        "isPro": false,
        "fullname": "Dongyu Yan",
        "user": "StarYDY",
        "type": "user"
      },
      "summary": "テクスチャマップの作成は3Dモデリングの重要な部分で、渲染キャラクターの質を決定します。最近、拡散ベースの方法はテクスチャ生成の新しい道を開きました。しかし、制御の柔軟性の制限と提示モデルの制限が、作成者が望む結果を作成することを防ぎます。さらに、生成された多点画像の間の不連続性は、テクスチャ生成の質を悪くすることを招きます。これらの問題に対処し、FlexPainter、柔軟な多モデル条件付きガイドングを可能にし、高度な一致性を達成する新しいテクスチャ生成パイプラインを紹介します。共有条件付き埋め込み空間を構築し、異なる入力モデルの間で柔軟な集約を行います。この埋め込み空間を利用し、画像基のCFG方法を提案し、構造的およびスタイル情報を分解し、参照画像基のスタイリズムを実現します。画像拡散先に含まれる3D知識を活用し、最初にグリッド表現を用いて同時に多点画像を生成し、グローバル的理解を強化します。また、拡散サンプリングの際に視点の同期と適応的な重み付けモジュールを提案し、ローカルの一致性を確保します。最後に、3D知識を持つテクスチャ完成モデルとテクスチャ強化モデルを組み合わせて、無間断で高解像度のテクスチャマップを生成します。詳細な実験は、我々のフレームワークが柔軟性と生成質の両方で最先端の方法を大幅に上回ることを示します。",
      "upvotes": 8,
      "discussionId": "68425ef53b5bb39c4564888b",
      "projectPage": "https://starydy.xyz/FlexPainter/",
      "githubRepo": "https://github.com/StarRealMan/FlexPainter",
      "ai_summary": "FlexPainter, a novel texture generation pipeline, uses a shared conditional embedding space to enable flexible multi-modal guidance, ensuring high-quality and consistent texture map generation using image diffusion priors and a 3D-aware model.",
      "ai_keywords": [
        "diffusion-based methods",
        "texture generation",
        "flexible multi-modal conditional guidance",
        "conditional embedding space",
        "image-based CFG method",
        "structural information",
        "style information",
        "reference image-based stylization",
        "image diffusion prior",
        "grid representation",
        "view synchronization",
        "adaptive weighting module",
        "3D-aware texture completion model",
        "texture enhancement model"
      ]
    },
    "publishedAt": "2025-06-03T04:36:03.000Z",
    "title": "FlexPainter: Flexible and Multi-View Consistent Texture Generation",
    "summary": "Texture map production is an important part of 3D modeling and determines the\nrendering quality. Recently, diffusion-based methods have opened a new way for\ntexture generation. However, restricted control flexibility and limited prompt\nmodalities may prevent creators from producing desired results. Furthermore,\ninconsistencies between generated multi-view images often lead to poor texture\ngeneration quality. To address these issues, we introduce FlexPainter,\na novel texture generation pipeline that enables flexible multi-modal\nconditional guidance and achieves highly consistent texture generation. A\nshared conditional embedding space is constructed to perform flexible\naggregation between different input modalities. Utilizing such embedding space,\nwe present an image-based CFG method to decompose structural and style\ninformation, achieving reference image-based stylization. Leveraging the 3D\nknowledge within the image diffusion prior, we first generate multi-view images\nsimultaneously using a grid representation to enhance global understanding.\nMeanwhile, we propose a view synchronization and adaptive weighting module\nduring diffusion sampling to further ensure local consistency. Finally, a\n3D-aware texture completion model combined with a texture enhancement model is\nused to generate seamless, high-resolution texture maps. Comprehensive\nexperiments demonstrate that our framework significantly outperforms\nstate-of-the-art methods in both flexibility and generation quality.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.02620.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64049ae20ab5e22719f35103",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678023295407-noauth.jpeg",
      "fullname": "Dongyu Yan",
      "name": "StarYDY",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.04209",
      "authors": [
        {
          "_id": "68413c8eb64ba498925da6a8",
          "user": {
            "_id": "65d45fbf9f087171b805c428",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d45fbf9f087171b805c428/bgxwcn2p_D9qEa5vynSxV.jpeg",
            "isPro": false,
            "fullname": "Jingfeng Yang",
            "user": "JingfengY",
            "type": "user"
          },
          "name": "Jingfeng Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:26:26.842Z",
          "hidden": false
        },
        {
          "_id": "68413c8eb64ba498925da6a9",
          "user": {
            "_id": "64ea89932ca4ff1d53b77548",
            "avatarUrl": "/avatars/ce3df67ba3ea3197ebf74fbe5e2c0e48.svg",
            "isPro": false,
            "fullname": "Ziyang Wu",
            "user": "robinwuzy",
            "type": "user"
          },
          "name": "Ziyang Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-06T07:41:57.098Z",
          "hidden": false
        },
        {
          "_id": "68413c8eb64ba498925da6aa",
          "name": "Yue Zhao",
          "hidden": false
        },
        {
          "_id": "68413c8eb64ba498925da6ab",
          "name": "Yi Ma",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-04T17:51:56.000Z",
      "submittedOnDailyAt": "2025-06-06T00:43:44.611Z",
      "title": "言語-画像の対応を固定したテキストエンコーダーを用いる",
      "submittedOnDailyBy": {
        "_id": "65d45fbf9f087171b805c428",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d45fbf9f087171b805c428/bgxwcn2p_D9qEa5vynSxV.jpeg",
        "isPro": false,
        "fullname": "Jingfeng Yang",
        "user": "JingfengY",
        "type": "user"
      },
      "summary": "現在、言語と画像の対応関係を確立する最も優れたアプローチは、CLIPやその変体などの対比的学習を通じて文と画像エンコーダーを共に事前学習することである。本稿では、このような高額な共通学習が必要かどうかを質疑し、特に固定された大規模な言語モデル（LLM）がより良い文エンコーダーとしてビジュアル表現学習をガイドすることができるかを検討する。すなわち、LLMからの固定された文エンコーダーを用いて言語-画像の対応関係を学習するために、画像エンコーダーのみを学習するLIFT（Language-Image alignment with a Fixed Text encoder）を提案する。そのようなものは、構成的理解と長いキャプションに関する多くのシナリオでCLIPを超える高い効果性を示し、計算効率の大幅な向上を収得することが見られる。本稿は、LLMからの文埋め込みをビジュアル学習にガイドする方法をシステマ的に調査する最初のステップを踏み出し、言語対応したビジュアル表現の学習のデザイン選択肢を提示することを示している。",
      "upvotes": 7,
      "discussionId": "68413c8fb64ba498925da720",
      "projectPage": "https://jingfeng0705.github.io/LIFT/lift.html",
      "githubRepo": "https://github.com/Jingfeng0705/LIFT",
      "ai_summary": "Learning Language-Image alignment with a Fixed Text encoder (LIFT) using pre-trained large language models effectively guides visual representation learning, outperforming joint training methods like CLIP in compositional understanding and long captions.",
      "ai_keywords": [
        "contrastive learning",
        "CLIP",
        "pre-trained fixed large language model",
        "LLM",
        "Language-Image alignment",
        "LIFT",
        "image encoder",
        "compositional understanding",
        "long captions"
      ]
    },
    "publishedAt": "2025-06-04T13:51:56.000Z",
    "title": "Language-Image Alignment with Fixed Text Encoders",
    "summary": "Currently, the most dominant approach to establishing language-image\nalignment is to pre-train text and image encoders jointly through contrastive\nlearning, such as CLIP and its variants. In this work, we question whether such\na costly joint training is necessary. In particular, we investigate if a\npre-trained fixed large language model (LLM) offers a good enough text encoder\nto guide visual representation learning. That is, we propose to learn\nLanguage-Image alignment with a Fixed Text encoder (LIFT) from an LLM by\ntraining only the image encoder. Somewhat surprisingly, through comprehensive\nbenchmarking and ablation studies, we find that this much simplified framework\nLIFT is highly effective and it outperforms CLIP in most scenarios that involve\ncompositional understanding and long captions, while achieving considerable\ngains in computational efficiency. Our work takes a first step towards\nsystematically exploring how text embeddings from LLMs can guide visual\nlearning and suggests an alternative design choice for learning\nlanguage-aligned visual representations.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04209.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "65d45fbf9f087171b805c428",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d45fbf9f087171b805c428/bgxwcn2p_D9qEa5vynSxV.jpeg",
      "fullname": "Jingfeng Yang",
      "name": "JingfengY",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.01011",
      "authors": [
        {
          "_id": "6842746e8edd398d01b68e03",
          "name": "Siqi Hui",
          "hidden": false
        },
        {
          "_id": "6842746e8edd398d01b68e04",
          "name": "Yiren Song",
          "hidden": false
        },
        {
          "_id": "6842746e8edd398d01b68e05",
          "name": "Sanping Zhou",
          "hidden": false
        },
        {
          "_id": "6842746e8edd398d01b68e06",
          "name": "Ye Deng",
          "hidden": false
        },
        {
          "_id": "6842746e8edd398d01b68e07",
          "name": "Wenli Huang",
          "hidden": false
        },
        {
          "_id": "6842746e8edd398d01b68e08",
          "name": "Jinjun Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-01T13:44:20.000Z",
      "submittedOnDailyAt": "2025-06-06T03:26:27.710Z",
      "title": "自動再生攻撃に対応する言語偏向による画像マーキング手法：アプローチ",
      "submittedOnDailyBy": {
        "_id": "64311a95034ecbefddd141ef",
        "avatarUrl": "/avatars/b6dc5ca373bedbaa368208517954c375.svg",
        "isPro": true,
        "fullname": "Yiren Song",
        "user": "yiren98",
        "type": "user"
      },
      "summary": "自動回帰（AR）画像生成モデルは、合成質の進歩により注目を集めてきました。これにより、水印を埋め込む必要性が高まり、強力な水印技術が必要となりました。しかし、現在の生成中の水印技術は主にディフフェーションモデル向けで、水印はディフフェーション潜在状態内に埋め込まれています。この設計は、ARモデルへの直接的な適用に大きな課題をもたらしています。また、ディフフェーションベースの再生攻撃は、潜在状態の摂動によりその水印を効果的に消去できます。これらの課題に対処するために、我々はLexical Bias Watermarking（LBW）を提案します。LBWは、ARモデルに向けて設計され、再生攻撃を抵抗することを目的としています。LBWは、生成時にトークン選択を特定の「緑リスト」にバイアスしてトークンマップに直接水印を埋め込みます。このアプローチは、現在のARモデルとの無間適合を保証し、事後の水印技術に自然に適用できます。白ボックス攻撃に対する安全性向上のために、ディフフェーションベースの再生攻撃を抵抗するためには、一つの「緑リスト」を使用しないで、各画像に対しては緑リストのプールからランダムにサンプリングします。水印検出は、トークン分布の定量化と統計分析により行われます。拡大的な実験は、LBWが再生攻撃を抵抗することを特に示し、水印の強固性が上位にあることを明らかにしました。",
      "upvotes": 7,
      "discussionId": "6842747b8edd398d01b69110",
      "ai_summary": "A novel watermarking technique, Lexical Bias Watermarking, enhances the security of autoregressive image generation models by embedding watermarks into token selection, demonstrating superior resistance to regeneration attacks.",
      "ai_keywords": [
        "autoregressive models",
        "in-generation watermarking",
        "diffusion models",
        "diffusion latent states",
        "token prediction",
        "regeneration attacks",
        "Lexical Bias Watermarking",
        "token maps",
        "green list",
        "watermark detection",
        "quantization",
        "statistical analysis",
        "token distribution"
      ]
    },
    "publishedAt": "2025-06-01T09:44:20.000Z",
    "title": "Autoregressive Images Watermarking through Lexical Biasing: An Approach\n  Resistant to Regeneration Attack",
    "summary": "Autoregressive (AR) image generation models have gained increasing attention\nfor their breakthroughs in synthesis quality, highlighting the need for robust\nwatermarking to prevent misuse. However, existing in-generation watermarking\ntechniques are primarily designed for diffusion models, where watermarks are\nembedded within diffusion latent states. This design poses significant\nchallenges for direct adaptation to AR models, which generate images\nsequentially through token prediction. Moreover, diffusion-based regeneration\nattacks can effectively erase such watermarks by perturbing diffusion latent\nstates. To address these challenges, we propose Lexical Bias Watermarking\n(LBW), a novel framework designed for AR models that resists regeneration\nattacks. LBW embeds watermarks directly into token maps by biasing token\nselection toward a predefined green list during generation. This approach\nensures seamless integration with existing AR models and extends naturally to\npost-hoc watermarking. To increase the security against white-box attacks,\ninstead of using a single green list, the green list for each image is randomly\nsampled from a pool of green lists. Watermark detection is performed via\nquantization and statistical analysis of the token distribution. Extensive\nexperiments demonstrate that LBW achieves superior watermark robustness,\nparticularly in resisting regeneration attacks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01011.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64311a95034ecbefddd141ef",
      "avatarUrl": "/avatars/b6dc5ca373bedbaa368208517954c375.svg",
      "fullname": "Yiren Song",
      "name": "yiren98",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 21
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.05209",
      "authors": [
        {
          "_id": "684247f35d537e0e5ecb724b",
          "name": "Nikhil Kandpal",
          "hidden": false
        },
        {
          "_id": "684247f35d537e0e5ecb724c",
          "name": "Brian Lester",
          "hidden": false
        },
        {
          "_id": "684247f35d537e0e5ecb724d",
          "name": "Colin Raffel",
          "hidden": false
        },
        {
          "_id": "684247f35d537e0e5ecb724e",
          "user": {
            "_id": "636071759ddc44e710e0f5ce",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/636071759ddc44e710e0f5ce/-gmEhY5PidmSXIQPi2-QB.jpeg",
            "isPro": true,
            "fullname": "Sebastian Majstorovic",
            "user": "storytracer",
            "type": "user"
          },
          "name": "Sebastian Majstorovic",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-06T07:41:37.270Z",
          "hidden": false
        },
        {
          "_id": "684247f35d537e0e5ecb724f",
          "name": "Stella Biderman",
          "hidden": false
        },
        {
          "_id": "684247f35d537e0e5ecb7250",
          "name": "Baber Abbasi",
          "hidden": false
        },
        {
          "_id": "684247f35d537e0e5ecb7251",
          "name": "Luca Soldaini",
          "hidden": false
        },
        {
          "_id": "684247f35d537e0e5ecb7252",
          "name": "Enrico Shippole",
          "hidden": false
        },
        {
          "_id": "684247f35d537e0e5ecb7253",
          "name": "A. Feder Cooper",
          "hidden": false
        },
        {
          "_id": "684247f35d537e0e5ecb7254",
          "name": "Aviya Skowron",
          "hidden": false
        },
        {
          "_id": "684247f35d537e0e5ecb7255",
          "name": "John Kirchenbauer",
          "hidden": false
        },
        {
          "_id": "684247f35d537e0e5ecb7256",
          "name": "Shayne Longpre",
          "hidden": false
        },
        {
          "_id": "684247f35d537e0e5ecb7257",
          "name": "Lintang Sutawika",
          "hidden": false
        },
        {
          "_id": "684247f35d537e0e5ecb7258",
          "name": "Alon Albalak",
          "hidden": false
        },
        {
          "_id": "684247f35d537e0e5ecb7259",
          "name": "Zhenlin Xu",
          "hidden": false
        },
        {
          "_id": "684247f35d537e0e5ecb725a",
          "name": "Guilherme Penedo",
          "hidden": false
        },
        {
          "_id": "684247f35d537e0e5ecb725b",
          "name": "Loubna Ben Allal",
          "hidden": false
        },
        {
          "_id": "684247f35d537e0e5ecb725c",
          "name": "Elie Bakouch",
          "hidden": false
        },
        {
          "_id": "684247f35d537e0e5ecb725d",
          "name": "John David Pressman",
          "hidden": false
        },
        {
          "_id": "684247f35d537e0e5ecb725e",
          "name": "Honglu Fan",
          "hidden": false
        },
        {
          "_id": "684247f35d537e0e5ecb725f",
          "name": "Dashiell Stander",
          "hidden": false
        },
        {
          "_id": "684247f35d537e0e5ecb7260",
          "name": "Guangyu Song",
          "hidden": false
        },
        {
          "_id": "684247f35d537e0e5ecb7261",
          "name": "Aaron Gokaslan",
          "hidden": false
        },
        {
          "_id": "684247f35d537e0e5ecb7262",
          "name": "Tom Goldstein",
          "hidden": false
        },
        {
          "_id": "684247f35d537e0e5ecb7263",
          "name": "Brian R. Bartoldson",
          "hidden": false
        },
        {
          "_id": "684247f35d537e0e5ecb7264",
          "name": "Bhavya Kailkhura",
          "hidden": false
        },
        {
          "_id": "684247f35d537e0e5ecb7265",
          "name": "Tyler Murray",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-05T16:21:30.000Z",
      "submittedOnDailyAt": "2025-06-06T05:47:06.933Z",
      "title": "「公開領域と開放許諾ライセンスのテキストの8TBデータセット Common Pile v0.1」",
      "submittedOnDailyBy": {
        "_id": "5e6a3d4ea9afd5125d9ec064",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1584020801691-noauth.jpeg",
        "isPro": true,
        "fullname": "Stefan Schweter",
        "user": "stefan-it",
        "type": "user"
      },
      "summary": "大語言モデル（LLMs）は通常、無許可テキストの巨大な量で訓練され、この実践は知的財産権侵害の可能性や倫理的な懸念により視察されています。LLMsに開放的許可テキストを用いて訓練することは、これらの問題を解決する最初のステップとなりますが、先行のデータ収集の努力は、性能の良いLLMsを生成するためには、サイズが小さくまたは品質が低いデータセットを生み出しました。この隙を補うために、Common Pile v0.1という8テキバイトの開放的許可テキストのコレクションを集め、整備し、リリースします。Common Pileは、研究論文、コード、本、エンシャンドラ、教育材料、音声翻訳など、多様な領域からの30つのソースから構成されています。重要なことに、2つの7億パラメータのLLMsをCommon Pileのテキストで訓練し、Comma v0.1-1TとComma v0.1-2Tを作成し、それぞれ1トリリオンと2トリリオンのトークンで訓練されました。両モデルは、無許可テキストで訓練されたLLMsと同じ計算バジュードで比較的性能を達成しました。Common Pile v0.1自身をリリースすることではなく、その作成に使用されたコードやComma v0.1モデルの訓練ミッションとチェックポイントも一緒にリリースします。",
      "upvotes": 6,
      "discussionId": "684247f85d537e0e5ecb73d3",
      "projectPage": "https://huggingface.co/common-pile",
      "githubRepo": "https://github.com/r-three/common-pile",
      "ai_summary": "The Common Pile v0.1 dataset, an 8-terabyte collection of openly licensed text, is used to train competitive 7 billion parameter LLMs.",
      "ai_keywords": [
        "Large language models",
        "LLMs",
        "openly licensed text",
        "Common Pile v0.1",
        "parameter-efficient fine-tuning",
        "Llama 1 and 2 7B",
        "training mixture",
        "checkpoints"
      ]
    },
    "publishedAt": "2025-06-05T12:21:30.000Z",
    "title": "The Common Pile v0.1: An 8TB Dataset of Public Domain and Openly\n  Licensed Text",
    "summary": "Large language models (LLMs) are typically trained on enormous quantities of\nunlicensed text, a practice that has led to scrutiny due to possible\nintellectual property infringement and ethical concerns. Training LLMs on\nopenly licensed text presents a first step towards addressing these issues, but\nprior data collection efforts have yielded datasets too small or low-quality to\nproduce performant LLMs. To address this gap, we collect, curate, and release\nthe Common Pile v0.1, an eight terabyte collection of openly licensed text\ndesigned for LLM pretraining. The Common Pile comprises content from 30 sources\nthat span diverse domains including research papers, code, books,\nencyclopedias, educational materials, audio transcripts, and more. Crucially,\nwe validate our efforts by training two 7 billion parameter LLMs on text from\nthe Common Pile: Comma v0.1-1T and Comma v0.1-2T, trained on 1 and 2 trillion\ntokens respectively. Both models attain competitive performance to LLMs trained\non unlicensed text with similar computational budgets, such as Llama 1 and 2\n7B. In addition to releasing the Common Pile v0.1 itself, we also release the\ncode used in its creation as well as the training mixture and checkpoints for\nthe Comma v0.1 models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05209.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5e6a3d4ea9afd5125d9ec064",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1584020801691-noauth.jpeg",
      "fullname": "Stefan Schweter",
      "name": "stefan-it",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2725
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.04405",
      "authors": [
        {
          "_id": "6842454fbdc448822b2f1c03",
          "name": "Ran Xu",
          "hidden": false
        },
        {
          "_id": "6842454fbdc448822b2f1c04",
          "name": "Yuchen Zhuang",
          "hidden": false
        },
        {
          "_id": "6842454fbdc448822b2f1c05",
          "name": "Yishan Zhong",
          "hidden": false
        },
        {
          "_id": "6842454fbdc448822b2f1c06",
          "name": "Yue Yu",
          "hidden": false
        },
        {
          "_id": "6842454fbdc448822b2f1c07",
          "name": "Xiangru Tang",
          "hidden": false
        },
        {
          "_id": "6842454fbdc448822b2f1c08",
          "name": "Hang Wu",
          "hidden": false
        },
        {
          "_id": "6842454fbdc448822b2f1c09",
          "name": "May D. Wang",
          "hidden": false
        },
        {
          "_id": "6842454fbdc448822b2f1c0a",
          "name": "Peifeng Ruan",
          "hidden": false
        },
        {
          "_id": "6842454fbdc448822b2f1c0b",
          "name": "Donghan Yang",
          "hidden": false
        },
        {
          "_id": "6842454fbdc448822b2f1c0c",
          "name": "Tao Wang",
          "hidden": false
        },
        {
          "_id": "6842454fbdc448822b2f1c0d",
          "name": "Guanghua Xiao",
          "hidden": false
        },
        {
          "_id": "6842454fbdc448822b2f1c0e",
          "name": "Carl Yang",
          "hidden": false
        },
        {
          "_id": "6842454fbdc448822b2f1c0f",
          "name": "Yang Xie",
          "hidden": false
        },
        {
          "_id": "6842454fbdc448822b2f1c10",
          "user": {
            "_id": "65cae89119683f9817c049ea",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65cae89119683f9817c049ea/A0XxjmaJldu28JhFvWmpP.jpeg",
            "isPro": false,
            "fullname": "Wenqi Shi",
            "user": "wshi83",
            "type": "user"
          },
          "name": "Wenqi Shi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-06T07:41:39.845Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-04T19:38:55.000Z",
      "submittedOnDailyAt": "2025-06-06T00:21:54.285Z",
      "title": "MedAgentGym: スケール化されたコードベースの医療論理のためのLLMアガントの訓練",
      "submittedOnDailyBy": {
        "_id": "65cae89119683f9817c049ea",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65cae89119683f9817c049ea/A0XxjmaJldu28JhFvWmpP.jpeg",
        "isPro": false,
        "fullname": "Wenqi Shi",
        "user": "wshi83",
        "type": "user"
      },
      "summary": "MedAgentGYMは、最初の公開できる学習環境です。この環境は、大規模な言語モデル（LLM）アガントの基礎的な医療計画を強化するために設計されています。MedAgentGYMは、129カテゴリからなる72,413タスクインスタンスを含み、真実の医療バイオメディカルシナリオから得られます。タスクは、実行可能なコーディング環境に収められ、詳細なタスク説明、相互作用可能なフィードバック機構、確認可能な真実の注釈、スケーラブルな学習プロジェクト生成を特徴としています。30以上のLLMの拡張検証により、商業APIベースモデルと開放ソースコンタラピーとの間に顕著な性能差異が明らかになりました。MedAgentGYMを活用して、Med-Copilot-7Bは、監督学習（+36.44%）と継続的な強化学習（+42.47%）を通じて大幅な性能向上を収め、gpt-4oと競争的な価格対応で個人際関係保護のための選択肢として立ち上がりました。MedAgentGYMは、一様な実行環境内にカテゴリーごとに拡張可能な詳細なベンチマークとアクセス可能な学習ツーリッズを提供し、LLMベースのコーディングアシスタントを開発するための統合プラットフォームとして提供します。",
      "upvotes": 4,
      "discussionId": "68424552bdc448822b2f1cd0",
      "githubRepo": "https://github.com/wshi83/MedAgentGym",
      "ai_summary": "MedAgentGYM, a training environment for coding-based medical reasoning in LLMs, enhances performance through supervised fine-tuning and reinforcement learning, providing a benchmark and expandable resource.",
      "ai_keywords": [
        "large language model",
        "MedAgentGYM",
        "task instances",
        "biomedical scenarios",
        "coding environments",
        "task descriptions",
        "interactive feedback",
        "ground-truth annotations",
        "training trajectories",
        "LLMs",
        "supervised fine-tuning",
        "reinforcement learning",
        "Med-Copilot-7B",
        "gpt-4o",
        "coding assistants",
        "biomedical research"
      ]
    },
    "publishedAt": "2025-06-04T15:38:55.000Z",
    "title": "MedAgentGym: Training LLM Agents for Code-Based Medical Reasoning at\n  Scale",
    "summary": "We introduce MedAgentGYM, the first publicly available training environment\ndesigned to enhance coding-based medical reasoning capabilities in large\nlanguage model (LLM) agents. MedAgentGYM comprises 72,413 task instances across\n129 categories derived from authentic real-world biomedical scenarios. Tasks\nare encapsulated within executable coding environments, each featuring detailed\ntask descriptions, interactive feedback mechanisms, verifiable ground-truth\nannotations, and scalable training trajectory generation. Extensive\nbenchmarking of over 30 LLMs reveals a notable performance disparity between\ncommercial API-based models and open-source counterparts. Leveraging\nMedAgentGYM, Med-Copilot-7B achieves substantial performance gains through\nsupervised fine-tuning (+36.44%) and continued reinforcement learning\n(+42.47%), emerging as an affordable and privacy-preserving alternative\ncompetitive with gpt-4o. By offering both a comprehensive benchmark and\naccessible, expandable training resources within unified execution\nenvironments, MedAgentGYM delivers an integrated platform to develop LLM-based\ncoding assistants for advanced biomedical research and practice.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04405.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65cae89119683f9817c049ea",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65cae89119683f9817c049ea/A0XxjmaJldu28JhFvWmpP.jpeg",
      "fullname": "Wenqi Shi",
      "name": "wshi83",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.20914",
      "authors": [
        {
          "_id": "68425a585738dda052ea4c91",
          "name": "Jianman Lin",
          "hidden": false
        },
        {
          "_id": "68425a585738dda052ea4c92",
          "name": "Haojie Li",
          "hidden": false
        },
        {
          "_id": "68425a585738dda052ea4c93",
          "name": "Chunmei Qing",
          "hidden": false
        },
        {
          "_id": "68425a585738dda052ea4c94",
          "name": "Zhijing Yang",
          "hidden": false
        },
        {
          "_id": "68425a585738dda052ea4c95",
          "name": "Liang Lin",
          "hidden": false
        },
        {
          "_id": "68425a585738dda052ea4c96",
          "name": "Tianshui Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-27T09:05:28.000Z",
      "submittedOnDailyAt": "2025-06-06T01:33:43.629Z",
      "title": "ジェオメトリー可変であり、外観を保つ物体の組み合わせ",
      "submittedOnDailyBy": {
        "_id": "6332e2689bf698ce68a22e8c",
        "avatarUrl": "/avatars/c1922acfda2e6d2fe7b03194a404eb10.svg",
        "isPro": false,
        "fullname": "JIANTAO LIN",
        "user": "LTT",
        "type": "user"
      },
      "summary": "一般物体构成（GOC）は、目標物体を背景スペースに無間に統合し、選択された幾何学的特性を保っながら、その細かい外観詳細を同時に保存することを目的としています。最近のアプローチは、セマンティックエンボーディングを構築し、それを高度なディフュージョンモデルに統合し、幾何学的なエジェクション可能な生成を可能にします。しかし、これらの高度なエンボーディングは、その上位セマンティックカテゴリーのみを記述し、細かい外観詳細を無駄に捨てることが実存しています。我々は、幾何学的なエジェクションと外観の保存を可能にするディセンテルド幾何学的なディフュージョン（DGAD）モデルを導入します。これは、最初に、セマンティックエンボーディングを利用して、選択された幾何学的変形を隠れて捉え、次に、細かい外観特徴を幾何学的なエジェクションされた表現と一致させるためのクロスアテンション検索機構を使用し、物体の合成での精密な幾何学的なエジェクションと忠実な外観の保存を可能にします。特に、DGADは、CLIP/DINOからのエントリードネットワークを基に、セマンティックエンボーディングと外観保存表現を抽出し、それらは分離して統合され、エンコーディングとデコーディングパイプラインに無間に統合されます。まず、強い空間的認識能力を持つ予ち学習されたディフュージョンモデルにセマンティックエンボーディングを統合し、物体の幾何学を隠れて捉え、柔軟な物体操作を可能にし、効果的な編集可能性を確保します。次に、幾何学的なエジェクションを隠れて学習した物体のエントリーを利用し、外観特徴を対応する領域と空間的に一致させるための密集なクロスアテンション機構を設計し、忠実な外観の一貫性を確保します。公共ベンチマーク上での拡大的な実験は、提案されたDGADフレームワークの効果性を示しています。",
      "upvotes": 4,
      "discussionId": "68425a595738dda052ea4ce4",
      "githubRepo": "https://github.com/jianmanlincjx/DGAD",
      "ai_summary": "The Disentangled Geometry-editable and Appearance-preserving Diffusion (DGAD) model effectively integrates target objects into background scenes by using semantic embeddings for geometry and cross-attention for appearance alignment.",
      "ai_keywords": [
        "disentangled geometry-editable",
        "appearance-preserving diffusion",
        "diffusion models",
        "cross-attention retrieval",
        "CLIP/DINO",
        "reference networks",
        "semantic embeddings",
        "appearance-preserving representations",
        "flexible object manipulation",
        "spatial reasoning capabilities",
        "dense cross-attention mechanism",
        "public benchmarks"
      ]
    },
    "publishedAt": "2025-05-27T05:05:28.000Z",
    "title": "Geometry-Editable and Appearance-Preserving Object Compositon",
    "summary": "General object composition (GOC) aims to seamlessly integrate a target object\ninto a background scene with desired geometric properties, while simultaneously\npreserving its fine-grained appearance details. Recent approaches derive\nsemantic embeddings and integrate them into advanced diffusion models to enable\ngeometry-editable generation. However, these highly compact embeddings encode\nonly high-level semantic cues and inevitably discard fine-grained appearance\ndetails. We introduce a Disentangled Geometry-editable and\nAppearance-preserving Diffusion (DGAD) model that first leverages semantic\nembeddings to implicitly capture the desired geometric transformations and then\nemploys a cross-attention retrieval mechanism to align fine-grained appearance\nfeatures with the geometry-edited representation, facilitating both precise\ngeometry editing and faithful appearance preservation in object composition.\nSpecifically, DGAD builds on CLIP/DINO-derived and reference networks to\nextract semantic embeddings and appearance-preserving representations, which\nare then seamlessly integrated into the encoding and decoding pipelines in a\ndisentangled manner. We first integrate the semantic embeddings into\npre-trained diffusion models that exhibit strong spatial reasoning capabilities\nto implicitly capture object geometry, thereby facilitating flexible object\nmanipulation and ensuring effective editability. Then, we design a dense\ncross-attention mechanism that leverages the implicitly learned object geometry\nto retrieve and spatially align appearance features with their corresponding\nregions, ensuring faithful appearance consistency. Extensive experiments on\npublic benchmarks demonstrate the effectiveness of the proposed DGAD framework.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20914.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6332e2689bf698ce68a22e8c",
      "avatarUrl": "/avatars/c1922acfda2e6d2fe7b03194a404eb10.svg",
      "fullname": "JIANTAO LIN",
      "name": "LTT",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.05348",
      "authors": [
        {
          "_id": "6842a994497e2b62234145d7",
          "name": "Yifan Wang",
          "hidden": false
        },
        {
          "_id": "6842a994497e2b62234145d8",
          "name": "Peishan Yang",
          "hidden": false
        },
        {
          "_id": "6842a994497e2b62234145d9",
          "name": "Zhen Xu",
          "hidden": false
        },
        {
          "_id": "6842a994497e2b62234145da",
          "name": "Jiaming Sun",
          "hidden": false
        },
        {
          "_id": "6842a994497e2b62234145db",
          "name": "Zhanhua Zhang",
          "hidden": false
        },
        {
          "_id": "6842a994497e2b62234145dc",
          "name": "Yong Chen",
          "hidden": false
        },
        {
          "_id": "6842a994497e2b62234145dd",
          "name": "Hujun Bao",
          "hidden": false
        },
        {
          "_id": "6842a994497e2b62234145de",
          "name": "Sida Peng",
          "hidden": false
        },
        {
          "_id": "6842a994497e2b62234145df",
          "name": "Xiaowei Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-05T17:59:57.000Z",
      "submittedOnDailyAt": "2025-06-06T07:18:31.313Z",
      "title": "FreeTimeGS: どこでどの時間でも自由なガウシアンへのアクセスを提供し、動的なスケーニングの再構築に適したソリューション",
      "submittedOnDailyBy": {
        "_id": "6768fc1b75d8e8d042d26732",
        "avatarUrl": "/avatars/6e8c8b26effa41ba4073e69857b0c80a.svg",
        "isPro": false,
        "fullname": "Yifan Wang",
        "user": "wyf2020",
        "type": "user"
      },
      "summary": "この論文は、複雑な動きを持つ動的な3Dシーンの再構築について課題を解決します。最近の研究では、標準空間で3Dガウス素性を定義し、変形フィールドを使用して標準素性を観測空間へとマッピングし、実時間的な動的な視点合成を実現しました。しかし、これらの方法は、変形フィールドの最適化が難しいため、複雑な動きを持つシーンを処理するのに難しくなります。この問題を克服するために、私たちはFreeTimeGSを提案します。これは新しい4D表現で、ガウス素性が任意の時間と位置で出現することを可能にします。標準ガウス素性に比べ、この表現は強い柔軟性を持っているため、動的な3Dシーンのモデリング能力を向上させます。また、各ガウス素性に動作関数を付与し、時間にわたって隣接領域へ移動することを可能にし、時間的な冗餘を減らします。複数のデータセットにおける実験結果は、私たちの方法の渲染質量が最近の方法に比べて大幅に上回ります。",
      "upvotes": 3,
      "discussionId": "6842a996497e2b622341467e",
      "projectPage": "https://zju3dv.github.io/freetimegs/",
      "ai_summary": "A novel 4D representation, FreeTimeGS, enhances the modeling of dynamic 3D scenes by enabling Gaussian primitives to appear at arbitrary times and locations, improving rendering quality compared to existing methods.",
      "ai_keywords": [
        "3D Gaussian primitives",
        "canonical space",
        "deformation fields",
        "real-time dynamic view synthesis",
        "4D representation",
        "motion function",
        "temporal redundancy"
      ]
    },
    "publishedAt": "2025-06-05T13:59:57.000Z",
    "title": "FreeTimeGS: Free Gaussians at Anytime and Anywhere for Dynamic Scene\n  Reconstruction",
    "summary": "This paper addresses the challenge of reconstructing dynamic 3D scenes with\ncomplex motions. Some recent works define 3D Gaussian primitives in the\ncanonical space and use deformation fields to map canonical primitives to\nobservation spaces, achieving real-time dynamic view synthesis. However, these\nmethods often struggle to handle scenes with complex motions due to the\ndifficulty of optimizing deformation fields. To overcome this problem, we\npropose FreeTimeGS, a novel 4D representation that allows Gaussian primitives\nto appear at arbitrary time and locations. In contrast to canonical Gaussian\nprimitives, our representation possesses the strong flexibility, thus improving\nthe ability to model dynamic 3D scenes. In addition, we endow each Gaussian\nprimitive with an motion function, allowing it to move to neighboring regions\nover time, which reduces the temporal redundancy. Experiments results on\nseveral datasets show that the rendering quality of our method outperforms\nrecent methods by a large margin.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05348.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6768fc1b75d8e8d042d26732",
      "avatarUrl": "/avatars/6e8c8b26effa41ba4073e69857b0c80a.svg",
      "fullname": "Yifan Wang",
      "name": "wyf2020",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.05282",
      "authors": [
        {
          "_id": "68425fce548d527097ac00bb",
          "name": "Tao Sun",
          "hidden": false
        },
        {
          "_id": "68425fce548d527097ac00bc",
          "name": "Liyuan Zhu",
          "hidden": false
        },
        {
          "_id": "68425fce548d527097ac00bd",
          "name": "Shengyu Huang",
          "hidden": false
        },
        {
          "_id": "68425fce548d527097ac00be",
          "name": "Shuran Song",
          "hidden": false
        },
        {
          "_id": "68425fce548d527097ac00bf",
          "name": "Iro Armeni",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-05T17:36:03.000Z",
      "submittedOnDailyAt": "2025-06-06T01:56:27.167Z",
      "title": "Rectified Point Flow: 通用の点群データの姿勢推定法",
      "submittedOnDailyBy": {
        "_id": "6503916e0905dd866fd129cb",
        "avatarUrl": "/avatars/818716460d9c9aaa056e0f1b43816c6a.svg",
        "isPro": false,
        "fullname": "Liyuan Zhu",
        "user": "liyzzz",
        "type": "user"
      },
      "summary": "Rectified Point Flowを紹介します。これは、pairwise point cloud registrationとmulti-part shape assemblyを1つの条件付き生成問題として公式化する統一パラメータ化です。無姿勢の点セットを与えると、我々の方法は、ノイズを含む点をターゲット位置へと運搬するための連続的な点ごとの速度場を学習します。これにより、部品の姿勢が復元されます。先行研究と比較して、我々の方法は、汎用的な対称性処理を用いた部品ごとの姿勢の予測を代わり、内訳的に対称性を学習し、対称性ラベルを必要としません。自動転写されたエンコーダーと組み合わせて、我々の方法は6つのベンチマークで新しい最先端の性能を達成します。特に、統一的な公式化は、多様なデータセットによる効果的な併列訓練を可能にし、共有的な幾何的な先驅を学習し、精度を向上させます。プロジェクトページは、https://rectified-pointflow.github.io/です。",
      "upvotes": 3,
      "discussionId": "68425fcf548d527097ac011c",
      "projectPage": "https://rectified-pointflow.github.io/",
      "githubRepo": "https://github.com/GradientSpaces/Rectified-Point-Flow",
      "ai_summary": "Rectified Point Flow unifies pairwise point cloud registration and multi-part shape assembly through a continuous point-wise velocity field, achieving state-of-the-art performance on various benchmarks.",
      "ai_keywords": [
        "Rectified Point Flow",
        "pairwise point cloud registration",
        "multi-part shape assembly",
        "continuous point-wise velocity field",
        "self-supervised encoder",
        "overlapping points",
        "geometric priors"
      ]
    },
    "publishedAt": "2025-06-05T13:36:03.000Z",
    "title": "Rectified Point Flow: Generic Point Cloud Pose Estimation",
    "summary": "We introduce Rectified Point Flow, a unified parameterization that formulates\npairwise point cloud registration and multi-part shape assembly as a single\nconditional generative problem. Given unposed point clouds, our method learns a\ncontinuous point-wise velocity field that transports noisy points toward their\ntarget positions, from which part poses are recovered. In contrast to prior\nwork that regresses part-wise poses with ad-hoc symmetry handling, our method\nintrinsically learns assembly symmetries without symmetry labels. Together with\na self-supervised encoder focused on overlapping points, our method achieves a\nnew state-of-the-art performance on six benchmarks spanning pairwise\nregistration and shape assembly. Notably, our unified formulation enables\neffective joint training on diverse datasets, facilitating the learning of\nshared geometric priors and consequently boosting accuracy. Project page:\nhttps://rectified-pointflow.github.io/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05282.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6503916e0905dd866fd129cb",
      "avatarUrl": "/avatars/818716460d9c9aaa056e0f1b43816c6a.svg",
      "fullname": "Liyuan Zhu",
      "name": "liyzzz",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.00830",
      "authors": [
        {
          "_id": "684264d1a9584289f0053f5c",
          "name": "Zhengcong Fei",
          "hidden": false
        },
        {
          "_id": "684264d1a9584289f0053f5d",
          "name": "Hao Jiang",
          "hidden": false
        },
        {
          "_id": "684264d1a9584289f0053f5e",
          "user": {
            "_id": "65bef422fdb8d33cefeaccc3",
            "avatarUrl": "/avatars/d40b0d7dda21fa1a68c291d11bc357ec.svg",
            "isPro": false,
            "fullname": "Qiu Di",
            "user": "diqiu7",
            "type": "user"
          },
          "name": "Di Qiu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-06T07:40:29.181Z",
          "hidden": false
        },
        {
          "_id": "684264d1a9584289f0053f5f",
          "name": "Baoxuan Gu",
          "hidden": false
        },
        {
          "_id": "684264d1a9584289f0053f60",
          "name": "Youqiang Zhang",
          "hidden": false
        },
        {
          "_id": "684264d1a9584289f0053f61",
          "name": "Jiahua Wang",
          "hidden": false
        },
        {
          "_id": "684264d1a9584289f0053f62",
          "name": "Jialin Bai",
          "hidden": false
        },
        {
          "_id": "684264d1a9584289f0053f63",
          "name": "Debang Li",
          "hidden": false
        },
        {
          "_id": "684264d1a9584289f0053f64",
          "name": "Mingyuan Fan",
          "hidden": false
        },
        {
          "_id": "684264d1a9584289f0053f65",
          "name": "Guibin Chen",
          "hidden": false
        },
        {
          "_id": "684264d1a9584289f0053f66",
          "name": "Yahui Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-01T04:27:13.000Z",
      "submittedOnDailyAt": "2025-06-06T02:21:50.161Z",
      "title": "SkyReels-Audio: オーバニーエコーディションテーブルドラミングトランスフォーマーズによる映画中のオーディオ条件付きのテーブルトークイメージ",
      "submittedOnDailyBy": {
        "_id": "65bef422fdb8d33cefeaccc3",
        "avatarUrl": "/avatars/d40b0d7dda21fa1a68c291d11bc357ec.svg",
        "isPro": false,
        "fullname": "Qiu Di",
        "user": "diqiu7",
        "type": "user"
      },
      "summary": "オーディオ条件付きのタイピングポートレットの生成と編集において、テキスト、画像、映像などの多様な入力を元にガイドする手法はまだ見られていません。本論文では、高品質で時系列的に一貫したタイピングポートレットビデオの合成を目的としたSkyReels-Audioの一連のフレームワークを提出します。このフレームワークは、事前学習されたビデオディフュージョントランスフォーマーを基盤として構築されており、無限長の生成と編集を可能にし、多様な条件付けを可能にします。また、長いビデオシーケンスにおける細かい多様な制御を可能にします。これを実現するためには、ファシールムの動きとオーディオの同期を進歩的に調整する組み合わせだージュ学習戦略を用いています。また、ファシールムの局所的な一貫性を向上させるために、ファシールムマスク損失とオーディオガイドドライバーフレームワークを導入しています。さらに、スライディングウィンドウデノイズアプローチを用いて、時系列的なセグメント間での潜在表現を融合させ、長期間および多様な識別子においてもビジュアルの品質と時系列的一貫性を保証します。より重要な点については、高品質のサンクローナイションされたオーディオ、ビデオ、テキストの記述を含むデータタイプのデータパイプラインを構築しました。詳細なベンチマーク評価により、SkyReels-Audioは、唇同期の精度、識別子の一貫性、写真の実感性などにおいて、特に複雑な条件の下でも優れた性能を収めています。",
      "upvotes": 3,
      "discussionId": "684264d2a9584289f0053fc9",
      "ai_summary": "SkyReels-Audio is a unified framework using pretrained video diffusion transformers for generating high-fidelity and coherent audio-conditioned talking portrait videos, supported by a hybrid curriculum learning strategy and advanced loss mechanisms.",
      "ai_keywords": [
        "video diffusion transformers",
        "infinite-length generation",
        "multimodal inputs",
        "hybrid curriculum learning",
        "facial mask loss",
        "classifier-free guidance mechanism",
        "sliding-window denoising",
        "lip-sync accuracy",
        "identity consistency",
        "realistic facial dynamics"
      ]
    },
    "publishedAt": "2025-06-01T00:27:13.000Z",
    "title": "SkyReels-Audio: Omni Audio-Conditioned Talking Portraits in Video\n  Diffusion Transformers",
    "summary": "The generation and editing of audio-conditioned talking portraits guided by\nmultimodal inputs, including text, images, and videos, remains under explored.\nIn this paper, we present SkyReels-Audio, a unified framework for synthesizing\nhigh-fidelity and temporally coherent talking portrait videos. Built upon\npretrained video diffusion transformers, our framework supports infinite-length\ngeneration and editing, while enabling diverse and controllable conditioning\nthrough multimodal inputs. We employ a hybrid curriculum learning strategy to\nprogressively align audio with facial motion, enabling fine-grained multimodal\ncontrol over long video sequences. To enhance local facial coherence, we\nintroduce a facial mask loss and an audio-guided classifier-free guidance\nmechanism. A sliding-window denoising approach further fuses latent\nrepresentations across temporal segments, ensuring visual fidelity and temporal\nconsistency across extended durations and diverse identities. More importantly,\nwe construct a dedicated data pipeline for curating high-quality triplets\nconsisting of synchronized audio, video, and textual descriptions.\nComprehensive benchmark evaluations show that SkyReels-Audio achieves superior\nperformance in lip-sync accuracy, identity consistency, and realistic facial\ndynamics, particularly under complex and challenging conditions.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.00830.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65bef422fdb8d33cefeaccc3",
      "avatarUrl": "/avatars/d40b0d7dda21fa1a68c291d11bc357ec.svg",
      "fullname": "Qiu Di",
      "name": "diqiu7",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.04245",
      "authors": [
        {
          "_id": "68425054feb46a093178003f",
          "user": {
            "_id": "64ff4b1a0e8369f6a8c47c7e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ff4b1a0e8369f6a8c47c7e/X8ocOagj89gexSa253q_8.png",
            "isPro": false,
            "fullname": "Eric Lan",
            "user": "Eric-Lan",
            "type": "user"
          },
          "name": "Guangchen Lan",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-06-06T02:20:41.949Z",
          "hidden": false
        },
        {
          "_id": "68425054feb46a0931780040",
          "name": "Huseyin A. Inan",
          "hidden": false
        },
        {
          "_id": "68425054feb46a0931780041",
          "user": {
            "_id": "65e88cdd95a27dfbf6b4e63b",
            "avatarUrl": "/avatars/3d2d270398f0824b392f99e158e94f26.svg",
            "isPro": false,
            "fullname": "Sahar Abdelnabi",
            "user": "sahar-abdelnabi",
            "type": "user"
          },
          "name": "Sahar Abdelnabi",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-06T02:20:06.391Z",
          "hidden": false
        },
        {
          "_id": "68425054feb46a0931780042",
          "name": "Janardhan Kulkarni",
          "hidden": false
        },
        {
          "_id": "68425054feb46a0931780043",
          "user": {
            "_id": "6380a37a5c62156ce7dff8b9",
            "avatarUrl": "/avatars/fbe5a20869cb55ec43759c1b5f9c4135.svg",
            "isPro": false,
            "fullname": "Lukas Wutschitz",
            "user": "wulu",
            "type": "user"
          },
          "name": "Lukas Wutschitz",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-06-06T09:45:54.243Z",
          "hidden": false
        },
        {
          "_id": "68425054feb46a0931780044",
          "name": "Reza Shokri",
          "hidden": false
        },
        {
          "_id": "68425054feb46a0931780045",
          "name": "Christopher G. Brinton",
          "hidden": false
        },
        {
          "_id": "68425054feb46a0931780046",
          "name": "Robert Sim",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T21:26:21.000Z",
      "submittedOnDailyAt": "2025-06-06T00:52:31.028Z",
      "title": "LLMのコンテキストフィールドの整徹性における理由論と強化学習",
      "submittedOnDailyBy": {
        "_id": "64ff4b1a0e8369f6a8c47c7e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ff4b1a0e8369f6a8c47c7e/X8ocOagj89gexSa253q_8.png",
        "isPro": false,
        "fullname": "Eric Lan",
        "user": "Eric-Lan",
        "type": "user"
      },
      "summary": "自ユーザーを代理して決策を取る自動主義アグエントの時代が開始し、コンテキストの整徹性（CI）の確保がこの分野の中心的な問題となります。CIは、特定のタスクを実行する際に適切な情報を共有することができるかどうかを判断することです。我々は、CIの実現にはアグエントが実行中のコンテキストについて理由を与える必要があることを主張します。これを検証するために、最初にLLMsについて、CIに関する理由を明確にするように促すことを試みました。その後、このアプローチを拡張し、強化学習（RL）フレームワークを開発し、モデルに必要な理由をより深く与えることを図りました。合成的な、自動的に作成されたデータセット（例700件のみであり、多様なコンテキストと情報公開のルールを含む）を使用して、我々の方法は、多くのモデルサイズとファミリーでタスクの実行性能を維持する同時に、不適切な情報公開を大幅に減少することを示しました。重要なことに、この合成的なデータセットからの向上は、人間のアノテーションを含む既存のCIベンチマーク（例えば、PrivacyLens）にも影響を与え、AIアシスタントのアクションとツールコールにおけるプライバシー漏れの評価についても評価されます。",
      "upvotes": 3,
      "discussionId": "68425056feb46a09317800d9",
      "ai_summary": "A reinforcement learning framework for LLMs enhances contextual integrity by reducing inappropriate information disclosure and maintaining task performance across various benchmarks.",
      "ai_keywords": [
        "LLMs",
        "reinforcement learning",
        "contextual integrity",
        "information disclosure",
        "synthetic dataset",
        "PrivacyLens",
        "privacy leakage",
        "AI assistants"
      ]
    },
    "publishedAt": "2025-05-29T17:26:21.000Z",
    "title": "Contextual Integrity in LLMs via Reasoning and Reinforcement Learning",
    "summary": "As the era of autonomous agents making decisions on behalf of users unfolds,\nensuring contextual integrity (CI) -- what is the appropriate information to\nshare while carrying out a certain task -- becomes a central question to the\nfield. We posit that CI demands a form of reasoning where the agent needs to\nreason about the context in which it is operating. To test this, we first\nprompt LLMs to reason explicitly about CI when deciding what information to\ndisclose. We then extend this approach by developing a reinforcement learning\n(RL) framework that further instills in models the reasoning necessary to\nachieve CI. Using a synthetic, automatically created, dataset of only sim700\nexamples but with diverse contexts and information disclosure norms, we show\nthat our method substantially reduces inappropriate information disclosure\nwhile maintaining task performance across multiple model sizes and families.\nImportantly, improvements transfer from this synthetic dataset to established\nCI benchmarks such as PrivacyLens that has human annotations and evaluates\nprivacy leakage of AI assistants in actions and tool calls.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04245.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ff4b1a0e8369f6a8c47c7e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ff4b1a0e8369f6a8c47c7e/X8ocOagj89gexSa253q_8.png",
      "fullname": "Eric Lan",
      "name": "Eric-Lan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.05278",
      "authors": [
        {
          "_id": "68426dfeb5f4d2d0f8fd098e",
          "user": {
            "_id": "60adfff0306d6873ec42d545",
            "avatarUrl": "/avatars/4a63f90638dbffebfeeee181a6d0220c.svg",
            "isPro": false,
            "fullname": "Nan",
            "user": "NanHUO",
            "type": "user"
          },
          "name": "Nan Huo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-06T07:40:24.419Z",
          "hidden": false
        },
        {
          "_id": "68426dfeb5f4d2d0f8fd098f",
          "name": "Jinyang Li",
          "hidden": false
        },
        {
          "_id": "68426dfeb5f4d2d0f8fd0990",
          "name": "Bowen Qin",
          "hidden": false
        },
        {
          "_id": "68426dfeb5f4d2d0f8fd0991",
          "name": "Ge Qu",
          "hidden": false
        },
        {
          "_id": "68426dfeb5f4d2d0f8fd0992",
          "name": "Xiaolong Li",
          "hidden": false
        },
        {
          "_id": "68426dfeb5f4d2d0f8fd0993",
          "name": "Xiaodong Li",
          "hidden": false
        },
        {
          "_id": "68426dfeb5f4d2d0f8fd0994",
          "name": "Chenhao Ma",
          "hidden": false
        },
        {
          "_id": "68426dfeb5f4d2d0f8fd0995",
          "name": "Reynold Cheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-05T17:33:02.000Z",
      "submittedOnDailyAt": "2025-06-06T02:57:34.041Z",
      "title": "Micro-Act: クエスト回答での知識衝突を軽減するための行動可能な自己理由論理",
      "submittedOnDailyBy": {
        "_id": "6419435385030eca6ac94701",
        "avatarUrl": "/avatars/c49ff1991739de49ec98c8310ab21e46.svg",
        "isPro": false,
        "fullname": "Ge Qu",
        "user": "gq2138",
        "type": "user"
      },
      "summary": "レビュアル・アウゲーション（RAG）システムは、検索された外部知識が大規模な言語モデル（LLMs）の固有のパラメトリック知識と矛盾していることで、知識コンフリクトに苦戦します。これは、問題回答（QA）などのダウンストラムタスクの性能に不利に影響します。現在のアプローチは、2つの知識ソースを並列で比較してコンフリクトを軽減することを試みていますが、これは過剰なコンテキストをLLMsに負担させ、矛盾の識別と軽減において妨げることになります。この問題に対処するために、我々は、上下位の行動空間を持つマイクロアクション（Micro-Act）フレームワークを提案します。このフレームワークは、コンテキストの複雑さを自動的に認識し、各知識ソースを細かい比較の順番に適応的に分解します。これらの比較は、行動可能なステップとして表現され、表面的なコンテキストを超える推理により可能になります。5データセットの幅広い実験を通じて、Micro-Actはすべての5データセットと3コンフリクトタイプの上で、状態の最先端のベースラインと比較してQAの精度が顕著に向上します。特に、時間的や語義的なタイプでは、すべてのベースラインが顕著に失敗しているのに対して、Micro-Actは優れた性能を示します。より重要なのは、Micro-Actは非コンフリクトの問題にも強固な性能を示し、実世界的なRAGアプリケーションでの実用的な価値を示しています。",
      "upvotes": 2,
      "discussionId": "68426dfeb5f4d2d0f8fd09c7",
      "ai_summary": "A framework called Micro-Act addresses Knowledge Conflicts in Retrieval-Augmented Generation by adaptively decomposing knowledge sources, leading to improved QA accuracy compared to existing methods.",
      "ai_keywords": [
        "Retrieval-Augmented Generation",
        "Knowledge Conflicts",
        "large language models",
        "parametric knowledge",
        "question answering",
        "hierarchical action space",
        "context complexity",
        "fine-grained comparisons",
        "actionable steps",
        "benchmark datasets",
        "QA accuracy",
        "conflict types",
        "temporal conflicts",
        "semantic conflicts",
        "non-conflict questions"
      ]
    },
    "publishedAt": "2025-06-05T13:33:02.000Z",
    "title": "Micro-Act: Mitigate Knowledge Conflict in Question Answering via\n  Actionable Self-Reasoning",
    "summary": "Retrieval-Augmented Generation (RAG) systems commonly suffer from Knowledge\nConflicts, where retrieved external knowledge contradicts the inherent,\nparametric knowledge of large language models (LLMs). It adversely affects\nperformance on downstream tasks such as question answering (QA). Existing\napproaches often attempt to mitigate conflicts by directly comparing two\nknowledge sources in a side-by-side manner, but this can overwhelm LLMs with\nextraneous or lengthy contexts, ultimately hindering their ability to identify\nand mitigate inconsistencies. To address this issue, we propose Micro-Act a\nframework with a hierarchical action space that automatically perceives context\ncomplexity and adaptively decomposes each knowledge source into a sequence of\nfine-grained comparisons. These comparisons are represented as actionable\nsteps, enabling reasoning beyond the superficial context. Through extensive\nexperiments on five benchmark datasets, Micro-Act consistently achieves\nsignificant increase in QA accuracy over state-of-the-art baselines across all\n5 datasets and 3 conflict types, especially in temporal and semantic types\nwhere all baselines fail significantly. More importantly, Micro-Act exhibits\nrobust performance on non-conflict questions simultaneously, highlighting its\npractical value in real-world RAG applications.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05278.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6419435385030eca6ac94701",
      "avatarUrl": "/avatars/c49ff1991739de49ec98c8310ab21e46.svg",
      "fullname": "Ge Qu",
      "name": "gq2138",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.05229",
      "authors": [
        {
          "_id": "6842bc6855574a112d5733cc",
          "name": "Danil Sivtsov",
          "hidden": false
        },
        {
          "_id": "6842bc6855574a112d5733cd",
          "name": "Ivan Rodkin",
          "hidden": false
        },
        {
          "_id": "6842bc6855574a112d5733ce",
          "name": "Gleb Kuzmin",
          "hidden": false
        },
        {
          "_id": "6842bc6855574a112d5733cf",
          "name": "Yuri Kuratov",
          "hidden": false
        },
        {
          "_id": "6842bc6855574a112d5733d0",
          "name": "Ivan Oseledets",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-05T16:43:48.000Z",
      "submittedOnDailyAt": "2025-06-06T08:35:33.323Z",
      "title": "対角バッチグリッドで長期コンテキストの再帰的メモリトランスフォーマーでの並列化を解放する",
      "submittedOnDailyBy": {
        "_id": "618b9540682ec1c38327e586",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/618b9540682ec1c38327e586/v_ZBkfh8O9Zh6C2YQpuBX.jpeg",
        "isPro": false,
        "fullname": "Yury Kuratov",
        "user": "yurakuratov",
        "type": "user"
      },
      "summary": "Transformerモデルは、長文脈推論に対して、二次元時間と線形メモリ複雑性により困難を見出す。Recurrent Memory Transformers（RMTs）は、この問題を解決するために、アスインコストを線形時間と定数メモリ使用に抑える。しかし、そのメモリ更新機構は、順次実行により性能バックロックを引き起こす。\n\n我々は、Segmentごとの並列性を解放しながら、正確な再帰を維持するスケジューリングシナプスであるDiagonal Batchingを導入します。このアプローチは、順次制約を除去し、単一の長文脈入力でも複雑なバッチングとパイプラインテクニックを必要としないように、効率的なGPU推論を可能にします。この手法は、すべての実行時計算再配列化であるため、既存のRMTモデルは再学習を必要とさせません。\n\nLLaMA-1B ARMTモデルに対して応用されたDiagonal Batchingは、標準の全注意LLaMA-1Bより3.3倍のスピードアップと、131,072トークンシーケンスの順次RMT実装より1.8倍のスピードアップを収めます。順次バックロックを除去することで、Diagonal Batchingは推論コストとラテンシーを減少させ、RMTの実用的な解決策としての強化を実現します。",
      "upvotes": 2,
      "discussionId": "6842bc6955574a112d573421",
      "ai_summary": "Diagonal Batching enables parallel inference in Recurrent Memory Transformers, significantly improving speed and efficiency for long-context tasks.",
      "ai_keywords": [
        "Transformer models",
        "long-context inference",
        "quadratic time complexity",
        "linear memory complexity",
        "Recurrent Memory Transformers",
        "RMTs",
        "memory update mechanism",
        "sequential execution",
        "Diagonal Batching",
        "run-time computation reordering",
        "parallelism",
        "GPU inference",
        "LLaMA-1B ARMT model",
        "full-attention LLaMA-1B",
        "inference cost",
        "latency"
      ]
    },
    "publishedAt": "2025-06-05T12:43:48.000Z",
    "title": "Diagonal Batching Unlocks Parallelism in Recurrent Memory Transformers\n  for Long Contexts",
    "summary": "Transformer models struggle with long-context inference due to their\nquadratic time and linear memory complexity. Recurrent Memory Transformers\n(RMTs) offer a solution by reducing the asymptotic cost to linear time and\nconstant memory usage. However, their memory update mechanism leads to\nsequential execution, causing a performance bottleneck.\n  We introduce Diagonal Batching, a scheduling scheme that unlocks parallelism\nacross segments in RMTs while preserving exact recurrence. This approach\neliminates the sequential constraint, enabling efficient GPU inference even for\nsingle long-context inputs without complex batching and pipelining techniques.\nBecause the technique is purely a run-time computation reordering, existing RMT\nmodels adopt it with no retraining.\n  Applied to a LLaMA-1B ARMT model, Diagonal Batching yields a 3.3x speedup\nover standard full-attention LLaMA-1B and a 1.8x speedup over the sequential\nRMT implementation on 131,072-token sequences. By removing sequential\nbottleneck, Diagonal Batching reduces inference cost and latency, thereby\nstrengthening RMTs as a practical solution for real-world, long-context\napplications.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05229.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "618b9540682ec1c38327e586",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/618b9540682ec1c38327e586/v_ZBkfh8O9Zh6C2YQpuBX.jpeg",
      "fullname": "Yury Kuratov",
      "name": "yurakuratov",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.04734",
      "authors": [
        {
          "_id": "6842537f1c4f28a2031f499c",
          "user": {
            "_id": "632c30576bcb864974cc40a8",
            "avatarUrl": "/avatars/96aa948ad1dd35d355e20b5765a2563a.svg",
            "isPro": false,
            "fullname": "sunlin",
            "user": "lincharliesun",
            "type": "user"
          },
          "name": "Lin Sun",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-06T07:41:08.211Z",
          "hidden": false
        },
        {
          "_id": "6842537f1c4f28a2031f499d",
          "name": "Weihong Lin",
          "hidden": false
        },
        {
          "_id": "6842537f1c4f28a2031f499e",
          "name": "Jinzhu Wu",
          "hidden": false
        },
        {
          "_id": "6842537f1c4f28a2031f499f",
          "name": "Yongfu Zhu",
          "hidden": false
        },
        {
          "_id": "6842537f1c4f28a2031f49a0",
          "name": "Xiaoqi Jian",
          "hidden": false
        },
        {
          "_id": "6842537f1c4f28a2031f49a1",
          "name": "Guangxiang Zhao",
          "hidden": false
        },
        {
          "_id": "6842537f1c4f28a2031f49a2",
          "name": "Change Jia",
          "hidden": false
        },
        {
          "_id": "6842537f1c4f28a2031f49a3",
          "name": "Linglin Zhang",
          "hidden": false
        },
        {
          "_id": "6842537f1c4f28a2031f49a4",
          "name": "Sai-er Hu",
          "hidden": false
        },
        {
          "_id": "6842537f1c4f28a2031f49a5",
          "name": "Yuhan Wu",
          "hidden": false
        },
        {
          "_id": "6842537f1c4f28a2031f49a6",
          "name": "Xiangzheng Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-05T08:09:11.000Z",
      "submittedOnDailyAt": "2025-06-06T01:04:27.438Z",
      "title": "評価はすべてであれば十分：評価設計によるLLM推論能力の戦略的な過評価",
      "submittedOnDailyBy": {
        "_id": "632c30576bcb864974cc40a8",
        "avatarUrl": "/avatars/96aa948ad1dd35d355e20b5765a2563a.svg",
        "isPro": false,
        "fullname": "sunlin",
        "user": "lincharliesun",
        "type": "user"
      },
      "summary": "Deepseek-R1-Distill 系列のモデルは、数学、科学、プログラミング、その他の分野での強い性能をもってオープンソースコミュニティに広く採用されています。しかし、我々の研究によると、それらのベンチマーク評価結果は、多様な要因による大きな波動を伴います。評価条件の微妙な違いが、結果について大幅な変化を引き起こすことがあります。同様の現象は、Deepseek-R1-Distill 系列に基づいて微調節されたその他のオープンソース推論モデルにも見られ、QwQ-32B モデルにも見られ、その主張された性能向上が信頼性のある再現できないことになります。そこで、我々はモデルの性能評価のより厳密なパラダイムの構築を主張し、Deepseek-R1-Distill 系列モデルの実験的評価を提供します。",
      "upvotes": 2,
      "discussionId": "684253811c4f28a2031f4a11",
      "ai_summary": "Empirical assessments reveal significant fluctuations in benchmark evaluation results of Deepseek-R1-Distill models, questioning the reliability of claimed performance improvements and advocating for a more rigorous evaluation paradigm.",
      "ai_keywords": [
        "reasoning models",
        "Deepseek-R1-Distill",
        "benchmark evaluation",
        "open-source inference models",
        "performance variations",
        "QwQ-32B model"
      ]
    },
    "publishedAt": "2025-06-05T04:09:11.000Z",
    "title": "Evaluation is All You Need: Strategic Overclaiming of LLM Reasoning\n  Capabilities Through Evaluation Design",
    "summary": "Reasoning models represented by the Deepseek-R1-Distill series have been\nwidely adopted by the open-source community due to their strong performance in\nmathematics, science, programming, and other domains. However, our study\nreveals that their benchmark evaluation results are subject to significant\nfluctuations caused by various factors. Subtle differences in evaluation\nconditions can lead to substantial variations in results. Similar phenomena are\nobserved in other open-source inference models fine-tuned based on the\nDeepseek-R1-Distill series, as well as in the QwQ-32B model, making their\nclaimed performance improvements difficult to reproduce reliably. Therefore, we\nadvocate for the establishment of a more rigorous paradigm for model\nperformance evaluation and present our empirical assessments of the\nDeepseek-R1-Distill series models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04734.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "632c30576bcb864974cc40a8",
      "avatarUrl": "/avatars/96aa948ad1dd35d355e20b5765a2563a.svg",
      "fullname": "sunlin",
      "name": "lincharliesun",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.02751",
      "authors": [
        {
          "_id": "68425f88fa50fdb6ce3674b5",
          "user": {
            "_id": "67e6679e4b036872ccb9448d",
            "avatarUrl": "/avatars/04dff7f816046f6e82910c894db10ab1.svg",
            "isPro": false,
            "fullname": "fcyycf",
            "user": "fcy99",
            "type": "user"
          },
          "name": "Chuanyu Fu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-06T07:40:53.825Z",
          "hidden": false
        },
        {
          "_id": "68425f88fa50fdb6ce3674b6",
          "name": "Yuqi Zhang",
          "hidden": false
        },
        {
          "_id": "68425f88fa50fdb6ce3674b7",
          "name": "Kunbin Yao",
          "hidden": false
        },
        {
          "_id": "68425f88fa50fdb6ce3674b8",
          "name": "Guanying Chen",
          "hidden": false
        },
        {
          "_id": "68425f88fa50fdb6ce3674b9",
          "name": "Yuan Xiong",
          "hidden": false
        },
        {
          "_id": "68425f88fa50fdb6ce3674ba",
          "name": "Chuan Huang",
          "hidden": false
        },
        {
          "_id": "68425f88fa50fdb6ce3674bb",
          "name": "Shuguang Cui",
          "hidden": false
        },
        {
          "_id": "68425f88fa50fdb6ce3674bc",
          "name": "Xiaochun Cao",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/67e6679e4b036872ccb9448d/qy_cnBlPGQPb1e8qZyWGs.mp4"
      ],
      "publishedAt": "2025-06-03T11:13:48.000Z",
      "submittedOnDailyAt": "2025-06-06T06:39:38.846Z",
      "title": "RobustSplat: 密度と動力学の解離による瞬間無しの3DGS",
      "submittedOnDailyBy": {
        "_id": "67e6679e4b036872ccb9448d",
        "avatarUrl": "/avatars/04dff7f816046f6e82910c894db10ab1.svg",
        "isPro": false,
        "fullname": "fcyycf",
        "user": "fcy99",
        "type": "user"
      },
      "summary": "3D Gaussian Splatting (3DGS)は、新視点合成と3Dモデリングにおいて時間実時で写真写真のレンダリングを行うために注目を集めています。しかし、現在の方法は、瞬時物体によって影響されたスケーンを正確にモデル化することが難しく、レンダリング画像にエフェクトを生じます。我々は、Gaussian densificationプロセスが、スケーンの詳細を抜き出すことを効果的に行うにつれて、瞬時的なディステラビューをモデル化するために追加的なGaussianを生成し、これによりエフェクトを生じることを見出しました。これを解決するために、我々はRobustSplatという強固な解決策を提案します。これは、2つの重要な設計に基づいています。1つ目に、Gaussianの増殖を遅延するステラジェストを導入し、静的なスケーン構造の最適化を優先し、早期の最適化で瞬時物体による過学習を抑えます。2つ目に、スケール連鎖マスクブーストリュープアプローチを設計し、低解像度の特徴類似性サブジェクションを利用して確信的な初期の瞬時マスクの推定を行い、その強い語意的な一致性と噪音の耐性を活用し、高解像度のサブジェクションを利用してより正確なマスク予測を実現することを目指します。複数の難しいデータセット上での拡張的な実験により、我々の方法は現在の方法を上回り、強固さと効果性を明らかにしました。我々のプロジェクトページは、https://fcyycf.github.io/RobustSplat/ です。",
      "upvotes": 2,
      "discussionId": "68425f8afa50fdb6ce367531",
      "projectPage": "https://fcyycf.github.io/RobustSplat/",
      "githubRepo": "https://github.com/fcyycf/RobustSplat",
      "ai_summary": "RobustSplat addresses artifacts in 3D Gaussian Splatting caused by transient objects through delayed Gaussian growth and scale-cascaded mask bootstrapping.",
      "ai_keywords": [
        "Gaussian Splatting",
        "novel-view synthesis",
        "3D modeling",
        "Gaussian densification",
        "transient objects",
        "Gaussian growth",
        "delayed Gaussian growth",
        "scale-cascaded mask bootstrapping",
        "feature similarity",
        "mask prediction"
      ]
    },
    "publishedAt": "2025-06-03T07:13:48.000Z",
    "title": "RobustSplat: Decoupling Densification and Dynamics for Transient-Free\n  3DGS",
    "summary": "3D Gaussian Splatting (3DGS) has gained significant attention for its\nreal-time, photo-realistic rendering in novel-view synthesis and 3D modeling.\nHowever, existing methods struggle with accurately modeling scenes affected by\ntransient objects, leading to artifacts in the rendered images. We identify\nthat the Gaussian densification process, while enhancing scene detail capture,\nunintentionally contributes to these artifacts by growing additional Gaussians\nthat model transient disturbances. To address this, we propose RobustSplat, a\nrobust solution based on two critical designs. First, we introduce a delayed\nGaussian growth strategy that prioritizes optimizing static scene structure\nbefore allowing Gaussian splitting/cloning, mitigating overfitting to transient\nobjects in early optimization. Second, we design a scale-cascaded mask\nbootstrapping approach that first leverages lower-resolution feature similarity\nsupervision for reliable initial transient mask estimation, taking advantage of\nits stronger semantic consistency and robustness to noise, and then progresses\nto high-resolution supervision to achieve more precise mask prediction.\nExtensive experiments on multiple challenging datasets show that our method\noutperforms existing methods, clearly demonstrating the robustness and\neffectiveness of our method. Our project page is\nhttps://fcyycf.github.io/RobustSplat/.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/67e6679e4b036872ccb9448d/qy_cnBlPGQPb1e8qZyWGs.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.02751.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67e6679e4b036872ccb9448d",
      "avatarUrl": "/avatars/04dff7f816046f6e82910c894db10ab1.svg",
      "fullname": "fcyycf",
      "name": "fcy99",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.23115",
      "authors": [
        {
          "_id": "6842afaa6b5d1e675f254cf1",
          "name": "Yunshen Wang",
          "hidden": false
        },
        {
          "_id": "6842afaa6b5d1e675f254cf2",
          "name": "Yicheng Liu",
          "hidden": false
        },
        {
          "_id": "6842afaa6b5d1e675f254cf3",
          "name": "Tianyuan Yuan",
          "hidden": false
        },
        {
          "_id": "6842afaa6b5d1e675f254cf4",
          "name": "Yucheng Mao",
          "hidden": false
        },
        {
          "_id": "6842afaa6b5d1e675f254cf5",
          "name": "Yingshi Liang",
          "hidden": false
        },
        {
          "_id": "6842afaa6b5d1e675f254cf6",
          "name": "Xiuyu Yang",
          "hidden": false
        },
        {
          "_id": "6842afaa6b5d1e675f254cf7",
          "name": "Honggang Zhang",
          "hidden": false
        },
        {
          "_id": "6842afaa6b5d1e675f254cf8",
          "name": "Hang Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T05:34:22.000Z",
      "submittedOnDailyAt": "2025-06-06T07:37:16.625Z",
      "title": "ディフュージョンベースの生成モデルを用いた自動運転の3次元占有率予測",
      "submittedOnDailyBy": {
        "_id": "634aab35dcf125e4dafc87b1",
        "avatarUrl": "/avatars/aa2db84fd423e9eefe3ef3167c9d3999.svg",
        "isPro": false,
        "fullname": "YangXiuyu",
        "user": "gzzyyxy",
        "type": "user"
      },
      "summary": "3Dオケイャンプグリッドの決定を視覚入力から正確に予測することは自動運転に重要ですが、現在の分類的な方法は、ノイズのあるデータ、不完全な観測、3Dシーンにおける複雑な構造に対して困難を抱えます。本研究では、ディフュージョンモデルを用いて3Dオケイャンプ予測を生成モデリングタスクとして再構成し、潜在的なデータ分布を学習し、3Dシーンの先驗知識を統合します。このアプローチは予測の一致性、ノイズの耐性を高め、3D空間構造の複雑性をより良く扱います。拡張された実験により、ディフュージョンベースの生成モデルは最先端の分類的なアプローチを上回り、よりリアルズマイナスで正確なオケイャンプ予測を提供し、特に遮蔽されたもしくは視界が低い領域でも特に優秀です。また、向上された予測は下流の計画タスクに大幅に利益を与え、我々の方法の実用的な優しさが自動運転の実世界エンドラインアプリケーションにおいて明らかにしています。",
      "upvotes": 2,
      "discussionId": "6842afac6b5d1e675f254db5",
      "ai_summary": "Diffusion models improve 3D occupancy prediction from visual inputs, enhancing accuracy and robustness in complex and occluded scenes, which benefits autonomous driving.",
      "ai_keywords": [
        "diffusion models",
        "generative modeling",
        "3D occupancy grids",
        "autonomous driving",
        "noise robustness",
        "3D scene priors",
        "downstream planning tasks"
      ]
    },
    "publishedAt": "2025-05-29T01:34:22.000Z",
    "title": "Diffusion-Based Generative Models for 3D Occupancy Prediction in\n  Autonomous Driving",
    "summary": "Accurately predicting 3D occupancy grids from visual inputs is critical for\nautonomous driving, but current discriminative methods struggle with noisy\ndata, incomplete observations, and the complex structures inherent in 3D\nscenes. In this work, we reframe 3D occupancy prediction as a generative\nmodeling task using diffusion models, which learn the underlying data\ndistribution and incorporate 3D scene priors. This approach enhances prediction\nconsistency, noise robustness, and better handles the intricacies of 3D spatial\nstructures. Our extensive experiments show that diffusion-based generative\nmodels outperform state-of-the-art discriminative approaches, delivering more\nrealistic and accurate occupancy predictions, especially in occluded or\nlow-visibility regions. Moreover, the improved predictions significantly\nbenefit downstream planning tasks, highlighting the practical advantages of our\nmethod for real-world autonomous driving applications.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23115.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "634aab35dcf125e4dafc87b1",
      "avatarUrl": "/avatars/aa2db84fd423e9eefe3ef3167c9d3999.svg",
      "fullname": "YangXiuyu",
      "name": "gzzyyxy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.03238",
      "authors": [
        {
          "_id": "684158e2f11e4b2c51fce923",
          "user": {
            "_id": "6496eae78a7c70379a512e39",
            "avatarUrl": "/avatars/f5ab483ae93cc04b43e825dfd9440905.svg",
            "isPro": false,
            "fullname": "Ziheng Zhao",
            "user": "zzh99",
            "type": "user"
          },
          "name": "Ziheng Zhao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T10:00:00.086Z",
          "hidden": false
        },
        {
          "_id": "684158e2f11e4b2c51fce924",
          "name": "Lisong Dai",
          "hidden": false
        },
        {
          "_id": "684158e2f11e4b2c51fce925",
          "name": "Ya Zhang",
          "hidden": false
        },
        {
          "_id": "684158e2f11e4b2c51fce926",
          "name": "Yanfeng Wang",
          "hidden": false
        },
        {
          "_id": "684158e2f11e4b2c51fce927",
          "name": "Weidi Xie",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T17:57:34.000Z",
      "submittedOnDailyAt": "2025-06-06T00:32:31.925Z",
      "title": "リセットされた全身CT画像の解釈：悪性ものシンポジション的なアプローチ",
      "submittedOnDailyBy": {
        "_id": "6496eae78a7c70379a512e39",
        "avatarUrl": "/avatars/f5ab483ae93cc04b43e825dfd9440905.svg",
        "isPro": false,
        "fullname": "Ziheng Zhao",
        "user": "zzh99",
        "type": "user"
      },
      "summary": "CT画像の自動訳譯―特に、多平面と全身スキームでの異常検出の定位と説明―は、臨床放射線医学では重大な課題です。本研究は、以下の4つの主な貢献によりこの課題を解決することを目的としています： (i) 分類学について、高級放射線医師と協力して、全体の体験域における404件の代表的な異常検出を含む、一構想の分類システムを提案します。 (ii) データについて、多平面と全体の体験域からの14,500点以上のCT画像を含むデータセットを提供し、19,000点以上の異常検出に対して細かくゲーティングアノテーションを提供し、各々の異常検出に詳細な説明を付け、分類システムにカスタム化します。 (iii) モデル開発について、OminiAbnorm-CTを提案し、多平面と全身のCT画像上での異常検出の自動ゲーティングと説明を行うことができ、さらに、可視的プロンプトを通じて柔軟な相互作用を可能にします。 (iv) ベンチマークについて、実際の臨床スキームに基づいた3つの代表的な評価タスクを設定します。拡大的な実験を通じて、OminiAbnorm-CTはすべてのタスクとメトリックにおいて現在の方法よりも显著に優れていることを示します。",
      "upvotes": 1,
      "discussionId": "684158e3f11e4b2c51fce9d7",
      "ai_summary": "OminiAbnorm-CT, a model for automated interpretation of CT images, outperforms existing methods in localizing and describing abnormalities across different body regions using text queries and visual prompts.",
      "ai_keywords": [
        "OminiAbnorm-CT"
      ]
    },
    "publishedAt": "2025-06-03T13:57:34.000Z",
    "title": "Rethinking Whole-Body CT Image Interpretation: An Abnormality-Centric\n  Approach",
    "summary": "Automated interpretation of CT images-particularly localizing and describing\nabnormal findings across multi-plane and whole-body scans-remains a significant\nchallenge in clinical radiology. This work aims to address this challenge\nthrough four key contributions: (i) On taxonomy, we collaborate with senior\nradiologists to propose a comprehensive hierarchical classification system,\nwith 404 representative abnormal findings across all body regions; (ii) On\ndata, we contribute a dataset containing over 14.5K CT images from multiple\nplanes and all human body regions, and meticulously provide grounding\nannotations for over 19K abnormalities, each linked to the detailed description\nand cast into the taxonomy; (iii) On model development, we propose\nOminiAbnorm-CT, which can automatically ground and describe abnormal findings\non multi-plane and whole-body CT images based on text queries, while also\nallowing flexible interaction through visual prompts; (iv) On benchmarks, we\nestablish three representative evaluation tasks based on real clinical\nscenarios. Through extensive experiments, we show that OminiAbnorm-CT can\nsignificantly outperform existing methods on all the tasks and metrics.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03238.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6496eae78a7c70379a512e39",
      "avatarUrl": "/avatars/f5ab483ae93cc04b43e825dfd9440905.svg",
      "fullname": "Ziheng Zhao",
      "name": "zzh99",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.02587",
      "authors": [
        {
          "_id": "68428d18af4573dbb7cba864",
          "user": {
            "_id": "6526503e39fd3599e87c5c53",
            "avatarUrl": "/avatars/f45560d4f6bbb4f0dc06b27a46429726.svg",
            "isPro": false,
            "fullname": "Weiduo Yuan",
            "user": "Yewandou",
            "type": "user"
          },
          "name": "Weiduo Yuan",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-06T06:39:23.138Z",
          "hidden": false
        },
        {
          "_id": "68428d18af4573dbb7cba865",
          "name": "Jerry Li",
          "hidden": false
        },
        {
          "_id": "68428d18af4573dbb7cba866",
          "name": "Justin Yue",
          "hidden": false
        },
        {
          "_id": "68428d18af4573dbb7cba867",
          "name": "Divyank Shah",
          "hidden": false
        },
        {
          "_id": "68428d18af4573dbb7cba868",
          "name": "Konstantinos Karydis",
          "hidden": false
        },
        {
          "_id": "68428d18af4573dbb7cba869",
          "name": "Hang Qiu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T08:07:18.000Z",
      "submittedOnDailyAt": "2025-06-06T05:17:25.580Z",
      "title": "BEVCALIB: ギャンブルビーストビューによるギオメトリーガイドされたLiDAR-カメラの調整",
      "submittedOnDailyBy": {
        "_id": "6526503e39fd3599e87c5c53",
        "avatarUrl": "/avatars/f45560d4f6bbb4f0dc06b27a46429726.svg",
        "isPro": false,
        "fullname": "Weiduo Yuan",
        "user": "Yewandou",
        "type": "user"
      },
      "summary": "準確なLiDAR-カメラの補正は、自動運転システムとロボットシステムの多モーダル認識の融合に基礎的な重要性を持つ。傳統的な補正方法は、制御された環境で様々なデータを収集する必要があり、車両/ロボットの動作中に発生する変換変化を補正できない。この論文では、最初のモデルを提案し、バードの眼の視点（BEV）の特徴を使用して、ライダーカメラの補正を実行することを目指しています。これを実現するためには、カメラのBEV特徴とライダーのBEV特徴を別々に抽出し、共有のBEV特徴空間に融合させることです。BEV特徴からの構造的情報を最大限に活用するために、新しい特徴選択器を導入し、変換デコーダーで最も重要な特徴をフィルタリングし、メモリ消費を減らし、効率的な学習を可能にします。KITTI、NuScenesおよび我々のデータセットにおいて拡散的な評価を行い、BEVCALIBが新たな最先端となることを示します。さまざまなノイズ条件の下では、KITTIデータセットでは平均で（47.08%，82.32%）、NuScenesデータセットでは平均で（78.17%，68.29%）、文献で最も良い基準に比べて優位を採れます。開放ソース領域では、最良の再現可能な基準を10倍以上向上させます。我々のコードとデモ結果は、https://cisl.ucr.edu/BEVCalib から利用できます。",
      "upvotes": 1,
      "discussionId": "68428d1baf4573dbb7cba8f5",
      "projectPage": "https://cisl.ucr.edu/BEVCalib/",
      "githubRepo": "https://github.com/UCR-CISL/BEVCalib",
      "ai_summary": "BEVCALIB model uses bird's-eye view features for accurate LiDAR-camera calibration from raw data, demonstrating superior performance under various noise conditions.",
      "ai_keywords": [
        "bird's-eye view",
        "BEVCALIB",
        "camera BEV features",
        "LiDAR BEV features",
        "shared BEV feature space",
        "feature selector",
        "transformation decoder",
        "KITTI",
        "NuScenes",
        "reproducible baseline"
      ]
    },
    "publishedAt": "2025-06-03T04:07:18.000Z",
    "title": "BEVCALIB: LiDAR-Camera Calibration via Geometry-Guided Bird's-Eye View\n  Representations",
    "summary": "Accurate LiDAR-camera calibration is fundamental to fusing multi-modal\nperception in autonomous driving and robotic systems. Traditional calibration\nmethods require extensive data collection in controlled environments and cannot\ncompensate for the transformation changes during the vehicle/robot movement. In\nthis paper, we propose the first model that uses bird's-eye view (BEV) features\nto perform LiDAR camera calibration from raw data, termed BEVCALIB. To achieve\nthis, we extract camera BEV features and LiDAR BEV features separately and fuse\nthem into a shared BEV feature space. To fully utilize the geometric\ninformation from the BEV feature, we introduce a novel feature selector to\nfilter the most important features in the transformation decoder, which reduces\nmemory consumption and enables efficient training. Extensive evaluations on\nKITTI, NuScenes, and our own dataset demonstrate that BEVCALIB establishes a\nnew state of the art. Under various noise conditions, BEVCALIB outperforms the\nbest baseline in the literature by an average of (47.08%, 82.32%) on KITTI\ndataset, and (78.17%, 68.29%) on NuScenes dataset, in terms of (translation,\nrotation), respectively. In the open-source domain, it improves the best\nreproducible baseline by one order of magnitude. Our code and demo results are\navailable at https://cisl.ucr.edu/BEVCalib.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.02587.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6526503e39fd3599e87c5c53",
      "avatarUrl": "/avatars/f45560d4f6bbb4f0dc06b27a46429726.svg",
      "fullname": "Weiduo Yuan",
      "name": "Yewandou",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.04996",
      "authors": [
        {
          "_id": "68429955c49e8ad3f997b24a",
          "user": {
            "_id": "622dc11fe27c88667db093fc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1667052350862-622dc11fe27c88667db093fc.jpeg",
            "isPro": false,
            "fullname": "Edoardo Bianchi",
            "user": "EdBianchi",
            "type": "user"
          },
          "name": "Edoardo Bianchi",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-06T07:31:34.170Z",
          "hidden": false
        },
        {
          "_id": "68429955c49e8ad3f997b24b",
          "name": "Antonio Liotta",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-05T13:05:23.000Z",
      "submittedOnDailyAt": "2025-06-06T06:04:33.210Z",
      "title": "PATS: スキル認識に基づく時系列サンプリングでの多角度スポーツスキル評価",
      "submittedOnDailyBy": {
        "_id": "622dc11fe27c88667db093fc",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1667052350862-622dc11fe27c88667db093fc.jpeg",
        "isPro": false,
        "fullname": "Edoardo Bianchi",
        "user": "EdBianchi",
        "type": "user"
      },
      "summary": "自動化スポーツスキル評価は、エクスプレスかニヴァンスの行動パターンを区別するために、基本的な移動パターンを捉える必要がありますが、現在のビデオサンプリング方法は、スキル評価に必要な時系列連続性を破壊します。このために、我々は、多視点スキル評価において完全な基本的な移動を保持するための新しいサンプリング戦略を紹介します。これは、時系列的なセグメント内で完全な基本的な移動を保持するためのProficiency-Aware Temporal Sampling (PATS)です。PATSは、各分析される部分に重要な性能成分の完全な実行を確保することを目的として、ビデオを適応的にセグメントします。このプロセスは、情報カバーを最大化しながら時系列的な連続性を維持するために、複数のセグメントにわたします。EgoExo4Dベンチマーク上でSkillFormerを用いて評価したところ、PATSはすべての視点設定で最先端の精度を超えました（+0.65%〜+3.05%）、難しいドメインでは大幅な効果を示しました（ブロウディング:+26.22%、ミュージック:+2.39%、バスケットボール:+1.13%）。システム的な分析により、PATSは、高頻度サンプリングのための動的なスポーツから、順序的スキルのための細かいグランドセグメントへの多様なアクティビティ特徴に適応し、時系列的なサンプリングの適応的なアプローチとしての効果性を示し、実世界的なアプリケーションの自動化スキル評価に進むことを示しました。",
      "upvotes": 0,
      "discussionId": "68429956c49e8ad3f997b288",
      "ai_summary": "PATS is a novel temporal sampling method that enhances video analysis of athletic skills by ensuring complete movement patterns are captured, outperforming existing methods across various domains.",
      "ai_keywords": [
        "Proficiency-Aware Temporal Sampling",
        "PATS",
        "EgoExo4D benchmark",
        "SkillFormer",
        "temporal continuity",
        "temporal coherence",
        "fundamental movement patterns",
        "dynamic sports",
        "sequential skills"
      ]
    },
    "publishedAt": "2025-06-05T09:05:23.000Z",
    "title": "PATS: Proficiency-Aware Temporal Sampling for Multi-View Sports Skill\n  Assessment",
    "summary": "Automated sports skill assessment requires capturing fundamental movement\npatterns that distinguish expert from novice performance, yet current video\nsampling methods disrupt the temporal continuity essential for proficiency\nevaluation. To this end, we introduce Proficiency-Aware Temporal Sampling\n(PATS), a novel sampling strategy that preserves complete fundamental movements\nwithin continuous temporal segments for multi-view skill assessment. PATS\nadaptively segments videos to ensure each analyzed portion contains full\nexecution of critical performance components, repeating this process across\nmultiple segments to maximize information coverage while maintaining temporal\ncoherence. Evaluated on the EgoExo4D benchmark with SkillFormer, PATS surpasses\nthe state-of-the-art accuracy across all viewing configurations (+0.65% to\n+3.05%) and delivers substantial gains in challenging domains (+26.22%\nbouldering, +2.39% music, +1.13% basketball). Systematic analysis reveals that\nPATS successfully adapts to diverse activity characteristics-from\nhigh-frequency sampling for dynamic sports to fine-grained segmentation for\nsequential skills-demonstrating its effectiveness as an adaptive approach to\ntemporal sampling that advances automated skill assessment for real-world\napplications.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04996.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "622dc11fe27c88667db093fc",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1667052350862-622dc11fe27c88667db093fc.jpeg",
      "fullname": "Edoardo Bianchi",
      "name": "EdBianchi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  }
]